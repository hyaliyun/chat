import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,i,r){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-8ddfffa3"]]),A=JSON.parse('[{"question":"# File Management and Comparison Task Problem Statement You are tasked with developing a Python program that performs specific file and directory operations using various modules described in the provided documentation. You will write a function `manage_files_and_dirs` that: 1. Creates a temporary directory and within it creates two subdirectories `dir1` and `dir2`. 2. Creates a few text files with sample content in `dir1` and copies them to `dir2` using `shutil`. 3. Compares the contents of `dir1` and `dir2` using a method from the `filecmp` module, returning details of any differences. 4. Lists all the text files in `dir1` and `dir2` using `glob` and compares their properties using `os.path` (e.g., size and modification time). 5. Finally, cleans up by removing the temporary directory and all its contents. Input and Output Formats - This function does not take any inputs. - The function returns a dictionary with the following keys: - `\\"comparison_result\\"`: Result of the directory comparison as returned by `filecmp`. - `\\"files_properties\\"`: A list of dictionaries with properties of each file in `dir1` and `dir2`. Constraints - Ensure that the comparisons are detailed, including checks for file size and modification time. - Utilize the appropriate modules and methods as described in the documentation for operations. Example Here is an example dictionary format for the expected output: ```python { \\"comparison_result\\": { \\"diff_files\\": [], \\"left_only\\": [], \\"right_only\\": [], \\"common_files\\": [\\"example.txt\\", \\"sample.txt\\"] }, \\"files_properties\\": [ {\\"path\\": \\"dir1/example.txt\\", \\"size\\": 1024, \\"mtime\\": 1635600000}, {\\"path\\": \\"dir2/example.txt\\", \\"size\\": 1024, \\"mtime\\": 1635600000}, {\\"path\\": \\"dir1/sample.txt\\", \\"size\\": 2048, \\"mtime\\": 1635600100}, {\\"path\\": \\"dir2/sample.txt\\", \\"size\\": 2048, \\"mtime\\": 1635600100} ] } ``` Note - Ensure your implementation handles any possible exceptions or errors gracefully. - Use Python\'s built-in libraries and functions for file operations.","solution":"import os import tempfile import shutil from filecmp import dircmp from glob import glob import time def manage_files_and_dirs(): result = { \\"comparison_result\\": {}, \\"files_properties\\": [] } # Create a temporary directory with tempfile.TemporaryDirectory() as tempdir: dir1 = os.path.join(tempdir, \\"dir1\\") dir2 = os.path.join(tempdir, \\"dir2\\") os.makedirs(dir1) os.makedirs(dir2) # Create sample text files in dir1 sample_files = { \\"example.txt\\": \\"This is an example file.\\", \\"sample.txt\\": \\"This is a sample file.\\" } for filename, content in sample_files.items(): with open(os.path.join(dir1, filename), \'w\') as f: f.write(content) # Copy files from dir1 to dir2 using shutil for filename in os.listdir(dir1): full_file_name = os.path.join(dir1, filename) if os.path.isfile(full_file_name): shutil.copy(full_file_name, dir2) # Compare contents of dir1 and dir2 using filecmp comparison = dircmp(dir1, dir2) result[\\"comparison_result\\"] = { \\"diff_files\\": comparison.diff_files, \\"left_only\\": comparison.left_only, \\"right_only\\": comparison.right_only, \\"common_files\\": comparison.common_files } # List all text files in dir1 and dir2 using glob and compare their properties using os.path for d in [dir1, dir2]: for file_path in glob(os.path.join(d, \\"*.txt\\")): file_stat = os.stat(file_path) file_info = { \\"path\\": file_path, \\"size\\": file_stat.st_size, \\"mtime\\": file_stat.st_mtime } result[\\"files_properties\\"].append(file_info) return result"},{"question":"**Objective:** Demonstrate your understanding of integrating Python with C using the utility functions provided in this package. **Task:** You are required to implement a Python function `dynamic_library_loader` that dynamically loads a shared library (also known as a dynamic-link library on Windows or a shared object on Unix) at runtime. This function should utilize relevant utility functions for importing modules and handling their resources. **Requirements:** 1. **Function Signature:** ```python def dynamic_library_loader(library_path: str) -> bool: ``` 2. **Input:** - `library_path`: A string representing the file path to the shared library. 3. **Output:** - Return `True` if the library is successfully loaded and any critical function (e.g., `initialize`) within the library is callable. - Return `False` if there are any errors while loading the library or accessing the critical function. 4. **Constraints:** - You are required to handle exceptions and errors gracefully. - Use appropriate utility functions for importing modules from C, if any are relevant. - Ensure that you manage resources properly (e.g., release any handles or resources after usage). 5. **Performance:** - The function should be efficient and handle typical error scenarios with minimal overhead. **Example Usage:** ```python # Assuming \'mylibrary.so\' or \'mylibrary.dll\' exists at the specified path result = dynamic_library_loader(\\"/path/to/mylibrary.so\\") print(result) # Expected: True if the library and \'initialize\' function are properly loaded and callable, otherwise False ``` **Hint:** Refer to the utility functions mentioned in the documentation for loading modules and managing resources, especially under \\"Importing Modules\\" and \\"Process Control.\\"","solution":"import os import ctypes def dynamic_library_loader(library_path: str) -> bool: Dynamically loads a shared library and checks for a critical function. Args: - library_path (str): The file path to the shared library. Returns: - bool: True if the library is successfully loaded and the critical function is callable, otherwise False. if not os.path.exists(library_path): return False try: # Load the shared library loaded_library = ctypes.CDLL(library_path) # Check if the \'initialize\' function is present and callable initialize_func = getattr(loaded_library, \'initialize\', None) if initialize_func is None or not callable(initialize_func): return False return True except Exception as e: # Handle any exceptions that occur during loading and usage return False"},{"question":"<|Analysis Begin|> The provided documentation describes the `torch.cond` function in PyTorch, which introduces structured control flow based on dynamic and data-dependent conditions. This function allows users to define a conditional flow in their models, which can help adapt the model architecture based on the input data\'s value or shape. Some important points from the documentation include: - `torch.cond` is a prototype feature with limited support and no training capabilities yet. - It takes a predicate, two functions (`true_fn` and `false_fn`), and a set of operands. - Example usage demonstrates both shape and data-dependent control flows. - Usage involves both eager execution and exporting the model for transformations and deployment. Given this context, the `torch.cond` provides a rich basis for creating a challenging and comprehensive assessment question. The question could focus on implementing a function leveraging `torch.cond` to demonstrate understanding of conditional model behavior, dynamic shapes, and data preparedness. <|Analysis End|> <|Question Begin|> # Question: Implement Data-Dependent Conditional Behavior with `torch.cond` In this task, you are required to implement a PyTorch module that applies different transformations to the input tensor based on the sum of its elements. You must use the `torch.cond` function to achieve this. # Requirements: 1. Implement a PyTorch module `DataDependentTransformer`. 2. The module should use `torch.cond` to check if the sum of the input tensor elements is greater than a given threshold. 3. If the sum is greater than the threshold, you should apply one transformation to the tensor (e.g., compute the cosine of the tensor). 4. If the sum is less than or equal to the threshold, you should apply a different transformation (e.g., compute the sine of the tensor). # Function Signature ```python import torch class DataDependentTransformer(torch.nn.Module): def __init__(self, threshold: float): Initialize with a threshold value. :param threshold: The threshold to compare the sum of tensor elements. super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Apply a conditional transformation based on the sum of tensor elements. :param x: A 1D input tensor. :return: A tensor that has undergone a transformation based on the sum of its elements. def true_fn(x: torch.Tensor): # Define transformation for sum > threshold return x.cos() def false_fn(x: torch.Tensor): # Define transformation for sum <= threshold return x.sin() return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) ``` # Example Usage ```python # Example usage transformer = DataDependentTransformer(threshold=4.0) # Test with a tensor with a sum <= 4.0 input_tensor1 = torch.tensor([0.5, 0.5, 0.5]) output1 = transformer(input_tensor1) print(output1) # This should print the sine values of input_tensor1 # Test with a tensor with a sum > 4.0 input_tensor2 = torch.tensor([2.0, 2.0, 1.0]) output2 = transformer(input_tensor2) print(output2) # This should print the cosine values of input_tensor2 ``` # Constraints: 1. You may assume the input tensors will be 1-dimensional. 2. The transformation functions (`true_fn` and `false_fn`) should be simple trigonometric operations (cosine and sine functions, respectively). # Evaluation Criteria: 1. **Correctness**: The module should correctly apply the appropriate transformation based on the sum of the tensor. 2. **Code Quality**: The implementation should be clean, well-documented, and use PyTorch idioms correctly. 3. **Performance**: The implementation should be efficient and make proper use of `torch.cond`. Good luck!","solution":"import torch class DataDependentTransformer(torch.nn.Module): def __init__(self, threshold: float): Initialize with a threshold value. :param threshold: The threshold to compare the sum of tensor elements. super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Apply a conditional transformation based on the sum of tensor elements. :param x: A 1D input tensor. :return: A tensor that has undergone a transformation based on the sum of its elements. def true_fn(x: torch.Tensor): # Define transformation for sum > threshold return x.cos() def false_fn(x: torch.Tensor): # Define transformation for sum <= threshold return x.sin() return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,))"},{"question":"**Objective**: Implement a Python function that emulates a part of the system functionality using Python\'s standard libraries. Demonstrate understanding of file system operations, signal handling, and custom error handling. Question Write a Python function `interactive_file_check(filename: str) -> bool` that takes a filename, checks if it is an interactive file, and returns `True` if it is interactive and `False` otherwise. Mimic the behavior of `Py_FdIsInteractive(FILE *fp, const char *filename)` using Python\'s sys and os modules. The function should: 1. Check if the file represented by the filename is an interactive file. 2. Consider a file interactive if its file descriptor belongs to a terminal (tty) or if the filename is `\'<stdin>\'` or `\'???\'`. 3. Implement custom error handling to raise a `TypeError` if the filename is not a string. **Constraints**: - You are not allowed to use the C API directly. - Your implementation should work with standard Python modules. ```python def interactive_file_check(filename: str) -> bool: Check if the given filename represents an interactive file. Parameters: filename (str): The name of the file to be checked. Returns: bool: True if the file is interactive, otherwise False. Raises: TypeError: If the filename is not a string. # Implementation goes here ``` **Example Usage**: ```python interactive_file_check(\'/dev/tty\') # Should return True if the file descriptor is a terminal interactive_file_check(\'<stdin>\') # Should return True interactive_file_check(\'non_interactive_file.txt\') # Should return False ``` **Hints**: - Use `os.isatty()` to check if a file descriptor is a terminal. - Use the `sys` module to handle files like `stdin`. Performance Requirements - The function should have a time complexity of O(1) for checking the interactivity of the file based on the provided conditions. Additional Note Make sure to handle edge cases and invalid inputs appropriately.","solution":"import os import sys def interactive_file_check(filename: str) -> bool: Check if the given filename represents an interactive file. Parameters: filename (str): The name of the file to be checked. Returns: bool: True if the file is interactive, otherwise False. Raises: TypeError: If the filename is not a string. if not isinstance(filename, str): raise TypeError(\\"The filename must be a string.\\") if filename in (\'<stdin>\', \'???\'): return True try: with open(filename, \'r\') as f: return os.isatty(f.fileno()) except (OSError, IOError): return False"},{"question":"Asynchronous Web Scraper You are required to write an asynchronous web scraper that can fetch HTML content from multiple URLs concurrently and process the content to extract all the hyperlink references (`<a href=\\"\\">` tags). Requirements 1. **Input:** - A list of URLs. Each URL is a string. - Example: `[\\"http://example.com\\", \\"http://example.org\\"]` 2. **Output:** - A dictionary where each key is a URL from the input list, and each value is a list of all hyperlink references found on that page. - Example: ```python { \\"http://example.com\\": [\\"http://example.com/link1\\", \\"http://example.com/link2\\"], \\"http://example.org\\": [\\"http://example.org/sample\\"] } ``` 3. **Constraints:** - Minimum of 1 URL and a maximum of 20 URLs in the input list. - You must use the `asyncio` module to handle the asynchronous fetching of HTML content. - Use the `aiohttp` library for making HTTP requests. - Extract hyperlinks using `BeautifulSoup` from the `bs4` library. - You should handle potential exceptions gracefully (e.g., HTTP errors, invalid URLs). 4. **Performance Requirements:** - The program should maximize concurrency to reduce the total time for fetching all URLs. Example Usage ```python import asyncio # List of URLs to be scraped urls = [ \\"http://example.com\\", \\"http://example.org\\" ] # Calling the main coroutine async def main(): result = await fetch_all_links(urls) print(result) # Start the event loop asyncio.run(main()) ``` Function Signature ```python import asyncio from typing import List, Dict async def fetch_all_links(urls: List[str]) -> Dict[str, List[str]]: # Your code here # pass ``` Steps 1. **Create an asynchronous function** `fetch_html(url: str) -> str` that fetches and returns HTML content from a given URL. 2. **Create an asynchronous function** `extract_links(html: str) -> List[str]` that extracts all hyperlink references from the given HTML content using `BeautifulSoup`. 3. **Implement the `fetch_all_links` function** which concurrently fetches HTML content for all URLs and then processes each page to extract the links concurrently. 4. **Handle exceptions** appropriately by ensuring that an error for one URL does not stop the processing for the remaining URLs. Hints - You might need to use `asyncio.gather` or `asyncio.create_task` to manage multiple asynchronous tasks concurrently. - Use `aiohttp.ClientSession` for making HTTP requests. Good luck!","solution":"import asyncio from typing import List, Dict import aiohttp from bs4 import BeautifulSoup async def fetch_html(url: str) -> str: Fetches the HTML content of the given URL asynchronously. async with aiohttp.ClientSession() as session: try: async with session.get(url) as response: response.raise_for_status() return await response.text() except Exception as e: print(f\\"Error fetching {url}: {e}\\") return \\"\\" def extract_links(html: str) -> List[str]: Extracts all hyperlink references from the given HTML content. soup = BeautifulSoup(html, \'html.parser\') return [a.get(\'href\') for a in soup.find_all(\'a\', href=True)] async def fetch_all_links(urls: List[str]) -> Dict[str, List[str]]: Concurrently fetch HTML content for all URLs and extract hyperlinks. async def fetch_and_extract(url): html = await fetch_html(url) return url, extract_links(html) tasks = [fetch_and_extract(url) for url in urls] results = await asyncio.gather(*tasks) return {url: links for url, links in results}"},{"question":"Develop a function that utilizes the `html` module to process a list of strings. The function must: 1. Escape any potentially unsafe HTML characters in each string. 2. Unescape any previously escaped HTML characters in each string. # Function Signature ```python def process_html_strings(strings: List[str]) -> List[Tuple[str, str]]: pass ``` # Input - `strings`: A list of strings. Each string may contain characters that need to be HTML-escaped or already contain HTML-escaped sequences. # Output - Return a list of tuples. Each tuple corresponds to an input string and contains two elements: 1. The string with HTML characters escaped. 2. The string with HTML escape sequences converted back to their original characters. # Constraints - The length of `strings` list will not exceed 1000. - Each string\'s length will not exceed 1000 characters. # Example ```python input_strings = [\\"<div>Some text & more text</div>\\", \\"5 > 3 & 3 < 5\\", \\"A quote: &quot;Hello&quot;\\"] output = process_html_strings(input_strings) # Expected output: # [ # (\\"&lt;div&gt;Some text &amp; more text&lt;/div&gt;\\", \\"<div>Some text & more text</div>\\"), # (\\"5 &gt; 3 &amp; 3 &lt; 5\\", \\"5 > 3 & 3 < 5\\"), # (\\"A quote: &amp;quot;Hello&amp;quot;\\", \\"A quote: \\"Hello\\"\\") # ] ``` # Performance Requirements - Aim for an optimal solution that processes each string in linear time relative to its length. # Note - Use `html.escape` and `html.unescape` functions for the respective operations.","solution":"from typing import List, Tuple import html def process_html_strings(strings: List[str]) -> List[Tuple[str, str]]: Process a list of strings to escape HTML characters and unescape existing HTML entities. Args: - strings (List[str]): A list of strings to process. Returns: - List[Tuple[str, str]]: A list of tuples, where the first element is the escaped string and the second element is the unescaped string. result = [] for s in strings: escaped = html.escape(s) unescaped = html.unescape(s) result.append((escaped, unescaped)) return result"},{"question":"# WSGI Application Development You are tasked with building a small WSGI application that serves dynamic content, handles different HTTP methods, and uses the `wsgiref` utilities to manage WSGI environments and headers. Requirements 1. **Application Overview:** - Create a WSGI application callable named `blog_app` that accepts two parameters, `environ` and `start_response`. - Your application should support both `GET` and `POST` methods. - For `GET` requests, it should return a simple HTML form. - For `POST` requests, it should process the form data and return a confirmation message. 2. **Environment Preparation:** - Use `wsgiref.util.setup_testing_defaults` to ensure your `environ` has all necessary WSGI parameters. - Use `wsgiref.util.request_uri` to construct the full request URI. 3. **Response Headers:** - Use the `wsgiref.headers.Headers` class to manage your response headers. 4. **Validation:** - Wrap your `blog_app` application with `wsgiref.validate.validator` to ensure it conforms to the WSGI specification. 5. **Server Initialization:** - Create a simple HTTP server using `wsgiref.simple_server.make_server` to serve your WSGI application at `localhost` on port `8000`. Implementation Details - Ensure your code includes proper handling of different HTTP methods. - Do not use any other external libraries; use only Python standard libraries and `wsgiref`. - Handle edge cases such as missing form data gracefully and return appropriate HTTP status codes. - Be sure to include adequate comments in your code to explain your logic. Here is a skeleton structure to get you started: ```python from wsgiref.util import setup_testing_defaults, request_uri from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def blog_app(environ, start_response): setup_testing_defaults(environ) method = environ[\'REQUEST_METHOD\'] headers = Headers([]) if method == \'GET\': status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/html\') start_response(status, headers.items()) body = \'\'\' <html> <body> <form method=\\"post\\" action=\\"/\\"> <label for=\\"name\\">Name:</label><br> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"><br> <label for=\\"content\\">Content:</label><br> <textarea id=\\"content\\" name=\\"content\\"></textarea><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> \'\'\' return [body.encode(\'utf-8\')] elif method == \'POST\': # Process form data and return a success message pass # Sample implementation details: # 1. Read POST data from environ[\'wsgi.input\'] # 2. Parse the form data # 3. Set appropriate headers # 4. Generate and return the response body else: status = \'405 Method Not Allowed\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Method Not Allowed\'] # Wrap the application with a validator validated_app = validator(blog_app) if __name__ == \'__main__\': httpd = make_server(\'\', 8000, validated_app) print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Implement the POST request handling logic and ensure your application works correctly as prescribed. Submission Submit the final Python script that defines the WSGI application and runs the server. Include a README file with instructions on how to run your server and test different functionalities.","solution":"from cgi import FieldStorage from wsgiref.util import setup_testing_defaults, request_uri from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def blog_app(environ, start_response): setup_testing_defaults(environ) method = environ[\'REQUEST_METHOD\'] headers = Headers([]) if method == \'GET\': status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/html\') start_response(status, headers.items()) body = \'\'\' <html> <body> <form method=\\"post\\" action=\\"/\\"> <label for=\\"name\\">Name:</label><br> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"><br> <label for=\\"content\\">Content:</label><br> <textarea id=\\"content\\" name=\\"content\\"></textarea><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> \'\'\' return [body.encode(\'utf-8\')] elif method == \'POST\': # Read the POST data form = FieldStorage(fp=environ[\'wsgi.input\'], environ=environ, keep_blank_values=True) name = form.getvalue(\'name\', \'\') content = form.getvalue(\'content\', \'\') # Form response message status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/html\') start_response(status, headers.items()) body = f\'\'\' <html> <body> <h1>Form Submitted</h1> <p>Name: {name}</p> <p>Content: {content}</p> </body> </html> \'\'\' return [body.encode(\'utf-8\')] else: status = \'405 Method Not Allowed\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Method Not Allowed\'] # Wrap the application with a validator validated_app = validator(blog_app) if __name__ == \'__main__\': httpd = make_server(\'\', 8000, validated_app) print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# Quoted-Printable Codec Implementation Your task is to implement quoted-printable encoding and decoding functions similar to those provided by the `quopri` module. You need to ensure that your functions adhere to the quoted-printable encoding rules as defined in RFC 1521 and RFC 1522. You will implement the following two functions: 1. `qp_encode(input_bytes: bytes, quotetabs: bool = False, header: bool = False) -> bytes` - Encodes the input bytes into quoted-printable form. - Parameters: - `input_bytes`: A bytes object containing the data to encode. - `quotetabs`: A boolean flag indicating whether to encode embedded spaces and tabs (default is `False`). - `header`: A boolean flag indicating whether to encode spaces as underscores (default is `False`). - Returns: - A bytes object containing the quoted-printable encoded data. 2. `qp_decode(input_bytes: bytes, header: bool = False) -> bytes` - Decodes the quoted-printable encoded bytes back into their original form. - Parameters: - `input_bytes`: A bytes object containing quoted-printable encoded data. - `header`: A boolean flag indicating whether to decode underscores as spaces (default is `False`). - Returns: - A bytes object containing the decoded data. # Requirements - Encode spaces and tabs correctly according to the `quotetabs` parameter. - Always encode spaces and tabs at the end of lines. - Encode non-printable characters using the =XX format, where XX is the character\'s ASCII hex value. - If `header` is `True`, encode spaces as underscores during encoding, and decode underscores as spaces during decoding. # Example Usage ```python # Example input data = b\\"hello worldnThis is a testtdataencodedn\\" # Encoding encoded_data = qp_encode(data) print(encoded_data) # Decoding decoded_data = qp_decode(encoded_data) print(decoded_data) # Example with header encoded_header = qp_encode(data, header=True) print(encoded_header) decoded_header = qp_decode(encoded_header, header=True) print(decoded_header) ``` # Constraints - You may not use the `quopri` module directly in your implementation. - Your solution should be efficient and handle typical MIME data sizes gracefully. - Assume input bytes are in the range of printable ASCII characters (0x20 to 0x7E), plus newline (`n`) and carriage return (`r`). # Performance Requirements - Expected to handle input sizes up to 2^20 bytes efficiently. Implement these functions ensuring adherence to the described encoding and decoding rules.","solution":"def qp_encode(input_bytes: bytes, quotetabs: bool = False, header: bool = False) -> bytes: def qp_enc_char(char): return b\\"=%02X\\" % char encoded = bytearray() for char in input_bytes: if header and char == 0x20: encoded.extend(b\'_\') elif 0x20 <= char <= 0x7E and char != 0x3D and (char not in (0x09, 0x20) or not quotetabs): encoded.append(char) else: if char in (0x09, 0x20): encoded.extend(qp_enc_char(char)) else: encoded.extend(qp_enc_char(char)) return bytes(encoded) def qp_decode(input_bytes: bytes, header: bool = False) -> bytes: def qp_dec_char(hex_chars): return bytes([int(hex_chars, 16)]) decoded = bytearray() i = 0 while i < len(input_bytes): char = input_bytes[i] if header and char == 0x5F: decoded.append(0x20) elif char == 0x3D and i + 2 < len(input_bytes): hex_chars = input_bytes[i+1:i+3] decoded.extend(qp_dec_char(hex_chars)) i += 2 else: decoded.append(char) i += 1 return bytes(decoded)"},{"question":"# Pandas Windowing Operations Assessment Objective To test the understanding and application of pandas windowing operations, including rolling, expanding, exponentially weighted windows, and custom window functions. Problem Statement You are provided with a dataset containing daily stock prices for multiple companies over a one-year period. You need to perform various windowing operations to analyze this data and extract meaningful insights. Input The input will be a CSV file `stock_prices.csv` containing the following columns: - `Date`: The date of the stock price observation (in the format YYYY-MM-DD). - `Company`: The name of the company. - `Open`: The opening price of the stock. - `Close`: The closing price of the stock. - `High`: The highest price of the stock for that day. - `Low`: The lowest price of the stock for that day. - `Volume`: The number of shares traded on that day. Tasks 1. **Rolling Window**: Calculate a 7-day rolling mean of the `Close` prices for each company. Your result should include the rolling mean along with the original data. 2. **Expanding Window**: Compute the expanding window maximum of the `High` prices for each company. 3. **Exponentially Weighted Window**: Calculate the exponentially weighted moving average (EWMA) of the `Close` prices for each company with a span of 10 days. 4. **Custom Window**: Implement a custom window function using `BaseIndexer` that provides an expanding window if the `Volume` traded is above a threshold (e.g., 1,000,000 shares) and a rolling window of 3 days otherwise. Calculate the sum of `Volume` for this custom window configuration. 5. **Groupby and Windowing**: Group the data by `Company` and calculate the 5-day rolling correlation between `Open` and `Close` prices for each company. Output - A DataFrame with the original data and the 7-day rolling mean of `Close` prices. - A DataFrame with the original data and the expanding window maximum of `High` prices. - A DataFrame with the original data and the EWMA of `Close` prices. - A DataFrame with the original data and the sum of `Volume` for the custom window function. - A DataFrame with the original data and the 5-day rolling correlation between `Open` and `Close` prices for each company. Constraints - Ensure the rolling and expanding operations respect the `Company` groups. - Handle any missing data appropriately by using the default `min_periods` parameter. - Assume valid input data with proper formatting. Performance Requirements - The operations should be performed efficiently to handle large datasets with potentially millions of rows. - Utilize pandas\' optimized functions and Numba engine where necessary to achieve better performance. Example Below is an example of how the functions should be implemented. ```python import pandas as pd import numpy as np from pandas.api.indexers import BaseIndexer import numba # Load data df = pd.read_csv(\'stock_prices.csv\', parse_dates=[\'Date\']) df.set_index(\'Date\', inplace=True) # 1. Rolling Window: 7-day Rolling Mean of Close Prices df[\'Rolling_Close_Mean\'] = df.groupby(\'Company\')[\'Close\'].rolling(window=7).mean().reset_index(level=0, drop=True) # 2. Expanding Window: Expanding Max of High Prices df[\'Expanding_High_Max\'] = df.groupby(\'Company\')[\'High\'].expanding().max().reset_index(level=0, drop=True) # 3. Exponentially Weighted Window: EWMA of Close Prices with Span of 10 df[\'EWMA_Close\'] = df.groupby(\'Company\')[\'Close\'].ewm(span=10).mean().reset_index(level=0, drop=True) # 4. Custom Window Function: Expanding or 3-Day Rolling Sum of Volume class CustomIndexer(BaseIndexer): def get_window_bounds(self, num_values, min_periods, center, closed, step): start = np.zeros(num_values, dtype=np.int64) end = np.arange(1, num_values + 1, dtype=np.int64) for i in range(num_values): if self.df.iloc[i][\'Volume\'] > 1000000: start[i] = 0 else: start[i] = max(0, i - 2) return start, end df[\'Custom_Volume_Sum\'] = df.groupby(\'Company\').apply( lambda x: x.rolling(CustomIndexer(window_size=3, df=x)).Volume.sum()).reset_index(level=0, drop=True) # 5. Groupby and Windowing: 5-Day Rolling Correlation between Open and Close Prices df[\'Rolling_Open_Close_Corr\'] = df.groupby(\'Company\').apply( lambda x: x[\'Open\'].rolling(window=5).corr(x[\'Close\'])).reset_index(level=0, drop=True) # Display Result (Example) print(df.head()) ``` Make sure to write clean, modular code and include any necessary imports. The functions should be tested on the provided sample dataset to ensure correctness.","solution":"import pandas as pd import numpy as np from pandas.api.indexers import BaseIndexer def load_data(filepath): df = pd.read_csv(filepath, parse_dates=[\'Date\']) df.set_index(\'Date\', inplace=True) return df def calculate_7day_rolling_mean(df): df[\'7Day_Rolling_Close_Mean\'] = df.groupby(\'Company\')[\'Close\'].rolling(window=7).mean().reset_index(level=0, drop=True) return df def calculate_expanding_max(df): df[\'Expanding_High_Max\'] = df.groupby(\'Company\')[\'High\'].expanding().max().reset_index(level=0, drop=True) return df def calculate_ewma(df): df[\'EWMA_Close\'] = df.groupby(\'Company\')[\'Close\'].ewm(span=10).mean().reset_index(level=0, drop=True) return df class CustomIndexer(BaseIndexer): def __init__(self, window_size, df): self.window_size = window_size self.df = df def get_window_bounds(self, num_values, min_periods, center, closed, step): start = np.zeros(num_values, dtype=np.int64) end = np.arange(1, num_values + 1, dtype=np.int64) for i in range(num_values): if self.df.iloc[i][\'Volume\'] > 1000000: start[i] = 0 else: start[i] = max(0, i - self.window_size + 1) return start, end def calculate_custom_volume_sum(df): df[\'Custom_Volume_Sum\'] = df.groupby(\'Company\').apply( lambda x: x.rolling(CustomIndexer(window_size=3, df=x)).Volume.sum()).reset_index(level=0, drop=True) return df def calculate_5day_rolling_corr(df): df[\'5Day_Rolling_Open_Close_Corr\'] = df.groupby(\'Company\').apply( lambda x: x[\'Open\'].rolling(window=5).corr(x[\'Close\'])).reset_index(level=0, drop=True) return df def process_stock_data(filepath): df = load_data(filepath) df = calculate_7day_rolling_mean(df) df = calculate_expanding_max(df) df = calculate_ewma(df) df = calculate_custom_volume_sum(df) df = calculate_5day_rolling_corr(df) return df"},{"question":"# PyTorch Distributed RPC: Implementing and Testing RRef Protocol Objective: To assess your understanding of PyTorch\'s distributed RPC framework, specifically the Remote Reference Protocol (RRef). You will need to implement a function to simulate creating, sharing, and managing RRefs in a distributed environment. Task: Implement a function `simulate_rref_protocol` that will create a distributed environment with multiple workers. The function should: 1. Initialize a specified number of workers (`num_workers`). 2. Create an `OwnerRRef` on a specified worker. 3. Share this `OwnerRRef` with other user workers as specified in the input list. 4. Handle reference counting and ensure proper deletion notifications as per Guarantees G1 and G2. 5. Simulate remote procedure calls (RPCs) in various scenarios (owner to user, user to user, user to owner). Function Signature: ```python import torch import torch.distributed.rpc as rpc from typing import List def simulate_rref_protocol(num_workers: int, owner_index: int, user_indices: List[int]) -> None: pass ``` Constraints: - Workers are indexed from `0` to `num_workers - 1`. - `owner_index` represents the worker holding the `OwnerRRef`. - `user_indices` is a list of indices representing workers that will hold `UserRRefs`. - Ensure that the owner receives appropriate notifications for `UserRRef` deletions (G1). - Ensure the parent `UserRRef` is not deleted until the child `UserRRef` is confirmed by the owner (G2). - Simulate failure scenarios where messages may arrive out-of-order and ensure robustness. Example: ```python # Example test case def example_test(): num_workers = 3 owner_index = 0 user_indices = [1, 2] simulate_rref_protocol(num_workers, owner_index, user_indices) example_test() ``` Notes: 1. Use `torch.distributed.rpc` to handle RPC calls and RRefs. 2. You may need to use synchronization primitives to ensure proper order and notifications. 3. Design the test case to validate the Guarantees G1 and G2 through logs or assertions. 4. Clean up the RPC framework on completion to avoid resource leakage. Additional Resources: Refer to the provided documentation on Remote Reference Protocol to understand the underlying mechanics and requirements for the implementation.","solution":"import torch import torch.distributed.rpc as rpc from typing import List import time def run_worker(rank, world_size, owner_index, user_indices): rpc.init_rpc( name=f\\"worker{rank}\\", rank=rank, world_size=world_size ) if rank == owner_index: rref = rpc.RRef(\\"This is an RRef string owned by worker {owner_index}\\") for idx in user_indices: rpc.remote(f\\"worker{idx}\\", use_rref, args=(rref,)) # Simulating owner work time.sleep(2) else: # Simulating users\' work time.sleep(2) rpc.shutdown() def use_rref(owner_rref): print(\\"Received OwnerRRef on another worker\\") return owner_rref.to_here() def simulate_rref_protocol(num_workers: int, owner_index: int, user_indices: List[int]) -> None: world_size = num_workers workers = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=run_worker, args=(rank, world_size, owner_index, user_indices)) p.start() workers.append(p) for p in workers: p.join()"},{"question":"Coding Assessment Question # Objective To gauge your understanding of email parsing functionalities in Python, specifically using the `email.parser` module\'s `Parser` and `FeedParser` APIs, including handling different types of email messages. # Question **Scenario:** You have been tasked with developing a tool that processes received email messages from different sources. The tool needs to handle both compliant and non-compliant email messages, distinguishing between simple text messages, MIME messages, and messages with specific defects. Additionally, the processing should support both complete message content in memory and incremental reading from streams. # Tasks 1. **Function 1**: `parse_email_message_from_file(file_path)` - **Input**: `file_path` (string) — Path to a text file containing the complete email message. - **Output**: Dictionary with the following keys: - `headers` (dict): A dictionary of email headers. - `body` (str): The body of the email message. - `is_multipart` (bool): Whether the email is a multipart message. - `defects` (list): Any defects found in the email message. - **Instructions**: - Use the `BytesParser` class to parse the email message from the file. - Extract headers, body, multipart status, and defects from the parsed email message. 2. **Function 2**: `incremental_email_parser(streaming_source)` - **Input**: `streaming_source` (iterable) — An iterable that yields parts of the email message as bytes. - **Output**: Dictionary with the same structure as defined in Function 1. - **Instructions**: - Use the `BytesFeedParser` class to parse the email message incrementally. - Feed the parser with parts of the message from the `streaming_source`. - Close the parser once the data is fully fed and extract required information as in Function 1. # Constraints - You must handle both MIME and non-MIME messages. - Email messages may not always be standards-compliant; handle defects gracefully. - Ensure the solution works efficiently for large email messages. # Example Usage ```python # Example usage for parsing from file result = parse_email_message_from_file(\'path_to_email_file.eml\') print(result) # Example usage for incremental parsing streaming_source = [b\'part of email bytes 1\', b\'part of email bytes 2\', ...] result = incremental_email_parser(streaming_source) print(result) ``` Please implement both functions and ensure they handle different types of email messages effectively.","solution":"from email.parser import BytesParser, BytesFeedParser from email.policy import default def parse_email_message_from_file(file_path): Parses an email message from a file and returns its components. Args: file_path (string): Path to a text file containing the complete email message. Returns: dict: Contains \'headers\', \'body\', \'is_multipart\', and \'defects\'. with open(file_path, \\"rb\\") as f: msg = BytesParser(policy=default).parse(f) headers = dict(msg.items()) body = get_email_body(msg) is_multipart = msg.is_multipart() defects = [str(d) for d in msg.defects] return { \\"headers\\": headers, \\"body\\": body, \\"is_multipart\\": is_multipart, \\"defects\\": defects } def get_email_body(msg): Helper function to extract the body from an email message. Args: msg: Email message object. Returns: str: Body of the email message. if msg.is_multipart(): # If multipart, concatenate parts return \\"nn\\".join([part.get_payload(decode=True).decode(\'utf-8\', errors=\'replace\') for part in msg.iter_parts() if part.get_content_type() == \'text/plain\']) else: # If not multipart, get plain payload return msg.get_payload(decode=True).decode(\'utf-8\', errors=\'replace\') def incremental_email_parser(streaming_source): Parses an email message incrementally and returns its components. Args: streaming_source (iterable): An iterable that yields parts of the email message as bytes. Returns: dict: Contains \'headers\', \'body\', \'is_multipart\', and \'defects\'. parser = BytesFeedParser(policy=default) for chunk in streaming_source: parser.feed(chunk) msg = parser.close() headers = dict(msg.items()) body = get_email_body(msg) is_multipart = msg.is_multipart() defects = [str(d) for d in msg.defects] return { \\"headers\\": headers, \\"body\\": body, \\"is_multipart\\": is_multipart, \\"defects\\": defects }"},{"question":"You are given a list of files in a source tree and a set of commands that dictate which files should be included or excluded in the distribution package. Your task is to write a function that processes these commands and returns the final list of files that should be included in the distribution. # Function Signature ```python def process_manifest(files: list, commands: list) -> list: Process the manifest commands to determine the final set of files to include in the distribution. Parameters: - files (list): A list of file paths (strings) in the source tree. - commands (list): A list of manifest commands (strings). Returns: - list: A list of file paths (strings) that should be included in the distribution. ``` # Input - `files`: A list of strings representing file paths in the source tree. Example: ```python files = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"data/input.txt\\", \\"scripts/install.sh\\" ] ``` - `commands`: A list of strings representing the manifest commands. Example: ```python commands = [ \\"include src/*.py\\", \\"exclude src/module2.py\\", \\"recursive-include data *.txt\\", \\"prune scripts\\" ] ``` # Output - A list of strings representing the file paths that should be included in the distribution after processing all commands. # Constraints - The commands in the list can be `include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, `graft`. - The filenames and directory names can contain Unicode characters and have varying lengths. - The list of commands will be non-empty. - Performance should be considered if the number of files and commands is large. # Example ```python files = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"data/input.txt\\", \\"scripts/install.sh\\" ] commands = [ \\"include src/*.py\\", \\"exclude src/module2.py\\", \\"recursive-include data *.txt\\", \\"prune scripts\\" ] print(process_manifest(files, commands)) ``` Output: ```python [\'src/module1.py\', \'data/input.txt\'] ``` Your implementation should correctly parse and apply these commands, returning the final list of files that should be included in the distribution package.","solution":"import fnmatch import os def process_manifest(files, commands): included_files = set() def match_pattern(files, pattern): return fnmatch.filter(files, pattern) for command in commands: parts = command.split() cmd = parts[0] if cmd == \\"include\\": pattern = parts[1] for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \\"exclude\\": pattern = parts[1] for file in files: if fnmatch.fnmatch(file, pattern): included_files.discard(file) elif cmd == \\"recursive-include\\": dir_pattern = parts[1] file_pattern = parts[2] for file in files: if file.startswith(dir_pattern) and fnmatch.fnmatch(os.path.basename(file), file_pattern): included_files.add(file) elif cmd == \\"recursive-exclude\\": dir_pattern = parts[1] file_pattern = parts[2] for file in files: if file.startswith(dir_pattern) and fnmatch.fnmatch(os.path.basename(file), file_pattern): included_files.discard(file) elif cmd == \\"global-include\\": pattern = parts[1] for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \\"global-exclude\\": pattern = parts[1] for file in files: if fnmatch.fnmatch(file, pattern): included_files.discard(file) elif cmd == \\"prune\\": dir_pattern = parts[1] for file in files: if file.startswith(dir_pattern): included_files.discard(file) elif cmd == \\"graft\\": dir_pattern = parts[1] for file in files: if file.startswith(dir_pattern): included_files.add(file) return sorted(included_files)"},{"question":"Objective Implement a function that performs element-wise operations using special functions from the `torch.special` module. Your function will accept a tensor and apply specific special functions to its elements based on given conditions. Problem Statement Write a function `apply_special_functions(tensor: torch.Tensor) -> torch.Tensor` that takes a 1-dimensional PyTorch tensor of floating-point numbers and returns a new tensor where the following operations have been applied element-wise based on specific conditions: - For elements `x < 0`, apply the `erf` function. - For elements `0 <= x < 1`, apply the `log1p` function. - For elements `x >= 1`, apply the `expit` function. Use the respective functions from the `torch.special` module: - `torch.special.erf` - `torch.special.log1p` - `torch.special.expit` Input Format - `tensor` (torch.Tensor): A 1-dimensional tensor of type `torch.float32` that contains the input data. Output Format - Returns a new tensor (torch.Tensor) with the same shape as the input tensor where the specified operations have been applied. Constraints - You may assume the tensor will always be of type `torch.float32` and 1-dimensional. - The tensor will have at least one element and no more than 10^6 elements. Example ```python import torch import torch.special def apply_special_functions(tensor: torch.Tensor) -> torch.Tensor: # Your implementation here # Example usage: input_tensor = torch.tensor([-1.5, 0.5, 1.5], dtype=torch.float32) result = apply_special_functions(input_tensor) print(result) ``` Given the input tensor `[-1.5, 0.5, 1.5]`, the expected output is: Using `torch.special.erf` for `-1.5` (since it\'s < 0), Using `torch.special.log1p` for `0.5` (since `0 <= 0.5 < 1`), Using `torch.special.expit` for `1.5` (since it\'s >= 1): The output should be: ``` tensor([-0.9661, 0.4055, 0.8176]) ``` Notes - Ensure that your implementation uses in-place and efficient tensor operations where applicable to achieve good performance. - Torch and its submodules should be properly imported and utilized in your implementation.","solution":"import torch import torch.special def apply_special_functions(tensor: torch.Tensor) -> torch.Tensor: Applies special functions to elements of the tensor based on specified conditions. For elements x < 0, applies torch.special.erf. For elements 0 <= x < 1, applies torch.special.log1p. For elements x >= 1, applies torch.special.expit. Args: tensor (torch.Tensor): A 1-dimensional tensor of type torch.float32 Returns: torch.Tensor: A new tensor with special functions applied element-wise based on conditions. # Apply special functions erf_part = torch.special.erf(tensor[tensor < 0]) log1p_part = torch.special.log1p(tensor[(tensor >= 0) & (tensor < 1)]) expit_part = torch.special.expit(tensor[tensor >= 1]) # Create an output tensor and fill in the transformed values output_tensor = tensor.clone() output_tensor[tensor < 0] = erf_part output_tensor[(tensor >= 0) & (tensor < 1)] = log1p_part output_tensor[tensor >= 1] = expit_part return output_tensor"},{"question":"**Objective:** Demonstrate proficiency in data compression and decompression using Python\'s `zlib` module. **Problem Statement:** You are given a large string of text data that you need to compress to save space and then decompress back to its original form. Write a function `compress_and_decompress` that takes a string of text as input, compresses it using the `zlib` module at the highest compression level, and then decompresses it back to ensure the data integrity is maintained. **Function Signature:** ```python def compress_and_decompress(text: str) -> str: pass ``` **Input:** - `text` (str): A large string of text data (1 <= len(text) <= 1,000,000). **Output:** - A string which is the decompressed version of the compressed data and should match the original input text. **Constraints:** - Use the highest level of compression (level 9). - Ensure that the decompressed text is exactly the same as the input text. **Example:** ```python input_text = \\"This is a test string. \\" * 1000 output_text = compress_and_decompress(input_text) assert input_text == output_text ``` **Instructions:** 1. Use `zlib.compress` to compress the input text. 2. Use `zlib.decompress` to decompress the compressed data back to its original form. 3. Ensure the decompressed text matches the input text. Hints: - Use `text.encode(\'utf-8\')` before compression to handle string to byte conversion. - Use `compressed_data.decode(\'utf-8\')` after decompression if needed to convert bytes back to string. You can refer to the zlib manual for details on the parameters: http://www.zlib.net/manual.html.","solution":"import zlib def compress_and_decompress(text: str) -> str: Compresses the given text using zlib at the highest compression level and then decompresses it back to its original form to ensure data integrity. Args: text (str): The text to be compressed and decompressed. Returns: str: The decompressed text, which should match the original input text. # Encode the text to bytes text_bytes = text.encode(\'utf-8\') # Compress the bytes using the highest compression level compressed_data = zlib.compress(text_bytes, level=9) # Decompress the data back to bytes decompressed_data = zlib.decompress(compressed_data) # Decode the bytes back to a string decompressed_text = decompressed_data.decode(\'utf-8\') return decompressed_text"},{"question":"Objective You are to write a Python script that mimics parts of the behavior of the `sdist` command by creating a simple source distribution script. This involves specifying lists of files and creating archives in different formats based on given input. Task 1. **Function Implementation**: - Write a function `create_manifest(directory: str) -> List[str]` that scans a given directory and returns a list of all the files that should be included in the source distribution. - Write another function `create_source_distribution(file_list: List[str], archive_format: str, output_name: str)` that takes the list of files from `create_manifest`, an archive format (`zip`, `tar`, `gztar`, etc.), and an output file name to create the desired archive. 2. **Input and Output**: - `create_manifest(directory: str) -> List[str]`: - **Input**: A string `directory` representing the path of the directory to scan. - **Output**: A list of strings where each element is a relative path to a file that should be included in the source distribution. - `create_source_distribution(file_list: List[str], archive_format: str, output_name: str)`: - **Input**: - `file_list`: List of file paths to be included in the archive. - `archive_format`: Specifies the format of the output archive, e.g., \'zip\', \'tar\', \'gztar\', etc. - `output_name`: Specifies the desired name of the output archive file without any extension. - **Output**: This function should not return anything but should create an archive file with the specified name and format in the current working directory. 3. **Constraints**: - The directory may contain subdirectories, and all relevant files should be included in the file list recursively. - The script must handle at least the `zip` and `gztar` formats. - When creating the archive, the directory structure should be preserved. 4. **Performance Requirements**: - The solution should handle directories with large numbers of files efficiently. - Avoid any unnecessary file operations or traversals. Example ```python # Example usage # Assuming the directory structure is as follows: # sample_dir/ # README.txt # setup.py # src/ # module1.py # module2.py # tests/ # test_module1.py # Function calls file_list = create_manifest(\'sample_dir\') # file_list should be: # [\'README.txt\', \'setup.py\', \'src/module1.py\', \'src/module2.py\', \'tests/test_module1.py\'] create_source_distribution(file_list, \'zip\', \'my_package\') # This should create a file named \'my_package.zip\' in the current working directory with the correct structure ``` Notes - You may use standard Python libraries for file handling and archive creation. - Ensure that your solution is readable and includes appropriate comments.","solution":"import os from typing import List import shutil import tarfile import zipfile def create_manifest(directory: str) -> List[str]: Scans the given directory and returns a list of files to be included in the source distribution. Parameters: directory (str): The directory to scan. Returns: List[str]: A list of relative file paths. file_list = [] for root, _, files in os.walk(directory): for file in files: file_path = os.path.relpath(os.path.join(root, file), directory) file_list.append(file_path) return file_list def create_source_distribution(file_list: List[str], archive_format: str, output_name: str): Creates an archive of the specified format containing the files in `file_list`. Parameters: file_list (List[str]): List of file paths to include in the archive. archive_format (str): The format of the archive (e.g., \'zip\', \'gztar\'). output_name (str): The desired name of the output archive file (without any extension). if archive_format == \'zip\': with zipfile.ZipFile(f\\"{output_name}.zip\\", \'w\') as zip_file: for file in file_list: zip_file.write(file, os.path.join(output_name, file)) elif archive_format == \'gztar\': with tarfile.open(f\\"{output_name}.tar.gz\\", \'w:gz\') as tar_file: for file in file_list: tar_file.add(file, arcname=os.path.join(output_name, file)) else: raise ValueError(f\\"Unsupported archive format: {archive_format}\\")"},{"question":"# Advanced Logging Configuration and Customization As a seasoned developer, you are tasked with setting up a robust logging system for a Python application that runs both in a development environment with detailed debug information and in a production environment with strict performance requirements. Requirements: 1. **Logger Configuration**: - Create a logger named `appLogger`. - Configure it to log messages at `DEBUG` level to a rotating file handler and at `ERROR` level to the console. - Use different formats for file and console logging. 2. **Custom Filters and Handlers**: - Implement a custom filter that adds contextual information about the current user (for example, a user ID fetched from a mock function) to each log record. - Setup a handler that sends logs to an email address when a `CRITICAL` error occurs. 3. **Context Manager for Selective Logging**: - Provide a context manager that temporarily changes the log level and adds an additional handler for debugging specific sections of code. This context manager should only be active during the block of code it wraps. 4. **Multi-threading Logging Setup**: - Demonstrate that your logging setup works correctly in a multi-threaded application where log messages from different threads are correctly attributed. Input/Output: - **Input**: Not applicable (configuration and code setup). - **Output**: Not applicable (logging outputs to files, console, and email). Constraints: - Use only the built-in `logging` module and standard library components. - The email handler should be a simple demonstration (it does not need to send real emails, but the setup should be correct). Performance requirements: - Ensure non-blocking logging in performance-critical sections. - The logging setup should handle a large volume of log messages without losing any. # Your Task: Implement the setup as described. Provide the following: 1. A Python script `logging_setup.py` that configures the logger as specified. 2. Implementation of the custom filter and email handler within the script. 3. A context manager class for selective logging. 4. A demonstration section in the script that shows multi-threaded logging and usage of the context manager. # Example for Custom Filter Implementation: ```python import logging def mock_get_current_user(): return \\"user123\\" class ContextFilter(logging.Filter): def filter(self, record): record.user = mock_get_current_user() return True # Usage in Logger: logger = logging.getLogger(\'appLogger\') user_filter = ContextFilter() logger.addFilter(user_filter) ``` # Example for Context Manager Implementation: ```python from contextlib import contextmanager class LoggingContext: def __init__(self, logger, level=None, additional_handler=None): self.logger = logger self.level = level self.additional_handler = additional_handler def __enter__(self): if self.level: self.old_level = self.logger.level self.logger.setLevel(self.level) if self.additional_handler: self.logger.addHandler(self.additional_handler) def __exit__(self, exc_type, exc_value, traceback): if self.level: self.logger.setLevel(self.old_level) if self.additional_handler: self.logger.removeHandler(self.additional_handler) # Usage in Code logger = logging.getLogger(\'appLogger\') with LoggingContext(logger, level=logging.DEBUG, additional_handler=logging.StreamHandler()): logger.debug(\\"This is a debug message within context\\") ``` # Additional Notes: - Validate your setup by running the script and observing the log outputs. - Document any assumptions and provide comments for clarity within the script.","solution":"import logging from logging.handlers import RotatingFileHandler, SMTPHandler from contextlib import contextmanager import threading import time # Mock function to get current user, for demonstration purposes def mock_get_current_user(): return \\"user123\\" # Custom filter to add user context class ContextFilter(logging.Filter): def filter(self, record): record.user = mock_get_current_user() return True # Logger setup def setup_logger(): logger = logging.getLogger(\'appLogger\') logger.setLevel(logging.DEBUG) # Rotating file handler file_handler = RotatingFileHandler(\'app.log\', maxBytes=1024*1024, backupCount=5) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(user)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) logger.addHandler(file_handler) # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.ERROR) console_formatter = logging.Formatter(\'%(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) logger.addHandler(console_handler) # Email handler for critical errors (demonstration purposes) email_handler = SMTPHandler(mailhost=(\'localhost\', 25), fromaddr=\'error_logger@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Critical Error\') email_handler.setLevel(logging.CRITICAL) email_formatter = logging.Formatter(\'Timestamp: %(asctime)snLogger: %(name)snUser: %(user)snLevel: %(levelname)snMessage: %(message)s\') email_handler.setFormatter(email_formatter) logger.addHandler(email_handler) # Adding the custom context filter user_filter = ContextFilter() logger.addFilter(user_filter) return logger # Context manager for selective logging class LoggingContext: def __init__(self, logger, level=None, additional_handler=None): self.logger = logger self.level = level self.additional_handler = additional_handler def __enter__(self): if self.level is not None: self.old_level = self.logger.level self.logger.setLevel(self.level) if self.additional_handler is not None: self.logger.addHandler(self.additional_handler) def __exit__(self, exc_type, exc_value, traceback): if self.level is not None: self.logger.setLevel(self.old_level) if self.additional_handler is not None: self.logger.removeHandler(self.additional_handler) # Demo multi-threaded logging def logging_thread_function(logger, thread_number): for _ in range(5): logger.debug(f\\"Thread {thread_number}: Debug message\\") logger.error(f\\"Thread {thread_number}: Error message\\") time.sleep(0.1) def main(): logger = setup_logger() # Start multi-threaded logging demo threads = [] for i in range(5): thread = threading.Thread(target=logging_thread_function, args=(logger, i)) threads.append(thread) thread.start() for thread in threads: thread.join() # Context manager usage with LoggingContext(logger, level=logging.DEBUG, additional_handler=logging.StreamHandler()): logger.debug(\\"This is a debug message within context\\") if __name__ == \\"__main__\\": main()"},{"question":"Probabilistic Modelling with PyTorch Distributions **Objective:** Demonstrate your understanding of the `torch.distributions` module by implementing functions that utilize various probabilistic distributions. # Problem Statement You are required to implement a class `DistributionModel` which provides the following functionalities: 1. **Initialize multiple distributions:** The class should initialize specific distributions (`Normal`, `Bernoulli`, and `Beta`) with given parameters. 2. **Sample from distributions:** The class should provide methods to sample a specified number of values from the initialized distributions. 3. **Compute distribution metrics:** The class should provide methods to compute the mean and variance of each distribution. 4. **Compute KL divergence:** The class should compute the Kullback–Leibler (KL) divergence between two specified distributions. 5. **Transform distributions:** The class should apply a given transformation (e.g., log, exponential) to a distribution and return the transformed distribution. # Class Definition ```python import torch import torch.distributions as dist class DistributionModel: def __init__(self, normal_params, bernoulli_params, beta_params): Initializes Normal, Bernoulli, and Beta distributions with given parameters. Parameters: - normal_params (tuple): (mean, std) for the Normal distribution - bernoulli_params (float): probability of success for the Bernoulli distribution - beta_params (tuple): (alpha, beta) for the Beta distribution self.normal_dist = dist.Normal(*normal_params) self.bernoulli_dist = dist.Bernoulli(bernoulli_params) self.beta_dist = dist.Beta(*beta_params) def sample_normal(self, num_samples): Samples `num_samples` values from the Normal distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.normal_dist.sample((num_samples,)) def sample_bernoulli(self, num_samples): Samples `num_samples` values from the Bernoulli distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.bernoulli_dist.sample((num_samples,)) def sample_beta(self, num_samples): Samples `num_samples` values from the Beta distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.beta_dist.sample((num_samples,)) def get_distribution_stats(self): Computes the mean and variance of the initialized distributions. Returns: - stats (dict): dictionary containing mean and variance for each distribution stats = { \'Normal\': {\'mean\': self.normal_dist.mean.item(), \'variance\': self.normal_dist.variance.item()}, \'Bernoulli\': {\'mean\': self.bernoulli_dist.mean.item(), \'variance\': self.bernoulli_dist.variance.item()}, \'Beta\': {\'mean\': self.beta_dist.mean.item(), \'variance\': self.beta_dist.variance.item()} } return stats def compute_kl_divergence(self, dist1, dist2): Computes the KL divergence between two distributions. Parameters: - dist1: first distribution instance - dist2: second distribution instance Returns: - kl_divergence (Tensor): computed KL divergence return dist.kl_divergence(dist1, dist2) def transform_distribution(self, distribution, transform): Applies a transformation (log or exponential) to the given distribution. Parameters: - distribution: distribution instance to transform - transform (str): \'log\' or \'exp\' Returns: - transformed_distribution (TransformedDistribution): transformed distribution if transform == \'log\': transform_func = dist.transforms.ExpTransform() elif transform == \'exp\': transform_func = dist.transforms.LogTransform() else: raise ValueError(\\"Transform must be \'log\' or \'exp\'\\") return dist.TransformedDistribution(distribution, [transform_func]) # Example Usage: # normal_params = (0.0, 1.0) # bernoulli_params = 0.5 # beta_params = (2.0, 5.0) # model = DistributionModel(normal_params, bernoulli_params, beta_params) # samples_normal = model.sample_normal(5) # samples_bernoulli = model.sample_bernoulli(5) # samples_beta = model.sample_beta(5) # stats = model.get_distribution_stats() # kl_div = model.compute_kl_divergence(model.normal_dist, model.beta_dist) # transformed_dist = model.transform_distribution(model.normal_dist, \'log\') ``` # Constraints and Notes: 1. Use the `torch.distributions` module for all operations. 2. Ensure all methods are efficient and avoid unnecessary computations. 3. The `compute_kl_divergence` method should handle distributions of the same type only. # Testing: To verify your implementation, you may write unit tests to check: - Sampling functions return the correct shape and number of samples. - Statistic computation methods (mean and variance) return accurate values. - KL divergence computation between similar distributions. - Transformation functions apply the correct transformations to the distributions.","solution":"import torch import torch.distributions as dist class DistributionModel: def __init__(self, normal_params, bernoulli_params, beta_params): Initializes Normal, Bernoulli, and Beta distributions with given parameters. Parameters: - normal_params (tuple): (mean, std) for the Normal distribution - bernoulli_params (float): probability of success for the Bernoulli distribution - beta_params (tuple): (alpha, beta) for the Beta distribution self.normal_dist = dist.Normal(*normal_params) self.bernoulli_dist = dist.Bernoulli(bernoulli_params) self.beta_dist = dist.Beta(*beta_params) def sample_normal(self, num_samples): Samples `num_samples` values from the Normal distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.normal_dist.sample((num_samples,)) def sample_bernoulli(self, num_samples): Samples `num_samples` values from the Bernoulli distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.bernoulli_dist.sample((num_samples,)) def sample_beta(self, num_samples): Samples `num_samples` values from the Beta distribution. Parameters: - num_samples (int): number of values to sample Returns: - samples (Tensor): sampled values return self.beta_dist.sample((num_samples,)) def get_distribution_stats(self): Computes the mean and variance of the initialized distributions. Returns: - stats (dict): dictionary containing mean and variance for each distribution stats = { \'Normal\': {\'mean\': self.normal_dist.mean.item(), \'variance\': self.normal_dist.variance.item()}, \'Bernoulli\': {\'mean\': self.bernoulli_dist.mean.item(), \'variance\': self.bernoulli_dist.variance.item()}, \'Beta\': {\'mean\': self.beta_dist.mean.item(), \'variance\': self.beta_dist.variance.item()} } return stats def compute_kl_divergence(self, dist1, dist2): Computes the KL divergence between two distributions. Parameters: - dist1: first distribution instance - dist2: second distribution instance Returns: - kl_divergence (Tensor): computed KL divergence return dist.kl_divergence(dist1, dist2) def transform_distribution(self, distribution, transform): Applies a transformation (log or exponential) to the given distribution. Parameters: - distribution: distribution instance to transform - transform (str): \'log\' or \'exp\' Returns: - transformed_distribution (TransformedDistribution): transformed distribution if transform == \'exp\': transform_func = dist.transforms.ExpTransform() elif transform == \'log\': transform_func = dist.transforms.ExpTransform().inv else: raise ValueError(\\"Transform must be \'log\' or \'exp\'\\") return dist.TransformedDistribution(distribution, [transform_func])"},{"question":"# Advanced Problem: File Analysis and Custom Summary Objective: Demonstrate understanding of file operations, data structures, and functional programming in Python. Problem Statement: You are tasked with processing a large log file from a server. The log file contains multiple entries, each recording an event with a timestamp, event type, and user ID. Your goal is to write a Python function that reads this log file, processes the data, and generates a summary of events per user. File format: - Each line in the log file represents a single event. - Each event is in the format: `timestamp event_type user_id` - The `timestamp` is in ISO format: `YYYY-MM-DDTHH:MM:SS` (e.g., `2023-10-30T14:23:01`) - The `event_type` is a string (e.g., \\"login\\", \\"logout\\", \\"file_upload\\", etc.) - The `user_id` is a unique identifier for a user (e.g., \\"user123\\") Function Signature: ```python def summarize_user_events(log_file_path: str) -> dict: ``` Input: - `log_file_path`: A string representing the path to the log file. Output: - A dictionary where the keys are unique user IDs, and the values are dictionaries. Each of these dictionaries should have keys as event types and values as the count of occurrences of these events for the given user. Constraints: - The log file can be very large, ensure efficient file reading and data processing. - Use appropriate Python data structures to manage and manipulate the data. Example: Given the log file content: ``` 2023-10-30T14:23:01 login user123 2023-10-30T14:24:10 file_upload user123 2023-10-30T14:25:12 login user456 2023-10-30T14:26:45 logout user123 2023-10-30T14:27:10 file_upload user456 2023-10-30T14:28:18 logout user456 ``` The function `summarize_user_events(\\"path/to/logfile.txt\\")` should return: ```python { \\"user123\\": { \\"login\\": 1, \\"file_upload\\": 1, \\"logout\\": 1, }, \\"user456\\": { \\"login\\": 1, \\"file_upload\\": 1, \\"logout\\": 1, } } ``` Note: - Ensure the function is resilient to potential issues with file reading, such as missing or corrupted lines. - Provide meaningful documentation and comments to explain your code logic.","solution":"import collections def summarize_user_events(log_file_path: str) -> dict: Process the log file and generate a summary of events per user. Parameters: log_file_path (str): The path to the log file. Returns: dict: A dictionary where keys are user IDs and values are dictionaries of event counts. user_events_summary = collections.defaultdict(lambda: collections.defaultdict(int)) with open(log_file_path, \'r\') as file: for line in file: try: timestamp, event_type, user_id = line.strip().split() user_events_summary[user_id][event_type] += 1 except ValueError: # Log or handle the error with improper line format if needed continue return user_events_summary"},{"question":"Problem Statement You are given a dataset `heart_disease.csv` which contains medical data about patients. Each row corresponds to a patient and the columns represent various medical indicators. The goal is to predict whether a patient has heart disease (`target=1`) or not (`target=0`). The dataset is highly imbalanced with more `0` cases than `1` cases. Tasks 1. **Data Preprocessing**: - Load the dataset and inspect the first few rows. - Standardize the features using `StandardScaler`. 2. **Model Training**: - Train two different SVM models (`SVC` and `LinearSVC`) on the standardized data. - Use `GridSearchCV` to find the best hyperparameters for each model. For `SVC`, optimize both RBF and linear kernels. For `LinearSVC`, optimize the regularization parameter `C`. 3. **Performance Evaluation**: - Evaluate the performance of both models on a test set using accuracy, precision, recall, and F1-score. - Plot the confusion matrix for the best model. 4. **Handling Imbalance**: - Retrain the best performing model from step 3 using `class_weight=\'balanced\'`. - Compare the performance of the model before and after handling the imbalance. 5. **Custom Kernel**: - Define a custom polynomial kernel function. - Train an `SVC` model using this custom kernel and evaluate its performance. Implementation Details 1. **Data Preprocessing**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load dataset data = pd.read_csv(\'heart_disease.csv\') # Split into features and target X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) ``` 2. **Model Training**: ```python from sklearn.svm import SVC, LinearSVC from sklearn.model_selection import GridSearchCV # SVC with GridSearchCV svc = SVC() svc_params = { \'kernel\': [\'linear\', \'rbf\'], \'C\': [0.1, 1, 10], \'gamma\': [\'scale\', \'auto\'] } svc_grid = GridSearchCV(svc, svc_params, cv=5) svc_grid.fit(X_train, y_train) # LinearSVC with GridSearchCV linear_svc = LinearSVC() linear_svc_params = { \'C\': [0.1, 1, 10] } linear_svc_grid = GridSearchCV(linear_svc, linear_svc_params, cv=5) linear_svc_grid.fit(X_train, y_train) ``` 3. **Performance Evaluation**: ```python from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix import matplotlib.pyplot as plt import seaborn as sns # Predict on test set svc_best = svc_grid.best_estimator_ y_pred_svc = svc_best.predict(X_test) linear_svc_best = linear_svc_grid.best_estimator_ y_pred_linear_svc = linear_svc_best.predict(X_test) # Evaluate SVC svc_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_svc), \'precision\': precision_score(y_test, y_pred_svc), \'recall\': recall_score(y_test, y_pred_svc), \'f1_score\': f1_score(y_test, y_pred_svc) } # Evaluate LinearSVC linear_svc_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_linear_svc), \'precision\': precision_score(y_test, y_pred_linear_svc), \'recall\': recall_score(y_test, y_pred_linear_svc), \'f1_score\': f1_score(y_test, y_pred_linear_svc) } # Confusion matrix for best model def plot_confusion_matrix(y_true, y_pred): cm = confusion_matrix(y_true, y_pred) sns.heatmap(cm, annot=True, fmt=\'d\', cmap=\'Blues\') plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.show() plot_confusion_matrix(y_test, y_pred_svc) ``` 4. **Handling Imbalance**: ```python # Retrain SVC with class_weight=\'balanced\' svc_balanced = SVC(class_weight=\'balanced\') svc_balanced_grid = GridSearchCV(svc_balanced, svc_params, cv=5) svc_balanced_grid.fit(X_train, y_train) # Evaluate balanced SVC svc_balanced_best = svc_balanced_grid.best_estimator_ y_pred_svc_balanced = svc_balanced_best.predict(X_test) svc_balanced_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_svc_balanced), \'precision\': precision_score(y_test, y_pred_svc_balanced), \'recall\': recall_score(y_test, y_pred_svc_balanced), \'f1_score\': f1_score(y_test, y_pred_svc_balanced) } # Compare performance before and after balancing print(\\"SVC Metrics: \\", svc_metrics) print(\\"Balanced SVC Metrics: \\", svc_balanced_metrics) ``` 5. **Custom Kernel**: ```python def custom_kernel(X, Y): return (X @ Y.T) ** 3 # Polynomial kernel example # SVC with custom kernel svc_custom = SVC(kernel=custom_kernel) svc_custom.fit(X_train, y_train) # Predict and evaluate custom kernel SVC y_pred_custom = svc_custom.predict(X_test) custom_kernel_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_custom), \'precision\': precision_score(y_test, y_pred_custom), \'recall\': recall_score(y_test, y_pred_custom), \'f1_score\': f1_score(y_test, y_pred_custom) } print(\\"Custom Kernel SVC Metrics: \\", custom_kernel_metrics) ``` Constraints: - Assume `heart_disease.csv` is available in the current working directory. - You must use the scikit-learn package as the primary resource. - Handle any missing values as you see fit, ensuring it doesn\'t disrupt the flow of the exercises. - Document each step clearly and ensure that the code is clean and well-commented. Submission: - Provide a Jupyter notebook containing your solution. - Ensure it has proper markdown cells explaining each step and the results. - Include the final evaluation metrics and confusion matrix plots.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix import matplotlib.pyplot as plt import seaborn as sns # Load and preprocess the data def load_and_preprocess_data(filepath): data = pd.read_csv(filepath) X = data.drop(columns=[\'target\']) y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test # Train SVC and LinearSVC models with GridSearchCV def train_models(X_train, y_train): svc_params = { \'kernel\': [\'linear\', \'rbf\'], \'C\': [0.1, 1, 10], \'gamma\': [\'scale\', \'auto\'] } svc = SVC() svc_grid = GridSearchCV(svc, svc_params, cv=5) svc_grid.fit(X_train, y_train) linear_svc_params = {\'C\': [0.1, 1, 10]} linear_svc = LinearSVC() linear_svc_grid = GridSearchCV(linear_svc, linear_svc_params, cv=5) linear_svc_grid.fit(X_train, y_train) return svc_grid, linear_svc_grid # Evaluate model performance def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred) } return metrics # Plot confusion matrix def plot_confusion_matrix(y_true, y_pred): cm = confusion_matrix(y_true, y_pred) sns.heatmap(cm, annot=True, fmt=\'d\', cmap=\'Blues\') plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.show() # Retrain with class weight balanced def retrain_with_balanced_class(weighted_grid, X_train, y_train): weighted_grid.fit(X_train, y_train) return weighted_grid # Custom polynomial kernel def custom_kernel(X, Y): return (X @ Y.T) ** 3 # Polynomial kernel example # Train SVC with custom kernel def train_custom_kernel_svc(X_train, y_train): svc_custom = SVC(kernel=custom_kernel) svc_custom.fit(X_train, y_train) return svc_custom"},{"question":"Coding Assessment: Tensor Attributes and Manipulation with PyTorch # Objective: This question aims to assess your understanding of PyTorch tensor attributes (`torch.dtype`, `torch.device`, and `torch.layout`) and how to manipulate tensors based on these properties. # Problem Statement: You are given a task to create and manipulate tensors in PyTorch based on the provided constraints. Implement the following functions: 1. **create_tensor_with_dtype**: - **Input**: - `shape` (tuple of ints): The desired shape of the tensor. - `dtype` (str): The dtype of the tensor as a string (e.g., \'torch.float32\'). - **Output**: - Return a tensor of the specified shape and dtype filled with random numbers drawn from a uniform distribution over `[0, 1)`. 2. **move_tensor_to_device**: - **Input**: - `tensor` (torch.Tensor): The tensor to be moved. - `device` (str): The target device as a string (e.g., \'cuda:0\'). - **Output**: - Move the tensor to the specified device and return it. 3. **reshape_tensor_with_layout**: - **Input**: - `tensor` (torch.Tensor): The tensor to be reshaped. - `new_shape` (tuple of ints): The desired new shape for the tensor. - `layout` (str): The desired layout of the tensor as a string (either \'torch.strided\' or \'torch.sparse_coo\'). - **Output**: - Reshape the tensor to the new shape with the specified layout and return it. 4. **change_memory_format**: - **Input**: - `tensor` (torch.Tensor): The tensor to change memory format. - `memory_format` (str): The desired memory format (\'contiguous_format\', \'channels_last\', \'channels_last_3d\', \'preserve_format\'). - **Output**: - Change the tensor\'s memory format and return it. # Constraints: - Assume the environment supports CUDA for device operations when applicable. - The tensor dtype strings must match one of the `torch.dtype` strings (e.g., \'torch.float32\', \'torch.int64\'). - Valid layouts are \'torch.strided\' and \'torch.sparse_coo\'. - Valid memory formats are as described: \'contiguous_format\', \'channels_last\', \'channels_last_3d\', \'preserve_format\'. - If an invalid dtype, device, layout, or memory format is provided, raise a `ValueError` with an appropriate message. # Example: ```python import torch # Example for create_tensor_with_dtype tensor = create_tensor_with_dtype((2, 3), \'torch.float32\') print(tensor.dtype) # torch.float32 print(tensor.shape) # torch.Size([2, 3]) # Example for move_tensor_to_device tensor_cuda = move_tensor_to_device(tensor, \'cuda:0\') print(tensor_cuda.device) # cuda:0 # Example for reshape_tensor_with_layout reshaped_tensor = reshape_tensor_with_layout(tensor, (3, 2), \'torch.strided\') print(reshaped_tensor.shape) # torch.Size([3, 2]) print(reshaped_tensor.layout) # torch.strided # Example for change_memory_format changed_format_tensor = change_memory_format(tensor, \'channels_last\') print(changed_format_tensor.is_contiguous(memory_format=torch.channels_last)) # True ``` # Solution Template: ```python import torch def create_tensor_with_dtype(shape, dtype): # Check if the dtype is valid if not hasattr(torch, dtype): raise ValueError(f\\"Invalid dtype: {dtype}\\") return torch.rand(shape).type(getattr(torch, dtype)) def move_tensor_to_device(tensor, device): # Check if the device is valid try: device_obj = torch.device(device) except: raise ValueError(f\\"Invalid device: {device}\\") return tensor.to(device_obj) def reshape_tensor_with_layout(tensor, new_shape, layout): # Check if the layout is valid if layout not in [\'torch.strided\', \'torch.sparse_coo\']: raise ValueError(f\\"Invalid layout: {layout}\\") if layout == \'torch.strided\': return tensor.reshape(new_shape).contiguous() else: return tensor.to_sparse().reshape(new_shape) def change_memory_format(tensor, memory_format): # Check if the memory format is valid valid_formats = [\'contiguous_format\', \'channels_last\', \'channels_last_3d\', \'preserve_format\'] if memory_format not in valid_formats: raise ValueError(f\\"Invalid memory format: {memory_format}\\") memory_format_dict = { \'contiguous_format\': torch.contiguous_format, \'channels_last\': torch.channels_last, \'channels_last_3d\': torch.channels_last_3d, \'preserve_format\': torch.preserve_format } return tensor.contiguous(memory_format=memory_format_dict[memory_format]) ```","solution":"import torch def create_tensor_with_dtype(shape, dtype): # Check if the dtype string is valid within torch try: dtype_attr = eval(dtype) except AttributeError: raise ValueError(f\\"Invalid dtype: {dtype}\\") return torch.rand(shape).to(dtype=dtype_attr) def move_tensor_to_device(tensor, device): # Check if the device string is valid within torch try: device_obj = torch.device(device) except RuntimeError: raise ValueError(f\\"Invalid device: {device}\\") return tensor.to(device_obj) def reshape_tensor_with_layout(tensor, new_shape, layout): # Check if the layout string is valid within torch if layout not in [\'torch.strided\', \'torch.sparse_coo\']: raise ValueError(f\\"Invalid layout: {layout}\\") if layout == \'torch.strided\': return tensor.reshape(new_shape).contiguous() elif layout == \'torch.sparse_coo\': sparse_tensor = tensor.to_sparse() return sparse_tensor.reshape(new_shape) def change_memory_format(tensor, memory_format): # Check if the memory format string is valid within torch valid_formats = [\'contiguous_format\', \'channels_last\', \'channels_last_3d\', \'preserve_format\'] if memory_format not in valid_formats: raise ValueError(f\\"Invalid memory format: {memory_format}\\") memory_format_dict = { \'contiguous_format\': torch.contiguous_format, \'channels_last\': torch.channels_last, \'channels_last_3d\': torch.channels_last_3d, \'preserve_format\': torch.preserve_format } return tensor.contiguous(memory_format=memory_format_dict[memory_format])"},{"question":"# Virtual Environment and Package Management Automation **Objective:** You are tasked with writing a Python function that automates the creation and management of a Python virtual environment along with package installations and upgrades. This will demonstrate your understanding of fundamental and advanced concepts related to virtual environments and package management using `python310`. **Task:** Write a Python function `setup_environment(env_name: str, packages: dict, upgrade: bool) -> None` that performs the following tasks: 1. Creates a virtual environment with the name specified by `env_name`. 2. Activates the created virtual environment. 3. Installs the packages specified in the `packages` dictionary where keys are package names and values are versions. If the version is `None`, install the latest version of the package. 4. If upgrade is `True`, upgrade all installed packages to their latest versions. **Input:** - `env_name`: A string representing the name of the virtual environment. - `packages`: A dictionary where each key-value pair represents a package name and its version. Version can be `None` to indicate the latest version. - `upgrade`: A boolean flag indicating whether to upgrade all installed packages to the latest versions. **Example:** ```python def setup_environment(env_name: str, packages: dict, upgrade: bool) -> None: # Your implementation here # Example usage: packages = { \\"requests\\": \\"2.6.0\\", \\"flask\\": None, \\"numpy\\": \\"1.9.2\\" } setup_environment(\\"my_project_env\\", packages, upgrade=True) ``` **Expected Output:** The function will create and activate the specified virtual environment, install the desired packages (and the specified versions), and potentially upgrade the packages if required. Ensure all operations reflect correctly in the shell, and the virtual environment is set up as expected. **Constraints:** - Ensure the virtual environment creation directory does not clash with any existing directories. - Handle any potential errors gracefully, providing meaningful error messages. - The function should be platform-independent (consider handling different OS-specific commands). **Performance Requirement:** - The function should perform the tasks efficiently and provide feedback on the progress. - Any installation or upgrade operations must complete within reasonable time limits. This task helps validate your understanding of Python virtual environments, and package management concepts using `venv` and `pip`.","solution":"import os import subprocess import sys from typing import Dict def setup_environment(env_name: str, packages: Dict[str, str], upgrade: bool) -> None: Creates a virtual environment, installs specified packages, and optionally upgrades all packages to their latest versions. :param env_name: Name of the virtual environment to create. :param packages: A dictionary of package names and versions. :param upgrade: Boolean flag to upgrade all packages to the latest versions. # Create virtual environment subprocess.run([sys.executable, \'-m\', \'venv\', env_name], check=True) # Activate the virtual environment if os.name == \'nt\': # Windows activate_script = os.path.join(env_name, \'Scripts\', \'activate\') else: # Unix or MacOS activate_script = os.path.join(env_name, \'bin\', \'activate\') command_prefix = f\'. {activate_script} && \' # Install specified packages for package, version in packages.items(): if version: subprocess.run(command_prefix + f\\"pip install {package}=={version}\\", shell=True, check=True) else: subprocess.run(command_prefix + f\\"pip install {package}\\", shell=True, check=True) # Upgrade packages if upgrade flag is True if upgrade: subprocess.run(command_prefix + \\"pip install --upgrade pip\\", shell=True, check=True) subprocess.run(command_prefix + \\"pip list --outdated --format=freeze | cut -d = -f 1 | xargs -n 1 pip install -U\\", shell=True, check=True)"},{"question":"# Question: Advanced Seaborn Plotting with Matplotlib You are required to create a set of visualizations using Seaborn and Matplotlib based on the `diamonds` dataset. Follow the detailed steps provided to ensure your solution meets the requirements. Dataset Load the `diamonds` dataset using the following code: ```python from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") ``` # Instructions 1. **Scatter Plot with Custom Annotations**: - Create a scatter plot of `carat` vs. `price` using `seaborn.objects.Plot`. - Place this plot on a Matplotlib figure. - Add a rectangular annotation at the top right (away from the data points), saying \\"Diamonds: very sparkly!\\". 2. **Histogram with Facet**: - Create a histogram of `price` using `seaborn.objects.Plot` with bars colored according to the `cut` of the diamonds. - Use log scaling for the x-axis. - Use faceting to create separate rows for each `cut` value. - Place this plot on a subfigure next to the scatter plot from step 1. 3. **Display the Combined Figure**: - Use `matplotlib.figure.Figure` to create a combined figure of size 14x7. - Use `subfigures` to create two side-by-side subfigures. - Place the scatter plot in the first subfigure and the faceted histogram in the second subfigure. # Expected Output The figure should have two subplots arranged horizontally: - The left subplot should be a scatter plot of `carat` vs. `price` with a custom annotation \\"Diamonds: very sparkly!\\". - The right subplot should be a faceted histogram of `price` with log-scaled x-axis and colored bars according to `cut`. # Code Template You may use the following code template to structure your solution: ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create scatter plot with custom annotations scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create the main figure with two subfigures main_fig = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") left_subfig, right_subfig = main_fig.subfigures(1, 2) # Plot scatter on the left subfigure scatter_ax = scatter_plot.on(left_subfig).plot() ax = left_subfig.axes[0] rect = mpl.patches.Rectangle(xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False) ax.add_artist(rect) ax.text(x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes) # Create histogram with faceting and log-scaled x-axis hist_plot = ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Plot histogram on the right subfigure hist_plot.on(right_subfig).plot() plt.show() ``` # Constraints - Your code should handle the `diamonds` dataset without any modifications. - Ensure you follow the specified plot requirements and annotations. # Note Points will be awarded for readability, code efficiency, and correctness.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create scatter plot with custom annotations scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create the main figure with two subfigures main_fig = mpl.figure.Figure(figsize=(14, 7), dpi=100) # Create two subfigures within the main figure, arranged side by side left_subfig, right_subfig = main_fig.subfigures(1, 2) # Plot scatter on the left subfigure scatter_ax = scatter_plot.on(left_subfig).plot() ax = left_subfig.axes[0] rect = mpl.patches.Rectangle( xy=(0.6, 0.9), width=0.4, height=0.1, color=\\"C1\\", alpha=0.2, transform=ax.transAxes, clip_on=False ) ax.add_artist(rect) ax.text( 0.8, 0.95, \\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, bbox=dict(facecolor=\'white\', alpha=0.5, edgecolor=\'none\') ) # Create histogram with faceting and log-scaled x-axis hist_plot = ( so.Plot(diamonds, x=\\"price\\", color=\\"cut\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Plot histogram on the right subfigure hist_plot.on(right_subfig).plot() plt.show()"},{"question":"# Email Policy Configuration and Usage In this task, you will implement and use different `email.policy` configurations to manage email message handling. Task Description Implement the function `custom_email_policies(filepath: str) -> dict` which takes a path to a file containing an email message and returns a dictionary with different policy-applied contents of the email. Use the following steps: 1. Parse the email from the provided file using the default email policy. 2. Clone the default policy and modify the `linesep` and `max_line_length` attributes. 3. Combine the `SMTP` policy with a strict policy to create an `SMTP_strict` policy. 4. Generate the email contents using each custom policy and store them in the dictionary with relevant keys. Input - `filepath`: A string containing the path to the file with the email message in binary format. Output - A dictionary with keys `default`, `custom_policy`, and `strict_smtp` and the corresponding flattened email messages according to these policies. Example ```python filepath = \'example_email.txt\' result = custom_email_policies(filepath) # result should contain the email serialized using default, custom, and strict SMTP policies ``` Constraints - Assume the input file is correctly formatted. - Implement custom policies with the following attributes: - Change `linesep` to `\'rn\'`. - Set `max_line_length` to `100` characters. Implementation Notes - Use `message_from_binary_file` to parse the email. - Utilize `BytesGenerator` to generate email bytes with different policies. - Make sure to provide appropriate exception handling for policy-specific errors if `raise_on_defect` is true. ```python from email import message_from_binary_file from email.policy import default, SMTP, strict from email.generator import BytesGenerator from io import BytesIO def custom_email_policies(filepath: str) -> dict: with open(filepath, \'rb\') as f: msg = message_from_binary_file(f, policy=default) # Create a custom policy by cloning the default and modifying attributes custom_policy = default.clone(linesep=\'rn\', max_line_length=100) # Combine SMTP and strict policies strict_smtp_policy = SMTP + strict # Function to generate the email bytes for a given policy def generate_email_content(msg, policy): output = BytesIO() gen = BytesGenerator(output, policy=policy) gen.flatten(msg) return output.getvalue().decode(\'ascii\') # Generating outputs for different policies results = { \'default\': generate_email_content(msg, default), \'custom_policy\': generate_email_content(msg, custom_policy), \'strict_smtp\': generate_email_content(msg, strict_smtp_policy) } return results ``` Make sure to follow best practices in exception handling and document your code for clarity.","solution":"import email from email import policy from email.generator import BytesGenerator from io import BytesIO def custom_email_policies(filepath: str) -> dict: with open(filepath, \'rb\') as f: msg = email.message_from_binary_file(f, policy=policy.default) custom_policy = policy.default.clone(linesep=\'rn\', max_line_length=100) strict_smtp_policy = policy.SMTP + policy.strict def generate_email_content(msg, policy): output = BytesIO() gen = BytesGenerator(output, policy=policy) gen.flatten(msg) return output.getvalue().decode(\'ascii\') results = { \'default\': generate_email_content(msg, policy.default), \'custom_policy\': generate_email_content(msg, custom_policy), \'strict_smtp\': generate_email_content(msg, strict_smtp_policy) } return results"},{"question":"Ensemble Learning with Feature Selection in Scikit-learn Objective: Implement a Python function using scikit-learn that performs classification on a given dataset using an ensemble method (Random Forest). The function should also perform feature selection before training the classifier. You are required to select the top-k features based on their importance scores provided by a feature selection technique. Function Signature: ```python def ensemble_feature_selection_classification(X_train, y_train, X_test, k): Perform classification using an ensemble method with feature selection. Parameters: - X_train (pd.DataFrame): Training feature dataset. - y_train (pd.Series): Training target labels. - X_test (pd.DataFrame): Test feature dataset. - k (int): Number of top features to select based on importance scores. Returns: - pd.Series: Predicted labels for the test dataset. ``` Requirements: 1. **Feature Selection**: - Use the `SelectFromModel` method from `sklearn.feature_selection` to select the top-k features from the training dataset based on feature importance. 2. **Classification**: - Train a `RandomForestClassifier` using the selected features. - Use the trained classifier to predict labels for the test dataset. 3. **Input/Output Formats**: - **Input**: - `X_train`: DataFrame of shape (n_samples, n_features) containing the training data. - `y_train`: Series of length n_samples containing the training labels. - `X_test`: DataFrame of shape (m_samples, n_features) containing the test data. - `k`: Integer, the number of top features to select. - **Output**: - Return a Series of length m_samples containing the predicted labels for `X_test`. 4. **Constraints**: - Assume `X_train` and `X_test` are cleaned and preprocessed. - The training and testing datasets can be of significant size (thousands of samples and features). 5. **Performance Requirements**: - The solution should efficiently handle large datasets. - The feature selection and classification should be completed within reasonable computation time given the usual constraints of a standard computational environment. Example Usage: ```python import pandas as pd from sklearn.datasets import make_classification # Generate a synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test = X[:800], X[800:] y_train, y_test = y[:800], y[800:] # Convert to DataFrame/Series X_train_df = pd.DataFrame(X_train) X_test_df = pd.DataFrame(X_test) y_train_series = pd.Series(y_train) # Number of top features to select k = 5 # Perform classification predicted_labels = ensemble_feature_selection_classification(X_train_df, y_train_series, X_test_df, k) # Display predicted labels print(predicted_labels) ``` Notes: - Ensure the `RandomForestClassifier` and `SelectFromModel` are properly imported from scikit-learn. - The function should handle exceptions appropriately, especially regarding matrix dimensions and feature selection constraints.","solution":"import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import SelectFromModel def ensemble_feature_selection_classification(X_train, y_train, X_test, k): Perform classification using an ensemble method with feature selection. Parameters: - X_train (pd.DataFrame): Training feature dataset. - y_train (pd.Series): Training target labels. - X_test (pd.DataFrame): Test feature dataset. - k (int): Number of top features to select based on importance scores. Returns: - pd.Series: Predicted labels for the test dataset. # Initialize the Random Forest model rf = RandomForestClassifier(n_estimators=100, random_state=42) # Fit the model to the training data rf.fit(X_train, y_train) # Use SelectFromModel to select the top k features selector = SelectFromModel(rf, max_features=k, prefit=True) X_train_selected = selector.transform(X_train) X_test_selected = selector.transform(X_test) # Train the Random Forest model on the selected features rf_selected = RandomForestClassifier(n_estimators=100, random_state=42) rf_selected.fit(X_train_selected, y_train) # Predict the labels for the test set y_pred = rf_selected.predict(X_test_selected) return pd.Series(y_pred)"},{"question":"# **Coding Assessment Task: Debug Dynamic Inventory Management System** **Objective:** Implement and debug a basic dynamic inventory management system using the `pdb` module for debugging. The system should manage a list of inventory items, supporting operations to add, remove, and update items, and it should handle and debug specific exceptions gracefully. **System Requirements:** You need to implement a class `Inventory` with the following methods: 1. `add_item(self, item_name: str, quantity: int, price: float) -> None`: - Adds a new item to the inventory. - Throws a `ValueError` if the item already exists. 2. `remove_item(self, item_name: str) -> None`: - Removes an item from the inventory. - Throws a `ValueError` if the item doesn\'t exist. 3. `update_item(self, item_name: str, quantity: int, price: float) -> None`: - Updates the quantity and price of an item in the inventory. - Throws a `ValueError` if the item doesn\'t exist. 4. `get_inventory(self) -> dict`: - Returns the current state of the inventory. **Debugging Requirements:** 1. Utilize `pdb` to set breakpoints in each method to inspect the input arguments and current inventory state. 2. Use `pdb` post-mortem debugging in the case of exceptions to inspect the error state. **Input/Output:** Your code should be executable without input from stdin or output to stdout. The Inventory class should be tested using direct method calls. **Implementation Details:** 1. Write the code for the `Inventory` class. 2. Write a separate test function `test_inventory_operations()` that demonstrates: - Adding, removing, and updating items. - Handling errors using pdb\'s `post_mortem` debugging. - Using breakpoints to debug the flow of operations through the `pdb` module. **Constraints and Evaluation:** - Assume item names are unique strings and quantities/prices are positive numbers. - Your implementation should focus on using `pdb` effectively to debug and handle edge cases. - Ensure the code is well-commented, explaining how `pdb` is used. ```python import pdb class Inventory: def __init__(self): self.items = {} def add_item(self, item_name, quantity, price): pdb.set_trace() if item_name in self.items: raise ValueError(\\"Item already exists\\") self.items[item_name] = {\'quantity\': quantity, \'price\': price} def remove_item(self, item_name): pdb.set_trace() if item_name not in self.items: raise ValueError(\\"Item does not exist\\") del self.items[item_name] def update_item(self, item_name, quantity, price): pdb.set_trace() if item_name not in self.items: raise ValueError(\\"Item does not exist\\") self.items[item_name] = {\'quantity\': quantity, \'price\': price} def get_inventory(self): pdb.set_trace() return self.items def test_inventory_operations(): try: inv = Inventory() inv.add_item(\\"apple\\", 10, 0.5) inv.update_item(\\"apple\\", 15, 0.55) inv.remove_item(\\"apple\\") # Causes an error to invoke post mortem debugging. inv.remove_item(\\"banana\\") except Exception as e: pdb.post_mortem() # Run test function if __name__ == \\"__main__\\": test_inventory_operations() ``` Write the implementation for the `Inventory` class and the `test_inventory_operations` function. Ensure the use of `pdb` to set breakpoints and handle exceptions for debugging purposes.","solution":"import pdb class Inventory: def __init__(self): self.items = {} def add_item(self, item_name, quantity, price): if item_name in self.items: raise ValueError(\\"Item already exists\\") self.items[item_name] = {\'quantity\': quantity, \'price\': price} def remove_item(self, item_name): if item_name not in self.items: raise ValueError(\\"Item does not exist\\") del self.items[item_name] def update_item(self, item_name, quantity, price): if item_name not in self.items: raise ValueError(\\"Item does not exist\\") self.items[item_name] = {\'quantity\': quantity, \'price\': price} def get_inventory(self): return self.items def debug_wrapper(): try: inv = Inventory() pdb.set_trace() # Set a break point here to debug initial additions inv.add_item(\\"apple\\", 10, 0.5) inv.add_item(\\"banana\\", 20, 0.3) pdb.set_trace() # Set a break point here to debug updates inv.update_item(\\"apple\\", 15, 0.6) pdb.set_trace() # Set a break point here to debug removals inv.remove_item(\\"banana\\") pdb.set_trace() # Set a break point here to debug the state of inventory print(inv.get_inventory()) # Causes an error to invoke post mortem debugging. inv.remove_item(\\"orange\\") except Exception as e: pdb.post_mortem() # Uncomment below lines to run debug session # if __name__ == \\"__main__\\": # debug_wrapper()"},{"question":"# Question: Implementing a Custom MemoryView Wrapper in Python You have been provided with a detailed explanation of the `memoryview` object in Python. Your task is to write a Python function that mimics some behavior of the memoryview functions described but in pure Python. You need to create a class `CustomMemoryView` that provides similar functionality. # Objectives: 1. Implement the `CustomMemoryView` class. 2. Implement the following methods in this class: - `from_object(obj)`: Initialize a memory view from an object that supports the buffer protocol (like bytes or bytearray). - `to_bytes()`: Convert the memory view back to a bytes object. - `is_contiguous()`: Check if the memory is contiguous. # Constraints: 1. `obj` provided to `from_object` will always be an instance of `bytes` or `bytearray`. 2. You are not allowed to use the built-in `memoryview` within your implementation. 3. Assume the input `obj` will always be a flat, contiguous buffer. # Example Usage: ```python # Create an instance of CustomMemoryView from a bytes object mv = CustomMemoryView.from_object(b\'hello world\') # Convert the memory view back to a bytes object print(mv.to_bytes()) # Output: b\'hello world\' # Check if the memoryview is contiguous print(mv.is_contiguous()) # Output: True ``` # Implementation: Implement the `CustomMemoryView` class and its methods. ```python class CustomMemoryView: def __init__(self): self._buffer = None @classmethod def from_object(cls, obj): instance = cls() instance._buffer = obj return instance def to_bytes(self): return bytes(self._buffer) def is_contiguous(self): # Since we are only dealing with bytes/bytearray, it is always contiguous return True # Example usage mv = CustomMemoryView.from_object(b\'hello world\') print(mv.to_bytes()) # Output: b\'hello world\' print(mv.is_contiguous()) # Output: True ``` Your implementation should handle the three methods as described. Ensure your code is well-structured and adheres to Python\'s naming and style conventions.","solution":"class CustomMemoryView: def __init__(self, buffer): self._buffer = buffer @classmethod def from_object(cls, obj): return cls(obj) def to_bytes(self): return bytes(self._buffer) def is_contiguous(self): return True"},{"question":"**Coding Assessment Question:** # Objective: Implement a class `DBManager` that manages a persistent dictionary using the `dbm` module. Your class should demonstrate the ability to handle creating, reading, updating, deleting entries, and closing the database using context management. # Class Requirements: 1. **Initialization and Database Opening:** - __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666) - Parameters: - `filename`: Name of the database file. - `flag`: Mode to open the database (`\'r\'`, `\'w\'`, `\'c\'`, `\'n\'`). Default is `\'c\'`. - `mode`: The Unix mode of the file if it needs to be created. Default is `0o666`. 2. **Context Management:** - Should support `with` statement to automatically close the database. 3. **Methods:** - `insert(self, key: str, value: str)`: Inserts a key-value pair into the database. - `fetch(self, key: str) -> Optional[str]`: Fetches the value associated with the key. Returns `None` if the key does not exist. - `delete(self, key: str)`: Deletes a key-value pair from the database. - `list_keys(self) -> List[str]`: Returns a list of all keys in the database. - `update(self, key: str, new_value: str)`: Updates the value associated with the key. # Constraints: - Keys and values must be stored as bytes. - Handle appropriate exception cases, such as accessing a non-existent key or modifying a read-only database. # Example: ```python from typing import Optional, List class DBManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): # Implement initialization and database opening code here pass def __enter__(self): # Implement context management enter method here pass def __exit__(self, exc_type, exc_val, exc_tb): # Implement context management exit method here pass def insert(self, key: str, value: str): # Implement insert method here pass def fetch(self, key: str) -> Optional[str]: # Implement fetch method here pass def delete(self, key: str): # Implement delete method here pass def list_keys(self) -> List[str]: # Implement list_keys method here pass def update(self, key: str, new_value: str): # Implement update method here pass # Example usage: with DBManager(\'testdb\', \'c\') as db: db.insert(\'hello\', \'world\') print(db.fetch(\'hello\')) # Output: \'world\' db.update(\'hello\', \'Python\') print(db.fetch(\'hello\')) # Output: \'Python\' db.delete(\'hello\') print(db.fetch(\'hello\')) # Output: None print(db.list_keys()) # Output: [] ``` # Notes: - Ensure that the database is properly closed after operations, especially in case of exceptions. - Test your implementation thoroughly to handle various edge cases like empty databases, key collisions, and read-only access.","solution":"import dbm from typing import Optional, List class DBManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.filename = filename self.flag = flag self.mode = mode self.db = None def __enter__(self): self.db = dbm.open(self.filename, self.flag, self.mode) return self def __exit__(self, exc_type, exc_val, exc_tb): if self.db: self.db.close() def insert(self, key: str, value: str): self.db[key.encode()] = value.encode() def fetch(self, key: str) -> Optional[str]: value = self.db.get(key.encode()) return value.decode() if value else None def delete(self, key: str): if key.encode() in self.db: del self.db[key.encode()] def list_keys(self) -> List[str]: return [key.decode() for key in self.db.keys()] def update(self, key: str, new_value: str): if key.encode() in self.db: self.db[key.encode()] = new_value.encode()"},{"question":"**Objective:** You will demonstrate your understanding of scikit-learn\'s decision trees by implementing a function that fits a DecisionTreeClassifier to a given dataset and returns the model\'s accuracy on a test set. Additionally, you will visualize the constructed decision tree and export it in both textual and Graphviz formats. **Function Specifications:** Please implement the following function: ```python def decision_tree_classification(train_X, train_y, test_X, test_y, max_depth=None, export_text_flag=False, export_graphviz_flag=False): Fits a DecisionTreeClassifier to the training data and evaluates its accuracy on the test data. Optionally exports the tree in textual and Graphviz formats. Parameters: - train_X: list of lists or 2D numpy array, training features (n_samples, n_features) - train_y: list or 1D numpy array, target values for training (n_samples,) - test_X: list of lists or 2D numpy array, test features (n_samples, n_features) - test_y: list or 1D numpy array, target values for testing (n_samples,) - max_depth: int, default=None, maximum depth of the tree - export_text_flag: bool, default=False, if True, export the tree in a textual format - export_graphviz_flag: bool, default=False, if True, export the tree in Graphviz format and save as \\"tree.dot\\" Returns: - accuracy: float, accuracy of the model on the test data Raises: - ValueError: if any input validation fails pass ``` **Constraints:** 1. Ensure that `train_X`, `train_y`, `test_X`, and `test_y` are either lists of appropriate dimensions or numpy arrays. 2. The function should raise a `ValueError` if the input shapes do not match the requirements. 3. Use `DecisionTreeClassifier` from scikit-learn to fit the model. 4. Calculate and return the accuracy of the trained model on the test set using `accuracy_score` from scikit-learn. 5. Use `plot_tree` function to visualize the decision tree. 6. If `export_text_flag` is True, use `export_text` to print the tree. 7. If `export_graphviz_flag` is True, use `export_graphviz` to export the tree and save it as \\"tree.dot\\". **Input and Output Formats:** - Input: - train_X: List of lists or 2D numpy array of shape (n_samples, n_features) representing the training data. - train_y: List or 1D numpy array of shape (n_samples,) representing the training labels. - test_X: List of lists or 2D numpy array of shape (n_samples, n_features) representing the test data. - test_y: List or 1D numpy array of shape (n_samples,) representing the test labels. - max_depth: (Optional) Integer specifying the maximum depth of the tree. - export_text_flag: (Optional) Boolean flag indicating whether to export the tree in textual format. - export_graphviz_flag: (Optional) Boolean flag indicating whether to export the tree in Graphviz format. - Output: - accuracy: A float representing the fraction of correctly classified test samples. **Example:** ```python train_X = [[0, 0], [1, 1], [0, 1], [1, 0]] train_y = [0, 1, 0, 1] test_X = [[2, 2], [3, 3]] test_y = [1, 1] accuracy = decision_tree_classification(train_X, train_y, test_X, test_y, max_depth=2, export_text_flag=True, export_graphviz_flag=True) print(accuracy) # Example output: 1.0 ``` **Note:** - This function should make use of the imported libraries such as `numpy`, `sklearn.tree.DecisionTreeClassifier`, `sklearn.metrics.accuracy_score`, `sklearn.tree.plot_tree`, `sklearn.tree.export_text`, and `sklearn.tree.export_graphviz`.","solution":"import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.tree import plot_tree, export_text, export_graphviz def decision_tree_classification(train_X, train_y, test_X, test_y, max_depth=None, export_text_flag=False, export_graphviz_flag=False): Fits a DecisionTreeClassifier to the training data and evaluates its accuracy on the test data. Optionally exports the tree in textual and Graphviz formats. Parameters: - train_X: list of lists or 2D numpy array, training features (n_samples, n_features) - train_y: list or 1D numpy array, target values for training (n_samples,) - test_X: list of lists or 2D numpy array, test features (n_samples, n_features) - test_y: list or 1D numpy array, target values for testing (n_samples,) - max_depth: int, default=None, maximum depth of the tree - export_text_flag: bool, default=False, if True, export the tree in a textual format - export_graphviz_flag: bool, default=False, if True, export the tree in Graphviz format and save as \\"tree.dot\\" Returns: - accuracy: float, accuracy of the model on the test data Raises: - ValueError: if any input validation fails # Validate input train_X = np.array(train_X) train_y = np.array(train_y) test_X = np.array(test_X) test_y = np.array(test_y) if len(train_X.shape) != 2 or len(test_X.shape) != 2: raise ValueError(\\"train_X and test_X must be 2D arrays.\\") if len(train_y.shape) != 1 or len(test_y.shape) != 1: raise ValueError(\\"train_y and test_y must be 1D arrays.\\") if train_X.shape[0] != train_y.shape[0]: raise ValueError(\\"train_X and train_y must have the same number of samples.\\") if test_X.shape[0] != test_y.shape[0]: raise ValueError(\\"test_X and test_y must have the same number of samples.\\") # Create decision tree classifier clf = DecisionTreeClassifier(max_depth=max_depth) # Fit the model clf.fit(train_X, train_y) # Predict on test data predictions = clf.predict(test_X) # Calculate accuracy accuracy = accuracy_score(test_y, predictions) # Export the tree as text if flag is true if export_text_flag: print(export_text(clf, feature_names=[f\\"f{i}\\" for i in range(train_X.shape[1])])) # Export the tree to a .dot file if flag is true if export_graphviz_flag: export_graphviz(clf, out_file=\\"tree.dot\\", feature_names=[f\\"f{i}\\" for i in range(train_X.shape[1])]) return accuracy"},{"question":"# Objective Your task is to design a Python function that dynamically imports a module from a given file path and verifies if a specified function exists within that module. If the function exists, your program should execute it with the provided arguments and return the result. If the function does not exist, raise an appropriate exception. # Requirements 1. Use the `importlib` module to handle the dynamic importing. 2. Verify the existence of the function within the imported module. 3. Handle errors gracefully and raise a `FunctionNotFoundError` if the specified function is not found in the imported module. # Function Signature ```python def dynamic_import_and_execute(module_path: str, function_name: str, *args, **kwargs) -> Any: Imports a module from the given file path, verifies a function\'s existence, and executes it if present. Parameters: - module_path (str): The file path to the module to be imported. - function_name (str): The name of the function to be verified and executed. - args: Positional arguments to pass to the function. - kwargs: Keyword arguments to pass to the function. Returns: - Any: The result of the function execution if successful. Raises: - FunctionNotFoundError: If the specified function is not found in the imported module. pass ``` # Constraints - You can assume that the module file is a valid Python file. - You can assume the module file does not have any import errors and can be imported using the provided path. # Example Usage ```python # Assuming we have a file `math_operations.py` with the following content: # def add(a, b): # return a + b result = dynamic_import_and_execute(\'path/to/math_operations.py\', \'add\', 5, 10) print(result) # Output: 15 # If the function does not exist try: dynamic_import_and_execute(\'path/to/math_operations.py\', \'subtract\', 5, 10) except FunctionNotFoundError as e: print(e) # Output: The function \'subtract\' was not found in the module. ``` # Performance Requirements - The solution should efficiently handle the importing and function verification process. - Ensure that the function is executed correctly only if it is present in the imported module. # Exception Handling - Define a custom exception `FunctionNotFoundError` which should be raised when the specified function is not found in the module. # Implementation Notes - You may need to utilize `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` for importing from a path. - Use `hasattr` to verify the existence of the function within the module. # Additional Information - The solution should be compatible with Python 3.10. - Ensure the function is robust and handles edge cases gracefully.","solution":"import importlib.util import os class FunctionNotFoundError(Exception): pass def dynamic_import_and_execute(module_path: str, function_name: str, *args, **kwargs): Imports a module from the given file path, verifies a function\'s existence, and executes it if present. Parameters: - module_path (str): The file path to the module to be imported. - function_name (str): The name of the function to be verified and executed. - args: Positional arguments to pass to the function. - kwargs: Keyword arguments to pass to the function. Returns: - Any: The result of the function execution if successful. Raises: - FunctionNotFoundError: If the specified function is not found in the imported module. if not os.path.isfile(module_path): raise ValueError(f\\"The module file does not exist at: {module_path}\\") module_name = os.path.splitext(os.path.basename(module_path))[0] spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) if not hasattr(module, function_name): raise FunctionNotFoundError(f\\"The function \'{function_name}\' was not found in the module \'{module_name}\'.\\") func = getattr(module, function_name) return func(*args, **kwargs)"},{"question":"Objective Demonstrate your understanding of seaborn by creating a customized line plot based on the given specifications. Task You are provided with the `fmri` dataset from seaborn, which contains time series data about brain activity at different time points for different events and regions. Your objective is to generate a plot that meets the following criteria: 1. Load the `fmri` dataset using `sns.load_dataset(\\"fmri\\")`. 2. Create a line plot with: - `timepoint` on the x-axis. - `signal` on the y-axis. - Different lines for each `region`, distinguished by different hues. - Different styles for each `event`. - Error bars representing 2 standard errors (`err_style=\'bars\', errorbar=(\'se\', 2)`). 3. Further customize the plot to improve its readability: - Add markers to the lines. - Use a custom palette of your choice. - Set the linewidth to 1. - Ensure that the legend is displayed in full. Input and Output - **Input**: You do not need to take any input from the user. The dataset is loaded directly within the code. - **Output**: The output should be the generated plot displayed using `plt.show()`. - **Constraints**: - Ensure that the plot is clear and well-labeled. - Use only seaborn and matplotlib packages. ```python import seaborn as sns import matplotlib.pyplot as plt # Write your code here ``` Example The below example outlines parts of the implementation, but not the complete solution. Your task is to fill in the details to meet the requirements specified. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot with the specified customizations sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, err_style=\\"bars\\", errorbar=(\\"se\\", 2), linewidth=1, palette=\\"crest\\" # You can choose your own palette ) # Ensure that the legend is displayed in full plt.legend(loc=\'best\', title=\'Legend\') # Show the plot plt.show() ``` Your task is to complete this code to meet all specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_fmri(): # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot with the specified customizations sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, err_style=\\"bars\\", errorbar=(\\"se\\", 2), linewidth=1, palette=\\"tab10\\" # Custom palette ) # Ensure that the legend is displayed in full plt.legend(loc=\'best\', title=\'Legend\') # Show the plot plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.title(\'FMRI Data Line Plot\') plt.show() # Call the function to generate the plot plot_fmri()"},{"question":"Serialization and Deserialization with PyTorch **Objective**: Demonstrate your understanding of PyTorch\'s tensor and module serialization mechanisms by implementing functions to save and load model states, ensuring efficient storage practices. **Problem Statement**: You are provided with a PyTorch model `MyComplexModel` and a set of tensors representing training data as well as some metadata structures. Your task is to: 1. **Serialize the model state and the tensors**: - Implement a function `save_model_and_data(model, data, metadata, file_path)` that saves the model\'s state dictionary and the given data (tensors) into a specified file. Ensure that storage is optimized if any tensor views are involved. 2. **Deserialize and reconstruct the model and data**: - Implement a function `load_model_and_data(file_path, model_class)` that loads the saved state dictionary and data from the file, reconstructing the model, data, and metadata structures. **Function Signatures**: ```python import torch def save_model_and_data(model: torch.nn.Module, data: dict, metadata: dict, file_path: str) -> None: Save the model state dictionary, tensors in `data`, and `metadata` to the given file path. Args: - model (torch.nn.Module): The PyTorch model to serialize. - data (dict): Dictionary containing tensors to be saved. - metadata (dict): Dictionary containing additional metadata to be saved. - file_path (str): Path to the file where data should be saved. Returns: - None pass def load_model_and_data(file_path: str, model_class: type) -> (torch.nn.Module, dict, dict): Load the model state dictionary, tensors, and metadata from the specified file path, and reconstruct the model. Args: - file_path (str): Path to the file from which data should be loaded. - model_class (type): A class reference to the PyTorch model class that needs to be reconstructed. Returns: - (torch.nn.Module, dict, dict): The loaded model, data dictionary, and metadata dictionary. pass ``` **Requirements**: - The model should be an instance of `model_class`. - The `data` dictionary contains tensors and may involve tensors that share storage (e.g., views). - The `metadata` is a dictionary containing additional information that needs to be saved alongside the model and tensors. - The `save_model_and_data` function should ensure that any view relationships in the tensors are preserved in an optimized fashion while saving. - The `load_model_and_data` function should accurately restore the model and data while preserving any view/ storage relationships. **Example**: ```python class MyComplexModel(torch.nn.Module): def __init__(self): super(MyComplexModel, self).__init__() self.fc1 = torch.nn.Linear(10, 5) self.fc2 = torch.nn.Linear(5, 2) def forward(self, x): return self.fc2(torch.relu(self.fc1(x))) # Creating a dummy model and data model = MyComplexModel() data = { \\"input_tensor\\": torch.arange(10, dtype=torch.float32), \\"input_views\\": torch.arange(10, dtype=torch.float32).view(2, 5) } metadata = {\\"description\\": \\"dummy example\\"} # Saving the model and data to \'model_data.pt\' file_path = \\"model_data.pt\\" save_model_and_data(model, data, metadata, file_path) # Loading the model and data from \'model_data.pt\' loaded_model, loaded_data, loaded_metadata = load_model_and_data(file_path, MyComplexModel) assert metadata == loaded_metadata assert all(torch.equal(data[key], loaded_data[key]) for key in data) assert isinstance(loaded_model, MyComplexModel) ``` Ensure your implementation passes the example test case provided.","solution":"import torch def save_model_and_data(model: torch.nn.Module, data: dict, metadata: dict, file_path: str) -> None: Save the model state dictionary, tensors in `data`, and `metadata` to the given file path. Args: - model (torch.nn.Module): The PyTorch model to serialize. - data (dict): Dictionary containing tensors to be saved. - metadata (dict): Dictionary containing additional metadata to be saved. - file_path (str): Path to the file where data should be saved. Returns: - None # Prepare the state to save state = { \\"model_state_dict\\": model.state_dict(), \\"data\\": data, \\"metadata\\": metadata } # Save to file torch.save(state, file_path) def load_model_and_data(file_path: str, model_class: type) -> (torch.nn.Module, dict, dict): Load the model state dictionary, tensors, and metadata from the specified file path, and reconstruct the model. Args: - file_path (str): Path to the file from which data should be loaded. - model_class (type): A class reference to the PyTorch model class that needs to be reconstructed. Returns: - (torch.nn.Module, dict, dict): The loaded model, data dictionary, and metadata dictionary. # Load the state from file state = torch.load(file_path) # Reconstruct the model model = model_class() model.load_state_dict(state[\\"model_state_dict\\"]) return model, state[\\"data\\"], state[\\"metadata\\"]"},{"question":"You are provided with two CSV files, `sales_data.csv` and `inventory_data.csv`. These files contain the following data: `sales_data.csv`: ``` date,store,product,quantity_sold 2023-01-01,1,A,100 2023-01-01,1,B,150 2023-01-01,2,A,200 2023-01-01,2,C,300 2023-01-02,1,A,130 2023-01-02,1,B,120 2023-01-02,2,B,170 2023-01-02,2,C,180 ``` `inventory_data.csv`: ``` date,store,product,stock 2023-01-01,1,A,500 2023-01-01,1,B,600 2023-01-01,2,A,400 2023-01-01,2,C,300 2023-01-02,1,A,470 2023-01-02,1,B,530 2023-01-02,2,A,390 2023-01-02,2,B,420 2023-01-02,2,C,120 ``` Write a function `compute_inventory_status` that achieves the following objectives: 1. **Merge** the two DataFrames on `date`, `store`, and `product`. 2. **Calculate** the remaining stock after sales for each row. Create a new column `remaining_stock` to store this value. 3. **Identify** any inconsistencies where products sold exceed available stock and store this information in a separate DataFrame. This DataFrame should have columns `date`, `store`, `product`, `quantity_sold`, `stock`, and `remaining_stock` for those instances. 4. **Analyze** which store had the highest sales for each product on each day. 5. **Compare** the stock changes from `2023-01-01` to `2023-01-02`. Output a DataFrame that shows the differences in stock quantities between these two dates. 6. **Save** the results to new CSV files: - The merged DataFrame with the `remaining_stock` column to `merged_inventory_sales.csv`. - The inconsistencies DataFrame to `inconsistencies.csv`. # Function Signature ```python import pandas as pd def compute_inventory_status(sales_file: str, inventory_file: str) -> None: # Your implementation here pass ``` # Input - `sales_file` (str): Path to the sales data CSV file. - `inventory_file` (str): Path to the inventory data CSV file. # Output - This function does not return anything. Instead, it saves two CSV files as mentioned in step 6. # Constraints - Assume both CSV files are properly formatted and contain valid data. - The data only spans over the two provided dates, `2023-01-01` and `2023-01-02`. # Example Usage ```python compute_inventory_status(\'sales_data.csv\', \'inventory_data.csv\') ``` Upon running the function, it should generate the files `merged_inventory_sales.csv` and `inconsistencies.csv` in the current working directory.","solution":"import pandas as pd def compute_inventory_status(sales_file: str, inventory_file: str): # Read the input CSV files sales_df = pd.read_csv(sales_file) inventory_df = pd.read_csv(inventory_file) # Merge the two DataFrames on \'date\', \'store\', and \'product\' merged_df = pd.merge(sales_df, inventory_df, on=[\'date\', \'store\', \'product\']) # Calculate the remaining stock after sales for each row merged_df[\'remaining_stock\'] = merged_df[\'stock\'] - merged_df[\'quantity_sold\'] # Identify inconsistencies where products sold exceed available stock inconsistencies_df = merged_df[merged_df[\'remaining_stock\'] < 0] # Analyze which store had the highest sales for each product on each day highest_sales_df = sales_df.groupby([\'date\', \'product\']) .apply(lambda x: x.loc[x[\'quantity_sold\'].idxmax()]) .reset_index(drop=True) # Compare the stock changes from \'2023-01-01\' to \'2023-01-02\' stock_20230101 = inventory_df[inventory_df[\'date\'] == \'2023-01-01\'] stock_20230102 = inventory_df[inventory_df[\'date\'] == \'2023-01-02\'] stock_changes_df = pd.merge(stock_20230101, stock_20230102, on=[\'store\', \'product\'], suffixes=(\'_01\', \'_02\')) stock_changes_df[\'stock_change\'] = stock_changes_df[\'stock_02\'] - stock_changes_df[\'stock_01\'] # Save the results to new CSV files merged_df.to_csv(\'merged_inventory_sales.csv\', index=False) inconsistencies_df.to_csv(\'inconsistencies.csv\', index=False) return highest_sales_df, stock_changes_df # Returning for potential further use or testing"},{"question":"Color Space Conversions and Image Manipulation **Objective:** Use the `colorsys` module to manipulate the colors of an image by converting colors between different color spaces. **Problem Statement:** Write a function `manipulate_image_colors(image, conversion_function)` that takes: - `image`: A list of lists representing an image, where each inner list contains three floating-point values between 0 and 1, representing the RGB values of a pixel (e.g., `[[0.5, 0.4, 0.3], [0.1, 0.2, 0.3], ...]`). - `conversion_function`: A string representing the conversion function to apply to each pixel. The string can be one of the following: - \'rgb_to_yiq\' - \'yiq_to_rgb\' - \'rgb_to_hls\' - \'hls_to_rgb\' - \'rgb_to_hsv\' - \'hsv_to_rgb\' Your function should apply the specified conversion function to each pixel in the image and return the modified image. **Input:** - `image` (list of lists): A list of lists where each inner list contains three floating-point RGB values between 0 and 1. - `conversion_function` (string): The conversion function to apply. **Output:** - A list of lists representing the modified image, where each inner list contains three floating-point values representing the converted color values. **Constraints:** - You may assume the input image will have at least one pixel and that all color values are valid floating-point numbers between 0 and 1. - You need to handle invalid conversion functions gracefully by raising a `ValueError` with an appropriate error message. **Example:** ```python import colorsys def manipulate_image_colors(image, conversion_function): if conversion_function not in {\'rgb_to_yiq\', \'yiq_to_rgb\', \'rgb_to_hls\', \'hls_to_rgb\', \'rgb_to_hsv\', \'hsv_to_rgb\'}: raise ValueError(f\\"Invalid conversion function: {conversion_function}\\") conversion_func = getattr(colorsys, conversion_function) return [list(conversion_func(*pixel)) for pixel in image] # Example usage: image = [ [0.2, 0.4, 0.4], [0.5, 0.1, 0.3] ] converted_image = manipulate_image_colors(image, \'rgb_to_hsv\') print(converted_image) # Expected output: [[0.5, 0.5, 0.4], [0.9444444444444444, 0.8, 0.5]] ``` **Notes:** - You may directly use the `colorsys` module functions provided to perform the conversions. - Ensure your solution is efficient and handles the conversion correctly for each pixel in the image.","solution":"import colorsys def manipulate_image_colors(image, conversion_function): Manipulates the colors of an image by converting colors between different color spaces. Args: image (list of lists): A list of lists containing RGB values between 0 and 1. conversion_function (str): The conversion function to apply, one of \'rgb_to_yiq\', \'yiq_to_rgb\', \'rgb_to_hls\', \'hls_to_rgb\', \'rgb_to_hsv\', or \'hsv_to_rgb\'. Returns: list of lists: A list of lists with the converted color values. Raises: ValueError: If the conversion_function is not valid. if conversion_function not in {\'rgb_to_yiq\', \'yiq_to_rgb\', \'rgb_to_hls\', \'hls_to_rgb\', \'rgb_to_hsv\', \'hsv_to_rgb\'}: raise ValueError(f\\"Invalid conversion function: {conversion_function}\\") conversion_func = getattr(colorsys, conversion_function) return [list(conversion_func(*pixel)) for pixel in image]"},{"question":"Objective: Implement a command-line utility in Python that performs the following tasks: 1. Processes command line arguments to determine the source file, destination directory, and the number of lines to be copied from the source file to a new file in the destination directory. 2. Uses string pattern matching to find and replace certain patterns in the lines extracted from the source file. 3. Utilizes file system operations to copy the processed file to the destination directory. 4. Ensures that erroneous inputs and operations are handled gracefully, with appropriate error messages. Detailed Requirements: 1. **Command Line Arguments**: - The script should accept three command line arguments: - `source_file`: Path to the source file. - `destination_dir`: Path to the destination directory. - `num_lines`: The number of lines to be copied from the source file. - Example usage: ```sh python script.py source.txt target_dir 5 ``` 2. **String Pattern Matching**: - For each line copied from the source file, replace occurrences of the word \\"foo\\" with \\"bar\\" using regular expressions. 3. **File Operations**: - Read the specified number of lines from the source file. - Apply the string replacement to these lines. - Write the modified lines to a new file in the destination directory. The new file should have the same name as the source file with a suffix `_modified` before the file extension. For example, if the source file is `source.txt`, the new file should be `source_modified.txt`. 4. **Error Handling**: - If the source file does not exist, print an error message and exit. - If the destination directory does not exist, print an error message and exit. - If the number of lines specified is greater than the total number of lines in the source file, copy only the available lines and print a warning message. Constraints: - You must use the `argparse` module for processing command line arguments. - Use the `re` module for string pattern matching. - Use the `os` and `shutil` modules for file and directory operations. Function Signature: ```python def main(): pass if __name__ == \\"__main__\\": main() ``` Expected Input and Output: - **Input**: Command line arguments specifying the source file, destination directory, and number of lines. - **Output**: A new file in the destination directory with modified contents, and appropriate error or warning messages printed to the console. Example: Assuming `source.txt` contains the following lines: ``` foo is a placeholder This is a sample line foo should be replaced More lines with foo Last line without foo ``` Running the script with: ```sh python script.py source.txt target_dir 3 ``` Should create a file `target_dir/source_modified.txt` with the following contents: ``` bar is a placeholder This is a sample line bar should be replaced ``` Note: Ensure that your script can handle edge cases and provides meaningful error messages for invalid inputs.","solution":"import argparse import os import re import shutil def process_lines(lines): Processes lines to replace occurrences of \'foo\' with \'bar\'. return [re.sub(r\'foo\', \'bar\', line) for line in lines] def main(): parser = argparse.ArgumentParser(description=\\"Copy and modify lines from a source file to a destination directory.\\") parser.add_argument(\'source_file\', type=str, help=\'Path to the source file\') parser.add_argument(\'destination_dir\', type=str, help=\'Path to the destination directory\') parser.add_argument(\'num_lines\', type=int, help=\'Number of lines to be copied from the source file\') args = parser.parse_args() source_file = args.source_file destination_dir = args.destination_dir num_lines = args.num_lines # Check if source file exists if not os.path.isfile(source_file): print(f\\"Error: The source file \'{source_file}\' does not exist.\\") return # Check if destination directory exists if not os.path.isdir(destination_dir): print(f\\"Error: The destination directory \'{destination_dir}\' does not exist.\\") return # Read and process lines from the source file try: with open(source_file, \'r\') as f: lines = f.readlines() except IOError as e: print(f\\"Error reading source file: {e}\\") return # Check number of lines if num_lines > len(lines): print(f\\"Warning: Requested {num_lines} lines, but the source file only has {len(lines)} lines.\\") num_lines = len(lines) # Extract and process the required lines lines_to_copy = lines[:num_lines] processed_lines = process_lines(lines_to_copy) # Create the modified filename base_name = os.path.basename(source_file) name, ext = os.path.splitext(base_name) target_file = os.path.join(destination_dir, f\\"{name}_modified{ext}\\") # Write the processed lines to the new file try: with open(target_file, \'w\') as f: f.writelines(processed_lines) print(f\\"Successfully wrote to {target_file}\\") except IOError as e: print(f\\"Error writing to target file: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Floating Point Arithmetic and Precision Handling **Objective:** In this exercise, you will demonstrate your understanding of floating-point arithmetic issues and how to handle them using various Python tools explained in the provided documentation. **Problem Statement:** Write a Python function `accurate_sum` that takes a list of floating-point numbers and returns their sum with high precision. Additionally, write a function `demonstrate_error` that demonstrates the representation error by showing the inexact representation of simple floating-point arithmetic. **Function Specifications:** 1. `accurate_sum(numbers: List[float]) -> float` - **Input**: A list of floating-point numbers. - **Output**: A float that represents the accurate sum of the list using a precise method. - **Constraints**: The list can have up to (10^6) numbers, and each number can be as large as (10^9). 2. `demonstrate_error() -> str` - **Output**: A string that explains and provides an example of floating-point representation error with 0.1. **Example Usage:** ```python def accurate_sum(numbers): # Implementation using a method to ensure high precision pass def demonstrate_error(): # Implementation to demonstrate the representation error pass # Example case numbers = [0.1, 0.1, 0.1] print(accurate_sum(numbers)) # Should return 0.3 using a precise method print(demonstrate_error()) # Should explain and demonstrate the inaccuracy with 0.1 + 0.1 + 0.1 ``` **Explanation:** For `accurate_sum`, you may use the `decimal` module or `math.fsum()` to ensure the sum is calculated with high precision. For `demonstrate_error`, you need to show how adding 0.1 three times does not exactly equal 0.3 due to floating-point precision errors, and explain the underlying representation issue using the concepts from the provided documentation. **Notes:** - Make sure to handle edge cases where the list might be empty. - Ensure that the methods used for summation mitigate the loss of precision as described in the documentation.","solution":"from typing import List import decimal import math def accurate_sum(numbers: List[float]) -> float: Returns the sum of `numbers` with high precision. # Using math.fsum to ensure precision return math.fsum(numbers) def demonstrate_error() -> str: Demonstrates the floating-point representation error. example = 0.1 + 0.1 + 0.1 explanation = ( f\\"Due to floating-point precision issues, 0.1 + 0.1 + 0.1 does not equal exactly 0.3.n\\" f\\"Instead, it equals: {example}.n\\" \\"This is because floating-point numbers are represented as binary fractions, \\" \\"and some decimal fractions cannot be represented precisely as binary fractions.\\" ) return explanation"},{"question":"**Problem Statement: Encoding Harmony** You\'ll be tasked with implementing a function that takes a string and applies a series of encoding and decoding steps using the `base64` module methods, then returns the final decoded string. # Function Signature ```python def encoding_harmony(input_string: str) -> str: pass ``` # Input - A single input string `input_string` which can be: - ASCII characters only. - Of length between 1 and 100. # Output - A final decoded string after applying the steps described below. # Encoding/Decoding Steps 1. **Base64 Encode:** - Apply `base64.urlsafe_b64encode` to the input string after converting it to bytes (`input_string.encode(\'utf-8\')`). 2. **Base64 Decode:** - Decode the result of Step 1 using `base64.urlsafe_b64decode`. 3. **Base32 Encode:** - Encode the result of Step 2 using `base64.b32encode`. 4. **Base32 Decode:** - Decode the result of Step 3 using `base64.b32decode`. 5. **Base85 Encode:** - Encode the result of Step 4 using `base64.b85encode`. 6. **Base85 Decode:** - Decode the result of Step 5 using `base64.b85decode`. 7. **Final Result:** - Convert the bytes result of Step 6 back to a string using `.decode(\'utf-8\')`. # Constraints - If any step in the encoding process raises an exception, catch the exception and return the string \\"Error during encoding/decoding\\". # Example ```python input_string = \\"Hello, World!\\" output = encoding_harmony(input_string) print(output) # Should output: \\"Hello, World!\\" ``` # Notes - You must use the respective `base64` functions as described in each step. - Handle exceptions that might occur during decoding phases. - Ensure that the final output is a string, not bytes. **Write your implementation in the function `encoding_harmony` provided.**","solution":"import base64 def encoding_harmony(input_string: str) -> str: try: # Step 1: Base64 Encode step1 = base64.urlsafe_b64encode(input_string.encode(\'utf-8\')) # Step 2: Base64 Decode step2 = base64.urlsafe_b64decode(step1) # Step 3: Base32 Encode step3 = base64.b32encode(step2) # Step 4: Base32 Decode step4 = base64.b32decode(step3) # Step 5: Base85 Encode step5 = base64.b85encode(step4) # Step 6: Base85 Decode step6 = base64.b85decode(step5) # Step 7: Convert to string final_result = step6.decode(\'utf-8\') return final_result except Exception: return \\"Error during encoding/decoding\\""},{"question":"# Advanced Seaborn Plotting with `seaborn.objects` (seaborn) Objective You are tasked with creating a series of composite plots using the Seaborn library\'s Seaborn Objects that demonstrate an understanding of fundamental and advanced concepts of Seaborn. Dataset - Use the \\"penguins\\" dataset available through `seaborn.load_dataset(\\"penguins\\")`. Instructions 1. **Dataset Loading**: Load the \\"penguins\\" dataset. 2. **Plot Requirements**: 1. Create a composite plot consisting of the following layers: - Plot showing a dot at the average (mean) position of `body_mass_g` for each species, differentiated by `sex` and using different colors for each `sex`. - Add error bars representing the standard deviation (`sd`) around these means. 2. Create a faceted plot by `species` showing: - A line plot of `body_mass_g` by `sex` with markers at data points. - Error bars representing the standard deviation around the mean line. 3. Create another plot showing: - The range between `bill_depth_mm` and `bill_length_mm` for each penguin (using `penguin` as an identifier). - Color the ranges by `island`. 3. **Customization**: - Ensure that lines have the attribute `linewidth=2`. - Ensure that points have the attribute `pointsize=6`. Expected Output Your code should produce three plots as specified above using the Seaborn library. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Plot 1 ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Plot 2 ( so.PPlot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) # Plot 3 ( penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\", linewidth=2) ) ``` This task will assess your ability to work with Seaborn\'s objects-oriented plotting interface, understand the use of statistical transformations, and apply customizations to create informative visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Plot 1: Dot at the average position of body_mass_g for each species differentiated by sex and colored by sex, # with error bars for standard deviation ( so.Plot(penguins, x=\'body_mass_g\', y=\'species\', color=\'sex\') .add(so.Dot(pointsize=6), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\'sd\'), so.Dodge()) ) # Plot 2: Faceted plot by species showing line plot with markers of body_mass_g by sex with error bars for standard deviation ( so.Plot(penguins, x=\'sex\', y=\'body_mass_g\') .facet(\'species\') .add(so.Line(linewidth=2, marker=\'o\'), so.Agg()) .add(so.Range(), so.Est(errorbar=\'sd\')) ) # Plot 3: Showing range between bill_depth_mm and bill_length_mm for each penguin colored by island ( penguins .assign(penguin=range(len(penguins))) # Create a unique identifier for each penguin .pipe(so.Plot, x=\'penguin\', ymin=\'bill_depth_mm\', ymax=\'bill_length_mm\') .add(so.Range(linewidth=2), color=\'island\') )"},{"question":"# CSV File Processing Challenge In this task, you are required to utilize Python\'s `csv` module to read a given CSV file, perform specific data manipulations, and write the results to a new CSV file. This will assess your understanding of file reading/writing, manipulating data, and using the `csv` module effectively. Input: * You will receive a CSV file named `input.csv` containing the following columns: * `EmployeeID` (integer) * `Name` (string) * `Department` (string) * `Salary` (integer) Output: * You are to generate a new CSV file named `output.csv` which should contain: * A list of employees whose `Salary` is greater than the average salary of all employees. * The new CSV should have the following columns: * `EmployeeID` * `Name` * `Department` * `Salary` Constraints: * You must use Python\'s `csv` module to perform all file operations. * Ensure that your code handles possible I/O errors gracefully. Example: Given `input.csv`: ``` EmployeeID,Name,Department,Salary 1,John Doe,Engineering,70000 2,Jane Smith,Marketing,60000 3,Bob Johnson,Sales,50000 4,Alice Brown,Engineering,80000 5,Eve Davis,Marketing,75000 ``` The calculated average salary is 67000. The `output.csv` should contain: ``` EmployeeID,Name,Department,Salary 1,John Doe,Engineering,70000 4,Alice Brown,Engineering,80000 5,Eve Davis,Marketing,75000 ``` Instructions: 1. Read the contents of the `input.csv` file. 2. Calculate the average salary. 3. Filter and write employees with a salary greater than the average to `output.csv`. Code Implementation: ```python import csv def process_csv(input_file, output_file): try: with open(input_file, mode=\'r\', newline=\'\') as file: reader = csv.DictReader(file) employees = [row for row in reader] if not employees: raise ValueError(\\"The input CSV file is empty or not formatted correctly.\\") salaries = [int(emp[\'Salary\']) for emp in employees] average_salary = sum(salaries) / len(salaries) high_earners = [emp for emp in employees if int(emp[\'Salary\']) > average_salary] with open(output_file, mode=\'w\', newline=\'\') as file: fieldnames = [\'EmployeeID\', \'Name\', \'Department\', \'Salary\'] writer = csv.DictWriter(file, fieldnames=fieldnames) writer.writeheader() for emp in high_earners: writer.writerow(emp) except (IOError, ValueError) as e: print(f\\"Error processing file: {e}\\") # Example usage: # process_csv(\'input.csv\', \'output.csv\') ``` Test your implementation with the given example of `input.csv` and verify if `output.csv` matches the expected output.","solution":"import csv def process_csv(input_file, output_file): try: # Step 1: Read the contents of the input CSV file. with open(input_file, mode=\'r\', newline=\'\') as file: reader = csv.DictReader(file) employees = [row for row in reader] if not employees: raise ValueError(\\"The input CSV file is empty or not formatted correctly.\\") # Step 2: Calculate the average salary. salaries = [int(emp[\'Salary\']) for emp in employees] average_salary = sum(salaries) / len(salaries) # Step 3: Filter employees with salary greater than the average. high_earners = [emp for emp in employees if int(emp[\'Salary\']) > average_salary] # Step 4: Write the filtered employees to the output CSV file. with open(output_file, mode=\'w\', newline=\'\') as file: fieldnames = [\'EmployeeID\', \'Name\', \'Department\', \'Salary\'] writer = csv.DictWriter(file, fieldnames=fieldnames) writer.writeheader() for emp in high_earners: writer.writerow(emp) except (IOError, ValueError) as e: print(f\\"Error processing file: {e}\\") # Example usage: # process_csv(\'input.csv\', \'output.csv\')"},{"question":"# Advanced Coding Assessment: Serialization and Database Integration Objective Implement a Python program that demonstrates data persistence by using the `pickle` and `sqlite3` modules. The task is to create a program that stores and retrieves user data in a persistent manner. Problem Statement Design a Python application that collects user information, serializes the data using the `pickle` module, and stores the serialized data in an SQLite database. The program should also be able to retrieve and deserialize the data upon request. Requirements 1. **User Data Collection:** - Collect the following data from the user: - Name (string) - Age (integer) - Email (string) - Address (string) 2. **Serialization:** - Serialize the collected user data using the `pickle` module. 3. **Database Storage:** - Store the serialized user data in an SQLite database under a table named `user_data`. The table should have at least two columns: - `id` (INTEGER PRIMARY KEY AUTOINCREMENT) - `data` (BLOB) - for storing the serialized data. 4. **Data Retrieval:** - Implement functionality to retrieve and deserialize user data from the database using a provided user ID. 5. **Constraints:** - Ensure that all operations (serialization, database insertion, deserialization, and retrieval) handle exceptions gracefully and provide meaningful error messages. Input and Output - **Input:** - User data entered via standard input for the collection process. - User ID for data retrieval entered via standard input. - **Output:** - Confirmation messages for data insertion. - Deserialized user data output to the console in a readable format. Function Definitions You are required to implement the following functions: 1. `collect_user_data() -> dict`: - Collect user information from standard input and return as a dictionary. 2. `serialize_data(data: dict) -> bytes`: - Serialize the input dictionary using `pickle` and return the serialized data as bytes. 3. `store_data_in_db(serialized_data: bytes) -> None`: - Store the serialized data in the `user_data` table of an SQLite database. 4. `retrieve_data_from_db(user_id: int) -> dict`: - Retrieve and deserialize data from the database given a user ID and return the data as a dictionary. 5. `main()`: - Main driver function to coordinate data collection, serialization, database storage, and retrieval based on user choice. Example Usage ```python if __name__ == \\"__main__\\": main() ``` - When running `main()`, the program should prompt the user to either: 1. Enter new user data and store it in the database. 2. Retrieve and display data for a given user ID from the database. Performance Requirements - Ensure efficient handling of data serialization and database operations. - The solution should gracefully handle a reasonable volume of user data entries. Implement this program to demonstrate your understanding of Python data persistence using `pickle` and `sqlite3`.","solution":"import pickle import sqlite3 import os def collect_user_data(): Collects user information from standard input and returns as a dictionary. user_data = {} user_data[\'name\'] = input(\\"Enter your name: \\") user_data[\'age\'] = int(input(\\"Enter your age: \\")) user_data[\'email\'] = input(\\"Enter your email: \\") user_data[\'address\'] = input(\\"Enter your address: \\") return user_data def serialize_data(data): Serializes the input dictionary using `pickle` and returns the serialized data as bytes. return pickle.dumps(data) def initialize_database(): Initializes the SQLite database and creates the `user_data` table if not exists. conn = sqlite3.connect(\'user_data.db\') cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS user_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, data BLOB )\'\'\') conn.commit() conn.close() def store_data_in_db(serialized_data): Stores the serialized data in the `user_data` table of the SQLite database. initialize_database() conn = sqlite3.connect(\'user_data.db\') cursor = conn.cursor() cursor.execute(\\"INSERT INTO user_data (data) VALUES (?)\\", (serialized_data,)) conn.commit() conn.close() print(f\\"Data successfully stored with ID: {cursor.lastrowid}\\") def retrieve_data_from_db(user_id): Retrieves and deserializes data from the database given a user ID and returns the data as a dictionary. conn = sqlite3.connect(\'user_data.db\') cursor = conn.cursor() cursor.execute(\\"SELECT data FROM user_data WHERE id=?\\", (user_id,)) row = cursor.fetchone() conn.close() if row: return pickle.loads(row[0]) else: raise ValueError(\\"User ID not found in the database.\\") def main(): while True: choice = input(\\"Enter 1 to add user data or 2 to retrieve user data: \\") if choice == \'1\': user_data = collect_user_data() serialized_data = serialize_data(user_data) store_data_in_db(serialized_data) elif choice == \'2\': user_id = int(input(\\"Enter user ID to retrieve data: \\")) try: user_data = retrieve_data_from_db(user_id) print(\\"Retrieved User Data:\\", user_data) except ValueError as e: print(e) else: print(\\"Invalid choice, please try again.\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a custom import hook to dynamically load and cache modules. Problem Statement You are tasked with creating a custom import system that caches module imports and allows dynamic loading of modules. Specifically, you need to: 1. Implement a custom meta path finder and a path entry finder. 2. Cache imported modules so subsequent imports of the same module use the cached version. 3. Support loading modules from a specified directory (`custom_path`). # Requirements - Create a custom meta path finder that: - Checks the custom path for modules. - Caches the modules in a custom cache. - Create a path entry finder that: - Loads and executes the module from the custom path. - Adds the loaded module to both the custom cache and `sys.modules`. # Expected Input and Output - **Input:** - `custom_path`: A string indicating the directory to search for modules (e.g., \\"/path/to/modules\\"). - `module_name`: The name of the module to be imported. - **Output:** - The imported module should be present in `sys.modules` and the custom cache. # Constraints - Your implementation should handle cases where the module is not found and raise an appropriate `ModuleNotFoundError`. - The custom finder and loader should be compatible with Python\'s import machinery. # Implementation Steps 1. **CustomMetaPathFinder Class:** - Implement the `find_spec` method to handle searching the custom path. 2. **CustomPathEntryFinder Class:** - Implement the `find_spec` method to load and execute the module from the custom path. 3. **Register Custom Finder:** - Add the custom meta path finder to `sys.meta_path`. # Example ```python import sys import os import importlib.util # Custom Cache custom_cache = {} # Custom Meta Path Finder class CustomMetaPathFinder: def __init__(self, custom_path): self.custom_path = custom_path def find_spec(self, fullname, path, target=None): if fullname in custom_cache: return custom_cache[fullname] module_path = os.path.join(self.custom_path, fullname + \'.py\') if os.path.exists(module_path): loader = CustomPathEntryFinder(module_path) spec = importlib.util.spec_from_file_location(fullname, module_path, loader=loader) custom_cache[fullname] = spec return spec raise ModuleNotFoundError(f\\"No module named \'{fullname}\'\\") # Custom Path Entry Finder class CustomPathEntryFinder: def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None def exec_module(self, module): with open(self.module_path, \'r\') as file: exec(file.read(), module.__dict__) # Register Custom Meta Path Finder def register_custom_importer(custom_path): sys.meta_path.insert(0, CustomMetaPathFinder(custom_path)) # Test the custom importer if __name__ == \\"__main__\\": custom_path = \\"/path/to/modules\\" # Adjust this path to your module directory register_custom_importer(custom_path) # Assuming there\'s a module named \'example\' in the custom path import example example.some_function() ``` # Explanation - The `CustomMetaPathFinder` checks if the module is in the custom cache or the specified custom path. - The `CustomPathEntryFinder` loads and executes the module code, simulating how Python\'s built-in loaders work. - The custom cache ensures that modules are only loaded once and subsequent imports use the cached module. - Registering the custom importer modifies `sys.meta_path` to include the custom finder.","solution":"import sys import os import importlib.util # Custom Cache custom_cache = {} class CustomMetaPathFinder: def __init__(self, custom_path): self.custom_path = custom_path def find_spec(self, fullname, path, target=None): if fullname in custom_cache: return custom_cache[fullname] module_path = os.path.join(self.custom_path, fullname + \'.py\') if os.path.exists(module_path): loader = CustomPathEntryFinder(module_path) spec = importlib.util.spec_from_file_location(fullname, module_path, loader=loader) custom_cache[fullname] = spec return spec return None class CustomPathEntryFinder: def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None def exec_module(self, module): with open(self.module_path, \'r\') as file: exec(file.read(), module.__dict__) def register_custom_importer(custom_path): sys.meta_path.insert(0, CustomMetaPathFinder(custom_path))"},{"question":"# Advanced Python Coding Assessment: Managing Tar Archives **Objective:** To assess your understanding and proficiency in using the `tarfile` module to handle tar archives, including creating, extracting, and manipulating files within tar archives. **Question:** You are required to implement a function called `tarfile_operations` that performs the following operations on tar archives: 1. Creates a new tar file with the specified `tarfile_name`. 2. Adds the provided list of `file_paths` to the tar archive. 3. Extracts all files from the tar archive to a specified `extract_path`. 4. Lists the contents of the tar archive and returns them as a list of filenames. **Function Signature:** ```python def tarfile_operations(tarfile_name: str, file_paths: list, extract_path: str) -> list: pass ``` **Input:** - `tarfile_name` (str): The name of the tar file to create. - `file_paths` (list): A list of file paths to be added to the tar archive. - `extract_path` (str): Path to the directory where files will be extracted. **Output:** - A list of filenames present in the tar archive. **Constraints:** - All specified file paths in `file_paths` exist and are accessible. - The function should properly handle exceptions and ensure resources (like file handles) are closed appropriately. - Use the `tarfile` module for all tar operations. **Example:** ```python # Sample file structure # /example_dir/ # |-- file1.txt # |-- file2.txt # |-- subdir/ # |-- file3.txt tarfile_name = \\"example_archive.tar.gz\\" file_paths = [\\"/example_dir/file1.txt\\", \\"/example_dir/file2.txt\\", \\"/example_dir/subdir/file3.txt\\"] extract_path = \\"/extract_here/\\" result = tarfile_operations(tarfile_name, file_paths, extract_path) # Expected output # result = [\'example_dir/file1.txt\', \'example_dir/file2.txt\', \'example_dir/subdir/file3.txt\'] ``` **Guidelines:** - Use `tarfile.open` in the appropriate mode to create and read tar files. - Use context managers to ensure files are appropriately closed after operations. - Implement proper error handling. - Ensure the extracted files retain their original structure relative to the extract_path. **Note:** - You may assume that the directories and files exist and have the correct permissions. - Focus on handling various operations of the `tarfile` module as specified.","solution":"import os import tarfile def tarfile_operations(tarfile_name: str, file_paths: list, extract_path: str) -> list: Creates a tar archive, adds files to it, extracts the files, and lists the contents. Parameters: tarfile_name (str): The name of the tar file to create. file_paths (list): A list of file paths to be added to the tar archive. extract_path (str): Path to the directory where files will be extracted. Returns: list: A list of filenames present in the tar archive. # Create tar file and add specified files with tarfile.open(tarfile_name, \'w:gz\') as tar: for file_path in file_paths: tar.add(file_path, arcname=os.path.relpath(file_path)) # Extract all files from the tar archive with tarfile.open(tarfile_name, \'r:gz\') as tar: tar.extractall(path=extract_path) # List the contents of the tar archive with tarfile.open(tarfile_name, \'r:gz\') as tar: filenames = tar.getnames() return filenames"},{"question":"**Question: Implement a POP3 Email Client Using Python\'s `poplib`** **Objective:** Create a simple command-line email client that connects to a POP3 server, logs in, retrieves email messages, and allows the user to interact with their mailbox. **Instructions:** 1. Implement a class `EmailClient` that uses the `poplib.POP3` or `poplib.POP3_SSL` class to connect to a POP3 server. 2. Your class should have the following methods: - `__init__(self, host, port, use_ssl)`: Initializes the connection. If `use_ssl` is `True`, use `poplib.POP3_SSL`, otherwise use `poplib.POP3`. - `login(self, username, password)`: Logs into the server using the provided username and password. - `get_mail_count(self)`: Returns the number of messages in the mailbox. - `list_messages(self)`: Lists all message numbers and sizes. - `retrieve_message(self, message_number)`: Retrieves and returns the full content of the specified message. - `delete_message(self, message_number)`: Flags the specified message for deletion. - `quit(self)`: Logs out and closes the connection. **Input:** - The class constructor takes three parameters: `host` (string), `port` (integer), and `use_ssl` (boolean). - The `login` method takes two parameters: `username` (string) and `password` (string). - The methods `retrieve_message` and `delete_message` take one parameter: `message_number` (integer). **Output:** - `get_mail_count` returns an integer representing the number of messages. - `list_messages` returns a list of tuples, where each tuple contains a message number and size. - `retrieve_message` returns a string containing the full content of the message. - `delete_message` performs the deletion operation and does not return anything. **Constraints:** - Your solution should handle exceptions appropriately, especially for network issues and server errors. - Ensure the connection is secure if `use_ssl` is `True`. **Example Usage:** ```python client = EmailClient(\'pop.example.com\', 995, True) client.login(\'user@example.com\', \'password\') # Get the number of emails print(client.get_mail_count()) # List all messages print(client.list_messages()) # Retrieve the first email print(client.retrieve_message(1)) # Delete the first email client.delete_message(1) # Close the connection client.quit() ``` **Important:** - Do not store the username and password in plain text in your code. - This is a simplified use case. In real-world applications, consider more robust error handling and security measures.","solution":"import poplib from email.parser import BytesParser class EmailClient: def __init__(self, host, port, use_ssl=True): self.host = host self.port = port if use_ssl: self.server = poplib.POP3_SSL(host, port) else: self.server = poplib.POP3(host, port) def login(self, username, password): self.server.user(username) self.server.pass_(password) def get_mail_count(self): return len(self.server.list()[1]) def list_messages(self): return [tuple(line.decode().split()) for line in self.server.list()[1]] def retrieve_message(self, message_number): response, lines, _ = self.server.retr(message_number) message_data = b\'n\'.join(lines) message = BytesParser().parsebytes(message_data) return message def delete_message(self, message_number): self.server.dele(message_number) def quit(self): self.server.quit()"},{"question":"**Objective**: Implement a robust Python function using `set` and `frozenset` to solve the following problem. **Question**: You are given a list of tuples, where each tuple contains two elements: a string and a list of integers. Your task is to write a function `group_frozensets(tuples_list)` that processes this list in the following manner: 1. For each unique string, create a `frozenset` containing all integers associated with that string across all tuples. 2. Return a dictionary where the keys are the unique strings, and the values are the corresponding `frozenset` of integers. **Constraints**: - Each string in the tuple is guaranteed to be unique within a single tuple but may repeat across multiple tuples. - The list of integers may contain duplicate values within a single tuple, but these should be stored uniquely in the `frozenset`. **Input Format**: - A list of tuples `tuples_list`, where each tuple is in the format `(str, List[int])`. **Output Format**: - A dictionary with strings as keys and `frozenset` of integers as values. **Example**: ```python def group_frozensets(tuples_list): pass # Implement this function # Example usage: tuples_list = [ (\\"apple\\", [1, 2, 3]), (\\"banana\\", [2, 3, 4]), (\\"apple\\", [2, 5]) ] # Expected output: # { # \\"apple\\": frozenset({1, 2, 3, 5}), # \\"banana\\": frozenset({2, 3, 4}) # } print(group_frozensets(tuples_list)) ``` **Evaluation Criteria**: - Correctness: The function should construct and return the correct dictionary as per the requirements. - Efficiency: The function should handle large inputs efficiently. - Code Quality: The function should be written clearly and follow Python best practices. **Notes**: - Make use of `set` and `frozenset` operations to ensure uniqueness and immutability where appropriate. - Avoid using additional collections or data structures not mentioned in the problem statement.","solution":"def group_frozensets(tuples_list): Process a list of tuples containing a string and a list of integers. For each unique string, create a frozenset containing all integers associated with that string across all tuples. Args: tuples_list (list of tuples): A list where each tuple contains a string and a list of integers. Returns: dict: A dictionary where keys are unique strings and values are frozenset of integers. result = {} for key, int_list in tuples_list: if key not in result: result[key] = set() result[key].update(int_list) # Convert sets to frozensets for key in result: result[key] = frozenset(result[key]) return result"},{"question":"# Pandas Sparse Series Manipulation You are given a CSV file named `data.csv` containing a large dataset with missing or zero values, which you need to handle efficiently using pandas sparse data structures to minimize memory usage. This file contains the following columns: - `A`: integer values (contains many zeros). - `B`: float values with many `NaN`s. - `C`: string values (sparse due to repetitions). Your task is to: 1. Load the dataset from the CSV file. 2. Convert columns `A` and `B` into sparse representations. 3. Determine the memory usage before and after conversion. 4. Perform basic arithmetic operations on the sparse series. 5. Convert the modified sparse series back to a dense format and save the updated dataset to a new CSV file named `processed_data.csv`. **Function Signature:** ```python def process_sparse_data(input_csv: str, output_csv: str) -> None: pass ``` # Details: - **Input:** - `input_csv`: Path to the input CSV file (`str`). - `output_csv`: Path to the output CSV file (`str`). - **Output:** - No return value. The result should be saved in the specified `output_csv`. # Constraints: - Use `SparseArray` and `SparseDtype` from pandas for sparse data representation. - Ensure the transformations handle large datasets efficiently. - Memory usage before and after conversion must be printed. # Example: Assume `data.csv` contains: ``` A,B,C 0,0.5,x 0,1.0,y 0,,z 3,,z 0,2.5,x ``` **Expected Output:** - Before conversion memory usage: XX bytes - After conversion memory usage: YY bytes **Content of `processed_data.csv`:** ``` A,B,C 0,0.5,x 0,1.0,y 0,,z 3,,z 0,2.5,x ``` **Note:** Actual memory usage values will depend on the specific dataset size and structure. **Hint:** Use `.memory_usage()` method to get memory usage information for both dense and sparse formats.","solution":"import pandas as pd def process_sparse_data(input_csv: str, output_csv: str) -> None: # Step 1: Load the dataset from the CSV file df = pd.read_csv(input_csv) # Step 2: Convert columns \'A\' and \'B\' into sparse representations df[\'A\'] = pd.arrays.SparseArray(df[\'A\'], dtype=pd.SparseDtype(int)) df[\'B\'] = pd.arrays.SparseArray(df[\'B\'], dtype=pd.SparseDtype(float)) # Step 3: Determine the memory usage before and after conversion memory_before = df.memory_usage(deep=True).sum() print(f\\"Before conversion memory usage: {memory_before} bytes\\") memory_after = df.memory_usage(deep=True).sum() print(f\\"After conversion memory usage: {memory_after} bytes\\") # Step 4: Perform basic arithmetic operations on the sparse series (example operation) df[\'A\'] = df[\'A\'] * 2 df[\'B\'] = df[\'B\'] + 1.0 # Step 5: Convert the modified sparse series back to a dense format and save the dataset df[\'A\'] = df[\'A\'].sparse.to_dense() df[\'B\'] = df[\'B\'].sparse.to_dense() df.to_csv(output_csv, index=False)"},{"question":"# Exception Management in Python Python\'s C API provides extensive functionality to manage exceptions. Your task is to simulate some of this functionality in Python. Specifically, you need to create a class to handle exception context stacks and provide utility methods that mimic the behavior of the C API functions described in the documentation. Write a Python class `ExceptionManager` with the following methods: 1. `set_exception(exception_type, message)`: Simulates the behavior of `PyErr_SetString`. It should set the type of the current exception and the associated message. 2. `clear_exception()`: Simulates `PyErr_Clear`. It should clear the current exception. 3. `fetch_exception()`: Simulates `PyErr_Fetch`. It should return the current exception type and message and then clear them. 4. `print_exception()`: Simulates `PyErr_Print()`. It should print the current exception and traceback to the standard output and then clear them. 5. `raise_exception()`: Simulates `PyErr_CheckSignals` by simulating dealing with and printing an exception if found. Additionally, you need to create a custom exception, `CustomException`, for use with the `ExceptionManager`. Implement the `ExceptionManager` class with methods that internally use stack-based structures to manage exceptions. Below are some constraints and requirements: # Constraints: - Use standard Python exception types and messages. - You do not need to use actual C-extensions but should understand how exceptions are managed at a higher level based on the provided documentation. - The methods should be able to handle only one exception at a time; nested exceptions are not required. - Demonstrate the use of `ExceptionManager` with a few test cases including standard exceptions and `CustomException`. # Example Usage: ```python try: manager = ExceptionManager() # Set an exception manager.set_exception(IndexError, \\"List index out of range\\") # Fetch and print the exception etype, evalue = manager.fetch_exception() print(f\\"Fetched Exception: {etype.__name__}: {evalue}\\") # Set a custom exception manager.set_exception(CustomException, \\"This is a custom error\\") # Raise and print the exception manager.raise_exception() # Clear the exception manager.clear_exception() # Try to raise again to show it\'s cleared manager.raise_exception() # Should print \\"No exception set.\\" except Exception as e: print(f\\"Unexpected exception: {e}\\") ``` # Required Output: The Python class must properly simulate C API Exception Handling as described, and the example usage should provide output confirming the correct management of exceptions.","solution":"class CustomException(Exception): pass class ExceptionManager: def __init__(self): self.current_exception_type = None self.current_exception_message = None def set_exception(self, exception_type, message): self.current_exception_type = exception_type self.current_exception_message = message def clear_exception(self): self.current_exception_type = None self.current_exception_message = None def fetch_exception(self): etype = self.current_exception_type evalue = self.current_exception_message self.clear_exception() return etype, evalue def print_exception(self): if self.current_exception_type and self.current_exception_message: print(f\\"Exception: {self.current_exception_type.__name__}: {self.current_exception_message}\\") self.clear_exception() else: print(\\"No exception set.\\") def raise_exception(self): if self.current_exception_type and self.current_exception_message: raise self.current_exception_type(self.current_exception_message) else: print(\\"No exception set.\\")"},{"question":"You are given a large text corpus and asked to perform several text processing tasks using Python. Your solution should demonstrate proficiency in string manipulation, regular expressions, and text formatting as described in the Python310 documentation. Task 1. **Extracting Specific Patterns**: - Extract all email addresses from the given text. An email address consists of a local part, an \'@\' symbol, and a domain part (e.g., `example@domain.com`). 2. **Summarizing Content**: - Find all sentences ending with a specific word and count the occurrences of those sentences. A sentence is defined as a sequence of characters ending with a period (`.`) or a question mark (`?`). 3. **Reformatting the Text**: - Replace all phone numbers in the text with a standardized format `+1-XXX-XXX-XXXX`. Phone numbers can appear in various formats (e.g., `(123) 456-7890`, `123-456-7890`, `123.456.7890`). 4. **Generating a Summary Report**: - Create a summary report that includes: - The total number of email addresses found. - The total number of sentences ending with the given word. - The first three email addresses found. Input and Output - **Input**: - A multiline string `text` containing the text corpus. - A string `word` specifying the word used to identify sentences. - **Output**: - A formatted string representing the summary report. Constraints - All email addresses in the text are valid. - You can assume that the given text corpus is not empty. Example ```python text = Hello john.doe@example.com, we are contacting you regarding your account. Please call (123) 456-7890 if you have any questions. Also, reach out to jane.doe@company.com or admin@service.org for support. You can also email us at support@service.org. This message ends with example. Have a good day? word = \\"example\\" def process_text(text: str, word: str) -> str: # Implement the required functionality here pass summary_report = process_text(text, word) print(summary_report) ``` Expected output: ``` Total email addresses found: 4 Total sentences ending with \'example\': 1 First three email addresses: john.doe@example.com, jane.doe@company.com, admin@service.org ``` Notes - Be sure to utilize regular expressions and string formatting techniques effectively to meet the requirements. - Pay attention to edge cases such as different phone number formats and sentences spanning multiple lines.","solution":"import re def process_text(text: str, word: str) -> str: # Extract all email addresses email_pattern = r\'[w.-]+@[w.-]+.w+\' emails = re.findall(email_pattern, text) # Find all sentences ending with the specified word and count occurrences sentence_pattern = r\'[^.?!]*(?:[.?!]s|s)\' + re.escape(word) + r\'b[^.?!]*[.?!]\' sentences_with_word = re.findall(sentence_pattern, text, re.IGNORECASE) # Replace all phone numbers with standardized format phone_patterns = [ r\'(d{3}) d{3}-d{4}\', # (123) 456-7890 r\'d{3}-d{3}-d{4}\', # 123-456-7890 r\'d{3}.d{3}.d{4}\' # 123.456.7890 ] standardized_phone = \\"+1-XXX-XXX-XXXX\\" for pattern in phone_patterns: text = re.sub(pattern, standardized_phone, text) # Generate summary report total_emails = len(emails) total_sentences_ending_with_word = len(sentences_with_word) first_three_emails = \', \'.join(emails[:3]) summary_report = f\\"Total email addresses found: {total_emails}n\\" summary_report += f\\"Total sentences ending with \'{word}\': {total_sentences_ending_with_word}n\\" summary_report += f\\"First three email addresses: {first_three_emails}\\" return summary_report"},{"question":"# Advanced Python System Utilities Assignment **Objective:** Implement a command-line Python utility for managing system log files. This assignment will test your ability to use Python\'s `os` module for file operations, `argparse` for command-line parsing, and `logging` for logging operations. **Task Description:** You are required to create a command-line utility named `log_manager.py` that performs the following functions: 1. **List** all log files in a specified directory. 2. **Show** the content of a specified log file. 3. **Archive** old log files by moving them to an \'archive\' directory if they are older than a specified number of days. 4. **Delete** log files that are older than a specified number of days. **Instructions:** 1. **Command-line Arguments:** - The utility should accept the following command-line arguments using `argparse`: - `--action` (`list`, `show`, `archive`, `delete`) - The type of action to perform. - `--directory` (`str`) - The path to the log directory. - `--file` (`str`) - The name of the log file (only for `show`). - `--days` (`int`) - The threshold number of days for `archive` and `delete` actions. 2. **Logging:** - Use Python’s `logging` module to log the actions taken, with the log level set to INFO. - Log details such as the action performed, the file(s) affected, and any errors encountered. 3. **Constraints and Limitations:** - Assume that log files have the extension `.log`. - The log directory and \'archive\' directory should exist. Create the \'archive\' directory if it does not exist when performing the `archive` action. - Handle exceptions gracefully and log error messages appropriately. 4. **Input and Output:** - Expected input formats are defined by the command-line arguments. - The output should primarily be logged to a `log_manager.log` file in the current working directory, utilizing the `logging` module. **Example Usage:** ``` python log_manager.py --action list --directory /var/logs python log_manager.py --action show --directory /var/logs --file system.log python log_manager.py --action archive --directory /var/logs --days 30 python log_manager.py --action delete --directory /var/logs --days 30 ``` **Performance Requirements:** - Be mindful of efficiently handling file operations to avoid unnecessary overhead, especially in directories with many files. Use appropriate methods from the `os` module for listing, moving, and deleting files. **Assessment Criteria:** - Correct usage of `argparse` for command-line argument handling. - Effective use of `os` module for file system operations. - Proper logging of actions using the `logging` module. - Code readability and proper handling of exceptions.","solution":"import os import argparse import logging from datetime import datetime, timedelta import shutil # Configure Logging logging.basicConfig(filename=\'log_manager.log\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def list_logs(directory): try: logs = [f for f in os.listdir(directory) if f.endswith(\'.log\')] logging.info(f\\"Listed {len(logs)} log files in directory {directory}.\\") for log in logs: print(log) except Exception as e: logging.error(f\\"Error listing log files in {directory}: {e}\\") def show_log(directory, filename): try: path = os.path.join(directory, filename) with open(path, \'r\') as file: content = file.read() logging.info(f\\"Showed content of log file {filename}.\\") print(content) except Exception as e: logging.error(f\\"Error showing log file {filename}: {e}\\") def archive_logs(directory, days): try: archive_dir = os.path.join(directory, \'archive\') if not os.path.exists(archive_dir): os.makedirs(archive_dir) threshold_date = datetime.now() - timedelta(days=days) for log in os.listdir(directory): if log.endswith(\'.log\'): log_path = os.path.join(directory, log) log_mtime = datetime.fromtimestamp(os.path.getmtime(log_path)) if log_mtime < threshold_date: shutil.move(log_path, archive_dir) logging.info(f\\"Archived log file {log}.\\") else: logging.info(f\\"Skipped archiving log file {log}, not older than {days} days.\\") except Exception as e: logging.error(f\\"Error archiving log files older than {days} days: {e}\\") def delete_logs(directory, days): try: threshold_date = datetime.now() - timedelta(days=days) for log in os.listdir(directory): if log.endswith(\'.log\'): log_path = os.path.join(directory, log) log_mtime = datetime.fromtimestamp(os.path.getmtime(log_path)) if log_mtime < threshold_date: os.remove(log_path) logging.info(f\\"Deleted log file {log}.\\") else: logging.info(f\\"Skipped deleting log file {log}, not older than {days} days.\\") except Exception as e: logging.error(f\\"Error deleting log files older than {days} days: {e}\\") def main(): parser = argparse.ArgumentParser(description=\'Manage log files.\') parser.add_argument(\'--action\', required=True, choices=[\'list\', \'show\', \'archive\', \'delete\'], help=\'The action to perform on the log files.\') parser.add_argument(\'--directory\', required=True, type=str, help=\'The directory containing the log files.\') parser.add_argument(\'--file\', type=str, help=\'The log file to show (only for show action).\') parser.add_argument(\'--days\', type=int, help=\'Threshold days for archive and delete actions.\') args = parser.parse_args() if args.action == \'list\': list_logs(args.directory) elif args.action == \'show\': if not args.file: parser.error(\\"--file is required for show action\\") show_log(args.directory, args.file) elif args.action == \'archive\': if args.days is None: parser.error(\\"--days is required for archive action\\") archive_logs(args.directory, args.days) elif args.action == \'delete\': if args.days is None: parser.error(\\"--days is required for delete action\\") delete_logs(args.directory, args.days) if __name__ == \'__main__\': main()"},{"question":"Priority Task Manager You have been given the task to implement a priority-based task manager using Python\'s `queue` module. The task manager should handle tasks with different priority levels and ensure that tasks with the highest priority are processed first. # Task 1. Implement a `TaskManager` class that uses `queue.PriorityQueue` to manage tasks. 2. The `TaskManager` class should support the following functionalities: - Add a new task with a specified priority. - Get the next task based on priority. - Indicate that a task is done. - Block until all tasks have been completed. # Specifications - **Class**: `TaskManager` - **Methods**: - `__init__(self)`: Initializes an empty priority queue. - `add_task(self, task: str, priority: int)`: Adds a task with the given priority to the queue. - `get_next_task(self) -> str`: Retrieves and returns the next task (the one with the highest priority). - `task_done(self)`: Indicates completion of the most recently retrieved task. - `wait_for_all_tasks(self)`: Blocks until all tasks have been marked as done. # Constraints 1. `priority` is an integer where a lower number means a higher priority. 2. Tasks are represented as strings. 3. There can be up to 1000 tasks in the queue at any time. # Example Usage ```python tm = TaskManager() tm.add_task(\\"Write unit tests\\", 2) tm.add_task(\\"Refactor code\\", 1) tm.add_task(\\"Update documentation\\", 3) task = tm.get_next_task() # Should return \\"Refactor code\\" tm.task_done() task = tm.get_next_task() # Should return \\"Write unit tests\\" tm.task_done() tm.wait_for_all_tasks() # Should block until all tasks are done task = tm.get_next_task() # Should return \\"Update documentation\\" ``` # Performance Requirements The implementation must correctly handle concurrent producers (adding tasks) and consumers (retrieving tasks) using threading. You may refer to the provided documentation for handling queue operations and exceptions appropriately.","solution":"import queue class TaskManager: def __init__(self): # Initialize a priority queue self._queue = queue.PriorityQueue() def add_task(self, task, priority): Adds a task with the specified priority to the queue. :param task: The name of the task to add. :param priority: The priority of the task (lower number means higher priority). # Since python\'s PriorityQueue is ascending, we use (priority, task) as the tuple. self._queue.put((priority, task)) def get_next_task(self): Retrieves and returns the next task based on the highest priority. :return: The name of the task with the highest priority. priority, task = self._queue.get() return task def task_done(self): Indicates that the most recently retrieved task is done. self._queue.task_done() def wait_for_all_tasks(self): Blocks until all tasks have been marked as done. self._queue.join()"},{"question":"# Complex Number Operations in PyTorch **Objective:** Write a function that takes two complex tensors, computes their element-wise product, and returns the magnitude and angle of the resulting tensor. **Function Signature:** ```python import torch def complex_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor): pass ``` **Input:** - `tensor1: torch.Tensor` - A complex tensor of shape `(n, m)` where each element is of type `torch.cfloat` or `torch.cdouble`. - `tensor2: torch.Tensor` - Another complex tensor of shape `(n, m)` with the same dtype as `tensor1`. **Output:** - Returns a tuple of two tensors: 1. `magnitude: torch.Tensor` - A tensor of shape `(n, m)` representing the magnitude of the element-wise product of `tensor1` and `tensor2`. 2. `angle: torch.Tensor` - A tensor of shape `(n, m)` representing the angle of the element-wise product of `tensor1` and `tensor2`. **Constraints:** - The tensors `tensor1` and `tensor2` will have the same shape and dtype. - Ensure the operations make use of PyTorch\'s vectorized functions for efficiency. **Example:** ```python tensor1 = torch.tensor([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]], dtype=torch.cfloat) tensor2 = torch.tensor([[1 - 1j, 2 - 2j], [3 - 3j, 4 - 4j]], dtype=torch.cfloat) magnitude, angle = complex_operations(tensor1, tensor2) print(magnitude) # Output: tensor([[ 2.0000, 8.0000], # [18.0000, 32.0000]]) print(angle) # Output: tensor([[ 0.0000, 0.0000], # [ 0.0000, 0.0000]]) ``` **Notes:** - Use `torch.abs` to compute the magnitude of the complex tensor. - Use `torch.angle` to compute the angle of the complex tensor. - Remember to handle the operations efficiently using PyTorch\'s built-in functions.","solution":"import torch def complex_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor): Computes the element-wise product of two complex tensors and returns the magnitude and angle of the result. Parameters: tensor1 (torch.Tensor): A complex tensor of shape (n, m). tensor2 (torch.Tensor): Another complex tensor of shape (n, m). Returns: (torch.Tensor, torch.Tensor): A tuple containing two tensors of shape (n, m) which are: - Magnitude of the element-wise product. - Angle of the element-wise product. product = tensor1 * tensor2 magnitude = torch.abs(product) angle = torch.angle(product) return magnitude, angle"},{"question":"# PyTorch Storage Manipulation **Problem Statement:** You are tasked with implementing two functions to demonstrate your understanding of PyTorch `torch.UntypedStorage` and their manipulation. The functions will leverage the underlying storage to directly modify or compare tensor data. 1. **Function 1: zero_out_storage**: - **Input:** A tensor `t` of any shape with float datatype. - **Output:** The same tensor `t` but with all its elements set to zero using direct manipulation of its `torch.UntypedStorage`. 2. **Function 2: compare_storages**: - **Input:** Two tensors `t1` and `t2` which may or may not share the same underlying storage. - **Output:** `True` if `t1` and `t2` share the same underlying storage, and `False` otherwise. **Constraints:** - You must directly manipulate the tensor\'s storage where specified. - Do not use high-level tensor functions like `torch.zeros_like` for the first function. - You can assume the inputs are always tensors with float elements for simplifying the problem. **Function Signatures:** ```python import torch def zero_out_storage(t: torch.Tensor) -> torch.Tensor: Sets all elements of the tensor \'t\' to zero using direct manipulation of its storage. Parameters: t (torch.Tensor): Input tensor of any shape with float datatype. Returns: torch.Tensor: Tensor with all elements set to zero. pass def compare_storages(t1: torch.Tensor, t2: torch.Tensor) -> bool: Compares the storages of two tensors and returns True if they share the same underlying storage, otherwise False. Parameters: t1 (torch.Tensor): First input tensor. t2 (torch.Tensor): Second input tensor. Returns: bool: True if both tensors share the same storage, False otherwise. pass ``` **Examples:** ```python # Example for zero_out_storage t = torch.tensor([1.0, 2.0, 3.0]) zero_out_storage(t) print(t) # Expected output: tensor([0.0, 0.0, 0.0]) # Example for compare_storages t1 = torch.ones(5) t2 = t1.clone() print(compare_storages(t1, t2)) # Expected output: False t1_view = t1.view(5, 1) print(compare_storages(t1, t1_view)) # Expected output: True ``` **Notes:** - For `zero_out_storage`, use the `untyped_storage()` method to access the tensor\'s underlying storage and set each byte to zero. - For `compare_storages`, use the `data_ptr()` method to compare the storage pointers of the two tensors.","solution":"import torch def zero_out_storage(t: torch.Tensor) -> torch.Tensor: Sets all elements of the tensor \'t\' to zero using direct manipulation of its storage. Parameters: t (torch.Tensor): Input tensor of any shape with float datatype. Returns: torch.Tensor: Tensor with all elements set to zero. storage = t.untyped_storage() for i in range(len(storage)): storage[i] = 0 return t def compare_storages(t1: torch.Tensor, t2: torch.Tensor) -> bool: Compares the storages of two tensors and returns True if they share the same underlying storage, otherwise False. Parameters: t1 (torch.Tensor): First input tensor. t2 (torch.Tensor): Second input tensor. Returns: bool: True if both tensors share the same storage, False otherwise. return t1.untyped_storage().data_ptr() == t2.untyped_storage().data_ptr()"},{"question":"# Configuration File Parser Challenge Objective You are required to implement a function that reads a configuration file (INI format), modifies some of its values based on specific rules, and writes the updated configuration back to a new file. Requirements 1. **Function Signature**: ```python def update_configuration(config_file_path: str, output_file_path: str) -> None: ``` 2. **Inputs**: - `config_file_path` (`str`): The file path of the input configuration file. - `output_file_path` (`str`): The file path where the updated configuration should be written. 3. **Outputs**: - The function should not return any value. The updated configuration should be written to the file specified by `output_file_path`. 4. **Behavior**: - The function should read the configuration from `config_file_path`. - Modify the following keys: - If there is a section named `database`, add a key named `port` with a value of `5432` if it doesn\'t already exist. - In the section named `server`, change the value of the key `host` to `127.0.0.1`. - For any section that has a key `enabled`, ensure its value is a boolean (`True` or `False`). - Write the updated configuration to `output_file_path`. 5. **Constraints**: - Raise an appropriate exception if the input file does not exist or cannot be read. - Handle cases where sections and keys mentioned might not exist in the input file gracefully. Example: Assume the following input configuration file `config.ini`: ```ini [database] name = test_db [server] host = localhost port = 8080 [feature] enabled = yes ``` After running `update_configuration(\\"config.ini\\", \\"updated_config.ini\\")`, the `updated_config.ini` should look like: ```ini [database] name = test_db port = 5432 [server] host = 127.0.0.1 port = 8080 [feature] enabled = True ``` Note that: - The `port` key under `[database]` is added with the value `5432`. - The `host` key under `[server]` is updated to `127.0.0.1`. - The `enabled` key under `[feature]` retains or changes its value to a boolean. Ensure your function follows the constraints and handles various edge cases appropriately. Happy coding!","solution":"import configparser import os def update_configuration(config_file_path: str, output_file_path: str) -> None: Reads a configuration file (INI format), modifies some of its values based on specific rules, and writes the updated configuration back to a new file. Args: - config_file_path (str): The file path of the input configuration file. - output_file_path (str): The file path where the updated configuration should be written. Returns: None if not os.path.exists(config_file_path): raise FileNotFoundError(f\\"The file {config_file_path} does not exist.\\") config = configparser.ConfigParser() config.read(config_file_path) # Add port to \'database\' section if it doesn\'t exist if \'database\' in config: if \'port\' not in config[\'database\']: config[\'database\'][\'port\'] = \'5432\' # Change host in \'server\' section to 127.0.0.1 if \'server\' in config: config[\'server\'][\'host\'] = \'127.0.0.1\' # Ensure \'enabled\' values are booleans for section in config.sections(): if \'enabled\' in config[section]: enabled_value = config[section][\'enabled\'].lower() if enabled_value in [\'true\', \'yes\', \'1\']: config[section][\'enabled\'] = \'True\' elif enabled_value in [\'false\', \'no\', \'0\']: config[section][\'enabled\'] = \'False\' with open(output_file_path, \'w\') as configfile: config.write(configfile)"},{"question":"Objective: You are required to demonstrate your understanding of the `operator` module by creating a function that can process a list of operations on a dictionary and return the updated dictionary. Problem Statement: Implement a function `process_operations(d: dict, operations: list) -> dict` that takes: - `d`: a dictionary where the keys are strings and the values are integers. - `operations`: a list of tuples where each tuple represents an operation. The first element of the tuple is an operation type (a string) and the subsequent elements are the parameters for that operation. The supported operations and their corresponding `operator` functions are: 1. **Set Item**: `(\\"setitem\\", key, value)` - Sets the item in the dictionary at `key` to `value`. 2. **Delete Item**: `(\\"delitem\\", key)` - Deletes the item from the dictionary at `key`. 3. **Increment Item**: `(\\"iadd\\", key, increment)` - Increments the item at `key` by `increment`. 4. **Multiply Item**: `(\\"imul\\", key, factor)` - Multiplies the item at `key` by `factor`. Your function should process each operation in the order they are given and return the updated dictionary. Constraints: - The dictionary and operations list will always have valid data types as described. - Operations will be valid and will refer only to existing keys (for `iadd` and `imul`). Example: ```python from operator import setitem, delitem, iadd, imul def process_operations(d, operations): for op in operations: if op[0] == \\"setitem\\": setitem(d, op[1], op[2]) elif op[0] == \\"delitem\\": delitem(d, op[1]) elif op[0] == \\"iadd\\": d[op[1]] = iadd(d[op[1]], op[2]) elif op[0] == \\"imul\\": d[op[1]] = imul(d[op[1]], op[2]) return d # Example usage: d = {\\"a\\": 10, \\"b\\": 20} operations = [(\\"setitem\\", \\"a\\", 5), (\\"iadd\\", \\"a\\", 3), (\\"imul\\", \\"b\\", 2), (\\"delitem\\", \\"a\\")] result = process_operations(d, operations) # result should be {\\"b\\": 40} ``` You are also provided with an initial test suite. Ensure that your function passes these tests: ```python def test_process_operations(): assert process_operations({\\"x\\": 1, \\"y\\": 2}, [(\\"setitem\\", \\"x\\", 10), (\\"imul\\", \\"y\\", 3)]) == {\\"x\\": 10, \\"y\\": 6} assert process_operations({\\"foo\\": 5}, [(\\"iadd\\", \\"foo\\", 10), (\\"imul\\", \\"foo\\", 2)]) == {\\"foo\\": 30} assert process_operations({}, [(\\"setitem\\", \\"bar\\", 100)]) == {\\"bar\\": 100} assert process_operations({\\"key\\": 0}, [(\\"delitem\\", \\"key\\")]) == {} print(\\"All tests passed.\\") test_process_operations() ``` Solve the problem by implementing the `process_operations` function and ensure it passes all the provided tests.","solution":"from operator import setitem, delitem, iadd, imul def process_operations(d, operations): Processes a list of operations on a dictionary and returns the updated dictionary. Parameters: d (dict): A dictionary where the keys are strings and the values are integers. operations (list): A list of tuples representing the operations to be performed on the dictionary. Returns: dict: The updated dictionary. for op in operations: if op[0] == \\"setitem\\": setitem(d, op[1], op[2]) elif op[0] == \\"delitem\\": delitem(d, op[1]) elif op[0] == \\"iadd\\": d[op[1]] = iadd(d[op[1]], op[2]) elif op[0] == \\"imul\\": d[op[1]] = imul(d[op[1]], op[2]) return d"},{"question":"You are required to write a function that processes XML data for output. The function should be able to escape special characters, unescape them when needed, and format attribute values appropriately. # Function Signature ```python def process_xml(data: str, entities: dict = None, action: str = \'escape\') -> str: Processes XML data. Parameters: - data (str): The raw data to be processed. - entities (dict, optional): Additional entities to process. Defaults to None. - action (str): The action to perform. Should be \'escape\', \'unescape\', or \'quoteattr\'. Defaults to \'escape\'. Returns: - str: The processed data. ... ``` # Input - `data`: A string representing the XML data to process. - `entities`: An optional dictionary where keys and values are strings representing additional entities for escape or unescape actions. - `action`: A string indicating what action to perform on the data. Options are: - `\'escape\'`: Escapes the data. - `\'unescape\'`: Unescapes the data. - `\'quoteattr\'`: Formats the data for safe use as an XML attribute value. # Output - The function should return the processed data as a string. # Constraints - The entities dictionary, if provided, will contain only string values. - The action parameter will only have values \'escape\', \'unescape\', or \'quoteattr\'. # Example ```python print(process_xml(\'AT&T is <cool>\', {\'@\': \'[at]\'})) # Should return \'AT&amp;T is &lt;cool&gt;\' print(process_xml(\'AT&amp;T is &lt;cool&gt;\', {\'@\': \'[at]\'}, \'unescape\')) # Should return \'AT&T is <cool>\' print(process_xml(\'AT&T is cool\', {}, \'quoteattr\')) # Should return \'\\"AT&amp;T is cool\\"\' print(process_xml(\'John\'s <attr=\\"value\\">\', {\'<\': \'&lt;\', \'>\': \'&gt;\'}, \'quoteattr\')) # Should return \'\\"John\'s &lt;attr=&quot;value&quot;&gt;\\"\' ``` # Notes - Use the xml.sax.saxutils module to implement the escaping, unescaping, and quoting functionalities. - Make sure to handle the default behavior when the entities parameter is not provided.","solution":"import xml.sax.saxutils as saxutils def process_xml(data: str, entities: dict = None, action: str = \'escape\') -> str: Processes XML data. Parameters: - data (str): The raw data to be processed. - entities (dict, optional): Additional entities to process. Defaults to None. - action (str): The action to perform. Should be \'escape\', \'unescape\', or \'quoteattr\'. Defaults to \'escape\'. Returns: - str: The processed data. if entities is None: entities = {} if action == \'escape\': processed_data = saxutils.escape(data, entities) elif action == \'unescape\': processed_data = saxutils.unescape(data, entities) elif action == \'quoteattr\': processed_data = saxutils.quoteattr(data, entities) else: raise ValueError(\\"Invalid action. Please choose \'escape\', \'unescape\', or \'quoteattr\'.\\") return processed_data"},{"question":"**Objective**: Demonstrate the ability to utilize the asyncio module to perform asynchronous programming, manage multiple tasks, handle inter-task communication using asyncio queues, and ensure proper synchronization using asyncio synchronization primitives. **Question**: You are tasked with simulating a simplified concurrent web scraping system. Given a list of URLs, you need to download data from these URLs concurrently using the asyncio module. The downloaded data should be processed and stored using a FIFO queue to maintain order. To ensure efficient handling, implement thread-safe synchronization using a semaphore. Create a Python function `fetch_and_process(urls: List[str], max_concurrent_tasks: int) -> None` that performs the following: 1. Concurrently fetches data from a list of URLs. 2. Uses a semaphore to limit the number of concurrent fetch operations to `max_concurrent_tasks`. 3. Utilizes an asyncio FIFO queue to store the fetched data in the order the URLs are provided. 4. Processes the data (for simulation purposes, you can just print the data or use `asyncio.sleep` to simulate processing time). 5. Ensures orderly fetching, processing, and displaying results. **Function Signature**: ```python from typing import List async def fetch_and_process(urls: List[str], max_concurrent_tasks: int) -> None: pass ``` **Input**: - `urls`: List of URLs to be fetched (List[str]). - `max_concurrent_tasks`: Maximum number of concurrent fetch operations allowed (int). **Output**: - The function doesn\'t return any value but should print the processed data from each fetched URL. **Constraints**: - Assume `urls` will contain valid URL strings. - Simulate data fetching by using `asyncio.sleep` with a random delay. **Example Usage**: ```python import asyncio import random async def fetch_and_process(urls: List[str], max_concurrent_tasks: int) -> None: # Your implementation here pass urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\"] max_concurrent_tasks = 2 # Run the asyncio function asyncio.run(fetch_and_process(urls, max_concurrent_tasks)) ``` **Notes**: - Use `asyncio.Semaphore` to limit the number of concurrent tasks. - Use `asyncio.Queue` for queue operations. - Simulating data fetch can be done using `asyncio.sleep(random.uniform(0.5, 1.5))`. - Ensure to use proper error handling for task cancellations and timeout scenarios.","solution":"import asyncio import random from typing import List async def fetch_data(url: str) -> str: # Simulate network delay await asyncio.sleep(random.uniform(0.5, 1.5)) return f\\"Data from {url}\\" async def worker(semaphore: asyncio.Semaphore, queue: asyncio.Queue, url: str): async with semaphore: data = await fetch_data(url) await queue.put((url, data)) async def process_data(queue: asyncio.Queue): while True: url, data = await queue.get() if url is None: break # Simulate data processing print(f\\"Processing {data} from {url}\\") queue.task_done() async def fetch_and_process(urls: List[str], max_concurrent_tasks: int) -> None: semaphore = asyncio.Semaphore(max_concurrent_tasks) queue = asyncio.Queue() # Create tasks for fetching data fetch_tasks = [worker(semaphore, queue, url) for url in urls] # Create task for processing data process_task = asyncio.create_task(process_data(queue)) # Wait for all fetch tasks to complete await asyncio.gather(*fetch_tasks) # Signal the processor to stop await queue.put((None, None)) # Wait for the processing task to finish await process_task"},{"question":"Coding Assessment Question # Objective The objective of this assessment is to evaluate the student\'s understanding of PyTorch probability distributions and their ability to implement and utilize these distributions in a practical scenario. # Problem Statement You are required to implement a custom probability distribution using PyTorch\'s `torch.distributions` module. Specifically, you will create a mixture distribution using the `Normal` and `Bernoulli` distributions. The mixture distribution should be able to sample from either a normal distribution centered at `mean1` with `std1` or a normal distribution centered at `mean2` with `std2`, with a probability `p`. # Your Task 1. **Define the `MixtureDistribution` class that inherits from `torch.distributions.Distribution`.** 2. **Implement the `__init__` and `sample` methods within the `MixtureDistribution` class.** 3. **Implement the `log_prob` method for calculating the log probability of a given value.** The `MixtureDistribution` Class - **`__init__(self, mean1: float, std1: float, mean2: float, std2: float, p: float, validate_args=None)`:** - Initializes the mixture distribution with the parameters: - `mean1`: Mean of the first normal distribution. - `std1`: Standard deviation of the first normal distribution. - `mean2`: Mean of the second normal distribution. - `std2`: Standard deviation of the second normal distribution. - `p`: Probability of sampling from the first normal distribution. - **`sample(self, sample_shape=torch.Size())`:** - Returns a sample from the mixture distribution. Should first sample from a Bernoulli distribution with probability `p` to decide which normal distribution to sample from. - **`log_prob(self, value)`:** - Returns the log probability of a given value under the mixture distribution. # Example ```python # Example usage of MixtureDistribution mixture_dist = MixtureDistribution(mean1=0.0, std1=1.0, mean2=5.0, std2=1.0, p=0.3) sample = mixture_dist.sample() print(f\\"Sample: {sample}\\") log_prob = mixture_dist.log_prob(sample) print(f\\"Log Probability: {log_prob}\\") ``` # Constraints - You may assume that all inputs are valid and within reasonable ranges. - The implementation should efficiently handle the sampling and log probability calculations. # Performance - The implementation should avoid unnecessary computations and leverage PyTorch\'s vectorized operations where possible. - Memory usage should be considered and minimized while maintaining clarity and functionality. # Expected Output - A `sample` method that returns a valid sample from the mixture distribution. - A `log_prob` method that calculates the log probability of a given value according to the mixture distribution.","solution":"import torch from torch.distributions import Distribution, Normal, Bernoulli class MixtureDistribution(Distribution): def __init__(self, mean1: float, std1: float, mean2: float, std2: float, p: float, validate_args=None): super().__init__(validate_args=validate_args) self.mean1 = mean1 self.std1 = std1 self.mean2 = mean2 self.std2 = std2 self.p = p self.normal1 = Normal(mean1, std1) self.normal2 = Normal(mean2, std2) self.bernoulli = Bernoulli(p) def sample(self, sample_shape=torch.Size()): mask = self.bernoulli.sample(sample_shape) return mask * self.normal1.sample(sample_shape) + (1 - mask) * self.normal2.sample(sample_shape) def log_prob(self, value): log_prob_norm1 = self.normal1.log_prob(value) log_prob_norm2 = self.normal2.log_prob(value) return torch.log(self.p * torch.exp(log_prob_norm1) + (1 - self.p) * torch.exp(log_prob_norm2))"},{"question":"# Profiling and Analyzing Python Code Performance Objective Write a Python script that profiles a given function and analyzes the profile data to identify performance bottlenecks. Tasks 1. **Create a Profile:** Write a function called `profile_function` that takes another function and its arguments, profiles it using `cProfile`, and saves the profile data to a file named `profile_data`. 2. **Analyze the Profile Data:** Write a function called `analyze_profile` that reads the profile data from the file and uses the `pstats` module to: - Sort the statistics by the cumulative time spent in each function. - Print the top 10 functions consuming the most cumulative time. - Print a detailed report of the callers for the functions with the highest cumulative time. 3. **Testing the Profile Function:** Test your `profile_function` and `analyze_profile` functions with the following sample code: ```python import time def sample_function(): def long_running_function(): for i in range(1000000): pass def quick_function(): time.sleep(0.001) long_running_function() quick_function() profile_function(sample_function) analyze_profile() ``` # Constraints 1. Use only built-in libraries `cProfile` and `pstats` for profiling and analyzing. 2. The profiled file should be named `profile_data`. 3. Ensure the printed report does not exceed 10 most significant lines for brevity. # Expected Output ```plaintext Top 10 functions sorted by cumulative time: ncalls tottime percall cumtime percall filename:lineno(function) ... ... Callers for top cumulative time-consuming functions: Function: <function_name> ncalls tottime cumtime filename:lineno(function) ... ... ... ``` Make sure your code adheres to the example output format and captures the necessary reports and information required for performance optimization insights.","solution":"import cProfile import pstats def profile_function(func, *args, **kwargs): Profiles the given function with the provided arguments and saves the profile data to \'profile_data\'. profiler = cProfile.Profile() profiler.runcall(func, *args, **kwargs) profiler.dump_stats(\'profile_data\') def analyze_profile(): Analyzes the profile data from \'profile_data\' file, sorts by cumulative time, and prints the top 10 functions by cumulative time along with their caller details. profile_data = \'profile_data\' stats = pstats.Stats(profile_data) stats.sort_stats(\'cumulative\') print(\\"Top 10 functions sorted by cumulative time:\\") stats.print_stats(10) print(\\"nCallers for top cumulative time-consuming functions:\\") for func in stats.fcn_list[:10]: stats.print_callees(func)"},{"question":"# Machine Learning Data Preprocessing You have been provided with a dataset in CSV format containing information on different products sold by a company. The dataset includes columns for product ID, name, category, date of sale, units sold, unit price, and country. Your task is to preprocess this data to extract meaningful insights. Requirements: 1. **Data Loading and Initial Checks**: - Load the dataset from the CSV file into a pandas DataFrame. - Print the first 5 rows of the DataFrame. - Display the summary statistics of the DataFrame. 2. **Data Cleaning**: - Check for missing values and handle them appropriately by filling or dropping as you deem suitable. Document your justification for the chosen method. - Convert the `date_of_sale` column to a datetime object. 3. **Data Transformation**: - Add a new column `total_sales` that represents the total sales amount for each row (units sold * unit price). - Create another column `category_sales` that represents the total sales amount for each category. 4. **Aggregations**: - Provide a summary of total sales and average unit price for each country. - Find the category with the highest total sales. 5. **Reshaping and Sorting**: - Pivot the DataFrame to show the `total_sales` for each category against each country. - Sort the pivot table based on total sales in descending order. 6. **Exporting Data**: - Save the cleaned and transformed DataFrame to a new CSV file. Input: - You will be provided with a file named `sales_data.csv`. Output: - Print the first 5 rows and summary statistics of the DataFrame. - Display information on how you handled missing values. - Show the DataFrame with the added `total_sales` and `category_sales` columns. - Print the summary of total sales and average unit price for each country. - Identify and print the category with the highest total sales. - Output the pivot table sorted in descending order based on total sales. - Save the final DataFrame to `cleaned_sales_data.csv`. Constraints: - Assume the dataset is reasonably sized and can fit into memory without performance issues. Here is a sample template for your code: ```python import pandas as pd # Load the dataset df = pd.read_csv(\'sales_data.csv\') # Display initial DataFrame print(df.head()) print(df.describe()) # Check for and handle missing values Your code to handle missing values # Convert \'date_of_sale\' to datetime df[\'date_of_sale\'] = pd.to_datetime(df[\'date_of_sale\']) # Add \'total_sales\' column df[\'total_sales\'] = df[\'units_sold\'] * df[\'unit_price\'] # Calculate \'category_sales\' category_sales = df.groupby(\'category\')[\'total_sales\'].transform(\'sum\') df[\'category_sales\'] = category_sales # Summary of total sales and average unit price for each country country_summary = df.groupby(\'country\').agg({\'total_sales\': \'sum\', \'unit_price\': \'mean\'}).reset_index() print(country_summary) # Identify category with highest total sales top_category = df.groupby(\'category\')[\'total_sales\'].sum().idxmax() print(f\\"Category with highest total sales: {top_category}\\") # Pivot table of total sales for each category versus each country pivot_table = df.pivot_table(values=\'total_sales\', index=\'category\', columns=\'country\', aggfunc=\'sum\').fillna(0) pivot_table = pivot_table.sort_values(by=pivot_table.columns[0], axis=0, ascending=False) print(pivot_table) # Output the final DataFrame df.to_csv(\'cleaned_sales_data.csv\', index=False) ```","solution":"import pandas as pd def preprocess_sales_data(file_path, output_path): # Load the dataset df = pd.read_csv(file_path) # Display initial DataFrame initial_head = df.head() initial_summary = df.describe() # Check for and handle missing values missing_values_summary = df.isnull().sum() df = df.dropna() # Dropping rows with any missing values for simplicity justifiably assuming the size is manageable # Convert \'date_of_sale\' to datetime df[\'date_of_sale\'] = pd.to_datetime(df[\'date_of_sale\']) # Add \'total_sales\' column df[\'total_sales\'] = df[\'units_sold\'] * df[\'unit_price\'] # Calculate \'category_sales\' df[\'category_sales\'] = df.groupby(\'category\')[\'total_sales\'].transform(\'sum\') # Summary of total sales and average unit price for each country country_summary = df.groupby(\'country\').agg({\'total_sales\': \'sum\', \'unit_price\': \'mean\'}).reset_index() # Identify category with highest total sales top_category = df.groupby(\'category\')[\'total_sales\'].sum().idxmax() # Pivot table of total sales for each category versus each country pivot_table = df.pivot_table(values=\'total_sales\', index=\'category\', columns=\'country\', aggfunc=\'sum\').fillna(0) pivot_table = pivot_table.sort_values(by=pivot_table.columns[0], axis=0, ascending=False) # Save the cleaned and transformed DataFrame to a new CSV file df.to_csv(output_path, index=False) return { \'initial_head\': initial_head, \'initial_summary\': initial_summary, \'missing_values_summary\': missing_values_summary, \'country_summary\': country_summary, \'top_category\': top_category, \'pivot_table\': pivot_table }"},{"question":"# Python Coding Assessment Question: Analyze Python Installation Paths and Configuration Variables You are tasked with writing a Python script that analyzes the current Python installation\'s configuration and paths to help in setting up a development environment or debugging installation issues. # Objectives: 1. Write a function `fetch_installed_paths()` that returns the default installation paths for the current platform. 2. Write a function `fetch_configuration_variable(variable_name)` that takes a single configuration variable name as an argument and returns its value. 3. Write a function `is_python_source_build()` that returns whether the current Python interpreter was built from source. 4. Write a function `analyze_environment()` that uses the above functions to: - Print the default installation paths. - Print the value of a configuration variable, `\'Py_ENABLE_SHARED\'`. - Print whether the current Python is built from source. # Function Specifications: `fetch_installed_paths()` - **Input**: None - **Output**: Dictionary with path names as keys and their corresponding paths as values. - **Behavior**: Use `sysconfig.get_paths()` to fetch the default installation paths for the current platform. `fetch_configuration_variable(variable_name)` - **Input**: A string, `variable_name`. - **Output**: The value of the configuration variable, or `None` if the variable is not found. - **Behavior**: Utilize `sysconfig.get_config_var()` to fetch the value of the given configuration variable. `is_python_source_build()` - **Input**: None - **Output**: A boolean, `True` if the Python interpreter was built from source, `False` otherwise. - **Behavior**: This function should use `sysconfig.is_python_build()` to determine if the current interpreter was built from source. `analyze_environment()` - **Input**: None - **Output**: Prints relevant information to the console. - **Behavior**: - Fetch and print the default installation paths. - Fetch and print the value of the configuration variable `\'Py_ENABLE_SHARED\'`. - Print whether the Python interpreter was built from source. Example Output: ``` Default Installation Paths: {\'stdlib\': \'/usr/local/lib/python3.10\', \'platstdlib\': \'/usr/local/lib/python3.10\', ...} Value of \'Py_ENABLE_SHARED\': 0 Is Python built from source: False ``` # Constraints: - You should handle cases where some expected configuration variables might not be available. - Assume the target environment will always have Python 3.10 or greater. # Guidelines: - Make sure your code is clean and well-documented. - Ensure that your function handles exceptions appropriately, especially for cases where the expected configuration might not exist. # Submission: Submit your script as a single Python file named `sysconfig_analysis.py`.","solution":"import sysconfig def fetch_installed_paths(): Returns the default installation paths for the current platform. return sysconfig.get_paths() def fetch_configuration_variable(variable_name): Returns the value of a configuration variable. Args: variable_name (str): The configuration variable name. Returns: The value of the configuration variable, or None if not found. return sysconfig.get_config_var(variable_name) def is_python_source_build(): Returns whether the current Python interpreter was built from source. Returns: bool: True if built from source, False otherwise. return sysconfig.is_python_build() def analyze_environment(): Prints the default installation paths, value of \'Py_ENABLE_SHARED\', and whether the Python interpreter was built from source. paths = fetch_installed_paths() py_enable_shared = fetch_configuration_variable(\'Py_ENABLE_SHARED\') source_build = is_python_source_build() print(\\"Default Installation Paths:\\") print(paths) print(f\\"Value of \'Py_ENABLE_SHARED\': {py_enable_shared}\\") print(f\\"Is Python built from source: {source_build}\\")"},{"question":"Objective: Implement a function that can generate an iterable of all possible **Pythagorean triples** (a, b, c) where `a < b < c` and `a^2 + b^2 = c^2` within a given range of numbers. Use the `itertools` module to make your implementation memory efficient and Pythonic. Problem Statement: Write a function `generate_pythagorean_triples(n: int) -> Iterable[Tuple[int, int, int]]` that accepts an integer `n` and returns an iterable of tuples, each containing three integers (a, b, c) representing Pythagorean triples, where `1 <= a < b < c <= n`. Constraints: - Use functions from the `itertools` module. - The function should generate the triples in lexicographic order. - The resulting iterable should not contain duplicates. Input: - `n` (an integer, where 1 <= n <= 100) Output: - An iterable of tuples, each representing a Pythagorean triple (a, b, c). Examples: ```python >>> list(generate_pythagorean_triples(10)) [(3, 4, 5), (6, 8, 10)] >>> list(generate_pythagorean_triples(15)) [(3, 4, 5), (5, 12, 13), (6, 8, 10), (9, 12, 15)] ``` Considerations: - Use `itertools.combinations` to generate all possible pairs `(a, b)`. - Calculate `c` as `sqrt(a^2 + b^2)` and check if `c` is an integer. - The generated triples must be in the format `a < b < c`. - Make use of other `itertools` functions where applicable to ensure the function is highly efficient. Skeleton Code: ```python from itertools import combinations def generate_pythagorean_triples(n: int): # Your implementation here pass ``` # Note: - You should not use any libraries other than `itertools` and standard math functions.","solution":"from itertools import combinations from typing import Iterable, Tuple import math def generate_pythagorean_triples(n: int) -> Iterable[Tuple[int, int, int]]: Generate an iterable of all possible Pythagorean triples (a, b, c), where a < b < c and a^2 + b^2 = c^2 within a given range of numbers from 1 to n. for a, b in combinations(range(1, n+1), 2): c = math.isqrt(a**2 + b**2) if c > n: continue if a < b < c <= n and a**2 + b**2 == c**2: yield (a, b, c)"},{"question":"Coding Assessment Question # Objective Write a Python function that searches for files within a directory tree following specific patterns and outputs the results in a structured format. The function should demonstrate comprehension of the `glob` module\'s functionality including recursive searches and escaping special characters. # Problem Statement Create a function `search_files(patterns: list, root_directory: str, recursive: bool) -> dict` that: 1. Takes a list of pathname patterns to search for files. 2. Searches within a specified root directory. 3. Uses recursive search if specified. The function should return a dictionary where the keys are the patterns and the values are lists of files that match those patterns. # Input 1. `patterns` (list of str): List of pathname patterns (strings) to search for. 2. `root_directory` (str): Root directory to begin the search. 3. `recursive` (bool): If `True`, perform a recursive search; otherwise, perform a non-recursive search. # Output - A dictionary where each key is a pattern from the `patterns` list and each value is a list of matching files (including relative paths). # Example ```python import os # Example Directory Structure: # root/ # ├── file1.txt # ├── file2.jpg # ├── subdir1/ # │ ├── file3.txt # │ └── file4.gif # ├── subdir2/ # │ ├── file5.txt # │ └── file6.gif # └── .hiddenfile # Function call: search_files([\'*.txt\', \'*.gif\', \'.hiddenfile\'], \'root\', recursive=True) # Expected output: { \'*.txt\': [\'file1.txt\', \'subdir1/file3.txt\', \'subdir2/file5.txt\'], \'*.gif\': [\'subdir1/file4.gif\', \'subdir2/file6.gif\'], \'.hiddenfile\': [\'.hiddenfile\'] } ``` # Constraints 1. Do not use external libraries other than `glob` and `os`. 2. Handle both Unix-based and Windows-based paths. 3. Ensure the function is efficient even with large directory structures. # Notes - Use `glob.glob()` for non-recursive searches. - Use `glob.iglob()` for recursive searches. - Properly escape patterns if necessary using `glob.escape()`. - Ensure that hidden files are considered if they match the specified patterns. # Testing The function will be tested with various directory structures and patterns to ensure correctness and performance.","solution":"import os import glob def search_files(patterns, root_directory, recursive): Searches for files within a directory tree following specific patterns. Parameters: - patterns (list): List of pathname patterns to search for. - root_directory (str): Root directory to begin the search. - recursive (bool): Perform recursive search if True. Returns: - dict: A dictionary with patterns as keys and lists of matching files as values. result = {} for pattern in patterns: if recursive: matches = glob.iglob(os.path.join(root_directory, \'**\', pattern), recursive=True) else: matches = glob.glob(os.path.join(root_directory, pattern)) result[pattern] = [os.path.relpath(match, root_directory) for match in matches] return result"},{"question":"Objective The objective is to create a pandas DataFrame that analyzes sales data. You will be required to implement functions that demonstrate your ability to manipulate, analyze, and visualize data using pandas. Problem Statement You are given a CSV file named `sales_data.csv` containing sales data information in the following format: | Date | Store | Product | Sales | |------------|---------|-------------|-------| | 2023-01-01 | Store_A | Product_1 | 150 | | 2023-01-01 | Store_A | Product_2 | 200 | | 2023-01-01 | Store_B | Product_1 | 300 | | 2023-02-01 | Store_A | Product_1 | 180 | | 2023-02-01 | Store_B | Product_2 | 250 | The CSV file might include more rows than the example above. The task requires the following steps: # Part 1: Data Loading and Cleaning 1. Write a function `load_data(filepath)` that loads data from the given CSV file into a pandas DataFrame. 2. In the same function, check for missing or null values and handle them appropriately by removing or imputing them. ```python import pandas as pd def load_data(filepath: str) -> pd.DataFrame: Load sales data from a CSV file and handle missing values. :param filepath: str: The path to the CSV file. :return: pd.DataFrame: The cleaned sales data DataFrame. pass ``` # Part 2: Data Manipulation 3. Write a function `aggregate_sales(data)` that takes the cleaned DataFrame and returns a new DataFrame that shows total sales for each month. ```python def aggregate_sales(data: pd.DataFrame) -> pd.DataFrame: Aggregate sales data by month. :param data: pd.DataFrame: The cleaned sales data DataFrame. :return: pd.DataFrame: A DataFrame with total sales per month. pass ``` # Part 3: Data Analysis 4. Write a function `store_sales_comparison(data)` that returns a DataFrame comparing total sales for each store. ```python def store_sales_comparison(data: pd.DataFrame) -> pd.DataFrame: Compare total sales for each store. :param data: pd.DataFrame: The cleaned sales data DataFrame. :return: pd.DataFrame: A DataFrame with total sales for each store. pass ``` # Part 4: Data Visualization 5. Write a function `plot_sales_trends(data)` that plots sales trends over time for each store and product on a line plot. ```python import matplotlib.pyplot as plt def plot_sales_trends(data: pd.DataFrame): Plot sales trends over time for each store and product. :param data: pd.DataFrame: The cleaned sales data DataFrame. pass ``` # Example Usage Given the CSV file path `path_to_csv = \\"sales_data.csv\\"`, you should be able to: 1. Load the data: ```python data = load_data(\\"sales_data.csv\\") ``` 2. Aggregate sales by month: ```python monthly_sales = aggregate_sales(data) ``` 3. Compare total sales by store: ```python store_comparison = store_sales_comparison(data) ``` 4. Plot sales trends: ```python plot_sales_trends(data) ``` Evaluation Criteria - Correctly implements data loading and handling missing values. - Correctly manipulates data to aggregate monthly sales. - Correctly compares sales between stores. - Correctly visualizes sales trends using matplotlib. - Code readability, appropriate use of pandas functions, and overall efficiency. Note: Ensure that your code is well-documented and includes appropriate error handling.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(filepath: str) -> pd.DataFrame: Load sales data from a CSV file and handle missing values. :param filepath: str: The path to the CSV file. :return: pd.DataFrame: The cleaned sales data DataFrame. data = pd.read_csv(filepath) data.dropna(inplace=True) return data def aggregate_sales(data: pd.DataFrame) -> pd.DataFrame: Aggregate sales data by month. :param data: pd.DataFrame: The cleaned sales data DataFrame. :return: pd.DataFrame: A DataFrame with total sales per month. data[\'Date\'] = pd.to_datetime(data[\'Date\']) monthly_sales = data.groupby(data[\'Date\'].dt.to_period(\'M\')).agg({\'Sales\': \'sum\'}).reset_index() monthly_sales[\'Date\'] = monthly_sales[\'Date\'].dt.to_timestamp() return monthly_sales def store_sales_comparison(data: pd.DataFrame) -> pd.DataFrame: Compare total sales for each store. :param data: pd.DataFrame: The cleaned sales data DataFrame. :return: pd.DataFrame: A DataFrame with total sales for each store. total_sales_per_store = data.groupby(\'Store\').agg({\'Sales\': \'sum\'}).reset_index() return total_sales_per_store def plot_sales_trends(data: pd.DataFrame): Plot sales trends over time for each store and product. :param data: pd.DataFrame: The cleaned sales data DataFrame. data[\'Date\'] = pd.to_datetime(data[\'Date\']) for store in data[\'Store\'].unique(): store_data = data[data[\'Store\'] == store] for product in store_data[\'Product\'].unique(): product_data = store_data[store_data[\'Product\'] == product] plt.plot(product_data[\'Date\'], product_data[\'Sales\'], label=f\'{store} - {product}\') plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.title(\'Sales Trends Over Time\') plt.legend() plt.show()"},{"question":"Question: Implementing a Multi-Threaded Web Scraper # Objective You are to implement a multi-threaded web scraper that fetches web pages from a list of URLs and counts the number of times a specified keyword appears in the content of each page. The goal is to demonstrate understanding and correct usage of Python\'s threading module, including synchronization mechanisms. # Specifications 1. **Function Signature**: ```python def keyword_counter(urls: list, keyword: str) -> dict: ``` 2. **Inputs**: - `urls` (list): A list of URLs to be scraped. Example: `[\\"http://example.com\\", \\"http://example2.com\\"]` - `keyword` (str): The keyword to count on each web page. Example: `\\"python\\"` 3. **Output**: - Returns a dictionary where each key is a URL and each value is the count of the keyword on the corresponding web page. Example: `{\\"http://example.com\\": 10, \\"http://example2.com\\": 5}` 4. **Constraints**: - You must use the `threading` module to create a thread for each URL. - Use a `Lock` object to synchronize access to shared resources. - Handle exceptions gracefully and ensure all threads complete execution before the function returns. - Assume that the URLs provided are valid and accessible. 5. **Performance**: - The function should handle at least 20 URLs efficiently. - Use appropriate synchronization techniques to ensure thread safety. 6. **Example Usage**: ```python urls = [\\"http://example.com\\", \\"http://example.org\\"] keyword = \\"sample\\" result = keyword_counter(urls, keyword) print(result) # Example output: {\\"http://example.com\\": 8, \\"http://example.org\\": 15} ``` 7. **Hints**: - Use the `requests` library to fetch web page content. - Consider using a shared dictionary to store the results and a lock to manage access. - Ensure that each thread is responsible for one URL and updates the shared dictionary accordingly. # Advanced Considerations (optional): For bonus points, you can extend the function to handle the following: - Implement retries for failed URL requests. - Use a thread pool to limit the maximum number of concurrent threads to a specified number, e.g., max 10 threads at a time. - Log the start and completion of each thread with appropriate timestamps.","solution":"import threading import requests def keyword_counter(urls: list, keyword: str) -> dict: results = {} lock = threading.Lock() def fetch_and_count(url): try: response = requests.get(url) content = response.text count = content.lower().count(keyword.lower()) with lock: results[url] = count except Exception as e: with lock: results[url] = f\\"Error: {str(e)}\\" threads = [] for url in urls: thread = threading.Thread(target=fetch_and_count, args=(url,)) thread.start() threads.append(thread) for thread in threads: thread.join() return results"},{"question":"**Coding Assessment Question: Seaborn Advanced Plotting** **Objective:** Create a comprehensive visualization project using the seaborn package, demonstrating your ability to create and customize various plots, use facets, and handle error bars efficiently. **Problem Statement:** You are provided with two datasets: `tips` and `glue`. Your task is to create an advanced visualization containing the following: 1. **Scatter Plot with Dots:** - Plot the `total_bill` vs `tip` from the `tips` dataset as a scatter plot. - Add a layer of dots with a white edge color. - Ensure that the plot handles overplotting using dodging and jittering. 2. **Faceted Plot:** - Use the `glue` dataset to create a faceted plot of `Score` vs `Model`, with facets based on the `Task` variable. - Limit the x-axis to range between -5 and 105. - Add dots based on the `Year` for color and `Encoder` for the marker type. - Adjust the scale to use the `flare` color palette and different markers for different encoders. 3. **Plot with Error Bars:** - Create a plot from the `tips` dataset showing the relationship between `total_bill` and `day`. - Add a layer of small dots shifted along the y-axis with some jitter. - Add another layer aggregating data points. - Finally, show error bars representing the standard error (SE) multiplied by 2. **Instructions:** - Implement the task using the seaborn package. - Ensure the plots are clear, properly labeled, and aesthetically pleasing. - Your implementation should be modular, and you can create functions if necessary. - Comment on your code for better readability and understanding. **Requirements:** - **Input:** The function should not require any input parameters. The datasets `tips` and `glue` should be loaded within the function. - **Output:** The function should output the final plots. - **Performance:** The solution should be efficient and handle the datasets provided without performance issues. **Note:** The datasets `tips` and `glue` can be loaded using the seaborn `load_dataset` function. Here is a starting template for your implementation: ```python import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Scatter Plot with Dots p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") p1 = p1.add(so.Dot()).add(so.Dot(edgecolor=\\"w\\")) p1.plot() # Faceted Plot p2 = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) p2 = p2.add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") p2 = p2.scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") p2.plot() # Plot with Error Bars p3 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p3.plot() # Call the function to create visualizations create_visualizations() ``` **Good luck!**","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_visualizations(): # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Scatter Plot with Dots p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") p1 = p1.add(so.Dot(), so.Jitter(.2, .2)).add(so.Dot(edgecolor=\\"w\\")) p1.plot() plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.show() # Faceted Plot p2 = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) p2 = p2.add(so.Dot(), color=\\"Year\\", marker=\\"Encoder\\") p2 = p2.scale(marker=[\\"o\\", \\"s\\", \\"^\\"], color=\\"flare\\") # Assuming there are different encoders p2.plot() plt.title(\'Faceted Plot of Score vs Model by Task\') plt.show() # Plot with Error Bars p3 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p3.plot() plt.title(\'Total Bill vs Day with Error Bars\') plt.show() # Call the function to create visualizations if __name__ == \\"__main__\\": create_visualizations()"},{"question":"You are tasked with conducting a detailed analysis of the penguins dataset using seaborn\'s `lmplot`. The dataset contains measurements of penguins\' body features, which can be categorized by species and sex. **Objective**: Write a Python function named `analyze_penguins` that uses seaborn\'s `lmplot` to produce a series of visualizations based on the penguins dataset. Your task is to create a multi-faceted analysis that will help in understanding the relationship between `bill_length_mm` and `bill_depth_mm`, conditioned on both `species` and `sex`. **Function Signature**: ```python def analyze_penguins(): pass ``` **Instructions**: 1. Load the penguins dataset using seaborn\'s `load_dataset` function. 2. Generate a scatter plot with a regression line comparing `bill_length_mm` and `bill_depth_mm`. 3. Create a plot conditioned on `species` using color differentiation. 4. Create a plot conditioned on `sex` with subplots arranged into columns. 5. Generate a grid of subplots conditioned on both `species` and `sex`, with custom subplot sizes. 6. Generate a similar grid as in step 5 but allow the axis limits to vary across subplots. 7. Ensure all plots have appropriate titles and labels. **Constraints**: - Utilize seaborn\'s `lmplot` function and appropriate parameters to achieve the plots. - Ensure the function does not return any value but displays the plots. # Expected Output: Your function should produce and show the following plots: 1. A scatter plot with a regression line for `bill_length_mm` vs. `bill_depth_mm`. 2. A scatter plot with a regression line, where points are colored by `species`. 3. A series of subplots (columns) for each `sex`, showing the relationship between `bill_length_mm` and `bill_depth_mm`. 4. A grid of subplots conditioned on both `species` and `sex`. 5. Same as above, but with varying axis limits for each subplot. You can use the following template to get started: ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic scatter plot with regression line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\'Basic regression plot\') plt.show() # 2. Regression plot colored by species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\'Regression plot by species\') plt.show() # 3. Regression plot split by sex sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", height=4) plt.suptitle(\'Regression plot split by sex\', y=1.02) plt.show() # 4. Regression plot split by species and sex sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3) plt.suptitle(\'Regression plot split by species and sex\', y=1.05, fontsize=14) plt.show() # 5. Regression plot with variable axis limits sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3, facet_kws=dict(sharex=False, sharey=False)) plt.suptitle(\'Regression plot with variable limits\', y=1.05, fontsize=14) plt.show() ``` **Ensure that you run your function for verification and visualization of the output.**","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic scatter plot with regression line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\'Basic regression plot\') plt.show() # 2. Regression plot colored by species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\'Regression plot by species\') plt.show() # 3. Regression plot split by sex sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", height=4) plt.suptitle(\'Regression plot split by sex\', y=1.02) plt.show() # 4. Regression plot split by species and sex sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3) plt.suptitle(\'Regression plot split by species and sex\', y=1.05, fontsize=14) plt.show() # 5. Regression plot with variable axis limits sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3, facet_kws=dict(sharex=False, sharey=False)) plt.suptitle(\'Regression plot with variable limits\', y=1.05, fontsize=14) plt.show()"},{"question":"**Problem Statement:** You are tasked with implementing a dynamic scheduler for tasks where each task has a priority value. The scheduler should support the following operations: - Add a new task. - Remove an existing task. - Pop the task with the highest priority. - Query the task with the highest priority without removing it. To simulate this, implement a class `DynamicScheduler` that supports these operations efficiently using the `heapq` module. Your implementation should maintain the heap property and handle tasks with equal priorities in the order they were added. **Class Specification:** ```python import heapq from typing import Any, Tuple class DynamicScheduler: def __init__(self): # Initialize necessary data structures pass def add_task(self, task: Any, priority: int) -> None: Add a new task with the given priority to the scheduler. If the task already exists, update its priority. :param task: Any - The identifier for the task (can be any hashable type). :param priority: int - The priority of the task. pass def remove_task(self, task: Any) -> None: Remove the specified task from the scheduler. :param task: Any - The identifier for the task to be removed. :raises KeyError: If the task is not found. pass def pop_task(self) -> Tuple[Any, int]: Remove and return the task with the highest priority. :return: Tuple[Any, int] - The identifier and priority of the removed task. :raises KeyError: If the scheduler is empty. pass def peek_task(self) -> Tuple[Any, int]: Return the task with the highest priority without removing it. :return: Tuple[Any, int] - The identifier and priority of the highest-priority task. :raises KeyError: If the scheduler is empty. pass ``` **Function Specifications:** 1. `add_task(self, task: Any, priority: int) -> None` - Adds a new task to the scheduler with the given priority. - If the task already exists, its priority should be updated. 2. `remove_task(self, task: Any) -> None` - Removes the specified task from the scheduler. - Raises `KeyError` if the task does not exist in the scheduler. 3. `pop_task(self) -> Tuple[Any, int]` - Removes and returns the task with the highest priority (in case of a tie, the one added first). - Raises `KeyError` if the scheduler is empty. 4. `peek_task(self) -> Tuple[Any, int]` - Returns the task with the highest priority without removing it from the scheduler. - Raises `KeyError` if the scheduler is empty. **Constraints:** - The implementation should maintain the heap invariant efficiently using operations provided by the `heapq` module. - Tasks with the same priority should be returned in the order they were added. - The task identifiers can be any hashable type (e.g., strings, integers). **Performance Requirements:** - All operations should aim to be as efficient as possible given the properties of the heap data structure (average time complexity should be logarithmic for add and pop operations). **Example Usage:** ```python scheduler = DynamicScheduler() scheduler.add_task(\'task1\', 5) scheduler.add_task(\'task2\', 3) scheduler.add_task(\'task3\', 4) print(scheduler.pop_task()) # (\'task2\', 3) print(scheduler.peek_task()) # (\'task3\', 4) scheduler.add_task(\'task2\', 2) print(scheduler.pop_task()) # (\'task2\', 2) scheduler.remove_task(\'task1\') ``` Implement the `DynamicScheduler` class based on the specifications above.","solution":"import heapq from typing import Any, Tuple, List import itertools class DynamicScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: Any, priority: int) -> None: Add a new task with the given priority to the scheduler. If the task already exists, update its priority. :param task: Any - The identifier for the task (can be any hashable type). :param priority: int - The priority of the task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task: Any) -> None: Remove the specified task from the scheduler. :param task: Any - The identifier for the task to be removed. :raises KeyError: If the task is not found. entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> Tuple[Any, int]: Remove and return the task with the highest priority. :return: Tuple[Any, int] - The identifier and priority of the removed task. :raises KeyError: If the scheduler is empty. while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return (task, priority) raise KeyError(\'pop from an empty priority queue\') def peek_task(self) -> Tuple[Any, int]: Return the task with the highest priority without removing it. :return: Tuple[Any, int] - The identifier and priority of the highest-priority task. :raises KeyError: If the scheduler is empty. while self.pq: priority, count, task = self.pq[0] if task is self.REMOVED: heapq.heappop(self.pq) # remove the REMOVED task else: return (task, priority) raise KeyError(\'peek from an empty priority queue\')"},{"question":"Objective: Implement a class `LogFileHandler` using Python\'s `io` module, which manages the writing and reading of log entries to and from a log file. This class should handle both text and binary log entries, and efficiently manage the file operations using buffering. Specifications: 1. **Class**: `LogFileHandler` 2. **Methods**: - **`__init__(self, file_path: str, mode: str = \'t\')`**: - Initializes the log file handler. - `file_path`: The path to the log file. - `mode`: Can be `\'t\'` for text mode or `\'b\'` for binary mode (default is `\'t\'`). - **`write_log(self, log_entry)`**: - Writes a log entry to the file. - For text mode (`\'t\'`), `log_entry` will be of type `str`. - For binary mode (`\'b\'`), `log_entry` will be of type `bytes`. - **`read_logs(self) -> list`**: - Reads all log entries from the file and returns a list. - Each log entry is a separate element in the list. - For text mode (`\'t\'`), returns a list of strings. - For binary mode (`\'b\'`), returns a list of bytes objects. - **`close(self)`**: - Closes the log file properly. 3. **Constraints**: - Only use the `io` module for file operations. - Ensure efficient reading and writing using appropriate buffering techniques. - Handle encoding explicitly in text mode, defaulting to `utf-8`. Requirements: - The class should manage the file opening and closing correctly. - Handle potential errors gracefully, providing appropriate error messages. - Ensure that the class is usable within a `with` statement as a context manager. Example Usage: ```python # For text logs with LogFileHandler(\'logs.txt\', mode=\'t\') as log_handler: log_handler.write_log(\\"Log entry one\\") log_handler.write_log(\\"Log entry two\\") print(log_handler.read_logs()) # Output: [\'Log entry onen\', \'Log entry twon\'] # For binary logs with LogFileHandler(\'logs.bin\', mode=\'b\') as log_handler: log_handler.write_log(b\'x01x02x03\') log_handler.write_log(b\'x04x05x06\') print(log_handler.read_logs()) # Output: [b\'x01x02x03\', b\'x04x05x06\'] ``` Notes: - Be mindful of newline characters in text mode. - Use the appropriate buffering classes (`BufferedWriter`, `BufferedReader`) from the `io` module to manage file operations.","solution":"import io class LogFileHandler: def __init__(self, file_path: str, mode: str = \'t\'): self.file_path = file_path self.mode = mode self.file = None def __enter__(self): if \'b\' in self.mode: self.file = open(self.file_path, \'wb+\' if \'b\' in self.mode else \'w+\', buffering=0) else: self.file = open(self.file_path, \'w+\', encoding=\'utf-8\', buffering=1) return self def __exit__(self, exc_type, exc_val, exc_tb): if self.file: self.file.close() def write_log(self, log_entry): if \'b\' in self.mode: if isinstance(log_entry, bytes): self.file.write(log_entry) else: raise TypeError(\\"For binary mode, log_entry must be of type \'bytes\'.\\") else: if isinstance(log_entry, str): self.file.write(log_entry + \'n\') else: raise TypeError(\\"For text mode, log_entry must be of type \'str\'.\\") def read_logs(self) -> list: self.file.seek(0) # reset file pointer to beginning if \'b\' in self.mode: return list(iter(lambda: self.file.read(1), b\'\')) else: return self.file.readlines() def close(self): if self.file: self.file.close()"},{"question":"**Objective:** Assess the student\'s ability to use the `seaborn` library, specifically the `seaborn.objects` module, to create and customize data visualizations with advanced features. **Description:** Using the dataset provided by the `seaborn.load_dataset` function, your task is to create a bar plot that demonstrates the following concepts: 1. Loading a dataset. 2. Creating a bar plot with an aggregation function based on two given variables. 3. Applying different aggregation methods, including mean and a custom function. 4. Grouping data based on an additional categorical variable with different colors. 5. Ensuring the plot is properly labeled and aesthetically pleasing. **Instructions:** 1. Use the `seaborn` library to load the \\"diamonds\\" dataset. 2. Create a bar plot where the `clarity` variable is plotted on the x-axis and the `carat` variable is aggregated based on different methods. 3. Add the following aggregation methods: - Default mean aggregation. - Median aggregation. - A custom aggregation function that calculates the interquartile range (IQR) of `carat` values. 4. Group the data by the `cut` variable and color the bars accordingly. 5. Ensure that the plot has appropriate axis labels, a title, and a legend. **Constraints:** - Use only the `seaborn` and `matplotlib` libraries for creating the plot. - Ensure the code is well-documented with comments explaining each step. **Input Format:** None. The dataset should be loaded directly using seaborn\'s `load_dataset`. **Output Format:** A bar plot visualizing the aggregation of `carat` by `clarity` and grouped by `cut` with the specified aggregation methods. **Example:** Here is an example of what the process might look like, but you are required to implement all the specified features: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create a basic plot with mean aggregation p = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p.add(so.Bar(), so.Agg()).show() ``` Now extend this to include all the features mentioned above. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_diamonds_aggregation(): Plots a bar plot of the \'diamonds\' dataset, showcasing the aggregation of \'carat\' by \'clarity\' and grouped by \'cut\' with different aggregation methods. # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Define custom aggregation functions def iqr(x): return np.percentile(x, 75) - np.percentile(x, 25) # Create the bar plot plt.figure(figsize=(14, 8)) # Mean aggregation mean_plot = sns.barplot(data=diamonds, x=\\"clarity\\", y=\\"carat\\", hue=\\"cut\\", estimator=np.mean, ci=None, palette=\\"tab10\\") plt.title(\\"Mean Carat by Clarity and Cut\\") plt.xlabel(\\"Clarity\\") plt.ylabel(\\"Mean Carat\\") plt.legend(title=\\"Cut\\") plt.show() # Median aggregation median_plot = sns.barplot(data=diamonds, x=\\"clarity\\", y=\\"carat\\", hue=\\"cut\\", estimator=np.median, ci=None, palette=\\"tab10\\") plt.title(\\"Median Carat by Clarity and Cut\\") plt.xlabel(\\"Clarity\\") plt.ylabel(\\"Median Carat\\") plt.legend(title=\\"Cut\\") plt.show() # Custom IQR aggregation iqr_plot = sns.barplot(data=diamonds, x=\\"clarity\\", y=\\"carat\\", hue=\\"cut\\", estimator=iqr, ci=None, palette=\\"tab10\\") plt.title(\\"IQR of Carat by Clarity and Cut\\") plt.xlabel(\\"Clarity\\") plt.ylabel(\\"IQR of Carat\\") plt.legend(title=\\"Cut\\") plt.show() # Call the function to generate the plots plot_diamonds_aggregation()"},{"question":"**Data Validation and Processing with Scikit-Learn Utilities** --- **Objective:** You are given a dataset that needs to be validated and processed before it can be used for machine learning. Your task is to implement a function using scikit-learn utilities that performs the following steps: 1. **Validation:** - Ensure that the input dataset does not contain any NaNs or infinite values. - Check that the dataset is a 2D array. 2. **Processing:** - Normalize each row of the dataset to have unit L2 norm. - Compute the truncated Singular Value Decomposition (SVD) to extract the top `k` components of the dataset. --- **Function Signature:** ```python from typing import Tuple import numpy as np def validate_and_process_data(data: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: pass ``` **Parameters:** - `data` (np.ndarray): A 2D numpy array representing the dataset. - `k` (int): The number of top components to extract using truncated SVD. **Returns:** - A tuple containing three elements: - `U` (np.ndarray): The left singular vectors of the truncated SVD. - `S` (np.ndarray): The singular values of the truncated SVD. - `VT` (np.ndarray): The right singular vectors of the truncated SVD. **Constraints:** - The input array `data` must be a 2D array. - The input `data` array must not contain NaN or infinite values. - `k` should be a positive integer and less than or equal to the number of columns in `data`. **Example:** ```python import numpy as np # Sample data data = np.array([ [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0] ]) k = 2 # Expected output: The components of the truncated SVD of the normalized data U, S, VT = validate_and_process_data(data, k) print(U) print(S) print(VT) ``` **Note:** 1. Refer to the scikit-learn utilities documentation for specific functions/methods used in validation and SVD computation. 2. Ensure that your implementation handles different edge cases, including invalid inputs and extreme values. --- **Tips:** - Use `check_array` from `sklearn.utils` for input validation. - Use `inplace_csr_row_normalize_l2` for normalizing rows to unit L2 norm. - Use `randomized_svd` from `sklearn.utils.extmath` for truncated SVD computation.","solution":"from typing import Tuple import numpy as np from sklearn.utils import check_array from sklearn.preprocessing import normalize from sklearn.utils.extmath import randomized_svd def validate_and_process_data(data: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Validate and process the given dataset. :param data: A 2D numpy array representing the dataset. :param k: The number of top components to extract using truncated SVD. :return: A tuple containing the left singular vectors, the singular values, and the right singular vectors. # Validate the input data array data = check_array(data, ensure_2d=True, force_all_finite=True) # Ensure k is valid if k <= 0 or k > data.shape[1]: raise ValueError(\\"k must be a positive integer less than or equal to the number of columns in the dataset\\") # Normalize each row to have unit L2 norm data_normalized = normalize(data, norm=\'l2\', axis=1) # Compute the truncated SVD U, S, VT = randomized_svd(data_normalized, n_components=k, random_state=None) return U, S, VT"},{"question":"**Problem Statement:** You are required to implement a function `compare_bytecodes(func1: callable, func2: callable) -> Tuple[bool, List[str]]` that compares the bytecodes of two given Python functions or methods and checks if they are functionally equivalent. The function should return a tuple containing a boolean value indicating whether the bytecodes are equivalent and a list of any differences found in terms of bytecode instructions. **Requirements:** 1. The function `compare_bytecodes` should take two callable objects as input. 2. Use the `dis.Bytecode` class to analyze the bytecodes of the provided functions. 3. Compare the sequence of bytecode instructions for both functions comprehensively. 4. Detect and list any differences in the bytecode operations, their arguments, or structure. 5. Return a tuple where the first element is a boolean indicating whether the bytecodes are equivalent and the second element is a list of strings describing the differences (if any). **Input:** - Two callable objects `func1` and `func2`. **Output:** - A tuple `(bool, List[str])`. The boolean indicates whether the bytecodes are equivalent. The list contains descriptions of any differences. **Constraints:** - The provided functions will not contain any syntax errors. - Assume that the functions do not have nested functions or classes. **Example:** ```python def func_a(x): return x + 1 def func_b(x): y = 1 return x + y assert compare_bytecodes(func_a, func_b) == ( False, [ \\"Line 1: Instruction mismatch - func_a: LOAD_CONST 1, func_b: LOAD_FAST 1\\", \\"Line 1: Instruction mismatch - func_a: RETURN_VALUE, func_b: LOAD_CONST 1\\", ] ) def func_c(x): return x + 1 assert compare_bytecodes(func_a, func_c) == (True, []) ``` **Additional Notes:** - You might want to utilize the `Instruction` named tuple provided by the `get_instructions()` method of the `dis` module to retrieve each bytecode instruction. - Consider differences in instructions, their arguments, as well as the overall length of the bytecode sequences. - Pay attention to handling each line of code and its corresponding bytecode instructions correctly. **Hint:** - You can use `dis.Bytecode(func)` to get the bytecode object for the function `func`. - Iterating over the `Bytecode` object will yield `Instruction` instances.","solution":"import dis from typing import Tuple, List def compare_bytecodes(func1: callable, func2: callable) -> Tuple[bool, List[str]]: Compares the bytecodes of two given Python functions or methods and checks if they are functionally equivalent. Returns a tuple containing a boolean value indicating whether the bytecodes are equivalent and a list of any differences found in terms of bytecode instructions. bytecode1 = list(dis.Bytecode(func1)) bytecode2 = list(dis.Bytecode(func2)) if len(bytecode1) != len(bytecode2): return False, [\\"Bytecode length mismatch\\"] differences = [] for inst1, inst2 in zip(bytecode1, bytecode2): if inst1.opname != inst2.opname or inst1.argval != inst2.argval: differences.append( f\\"Line {inst1.starts_line if inst1.starts_line else \'unknown\'}: \\" f\\"Instruction mismatch - {func1.__name__}: {inst1.opname} {inst1.argval}, \\" f\\"{func2.__name__}: {inst2.opname} {inst2.argval}\\" ) return (len(differences) == 0), differences"},{"question":"Objective Write a Python function that creates a custom class, `LowerCaser`, which wraps the built-in file object and modifies its `read` and `readline` methods to convert the output to lowercase. Additionally, implement and use a custom `open` function using the `builtins` module. Function Signature ```python def open(path: str, mode: str): # Implement your custom open function here class LowerCaser: def __init__(self, file): # Initialize the wrapper with the file object def read(self, count: int = -1) -> str: # Implement the read method to return content in lowercase def readline(self) -> str: # Implement the readline method to return a single line in lowercase ``` Inputs - `path` (str): The file path for the file to be opened. - `mode` (str): The mode in which the file needs to be opened (e.g., \'r\' for reading). Outputs - The `LowerCaser` class methods should return the file contents in lowercase. Constraints - You must use the `builtins.open` function to open the file inside your custom `open` function. - The `LowerCaser` class must implement the `read` and `readline` methods that read data from the file in lowercase. - Ensure proper handling of the open modes, especially reading modes like \'r\' and \'rb\'. # Example Usage ```python # Assume \'testfile.txt\' contains the text \\"Hello WorldnPYTHON Programming\\" f = open(\'testfile.txt\', \'r\') print(f.read()) # Output: \\"hello worldnpython programming\\" f = open(\'testfile.txt\', \'r\') print(f.readline()) # Output: \\"hello world\\" print(f.readline()) # Output: \\"python programming\\" ``` Notes - Make sure to handle file closing correctly, possibly by implementing a `__del__` method or using context management if needed.","solution":"import builtins class LowerCaser: def __init__(self, file): self.file = file def read(self, count: int = -1) -> str: content = self.file.read(count) return content.lower() def readline(self) -> str: line = self.file.readline() return line.lower() def close(self): self.file.close() def open(path: str, mode: str): file = builtins.open(path, mode) return LowerCaser(file)"},{"question":"# Seaborn Practical Coding Challenge You are given a dataset that represents brain network measurements at different time points. Your task is to load this dataset, process it, and generate customized visualizations using Seaborn\'s `objects` interface. This will demonstrate your ability to handle data manipulation, group the data, and create advanced visualizations. Requirements 1. **Data Loading and Processing** - Load the dataset `brain_networks` from seaborn, using header rows for multi-indexing and setting the first column as the index. - Rename the index axis to \\"timepoint\\". - Reshape the dataframe by stacking the multi-index levels, grouping by \\"timepoint\\", \\"network\\", and \\"hemi\\", and then taking the mean. - Unstack the \\"network\\" level, reset the index, and query the data to include only timepoints less than 100. 2. **Visualization** - Create a seaborn `Plot` object from the processed dataframe. - Use `so.Pair()`, specifying: - `x` attributes as [\\"5\\", \\"8\\", \\"12\\", \\"15\\"]. - `y` attributes as [\\"6\\", \\"13\\", \\"16\\"]. - Configure the layout size to 8x5 and share both x and y axes. - Add trajectory paths using the `so.Paths()` mark. - Customize the paths to have a linewidth of 1 and alpha of 0.8, and color them based on the \\"hemi\\" variable. Input and Output - **Input:** None. (The function should internally load the data.) - **Output:** A visualization plot displaying trajectories with the specified configuration. Constraints - Ensure you use appropriate seaborn functions and methods for data manipulation and visualization. - Adhere to the specified customization options for the paths in the plot. - Your code should be efficient and handle the dataset transformation correctly. Here is the structure of the expected function: ```python import seaborn.objects as so from seaborn import load_dataset def generate_brain_network_plot(): # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Show the plot p.show() ``` Write the function `generate_brain_network_plot()` that performs the above steps and generates the specified visualization.","solution":"import seaborn.objects as so from seaborn import load_dataset def generate_brain_network_plot(): # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Show the plot p.show()"},{"question":"Objective: To test the understanding of the `gettext` module in Python, including its implementation for internationalization and localization in both a single module and an entire application. Question: You are tasked with internationalizing a Python application that outputs messages based on user input. You will need to use the `gettext` module to support translations into multiple languages. **Scenario**: Your application currently prints a hardcoded message in English based on user input. The goal is to support translations of this message into French and German. **Requirements**: 1. **Prepare and Mark Translatable Strings**: - Modify the Python script to mark the translatable strings using `_(\'...\')`. 2. **Generate Message Catalogs**: - (This step is theoretical for the assessment. Describe how you would use `xgettext` or `pygettext.py` to generate `.po` files from the Python code.) 3. **Create Translations**: - Write a simulated message catalog in `.mo` files for French and German, using dictionaries to represent the translations in your script. 4. **Implement Localization**: - Write a Python function `setup_language(language_code: str)` that sets up the translation based on the user\'s language choice (`\'en\'` for English, `\'fr\'` for French, `\'de\'` for German). - Write a main script that asks the user for their language choice and the input required for the message, then prints the translated output. Input Format: The `setup_language` function takes a `language_code` as input which is a string representing the language choice (`\'en\'`, `\'fr\'`, `\'de\'`). Output Format: The script should print the translated message based on the user input. If the translation is not available for a given language, fallback to English is used. Example: ```python # Script before internationalization message = \\"Enter your name: \\" print(message) name = input() print(f\\"Hello, {name}\\") # After internationalization import gettext # Define translations (simulating .mo files) translations = { \'fr\': { \\"Enter your name: \\": \\"Entrez votre nom: \\", \\"Hello, %s\\": \\"Bonjour, %s\\" }, \'de\': { \\"Enter your name: \\": \\"Geben Sie Ihren Namen ein: \\", \\"Hello, %s\\": \\"Hallo, %s\\" } } def setup_language(language_code): if language_code in translations: lang = translations[language_code] for key, value in lang.items(): gettext.bindtextdomain(key, localedir=None) gettext.textdomain(key) _ = gettext.gettext(key) else: gettext.textdomain(\'en\') _ = gettext.gettext return _ def main(): language_code = input(\\"Choose your language (en/fr/de): \\") _ = setup_language(language_code) message = _(\\"Enter your name: \\") print(message) name = input() greeting = _(\\"Hello, %s\\") % name print(greeting) if __name__ == \\"__main__\\": main() ``` **Constraints**: - Use only the `gettext` module for translation functionality. - Assume translations are provided as dictionary entries simulating compiled `.mo` files. **Performance Requirements**: - The translation setup should be executed efficiently based on the language choice.","solution":"import gettext from importlib import reload # Define translations (simulating .mo files) translations = { \'fr\': { \\"Enter your name: \\": \\"Entrez votre nom: \\", \\"Hello, %s\\": \\"Bonjour, %s\\" }, \'de\': { \\"Enter your name: \\": \\"Geben Sie Ihren Namen ein: \\", \\"Hello, %s\\": \\"Hallo, %s\\" } } class Translations: current_language = \'en\' @staticmethod def setup_language(language_code): if language_code in translations: Translations.current_language = language_code else: Translations.current_language = \'en\' # Setting up gettext translator object lang_trans = translations.get(Translations.current_language, {}) gettext.translation(\'messages\', localedir=None, languages=[], fallback=True, codeset=\'utf-8\').install() gettext.gettext = lambda text: lang_trans.get(text, text) @staticmethod def gettext(text): lang_trans = translations.get(Translations.current_language, {}) return lang_trans.get(text, text) def main(): language_code = input(\\"Choose your language (en/fr/de): \\") Translations.setup_language(language_code) _ = Translations.gettext message = _(\\"Enter your name: \\") print(message) name = input() greeting = _(\\"Hello, %s\\") % name print(greeting) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment: Coroutine Objects Objective Demonstrate your understanding of Python coroutines by implementing and manipulating coroutine objects using the `async` keyword. Problem Statement You are required to implement a function that performs the following tasks: 1. Accepts an asynchronous function, its arguments, and keyword arguments. 2. Executes the given asynchronous function as a coroutine. 3. Ensures the given function is indeed a coroutine function. 4. Collects and returns the result of the coroutine. Function Signature ```python import types async def run_coroutine(async_fn: types.CoroutineType, *args, **kwargs) -> any: Executes a coroutine function with provided arguments and returns the result. Parameters: async_fn (types.CoroutineType): The asynchronous function to be executed. *args: Positional arguments to pass to the asynchronous function. **kwargs: Keyword arguments to pass to the asynchronous function. Returns: Result of the executed coroutine. # Your implementation here ``` Input - `async_fn`: An asynchronous function declared with the `async` keyword. - `*args`: Positional arguments to be passed to the asynchronous function. - `**kwargs`: Keyword arguments to be passed to the asynchronous function. Output - The result produced by executing the asynchronous function, which should be of any type since coroutine results can vary. Constraints - Implement this function considering the Python 3.10 specifications for coroutine objects. - Ensure proper error handling if the provided function is not a coroutine. - Assume the asynchronous function provided will not raise exceptions during execution. Example Usage ```python import asyncio async def sample_async_function(x, y): await asyncio.sleep(1) return x + y # Usage example result = await run_coroutine(sample_async_function, 3, 5) print(result) # Output should be 8 ``` Additional Notes - You may utilize the `asyncio` module to handle the execution of the coroutine. - Ensure that your implementation accurately checks if the provided function is a coroutine. # Submission Guidelines Submit your solution as a single Python file with the implementation of the `run_coroutine` function.","solution":"import asyncio import types async def run_coroutine(async_fn, *args, **kwargs) -> any: Executes a coroutine function with provided arguments and returns the result. Parameters: async_fn (types.CoroutineType): The asynchronous function to be executed. *args: Positional arguments to pass to the asynchronous function. **kwargs: Keyword arguments to pass to the asynchronous function. Returns: Result of the executed coroutine. if not asyncio.iscoroutinefunction(async_fn): raise TypeError(\'Given function is not a coroutine function\') coroutine = async_fn(*args, **kwargs) result = await coroutine return result"},{"question":"Objective Your task is to create a simple command-line calculator that can perform basic arithmetic operations. You will use the `cmd` module to create a custom command interpreter that reads user input, performs the desired operations, and returns the result. Requirements 1. Create a subclass of `cmd.Cmd` named `CalculatorShell`. 2. Implement the following commands: - `add [number1] [number2]`: Adds two numbers. - `subtract [number1] [number2]`: Subtracts the second number from the first. - `multiply [number1] [number2]`: Multiplies two numbers. - `divide [number1] [number2]`: Divides the first number by the second. 3. Implement basic error handling for invalid inputs (e.g., non-numeric inputs, division by zero). 4. Add a `help` command that lists all available commands and their descriptions. 5. Implement a `history` command that shows the history of all executed commands. 6. Add a `bye` command to exit the interpreter. Constraints - Ensure that all operations are performed with floats (use `float()` conversion). - Use `self.cmdqueue` to manage command execution in order. - Implement `precmd()` to log all commands to a file `command_history.txt`. - Use the provided sample session to guide the implementation. Sample Session ``` Welcome to the calculator shell. Type help or ? to list commands. (calculator) add 5 3 Result: 8.0 (calculator) subtract 10 2 Result: 8.0 (calculator) multiply 4 5 Result: 20.0 (calculator) divide 20 4 Result: 5.0 (calculator) divide 10 0 Error: Division by zero is not allowed. (calculator) history Command History: 1. add 5 3 2. subtract 10 2 3. multiply 4 5 4. divide 20 4 5. divide 10 0 (calculator) bye Thank you for using Calculator Shell. ``` Implementation Details - Use the `cmd` module as described in the provided documentation. - Ensure that each command method (`do_*`) outputs the expected result, and use `print()` for showing results and errors. - Maintain a history list in the class to store and display command history. Good luck!","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \\"Welcome to the calculator shell. Type help or ? to list commands.\\" prompt = \\"(calculator) \\" def __init__(self): super().__init__() self.history = [] def precmd(self, line): with open(\'command_history.txt\', \'a\') as f: f.write(line + \'n\') return line def do_add(self, args): try: numbers = args.split() result = float(numbers[0]) + float(numbers[1]) self.history.append(f\\"add {numbers[0]} {numbers[1]}\\") print(f\\"Result: {result}\\") except (IndexError, ValueError) as e: print(\\"Error: Invalid input, please provide two numbers.\\") def do_subtract(self, args): try: numbers = args.split() result = float(numbers[0]) - float(numbers[1]) self.history.append(f\\"subtract {numbers[0]} {numbers[1]}\\") print(f\\"Result: {result}\\") except (IndexError, ValueError) as e: print(\\"Error: Invalid input, please provide two numbers.\\") def do_multiply(self, args): try: numbers = args.split() result = float(numbers[0]) * float(numbers[1]) self.history.append(f\\"multiply {numbers[0]} {numbers[1]}\\") print(f\\"Result: {result}\\") except (IndexError, ValueError) as e: print(\\"Error: Invalid input, please provide two numbers.\\") def do_divide(self, args): try: numbers = args.split() if float(numbers[1]) == 0: print(\\"Error: Division by zero is not allowed.\\") else: result = float(numbers[0]) / float(numbers[1]) self.history.append(f\\"divide {numbers[0]} {numbers[1]}\\") print(f\\"Result: {result}\\") except (IndexError, ValueError) as e: print(\\"Error: Invalid input, please provide two numbers.\\") def do_history(self, arg): print(\\"Command History:\\") for idx, command in enumerate(self.history, start=1): print(f\\"{idx}. {command}\\") def do_bye(self, arg): print(\\"Thank you for using Calculator Shell.\\") return True if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"# Python Coding Assessment Question Objective Design a Python function that simulates a simplified version of Python\'s memory management for custom objects. Your task is to manage the reference counting of these objects and handle exceptions accordingly. Task You need to implement a `ManagedObject` class in Python with the following specifications: 1. The class should simulate reference counting to manage object life cycles. 2. Implement a method to increment the reference count. 3. Implement a method to decrement the reference count and delete the object if the reference count reaches zero. 4. Ensure that exception handling is in place to catch and handle scenarios where reference counts might become inconsistent (e.g., decrementing below zero). 5. Provide a way to query the current reference count of the object. Input and Output Formats: - Implement the `ManagedObject` class with the methods as described. - The class should be used as follows: ```python # Example usage: obj = ManagedObject(\'example\') print(obj.get_ref_count()) # Output: 1 obj.increment_ref_count() print(obj.get_ref_count()) # Output: 2 obj.decrement_ref_count() print(obj.get_ref_count()) # Output: 1 obj.decrement_ref_count() # This should trigger deletion of the object ``` - If a method is called to decrement the reference count below zero, raise a `ValueError` with a message indicating \\"Reference count below zero\\". Constraints: - You cannot use built-in functions or libraries that provide similar functionality (e.g., `sys.getrefcount`). - The focus should be on correctly managing reference counts and handling exceptions for incorrect operations. Performance Requirements: - The implementation should be able to handle multiple reference counting operations efficiently without unnecessary overhead. Code Skeleton: ```python class ManagedObject: def __init__(self, name): # Initialize the object with a reference count of 1 self.name = name self.ref_count = 1 def increment_ref_count(self): # Increment the reference count self.ref_count += 1 def decrement_ref_count(self): # Decrement the reference count and delete if zero self.ref_count -= 1 if self.ref_count < 0: raise ValueError(\\"Reference count below zero\\") elif self.ref_count == 0: self.delete() def delete(self): # Simulate object deletion print(f\\"Object \'{self.name}\' deleted\\") def get_ref_count(self): # Return the current reference count return self.ref_count ``` Your implementation should ensure proper management of the `ref_count` and handle exceptions as described.","solution":"class ManagedObject: def __init__(self, name): Initialize the object with a reference count of 1. self.name = name self.ref_count = 1 def increment_ref_count(self): Increment the reference count. self.ref_count += 1 def decrement_ref_count(self): Decrement the reference count and delete the object if the reference count reaches zero. Raise a ValueError if the reference count goes below zero. self.ref_count -= 1 if self.ref_count < 0: raise ValueError(\\"Reference count below zero\\") elif self.ref_count == 0: self.delete() def delete(self): Simulate object deletion. print(f\\"Object \'{self.name}\' deleted\\") def get_ref_count(self): Return the current reference count. return self.ref_count"},{"question":"**Problem Statement: Operations with Rational Numbers** You are tasked with implementing a Python function that takes a list of strings representing rational numbers and performs several operations on them using the `fractions` module. # Function Signature ```python def process_fractions(fractions_list: list) -> dict: pass ``` # Input - `fractions_list` (list): A list of strings, where each string represents a rational number. Each string will be in one of the following formats: - \\"numerator/denominator\\" - \\"floating_point_number\\" - \\"scientific_notation_number\\" # Output - The function should return a dictionary with the following keys: - `\'parsed_fractions\'`: A list of `Fraction` instances corresponding to the input strings. - `\'sum\'`: The sum of all the fractions. - `\'product\'`: The product of all the fractions. - `\'min_fraction\'`: The smallest fraction in the list. - `\'max_fraction\'`: The largest fraction in the list. - `\'approx_ratios\'`: A list of integer ratios (as tuples) for each fraction, where the denominator is limited to 1000. # Example ```python from fractions import Fraction def process_fractions(fractions_list: list) -> dict: parsed_fractions = [Fraction(f) for f in fractions_list] sum_fractions = sum(parsed_fractions, start=Fraction(0)) product_fractions = Fraction(1) for f in parsed_fractions: product_fractions *= f min_fraction = min(parsed_fractions) max_fraction = max(parsed_fractions) approx_ratios = [f.limit_denominator(1000).as_integer_ratio() for f in parsed_fractions] return { \'parsed_fractions\': parsed_fractions, \'sum\': sum_fractions, \'product\': product_fractions, \'min_fraction\': min_fraction, \'max_fraction\': max_fraction, \'approx_ratios\': approx_ratios } # Example usage fractions_list = [\'3/4\', \'1.25\', \'2e-3\', \'-0.5\', \'4/5\'] result = process_fractions(fractions_list) print(result) ``` # Constraints - You can assume the input list contains at least one element. - Each string in the input list represents a valid number. # Notes - Be sure to handle edge cases such as very small numbers, negative numbers, or numbers in scientific notation correctly. - Use the `fractions` module as demonstrated in the documentation for handling rational numbers.","solution":"from fractions import Fraction from typing import List, Dict def process_fractions(fractions_list: List[str]) -> Dict[str, any]: parsed_fractions = [Fraction(f) for f in fractions_list] sum_fractions = sum(parsed_fractions, start=Fraction(0)) product_fractions = Fraction(1) for f in parsed_fractions: product_fractions *= f min_fraction = min(parsed_fractions) max_fraction = max(parsed_fractions) approx_ratios = [f.limit_denominator(1000).as_integer_ratio() for f in parsed_fractions] return { \'parsed_fractions\': parsed_fractions, \'sum\': sum_fractions, \'product\': product_fractions, \'min_fraction\': min_fraction, \'max_fraction\': max_fraction, \'approx_ratios\': approx_ratios }"},{"question":"# Python Shelve Module Assessment **Objective**: Implement a function to manage a simple key-value store using the `shelve` module, demonstrating your understanding of persistent storage in Python. Problem Statement: You are required to write a function `manage_shelf(filename: str, operations: List[Tuple[str, Union[str, Any]]], writeback: bool = False) -> List[Any]:` that performs a series of operations on a shelved dictionary and returns the results of the \'get\' operations. Each operation is a tuple where the first element is an operation type (`\'put\'`, `\'get\'`, `\'delete\'`, `\'sync\'`, `\'close\'`), and the second element is either the key or a tuple of key and value. **Function Signature**: ```python from typing import List, Tuple, Union, Any def manage_shelf(filename: str, operations: List[Tuple[str, Union[str, Any]]], writeback: bool = False) -> List[Any]: ``` Input: - `filename` (str): The name of the file to be used for the shelf. - `operations` (List[Tuple[str, Union[str, Any]]]): A list of operations to perform on the shelf. Each operation is a tuple: - `\'put\'`: (`\'put\'`, (`key`, `value`)) - Stores the value at the given key. - `\'get\'`: (`\'get\'`, `key`) - Retrieves the value associated with the key. - `\'delete\'`: (`\'delete\'`, `key`) - Deletes the entry associated with the key. - `\'sync\'`: (`\'sync\'`,) - Synchronizes the shelf. - `\'close\'`: (`\'close\'`,) - Closes the shelf. - `writeback` (bool): Determines whether changes to mutable entries are automatically written back. Output: - A list of results for the `\'get\'` operations in the order they appeared in the input operations list. Constraints: - Keys are always strings. - Values can be any picklable Python object. - The shelf must be properly closed at the end of the function, whether explicitly or via context management. Example: ```python operations = [ (\'put\', (\'key1\', \'value1\')), (\'get\', \'key1\'), (\'put\', (\'key2\', [1, 2, 3])), (\'get\', \'key2\'), (\'delete\', \'key1\'), (\'get\', \'key1\'), (\'sync\',), (\'close\',) ] print(manage_shelf(\'test_shelf\', operations)) ``` Expected Output: ```python [\'value1\', [1, 2, 3], None] ``` Instructions and Considerations: 1. Use the `shelve` module to implement the shelf-based dictionary. 2. Ensure that the function handles the opening, synchronization, and closing of the shelf appropriately. 3. Consider the effect of the `writeback` parameter on performance and memory usage. 4. Handle exceptions gracefully, especially when attempting to get or delete non-existent keys.","solution":"from typing import List, Tuple, Union, Any import shelve def manage_shelf(filename: str, operations: List[Tuple[str, Union[str, Any]]], writeback: bool = False) -> List[Any]: results = [] with shelve.open(filename, writeback=writeback) as shelf: for operation in operations: op_type = operation[0] if op_type == \'put\': key, value = operation[1] shelf[key] = value elif op_type == \'get\': key = operation[1] results.append(shelf.get(key, None)) elif op_type == \'delete\': key = operation[1] if key in shelf: del shelf[key] elif op_type == \'sync\': shelf.sync() elif op_type == \'close\': shelf.close() break return results"},{"question":"# **Advanced Python Collections Challenge** Problem Statement You are working on a project where you need to manage configurations and settings in a layered environment (user-configurable settings, environment variables, and default settings). You decide to use the `ChainMap` collection to handle these configurations efficiently because it allows you to link multiple dictionaries and search them as a single unit. Implement a class `ConfigManager` that uses `ChainMap` to manage these configurations. Your implementation should include the following functionalities: 1. **Initialization**: - The `ConfigManager` should be initialized with three dictionaries: `user_configs`, `env_configs`, and `default_configs`. These correspond to user settings, environment variables, and default settings, respectively. - If any dictionary is not provided, it should default to an empty dictionary. 2. **Methods**: - `get_value(key)`: This method should return the value associated with `key` from the linked dictionaries in the order of `user_configs`, `env_configs`, and `default_configs`. If the `key` does not exist in any map, it should return `None`. - `set_user_value(key, value)`: This method should set a value for a key in the `user_configs` dictionary. - `delete_user_value(key)`: This method should delete a key from the `user_configs` dictionary. - `get_all_keys()`: This method should return a list of all unique keys available in all layers of the configuration. 3. **Attributes**: - The class should have a `chain` attribute that stores the `ChainMap` instance linking the three dictionaries. Input and Output - **Input**: The dictionaries `user_configs`, `env_configs`, and `default_configs` on initialization, and keys/values for the methods. - **Output**: The result of `get_value`, `get_all_keys` method calls, and updated dictionaries after setting or deleting user values. Constraints - Assume all values are strings. - The keys are unique within each dictionary but can repeat across the three dictionaries. Example Usage ```python default_configs = {\'theme\':\'light\', \'show_tooltips\': \'true\'} env_configs = {\'user\':\'alice\', \'theme\':\'dark\'} user_configs = {\'show_tooltips\': \'false\'} manager = ConfigManager(user_configs, env_configs, default_configs) assert manager.get_value(\'theme\') == \'false\' assert manager.get_value(\'user\') == \'alice\' assert manager.get_value(\'non_existent_key\') == None manager.set_user_value(\'language\', \'en\') assert manager.get_value(\'language\') == \'en\' manager.delete_user_value(\'show_tooltips\') assert manager.get_value(\'show_tooltips\') == \'true\' assert set(manager.get_all_keys()) == {\'theme\', \'show_tooltips\', \'user\', \'language\'} ``` Implement the `ConfigManager` class and ensure it passes the example usage scenario. Your Solution ```python from collections import ChainMap class ConfigManager: def __init__(self, user_configs=None, env_configs=None, default_configs=None): if user_configs is None: user_configs = {} if env_configs is None: env_configs = {} if default_configs is None: default_configs = {} self.chain = ChainMap(user_configs, env_configs, default_configs) def get_value(self, key): return self.chain.get(key, None) def set_user_value(self, key, value): self.chain.maps[0][key] = value def delete_user_value(self, key): if key in self.chain.maps[0]: del self.chain.maps[0][key] def get_all_keys(self): return list({key for mapping in self.chain.maps for key in mapping}) ``` Test the `ConfigManager` class to ensure your implementation works correctly as per the given requirements.","solution":"from collections import ChainMap class ConfigManager: def __init__(self, user_configs=None, env_configs=None, default_configs=None): if user_configs is None: user_configs = {} if env_configs is None: env_configs = {} if default_configs is None: default_configs = {} self.chain = ChainMap(user_configs, env_configs, default_configs) def get_value(self, key): return self.chain.get(key, None) def set_user_value(self, key, value): self.chain.maps[0][key] = value def delete_user_value(self, key): if key in self.chain.maps[0]: del self.chain.maps[0][key] def get_all_keys(self): return list({key for mapping in self.chain.maps for key in mapping})"},{"question":"**Task:** You are tasked with creating a Python module that processes log files from a web server. The purpose of the module is to read a log file, analyze the data, and provide useful summaries and statistics. Specifically, you need to implement the following functions in a Python script: 1. **`read_log(file_path: str) -> list`** - **Input:** A string `file_path` representing the path to the log file. - **Output:** A list of dictionaries where each dictionary represents a log entry. Each log entry consists of the following fields (assume space-separated values in the log file): - IP address - Timestamp - HTTP method - URL - HTTP status code - Response size in bytes 2. **`summarize_by_status(log_data: list) -> dict`** - **Input:** A list of dictionaries representing log entries (as returned by `read_log`). - **Output:** A dictionary summarizing the number of occurrences of each HTTP status code. 3. **`find_top_ips(log_data: list, n: int) -> list`** - **Input:** A list of dictionaries representing log entries and an integer `n`. - **Output:** A list of the top `n` IP addresses that made the most requests, sorted in descending order of request count. 4. **`average_response_size(log_data: list) -> float`** - **Input:** A list of dictionaries representing log entries. - **Output:** A float representing the average response size in bytes across all log entries. **Constraints and Requirements:** - The log file is guaranteed to follow the specified format. - You can assume that the log file is reasonably sized and fits into memory. - Implement modular code by defining each function separately. - Optimize for readability and maintainability. - Write efficient code considering the performance for large log files. # Example: Assume a sample log file `webserver.log` with the following contents: ``` 192.168.1.1 [10/Oct/2023:13:55:36 -0400] GET /index.html 200 1024 192.168.1.2 [10/Oct/2023:13:56:12 -0400] POST /form 404 512 192.168.1.1 [10/Oct/2023:14:03:45 -0400] GET /home.html 200 256 ``` - Calling `read_log(\'webserver.log\')` should return: ```python [ {\'IP\': \'192.168.1.1\', \'Timestamp\': \'[10/Oct/2023:13:55:36 -0400]\', \'Method\': \'GET\', \'URL\': \'/index.html\', \'Status\': \'200\', \'Size\': 1024}, {\'IP\': \'192.168.1.2\', \'Timestamp\': \'[10/Oct/2023:13:56:12 -0400]\', \'Method\': \'POST\', \'URL\': \'/form\', \'Status\': \'404\', \'Size\': 512}, {\'IP\': \'192.168.1.1\', \'Timestamp\': \'[10/Oct/2023:14:03:45 -0400]\', \'Method\': \'GET\', \'URL\': \'/home.html\', \'Status\': \'200\', \'Size\': 256} ] ``` - Calling `summarize_by_status(log_data)` should return: ```python {\'200\': 2, \'404\': 1} ``` - Calling `find_top_ips(log_data, 1)` should return: ```python [\'192.168.1.1\'] ``` - Calling `average_response_size(log_data)` should return: ```python 597.33 ``` **Submission:** Provide a Python script implementing these functions. Ensure that it is well-structured and includes necessary documentation for comprehensibility and future maintenance.","solution":"def read_log(file_path: str) -> list: Reads a log file and returns a list of dictionaries, each representing a log entry. log_entries = [] with open(file_path, \'r\') as file: for line in file: parts = line.split() log_entry = { \'IP\': parts[0], \'Timestamp\': parts[1] + \' \' + parts[2], \'Method\': parts[3], \'URL\': parts[4], \'Status\': parts[5], \'Size\': int(parts[6]), } log_entries.append(log_entry) return log_entries def summarize_by_status(log_data: list) -> dict: Summarizes the number of occurrences of each HTTP status code. summary = {} for entry in log_data: status = entry[\'Status\'] if status in summary: summary[status] += 1 else: summary[status] = 1 return summary def find_top_ips(log_data: list, n: int) -> list: Returns a list of the top `n` IP addresses that made the most requests. ip_count = {} for entry in log_data: ip = entry[\'IP\'] if ip in ip_count: ip_count[ip] += 1 else: ip_count[ip] = 1 sorted_ips = sorted(ip_count.items(), key=lambda x: x[1], reverse=True) return [ip for ip, count in sorted_ips[:n]] def average_response_size(log_data: list) -> float: Returns the average response size in bytes across all log entries. total_size = 0 for entry in log_data: total_size += entry[\'Size\'] return total_size / len(log_data)"},{"question":"# HTML Parsing and Custom Tag Handling **Problem Statement:** You are required to write a Python program that subclasses the `HTMLParser` class from the `html.parser` module. The goal is to create a custom HTML parser that extracts all the links (`<a>` tags) from a given HTML document, along with their `href` attributes. Each link should be stored in a dictionary where the key is the link text and the value is the href attribute. If the link text is empty, use a placeholder string like \\"No Text\\". # Requirements: 1. You must define a class `LinkExtractor` that subclasses `HTMLParser`. 2. Override the necessary methods to handle extraction of `<a>` tags and their `href` attributes. 3. The parser should store links in a dictionary with text as keys and `href` as values. 4. Ensure that if multiple links have the same text, their hrefs should be combined in a list. 5. Create a method in the `LinkExtractor` class called `get_links()` that returns the dictionary of links. # Input: - A single string `html_content` which represents the HTML document. # Output: - A dictionary where each key is the link text and each value is a list of href attributes. # Constraints: - `html_content` length will be between 1 and 10000 characters. - The HTML content is guaranteed to be valid. # Example: ```python from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = {} self.current_data = \\"\\" def handle_starttag(self, tag, attrs): if tag == \'a\': href = dict(attrs).get(\'href\') if href: self.links[self.current_data] = self.links.get(self.current_data, []) + [href] self.current_data = \\"\\" def handle_data(self, data): self.current_data = data def get_links(self): return self.links # Example Usage: html_content = \'\'\' <html> <head><title>Test</title></head> <body> <h1>Sample Document</h1> <p>Here is a <a href=\\"https://example.com\\">link</a> to example.com.</p> <p>Another link to the <a href=\\"https://example.com\\">same place</a>.</p> <p>A link with no text <a href=\\"https://empty.com\\"></a>.</p> </body> </html> \'\'\' parser = LinkExtractor() parser.feed(html_content) print(parser.get_links()) # Expected Output: # {\'link\': [\'https://example.com\'], \'same place\': [\'https://example.com\'], \'Another link to the\': [\'https://example.com\'], \'Here is a\': [\'https://example.com\'], \'\': [\'https://empty.com\']} ``` Implement the `LinkExtractor` class to solve this problem.","solution":"from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = {} self.current_href = None self.current_text = \\"\\" def handle_starttag(self, tag, attrs): if tag == \'a\': href = dict(attrs).get(\'href\') if href: self.current_href = href def handle_endtag(self, tag): if tag == \'a\' and self.current_href: text = self.current_text.strip() or \\"No Text\\" if text in self.links: self.links[text].append(self.current_href) else: self.links[text] = [self.current_href] self.current_href = None self.current_text = \\"\\" def handle_data(self, data): if self.current_href: self.current_text += data def get_links(self): return self.links # Example Usage: # html_content = \'\'\' # <html> # <head><title>Test</title></head> # <body> # <h1>Sample Document</h1> # <p>Here is a <a href=\\"https://example.com\\">link</a> to example.com.</p> # <p>Another link to the <a href=\\"https://example.com\\">same place</a>.</p> # <p>A link with no text <a href=\\"https://empty.com\\"></a>.</p> # </body> # </html> # \'\'\' # # parser = LinkExtractor() # parser.feed(html_content) # print(parser.get_links()) # Expected Output: # {\'link\': [\'https://example.com\'], \'same place\': [\'https://example.com\'], \'No Text\': [\'https://empty.com\']}"},{"question":"Objective: Assess your understanding of combining seaborn\'s `Plot` interface with matplotlib objects for customization and embedding. Task: Write a Python function `generate_custom_plot` that accepts a seaborn dataset name, a list of numeric column names to plot, and an optional title. Your task is to: 1. Load the dataset using `seaborn.load_dataset`. 2. Create a `seaborn.objects.Plot` for the specified numeric columns. 3. Customize the plot by: - Using a `matplotlib.figure.Figure` object. - Adding a custom annotation (text or shape) to the plot. - Optionally setting a title for the plot. You should use both seaborn and matplotlib functionalities to achieve the required customizations. Signature: ```python def generate_custom_plot(dataset_name: str, numeric_columns: list, title: str = None) -> plt.Figure: pass ``` Input: - `dataset_name` (str): The name of a seaborn dataset. - `numeric_columns` (list): A list with exactly two strings representing the names of numeric columns to be used for the x and y axes. - `title` (str, optional): An optional title for the plot. Default is `None`. Output: - A `matplotlib.figure.Figure` object with the specified custom plot. Constraints: - Ensure that the function works for any seaborn dataset with at least two numeric columns. - You must handle cases where the specified columns do not exist or are not numeric by raising a ValueError. Example: ```python # Example usage fig = generate_custom_plot(\\"diamonds\\", [\\"carat\\", \\"price\\"], title=\\"Carat vs Price for Diamonds\\") fig.show() ``` In this example: - `generate_custom_plot` loads the `diamonds` dataset. - Creates a scatter plot using `carat` for the x-axis and `price` for the y-axis with the title \\"Carat vs Price for Diamonds\\". - Adds a custom annotation to the plot. Notes: - Think about how you can leverage the seaborn and matplotlib functionalities to complete the task. - You may use any appropriate seaborn `objects.Plot` functionalities and matplotlib object methods to create and customize the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_plot(dataset_name: str, numeric_columns: list, title: str = None) -> plt.Figure: if len(numeric_columns) != 2: raise ValueError(\\"numeric_columns must contain exactly two column names.\\") # Load dataset data = sns.load_dataset(dataset_name) if any(col not in data.columns for col in numeric_columns) or not all(data[col].dtype.kind in \'ifc\' for col in numeric_columns): raise ValueError(\\"Specified columns must exist and be numeric.\\") # Initialize the Plot object plot_data = data[numeric_columns] plot = sns.scatterplot(x=numeric_columns[0], y=numeric_columns[1], data=plot_data) # Create a matplotlib figure fig = plot.get_figure() # Add custom annotation plt.annotate(\'Custom Annotation\', xy=(0.5, 0.5), xycoords=\'axes fraction\', fontsize=12, color=\'red\') # Set title if provided if title: plt.title(title) return fig"},{"question":"# Descriptor Objects in Python Python provides a mechanism called descriptors, which are objects that describe attributes of other objects. In this task, you are required to implement a custom descriptor to control access to an attribute in a class. Objective Your task is to implement a custom descriptor class that uses the methods detailed in the documentation to control access to a class attribute. Specifically, you will: 1. Create a descriptor called `ReadOnlyAttribute` that allows reading the attribute but prevents writing to it. 2. Implement a function `_is_data_descriptor` to check if the descriptor describes a data attribute. # Requirements 1. **Class `ReadOnlyAttribute`**: - The class should accept an attribute name and store it. - Implement `__get__` to return the attribute\'s value. - Implement `__set__` to raise an `AttributeError` when there is an attempt to modify the value. 2. **Function `_is_data_descriptor`**: - This function should accept a descriptor object and use the `PyDescr_IsData()` function to determine if it is a data descriptor. - Return `True` if it is a data descriptor, `False` otherwise. # Input and Output - The `ReadOnlyAttribute` class constructor will take one argument: `attr_name` (a string representing the name of the attribute). - The `__get__` method will return the value of the attribute. - The `__set__` method will raise an `AttributeError` when attempting to set the value. - The `_is_data_descriptor` function takes a descriptor object as input and returns a boolean. # Example ```python class MyClass: my_attr = ReadOnlyAttribute(\'my_attr\') def __init__(self, value): self.my_attr = value # This should raise an AttributeError after descriptor is set. obj = MyClass(10) print(obj.my_attr) # Should print 10 # Should raise an AttributeError if you try to set the attribute try: obj.my_attr = 20 except AttributeError as e: print(e) # Output: Cannot modify read-only attribute print(_is_data_descriptor(obj.my_attr)) # Should return False ``` Constraints - You are not allowed to use any external libraries. - Follow Python\'s official naming conventions for class names and method names. - Handle exceptions gracefully and provide informative error messages. You will be evaluated based on: 1. Correctness of the descriptor implementation. 2. Proper use and understanding of descriptor concepts. 3. Implementation of the `_is_data_descriptor` using the given functions. Good luck!","solution":"class ReadOnlyAttribute: def __init__(self, attr_name): self.attr_name = attr_name def __get__(self, instance, owner): if instance is None: return self return instance.__dict__[self.attr_name] def __set__(self, instance, value): raise AttributeError(\\"Cannot modify read-only attribute\\") def _is_data_descriptor(desc): return hasattr(desc, \'__set__\') or hasattr(desc, \'__delete__\') # Example class to demonstrate the ReadOnlyAttribute descriptor class MyClass: my_attr = ReadOnlyAttribute(\'my_attr\') def __init__(self, value): # Directly set value in __dict__ to bypass __set__ self.__dict__[\'my_attr\'] = value"},{"question":"Question: Complex Number Operations # Objective: Write a Python function that takes four complex numbers and performs a series of operations, returning a specific result based on the operations described below. # Function Signature: ```python def complex_operations(z1: complex, z2: complex, z3: complex, z4: complex) -> complex: pass ``` # Parameters: - `z1`, `z2`, `z3`, `z4` (complex): Four complex numbers to operate on. # Steps and Requirements: 1. **Multiply `z1` and `z2`**, then take the natural logarithm of the result. 2. **Add `z3` to `z4`**, then take the exponential of the result. 3. **Subtract** the result of step 2 from the result of step 1. 4. Return the **polar coordinates** of the result from step 3 as a tuple `(r, phi)`. # Constraints: - The input complex numbers are guaranteed to not cause any mathematical domain errors (e.g., log of zero). # Example: ```python >>> z1 = complex(1,2) >>> z2 = complex(2,3) >>> z3 = complex(3,4) >>> z4 = complex(4,5) >>> complex_operations(z1, z2, z3, z4) (1.9474253999783575, -0.9646585044076021) ``` # Explanation: - Multiply `z1` and `z2`: `(1+2j) * (2+3j) = -4 + 7j` - Natural logarithm of `-4+7j`: `cmath.log(-4 + 7j) ≈ 2.066 + 0.491j` - Add `z3` and `z4`: `(3+4j) + (4+5j) = 7 + 9j` - Exponential of `7+9j`: `cmath.exp(7 + 9j) ≈ 4.175 + 1.149j` - Subtract exponential from logarithm: `(2.066 + 0.491j) - (4.175 + 1.149j) = -2.109 - 0.658j` - Convert to polar coordinates: `cmath.polar(-2.109 - 0.658j) ≈ (2.207, -0.305)` You may use any function from the `cmath` module to achieve the desired result.","solution":"import cmath def complex_operations(z1: complex, z2: complex, z3: complex, z4: complex) -> tuple: Performs a series of operations on four complex numbers and returns the polar coordinates of the result. # Step 1: Multiply z1 and z2, then take the natural logarithm step1 = cmath.log(z1 * z2) # Step 2: Add z3 and z4, then take the exponential step2 = cmath.exp(z3 + z4) # Step 3: Subtract the result of step 2 from the result of step 1 step3 = step1 - step2 # Step 4: Return the polar coordinates of the result of step 3 return cmath.polar(step3)"},{"question":"# Advanced Coding Assessment - Python Mailbox Module Objective: Write a Python script to perform the following operations on mailboxes using the `mailbox` module: 1. **Read** messages from a `Maildir` mailbox. 2. **Filter** the messages based on whether they contain a specified keyword in the subject. 3. **Copy** the filtered messages into an `mbox` formatted mailbox. 4. **Ensure** that the message flags are correctly translated from Maildir to mbox format. Details: 1. **Input:** - A `Maildir` mailbox directory path: `maildir_path` (e.g., `\\"~/Maildir\\"`). - A keyword to filter messages by subject: `keyword` (e.g., `\\"python\\"`). - An `mbox` mailbox file path for the output: `mbox_path` (e.g., `\\"~/output.mbox\\"`). 2. **Output:** - The resulting `mbox` mailbox should only contain messages from the `Maildir` mailbox with subjects containing the `keyword`. - Each message should have its flags correctly translated to mbox-equivalent flags. Constraints: - You should **not** modify the original `Maildir` mailbox during the process. - The original messages should be left unchanged, specifically regarding their `Maildir` flags and content. - Handle potential parsing errors gracefully by skipping malformed messages. Requirements: - Implement a function `filter_and_transform_maildir_to_mbox(maildir_path: str, keyword: str, mbox_path: str) -> None`. - Ensure that the function reads from a `Maildir`, filters by subject, and writes to an `mbox`. Function Signature: ```python def filter_and_transform_maildir_to_mbox(maildir_path: str, keyword: str, mbox_path: str) -> None: pass ``` Example Usage: ```python filter_and_transform_maildir_to_mbox(\\"~/Maildir\\", \\"python\\", \\"~/output.mbox\\") ``` This will read messages from the Maildir at `~/Maildir`, filter by the keyword `\\"python\\"` in their subjects, and write these filtered messages to `~/output.mbox`. Hints: - Utilize the `mailbox` module\'s `Maildir` and `mbox` classes. - Use methods such as `iteritems` to iterate through the Maildir messages. - Look into `MaildirMessage` and `mboxMessage` for handling message formats and flags. - Ensure you lock and unlock mailboxes correctly where necessary to avoid any corruption during processing.","solution":"import mailbox def filter_and_transform_maildir_to_mbox(maildir_path: str, keyword: str, mbox_path: str) -> None: Filters messages from a Maildir mailbox by subject and copies them to an mbox mailbox. Args: maildir_path (str): Path to the Maildir mailbox. keyword (str): Keyword to filter messages by subject. mbox_path (str): Path to the output mbox file. maildir = mailbox.Maildir(maildir_path) mbox = mailbox.mbox(mbox_path) for key, message in maildir.iteritems(): subject = message[\'subject\'] # This returns None if there is no subject field if subject and keyword in subject: mbox_msg = mailbox.mboxMessage(message) mbox.add(mbox_msg) mbox.close()"},{"question":"Objective: Implement a custom dimensionality reduction and regression model using the Partial Least Squares (PLS) algorithm. The model will be evaluated on its ability to predict the target matrix ( Y ) from the input matrix ( X ). Problem Statement: You are required to implement the class `PLSModel` that mimics the behavior of `PLSCanonical` in scikit-learn. Your implementation should include the methods to fit the model on given data, transform the data, and predict targets. Class Definition: ```python import numpy as np class PLSModel: def __init__(self, n_components): Initialize the PLS model with the number of components. :param n_components: int: Number of components to keep after dimensionality reduction. self.n_components = n_components def fit(self, X, Y): Fit the PLS model to the data matrices X and Y. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :param Y: np.ndarray: Target matrix of shape (n_samples, n_targets). pass def transform(self, X): Transform the input matrix X to its lower-dimensional representation. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :return: np.ndarray: Transformed matrix of shape (n_samples, n_components). pass def predict(self, X): Predict the target matrix Y from the input matrix X. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :return: np.ndarray: Predicted target matrix of shape (n_samples, n_targets). pass ``` Requirements: 1. **Initialization (`__init__` method)**: - Initialize the model with the specified number of components. 2. **Fit (`fit` method)**: - Fit the model to the given input matrix ( X ) and target matrix ( Y ). - Follow the steps of the PLSCanonical algorithm (iterative deflation, computation of weights and loadings). 3. **Transform (`transform` method)**: - Apply the transformation to the input matrix ( X ) using the fitted model. 4. **Predict (`predict` method)**: - Predict the target matrix ( Y ) from the given input matrix ( X ). Input and Output Formats: 1. `__init__(self, n_components)` - `n_components` (int): Number of components. 2. `fit(self, X, Y)` - `X` (np.ndarray): Shape ((n_samples, n_features)), the input matrix. - `Y` (np.ndarray): Shape ((n_samples, n_targets)), the target matrix. - Output: None. 3. `transform(self, X)` - `X` (np.ndarray): Shape ((n_samples, n_features)), the input matrix. - Returns: Transformed matrix of shape ((n_samples, n_components)). 4. `predict(self, X)` - `X` (np.ndarray): Shape ((n_samples, n_features)), the input matrix. - Returns: Predicted matrix of shape ((n_samples, n_targets)). Constraints: - Assume ( X ) and ( Y ) are centered (i.e., have zero mean). - The number of components ( n_components ) will not exceed the minimum of the number of features and the number of samples. Performance Requirements: - The solution should handle datasets with up to ( 1000 ) samples and ( 100 ) features efficiently. Evaluation Criteria: - Correctness: The implementation should correctly fit, transform, and predict based on the PLS algorithm. - Efficiency: The implementation should efficiently handle the specified data sizes. - Clarity: The code should be clear and well-documented.","solution":"import numpy as np class PLSModel: def __init__(self, n_components): Initialize the PLS model with the number of components. :param n_components: int: Number of components to keep after dimensionality reduction. self.n_components = n_components def fit(self, X, Y): Fit the PLS model to the data matrices X and Y. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :param Y: np.ndarray: Target matrix of shape (n_samples, n_targets). n_samples, n_features = X.shape n_targets = Y.shape[1] self.W = np.zeros((n_features, self.n_components)) self.T = np.zeros((n_samples, self.n_components)) # X scores self.U = np.zeros((n_samples, self.n_components)) # Y scores self.P = np.zeros((n_features, self.n_components)) # X loadings self.Q = np.zeros((n_targets, self.n_components)) # Y loadings self.C = np.zeros((self.n_components, n_features)) X_residual = X.copy() Y_residual = Y.copy() for i in range(self.n_components): u = Y_residual[:, 0].copy() t_old = None for _ in range(500): # Iteration limit w = X_residual.T @ u w /= np.linalg.norm(w) t = X_residual @ w q = Y_residual.T @ t q /= np.linalg.norm(q) u = Y_residual @ q if t_old is not None and np.allclose(t, t_old): break t_old = t.copy() p = X_residual.T @ t / np.dot(t.T, t) c = u.T @ t / np.dot(t.T, t) self.W[:, i] = w self.T[:, i] = t self.U[:, i] = u self.P[:, i] = p self.Q[:, i] = q self.C[i, :] = c * w X_residual -= np.outer(t, p) Y_residual -= np.outer(t, q) def transform(self, X): Transform the input matrix X to its lower-dimensional representation. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :return: np.ndarray: Transformed matrix of shape (n_samples, n_components). return X @ self.W def predict(self, X): Predict the target matrix Y from the input matrix X. :param X: np.ndarray: Input matrix of shape (n_samples, n_features). :return: np.ndarray: Predicted target matrix of shape (n_samples, n_targets). T = self.transform(X) return T @ self.Q.T"},{"question":"# Question: Optimizing and Profiling Scikit-Learn Code You have been provided with the implementation of a simple k-means clustering algorithm. Your task is to optimize this implementation to improve its performance using vectorized operations with Numpy and other techniques discussed in the scikit-learn performance optimization documentation. Additionally, you need to profile the original and optimized code to demonstrate the performance improvements. Original Implementation Here is the basic implementation of the k-means algorithm: ```python import numpy as np from sklearn.datasets import make_blobs def kmeans(X, k, max_iter=100): # Randomly initialize centroids centroids = X[np.random.choice(X.shape[0], k, replace=False)] for i in range(max_iter): # Assign clusters clusters = np.zeros(X.shape[0]) for idx, point in enumerate(X): distances = np.linalg.norm(point - centroids, axis=1) clusters[idx] = np.argmin(distances) # Update centroids new_centroids = np.zeros_like(centroids) for c in range(k): points = X[clusters == c] new_centroids[c] = points.mean(axis=0) if np.all(centroids == new_centroids): break centroids = new_centroids return centroids, clusters # Generate synthetic data X, _ = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=42) # Profile original implementation %timeit kmeans(X, 3) ``` Requirements 1. **Optimization**: Rewrite the `kmeans` function using Numpy vectorized operations to eliminate explicit Python loops where possible. 2. **Profiling**: Profile the original and optimized code using the IPython `%timeit` and `%prun` magics to analyze performance improvements. 3. **Memory Profiling**: Use `memory_profiler` to check memory usage of the original and optimized functions. 4. **Documentation**: Provide comments and documentation explaining each step of your optimization. 5. **Output**: Output the time and memory improvements achieved. Constraints - You must use the provided `make_blobs` function to generate the test data for both the original and optimized code. - The function must retain the same functionality and produce the same outputs (within reasonable numerical tolerance). Expected Output - A detailed comparison of the execution time and memory usage of the original and the optimized k-means function. - A clear explanation of each optimization step taken. ```python # Your optimized k-means implementation here # Profile optimized implementation %timeit kmeans_optimized(X, 3) # Memory profiling from memory_profiler import profile @profile def run_kmeans(): kmeans(X, 3) @profile def run_kmeans_optimized(): kmeans_optimized(X, 3) run_kmeans() run_kmeans_optimized() ```","solution":"import numpy as np from sklearn.datasets import make_blobs def kmeans_optimized(X, k, max_iter=100): Perform k-means clustering using an optimized approach with Numpy vectorized operations. Parameters: X: np.ndarray, shape (n_samples, n_features) Data points to cluster. k: int Number of clusters. max_iter: int, optional, default: 100 Maximum number of iterations. Returns: centroids: np.ndarray, shape (k, n_features) Centroids found by the algorithm. clusters: np.ndarray, shape (n_samples,) Index of the cluster each sample belongs to. # Randomly initialize centroids centroids = X[np.random.choice(X.shape[0], k, replace=False)] for i in range(max_iter): # Assign clusters using vectorized operations distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) clusters = np.argmin(distances, axis=1) # Update centroids using vectorized operations new_centroids = np.array([X[clusters == c].mean(axis=0) for c in range(k)]) # Check for convergence if np.all(centroids == new_centroids): break centroids = new_centroids return centroids, clusters"},{"question":"**Question: Loading and Processing Mice Protein Dataset from OpenML Repository** In this task, you are required to load a dataset from the OpenML repository, preprocess it, and perform some basic data analysis. Specifically, you will work with the \\"MiceProtein\\" dataset. This dataset contains gene expressions of protein levels in the cerebral cortex of mice subjects. # Instructions: 1. **Load the dataset**: - Use the `fetch_openml` function from `sklearn.datasets` to load the \\"MiceProtein\\" dataset (dataset ID 40966). 2. **Preprocess the data**: - Convert all feature values to a floating point representation if they are not already. - Handle any categorical data by encoding it using appropriate scikit-learn encoders. - Normalize the dataset so that each feature has a mean of 0 and a standard deviation of 1. 3. **Data Analysis**: - Print the shape of the dataset. - Print the unique classes present in the target variable. - Calculate and print the mean and standard deviation of the first five features in the dataset. # Constraints: - You are not allowed to use any dataset other than the one specified. - Ensure that the dataset is normalized correctly. - You can assume all necessary libraries (`numpy`, `pandas`, `sklearn`, etc.) are pre-installed. # Expected Input and Output Format: - Load functions and preprocessing functions can be directly called. - Output should be printed as specified above: dataset shape, unique classes in target, mean and standard deviation of the first five features. # Performance Requirements: - The data loading and processing should complete within a reasonable timeframe, under typical usage conditions (i.e., don\'t use overly complex or inefficient methods). **Starter Code:** ```python from sklearn.datasets import fetch_openml from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import numpy as np import pandas as pd # Load the dataset mice = fetch_openml(data_id=40966, as_frame=True) # Inspect the dataset print(mice.data.shape) print(np.unique(mice.target)) # Convert feature values to floating point representation (if needed) and handle categorical data numeric_features = mice.data.select_dtypes(include=[np.number]).columns categorical_features = mice.data.select_dtypes(include=[\'object\']).columns # Preprocessing pipeline preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numeric_features), (\'cat\', OneHotEncoder(), categorical_features)]) pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Fit and transform the data X_preprocessed = pipeline.fit_transform(mice.data) # Output the results print(f\'Dataset shape: {X_preprocessed.shape}\') print(f\'Unique classes in target: {np.unique(mice.target)}\') print(f\'Mean of first five features: {X_preprocessed[:, :5].mean(axis=0)}\') print(f\'Standard deviation of first five features: {X_preprocessed[:, :5].std(axis=0)}\') ```","solution":"from sklearn.datasets import fetch_openml from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import numpy as np import pandas as pd def load_and_preprocess_mice_protein_dataset(): # Load the dataset mice = fetch_openml(data_id=40966, as_frame=True) # Convert feature values to floating point representation (if needed) and handle categorical data numeric_features = mice.data.select_dtypes(include=[np.number]).columns categorical_features = mice.data.select_dtypes(include=[\'object\']).columns # Preprocessing pipeline preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numeric_features), (\'cat\', OneHotEncoder(), categorical_features)]) pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Fit and transform the data X_preprocessed = pipeline.fit_transform(mice.data) return X_preprocessed, mice.target def analyze_mice_protein_dataset(X_preprocessed, target): # Print the shape of the dataset print(f\'Dataset shape: {X_preprocessed.shape}\') # Print the unique classes present in the target variable unique_classes = np.unique(target) print(f\'Unique classes in target: {unique_classes}\') # Calculate and print the mean and standard deviation of the first five features in the dataset mean_first_five_features = X_preprocessed[:, :5].mean(axis=0) std_first_five_features = X_preprocessed[:, :5].std(axis=0) print(f\'Mean of first five features: {mean_first_five_features}\') print(f\'Standard deviation of first five features: {std_first_five_features}\') return unique_classes, mean_first_five_features, std_first_five_features # Load and preprocess the dataset X_preprocessed, target = load_and_preprocess_mice_protein_dataset() # Perform analysis on the dataset analyze_mice_protein_dataset(X_preprocessed, target)"},{"question":"Coding Assessment Question **Title**: Implementing an Async Web Scraper **Objective**: The goal of this question is to assess the student\'s ability to use the asyncio library to implement an asynchronous web scraper that fetches data from multiple URLs concurrently while handling timeouts and ensuring the safe completion of tasks. **Problem Statement**: You are required to implement an asynchronous web scraper that fetches the content of multiple web pages concurrently. Your implementation should use the `asyncio` library to manage the asynchronous tasks. # Specifications: 1. **Function Name**: `async_web_scraper` 2. **Parameters**: - `urls` (list of str): A list of URLs to be fetched. - `timeout` (int): The maximum number of seconds to wait for each URL to respond. 3. **Returns**: - A dictionary with URLs as keys and their respective content (str) as values. If a timeout occurs, the value should be `\\"Timeout\\"`. 4. **Key Requirements**: - Fetch the URLs concurrently. - Handle timeouts using `asyncio.wait_for()`. - Ensure the implementation is non-blocking and runs concurrently. - Use `asyncio.gather()` to collect results efficiently. # Constraints: 1. You must use `asyncio` and related constructs (e.g., `asyncio.create_task`, `asyncio.gather`) to manage asynchronous execution. 2. Handle exceptions properly, ensuring that timeouts are correctly reported. 3. Do not use any third-party libraries for HTTP requests; use `aiohttp` or similar asynchronous HTTP functionalities. # Example Usage: ```python import aiohttp import asyncio async def async_web_scraper(urls, timeout): async with aiohttp.ClientSession() as session: async def fetch(url): try: async with session.get(url) as response: return await response.text() except asyncio.TimeoutError: return \\"Timeout\\" tasks = [asyncio.wait_for(fetch(url), timeout=timeout) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return {url: result if not isinstance(result, Exception) else \\"Timeout\\" for url, result in zip(urls, results)} # Example URL list urls = [ \'https://www.example.com\', \'https://www.python.org\', \'https://www.asyncio.org\' ] # Example call to the function result = asyncio.run(async_web_scraper(urls, timeout=5)) print(result) # Output: {\'https://www.example.com\': \'...\', \'https://www.python.org\': \'...\' , \'https://www.asyncio.org\': \'...\'} ``` # Notes: - Ensure that your code is efficient and handles edge cases, such as invalid URLs or network errors. - The actual HTTP requests should be made using asyncio-compatible libraries like `aiohttp`.","solution":"import aiohttp import asyncio async def async_web_scraper(urls, timeout): async with aiohttp.ClientSession() as session: async def fetch(url): try: async with session.get(url) as response: return await response.text() except asyncio.TimeoutError: return \\"Timeout\\" tasks = [asyncio.wait_for(fetch(url), timeout=timeout) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return {url: result if not isinstance(result, Exception) else \\"Timeout\\" for url, result in zip(urls, results)} # Example URL list for testing purposes urls = [ \'https://jsonplaceholder.typicode.com/posts\', \'https://jsonplaceholder.typicode.com/comments\', \'https://jsonplaceholder.typicode.com/albums\' ] # Example call to the function for testing purposes # You can run this script to see the output result = asyncio.run(async_web_scraper(urls, timeout=5)) print(result) # Output: {\'https://jsonplaceholder.typicode.com/posts\': \'...\', \'https://jsonplaceholder.typicode.com/comments\': \'...\', \'https://jsonplaceholder.typicode.com/albums\': \'...\'}"},{"question":"You are required to extend the functionality of the `Email` package\'s `ContentManager` and create a custom content manager that can handle JSON data in email messages. Your custom content manager should be able to: 1. Retrieve the JSON content from an email message. 2. Store a Python dictionary as JSON content in an email message. Your implementation should include: 1. A new class `JSONContentManager` that inherits from `ContentManager`. 2. A `get_content` method that: - Extracts the JSON content if the MIME type is `application/json`. - Raises a `KeyError` if the MIME type is anything else. 3. A `set_content` method that: - Converts a Python dictionary to a JSON string and attaches it to the email message with the MIME type `application/json`. - Raises a `TypeError` if the provided content is not a dictionary. Below is the skeleton of the `JSONContentManager` class. Complete the class by implementing the required methods and functionality. ```python import json from email.contentmanager import ContentManager from email.message import EmailMessage class JSONContentManager(ContentManager): def get_content(self, msg, *args, **kw): Retrieves the JSON content from an email message. :param msg: EmailMessage object :return: dict containing the JSON content :raises KeyError: if MIME type is not application/json # Your Code Here def set_content(self, msg, obj, *args, **kw): Stores a Python dictionary as JSON content in an email message. :param msg: EmailMessage object :param obj: dict containing the data to be stored :raises TypeError: if obj is not a dictionary # Your Code Here # Example usage msg = EmailMessage() json_content_manager = JSONContentManager() # Storing a Python dictionary as JSON content data = {\'name\': \'John Doe\', \'age\': 30} json_content_manager.set_content(msg, data) # Retrieving JSON content from the message retrieved_data = json_content_manager.get_content(msg) print(retrieved_data) # Output should be: {\'name\': \'John Doe\', \'age\': 30} ``` # Constraints - Only the MIME type `application/json` should be supported for JSON content. - Ensure proper error handling as specified in the method requirements. # Expected Input and Output - `set_content` should take an `EmailMessage` object and a dictionary, and store the dictionary as JSON content in the `EmailMessage`. - `get_content` should take an `EmailMessage` object and return the JSON content as a dictionary. # Performance Requirements - The implementation should efficiently handle encoding and decoding of JSON content, and manage MIME types appropriately.","solution":"import json from email.contentmanager import ContentManager from email.message import EmailMessage class JSONContentManager(ContentManager): def get_content(self, msg, *args, **kw): Retrieves the JSON content from an email message. :param msg: EmailMessage object :return: dict containing the JSON content :raises KeyError: if MIME type is not application/json if msg.get_content_type() != \'application/json\': raise KeyError(\'The MIME type is not application/json\') return json.loads(msg.get_payload()) def set_content(self, msg, obj, *args, **kw): Stores a Python dictionary as JSON content in an email message. :param msg: EmailMessage object :param obj: dict containing the data to be stored :raises TypeError: if obj is not a dictionary if not isinstance(obj, dict): raise TypeError(\'Content must be a dictionary\') payload = json.dumps(obj) msg.set_payload(payload) msg.set_type(\'application/json\')"},{"question":"You are provided with a dataset containing information about different species of penguins. Your task is to create a multi-faceted visualization that displays the distribution of penguin body masses categorized by species and gender, using the experimental `seaborn.objects` interface. # Input: - You will be working with the `penguins` dataset. - Assume the dataset has been loaded and cleaned, such as removing any missing values. # Instructions: 1. Import necessary libraries, including `seaborn` and `seaborn.objects`. 2. Load the `penguins` dataset. 3. Create the following visualizations: - A faceted bar plot showing the average body mass (`body_mass_g`) of penguins, categorized by `species` and `sex`. Use `facet` to create separate plots for each species. - A faceted scatter plot showing the relationship between `flipper_length_mm` and `body_mass_g`, separated by `species`. Overlay a linear regression line on each scatter plot. 4. Customize the plots: - Use different colors for different categories. - Include appropriate labels and titles for axes and the overall plot. - Customize the appearance of the plot using Seaborn theme settings. # Output: - The code should be executable and should generate the required visualizations. # Constraints: - Ensure the plots are clear and informative with appropriate customization for readability. - Use at least two different types of marks (e.g., `Bar` and `Dot`). - Apply a statistical transformation to aggregate body mass data in the bar plot. # Example Code Template: ```python # Import necessary libraries import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create a faceted bar plot for average body mass bar_plot = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\") .facet(col=\\"species\\") .add(so.Bar(), so.Agg(), so.Dodge()) .label(x=\\"Sex\\", y=\\"Average Body Mass (g)\\", title=\\"Average Body Mass by Species and Sex\\") ) # Display the bar plot bar_plot.show() # Create a faceted scatter plot with linear regression line scatter_plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"sex\\") .facet(col=\\"species\\") .add(so.Dot(), so.Dodge()) .add(so.Line(), so.PolyFit()) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Flipper Length vs. Body Mass by Species\\") ) # Display the scatter plot scatter_plot.show() # Apply Seaborn theme theme_dict = sns.axes_style(\\"whitegrid\\") so.Plot.config.theme.update(theme_dict) # Show plots using plt.show() if not in interactive environment plt.show() ```","solution":"# Import necessary libraries import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create a faceted bar plot for average body mass bar_plot = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\") .facet(col=\\"species\\") .add(so.Bar(), so.Agg(), so.Dodge()) .label(x=\\"Sex\\", y=\\"Average Body Mass (g)\\", title=\\"Average Body Mass by Species and Sex\\") ) # Display the bar plot bar_plot.show() # Create a faceted scatter plot with linear regression line scatter_plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"sex\\") .facet(col=\\"species\\") .add(so.Dot(), so.Dodge()) .add(so.Line(), so.PolyFit()) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Flipper Length vs. Body Mass by Species\\") ) # Display the scatter plot scatter_plot.show() # Apply Seaborn theme theme_dict = sns.axes_style(\\"whitegrid\\") so.Plot.config.theme.update(theme_dict) # Show plots using plt.show() if not in interactive environment plt.show()"},{"question":"**Question: Implement an Asynchronous File Reader with Platform Constraints** You are required to implement an asynchronous file reader using Python\'s `asyncio` module. The file reader should be capable of reading from a file and processing each line asynchronously. Your solution should account for platform-specific constraints, particularly focusing on the differences between Windows and Unix-based systems as described below: 1. **Windows**: - Use the `ProactorEventLoop` as it is the default event loop on Windows. - Implement the file reading mechanism using Windows-specific asynchronous I/O functions. 2. **Unix-based Systems**: - Use the default `SelectorEventLoop`. - Implement the file reading mechanism using Unix-specific asynchronous I/O functions, taking into consideration that `loop.add_reader()` and `loop.add_writer()` cannot be used to monitor file I/O on Unix. **Function Signature**: ```python import asyncio async def async_file_reader(file_path: str) -> None: Asynchronously reads and processes each line of the given file. :param file_path: Path to the file to be read asynchronously. ``` # Constraints and Requirements: 1. **Platform Detection**: Your implementation must detect the operating system and choose the appropriate event loop and file reading mechanism. 2. **File Reading**: The function should read the file line by line, print each line, and perform a dummy asynchronous task after reading each line (e.g., `await asyncio.sleep(0.1)`). 3. **Performance**: The solution should be efficient and handle large files without blocking the event loop. 4. **Error Handling**: Implement proper error handling to deal with file I/O errors and platform-specific issues gracefully. # Input: - `file_path` (str): The path to the file to be read. # Output: - The function should print each line of the file. # Example: Suppose you have a file at `\\"test.txt\\"` with the following content: ``` Line 1 Line 2 Line 3 ``` Running the function: ```python asyncio.run(async_file_reader(\\"test.txt\\")) ``` Should produce: ``` Line 1 Line 2 Line 3 ``` # Hints: - Use the `platform` module to detect the operating system. - Use appropriate asynchronous file reading methods based on the detected platform. - Remember to ensure compatibility with both Windows and Unix-based systems given their limitations regarding event loop methods.","solution":"import asyncio import platform async def process_line(line): print(line.strip()) await asyncio.sleep(0.1) async def async_file_reader(file_path: str) -> None: system = platform.system() if system == \\"Windows\\": # Use ProactorEventLoop for Windows asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) loop = asyncio.get_event_loop() else: # Use the default event loop (SelectorEventLoop) for Unix-based systems loop = asyncio.get_event_loop() async with aiofiles.open(file_path, mode=\'r\') as f: async for line in f: await process_line(line)"},{"question":"# Cross-Platform Asynchronous Task Scheduling with asyncio You are required to implement a cross-platform asynchronous task scheduler using Python\'s `asyncio` module. The challenge is to ensure that your implementation handles tasks correctly under different platform-specific limitations highlighted in the provided documentation. Objectives: 1. Create an asynchronous function `run_tasks(tasks: List[Callable[[], Awaitable[None]]])` which takes a list of asynchronous task functions and runs them concurrently. 2. Ensure that your function: - Handles different event loop limitations on Windows and macOS. - Validates that tasks are correctly scheduled and executed according to their completion times. Requirements: 1. All tasks provided to `run_tasks` should be asyncio coroutines. 2. On initiating the scheduler, an appropriate event loop should be detected or configured based on the operating system (Windows or macOS). 3. The function should ensure that the platform-specific limitations do not hinder the execution of tasks: - On Windows, verify that the `ProactorEventLoop` is used. - On macOS, ensure compatibility with character devices if running on macOS <= 10.8. Input: - A list of asynchronous task functions (e.g., `tasks: List[Callable[[], Awaitable[None]]]`). Output: - The function should not return any value. The successful execution of all provided tasks is expected. Example: ```python import asyncio from typing import List, Callable, Awaitable async def sample_task_1(): await asyncio.sleep(1) print(\\"Task 1 completed\\") async def sample_task_2(): await asyncio.sleep(2) print(\\"Task 2 completed\\") async def run_tasks(tasks: List[Callable[[], Awaitable[None]]]): # Your implementation here tasks = [sample_task_1, sample_task_2] asyncio.run(run_tasks(tasks)) ``` Constraints: - Do not use non-asyncio-based solutions (e.g., threading or multiprocessing). - Ensure the solution is compatible with the provided platform-specific limitations. Implement the `run_tasks` function ensuring cross-platform compatibility and validation of successful task execution.","solution":"import asyncio import sys from typing import List, Callable, Awaitable async def run_tasks(tasks: List[Callable[[], Awaitable[None]]]): # Adjust the event loop policy for Windows if sys.platform.startswith(\'win\'): asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) # Gather and run the tasks await asyncio.gather(*(task() for task in tasks))"},{"question":"# Coding Challenge: Sorting and Selecting with Generators and Itertools Background You have been given a task to process a large dataset of user activities efficiently. This dataset is represented as an iterable of dictionaries, where each dictionary contains details about a specific user activity. You need to implement a generator function to filter, sort, and transform this data according to specific criteria using generators and functions from the `itertools` and `functools` modules. Task Implement a function `process_user_activities(activities, activity_type, num_top)` that processes the iterable of user activity dictionaries and yields the top `num_top` user activities of `activity_type`. Each activity dictionary contains the following keys: - `\'id\'`: unique identifier for the activity (int) - `\'user\'`: user name (str) - `\'type\'`: type of activity (str) - `\'timestamp\'`: time of activity in Unix timestamp format (int) - `\'score\'`: activity score (int) **You need to:** 1. **Filter** the activities to include only those of the specified `activity_type`. 2. **Sort** the filtered activities by their `score` in descending order. 3. **Yield** only the top `num_top` activities. 4. For the final output, transform each activity dictionary to a new dictionary with only the keys `\'id\'`, `\'user\'`, and `\'score\'`. Input - `activities`: An iterable of dictionaries, each representing a user activity. - `activity_type`: A string, the type of activity to filter by. - `num_top`: An integer, the number of top activities to yield. Output - A generator yielding dictionaries containing only the keys `\'id\'`, `\'user\'`, and `\'score\'` of the top activities. Constraints - Assume `activities` can be a very large iterable (e.g., millions of entries), so the function should handle it efficiently using generator expressions and iterators. Example Usage ```python activities = [ {\'id\': 1, \'user\': \'Alice\', \'type\': \'login\', \'timestamp\': 1633036800, \'score\': 50}, {\'id\': 2, \'user\': \'Bob\', \'type\': \'purchase\', \'timestamp\': 1633036801, \'score\': 70}, {\'id\': 3, \'user\': \'Alice\', \'type\': \'login\', \'timestamp\': 1633036802, \'score\': 60}, {\'id\': 4, \'user\': \'Bob\', \'type\': \'login\', \'timestamp\': 1633036803, \'score\': 80}, {\'id\': 5, \'user\': \'Alice\', \'type\': \'purchase\', \'timestamp\': 1633036804, \'score\': 90}, ] result = process_user_activities(activities, \'login\', 2) for activity in result: print(activity) # Expected output: # {\'id\': 4, \'user\': \'Bob\', \'score\': 80} # {\'id\': 3, \'user\': \'Alice\', \'score\': 60} ``` Notes - You can utilize `itertools.islice`, `itertools.filterfalse`, `functools.partial`, and generator expressions to implement the function. - Avoid creating large intermediate data structures in memory. Good luck!","solution":"from heapq import nlargest def process_user_activities(activities, activity_type, num_top): Filters the activities by activity_type, sorts them by score in descending order, and yields the top num_top activities. filtered_activities = (activity for activity in activities if activity[\'type\'] == activity_type) top_activities = nlargest(num_top, filtered_activities, key=lambda x: x[\'score\']) for activity in top_activities: yield { \'id\': activity[\'id\'], \'user\': activity[\'user\'], \'score\': activity[\'score\'] }"},{"question":"# SVM Classification and Model Tuning with Scikit-learn Problem Statement You are provided with a dataset containing features of a certain type of plant (e.g., the Iris dataset) and are tasked with classifying the species of the plant using Support Vector Machines (SVM). Your goal is to design a model that can accurately classify the plant species. You will be required to preprocess the data, select and train an SVM classifier, optimize its parameters, and evaluate its performance. Dataset The dataset consists of the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Where `species` is the target variable with three classes (e.g., \'setosa\', \'versicolor\', \'virginica\'). You may use a synthetic example or any publicly available dataset for SVM classification. Tasks 1. **Load the Dataset**: - Load the dataset into a Pandas DataFrame. - Perform a quick exploratory data analysis (EDA) to understand the features and target variable. 2. **Preprocess the Data**: - Handle missing values if any. - Encode the categorical target variable. - Scale the feature variables to have zero mean and unit variance. 3. **Split the Dataset**: - Split the dataset into training and test sets using an 80-20 split. 4. **Model Training and Selection**: - Train an SVM classifier using the training set. - Test at least two different kernel functions (e.g., \'linear\' and \'rbf\'). - Use GridSearchCV to find the best parameters for the chosen kernels, tuning at least `C` and `gamma` (for \'rbf\' kernel). 5. **Evaluation**: - Evaluate the model’s performance on the test set using accuracy, precision, recall, and F1-score. - Print the classification report for the best model. - Visualize the decision boundaries for each of the kernels. 6. **Support Vectors**: - Retrieve and print the support vectors for the best model. - Identify the indices and the count of support vectors for each class. Implementation Details - You must use the `scikit-learn` library for implementing the SVM model. - Ensure your code is modular, with functions for each task (e.g., data loading, preprocessing, model training, etc.). - Comment your code appropriately to explain the steps taken. Sample Input and Output ```python # Sample Input dataset = pd.read_csv(\'path_to_dataset.csv\') # Function to split data X_train, X_test, y_train, y_test = preprocess_and_split_data(dataset) # Function to perform grid search best_model = perform_grid_search(X_train, y_train) # Function to evaluate and print results evaluate_model(best_model, X_test, y_test) ``` Expected output includes: - Best model parameters - Classification report - Accuracy, precision, recall, and F1-score - Visualization of decision boundaries - Support vectors and their indices/counts Constraints - Ensure the code handles cases where the dataset may have missing values or imbalanced classes. - Optimize for computational efficiency where possible. Good luck!","solution":"import pandas as pd from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.svm import SVC from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score import numpy as np import matplotlib.pyplot as plt def load_and_preprocess_data(): Loads the Iris dataset, preprocesses the data by encoding labels and scaling features, and splits it into training and test sets. iris = datasets.load_iris() df = pd.DataFrame(data=iris.data, columns=iris.feature_names) df[\'species\'] = iris.target # Encode target labels le = LabelEncoder() df[\'species\'] = le.fit_transform(df[\'species\']) # Handle missing values (if any) - for simplicity, assuming no missing values in iris dataset # Separate features and target X = df.drop(columns=[\'species\']) y = df[\'species\'] # Scale features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_and_tune_model(X_train, y_train): Trains an SVM model with GridSearchCV to find the best parameters. param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=3) grid_search.fit(X_train, y_train) return grid_search.best_estimator_ def evaluate_model(model, X_test, y_test): Evaluates the SVM model and prints detailed classification report and scores. y_pred = model.predict(X_test) print(\\"Best Model Parameters:\\", model.get_params()) print(\\"Classification Report:n\\", classification_report(y_test, y_pred)) print(\\"Accuracy:\\", accuracy_score(y_test, y_pred)) print(\\"Precision:\\", precision_score(y_test, y_pred, average=\'weighted\')) print(\\"Recall:\\", recall_score(y_test, y_pred, average=\'weighted\')) print(\\"F1-Score:\\", f1_score(y_test, y_pred, average=\'weighted\')) return y_pred def plot_decision_boundaries(model, X, y): Plots the decision boundaries for the SVM model. X = X[:, :2] # We will use only the first two features for visualization purposes h = .02 # step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(\'Sepal length\') plt.ylabel(\'Sepal width\') plt.title(\'Decision boundaries\') plt.show() def main(): X_train, X_test, y_train, y_test = load_and_preprocess_data() best_model = train_and_tune_model(X_train, y_train) evaluate_model(best_model, X_test, y_test) plot_decision_boundaries(best_model, X_train, y_train) if __name__ == \\"__main__\\": main()"},{"question":"Question: Custom Accelerator Manager **Objective:** Demonstrate your understanding of device management in PyTorch using the `torch.accelerator` module by implementing a custom class that manages device settings and synchronization. **Problem Statement:** You are required to implement a class `AcceleratorManager` which manages accelerator devices and streams. Your class should use the functions provided in the `torch.accelerator` module to provide the following functionalities: # Class Methods: 1. **`__init__(self)`**: Initializes the manager and sets the default device to the first available accelerator device. 2. **`get_device_count(self) -> int`**: Returns the number of available accelerator devices. 3. **`is_accelerator_available(self) -> bool`**: Returns `True` if accelerators are available, `False` otherwise. 4. **`get_current_device(self) -> int`**: Returns the index of the current device. 5. **`set_device(self, device_index: int)`**: Sets the current device to the device index provided. If the device index is invalid, it should print an error message. 6. **`synchronize_device(self)`**: Synchronizes the current device stream. # Constraints: 1. Assume the module `torch.accelerator` and all its functions are available and properly imported. 2. You are not required to handle GPU-specific code beyond what the module provides. 3. Ensure input to `set_device` method is always a non-negative integer and within the range of available devices. If not, handle the error gracefully. # Example Usage: ```python manager = AcceleratorManager() print(manager.get_device_count()) # Output depends on the number of devices print(manager.is_accelerator_available()) # Output: True/False print(manager.get_current_device()) # Output: 0 (initial device) manager.set_device(1) print(manager.get_current_device()) # Output: 1 manager.synchronize_device() ``` # Performance Requirements: The methods should execute efficiently, ensuring minimal overhead for device switching and synchronization tasks. Implement the `AcceleratorManager` class with the specified methods.","solution":"import torch class AcceleratorManager: def __init__(self): Initializes the manager and sets the default device to the first available accelerator device. if not torch.cuda.is_available(): raise RuntimeError(\\"No accelerator device available.\\") self.current_device = 0 torch.cuda.set_device(self.current_device) def get_device_count(self) -> int: Returns the number of available accelerator devices. return torch.cuda.device_count() def is_accelerator_available(self) -> bool: Returns True if accelerators are available, False otherwise. return torch.cuda.is_available() def get_current_device(self) -> int: Returns the index of the current device. return torch.cuda.current_device() def set_device(self, device_index: int): Sets the current device to the device index provided. If the device index is invalid, it should print an error message. if device_index < 0 or device_index >= self.get_device_count(): print(f\\"Invalid device index: {device_index}\\") else: torch.cuda.set_device(device_index) self.current_device = device_index def synchronize_device(self): Synchronizes the current device stream. torch.cuda.synchronize(self.current_device)"},{"question":"**Python Iterator Implementation Task** In this task, you will implement a custom iterator in Python that mimics the behavior of sequence iterators and callable iterators as described in the provided documentation. # Requirements 1. **Sequence Iterator** - Implement a class `MySeqIterator` that takes a sequence (e.g., list, tuple) as input and iterates over the elements using the `__getitem__()` method. - The iteration should end when an `IndexError` is raised. 2. **Callable Iterator** - Implement a class `MyCallIterator` that takes a callable object and a sentinel value as input. - The iterator should call the callable object and yield its return value at each step. - The iteration should end when the callable returns the sentinel value. # Expected Input and Output MySeqIterator - **Input**: A sequence (like a list or a tuple). - **Output**: An iterator object that iterates over the sequence elements. MyCallIterator - **Input**: A callable object (a function that takes no arguments) and a sentinel value. - **Output**: An iterator object that yields the result of calling the callable object until the sentinel value is returned. # Constraints: - The sequence input to `MySeqIterator` should support the `__getitem__()` method. - The callable input to `MyCallIterator` should be a Python callable that takes no parameters. - The performance should be efficient for large sequences and a large number of callable invocations. # Example Usage ```python # Example for MySeqIterator seq_iter = MySeqIterator([1, 2, 3, 4]) for item in seq_iter: print(item) # Outputs: 1 2 3 4 # Example for MyCallIterator def counter(): count = 0 while True: yield count count += 1 call_iter = MyCallIterator(counter(), 5) for item in call_iter: print(item) # Outputs: 0 1 2 3 4 ``` # Your Implementation ```python class MySeqIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): try: value = self.sequence[self.index] except IndexError: raise StopIteration self.index += 1 return value class MyCallIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value ``` Implement the above classes and ensure they meet the requirements.","solution":"class MySeqIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): try: value = self.sequence[self.index] except IndexError: raise StopIteration self.index += 1 return value class MyCallIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"Objective: The goal of this assessment is to evaluate your understanding of different Naive Bayes classifiers available in the scikit-learn library, alongside your ability to implement, train, and evaluate them on a given dataset. Problem Statement: You are provided with a dataset related to spam email classification. Your task is to implement, train, and evaluate the following Naive Bayes classifiers: 1. Multinomial Naive Bayes (`MultinomialNB`) 2. Complement Naive Bayes (`ComplementNB`) 3. Bernoulli Naive Bayes (`BernoulliNB`) You need to follow these steps: 1. **Data Preparation:** Load the dataset, preprocess it appropriately, and split it into training and test sets. 2. **Training:** Train the three specified Naive Bayes classifiers on the training data. 3. **Evaluation:** Evaluate the classifiers on the test data and report their performance in terms of accuracy. 4. **Comparison:** Compare the performance of these classifiers and provide insights into their appropriateness for the spam email classification task. Dataset: You can use the `fetch_20newsgroups` dataset from `sklearn.datasets` module, where you will filter categories related to \'sci.space\' and \'rec.autos\' for this binary classification task. Assume these categories as non-spam and spam emails, respectively. Implementation Details: - Use `train_test_split` from `sklearn.model_selection` to split the data. - Use `CountVectorizer` from `sklearn.feature_extraction.text` to convert the text data into feature vectors. - For the `BernoulliNB` classifier, binarize the data using the `binarize` parameter. Constraints: - Do not change the categories or their labels. - Report accuracy scores rounded to 4 decimal places. - Use a random seed of 42 for reproducibility. Input: The function should take no input. Output: Print the accuracy of each classifier in the format specified below. Example Output: ```plaintext MultinomialNB Accuracy: 0.XXXX ComplementNB Accuracy: 0.XXXX BernoulliNB Accuracy: 0.XXXX ``` Starter Code: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score def naive_bayes_spam_classifier(): # Load the dataset categories = [\'sci.space\', \'rec.autos\'] data = fetch_20newsgroups(subset=\'all\', categories=categories, shuffle=True, random_state=42) # Convert text data to feature vectors vectorizer = CountVectorizer() X = vectorizer.fit_transform(data.data) y = data.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # MultinomialNB Classifier multinomial_nb = MultinomialNB() multinomial_nb.fit(X_train, y_train) y_pred_mnb = multinomial_nb.predict(X_test) mnb_accuracy = accuracy_score(y_test, y_pred_mnb) print(f\\"MultinomialNB Accuracy: {mnb_accuracy:.4f}\\") # ComplementNB Classifier complement_nb = ComplementNB() complement_nb.fit(X_train, y_train) y_pred_cnb = complement_nb.predict(X_test) cnb_accuracy = accuracy_score(y_test, y_pred_cnb) print(f\\"ComplementNB Accuracy: {cnb_accuracy:.4f}\\") # BernoulliNB Classifier bernoulli_nb = BernoulliNB(binarize=0.0) bernoulli_nb.fit(X_train, y_train) y_pred_bnb = bernoulli_nb.predict(X_test) bnb_accuracy = accuracy_score(y_test, y_pred_bnb) print(f\\"BernoulliNB Accuracy: {bnb_accuracy:.4f}\\") # Call the function to execute the classifiers and print their accuracies naive_bayes_spam_classifier() ``` Submission - Provide your implemented function in a Python script. - Ensure your solution is efficient and follows the guidelines provided.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score def naive_bayes_spam_classifier(): # Load the dataset categories = [\'sci.space\', \'rec.autos\'] data = fetch_20newsgroups(subset=\'all\', categories=categories, shuffle=True, random_state=42) # Convert text data to feature vectors vectorizer = CountVectorizer() X = vectorizer.fit_transform(data.data) y = data.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # MultinomialNB Classifier multinomial_nb = MultinomialNB() multinomial_nb.fit(X_train, y_train) y_pred_mnb = multinomial_nb.predict(X_test) mnb_accuracy = accuracy_score(y_test, y_pred_mnb) print(f\\"MultinomialNB Accuracy: {mnb_accuracy:.4f}\\") # ComplementNB Classifier complement_nb = ComplementNB() complement_nb.fit(X_train, y_train) y_pred_cnb = complement_nb.predict(X_test) cnb_accuracy = accuracy_score(y_test, y_pred_cnb) print(f\\"ComplementNB Accuracy: {cnb_accuracy:.4f}\\") # BernoulliNB Classifier bernoulli_nb = BernoulliNB(binarize=0.0) bernoulli_nb.fit(X_train, y_train) y_pred_bnb = bernoulli_nb.predict(X_test) bnb_accuracy = accuracy_score(y_test, y_pred_bnb) print(f\\"BernoulliNB Accuracy: {bnb_accuracy:.4f}\\") # Call the function to execute the classifiers and print their accuracies naive_bayes_spam_classifier()"},{"question":"**Title**: Implement a Basic HTTP Client for Fetching Data from a Web Server **Objective**: Demonstrate your understanding of the `http.client` module by implementing a function to send HTTP GET requests and handle responses. **Question**: You are required to implement a Python function `fetch_web_content(url: str) -> dict` that connects to a web server, makes an HTTP GET request, and returns the response content and metadata. # Specifications: - **Parameters**: - `url` (str): The URL to which the GET request will be made. The URL will always include the scheme (`http` or `https`). - **Returns**: - A dictionary with the following keys: - `\'status\'`: The HTTP status code of the response (e.g., 200, 404). - `\'reason\'`: The reason phrase returned by the server (e.g., \'OK\', \'Not Found\'). - `\'headers\'`: A dictionary of response headers. - `\'body\'`: The response body as a string. # Constraints: - You should use the `http.client` module to create the HTTP/HTTPS connection, send the request, and retrieve the response. - Ensure that connections are properly closed after the request and response handling. - Handle the case where the URL is invalid by catching `http.client.InvalidURL` and returning `None`. - Handle the case where the server cannot be reached by catching `http.client.RemoteDisconnected` and returning `None`. - Handle other exceptions by returning `None`. # Example Usage: ```python result = fetch_web_content(\\"https://www.python.org\\") print(result) ``` Example output: ```python { \'status\': 200, \'reason\': \'OK\', \'headers\': { \'Content-Type\': \'text/html; charset=utf-8\', # other headers... }, \'body\': \'<!doctype html>n<html>n<head>n<title>Welcome to Python.org</title>...</html>\' } ``` # Implementation Notes: 1. Parse the URL to extract the scheme, host, port, and path. 2. Use `HTTPConnection` for `http` URLs and `HTTPSConnection` for `https` URLs. 3. Set appropriate headers if needed. 4. Retrieve and store the response status, reason, headers, and body. 5. Handle exceptions and ensure the connection is closed properly. Happy coding!","solution":"import http.client from urllib.parse import urlparse def fetch_web_content(url: str) -> dict: Makes an HTTP GET request to the given URL and returns the response content and metadata. Parameters: - url (str): The URL to which the GET request will be made. Returns: - dict: A dictionary with keys \'status\', \'reason\', \'headers\', and \'body\' or None if an error occurs. try: parsed_url = urlparse(url) if parsed_url.scheme == \\"http\\": conn = http.client.HTTPConnection(parsed_url.netloc) elif parsed_url.scheme == \\"https\\": conn = http.client.HTTPSConnection(parsed_url.netloc) else: return None conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() headers = {k: v for k, v in response.getheaders()} body = response.read().decode() result = { \'status\': response.status, \'reason\': response.reason, \'headers\': headers, \'body\': body } conn.close() return result except (http.client.InvalidURL, http.client.RemoteDisconnected): return None except Exception: return None"},{"question":"You are given the task to process and filter an XML document. The document contains some encoded special characters, and you need to escape and unescape these characters as necessary. Additionally, you need to generate a new XML document from the processed content. Task 1. **Escape Special Characters:** Write a function `escape_special_characters(data: str, entities: dict) -> str` that takes a string `data` and a dictionary `entities` and returns the string with special characters escaped using the `xml.sax.saxutils.escape` function. 2. **Unescape Special Characters:** Write a function `unescape_special_characters(data: str, entities: dict) -> str` that takes a string `data` and a dictionary `entities` and returns the string with special characters unescaped using the `xml.sax.saxutils.unescape` function. 3. **Generate XML Document:** Write a class `CustomXMLGenerator` that extends `xml.sax.saxutils.XMLGenerator`. This class should have an additional method `generate_element_with_attributes` that takes an element name, attributes dictionary, and content, and writes a complete XML element to the output. Expected Function Signatures ```python def escape_special_characters(data: str, entities: dict) -> str: pass def unescape_special_characters(data: str, entities: dict) -> str: pass class CustomXMLGenerator(xml.sax.saxutils.XMLGenerator): def __init__(self, out=None, encoding=\'iso-8859-1\', short_empty_elements=False): super().__init__(out, encoding, short_empty_elements) def generate_element_with_attributes(self, name: str, attributes: dict, content: str): pass ``` Input and Output Formats 1. **escape_special_characters(data, entities)**: - **Input:** A string `data` and a dictionary `entities`. - **Output:** A string with special characters escaped. 2. **unescape_special_characters(data, entities)**: - **Input:** A string `data` and a dictionary `entities`. - **Output:** A string with special characters unescaped. 3. **CustomXMLGenerator**: - **Input:** Methods for generating elements with attributes and content within the context of an XML document. - **Output:** XML representation written to the specified output stream. Example ```python # Example usage of escape_special_characters data = \\"Tom & Jerry < Cartoons >\\" entities = {\\"Tom\\": \\"TOM\\", \\"Jerry\\": \\"JERRY\\"} escaped_data = escape_special_characters(data, entities) print(escaped_data) # Should print: TOM &amp; JERRY &lt; Cartoons &gt; # Example usage of unescape_special_characters data = \\"TOM &amp; JERRY &lt; Cartoons &gt;\\" entities = {\\"TOM\\": \\"Tom\\", \\"JERRY\\": \\"Jerry\\"} unescaped_data = unescape_special_characters(data, entities) print(unescaped_data) # Should print: Tom & Jerry < Cartoons > # Example usage of CustomXMLGenerator import io output_stream = io.StringIO() generator = CustomXMLGenerator(output_stream, encoding=\'utf-8\') attributes = {\\"id\\": \\"123\\", \\"name\\": \\"example\\"} content = \\"This is an example content.\\" generator.startDocument() generator.generate_element_with_attributes(\\"item\\", attributes, content) generator.endDocument() print(output_stream.getvalue()) # Should print: # <?xml version=\\"1.0\\" encoding=\\"utf-8\\"?> # <item id=\\"123\\" name=\\"example\\">This is an example content.</item> ``` Constraints * The `data` input will only contain alphanumeric characters and the special characters needed to be escaped or unescaped. * The `entities` dictionary will only contain valid key-value pairs where both the key and value are strings. * The XML elements and attributes in the `CustomXMLGenerator` class should follow standard XML conventions. Ensure that your functions handle edge cases and provide appropriate error handling where necessary.","solution":"import xml.sax.saxutils def escape_special_characters(data: str, entities: dict) -> str: Returns the string with special characters escaped. Args: data (str): The string to escape. entities (dict): A dictionary of entities to replace. Returns: str: The escaped string. return xml.sax.saxutils.escape(data, entities) def unescape_special_characters(data: str, entities: dict) -> str: Returns the string with special characters unescaped. Args: data (str): The string to unescape. entities (dict): A dictionary of entities to replace. Returns: str: The unescaped string. return xml.sax.saxutils.unescape(data, entities) from xml.sax.saxutils import XMLGenerator class CustomXMLGenerator(XMLGenerator): def __init__(self, out=None, encoding=\'iso-8859-1\', short_empty_elements=False): super().__init__(out, encoding, short_empty_elements) def generate_element_with_attributes(self, name: str, attributes: dict, content: str): Writes a complete XML element with the given name, attributes, and content to the output. Args: name (str): The name of the element. attributes (dict): A dictionary of attributes for the element. content (str): The content of the element. self.startElement(name, attributes) self.characters(content) self.endElement(name)"},{"question":"**XML Manipulation and Parsing** # Objective: Implement a Python function that reads an XML file, modifies it based on certain criteria, and writes the modified content back to another XML file. # Details: Write a Python function `process_xml(input_file: str, output_file: str, tag_name: str, attribute_name: str, attribute_value: str) -> int` that performs the following operations: 1. **Read the XML file** specified by `input_file`. 2. **Find all elements** with the specified `tag_name`. 3. **Update the attribute** `attribute_name` of those elements with the new `attribute_value`. 4. **Write the modified XML content** to the file specified by `output_file`. 5. **Return the count** of elements that were updated. # Constraints: - The input XML file is guaranteed to be well-formed. - Elements may or may not have the attribute specified by `attribute_name`. - If an element does not have the attribute, it should be added with the provided `attribute_value`. # Example: Given the following `input_file` content: ```xml <root> <item id=\\"1\\">Item 1</item> <item>Item 2</item> <item id=\\"3\\">Item 3</item> <other>Not an item</other> </root> ``` Calling `process_xml(\'input.xml\', \'output.xml\', \'item\', \'id\', \'updated\')` should produce an `output_file` with content: ```xml <root> <item id=\\"updated\\">Item 1</item> <item id=\\"updated\\">Item 2</item> <item id=\\"updated\\">Item 3</item> <other>Not an item</other> </root> ``` And the function should return `3` since three `item` elements were updated. # Function Signature: ```python def process_xml(input_file: str, output_file: str, tag_name: str, attribute_name: str, attribute_value: str) -> int: pass ``` # Additional Information: - You may use the `xml.etree.ElementTree` module for XML parsing and manipulation. - Ensure the output file retains correct XML formatting and encoding.","solution":"import xml.etree.ElementTree as ET def process_xml(input_file: str, output_file: str, tag_name: str, attribute_name: str, attribute_value: str) -> int: Reads an XML file, modifies elements based on specified criteria, and writes the modified content to another XML file. Parameters: - input_file: str : Path to the input XML file. - output_file: str : Path to the output XML file. - tag_name: str : Name of the tag to search for elements. - attribute_name: str : Name of the attribute to modify. - attribute_value: str : New value for the specified attribute. Returns: - int : Count of elements that were updated. tree = ET.parse(input_file) root = tree.getroot() # Initialize count of updated elements updated_count = 0 # Iterate over all elements with the specified tag name for element in root.findall(tag_name): element.set(attribute_name, attribute_value) updated_count += 1 # Write the modified XML content to the output file tree.write(output_file) return updated_count"},{"question":"# Distributed Checkpoint in PyTorch You are tasked with simulating a distributed training environment using PyTorch\'s `torch.distributed.checkpoint` module for saving and loading checkpoints. Your goal is to manage the state dictionaries of a model and optimizer across multiple devices. # Objective Implement a script that: 1. Initializes a simple neural network and optimizer. 2. Saves the model and optimizer states using `torch.distributed.checkpoint.save`. 3. Loads the model and optimizer states into new model and optimizer instances using `torch.distributed.checkpoint.load`. # Instructions 1. **Model Definition**: - Create a simple feedforward neural network using `torch.nn.Module`. 2. **Checkpointing Operations**: - Implement a function `save_checkpoint(model, optimizer, path)` that saves the model and optimizer states using `torch.distributed.checkpoint.save`. - Implement a function `load_checkpoint(model, optimizer, path)` that loads the model and optimizer states using `torch.distributed.checkpoint.load`. 3. **Simulation of Distributed Setting**: - Mock a distributed setting using multiple dummy ranks. - Ensure the saved checkpoint files are distributed among ranks. 4. **State Dictionary Handling**: - Use `torch.distributed.checkpoint.state_dict.get_model_state_dict` and `torch.distributed.checkpoint.state_dict.get_optimizer_state_dict` for obtaining consistent state dictionaries for saving. - For loading, use the corresponding setter functions to properly restore states to the new model and optimizer. # Code Template Start from the following code skeleton: ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.checkpoint import save, load # Define a simple feedforward neural network class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def save_checkpoint(model, optimizer, path): # Obtain state dictionaries model_state_dict = torch.distributed.checkpoint.state_dict.get_model_state_dict(model) optimizer_state_dict = torch.distributed.checkpoint.state_dict.get_optimizer_state_dict(optimizer) # Save the state dictionaries save([model_state_dict, optimizer_state_dict], path) def load_checkpoint(model, optimizer, path): # Load the state dictionaries loaded_state_dict = load(path) # Set the state dictionaries torch.distributed.checkpoint.state_dict.set_model_state_dict(model, loaded_state_dict[0]) torch.distributed.checkpoint.state_dict.set_optimizer_state_dict(optimizer, loaded_state_dict[1]) def main(): # Initialize model, optimizer and paths model = SimpleNN(input_size=10, hidden_size=20, output_size=1) optimizer = optim.SGD(model.parameters(), lr=0.01) checkpoint_path = \'./checkpoint/\' # Save checkpoint save_checkpoint(model, optimizer, checkpoint_path) # Create new model and optimizer instances new_model = SimpleNN(input_size=10, hidden_size=20, output_size=1) new_optimizer = optim.SGD(new_model.parameters(), lr=0.01) # Load checkpoint load_checkpoint(new_model, new_optimizer, checkpoint_path) # Verify state restoration original_params = list(model.parameters()) new_params = list(new_model.parameters()) assert all(torch.equal(op, np) for op, np in zip(original_params, new_params)), \\"Model state not restored properly\\" print(\\"Model and optimizer states restored successfully\\") if __name__ == \'__main__\': main() ``` # Input and Output - **Input**: No user input is required; the script will initialize and handle everything. - **Output**: Confirmations and error messages during run-time. # Constraints - Perform file operations on a local filesystem. - Ensure the scripts handle CPU and GPU (if available) contexts properly. - Mocking of distributed settings could involve creating multiple files corresponding to different ranks. # Performance - Ensure that the functions operate efficiently and avoid redundant computations. - Handle potential synchronization issues during save/load operations in the simulated distributed environment.","solution":"import torch import torch.nn as nn import torch.optim as optim import os from torch.distributed import init_process_group, destroy_process_group # Define a simple feedforward neural network class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def save_checkpoint(model, optimizer, path, rank): os.makedirs(path, exist_ok=True) model_state_dict = model.state_dict() optimizer_state_dict = optimizer.state_dict() torch.save(model_state_dict, os.path.join(path, f\\"model_state_rank_{rank}.pt\\")) torch.save(optimizer_state_dict, os.path.join(path, f\\"optimizer_state_rank_{rank}.pt\\")) def load_checkpoint(model, optimizer, path, rank): model_state_dict = torch.load(os.path.join(path, f\\"model_state_rank_{rank}.pt\\")) optimizer_state_dict = torch.load(os.path.join(path, f\\"optimizer_state_rank_{rank}.pt\\")) model.load_state_dict(model_state_dict) optimizer.load_state_dict(optimizer_state_dict) def run_training(rank, world_size): torch.manual_seed(0) model = SimpleNN(input_size=10, hidden_size=20, output_size=1) optimizer = optim.SGD(model.parameters(), lr=0.01) checkpoint_path = \'./checkpoint_distributed/\' if rank == 0: # Save checkpoint only on rank 0 save_checkpoint(model, optimizer, checkpoint_path, rank) torch.distributed.barrier() # Create new model and optimizer instances new_model = SimpleNN(input_size=10, hidden_size=20, output_size=1) new_optimizer = optim.SGD(new_model.parameters(), lr=0.01) load_checkpoint(new_model, new_optimizer, checkpoint_path, rank) # Verify state restoration original_params = list(model.parameters()) new_params = list(new_model.parameters()) assert all(torch.equal(op, np) for op, np in zip(original_params, new_params)), \\"Model state not restored properly\\" print(f\\"Rank {rank}: Model and optimizer states restored successfully\\") def main(): world_size = 2 os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' init_process_group(backend=\'gloo\', rank=0, world_size=world_size) run_training(0, world_size) destroy_process_group() if __name__ == \'__main__\': main()"},{"question":"# Logging System Events Using `syslog` You are tasked with creating a logging system for a Unix-based application that needs to send different types of log messages to the system logger using Python\'s `syslog` module. Your goal is to implement a Python class `SysLogger` that encapsulates the functionality of `syslog` and provides an easy-to-use interface for logging messages with different priorities and to different facilities. Requirements 1. **Class Definition**: Define a class `SysLogger`. 2. **Initialization**: The constructor should initialize the logger with a default `ident`, `logoption`, and `facility`. 3. **Logging Functions**: - `log_message(message, priority=syslog.LOG_INFO)`: Logs a message with the specified priority. - `set_log_options(options)`: Sets the logging options. - `set_facility(facility)`: Sets the logging facility. - `close_logger()`: Closes the logger. 4. **Priority Mask**: - `set_priority_mask(mask)`: Sets the logging priority mask using `syslog.setlogmask()`. Input Format - The constructor should handle the following optional parameters: - `ident`: A string to identify the source of the log messages. - `logoption`: An integer representing option flags combined using bitwise OR. - `facility`: An integer representing the logging facility. Output Format - The methods should print any relevant confirmation messages to the console (e.g., when a log message is sent). Constraints - Ensure that the class maintains efficiency in setting and changing logging parameters. - Handle any possible exceptions that may arise during logging. Performance Requirements - The logging system should not introduce significant overhead and should be able to handle rapid successive log messages. # Example Usage ```python import syslog class SysLogger: def __init__(self, ident=\'default_ident\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER): # Initialize logger pass def log_message(self, message, priority=syslog.LOG_INFO): # Log message with specified priority pass def set_log_options(self, options): # Set logging options pass def set_facility(self, facility): # Set logging facility pass def close_logger(self): # Close logger pass def set_priority_mask(self, mask): # Set priority mask pass # Example Usage logger = SysLogger(ident=\'MyApp\', logoption=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_MAIL) logger.log_message(\'Application started\', syslog.LOG_INFO) logger.set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) logger.log_message(\'This is a warning\', syslog.LOG_WARNING) logger.close_logger() ``` Implement the class `SysLogger` according to the specifications and ensure it passes the example usage scenario.","solution":"import syslog class SysLogger: def __init__(self, ident=\'default_ident\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER): Initializes the SysLogger with the given ident, logoption, and facility. self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) def log_message(self, message, priority=syslog.LOG_INFO): Logs a message with the specified priority. syslog.syslog(priority, message) print(f\'Logged message: {message} with priority: {priority}\') def set_log_options(self, options): Sets the logging options. self.logoption = options syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) print(f\'Set log options: {options}\') def set_facility(self, facility): Sets the logging facility. self.facility = facility syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) print(f\'Set facility: {facility}\') def close_logger(self): Closes the logger. syslog.closelog() print(\'Logger closed\') def set_priority_mask(self, mask): Sets the logging priority mask. syslog.setlogmask(mask) print(f\'Set priority mask: {mask}\')"},{"question":"# Multiprocessing with PyTorch Elastic Your task is to create a function that starts multiple worker processes to perform a simple computation (e.g., multiplying elements in a given list by 2) and then gather results from all workers. Function Signature ```python def start_and_execute_multiprocessing(data: List[int], num_workers: int) -> List[int]: Starts multiple worker processes to multiply elements in a given list by 2 and gather results. Args: - data (List[int]): A list of integers that needs to be processed. - num_workers (int): The number of worker processes to start. Returns: - List[int]: A list of processed integers where each integer is multiplied by 2. ``` Requirements 1. Use `torch.distributed.elastic.multiprocessing.start_processes`. 2. Define a worker function that performs the actual multiplication. 3. Utilize appropriate context classes (`MultiprocessContext`, `SubprocessContext`) as needed. 4. Collect results from all worker processes and return the combined results. Example ```python data = [1, 2, 3, 4, 5] num_workers = 3 output = start_and_execute_multiprocessing(data, num_workers) print(output) # Expected: [2, 4, 6, 8, 10] ``` Constraints - Ensure that the function efficiently handles the multiprocessing and combines the results correctly. - You may assume that the length of the data list is divisible by the number of workers. **Note**: Please provide a complete function implementation that meets the requirements above using PyTorch Elastic\'s multiprocessing capabilities.","solution":"import torch import torch.multiprocessing as mp def worker_process(data_partition, return_list, index): # Simple computation: multiplying elements in data_partition by 2 result = [x * 2 for x in data_partition] return_list[index] = result def start_and_execute_multiprocessing(data, num_workers): # Ensure that the length of data list is divisible by the number of workers assert len(data) % num_workers == 0, \\"The length of data must be divisible by the number of workers\\" partition_size = len(data) // num_workers manager = mp.Manager() return_list = manager.list([None] * num_workers) processes = [] for i in range(num_workers): data_partition = data[i * partition_size: (i+1) * partition_size] process = mp.Process(target=worker_process, args=(data_partition, return_list, i)) processes.append(process) process.start() for process in processes: process.join() # Collect results from the return list result = [] for part in return_list: result.extend(part) return result"},{"question":"Objective You are tasked with creating a function to handle missing values in a pandas DataFrame. Specifically, you need to implement a function that identifies missing values in both numeric (NA) and datetime (NaT) columns, replaces them with specified placeholder values, and returns the cleaned DataFrame. Function Signature ```python def clean_missing_values(df: pd.DataFrame, num_placeholder: float, datetime_placeholder: str) -> pd.DataFrame: pass ``` Input 1. **df (pd.DataFrame)**: A pandas DataFrame that may contain missing values. 2. **num_placeholder (float)**: A float value that will replace any \'NA\' (nullable dtypes) in the numeric columns. 3. **datetime_placeholder (str)**: A string representing a date/time (in \\"YYYY-MM-DD\\" format) that will replace any \'NaT\' in the datetime/timedelta columns. Output - **pd.DataFrame**: The cleaned DataFrame with all missing values replaced by the specified placeholder values. Constraints - The DataFrame `df` may include a mix of numeric, datetime, and other data types, but you only need to handle missing values for numeric and datetime/timedelta columns. - The `datetime_placeholder` should be converted to a pandas datetime object before replacing NaT values. Example Usage ```python import pandas as pd import numpy as np # Example DataFrame data = { \'numeric_column\': [1, 2, np.nan, 4], \'datetime_column\': [pd.Timestamp(\'2021-01-01\'), pd.NaT, pd.Timestamp(\'2021-01-03\'), pd.Timestamp(\'2021-01-04\')], \'string_column\': [\'a\', \'b\', \'c\', \'d\'] } df = pd.DataFrame(data) # Function call cleaned_df = clean_missing_values(df, num_placeholder=0.0, datetime_placeholder=\'2021-01-01\') print(cleaned_df) ``` Expected Output ``` numeric_column datetime_column string_column 0 1.0 2021-01-01 a 1 2.0 2021-01-01 b 2 0.0 2021-01-03 c 3 4.0 2021-01-04 d ``` Instructions - Implement the `clean_missing_values` function as described. - Ensure that all missing numeric values (NA) are replaced by `num_placeholder`. - Ensure that all missing datetime values (NaT) are replaced by `datetime_placeholder`. - Validate the format of `datetime_placeholder`. If it is not a valid date string, raise a `ValueError`.","solution":"import pandas as pd import numpy as np def clean_missing_values(df: pd.DataFrame, num_placeholder: float, datetime_placeholder: str) -> pd.DataFrame: Replaces missing values in numeric and datetime columns of a DataFrame with specified placeholders. Parameters: df (pd.DataFrame): The DataFrame containing data with potential missing values. num_placeholder (float): The placeholder for missing numeric values. datetime_placeholder (str): The placeholder for missing datetime values in \\"YYYY-MM-DD\\" format. Returns: pd.DataFrame: The DataFrame with missing values replaced. # Attempt to convert the datetime placeholder to a pandas datetime object try: datetime_placeholder = pd.to_datetime(datetime_placeholder) except ValueError: raise ValueError(\\"Invalid datetime format for datetime_placeholder. Use \'YYYY-MM-DD\'.\\") # Replace NA in numeric columns numeric_columns = df.select_dtypes(include=[np.number]).columns df[numeric_columns] = df[numeric_columns].fillna(num_placeholder) # Replace NaT in datetime columns datetime_columns = df.select_dtypes(include=[np.datetime64]).columns df[datetime_columns] = df[datetime_columns].fillna(datetime_placeholder) return df"},{"question":"**MaskedTensor Custom Function** Your task is to create a function that computes the element-wise product of two masked tensors, followed by a masked sum reduction and reshaping of the result. Specifically, you need to implement the function `masked_tensor_operations` that performs the following steps: 1. **Element-wise Product**: Compute the element-wise product of two masked tensors. The masks of both tensors should match for the operation to be valid. 2. **Masked Sum Reduction**: Perform a masked sum reduction on the resulting tensor from step 1 to compute the sum of all unmasked elements. 3. **Reshaping**: Reshape the tensor obtained from step 1 into a shape specified by the user. # Function Signature ```python def masked_tensor_operations(tensor1: torch.Tensor, mask1: torch.Tensor, tensor2: torch.Tensor, mask2: torch.Tensor, shape: Tuple[int, ...]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Compute the element-wise product of two masked tensors, followed by a masked sum reduction and reshaping. Parameters: tensor1 (torch.Tensor): The first input tensor. mask1 (torch.Tensor): The mask for the first input tensor. tensor2 (torch.Tensor): The second input tensor. mask2 (torch.Tensor): The mask for the second input tensor. shape (Tuple[int, ...]): The target shape for reshaping the tensor after the element-wise product. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the element-wise product tensor, the result of the masked sum reduction, and the reshaped tensor. ``` # Constraints 1. The shape of `tensor1`, `mask1`, `tensor2`, and `mask2` should be the same. 2. The masks must be of boolean type. 3. The target `shape` must be compatible with the tensor obtained after the element-wise product. # Input 1. `tensor1`: PyTorch tensor of any shape. Example: `torch.tensor([[1, 2], [3, 4]])` 2. `mask1`: Boolean mask tensor of the same shape as `tensor1`. Example: `torch.tensor([[True, False], [True, True]])` 3. `tensor2`: PyTorch tensor of the same shape as `tensor1`. Example: `torch.tensor([[5, 6], [7, 8]])` 4. `mask2`: Boolean mask tensor of the same shape as `tensor1`. Example: `torch.tensor([[True, False], [True, True]])` 5. `shape`: Tuple indicating the target shape for reshaping. Example: `(1, 4)` # Output A tuple comprising of: 1. A `torch.Tensor` representing the element-wise product result. 2. A `torch.Tensor` representing the sum of all unmasked elements in the element-wise product tensor. 3. A `torch.Tensor` representing the reshaped tensor. # Example ```python tensor1 = torch.tensor([[1, 2], [3, 4]]) mask1 = torch.tensor([[True, False], [True, True]]) tensor2 = torch.tensor([[5, 6], [7, 8]]) mask2 = torch.tensor([[True, False], [True, True]]) shape = (1, 4) product_tensor, masked_sum, reshaped_tensor = masked_tensor_operations(tensor1, mask1, tensor2, mask2, shape) print(product_tensor) # Output: MaskedTensor(tensor([[ 5, -- ], [21, 32]]), mask=tensor([[True, False], [True, True]])) print(masked_sum) # Output: tensor(58) print(reshaped_tensor) # Output: MaskedTensor(tensor([[ 5, -- , 21, 32]]), mask=tensor([[ True, False, True, True]])) ``` # Notes: - Raise appropriate errors if the input constraints are not met. - Make use of `torch.masked` functionalities to handle the masked operations effectively. - Check for mask matching before proceeding with the operations.","solution":"import torch from typing import Tuple def masked_tensor_operations(tensor1: torch.Tensor, mask1: torch.Tensor, tensor2: torch.Tensor, mask2: torch.Tensor, shape: Tuple[int, ...]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Compute the element-wise product of two masked tensors, followed by a masked sum reduction and reshaping. Parameters: tensor1 (torch.Tensor): The first input tensor. mask1 (torch.Tensor): The mask for the first input tensor. tensor2 (torch.Tensor): The second input tensor. mask2 (torch.Tensor): The mask for the second input tensor. shape (Tuple[int, ...]): The target shape for reshaping the tensor after the element-wise product. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the element-wise product tensor, the result of the masked sum reduction, and the reshaped tensor. # Check if the shapes of tensors and masks match assert tensor1.shape == mask1.shape == tensor2.shape == mask2.shape, \\"Shapes of tensors and masks must match.\\" assert mask1.dtype == torch.bool and mask2.dtype == torch.bool, \\"Masks must be of boolean type.\\" # Element-wise product of the two tensors product_tensor = tensor1 * tensor2 # Combined mask (element-wise AND operation) combined_mask = mask1 & mask2 # Mask the product tensor masked_product_tensor = torch.where(combined_mask, product_tensor, torch.zeros_like(product_tensor)) # Masked sum reduction (sum of only the masked elements) masked_sum = masked_product_tensor.sum() # Reshape the masked_product_tensor to the desired shape reshaped_tensor = masked_product_tensor.reshape(shape) # Reshape the combined_mask to the desired shape reshaped_mask = combined_mask.reshape(shape) # Return the tuple of results return masked_product_tensor, masked_sum, reshaped_tensor, reshaped_mask"},{"question":"# Biclustering Implementation and Evaluation You are tasked with implementing a simplified version of the Spectral Biclustering algorithm using the concepts outlined in the documentation. Your task is to perform the following steps: 1. **Preprocess the Data Matrix**: Normalize the input data matrix using independent row and column normalization. 2. **Perform Singular Value Decomposition (SVD)**: Compute the first few singular vectors of the normalized matrix. 3. **Cluster Rows and Columns**: Use k-means clustering on the singular vectors to cluster the rows and columns. 4. **Evaluate the Clustering**: Implement a function to evaluate the biclustering results using the Jaccard index. # Input and Output Format Input - A 2D numpy array `data` of shape `(m, n)`, representing the data matrix. - An integer `num_clusters` representing the number of clusters for both rows and columns. Output - A tuple of two lists: (`row_labels`, `column_labels`). - `row_labels`: A list of integers representing the cluster labels assigned to the rows. - `column_labels`: A list of integers representing the cluster labels assigned to the columns. # Constraints - You may use numpy and scikit-learn libraries. - Ensure the code is efficient and can handle large matrices up to shape `(1000, 1000)`. # Performance Requirements - Aim for linearithmic time complexity, particularly focusing on efficient matrix operations. - Optimize the k-means clustering step for large matrices. # Function Signature ```python import numpy as np from typing import Tuple, List def spectral_biclustering(data: np.ndarray, num_clusters: int) -> Tuple[List[int], List[int]]: pass ``` # Example Input ```python data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) num_clusters = 2 ``` Output ```python ([0, 0, 1], [0, 1, 1]) ``` # Implementation Tips 1. **Preprocessing**: Normalize the data matrix by dividing each row by its mean and each column by its mean: ```python row_means = np.mean(data, axis=1) col_means = np.mean(data, axis=0) normalized_data = data / row_means[:, np.newaxis] normalized_data = normalized_data / col_means ``` 2. **SVD**: Use numpy\'s `np.linalg.svd` function to compute the singular vectors: ```python U, s, Vt = np.linalg.svd(normalized_data, full_matrices=False) ``` 3. **Clustering**: Use k-means clustering from scikit-learn to cluster the rows and columns: ```python from sklearn.cluster import KMeans row_clusters = KMeans(n_clusters=num_clusters).fit(U[:, :num_clusters]) col_clusters = KMeans(n_clusters=num_clusters).fit(Vt[:num_clusters, :].T) ``` 4. **Evaluation**: Implement the Jaccard index to compare the found biclusters with true biclusters if available. **Note:** You can refer to the scikit-learn documentation and examples provided for additional guidance on implementing this algorithm.","solution":"import numpy as np from sklearn.cluster import KMeans from typing import Tuple, List def spectral_biclustering(data: np.ndarray, num_clusters: int) -> Tuple[List[int], List[int]]: # Step 1: Normalize the data matrix by row and column row_means = np.mean(data, axis=1, keepdims=True) col_means = np.mean(data, axis=0, keepdims=True) normalized_data = data / row_means normalized_data = normalized_data / col_means # Step 2: Perform Singular Value Decomposition (SVD) U, s, Vt = np.linalg.svd(normalized_data, full_matrices=False) # Step 3: Cluster rows and columns using k-means row_model = KMeans(n_clusters=num_clusters, random_state=0).fit(U[:, :num_clusters]) col_model = KMeans(n_clusters=num_clusters, random_state=0).fit(Vt.T[:, :num_clusters]) row_labels = row_model.labels_.tolist() column_labels = col_model.labels_.tolist() return row_labels, column_labels"},{"question":"Objective The objective of this assessment is to demonstrate the ability to perform feature extraction from data using various tools provided by `scikit-learn`. This will involve using `DictVectorizer` to handle structured data, `FeatureHasher` for efficient memory usage in feature transformation, `CountVectorizer` for text tokenization and count, and `TfidfTransformer` for text normalization. You will combine these techniques to prepare a dataset for machine learning. Problem Statement You are provided with a dataset containing user reviews. Each review entry has the following structure: ```python { \'username\': \'johndoe\', \'review\': \'The product was great!\', \'rating\': 4.5, \'categories\': [\'electronics\', \'home_appliances\'] } ``` Your task is to write Python functions using `scikit-learn` to achieve the following: 1. **Transform the structured data** (`username`, `rating`, `categories`) into numerical features using `DictVectorizer`. 2. **Transform the `review` text data** into numerical features using `CountVectorizer` and `TfidfTransformer`. 3. **Combine both feature sets** into a single sparse matrix suitable for a machine learning model. Requirements 1. Implement the function `transform_structured_data(entries: List[Dict[str, Any]]) -> sparse.csr_matrix`. This function should: - Take a list of dictionary entries as input. - Use `DictVectorizer` to transform the entry fields (`username`, `rating`, `categories`). - Return a sparse matrix. 2. Implement the function `transform_reviews(entries: List[Dict[str, Any]]) -> sparse.csr_matrix`. This function should: - Take a list of dictionary entries as input. - Use `CountVectorizer` to convert the review texts into term frequency vectors. - Use `TfidfTransformer` to normalize these vectors. - Return a sparse matrix. 3. Implement the function `combine_features(structured_features: sparse.csr_matrix, text_features: sparse.csr_matrix) -> sparse.csr_matrix`. This function should: - Take two sparse matrices (structured and text features) as input. - Combine them horizontally to form a single feature matrix suitable for machine learning. - Return the combined sparse matrix. Constraints - Use default settings for all transformers unless specified. - Ensure the transformations handle cases where some entries might be missing certain fields (e.g., categories or reviews). Example Usage ```python entries = [ {\'username\': \'johndoe\', \'review\': \'The product was great!\', \'rating\': 4.5, \'categories\': [\'electronics\', \'home_appliances\']}, {\'username\': \'janedoe\', \'review\': \'Not bad.\', \'rating\': 3.0, \'categories\': [\'electronics\']}, # add more entries as needed ] structured_features = transform_structured_data(entries) text_features = transform_reviews(entries) final_features = combine_features(structured_features, text_features) print(final_features.shape) # should print the shape of the combined feature matrix. ``` Notes - You may use `sklearn.feature_extraction.DictVectorizer`, `sklearn.feature_extraction.text.CountVectorizer`, and `sklearn.feature_extraction.text.TfidfTransformer` classes from the `scikit-learn` library. - Ensure the functions handle edge cases and missing data gracefully. Good luck!","solution":"from typing import List, Dict, Any from sklearn.feature_extraction import DictVectorizer from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from scipy import sparse def transform_structured_data(entries: List[Dict[str, Any]]) -> sparse.csr_matrix: dict_vectorizer = DictVectorizer(sparse=True) transformed_entries = [] for entry in entries: transformed_entry = { \'username\': entry.get(\'username\', \'\'), \'rating\': entry.get(\'rating\', 0), } categories = entry.get(\'categories\', []) for category in categories: transformed_entry[f\'category_{category}\'] = 1 transformed_entries.append(transformed_entry) structured_matrix = dict_vectorizer.fit_transform(transformed_entries) return structured_matrix def transform_reviews(entries: List[Dict[str, Any]]) -> sparse.csr_matrix: count_vectorizer = CountVectorizer() reviews = [entry.get(\'review\', \'\') for entry in entries] term_frequency_matrix = count_vectorizer.fit_transform(reviews) tfidf_transformer = TfidfTransformer() tfidf_matrix = tfidf_transformer.fit_transform(term_frequency_matrix) return tfidf_matrix def combine_features(structured_features: sparse.csr_matrix, text_features: sparse.csr_matrix) -> sparse.csr_matrix: combined_matrix = sparse.hstack([structured_features, text_features]).tocsr() return combined_matrix"},{"question":"**Question**: A company has a large set of precompiled .pyc files that they need to process. These .pyc files contain Python objects that have been serialized using the `marshal` module. Your task is to implement functions that can read these files, extract the contained Python objects, and perform some operations on them to produce useful analytics. To demonstrate your comprehension of the `marshal` module and associated concepts, implement the following: 1. **Function**: `extract_data(file_path: str) -> object` - **Input**: a string `file_path`, representing the path to a binary file containing marshalled Python objects. - **Output**: the deserialized Python object. - **Details**: Read the binary file, deserialize the content using `marshal.load`, and return the resulting Python object. 2. **Function**: `serialize_data(data: object, file_path: str, version: int = marshal.version) -> None` - **Input**: - `data`: a supported Python object. - `file_path`: a string representing where to save the serialized data. - `version`: an optional integer specifying the marshalling version (default is `marshal.version`). - **Output**: None. - **Details**: Serialize the provided Python object using `marshal.dump` and save it to the specified file path. 3. **Function**: `analyze_data(data: object) -> dict` - **Input**: a Python object. - **Output**: a dictionary containing statistics about the object. - **Details**: Perform analysis on the Python object. For example, if the object is a list of numbers, calculate the mean and median. If it\'s a dictionary, provide statistics on the keys and values (e.g., count of keys, type distribution). The specifics depend on the type of the object. Here is an example of how your functions will be used: ```python # Example usage: obj = extract_data(\'example.pyc\') analysis = analyze_data(obj) serialize_data(analysis, \'analysis_result.pyc\') ``` **Constraints and Limitations**: - You can assume the input files exist and are accessible. - The Python objects are supported types by the marshal module. - Implement robust error handling where appropriate. - Follow Python best practices and optimize for readability and performance. **Performance Requirements**: - The functions should be able to handle large files and perform efficiently. - Ensure minimal overhead due to object serialization/deserialization. By implementing these functions, you demonstrate an understanding of binary serialization with the `marshal` module and the ability to manipulate and analyze serialized data.","solution":"import marshal import statistics from typing import Any, Dict def extract_data(file_path: str) -> Any: Reads a binary file containing marshalled Python objects, and returns the deserialized object. :param file_path: Path to the binary file :return: Deserialized Python object try: with open(file_path, \'rb\') as file: data = marshal.load(file) return data except Exception as e: raise RuntimeError(f\\"Failed to load data from {file_path}: {e}\\") def serialize_data(data: Any, file_path: str, version: int = marshal.version) -> None: Serializes a Python object and writes it to a binary file. :param data: Python object to serialize :param file_path: Path to save the serialized data :param version: Version of the marshal format try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except Exception as e: raise RuntimeError(f\\"Failed to save data to {file_path}: {e}\\") def analyze_data(data: Any) -> Dict[str, Any]: Analyzes a Python object and returns a dictionary of statistics. :param data: Python object to analyze :return: Dictionary containing statistics about the object analysis = {} if isinstance(data, list) and data and all(isinstance(item, (int, float)) for item in data): analysis[\'type\'] = \'list of numbers\' analysis[\'count\'] = len(data) analysis[\'mean\'] = statistics.mean(data) analysis[\'median\'] = statistics.median(data) elif isinstance(data, dict): analysis[\'type\'] = \'dictionary\' analysis[\'key_count\'] = len(data.keys()) analysis[\'value_count\'] = len(data.values()) analysis[\'keys_type_distribution\'] = {type(k).__name__: sum(1 for _k in data.keys() if isinstance(_k, type(k))) for k in data.keys()} analysis[\'values_type_distribution\'] = {type(v).__name__: sum(1 for _v in data.values() if isinstance(_v, type(v))) for v in data.values()} else: analysis[\'type\'] = \'unsupported\' analysis[\'description\'] = \'Analysis not supported for this type\' return analysis"},{"question":"**Objective**: Implement a function that uses the `SGDClassifier` to perform binary classification on a given dataset and evaluates the trained model using various metrics. Requirements 1. **Data Scaling**: Use `StandardScaler` to standardize the features. 2. **Model Training**: Use `SGDClassifier` with `log_loss` as the loss function and `l2` as the penalty. 3. **Evaluation**: Evaluate the model using `accuracy`, `precision`, `recall`, and `f1-score`. 4. **Function Signature**: ```python from sklearn.base import BaseEstimator from typing import Any, Dict, Tuple def train_and_evaluate_sgd_classifier(X_train: Any, y_train: Any, X_test: Any, y_test: Any) -> Dict[str, float]: Parameters: X_train: Training features, array-like of shape (n_samples, n_features) y_train: Training labels, array-like of shape (n_samples,) X_test: Test features, array-like of shape (n_samples, n_features) y_test: Test labels, array-like of shape (n_samples,) Returns: A dictionary with the following keys and their respective values: - \'accuracy\': Accuracy of the model on the test set. - \'precision\': Precision of the model on the test set. - \'recall\': Recall of the model on the test set. - \'f1_score\': F1 score of the model on the test set. # Your implementation here # Example Usage: # X_train, y_train, X_test, y_test = some_data_loading_function() # metrics = train_and_evaluate_sgd_classifier(X_train, y_train, X_test, y_test) # print(metrics) ``` Constraints - Assume that `X_train`, `y_train`, `X_test`, and `y_test` are already loaded and properly formatted. - You must use SGDClassifier with `loss=\'log_loss\'`, `penalty=\'l2\'`, and an appropriate number of iterations. - Standardize the features using `StandardScaler`. - Shuffle the training data before fitting the model or use `shuffle=True` in SGDClassifier. Performance Requirements - Model training should be efficient and well-optimized for input sizes up to 100,000 samples and 1,000 features. Evaluation Criteria - Correctness of the implementation. - Appropriate usage of `SGDClassifier` and feature scaling. - Quality and clarity of the code. - Proper calculation and return of evaluation metrics.","solution":"from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from typing import Any, Dict def train_and_evaluate_sgd_classifier(X_train: Any, y_train: Any, X_test: Any, y_test: Any) -> Dict[str, float]: Train an SGDClassifier on given training data and evaluate it on the test data. Parameters: X_train: Training features, array-like of shape (n_samples, n_features) y_train: Training labels, array-like of shape (n_samples,) X_test: Test features, array-like of shape (n_samples, n_features) y_test: Test labels, array-like of shape (n_samples,) Returns: A dictionary with the following keys and their respective values: - \'accuracy\': Accuracy of the model on the test set. - \'precision\': Precision of the model on the test set. - \'recall\': Recall of the model on the test set. - \'f1_score\': F1 score of the model on the test set. # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the SGDClassifier clf = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', shuffle=True, random_state=42) clf.fit(X_train_scaled, y_train) # Make predictions y_pred = clf.predict(X_test_scaled) # Calculate evaluation metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred) } return metrics"},{"question":"You are tasked with writing a Python function that takes a list of characters and returns a list of their properties and representations using the `curses.ascii` module. # Function Signature ```python def ascii_character_analysis(chars: list) -> list: pass ``` # Input - A list `chars` of characters (single-character strings), where each character can be an ASCII character. # Output - A list of dictionaries, one for each input character, with the properties: - `\\"character\\"`: The original character. - `\\"is_alnum\\"`: Boolean indicating if the character is alphanumeric. - `\\"is_alpha\\"`: Boolean indicating if the character is alphabetic. - `\\"is_ctrl\\"`: Boolean indicating if the character is a control character. - `\\"is_printable\\"`: Boolean indicating if the character is printable. - `\\"representation\\"`: String representation of the character using `curses.ascii.unctrl`. # Constraints - The input list can contain between 1 to 100 characters. - All characters will be ASCII characters. # Example ```python >>> ascii_character_analysis([\'A\', \'1\', \'n\', \'x1b\', \' \']) [ {\'character\': \'A\', \'is_alnum\': True, \'is_alpha\': True, \'is_ctrl\': False, \'is_printable\': True, \'representation\': \'A\'}, {\'character\': \'1\', \'is_alnum\': True, \'is_alpha\': False, \'is_ctrl\': False, \'is_printable\': True, \'representation\': \'1\'}, {\'character\': \'n\', \'is_alnum\': False, \'is_alpha\': False, \'is_ctrl\': True, \'is_printable\': False, \'representation\': \'^J\'}, {\'character\': \'x1b\', \'is_alnum\': False, \'is_alpha\': False, \'is_ctrl\': True, \'is_printable\': False, \'representation\': \'^[\'}, {\'character\': \' \', \'is_alnum\': False, \'is_alpha\': False, \'is_ctrl\': False, \'is_printable\': True, \'representation\': \' \'} ] ``` # Evaluation Criteria - Correctness: The function must accurately determine and return the properties and representations of each character. - Efficiency: The function should handle the input size constraint efficiently. - Edge Cases: Handle edge cases where characters might be control characters, whitespace characters, etc., accurately.","solution":"import curses.ascii def ascii_character_analysis(chars: list) -> list: result = [] for char in chars: char_info = { \\"character\\": char, \\"is_alnum\\": curses.ascii.isalnum(char), \\"is_alpha\\": curses.ascii.isalpha(char), \\"is_ctrl\\": curses.ascii.isctrl(char), \\"is_printable\\": curses.ascii.isprint(char), \\"representation\\": curses.ascii.unctrl(char) } result.append(char_info) return result"},{"question":"# Task Implement a function `calculate_class_results` that processes a list of student scores and returns the final results. # Details Given a list of students\' scores and other details, you need to calculate the status of each student based on their scores and criteria given below: # Function Signature ```python def calculate_class_results(students: list) -> list: ``` # Input - `students`: A list of dictionaries, where each dictionary represents a student and has the following keys: - `name`: String, the name of the student. - `scores`: List of integers, representing the student\'s scores in various subjects. # Output - Return a list of results, where each result is a dictionary with the following keys: - `name`: String, the name of the student. - `total_score`: Integer, the sum of the student\'s scores. - `average_score`: Float, the average of the student\'s scores. - `passed`: Boolean: - `True` if the average score is 60 or above. - `False` otherwise. # Constraints - Each student will have at least one score. - Scores will be between 0 and 100, inclusive. # Example ```python students = [ {\'name\': \'Alice\', \'scores\': [80, 90, 85]}, {\'name\': \'Bob\', \'scores\': [70, 75, 65, 60]}, {\'name\': \'Charlie\', \'scores\': [50, 40, 45]}, ] calculate_class_results(students) ``` # Expected Output ```python [ {\'name\': \'Alice\', \'total_score\': 255, \'average_score\': 85.0, \'passed\': True}, {\'name\': \'Bob\', \'total_score\': 270, \'average_score\': 67.5, \'passed\': True}, {\'name\': \'Charlie\', \'total_score\': 135, \'average_score\': 45.0, \'passed\': False} ] ``` # Implementation 1. Define the function `calculate_class_results`. 2. Loop through each student in the input list. 3. Calculate the total and average score. 4. Determine if the student passed or failed based on their average score. 5. Return a list of results containing each student\'s name, total score, average score, and pass status. # Notes - Make sure to include docstrings for the function. - Follow PEP 8 guidelines for coding style.","solution":"def calculate_class_results(students: list) -> list: Calculate the class results for a list of students. Args: students (list): A list of dictionaries where each dictionary contains: - name (str): The name of the student. - scores (list): A list of integers representing scores. Returns: list: A list of dictionaries where each dictionary contains: - name (str): The name of the student. - total_score (int): The sum of the student\'s scores. - average_score (float): The average of the student\'s scores. - passed (bool): True if average_score is 60 or above, False otherwise. results = [] for student in students: total_score = sum(student[\'scores\']) average_score = total_score / len(student[\'scores\']) passed = average_score >= 60 results.append({ \'name\': student[\'name\'], \'total_score\': total_score, \'average_score\': average_score, \'passed\': passed }) return results"},{"question":"Problem Statement You are tasked with implementing a function that processes a list of mixed data types to categorize and count the occurrences of specific Python built-in constants in the list. The function should return a dictionary containing the count of each constant found in the list. Constants to be counted: - `True` - `False` - `None` - `NotImplemented` - `Ellipsis` Function Signature ```python def count_constants(data: list) -> dict: This function takes a list of mixed data types and returns a dictionary with the counts of specific Python built-in constants. Args: data (list): A list containing elements of various data types. Returns: dict: A dictionary where the keys are the constant names (as strings) and the values are the counts of how many times each constant appeared in the input list. pass ``` Input - A list of mixed data types. The list can contain elements such as integers, strings, floats, booleans, `None`, `NotImplemented`, and `Ellipsis`. Output - A dictionary with the following structure: ```python { \\"True\\": count_of_True, \\"False\\": count_of_False, \\"None\\": count_of_None, \\"NotImplemented\\": count_of_NotImplemented, \\"Ellipsis\\": count_of_Ellipsis } ``` where `count_of_<constant>` is replaced with the actual count of each constant in the list. Constraints - The list can have a maximum length of 1000 elements. - The list can contain any type of elements. - You should not count representations of the constants (e.g., the string \'True\' should not be counted as the boolean `True`). Example ```python data = [True, False, None, Ellipsis, NotImplemented, True, \\"True\\", 0, 1, None] result = count_constants(data) # Expected Output: {\\"True\\": 2, \\"False\\": 1, \\"None\\": 2, \\"NotImplemented\\": 1, \\"Ellipsis\\": 1} ``` Notes - Ensure you carefully handle each constant as per its unique property\'s description in the provided documentation. - Your solution should be efficient and handle edge cases such as empty lists or lists without any of the specified constants.","solution":"def count_constants(data: list) -> dict: This function takes a list of mixed data types and returns a dictionary with the counts of specific Python built-in constants. Args: data (list): A list containing elements of various data types. Returns: dict: A dictionary where the keys are the constant names (as strings) and the values are the counts of how many times each constant appeared in the input list. constants = { \\"True\\": 0, \\"False\\": 0, \\"None\\": 0, \\"NotImplemented\\": 0, \\"Ellipsis\\": 0, } for item in data: if item is True: constants[\\"True\\"] += 1 elif item is False: constants[\\"False\\"] += 1 elif item is None: constants[\\"None\\"] += 1 elif item is NotImplemented: constants[\\"NotImplemented\\"] += 1 elif item is Ellipsis: constants[\\"Ellipsis\\"] += 1 return constants"},{"question":"Using the seaborn library and the `Plot` class from seaborn `objects`, you are required to create a multi-faceted bar plot from the `tips` dataset that: 1. Plots the number of tips given each day of the week. 2. Further breaks down the counts by the gender of the person paying the bill (`sex`). 3. Additionally, split the bars based on whether the bill amount was greater than 20 or not to show how different bill amounts are distributed across different days and payers\' genders. **Input:** - A dataset loaded using `seaborn.load_dataset(\\"tips\\")`. **Output:** - A seaborn plot displayed using matplotlib\'s `show()` method. **Constraints:** - You must use the `seaborn.objects.Plot` class to create the plot. - Colors for gender should be distinct. - Bars should be grouped clearly using `Dodge()` so that comparisons can be made easily. **Solution Requirements:** - Your solution code should be clean, well-commented with clear annotations explaining each step. - You should demonstrate the transformation to categorize the bill amounts and how to use seaborn objects to dodge/group the bars. - The plot should be well-labeled for clear interpretation. Here is how your function signature should look: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_tips_data(): tips = load_dataset(\\"tips\\") # Add your data transformation and plotting code here plt.show() ``` An example of what the plot would look like can be inferred as follows: - The x-axis represents days of the week. - The bars represent count of tips per day, further split by `sex`. - Each bar group is further split to show counts for bill amounts above and below 20. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt import pandas as pd def plot_tips_data(): tips = load_dataset(\\"tips\\") # Create a new column to categorize bill amounts tips[\'bill_category\'] = tips[\'total_bill\'].apply(lambda x: \'Greater than 20\' if x > 20 else \'Less than or equal to 20\') # Create the plot using seaborn objects interface p = (so.Plot(tips, x=\'day\', y=tips.groupby([\'day\', \'sex\', \'bill_category\']).size(), color=\'sex\') .facet(\'bill_category\') .add(so.Bars(), so.Dodge(), so.Stack())) # Enhance readability with proper labels and titles p.scale(color=\'categorical\', palette=\'Paired\') # Assign distinct colors for gender p.label(x=\'Day of Week\', y=\'Count of Tips\', color=\'Gender\', title=\'Distribution of Tips by Day and Gender with Bill Amount Category\') # Render the plot p.show()"},{"question":"# Advanced Seaborn Plotting **Objective:** Demonstrate your understanding of seaborn\'s object-oriented interface to create complex kernel density estimation (KDE) plots. Your task is to generate several KDE plots with specific customizations and transformations. **Task:** Using the `penguins` dataset provided by seaborn, complete the following steps: 1. Load the `penguins` dataset. 2. Create a KDE plot of the `flipper_length_mm` with a smoothing bandwidth adjustment of `0.5`. 3. Combine a histogram of `flipper_length_mm` with the density plot created. 4. Create a KDE plot for `body_mass_g` conditioned by `species` and with different colors for each species. Show both combined and separate (conditional) densities. 5. Create a faceted KDE plot of `flipper_length_mm` conditioned by `sex` and colored by `species`. Ensure that each facet shows density normalized within the facet. 6. Create a stacked KDE plot of `bill_length_mm` colored by `sex`. 7. Generate a cumulative density plot for `bill_depth_mm`. **Input:** - `penguins`: The seaborn `penguins` dataset. **Output:** - Several KDE plots according to the specifications above. Here is an outline of the expected solution structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task 1: Basic KDE plot with bandwidth adjustment p1 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(bw_adjust=0.5)) p1.show() # Task 2: Combine KDE with histogram p2 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(alpha=0.3), so.Hist()).add(so.Line(), so.KDE(bw_adjust=0.5)) p2.show() # Task 3: KDE conditioned by species p3_combined = so.Plot(penguins, x=\\"body_mass_g\\", color=\\"species\\").add(so.Area(), so.KDE()) p3_conditional = so.Plot(penguins, x=\\"body_mass_g\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)) p3_combined.show() p3_conditional.show() # Task 4: Faceted KDE plot by sex p4 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\").add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") p4.show() # Task 5: Stacked KDE plot by sex p5 = so.Plot(penguins, x=\\"bill_length_mm\\").add(so.Area(), so.KDE(), so.Stack(), color=\\"sex\\") p5.show() # Task 6: Cumulative density plot p6 = so.Plot(penguins, x=\\"bill_depth_mm\\").add(so.Line(), so.KDE(cumulative=True)) p6.show() ``` Ensure your plots are clearly labeled, and interpret any notable features of the resulting plots as part of your solution.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task 1: Basic KDE plot with bandwidth adjustment p1 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(bw_adjust=0.5)) p1.show() # Task 2: Combine KDE with histogram p2 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(alpha=0.3), so.Hist()).add(so.Line(), so.KDE(bw_adjust=0.5)) p2.show() # Task 3: KDE conditioned by species p3_combined = so.Plot(penguins, x=\\"body_mass_g\\", color=\\"species\\").add(so.Area(), so.KDE()) p3_conditional = so.Plot(penguins, x=\\"body_mass_g\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)) p3_combined.show() p3_conditional.show() # Task 4: Faceted KDE plot by sex p4 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\").add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") p4.show() # Task 5: Stacked KDE plot by sex p5 = so.Plot(penguins, x=\\"bill_length_mm\\").add(so.Area(), so.KDE(), so.Stack(), color=\\"sex\\") p5.show() # Task 6: Cumulative density plot p6 = so.Plot(penguins, x=\\"bill_depth_mm\\").add(so.Line(), so.KDE(cumulative=True)) p6.show()"},{"question":"# Advanced Coding Assessment: Nearest Neighbors and Anomaly Detection Objective: To assess your understanding of the `sklearn.neighbors` module, you will implement a custom anomaly detection model that identifies outliers in a dataset using the k-nearest neighbors approach. Your model should be capable of handling both dense and sparse datasets. Problem Statement: You are required to create a function `detect_anomaly` that identifies outlier points in a given dataset using the k-nearest neighbors method. The function will compute the average distance to the k-nearest neighbors for each point and flag points with an average distance greater than a specified threshold as anomalies. Function Signature: ```python def detect_anomaly(X, k, threshold, algorithm=\'auto\'): Detects anomalies in the dataset using k-nearest neighbors. Parameters: - X: array-like, shape (n_samples, n_features) The input data. Can be a dense (NumPy array) or sparse matrix (scipy.sparse). - k: int The number of nearest neighbors to use. - threshold: float The distance threshold above which a point is considered an anomaly. - algorithm: {\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'}, optional Algorithm to use for nearest neighbors search. Default is \'auto\'. Returns: - anomalies: list of int Indices of the points that are considered anomalies. pass ``` Input: - `X`: A 2D array-like structure with shape (n_samples, n_features) representing the dataset. - `k`: An integer representing the number of nearest neighbors to use. - `threshold`: A float representing the threshold distance above which a point is considered an anomaly. - `algorithm`: A string representing the nearest neighbors search algorithm to use. Options are \'auto\', \'ball_tree\', \'kd_tree\', \'brute\'. Default is \'auto\'. Output: - A list of integers representing the indices of the points that are considered anomalies. Constraints: - Each point in the dataset must have at least `k` neighbors. - The function must efficiently handle both small and large datasets. Example: ```python import numpy as np from scipy.sparse import csr_matrix # Example dataset (dense) X_dense = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [8, 8], [25, 80]]) # Example dataset (sparse) X_sparse = csr_matrix([[1, 2], [2, 3], [3, 4], [8, 7], [8, 8], [25, 80]]) # Parameters k = 2 threshold = 15.0 print(detect_anomaly(X_dense, k, threshold)) # Output: [5] print(detect_anomaly(X_sparse, k, threshold)) # Output: [5] ``` Guidelines: 1. Use the `NearestNeighbors` class from `sklearn.neighbors` to find k-nearest neighbors. 2. Calculate the average distance to the k-nearest neighbors for each point. 3. Return the indices of points where the average distance exceeds the given threshold. 4. Your implementation should handle both dense (NumPy arrays) and sparse (scipy.sparse matrices) datasets. Hints: - Utilize the `kneighbors` method of the `NearestNeighbors` class to find the distances to the nearest neighbors. - To support sparse matrices, you may need to conditionally handle the input data format using the appropriate distance metrics. Test Cases: Ensure your code passes the following test cases: 1. A small, dense dataset with obvious outlier(s). 2. A small, sparse dataset with similar outlier(s). 3. A large dataset to test the efficiency of your implementation. You are encouraged to test your solution thoroughly with additional edge cases to ensure robustness.","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors def detect_anomaly(X, k, threshold, algorithm=\'auto\'): Detects anomalies in the dataset using k-nearest neighbors. Parameters: - X: array-like, shape (n_samples, n_features) The input data. Can be a dense (NumPy array) or sparse matrix (scipy.sparse). - k: int The number of nearest neighbors to use. - threshold: float The distance threshold above which a point is considered an anomaly. - algorithm: {\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'}, optional Algorithm to use for nearest neighbors search. Default is \'auto\'. Returns: - anomalies: list of int Indices of the points that are considered anomalies. nbrs = NearestNeighbors(n_neighbors=k+1, algorithm=algorithm).fit(X) distances, indices = nbrs.kneighbors(X) mean_distances = np.mean(distances[:, 1:], axis=1) anomalies = [index for index, distance in enumerate(mean_distances) if distance > threshold] return anomalies"},{"question":"**Question:** You are tasked with creating a Python script that processes multiple text files and compresses them into a ZIP archive. Additionally, the script should be able to extract specific files from a provided ZIP archive and handle errors appropriately. Follow the detailed instructions below to implement the necessary functionalities: # Part 1: Creating a ZIP Archive Implement a function `create_zip_archive(output_zip, input_files)` that takes the following parameters: - `output_zip` (str): The name of the output ZIP file. - `input_files` (list): A list of paths to the input text files to be compressed into the ZIP archive. The function should: 1. Create a new ZIP file with the name `output_zip`. 2. Add each file in the `input_files` list to the ZIP file using the `ZIP_DEFLATED` compression method. 3. Handle the case where an input file does not exist by skipping it and printing a warning message. # Part 2: Extracting Files from a ZIP Archive Implement a function `extract_files_from_zip(input_zip, output_dir, files_to_extract)` that takes the following parameters: - `input_zip` (str): The name of the input ZIP archive. - `output_dir` (str): The directory where the extracted files should be saved. - `files_to_extract` (list): A list of filenames to be extracted from the ZIP archive. The function should: 1. Open the ZIP file `input_zip` for reading. 2. Extract only the files specified in `files_to_extract` to the `output_dir`. 3. Handle the case where a specified file does not exist in the archive by printing a warning message. 4. Close the ZIP file. # Part 3: Putting It All Together Finally, create a main script that demonstrates the usage of the above functions: 1. Call `create_zip_archive` with an example list of text files. 2. Call `extract_files_from_zip` to extract a subset of files from the created ZIP archive. Ensure your code includes appropriate error handling and is clearly commented. # Example Usage ```python if __name__ == \\"__main__\\": output_zip = \\"example.zip\\" input_files = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] create_zip_archive(output_zip, input_files) input_zip = \\"example.zip\\" output_dir = \\"extracted_files\\" files_to_extract = [\\"file1.txt\\", \\"file3.txt\\"] extract_files_from_zip(input_zip, output_dir, files_to_extract) ``` # Constraints - You can assume all paths provided are valid and in the correct format. - The `output_zip` should be overwritten if it already exists. - Use `os` and `zipfile` modules to assist with file and ZIP operations. # Submission Submit a single Python file with the function implementations and the main script. Ensure your code follows PEP 8 guidelines for style and formatting.","solution":"import os import zipfile def create_zip_archive(output_zip, input_files): Creates a ZIP archive from a list of input text files. Args: output_zip (str): The name of the output ZIP file. input_files (list): A list of paths to the input text files to be compressed into the ZIP archive. with zipfile.ZipFile(output_zip, \'w\', zipfile.ZIP_DEFLATED) as zipf: for file in input_files: if os.path.exists(file): zipf.write(file, os.path.basename(file)) else: print(f\\"Warning: {file} does not exist and will be skipped.\\") def extract_files_from_zip(input_zip, output_dir, files_to_extract): Extracts specific files from a ZIP archive. Args: input_zip (str): The name of the input ZIP archive. output_dir (str): The directory where the extracted files should be saved. files_to_extract (list): A list of filenames to be extracted from the ZIP archive. if not os.path.isdir(output_dir): os.makedirs(output_dir) with zipfile.ZipFile(input_zip, \'r\') as zipf: for file in files_to_extract: try: zipf.extract(file, output_dir) except KeyError: print(f\\"Warning: {file} does not exist in the archive and will be skipped.\\") if __name__ == \\"__main__\\": # Example usage output_zip = \\"example.zip\\" input_files = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] create_zip_archive(output_zip, input_files) input_zip = \\"example.zip\\" output_dir = \\"extracted_files\\" files_to_extract = [\\"file1.txt\\", \\"file3.txt\\"] extract_files_from_zip(input_zip, output_dir, files_to_extract)"},{"question":"<|Analysis Begin|> The documentation provided gives a detailed overview of the `telnetlib` module, which is used to interact with Telnet servers. It includes the `Telnet` class and its various methods for opening a connection, reading data, writing data, interacting with the server, and managing the connection. Notably, the `telnetlib` module is deprecated as of Python 3.11, but it is still functional in Python 3.10, which is the target for our question. The `Telnet` class methods provide various ways to handle data transfer, such as `read_until`, `read_all`, `read_some`, `write`, and `interact`. Additionally, there are methods to manage the connection, such as `open`, `close`, and `get_socket`. Given the detailed methods and their functionalities, a challenging question would be to combine creating a Telnet session, authenticating the user, executing multiple commands, and handling the connection\'s lifecycle, which would require a solid grasp on both the usage of the `telnetlib` package and Python programming concepts. <|Analysis End|> <|Question Begin|> # Telnet Client Implementation with Command Execution You are tasked with creating a Python function that connects to a Telnet server, logs in using provided credentials, executes a series of commands, and retrieves the output of these commands. The function should properly manage the Telnet connection and handle potential errors that might occur during communication with the server. Function Signature ```python def execute_telnet_commands(host: str, username: str, password: str, commands: list) -> str: pass ``` Parameters - `host` (str): The hostname or IP address of the Telnet server. - `username` (str): The username needed to log in to the server. - `password` (str): The password needed to log in to the server. - `commands` (list): A list of strings, where each string is a command to be executed on the Telnet server. Returns - `str`: The combined output from all executed commands in the form of a decoded string. Constraints 1. Do not use deprecated or discouraged methods, as specified in the documentation. 2. The function should handle connection errors gracefully and ensure the connection is closed properly, even in case of an error. 3. The function must ensure each command is executed in sequence and the responses are collected. Example Usage ```python host = \\"localhost\\" username = \\"user\\" password = \\"pass\\" commands = [\\"ls\\", \\"echo Hello World\\", \\"whoami\\"] output = execute_telnet_commands(host, username, password, commands) print(output) ``` Expected Output The output will be a string containing the results of the commands executed on the server, concatenated together. For example: ``` file1.txt file2.txt Hello World user ``` Additional Details - Use the `telnetlib.Telnet` class to handle the Telnet connection. - Utilize the `read_until`, `write`, and `read_all` methods as necessary to handle reading from and writing to the server. - Include necessary error handling for connection failures, authentication errors, and command execution issues. Your task is to implement the `execute_telnet_commands` function fulfilling the requirements and constraints listed above.","solution":"import telnetlib def execute_telnet_commands(host: str, username: str, password: str, commands: list) -> str: try: # Connect to the Telnet server tn = telnetlib.Telnet(host) # Read until the login prompt and send the username tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") # Read until the password prompt and send the password tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") # Collect the output output = b\\"\\" for command in commands: # Write each command followed by a newline tn.write(command.encode(\'ascii\') + b\\"n\\") # Wait for the command to execute and read the resulting output # We can use read_some to read some data or read_until specific prompt output += tn.read_until(b\\" \\") # Close the connection tn.close() return output.decode(\'ascii\') except Exception as e: return str(e)"},{"question":"**Objective**: Design a class using the `dataclasses` module which tracks items in a store inventory, including some additional features related to handling discounts and calculating total inventory value. Problem Statement You are required to create a `dataclass` called `StoreItem` which will be used to manage inventory items in a store. Additionally, create a second `dataclass` called `StoreInventory` which manages a collection of `StoreItem` instances. 1. **StoreItem** - Fields: - `item_id`: `int` - a unique identifier for the item. - `name`: `str` - the name of the item. - `unit_price`: `float` - the price per unit of the item. - `quantity_on_hand`: `int` - the number of units available in the store. This field should default to `0`. - `discount`: `float` - a discount percentage applicable to the item, ranging from `0.0` to `1.0`. This field should default to `0.0`. - Methods: - `total_cost` - Calculate the total cost of the item in stock considering the discount. This method should return a float value. Example usage: ```python item = StoreItem(item_id=1, name=\'Widget\', unit_price=10.0, quantity_on_hand=5, discount=0.1) total_cost = item.total_cost() # Should return 45.0 ``` 2. **StoreInventory** - Fields: - `items`: `List[StoreItem]` - a list of `StoreItem` instances. This field should use a `default_factory` to initialize an empty list. - Methods: - `add_item` - Accepts a `StoreItem` instance and adds it to the inventory. - `total_inventory_value` - Calculates the total value of all items in the inventory, accounting for their quantities and discounts. This method should return a float value. Example usage: ```python inventory = StoreInventory() item1 = StoreItem(item_id=1, name=\'Widget\', unit_price=10.0, quantity_on_hand=5, discount=0.1) item2 = StoreItem(item_id=2, name=\'Gadget\', unit_price=15.0, quantity_on_hand=3) inventory.add_item(item1) inventory.add_item(item2) total_value = inventory.total_inventory_value() # Should return 89.0 ``` To test their understanding, students are expected to implement the two `dataclass`es with appropriate methods and default values, ensuring that all calculations are correctly implemented. Proper usage of `dataclasses.field` and handling of default values or factories should also be demonstrated. **Constraints**: - The `unit_price` and `discount` should always be non-negative. - Ensure methods perform type checks where applicable. **Evaluation Criteria**: - Correct use of `dataclass` decorators and field definitions. - Proper implementation of the required methods. - Handling of default values and usage of `default_factory`. - Correctness of calculations and type handling within methods.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class StoreItem: item_id: int name: str unit_price: float quantity_on_hand: int = 0 discount: float = 0.0 def __post_init__(self): if self.unit_price < 0: raise ValueError(\\"unit_price must be non-negative\\") if not (0.0 <= self.discount <= 1.0): raise ValueError(\\"discount must be between 0.0 and 1.0\\") def total_cost(self) -> float: discount_price = self.unit_price * (1 - self.discount) return discount_price * self.quantity_on_hand @dataclass class StoreInventory: items: List[StoreItem] = field(default_factory=list) def add_item(self, item: StoreItem): self.items.append(item) def total_inventory_value(self) -> float: return sum(item.total_cost() for item in self.items)"},{"question":"Using the pandas library, you are required to implement a function that processes and analyzes a dataset containing information about various products. **Function Specification**: ```python def process_product_data(df: pd.DataFrame) -> pd.DataFrame: Processes the product data DataFrame and returns a summary DataFrame. Args: df (pd.DataFrame): Input DataFrame containing product information. Returns: pd.DataFrame: Output DataFrame containing processed data: - Includes columns: \'category\', \'total_sales\', \'average_price\', \'num_products\' - total_sales: Sum of the \'sales\' column for each category - average_price: Mean of the \'price\' column for each category - num_products: Count of products in each category pass ``` **Details and Requirements**: - The input DataFrame `df` will contain the following columns: - `product_id`: Unique identifier for each product (int). - `category`: Category each product belongs to (str). - `price`: Price of the product (float). - `sales`: Sales of the product (int). - Your function should: - Group the data by `category`. - Compute the following for each category: - `total_sales`: The sum of the `sales` column. - `average_price`: The mean of the `price` column. - `num_products`: The count of distinct products in the category. - Return a DataFrame with the columns: `category`, `total_sales`, `average_price`, `num_products`. - Ensure the column names and order in the output DataFrame are as specified. **Example**: ```python import pandas as pd data = { \'product_id\': [1, 2, 3, 4, 5], \'category\': [\'Electronics\', \'Electronics\', \'Furniture\', \'Furniture\', \'Toys\'], \'price\': [299.99, 199.99, 399.99, 299.99, 19.99], \'sales\': [150, 120, 80, 60, 200] } df = pd.DataFrame(data) result = process_product_data(df) print(result) ``` **Expected Output**: ``` category total_sales average_price num_products 0 Electronics 270 249.99 2 1 Furniture 140 349.99 2 2 Toys 200 19.99 1 ``` **Constraints**: - Assume the input DataFrame `df` always has valid data (no missing values). **Evaluation Criteria**: - Correct implementation of data grouping and aggregation. - Proper handling of column names and order in the output DataFrame. - Efficiency and clarity of the code.","solution":"import pandas as pd def process_product_data(df: pd.DataFrame) -> pd.DataFrame: Processes the product data DataFrame and returns a summary DataFrame. Args: df (pd.DataFrame): Input DataFrame containing product information. Returns: pd.DataFrame: Output DataFrame containing processed data: - Includes columns: \'category\', \'total_sales\', \'average_price\', \'num_products\' - total_sales: Sum of the \'sales\' column for each category - average_price: Mean of the \'price\' column for each category - num_products: Count of products in each category summary_df = df.groupby(\'category\').agg( total_sales=(\'sales\', \'sum\'), average_price=(\'price\', \'mean\'), num_products=(\'product_id\', \'count\') ).reset_index() return summary_df"},{"question":"# Question You have been hired by a scientific research team to develop a Python program that assists with various mathematical calculations. Your program must incorporate several functionalities offered by the Python `math` module to compute certain values as required by the team. Implement a function `scientific_calculations` that takes a list of tuples as input. Each tuple contains a string specifying the type of calculation and the corresponding parameters required for that calculation. The function should interpret the string type and call the corresponding `math` function, returning a list of results for each calculation requested. Function Signature ```python def scientific_calculations(calculations: list[tuple[str, ...]]) -> list: ``` Input - `calculations`: A list of tuples. Each tuple will contain: - A string specifying the type of calculation. The string must be one of the following: `\\"ceil\\"`, `\\"floor\\"`, `\\"comb\\"`, `\\"factorial\\"`, `\\"gcd\\"`, `\\"isqrt\\"`, `\\"lcm\\"`, `\\"log\\"`, `\\"pow\\"`, or `\\"sqrt\\"`. - The rest of the elements in the tuple will be the parameters required for the specified mathematical operation. Output - The function should return a list of results for each calculation in the same order as they appear in the input list. Each result can be an integer or a float, depending on the operation. Constraints - You can assume that the input will always be in the correct format. - All input values will be within reasonable bounds for their respective operations. - Handle any potential exceptions gracefully and provide meaningful error messages if a calculation cannot be performed due to improper parameters. # Examples ```python print(scientific_calculations([ (\\"ceil\\", 3.14), (\\"floor\\", 3.99), (\\"comb\\", 5, 3), (\\"factorial\\", 5), (\\"gcd\\", 48, 180), (\\"isqrt\\", 16), (\\"lcm\\", 3, 4), (\\"log\\", 1024, 2), (\\"pow\\", 2, 10), (\\"sqrt\\", 25) ])) # Output: # [4, 3, 10, 120, 12, 4, 12, 10.0, 1024.0, 5.0] ``` Additional Notes - Your implementation should use the `math` module functions directly to perform calculations. - Ensure that the code is modular and well-documented, following good coding practices.","solution":"import math def scientific_calculations(calculations): Accepts a list of tuples where each tuple contains a string specifying the type of calculation and the corresponding parameters required for that calculation. Returns a list of results for each calculation requested. results = [] for calculation in calculations: try: operation = calculation[0] params = calculation[1:] if operation == \\"ceil\\": results.append(math.ceil(*params)) elif operation == \\"floor\\": results.append(math.floor(*params)) elif operation == \\"comb\\": results.append(math.comb(*params)) elif operation == \\"factorial\\": results.append(math.factorial(*params)) elif operation == \\"gcd\\": results.append(math.gcd(*params)) elif operation == \\"isqrt\\": results.append(math.isqrt(*params)) elif operation == \\"lcm\\": results.append(math.lcm(*params)) elif operation == \\"log\\": results.append(math.log(*params)) elif operation == \\"pow\\": results.append(math.pow(*params)) elif operation == \\"sqrt\\": results.append(math.sqrt(*params)) else: results.append(f\\"Unsupported operation: {operation}\\") except Exception as e: results.append(f\\"Error: {str(e)}\\") return results"},{"question":"In this coding assessment, you are required to demonstrate your understanding of cryptographic hashing by implementing a function that verifies the integrity of data using the `hashlib` module in Python. # Task Implement a function `verify_file_integrity(file_path: str, expected_checksum: str, hash_type: str) -> bool` that checks if the hash of a given file matches a specified checksum. Function Signature ```python def verify_file_integrity(file_path: str, expected_checksum: str, hash_type: str) -> bool: pass ``` # Input 1. `file_path` (str): The path to the file whose integrity needs to be verified. 2. `expected_checksum` (str): The expected hash value of the file. 3. `hash_type` (str): The type of hash algorithm to use (e.g., \'md5\', \'sha256\', \'blake2b\'). # Output - Returns a boolean value: - `True` if the hash calculated from the file matches the `expected_checksum`. - `False` otherwise. # Constraints - The file sizes are limited to a maximum of 1 GB. - Valid `hash_type` values include \'md5\', \'sha1\', \'sha256\', \'sha512\', \'blake2b\', etc. # Performance Requirements - The function should handle reading large files efficiently without loading the entire file into memory at once. # Example ```python file_path = \\"example.txt\\" expected_checksum = \\"5d41402abc4b2a76b9719d911017c592\\" hash_type = \\"md5\\" # Assuming the content of example.txt is \\"hello\\" print(verify_file_integrity(file_path, expected_checksum, hash_type)) # Output: True ``` # Additional Notes - You may assume that all input files exist at the specified path and are readable. - Use the `hashlib` module to perform hash calculations. - Make sure to handle common issues such as reading the file in chunks to accommodate large files. # Hints - Utilize the `hashlib` functions corresponding to the hash type provided. - Use file handling techniques to read the file in chunks to ensure efficient memory usage.","solution":"import hashlib def verify_file_integrity(file_path: str, expected_checksum: str, hash_type: str) -> bool: Verifies if the hash of the given file matches the expected checksum. Args: file_path (str): The path to the file to be checked. expected_checksum (str): The expected hash value. hash_type (str): The type of hash (\'md5\', \'sha1\', \'sha256\', etc.). Returns: bool: True if the file\'s hash matches the expected checksum, False otherwise. try: # Initialize the hash object hash_obj = hashlib.new(hash_type) except ValueError: # If the hash type is not valid, return False return False # Read file in chunks to handle large files efficiently try: with open(file_path, \'rb\') as f: while chunk := f.read(4096): # Read file in 4KB chunks hash_obj.update(chunk) except FileNotFoundError: # If the file does not exist, return False return False # Compare the calculated hash with the expected checksum return hash_obj.hexdigest() == expected_checksum"},{"question":"# Advanced Python Slice Manipulation Objective: Create a series of functions to handle slice objects in Python using the provided C-API functions. This will evaluate your ability to understand and implement Python slicing behavior and handle boundary conditions effectively. Functions to Implement: 1. **verify_slice**: - **Input**: - `obj` (Any): An object to be checked. - **Output**: - `bool`: Returns `True` if the object is a slice; otherwise, `False`. - **Constraints**: - The function should utilize `PySlice_Check`. 2. **create_slice**: - **Input**: - `start`, `stop`, `step` (Optional[int]): Optional start, stop, and step indices for the slice. - **Output**: - `slice`: Returns a new slice object with the provided indices. - **Constraints**: - The function should use `PySlice_New`. 3. **retrieve_indices**: - **Input**: - `slice_obj` (slice): A slice object. - `seq_len` (int): The length of the sequence this slice will be applied to. - **Output**: - `Tuple`: A tuple of the form `(start, stop, step, length)` where `start`, `stop`, `step` are the attributes of the slice and `length` is the calculated effective length of the slice for the sequence. - **Constraints**: - The function should use `PySlice_GetIndicesEx`. 4. **adjusted_slice**: - **Input**: - `length` (int): Length of a sequence. - `start`, `stop`, `step` (int): Start, stop, and step attributes of a slice. - **Output**: - `Tuple[int, int, int, int]`: A tuple `(start, stop, step, adjusted_length)` where `start`, `stop`, and `step` are potentially adjusted to fit within bounds and `adjusted_length` is the new length of the slice using `PySlice_AdjustIndices`. - **Constraints**: - The function should handle out-of-bounds index adjustment. # Example Usage: ```python s = create_slice(1, 10, 2) print(verify_slice(s)) # Output: True indices = retrieve_indices(s, 15) print(indices) # Output: (1, 10, 2, 5) adj = adjusted_slice(10, -15, 20, 2) print(adj) # Output: (0, 10, 2, 5) ``` # Note: Although the functions are designed to align with certain C-API functionalities, use standard Python slice operations (`slice`, `isinstance`, etc.) for the actual implementations. # Performance Requirements: - Ensure no deprecated methods are directly used. - Handle edge cases like negative indices or steps gracefully. - Make these functions as close to real, efficient implementations in pure Python as possible.","solution":"def verify_slice(obj): Returns True if the object is a slice, otherwise False. return isinstance(obj, slice) def create_slice(start=None, stop=None, step=None): Creates a new slice object with the provided start, stop, and step indices. return slice(start, stop, step) def retrieve_indices(slice_obj, seq_len): Retrieves the start, stop, step, and length of the slice for a given sequence length. start, stop, step = slice_obj.indices(seq_len) length = max(0, (stop - start + (step - (1 if step > 0 else -1))) // step) return start, stop, step, length def adjusted_slice(length, start, stop, step): Adjusts the slice start, stop, and step to fit within bounds and calculates the effective length. slice_obj = slice(start, stop, step) start, stop, step = slice_obj.indices(length) adjusted_length = max(0, (stop - start + (step - (1 if step > 0 else -1))) // step) return start, stop, step, adjusted_length"},{"question":"**Custom Scikit-learn Estimator Implementation** Your task is to create a custom transformer and classifier estimator class that integrates smoothly with scikit-learn pipelines. This will test your understanding of the scikit-learn API and your ability to develop compliant custom estimators. # Question **Problem Statement:** Implement a class `CustomTransformer` that inherits from `BaseEstimator` and `TransformerMixin`, and a class `CustomClassifier` that inherits from `BaseEstimator` and `ClassifierMixin`. 1. **CustomTransformer**: - This transformer should normalize the input data by subtracting the mean and dividing by the standard deviation. - Implement the following methods: - `__init__(self)`: Initialize without any parameters. - `fit(self, X, y=None)`: Calculate and store the mean and standard deviation of the input data. - `transform(self, X)`: Apply the normalization using the stored mean and standard deviation. 2. **CustomClassifier**: - This classifier should predict the class of an input data point based on the nearest neighbor\'s class from the training data. - Implement the following methods: - `__init__(self)`: Initialize without any parameters. - `fit(self, X, y)`: Store the training data and target values. - `predict(self, X)`: Predict the target for each input data point based on the nearest neighbor from the stored training data. # Implementation Details: - **Input Format:** - The input data `X` for `fit` and `transform` methods will be a 2D NumPy array of shape `(n_samples, n_features)`. - The target data `y` for `fit` method of `CustomClassifier` will be a 1D NumPy array of shape `(n_samples,)`. - **Constraints:** - You should use only NumPy for array manipulations. - Ensure that the `CustomTransformer` and `CustomClassifier` are compatible with scikit-learn pipelines. # Example Usage: ```python import numpy as np from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array from sklearn.metrics import euclidean_distances class CustomTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = check_array(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X): check_is_fitted(self, [\'mean_\', \'std_\']) X = check_array(X) return (X - self.mean_) / self.std_ class CustomClassifier(BaseEstimator, ClassifierMixin): def __init__(self): pass def fit(self, X, y): X, y = check_array(X), check_array(y, ensure_2d=False) self.X_ = X self.y_ = y self.classes_, y_indices = np.unique(y, return_inverse=True) return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] # Example data X_train = np.array([[1, 2], [3, 4], [5, 6]]) y_train = np.array([0, 1, 0]) X_test = np.array([[1, 2], [5, 5]]) # Create pipeline pipeline = Pipeline([ (\'transform\', CustomTransformer()), (\'classify\', CustomClassifier()) ]) # Fit pipeline pipeline.fit(X_train, y_train) # Predict using the pipeline preds = pipeline.predict(X_test) print(preds) ``` **Expected Output:** The pipeline should output the predicted class labels for the input `X_test` data points. Your implementation will be evaluated based on correctness, adherence to scikit-learn API conventions, and compatibility with scikit-learn\'s pipeline and model selection tools.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array from sklearn.metrics import euclidean_distances class CustomTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = check_array(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X): check_is_fitted(self, [\'mean_\', \'std_\']) X = check_array(X) return (X - self.mean_) / self.std_ class CustomClassifier(BaseEstimator, ClassifierMixin): def __init__(self): pass def fit(self, X, y): X, y = check_array(X), check_array(y, ensure_2d=False) self.X_ = X self.y_ = y self.classes_, y_indices = np.unique(y, return_inverse=True) return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest]"},{"question":"# XML DOM Manipulation Assessment You are required to write a Python function that constructs and manipulates an XML document using the `xml.dom` module. Ensure your solution adheres to the DOM Level 2 specifications. Task: 1. **Create an XML Document**: - The root element should be `<library>`. - The library should contain a child element `<book>` with the following attributes and children: - Attributes: `id`, `title`, `author` - Child: `<genre>` 2. **Manipulate the XML Document**: - Add a second `<book>` element with different attribute values and genre to the `<library>`. - Remove the `author` attribute from the first `<book>` element. - Modify the `title` attribute of the second `<book>` element. - Retrieve and return the text content of the `<genre>` element of the first `<book>` element. Input: - There are no direct inputs to this function. Output: - A string representing the text content of the `<genre>` element from the first `<book>`. Constraints: - You must use methods provided by the `xml.dom` module to manipulate the XML document. - Follow proper DOM exception handling practices. Function Signature: ```python def manipulate_xml_document() -> str: pass ``` Example: ```python result = manipulate_xml_document() print(result) ``` Expected output would be the genre of the first book, for instance: ``` \\"Science Fiction\\" ``` **Notes**: - Use `xml.dom.minidom` for creating and manipulating XML. - Focus on creating elements, modifying attributes, and handling nodes according to the DOM API described in the documentation.","solution":"from xml.dom.minidom import Document def manipulate_xml_document() -> str: # Create a new XML document doc = Document() # Create the root element <library> library = doc.createElement(\\"library\\") doc.appendChild(library) # Create the first <book> element book1 = doc.createElement(\\"book\\") book1.setAttribute(\\"id\\", \\"1\\") book1.setAttribute(\\"title\\", \\"Dune\\") book1.setAttribute(\\"author\\", \\"Frank Herbert\\") genre1 = doc.createElement(\\"genre\\") genre1.appendChild(doc.createTextNode(\\"Science Fiction\\")) book1.appendChild(genre1) library.appendChild(book1) # Create the second <book> element book2 = doc.createElement(\\"book\\") book2.setAttribute(\\"id\\", \\"2\\") book2.setAttribute(\\"title\\", \\"1984\\") book2.setAttribute(\\"author\\", \\"George Orwell\\") genre2 = doc.createElement(\\"genre\\") genre2.appendChild(doc.createTextNode(\\"Dystopian\\")) book2.appendChild(genre2) library.appendChild(book2) # Remove the author attribute from the first <book> element book1.removeAttribute(\\"author\\") # Modify the title attribute of the second <book> element book2.setAttribute(\\"title\\", \\"Brave New World\\") # Retrieve and return the text content of the <genre> element of the first <book> element genre_text = book1.getElementsByTagName(\\"genre\\")[0].firstChild.nodeValue return genre_text"},{"question":"# Coding Assessment: Implementing a History-Based Calculator with `argparse` **Objective:** You are required to create a command-line utility that functions as a history-based calculator using the `argparse` module. The utility will support basic arithmetic operations and maintain a history of calculations that users can query later. **Requirements:** 1. **Basic Arithmetic Operations**: - The utility should support addition, subtraction, multiplication, and division. - These operations should be specified using sub-commands. 2. **History Management**: - The utility should store the results of each operation in a history file. - Users should be able to query the history to see all past calculations. 3. **Custom Actions and Handling**: - Implement custom argument types and actions where necessary to enforce correct input and output formats. - Properly handle errors and invalid inputs. # Detailed Specifications: 1. **Argument Parser Setup**: - The utility should use `argparse.ArgumentParser` and at least one sub-parser to handle different operations. - Sub-commands should include `add`, `sub`, `mul`, `div`, and `history`. 2. **Sub-Commands**: - Each arithmetic sub-command (`add`, `sub`, `mul`, `div`) should accept two positional arguments of type `float`: - Example: `calc.py add 1.1 2.2` should output `Result: 3.3`. - The `history` sub-command should provide an option to limit the number of history entries displayed: - Example: `calc.py history --limit 5`. 3. **History Management**: - Results of each operation should be stored in a file named `history.txt`. - Each entry in the file should include the operation, operands, and result. 4. **Error Handling**: - Implement robust error handling for invalid operations (e.g., division by zero) and argument parsing. - Utilize `argparse.ArgumentParser` features to display meaningful error messages. 5. **Custom Argument Types**: - Create custom argument types or actions if necessary for validating inputs and reading/writing the history. # Implementation Constraints: - The script should be executable from the command line. - It should be modular, leveraging functions to handle individual tasks (e.g., each arithmetic operation should be a separate function). # Example Usage: ```sh # Performing arithmetic operations python calc.py add 1.5 2.5 Result: 4.0 python calc.py div 4 2 Result: 2.0 # Handling division by zero python calc.py div 4 0 Error: Division by zero is not allowed. # Querying history python calc.py history --limit 2 1: 1.5 + 2.5 = 4.0 2: 4 / 2 = 2.0 ``` # Code Skeleton: ```python import argparse import sys def add(args): result = args.a + args.b print(f\\"Result: {result}\\") # Write to history file with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} + {args.b} = {result}n\\") def sub(args): result = args.a - args.b print(f\\"Result: {result}\\") # Write to history file with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} - {args.b} = {result}n\\") def mul(args): result = args.a * args.b print(f\\"Result: {result}\\") # Write to history file with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} * {args.b} = {result}n\\") def div(args): if args.b == 0: print(\\"Error: Division by zero is not allowed.\\") return result = args.a / args.b print(f\\"Result: {result}\\") # Write to history file with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} / {args.b} = {result}n\\") def history(args): try: with open(\'history.txt\', \'r\') as f: lines = f.readlines() limit = args.limit if args.limit else len(lines) for i, line in enumerate(lines[-limit:], 1): print(f\\"{i}: {line.strip()}\\") except FileNotFoundError: print(\\"No history available.\\") # Define main function to set up ArgumentParser and subparsers def main(): parser = argparse.ArgumentParser(description=\\"A history-based calculator.\\") subparsers = parser.add_subparsers() # Add sub-command parser_add = subparsers.add_parser(\'add\', help=\'Perform addition\') parser_add.add_argument(\'a\', type=float, help=\'First operand\') parser_add.add_argument(\'b\', type=float, help=\'Second operand\') parser_add.set_defaults(func=add) # Sub sub-command parser_sub = subparsers.add_parser(\'sub\', help=\'Perform subtraction\') parser_sub.add_argument(\'a\', type=float, help=\'First operand\') parser_sub.add_argument(\'b\', type=float, help=\'Second operand\') parser_sub.set_defaults(func=sub) # Mul sub-command parser_mul = subparsers.add_parser(\'mul\', help=\'Perform multiplication\') parser_mul.add_argument(\'a\', type=float, help=\'First operand\') parser_mul.add_argument(\'b\', type=float, help=\'Second operand\') parser_mul.set_defaults(func=mul) # Div sub-command parser_div = subparsers.add_parser(\'div\', help=\'Perform division\') parser_div.add_argument(\'a\', type=float, help=\'First operand\') parser_div.add_argument(\'b\', type=float, help=\'Second operand\') parser_div.set_defaults(func=div) # History sub-command parser_history = subparsers.add_parser(\'history\', help=\'Show calculation history\') parser_history.add_argument(\'--limit\', type=int, help=\'Limit the number of history entries displayed\') parser_history.set_defaults(func=history) # Parse arguments and call appropriate function if len(sys.argv) == 1: parser.print_help(sys.stderr) sys.exit(1) args = parser.parse_args() args.func(args) if __name__ == \\"__main__\\": main() ``` # Instructions: 1. Implement the provided code skeleton and complete the necessary functionalities. 2. Ensure your program handles different edge cases and invalid inputs gracefully. 3. Test your program thoroughly to confirm it meets the specified requirements. **Note**: Submitting a well-documented and thoroughly tested solution will showcase your proficiency with `argparse` and ability to develop robust command-line utilities.","solution":"import argparse import sys def add(args): result = args.a + args.b print(f\\"Result: {result}\\") with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} + {args.b} = {result}n\\") def sub(args): result = args.a - args.b print(f\\"Result: {result}\\") with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} - {args.b} = {result}n\\") def mul(args): result = args.a * args.b print(f\\"Result: {result}\\") with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} * {args.b} = {result}n\\") def div(args): if args.b == 0: print(\\"Error: Division by zero is not allowed.\\") return result = args.a / args.b print(f\\"Result: {result}\\") with open(\'history.txt\', \'a\') as f: f.write(f\\"{args.a} / {args.b} = {result}n\\") def show_history(args): try: with open(\'history.txt\', \'r\') as f: lines = f.readlines() limit = args.limit if args.limit else len(lines) for i, line in enumerate(lines[-limit:], 1): print(f\\"{i}: {line.strip()}\\") except FileNotFoundError: print(\\"No history available.\\") def main(): parser = argparse.ArgumentParser(description=\\"A history-based calculator.\\") subparsers = parser.add_subparsers() parser_add = subparsers.add_parser(\'add\', help=\'Perform addition\') parser_add.add_argument(\'a\', type=float, help=\'First operand\') parser_add.add_argument(\'b\', type=float, help=\'Second operand\') parser_add.set_defaults(func=add) parser_sub = subparsers.add_parser(\'sub\', help=\'Perform subtraction\') parser_sub.add_argument(\'a\', type=float, help=\'First operand\') parser_sub.add_argument(\'b\', type=float, help=\'Second operand\') parser_sub.set_defaults(func=sub) parser_mul = subparsers.add_parser(\'mul\', help=\'Perform multiplication\') parser_mul.add_argument(\'a\', type=float, help=\'First operand\') parser_mul.add_argument(\'b\', type=float, help=\'Second operand\') parser_mul.set_defaults(func=mul) parser_div = subparsers.add_parser(\'div\', help=\'Perform division\') parser_div.add_argument(\'a\', type=float, help=\'First operand\') parser_div.add_argument(\'b\', type=float, help=\'Second operand\') parser_div.set_defaults(func=div) parser_history = subparsers.add_parser(\'history\', help=\'Show calculation history\') parser_history.add_argument(\'--limit\', type=int, help=\'Limit the number of history entries displayed\') parser_history.set_defaults(func=show_history) if len(sys.argv) == 1: parser.print_help(sys.stderr) sys.exit(1) args = parser.parse_args() args.func(args) if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are tasked with creating a composite seaborn plot that effectively communicates multiple dimensions of the `penguins` dataset using different plot elements and customizations. Instructions: 1. Load the `penguins` dataset provided by seaborn. 2. Create a composite plot focusing on the following: - Display the distribution of `body_mass_g` for each species grouped by `sex`. - Use `Dash` to indicate the mean `body_mass_g` for each group. - Add `Dots` to represent individual `body_mass_g` values. - Color code the plots by `sex`. - Use `Dodge` to separate the elements based on `sex`. 3. Customize the plot: - Set the transparency (`alpha`) of the `Dash` elements to 0.6. - Map the width of the `Dash` to `flipper_length_mm`. - Set the width of the plot elements to adapt dynamically when dodged. You should implement a function `create_composite_plot()` that performs the above steps. Ensure your plot is clear and informative. Function Signature: ```python def create_composite_plot(): pass ``` Constraints: - Do not use any additional libraries other than seaborn and its dependencies. Example: Here is an example of how your composite plot might look. You do NOT need to follow this style exactly, but the key components must be present. ```python import seaborn.objects as so from seaborn import load_dataset def create_composite_plot(): penguins = load_dataset(\\"penguins\\") p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add marks and customizations ( p.add(so.Dash(alpha=0.6), linewidth=\\"flipper_length_mm\\") .add(so.Dots(), so.Dodge(), so.Jitter()) .add(so.Dash(), so.Agg(), so.Dodge()) .scale() ) p.plot() ``` Your task is to complete this function such that when called, it will generate the required composite plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_composite_plot(): Creates a composite seaborn plot for the penguins dataset. This plot shows the distribution of body_mass_g for each species grouped by sex, includes dashed lines for means, individual dots for body_mass_g values, and customizes the plot design elements. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add plot elements and customizations ( p.add(so.Dash(linewidth=\\"flipper_length_mm\\"), so.Agg(), so.Dodge(), alpha=0.6) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Render the plot p.plot()"},{"question":"Objective Design and implement a function that takes two strings as input and returns a detailed comparison report highlighting the differences between these two strings. Your function must use the `re` module to find all email addresses and the `difflib` module to compare the sequences and highlight differences. Function Signature ```python def compare_strings(string1: str, string2: str) -> str: pass ``` Input 1. `string1` (str): The first input string. This string may contain text including email addresses. 2. `string2` (str): The second input string. This string may also contain text including email addresses. Output - A formatted multi-line string that includes: 1. A list of email addresses found in `string1` and `string2`. 2. A detailed diff comparison showing the differences between `string1` and `string2`. Constraints 1. You must use the `re` module to extract email addresses. 2. You must use the `difflib` module to generate the diff comparison. Example ```python string1 = \\"Hello, you can reach me at example1@example.com or at work@example.com\\" string2 = \\"Hello, you can reach me at example2@sample.com or at work@example.com\\" output = compare_strings(string1, string2) print(output) ``` Expected Output ``` Emails in string1: - example1@example.com - work@example.com Emails in string2: - example2@sample.com - work@example.com Differences: - Hello, you can reach me at example1@example.com or at work@example.com ? ^ + Hello, you can reach me at example2@sample.com or at work@example.com ? ^ ``` Notes 1. Email addresses are sequences of characters that include an \'@\' symbol followed by a domain. 2. Use the `re.findall()` method to extract email addresses. 3. Use `difflib.unified_diff` or similar functions to generate and format the differences.","solution":"import re import difflib def compare_strings(string1: str, string2: str) -> str: email_pattern = r\'b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}b\' emails_1 = re.findall(email_pattern, string1) emails_2 = re.findall(email_pattern, string2) detailed_diff = difflib.unified_diff(string1.splitlines(), string2.splitlines(), lineterm=\'\') output = [] output.append(\\"Emails in string1:\\") for email in emails_1: output.append(f\\"- {email}\\") output.append(\\"nEmails in string2:\\") for email in emails_2: output.append(f\\"- {email}\\") output.append(\\"nDifferences:\\") for line in detailed_diff: output.append(line) return \\"n\\".join(output)"},{"question":"# Coding Challenge: Creating a Python C Extension using the Limited API Objective: Write a Python C extension that provides a custom Python object type. This custom object should be a simple counter that can be incremented, decremented, and reset. Your implementation should use the **Limited API** to ensure that the extension works across multiple Python 3.x versions without recompilation. Requirements: 1. **Custom Counter Object:** - Create a custom object type `CounterObject` that holds an integer value. - Provide methods to increment (`increment()`), decrement (`decrement()`), and reset (`reset()`) the counter value. - Provide a method to retrieve the current value (`get_value()`). 2. **Module Definition:** - Define a Python module named `counter`. - Ensure the module and its methods are properly exposed to Python. - Make sure the extension is compiled with the `Py_LIMITED_API` defined to ensure compatibility with Python 3.x versions. 3. **Compatibility:** - Test the extension on at least Python 3.7 and Python 3.10 to verify it works without recompilation. Input and Output: - The C extension should not require any input directly in the Python script. - The expected functions will operate on the custom `CounterObject` and manipulation methods will print results to the console to verify functionality. Constraints: - The counter default value should be `0`. - Only use the Limited API for creating and managing the custom object. Example Usage: ```python import counter my_counter = counter.CounterObject() print(my_counter.get_value()) # Output: 0 my_counter.increment() print(my_counter.get_value()) # Output: 1 my_counter.decrement() print(my_counter.get_value()) # Output: 0 my_counter.reset() print(my_counter.get_value()) # Output: 0 ``` # Instructions: 1. Create the C extension source file `counter.c` with the necessary implementation using the Limited API. 2. Write a `setup.py` script to build the extension using `setuptools`. 3. Build the extension and test it on different Python versions to confirm compatibility. Resources: - You may refer to Python’s official documentation on writing C extensions (https://docs.python.org/3/extending/extending.html). - Ensure to review the provided Limited API functions and their documentation to understand their usage. Submission: - Submit the `counter.c`, `setup.py`, and a test script `test_counter.py`. - The `test_counter.py` should demonstrate the example usage and validate the correctness of the custom object methods.","solution":"def counter(): Represents a simple counter that can be incremented, decremented, and reset. class CounterObject: def __init__(self): self.value = 0 def increment(self): self.value += 1 def decrement(self): self.value -= 1 def reset(self): self.value = 0 def get_value(self): return self.value return CounterObject"},{"question":"**Coding Assessment Question** # Task Design a command-line tool using the `argparse` module that manipulates files and directories using the `os` module. # Objective Create a Python script named `file_manager.py` that performs the following operations based on the provided command-line arguments: 1. **Listing Directory Contents:** List all files and directories in the specified path. 2. **Creating a Directory:** Create a new directory at the specified path. 3. **Deleting a File or Directory:** Delete the specified file or directory. 4. **Moving/Renaming a File or Directory:** Move or rename a file or directory to a new location. 5. **Writing to a File:** Write the specified content to a file. Input/Output Format **Input:** Command-line arguments for the script will include the following: - `action`: Specifies the action to be performed. Possible values are \'list\', \'mkdir\', \'delete\', \'move\', or \'write\'. - `path`: Specifies the target path for the action (file or directory). - `new_path` (optional): For \'move\' action, the new location or new name. Not required for other actions. - `content` (optional): For \'write\' action, the content to be written to the file. Not required for other actions. **Output:** Based on the requested action, - If listing directory contents, print all files and directories in the specified path. - If creating or deleting a directory/file, print a success message. - If moving or renaming, print a success message. - If writing to a file, print a success message. Constraints - Ensure that the script properly handles errors such as invalid paths, permissions issues, etc. - The script should be able to handle relative and absolute paths. - The `list` action should handle directories recursively. Performance Requirements - For the `list` action, ensure efficient traversal using appropriate techniques to minimize memory usage for large directories. # Example ```shell python file_manager.py list /path/to/directory # Lists all files and directories in /path/to/directory python file_manager.py mkdir /path/to/new/directory # Creates a new directory at /path/to/new/directory and prints success message python file_manager.py delete /path/to/file_or_directory # Deletes the specified file or directory and prints success message python file_manager.py move /path/to/file_or_directory /new/path/or/name # Moves or renames the specified file or directory and prints success message python file_manager.py write /path/to/file \\"Some content to write\\" # Writes the specified content to the file and prints success message ``` # Instructions 1. Import the necessary modules (`os`, `argparse`, etc.). 2. Parse command-line arguments using `argparse`. 3. Implement the desired actions using appropriate `os` functions. 4. Ensure error handling for invalid paths, permission issues, etc. 5. Provide clear and informative print statements for each operation’s result. # Submission Submit your `file_manager.py` script file that meets these requirements.","solution":"import os import argparse def list_directory_contents(path): try: for root, dirs, files in os.walk(path): level = root.replace(path, \'\').count(os.sep) indent = \' \' * 4 * level print(f\'{indent}{os.path.basename(root)}/\') sub_indent = \' \' * 4 * (level + 1) for f in files: print(f\'{sub_indent}{f}\') except Exception as e: print(f\\"Error listing directory contents: {e}\\") def create_directory(path): try: os.makedirs(path) print(f\\"Directory created: {path}\\") except Exception as e: print(f\\"Error creating directory: {e}\\") def delete_path(path): try: if os.path.isfile(path) or os.path.islink(path): os.remove(path) elif os.path.isdir(path): os.rmdir(path) print(f\\"Deleted: {path}\\") except Exception as e: print(f\\"Error deleting path: {e}\\") def move_or_rename_path(path, new_path): try: os.rename(path, new_path) print(f\\"Moved/Renamed: {path} to {new_path}\\") except Exception as e: print(f\\"Error moving/renaming path: {e}\\") def write_to_file(path, content): try: with open(path, \'w\') as file: file.write(content) print(f\\"Content written to file: {path}\\") except Exception as e: print(f\\"Error writing to file: {e}\\") def main(): parser = argparse.ArgumentParser(description=\\"File Manager\\") parser.add_argument(\'action\', choices=[\'list\', \'mkdir\', \'delete\', \'move\', \'write\'], help=\\"The action to perform\\") parser.add_argument(\'path\', help=\\"The target path for the action\\") parser.add_argument(\'--new_path\', help=\\"For \'move\' action, the new location or new name\\") parser.add_argument(\'--content\', help=\\"For \'write\' action, the content to be written to the file\\") args = parser.parse_args() if args.action == \'list\': list_directory_contents(args.path) elif args.action == \'mkdir\': create_directory(args.path) elif args.action == \'delete\': delete_path(args.path) elif args.action == \'move\': if not args.new_path: print(\\"Error: \'move\' action requires --new_path argument\\") else: move_or_rename_path(args.path, args.new_path) elif args.action == \'write\': if not args.content: print(\\"Error: \'write\' action requires --content argument\\") else: write_to_file(args.path, args.content) if __name__ == \\"__main__\\": main()"},{"question":"Objective Design a function to analyze and visualize the distribution of a dataset\'s numeric variable. You should aim to create insightful plots that help understand the distribution, taking into account different categories within a dataset. Problem Statement Using the `penguins` dataset from Seaborn, implement a function `analyze_penguin_flipper_lengths` that performs the following tasks: 1. **Histogram Plot**: - Plot the distribution of `flipper_length_mm`. - Customize the number of bins to 30. - Fill the bars with a different color for each penguin species using the `hue` parameter. 2. **Kernel Density Estimate (KDE) Plot**: - Plot the KDE of `flipper_length_mm` for each species. - Use a bandwidth adjustment parameter of 0.5 to ensure the plot is well-smoothed. - Fill the area under the density curve for better visual distinction. 3. **Conditional Subsetting using Facets**: - Create a facet grid of KDE plots that shows the distribution of `flipper_length_mm` grouped by `sex`. - Do not use the fill option in this plot. 4. **Empirical Cumulative Distribution Function (ECDF) Plot**: - Plot the ECDF of `flipper_length_mm`. - Display separate lines for each species using the `hue` parameter. 5. **Functions and Parameters**: The function should take no parameters and should read the `penguins` dataset internally. Function Signature ```python def analyze_penguin_flipper_lengths(): pass ``` Constraints - Use the Seaborn library for all plots. - Ensure all plots include appropriate labels and titles for clarity. - Handle any missing data in the dataset by removing rows with missing values. - The output should be four distinct plots as described. Each plot should be automatically displayed when the function is called. Example When you call the function `analyze_penguin_flipper_lengths()`, it should output four plots: 1. A histogram plot with 30 bins and the bars colored by species. 2. A filled KDE plot with adjusted bandwidth for each species. 3. A facet grid of KDE plots grouped by sex. 4. An ECDF plot with lines colored by species. You can assume that the `penguins` dataset is available and can be loaded using `sns.load_dataset(\\"penguins\\")`. Notes - Pay careful attention to the visual distinctiveness and clarity of each plot. - Explore the documentation for any additional parameters that might enhance the visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguin_flipper_lengths(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # 1. Histogram Plot plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'flipper_length_mm\', bins=30, hue=\'species\', multiple=\'stack\') plt.title(\'Distribution of Penguin Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # 2. Kernel Density Estimate (KDE) Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', fill=True, bw_adjust=0.5) plt.title(\'KDE of Penguin Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() # 3. Facet Grid of KDE Plot (grouped by sex) g = sns.FacetGrid(penguins, col=\'sex\', height=6, aspect=1.2) g.map(sns.kdeplot, \'flipper_length_mm\') g.add_legend() g.set_titles(col_template=\'{col_name} Penguins\') g.set_axis_labels(\'Flipper Length (mm)\', \'Density\') plt.suptitle(\'KDE of Penguin Flipper Lengths by Sex\', y=1.02) plt.show() # 4. Empirical Cumulative Distribution Function (ECDF) Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\') plt.title(\'ECDF of Penguin Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.show()"},{"question":"You are given a dataset of car attributes. The dataset contains the following columns: - `mpg`: Miles per gallon - `cylinders`: Number of cylinders - `displacement`: Engine displacement (cu. inches) - `horsepower`: Engine horsepower - `weight`: Weight of the car (lbs) - `acceleration`: Time taken to accelerate from 0 to 60 mph (seconds) - `model_year`: Model year of the car - `origin`: Origin of the car (1: USA, 2: Europe, 3: Japan) - `name`: Name of the car Your task is to create a comprehensive visualization using seaborn that includes the following: 1. A scatter plot showing the relationship between `weight` and `mpg`. 2. Use hue to differentiate the scatter plot by `origin`. 3. Use size to represent the `horsepower` of the cars. 4. Create a line plot to show the trend of `mpg` across different `model_year` values, using hue and style to differentiate by `origin`. 5. Create a faceted grid of scatter plots, where each facet represents a different `cylinders` value. In each scatter plot, show the relationship between `displacement` and `mpg`, using hue to differentiate by `origin`. Input: - The dataset will be provided in a CSV file named `cars_dataset.csv`. Output: - A Python script that produces the specified visualizations. Constraints: - Ensure that the plots are well-labeled and include appropriate legends. - Use a consistent color palette across all plots for hues representing `origin`. Example Code: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset cars = pd.read_csv(\'cars_dataset.csv\') # Setting the theme sns.set_theme(style=\\"darkgrid\\") # Scatter plot of weight vs mpg with hue by origin and size by horsepower scatter_plot = sns.relplot( data=cars, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", size=\\"horsepower\\", sizes=(40, 400), alpha=0.5, palette=\'muted\', height=6 ) scatter_plot.set(ylim=(0, 50), xlim=(1500, 5000)) scatter_plot.fig.suptitle(\'Scatter plot of Weight vs MPG\', y=1.05) # Line plot of mpg over model years, differentiated by origin line_plot = sns.relplot( data=cars, x=\\"model_year\\", y=\\"mpg\\", hue=\\"origin\\", style=\\"origin\\", kind=\\"line\\", markers=True, dashes=False, height=6 ) line_plot.set(ylim=(0, 50)) line_plot.fig.suptitle(\'Trend of MPG over Model Years\', y=1.05) # Faceted scatter plot of displacement vs mpg, faceted by cylinders facet_plot = sns.relplot( data=cars, x=\\"displacement\\", y=\\"mpg\\", hue=\\"origin\\", col=\\"cylinders\\", col_wrap=3, kind=\\"scatter\\", height=4, aspect=1 ) facet_plot.set(ylim=(0, 50), xlim=(50, 500)) facet_plot.fig.suptitle(\'Faceted Scatter plot of Displacement vs MPG\', y=1.05) plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(csv_file): # Load the dataset cars = pd.read_csv(csv_file) # Setting the theme sns.set_theme(style=\\"darkgrid\\") # Scatter plot of weight vs mpg with hue by origin and size by horsepower scatter_plot = sns.relplot( data=cars, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", size=\\"horsepower\\", sizes=(40, 400), alpha=0.5, palette=\'muted\', height=6 ) scatter_plot.set(ylim=(0, 50), xlim=(1500, 5000)) scatter_plot.fig.suptitle(\'Scatter plot of Weight vs MPG\', y=1.05) # Line plot of mpg over model years, differentiated by origin line_plot = sns.relplot( data=cars, x=\\"model_year\\", y=\\"mpg\\", hue=\\"origin\\", style=\\"origin\\", kind=\\"line\\", markers=True, dashes=False, height=6 ) line_plot.set(ylim=(0, 50)) line_plot.fig.suptitle(\'Trend of MPG over Model Years\', y=1.05) # Faceted scatter plot of displacement vs mpg, faceted by cylinders facet_plot = sns.relplot( data=cars, x=\\"displacement\\", y=\\"mpg\\", hue=\\"origin\\", col=\\"cylinders\\", col_wrap=3, kind=\\"scatter\\", height=4, aspect=1 ) facet_plot.set(ylim=(0, 50), xlim=(50, 500)) facet_plot.fig.suptitle(\'Faceted Scatter plot of Displacement vs MPG\', y=1.05) plt.show() # Example usage: # create_visualizations(\'cars_dataset.csv\')"},{"question":"# Question: Implement and Analyze 2D Fourier Transform Using PyTorch\'s `torch.fft` module, implement a function that performs a 2D Fourier transform on a given grayscale image, shifts the zero-frequency component to the center of the spectrum, and then applies an inverse 2D Fourier transform to reconstruct the image. The function should return the magnitudes of the shifted Fourier transform and the reconstructed image. Input: - A 2D PyTorch tensor of shape `(H, W)` representing a grayscale image. Output: - A tuple of two 2D PyTorch tensors: - The magnitude of the shifted 2D Fourier transform of the input image. - The reconstructed image from the inverse 2D Fourier transform. Constraints: - The input image tensor will have a shape with height (`H`) and width (`W`) both being non-zero, positive integers. - The function should use PyTorch\'s `torch.fft` module. Performance Requirements: - The implementation should be efficient and handle reasonably large images (e.g., 1024x1024 pixels) in a practical amount of time. Function Signature: ```python def fourier_transform_analysis(image: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: pass ``` # Example Usage: ```python import torch import matplotlib.pyplot as plt # Example grayscale image of shape (H=256, W=256) H, W = 256, 256 image = torch.randn(H, W) # Call the function fft_magnitude, reconstructed_image = fourier_transform_analysis(image) # Visualization (using matplotlib) plt.subplot(1, 3, 1) plt.title(\\"Original Image\\") plt.imshow(image.numpy(), cmap=\'gray\') plt.subplot(1, 3, 2) plt.title(\\"FFT Magnitude Spectrum\\") plt.imshow(torch.log(1 + fft_magnitude).numpy(), cmap=\'gray\') plt.subplot(1, 3, 3) plt.title(\\"Reconstructed Image\\") plt.imshow(reconstructed_image.numpy(), cmap=\'gray\') plt.show() ``` # Implementation Details: 1. **2D Fourier Transform**: Use the `torch.fft.fft2` function to compute the 2D FFT of the input image. 2. **Shift Zero-Frequency Component**: Use the `torch.fft.fftshift` function to shift the zero-frequency component to the center. 3. **Magnitude Spectrum**: Compute the magnitude of the shifted FFT output. 4. **Inverse 2D Fourier Transform**: Use the `torch.fft.ifft2` function on the non-shifted FFT to compute the inverse transform. 5. **Return Values**: Convert the real and imaginary parts of the inverse FFT result back to a real image and return it along with the FFT magnitude spectrum. # Note: Make sure to handle any complex values properly when computing the magnitude and returning the reconstructed image.","solution":"import torch from typing import Tuple def fourier_transform_analysis(image: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Performs a 2D Fourier transform on a given grayscale image, shifts the zero-frequency component to the center of the spectrum, and applies an inverse 2D Fourier transform to reconstruct the image. Args: image (torch.Tensor): A 2D tensor representing a grayscale image. Returns: Tuple[torch.Tensor, torch.Tensor]: - The magnitude of the shifted 2D Fourier transform of the input image. - The reconstructed image from the inverse 2D Fourier transform. # Perform 2D Fourier transform fft2 = torch.fft.fft2(image) # Shift zero-frequency component to the center fft2_shifted = torch.fft.fftshift(fft2) # Compute magnitude of the shifted FFT magnitude_spectrum = torch.abs(fft2_shifted) # Perform inverse shift ifft2_shifted = torch.fft.ifftshift(fft2_shifted) # Apply inverse 2D Fourier transform reconstructed_image = torch.fft.ifft2(ifft2_shifted) # Take the real part of the reconstructed image reconstructed_image = torch.real(reconstructed_image) return magnitude_spectrum, reconstructed_image"},{"question":"# Coding Exercise: Implementing Low-Level Dictionary Operations Objective: To test your understanding of dictionary operations and error handling by implementing low-level, C-like interfaces in Python. Problem Statement: You need to implement a class `PyDict` that mimics several low-level dictionary operations mentioned in the provided documentation. Your class will utilize built-in dictionary manipulations to mirror the C API\'s functionality but in Python. Class Details: **Class Name:** `PyDict` # Methods: 1. `__init__(self)`: Initializes an empty dictionary. 2. `check(self)`: Returns `True` if the internal object is a dictionary. 3. `check_exact(self)`: Returns `True` if the internal object is strictly a dictionary (not a subtype). 4. `clear(self)`: Empties the dictionary of all key-value pairs. 5. `contains(self, key)`: Returns `True` if the dictionary contains the key, `False` otherwise. 6. `set_item(self, key, value)`: Inserts `value` into the dictionary with the key `key`. Returns `None`. 7. `del_item(self, key)`: Deletes the item with the key `key` from the dictionary. Raises `KeyError` if the key is not present. 8. `get_item(self, key)`: Returns the value associated with `key` in the dictionary. Returns `None` without setting an exception if the key is not present. 9. `set_default(self, key, default)`: If `key` is in the dictionary, returns its value. If not, inserts `key` with a value of `default` and returns `default`. 10. `items(self)`: Returns a list of tuples representing the dictionary\'s items. 11. `keys(self)`: Returns a list of keys from the dictionary. 12. `values(self)`: Returns a list of values from the dictionary. 13. `size(self)`: Returns the number of items in the dictionary. 14. `merge(self, other, override=True)`: Merges another dictionary or dictionary-like object into this one. If `override` is `True`, existing keys will be replaced. If `override` is `False`, existing keys will not be replaced. # Constraints: - Keys must be hashable. - Methods should imitate the behavior of the respective C functions as closely as possible. - Raise appropriate errors where applicable (e.g., `KeyError`). Example Usage: ```python # Create an instance of PyDict pd = PyDict() # Insert items pd.set_item(\\"a\\", 1) pd.set_item(\\"b\\", 2) # Check items assert pd.contains(\\"a\\") == True assert pd.contains(\\"c\\") == False # Get and set default assert pd.get_item(\\"a\\") == 1 assert pd.set_default(\\"c\\", 3) == 3 assert pd.get_item(\\"c\\") == 3 # Size of dictionary assert pd.size() == 3 # Merge dictionaries pd2 = PyDict() pd2.set_item(\\"a\\", 100) pd2.set_item(\\"d\\", 4) pd.merge(pd2, override=False) assert pd.get_item(\\"a\\") == 1 # Since override is False pd.merge(pd2) assert pd.get_item(\\"a\\") == 100 # Since override is True # Delete item pd.del_item(\\"d\\") # pd.del_item(\\"d\\") # This would raise a KeyError # Check items, keys, and values assert len(pd.items()) == 3 assert len(pd.keys()) == 3 assert len(pd.values()) == 3 # Clear dictionary pd.clear() assert pd.size() == 0 ``` Implement the `PyDict` class based on the specifications above. ```python class PyDict: def __init__(self): self.dict = {} def check(self): return isinstance(self.dict, dict) def check_exact(self): return type(self.dict) is dict def clear(self): self.dict.clear() def contains(self, key): return key in self.dict def set_item(self, key, value): self.dict[key] = value def del_item(self, key): del self.dict[key] def get_item(self, key): return self.dict.get(key, None) def set_default(self, key, default): return self.dict.setdefault(key, default) def items(self): return list(self.dict.items()) def keys(self): return list(self.dict.keys()) def values(self): return list(self.dict.values()) def size(self): return len(self.dict) def merge(self, other, override=True): for key, value in other.items(): if override or key not in self.dict: self.dict[key] = value ``` Note: This class should help you understand how low-level operations mimic higher-level counterparts, requiring careful attention to error handling, performance considerations, and adhering to method contracts.","solution":"class PyDict: def __init__(self): self.dict = {} def check(self): return isinstance(self.dict, dict) def check_exact(self): return type(self.dict) is dict def clear(self): self.dict.clear() def contains(self, key): return key in self.dict def set_item(self, key, value): self.dict[key] = value def del_item(self, key): if key in self.dict: del self.dict[key] else: raise KeyError(f\\"Key {key} not found\\") def get_item(self, key): return self.dict.get(key, None) def set_default(self, key, default): return self.dict.setdefault(key, default) def items(self): return list(self.dict.items()) def keys(self): return list(self.dict.keys()) def values(self): return list(self.dict.values()) def size(self): return len(self.dict) def merge(self, other, override=True): if not isinstance(other, dict): raise ValueError(\\"Argument must be a dictionary\\") for key, value in other.items(): if override or key not in self.dict: self.dict[key] = value"},{"question":"# Question: Implementing a Secure Messaging System You are tasked with implementing a secure messaging system using Python\'s cryptographic services. Your system should be able to: 1. Generate secure hashes of messages. 2. Use keyed hashing for message authentication. 3. Generate secure tokens for user authentication. Your implementation should include the following functions: 1. `generate_hash(message: str) -> str`: - Generates a SHA-256 hash of the given message. - Uses the `hashlib` module. - **Input**: A string message. - **Output**: A hexadecimal string representing the SHA-256 hash of the message. 2. `generate_keyed_hash(message: str, key: str) -> str`: - Generates a keyed hash of the given message using the provided key. - Uses the `hmac` module with SHA-256 as the hash function. - **Input**: A string message and a string key. - **Output**: A hexadecimal string representing the keyed hash of the message. 3. `generate_secure_token(byte_length: int) -> str`: - Generates a secure token with a specified number of bytes. - Uses the `secrets` module. - **Input**: An integer representing the number of bytes for the token. - **Output**: A string representing the secure token. # Constraints - The `message` in `generate_hash` and `generate_keyed_hash` can be any UTF-8 string. - The `key` in `generate_keyed_hash` should be a sufficiently random and private string. - The `byte_length` for `generate_secure_token` should be a positive integer greater than 0. # Example Usage ```python # Generate a simple hash hash_value = generate_hash(\\"This is a secret message\\") print(hash_value) # Expected output: Hashed value in hex format # Generate a keyed hash keyed_hash_value = generate_keyed_hash(\\"This is a secret message\\", \\"my_private_key\\") print(keyed_hash_value) # Expected output: Keyed hash value in hex format # Generate a secure token token = generate_secure_token(16) print(token) # Expected output: Secure token string ``` Implement the three functions based on the requirements above.","solution":"import hashlib import hmac import secrets def generate_hash(message: str) -> str: Generates a SHA-256 hash of the given message. Parameters: message (str): The input message to hash. Returns: str: The SHA-256 hash of the message in hexadecimal format. sha256_hash = hashlib.sha256(message.encode(\'utf-8\')).hexdigest() return sha256_hash def generate_keyed_hash(message: str, key: str) -> str: Generates a keyed hash of the given message using the provided key. Parameters: message (str): The input message to hash. key (str): The key used for the HMAC. Returns: str: The keyed hash of the message in hexadecimal format. keyed_hash = hmac.new(key.encode(\'utf-8\'), message.encode(\'utf-8\'), hashlib.sha256).hexdigest() return keyed_hash def generate_secure_token(byte_length: int) -> str: Generates a secure token with a specified number of bytes. Parameters: byte_length (int): The number of bytes for the token. Returns: str: A string representing the secure token. secure_token = secrets.token_hex(byte_length) return secure_token"},{"question":"# Scheduling Events using the sched Module You are required to implement a function `schedule_tasks` that takes a list of task descriptions and schedules them using the `sched` module. Each task description is a tuple in the format `(time, priority, action, args, kwargs)`. You need to print the output of each action when it is executed by the scheduler. Function Signature ```python def schedule_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]]) -> None: pass ``` Input - `tasks`: a list of tuples. Each tuple contains: - `time` (float): the specific time at which the task should run, or the delay before it runs. - `priority` (int): the priority of the task. - `action` (callable): the function to execute. - `args` (tuple): the positional arguments to pass to the action. - `kwargs` (dict): the keyword arguments to pass to the action. Output - The function should print the result of each action when it is called by the scheduler. Constraints - You can assume the `time` values provided are valid and in the order they should be executed for simplicity. - Make sure to handle both absolute and relative time scheduling. - The action functions will always print something to the console when they\'re called. Example ```python import time def sample_action(text): print(f\\"{time.time()}: {text}\\") tasks = [ (time.time() + 5, 1, sample_action, (\\"Task 1\\",), {}), (time.time() + 3, 2, sample_action, (\\"Task 2\\",), {}), (time.time() + 10, 1, sample_action, (\\"Task 3\\",), {}), ] schedule_tasks(tasks) ``` Explanation - Task 2 will run first because it has a higher priority despite being scheduled earlier. - Task 1 will run next. - Task 3 will run last due to the delay. Complete the function `schedule_tasks` to properly schedule and execute tasks as described.","solution":"import sched import time from typing import List, Tuple, Callable, Dict def schedule_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]]) -> None: scheduler = sched.scheduler(time.time, time.sleep) for task in tasks: task_time, priority, action, args, kwargs = task scheduler.enterabs(task_time, priority, action, argument=args, kwargs=kwargs) scheduler.run()"},{"question":"# Question: Advanced Seaborn Data Visualization You have been given a dataset containing daily temperature readings across multiple years. Your task is to perform data manipulation and visualization using seaborn. More specifically, you are required to plot the annual temperature trend for each year in the dataset. Data Description: - The dataset is named `daily_temps` and contains the following columns: - `Date` (datetime64): the date of the observation. - `Temperature` (float): the recorded temperature in degrees Celsius. Requirements: 1. Load the dataset `daily_temps`. 2. Create a Seaborn `Plot` object to visualize the daily temperature trend for each year. 3. Display individual annual trends faceted by decade and color-coded by year. 4. Customize the plot with appropriate line widths and colors. 5. Label the plot with appropriate titles for each decade. Constraints: - Handle missing data appropriately if present. - Ensure that the plot is easily interpretable and visually appealing. Expected Input: ```python No input required. Use the provided dataset `daily_temps`. ``` Expected Output: A faceted line plot displaying annual temperature trends, color-coded by year and organized by decade. Example Code: The following template can help to start with, but you must customize it to meet all requirements: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Sample dataset loading # daily_temps = load_dataset(\'daily_temps\') # Example: Generating example data (for illustration, replace with actual dataset) # ----------------------------------------- dates = pd.date_range(\'2000-01-01\', \'2010-12-31\', freq=\'D\') temperatures = np.random.normal(10, 5, len(dates)) daily_temps = pd.DataFrame({\'Date\': dates, \'Temperature\': temperatures}) # ----------------------------------------- # Ensure \'Date\' column is in datetime format (if not already) daily_temps[\'Date\'] = pd.to_datetime(daily_temps[\'Date\']) # Create and customize the plot plot = ( so.Plot( x=daily_temps[\'Date\'].dt.day_of_year, y=daily_temps[\'Temperature\'], color=daily_temps[\'Date\'].dt.year ) .facet(daily_temps[\'Date\'].dt.year // 10 * 10) # Facet by decade .add(so.Lines(linewidth=0.5, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"{}s\\".format) ) # Display the plot plot.show() ``` Modify and complete this code to fully satisfy the problem requirements. Ensure your final implementation produces a clean, well-labeled, and informative plot.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt from seaborn.objects import Plot # Sample dataset loading (replace with actual data source) # Example: Generating example data (for illustration purpose, replace with the actual dataset) # ----------------------------------------- dates = pd.date_range(\'2000-01-01\', \'2010-12-31\', freq=\'D\') temperatures = np.random.normal(10, 5, len(dates)) daily_temps = pd.DataFrame({\'Date\': dates, \'Temperature\': temperatures}) # ----------------------------------------- # Ensure \'Date\' column is in datetime format (if not already) daily_temps[\'Date\'] = pd.to_datetime(daily_temps[\'Date\']) # Extract year and day of the year from the date daily_temps[\'Year\'] = daily_temps[\'Date\'].dt.year daily_temps[\'DayOfYear\'] = daily_temps[\'Date\'].dt.day_of_year daily_temps[\'Decade\'] = (daily_temps[\'Year\'] // 10) * 10 # Create and customize the plot plt.figure(figsize=(15, 10)) plot = sns.relplot( data=daily_temps, x=\'DayOfYear\', y=\'Temperature\', hue=\'Year\', kind=\'line\', height=6, aspect=1.5, col=\'Decade\', col_wrap=2, linewidth=0.8, palette=\'tab20\', facet_kws={\'sharey\': False, \'sharex\': True}, ) # Add titles and customize plot.set_titles(\\"{col_name} Decade\\") plot.set_axis_labels(\\"Day of Year\\", \\"Temperature (°C)\\") plot.fig.suptitle(\'Annual Temperature Trends by Decade\', fontsize=16) plt.subplots_adjust(top=0.9) # Display the plot plt.show()"},{"question":"# HMAC Message Integrity Verification In this task, you are asked to create a function that helps ensure the integrity and authenticity of a message using the HMAC (Hash-based Message Authentication Code) mechanism. This function will implement several aspects of the `hmac` module. You need to implement a function `verify_message_integrity(key, original_msg, compromised_msg, digestmod)` that accepts: 1. `key` (bytes or bytearray): The secret key used for the HMAC. 2. `original_msg` (str): The original message that we want to verify. 3. `compromised_msg` (str): Another message that might be tampered with. 4. `digestmod` (str): The hash function to use (e.g., \'sha256\'). The function should: 1. Compute the HMAC of the original message using the provided key and digest method. 2. Compute the HMAC of the potentially compromised message using the same key and digest method. 3. Return a tuple with two elements: - `hexdigest` of the original message HMAC. - A boolean indicating whether the two messages\' HMAC values are identical or not (using `hmac.compare_digest`). # Constraints and Limitations - The key should be a non-empty bytes or bytearray. - The `digestmod` should be a valid hash function name recognized by `hashlib`. - The function should handle the messages as strings and convert them appropriately for HMAC processing. # Example Usage ```python import hmac import hashlib def verify_message_integrity(key, original_msg, compromised_msg, digestmod): # Your implementation here # Example test case key = b\'secret_key\' original_msg = \\"Message to protect\\" compromised_msg = \\"Message to tamper\\" digestmod = \'sha256\' result = verify_message_integrity(key, original_msg, compromised_msg, digestmod) print(result) # Should print a tuple with the hexdigest of original message and a boolean False ``` # Performance Requirements - The function should efficiently handle digest computation and comparison. - The function should secure against timing attacks during digest comparison. Implement the function `verify_message_integrity` satisfying the above requirements and constraints.","solution":"import hmac import hashlib def verify_message_integrity(key, original_msg, compromised_msg, digestmod): Verifies the integrity of a message using HMAC. Args: key (bytes or bytearray): The secret key used for the HMAC. original_msg (str): The original message to verify. compromised_msg (str): Another message that might be tampered with. digestmod (str): The hash function to use (e.g., \'sha256\'). Returns: tuple: (hexdigest of the original message HMAC, boolean indicating if the messages are identical) # Convert messages to bytes original_msg_bytes = original_msg.encode() compromised_msg_bytes = compromised_msg.encode() # Create HMAC objects original_hmac = hmac.new(key, original_msg_bytes, digestmod) compromised_hmac = hmac.new(key, compromised_msg_bytes, digestmod) # Generate HMAC hex digests original_hmac_hexdigest = original_hmac.hexdigest() compromised_hmac_hexdigest = compromised_hmac.hexdigest() # Compare HMAC values securely hmac_equal = hmac.compare_digest(original_hmac_hexdigest, compromised_hmac_hexdigest) return original_hmac_hexdigest, hmac_equal"},{"question":"**Objective**: Test the understanding and application of the `pathlib` module in Python, focusing on both pure paths and concrete paths. **Problem Statement**: You are given the root directory of a project and you need to perform a series of operations to summarize the structure and contents of the directory, focusing on Python source files. Implement a function `summarize_python_project(root_dir: str) -> Dict[str, Any]` that will return a summary dictionary containing: 1. A list of all subdirectories within the root directory. 2. A list of all Python source files within the root directory and its subdirectories. 3. The absolute path and size of each Python source file. 4. Relative paths of Python source files grouped by the directory they reside in. Requirements: - **Input**: `root_dir` (a string) - the root directory of the project. - **Output**: Dictionary with the following structure: ```python { \\"subdirectories\\": [list_of_subdirectories], \\"python_files\\": { \\"absolute_paths\\": [absolute_paths_of_python_files], \\"sizes\\": [sizes_of_files_in_bytes], \\"grouped_by_directory\\": { \\"directory_name\\": [list_of_relative_paths] } } } ``` Constraints: - The function has to handle large directory trees efficiently. - Use appropriate error handling for file I/O operations. Example Usage: ```python result = summarize_python_project(\\"/path/to/project\\") print(result) ``` Implementation Notes: - Use the `Path` class from the `pathlib` module to handle paths. - Use methods provided by `Path` to retrieve subdirectories, list files and their properties. - Recursively search through subdirectories to find Python source files. - Ensure that the returned paths are appropriate to their context (relative or absolute as specified). **Hints**: - Use `Path.iterdir()` for directory listing. - Use `Path.glob(\\"**/*.py\\")` for recursive searching of Python files. - Use `Path().resolve()` to get the absolute path. - Use `Path().relative_to(root)` to get the relative path. **Submission**: Submit a Python file containing your function `summarize_python_project`. Ensure your code is well-documented and tested with comments or print statements displaying results for a sample directory.","solution":"from pathlib import Path from typing import Dict, Any def summarize_python_project(root_dir: str) -> Dict[str, Any]: Summarizes the structure and contents of a Python project directory. Parameters: root_dir (str): The root directory of the project. Returns: Dict[str, Any]: A dictionary with subdirectories and python file details. root_path = Path(root_dir) summary = { \\"subdirectories\\": [], \\"python_files\\": { \\"absolute_paths\\": [], \\"sizes\\": [], \\"grouped_by_directory\\": {} } } # List all subdirectories for subdir in root_path.iterdir(): if subdir.is_dir(): summary[\\"subdirectories\\"].append(str(subdir.relative_to(root_path))) # Find all python files for py_file in root_path.glob(\\"**/*.py\\"): abs_path = py_file.resolve() file_size = py_file.stat().st_size relative_dir = str(py_file.parent.relative_to(root_path)) # Add to absolute paths and sizes summary[\\"python_files\\"][\\"absolute_paths\\"].append(str(abs_path)) summary[\\"python_files\\"][\\"sizes\\"].append(file_size) # Group by directory if relative_dir not in summary[\\"python_files\\"][\\"grouped_by_directory\\"]: summary[\\"python_files\\"][\\"grouped_by_directory\\"][relative_dir] = [] summary[\\"python_files\\"][\\"grouped_by_directory\\"][relative_dir].append(str(py_file.relative_to(root_path))) return summary"},{"question":"**Coding Question: Implementing a Custom Policy and Processing Email Messages** # Objective: You are required to implement a function `process_email_content` that customizes the email policies, reads an email message from a file, processes it using the customized policy, and writes the processed email back to a new file. # Function Signature: ```python def process_email_content(input_filename: str, output_filename: str, max_line_length: int, linesep: str, cte_type: str) -> None: ``` # Parameters: - `input_filename` (str): The name of the input file containing the raw email message. - `output_filename` (str): The name of the output file where the processed email message will be written. - `max_line_length` (int): The maximum line length to be used in the policy. - `linesep` (str): The line separator to be used in the policy (e.g., \'n\' or \'rn\'). - `cte_type` (str): The content transfer encoding type (\'7bit\' or \'8bit\'). # Requirements: 1. Read the email message from the file specified by `input_filename`. 2. Create a custom `EmailPolicy` with the specified `max_line_length`, `linesep`, and `cte_type`. 3. Use this custom policy to regenerate the email message. 4. Write the processed email message to the file specified by `output_filename`. # Constraints: - The email message in the input file will be in a format compatible with the email package. - Ensure the policy settings do not violate content transfer encoding constraints (i.e., \'cte_type=8bit\' can only be used with `BytesGenerator`). # Example Usage: ```python process_email_content(\'raw_email.txt\', \'processed_email.txt\', 80, \'n\', \'7bit\') ``` # Notes: - You may find it useful to refer to the `email.message_from_binary_file`, `email.generator.BytesGenerator`, and `email.policy` modules. - Focus on handling the policies correctly to truly demonstrate your understanding of how the email policy module influences email generation. This question tests the ability to comprehend and utilize customized policy settings in the email module, an essential skill for handling email processing in Python.","solution":"from email import policy from email.parser import BytesParser from email.generator import BytesGenerator def process_email_content(input_filename: str, output_filename: str, max_line_length: int, linesep: str, cte_type: str) -> None: # Define the custom policy custom_policy = policy.default.clone( max_line_length=max_line_length, linesep=linesep, cte_type=cte_type ) # Read the email message from the input file with open(input_filename, \'rb\') as fp: msg = BytesParser(policy=custom_policy).parse(fp) # Write the processed email message to the output file with open(output_filename, \'wb\') as fp: generator = BytesGenerator(fp, policy=custom_policy) generator.flatten(msg)"},{"question":"# Task You are required to implement a function `complex_path_ops(root_path: str, file_name: str) -> dict` that performs multiple operations on a given root path and a file name, using the `os.path` module functionalities. Function signature: ```python def complex_path_ops(root_path: str, file_name: str) -> dict: ``` # Inputs: - `root_path`: A string representing the root directory path. - `file_name`: A string representing the name of a file. # Output: - A dictionary containing: - `absolute_path`: The absolute normalized path of the combined root and file name. - `dir_name`: The directory name of the absolute path. - `base_name`: The base name of the absolute path. - `file_exists`: Boolean indicating whether the file exists in the given path. - `normal_name`: The normalized path after collapsing redundant separators. - `join_with_root`: The joined path of root and file name. - `split_drive`: The split drive part and the remaining path. - `split_ext`: The root path and file extension. # Constraints: - The function should handle both UNIX and Windows-style paths. # Example: ```python import os def complex_path_ops(root_path: str, file_name: str) -> dict: result = {} combined_path = os.path.join(root_path, file_name) result[\'absolute_path\'] = os.path.abspath(combined_path) result[\'dir_name\'] = os.path.dirname(result[\'absolute_path\']) result[\'base_name\'] = os.path.basename(result[\'absolute_path\']) result[\'file_exists\'] = os.path.exists(result[\'absolute_path\']) result[\'normal_name\'] = os.path.normpath(combined_path) result[\'join_with_root\'] = combined_path result[\'split_drive\'] = os.path.splitdrive(combined_path) result[\'split_ext\'] = os.path.splitext(combined_path) return result root_path = \\"/home/user/docs\\" file_name = \\"example.txt\\" print(complex_path_ops(root_path, file_name)) ``` # Explanation: Given the `root_path` as \\"/home/user/docs\\" and `file_name` as \\"example.txt\\": - `absolute_path`: Will be the complete absolute path \\"/home/user/docs/example.txt\\". - `dir_name`: The directory name \\"/home/user/docs\\". - `base_name`: The base name \\"example.txt\\". - `file_exists`: Boolean to check if the file \\"/home/user/docs/example.txt\\" exists. - `normal_name`: The path rounded to normalized path format to handle any redundant separators. - `join_with_root`: The path created by joining \\"/home/user/docs\\" and \\"example.txt\\". - `split_drive`: The split drive parts based on the operating system. - `split_ext`: The split parts of the path, where one is the root, and the other is the extension of the file. Feel free to modify and create more test cases to verify the function\'s correctness and robustness.","solution":"import os def complex_path_ops(root_path: str, file_name: str) -> dict: result = {} combined_path = os.path.join(root_path, file_name) result[\'absolute_path\'] = os.path.abspath(combined_path) result[\'dir_name\'] = os.path.dirname(result[\'absolute_path\']) result[\'base_name\'] = os.path.basename(result[\'absolute_path\']) result[\'file_exists\'] = os.path.exists(result[\'absolute_path\']) result[\'normal_name\'] = os.path.normpath(combined_path) result[\'join_with_root\'] = combined_path result[\'split_drive\'] = os.path.splitdrive(result[\'absolute_path\']) result[\'split_ext\'] = os.path.splitext(result[\'absolute_path\']) return result"},{"question":"**Question: Visualizing Categorical Data with Seaborn** You are given two datasets: `tips` and `titanic`, preloaded with seaborn. Your task is to visualize various aspects of these datasets using Seaborn\'s categorical plots. Follow the instructions below to create the required visualizations. 1. **Load the datasets**: Use seaborn\'s built-in functions to load the `tips` and `titanic` datasets. 2. **Create a categorical scatter plot**: - Plot the relationship between the `day` and `total_bill` columns from the `tips` dataset using a `swarmplot`. - Use different colors for the `smoker` categories. 3. **Visualize the distribution of total bills**: - Create a `violinplot` of `total_bill` grouped by `day` in the `tips` dataset. - Split the violins based on the `sex` column and adjust the bandwidth using the `bw_adjust` parameter. 4. **Compare survival rates**: - Use the `titanic` dataset to create a `pointplot` that shows the survival rate as a function of the `sex` and `class` categories. - Use different markers and line styles for each sex and class combination. 5. **Faceting**: - Create a faceted `boxplot` to visualize the distribution of `fare` paid by passengers from different `embark_town` in the `titanic` dataset. - Use `row` facet for `class`. - Apply appropriate axis labels and titles for each facet. **Constraints and Requirements**: - Use seaborn for all visualizations. - Ensure proper titles and labels for axes in each plot. - Manage overlapping points and adjust the aesthetics for clear visual representation. **Input and Output Formats**: - **Input**: The code will directly use seaborn\'s datasets, no explicit input from the user. - **Output**: The output should be visualizations displayed as plots. **Example Usage**: ```python # Example function call to load data and create visualizations import numpy as np import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"ticks\\", color_codes=True) np.random.seed(sum(map(ord, \\"categorical\\"))) # Load datasets tips = sns.load_dataset(\\"tips\\") titanic = sns.load_dataset(\\"titanic\\") # Create visualizations (implement your solution here) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load datasets tips = sns.load_dataset(\\"tips\\") titanic = sns.load_dataset(\\"titanic\\") # Categorical scatter plot plt.figure(figsize=(10, 6)) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", data=tips) plt.title(\\"Scatter plot of Total Bill vs Day (colored by Smoker)\\") plt.ylabel(\\"Total Bill\\") plt.xlabel(\\"Day\\") plt.legend(title=\\"Smoker\\") plt.show() # Violin plot to visualize distribution of total bills plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, bw=0.5, split=True) plt.title(\\"Violin plot of Total Bill vs Day (split by Sex)\\") plt.ylabel(\\"Total Bill\\") plt.xlabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # Compare survival rates using a point plot plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", data=titanic, markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\\"Survival Rate by Class and Sex\\") plt.ylabel(\\"Survival Rate\\") plt.xlabel(\\"Class\\") plt.legend(title=\\"Sex\\") plt.show() # Faceted boxplot to visualize fare distribution g = sns.catplot(x=\\"embark_town\\", y=\\"fare\\", hue=\\"embark_town\\", data=titanic, kind=\\"box\\", height=5, aspect=1.5, row=\\"class\\", palette=\\"Set3\\") g.set_titles(\\"Class: {row_name}\\") g.set_axis_labels(\\"Embark Town\\", \\"Fare\\") g.fig.suptitle(\\"Boxplot of Fare Paid by Passengers (Faceted by Class)\\", y=1.03) plt.show()"},{"question":"# Custom Asyncio Event Loop Policy with Process Watcher **Objective**: Implement a custom event loop policy that integrates a specialized process watcher capable of handling subprocess lifecycle events efficiently using threads. **Problem Statement**: You are required to create a custom event loop policy and a threaded child watcher to manage subprocesses in an efficient and thread-safe manner. 1. **Custom Event Loop Policy**: - Subclass `asyncio.DefaultEventLoopPolicy` and override the `get_event_loop` method to include custom behavior. - Your custom event loop policy should log (print) a message each time `get_event_loop` is called, and then return the loop as usual. 2. **Custom ThreadedChildWatcher**: - Implement a subclass of `asyncio.ThreadedChildWatcher`. - Override the `add_child_handler`, `remove_child_handler`, and `attach_loop` methods to include custom behavior: - `add_child_handler`: Log (print) the PID of the process being watched. - `remove_child_handler`: Log (print) the PID of the process whose handler is being removed. - `attach_loop`: Log (print) a message each time the watcher is attached to an event loop. 3. **Integration**: - Set your custom event loop policy as the current policy. - Configure your custom policy to use the custom threaded child watcher. - Write a function `run_custom_event_loop` that demonstrates the creation of the event loop, spawns a subprocess, and uses the custom child watcher to monitor it. **Constraints**: - The system must be able to run on both Unix and Windows platforms, where applicable. - Your implementation should handle errors gracefully and log unexpected issues. **Input/Output**: - Your implementation does not need to take input from the user. - Ensure that all log messages inform the steps being taken by the event loop and the child watcher. **Code Skeleton**: ```python import asyncio import subprocess class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Custom get_event_loop called\\") return super().get_event_loop() class MyThreadedChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding handler for PID {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): print(f\\"Removing handler for PID {pid}\\") return super().remove_child_handler(pid) def attach_loop(self, loop): print(\\"Attaching watcher to event loop\\") super().attach_loop(loop) async def create_subprocess(): # Function to create a subprocess process = await asyncio.create_subprocess_exec( \'ls\', stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = await process.communicate() print(f\'[stdout]n{stdout.decode()}\') print(f\'[stderr]n{stderr.decode()}\') return process def run_custom_event_loop(): asyncio.set_event_loop_policy(MyEventLoopPolicy()) policy = asyncio.get_event_loop_policy() watcher = MyThreadedChildWatcher() asyncio.set_child_watcher(watcher) loop = policy.new_event_loop() asyncio.set_event_loop(loop) watcher.attach_loop(loop) try: loop.run_until_complete(create_subprocess()) finally: loop.close() if __name__ == \\"__main__\\": run_custom_event_loop() ``` You are allowed to modify the `create_subprocess` function to fit your testing environment, ensuring compatibility with available commands or subprocess tasks.","solution":"import asyncio import subprocess class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Custom get_event_loop called\\") return super().get_event_loop() class MyThreadedChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding handler for PID {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): print(f\\"Removing handler for PID {pid}\\") return super().remove_child_handler(pid) def attach_loop(self, loop): print(\\"Attaching watcher to event loop\\") super().attach_loop(loop) async def create_subprocess(): # Function to create a subprocess process = await asyncio.create_subprocess_exec( \'echo\', \'hello\', stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = await process.communicate() print(f\'[stdout]n{stdout.decode()}\') print(f\'[stderr]n{stderr.decode()}\') return process def run_custom_event_loop(): asyncio.set_event_loop_policy(MyEventLoopPolicy()) policy = asyncio.get_event_loop_policy() watcher = MyThreadedChildWatcher() asyncio.set_child_watcher(watcher) loop = policy.new_event_loop() asyncio.set_event_loop(loop) watcher.attach_loop(loop) try: loop.run_until_complete(create_subprocess()) finally: loop.close() if __name__ == \\"__main__\\": run_custom_event_loop()"},{"question":"# Python Script Module Analysis Using the `modulefinder` module, write a Python function `analyze_script_imports(script_path: str, exclude_modules: list, output_file: str) -> None` that performs the following tasks: 1. **Analyzes** the imported modules of the given script located at `script_path`. 2. **Excludes** specified modules listed in `exclude_modules` from the analysis. 3. **Generates an output report** that includes: - A list of all modules that were successfully imported by the script. - A list of all modules that seem to be missing. Your function should write the results to the file specified by `output_file` in the following format: ``` Loaded modules: module_name1: [list_of_global_variables] module_name2: [list_of_global_variables] ... Modules not imported: module_name1 module_name2 ... ``` Input - `script_path` (str): The path to the Python script to be analyzed. - `exclude_modules` (list): A list of module names to be excluded from the analysis. - `output_file` (str): The path to the output file where the results will be written. Output - The function writes the analysis results to the specified output file in the described format. Constraints - The function should handle any import errors gracefully. - The list of global variables for each loaded module should include only the first three global variable names (if available). - The output file must be created if it does not exist and overwritten if it does. Example To analyze a script `bacon.py` and exclude the modules `re` and `itertools`, you would call: ```python analyze_script_imports(\'bacon.py\', [\'re\', \'itertools\'], \'output_report.txt\') ``` If `bacon.py` contains the following: ```python import re, itertools try: import baconhameggs except ImportError: pass try: import guido.python.ham except ImportError: pass ``` The contents of `output_report.txt` might be: ``` Loaded modules: sre_compile: [isstring, _sre, _optimize_unicode] sre_constants: [REPEAT_ONE, makedict, AT_END_LINE] sys: [] sre_parse: [_PATTERNENDERS, SRE_FLAG_UNICODE] __main__: [] --------------------------------------------------- Modules not imported: guido.python.ham baconhameggs ``` Your function should be robust and test for various edge cases such as missing files, modules with no global variables, and different levels of nested imports.","solution":"import modulefinder def analyze_script_imports(script_path: str, exclude_modules: list, output_file: str) -> None: Analyze the imported modules of a given script and write the analysis to an output file. Parameters: - script_path (str): Path to the script to analyze. - exclude_modules (list): List of module names to exclude from the analysis. - output_file (str): Path to the file where the analysis results will be written. finder = modulefinder.ModuleFinder(excludes=exclude_modules) try: finder.run_script(script_path) except Exception as e: raise RuntimeError(f\\"Error running script: {e}\\") loaded_modules = {name: mod for name, mod in finder.modules.items() if name not in exclude_modules} missing_modules = set(finder.badmodules.keys()) - set(exclude_modules) with open(output_file, \'w\') as f: f.write(\\"Loaded modules:n\\") for name, mod in loaded_modules.items(): global_names = list(mod.globalnames.keys())[:3] f.write(f\\"{name}: {global_names}n\\") f.write(\\"nModules not imported:n\\") for name in missing_modules: f.write(f\\"{name}n\\")"},{"question":"# Character Set Encoding and Conversion You are to implement a Python function utilizing the `email.charset` module\'s `Charset` class to handle character set encoding and conversion for email messages. Function Signature ```python def encode_email_text(input_charset: str, text: str) -> dict: pass ``` Parameters - `input_charset` (str): The initial character set of the input text. - `text` (str): The plain text input that needs to be encoded and converted. Returns - (dict): A dictionary with the following keys: - `input_codec` (str): The name of the Python codec used for input conversion to Unicode. - `output_codec` (str): The name of the Python codec used for output conversion from Unicode. - `header_encoding` (str): The encoding used for email headers (quoted-printable or base64). - `body_encoding` (str): The encoding used for the email body (quoted-printable or base64). - `header_encoded_text` (str): The text encoded for email headers. - `body_encoded_text` (str): The text encoded for email bodies. Constraints - The `input_charset` should be a valid character set known to Python\'s codecs module. - Assume the `text` input is always a valid string. Example ```python input_charset = \'iso-8859-1\' text = \'This is a sample email text\' result = encode_email_text(input_charset, text) print(result) # Example output might be: # { # \'input_codec\': \'latin_1\', # \'output_codec\': \'latin_1\', # \'header_encoding\': \'quoted-printable\', # \'body_encoding\': \'quoted-printable\', # \'header_encoded_text\': \'This is a sample email text\', # \'body_encoded_text\': \'This is a sample email text\' # } ``` Explanation 1. Create an instance of the `Charset` class using the provided `input_charset`. 2. Determine and store the `input_codec`, `output_codec`, `header_encoding`, and `body_encoding`. 3. Encode the `text` for both headers and body as per the determined encoding methods. 4. Return the information as a dictionary. Notes - Account for the scenario where no encoding is needed for headers or bodies. - Use exceptions to handle potential errors arising from invalid codecs or character sets.","solution":"from email.charset import Charset, QP, BASE64 def encode_email_text(input_charset: str, text: str) -> dict: # Create a Charset instance with the input_charset charset = Charset(input_charset) # Determine the input and output codecs input_codec = charset.input_charset output_codec = charset.output_charset # Determine the header and body encodings header_encoding = \'quoted-printable\' if charset.header_encoding == QP else \'base64\' body_encoding = \'quoted-printable\' if charset.body_encoding == QP else \'base64\' # Encode the text for headers and body header_encoded_text = text.encode(input_codec).decode(output_codec) body_encoded_text = text.encode(input_codec).decode(output_codec) # Return the dictionary with the results return { \'input_codec\': input_codec, \'output_codec\': output_codec, \'header_encoding\': header_encoding, \'body_encoding\': body_encoding, \'header_encoded_text\': header_encoded_text, \'body_encoded_text\': body_encoded_text }"},{"question":"You are given a DataFrame and the requirement to style this DataFrame using pandas\' `Styler` functionalities. Your task is to create a stylized HTML representation of the DataFrame with specific style requirements. This will demonstrate your understanding of advanced pandas functionalities. Problem Description You will implement a function `style_dataframe(df: pd.DataFrame) -> str`: 1. The input is a DataFrame with at least the following columns: `[\'Category\', \'Value1\', \'Value2\']`. 2. Style the DataFrame with the following requirements: - Apply a gradient background color for `Value1` and `Value2` columns. - Highlight the maximum value in `Value1` with a light green background. - Highlight the minimum value in `Value2` with a light red background. - Set the table caption to \\"Styled DataFrame\\". - Ensure the row and column headers are bold. 3. Convert the styled DataFrame to an HTML string and return it. Function Signature ```python import pandas as pd def style_dataframe(df: pd.DataFrame) -> str: pass ``` Example ```python import pandas as pd data = { \'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Value1\': [1.5, 2.5, 3.5, 4.5], \'Value2\': [7.0, 6.5, 5.5, 5.0] } df = pd.DataFrame(data) html_output = style_dataframe(df) print(html_output) ``` In this example, the `html_output` should be an HTML representation of the provided DataFrame with the specified styles applied. Constraints - You must use the pandas library (import as pd). - Ensure you handle a DataFrame with any number of rows, as long as the required columns exist. - Use the `Styler` functionalities discussed in the documentation. Evaluation Criteria - Correctness of the styles applied as defined in the problem statement. - The correctness of the HTML output format. - Efficient and clean Pandas operations.","solution":"import pandas as pd def style_dataframe(df: pd.DataFrame) -> str: Styles the DataFrame and converts it to an HTML string with the following requirements: - Gradient background on \'Value1\' and \'Value2\' columns. - Highlight the maximum value in \'Value1\' with a light green background. - Highlight the minimum value in \'Value2\' with a light red background. - Set the table caption to \\"Styled DataFrame\\". - Ensure the row and column headers are bold. Args: df (pd.DataFrame): The DataFrame to be styled. Returns: str: HTML representation of the styled DataFrame. def highlight_max(s): return [\'background-color: lightgreen\' if v == max(s) else \'\' for v in s] def highlight_min(s): return [\'background-color: lightcoral\' if v == min(s) else \'\' for v in s] styled = ( df.style .apply(lambda x: [\'font-weight: bold\' for _ in x], axis=0) # Column headers .apply(lambda x: [\'font-weight: bold\' for _ in x], axis=1) # Row headers .applymap(lambda x: \'background-color: lightyellow\', subset=[\'Value1\', \'Value2\']) .background_gradient(subset=[\'Value1\', \'Value2\'], cmap=\'viridis\') .apply(highlight_max, subset=[\'Value1\']) .apply(highlight_min, subset=[\'Value2\']) .set_caption(\\"Styled DataFrame\\") ) return styled.to_html()"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s probability distributions by implementing a function that performs specific statistical operations on a given dataset using various distributions available in PyTorch. Problem Statement: You are given n-dimensional data in the form of a `torch.Tensor`. Implement a function `calculate_distribution_metrics` that performs the following tasks: 1. Fits the data to the given probability distributions: `Normal`, `Exponential`, and `Bernoulli`. 2. Computes the log-probabilities for each data point under each fitted distribution. 3. Returns a dictionary containing the log-probabilities for all distributions. ```python def calculate_distribution_metrics(data: torch.Tensor) -> dict: Fits the data to different probability distributions and calculates log-probabilities. Params: - data (torch.Tensor): n-dimensional tensor containing the data to be evaluated. Returns: - dict: Contains the following key-value pairs: - \'normal_log_probs\': Log-probabilities of data points under the fitted Normal distribution. - \'exponential_log_probs\': Log-probabilities of data points under the fitted Exponential distribution. - \'bernoulli_log_probs\': Log-probabilities of data points under the fitted Bernoulli distribution. import torch from torch.distributions import Normal, Exponential, Bernoulli # Your implementation here return { \'normal_log_probs\': None, # Replace None with actual log probs tensor from Normal distribution \'exponential_log_probs\': None, # Replace None with actual log probs tensor from Exponential distribution \'bernoulli_log_probs\': None # Replace None with actual log probs tensor from Bernoulli distribution } ``` Constraints: - The input tensor `data` will be a 1D tensor containing continuous values. - You may assume that the input data can be normalized/scaled appropriately for fitting the Bernoulli distribution. Example: ```python import torch data = torch.tensor([2.3, 1.7, 1.8, 2.0, 1.5]) result = calculate_distribution_metrics(data) # The result dictionary should contain log-probabilities for each distribution print(result[\'normal_log_probs\']) # Should print the log-probabilities for the normal distribution print(result[\'exponential_log_probs\']) # Should print the log-probabilities for the exponential distribution print(result[\'bernoulli_log_probs\']) # Should print the log-probabilities for the bernoulli distribution ``` Additional Notes: - Ensure that you handle edge cases where the data may contain extreme values. - You may use methods like `mean`, `stddev`, `log_prob`, and others provided by the distributions to calculate the necessary metrics.","solution":"import torch from torch.distributions import Normal, Exponential, Bernoulli def calculate_distribution_metrics(data: torch.Tensor) -> dict: Fits the data to different probability distributions and calculates log-probabilities. Params: - data (torch.Tensor): n-dimensional tensor containing the data to be evaluated. Returns: - dict: Contains the following key-value pairs: - \'normal_log_probs\': Log-probabilities of data points under the fitted Normal distribution. - \'exponential_log_probs\': Log-probabilities of data points under the fitted Exponential distribution. - \'bernoulli_log_probs\': Log-probabilities of data points under the fitted Bernoulli distribution. assert isinstance(data, torch.Tensor), \\"Input data must be a torch.Tensor\\" # Fit data to Normal distribution normal_dist = Normal(data.mean(), data.std()) normal_log_probs = normal_dist.log_prob(data) # Fit data to Exponential distribution by converting data to positive (exponential lambda must be positive) positive_data = data - data.min() + 1 # shifting data to be positive exponential_dist = Exponential(positive_data.mean()) exponential_log_probs = exponential_dist.log_prob(positive_data) # Fit data to Bernoulli distribution by normalizing data between 0 and 1 min_data, max_data = data.min(), data.max() normalized_data = (data - min_data) / (max_data - min_data) bernoulli_dist = Bernoulli(normalized_data.mean()) bernoulli_log_probs = bernoulli_dist.log_prob(normalized_data.round()) return { \'normal_log_probs\': normal_log_probs, \'exponential_log_probs\': exponential_log_probs, \'bernoulli_log_probs\': bernoulli_log_probs }"},{"question":"**Enum Warehouse** Enumerations are useful for defining a set of symbolic names bound to unique values. In this exercise, you will write code to simulate a warehouse inventory system using enums provided by the `enum` module. # Task: 1. You need to create an `Enum` class `Category` that contains the following categories: `ELECTRONICS`, `FURNITURE`, `CLOTHING`, `FOOD`, and `TOYS`. 2. Implement a class `Product` that: - Initializes with a `name` (string), `price` (float), and `category` (of type `Category`). - Implements a method `__str__()` to return a string representation of the product in the format \\"Product(name: <name>, price: <price>, category: <category name>)\\". 3. Implement a class `Warehouse` that: - Initializes with an empty list of products. - Provides a method `add_product(product: Product)` to add a product to the warehouse. - Provides a method `filter_by_category(category: Category)` that returns a list of all products in the specified category. - Provides a method `total_inventory_value()` that returns the total value of all products in the warehouse. 4. Ensure the `Warehouse` class does not allow duplicate products (based on product name). # Input: There are no direct inputs. The task is to implement and validate the classes based on the given descriptions. # Output: For testing, you will create instances of the classes and demonstrate their functionality. # Example: ```python from enum import Enum, auto # Step 1: Define Enum class Category(Enum): ELECTRONICS = auto() FURNITURE = auto() CLOTHING = auto() FOOD = auto() TOYS = auto() # Step 2: Define Product Class class Product: def __init__(self, name: str, price: float, category: Category): self.name = name self.price = price self.category = category def __str__(self): return f\\"Product(name: {self.name}, price: {self.price}, category: {self.category.name})\\" # Step 3: Define Warehouse Class class Warehouse: def __init__(self): self.products = [] def add_product(self, product: Product): # Ensure no duplicate products based on name if any(p.name == product.name for p in self.products): return \\"Product already exists.\\" self.products.append(product) def filter_by_category(self, category: Category): return [product for product in self.products if product.category == category] def total_inventory_value(self): return sum(product.price for product in self.products) # Step 4: Test the Classes warehouse = Warehouse() product1 = Product(\\"Laptop\\", 1500.0, Category.ELECTRONICS) product2 = Product(\\"Sofa\\", 700.0, Category.FURNITURE) product3 = Product(\\"T-shirt\\", 25.0, Category.CLOTHING) warehouse.add_product(product1) warehouse.add_product(product2) warehouse.add_product(product3) print(f\\"Total Inventory Value: {warehouse.total_inventory_value()}\\") print(\\"Electronics:\\", warehouse.filter_by_category(Category.ELECTRONICS)) # Output: # Total Inventory Value: 2225.0 # Electronics: [Product(name: Laptop, price: 1500.0, category: ELECTRONICS)] ``` # Constraints: - Assume product names are unique identifiers. - Product prices will be non-negative floats. - New categories may not be added without modifying the `Category` Enum.","solution":"from enum import Enum, auto # Step 1: Define Enum class Category(Enum): ELECTRONICS = auto() FURNITURE = auto() CLOTHING = auto() FOOD = auto() TOYS = auto() # Step 2: Define Product Class class Product: def __init__(self, name: str, price: float, category: Category): self.name = name self.price = price self.category = category def __str__(self): return f\\"Product(name: {self.name}, price: {self.price}, category: {self.category.name})\\" # Step 3: Define Warehouse Class class Warehouse: def __init__(self): self.products = [] def add_product(self, product: Product): # Ensure no duplicate products based on name if any(p.name == product.name for p in self.products): return \\"Product already exists.\\" self.products.append(product) def filter_by_category(self, category: Category): return [product for product in self.products if product.category == category] def total_inventory_value(self): return sum(product.price for product in self.products)"},{"question":"You are required to use the `filecmp` module to create a function that compares two directory trees recursively. The function should generate a detailed report of the comparison, listing files that are identical, files that differ, and files that could not be compared due to errors. Function Signature ```python def compare_directories(dir1: str, dir2: str) -> dict: pass ``` Input - `dir1` (str): The path to the first directory. - `dir2` (str): The path to the second directory. Output - Returns a dictionary with the following structure: ```python { \\"identical_files\\": list, # List of file paths that are identical in both directories \\"different_files\\": list, # List of file paths that differ between the directories \\"error_files\\": list # List of file paths that could not be compared } ``` Example ```python result = compare_directories(\\"/path/to/dir1\\", \\"/path/to/dir2\\") print(result) ``` Output format: ```python { \\"identical_files\\": [\\"/path/to/dir1/file1\\", \\"/path/to/dir1/file2\\"], \\"different_files\\": [\\"/path/to/dir1/file3\\", \\"/path/to/dir2/file4\\"], \\"error_files\\": [\\"/path/to/dir1/file5\\"] } ``` Constraints - Handle any errors or exceptions that might arise due to file permissions or missing files. - Use the `filecmp` module for comparison operations. - The function should work efficiently even for directories with a large number of files. Hints - Utilize the `dircmp` class and its methods to efficiently compare the directory structures. - Recursive traversal of the directory might be needed to get a full comparison report. - You can refer to the example provided in the `filecmp` documentation to understand how to traverse and compare directories recursively. Good luck!","solution":"import filecmp import os def compare_directories(dir1: str, dir2: str) -> dict: comparison_results = { \\"identical_files\\": [], \\"different_files\\": [], \\"error_files\\": [] } def recursive_compare(dcmp): for name in dcmp.same_files: comparison_results[\\"identical_files\\"].append(os.path.join(dcmp.left, name)) for name in dcmp.diff_files: comparison_results[\\"different_files\\"].append(os.path.join(dcmp.left, name)) for name in dcmp.funny_files: comparison_results[\\"error_files\\"].append(os.path.join(dcmp.left, name)) for sub_dcmp in dcmp.subdirs.values(): recursive_compare(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) recursive_compare(dcmp) return comparison_results"},{"question":"# Question: Type-Safe Linked List Implementation Objective Create a type-safe implementation of a singly linked list in Python using the `typing` module to enforce type hints and generics. Requirements 1. Implement a generic `Node` class that can store a value of any type and a reference to the next node. 2. Implement a generic `LinkedList` class with the following methods: - `__init__`: Initializes an empty linked list. - `append`: Adds a new node with the given value to the end of the list. - `prepend`: Adds a new node with the given value to the start of the list. - `find`: Returns the first node with the specified value. - `delete`: Removes the first node with the specified value. - `__str__`: Returns a string representation of the linked list. Constraints - Use the `typing` module to ensure type safety. - The solution should handle edge cases such as operations on an empty list. Function Signatures ```python from typing import TypeVar, Generic, Optional T = TypeVar(\'T\') # Declare type variable class Node(Generic[T]): def __init__(self, value: T, next_node: Optional[\'Node[T]\'] = None) -> None: self.value = value self.next_node = next_node class LinkedList(Generic[T]): def __init__(self) -> None: self.head: Optional[Node[T]] = None def append(self, value: T) -> None: ... def prepend(self, value: T) -> None: ... def find(self, value: T) -> Optional[Node[T]]: ... def delete(self, value: T) -> bool: ... def __str__(self) -> str: ... ``` Example Usage ```python int_list = LinkedList[int]() int_list.append(1) int_list.append(2) int_list.append(3) print(int_list) # Output: 1 -> 2 -> 3 int_list.prepend(0) print(int_list) # Output: 0 -> 1 -> 2 -> 3 node = int_list.find(2) if node: print(node.value) # Output: 2 int_list.delete(1) print(int_list) # Output: 0 -> 2 -> 3 ``` Points to Evaluate - Correctness of the linked list operations. - Proper use of the `typing` module and generics. - Handling of edge cases. - Code readability and adherence to Python conventions.","solution":"from typing import TypeVar, Generic, Optional T = TypeVar(\'T\') # Declare type variable class Node(Generic[T]): def __init__(self, value: T, next_node: Optional[\'Node[T]\'] = None) -> None: self.value = value self.next_node = next_node class LinkedList(Generic[T]): def __init__(self) -> None: self.head: Optional[Node[T]] = None def append(self, value: T) -> None: new_node = Node(value) if self.head is None: self.head = new_node return current = self.head while current.next_node is not None: current = current.next_node current.next_node = new_node def prepend(self, value: T) -> None: new_node = Node(value, self.head) self.head = new_node def find(self, value: T) -> Optional[Node[T]]: current = self.head while current is not None: if current.value == value: return current current = current.next_node return None def delete(self, value: T) -> bool: if self.head is None: return False if self.head.value == value: self.head = self.head.next_node return True current = self.head while current.next_node is not None: if current.next_node.value == value: current.next_node = current.next_node.next_node return True current = current.next_node return False def __str__(self) -> str: values = [] current = self.head while current is not None: values.append(str(current.value)) current = current.next_node return \' -> \'.join(values)"},{"question":"# PyTorch Coding Assessment Question Objective: Implement a custom neural network class in PyTorch and appropriately initialize its parameters using different initialization methods from the `torch.nn.init` module. Problem Statement: Design a neural network class that includes three linear layers. `initialize_parameters` method should initialize the weights of these layers using the `torch.nn.init` functions provided. Specifically, use different initialization strategies for each layer as specified below: 1. The first layer should be initialized using `kaiming_uniform_`. 2. The second layer should be initialized using `xavier_normal_`. 3. The third layer should be initialized using `orthogonal_`. Input and Output: - **Input:** No explicit input is required. The class itself will define and initialize its layers. - **Output:** The `initialize_parameters` method should produce a neural network with layers correctly initialized as per the stated methods. Constraints: - Do not use autograd for initialization since these functions inherently run in `torch.no_grad` mode. - The network should not perform any forward or backward computations - focus is purely on initialization. Implementation: 1. Define a class `CustomNeuralNetwork` that inherits from `torch.nn.Module`. 2. In the `__init__` method, define three layers using `torch.nn.Linear`. 3. Implement `initialize_parameters` which initializes each layer’s weights using specified methods. Example Usage: ```python import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomNeuralNetwork, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.layer2 = nn.Linear(hidden_size, hidden_size) self.layer3 = nn.Linear(hidden_size, output_size) self.initialize_parameters() def initialize_parameters(self): init.kaiming_uniform_(self.layer1.weight, nonlinearity=\'relu\') init.xavier_normal_(self.layer2.weight) init.orthogonal_(self.layer3.weight) # Define the network dimensions input_size = 10 hidden_size = 5 output_size = 1 # Instantiate the network net = CustomNeuralNetwork(input_size, hidden_size, output_size) # Check the initialized weights print(net.layer1.weight) print(net.layer2.weight) print(net.layer3.weight) ``` Evaluation: Your solution will be evaluated based on: - Correct implementation of the neural network class. - Correct usage of the initialization methods from `torch.nn.init`. - Structure, readability, and clarity of your code.","solution":"import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomNeuralNetwork, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.layer2 = nn.Linear(hidden_size, hidden_size) self.layer3 = nn.Linear(hidden_size, output_size) self.initialize_parameters() def initialize_parameters(self): init.kaiming_uniform_(self.layer1.weight, nonlinearity=\'relu\') init.xavier_normal_(self.layer2.weight) init.orthogonal_(self.layer3.weight)"},{"question":"# CSV Data Processing and Transformation Objective: To assess your understanding of the Python `csv` module, you are required to implement functions to read, process, and write CSV data. Task Description: You will implement two main functions: 1. `read_and_process_csv(input_file: str) -> List[Dict[str, Any]]` 2. `write_csv(output_file: str, data: List[Dict[str, Any]]) -> None` **Function 1: read_and_process_csv** - This function reads a CSV file and processes the data. - The input file will have the following fields: `name`, `age`, and `city`. - You are required to: - Read the CSV file using `csv.DictReader`. - Convert the `age` field to an integer. - Capitalize the `city` field. - Return a list of dictionaries representing the processed data. ```python def read_and_process_csv(input_file: str) -> List[Dict[str, Any]]: Reads and processes a CSV file. Args: - input_file (str): The path to the input CSV file. Returns: - List[Dict[str, Any]]: A list of dictionaries representing the processed data. pass ``` **Function 2: write_csv** - This function writes the processed data to a CSV file. - It should write the data as-is without any further transformations. - Ensure that the output CSV file has the following header fields: `name`, `age`, and `city`. - Use `csv.DictWriter` for writing the data. ```python def write_csv(output_file: str, data: List[Dict[str, Any]]) -> None: Writes the processed data to a CSV file. Args: - output_file (str): The path to the output CSV file. - data (List[Dict[str, Any]]): The processed data. pass ``` Constraints: - Do not use any libraries other than Python\'s standard library. - Ensure proper error handling for file read/write operations. - Validate that the `age` field is a valid integer. If not, set the age to `0`. Example: Input CSV file (`input.csv`): ``` name,age,city Alice,30,new york Bob,25,san francisco Charlie,twenty-five,los angeles ``` Output CSV file (`output.csv`): ``` name,age,city Alice,30,NEW YORK Bob,25,SAN FRANCISCO Charlie,0,LOS ANGELES ``` Include the necessary imports and make sure your code is well-documented and clean.","solution":"import csv from typing import List, Dict, Any def read_and_process_csv(input_file: str) -> List[Dict[str, Any]]: Reads and processes a CSV file. Args: - input_file (str): The path to the input CSV file. Returns: - List[Dict[str, Any]]: A list of dictionaries representing the processed data. processed_data = [] with open(input_file, mode=\'r\', newline=\'\') as file: csv_reader = csv.DictReader(file) for row in csv_reader: processed_row = { \'name\': row[\'name\'], \'age\': int(row[\'age\']) if row[\'age\'].isdigit() else 0, \'city\': row[\'city\'].upper() } processed_data.append(processed_row) return processed_data def write_csv(output_file: str, data: List[Dict[str, Any]]) -> None: Writes the processed data to a CSV file. Args: - output_file (str): The path to the output CSV file. - data (List[Dict[str, Any]]): The processed data. with open(output_file, mode=\'w\', newline=\'\') as file: fieldnames = [\'name\', \'age\', \'city\'] csv_writer = csv.DictWriter(file, fieldnames=fieldnames) csv_writer.writeheader() for row in data: csv_writer.writerow(row)"},{"question":"# Python Coding Assessment **Objective**: Test your understanding and ability to utilize Python\'s `collections` module. **Problem Description**: The `collections` module in Python provides alternatives to Python\'s general-purpose built-in containers like `dict`, `list`, `set`, and `tuple`. One of the useful data structures within this module is the `deque` (double-ended queue), which allows you to append and pop elements from both ends efficiently. Create a class `DequeOperations` that simulates certain operations on a `deque` which will be defined by a series of commands. Each command tells your class to perform a specific action. **Functions to Implement**: 1. `__init__(self)`: Initialize an empty deque. 2. `perform_operations(self, operations: List[str]) -> List[Any]`: Perform a series of operations on the deque and return the results of operations that produce a result. **Input**: - `operations`: A list of command strings. Each command string can be one of the following: - `\\"append <value>\\"`: Append `<value>` to the right end of the deque. - `\\"appendleft <value>\\"`: Append `<value>` to the left end of the deque. - `\\"pop\\"`: Pop an element from the right end of the deque. This operation will return the popped element. - `\\"popleft\\"`: Pop an element from the left end of the deque. This operation will return the popped element. - `\\"peek\\"`: Return the element at the right end without popping it. - `\\"peekleft\\"`: Return the element at the left end without popping it. **Output**: - A list of results from operations that produce an output (`pop`, `popleft`, `peek`, `peekleft`). **Constraints**: - You can assume the operations are valid and appropriately spaced by single spaces. - For any `pop` or `popleft` operations, the deque will not be empty when these operations are called. **Example**: ```python # Initialize the operations list operations = [\\"append 1\\", \\"append 2\\", \\"appendleft 0\\", \\"peek\\", \\"peekleft\\", \\"pop\\", \\"popleft\\"] # Create an instance of DequeOperations and perform the operations deque_operations = DequeOperations() output = deque_operations.perform_operations(operations) # Expected Output: [2, 0, 2, 0] print(output) ``` Implement the `DequeOperations` class in Python to pass the above scenarios. This will test your ability to work with the `collections.deque` and implement functionality using Python standard libraries effectively. **Note**: Make sure to import `collections.deque` in your implementation.","solution":"from collections import deque class DequeOperations: def __init__(self): self.deque = deque() def perform_operations(self, operations): results = [] for operation in operations: parts = operation.split() command = parts[0] if command == \\"append\\": value = parts[1] self.deque.append(value) elif command == \\"appendleft\\": value = parts[1] self.deque.appendleft(value) elif command == \\"pop\\": results.append(self.deque.pop()) elif command == \\"popleft\\": results.append(self.deque.popleft()) elif command == \\"peek\\": results.append(self.deque[-1]) elif command == \\"peekleft\\": results.append(self.deque[0]) return results"},{"question":"You have been provided with a dataset called `seaice` which contains two columns: - **Date**: The dates of observations. - **Extent**: The sea ice extent measured on those dates. Your task is to write a function `plot_seaice_extent_with_facet` that performs the following steps: 1. **Load the seaborn package and the seaice dataset.** 2. **Create a faceted line plot** that shows the `Extent` over time (`Date`). Each facet should represent a different decade. For instance, the 1980s, 1990s, 2000s, and so on. 3. **Color the lines** according to the year of the observation within each facet. 4. **Customize the appearance** of the lines: - Set the linewidth to 0.5 and semi-transparent color before adding the main lines. - Add a second layer of lines with a linewidth of 1. 5. **Scale the colors** to improve readability. 6. **Set the layout size** of the plot to (8, 4). 7. **Label the title** of each facet with the corresponding decade. # Expected Function Signature ```python def plot_seaice_extent_with_facet(): pass ``` # Constraints and Requirements - Use the seaborn objects interface, particularly `so.Plot` and `so.Lines`. - Facet the data by decade using the `.facet()` method. - Ensure each line within a facet is colored by the year. - The plot should be clean and thoroughly labeled with readable, appropriately scaled colors. # Example Use and Output The function does not need to return anything. It should display the faceted line plot as described above when executed. You can assume that the `load_dataset(\\"seaice\\")` function is available to load the data. Here is what the beginning of your function might look like: ```python import seaborn.objects as so from seaborn import load_dataset def plot_seaice_extent_with_facet(): seaice = load_dataset(\\"seaice\\") ... ``` Good luck, and make sure to test your solution thoroughly!","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def plot_seaice_extent_with_facet(): # Load the dataset seaice = load_dataset(\\"seaice\\") # Ensure the Date column is a datetime object seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) # Create a new column for decades seaice[\'Decade\'] = (seaice[\'Date\'].dt.year // 10) * 10 # Create a new column for the year seaice[\'Year\'] = seaice[\'Date\'].dt.year # Create the plot p = so.Plot(seaice, y=\\"Extent\\", x=\\"Date\\", color=\\"Year\\") # Add lines to the plot with different linewidths and transparency p = p.add(so.Line(linewidth=0.5, alpha=0.5)) .add(so.Line(linewidth=1)) # Facet the plot by Decade and customize the layout p = p.facet(\\"Decade\\").scale(color=\\"viridis\\").layout(size=(8, 4)).label(title=\\"{value}s\\") # Show the plot p.show()"},{"question":"# Python asyncio Assessment Question **Objective:** Write an asyncio-based Python program to efficiently manage multiple concurrent tasks, handle cancellations, and manage timeouts. # Problem Statement You are tasked with developing a program that simulates a simplified version of task manager software using Python\'s asyncio library. The program should create several tasks, run them concurrently, manage timeouts for operations, and handle cancellations gracefully. **Requirements:** 1. Create an asyncio coroutine named `perform_task` that simulates performing a task. The task takes two parameters: an identifier (string) and a duration (integer) in seconds. The coroutine should print a message when it starts, sleeps for the given duration, and then prints a completion message. 2. Create an asyncio coroutine named `main` that: - Launches several `perform_task` coroutines concurrently using `asyncio.create_task` for tasks with varying durations. - Uses `asyncio.gather` to wait for the completion of all tasks, with a total timeout of 10 seconds. If the timeout is reached, cancel all running tasks and print a timeout message. - Ensures that the program handles cancellations and timeout errors gracefully. Use appropriate mechanisms to ensure that any cancelled tasks are properly reported. 3. Ensure proper isolation of these tasks such that a cancellation of one task does not affect the remaining tasks. **Input and Output:** - There are no specific inputs to the program. You can hardcode the task identifiers and durations within the `main` coroutine. - The program should output the start and completion of each task, and any timeout or cancellation messages. # Constraints: - All coroutines and tasks should be defined and managed using the asyncio library. - Tasks might be of varying durations, but the total run time should be limited to a maximum of 10 seconds. # Performance Requirements: - The solution should efficiently handle concurrent task execution and appropriately manage task timeouts and cancellations. # Example: ```python import asyncio async def perform_task(identifier, duration): try: print(f\'Task {identifier} started, will take {duration} seconds\') await asyncio.sleep(duration) print(f\'Task {identifier} completed\') except asyncio.CancelledError: print(f\'Task {identifier} was cancelled\') async def main(): tasks = [ asyncio.create_task(perform_task(\'Task1\', 5)), asyncio.create_task(perform_task(\'Task2\', 3)), asyncio.create_task(perform_task(\'Task3\', 8)), asyncio.create_task(perform_task(\'Task4\', 12)), ] try: await asyncio.gather(*tasks, return_exceptions=False) except asyncio.TimeoutError: for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'Timeout! Some tasks were cancelled after 10 seconds\') asyncio.run(main()) ``` # Notes: - The use of try-except blocks ensures that tasks handle Cancellation properly. - This program provides a clear idea of concurrency control, managing the maximum time for execution, and handling exceptions appropriately.","solution":"import asyncio async def perform_task(identifier, duration): try: print(f\'Task {identifier} started, will take {duration} seconds\') await asyncio.sleep(duration) print(f\'Task {identifier} completed\') except asyncio.CancelledError: print(f\'Task {identifier} was cancelled\') async def main(): tasks = [ asyncio.create_task(perform_task(\'Task1\', 5)), asyncio.create_task(perform_task(\'Task2\', 3)), asyncio.create_task(perform_task(\'Task3\', 8)), asyncio.create_task(perform_task(\'Task4\', 12)), ] try: await asyncio.wait_for(asyncio.gather(*tasks), timeout=10) except asyncio.TimeoutError: for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'Timeout! Some tasks were cancelled after 10 seconds\') asyncio.run(main())"},{"question":"**Objective:** To assess your understanding of the seaborn library and its new high-level plotting interface, you are required to write code that loads a dataset, transforms the data, and visualizes it using various plot types. **Requirements:** 1. Load the `fmri` dataset from seaborn. 2. Filter the dataset to only include the `parietal` region. 3. Create a plot that shows: - A line representing the average signal over time, grouped by the event type. - A band representing the variation (min, max) of the signals over time. **Detailed Steps:** 1. **Load the Dataset:** Use `seaborn.load_dataset` to load the `fmri` dataset. 2. **Data Transformation:** - Filter the dataset to only include the rows where the region is `parietal`. - Ensure the dataset is ready for plotting by checking that necessary columns (e.g., `timepoint`, `signal`, `event`) are correctly formatted. 3. **Plotting:** - Create a plot object using `seaborn.objects` (aliased as `so`). - Use `so.Line` to plot the average signal over time for each event type. - Use `so.Band` to add a band showing the min and max signals over time. - Customize the plot to make it visually informative (e.g., by adding appropriate labels, title, and color differentiation). **Expected Input:** No specific input, as the dataset is loaded within the code. **Expected Output:** A plot showing the average signal with a band overlay, grouped by event type and over time. **Constraints:** - Ensure your code is optimized and doesn\'t include unnecessary steps. - Use proper seaborn and pandas methods to filter and transform the data. - Clearly label the plot for better understanding. **Performance Considerations:** The dataset is relatively small, so focus on code clarity and correctness. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset fmri = load_dataset(\\"fmri\\") # Transform data fmri_filtered = fmri[fmri[\'region\'] == \'parietal\'] # Create plot plot = ( so.Plot(fmri_filtered, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) plot.show() ``` Ensure that your code runs and produces the correct plot as described.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def load_and_transform_data(): # Load dataset fmri = load_dataset(\\"fmri\\") # Filter dataset to only include \'parietal\' region fmri_filtered = fmri[fmri[\'region\'] == \'parietal\'] return fmri_filtered def create_plot(fmri_filtered): # Create plot plot = ( so.Plot(fmri_filtered, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) return plot"},{"question":"Objective Write a Python function that performs a series of file and directory manipulations. This task will test your understanding of the `pathlib`, `os.path`, `shutil`, and `tempfile` modules. Problem Statement Create a function named `organize_files` that takes in a single argument `source_dir`, which is a string representing the path to a directory. The function should perform the following steps: 1. **Create Temporary Directory**: Create a temporary directory using `tempfile.TemporaryDirectory()` to store intermediate files. 2. **List All Files**: List all files in the `source_dir` (including those in its subdirectories) using the `pathlib` module. 3. **Copy and Organize Files by Extension**: - For each file in the `source_dir`, copy it to the temporary directory, organizing them into subdirectories based on their file extensions (e.g., all `.txt` files go into a directory named `txt`). 4. **Create a Summary File**: In the `source_dir`, create a summary file named `summary.txt` that contains the count of files for each file extension. 5. **Move Organized Files to Source Directory**: - Move all the subdirectories from the temporary directory back to the `source_dir`. Input - `source_dir` (str): The path to the source directory. Output - The function should not return anything. Instead, it should perform the operations described above. Example ```python import os # Creating a sample directory for demonstration os.makedirs(\\"test_source\\", exist_ok=True) with open(\\"test_source/file1.txt\\", \\"w\\") as f: f.write(\\"Sample text file 1\\") with open(\\"test_source/file2.txt\\", \\"w\\") as f: f.write(\\"Sample text file 2\\") with open(\\"test_source/image1.jpg\\", \\"w\\") as f: f.write(\\"Sample image file 1\\") with open(\\"test_source/image2.png\\", \\"w\\") as f: f.write(\\"Sample image file 2\\") organize_files(\\"test_source\\") # After running the function, the structure of `test_source`: # test_source/ # ├── txt/ # │ ├── file1.txt # │ └── file2.txt # ├── jpg/ # │ └── image1.jpg # ├── png/ # │ └── image2.png # └── summary.txt (Content: \\"txt: 2njpg: 1npng: 1n\\") ``` Constraints - You must use the `pathlib` and `tempfile` modules. - Use appropriate methods from `shutil` for file operations. Performance Requirements - The function should handle directories with up to 10,000 files efficiently. Notes - Ensure you handle different cases where file names may have the same extension but different cases (e.g., `.TXT` and `.txt` should be treated the same). - You can assume that the `source_dir` exists and is accessible.","solution":"import os from pathlib import Path import shutil import tempfile def organize_files(source_dir): source_path = Path(source_dir) with tempfile.TemporaryDirectory() as temp_dir: temp_path = Path(temp_dir) # Dictionary to hold counts of each extension file_counts = {} # List all files in source_dir recursively for file_path in source_path.rglob(\'*.*\'): if file_path.is_file(): extension = file_path.suffix[1:].lower() # Get file extension without dot and in lower case extension_dir = temp_path / extension # Create extension directory if it doesn\'t exist extension_dir.mkdir(exist_ok=True) # Copy file to temporary directory shutil.copy(file_path, extension_dir / file_path.name) # Update file_counts dictionary file_counts[extension] = file_counts.get(extension, 0) + 1 # Write summary file in source_dir summary_file_path = source_path / \'summary.txt\' with open(summary_file_path, \'w\') as summary_file: for ext, count in file_counts.items(): summary_file.write(f\\"{ext}: {count}n\\") # Move organized files back to source_dir for extension_dir in temp_path.iterdir(): if extension_dir.is_dir(): shutil.move(str(extension_dir), str(source_path / extension_dir.name))"},{"question":"Objective: Demonstrate understanding and utilization of the `queue` module in Python for managing multi-producer, multi-consumer scenarios in a thread-safe manner. Problem Statement: You are required to implement a multi-threaded system that processes tasks using different types of queues provided by the `queue` module. Your task is to create a threaded environment where multiple producer threads add tasks to a queue and multiple consumer threads process these tasks. Specifically, you need to: 1. Create two types of queues: a `PriorityQueue` (for high-priority tasks) and a `Queue` (for regular tasks). 2. Implement multiple producer threads that randomly generate tasks of varying priority and add them to the appropriate queue. Tasks with priority less than 5 should go into the `PriorityQueue`, and others into the `Queue`. 3. Implement multiple consumer threads that continuously process tasks from either queue, prioritizing tasks from the `PriorityQueue`. 4. Ensure proper thread synchronization, data integrity, and thread safety using the `queue` module. Requirements: 1. Implement the producer function to create tasks with random priorities. 2. Implement the consumer function to process tasks with priority handling. 3. Set up and start multiple producer and consumer threads. 4. Use `task_done` and `join` methods to ensure all tasks are completed before the program ends. Constraints: - **Queues**: Use `queue.PriorityQueue` for high-priority tasks and `queue.Queue` for regular tasks. - **Producers**: At least 3 producer threads creating tasks every 0.1 to 0.5 seconds. - **Consumers**: At least 3 consumer threads processing tasks. Input: No direct input from the user. Tasks are generated randomly by producer threads. Output: Print statements detailing the task creation and processing for both high-priority and regular tasks. Example: ```python import threading import queue import random import time # Define the number of producers and consumers NUM_PRODUCERS = 3 NUM_CONSUMERS = 3 # Task creation function for producers def producer(pq, q): while True: priority = random.randint(1, 10) task = f\'Task with priority {priority}\' if priority < 5: pq.put((priority, task)) print(f\'Produced high-priority task: {task}\') else: q.put(task) print(f\'Produced regular task: {task}\') time.sleep(random.uniform(0.1, 0.5)) # Task processing function for consumers def consumer(pq, q): while True: try: if not pq.empty(): priority, task = pq.get() print(f\'Processing high-priority task: {task}\') pq.task_done() elif not q.empty(): task = q.get() print(f\'Processing regular task: {task}\') q.task_done() else: time.sleep(0.05) except queue.Empty: pass # Main function to start the producers and consumers if __name__ == \'__main__\': priority_queue = queue.PriorityQueue() regular_queue = queue.Queue() # Starting producer threads for _ in range(NUM_PRODUCERS): threading.Thread(target=producer, args=(priority_queue, regular_queue), daemon=True).start() # Starting consumer threads for _ in range(NUM_CONSUMERS): threading.Thread(target=consumer, args=(priority_queue, regular_queue), daemon=True).start() # Wait for all tasks to be processed try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Program terminated.\\") ``` Implement the above system, ensuring that all produced tasks are processed efficiently and thread safety is maintained throughout the program execution.","solution":"import threading import queue import random import time # Define the number of producers and consumers NUM_PRODUCERS = 3 NUM_CONSUMERS = 3 # Task creation function for producers def producer(pq, q): while True: priority = random.randint(1, 10) task = f\'Task with priority {priority}\' if priority < 5: pq.put((priority, task)) print(f\'Produced high-priority task: {task}\') else: q.put(task) print(f\'Produced regular task: {task}\') time.sleep(random.uniform(0.1, 0.5)) # Task processing function for consumers def consumer(pq, q): while True: if not pq.empty(): priority, task = pq.get() print(f\'Processing high-priority task: {task}\') pq.task_done() elif not q.empty(): task = q.get() print(f\'Processing regular task: {task}\') q.task_done() # Main function to start the producers and consumers def start_system(): priority_queue = queue.PriorityQueue() regular_queue = queue.Queue() # Starting producer threads for _ in range(NUM_PRODUCERS): threading.Thread(target=producer, args=(priority_queue, regular_queue), daemon=True).start() # Starting consumer threads for _ in range(NUM_CONSUMERS): threading.Thread(target=consumer, args=(priority_queue, regular_queue), daemon=True).start() # Wait for all tasks to be processed try: while True: if priority_queue.empty() and regular_queue.empty(): break time.sleep(1) except KeyboardInterrupt: print(\\"Program terminated.\\") if __name__ == \'__main__\': start_system()"},{"question":"**Question: Creating and Testing a Custom PyTorch Operator** You are tasked with creating a custom PyTorch operator that calculates the sum of squares of a tensor\'s elements. Once the operator is created, you need to implement gradient support and verify the implementation correctness, including gradient correctness. # Part A: Creating the Custom Operator 1. Implement a custom operator named `sum_of_squares` that takes a tensor and returns the sum of squares of its elements. 2. Register the operator using `torch.library.custom_op`. # Part B: Implementing Gradient Support 1. Extend the `sum_of_squares` operator to support gradient computation. 2. Register the gradient implementation using `torch.library.register_autograd`. # Part C: Testing the Custom Operator 1. Use `torch.library.opcheck` to test the operator for incorrect usage. 2. Implement a test to verify the gradient correctness using `torch.autograd.gradcheck`. # Constraints - The input to the `sum_of_squares` operator is a single tensor of type `torch.Tensor`. - The output is a single scalar of type `torch.Tensor`. - Use PyTorch\'s built-in utilities to ensure that your custom operator handles tensors on both CPU and CUDA devices correctly. # Performance Requirements - The implementation should efficiently handle large tensors (e.g., with elements in the order of millions). - Ensure that the operator is correctly registered and works within the PyTorch framework without performance bottlenecks. # Expected Input and Output - Input: A PyTorch tensor. - Output: A scalar representing the sum of squares of the input tensor elements. # Example ```python import torch # Assume sum_of_squares has been correctly registered and implemented x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) output = sum_of_squares(x) # Expected output: tensor(14.0) # Since 1^2 + 2^2 + 3^2 = 14 print(output) output.backward() # Expected gradient: tensor([2.0, 4.0, 6.0]) # Since the gradient of sum_of_squares(x) with respect to x_i is 2 * x_i print(x.grad) ``` # Submission Please submit a single Python script that includes: 1. Implementation of the custom operator `sum_of_squares`. 2. Gradient support for the custom operator. 3. Test cases to validate both the operator and its gradient.","solution":"import torch from torch.autograd import Function class SumOfSquares(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return torch.sum(input ** 2) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors return grad_output * 2 * input def sum_of_squares(input): return SumOfSquares.apply(input)"},{"question":"**Objective**: Implement a function that reads configuration data from multiple sources and provides access to the configuration values in an organized manner. The function should also support customizing the behavior of the parser. Problem Statement: Write a function called `load_configurations` that accepts the following parameters: 1. A list of file paths containing configuration data. 2. A dictionary of default values to be used if certain configurations are not specified. 3. A boolean flag `strict` to determine if duplicate sections or options should raise errors. 4. A boolean flag `allow_no_value` to indicate if options without values should be accepted. The function should: 1. Read and parse configuration data from the provided file paths. 2. Merge configurations, giving priority to configurations from later files in the list. 3. Provide a means to fetch configuration values with support for default and fallback values. 4. Customize parser behavior based on the `strict` and `allow_no_value` flags. Function Signature: ```python def load_configurations(file_paths: list, default_values: dict, strict: bool = True, allow_no_value: bool = False) -> configparser.ConfigParser: pass ``` Input: * `file_paths` (list): A list of file paths (strings) containing configuration data. * `default_values` (dict): A dictionary of default configuration values. * `strict` (bool): Flag to determine if duplicate sections or options should raise errors. * `allow_no_value` (bool): Flag to indicate if options without values should be accepted. Output: * Returns an instance of `configparser.ConfigParser` with combined configurations from the provided file paths. Example Usage: ```python file_paths = [\'config1.ini\', \'config2.ini\'] default_values = {\'ServerAliveInterval\': \'60\', \'Compression\': \'no\'} config = load_configurations(file_paths, default_values, strict=True, allow_no_value=False) # Fetching values print(config.get(\'DEFAULT\', \'ServerAliveInterval\')) # Should output \'60\' unless overridden in the files print(config.get(\'DEFAULT\', \'Compression\')) # Should output \'no\' unless overridden in the files print(config.get(\'some_section\', \'some_key\')) # Fetch a specific configuration value ``` Constraints: * Ensure that the files specified in `file_paths` exist. Handle cases where files might be missing by ignoring those files. * Ensure that all configuration values are strings, and other data types should be appropriately converted using provided getters. Performance Requirements: * The function should efficiently handle a large number of configuration files and large configuration files. Additional Notes: * Avoid using any external libraries except the standard `configparser` module. * Provide appropriate handling for interpolation of values within the configuration files.","solution":"import configparser import os def load_configurations(file_paths: list, default_values: dict, strict: bool = True, allow_no_value: bool = False) -> configparser.ConfigParser: Load configuration data from multiple files with the option to provide default values. Args: file_paths (list): A list of file paths containing configuration data. default_values (dict): A dictionary of default configuration values. strict (bool): Flag to determine if duplicate sections or options should raise errors. allow_no_value (bool): Flag to indicate if options without values should be accepted. Returns: configparser.ConfigParser: Configurations merged from the provided file paths. config = configparser.ConfigParser(strict=strict, allow_no_value=allow_no_value) # Load default values if default_values: config.read_dict({\'DEFAULT\': default_values}) # Read configuration from files for file_path in file_paths: if os.path.exists(file_path): config.read(file_path) return config"},{"question":"# Creating and Customizing a Source Distribution You are tasked with creating a source distribution for a Python package that includes a specific set of files. Your job is to write a Python script that generates this distribution using the **distutils** library, customizing it based on certain criteria specified below. Requirements: 1. Create a source distribution of a Python project using `setup.py`. 2. Ensure the distribution includes all `.py` files in the project, but excludes any files or directories that start with `_` (e.g., `_private.py`, `_hidden/`). 3. The distribution should be created in both `zip` and `gztar` formats. 4. The root owner and group for all files in the archive should be set to `root`. Input and Output Format: - **Input**: - Structure of the project as a dictionary. - A string name for the project. ```python project_structure = { \\"setup.py\\": \\"content of setup.py\\", \\"main.py\\": \\"content of main.py\\", \\"_private.py\\": \\"content of hidden.py\\", \\"utils/\\": { \\"__init__.py\\": \\"content of __init__.py\\", \\"tools.py\\": \\"content of tools.py\\", \\"_hidden/\\": { \\"secret.py\\": \\"content of secret.py\\" } }, \\"README.txt\\": \\"This is a readme file.\\" } project_name = \\"my_project\\" ``` - **Output**: - The script should generate a source distribution in `zip` and `gztar` formats, placed in the `dist/` folder of the project directory. Constraints: - You should not include any file or directory starting with `_` in the distribution. - Ensure the ownership details of the files within the archive are set to root. Implementation Details: 1. Write a Python function `create_distribution(project_structure: dict, project_name: str) -> None`. 2. This function should: - Create the necessary project structure on the filesystem. - Generate the `MANIFEST.in` file to include/exclude the appropriate files. - Execute the `setup.py sdist` command with `--formats=zip,gztar` and `--owner=root --group=root` to create the distribution. Example: Given the above input, the function should create the following files in a temporary directory: - `setup.py` - `main.py` - `utils/__init__.py` - `utils/tools.py` - `README.txt` And then execute the `setup.py sdist --formats=zip,gztar --owner=root --group=root` to create the distribution archives in the `dist/` directory. ```python def create_distribution(project_structure: dict, project_name: str) -> None: import os import shutil from distutils.core import run_setup # Helper function to create project structure def create_files(base_path, structure): for name, content in structure.items(): path = os.path.join(base_path, name) if isinstance(content, dict): os.makedirs(path, exist_ok=True) create_files(path, content) else: with open(path, \'w\') as file: file.write(content) # Create temporary directory for the project temp_dir = os.path.join(os.getcwd(), project_name) os.makedirs(temp_dir, exist_ok=True) # Generate project structure create_files(temp_dir, project_structure) # Create MANIFEST.in file for includes/excludes manifest_content = include *.py *.txt recursive-exclude * _* with open(os.path.join(temp_dir, \'MANIFEST.in\'), \'w\') as manifest_file: manifest_file.write(manifest_content) # Change to project directory os.chdir(temp_dir) # Run setup.py sdist command os.system(f\'python setup.py sdist --formats=zip,gztar --owner=root --group=root\') # Cleanup: Returning to original directory os.chdir(\'..\') shutil.rmtree(temp_dir) ```","solution":"def create_distribution(project_structure: dict, project_name: str) -> None: import os import shutil from distutils.core import run_setup # Helper function to create project structure def create_files(base_path, structure): for name, content in structure.items(): path = os.path.join(base_path, name) if isinstance(content, dict): os.makedirs(path, exist_ok=True) create_files(path, content) else: with open(path, \'w\') as file: file.write(content) # Create temporary directory for the project temp_dir = os.path.join(os.getcwd(), project_name) os.makedirs(temp_dir, exist_ok=True) # Generate project structure create_files(temp_dir, project_structure) # Create MANIFEST.in file for includes/excludes manifest_content = include *.py *.txt recursive-exclude * _* recursive-include * *.py with open(os.path.join(temp_dir, \'MANIFEST.in\'), \'w\') as manifest_file: manifest_file.write(manifest_content) # Change to project directory os.chdir(temp_dir) # Run setup.py sdist command os.system(f\'python setup.py sdist --formats=zip,gztar --owner=root --group=root\') # Cleanup: Returning to original directory os.chdir(\'..\') # Move produced distribution files from the temporary directory to the current directory\'s `dist` folder dist_dest = os.path.join(os.getcwd(), \'dist\') os.makedirs(dist_dest, exist_ok=True) for item in os.listdir(os.path.join(temp_dir, \'dist\')): s = os.path.join(temp_dir, \'dist\', item) d = os.path.join(dist_dest, item) shutil.move(s, d) # Remove the temporary directory shutil.rmtree(temp_dir)"},{"question":"# Advanced Python Function Implementation **Problem Statement:** You are required to implement a Python function `detect_prime_numbers` that analyzes a list of integers and identifies which of them are prime numbers. Additionally, the function should support customization for: - Number filtering constraints (e.g., filtering out even numbers before prime checking) - Custom messages for output for both prime and non-prime cases. The function should follow these specifications: 1. **Function Definition:** ```python def detect_prime_numbers(numbers: list, filter_fn=None, prime_message=\\"{} is a prime number\\", non_prime_message=\\"{} is not a prime number\\") -> list: ``` - `numbers`: A list of integers to be analyzed. - `filter_fn`: A lambda or function to filter numbers before checking for primes (default is `None`, implying no filtering). - `prime_message`: A formatted string for prime number messages (default: `\\"{} is a prime number\\"`). - `non_prime_message`: A formatted string for non-prime number messages (default: `\\"{} is not a prime number\\"`). 2. **Function Requirements:** - Apply `filter_fn` to `numbers` if provided. - Determine if each number in the filtered list is prime or not. - Return a list of formatted messages for each number based on whether it is prime or non-prime. 3. **Constraints:** - Numbers will be given as integers in the range of `1` to `10,000`. - Filtering should be optional, and prime checks must only happen after filtering. 4. **Performance Consideration:** - Ensure efficient prime checking to handle the upper limits of the constraints. **Example Usage:** ```python # Example filter function to exclude even numbers filter_even = lambda x: x % 2 != 0 # Calling the function without any filter print(detect_prime_numbers([2, 3, 4, 5, 6])) # Expected Output: # [\'2 is a prime number\', \'3 is a prime number\', \'4 is not a prime number\', \'5 is a prime number\', \'6 is not a prime number\'] # Calling the function with the filter_even lambda print(detect_prime_numbers([2, 3, 4, 5, 6], filter_fn=filter_even)) # Expected Output: # [\'3 is a prime number\', \'5 is a prime number\'] ``` **Note:** - Use sensible decomposition of tasks in helper functions where necessary. - Write docstrings for each function, following PEP 8 guidelines.","solution":"def is_prime(n): Check if a number is a prime. if n <= 1: return False if n == 2: return True if n % 2 == 0: return False p = 3 while p * p <= n: if n % p == 0: return False p += 2 return True def detect_prime_numbers(numbers: list, filter_fn=None, prime_message=\\"{} is a prime number\\", non_prime_message=\\"{} is not a prime number\\") -> list: Analyze a list of integers and identify which are prime numbers with optional filtering. :param numbers: List of integers to analyze :param filter_fn: Function to filter numbers before checking for primes (default None) :param prime_message: Format string for prime numbers (default \\"{} is a prime number\\") :param non_prime_message: Format string for non-prime numbers (default \\"{} is not a prime number\\") :return: List of formatted messages for each number if filter_fn: numbers = filter(filter_fn, numbers) result = [] for number in numbers: if is_prime(number): result.append(prime_message.format(number)) else: result.append(non_prime_message.format(number)) return result"},{"question":"Objective Demonstrate your understanding of the `runpy` module by implementing a function that dynamically executes a specified module twice. The first execution should run with default settings, and the second execution should run with an altered global namespace. Return the results of both executions. Function Signature ```python def execute_twice(module_name: str) -> tuple: pass ``` Input - `module_name` (str): The absolute name of the module to be executed. Output - A tuple containing two dictionaries: - The first dictionary represents the globals dictionary of the module executed with default settings. - The second dictionary represents the globals dictionary of the module executed with a pre-populated global namespace. Constraints - The module specified in `module_name` must exist and should be executable. - You may assume that any modifications to the `sys` module during execution do not need to be thread-safe. Example Usage ```python result1, result2 = execute_twice(\\"some_module\\") print(result1) # Output the globals dictionary from the first execution print(result2) # Output the globals dictionary from the second execution with a pre-populated global namespace ``` Notes - Use the `runpy.run_module` function from the `runpy` module to execute the specified module. - The pre-populated global namespace for the second execution should include an additional variable, e.g., `{\'custom_variable\': \'test_value\'}`. - Ensure that the first execution does not alter the state for the second execution. By completing this task, you will demonstrate your ability to dynamically execute Python code using the `runpy` module and manipulate the global namespace of the executed module.","solution":"import runpy def execute_twice(module_name: str) -> tuple: Executes the specified module twice with different global namespaces. Parameters: - module_name (str): The absolute name of the module to be executed. Returns: - tuple: A tuple containing the globals dictionary from both executions. # First execution with default settings result1 = runpy.run_module(module_name, run_name=\\"__main__\\", alter_sys=True) # Second execution with an altered global namespace custom_globals = {\\"custom_variable\\": \\"test_value\\"} result2 = runpy.run_module(module_name, init_globals=custom_globals, run_name=\\"__main__\\", alter_sys=True) return result1, result2"},{"question":"# Question: Creating and Debugging TorchScript Models Description In this task, you will create a PyTorch model, convert it to TorchScript, and perform some debugging operations. This exercise will test your ability to work with TorchScript, inspect generated code and graphs, and handle attributes and constants accordingly. Instructions 1. **Create a PyTorch Model:** - Implement a PyTorch neural network model in a class `MyModel` that contains a couple of `nn.Conv2d` layers and a `forward` method that uses `torch.nn.functional.relu`. 2. **Convert to TorchScript:** - Convert the PyTorch `MyModel` instance to a TorchScript module using `torch.jit.script`. 3. **Handle Attributes and Constants:** - Extend the `MyModel` class to include an attribute and a constant. Use the appropriate annotations to ensure TorchScript can process them correctly. 4. **Debug TorchScript Code:** - Inspect and print the generated TorchScript code and the IR graph. 5. **Mix Tracing and Scripting:** - Add a method `foo` to `MyModel` that performs a simple tensor operation and demonstrate calling this method from within the scripted `forward` method, ensuring to mix tracing and scripting where applicable. Constraints - You must properly handle attributes and constants according to TorchScript handling. - Provide clear and readable prints for the generated TorchScript code and IR graphs. - The conversion of the model should handle both scripting and tracing appropriately. Expected Input and Output - **Input:** None (the task involves constructing, converting, and analyzing a model internally). - **Output:** Print statements within your functions will suffice to demonstrate the conversion and debugging process. Skeleton Code ```python import torch import torch.nn as nn import torch.nn.functional as F class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) self.attribute = None # Add your attribute self.constant = None # Add your constant def forward(self, x): # Mix tracing and scripting x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) def foo(self, x): # Simple tensor operation return x * 2 # Convert MyModel to TorchScript model = MyModel() scripted_model = torch.jit.script(model) # Inspect and print TorchScript code and IR graph print(scripted_model.code) print(scripted_model.graph) # Test functionality with sample input input_tensor = torch.rand(1, 1, 32, 32) output = scripted_model(input_tensor) print(output) ``` Complete the skeleton code to meet all the requirements of the task.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) self.attribute = 10 # Example attribute self.constant = 5.0 # Example constant def forward(self, x): x = F.relu(self.conv1(x)) x = self.foo(x) # Call additional method in the forward pass return F.relu(self.conv2(x)) def foo(self, x): # Simple tensor operation return x * self.constant # Convert MyModel to TorchScript model = MyModel() scripted_model = torch.jit.script(model) # Inspect and print TorchScript code and IR graph print(scripted_model.code) print(scripted_model.graph) # Test functionality with sample input input_tensor = torch.rand(1, 1, 32, 32) output = scripted_model(input_tensor) print(output)"},{"question":"The `imp` module has been deprecated since Python 3.4 in favor of the `importlib` module. To ensure compatibility with legacy code and smooth transitions to modern Python practices, you need to write a wrapper that translates `imp` functions into their modern `importlib` counterparts. Task: Implement a Python function `load_module(module_name: str) -> object` that uses `importlib` to replicate the functionality of `imp.load_module()`. Your function should: 1. Find the specified module. 2. Load the module. 3. Return the loaded module object. You should use `importlib.util.find_spec()` to find the module specification and `importlib.util.module_from_spec()` to create and load the module from the specification. # Function Signature: ```python def load_module(module_name: str) -> object: ``` # Input: - `module_name` (str): The name of the module to be imported. It could be a standard library module, an installed package, or a custom module. # Output: - Returns the loaded module object. # Constraints: - The `module_name` parameter is a valid Python module name. - Use Python 3.4 and above. # Example Usage: ```python # Assuming `math` is not previously imported math_module = load_module(\'math\') print(math_module.sqrt(16)) # Output: 4.0 # Importing a custom module: my_module = load_module(\'mycustommodule\') print(my_module.some_function()) ``` # Notes: - **Hint**: Refer to the `importlib` documentation for functions like `importlib.util.find_spec()` and `importlib.util.module_from_spec()`. - **Important**: Ensure your code handles within-module path resolving. You should raise appropriate errors if the module cannot be found or if loading fails. Test Cases to Consider: - Load a built-in module. - Load an external package. - Load a user-defined module. - Handle scenarios where the module does not exist.","solution":"import importlib.util import sys def load_module(module_name: str) -> object: Loads and returns the specified module using importlib. :param module_name: The name of the module to load. :return: The loaded module object. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) sys.modules[module_name] = module return module"},{"question":"**Problem: Analyzing the Palmer Penguins Dataset** **Objective:** Create a Python function that generates multiple types of visualizations using the Seaborn library. The visualizations should analyze the relationship between bill length and bill depth for the Palmer Penguins dataset. Additionally, the resulting plots should be customized to enhance readability and provide insightful information. **Instructions:** 1. **Load the Dataset:** - Use `seaborn` to load the \\"penguins\\" dataset. 2. **Function Signature:** ```python def visualize_penguin_data(penguins): # Your code here ``` - The function takes one argument, `penguins`, which is a DataFrame containing the Penguins dataset. 3. **Generate Visualizations:** - Create and customize the following visualizations: a. **Scatter Plot with Marginal Histograms:** - Use `jointplot` to generate a scatter plot with marginal histograms of `bill_length_mm` vs. `bill_depth_mm`. b. **Density Plots Using KDE:** - Use `kdeplot` for both bivariate and univariate density curves. c. **Linear Regression with Marginals:** - Add a linear regression fit with marginals using `regplot`. d. **Hexbin Plot:** - Create a hexbin plot to visualize the joint distribution and add hexagonal bins. e. **Customizations:** - Apply various customizations such as changing marker styles, adjusting figure size, adding rug plots, and modifying marginal axis settings. 4. **Expected Output:** - The function should display the generated plots directly using Seaborn. The plots should include clearly labeled axes and titles for better comprehension. 5. **Constraints:** - Ensure that the visualizations handle different `hue` levels to differentiate between penguin species. - Customize the plots by adding suitable annotations and legends where necessary. 6. **Example Code:** ```python penguins = sns.load_dataset(\'penguins\') visualize_penguin_data(penguins) ``` **Notes:** - This question tests the student\'s ability to use Seaborn for data visualization, understand and apply different plotting techniques, and customize plots for better data analysis. Make sure to explain each step, providing comments within the code to demonstrate understanding.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_penguin_data(penguins): Generates multiple visualizations to analyze the relationship between bill length and bill depth for the Palmer Penguins dataset. Parameters: penguins (DataFrame): DataFrame containing the Penguins dataset. # Scatter Plot with Marginal Histograms scatter_plot = sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, hue=\'species\') scatter_plot.fig.suptitle(\'Scatter Plot with Marginal Histograms\') scatter_plot.set_axis_labels(\'Bill Length (mm)\', \'Bill Depth (mm)\') scatter_plot.fig.tight_layout() scatter_plot.fig.subplots_adjust(top=0.95) # Adjust title position # Bivariate and Univariate Density Plots plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True, common_norm=False, palette=\\"crest\\") plt.title(\\"Kernel Density Estimate Plot - Bill Length\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # Linear Regression with Marginals linear_regression_plot = sns.lmplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", data=penguins) linear_regression_plot.fig.suptitle(\'Linear Regression with Marginals\') linear_regression_plot.set_axis_labels(\'Bill Length (mm)\', \'Bill Depth (mm)\') linear_regression_plot.fig.tight_layout() linear_regression_plot.fig.subplots_adjust(top=0.9) # Adjust title position # Hexbin Plot plt.figure(figsize=(10, 6)) sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, kind=\\"hex\\", color=\\"b\\") plt.suptitle(\'Hexbin Plot\') plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show()"},{"question":"# Incremental Learning Pipeline with Out-of-Core Data **Objective:** Build an incremental learning pipeline using scikit-learn to classify text data that is too large to fit into memory all at once. **Description:** You are given a large dataset of text documents categorized into different classes. Your task is to build an incremental learning pipeline that reads this data in chunks, extracts features using the `HashingVectorizer`, and performs classification using the `SGDClassifier`. The goal is to efficiently handle the text data and train a model without loading the entire dataset into memory at once. **Requirements:** 1. Implement a generator function `stream_documents` that reads documents in chunks from a provided file or list of files. 2. Use `HashingVectorizer` from `sklearn.feature_extraction.text` for feature extraction. 3. Implement an incremental learning pipeline using `SGDClassifier` with the `partial_fit` method. 4. Evaluate and print the model\'s accuracy after processing each chunk. **Function Signatures:** ```python def stream_documents(file_path: str, chunk_size: int): Generator function that yields chunks of text documents from a file. Args: - file_path: str : Path to the file containing the text documents. - chunk_size: int : Number of documents to yield in each chunk. Yields: - List[str]: A list of text documents. pass def incremental_learning_pipeline(file_path: str, chunk_size: int, classes: list): Function to implement the incremental learning pipeline. Args: - file_path: str : Path to the file containing the text documents. - chunk_size: int : Number of documents to process in each chunk. - classes: list : List of all possible target classes. Returns: - SGDClassifier : The trained classifier. pass ``` **Constraints:** - Assume the file contains one document per line, with the format: `\\"<class_label>t<text_document>\\"`. - The list of possible classes is known and provided. - Ensure that your implementation handles feature extraction and model training incrementally. **Performance Requirements:** - Optimize memory usage by processing documents in chunks. - Ensure that the feature extraction and model training steps are efficient and scalable. **Example Usage:** ```python file_path = \\"path/to/large_text_file.txt\\" chunk_size = 1000 classes = [\\"class1\\", \\"class2\\", \\"class3\\"] # Stream documents and train model incrementally classifier = incremental_learning_pipeline(file_path, chunk_size, classes) # The classifier is now trained and can be used for predictions ``` **Note:** - You may use any suitable methods from the `os` or `io` modules to handle file operations. - Include error handling to manage any unexpected issues during file reading and processing.","solution":"import os from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_documents(file_path: str, chunk_size: int): Generator function that yields chunks of text documents from a file. Args: - file_path: str : Path to the file containing the text documents. - chunk_size: int : Number of documents to yield in each chunk. Yields: - List[str]: A list of text documents. with open(file_path, \'r\', encoding=\'utf-8\') as file: chunk = [] for line in file: chunk.append(line.strip()) if len(chunk) == chunk_size: yield chunk chunk = [] # yield the last chunk if not empty if chunk: yield chunk def incremental_learning_pipeline(file_path: str, chunk_size: int, classes: list): Function to implement the incremental learning pipeline. Args: - file_path: str : Path to the file containing the text documents. - chunk_size: int : Number of documents to process in each chunk. - classes: list : List of all possible target classes. Returns: - SGDClassifier : The trained classifier. vectorizer = HashingVectorizer() classifier = SGDClassifier() is_first_chunk = True for chunk in stream_documents(file_path, chunk_size): texts, labels = [], [] for document in chunk: label, text = document.split(\'t\', 1) texts.append(text) labels.append(label) X_chunk = vectorizer.transform(texts) y_chunk = labels # Perform partial fit on the classifier classifier.partial_fit(X_chunk, y_chunk, classes=classes) if is_first_chunk: is_first_chunk = False else: y_pred_chunk = classifier.predict(X_chunk) accuracy = accuracy_score(y_chunk, y_pred_chunk) print(f\\"Chunk processed. Accuracy: {accuracy:.4f}\\") return classifier"},{"question":"Objective: Demonstrate your understanding of using `seaborn.objects` to create and customize advanced visualizations in Python. Question: You are given a dataset containing information about different countries, including their health expenditure and life expectancy over various years. Your task is to create a visualization that shows the trajectory of health expenditure against life expectancy for each country over time, with specific customizations. Requirements: 1. Load the dataset `healthexp` from seaborn. 2. Create a plot object with the following specifications: - X-axis: `Spending_USD` - Y-axis: `Life_Expectancy` - Color-coded by `Country` 3. Add a `Path` mark to show the trajectory for each country. 4. Customize the plot with the following properties: - Markers at each point in the path (`marker=\'o\'`) - Smaller marker size (`pointsize=2`) - Thinner line width (`linewidth=0.75`) - White color for the marker (`fillcolor=\'w\'`) 5. Display the final plot. Constraints: - Ensure the dataset is sorted by `Country` and `Year` before plotting. - Use `seaborn.objects` for plotting. Input Format: None (the dataset is loaded within the script). Output Format: A plot displaying the specified trajectory with the given customizations. Example: Here\'s a hint for the beginning part of your solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load and sort the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot object p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Add Path mark and customize p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=.75, fillcolor=\\"w\\")) # Display the plot # (Your code to display the plot here) ``` Complete the code and display the plot with the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure_life_expectancy(): # Load and sort the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot object p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Add Path mark and customize p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Display the plot p.show()"},{"question":"**Pairwise Distance and Similarity Metrics Implementation** In this task, you are required to implement a function that computes both the pairwise distances and the pairwise kernel similarities between two given datasets using various metrics and kernels. # Function Signature ```python def pairwise_metrics(X, Y, metric=\'euclidean\'): This function calculates pairwise distances or kernel similarities between the row vectors of X and Y. Arguments: X : array-like of shape (n_samples_X, n_features) -- Input data. Y : array-like of shape (n_samples_Y, n_features) -- Input data. metric : str -- The metric to use for computing distances or kernel similarities. Possible values are \'euclidean\', \'manhattan\', \'cosine\', \'linear\', \'polynomial\', \'sigmoid\', \'rbf\', \'laplacian\', \'chi2\'. Default is \'euclidean\'. Returns: array-like -- Computed pairwise distances or kernel similarities between the row vectors of X and Y. pass ``` # Input: 1. **`X`**: a numpy array of shape (n_samples_X, n_features) 2. **`Y`**: a numpy array of shape (n_samples_Y, n_features) 3. **`metric`**: a string specifying the metric to compute distances or similarities - Possible values: \'euclidean\', \'manhattan\', \'cosine\', \'linear\', \'polynomial\', \'sigmoid\', \'rbf\', \'laplacian\', \'chi2\' - Default is \'euclidean\'. # Output: The function should return a numpy array containing the pairwise distances or kernel similarities based on the metric specified. # Constraints: 1. You can assume that input arrays `X` and `Y` always have the same number of features. 2. You must use appropriate functions from the `sklearn.metrics.pairwise` submodule to compute the distances or similarities as specified by the `metric`. # Example Usage: ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) # Euclidean distance print(pairwise_metrics(X, Y, metric=\'euclidean\')) # Expected output: array with Euclidean distances between rows of X and Y # Cosine similarity print(pairwise_metrics(X, Y, metric=\'cosine\')) # Expected output: array with cosine similarities between rows of X and Y # RBF kernel print(pairwise_metrics(X, Y, metric=\'rbf\')) # Expected output: array with RBF Gaussian kernel similarities between rows of X and Y ``` # Note: - Ensure that your implementation can handle large datasets efficiently. - Handle any edge cases or incorrect metric values appropriately by raising an informative error.","solution":"import numpy as np from sklearn.metrics.pairwise import ( euclidean_distances, manhattan_distances, cosine_similarity, linear_kernel, polynomial_kernel, sigmoid_kernel, rbf_kernel, laplacian_kernel, chi2_kernel ) def pairwise_metrics(X, Y, metric=\'euclidean\'): This function calculates pairwise distances or kernel similarities between the row vectors of X and Y. Arguments: X : array-like of shape (n_samples_X, n_features) -- Input data. Y : array-like of shape (n_samples_Y, n_features) -- Input data. metric : str -- The metric to use for computing distances or kernel similarities. Possible values are \'euclidean\', \'manhattan\', \'cosine\', \'linear\', \'polynomial\', \'sigmoid\', \'rbf\', \'laplacian\', \'chi2\'. Default is \'euclidean\'. Returns: array-like -- Computed pairwise distances or kernel similarities between the row vectors of X and Y. if metric == \'euclidean\': return euclidean_distances(X, Y) elif metric == \'manhattan\': return manhattan_distances(X, Y) elif metric == \'cosine\': return cosine_similarity(X, Y) elif metric == \'linear\': return linear_kernel(X, Y) elif metric == \'polynomial\': return polynomial_kernel(X, Y) elif metric == \'sigmoid\': return sigmoid_kernel(X, Y) elif metric == \'rbf\': return rbf_kernel(X, Y) elif metric == \'laplacian\': return laplacian_kernel(X, Y) elif metric == \'chi2\': return chi2_kernel(X, Y) else: raise ValueError(f\\"Unsupported metric: {metric}\\")"},{"question":"# Advanced Python Set Manipulation **Objective:** Implement a function in Python that demonstrates the creation, manipulation, and performance of various operations on sets and frozensets utilizing the provided functionalities for set objects. **Problem Statement:** You are tasked with writing a function `perform_set_operations` that: 1. Takes a list of integers as input. 2. Creates a `set` object and a `frozenset` object from the input list. 3. Adds an integer to the `set`. 4. Checks if a specific integer exists in both the `set` and `frozenset`. 5. Discards an integer from the `set`. 6. Clears all elements from the `set`. The function should return a tuple containing: - The final state of the `set`. - The result of the containment check in both the `set` and `frozenset`. **Function Signature:** ```python def perform_set_operations(input_list: list, add_element: int, check_element: int, remove_element: int) -> tuple: pass ``` **Expected Input and Output:** - **Input:** - `input_list`: A list of integers. - `add_element`: An integer to add to the `set`. - `check_element`: An integer to check for presence in both `set` and `frozenset`. - `remove_element`: An integer to remove from the `set`. - **Output:** - A tuple containing: - The final `set` after all operations. - Boolean result of checking `check_element` in the `set`. - Boolean result of checking `check_element` in the `frozenset`. **Example:** ```python input_list = [1, 2, 3, 4, 5] add_element = 6 check_element = 3 remove_element = 2 perform_set_operations(input_list, add_element, check_element, remove_element) ``` Expected Output: ```python (set(), True, True) ``` **Constraints:** - The `input_list` will contain integers only. - The integers for `add_element`, `check_element`, and `remove_element` will also be valid integers. Implement the function using appropriate functionality as mentioned in the documentation above. **Note:** - Ensure that you are following the constraints and performance requirements where applicable. - Be mindful of handling any errors or exceptions gracefully as per the specifications in the documentation.","solution":"def perform_set_operations(input_list: list, add_element: int, check_element: int, remove_element: int) -> tuple: Takes a list of integers and performs various set operations. Args: input_list (list): List of integers. add_element (int): Integer to add to the set. check_element (int): Integer to check for presence in the set and frozenset. remove_element (int): Integer to remove from the set. Returns: tuple: (final set, check_element in set, check_element in frozenset) # Create a set from the input list my_set = set(input_list) # Create a frozenset from the input list my_frozenset = frozenset(input_list) # Add an element to the set my_set.add(add_element) # Check if an element exists in both the set and the frozenset in_set = check_element in my_set in_frozenset = check_element in my_frozenset # Discard an element from the set my_set.discard(remove_element) # Clear all elements from the set my_set.clear() # Return the final set and the results of the containment checks return (my_set, in_set, in_frozenset)"},{"question":"Implement and Evaluate Isotonic Regression **Objective:** Implement a function to perform isotonic regression using scikit-learn\'s `IsotonicRegression` class. Given a set of data points, this function should fit the isotonic regression model, make predictions on test data, and calculate the mean squared error on the test data. **Details:** Input: - `X_train`: A list of floats representing the training data feature values. - `y_train`: A list of floats representing the training data target values. - `X_test`: A list of floats representing the test data feature values. - `y_test`: A list of floats representing the test data target values. - `increasing`: A boolean or string (\'auto\') parameter specifying the monotonicity constraint (default is \'auto\'). Output: - `mse`: A float representing the mean squared error between the predicted and actual values for the test data. **Function Signature:** ```python def isotonic_regression_mse(X_train, y_train, X_test, y_test, increasing=\'auto\') -> float: pass ``` Constraints: - Each list in the input will contain between 1 and 1000 floats. - Feature values (`X_train`, `X_test`) and target values (`y_train`, `y_test`) will be real numbers. - `X_train` and `X_test` lists may not be sorted. Performance Requirements: - The function should run efficiently within time limits for input sizes up to 1000. **Example:** ```python X_train = [1, 2, 3, 4, 5] y_train = [5, 6, 7, 8, 9] X_test = [2.5, 3.5, 4.5] y_test = [6.5, 7.5, 8.5] mse = isotonic_regression_mse(X_train, y_train, X_test, y_test, increasing=True) print(mse) # Expected output: 0.0 ``` In this example, establishing the isotonic regression on training data that naturally follows a linear increasing trend should fit perfectly, resulting in a mean squared error (MSE) of 0.0 on the test data. **Note:** The function should handle different scenarios where the data might not perfectly follow a monotonic trend.","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def isotonic_regression_mse(X_train, y_train, X_test, y_test, increasing=\'auto\') -> float: Fits an isotonic regression model and calculates mean squared error on test data. Parameters: - X_train: List of floats representing the training data features. - y_train: List of floats representing the training data targets. - X_test: List of floats representing the test data features. - y_test: List of floats representing the test data targets. - increasing: Boolean or \'auto\' for specifying monotonicity constraint (default is \'auto\'). Returns: - mse: Float representing the mean squared error on the test data. # Fit the isotonic regression model isotonic_reg = IsotonicRegression(increasing=increasing) y_train_ = isotonic_reg.fit_transform(X_train, y_train) # Predict values for the test data y_pred = isotonic_reg.transform(X_test) # Calculate the mean squared error mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"**Objective:** You are tasked with implementing a function that performs a comprehensive comparison of two directory trees and logs the differences, as well as generates a detailed report of files that are different. **Function Signature:** ```python def compare_directories(dir1: str, dir2: str) -> None: Compare two directories and their subdirectories, log all differences to a file, and print a detailed report of files that differ. Parameters: dir1 (str): Path to the first directory. dir2 (str): Path to the second directory. Returns: None ``` **Requirements:** 1. **Log Differences:** - Create a log file named `comparison_log.txt` in the current working directory. - Log entries should include differences in file contents, files present only in one directory, and files that could not be compared. - Each log entry should be formatted as: ``` [difference_type] file_name - reason ``` For example: ``` [diff_files] file1.txt - Different contents [left_only] file2.txt - Present only in dir1 [funny_files] file3.txt - Could not be compared ``` 2. **Generate and Print Report:** - Print a detailed report of files that are different in content. - The report should list each differing file and the directories where they are found. - Use recursion to ensure all subdirectories are compared. 3. **Constraints and Assumptions:** - You can assume the directory paths exist and are accessible. - Use the `filecmp` module\'s `dircmp` class for the comparison. - Utilize appropriate attributes and methods of the `dircmp` class to gather comparison data. # Example Usage ```python # Suppose we have two directory trees as follows: # dir1/ # ├── file1.txt # ├── file2.txt # └── subdir1/ # └── file3.txt # dir2/ # ├── file1.txt # ├── file4.txt # └── subdir1/ # └── file3.txt compare_directories(\'dir1\', \'dir2\') ``` - The `comparison_log.txt` might contain: ``` [diff_files] file1.txt - Different contents [left_only] file2.txt - Present only in dir1 [right_only] file4.txt - Present only in dir2 ``` - The printed report might look like: ``` Different files: - file1.txt found in dir1 and dir2 ``` Implement the `compare_directories` function to accomplish the stated objectives, utilizing the `filecmp` module\'s functionality as described.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> None: Compare two directories and their subdirectories, log all differences to a file, and print a detailed report of files that differ. Parameters: dir1 (str): Path to the first directory. dir2 (str): Path to the second directory. Returns: None log_filename = \\"comparison_log.txt\\" with open(log_filename, \\"w\\") as log_file: def log_difference(difference_type, file_name, reason): log_entry = f\\"[{difference_type}] {file_name} - {reason}n\\" log_file.write(log_entry) def print_diff_files(title, files): if files: print(f\\"{title}:\\") for filename in files: print(f\\" - {filename}\\") print() def recursive_compare(dcmp): for name in dcmp.diff_files: log_difference(\'diff_files\', name, \'Different contents\') for name in dcmp.left_only: log_difference(\'left_only\', name, \'Present only in dir1\') for name in dcmp.right_only: log_difference(\'right_only\', name, \'Present only in dir2\') for name in dcmp.common_files: log_difference(\'funny_files\', name, \'Could not be compared\') for sub_dcmp in dcmp.subdirs.values(): recursive_compare(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) recursive_compare(dcmp) print_diff_files(\\"Different files\\", dcmp.diff_files)"},{"question":"# Question: Build a Registration Form Using `tkinter` Objective Create a simple registration form using the `tkinter` package in Python. The form should accept several fields, validate the input data, and display the entered information upon submission. Requirements 1. **Widgets**: - Labels for each field (Name, Email, Age, Gender, Country). - Entry widgets for Name, Email, and Age. - A dropdown (Combobox) for Gender selection (options: Male, Female, Other). - A Listbox for Country selection with at least 5 different country names. - A submit button that triggers validation and shows a message with the entered information (use `tkinter.messagebox`). 2. **Geometry Management**: - Use `grid` geometry manager to place the widgets in a neat, organized manner. 3. **Validation**: - Name: Should not be empty. - Email: Should contain \'@\' and \'.\' symbols. - Age: Should be a number between 1 and 120. - Gender & Country: Must be selected. 4. **Output**: - Show a messagebox with all the entered details in a formatted string upon successful submission. - If any validation fails, display an appropriate error message in the messagebox. Input and Output Format - **Input**: Inputs through `tkinter` Entry fields and widgets. - **Output**: Display a messagebox with the entered values or errors. Coding Constraints and Performance 1. Use the `tkinter` package and related modules only. 2. Follow modular coding practices by defining functions for validation and form creation. 3. Ensure the GUI is responsive and user-friendly. Example Here is a schematic representation of the input form layout: ``` ------------------------------------------------------ | Name [__________________________] | | Email [__________________________] | | Age [__________________________] | | Gender [-----------v] | | | Male | | | | Female | | | | Other | | | Country [__________] | | | USA | | | | Canada | | | | UK | | | | India | | | | Japan | | |---------------------------------------------------- | [ Submit ] | ------------------------------------------------------ ``` The expected behavior is as follows: - Clicking the `Submit` button will validate the inputs. - If inputs are valid, a popup displays all the entered details. - If any input is invalid, the popup displays an error message. Implement this registration form in Python using the `tkinter` module.","solution":"import tkinter as tk from tkinter import messagebox from tkinter import ttk def validate_registration_form(name, email, age, gender, country): Validates the registration form. Returns a tuple (is_valid, message). if not name.strip(): return False, \\"Name cannot be empty.\\" if \'@\' not in email or \'.\' not in email: return False, \\"Invalid email format.\\" try: age = int(age) if age < 1 or age > 120: return False, \\"Age must be a number between 1 and 120.\\" except ValueError: return False, \\"Age must be a valid integer.\\" if gender not in [\'Male\', \'Female\', \'Other\']: return False, \\"Gender must be selected.\\" if country not in [\\"USA\\", \\"Canada\\", \\"UK\\", \\"India\\", \\"Japan\\"]: return False, \\"Country must be selected.\\" return True, \\"Validation Successful.\\" def on_submit(name_var, email_var, age_var, gender_var, country_listbox): name = name_var.get() email = email_var.get() age = age_var.get() gender = gender_var.get() country = country_listbox.get(tk.ACTIVE) is_valid, message = validate_registration_form(name, email, age, gender, country) if is_valid: messagebox.showinfo(\\"Registration Successful\\", f\\"Name: {name}nEmail: {email}nAge: {age}nGender: {gender}nCountry: {country}\\") else: messagebox.showerror(\\"Validation Error\\", message) def create_registration_form(): root = tk.Tk() root.title(\\"Registration Form\\") # Labels tk.Label(root, text=\\"Name\\").grid(row=0, column=0, padx=10, pady=5) tk.Label(root, text=\\"Email\\").grid(row=1, column=0, padx=10, pady=5) tk.Label(root, text=\\"Age\\").grid(row=2, column=0, padx=10, pady=5) tk.Label(root, text=\\"Gender\\").grid(row=3, column=0, padx=10, pady=5) tk.Label(root, text=\\"Country\\").grid(row=4, column=0, padx=10, pady=5) # Entry widgets name_var = tk.StringVar() email_var = tk.StringVar() age_var = tk.StringVar() tk.Entry(root, textvariable=name_var).grid(row=0, column=1, padx=10, pady=5) tk.Entry(root, textvariable=email_var).grid(row=1, column=1, padx=10, pady=5) tk.Entry(root, textvariable=age_var).grid(row=2, column=1, padx=10, pady=5) # Gender combobox gender_var = tk.StringVar() gender_combobox = ttk.Combobox(root, textvariable=gender_var) gender_combobox[\'values\'] = (\'Male\', \'Female\', \'Other\') gender_combobox.grid(row=3, column=1, padx=10, pady=5) # Country listbox country_listbox = tk.Listbox(root, height=5) countries = [\\"USA\\", \\"Canada\\", \\"UK\\", \\"India\\", \\"Japan\\"] for country in countries: country_listbox.insert(tk.END, country) country_listbox.grid(row=4, column=1, padx=10, pady=5) # Submit button submit_button = tk.Button(root, text=\\"Submit\\", command=lambda: on_submit(name_var, email_var, age_var, gender_var, country_listbox)) submit_button.grid(row=5, column=0, columnspan=2, pady=10) root.mainloop() if __name__ == \\"__main__\\": create_registration_form()"},{"question":"# Question: Implementing and Profiling a Function for Prime Number Detection Background You are required to implement a Python function that checks whether a given number is a prime number. You will then profile its performance to identify potential bottlenecks and suggest optimizations. Instructions 1. Implement the function `is_prime(n)` which takes an integer `n` and returns `True` if `n` is a prime number, otherwise `False`. 2. Profile the performance of the `is_prime` function for a range of integers from 1 to 10000 using Python\'s `cProfile` module to identify any bottlenecks in the implementation. 3. Suggest and implement any optimizations you find necessary based on your profiling results. 4. After optimizing, re-profile the performance of the `is_prime` function and compare the results with the initial profiling to ensure improvements have been made. Function Signature ```python def is_prime(n: int) -> bool: # your code here ``` Example ```python print(is_prime(2)) # True print(is_prime(4)) # False print(is_prime(13)) # True ``` Guidelines 1. For profiling, use the `cProfile` module. 2. Include at least one example of the profiling output before and after optimization. 3. The solution should correctly handle edge cases, such as very small or very large numbers. 4. Document any constraints or assumptions you make. 5. Ensure the function handles invalid inputs gracefully (e.g., negative numbers). Constraints - The function should aim to optimize for time complexity where feasible. - Focus on readability and performance in your implementation. Here is an example structure to start from: ```python import cProfile import pstats def is_prime(n: int) -> bool: Initial implementation code here # Profiling the initial implementation profiler = cProfile.Profile() profiler.enable() for i in range(1, 10001): is_prime(i) profiler.disable() # Print profiling results stats = pstats.Stats(profiler).strip_dirs().sort_stats(\\"cumtime\\") stats.print_stats(10) # Implement any optimizations here and re-profile def is_prime_optimized(n: int) -> bool: Optimized implementation code here # Profiling the optimized implementation profiler = cProfile.Profile() profiler.enable() for i in range(1, 10001): is_prime_optimized(i) profiler.disable() # Print profiling results of the optimized version stats = pstats.Stats(profiler).strip_dirs().sort_stats(\\"cumtime\\") stats.print_stats(10) ```","solution":"import cProfile import pstats import math def is_prime(n: int) -> bool: Check if a number is a prime number. Parameters: n (int): The number to be checked. Returns: bool: True if n is a prime number, False otherwise. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True # Profiling the initial implementation def profile_is_prime(): profiler = cProfile.Profile() profiler.enable() for i in range(1, 10001): is_prime(i) profiler.disable() stats = pstats.Stats(profiler).strip_dirs().sort_stats(\\"cumtime\\") stats.print_stats(10) profile_is_prime() # Optimized version of the function if necessary def is_prime_optimized(n: int) -> bool: Optimized check if a number is a prime number. Parameters: n (int): The number to be checked. Returns: bool: True if n is a prime number, False otherwise. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False for i in range(5, int(math.sqrt(n)) + 1, 6): if n % i == 0 or n % (i + 2) == 0: return False return True # Profiling the optimized implementation def profile_is_prime_optimized(): profiler = cProfile.Profile() profiler.enable() for i in range(1, 10001): is_prime_optimized(i) profiler.disable() stats = pstats.Stats(profiler).strip_dirs().sort_stats(\\"cumtime\\") stats.print_stats(10) profile_is_prime_optimized()"},{"question":"# Unicode Data Analysis and Conversion You are tasked with implementing a utility to analyze a list of characters and perform conversions based on their properties using the `unicodedata` module. Function Signature ```python def analyze_and_convert(chars: List[str]) -> Dict[str, Any]: ``` Parameters - `chars`: A list of strings where each string is a single Unicode character. Expected Output The function should return a dictionary with the following format: - Each key is a character from the input list. - Each value is another dictionary with the character\'s details, including: - `name`: The name of the character (use \'Unknown\' if not available). - `category`: The general category assigned to the character. - `decimal_value`: The decimal value if available, otherwise `None`. - `digit_value`: The digit value if available, otherwise `None`. - `numeric_value`: The numeric value if available, otherwise `None`. - `normalize_nfc`: The NFC normalized form of the character. - `is_normalized_nfc`: Boolean indicating if the character is in NFC form. Constraints - You may assume the input list has 1 to 1000 characters. - Each character in the list is guaranteed to be a valid Unicode character. Example ```python chars = [\'A\', \'9\', \'Ⅷ\', \'ç\', \'∫\', \'अ\'] result = analyze_and_convert(chars) expected_result = { \'A\': { \'name\': \'LATIN CAPITAL LETTER A\', \'category\': \'Lu\', \'decimal_value\': None, \'digit_value\': None, \'numeric_value\': None, \'normalize_nfc\': \'A\', \'is_normalized_nfc\': True, }, \'9\': { \'name\': \'DIGIT NINE\', \'category\': \'Nd\', \'decimal_value\': 9, \'digit_value\': 9, \'numeric_value\': 9.0, \'normalize_nfc\': \'9\', \'is_normalized_nfc\': True, }, \'Ⅷ\': { \'name\': \'ROMAN NUMERAL EIGHT\', \'category\': \'Nl\', \'decimal_value\': None, \'digit_value\': None, \'numeric_value\': 8.0, \'normalize_nfc\': \'Ⅷ\', \'is_normalized_nfc\': True, }, \'ç\': { \'name\': \'LATIN SMALL LETTER C WITH CEDILLA\', \'category\': \'Ll\', \'decimal_value\': None, \'digit_value\': None, \'numeric_value\': None, \'normalize_nfc\': \'ç\', \'is_normalized_nfc\': True, }, \'∫\': { \'name\': \'INTEGRAL\', \'category\': \'Sm\', \'decimal_value\': None, \'digit_value\': None, \'numeric_value\': None, \'normalize_nfc\': \'∫\', \'is_normalized_nfc\': True, }, \'अ\': { \'name\': \'DEVANAGARI LETTER A\', \'category\': \'Lo\', \'decimal_value\': None, \'digit_value\': None, \'numeric_value\': None, \'normalize_nfc\': \'अ\', \'is_normalized_nfc\': True, }, } assert result == expected_result ``` **Note:** Be sure to use the `unicodedata` module functions to retrieve the necessary data about each character.","solution":"import unicodedata from typing import List, Dict, Any def analyze_and_convert(chars: List[str]) -> Dict[str, Any]: result = {} for char in chars: char_info = {} try: char_info[\'name\'] = unicodedata.name(char) except ValueError: char_info[\'name\'] = \'Unknown\' char_info[\'category\'] = unicodedata.category(char) char_info[\'decimal_value\'] = unicodedata.decimal(char, None) char_info[\'digit_value\'] = unicodedata.digit(char, None) char_info[\'numeric_value\'] = unicodedata.numeric(char, None) char_info[\'normalize_nfc\'] = unicodedata.normalize(\'NFC\', char) char_info[\'is_normalized_nfc\'] = char_info[\'normalize_nfc\'] == char result[char] = char_info return result"},{"question":"# Python File Object Manipulation In this assessment, you are to demonstrate your understanding of file handling in Python by implementing a function that mimics some of the behaviors described in the provided documentation. Specifically, you will create a function that writes and reads data from a file object using Python\'s built-in I/O capabilities. Task Implement a function `manipulate_file(data, filepath)`, which will perform the following operations: 1. **Writing Data:** - Write the provided `data` (a list of strings) to the file located at `filepath`, ensuring each string is written on a new line. 2. **Reading Data:** - After writing, read the data back from the file. Read only 3 lines from the file if it contains more than 3 lines. 3. **Output:** - The function should return a list of the lines read from the file. Function Signature ```python def manipulate_file(data: List[str], filepath: str) -> List[str]: pass ``` Input - `data`: A list of strings. Each string should be written to the file on a new line. - `filepath`: A string representing the path to the file where data should be written and read from. Output - The function should return a list of at most 3 lines read from the file. If the file contains fewer than 3 lines, return all the lines available. Example ```python data = [\\"line1\\", \\"line2\\", \\"line3\\", \\"line4\\"] filepath = \\"example.txt\\" output = manipulate_file(data, filepath) print(output) ``` **Possible output:** ```python [\'line1\', \'line2\', \'line3\'] ``` Constraints - Ensure that the file is properly closed after all operations. - Handle any possible exceptions that may arise during file operations. Note While the functionality described above uses higher-level Python I/O capabilities, the question is inspired by the low-level file handling API described in the provided documentation. Your implementation should not use the C API directly but should instead utilize Python\'s built-in `open`, `write`, and `read` methods.","solution":"from typing import List def manipulate_file(data: List[str], filepath: str) -> List[str]: try: # Writing Data to File with open(filepath, \'w\') as file: for line in data: file.write(line + \'n\') # Reading Data from File lines_read = [] with open(filepath, \'r\') as file: for i in range(3): line = file.readline() if not line: break lines_read.append(line.strip()) return lines_read except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Calendar Analysis and Manipulation You are tasked with creating a program that utilizes the Python `calendar` module to analyze and manipulate date-related information. Implement the following functions: 1. **generate_calendar_html(year, month)** - **Input**: Two integers, `year` and `month`. - **Output**: A string containing the HTML representation of the calendar for the specified month and year. - **Constraints**: - The `year` must be a positive integer. - The `month` must be an integer between 1 and 12. - **Example**: ```python generate_calendar_html(2023, 10) ``` This function should return the HTML string of the calendar for October 2023. 2. **find_leap_years(start_year, end_year)** - **Input**: Two integers, `start_year` and `end_year`. - **Output**: An integer representing the number of leap years between `start_year` (inclusive) and `end_year` (exclusive). - **Constraints**: - Both `start_year` and `end_year` must be positive integers. - `end_year` must be greater than `start_year`. - **Example**: ```python find_leap_years(2000, 2020) ``` This function should return `5` since the leap years between 2000 and 2020 are 2000, 2004, 2008, 2012, and 2016. 3. **get_weekday_name(year, month, day)** - **Input**: Three integers, `year`, `month`, and `day`. - **Output**: A string representing the name of the weekday for the specified date. - **Constraints**: - The `year`, `month`, and `day` must form a valid date. - **Example**: ```python get_weekday_name(2023, 10, 10) ``` This function should return `\\"Tuesday\\"` as October 10, 2023, falls on a Tuesday. 4. **get_full_month_calendar(year, month)** - **Input**: Two integers, `year` and `month`. - **Output**: A list of lists, where each inner list represents a week, and each element in the inner list represents a day of the month or `0` for days outside the specified month. - **Constraints**: - The `year` must be a positive integer. - The `month` must be an integer between 1 and 12. - **Example**: ```python get_full_month_calendar(2023, 10) ``` This function should return: ```python [ [0, 0, 0, 0, 0, 0, 1], [2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22], [23, 24, 25, 26, 27, 28, 29], [30, 31, 0, 0, 0, 0, 0] ] ``` Implement these functions in Python, ensuring they handle edge cases and invalid inputs gracefully.","solution":"import calendar def generate_calendar_html(year, month): Returns the HTML representation of the calendar for the specified month and year. if not (1 <= month <= 12) or year <= 0: raise ValueError(\\"Invalid month or year\\") return calendar.HTMLCalendar().formatmonth(year, month) def find_leap_years(start_year, end_year): Returns the number of leap years between start_year (inclusive) and end_year (exclusive). if start_year <= 0 or end_year <= start_year: raise ValueError(\\"Invalid start year or end year\\") return sum(1 for year in range(start_year, end_year) if calendar.isleap(year)) def get_weekday_name(year, month, day): Returns the name of the weekday for the specified date. try: return calendar.day_name[calendar.weekday(year, month, day)] except ValueError: raise ValueError(\\"Invalid date\\") def get_full_month_calendar(year, month): Returns a list of lists representing the full month calendar. if not (1 <= month <= 12) or year <= 0: raise ValueError(\\"Invalid month or year\\") return calendar.monthcalendar(year, month)"},{"question":"Problem Statement You are required to implement a function `find_common_elements` that takes two lists of integers and returns a list of integers that are common to both lists. The returned list should not contain duplicate elements and should be sorted in ascending order. After implementing the function, you need to write unit tests using the `unittest` framework to verify the correctness of the function. Function Signature ```python def find_common_elements(list1: list[int], list2: list[int]) -> list[int]: pass ``` Input - `list1`: A list of integers. (0 <= len(list1) <= 1000) - `list2`: A list of integers. (0 <= len(list2) <= 1000) Output - A list of integers that are common to both `list1` and `list2`, sorted in ascending order without duplicates. Constraints - You should ensure that the returned list does not contain duplicates. - The returned list must be sorted in ascending order. - Consider the performance for larger inputs. Example ```python list1 = [1, 2, 2, 3, 4] list2 = [3, 4, 4, 5, 6] find_common_elements(list1, list2) ``` Expected Output: ```python [3, 4] ``` Unit Testing You need to write a test class `TestFindCommonElements` that inherits from `unittest.TestCase` and includes at least five test methods to verify the correctness of `find_common_elements`. Your test cases should cover: 1. Basic functionality with no duplicates in input lists. 2. Edge cases with empty input lists. 3. Cases with all elements being common. 4. Cases with no common elements. 5. Large input lists. Example of a test class template: ```python import unittest class TestFindCommonElements(unittest.TestCase): def test_common_elements_basic(self): list1 = [1, 2, 3] list2 = [2, 3, 4] self.assertEqual(find_common_elements(list1, list2), [2, 3]) def test_empty_lists(self): list1 = [] list2 = [] self.assertEqual(find_common_elements(list1, list2), []) def test_all_common_elements(self): list1 = [1, 2, 3] list2 = [1, 2, 3] self.assertEqual(find_common_elements(list1, list2), [1, 2, 3]) def test_no_common_elements(self): list1 = [1, 2, 3] list2 = [4, 5, 6] self.assertEqual(find_common_elements(list1, list2), []) def test_large_input(self): list1 = list(range(1000)) list2 = list(range(500, 1500)) self.assertEqual(find_common_elements(list1, list2), list(range(500, 1000))) if __name__ == \\"__main__\\": unittest.main() ``` Implement the `find_common_elements` function and the `TestFindCommonElements` class to complete this assessment. Submission Submit your `find_common_elements` function and the `TestFindCommonElements` test class in a single Python file.","solution":"def find_common_elements(list1: list[int], list2: list[int]) -> list[int]: Returns a sorted list of integers that are common to both list1 and list2, without duplicates. # Convert lists to sets to eliminate duplicates and perform intersection set1 = set(list1) set2 = set(list2) # Find common elements common_elements = set1.intersection(set2) # Convert the set to a sorted list and return return sorted(common_elements)"},{"question":"# Advanced Pattern Matching and Resource Management with Context Managers In this coding assessment, you will demonstrate your understanding of Python\'s advanced control flow and resource management features, specifically focusing on the `match` statement for pattern matching and the `with` statement for context management. Problem Statement You are given a configuration file in JSON format that contains various settings for multiple application components. Each component configuration is represented as a dictionary with different expected fields. The objective is to parse this configuration file and apply specific actions based on the type of component and its settings. You will implement a function `process_configuration(config_file: str) -> None` that: 1. Reads the JSON configuration file. 2. Uses pattern matching (`match`) to identify and process different types of components. 3. Uses the `with` statement to ensure proper resource management. Each component in the configuration can be one of the following (identified by the `type` field): - **Database** component, expected fields: - `type`: \\"database\\" - `dbname`: The name of the database - `user`: Database username - `password`: Database password - **Service** component, expected fields: - `type`: \\"service\\" - `name`: The name of the service - `url`: The service endpoint - **Cache** component, expected fields: - `type`: \\"cache\\" - `host`: Cache server host - `port`: Cache server port For each component type, perform the following actions: - For **Database**: Print a message like \\"Connecting to database {dbname} with user {user}\\". - For **Service**: Print a message like \\"Starting service {name} at {url}\\". - For **Cache**: Print a message like \\"Connecting to cache at {host}:{port}\\". Constraints 1. The JSON file is guaranteed to be well-formed. 2. Each component dictionary will contain a `type` field that will be one of the specified values. 3. Use the `match` statement for pattern matching. 4. Use the `with` statement for resource management when reading the file. Function Signature ```python def process_configuration(config_file: str) -> None: ``` Example Assume the content of `config_file` is as follows: ```json [ {\\"type\\": \\"database\\", \\"dbname\\": \\"example_db\\", \\"user\\": \\"admin\\", \\"password\\": \\"admin123\\"}, {\\"type\\": \\"service\\", \\"name\\": \\"auth\\", \\"url\\": \\"https://auth.example.com\\"}, {\\"type\\": \\"cache\\", \\"host\\": \\"localhost\\", \\"port\\": 6379} ] ``` Calling `process_configuration(\\"config.json\\")` should output: ``` Connecting to database example_db with user admin Starting service auth at https://auth.example.com Connecting to cache at localhost:6379 ``` Implement the function `process_configuration` to achieve the specified behavior.","solution":"import json def process_configuration(config_file: str) -> None: with open(config_file, \'r\') as file: components = json.load(file) for component in components: match component: case {\\"type\\": \\"database\\", \\"dbname\\": dbname, \\"user\\": user, \\"password\\": password}: print(f\\"Connecting to database {dbname} with user {user}\\") case {\\"type\\": \\"service\\", \\"name\\": name, \\"url\\": url}: print(f\\"Starting service {name} at {url}\\") case {\\"type\\": \\"cache\\", \\"host\\": host, \\"port\\": port}: print(f\\"Connecting to cache at {host}:{port}\\") case _: print(\\"Unknown component type\\")"},{"question":"# Shared Memory Management Using Python\'s `multiprocessing.shared_memory` Module **Objective**: Create a large list of integer values, distribute the generation of list elements over multiple processes, and share this list efficiently using shared memory. Implement code to generate parts of the list in parallel processes and combine the results back into the main process. **Background**: Using the `multiprocessing.shared_memory` module, shared memory allows direct access to values by multiple processes without copying the data between processes. This approach is particularly beneficial when dealing with large datasets or computationally expensive operations. **Requirements**: 1. **SharedListManager Class**: - Create a class `SharedListManager` to manage the creation, sharing, and cleanup of a list-like shared memory structure. - The class should include methods for: - Initializing shared memory with a specified size. - Distributing list generation among multiple processes. - Collecting and combining results efficiently into a shared list. - Cleaning up the memory (closing and unlinking). 2. **SharedIntegerSequence Generation**: - Write a function `generate_sequence(start, end)` that generates integers from `start` to `end` (inclusive) and assigns them to corresponding positions in a shared memory list. 3. **Entry Point**: - The main function should: - Initialize the shared list with a specified size. - Create multiple processes to generate segments of the list in parallel. - Ensure all processes complete and handle cleanup appropriately. # Inputs and Outputs - **Input**: - Size of the list to create (`n`). - Number of processes to use (`p`). - **Output**: - The fully populated list. # Constraints 1. Use the `multiprocessing` library functions as needed. 2. Ensure that data is correctly shared and retrieved across processes. 3. Implement reasonable error handling and cleanup procedures. 4. Maintain efficient performance, ensuring minimal overhead in process communication. # Example ```python import multiprocessing as mp from multiprocessing import shared_memory class SharedListManager: def __init__(self, size): # Initialize shared memory for list self.shm = shared_memory.SharedMemory(create=True, size=size * 4) # Assuming int size is 4 bytes self.size = size self.buf = self.shm.buf def generate_sequence(self, start, end): # Generate integers and place them in the shared memory buffer for i in range(start, end + 1): self.buf[(i-start)*4:(i-start+1)*4] = (i).to_bytes(4, byteorder=\'little\') def distribute_generation(self, processes): # Distribute the sequence generation among multiple processes size_per_process = self.size // processes remaining = self.size % processes jobs = [] start = 0 for i in range(processes): end = start + size_per_process - 1 + (1 if i < remaining else 0) p = mp.Process(target=self.generate_sequence, args=(start, end)) jobs.append(p) start = end + 1 for job in jobs: job.start() for job in jobs: job.join() def retrieve_list(self): # Retrieve the list from shared memory return [int.from_bytes(self.buf[i*4:(i+1)*4], byteorder=\'little\') for i in range(self.size)] def cleanup(self): # Clean up the shared memory self.shm.close() self.shm.unlink() def main(n, p): manager = SharedListManager(n) manager.distribute_generation(p) result = manager.retrieve_list() manager.cleanup() return result if __name__ == \'__main__\': n = 100 # Example list size p = 10 # Example number of processes print(main(n, p)) ``` This example should initialize a shared list, generate a sequence using 10 processes, and print the fully populated list. Ensure to handle any exceptions and cleanup the shared memory properly in your implementation.","solution":"import multiprocessing as mp from multiprocessing import shared_memory class SharedListManager: def __init__(self, size): # Initialize shared memory for list self.size = size self.shm = shared_memory.SharedMemory(create=True, size=size * 4) # Assuming int size is 4 bytes self.buf = self.shm.buf def generate_sequence(self, start, end): # Generate integers and place them in the shared memory buffer for i in range(start, end + 1): self.buf[(i * 4):(i * 4) + 4] = (i).to_bytes(4, byteorder=\'little\') def distribute_generation(self, processes): # Distribute the sequence generation among multiple processes size_per_process = self.size // processes jobs = [] start = 0 for i in range(processes): end = start + size_per_process - 1 + (1 if i < (self.size % processes) else 0) p = mp.Process(target=self.generate_sequence, args=(start, end)) jobs.append(p) start = end + 1 for job in jobs: job.start() for job in jobs: job.join() def retrieve_list(self): # Retrieve the list from shared memory return [int.from_bytes(self.buf[i*4:(i+1)*4], byteorder=\'little\') for i in range(self.size)] def cleanup(self): # Clean up the shared memory self.shm.close() self.shm.unlink() def main(n, p): manager = SharedListManager(n) manager.distribute_generation(p) result = manager.retrieve_list() manager.cleanup() return result if __name__ == \'__main__\': n = 100 # Example list size p = 10 # Example number of processes print(main(n, p))"},{"question":"Objective: To assess your understanding of Python\'s `errno` module and exceptions handling, especially how to interpret errno values and raise corresponding exceptions. Problem Statement Write a function `handle_errno(errno_value: int) -> str` that takes an integer `errno_value` representing an error number and performs the following tasks: - If the `errno_value` corresponds to a defined errno symbol, raise the appropriate exception. - If the `errno_value` does not correspond to a defined errno symbol, return the string \\"Unknown error\\". Additionally, write another function `errno_message(errno_value: int) -> str` that takes an integer `errno_value` and returns an error message using `os.strerror(errno_value)`. Function Signature: ```python def handle_errno(errno_value: int) -> str: # Implement this function def errno_message(errno_value: int) -> str: # Implement this function ``` Input: - An integer `errno_value`. Output: - For `handle_errno(errno_value: int)`: Raises an appropriate exception if `errno_value` is defined; otherwise, returns \\"Unknown error\\". - For `errno_message(errno_value: int)`: Returns the error message string for the given `errno_value`. Constraints: - You must use the provided `errno` module. - Do not use any additional libraries other than `os` and `errno`. Examples: ```python try: handle_errno(1) # This should raise a PermissionError except PermissionError: print(\\"PermissionError raised\\") try: handle_errno(2) # This should raise a FileNotFoundError except FileNotFoundError: print(\\"FileNotFoundError raised\\") print(handle_errno(99999)) # Output: \\"Unknown error\\" print(errno_message(1)) # Example Output: \\"Operation not permitted\\" print(errno_message(2)) # Example Output: \\"No such file or directory\\" ``` Notes: 1. Use `errno.errorcode` to map `errno_value` to its corresponding errno string name and raise the appropriate exception. 2. The exception raised should directly match the mappings provided in the documentation.","solution":"import errno import os def handle_errno(errno_value: int) -> str: Raises the appropriate exception based on the errno_value. If errno_value does not correspond to a defined errno, returns \\"Unknown error\\". if errno_value in errno.errorcode: error_name = errno.errorcode[errno_value] if error_name == \'EPERM\': raise PermissionError elif error_name == \'ENOENT\': raise FileNotFoundError # Add other error mappings as necessary. else: return \\"Unknown error\\" return \\"Unknown error\\" def errno_message(errno_value: int) -> str: Returns the error message string for the given errno_value. return os.strerror(errno_value)"},{"question":"Coding Assessment Question # Objective: Assess students\' capability in utilizing the Seaborn library for data visualization, particularly with `seaborn.objects` to plot trajectories and customize plot properties. # Problem Statement: You are tasked with visualizing the health expenditure and life expectancy data for various countries over a range of years. Use the provided `health_exp` dataset, which includes columns `Country`, `Year`, `Spending_USD`, and `Life_Expectancy`. # Requirements: 1. **Load and preprocess the data:** - Load the `healthexp` dataset using `seaborn.load_dataset()`. - Ensure the data is sorted by `Country` and `Year`. 2. **Create the Plot:** - Utilize the `so.Plot` class in Seaborn to create a plot with `Spending_USD` on the x-axis and `Life_Expectancy` on the y-axis. - Each country\'s data should be represented in a distinct color. - Use `so.Path` to visualize the trajectories of health expenditure and life expectancy over time. 3. **Customize the Path Mark:** - Add markers at data points. - Set the size of the points (`pointsize`) to 2. - Set the outline width of the paths (`linewidth`) to 0.75. - Set the fill color of the marker points to white (`fillcolor`). # Constraints: - Use the Seaborn library and its `objects` interface. - The plot should be clear and correctly labeled, supporting analysis of how health expenditure relates to life expectancy over time for different countries. # Input: The function does not accept any input parameters. # Output: The function should create and display the specified plot. # Example Usage: ```python def plot_health_expenditure_vs_life_expectancy(): import seaborn.objects as so from seaborn import load_dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Adding the path with specified properties p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=.75, fillcolor=\\"w\\")) # Display the plot p.show() plot_health_expenditure_vs_life_expectancy() ``` # Notes: - Ensure you have the necessary dependencies installed: Seaborn, and any other required libraries. - Focus on code readability and the clarity of visual output.","solution":"def plot_health_expenditure_vs_life_expectancy(): import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the data healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Customize the Path Mark p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=.75, fillcolor=\\"w\\")) # Display the plot p.show()"},{"question":"Objective: Create a visualization of the penguin dataset that includes a well-customized plot and demonstrates advanced use of seaborn\'s plotting capabilities. Description: Given the `penguins` dataset loaded using seaborn (`sns.load_dataset(\\"penguins\\")`), perform the following tasks: 1. **Load and Clean Data:** - Ensure that the dataset has no missing values. If missing values exist, handle them appropriately (e.g., by removing or imputing them). 2. **Create a Histogram:** - Create a histogram of the `bill_length_mm` variable. - Categorize the histogram by the `species` of the penguins using the `hue` parameter. 3. **Customize the Plot:** - Set an appropriate theme for the plot using seaborn\'s `sns.set_theme()` function. - Customize the legend: - Move the legend to the \\"upper left\\" of the plot. - Use `bbox_to_anchor` to adjust the legend position to be outside the plot area on the top left corner. - Ensure the legend has no title and no frame. 4. **Handle FacetGrid:** - Create a `FacetGrid` using `sns.displot` where the histograms are facetted by `island` (i.e., create separate histograms for each island). - Ensure the facet grid legend is also customized and moved to an appropriate position outside the plots. Constraints: - Use seaborn and matplotlib packages only for this task. - Ensure your code is well-documented and uses appropriate variable names. Example (Partial Code): ```python import seaborn as sns # Step 1: Load the dataset and clean it penguins = sns.load_dataset(\\"penguins\\") # Handle missing data penguins = penguins.dropna() # Step 2: Create the histogram ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Step 3: Customize the plot and legend sns.set_theme() sns.move_legend(ax, \\"upper left\\", bbox_to_anchor=(1, 1), frameon=False, title=None) # Step 4: Handle FacetGrid g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) ``` Expected Output: Include comments in your code to explain each step and provide a brief description of the visual output you aim to achieve.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_histogram(): # Step 1: Load the dataset and clean it penguins = sns.load_dataset(\\"penguins\\") # Handle missing data by dropping rows with missing values penguins = penguins.dropna() # Step 2: Set the seaborn theme for the plot sns.set_theme(style=\\"whitegrid\\") # Step 3: Create the histogram plt.figure(figsize=(10, 6)) ax = sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", palette=\\"pastel\\", edgecolor=\\".3\\", linewidth=.5) # Customize the legend ax.legend(title=None, frameon=False, loc=\'upper left\', bbox_to_anchor=(1, 1)) # Step 4: Create a FacetGrid with sns.displot g = sns.displot( data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, aspect=0.8, facet_kws=dict(legend_out=True) ) for ax in g.axes.flat: ax.legend(title=None, frameon=False, loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.show() # Call the function to create the plots create_penguin_histogram()"},{"question":"Context: You are tasked with managing a large dataset that needs to be frequently read and occasionally written to persistent storage. The dataset is represented as a dictionary where keys are strings, and values are lists of integers. You will use the `shelve` module to store and manage this data. Task: 1. Implement a function `initialize_shelf(filename: str) -> None` that: - Opens a shelf with the given filename. - Initializes the shelf with 100 keys, following the pattern `key_1`, `key_2`, ..., `key_100`. - Each key should map to a list of integers `[i, i+1, ..., i+9]`, where `i` is the suffix number of the key (e.g., `key_1` -> `[1, 2, ..., 10]`). - Closes the shelf properly afterward. 2. Implement a function `mutate_shelf(filename: str, key: str, values: List[int]) -> None` that: - Opens the shelf using the given filename. - If the `key` exists, appends the integer values to the list associated with the key. - If the `key` does not exist, creates a new entry for the key with the provided values. - Closes the shelf properly afterward. 3. Implement a function `summarize_shelf(filename: str) -> Dict[str, int]` that: - Opens the shelf using the given filename. - Calculates and returns a dictionary where each key is a key from the shelf, and the value is the sum of the list associated with that key. - Closes the shelf properly afterward. Input and Output: - `initialize_shelf(filename: str) -> None` - `filename`: A string representing the name of the shelf file to be created and initialized. - `mutate_shelf(filename: str, key: str, values: List[int]) -> None` - `filename`: A string representing the name of the shelf file to be accessed. - `key`: A string representing the key in the shelf to be mutated. - `values`: A list of integers to be appended to the existing list of integers at the given key. - `summarize_shelf(filename: str) -> Dict[str, int]` - `filename`: A string representing the name of the shelf file to be accessed. - Returns: A dictionary where each key is a key from the shelf, and the value is the sum of the list associated with that key. Constraints and Considerations: - Ensure that the shelf is always properly closed after accessing it. - Consider using `writeback=True` judiciously to manage memory usage. - Handle potential KeyErrors when accessing non-existing keys in the shelf. # Example Usage: ```python initialize_shelf(\'data.shelf\') mutate_shelf(\'data.shelf\', \'key_1\', [11, 12]) mutate_shelf(\'data.shelf\', \'key_101\', [101, 102]) summary = summarize_shelf(\'data.shelf\') print(summary) ``` Expected output: ```python {\'key_1\': 67, \'key_2\': 15, ..., \'key_101\': 203} ``` Note: The exact content of the output will depend on the specific keys and values stored during the mutation process.","solution":"import shelve def initialize_shelf(filename: str) -> None: with shelve.open(filename, writeback=True) as shelf: for i in range(1, 101): key = f\'key_{i}\' shelf[key] = list(range(i, i + 10)) def mutate_shelf(filename: str, key: str, values: list[int]) -> None: with shelve.open(filename, writeback=True) as shelf: if key in shelf: shelf[key].extend(values) else: shelf[key] = values def summarize_shelf(filename: str) -> dict[str, int]: result = {} with shelve.open(filename) as shelf: for key in shelf: result[key] = sum(shelf[key]) return result"},{"question":"Objective Implement a function that reads the contents of a specified text file, compresses the read data using bzip2, and writes the compressed data to another specified file. Then, read back the compressed file, decompress it, and verify the integrity of the decompressed data by comparing it to the original contents. Function signature ```python def compress_and_verify(input_file: str, compressed_file: str) -> bool: Compress the data from input_file and write it to compressed_file. Then verify the integrity by reading and decompressing the data from compressed_file and comparing it to the original data from input_file. Parameters: - input_file: str - The path to the input file containing the data to be compressed. - compressed_file: str - The path to the file where the compressed data should be written. Returns: - bool - True if the decompressed data matches the original data, False otherwise. ``` Input - `input_file` (str): The path to the plain text file from which to read the data to be compressed. - `compressed_file` (str): The path to the file where the compressed data should be saved. Output - `bool`: Returns `True` if the decompressed data matches the original data read from `input_file`, otherwise returns `False`. Constraints - You should handle any potential `IOError` during file operations and print an appropriate error message. - Assume that the input file is not empty and contains text data. - Compression should use the default compression level (9). Example Assume you have `original.txt` with the content: ``` Hello, this is a test file. It contains multiple lines of text. ``` ```python compress_and_verify(\'original.txt\', \'compressed.bz2\') ``` This should: 1. Read the contents of `original.txt`. 2. Compress the contents and write to `compressed.bz2`. 3. Read and decompress the data from `compressed.bz2`. 4. Verify that the decompressed data matches the original data and return `True`. Notes - Use `bz2.open` for file operations. - Leveraging both `compress` and `decompress` for one-shot operations could be beneficial. - The function should be efficient and handle the size constraints of typical text files gracefully.","solution":"import bz2 def compress_and_verify(input_file: str, compressed_file: str) -> bool: Compress the data from input_file and write it to compressed_file. Then verify the integrity by reading and decompressing the data from compressed_file and comparing it to the original data from input_file. Parameters: - input_file: str - The path to the input file containing the data to be compressed. - compressed_file: str - The path to the file where the compressed data should be written. Returns: - bool - True if the decompressed data matches the original data, False otherwise. try: # Read the original data from the input file with open(input_file, \'rb\') as file: original_data = file.read() # Compress the original data and write to the compressed file with bz2.open(compressed_file, \'wb\') as file: file.write(original_data) # Read and decompress the data from the compressed file with bz2.open(compressed_file, \'rb\') as file: decompressed_data = file.read() # Verify if the decompressed data matches the original data return decompressed_data == original_data except IOError as e: print(f\\"IOError: {e}\\") return False"},{"question":"# Semi-Supervised Learning with scikit-learn You are given a dataset consisting of labeled and unlabeled data points. Your task is to implement a semi-supervised learning model using scikit-learn\'s `LabelPropagation` algorithm and evaluate its performance. Dataset You have two files: 1. `labeled_data.csv`: Contains labeled data with features `f1`, `f2`, ..., `fn` and the `label`. 2. `unlabeled_data.csv`: Contains unlabeled data with features `f1`, `f2`, ..., `fn`. Instructions 1. Load the data from `labeled_data.csv` and `unlabeled_data.csv`. 2. Combine the labeled and unlabeled data, ensuring that unlabeled data points are marked with -1 for their labels. 3. Implement a semi-supervised learning model using scikit-learn\'s `LabelPropagation` algorithm. 4. Train the model on the combined dataset. 5. Predict the labels for the unlabeled data. 6. Evaluate the performance of the model using appropriate metrics. Expected Input and Output - **Input:** - `labeled_data.csv` (a CSV file) - `unlabeled_data.csv` (a CSV file) - **Output:** - Predicted labels for the unlabeled data as a numpy array. - Evaluation metrics (such as accuracy) for the model. Constraints - Ensure that the dataset is properly loaded and combined. - Handle any data preprocessing steps as required. - The evaluation metrics should reflect the model\'s ability to correctly classify labeled data. Performance Requirements - Efficiently handle the data loading and training process. - The solution should be robust and handle potential data inconsistencies gracefully. Example ```python import numpy as np import pandas as pd from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score # Load the data labeled_df = pd.read_csv(\'labeled_data.csv\') unlabeled_df = pd.read_csv(\'unlabeled_data.csv\') # Prepare the data X_labeled = labeled_df.drop(\'label\', axis=1).values y_labeled = labeled_df[\'label\'].values X_unlabeled = unlabeled_df.values y_unlabeled = -1 * np.ones(X_unlabeled.shape[0]) # Combine the data X = np.vstack([X_labeled, X_unlabeled]) y = np.concatenate([y_labeled, y_unlabeled]) # Implement the model model = LabelPropagation() model.fit(X, y) # Predict the labels for the unlabeled data predicted_labels = model.transduction_[-len(y_unlabeled):] # Evaluate the model predicted_labels_labeled_data = model.transduction_[:len(y_labeled)] accuracy = accuracy_score(y_labeled, predicted_labels_labeled_data) print(f\'Accuracy: {accuracy:.2f}\') # Output the predicted labels for the unlabeled data print(predicted_labels) ```","solution":"import numpy as np import pandas as pd from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score def semi_supervised_learning(labeled_data_path, unlabeled_data_path): # Load the data labeled_df = pd.read_csv(labeled_data_path) unlabeled_df = pd.read_csv(unlabeled_data_path) # Prepare the data X_labeled = labeled_df.drop(\'label\', axis=1).values y_labeled = labeled_df[\'label\'].values X_unlabeled = unlabeled_df.values y_unlabeled = -1 * np.ones(X_unlabeled.shape[0]) # Combine the data X = np.vstack([X_labeled, X_unlabeled]) y = np.concatenate([y_labeled, y_unlabeled]) # Implement the model model = LabelPropagation() model.fit(X, y) # Predict the labels for the unlabeled data predicted_labels = model.transduction_[-len(y_unlabeled):] # Evaluate the model predicted_labels_labeled_data = model.transduction_[:len(y_labeled)] accuracy = accuracy_score(y_labeled, predicted_labels_labeled_data) return predicted_labels, accuracy"},{"question":"**Coding Assessment Question** # Task You are given an AIFF (Audio Interchange File Format) file. Your task is to write a Python function that reads this file and extracts specific information from it. Additionally, you will write another function to create a new AIFF file with specified parameters. # Requirements 1. **Read AIFF file and extract information:** - Write a function `extract_aiff_info(filename: str) -> dict` that takes the name of an AIFF file as input and returns a dictionary with the following keys and their corresponding values: - `\'nchannels\'`: Number of audio channels (1 for mono, 2 for stereo). - `\'sampwidth\'`: Sample width in bytes. - `\'framerate\'`: Sampling rate in frames per second. - `\'nframes\'`: Number of audio frames. - `\'comptype\'`: Compression type. - `\'compname\'`: Compression name. - You should handle any potential errors during file reading appropriately. 2. **Create a new AIFF file:** - Write a function `create_aiff(filename: str, params: dict, data: bytes)` that takes the following arguments: - `filename`: The name of the new AIFF file to create. - `params`: A dictionary with the keys `\'nchannels\'`, `\'sampwidth\'`, `\'framerate\'`, `\'comptype\'`, and `\'compname\'` specifying the parameters for the new audio file. - `data`: A bytes-like object containing the audio frames. - The function should create an AIFF file with the specified parameters and write the provided audio data to it using the `aifc` module. # Example ```python # Example usage: info = extract_aiff_info(\'example.aiff\') print(info) # Output might be: # { # \'nchannels\': 2, # \'sampwidth\': 2, # \'framerate\': 44100, # \'nframes\': 100000, # \'comptype\': b\'NONE\', # \'compname\': b\'not compressed\' # } # Create a new AIFF file params = { \'nchannels\': 1, \'sampwidth\': 2, \'framerate\': 44100, \'comptype\': b\'NONE\', \'compname\': b\'not compressed\' } data = b\'x00x01x02x03...\' # Example audio data create_aiff(\'new_example.aiff\', params, data) ``` # Constraints - The input file must be a valid AIFF file. - Handle errors gracefully and return appropriate messages if the file cannot be read. - Ensure that the new AIFF file\'s header accurately reflects the audio data. # Performance Requirements - The functions should efficiently handle large AIFF files up to several hundred megabytes in size without excessive memory usage.","solution":"import aifc def extract_aiff_info(filename: str) -> dict: Extracts information from an AIFF file. Args: filename (str): The name of the AIFF file to read. Returns: dict: A dictionary containing information about the AIFF file. try: with aifc.open(filename, \'r\') as aiff_file: info = { \'nchannels\': aiff_file.getnchannels(), \'sampwidth\': aiff_file.getsampwidth(), \'framerate\': aiff_file.getframerate(), \'nframes\': aiff_file.getnframes(), \'comptype\': aiff_file.getcomptype(), \'compname\': aiff_file.getcompname() } return info except Exception as e: raise ValueError(f\\"Error reading AIFF file: {e}\\") def create_aiff(filename: str, params: dict, data: bytes): Creates a new AIFF file with given parameters and data. Args: filename (str): The name of the new AIFF file to create. params (dict): A dictionary containing parameters for the new audio file. data (bytes): A bytes-like object containing the audio frames. try: with aifc.open(filename, \'w\') as aiff_file: aiff_file.setnchannels(params[\'nchannels\']) aiff_file.setsampwidth(params[\'sampwidth\']) aiff_file.setframerate(params[\'framerate\']) aiff_file.setcomptype(params[\'comptype\'], params[\'compname\']) aiff_file.writeframes(data) except Exception as e: raise ValueError(f\\"Error creating AIFF file: {e}\\")"},{"question":"# Advanced Python Module Creation and Manipulation In this task, you will demonstrate your understanding of Python\'s module creation and management by implementing a custom Python module using the `python310` package. You will create a new module named \\"CustomModule\\", add functions to it, and manipulate its state. Requirements: 1. **Module Creation**: - Create a new Python module named \\"CustomModule\\" using `PyModule_NewObject`. 2. **Functions to be Added**: - Add a function named `add_numbers` that takes two integer arguments and returns their sum. - Add a function named `factorial` that computes the factorial of a given non-negative integer using recursion. 3. **Module State Management**: - Implement state management to store a counter that keeps track of how many times `add_numbers` has been called. 4. **Execution Phase**: - Ensure the module is properly initialized and ready to be used in Python scripts. Input and Output Formats: - **Input**: None - **Output**: You are required to simulate the creation of the module in Python and demonstrate function calls, showing the expected outputs. Constraints: - Use the `python310` package functionalities as described in the provided documentation. - Ensure proper error handling. - You are restricted to using the provided low-level module functions for the creation and manipulation of the module. Performance Requirements: - The `factorial` function must be implemented using recursion. - The implementation should efficiently manage the addition and invocation of module functions. # Example: ```python # You do not need to implement this part, but this is how the module might be used after your implementation. import CustomModule # Example function calls result_sum = CustomModule.add_numbers(5, 10) # Expected output: 15 factorial_result = CustomModule.factorial(5) # Expected output: 120 print(f\\"add_numbers function called: {CustomModule.call_count} times\\") # Example state access ``` Implement the project by writing Python code that creates and manages the required module with the specified functionalities.","solution":"import types # Custom module creation CustomModule = types.ModuleType(\\"CustomModule\\") # State management variable CustomModule.call_count = 0 def add_numbers(a, b): Returns the sum of two numbers and increments the call count. CustomModule.call_count += 1 return a + b def factorial(n): Returns the factorial of a non-negative integer n using recursion. if n < 0: raise ValueError(\\"Factorial is not defined for negative numbers\\") if n in [0, 1]: return 1 return n * factorial(n - 1) # Adding functions to the module CustomModule.add_numbers = add_numbers CustomModule.factorial = factorial # Making the module available import sys sys.modules[\\"CustomModule\\"] = CustomModule"},{"question":"Problem Statement: You are given a dataset containing sales information for different products in various regions over multiple years. Your task is to create a hierarchical `MultiIndex` DataFrame, perform various indexing, selection, and manipulation tasks, and return specific insights from the data. # Input: You will be given the following inputs: 1. A list of product names. 2. A list of regions. 3. A list of years. 4. A 3D list (nested lists) of sales data such that `sales_data[i][j][k]` represents the sales for product `i` in region `j` during year `k`. # Task: 1. Create a `MultiIndex` for the DataFrame using product names, regions, and years. 2. Fill the DataFrame with the provided sales data. 3. Perform the following operations: - Extract sales data for a specific product across all regions for a particular year. - Extract sales data for all products in a specific region across all years. - Compute the total sales for each product across all regions and years. - Compute the region-wise sales for all years combined, for a specific product. - Sort the DataFrame first by region, then by product, and finally by year. - Change the names of the levels to \\"Product\\", \\"Region\\", and \\"Year\\". - Reorder the levels to \\"Region\\", \\"Year\\", and \\"Product\\". - Remove unused levels if any, and display the resultant DataFrame. # Constraints: - The number of products, regions, and years will not exceed 100 each. - Sales data values are guaranteed to be non-negative integers. # Example: ```python products = [\\"ProductA\\", \\"ProductB\\"] regions = [\\"Region1\\", \\"Region2\\"] years = [2020, 2021] sales_data = [ [[10, 12], [14, 16]], # Sales data for ProductA in Region1, Region2 for 2020, 2021 [[20, 22], [24, 26]], # Sales data for ProductB in Region1, Region2 for 2020, 2021 ] # Expected output steps: # - Create a MultiIndex DataFrame. # - Extract required data as per tasks. # - Perform sorting, renaming, reordering and removal of unused levels. # Output should showcase key intermediate results and the final DataFrame. ``` # Submission: Please implement the function `process_sales_data(products, regions, years, sales_data)` that performs all the tasks described above and prints the intermediate and final results as specified.","solution":"import pandas as pd def process_sales_data(products, regions, years, sales_data): # Create a MultiIndex for the DataFrame index = pd.MultiIndex.from_product([products, regions, years], names=[\\"Product\\", \\"Region\\", \\"Year\\"]) data_flattened = [sales_data[i][j][k] for i in range(len(products)) for j in range(len(regions)) for k in range(len(years))] df = pd.DataFrame(data_flattened, index=index, columns=[\\"Sales\\"]).sort_index() # Extract sales data for a specific product across all regions for a particular year product_name = products[0] # example product year = years[0] # example year sales_for_product_year = df.loc[(product_name, slice(None), year)] # Extract sales data for all products in a specific region across all years region_name = regions[0] # example region sales_for_region_all_years = df.loc[(slice(None), region_name, slice(None))] # Compute the total sales for each product across all regions and years total_sales_per_product = df.groupby(level=\\"Product\\").sum() # Compute the region-wise sales for all years combined, for a specific product product_for_region_sales = df.loc[product_name].groupby(level=\\"Region\\").sum() # Sort the DataFrame first by region, then by product, and finally by year df_sorted = df.sort_index(level=[\\"Region\\", \\"Product\\", \\"Year\\"]) # Change the names of the levels to \\"Product\\", \\"Region\\", and \\"Year\\" df_sorted.rename_axis([\\"Product\\", \\"Region\\", \\"Year\\"], inplace=True) # Reorder the levels to \\"Region\\", \\"Year\\", and \\"Product\\" df_reordered = df_sorted.reorder_levels([\\"Region\\", \\"Year\\", \\"Product\\"]).sort_index() # Remove unused levels and display the resultant DataFrame df_final = df_reordered.reset_index().set_index([\\"Region\\", \\"Year\\", \\"Product\\"]) return { \\"df\\": df, \\"sales_for_product_year\\": sales_for_product_year, \\"sales_for_region_all_years\\": sales_for_region_all_years, \\"total_sales_per_product\\": total_sales_per_product, \\"product_for_region_sales\\": product_for_region_sales, \\"df_sorted\\": df_sorted, \\"df_reordered\\": df_reordered, \\"df_final\\": df_final }"},{"question":"# Task You are required to develop a secure password storage system using the `hashlib` module in Python 3.10. Your solution should involve creating hashes for passwords using salt and a key derivation function, ensuring that it is robust against brute-force attacks. # Requirements 1. **Function Definition:** Implement the function `store_password(password: str, salt: bytes, iterations: int) -> dict`. 2. **Inputs:** - `password` (str): The user’s password in plain text. - `salt` (bytes): A random salt value. - `iterations` (int): The number of iterations to perform in the key derivation function. 3. **Outputs:** - Return a dictionary containing: - `salt` (hex): The salt used for hashing (in hexadecimal format). - `iterations` (int): The number of iterations used. - `hash` (hex): The resulting hash of the password (in hexadecimal format). 4. **Constraints:** - The password, after being encoded to bytes, should be hashed using the `pbkdf2_hmac` function. - Choose a secure hash algorithm (`sha256` is recommended). - Ensure that appropriate error handling is in place for incorrect input types. # Example Usage: ```python from hashlib import pbkdf2_hmac import os def store_password(password: str, salt: bytes, iterations: int) -> dict: if not isinstance(password, str): raise ValueError(\\"Password must be a string.\\") if not isinstance(salt, bytes): raise ValueError(\\"Salt must be bytes.\\") if not isinstance(iterations, int) or iterations <= 0: raise ValueError(\\"Iterations must be a positive integer.\\") # Encoding password to bytes password_bytes = password.encode(\'utf-8\') # Using pbkdf2_hmac to derive the secure hash dk = pbkdf2_hmac(\'sha256\', password_bytes, salt, iterations) # Returning the results as a dictionary return { \'salt\': salt.hex(), \'iterations\': iterations, \'hash\': dk.hex() } # Example salt_value = os.urandom(16) # Randomly generated salt of 16 bytes iterations_count = 100000 # More iterations for better security password = \\"securepassword\\" stored_data = store_password(password, salt_value, iterations_count) print(stored_data) ``` The example demonstrates how you can use the function `store_password` to create a secure password storage mechanism. # Notes: - Make sure to use appropriate exception handling to manage incorrect inputs. - Generating random salt values securely can be done using `os.urandom()`. - You may use predefined values of salt and iterations for testing purposes if needed. - The iteration count should be chosen judiciously to balance security and performance.","solution":"import hashlib def store_password(password: str, salt: bytes, iterations: int) -> dict: if not isinstance(password, str): raise ValueError(\\"Password must be a string.\\") if not isinstance(salt, bytes): raise ValueError(\\"Salt must be bytes.\\") if not isinstance(iterations, int) or iterations <= 0: raise ValueError(\\"Iterations must be a positive integer.\\") # Encoding password to bytes password_bytes = password.encode(\'utf-8\') # Using pbkdf2_hmac to derive the secure hash dk = hashlib.pbkdf2_hmac(\'sha256\', password_bytes, salt, iterations) # Returning the results as a dictionary return { \'salt\': salt.hex(), \'iterations\': iterations, \'hash\': dk.hex() }"},{"question":"Using the `python310` package functions related to the iterator protocol, implement the following: 1. Write a C extension function `py_iter_sum` that takes an iterable Python object and returns the sum of all its elements using the `PyIter_Next` function. 2. Write a C extension function `py_aiter_collect` that takes an asynchronous iterable Python object and returns a Python list containing all its elements. This function should utilize `PyIter_Send`. Input Format 1. For `py_iter_sum`: - A single iterable Python object (e.g., list, tuple, etc.) 2. For `py_aiter_collect`: - A single asynchronous iterable Python object (e.g., async generator or any object implementing the `__aiter__` and `__anext__` methods). Output Format 1. For `py_iter_sum`: - A single integer or float representing the sum of all elements in the iterable. 2. For `py_aiter_collect`: - A Python list containing all elements generated by the asynchronous iterator. Example Usage Assuming the module name is `myiterator`: ```python import myiterator # Example for py_iter_sum print(myiterator.py_iter_sum([1, 2, 3, 4, 5])) # Output: 15 # Example for py_aiter_collect with an async generator async def async_gen(): yield 1 yield 2 yield 3 import asyncio result = asyncio.run(myiterator.py_aiter_collect(async_gen())) print(result) # Output: [1, 2, 3] ``` Constraints - Both functions should handle invalid inputs gracefully, returning an appropriate Python exception in error cases. - Make sure to manage reference counts properly to avoid memory leaks. Performance Requirements - `py_iter_sum` should have a time complexity of O(n), where n is the number of elements in the iterable. - `py_aiter_collect` should be able to handle large numbers of asynchronous yields efficiently, though exact performance will depend on the implementation of the async iterable.","solution":"from typing import Iterable, Any import asyncio def py_iter_sum(iterable: Iterable) -> float: Sum up all the elements in the iterable. :param iterable: An iterable with elements that can be summed. :return: Sum of the elements. total = 0 it = iter(iterable) try: while True: total += next(it) except StopIteration: pass return total async def py_aiter_collect(a_iterable: Any) -> list: Collect all elements from an async iterable into a list. :param a_iterable: An async iterable. :return: List of all elements from the async iterable. result = [] async for item in a_iterable: result.append(item) return result"},{"question":"# Question: Comprehensive `typing` Module Utilization You are required to implement a small library that manages user authentication tokens. The task involves defining types using the `typing` module to ensure type safety across the functions and classes in your library. Part 1: Type Aliases and NewType 1. Define a `Token` alias for the type `str`. 2. Define a distinct type `UserId`, derived from `int` type using `NewType`. This ensures user IDs are not mistakenly used as integers elsewhere. Part 2: Functions using Type Hints 3. Write a function `generate_token(user_id: UserId) -> Token` that takes a `UserId` and returns a `Token`. 4. Write a function `validate_token(token: Token) -> bool` that takes a `Token` and returns `True` if it\'s valid, `False` otherwise. For simplicity, consider tokens valid if they are exactly 32 characters long. Part 3: Protocols and Callable 5. Define a `TokenStorageProtocol` protocol that has two methods: - `save_token(user_id: UserId, token: Token) -> None` - `get_token(user_id: UserId) -> Optional[Token]` 6. Write a function `authenticate_user(user_id: UserId, token_storage: TokenStorageProtocol, token_validator: Callable[[Token], bool]) -> bool` that uses instances of `TokenStorageProtocol` and a token validator function to check if a user is authenticated. # Constraints: - `UserId` should be treated as a distinct type different from `int`. - `generate_token` should return a dummy token for the purpose of this question (e.g., `\\"dummy_token_\\" + str(user_id)`). - `validate_token` should only validate that the token length matches 32 characters. - Include examples in your function docstrings showcasing how they might be used. # Example Usage: ```python # Define types Token = ... UserId = ... # Functions def generate_token(user_id: UserId) -> Token: ... def validate_token(token: Token) -> bool: ... # Protocol class TokenStorageProtocol(Protocol): ... # Authentication def authenticate_user(user_id: UserId, token_storage: TokenStorageProtocol, token_validator: Callable[[Token], bool]) -> bool: ... # Example to follow usage u_id: UserId = UserId(1001) tkn: Token = generate_token(u_id) result: bool = validate_token(tkn) # Example implementation of TokenStorageProtocol class InMemoryTokenStorage: def __init__(self): self.storage = {} def save_token(self, user_id: UserId, token: Token) -> None: self.storage[user_id] = token def get_token(self, user_id: UserId) -> Optional[Token]: return self.storage.get(user_id) # Using authenticate_user function storage = InMemoryTokenStorage() storage.save_token(u_id, tkn) is_authenticated = authenticate_user(u_id, storage, validate_token) ``` Submit your implementation with type hints and the described functionality.","solution":"from typing import NewType, Optional, Protocol, Callable # Define type aliases and NewType Token = str UserId = NewType(\'UserId\', int) # Function to generate a token def generate_token(user_id: UserId) -> Token: Generates a dummy token for the given user_id. Example: generate_token(UserId(1001)) -> \'dummy_token_1001\' return \\"dummy_token_\\" + str(user_id) # Function to validate a token def validate_token(token: Token) -> bool: Validates the token length. Example: validate_token(\'dummy_token_1001\') -> False validate_token(\'x\' * 32) -> True return len(token) == 32 # Protocol for token storage class TokenStorageProtocol(Protocol): def save_token(self, user_id: UserId, token: Token) -> None: ... def get_token(self, user_id: UserId) -> Optional[Token]: ... # Function to authenticate user def authenticate_user(user_id: UserId, token_storage: TokenStorageProtocol, token_validator: Callable[[Token], bool]) -> bool: Authenticates a user based on the token stored. Example: storage = InMemoryTokenStorage() token = generate_token(UserId(1001)) storage.save_token(UserId(1001), token) authenticate_user(UserId(1001), storage, validate_token) -> True / False token = token_storage.get_token(user_id) if token is None: return False return token_validator(token) # Example implementation of TokenStorageProtocol class InMemoryTokenStorage: def __init__(self): self.storage = {} def save_token(self, user_id: UserId, token: Token) -> None: self.storage[user_id] = token def get_token(self, user_id: UserId) -> Optional[Token]: return self.storage.get(user_id)"},{"question":"**Coding Assessment Question** # Problem Description: You are tasked with implementing a basic file management system that logs file operations and ensures that all file handles are properly closed upon the program\'s termination. The `atexit` module should be used to achieve this. Your task is to create a class `FileManager` which will handle the following functionalities: 1. `open_file(filename, mode)`: Opens a file in the specified mode and returns the file handle. The filename and mode should be logged to a list `log`. 2. `close_file(file_handle)`: Closes the specified file handle if it is open and removes it from the list of open files. 3. `save_log(logfile)`: Writes all logged operations to the specified log file. This method should be automatically called at interpreter termination. # Requirements: - Use the `atexit` module to ensure that `save_log` is called at the end of the program. - The `log` list should store strings in the format \\"Opened <filename> in <mode>\\". - Manage file handles to prevent memory leaks or file access issues. - Implement error handling to properly log if a file could not be opened. # Constraints: - You should ensure that the `log` list and file handles are managed within the class without external interference. - The `open_file` function should raise an IOError if the file cannot be opened. # Input and Output: - Your class does not need input functions or interactive components; it will be tested in a controlled environment. - The `save_log` function will write to a file, but this will not be explicitly tested. # Performance: - Handle multiple files efficiently without unnecessary resource overhead. # Example: ```python import atexit class FileManager: def __init__(self): self.log = [] self.open_files = [] # Register the save_log method to be called at program termination atexit.register(self.save_log, \'operation_log.txt\') def open_file(self, filename, mode): try: file_handle = open(filename, mode) self.log.append(f\\"Opened {filename} in {mode}\\") self.open_files.append(file_handle) return file_handle except IOError as e: self.log.append(f\\"Failed to open {filename} in {mode}: {e}\\") raise IOError(f\\"Failed to open {filename} in {mode}\\") def close_file(self, file_handle): if file_handle in self.open_files: file_handle.close() self.open_files.remove(file_handle) def save_log(self, logfile): with open(logfile, \'w\') as log_file: for entry in self.log: log_file.write(entry + \'n\') # Usage example: # fm = FileManager() # file1 = fm.open_file(\'test1.txt\', \'w\') # file2 = fm.open_file(\'test2.txt\', \'r\') # fm.close_file(file1) # # At the end of the program, \'operation_log.txt\' will be written automatically. ``` # Notes: - You need to implement all methods as described. - Ensure proper use of the `atexit` module to register the `save_log` function.","solution":"import atexit class FileManager: def __init__(self): self.log = [] self.open_files = [] # Register the save_log method to be called at program termination atexit.register(self.save_log, \'operation_log.txt\') def open_file(self, filename, mode): try: file_handle = open(filename, mode) self.log.append(f\\"Opened {filename} in {mode}\\") self.open_files.append(file_handle) return file_handle except IOError as e: self.log.append(f\\"Failed to open {filename} in {mode}: {e}\\") raise IOError(f\\"Failed to open {filename} in {mode}\\") def close_file(self, file_handle): if file_handle in self.open_files: file_handle.close() self.open_files.remove(file_handle) def save_log(self, logfile): with open(logfile, \'w\') as log_file: for entry in self.log: log_file.write(entry + \'n\')"},{"question":"**Problem Statement: Organizing Project Files** You are tasked with organizing a set of project files stored in a directory. The directory contains various types of files, including text files, Python source files, and possibly other types. Your goal is to create a script that will: 1. Move all `.txt` files to a subdirectory named `texts/`. 2. Move all `.py` files to a subdirectory named `scripts/`. 3. Log the total number of files moved to each subdirectory. 4. Handle any other file types by printing a message indicating the file type and its location. # Requirements: 1. Create the necessary subdirectories (`texts/` and `scripts/`) if they do not exist. 2. Use the `pathlib` module extensively for path manipulations and filesystem operations. 3. The script should be able to handle nested directories (i.e., it should search for files recursively). # Input: - A single directory path as a string, pointing to the root of the project directory. # Output: - Messages indicating the number of files moved to each subdirectory. - Messages indicating any other file types and their paths. # Example: Suppose the project directory structure is as follows: ``` project/ ├── README.md ├── data │ └── data.txt ├── notes.txt └── script.py ``` The script should produce the following output: ``` Moved 2 files to \'texts/\'. Moved 1 file to \'scripts/\'. Other file types: - Markdown File: \'project/README.md\' ``` # Constraints: - Assume file names do not include special characters. - Assume you have read/write permissions for all files in the given directory. - The number of files can be large, so make sure your code is efficient. # Function Signature: ```python from pathlib import Path def organize_files(project_dir: str) -> None: # Your code here ``` # Implementation Guidelines: 1. Import the necessary classes and methods from the `pathlib` module. 2. Implement file moving logic using `Path` and related classes/methods. 3. Use appropriate error handling for cases like non-existent directories or permissions issues. 4. Provide detailed comments explaining each step of the implementation. # Hints: - Use `Path.glob()` or `Path.rglob()` for recursive file searching. - The method `Path.mkdir(parents=True, exist_ok=True)` can help in creating directories. - Utilize `Path.rename()` for moving files. - Remember to handle both absolute and relative paths appropriately.","solution":"from pathlib import Path def organize_files(project_dir: str) -> None: project_path = Path(project_dir) # Create subdirectories if they don\'t exist texts_path = project_path / \'texts\' scripts_path = project_path / \'scripts\' texts_path.mkdir(parents=True, exist_ok=True) scripts_path.mkdir(parents=True, exist_ok=True) txt_files_count = 0 py_files_count = 0 # Recursively search for files for file_path in project_path.rglob(\'*\'): if file_path.is_file(): if file_path.suffix == \'.txt\': target_path = texts_path / file_path.name file_path.rename(target_path) txt_files_count += 1 elif file_path.suffix == \'.py\': target_path = scripts_path / file_path.name file_path.rename(target_path) py_files_count += 1 else: print(f\\"Other file type: \'{file_path.suffix}\' at \'{file_path}\'\\") # Log the results print(f\\"Moved {txt_files_count} files to \'texts/\'.\\") print(f\\"Moved {py_files_count} files to \'scripts/\'.\\")"},{"question":"You are given a set of functions wrapped around the macros provided by the \\"datetime\\" module in Python. Your task is to implement these functions to create specific datetime objects and extract fields from them. Demonstrate your understanding by completing the implementations and performing given tasks. Functions to Implement: 1. **create_date**: - **Input**: Three integers `year`, `month`, and `day`. - **Output**: Return a `datetime.date` object initialized with the given `year`, `month`, and `day`. 2. **create_datetime**: - **Input**: Seven integers `year`, `month`, `day`, `hour`, `minute`, `second`, and `microsecond`. - **Output**: Return a `datetime.datetime` object initialized with the given parameters. 3. **extract_year**: - **Input**: A `datetime.date` or `datetime.datetime` object. - **Output**: Return the year field as an integer. 4. **extract_time_components**: - **Input**: A `datetime.datetime` object. - **Output**: Return a tuple of integers representing the `hour`, `minute`, `second`, and `microsecond` components of the given datetime object. 5. **is_utc**: - **Input**: A `datetime.datetime` object. - **Output**: Return a boolean indicating whether the datetime object is in the UTC timezone. Constraints: - You must use the respective macros provided in the documentation to create objects and extract fields. - Be mindful of the types and input validity as indicated in the provided document. # Example: ```python # Example usage: # Creating a date object date_obj = create_date(2023, 10, 5) assert date_obj.year == 2023 assert date_obj.month == 10 assert date_obj.day == 5 # Creating a datetime object datetime_obj = create_datetime(2023, 10, 5, 14, 30, 15, 123456) assert datetime_obj.year == 2023 assert datetime_obj.month == 10 assert datetime_obj.day == 5 assert datetime_obj.hour == 14 assert datetime_obj.minute == 30 assert datetime_obj.second == 15 assert datetime_obj.microsecond == 123456 # Extracting year year = extract_year(datetime_obj) assert year == 2023 # Extracting time components time_components = extract_time_components(datetime_obj) assert time_components == (14, 30, 15, 123456) # Checking UTC timezone utc_obj = datetime_obj.replace(tzinfo=timezone.utc) assert is_utc(utc_obj) == True ``` You can assume that all input values will be within the valid ranges for the respective fields. Your Implementation: ```python from datetime import datetime, date, time, timedelta, timezone def create_date(year, month, day): # Implementation here pass def create_datetime(year, month, day, hour, minute, second, microsecond): # Implementation here pass def extract_year(dt_obj): # Implementation here pass def extract_time_components(dt_obj): # Implementation here pass def is_utc(dt_obj): # Implementation here pass ```","solution":"from datetime import datetime, date, timezone def create_date(year, month, day): Returns a date object initialized with the given year, month, and day. return date(year, month, day) def create_datetime(year, month, day, hour, minute, second, microsecond): Returns a datetime object initialized with the given year, month, day, hour, minute, second, and microsecond. return datetime(year, month, day, hour, minute, second, microsecond) def extract_year(dt_obj): Returns the year of the datetime or date object. return dt_obj.year def extract_time_components(dt_obj): Returns a tuple of (hour, minute, second, microsecond) from the datetime object. return (dt_obj.hour, dt_obj.minute, dt_obj.second, dt_obj.microsecond) def is_utc(dt_obj): Returns True if the datetime object is in UTC timezone; False otherwise. return dt_obj.tzinfo is timezone.utc"},{"question":"# Custom Email Policy Implementation **Objective:** Create a custom policy for handling email headers and use it to serialize an email message with specific constraints. **Problem Statement:** Implement a function `custom_email_handling(file_path: str) -> str` that: 1. Reads an email message from a given file path. 2. Applies a custom policy that: - Limits the maximum line length (`max_line_length`) to 50 characters. - Ensures any defects encountered during parsing are raised as errors (`raise_on_defect = True`). - Uses `rn` as the line separator in the serialized output. 3. Serializes the email message using the custom policy. 4. Returns the serialized message as a string. **Input Format:** - `file_path` (str): A string representing the path to the email file to be read. **Output Format:** - Return a string representing the serialized email message with the applied custom policy. **Constraints:** - You may assume that the input file contains a valid email message. - Ensure your solution handles reading binary files as email files can contain non-text content. **Example:** Suppose the content of the file at `file_path` is: ``` From: sender@example.com To: receiver@example.com Subject: Test Email This is the body of the test email. ``` Assuming that might be a binary file, calling `custom_email_handling(file_path)` should serialize the message with the specified policy settings and return a string that could look like: ``` From: sender@example.comrn To: receiver@example.comrn Subject: Test Emailrn rn This is the body of the test email.rn ``` (Notice the serialized output should respect the line length and line separator settings from the custom policy). **Function Signature:** ```python def custom_email_handling(file_path: str) -> str: pass ``` **Notes:** - Use the `email` package\'s utilities like `email.message_from_binary_file` and customize the emailing behavior using the `email.policy` module. - The handling of defects and how headers are serialized should comply with the constraints specified in the custom policy.","solution":"from email import policy from email.parser import BytesParser import pathlib def custom_email_handling(file_path: str) -> str: # Define a custom policy with the specified constraints custom_policy = policy.default.clone(max_line_length=50, raise_on_defect=True, linesep=\'rn\') # Ensure the file is read in binary mode file_path = pathlib.Path(file_path) with file_path.open(\'rb\') as f: email_obj = BytesParser(policy=custom_policy).parse(f) # Serialize the email using the custom policy serialized_email = email_obj.as_string(policy=custom_policy) return serialized_email"},{"question":"# Problem Description You are tasked with simulating the behavior of a web crawler that adheres to the rules specified in a `robots.txt` file. You need to build a simplified version of the `RobotFileParser` class from the `urllib.robotparser` module. This will involve: 1. Reading and parsing a provided `robots.txt` file. 2. Determining whether a given user agent can fetch a specific URL based on the rules in the `robots.txt` file. 3. Retrieving crawl delay and sitemap information from the `robots.txt` file. # Requirements 1. **Class Definition**: - Define a class `SimpleRobotFileParser`. - Implement the following methods: - `__init__(self, url: str)`: Initializes the class with the URL of the `robots.txt` file. - `set_url(self, url: str)`: Sets a new URL for the `robots.txt` file. - `read(self)`: Reads and parses the `robots.txt` file from the URL. - `can_fetch(self, useragent: str, url: str) -> bool`: Returns `True` if the `useragent` is allowed to fetch the `url`, otherwise `False`. - `crawl_delay(self, useragent: str) -> int`: Returns the `Crawl-delay` parameter for the given `useragent`, or `None` if it’s not specified. - `site_maps(self) -> list`: Returns a list of sitemap URLs specified in the `robots.txt` file, or `None` if none are specified. 2. **Input and Output**: - **Input**: - `SimpleRobotFileParser` initialization with a URL (`str`) pointing to a `robots.txt` file. - Method calls: - `set_url(url: str)`: Update the URL of the `robots.txt` file. - `read()`: Read and parse the `robots.txt` file from the new URL. - `can_fetch(useragent: str, url: str) -> bool`: Check if the given user agent can access the URL. - `crawl_delay(useragent: str) -> int`: Retrieve the crawl delay value for the specified user agent. - `site_maps() -> list`: Retrieve the list of sitemaps from the `robots.txt` file. - **Output**: - A boolean value (`True` or `False`) for the `can_fetch()` method. - An integer or `None` for the `crawl_delay()` method. - A list or `None` for the `site_maps()` method. # Constraints - Assume the `robots.txt` file is not larger than 1MB. - The file\'s format adheres to the robots.txt standards. - Handle network errors gracefully when fetching the `robots.txt` file. # Example ```python # Example usage parser = SimpleRobotFileParser(\\"http://www.example.com/robots.txt\\") parser.read() print(parser.can_fetch(\\"Googlebot\\", \\"http://www.example.com/secret-page\\")) # Output: False print(parser.crawl_delay(\\"Googlebot\\")) # Output might be: 10 print(parser.site_maps()) # Output might be: [\\"http://www.example.com/sitemap.xml\\"] ``` # Notes - Do not use the `urllib.robotparser` module directly in your implementation. - You may use standard libraries such as `requests` for fetching the `robots.txt` file and `time` for handling delays.","solution":"import requests from urllib.parse import urlparse, urljoin class SimpleRobotFileParser: def __init__(self, url: str): self.url = url self.rules = {} self.sitemaps = [] def set_url(self, url: str): self.url = url def read(self): try: response = requests.get(self.url) response.raise_for_status() self._parse(response.text) except requests.RequestException: self.rules = {} self.sitemaps = [] def _parse(self, robots_txt: str): current_user_agent = None lines = robots_txt.splitlines() for line in lines: line = line.strip() if not line or line.startswith(\'#\'): continue if line.lower().startswith(\\"user-agent:\\"): current_user_agent = line.split(\\":\\", 1)[1].strip() if current_user_agent not in self.rules: self.rules[current_user_agent] = { \\"disallow\\": [], \\"allow\\": [], \\"crawl-delay\\": None } elif current_user_agent: if line.lower().startswith(\\"disallow:\\"): path = line.split(\\":\\", 1)[1].strip() self.rules[current_user_agent][\\"disallow\\"].append(path) elif line.lower().startswith(\\"allow:\\"): path = line.split(\\":\\", 1)[1].strip() self.rules[current_user_agent][\\"allow\\"].append(path) elif line.lower().startswith(\\"crawl-delay:\\"): delay = int(line.split(\\":\\", 1)[1].strip()) self.rules[current_user_agent][\\"crawl-delay\\"] = delay elif line.lower().startswith(\\"sitemap:\\"): sitemap_url = line.split(\\":\\", 1)[1].strip() self.sitemaps.append(sitemap_url) def can_fetch(self, useragent: str, url: str) -> bool: parsed_url = urlparse(self.url) base_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" path = url[len(base_url):] if useragent in self.rules: rules = self.rules[useragent] elif \'*\' in self.rules: rules = self.rules[\'*\'] else: return True for allow in rules[\\"allow\\"]: if path.startswith(allow): return True for disallow in rules[\\"disallow\\"]: if path.startswith(disallow): return False return True def crawl_delay(self, useragent: str) -> int: if useragent in self.rules: return self.rules[useragent][\\"crawl-delay\\"] elif \'*\' in self.rules: return self.rules[\'*\'][\\"crawl-delay\\"] return None def site_maps(self) -> list: return self.sitemaps if self.sitemaps else None"},{"question":"# Python Dictionary Manipulation with C API In this assessment, you\'re required to write C functions that interact with Python dictionaries using the C API functions provided. Your task will be to implement a comprehensive function that performs several dictionary operations. Task: Implement a C function `modify_dict` that takes a PyObject pointer to a dictionary and performs the following operations sequentially: 1. **Check type**: Ensure the input object is indeed a dictionary. 2. **Add Items**: Insert three key-value pairs into the dictionary: - \\"one\\" -> 1 - \\"two\\" -> 2 - \\"three\\" -> 3 3. **Check and Remove Item**: Check if the key \\"two\\" is in the dictionary. If it is, remove it. 4. **Merge with Another Dictionary**: Create another dictionary with the following items: - \\"two\\" -> \\"II\\" - \\"four\\" -> 4 Then, merge this dictionary into the original one, allowing the new values to overwrite existing ones. 5. **Retrieve Values**: Retrieve all the values of the modified dictionary and return them as a list. Function Signature: ```c PyObject* modify_dict(PyObject* p) ``` Implementation Details: - **Input**: - `p`: A pointer to a Python dictionary object. - **Output**: - A list of all values in the modified dictionary. - **Constraints**: - Use only the provided dictionary API functions for all operations. - Handle all potential errors gracefully, returning `NULL` and setting an appropriate error message if any operation fails. ```c #include <Python.h> PyObject* modify_dict(PyObject* p) { // Step 1: Check type if (!PyDict_Check(p)) { PyErr_SetString(PyExc_TypeError, \\"Input object must be a dictionary\\"); return NULL; } // Step 2: Add items PyObject *one = PyLong_FromLong(1); PyObject *two = PyLong_FromLong(2); PyObject *three = PyLong_FromLong(3); if (PyDict_SetItemString(p, \\"one\\", one) < 0 || PyDict_SetItemString(p, \\"two\\", two) < 0 || PyDict_SetItemString(p, \\"three\\", three) < 0) { Py_DECREF(one); Py_DECREF(two); Py_DECREF(three); return NULL; } Py_DECREF(one); Py_DECREF(two); Py_DECREF(three); // Step 3: Check and remove item if (PyDict_Contains(p, PyUnicode_FromString(\\"two\\"))) { if (PyDict_DelItemString(p, \\"two\\") < 0) { return NULL; } } // Step 4: Merge with another dictionary PyObject *new_dict = PyDict_New(); if (!new_dict) return NULL; PyObject *II = PyUnicode_FromString(\\"II\\"); PyObject *four = PyLong_FromLong(4); if (PyDict_SetItemString(new_dict, \\"two\\", II) < 0 || PyDict_SetItemString(new_dict, \\"four\\", four) < 0) { Py_DECREF(II); Py_DECREF(four); Py_DECREF(new_dict); return NULL; } Py_DECREF(II); Py_DECREF(four); if (PyDict_Merge(p, new_dict, 1) < 0) { Py_DECREF(new_dict); return NULL; } Py_DECREF(new_dict); // Step 5: Retrieve values PyObject *values = PyDict_Values(p); return values; } ``` Testing and Usage: To test your function, you can create a Python module with this C code and call the function from a Python script, passing in various dictionary objects to see the changes being made. **Note**: Ensure that your workspace is set up to compile and link against the Python libraries properly.","solution":"def modify_dict(d): Modifies the dictionary according to the given steps: 1. Adds three key-value pairs: \\"one\\" -> 1, \\"two\\" -> 2, \\"three\\" -> 3 2. Removes the key \\"two\\" if it exists 3. Merges another dictionary with the original one: {\\"two\\" -> \\"II\\", \\"four\\" -> 4} 4. Returns the values of the modified dictionary as a list :param d: The original dictionary :type d: dict :return: A list of the values of the modified dictionary :rtype: list if not isinstance(d, dict): raise TypeError(\\"Input object must be a dictionary\\") # Step 2: Add items d[\\"one\\"] = 1 d[\\"two\\"] = 2 d[\\"three\\"] = 3 # Step 3: Check and remove item if \\"two\\" in d: del d[\\"two\\"] # Step 4: Merge with another dictionary new_dict = {\\"two\\": \\"II\\", \\"four\\": 4} d.update(new_dict) # Step 5: Retrieve values values = list(d.values()) return values"},{"question":"**Objective:** Assess the students\' ability to load datasets using the scikit-learn `datasets` module, preprocess the data, and implement a basic machine learning pipeline. # Problem Description You need to load the `California Housing` dataset, preprocess the data, and implement a machine learning model to predict the median house value. # Instructions 1. Load the `California Housing` dataset using `sklearn.datasets.fetch_california_housing`. 2. Preprocess the data: 1. Split the data into training and testing sets (80-20 split). 2. Standardize the features to have zero mean and unit variance. 3. Implement a `Ridge` regression model using `sklearn.linear_model.Ridge`. 4. Train the model using the training data. 5. Evaluate the model using mean squared error on the test data. # Constraints - The solution should be implemented using Python and scikit-learn. - You may not use any other machine learning packages aside from scikit-learn. - Ensure to handle any potential missing values (although this specific dataset is clean). # Input and Output - No input is needed from the user. The data should be loaded programmatically. - The function should return the mean squared error of the model on the test data. # Example Function Signature ```python def california_housing_model(): from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error # 1. Load the California Housing dataset data = fetch_california_housing() # 2. Preprocess the data X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 3. Implement and train the Ridge regression model model = Ridge() model.fit(X_train, y_train) # 4. Evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse # Expected Output: A float value representing the mean squared error of the model on the test data. ``` # Notes - The `fetch_california_housing` function will automatically download the dataset if it\'s not already available. - Make sure your code is clean and well-documented.","solution":"def california_housing_model(): from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error # 1. Load the California Housing dataset data = fetch_california_housing() # 2. Preprocess the data X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 3. Implement and train the Ridge regression model model = Ridge() model.fit(X_train, y_train) # 4. Evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Advanced URL Handling with Urllib **Objective:** You are required to implement a set of functions using the `urllib` package to handle various URL-related tasks. This will test your understanding of opening and reading URLs, handling exceptions, parsing URLs, and adhering to `robots.txt` rules. **Task 1: Fetching Data from a URL** - Implement a function `fetch_data(url: str) -> str` that takes a URL as an input and returns the content of the URL as a string. - This function should handle `HTTPError` and `URLError` exceptions and return appropriate error messages as strings. **Task 2: Parsing a URL** - Implement a function `parse_url(url: str) -> dict` that takes a URL as input and returns a dictionary containing the scheme, netloc, path, params, query, and fragment of the URL. **Task 3: Checking Robots.txt Permissions** - Implement a function `can_fetch(url: str, user_agent: str = \'urllib_user\') -> bool` that takes a URL and an optional user agent string as inputs. - This function should use the robots.txt rules to check if the given user agent is allowed to fetch the URL. Return `True` if allowed, and `False` otherwise. **Input Format:** - For `fetch_data`, input will be a single string representing the URL. - For `parse_url`, input will be a single string representing the URL. - For `can_fetch`, input will be a string representing the URL and an optional string representing the user agent. **Output Format:** - For `fetch_data`, output will be the content of the URL as a string or an error message as a string. - For `parse_url`, output will be a dictionary with keys (`scheme`, `netloc`, `path`, `params`, `query`, `fragment`) and their corresponding values. - For `can_fetch`, output will be a boolean value. **Constraints:** - You may assume that all input URLs are syntactically valid. - Network-related functions (`fetch_data` and `can_fetch`) should handle exceptions gracefully and should not cause the program to crash. - The maximum expected length for any URL is 2048 characters. **Example Usage:** ```python # Task 1: Fetching Data from a URL print(fetch_data(\\"http://www.example.com\\")) # Expected output: \\"<html>...</html>\\" - HTML content of the example.com or error message. # Task 2: Parsing a URL print(parse_url(\\"http://www.example.com/path?query=param#fragment\\")) # Expected output: { # \\"scheme\\": \\"http\\", # \\"netloc\\": \\"www.example.com\\", # \\"path\\": \\"/path\\", # \\"params\\": \\"\\", # \\"query\\": \\"query=param\\", # \\"fragment\\": \\"fragment\\" # } # Task 3: Checking Robots.txt Permissions print(can_fetch(\\"http://www.example.com/some-page\\", user_agent=\\"my_bot\\")) # Expected output: True (if my_bot is allowed as per robots.txt) or False (otherwise) ``` **Hints:** - Use `urllib.request.urlopen` for fetching the content of a URL. - Use `urllib.parse.urlparse` for parsing the URL. - Use `urllib.robotparser.RobotFileParser` for parsing robots.txt files.","solution":"import urllib.request import urllib.error import urllib.parse import urllib.robotparser def fetch_data(url: str) -> str: Fetch data from the given URL and return the content as a string. Handles HTTPError and URLError exceptions. try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTPError: {e.reason}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\" def parse_url(url: str) -> dict: Parse the given URL and return a dictionary with scheme, netloc, path, params, query, and fragment. result = urllib.parse.urlparse(url) return { \\"scheme\\": result.scheme, \\"netloc\\": result.netloc, \\"path\\": result.path, \\"params\\": result.params, \\"query\\": result.query, \\"fragment\\": result.fragment, } def can_fetch(url: str, user_agent: str = \'urllib_user\') -> bool: Check whether the given user agent is allowed to fetch the URL based on robots.txt rules. parsed_url = urllib.parse.urlparse(url) base_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(base_url) try: rp.read() return rp.can_fetch(user_agent, url) except Exception: return False"},{"question":"# Advanced Python Debugging with `pdb` Objective Your task is to implement a Python program that uses the `pdb` debugger to debug a simulated complex program with various functions and error handling. This exercise will demonstrate your understanding of how to set breakpoints, step through code, inspect and modify variables, and handle post-mortem debugging. # Program Requirements You will create a Python script that includes the following: 1. A main function that calls two subsidiary functions. 2. A function that intentionally raises an exception to test post-mortem debugging. 3. Appropriate use of the `pdb` module for setting breakpoints and debugging the flow of the program. # Instructions 1. **Main Function and Subsidiary Functions:** - Define a function `main()` which calls two other functions, `func1()` and `func2()`. - Define `func1()` to perform a simple loop that prints numbers from 1 to 5. - Define `func2()` to intentionally raise a `ValueError`. 2. **Debug Setup:** - Use the `pdb.set_trace()` function within `main()` to start debugging. - Use `pdb.run()` to execute the `main()` function under debugger control within your script\'s main block. 3. **Post-mortem Debugging:** - Use `pdb.post_mortem()` to enter the debugger when the exception is raised in `func2()`. # Expected Input and Output - No specific input is required; the script runs independently. - Ensure that your output captures console-based debugger interactions. # Example Output Format ``` (pdb) continue > <string>(1)?() (Pdb) step 1 (pdb) next 2 ... (Pdb) continue ValueError: An intentional error for debugging. ... > <string>(9)?() (Pdb) post_mortem ... ``` # Constraints - Ensure your solution is Python 3.7+ compatible, as it utilizes `pdb` features introduced in Python 3.7. - Use comments to describe the purpose of each segment of your code. Sample Code Structure: ```python import pdb def func1(): for i in range(1, 6): print(i) def func2(): raise ValueError(\\"An intentional error for debugging.\\") def main(): pdb.set_trace() # Start debugger func1() func2() if __name__ == \\"__main__\\": try: pdb.run(\\"main()\\") except Exception as e: pdb.post_mortem() ``` Submission Submit your completed Python script with sufficient comments to describe the debugging process and your understanding of `pdb` commands used.","solution":"import pdb def func1(): Function to print numbers from 1 to 5. for i in range(1, 6): print(i) def func2(): Function to intentionally raise a ValueError. raise ValueError(\\"An intentional error for debugging.\\") def main(): Main function to call func1 and func2. pdb.set_trace() # Start the debugger func1() func2() if __name__ == \\"__main__\\": try: # Start the debugger and run the main function pdb.run(\\"main()\\") except Exception as e: pdb.post_mortem() # Post-mortem debugging on exception"},{"question":"<|Analysis Begin|> The provided documentation is for the Python `math` module, which includes various mathematical functions and constants. The functions cover number-theoretic operations, power and logarithmic computations, trigonometry, angular conversions, hyperbolic functions, and special functions like error functions and gamma functions. Additionally, constants such as `pi`, `e`, `tau`, `inf`, and `nan` are defined. To craft a challenging and comprehensive coding assessment question, we can select and synthesize multiple functions from this module, prompting students to demonstrate their understanding by applying these functions in a complex problem scenario. Given the variety of mathematical functions, an appropriate focus could be on implementing a function that involves mathematical computations, number theory, and precision handling. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Problem Statement: You are tasked with implementing a function that computes various mathematical properties and results based on two provided integers (a) and (b). Your function should return a dictionary containing the following keys and their corresponding values: 1. **\\"gcd\\"**: The greatest common divisor of (a) and (b) using `math.gcd`. 2. **\\"lcm\\"**: The least common multiple of (a) and (b) using the provided formula `abs(a * b) // gcd(a, b)`. 3. **\\"isqrt_a\\"**: The integer square root of (a) using `math.isqrt`. 4. **\\"isqrt_b\\"**: The integer square root of (b) using `math.isqrt`. 5. **\\"pow_exp\\"**: The result of (a^b) using `math.pow`. 6. **\\"exp_a\\"**: The natural exponent of (a) (i.e., (e^a)) using `math.exp`. 7. **\\"exp_b\\"**: The natural exponent of (b) (i.e., (e^b)) using `math.exp`. 8. **\\"log_a2\\"**: The base-2 logarithm of (a) using `math.log2`. 9. **\\"log_b10\\"**: The base-10 logarithm of (b) using `math.log10`. Input: - Two non-negative integers (a) and (b) where (a, b geq 0). Output: - A dictionary with the keys and values as described. Constraints: - The inputs (a) and (b) will be non-negative integers such that all computations are valid (i.e., no logarithm with a zero argument if avoidable). Function Signature: ```python import math def compute_math_properties(a: int, b: int) -> dict: # Implementation here ``` Example: ```python >>> compute_math_properties(6, 3) { \\"gcd\\": 3, \\"lcm\\": 6, \\"isqrt_a\\": 2, \\"isqrt_b\\": 1, \\"pow_exp\\": 216.0, \\"exp_a\\": 403.4287934927351, \\"exp_b\\": 20.085536923187668, \\"log_a2\\": 2.584962500721156, \\"log_b10\\": 0.4771212547196672 } ``` Notes: - Ensure your function has proper error handling for logarithm computations to avoid taking the log of zero. - The expected output format and precision should follow the `math` module\'s conventions for floating point arithmetic.","solution":"import math def compute_math_properties(a: int, b: int) -> dict: result = {} # Greatest common divisor result[\\"gcd\\"] = math.gcd(a, b) # Least common multiple (assuming math.gcd should handle the inputs accordingly) result[\\"lcm\\"] = abs(a * b) // result[\\"gcd\\"] if (a and b) else 0 # Integer square roots result[\\"isqrt_a\\"] = math.isqrt(a) result[\\"isqrt_b\\"] = math.isqrt(b) # Power computation result[\\"pow_exp\\"] = math.pow(a, b) # Natural exponentiation result[\\"exp_a\\"] = math.exp(a) result[\\"exp_b\\"] = math.exp(b) # Logarithms result[\\"log_a2\\"] = math.log2(a) if a > 0 else float(\'-inf\') result[\\"log_b10\\"] = math.log10(b) if b > 0 else float(\'-inf\') return result"},{"question":"# Advanced Python Typing: Implementing a Strict List Filter You are tasked with implementing a function that filters elements from a list according to strict type constraints using Python\'s `typing` module. This will test your understanding of both basic and advanced type hinting concepts. Requirements 1. **Function Signature**: The function should be named `filter_by_type`. 2. **Input**: - A list of elements of any type. - A type constraint to filter elements by. The type constraint should be defined using the `Type[TypeVar]` construct from the `typing` module. 3. **Output**: - A list of elements that matches the specified type constraint. 4. **Constraints**: - You must utilize type hints to enforce the input list\'s elements and type constraint precisely. - You should make use of the `TypeVar`, `Generic`, and any relevant types from the `typing` module. - The solution should also be capable of handling nested types like `List[List[int]]` or `Optional[int]`. # Example ```python from typing import Type, TypeVar, List, Optional T = TypeVar(\'T\') def filter_by_type(elements: List[Any], type_constraint: Type[T]) -> List[T]: pass # Example Usage: elements = [1, \\"hello\\", 3.14, 4, \\"world\\", None] filtered_ints = filter_by_type(elements, int) print(filtered_ints) # Output: [1, 4] nested_elements = [[1, 2], [\\"a\\", \\"b\\"], [3.14, 4]] filtered_nested_lists = filter_by_type(nested_elements, List[int]) print(filtered_nested_lists) # Output: [[1, 2]] ``` Detailed Instructions 1. **Type Variable**: - Define a type variable `T` using `TypeVar` that can be used for the type constraint in your function. 2. **Type Hinting Function**: - Use typed parameters for the input list and the type constraint. - The return type should be a list of the same type as specified by the type constraint. 3. **Type Checking**: - Implement logic to check each element\'s type against the specified type constraint. - Ensure nested types and optional types are properly handled. Good luck, and make sure your implementation strictly adheres to typing principles for maximal type safety and clarity.","solution":"from typing import Type, TypeVar, List, Any, Optional T = TypeVar(\'T\') def filter_by_type(elements: List[Any], type_constraint: Type[T]) -> List[T]: Filters a given list for elements matching the specified type constraint. Args: elements (List[Any]): A list of elements of any type. type_constraint (Type[T]): The type constraint to filter elements by. Returns: List[T]: A list of elements that match the specified type constraint. return [element for element in elements if isinstance(element, type_constraint)]"},{"question":"# **Question: Implementing Custom Pickling/Unpickling with Persistent IDs** Objectives: 1. Understand and implement custom serialization and deserialization using the `pickle` module. 2. Handle external references using persistent IDs. 3. Ensure that custom objects can be correctly pickled and unpickled. Problem Statement: You are working on a project where you need to serialize and deserialize a list of complex objects that reference external data stored in a simple key-value store (implemented using a dictionary). Your task is to implement custom pickler and unpickler classes to handle these references properly using persistent IDs. Instructions: 1. **Define a `KeyValueStore` Class**: - This class should act as a simple key-value store using a dictionary. - Methods: `add_record(key, value)`, `get_record(key)`, `delete_record(key)`. 2. **Define a `ComplexObject` Class**: - This class should represent objects that have an ID and reference data stored in `KeyValueStore`. - Attributes: `obj_id` (unique identifier), `data_key` (key to corresponding data in `KeyValueStore`). 3. **Implement Custom Pickler and Unpickler Classes**: - `CustomPickler`: A subclass of `pickle.Pickler` with an overridden `persistent_id` method to handle `ComplexObject` instances. - `CustomUnpickler`: A subclass of `pickle.Unpickler` with an overridden `persistent_load` method to resolve `ComplexObject` instances using the key-value store. 4. **Demonstrate the Functionality**: - Create an instance of `KeyValueStore` and add some records. - Create a list of `ComplexObject` instances referencing the records. - Serialize this list using `CustomPickler` to a bytes object. - Deserialize the bytes object using `CustomUnpickler` and verify that the objects are correctly restored. Specifications: - **KeyValueStore Class**: ```python class KeyValueStore: def __init__(self): self.store = {} def add_record(self, key, value): self.store[key] = value def get_record(self, key): return self.store.get(key) def delete_record(self, key): if key in self.store: del self.store[key] ``` - **ComplexObject Class**: ```python class ComplexObject: def __init__(self, obj_id, data_key): self.obj_id = obj_id self.data_key = data_key def __repr__(self): return f\\"ComplexObject(id={self.obj_id}, data_key=\'{self.data_key}\')\\" ``` - **CustomPickler Class**: ```python import pickle class CustomPickler(pickle.Pickler): def __init__(self, file, store): super().__init__(file) self.store = store def persistent_id(self, obj): if isinstance(obj, ComplexObject): return (\\"ComplexObject\\", obj.obj_id, obj.data_key) return None ``` - **CustomUnpickler Class**: ```python import pickle class CustomUnpickler(pickle.Unpickler): def __init__(self, file, store): super().__init__(file) self.store = store def persistent_load(self, pid): type_tag, obj_id, data_key = pid if type_tag == \\"ComplexObject\\": data = self.store.get_record(data_key) return ComplexObject(obj_id, data_key) raise pickle.UnpicklingError(f\\"unsupported persistent object: {pid}\\") ``` Example: ```python # Instantiate key-value store and add records store = KeyValueStore() store.add_record(\\"key1\\", \\"value1\\") store.add_record(\\"key2\\", \\"value2\\") # Create ComplexObject instances and serialize them objs = [ComplexObject(1, \\"key1\\"), ComplexObject(2, \\"key2\\")] # Serialize using CustomPickler import io output = io.BytesIO() CustomPickler(output, store).dump(objs) # Deserialize using CustomUnpickler input_data = output.getvalue() input_io = io.BytesIO(input_data) new_objs = CustomUnpickler(input_io, store).load() print(new_objs) # Should print the list of restored ComplexObject instances ```","solution":"import pickle class KeyValueStore: def __init__(self): self.store = {} def add_record(self, key, value): self.store[key] = value def get_record(self, key): return self.store.get(key) def delete_record(self, key): if key in self.store: del self.store[key] class ComplexObject: def __init__(self, obj_id, data_key): self.obj_id = obj_id self.data_key = data_key def __repr__(self): return f\\"ComplexObject(id={self.obj_id}, data_key=\'{self.data_key}\')\\" class CustomPickler(pickle.Pickler): def __init__(self, file, store): super().__init__(file) self.store = store def persistent_id(self, obj): if isinstance(obj, ComplexObject): return (\\"ComplexObject\\", obj.obj_id, obj.data_key) return None class CustomUnpickler(pickle.Unpickler): def __init__(self, file, store): super().__init__(file) self.store = store def persistent_load(self, pid): type_tag, obj_id, data_key = pid if type_tag == \\"ComplexObject\\": data = self.store.get_record(data_key) return ComplexObject(obj_id, data_key) raise pickle.UnpicklingError(f\\"unsupported persistent object: {pid}\\") # Example usage: # Instantiate key-value store and add records store = KeyValueStore() store.add_record(\\"key1\\", \\"value1\\") store.add_record(\\"key2\\", \\"value2\\") # Create ComplexObject instances and serialize them objs = [ComplexObject(1, \\"key1\\"), ComplexObject(2, \\"key2\\")] # Serialize using CustomPickler import io output = io.BytesIO() CustomPickler(output, store).dump(objs) # Deserialize using CustomUnpickler input_data = output.getvalue() input_io = io.BytesIO(input_data) new_objs = CustomUnpickler(input_io, store).load() print(new_objs) # Should print the list of restored ComplexObject instances"},{"question":"# Interactive Python Console Implementation Using the `code` module\'s `InteractiveConsole` and `InteractiveInterpreter` classes, implement a custom interactive Python console that logs each input and output interaction to a file called `interaction_log.txt`. The console should: 1. Prompt the user for input using `>>` for the initial prompt and `..` for continuation prompts. 2. Log each input statement and its corresponding interpreter response to `interaction_log.txt`. 3. Suppress the banner on start and the exit message on quit. Here is a breakdown of what you need to do: 1. Create a class `LoggableInteractiveConsole` that inherits from `code.InteractiveConsole`. 2. Override the `write()` method to log output to `interaction_log.txt`. 3. Override any other methods if necessary to ensure input is logged. 4. Add functionality to manage the prompt strings and interaction. **Example Usage:** ```python from your_module import LoggableInteractiveConsole # Initialize custom console console = LoggableInteractiveConsole() # Start interaction console.interact(banner=\'\', exitmsg=\'\') ``` **Expected Behavior:** - When the console starts, it should immediately prompt `>>`. - If the user inputs an incomplete command, the console should use `..` for the next line. - Each input and output should be recorded in `interaction_log.txt`. - No banner should be printed at start, and no exit message should be shown on quitting. **Input Constraints:** - Inputs will be standard Python commands. - Assume code the user inputs is valid until they quit the console with EOF (Ctrl+D or similar). **Performance Constraints:** - The solution should handle typical REPL interactions efficiently without noticeable lag. Implement this functionality to demonstrate your understanding of interactive interpreter mechanics using the `code` module.","solution":"import code class LoggableInteractiveConsole(code.InteractiveConsole): def __init__(self, filename=\'interaction_log.txt\', locals=None): super().__init__(locals) self.filename = filename self._append_log(\\"Starting new interactive session.n\\") def write(self, data): self._append_log(data) def push(self, line): self._append_log(\\"Input: \\" + line + \\"n\\") more = super().push(line) return more def raw_input(self, prompt=\'\'): line = input(prompt) self._append_log(\\"Prompt: \\" + prompt + \\" | Input: \\" + line + \\"n\\") return line def _append_log(self, text): with open(self.filename, \'a\') as f: f.write(text) # Interactive usage: # console = LoggableInteractiveConsole() # console.interact(banner=\'\', exitmsg=\'\')"},{"question":"Manipulating Audio Files with `aifc` Objective: Write a Python script using the `aifc` module to manipulate an AIFF file. Your task is to: 1. Open an AIFF file named `input.aiff` and read its properties. 2. Create a stereo (2 channels) version of the same audio with a modified frame rate and save it as `output.aiff`. Details: 1. **Reading from `input.aiff`**: - Open `input.aiff` in read mode and extract the following details: 1. Number of channels. 2. Sample width. 3. Frame rate. 4. Number of frames. 5. Compression type. 6. Compression name. 7. All audio frames. 2. **Modifying the Audio**: - The new frame rate should be exactly half of the original frame rate. - The audio should be converted to stereo by duplicating the single audio channel data if the input is mono. 3. **Writing to `output.aiff`**: - Create a new AIFF file named `output.aiff`. - Set the properties (number of channels, sample width, frame rate, etc.) for the new audio file. - Write the modified audio frames to `output.aiff`. Input and Output: - **Input**: AIFF file named `input.aiff`. - **Output**: Modified AIFF file named `output.aiff`. Constraints: - The input AIFF file, `input.aiff`, is guaranteed to exist in the same directory as your script. - The input file will have a maximum size of 10 MB. - The script should handle both mono and stereo input files. Performance Requirements: - Ensure that the script does not use excessive memory. Consider using efficient data structures and methods to handle audio frame data. Boilerplate: ```python import aifc def process_aiff(input_filename: str, output_filename: str): # Open the input file and read properties with aifc.open(input_filename, \'r\') as input_file: nchannels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() nframes = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() frames = input_file.readframes(nframes) # Modify the properties new_framerate = framerate // 2 new_nchannels = 2 new_frames = convert_to_stereo(frames, nchannels, sampwidth, nframes) # Write the output file with modified properties with aifc.open(output_filename, \'w\') as output_file: output_file.setnchannels(new_nchannels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.setcomptype(comptype, compname) output_file.writeframes(new_frames) def convert_to_stereo(frames, nchannels, sampwidth, nframes): # Your implementation to convert mono to stereo and handle frame data pass if __name__ == \\"__main__\\": input_filename = \\"input.aiff\\" output_filename = \\"output.aiff\\" process_aiff(input_filename, output_filename) ``` Implement the `convert_to_stereo` function and complete the `process_aiff` function to meet the requirements.","solution":"import aifc def process_aiff(input_filename: str, output_filename: str): # Open the input file and read properties with aifc.open(input_filename, \'r\') as input_file: nchannels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() nframes = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() frames = input_file.readframes(nframes) # Modify the properties new_framerate = framerate // 2 new_nchannels = 2 new_frames = convert_to_stereo(frames, nchannels, sampwidth, nframes) # Write the output file with modified properties with aifc.open(output_filename, \'w\') as output_file: output_file.setnchannels(new_nchannels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.setcomptype(comptype, compname) output_file.writeframes(new_frames) def convert_to_stereo(frames, nchannels, sampwidth, nframes): if nchannels == 2: return frames if nchannels == 1: new_frames = bytearray() for i in range(0, len(frames), sampwidth): mono_sample = frames[i:i+sampwidth] new_frames.extend(mono_sample) new_frames.extend(mono_sample) return bytes(new_frames) if __name__ == \\"__main__\\": input_filename = \\"input.aiff\\" output_filename = \\"output.aiff\\" process_aiff(input_filename, output_filename)"},{"question":"**Problem Statement:** You are required to write a Python script that behaves differently based on command-line arguments it receives. Your task is to create a script that: 1. Prints \\"Hello, World!\\" when no arguments are provided. 2. Prints the script name followed by the arguments if arguments are provided (e.g., `python3.10 script.py arg1 arg2` should print `script.py arg1 arg2`). 3. When invoked with the `-r` flag followed by a string, it should print the reversed string (e.g., `python3.10 script.py -r Hello` should print `olleH`). 4. When invoked with the `-s` flag followed by a list of numbers separated by spaces, it should print the sum of these numbers (e.g., `python3.10 script.py -s 1 2 3 4` should print `10`). 5. If the script encounters an unknown flag, it should print \\"Unknown flag\\" and exit. **Constraints:** - Do not use any third-party libraries. - Handle erroneous input gracefully. **Input and Output:** - The script will either receive no arguments or a combination of valid flags and their corresponding values. - Under normal execution: - No arguments: prints \\"Hello, World!\\" - Script name with arguments: prints `script_name arg1 arg2 ...` - `-r` flag with a string: prints reversed string. - `-s` flag with a list of numbers: prints the sum of numbers. - Any unknown flag: prints \\"Unknown flag\\". **Example Execution:** ```sh python3.10 script.py Hello, World! python3.10 script.py arg1 arg2 script.py arg1 arg2 python3.10 script.py -r Hello olleH python3.10 script.py -s 1 2 3 6 python3.10 script.py -x Unknown flag ``` Write your solution to handle these cases within the constraints provided.","solution":"import sys def main(): args = sys.argv[1:] if len(args) == 0: print(\\"Hello, World!\\") return elif args[0] == \'-r\' and len(args) == 2: print(args[1][::-1]) return elif args[0] == \'-s\' and len(args) > 1: try: numbers = list(map(int, args[1:])) print(sum(numbers)) except ValueError: print(\\"Error: All arguments after -s must be integers.\\") return elif args[0].startswith(\'-\'): print(\\"Unknown flag\\") return else: print(\' \'.join([sys.argv[0]] + args)) return if __name__ == \\"__main__\\": main()"},{"question":"**Multiclass Classification with Different Strategies** **Objective**: Implement and compare different multiclass classification strategies using scikit-learn. **Problem Description**: You are provided with the dataset of iris flowers, which includes three classes of the flower species. Your task is to implement and compare the performance of the following multiclass classification strategies: 1. One-Vs-Rest (OvR) 2. One-Vs-One (OvO) 3. Error-Correcting Output Codes (ECOC) **Dataset**: Use the `iris` dataset available from `sklearn.datasets`. **Steps**: 1. Load the `iris` dataset and split it into training and testing sets. 2. Implement the following multiclass strategies using `LinearSVC` as the base estimator: - One-Vs-Rest (OvR) - One-Vs-One (OvO) - Error-Correcting Output Codes (ECOC) with a code size of 1.5. 3. Train and evaluate the models on the test set. 4. Compare the performance of these models based on accuracy scores. **Requirements**: 1. Use appropriate scikit-learn estimators and meta-estimators. 2. Evaluate the models using accuracy as the performance metric. 3. Plot the confusion matrix and classification report for each strategy. **Input**: - None **Output**: - Print the accuracy score for each strategy. - Display the confusion matrix and classification report for each strategy. **Constraints**: - You must use `LinearSVC` as the base estimator for all strategies. - Use a test size of 20% for splitting the dataset. **Performance Requirements**: - Ensure the solution runs efficiently and completes within a reasonable time. **Example**: ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.multiclass import OutputCodeClassifier from sklearn.svm import LinearSVC from sklearn.metrics import classification_report, confusion_matrix, accuracy_score import matplotlib.pyplot as plt import seaborn as sns # Load dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # One-Vs-Rest ovr = OneVsRestClassifier(LinearSVC(random_state=42)).fit(X_train, y_train) y_pred_ovr = ovr.predict(X_test) print(\\"Accuracy for One-Vs-Rest:\\", accuracy_score(y_test, y_pred_ovr)) print(\\"Classification Report for One-Vs-Rest:n\\", classification_report(y_test, y_pred_ovr)) print(\\"Confusion Matrix for One-Vs-Rest:n\\", confusion_matrix(y_test, y_pred_ovr)) # One-Vs-One ovo = OneVsOneClassifier(LinearSVC(random_state=42)).fit(X_train, y_train) y_pred_ovo = ovo.predict(X_test) print(\\"Accuracy for One-Vs-One:\\", accuracy_score(y_test, y_pred_ovo)) print(\\"Classification Report for One-Vs-One:n\\", classification_report(y_test, y_pred_ovo)) print(\\"Confusion Matrix for One-Vs-One:n\\", confusion_matrix(y_test, y_pred_ovo)) # Error-Correcting Output Codes ecoc = OutputCodeClassifier(LinearSVC(random_state=42), code_size=1.5, random_state=42).fit(X_train, y_train) y_pred_ecoc = ecoc.predict(X_test) print(\\"Accuracy for ECOC:\\", accuracy_score(y_test, y_pred_ecoc)) print(\\"Classification Report for ECOC:n\\", classification_report(y_test, y_pred_ecoc)) print(\\"Confusion Matrix for ECOC:n\\", confusion_matrix(y_test, y_pred_ecoc)) # Plot Confusion Matrices fig, axes = plt.subplots(1, 3, figsize=(15, 5)) sns.heatmap(confusion_matrix(y_test, y_pred_ovr), annot=True, fmt=\'d\', ax=axes[0]) axes[0].set_title(\\"OvR Confusion Matrix\\") sns.heatmap(confusion_matrix(y_test, y_pred_ovo), annot=True, fmt=\'d\', ax=axes[1]) axes[1].set_title(\\"OvO Confusion Matrix\\") sns.heatmap(confusion_matrix(y_test, y_pred_ecoc), annot=True, fmt=\'d\', ax=axes[2]) axes[2].set_title(\\"ECOC Confusion Matrix\\") plt.show() ```","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier from sklearn.svm import LinearSVC from sklearn.metrics import classification_report, confusion_matrix, accuracy_score import matplotlib.pyplot as plt import seaborn as sns def multiclass_classification(): # Load dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # One-Vs-Rest ovr = OneVsRestClassifier(LinearSVC(random_state=42)).fit(X_train, y_train) y_pred_ovr = ovr.predict(X_test) accuracy_ovr = accuracy_score(y_test, y_pred_ovr) classification_report_ovr = classification_report(y_test, y_pred_ovr) confusion_matrix_ovr = confusion_matrix(y_test, y_pred_ovr) # One-Vs-One ovo = OneVsOneClassifier(LinearSVC(random_state=42)).fit(X_train, y_train) y_pred_ovo = ovo.predict(X_test) accuracy_ovo = accuracy_score(y_test, y_pred_ovo) classification_report_ovo = classification_report(y_test, y_pred_ovo) confusion_matrix_ovo = confusion_matrix(y_test, y_pred_ovo) # Error-Correcting Output Codes ecoc = OutputCodeClassifier(LinearSVC(random_state=42), code_size=1.5, random_state=42).fit(X_train, y_train) y_pred_ecoc = ecoc.predict(X_test) accuracy_ecoc = accuracy_score(y_test, y_pred_ecoc) classification_report_ecoc = classification_report(y_test, y_pred_ecoc) confusion_matrix_ecoc = confusion_matrix(y_test, y_pred_ecoc) return accuracy_ovr, classification_report_ovr, confusion_matrix_ovr, accuracy_ovo, classification_report_ovo, confusion_matrix_ovo, accuracy_ecoc, classification_report_ecoc, confusion_matrix_ecoc # Results can be printed using the above function in another module."},{"question":"Objective: You are required to create two custom classes, `Person` and `FamilyTree`, that demonstrate your understanding of shallow and deep copy operations in Python. For this task, classes should define their own `__copy__()` and `__deepcopy__()` methods. Requirements: 1. **Class `Person`**: - Properties: - `name`: a string representing the name of the person. - `age`: an integer representing the age of the person. - `friends`: a list of `Person` objects representing the person\'s friends. - Methods: - `__copy__()`: returns a shallow copy of the `Person` object. - `__deepcopy__(memo)`: returns a deep copy of the `Person` object, using the memo dictionary. 2. **Class `FamilyTree`**: - Properties: - `root`: a `Person` object representing the root of the family tree. - Methods: - `__copy__()`: returns a shallow copy of the `FamilyTree` object. - `__deepcopy__(memo)`: returns a deep copy of the `FamilyTree` object, using the memo dictionary. Input/Output: - The constructors for `Person` and `FamilyTree` should take in arguments that initialize all properties. - You should be able to demonstrate both shallow and deep copying through a series of assertions that validate copying behavior. Constraints: 1. Ensure that the `__deepcopy__()` methods use the `copy.deepcopy()` function with the memo dictionary. 2. The copying functionality should be demonstrated without causing any recursive loops. 3. Validate the design by creating instances and showing that after modifications, original and copied objects behave as expected. Example Usage: ```python # Create some Person instances alice = Person(name=\\"Alice\\", age=30, friends=[]) bob = Person(name=\\"Bob\\", age=24, friends=[alice]) charlie = Person(name=\\"Charlie\\", age=22, friends=[alice, bob]) # Create a FamilyTree instance family_tree = FamilyTree(root=alice) # Shallow copy the family tree shallow_copied_tree = copy.copy(family_tree) # Deep copy the family tree deep_copied_tree = copy.deepcopy(family_tree) # Test modifications shallow_copied_tree.root.name = \\"Ally\\" assert family_tree.root.name == \\"Ally\\" # Expectation from shallow copy deep_copied_tree.root.name = \\"Alicia\\" assert family_tree.root.name == \\"Ally\\" # Expectation from deep copy # Ensure friendships are properly copied alice.friends.append(bob) shallow_copied_alice = copy.copy(alice) assert shallow_copied_alice.friends is alice.friends # Reference equality in shallow copy deep_copied_alice = copy.deepcopy(alice) assert deep_copied_alice.friends is not alice.friends # Different object instances in deep copy print(\\"Assertions passed, shallow and deep copy functions are correctly implemented.\\") ``` **Note**: Ensure that the `copy` module is imported at the beginning of your code.","solution":"import copy class Person: def __init__(self, name, age, friends=None): self.name = name self.age = age self.friends = friends if friends is not None else [] def __copy__(self): # Creates a shallow copy of the Person object new_person = Person(self.name, self.age) new_person.friends = self.friends return new_person def __deepcopy__(self, memo): # Creates a deep copy of the Person object if self in memo: return memo[self] new_person = Person(self.name, self.age) memo[self] = new_person new_person.friends = copy.deepcopy(self.friends, memo) return new_person class FamilyTree: def __init__(self, root=None): self.root = root def __copy__(self): # Creates a shallow copy of the FamilyTree object new_tree = FamilyTree() new_tree.root = self.root return new_tree def __deepcopy__(self, memo): # Creates a deep copy of the FamilyTree object if self in memo: return memo[self] new_tree = FamilyTree() memo[self] = new_tree new_tree.root = copy.deepcopy(self.root, memo) return new_tree"},{"question":"**Coding Assessment Question** You are asked to implement a function, `execute_python_code`, which simulates the execution of Python code using the Python C API\'s provided functions. This function should accept the path to a Python file, compile its contents, execute it, and return the output of the execution. # Function Signature ```python def execute_python_code(file_path: str) -> str: pass ``` # Input - `file_path`: A string representing the path to the Python file that needs to be executed. # Output - Returns a string containing the output of the executed Python code from the provided file. # Requirements 1. The function should handle exceptions raised during the compilation or execution of the code and return an appropriate error message. 2. Ensure the file is opened in binary mode (`rb`) on Windows to handle line endings correctly. 3. Use the start symbol `Py_file_input` to compile the code as it is meant to be read from a file. # Constraints - Assume that the Python file at `file_path` contains valid Python code. - Do not use external libraries for executing Python code other than the provided C API functions. # Example Suppose you have a Python file named `example.py` with the following content: ```python print(\\"Hello, World!\\") ``` Calling the function with this file should return: ```python execute_python_code(\'example.py\') # Output: \\"Hello, World!n\\" ``` # Hints - Utilize `Py_CompileStringFlags` with `Py_file_input` for compilation. - For execution, use `PyEval_EvalCodeEx` or similar functions to evaluate the compiled code object. - You might need to redirect the standard output to capture the printed results. # Notes You may refer to appropriate sections of the provided documentation to help you structure your solution effectively.","solution":"import os import sys import traceback from io import StringIO from contextlib import redirect_stdout def execute_python_code(file_path: str) -> str: Executes the Python code from the given file and returns the output. :param file_path: Path to the Python file to execute. :return: Output of the executed file as a string. if not os.path.exists(file_path): return f\\"File not found: {file_path}\\" try: with open(file_path, \'rb\') as file: code = file.read() code_str = code.decode(\'utf-8\') compiled_code = compile(code_str, file_path, \'exec\') # Redirect stdout to capture print() output output = StringIO() with redirect_stdout(output): try: exec(compiled_code, {}) except Exception as e: return f\\"Error during execution: {str(e)}\\" return output.getvalue() except Exception as e: # Handle compilation errors return f\\"Error during compilation or file reading: {str(e)}, {traceback.format_exc()}\\""},{"question":"# Question: Lexical Analyzer for Basic Python Tokens Objective Design and implement a function called `lexical_analyzer` that processes a given string of Python code and returns a list of tokens based on Python\'s lexical analysis rules for identifiers, keywords, numeric literals, and basic operators. Input - `code_string`: A string containing Python code. The code may contain identifiers, keywords, numeric literals (integers and floats), and basic operators (`+`, `-`, `*`, `/`, `%`, `==`, `!=`, `<`, `>`, `<=`, `>=`). Output - A list of tokens extracted from the given `code_string`. Each token should be represented as a tuple where the first element is the type of token (`keyword`, `identifier`, `integer`, `float`, `operator`) and the second element is the token itself. Constraints - The input will contain well-formed Python code and does not require handling of complex structures like functions, classes, or modules. - The input will contain a maximum of 1,000 characters. Examples ```python # Example 1 input_code = \\"x = 10 + 20\\" output = lexical_analyzer(input_code) print(output) # Output: [(\'identifier\', \'x\'), (\'operator\', \'=\'), (\'integer\', \'10\'), (\'operator\', \'+\'), (\'integer\', \'20\')] # Example 2 input_code = \\"if count >= 10:\\" output = lexical_analyzer(input_code) print(output) # Output: [(\'keyword\', \'if\'), (\'identifier\', \'count\'), (\'operator\', \'>=\'), (\'integer\', \'10\'), (\'operator\', \':\')] # Example 3 input_code = \\"total_sum = 3.14 * radius ** 2\\" output = lexical_analyzer(input_code) print(output) # Output: [(\'identifier\', \'total_sum\'), (\'operator\', \'=\'), (\'float\', \'3.14\'), (\'operator\', \'*\'), (\'identifier\', \'radius\'), (\'operator\', \'**\'), (\'integer\', \'2\')] ``` Function Signature ```python def lexical_analyzer(code_string: str) -> list: pass ``` Notes - Focus on handling the identifiers, keywords, numeric literals, and basic operators as specified. - Consider using regular expressions or Python\'s built-in string manipulation methods to extract and classify tokens. - The function should be robust and account for common whitespace variations.","solution":"import re def lexical_analyzer(code_string: str) -> list: keywords = {\\"if\\", \\"else\\", \\"while\\", \\"for\\", \\"return\\", \\"in\\", \\"not\\", \\"and\\", \\"or\\", \\"is\\", \\"None\\", \\"True\\", \\"False\\"} operators = {\\"+\\", \\"-\\", \\"*\\", \\"/\\", \\"%\\", \\"==\\", \\"!=\\", \\"<\\", \\">\\", \\"<=\\", \\">=\\", \\"=\\", \\":\\", \\"**\\"} token_specification = [ (\'keyword\', r\'b(?:\' + \'|\'.join(re.escape(kw) for kw in keywords) + r\')b\'), (\'identifier\', r\'b[a-zA-Z_]w*b\'), (\'float\', r\'bd+.d*b\'), (\'integer\', r\'bd+b\'), (\'operator\', \'|\'.join(re.escape(op) for op in sorted(operators, key=len, reverse=True))), (\'whitespace\', r\'s+\'), ] token_re = re.compile(\'|\'.join(\'(?P<%s>%s)\' % pair for pair in token_specification)) tokens = [] for mo in token_re.finditer(code_string): kind = mo.lastgroup value = mo.group() if kind == \'whitespace\': continue tokens.append((kind, value)) return tokens"},{"question":"**Implementing and Testing Deterministic Algorithms in PyTorch** **Objective**: The goal of this coding question is to assess your understanding of PyTorch\'s deterministic features and the performance implications of filling uninitialized memory. # Problem Statement You are tasked with implementing a function in PyTorch that performs certain tensor operations in a deterministic manner. You need to ensure that the results are repeatable across different runs of the function. # Function Signature ```python import torch def deterministic_tensor_operations(seed: int, size: int, fill_memory: bool) -> torch.Tensor: Perform a series of tensor operations in a deterministic manner. Args: - seed (int): The random seed for reproducibility. - size (int): The size of the tensor to be created. - fill_memory (bool): Whether to fill uninitialized memory with known values. Returns: - torch.Tensor: The result of deterministic tensor operations. pass ``` # Instructions 1. Set the random seed using `torch.manual_seed(seed)`. 2. Enable deterministic algorithms using `torch.use_deterministic_algorithms(True)`. 3. Set the `torch.utils.deterministic.fill_uninitialized_memory` attribute to the value of `fill_memory`. 4. Create an uninitialized tensor of the specified size using `torch.empty(size)`. 5. Perform a sequence of operations on the tensor: - Add a scalar value to every element. - Multiply every element by another scalar. 6. Return the modified tensor. # Example ```python import torch result = deterministic_tensor_operations(42, 5, True) print(result) # Output should be a tensor with deterministic values result = deterministic_tensor_operations(42, 5, False) print(result) # Output should be a tensor with deterministic values, but performance should not be degraded by filling uninitialized memory ``` # Constraints - Ensure that your function is deterministic, meaning that given the same seed and input parameters, it should always produce the same output. - The size of the tensor will always be a positive integer. - The seed will be a non-negative integer. # Performance Evaluation Write a test script to measure the performance difference between `fill_memory` set to `True` and `fill_memory` set to `False`. Use a sufficiently large tensor size (e.g., `size=1000000`) to observe the performance difference. ```python import time seed = 123 size = 1000000 # Performance with fill_memory=True start_time = time.time() deterministic_tensor_operations(seed, size, True) end_time = time.time() print(\\"Time with fill_memory=True:\\", end_time - start_time) # Performance with fill_memory=False start_time = time.time() deterministic_tensor_operations(seed, size, False) end_time = time.time() print(\\"Time with fill_memory=False:\\", end_time - start_time) ```","solution":"import torch def deterministic_tensor_operations(seed: int, size: int, fill_memory: bool) -> torch.Tensor: Perform a series of tensor operations in a deterministic manner. Args: - seed (int): The random seed for reproducibility. - size (int): The size of the tensor to be created. - fill_memory (bool): Whether to fill uninitialized memory with known values. Returns: - torch.Tensor: The result of deterministic tensor operations. torch.manual_seed(seed) torch.use_deterministic_algorithms(True) torch.backends.cudnn.deterministic = True # Ensure cuDNN will use deterministic algorithms if isinstance(torch.backends.cudnn, object): if hasattr(torch.backends.cudnn, \'deterministic\'): torch.backends.cudnn.deterministic = True torch.utils.deterministic.fill_uninitialized_memory = fill_memory # Create an uninitialized tensor tensor = torch.empty(size) # Perform operations tensor = tensor.fill_(0.5) # Fill with a scalar value tensor = tensor * 2.0 # Multiply every element by another scalar return tensor"},{"question":"You are required to implement a class `CustomCodec` that utilizes the Python codec registry functionality for encoding and decoding text data. Your task is to ensure the class can register a custom codec, use it for encoding and decoding, and handle errors gracefully. # Class Definition ```python class CustomCodec: def __init__(self, encoding: str): Initialize the CustomCodec with the given encoding. :param encoding: A string representing the encoding to use. pass def register_custom_codec(self, search_function): Register a custom codec search function. :param search_function: A callable that defines the search function for the codec. :return: None pass def unregister_custom_codec(self, search_function): Unregister the custom codec search function and clear the registry\'s cache. :param search_function: The previously registered search function. :return: None pass def encode(self, text: str, errors: str = \'strict\') -> bytes: Encode the given text using the specified encoding and error handling scheme. :param text: The string to encode. :param errors: The error handling scheme (\'strict\', \'ignore\', \'replace\'). :return: The encoded byte string. pass def decode(self, data: bytes, errors: str = \'strict\') -> str: Decode the given data using the specified encoding and error handling scheme. :param data: The byte string to decode. :param errors: The error handling scheme (\'strict\', \'ignore\', \'replace\'). :return: The decoded string. pass def register_error_handler(self, name: str, handler): Register a custom error handling callback function. :param name: The name of the error handling scheme. :param handler: The callback function for the error handler. :return: None pass def lookup_error_handler(self, name: str): Lookup the error handling callback function registered under the given name. :param name: The name of the error handling scheme. :return: The error handling callback function. pass ``` # Requirements 1. **Initialization**: The class should store the encoding specified at initialization. 2. **Custom Codec Registration**: - `register_custom_codec(search_function)`: Register the provided search function for the codec. - `unregister_custom_codec(search_function)`: Unregister the provided search function and clear the registry\'s cache. If the search function is not registered, nothing should happen. 3. **Encoding and Decoding**: - `encode(text, errors)`: Encode the provided text using the stored encoding and the specified error handling scheme. - `decode(data, errors)`: Decode the provided byte string using the stored encoding and the specified error handling scheme. 4. **Error Handling**: - `register_error_handler(name, handler)`: Register a custom error handling callback function. - `lookup_error_handler(name)`: Lookup the error handling callback function registered under the given name. # Constraints - The custom codec search function should follow the standard codec search function conventions of Python. - Error handling schemes should include at least `strict`, `ignore`, and `replace`. - The custom error handling functions must adhere to the PyCodec function guidelines. # Example Usage ```python def custom_search_function(encoding): # Custom codec search logic pass def custom_error_handler(exc): # Custom error handling logic pass codec = CustomCodec(\'utf-8\') codec.register_custom_codec(custom_search_function) encoded_data = codec.encode(\'Hello, world!\') decoded_text = codec.decode(encoded_data) codec.register_error_handler(\'custom\', custom_error_handler) handler = codec.lookup_error_handler(\'custom\') print(decoded_text) # Should output: \'Hello, world!\' ``` Ensure your implementation correctly registers and unregisters codecs, handles encoding/decoding operations, and manages error handlers as stipulated. Use Python’s native codec registry functions wherever applicable. ---","solution":"import codecs class CustomCodec: def __init__(self, encoding): Initialize the CustomCodec with the given encoding. :param encoding: A string representing the encoding to use. self.encoding = encoding self.search_function = None self.handlers = {} def register_custom_codec(self, search_function): Register a custom codec search function. :param search_function: A callable that defines the search function for the codec. :return: None self.search_function = search_function codecs.register(self.search_function) def unregister_custom_codec(self, search_function): Unregister the custom codec search function and clear the registry\'s cache. :param search_function: The previously registered search function. :return: None if self.search_function == search_function: codecs.unregister(self.search_function) self.search_function = None def encode(self, text, errors=\'strict\'): Encode the given text using the specified encoding and error handling scheme. :param text: The string to encode. :param errors: The error handling scheme (\'strict\', \'ignore\', \'replace\'). :return: The encoded byte string. return text.encode(self.encoding, errors) def decode(self, data, errors=\'strict\'): Decode the given data using the specified encoding and error handling scheme. :param data: The byte string to decode. :param errors: The error handling scheme (\'strict\', \'ignore\', \'replace\'). :return: The decoded string. return data.decode(self.encoding, errors) def register_error_handler(self, name, handler): Register a custom error handling callback function. :param name: The name of the error handling scheme. :param handler: The callback function for the error handler. :return: None self.handlers[name] = handler codecs.register_error(name, handler) def lookup_error_handler(self, name): Lookup the error handling callback function registered under the given name. :param name: The name of the error handling scheme. :return: The error handling callback function. return self.handlers.get(name)"},{"question":"Objective You are required to demonstrate your understanding of PyTorch and its `functorch` extension by modifying a custom neural network to handle batch normalization issues when using `vmap`. Task You will implement a neural network that uses BatchNorm layers. You must then modify this network to ensure compatibility with `vmap` by replacing the BatchNorm layers with GroupNorm layers using the method described in Option 1 of the provided documentation. Detailed Instructions 1. **Define the Network**: - Create a neural network class `CustomNet` using PyTorch’s `nn.Module`. - The network should include at least one `nn.Conv2d` layer followed by a `nn.BatchNorm2d` layer. - Implement the `forward` method for this network. ```python import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU() def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) return x ``` 2. **Modify to Resolve Compatibility Issues**: - Implement a function `replace_batch_norm_with_group_norm` that takes an instance of `CustomNet` and replaces the BatchNorm layers with GroupNorm layers. - In GroupNorm, set the number of groups (G) such that `C // G == 32`. ```python def replace_batch_norm_with_group_norm(model, G=32): for name, module in model.named_modules(): if isinstance(module, nn.BatchNorm2d): num_groups = module.num_features // G setattr(model, name, nn.GroupNorm(num_groups, module.num_features)) return model ``` 3. **Test the Modified Network**: - Create an instance of the `CustomNet` and replace its BatchNorm layers. - Verify the replacement and show that the network operates as expected with dummy input data. ```python def test_modified_network(): model = CustomNet() model = replace_batch_norm_with_group_norm(model) dummy_input = torch.randn(1, 3, 32, 32) output = model(dummy_input) print(output.shape) test_modified_network() ``` Constraints - You must use the provided framework in the first step to define the custom network. - Follow Option 1 guideline strictly for replacing BatchNorm with GroupNorm. Performance Requirements - Ensure the modified network performs a forward pass without errors on dummy input data of the shape `(1, 3, 32, 32)`. This question assesses your understanding of neural network construction, module replacement, and handling batch normalization specific issues relevant to the `functorch` library\'s `vmap` function in PyTorch.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU() def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) return x def replace_batch_norm_with_group_norm(model, G=32): for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_groups = module.num_features // G setattr(model, name, nn.GroupNorm(num_groups, module.num_features)) return model"},{"question":"Objective To assess the student\'s understanding of Python\'s built-in functions, core data types, and their ability to manipulate these data types efficiently. Problem Statement You are required to implement a Python function that processes a list of strings, extracting specific information from each string and computing a final result. Specifically, you will create a function `process_strings(data: List[str]) -> Dict[str, int]` that processes a list of strings to calculate the frequency of each unique word across the entire list, ignoring case, and excluding any numeric digits from being part of any word. Input - `data` (List[str]): A list of strings where each string can contain multiple words, punctuation marks, and numbers. Each word is separated by whitespace characters. Output - Returns a dictionary (Dict[str, int]) where the keys are unique words found in the list (case insensitive and excluding digits), and the values are the frequency of these words across all strings. Constraints 1. Words are separated by whitespace. 2. Punctuation marks (e.g., ., !, ?, etc.) should be stripped off words. 3. Digits should not be considered part of any word. 4. All words should be counted in a case-insensitive manner. Example ```python data = [\\"Hello world! 123\\", \\"Hello again, world.\\"] expected_output = { \\"hello\\": 2, \\"world\\": 2, \\"again\\": 1 } assert process_strings(data) == expected_output ``` Performance Requirements - The function should efficiently handle lists containing up to 1000 strings, each string containing up to 1000 characters. Detailed Requirements 1. **Input Processing**: Iterate through each string in the input list. 2. **Normalization**: Convert all characters to lower case and strip off any punctuation marks. 3. **Digit Filtering**: Ensure that numeric digits are not included in any word. 4. **Count Calculation**: Maintain a frequency count of each unique word across the entire list. 5. **Output Generation**: Compile the frequency counts into a dictionary and return it. # Function Signature ```python from typing import List, Dict def process_strings(data: List[str]) -> Dict[str, int]: pass ``` # Hints - Utilize Python\'s `str` methods alongside built-in functions like `re` (regular expressions) for efficiently handling the transformation and extraction of words. - Consider using a dictionary or collections.Counter for keeping track of word frequencies.","solution":"from typing import List, Dict import re from collections import Counter def process_strings(data: List[str]) -> Dict[str, int]: Processes a list of strings and calculates the frequency of unique words. Parameters: - data: List[str] - list of strings to be processed. Returns: - Dict[str, int] - dictionary where keys are unique words and values are their frequencies. word_counter = Counter() for line in data: # Remove punctuation, convert to lowercase, and split into words cleaned_line = re.sub(r\'[^ws]\', \'\', line.lower()) words = cleaned_line.split() # Filter out words that contain digits filtered_words = [word for word in words if not any(char.isdigit() for char in word)] # Update the word counter word_counter.update(filtered_words) return dict(word_counter)"},{"question":"**Advanced Coding Assessment Question: Custom Module Importer** **Objective:** Demonstrate your understanding of the `importlib` module by implementing a custom module importer that dynamically loads Python modules from a given path. **Task:** You are required to implement a custom module importer that can dynamically import Python modules from a specified directory. The custom importer should: 1. **Search** for modules (.py files) in a specified directory. 2. **Load** the module using `importlib`. 3. **Cache** the imported modules to avoid re-importing them if they are requested again. 4. **Provide** a clear error message if a module cannot be found or imported. **Function Specification:** ```python import importlib.util import os import sys class CustomImporter: def __init__(self, module_path): self.module_path = module_path self.module_cache = {} def find_module(self, module_name): # Implement the search mechanism here def load_module(self, module_name): # Implement the module loading mechanism here # Function to be implemented def import_module_from_path(module_name, path): Imports a module dynamically from the specified path. :param module_name: str, Name of the module to import. :param path: str, Path where the module is located. :return: Module object if successfully imported, else raise ImportError. # Create an instance of the CustomImporter importer = CustomImporter(path) # Use the importer to find and load the module if importer.find_module(module_name): return importer.load_module(module_name) else: raise ImportError(f\\"Module {module_name} not found in path {path}\\") ``` **Input:** - `module_name` (string): Name of the module to be imported. - `path` (string): Filesystem path where the module is located. **Output:** - Return the imported module object if the import is successful. **Constraints:** - The module must be a valid Python file (`.py`). - Assume the module does not have any dependencies outside of the standard library. - Performance should be optimized such that repeated imports of the same module do not incur filesystem searches. **Example Usage:** ```python # Assuming there is a module \'example.py\' in the directory \'/path/to/modules\' module = import_module_from_path(\'example\', \'/path/to/modules\') module.some_function() ``` **Evaluation Criteria:** - Correctness: The implemented solution should correctly search, load, and cache modules. - Error Handling: Proper error messages should be raised for missing or invalid modules. - Performance: Efficient handling of repeated imports.","solution":"import importlib.util import os import sys class CustomImporter: def __init__(self, module_path): self.module_path = module_path self.module_cache = {} def find_module(self, module_name): module_filename = module_name + \'.py\' module_filepath = os.path.join(self.module_path, module_filename) return os.path.isfile(module_filepath) def load_module(self, module_name): if module_name in self.module_cache: return self.module_cache[module_name] module_filename = module_name + \'.py\' module_filepath = os.path.join(self.module_path, module_filename) if not os.path.isfile(module_filepath): raise ImportError(f\\"Module {module_name} not found in path {self.module_path}\\") spec = importlib.util.spec_from_file_location(module_name, module_filepath) if spec is None: raise ImportError(f\\"Cannot load spec for module {module_name} from {module_filepath}\\") module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) self.module_cache[module_name] = module return module def import_module_from_path(module_name, path): Imports a module dynamically from the specified path. :param module_name: str, Name of the module to import. :param path: str, Path where the module is located. :return: Module object if successfully imported, else raise ImportError. importer = CustomImporter(path) if importer.find_module(module_name): return importer.load_module(module_name) else: raise ImportError(f\\"Module {module_name} not found in path {path}\\")"},{"question":"# Question: Implement a Custom Module Importer in Python As an experienced Python programmer, you are tasked with implementing a custom module importer. The goal is to simulate the behavior of Python\'s standard import mechanism using pure Python, making use of some concepts and ideas from the C-based `PyImport_*` functions described in the documentation. Objectives: - Implement a function `custom_import(name: str) -> ModuleType` that imports a module by its name. - This function should replicate the behavior of `__import__`: - If the module is already imported, it should return the module from `sys.modules`. - If the module is not found, it should raise an `ImportError`. - Implement a function `reload_module(module: ModuleType) -> ModuleType` that reloads a module: - This function should reload the given module and update the module in `sys.modules`. - If the module cannot be reloaded, it should raise an `ImportError`. - Implement a function `add_module(name: str, module: ModuleType) -> None` that adds a module to `sys.modules`: - This function should add the given module to `sys.modules` under the given name. - If the module name already exists in `sys.modules`, it should do nothing. Constraints: - Your solution should not use the built-in `__import__` function directly within `custom_import`. - Ensure that appropriate exceptions are raised and handled according to the behavior described. - You may use standard libraries such as `imp`, `importlib`, and `sys`. Sample Usage: ```python # Assuming you have implemented the functions as described above # Importing a module math_module = custom_import(\'math\') print(math_module.sqrt(16)) # Should output: 4.0 # Adding a custom module import types my_module = types.ModuleType(\'my_module\') add_module(\'my_module\', my_module) # Accessing the custom module import my_module print(hasattr(my_module, \'__name__\')) # Should output: True # Reloading a module reloaded_math = reload_module(math_module) print(reloaded_math.sqrt(25)) # Should output: 5.0 ``` Good luck and happy coding!","solution":"import sys import importlib import types from types import ModuleType def custom_import(name: str) -> ModuleType: Import a module by its name. if name in sys.modules: return sys.modules[name] try: # Using importlib to import the module module = importlib.import_module(name) except ImportError: raise ImportError(f\\"Module {name} not found\\") return module def reload_module(module: ModuleType) -> ModuleType: Reload the given module and update it in sys.modules. try: reloaded_module = importlib.reload(module) except ImportError: raise ImportError(f\\"Module {module.__name__} cannot be reloaded\\") return reloaded_module def add_module(name: str, module: ModuleType) -> None: Add a module to sys.modules under the given name. if name not in sys.modules: sys.modules[name] = module"},{"question":"# Task You are provided with a Sun AU audio file, and your task is to read the file, apply a specific audio transformation, and then write the transformed audio data back to a new Sun AU file. # Requirements 1. **Function Signature**: ```python def transform_au_file(input_file: str, output_file: str) -> None: ``` 2. **Input**: - `input_file` (str): The path to the input Sun AU audio file. - `output_file` (str): The path where the transformed Sun AU audio file will be saved. 3. **Output**: - The function shall not return any value. The result will be saved in the `output_file`. 4. **Transformation**: - The transformation will be to **reverse the audio**. That is, the audio samples should be played backward. # Constraints - Ensure that the sample width, frame rate, and number of channels in the output file are identical to those in the input file. # Example Usage If `input_file.au` contains a Sun AU audio file with the audio samples `[1, 2, 3, 4]`, the `output_file.au` should contain the reversed samples `[4, 3, 2, 1]`. ```python transform_au_file(\'input_file.au\', \'output_file.au\') ``` # Notes - You may assume that the input file is a valid Sun AU audio file with no compression applied. - Handle any potential exceptions that might arise due to file I/O operations cleanly. # Evaluation Criteria 1. **Correctness**: The transformed audio file should correctly represent the reversed audio of the input file. 2. **Efficiency**: Handle the reading and writing of the audio data efficiently, keeping performance considerations in mind. 3. **Code Quality**: Use clear and maintainable code. Include comments where necessary to explain your logic.","solution":"import wave def transform_au_file(input_file: str, output_file: str) -> None: This function reads a Sun AU audio file, reverses the audio data, and writes it back to a new Sun AU audio file. Parameters: input_file (str): The path to the input Sun AU audio file. output_file (str): The path where the transformed Sun AU audio file will be saved. try: with wave.open(input_file, \'rb\') as inp_file: params = inp_file.getparams() frames = inp_file.readframes(params.nframes) # Reverse the audio frames reversed_frames = frames[::-1] with wave.open(output_file, \'wb\') as out_file: out_file.setparams(params) out_file.writeframes(reversed_frames) except wave.Error as e: print(\\"Error handling wave files:\\", e) except Exception as e: print(\\"An error occurred:\\", e)"},{"question":"**Coding Assessment Question: Resource Management and Debugging in Python** **Objective**: Demonstrate understanding and proficiency in handling resources and debugging in Python utilizing the Python Development Mode. **Problem Description**: You are given a script that processes data from a file and performs some asynchronous operations. The script currently has some unclosed file resources and unawaited coroutines that need to be fixed. Your task is to modify the script such that it properly handles resources and passes all checks and warnings when run in Python Development Mode. **Input**: 1. You will receive a filename as a command-line argument. 2. The file will contain newline-separated integers. **Output**: 1. Print the sum of integers in the file. 2. Print \\"Processing completed!\\" after all processing is done. **Requirements**: 1. The script should not produce any warnings related to unclosed resources. 2. The script should properly await all asynchronous operations. 3. You must run your script in Python Development Mode using `-X dev` to verify that it meets the above requirements. **Initial Script**: ```python import sys import asyncio async def process_data(data): await asyncio.sleep(1) # Simulate some async processing return sum(data) def main(): fp = open(sys.argv[1]) data = [int(line.strip()) for line in fp.readlines()] loop = asyncio.get_event_loop() result = loop.run_until_complete(process_data(data)) print(result) print(\\"Processing completed!\\") if __name__ == \\"__main__\\": main() ``` **Constraints**: - You must handle file resources using context managers or by explicitly closing them. - All asynchronous operations must be properly awaited. - Ensure there are no warnings/errors when running with `python3 -X dev script.py <filename>`. **Your Task**: Modify the script to ensure it meets the specified requirements and constraints. After making your changes, verify by running: ```sh python3 -X dev script.py <your_filename.txt> ``` Ensure there are no warnings or errors related to resource management or unawaited coroutines. **Note**: If your changes are correct, running the script in Python Development Mode should not produce any \\"ResourceWarning\\" or related warning messages.","solution":"import sys import asyncio async def process_data(data): await asyncio.sleep(1) # Simulate some async processing return sum(data) def main(): # Use context manager to handle file resource with open(sys.argv[1]) as fp: data = [int(line.strip()) for line in fp.readlines()] loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) result = loop.run_until_complete(process_data(data)) print(result) print(\\"Processing completed!\\") # Close the event loop to ensure proper resource cleanup loop.close() if __name__ == \\"__main__\\": # Ensure the script runs without warnings in Python development mode main()"},{"question":"# Command Interpreter Implementation Objective: You are required to implement a custom command interpreter using Python\'s `cmd` module. Your task is to create a simple task management shell that can add tasks, list all tasks, mark tasks as done, and delete tasks. Each task will have a description and a status (either \\"done\\" or \\"pending\\"). Requirements: - **Class Definition**: - Define a class `TaskShell` that inherits from `cmd.Cmd`. - **Command Methods**: - Implement the following commands as methods within the `TaskShell` class: - `do_add(self, arg)`: Adds a new task with the given description. The initial status should be \\"pending\\". - `do_list(self, arg)`: Lists all tasks with their statuses. - `do_done(self, arg)`: Marks the task with the given ID as \\"done\\". - `do_delete(self, arg)`: Deletes the task with the given ID. - `do_quit(self, arg)`: Exits the command interpreter. - Each method should have a docstring that explains its usage, which will be utilized in the help command. - **Command Properties**: - Customize the prompt to display `(task)`. - Display a welcoming message when the interpreter starts. Implementation Details: - You can store the tasks in a list of dictionaries, where each dictionary represents a task with keys: `\'id\'`, `\'description\'`, and `\'status\'`. - Ensure that task IDs are unique and assigned sequentially starting from 1. Input Format: - Commands and arguments are input via the command-line interface provided by your custom interpreter. Expected Output: - The output should be printed to the console in response to the commands. Example Interaction: ```python Welcome to the Task Management Shell. Type help or ? to list commands. (task) add Buy milk Task 1 added: Buy milk (task) add Read a book Task 2 added: Read a book (task) list 1. [pending] Buy milk 2. [pending] Read a book (task) done 1 Task 1 marked as done. (task) list 1. [done] Buy milk 2. [pending] Read a book (task) delete 1 Task 1 deleted. (task) list 2. [pending] Read a book (task) quit Thank you for using Task Management Shell. ``` Constraints: - The input for each command should be a single line. - Ensure proper error handling for invalid command arguments (e.g., non-existent task ID).","solution":"import cmd class TaskShell(cmd.Cmd): intro = \\"Welcome to the Task Management Shell. Type help or ? to list commands.n\\" prompt = \\"(task) \\" def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def get_task_by_id(self, task_id): for task in self.tasks: if task[\'id\'] == task_id: return task return None def do_add(self, arg): \\"Add a new task with a description: add <description>\\" task = { \'id\': self.next_id, \'description\': arg, \'status\': \'pending\' } self.tasks.append(task) self.next_id += 1 print(f\\"Task {task[\'id\']} added: {arg}\\") def do_list(self, arg): \\"List all tasks with their statuses\\" for task in self.tasks: print(f\\"{task[\'id\']}. [{task[\'status\']}] {task[\'description\']}\\") def do_done(self, arg): \\"Mark task as done by its ID: done <task_id>\\" try: task_id = int(arg) task = self.get_task_by_id(task_id) if task: task[\'status\'] = \'done\' print(f\\"Task {task_id} marked as done.\\") else: print(f\\"No task with ID {task_id}\\") except ValueError: print(\\"Please provide a valid task ID.\\") def do_delete(self, arg): \\"Delete a task by its ID: delete <task_id>\\" try: task_id = int(arg) task = self.get_task_by_id(task_id) if task: self.tasks.remove(task) print(f\\"Task {task_id} deleted.\\") else: print(f\\"No task with ID {task_id}\\") except ValueError: print(\\"Please provide a valid task ID.\\") def do_quit(self, arg): \\"Exit the command interpreter.\\" print(\\"Thank you for using Task Management Shell.\\") return True"},{"question":"# Handling Missing Values with Pandas Problem Statement You are provided with a DataFrame containing sales data, including both numeric and date information. Your task is to preprocess this data by handling missing values appropriately. The sales data has the following columns: - `SalesID` (int): The unique identifier of each sale. - `SaleAmount` (float): The amount of the sale. - `SaleDate` (datetime): The date of the sale. The DataFrame may contain missing values (`NA` for `SaleAmount` and `NaT` for `SaleDate`). You need to implement a function to clean this data according to the following rules: 1. Replace all `NA` values in the `SaleAmount` column with the mean of the non-missing values in that column. 2. Fill all `NaT` values in the `SaleDate` column with the earliest date in that column. Function Signature ```python import pandas as pd from pandas._libs import NaT def clean_sales_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df`: A pandas DataFrame containing the sales data with the columns `SalesID` (int), `SaleAmount` (float), and `SaleDate` (datetime). Output - Return a pandas DataFrame with the missing values handled as specified. Example Given the DataFrame: ``` SalesID SaleAmount SaleDate 0 1 100.0 2021-05-01 1 2 NaN NaT 2 3 150.0 2021-06-15 3 4 200.0 2021-07-20 ``` The cleaned DataFrame should look like: ``` SalesID SaleAmount SaleDate 0 1 100.000000 2021-05-01 1 2 150.000000 2021-05-01 2 3 150.000000 2021-06-15 3 4 200.000000 2021-07-20 ``` Constraints - The input DataFrame will always have the columns `SalesID`, `SaleAmount`, and `SaleDate`. - The `SalesID` column will not contain any missing values. - There will always be at least one non-missing value in the `SaleAmount` and `SaleDate` columns. Notes - Utilize pandas functionalities to handle missing data efficiently. - Ensure the solution is optimized for performance, especially when dealing with large datasets.","solution":"import pandas as pd def clean_sales_data(df: pd.DataFrame) -> pd.DataFrame: Cleans the sales data by filling missing values: - Replace NA in \'SaleAmount\' with the mean of non-missing values. - Replace NaT in \'SaleDate\' with the earliest date in non-missing values. Args: df (pd.DataFrame): DataFrame containing sales data with columns \'SalesID\', \'SaleAmount\', and \'SaleDate\'. Returns: pd.DataFrame: Cleaned DataFrame with missing values handled. # Calculate the mean of the \'SaleAmount\' column sale_amount_mean = df[\'SaleAmount\'].mean() # Calculate the earliest date in the \'SaleDate\' column earliest_date = df[\'SaleDate\'].min() # Replace NA values in \'SaleAmount\' with the calculated mean df[\'SaleAmount\'].fillna(sale_amount_mean, inplace=True) # Replace NaT values in \'SaleDate\' with the earliest date df[\'SaleDate\'].fillna(earliest_date, inplace=True) return df"},{"question":"**Problem Statement:** Implement a function to concurrently fetch the contents of multiple URLs using the `asyncio` high-level API. The function should take a list of URLs and return a dictionary where the keys are the URLs and the values are the fetched contents. If a URL cannot be fetched, the value should be `None`. **Function Signature:** ```python import asyncio import aiohttp async def fetch_all_contents(urls: List[str]) -> Dict[str, Optional[str]]: Given a list of URLs, fetch the contents concurrently using asyncio and aiohttp. :param urls: List of strings representing the URLs to fetch. :return: Dictionary where keys are URLs and values are the contents or None if fetch fails. ``` **Input Format:** - `urls`: A list of strings where each string is a URL. **Output Format:** - A dictionary where each key is a URL from the input list and the corresponding value is the content fetched from that URL. If the URL cannot be fetched successfully, the value should be `None`. **Constraints:** - You can assume that the list of URLs will have at most 100 URLs. - The fetching of each URL must be done concurrently. **Performance Requirements:** - The solution must be efficient and leverage asynchronous operations to fetch URLs concurrently, reducing the total amount of time taken compared to synchronous fetching. **Example:** ```python urls = [ \'http://example.com\', \'http://example.org\', \'http://nonexistent.url\' ] # Example output (the actual content will vary) { \'http://example.com\': \'Content of example.com\', \'http://example.org\': \'Content of example.org\', \'http://nonexistent.url\': None } ``` **Additional Information:** - You can use the `aiohttp` library for making HTTP requests asynchronously within the `asyncio` framework. - Make sure to handle any exceptions that occur during the fetching of URLs to ensure that the function returns `None` for any URL that cannot be fetched.","solution":"import asyncio import aiohttp from typing import List, Dict, Optional async def fetch_content(session: aiohttp.ClientSession, url: str) -> Optional[str]: Fetch the content of a URL using the given session. :param session: aiohttp ClientSession :param url: URL to fetch content from :return: Content of the URL or None if fetch fails try: async with session.get(url) as response: response.raise_for_status() # Will raise an exception for HTTP errors return await response.text() except (aiohttp.ClientError, asyncio.TimeoutError): return None async def fetch_all_contents(urls: List[str]) -> Dict[str, Optional[str]]: Given a list of URLs, fetch the contents concurrently using asyncio and aiohttp. :param urls: List of strings representing the URLs to fetch. :return: Dictionary where keys are URLs and values are the contents or None if fetch fails. async with aiohttp.ClientSession() as session: tasks = [fetch_content(session, url) for url in urls] contents = await asyncio.gather(*tasks) return dict(zip(urls, contents))"},{"question":"Objective To assess the student’s understanding of the `site` module, specifically the automatic inclusion of site-specific paths, manipulation of `sys.path`, and handling of .pth configuration files. Task You are to write a Python script that: 1. Adds a new directory to `sys.path` using the `site.addsitedir` function. 2. Creates and manages a .pth file to automatically include specific paths when the Python interpreter starts. Instructions 1. **Adding a Directory to `sys.path`**: - Implement a function `add_directory_to_sys_path(directory: str) -> None` that uses the `site.addsitedir` method to add a given directory to `sys.path`. 2. **Managing a .pth file**: - Implement a function `create_pth_file(pth_file: str, paths: list) -> None` that creates a .pth file and writes each path from the `paths` list to this file. - Ensure each path in the .pth file is written on a new line. - If a line starts with `import`, it should be executed upon startup. - If the .pth file already exists, append the new paths without duplicating existing ones. 3. **Combining Both Tasks**: - Create a script that: - Adds a specific directory to `sys.path`. - Creates a .pth configuration file that includes multiple paths, ensuring they are loaded when Python starts. Example Given the directory `/path/to/my-lib` and paths `[\\"/another/path\\", \\"import sys; print(sys.version)\\"]`, your script should: 1. Add `/path/to/my-lib` to `sys.path`. 2. Create or update a file `myconfig.pth` with the provided paths. Constraints - The .pth file should be stored within a directory in `site.USER_SITE`. - Do not add duplicate paths in `sys.path` or in the .pth file. Expected Functions ```python import os import site def add_directory_to_sys_path(directory: str) -> None: Adds the specified directory to sys.path using the site.addsitedir function. :param directory: The directory to add to sys.path # Your code here def create_pth_file(pth_file: str, paths: list) -> None: Creates a .pth file with the specified paths. If the file exists, appends new paths. :param pth_file: The .pth file to create or update :param paths: A list of paths to add to the .pth file # Your code here ``` Your solution should include handling edge cases such as: - Duplicate paths in the .pth file. - Proper path validation to ensure only existing directories are added. Submit the implemented `add_directory_to_sys_path` and `create_pth_file` functions along with a script demonstrating their usage.","solution":"import os import site import sys def add_directory_to_sys_path(directory: str) -> None: Adds the specified directory to sys.path using the site.addsitedir function. :param directory: The directory to add to sys.path if os.path.isdir(directory): site.addsitedir(directory) def create_pth_file(pth_file: str, paths: list) -> None: Creates a .pth file with the specified paths. If the file exists, appends new paths. :param pth_file: The .pth file to create or update :param paths: A list of paths to add to the .pth file if not os.path.isdir(os.path.dirname(pth_file)): raise FileNotFoundError(f\\"The directory `{os.path.dirname(pth_file)}` does not exist.\\") existing_paths = set() if os.path.exists(pth_file): with open(pth_file, \'r\') as f: existing_paths = {line.strip() for line in f} new_paths = [path for path in paths if path not in existing_paths] with open(pth_file, \'a\') as f: for path in new_paths: f.write(f\\"{path}n\\")"},{"question":"Objective The objective of this exercise is to assess your ability to create customized scatter plots using Seaborn\'s `so.Plot` API. You will be given a dataset and required to produce a specific type of customized plot demonstrating your understanding of data visualization concepts including color mapping, marker styles, and jitter. Problem Statement You are provided with the `mpg` dataset from Seaborn. Your task is to create a scatter plot using this dataset with the following specifications: 1. Plot `horsepower` on the x-axis and miles per gallon (`mpg`) on the y-axis. 2. Color the points based on the `origin` of the car. 3. Use different shapes for points based on the `cylinders`. 4. Apply jitter to the points to better visualize the distribution, particularly to avoid overplotting. 5. Customize the transparency of the points for better density visualization. Function Signature ```python def customized_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset mpg = load_dataset(\\"mpg\\") # Your code here to create the plot # Display the plot plt.show() ``` Expected Output When you call the `customized_scatter_plot()` function, it should display a scatter plot meeting the specifications above. Ensure that: - Different origins are represented by different colors. - Different cylinder counts are represented by different markers. - Jitter is applied to avoid overplotting. - Points are slightly transparent for better density visualization. Constraints - Use only the `seaborn.objects` module for creating the plot. - Do not save the plot to a file; only display it using `plt.show()`. This task will test your understanding of seaborn\'s customization capabilities, handling overlapping data points with jitter, and representing multidimensional data with color and shape coding.","solution":"def customized_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the scatter plot p = (so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\", color=\\"origin\\", marker=\\"cylinders\\") .add(so.Dot(), so.Jitter(), alpha=0.6)) # Show the plot p.show()"},{"question":"**Objective:** You are tasked with developing a concurrent program that processes a list of URLs to fetch their HTML content. This will assess your understanding of the \\"multiprocessing\\" package in Python. **Description:** Write a Python program using the \\"multiprocessing\\" package to: 1. Create a pool of worker processes to fetch the HTML content of a list of URLs concurrently. 2. Use a Queue to manage the results returned by the worker processes. 3. Ensure that the results are processed and displayed in the same order as the URLs given. **Function to Implement:** ```python import multiprocessing from queue import Empty import requests def fetch_url(url): Fetch the HTML content of the given URL. Args: - url (str): The URL to fetch. Returns: - str: HTML content of the URL. try: response = requests.get(url) return response.text except requests.RequestException as e: return f\\"Error fetching {url}: {str(e)}\\" def process_urls(urls): Process a list of URLs concurrently using a pool of worker processes. Args: - urls (list): List of URLs to fetch. Returns: - list: List of HTML contents or error messages corresponding to the URLs. # Create a results queue results = multiprocessing.Queue() # Define a worker function that fetches URL content and puts result in queue def worker(url): html_content = fetch_url(url) results.put((url, html_content)) # Create a pool of workers to process the URLs concurrently with multiprocessing.Pool(processes=min(4, len(urls))) as pool: pool.map(worker, urls) # Collect and return results from the queue in the same order as the input URLs output = [] while len(output) < len(urls): try: url, content = results.get(timeout=1) output.append((url, content)) except Empty: break return [content for (_, content) in sorted(output, key=lambda x: urls.index(x[0]))] # Example Usage: if __name__ == \\"__main__\\": urls = [\'http://example.com\', \'http://example.org\', \'http://example.net\'] results = process_urls(urls) for content in results: print(content[:100]) # Print first 100 characters of the fetched HTML content or error message ``` **Input and Output:** - Input: A list of URLs as strings (e.g., `[\'http://example.com\', \'http://example.org\']`). - Output: A list of HTML content strings or error messages in the same order as the input URLs. **Constraints:** - Do not use any global variables. - You must use the \\"multiprocessing\\" package to manage concurrency. - Ensure that worker processes handle exceptions raised by network calls gracefully. **Performance Requirements:** - The solution should efficiently utilize multiple processes. - Ensure that results are processed and displayed in the same order as the URLs provided. **Hints:** 1. Use `multiprocessing.Queue` to store results from the worker processes. 2. Use `multiprocessing.Pool` to manage the worker processes. 3. Ensure to handle network exceptions using try-except blocks within the `fetch_url` function. Happy coding!","solution":"import multiprocessing from queue import Empty import requests def fetch_url(url): Fetch the HTML content of the given URL. Args: - url (str): The URL to fetch. Returns: - str: HTML content of the URL. try: response = requests.get(url) return response.text except requests.RequestException as e: return f\\"Error fetching {url}: {str(e)}\\" def worker(url, queue): Worker function to fetch URL content and put the result in a queue. Args: - url (str): The URL to fetch. - queue (multiprocessing.Queue): The queue to put the result into. html_content = fetch_url(url) queue.put((url, html_content)) def process_urls(urls): Process a list of URLs concurrently using a pool of worker processes. Args: - urls (list): List of URLs to fetch. Returns: - list: List of HTML contents or error messages corresponding to the URLs. # Create a results queue results = multiprocessing.Queue() jobs = [] # Create and start a process for each URL for url in urls: p = multiprocessing.Process(target=worker, args=(url, results)) p.start() jobs.append(p) # Ensure all processes complete for job in jobs: job.join() # Collect and return results from the queue in the same order as the input URLs output = [] while len(output) < len(urls): try: url, content = results.get(timeout=1) output.append((url, content)) except Empty: break return [content for (_, content) in sorted(output, key=lambda x: urls.index(x[0]))] # Example Usage: if __name__ == \\"__main__\\": urls = [\'http://example.com\', \'http://example.org\', \'http://example.net\'] results = process_urls(urls) for content in results: print(content[:100]) # Print first 100 characters of the fetched HTML content or error message"},{"question":"Objective Your task is to implement a function that logs a specific type of event during a distributed training process using PyTorch\'s `torch.distributed.elastic.events` module. Background You are provided with a partial implementation of a distributed training scenario. Your goal is to enhance this by implementing event logging to keep track of specific training milestones. Function Specification - Implement the function `log_training_event` with the following specification: ```python import torch.distributed.elastic.events as events def log_training_event(event_name: str, metadata: dict) -> None: Records an event with the given name and metadata during a distributed training procedure. Parameters: ---------- event_name : str The name of the event to be recorded. metadata : dict A dictionary of metadata to be included with the event. Returns: ------- None pass ``` Input and Output - **Input**: - `event_name` (str): The name of the event to be recorded. - `metadata` (dict): A dictionary containing the metadata for the event. Keys and values are both strings. - **Output**: - This function does not return anything. Constraints - The metadata dictionary should not be empty; if it is, the function should raise a `ValueError`. - Use the `torch.distributed.elastic.events.record` method to log the event. - Metadata keys and values should be limited to alphanumeric characters and underscores to avoid any logging issues. Performance Requirements - Ensure that the event logging does not introduce significant latency into the training process. Example Usage ```python log_training_event(event_name=\\"EPOCH_START\\", metadata={\\"epoch\\": \\"1\\", \\"stage\\": \\"initial\\"}) ``` Here, an event named `EPOCH_START` marking the start of the first epoch has been logged with the specified metadata. # Evaluation Criteria - Correct implementation of the `log_training_event` function. - Proper handling of metadata constraints. - Efficient and error-free integration of the PyTorch event logging tools. - Clear, maintainable, and readable code. Notes You can assume the environment is set up for distributed training, and the necessary PyTorch packages are installed.","solution":"import torch.distributed.elastic.events as events import re def log_training_event(event_name: str, metadata: dict) -> None: Records an event with the given name and metadata during a distributed training procedure. Parameters: ---------- event_name : str The name of the event to be recorded. metadata : dict A dictionary of metadata to be included with the event. Returns: ------- None # Check if metadata is empty if not metadata: raise ValueError(\\"Metadata dictionary must not be empty.\\") # Validate the metadata keys and values for key, value in metadata.items(): if not re.match(r\\"^w+\\", key) or not re.match(r\\"^w+\\", value): raise ValueError(\\"Metadata keys and values must be alphanumeric characters and underscores only.\\") # Record the event events.record(event_name, **metadata)"},{"question":"**Objective:** Write a Python function using the Python/C API functions to manipulate sequences as described in the documentation. Implement a mini version of a list class, `MyList`, which models some basic list operations. **Problem Statement:** You are required to implement a class `MyList` that supports the following methods using the Python/C API functions provided above: 1. **`__init__(self, items)`**: Initialize the `MyList` with a given list of items. 2. **`__getitem__(self, index)`**: Retrieve an item at a specified index. 3. **`__setitem__(self, index, value)`**: Set the item at a specified index to a new value. 4. **`__len__(self)`**: Return the length of the list. 5. **`__delitem__(self, index)`**: Delete an item at a specified index. 6. **`__contains__(self, value)`**: Check if the list contains a specified value. 7. **`concat(self, other)`**: Concatenate another `MyList` instance to the current list. 8. **`repeat(self, count)`**: Repeat the contents of the list a specified number of times. **Constraints:** - All indices are 0-based. - Assume all input types and values are valid. - Methods should operate on Python sequence objects such as lists and tuples. ```python class MyList: def __init__(self, items): self.items = items def __getitem__(self, index): # Your code here using PySequence_GetItem def __setitem__(self, index, value): # Your code here using PySequence_SetItem def __len__(self): # Your code here using PySequence_Size def __delitem__(self, index): # Your code here using PySequence_DelItem def __contains__(self, value): # Your code here using PySequence_Contains def concat(self, other): # Your code here using PySequence_Concat def repeat(self, count): # Your code here using PySequence_Repeat ``` **Input Format:** - The constructor receives a single list `items` of elements. - Methods will be tested individually with appropriate arguments. **Output Format:** - `__getitem__`: Return the item at the given index. - `__setitem__`: No output, just modify the list. - `__len__`: Return the length of the list. - `__delitem__`: No output, just delete the item from the list. - `__contains__`: Return `True` or `False`. - `concat`: Return a new `MyList` instance with concatenated lists. - `repeat`: Return a new `MyList` instance with repeated items. **Example:** ```python ml = MyList([1, 2, 3]) print(ml[0]) # Output: 1 ml[1] = 4 print(ml.items) # Output: [1, 4, 3] print(len(ml)) # Output: 3 del ml[2] print(ml.items) # Output: [1, 4] print(2 in ml) # Output: False ml2 = MyList([5, 6]) concat_list = ml.concat(ml2) print(concat_list.items) # Output: [1, 4, 5, 6] repeat_list = ml.repeat(2) print(repeat_list.items) # Output: [1, 4, 1, 4] ``` Ensure your implementation correctly handles these operations using the Python/C API functions from the documentation.","solution":"class MyList: def __init__(self, items): self.items = items def __getitem__(self, index): return self.items[index] def __setitem__(self, index, value): self.items[index] = value def __len__(self): return len(self.items) def __delitem__(self, index): del self.items[index] def __contains__(self, value): return value in self.items def concat(self, other): return MyList(self.items + other.items) def repeat(self, count): return MyList(self.items * count)"},{"question":"# Advanced Coding Assessment: Understanding Python\'s Module Import Mechanisms Objective: Design and implement a sequence of functions to simulate an advanced module import system in Python, leveraging some of the principles outlined in the provided documentation. Problem Statement: Implement a class `CustomModuleImporter` supporting the following functionalities: 1. **Import Module**: Allow importing a module by name. 2. **Reload Module**: Reload an already imported module. 3. **Add Built-in Module**: Simulate the addition of built-in modules with initialization functions. 4. **Get Module Dictionary**: Retrieve the current dictionary of imported modules. The class should provide methods to perform these actions, closely mimicking the behavior of Python’s internal import system as described. Required Methods: 1. **import_module(self, name: str) -> object**: - Imports a module by name. - Adds the module to the internal dictionary. - If the module cannot be imported, raise an `ImportError`. 2. **reload_module(self, name: str) -> object**: - Reloads an already imported module. - If the module is not found, raise a `ModuleNotFoundError`. 3. **add_builtin_module(self, name: str, initfunc: callable) -> None**: - Adds a built-in module with a specific initialization function. - If the module already exists, raise a `ValueError`. 4. **get_module_dict(self) -> dict**: - Returns the internal dictionary of imported modules. Input and Output Formats: - `import_module(name)` - **Input:** `name` (string). **Output:** Module object or raises `ImportError`. - `reload_module(name)` - **Input:** `name` (string). **Output:** Module object or raises `ModuleNotFoundError`. - `add_builtin_module(name, initfunc)` - **Input:** `name` (string), `initfunc` (callable). **Output:** None or raises `ValueError`. - `get_module_dict()` - **Input:** None. **Output:** Dictionary of imported modules. Constraints: 1. Modules are represented as strings in the internal dictionary. 2. Assume that initialization functions for built-in modules return a string representing the module. 3. Only basic exceptions `ImportError`, `ModuleNotFoundError`, and `ValueError` need to be handled. Example: ```python class CustomModuleImporter: def __init__(self): self.modules = {} def import_module(self, name: str) -> object: # Implementation here def reload_module(self, name: str) -> object: # Implementation here def add_builtin_module(self, name: str, initfunc: callable) -> None: # Implementation here def get_module_dict(self) -> dict: # Implementation here # Example Usage importer = CustomModuleImporter() importer.add_builtin_module(\\"mod1\\", lambda: \\"Module 1\\") print(importer.import_module(\\"mod1\\")) # Output: \\"Module 1\\" print(importer.reload_module(\\"mod1\\")) # Output: \\"Module 1\\" print(importer.get_module_dict()) # Output: {\'mod1\': \'Module 1\'} ``` Evaluation Criteria: 1. **Accuracy**: Correctness of the implemented methods. 2. **Robustness**: Handling of edge cases and exceptions. 3. **Clarity**: Code readability and proper commenting. 4. **Efficiency**: Optimal usage of data structures and algorithms. Good luck!","solution":"class CustomModuleImporter: def __init__(self): self.modules = {} def import_module(self, name: str) -> object: if name in self.modules: return self.modules[name] else: raise ImportError(f\\"Module {name} cannot be imported\\") def reload_module(self, name: str) -> object: if name in self.modules: return self.modules[name] else: raise ModuleNotFoundError(f\\"Module {name} not found\\") def add_builtin_module(self, name: str, initfunc: callable) -> None: if name in self.modules: raise ValueError(f\\"Module {name} already exists\\") else: self.modules[name] = initfunc() def get_module_dict(self) -> dict: return self.modules"},{"question":"Objective Your task is to implement a Python function that performs various operations on floating point numbers using the provided floating point functions. Question You need to write a Python function that takes a list of strings representing floating-point numbers, and performs multiple operations including creating `PyFloatObject` instances, converting them to C doubles, and fetching float information. The function signature should be: ```python def float_operations(float_strings: List[str]) -> dict: ``` Input - `float_strings`: A list of strings, where each string represents a potential floating-point number. Output - A dictionary containing: - `\'max_float\'`: The maximum representable finite float. - `\'min_float\'`: The minimum normalized positive float. - `\'converted_floats\'`: A list of floating-point numbers converted from the input strings. - `\'double_values\'`: A list of C double values obtained from converting the floating-point numbers. Constraints - Each element in `float_strings` is a valid string representation of a floating-point number. Requirements 1. Use `PyFloat_FromString` to create `PyFloatObject` instances from the input strings. 2. Use `PyFloat_AsDouble` to convert the `PyFloatObject` instances to C doubles. 3. Obtain the maximum and minimum representable float values using `PyFloat_GetMax` and `PyFloat_GetMin` respectively. Example ```python input_strings = [\\"3.14\\", \\"2.71\\", \\"1.41\\"] output = float_operations(input_strings) print(output) ``` Expected output: ```python { \'max_float\': <maximum representable float>, \'min_float\': <minimum normalized positive float>, \'converted_floats\': [3.14, 2.71, 1.41], \'double_values\': [3.14, 2.71, 1.41] } ``` **Note**: The representation values of `\'max_float\'` and `\'min_float\'` will depend on the specific environment and version of Python. Implementation Skeleton ```python from typing import List from python310 import ( PyFloat_FromString, PyFloat_AsDouble, PyFloat_GetMax, PyFloat_GetMin ) def float_operations(float_strings: List[str]) -> dict: converted_floats = [] double_values = [] for fs in float_strings: pyfloat = PyFloat_FromString(fs) if pyfloat is not None: converted_floats.append(float(fs)) double_values.append(PyFloat_AsDouble(pyfloat)) max_float = PyFloat_GetMax() min_float = PyFloat_GetMin() return { \'max_float\': max_float, \'min_float\': min_float, \'converted_floats\': converted_floats, \'double_values\': double_values } ``` Write the function `float_operations(float_strings: List[str]) -> dict` to solve the problem as described.","solution":"from typing import List import sys def float_operations(float_strings: List[str]) -> dict: converted_floats = [] double_values = [] for fs in float_strings: # Attempt to convert string to float try: float_value = float(fs) except ValueError: continue # Skip any invalid strings converted_floats.append(float_value) double_values.append(float_value) # Get maximum and minimum float values max_float = sys.float_info.max min_float = sys.float_info.min return { \'max_float\': max_float, \'min_float\': min_float, \'converted_floats\': converted_floats, \'double_values\': double_values }"},{"question":"# Pandas Display Options Management Objective You are tasked with writing a function that manipulates pandas display options to customize how DataFrames are displayed in Jupyter Notebooks. Problem Write a function `customize_display_options` that accepts a DataFrame and a dictionary of options. The function should update the pandas display options according to the given dictionary, print the DataFrame, and then reset the options to their default values. Function Signature ```python def customize_display_options(df: pd.DataFrame, options: Dict[str, Any]) -> None: pass ``` Input 1. `df` (pd.DataFrame): The DataFrame to display with customized options. 2. `options` (Dict[str, Any]): A dictionary where keys are option names (`str`) and values are the option values. Output - The function should print the DataFrame configured according to the options in `options` dictionary. - After printing the DataFrame, reset the pandas display options to their default values and ensure no changes are permanently applied. Constraints - You may assume all option keys provided in `options` dictionary are valid. - Ensure that after the function execution, all options are reset to their pandas default state. Example ```python import pandas as pd from typing import Dict, Any def customize_display_options(df: pd.DataFrame, options: Dict[str, Any]) -> None: # Your implementation here pass # Example usage: df = pd.DataFrame(np.random.randn(10, 3), columns=list(\'ABC\')) options = { \'display.max_rows\': 5, \'display.max_columns\': 2, \'display.precision\': 4 } customize_display_options(df, options) # Expected behavior: Prints the DataFrame with max 5 rows, max 2 columns, and precision of 4 # After exiting the function, all options should be reset to their default values. print(pd.get_option(\'display.max_rows\')) # Should print default value print(pd.get_option(\'display.max_columns\')) # Should print default value print(pd.get_option(\'display.precision\')) # Should print default value ``` Notes - Use `pd.set_option` to set pandas options. - Use `pd.reset_option` to reset options to their default values. - Utilize `with pd.option_context` to manage the temporary setting and resetting of options if needed.","solution":"import pandas as pd from typing import Dict, Any def customize_display_options(df: pd.DataFrame, options: Dict[str, Any]) -> None: Updates pandas display options according to the options dictionary, prints the DataFrame, and then resets the options to default values. with pd.option_context(*sum(options.items(), ())): # Unpack options into context print(df) # Options are automatically reset to defaults after exiting the context"},{"question":"**Question:** You are provided with a dataset that requires several preprocessing steps before training a machine learning model. Your task is to implement a preprocessing pipeline using scikit-learn transformers to prepare the data for model training. # Dataset: The dataset consists of the following features: 1. `age`: Continuous numeric values. 2. `income`: Continuous numeric values. 3. `gender`: Categorical values (`\'Male\'`, `\'Female\'`). 4. `education`: Categorical values (`\'High School\'`, `\'Bachelors\'`, `\'Masters\'`, `\'PhD\'`). 5. `employment`: Categorical values (`\'Employed\'`, `\'Unemployed\'`). 6. `target`: Binary target variable (`0` or `1`). # Tasks: 1. **Imputation**: - Replace missing values in the numeric features (`age`, `income`) with the mean of the respective features. - Replace missing values in the categorical features (`gender`, `education`, `employment`) with the most frequent value of the respective features. 2. **Encoding**: - Encode the `gender`, `education`, and `employment` features using one-hot encoding. 3. **Scaling**: - Standardize the numeric features (`age`, `income`) to have zero mean and unit variance. 4. **Pipeline Creation**: - Create a preprocessing pipeline that performs the above steps and applies these transformations to the data. - Fit the pipeline to the given dataset and transform the data. 5. **Model Training**: - Train a Logistic Regression model using the preprocessed data. # Input: - A pandas DataFrame `df` with the aforementioned features, including missing values. # Output: - The fitted preprocessing pipeline. - The transformed feature matrix. - The trained Logistic Regression model. # Constraints: - Use scikit-learn transformers like `SimpleImputer`, `OneHotEncoder`, `StandardScaler`, `Pipeline`, and `LogisticRegression`. # Performance Requirements: - The resulting transformed feature matrix should have no missing values and should be appropriately scaled and encoded. ```python import pandas as pd from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.linear_model import LogisticRegression def preprocess_and_train(df): # Define the preprocessing steps for numeric features numeric_features = [\'age\', \'income\'] numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Define the preprocessing steps for categorical features categorical_features = [\'gender\', \'education\', \'employment\'] categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine the preprocessing steps using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create the full pipeline with the preprocessor and the classifier model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Separate target variable X = df.drop(\'target\', axis=1) y = df[\'target\'] # Fit the pipeline to the data and transform the data model.fit(X, y) # Extracting the preprocessed data preprocessed_data = model.named_steps[\'preprocessor\'].transform(X) return model.named_steps[\'preprocessor\'], preprocessed_data, model.named_steps[\'classifier\'] # Example usage # df = pd.read_csv(\'path_to_your_csv_file.csv\') # preprocessor, preprocessed_data, trained_model = preprocess_and_train(df) ``` # Notes: - Ensure the transformers handle any possible edge cases, such as unseen categories during encoding. - Test your implementation on a sample dataset to verify correctness.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.linear_model import LogisticRegression def preprocess_and_train(df): Preprocess the dataset and train a logistic regression model. Parameters: - df: pandas DataFrame with columns [\'age\', \'income\', \'gender\', \'education\', \'employment\', \'target\'] Returns: - preprocessor: fitted preprocessing pipeline - preprocessed_data: numpy array of the transformed feature matrix - trained_model: trained LogisticRegression model # Define the preprocessing steps for numeric features numeric_features = [\'age\', \'income\'] numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Define the preprocessing steps for categorical features categorical_features = [\'gender\', \'education\', \'employment\'] categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine the preprocessing steps using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create the full pipeline with the preprocessor and the classifier model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Separate target variable X = df.drop(\'target\', axis=1) y = df[\'target\'] # Fit the pipeline to the data and transform the data model.fit(X, y) # Extracting the preprocessed data preprocessed_data = model.named_steps[\'preprocessor\'].transform(X) return model.named_steps[\'preprocessor\'], preprocessed_data, model.named_steps[\'classifier\']"},{"question":"# URL Normalizer Problem Statement You are to write a function that normalizes a given list of URLs by performing the following operations: 1. Parse each URL into its components. 2. Ensure that the scheme is always `https`. 3. Convert the hostname to lowercase. 4. Remove any fragment identifiers. 5. Remove any default ports (80 for HTTP, 443 for HTTPS). 6. Sort query parameters alphabetically by key. You should then return the list of normalized URLs. Function Signature ```python from typing import List def normalize_urls(urls: List[str]) -> List[str]: pass ``` Input - A list of strings, `urls`, where each string is a valid URL. Output - A list of strings containing the normalized URLs. Constraints - Length of the list `urls` will be between 1 and 1000. - Each URL in the list will have a length up to 2048 characters. - Assume all URLs are well-formed and valid as per RFC 3986. Example ```python urls = [ \\"HTTP://Example.com:80/some/path?b=2&a=1#section1\\", \\"https://example.com:443/some/path?b=2&a=1#section2\\", \\"http://example.com/some/path?b=2&a=1\\", \\"https://example.com/some/path?b=2&a=1#section3\\", ] normalized_urls = normalize_urls(urls) print(normalized_urls) ``` Output: ```python [ \\"https://example.com/some/path?a=1&b=2\\", \\"https://example.com/some/path?a=1&b=2\\", \\"https://example.com/some/path?a=1&b=2\\", \\"https://example.com/some/path?a=1&b=2\\", ] ``` Explanation 1. The scheme of all the URLs is changed to `https`. 2. Hostnames are converted to lowercase. 3. Fragment identifiers are removed. 4. Default ports are removed. 5. Query parameters are sorted alphabetically by key. Notes - Utilize functions from `urllib.parse` such as `urlparse`, `urlunparse`, `parse_qs`, `urlencode`, and `urljoin` to break down and reconstruct the URLs as needed. - Handle edge cases such as URLs with no query parameters or fragment identifiers appropriately. - Ensure the normalized URLs are returned in the same order they were provided.","solution":"from typing import List from urllib.parse import urlparse, urlunparse, parse_qs, urlencode def normalize_urls(urls: List[str]) -> List[str]: normalized_urls = [] for url in urls: # Parse the URL into components parsed_url = urlparse(url) # Ensure the scheme is always \'https\' scheme = \'https\' # Convert the hostname to lowercase netloc = parsed_url.hostname.lower() # Remove default ports if parsed_url.port: if (parsed_url.scheme == \'http\' and parsed_url.port == 80) or (parsed_url.scheme == \'https\' and parsed_url.port == 443): netloc = parsed_url.hostname.lower() # Remove default port else: netloc = f\\"{parsed_url.hostname.lower()}:{parsed_url.port}\\" # Sort query parameters alphabetically by key query = parse_qs(parsed_url.query, keep_blank_values=True) sorted_query = sorted(query.items()) sorted_query_str = urlencode([(key, value) for key, values in sorted_query for value in values]) # Remove fragment identifiers fragment = \'\' # Reconstruct the URL with the normalized components normalized_url = urlunparse(( scheme, netloc, parsed_url.path, parsed_url.params, sorted_query_str, fragment )) normalized_urls.append(normalized_url) return normalized_urls"},{"question":"<|Analysis Begin|> The `urllib.request` module is an extensive library in Python 3.10 designed for opening and processing URLs. This module includes functionality for managing HTTP and FTP protocols, handling redirections, managing cookies, processing errors, and handling various types of authentication (Basic, Digest, and Proxy). It provides a flexible and powerful set of tools for interacting with web resources. Key components of the `urllib.request` module include: - **Functions**: `urlopen`, `install_opener`, `build_opener`, `pathname2url`, `url2pathname`, and `getproxies`. - **Classes**: `Request`, `OpenerDirector`, `BaseHandler`, `HTTPDefaultErrorHandler`, `HTTPRedirectHandler`, `HTTPCookieProcessor`, `ProxyHandler`, `HTTPPasswordMgr`, `HTTPPasswordMgrWithDefaultRealm`, `HTTPPasswordMgrWithPriorAuth`, `AbstractBasicAuthHandler`, `HTTPBasicAuthHandler`, `ProxyBasicAuthHandler`, `AbstractDigestAuthHandler`, `HTTPDigestAuthHandler`, `ProxyDigestAuthHandler`, `HTTPHandler`, `HTTPSHandler`, `FileHandler`, `DataHandler`, `FTPHandler`, `CacheFTPHandler`, `UnknownHandler`, `HTTPErrorProcessor`. The `urlopen` function is one of the most frequently used functions within this module, which facilitates opening URLs. It accepts various parameters such as `url`, `data`, `timeout`, `cafile`, `capath`, `cadefault`, and `context`. Understanding how to use this function properly, handling HTTP responses, and processing URLs effectively requires a good grasp of request-response cycles in web communication. Given this comprehensive functionality, a well-designed coding assessment question can test students\' knowledge of: 1. Constructing and sending HTTP requests using `urllib.request`. 2. Handling responses and possible exceptions. 3. Managing URLs, proxies, and authentication procedures. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Title: Implementing an HTTP Client with `urllib.request` Objective: Design and implement a Python function that fetches data from a provided URL. Your function should handle the following tasks: 1. Send a GET or POST request based on the provided parameters. 2. Handle and print different HTTP response codes. 3. Manage basic HTTP authentication if credentials are provided. 4. Handle possible errors gracefully and provide relevant output. Function Signature: ```python def fetch_url_data(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, credentials: tuple = None) -> str: ... ``` Parameters: - `url`: A string representing the URL to fetch data from. - `method`: A string representing the HTTP method to use (\\"GET\\" or \\"POST\\"). Default is \\"GET\\". - `data`: A dictionary representing data to send with a POST request. Default is `None`. - `headers`: A dictionary representing additional headers to include in the request. Default is `None`. - `credentials`: A tuple containing username and password for basic authentication. Default is `None`. Returns: - A string containing the fetched data from the URL. Constraints: - If `method` is \\"POST\\" and `data` is provided, convert the dictionary to the appropriate format and include it in the request. - If `credentials` are provided, use basic HTTP authentication. Example: ```python data = fetch_url_data( url=\\"http://www.example.com/api\\", method=\\"POST\\", data={\\"key\\": \\"value\\"}, headers={\\"Custom-Header\\": \\"HeaderValue\\"}, credentials=(\\"user\\", \\"password\\") ) print(data) ``` Notes: 1. Handle and print meaningful error messages for HTTP error codes like 404 (Not Found) and 500 (Internal Server Error). 2. For GET requests, include any provided headers. 3. For POST requests, convert the data dictionary to bytes after encoding it in `application/x-www-form-urlencoded` format, and include the `Content-Type` header appropriately. 4. If `credentials` are provided, configure the request to use basic HTTP authentication. Good luck! This will test your understanding of how to use the `urllib.request` module, handle HTTP requests and responses, and manage basic authentication.","solution":"import urllib.request import urllib.parse import base64 def fetch_url_data(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, credentials: tuple = None) -> str: if headers is None: headers = {} if method == \\"POST\\" and data is not None: data = urllib.parse.urlencode(data).encode() headers[\'Content-Type\'] = \'application/x-www-form-urlencoded\' else: data = None req = urllib.request.Request(url, data=data, headers=headers) req.get_method = lambda: method if credentials: username, password = credentials auth = base64.b64encode(f\\"{username}:{password}\\".encode()).decode(\'utf-8\') req.add_header(\\"Authorization\\", f\\"Basic {auth}\\") try: with urllib.request.urlopen(req) as response: result = response.read().decode() return result except urllib.error.HTTPError as e: return f\\"HTTPError: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\""},{"question":"# PyTorch Coding Assessment: Named Tensors **Objective**: The aim of this task is to assess your understanding of named tensors in PyTorch, including how to manage and manipulate tensor names correctly using built-in operations. **Problem Statement**: You are provided with two named tensors: `tensor_a` and `tensor_b` with shapes and names as follows: - `tensor_a`: Shape (3, 4, 5) with names `(\'X\', \'Y\', \'Z\')` - `tensor_b`: Shape (4, 5, 6) with names `(\'Y\', \'Z\', \'W\')` Using these tensors, perform the following operations and ensure that the resulting tensor maintains the correct names for each dimension: 1. **Matrix Multiplication**: Perform a matrix multiplication between `tensor_a` and `tensor_b`. The resulting tensor should have dimensions `(3, 4, 6)` with the correct names propagated accordingly. 2. **Sum Reduction**: Sum the resulting tensor from the matrix multiplication over dimension `Y`, which should reduce the number of dimensions by 1. The resulting tensor should have dimensions `(3, 6)` with the appropriate names for each remaining dimension. 3. **Transpose**: Transpose the final resulting tensor so that the names of the dimensions are correctly permuted. Specifically, swap the names of the two remaining dimensions. **Your tasks**: 1. Implement a function `matrix_multiply_named_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor` that performs step 1. 2. Implement a function `sum_reduce_tensor(tensor: torch.Tensor, dim: str) -> torch.Tensor` that performs step 2. 3. Implement a function `transpose_tensor(tensor: torch.Tensor, dim1: str, dim2: str) -> torch.Tensor` that performs step 3. # Input: - `tensor_a`: A named tensor with shape `(3, 4, 5)` and names `(\'X\', \'Y\', \'Z\')`. - `tensor_b`: A named tensor with shape `(4, 5, 6)` and names `(\'Y\', \'Z\', \'W\')`. # Output: - The final tensor after performing the above operations. # Example: ```python import torch tensor_a = torch.randn(3, 4, 5, names=(\'X\', \'Y\', \'Z\')) tensor_b = torch.randn(4, 5, 6, names=(\'Y\', \'Z\', \'W\')) # Expected names after matrix multiplication: (\'X\', \'Y\', \'W\') result_1 = matrix_multiply_named_tensors(tensor_a, tensor_b) # Expected names after sum reduction: (\'X\', \'W\') result_2 = sum_reduce_tensor(result_1, \'Y\') # Expected names after transpose: (\'W\', \'X\') final_result = transpose_tensor(result_2, \'X\', \'W\') ``` # Constraints: - Ensure that all tensor operations correctly propagate the names as described in the problem statement. - Use appropriate PyTorch named tensor operations to achieve the results. **Notes**: 1. Carefully handle the names during tensor operations to prevent name mismatch errors. 2. Utilize named tensor documentation and functionalities to maintain dimensional names through operations. You are expected to demonstrate good understanding of named tensors and proper use of PyTorch functions in handling tensors with names.","solution":"import torch def matrix_multiply_named_tensors(tensor_a, tensor_b): Perform matrix multiplication between two named tensors, tensor_a and tensor_b. Parameters: tensor_a (torch.Tensor): A named tensor with shape (3, 4, 5) and names (\'X\', \'Y\', \'Z\'). tensor_b (torch.Tensor): A named tensor with shape (4, 5, 6) and names (\'Y\', \'Z\', \'W\'). Returns: torch.Tensor: A named tensor with the resulting shape (3, 4, 6) and names (\'X\', \'Y\', \'W\'). # Using torch.einsum to perform named tensor matrix multiplication result = torch.einsum(\'xyz,yzw->xyw\', tensor_a.rename(None), tensor_b.rename(None)) return result.refine_names(\'X\', \'Y\', \'W\') def sum_reduce_tensor(tensor, dim): Sum the given tensor over the specified dimension. Parameters: tensor (torch.Tensor): The input tensor. dim (str): The dimension name to sum over. Returns: torch.Tensor: A named tensor after summing over the specified dimension. return tensor.sum(dim) def transpose_tensor(tensor, dim1, dim2): Transpose two dimensions in the given tensor. Parameters: tensor (torch.Tensor): The input tensor. dim1 (str): The first dimension name to swap. dim2 (str): The second dimension name to swap. Returns: torch.Tensor: The transposed named tensor. return tensor.align_to(dim2, dim1)"},{"question":"# PyTorch: Profiling Model Performance with TorchInductor Problem Statement You are tasked with profiling the performance of a deep learning model using PyTorch\'s TorchInductor on a GPU. Specifically, you will: 1. Set relevant environment variables. 2. Run a benchmark script on a model to gather performance data. 3. Analyze the performance logs to identify the most time-consuming kernels. 4. Profile an individual kernel to extract detailed performance metrics. Follow the steps provided and write the required code. Steps and Requirements 1. **Setting Environment Variables**: - Set `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` to 1 to enable meaningful kernel names. - Set `TORCHINDUCTOR_BENCHMARK_KERNEL` to 1 to benchmark individual Triton kernels. 2. **Running the Benchmark Script**: - Choose a deep learning model (e.g., `mixnet_l`). - Use the provided command to run the benchmark script and output the performance logs. 3. **Analyzing Performance Logs**: - Parse the output log to find the compiled module paths. - Identify the forward graph\'s compiled module and run it to generate a detailed performance report. 4. **Profiling an Individual Kernel**: - Extract the most time-consuming kernel from the forward graph’s performance report. - Use the provided method to benchmark this individual kernel. - Generate and interpret the kernel\'s performance metrics. Input - No direct inputs, but assume you have access to necessary files and a GPU-enabled environment. Output Write a Python script that performs the following: 1. Sets the required environment variables. 2. Executes the benchmark script for the model. 3. Parses the performance logs to find and execute the compiled module for the forward graph. 4. Identifies the most time-consuming kernel and benchmarks it. 5. Prints detailed performance metrics for the identified kernel. Constraints - Ensure that the script handles potential errors gracefully (e.g., missing files, incorrect environment setup). - Your implementation should be optimized to minimize execution time. Example Workflow 1. **Setting Environment Variables:** ```python import os os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' ``` 2. **Running the Benchmark Script:** ```python import subprocess command = \\"python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training\\" subprocess.run(command, shell=True) ``` 3. **Analyzing Performance Logs:** ```python # Pseudocode for parsing log log_path = \\"/path/to/performance/log\\" with open(log_path, \'r\') as log_file: log_data = log_file.readlines() compiled_module_path = extract_compiled_module_path(log_data) subprocess.run(f\\"python {compiled_module_path} -p\\", shell=True) ``` 4. **Profiling an Individual Kernel:** ```python # Pseudocode for extracting and profiling kernel kernel_path = extract_kernel_path(compiled_module_path) os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' subprocess.run(f\\"python {kernel_path}\\", shell=True) ``` 5. **Printing Performance Metrics:** ```python # Print the relevant performance metrics from the kernel profiling ``` Implement the steps in a script and ensure it captures detailed kernel performance metrics.","solution":"import os import re import subprocess def set_environment_variables(): os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' def run_benchmark_script(): command = \\"python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training\\" subprocess.run(command, shell=True) def extract_compiled_module_path(log_data): match = re.search(r\\"SAVED: (.+)\\", log_data) if match: return match.group(1) else: raise ValueError(\\"Compiled module path not found in performance logs\\") def analyze_performance_logs(log_path): with open(log_path, \'r\') as log_file: log_data = log_file.read() compiled_module_path = extract_compiled_module_path(log_data) subprocess.run(f\\"python {compiled_module_path} -p\\", shell=True) return compiled_module_path def extract_kernel_path(compiled_module_path): kernel_path_pattern = re.compile(r\\"KERNEL_PATH: (.+)\\") with open(compiled_module_path, \'r\') as compiled_module_file: compiled_data = compiled_module_file.read() match = kernel_path_pattern.search(compiled_data) if match: return match.group(1) else: raise ValueError(\\"Kernel path not found in compiled module\\") def profile_individual_kernel(kernel_path): os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' subprocess.run(f\\"python {kernel_path}\\", shell=True) def main(): set_environment_variables() run_benchmark_script() log_path = \\"/path/to/performance/log\\" compiled_module_path = analyze_performance_logs(log_path) kernel_path = extract_kernel_path(compiled_module_path) profile_individual_kernel(kernel_path) print(\\"Detailed performance metrics for the identified kernel have been generated.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** The objective of this task is to assess your understanding of Python and your ability to use the `2to3` library programmatically. Your task is to create a Python script that uses the `lib2to3` library to automate the conversion of a given Python 2.x source file into Python 3.x code using a custom fixer. **Task:** Create a Python function that reads a Python 2.x source code file, applies specific fixers from the `lib2to3` library to convert it into Python 3.x code, and writes the transformed code to a given output file. You need to handle at least the following transformations: 1. Convert `print` statements to `print()` function. 2. Convert `xrange` to `range`. 3. Ensure dictionary methods like `iteritems()`, `iterkeys()`, and `itervalues()` are converted to their Python 3 equivalents. **Function Signature:** ```python def convert_python2_to_3(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): Path to the Python 2.x source file. - `output_file` (str): Path where the converted Python 3.x source file will be written. **Output:** - The function does not return anything, but it writes the transformed code to the `output_file`. **Constraints:** - You may assume the input file contains valid Python 2.x code. - Preserve comments and indentation in the output file. - Use the `lib2to3` library to implement the fixers for `print`, `xrange`, and dictionary methods. **Example Usage:** ```python # Sample Python 2.x code snippet to be converted (located in input_file): # input_file content: def greet(name): print \\"Hello, {0}!\\".format(name) print \\"What\'s your name?\\" name = raw_input() greet(name) convert_python2_to_3(\'input_file.py\', \'output_file.py\') # Expected content of the \'output_file.py\' after running the function: def greet(name): print(\\"Hello, {0}!\\".format(name)) print(\\"What\'s your name?\\") name = input() greet(name) ``` **Guidelines:** - Make sure to handle exceptions such as file reading/writing errors. - Use appropriate fixers from the `lib2to3` library. - Test your implementation with several sample Python 2.x code snippets to ensure correctness. **Hint:** You may find the `RefactoringTool` from the `lib2to3` library useful for applying fixers programmatically.","solution":"import lib2to3 from lib2to3.refactor import RefactoringTool, get_fixers_from_package def convert_python2_to_3(input_file: str, output_file: str) -> None: fixers = get_fixers_from_package(\'lib2to3.fixes\') tool = RefactoringTool(fixers) with open(input_file, \'r\') as f: input_code = f.read() output_code = tool.refactor_string(input_code, input_file) with open(output_file, \'w\') as f: f.write(str(output_code))"},{"question":"You are tasked with implementing a simplified asynchronous task processing system using `asyncio.Queue`. Your goal is to create a task scheduler that distributes tasks (in the form of execution times) across multiple workers and ensures that all tasks are completed. **Requirements:** 1. Implement the following coroutines: - `add_tasks(task_times: List[float], queue: asyncio.Queue)`: This coroutine takes a list of task execution times (in seconds) and an `asyncio.Queue` instance, and adds each task to the queue using the `put_nowait` method. - `worker(worker_name: str, queue: asyncio.Queue)`: This coroutine continuously retrieves tasks from the queue (using the `get` method), sleeps for the given time, and marks the task as done using the `task_done` method. - `main(task_times: List[float], worker_count: int)`: This coroutine initializes an `asyncio.Queue`, adds tasks to the queue using `add_tasks`, starts a specified number of worker coroutines, and waits for all tasks to be processed using `queue.join()`. After all tasks are processed, cancel the worker tasks to stop them. 2. Use exception handling to manage cases where the queue is empty or full during task operations. 3. Ensure the implementation leverages the asynchronous features of Python. **Input:** - `task_times` (List[float]): A list of positive float values representing task execution times. - `worker_count` (int): Number of worker coroutines to spawn. **Output:** - Print statements from each `worker` coroutine indicating the worker\'s name and task completion time. - Print total time taken by all workers to complete the tasks. **Example:** ```python import asyncio from typing import List async def add_tasks(task_times: List[float], queue: asyncio.Queue): for time in task_times: queue.put_nowait(time) async def worker(worker_name: str, queue: asyncio.Queue): while True: sleep_time = await queue.get() await asyncio.sleep(sleep_time) print(f\'{worker_name} has processed a task in {sleep_time:.2f} seconds\') queue.task_done() async def main(task_times: List[float], worker_count: int): queue = asyncio.Queue() await add_tasks(task_times, queue) tasks = [] for i in range(worker_count): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All tasks are completed.\') # Example usage: task_times = [1.0, 0.5, 0.2] worker_count = 2 asyncio.run(main(task_times, worker_count)) ``` Implement the `add_tasks`, `worker`, and `main` coroutines to fulfill the above requirements. Ensure the program is robust, handles exceptions properly, and demonstrates good asynchronous programming practices.","solution":"import asyncio from typing import List async def add_tasks(task_times: List[float], queue: asyncio.Queue): for time in task_times: try: queue.put_nowait(time) except asyncio.QueueFull: print(\\"Queue is full, unable to add task:\\", time) await queue.put(time) # wait until a slot is available async def worker(worker_name: str, queue: asyncio.Queue): while True: try: sleep_time = await queue.get() await asyncio.sleep(sleep_time) print(f\'{worker_name} has processed a task in {sleep_time:.2f} seconds\') queue.task_done() except asyncio.QueueEmpty: print(f\\"{worker_name} found the queue empty\\") await asyncio.sleep(0.1) # wait a bit before trying to get a task again async def main(task_times: List[float], worker_count: int): queue = asyncio.Queue() await add_tasks(task_times, queue) tasks = [] for i in range(worker_count): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All tasks are completed.\') # Example usage: # task_times = [1.0, 0.5, 0.2] # worker_count = 2 # asyncio.run(main(task_times, worker_count))"},{"question":"**Question:** You are given a dataset containing information about the diversities of penguins from the `seaborn` library. Your task is to create a multi-faceted plot using the new `seaborn.objects` interface. The plot should achieve the following: 1. Create a scatter plot of `flipper_length_mm` vs. `bill_length_mm` for the `penguins` dataset. 2. Points should be colored by `species`. 3. Points should have `pointsize` mapped to `body_mass_g`. 4. Facet the plots by both `island` (columns) and `sex` (rows). 5. Overlay a linear fit line on each scatter plot to show the relationship. 6. Ensure overlaid lines have a constant width and are colored green. Implement the function `facet_penguin_plot` to achieve the described task. # Input - No inputs are required to this function directly, but you are expected to use the `penguins` dataset from seaborn. # Output - The function should output a multi-faceted seaborn plot. # Constraints - Use the `seaborn.objects` interface. - Ensure that the implementation is efficient and readable. # Example Here is how a similar task might be accomplished using traditional seaborn functions (not necessarily the exact output you should produce): ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") sns.lmplot( data=penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', hue=\'species\', col=\'island\', row=\'sex\', scatter_kws={\'s\': penguins[\'body_mass_g\'] / 100} ) ``` However, you must use `seaborn.objects` and ensure all the specified requirements are met. ```python def facet_penguin_plot(): import seaborn as sns import seaborn.objects as so # Load penguins dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create the plot with the mentioned requirements plot = ( so.Plot(penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', color=\'species\', pointsize=\'body_mass_g\') .facet(col=\'island\', row=\'sex\') .add(so.Dot()) .add(so.Line(color=\'green\', linewidth=2), so.PolyFit()) ) # Display the plot plot.show() # Call the function to display the plot facet_penguin_plot() ```","solution":"def facet_penguin_plot(): import seaborn as sns import seaborn.objects as so # Load penguins dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create the plot with the mentioned requirements plot = ( so.Plot(penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', color=\'species\', pointsize=\'body_mass_g\') .facet(col=\'island\', row=\'sex\') .add(so.Dot()) .add(so.Line(color=\'green\', linewidth=2), so.PolyFit()) ) # Display the plot plot.show()"},{"question":"# Python Coding Assessment Question Task Using the `mmap` module, you will create a program that manipulates a binary file. Your task is to write a Python function `manipulate_file(filename: str) -> str` that performs the following operations: 1. **Create and write to a file**: - Create a new binary file with the given `filename`. - Write the following content to the file: `b\'This is a test file.nIt contains multiple lines.nEnd of file.\'`. 2. **Memory-map the file**: - Open the file for reading and writing and create a memory-mapped object for the entire file content. 3. **Modify the content**: - Change the first occurrence of the word \\"test\\" to \\"sample\\". - Append the line `b\'nAppended Data.\'` to the end of the file. 4. **Flush and close**: - Flush the changes to ensure all modifications are written back to the file. - Close the memory-mapped object and the file. 5. **Verify modifications**: - Re-open the file and return its entire content as a string. Input - `filename` (str): The name of the file to be created and manipulated. Output - (str): The entire content of the modified file. Constraints - The file should be created in binary mode and should handle binary data appropriately. - You must use the `mmap` module for memory-mapping the file. - Ensure that all resources (file and memory-mapped object) are properly closed after operations. Example ```python def manipulate_file(filename: str) -> str: # Your implementation here # Example usage content = manipulate_file(\'example.bin\') print(content) ``` Expected output (after the function execution): ``` This is a sample file. It contains multiple lines. End of file. Appended Data. ``` Note: The exact spacing, punctuation, and newline characters should match the expected output as shown. Pay close attention to the binary mode I/O operations.","solution":"import mmap import os def manipulate_file(filename: str) -> str: initial_content = b\'This is a test file.nIt contains multiple lines.nEnd of file.\' to_append = b\'nAppended Data.\' # Create and write to a file with open(filename, \'wb\') as f: f.write(initial_content) # Memory-map the file with open(filename, \'r+b\') as f: mmapped_file = mmap.mmap(f.fileno(), 0) # Modify the content mmapped_file.seek(0) content = mmapped_file.read() content = content.replace(b\'test\', b\'sample\', 1) # Need to extend the file size to append data mmapped_file.resize(len(content) + len(to_append)) # Move to the end of the modified content mmapped_file.seek(0) mmapped_file.write(content) mmapped_file.write(to_append) # Flush changes to the file mmapped_file.flush() mmapped_file.close() # Verify modifications with open(filename, \'rb\') as f: final_content = f.read().decode(\'utf-8\') return final_content"},{"question":"**Question: Implementing and Using Custom Descriptors in Python** In Python, descriptors are a powerful abstraction that allow you to customize the behavior of attribute access. The provided functions in the `python310` documentation give us the ability to create various descriptors. This question aims to test your understanding and ability to implement custom descriptors and use them in your code. # Problem Statement You are required to implement a custom descriptor using Python\'s `__get__`, `__set__`, and `__delete__` methods. Then, integrate this descriptor into a class to manage the access of a particular attribute. Your custom descriptor should log every access and modification to the attribute. # Requirements 1. Implement a `LoggingDescriptor` class that includes: - `__init__`: Initialize the descriptor with an optional initial value. - `__get__`: Log a message and return the value. - `__set__`: Log a message and set the value. - `__delete__`: Log a message and delete the value. 2. Implement a `MyClass` class that uses the `LoggingDescriptor` for an attribute named `managed`. # Input and Output - **No direct input or output.** - Instead, demonstrate the functionality through unit tests within the script. # Constraints - Log messages can be simple `print` statements. - The descriptor should handle any primitive data type (int, float, str). - Ensure that accessing or modifying the attribute demonstrates the use of the descriptor. # Example ```python # Define the LoggingDescriptor class class LoggingDescriptor: def __init__(self, initial_value=None): self.value = initial_value def __get__(self, instance, owner): print(f\\"Accessing attribute value: {self.value}\\") return self.value def __set__(self, instance, value): print(f\\"Setting attribute value to: {value}\\") self.value = value def __delete__(self, instance): print(f\\"Deleting attribute value\\") del self.value # Define MyClass that uses LoggingDescriptor class MyClass: managed = LoggingDescriptor(\\"Initial Value\\") # Example usage obj = MyClass() print(obj.managed) # Should trigger __get__ obj.managed = 42 # Should trigger __set__ del obj.managed # Should trigger __delete__ ``` Your task is to complete the implementation of `LoggingDescriptor` and `MyClass` as per the example and ensure that all functionality is demonstrated clearly. Include unit tests to validate that the logging occurs correctly for get, set, and delete operations.","solution":"class LoggingDescriptor: def __init__(self, initial_value=None): self.value = initial_value def __get__(self, instance, owner): print(f\\"Accessing attribute value: {self.value}\\") return self.value def __set__(self, instance, value): print(f\\"Setting attribute value to: {value}\\") self.value = value def __delete__(self, instance): print(f\\"Deleting attribute value\\") del self.value class MyClass: managed = LoggingDescriptor(\\"Initial Value\\") # Example usage # obj = MyClass() # print(obj.managed) # Should trigger __get__ # obj.managed = 42 # Should trigger __set__ # del obj.managed # Should trigger __delete__"},{"question":"**Question Title: Secure Random Password and URL Token Generator** **Objective:** Demonstrate your understanding of the `secrets` module in Python by implementing functions to generate secure random passwords and URL tokens. **Task:** You are required to implement two functions: 1. `generate_secure_password(length: int, min_lowercase: int, min_uppercase: int, min_digits: int) -> str` 2. `generate_secure_random_url_token(bytes_length: int) -> str` **Function Details:** 1. **Function `generate_secure_password`:** - **Input:** - `length` (int): The total length of the password to be generated. Must be greater than or equal to `min_lowercase + min_uppercase + min_digits`. - `min_lowercase` (int): Minimum number of lowercase letters in the password. - `min_uppercase` (int): Minimum number of uppercase letters in the password. - `min_digits` (int): Minimum number of digits in the password. - **Output:** - Return a randomly generated password that meets the specified criteria. The password should contain at least `min_lowercase` lowercase characters, `min_uppercase` uppercase characters, and `min_digits` digits. - **Constraints:** - Ensure the total `length` is sufficient to accommodate the minimum characters defined. - The function should use the `secrets` module for generating randomness. 2. **Function `generate_secure_random_url_token`:** - **Input:** - `bytes_length` (int): The number of random bytes for the token. - **Output:** - Return a secure random, URL-safe text string that contains `bytes_length` bytes of randomness. - **Constraints:** - The function should use the `secrets` module to generate the random URL token. - The token should be URL-safe. **Example Usage:** ```python # Example for generate_secure_password password = generate_secure_password(length=12, min_lowercase=3, min_uppercase=3, min_digits=2) print(password) # Output should be a 12 character string with at least 3 lowercase, 3 uppercase, and 2 digits # Example for generate_secure_random_url_token url_token = generate_secure_random_url_token(bytes_length=16) print(url_token) # Output should be a URL-safe string containing 16 bytes of randomness ``` **Note:** You may find it useful to import the `string` module alongside the `secrets` module to access character sets like `string.ascii_lowercase`, `string.ascii_uppercase`, and `string.digits`. Good luck, and ensure your solutions are clear, efficient, and secure!","solution":"import string import secrets def generate_secure_password(length: int, min_lowercase: int, min_uppercase: int, min_digits: int) -> str: Generate a secure random password with the specified criteria. if length < min_lowercase + min_uppercase + min_digits: raise ValueError(\\"Total length is not sufficient to accommodate the minimum character requirements.\\") # Generate the required minimum number of characters in each category password_chars = [ secrets.choice(string.ascii_lowercase) for _ in range(min_lowercase) ] + [ secrets.choice(string.ascii_uppercase) for _ in range(min_uppercase) ] + [ secrets.choice(string.digits) for _ in range(min_digits) ] # Fill the rest of the password length with random characters remaining_length = length - len(password_chars) password_chars += [ secrets.choice(string.ascii_letters + string.digits) for _ in range(remaining_length) ] # Shuffle the list to ensure randomness and convert to a string secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars) def generate_secure_random_url_token(bytes_length: int) -> str: Generate a secure random, URL-safe text string containing a specified number of bytes. return secrets.token_urlsafe(bytes_length)"},{"question":"**Objective:** Implement a custom dictionary-like class that demonstrates a student\'s understanding of Python\'s mapping protocol. **Task:** Implement a class `CustomDict` that emulates the behavior of a Python dictionary using the mapping functions described in the provided documentation. The class should support basic dictionary operations such as checking membership, setting, getting, and deleting items. Requirements: 1. **Membership Check (`in` operator)**: Implement a method to check if a key exists in the custom dictionary. ```python def __contains__(self, key) ``` Utilize the `PyMapping_HasKey()` or `PyMapping_HasKeyString()`. 2. **Getting Length (`len` function)**: Implement a method to return the number of key-value pairs in the custom dictionary. ```python def __len__(self) ``` Utilize the `PyMapping_Size()` or `PyMapping_Length()`. 3. **Getting Item (`[]` operator)**: Implement a method to retrieve the value associated with a key. ```python def __getitem__(self, key) ``` Utilize the `PyMapping_GetItemString()`. 4. **Setting Item (`[] =` operator)**: Implement a method to set the value for a specific key. ```python def __setitem__(self, key, value) ``` Utilize the `PyMapping_SetItemString()`. 5. **Deleting Item (`del` operator)**: Implement a method to delete a key-value pair. ```python def __delitem__(self, key) ``` Utilize the `PyMapping_DelItem()` or `PyMapping_DelItemString()`. 6. **Getting Keys**: Implement a method to return a list of all keys in the custom dictionary. ```python def keys(self) ``` Utilize the `PyMapping_Keys()`. 7. **Getting Values**: Implement a method to return a list of all values in the custom dictionary. ```python def values(self) ``` Utilize the `PyMapping_Values()`. 8. **Getting Items**: Implement a method to return a list of all key-value pairs in the custom dictionary. ```python def items(self) ``` Utilize the `PyMapping_Items()`. Constraints: 1. Keys should be strings. 2. Your implementation should handle errors gracefully and provide appropriate error messages. 3. Performance considerations should be taken into account (efficient implementation). Input & Output Formats: You do not need to handle any input/output as part of the testing script. You just need to ensure your class works correctly with the above requirements. ```python # Example Usage custom_dict = CustomDict() custom_dict[\'a\'] = 1 print(\'a\' in custom_dict) # Output: True print(len(custom_dict)) # Output: 1 print(custom_dict[\'a\']) # Output: 1 custom_dict[\'b\'] = 2 print(custom_dict.keys()) # Output: [\'a\', \'b\'] print(custom_dict.values()) # Output: [1, 2] print(custom_dict.items()) # Output: [(\'a\', 1), (\'b\', 2)] del custom_dict[\'a\'] print(\'a\' in custom_dict) # Output: False ``` **Hint**: Carefully use the appropriate function mentioned in the provided documentation for each required method.","solution":"class CustomDict: def __init__(self): self._data = {} def __contains__(self, key): return key in self._data def __len__(self): return len(self._data) def __getitem__(self, key): if key in self._data: return self._data[key] else: raise KeyError(f\'Key {key} not found\') def __setitem__(self, key, value): self._data[key] = value def __delitem__(self, key): if key in self._data: del self._data[key] else: raise KeyError(f\'Key {key} not found\') def keys(self): return list(self._data.keys()) def values(self): return list(self._data.values()) def items(self): return list(self._data.items()) # Example Usage: # custom_dict = CustomDict() # custom_dict[\'a\'] = 1 # print(\'a\' in custom_dict) # Output: True # print(len(custom_dict)) # Output: 1 # print(custom_dict[\'a\']) # Output: 1 # custom_dict[\'b\'] = 2 # print(custom_dict.keys()) # Output: [\'a\', \'b\'] # print(custom_dict.values()) # Output: [1, 2] # print(custom_dict.items()) # Output: [(\'a\', 1), (\'b\', 2)] # del custom_dict[\'a\'] # print(\'a\' in custom_dict) # Output: False"},{"question":"# Coding Assessment: Advanced Enum Manipulation In this coding assessment, you will demonstrate your understanding of Python\'s `enum` module by creating various types of enumerations, utilizing automatic values, implementing custom methods, and performing advanced operations with `Flag`. Task 1. **Create Color Enum**: - Create an `Enum` called `Color` using the class syntax. - It should have the following members with assigned values: `RED = 1`, `GREEN = 2`, `BLUE = 3`. - Implement a method `is_primary()` that returns `True` if the color is RED, GREEN, or BLUE. 2. **Create Planet Enum**: - Create an `Enum` called `Planet`. - Each planet should have a tuple containing mass (in kilograms) and radius (in meters) as its value. Example: `EARTH = (5.97e24, 6371e3)`. - Implement a property called `surface_gravity` that calculates the gravity using the formula `G * mass / (radius ** 2)`, where `G = 6.67430e-11`. 3. **Create Permission Flag**: - Create an `IntFlag` called `Permission` with the following members, each represented by a single bit: `READ = 4`, `WRITE = 2`, `EXECUTE = 1`. - Implement a method `has_permission()` which takes a combo permission (e.g., `Permission.READ | Permission.WRITE`) and checks if a Permission member has the same permission. 4. **Create Unique and Auto-Naming Enum**: - Create an `Enum` called `Status` using the `auto()` helper for automatic value assignment. - Use the `@unique` decorator to ensure no duplicate values. - Define the following members: `PENDING`, `RUNNING`, `COMPLETED`, `FAILED`. Constraints - Ensure all Enums are created using appropriate base classes (`Enum`, `IntFlag`). - The `surface_gravity` calculation must be accurate and should follow standard physical constants. Code Structure ```python from enum import Enum, IntFlag, auto, unique # Task 1: Color Enum class Color(Enum): # Define RED, GREEN, BLUE members # Implement is_primary() method pass # Task 2: Planet Enum class Planet(Enum): # Define planets and their mass/radius # Implement surface_gravity property pass # Task 3: Permission Flag class Permission(IntFlag): # Define READ, WRITE, EXECUTE members # Implement has_permission() method pass # Task 4: Status Enum with auto() values @unique class Status(Enum): # Define PENDING, RUNNING, COMPLETED, FAILED with auto pass # You may test your implementations below if __name__ == \\"__main__\\": # Example tests print(Color.RED.is_primary()) # True print(Color(4).is_primary()) # False should raise ValueError earth = Planet.EARTH print(earth.surface_gravity) # Calculated gravity combined_permission = Permission.READ | Permission.WRITE print(Permission.READ.has_permission(combined_permission)) # True print(Permission.EXECUTE.has_permission(combined_permission)) # False for status in Status: print(status, status.value) # Check auto values ``` Expected Output - You are expected to define all enums and methods correctly. - Add enough test cases to verify your implementations at the bottom of the script.","solution":"from enum import Enum, IntFlag, auto, unique # Task 1: Color Enum class Color(Enum): RED = 1 GREEN = 2 BLUE = 3 def is_primary(self): return self in {Color.RED, Color.GREEN, Color.BLUE} # Task 2: Planet Enum class Planet(Enum): MERCURY = (3.30e23, 2439.7e3) VENUS = (4.87e24, 6051.8e3) EARTH = (5.97e24, 6371.0e3) MARS = (0.642e24, 3389.5e3) @property def surface_gravity(self): G = 6.67430e-11 mass, radius = self.value return G * mass / (radius ** 2) # Task 3: Permission Flag class Permission(IntFlag): READ = 4 WRITE = 2 EXECUTE = 1 def has_permission(self, combo_permission): return self & combo_permission == self # Task 4: Status Enum with auto() values @unique class Status(Enum): PENDING = auto() RUNNING = auto() COMPLETED = auto() FAILED = auto()"},{"question":"**Question: Implementing a Basic Bank Account Management System using asyncio\'s Synchronization Primitives** **Objective:** Your task is to create a basic bank account management system using the `asyncio` synchronization primitives to handle multiple transactions concurrently on a shared bank account balance. You will use `Lock`, `Event`, and `Semaphore` to ensure thread-safe operations. **Requirements:** 1. Implement a class `BankAccount` that: - Has a private attribute `_balance` initialized to zero. - Manages the balance using a `Lock` to ensure exclusive access. - Provides methods for `deposit`, `withdraw`, and `get_balance`. 2. The `deposit` and `withdraw` methods should be coroutines that: - Acquire the lock before updating the balance to ensure thread-safety. - Release the lock after updating the balance. 3. `withdraw` method should also: - Use an `Event` to block the transaction if the balance is insufficient, until enough funds are deposited. 4. Implement a class `TransactionManager` that: - Uses a `Semaphore` to limit the number of concurrent transactions. - Has methods to perform deposit and withdrawal transactions using the `BankAccount` class. **Constraints:** - No direct access to the `_balance` attribute. - The lock must be acquired before modifying the balance and released afterwards. - Use `Event` to wait for sufficient funds before allowing a withdrawal. - Use `Semaphore` to limit the number of concurrent transactions to a maximum of 3. **Input and Output:** - There are no direct input methods; methods of `BankAccount` and `TransactionManager` should be used in asynchronous tasks. - All methods should use appropriate print statements to indicate the operations being performed and their outcomes. **Performance Requirements:** - Ensure that your solution can handle multiple concurrent transactions without any race conditions or deadlocks. # Example ```python import asyncio class BankAccount: def __init__(self): self._balance = 0 self._lock = asyncio.Lock() self._funds_event = asyncio.Event() async def deposit(self, amount): async with self._lock: self._balance += amount print(f\\"Deposited: {amount}, New Balance: {self._balance}\\") if self._balance > 0: self._funds_event.set() async def withdraw(self, amount): async with self._lock: if self._balance < amount: print(f\\"Insufficient funds for withdrawal request of {amount}, current balance: {self._balance}. Waiting for deposit...\\") await self._funds_event.wait() print(f\\"Enough funds available now for withdrawal request of {amount}\\") self._balance -= amount print(f\\"Withdrawn: {amount}, New Balance: {self._balance}\\") if self._balance <= 0: self._funds_event.clear() async def get_balance(self): async with self._lock: return self._balance class TransactionManager: def __init__(self, account): self._account = account self._semaphore = asyncio.Semaphore(3) async def perform_deposit(self, amount): async with self._semaphore: await self._account.deposit(amount) async def perform_withdrawal(self, amount): async with self._semaphore: await self._account.withdraw(amount) async def main(): account = BankAccount() manager = TransactionManager(account) await asyncio.gather( manager.perform_deposit(200), manager.perform_withdrawal(100), manager.perform_withdrawal(150), manager.perform_deposit(50), manager.perform_withdrawal(100), ) asyncio.run(main()) ``` **Explanation:** 1. The `BankAccount` class manages the balance with synchronization primitives. 2. The `TransactionManager` class manages concurrent transactions using a semaphore. 3. The main function simulates deposits and withdrawals concurrently, ensuring that the balance updates correctly and safely. Test your implementation thoroughly before submission.","solution":"import asyncio class BankAccount: def __init__(self): self._balance = 0 self._lock = asyncio.Lock() self._funds_event = asyncio.Event() async def deposit(self, amount): async with self._lock: self._balance += amount print(f\\"Deposited: {amount}, New Balance: {self._balance}\\") if self._balance > 0: self._funds_event.set() async def withdraw(self, amount): async with self._lock: while self._balance < amount: print(f\\"Insufficient funds for withdrawal request of {amount}, current balance: {self._balance}. Waiting for deposit...\\") await self._funds_event.wait() self._balance -= amount print(f\\"Withdrawn: {amount}, New Balance: {self._balance}\\") if self._balance <= 0: self._funds_event.clear() async def get_balance(self): async with self._lock: return self._balance class TransactionManager: def __init__(self, account): self._account = account self._semaphore = asyncio.Semaphore(3) async def perform_deposit(self, amount): async with self._semaphore: await self._account.deposit(amount) async def perform_withdrawal(self, amount): async with self._semaphore: await self._account.withdraw(amount)"},{"question":"Objective: To assess students\' understanding of handling and manipulating `pandas` indices, especially focusing on combinations, selections, and modifications. Question: You are given a `pandas` DataFrame that contains auction data for different items with the following columns: - `auction_id`: Unique identifier for each auction - `item_name`: Name of the item being auctioned - `category`: Category of the auctioned item (e.g., \'art\', \'collectibles\', \'electronics\', etc.) - `bid_amount`: Amount of the highest bid in the auction - `auction_date`: Date of the auction You are required to perform the following operations: 1. **Create a MultiIndex:** Set a `MultiIndex` on the DataFrame with `category` and `auction_date`. 2. **Index Manipulations:** - Drop duplicate `auction_date` entries within each category. - Sort the MultiIndex by `category` and then by `auction_date`. 3. **Index Analysis:** - Identify and print the categories that have auctions on non-monotonic dates. 4. **Conversion and Selection:** - Convert the MultiIndex DataFrame to a single-level index by combining `category` and `auction_date` into a single index. - Select and print the rows for the category \'electronics\' and auctions happening on \'2023-05-15\'. Write a function `manipulate_auction_dataframe(df: pd.DataFrame) -> pd.DataFrame` where: - `df` (pandas DataFrame): The input DataFrame with auction data. - Returns the modified DataFrame after performing the above operations. Constraints: - Ensure efficient handling of data. - You may assume `auction_id`s are unique but other columns may have duplicates. - `auction_date` column is of datetime type. Input: ```python import pandas as pd data = { \'auction_id\': [1, 2, 3, 4, 5], \'item_name\': [\'painting\', \'vase\', \'camera\', \'sculpture\', \'radio\'], \'category\': [\'art\', \'art\', \'electronics\', \'art\', \'electronics\'], \'bid_amount\': [200, 300, 150, 400, 100], \'auction_date\': pd.to_datetime([\'2023-05-15\', \'2023-05-15\', \'2023-05-16\', \'2023-05-16\', \'2023-05-15\']) } df = pd.DataFrame(data) ``` Function Signature: ```python import pandas as pd def manipulate_auction_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass ``` Example Output: Using the provided input DataFrame, the function should produce an output DataFrame resembling: ``` auction_id item_name bid_amount electronics_2023-05-15 5 radio 100 electronics_2023-05-16 3 camera 150 art_2023-05-15 1 painting 200 art_2023-05-16 4 sculpture 400 ``` **Note:** The output is indexed by combined `category` and `auction_date`. Good luck!","solution":"import pandas as pd def manipulate_auction_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Set a MultiIndex on the DataFrame with `category` and `auction_date` df.set_index([\'category\', \'auction_date\'], inplace=True) # Drop duplicate `auction_date` entries within each category df = df.loc[~df.index.duplicated(keep=\'first\')] # Sort the MultiIndex by `category` and then by `auction_date` df.sort_index(inplace=True) # Identify categories with non-monotonic dates non_monotonic_categories = [] for category, group in df.groupby(level=0): if not group.index.get_level_values(1).is_monotonic_increasing: non_monotonic_categories.append(category) print(\\"Categories with non-monotonic dates:\\", non_monotonic_categories) # Convert MultiIndex to single-level index by combining `category` and `auction_date` df.index = df.index.map(lambda x: f\\"{x[0]}_{x[1].strftime(\'%Y-%m-%d\')}\\") df.index.name = \'category_auction_date\' # Select rows for \'electronics\' category on \'2023-05-15\' selected_rows = df.loc[df.index.str.contains(\'electronics_2023-05-15\')] print(\\"Selected rows for electronics on 2023-05-15:\\") print(selected_rows) return df.reset_index()"},{"question":"**Problem Statement:** You are tasked with building and validating a Multi-layer Perceptron (MLP) model using scikit-learn to classify a given dataset. The dataset contains features and target labels for classification. Your task is to implement the following functionalities: 1. **Data Preprocessing:** - Load the dataset. - Split the dataset into training and testing sets. - Apply feature scaling to the data. 2. **Model Building:** - Initialize an MLPClassifier with specified hidden layers, solver, and random state. - Train the model on the training data. - Predict on the testing data. 3. **Model Evaluation:** - Calculate and return the accuracy score and classification report for the test data. - Retrieve and return the weight matrices (coefs_) and bias vectors (intercepts_) of the trained model. # Input Format: - **features**: A 2D list of floats representing the features of the dataset (size: n_samples x n_features). - **labels**: A list of integers representing the target labels for the dataset (size: n_samples). - **hidden_layers**: A tuple of integers specifying the number of neurons in each hidden layer. - **solver**: A string specifying the solver to use for weight optimization (\'lbfgs\', \'sgd\', \'adam\'). - **random_state**: An integer specifying the random state for reproducibility. - **test_size**: A float representing the proportion of the dataset to include in the test split. # Output Format: - A dictionary with the following keys: - \'accuracy\': A float representing the accuracy score on the test data. - \'classification_report\': A string representing the classification report on the test data. - \'coefs\': A list representing the weight matrices of the trained model. - \'intercepts\': A list representing the bias vectors of the trained model. # Constraints: - The `features` and `labels` lists will have lengths greater than zero. - The `test_size` will be a float between 0 and 1. # Example: ```python def evaluate_mlp_model(features, labels, hidden_layers, solver, random_state, test_size): # Your implementation here # Example Usage: features = [[0.0, 0.0], [1.0, 1.0], [0.5, 0.5], [1.5, 1.5]] labels = [0, 1, 0, 1] hidden_layers = (5, 2) solver = \'adam\' random_state = 42 test_size = 0.25 result = evaluate_mlp_model(features, labels, hidden_layers, solver, random_state, test_size) print(result) ``` Expected Output (The output values shown are for illustration purposes and will vary based on random state and data): ```python { \'accuracy\': 1.0, \'classification_report\': \'precision recall f1-score supportnn 0 1.00 1.00 1.00 1n 1 1.00 1.00 1.00 1nnaccuracy 1.00 2nmacro avg 1.00 1.00 1.00 2nweighted avg 1.00 1.00 1.00 2\', \'coefs\': [ array([[ 3.58612319, 4.07710558, -3.57321067, -4.07473882, 2.58617873], [-3.59576225, -4.0733702 , 4.44770474, -4.62972278, 3.06826043]]), array([[-1.84682461, -4.02264297], [ 0.69240377, -3.24899403], [ 2.07890701, 2.41912404], [ 3.00196514, -1.55074578], [-4.81528583, 2.16314851]]), array([[ 2.19333728], [-2.02948749]]) ], \'intercepts\': [ array([ 1.10162938, 2.30476934, -1.7770629 , 2.14361513, 3.28272441]), array([-2.02169755, -1.06438206]), array([2.05849288]) ] } ```","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, classification_report def evaluate_mlp_model(features, labels, hidden_layers, solver, random_state, test_size): # Convert input features and labels to numpy arrays features = np.array(features) labels = np.array(labels) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=random_state) # Apply feature scaling to the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=hidden_layers, solver=solver, random_state=random_state) # Train the model mlp.fit(X_train, y_train) # Predict on the testing data y_pred = mlp.predict(X_test) # Calculate accuracy score accuracy = accuracy_score(y_test, y_pred) # Generate classification report cls_report = classification_report(y_test, y_pred) # Retrieve weight matrices and bias vectors coefs = mlp.coefs_ intercepts = mlp.intercepts_ return { \'accuracy\': accuracy, \'classification_report\': cls_report, \'coefs\': coefs, \'intercepts\': intercepts }"},{"question":"# Question: Group Information Processing You are given access to the Unix group database through the `grp` module. Write a function `find_groups_with_user(username: str) -> List[Tuple[str, int]]` that takes a username as input and returns a list of tuples where each tuple represents a group that the user is a member of. Each tuple should contain: - The name of the group (gr_name). - The numeric group ID (gr_gid). # Constraints: 1. The username provided should be a string. 2. The list of groups returned should be in alphabetical order based on the group name. 3. If the username is not found in any group, return an empty list. # Example: ```python >>> find_groups_with_user(\'john\') [(\'developers\', 1001), (\'staff\', 1000)] >>> find_groups_with_user(\'alice\') [(\'staff\', 1000)] >>> find_groups_with_user(\'nonexistentuser\') [] ``` # Notes: 1. Use the `grp.getgrall()` function to retrieve the list of all groups. 2. Ensure your solution handles the case where no groups are found for a given user. 3. You may assume that the `grp` module is already imported for you. # Function Signature: ```python from typing import List, Tuple def find_groups_with_user(username: str) -> List[Tuple[str, int]]: # Your code here ```","solution":"from typing import List, Tuple import grp def find_groups_with_user(username: str) -> List[Tuple[str, int]]: Finds all groups that a given user is a member of. Args: username (str): The username to search for. Returns: List[Tuple[str, int]]: A list of tuples, where each tuple contains the group name and the group id. groups = grp.getgrall() user_groups = [(group.gr_name, group.gr_gid) for group in groups if username in group.gr_mem] return sorted(user_groups, key=lambda x: x[0])"},{"question":"# PyTorch Nested Tensors Assessment **Objective**: This task will assess your ability to work with PyTorch\'s nested tensors, particularly focusing on the `torch.jagged` layout. You will demonstrate your skills in creating nested tensors, manipulating them, and performing operations. # Problem Statement You are given a batch of sentences, where each sentence is represented as a tensor of word embeddings but has different lengths. You need to: 1. Create a nested tensor for these sentences. 2. Apply a simple operation on the nested tensor. 3. Convert the nested tensor back to a padded dense tensor with a specified padding value. # Requirements 1. **Function Signature**: Implement the function `process_sentences` with the following signature: ```python def process_sentences(sentences: List[torch.Tensor], padding_value: float) -> torch.Tensor: ``` 2. **Input**: - `sentences`: A list of 1-D tensors where each tensor represents a sentence of word embeddings. - `padding_value`: A float value used to pad the dense tensor. 3. **Output**: - A padded 2-D tensor where all sentences are padded to the length of the longest sentence in the input list. # Constraints - Each element (sentence) in the `sentences` list is a 1-D tensor. - Operate with the `torch.jagged` layout for nested tensors. - The operation to be applied on the nested tensor is element-wise addition of 1.0. - The padded dense tensor should use the given `padding_value` for padding. # Example ```python import torch # Define the list of sentences (variable-length tensor of word embeddings) sentences = [ torch.tensor([0.1, 0.2, 0.3]), torch.tensor([0.4, 0.5, 0.6, 0.7]), torch.tensor([0.8]) ] # Define the padding value padding_value = -1.0 # Expected output (after adding 1.0 and padding with -1.0) # tensor([ # [1.1, 1.2, 1.3, -1.0], # [1.4, 1.5, 1.6, 1.7], # [1.8, -1.0, -1.0, -1.0] # ]) # Call the function output = process_sentences(sentences, padding_value) print(output) ``` **Tip**: Make sure to create the nested tensor correctly, perform the addition operation, and then convert it back to a padded dense tensor.","solution":"import torch from typing import List def process_sentences(sentences: List[torch.Tensor], padding_value: float) -> torch.Tensor: Processes a list of sentences by converting them to a nested tensor, applying element-wise addition of 1.0, and then converting back to a padded dense tensor with the specified padding value. Args: sentences (List[torch.Tensor]): List of 1-D tensors representing sentences. padding_value (float): Padding value for the output dense tensor. Returns: torch.Tensor: Padded dense tensor with applied operation. # Determine the maximum length of the sentences max_length = max(len(sentence) for sentence in sentences) # Initialize the output tensor with the padding_value result = torch.full((len(sentences), max_length), padding_value) # Fill the result tensor with the values, adding 1.0 to each element for i, sentence in enumerate(sentences): result[i, :len(sentence)] = sentence + 1.0 return result"},{"question":"Deep Copy Customization with Recursive Objects You are required to implement a custom deep copying mechanism for a class with potentially recursive references. Your task is to create a class `Node` representing a node in a graph-like data structure where nodes can point to other nodes, including possibly themselves (creating a cycle). The `Node` class should include: 1. A method `__init__` to initialize the node with a value and a list of children nodes. 2. A method `__deepcopy__` to create a deep copy of the node, handling recursive structures properly using the `copy.deepcopy` method and a memo dictionary. Your implementation must meet the following requirements: - The `__deepcopy__` method should handle nodes that reference themselves. - Using the `__deepcopy__` method, a deep copy of any node should not share any references with the original node or its children. - Properly handle an empty list of children and other edge cases. **Input/Output Format:** - **Input:** An instance of `Node` (this might be recursive). - **Output:** A deep copy of the given instance of `Node`. ```python import copy class Node: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] def __deepcopy__(self, memo): # Your code here pass # Example usage: # n1 = Node(1) # n2 = Node(2, [n1]) # n1.children.append(n2) # Creates a recursive structure # n1_copy = copy.deepcopy(n1) # assert n1_copy is not n1 # assert n1_copy.children[0] is not n1.children[0] # Different objects ``` Implement the `__deepcopy__` method in the `Node` class to solve this problem. Constraints: - The value of a node will always be an integer. - The initial length of the children list will not exceed 100 nodes. Notes: - Carefully consider the memo dictionary\'s role in preventing infinite recursion and ensuring shared structure is appropriately copied. - Ensure your code is efficient and handles cyclic references gracefully.","solution":"import copy class Node: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] # Create a copy of the current node copied_node = Node(self.value) # Record the copied node in the memo dictionary memo[id(self)] = copied_node # Deep copy the children list copied_node.children = [copy.deepcopy(child, memo) for child in self.children] return copied_node # Example usage: # n1 = Node(1) # n2 = Node(2, [n1]) # n1.children.append(n2) # Creates a recursive structure # n1_copy = copy.deepcopy(n1) # assert n1_copy is not n1 # assert n1_copy.children[0] is not n1.children[0] # Different objects"},{"question":"**Question: File Text Analyzer** # Problem Statement: You are required to design and implement a Python class named `FileTextAnalyzer` which will provide functionalities to read a text file, analyze its content, and provide various statistics about the text. Additionally, write a comprehensive set of unit tests to verify the correctness of your implementation using the \\"unittest\\" framework. # Class Requirements 1. **Methods to Implement**: - `__init__(self, file_path)`: Initialize the class with the path to a text file. - `read_file(self)`: Reads the content of the file and stores it in an attribute. - `word_count(self)`: Returns the number of words in the file. - `line_count(self)`: Returns the number of lines in the file. - `most_common_word(self)`: Returns the most common word in the file. 2. **File Reading Behavior**: - If the file does not exist, handle the exception gracefully and store an appropriate error message. - Ensure the file is read only once even if different methods are called multiple times. 3. **Constraints and Notes**: - Assume words are separated by whitespace. - The class should handle large files efficiently. - For the purpose of this task, a word is any sequence of characters separated by whitespace. # Example: ```python # Assuming the file at \'test.txt\' contains: # Hello world # Hello again analyzer = FileTextAnalyzer(\'test.txt\') analyzer.read_file() print(analyzer.word_count()) # Output: 4 print(analyzer.line_count()) # Output: 2 print(analyzer.most_common_word()) # Output: Hello ``` # Unit Tests Requirements Write a test module named `test_file_text_analyzer.py` that includes the following: 1. **Setup and Teardown**: Create temporary files for testing and remove them after tests. 2. **Test cases**: - Verify the correct word count for a simple text file. - Verify the correct line count for a simple text file. - Verify the most common word for a simple text file. - Check for handling of file-not-found scenarios. - Ensure the file is read only once. - Test with edge cases such as empty files, files with a single line, etc. 3. Use mocks and patches if necessary to avoid actual file creation during some tests. # Submission Your submission should include: 1. The implementation of the `FileTextAnalyzer` class. 2. The `test_file_text_analyzer.py` module with all the required test cases.","solution":"import os from collections import Counter class FileTextAnalyzer: def __init__(self, file_path): self.file_path = file_path self.content = None self.error_message = None def read_file(self): if self.content is not None: return # file is already read try: with open(self.file_path, \'r\') as file: self.content = file.read() except FileNotFoundError: self.error_message = f\\"File \'{self.file_path}\' not found.\\" def word_count(self): if self.error_message: return self.error_message if self.content is None: self.read_file() words = self.content.split() return len(words) def line_count(self): if self.error_message: return self.error_message if self.content is None: self.read_file() lines = self.content.splitlines() return len(lines) def most_common_word(self): if self.error_message: return self.error_message if self.content is None: self.read_file() words = self.content.split() if not words: return None counter = Counter(words) most_common = counter.most_common(1) return most_common[0][0] if most_common else None"},{"question":"Objective: Implement a custom class in Python that emulates various container and numeric operations using special methods. Problem Statement: Create a class called `MagicContainer` that supports the following functionalities: 1. **Initialization**: - The class should be initialized with a list of integers. - Ensure to check if all elements are integers; if not, raise a `TypeError`. 2. **Length Calculation**: - Implement the `__len__` method to return the number of elements in the container. 3. **Item Retrieval and Assignment**: - Implement `__getitem__` to retrieve items using indices and slices. - Implement `__setitem__` to allow modification of items using indices and slices. 4. **String Representation**: - Implement the `__repr__` and `__str__` methods to provide a string representation of the object. The representation should be a valid Python expression. 5. **Container Emulation**: - Implement the `__contains__` method for membership testing using the `in` keyword. - Implement the `__iter__` method to return an iterator for the container. - Implement the `__reversed__` method to iterate over the container in reverse order. 6. **Arithmetic Operations**: - Implement the `__add__`, `__sub__`, and `__mul__` methods to support addition, subtraction, and multiplication with other `MagicContainer` instances, returning a new `MagicContainer` with the results. Input Format: - Initialization: `MagicContainer([list of integers])` - Various index-based operations within the constraints of typical list usage in Python. - Operations with other instances of `MagicContainer`. Output Format: - Proper behaviors and string outputs based on standard Python container operations. Constraints: - Elements must be integers. - Use appropriate Python exceptions (like `TypeError`, `IndexError`). Example: ```python # Initialization mc = MagicContainer([1, 2, 3, 4]) # Length print(len(mc)) # Output: 4 # Item Retrieval print(mc[1]) # Output: 2 # Item Assignment mc[1] = 10 print(mc) # Output: MagicContainer([1, 10, 3, 4]) # String Representation print(repr(mc)) # Output: \\"MagicContainer([1, 10, 3, 4])\\" print(str(mc)) # Output: \\"[1, 10, 3, 4]\\" # Membership print(10 in mc) # Output: True # Iteration for item in mc: print(item) # Output: 1, 10, 3, 4 # Reverse Iteration for item in reversed(mc): print(item) # Output: 4, 3, 10, 1 # Arithmetic Operations mc2 = MagicContainer([5, 6, 7, 8]) mc3 = mc + mc2 print(mc3) # Output: MagicContainer([6, 16, 10, 12]) mc4 = mc - mc2 print(mc4) # Output: MagicContainer([-4, 4, -4, -4]) mc5 = mc * mc2 print(mc5) # Output: MagicContainer([5, 60, 21, 32]) ``` Instructions: 1. Implement the `MagicContainer` class as per the requirements. 2. Test the class thoroughly with various inputs to ensure all functionalities are working correctly. 3. Submit your class implementation and test cases.","solution":"class MagicContainer: def __init__(self, elements): if not all(isinstance(x, int) for x in elements): raise TypeError(\'All elements must be integers\') self.elements = elements def __len__(self): return len(self.elements) def __getitem__(self, index): return self.elements[index] def __setitem__(self, index, value): if isinstance(index, slice): if not all(isinstance(x, int) for x in value): raise TypeError(\'All elements must be integers\') elif not isinstance(value, int): raise TypeError(\'Value must be an integer\') self.elements[index] = value def __repr__(self): return f\\"MagicContainer({self.elements})\\" def __str__(self): return str(self.elements) def __contains__(self, item): return item in self.elements def __iter__(self): return iter(self.elements) def __reversed__(self): return reversed(self.elements) def __add__(self, other): if isinstance(other, MagicContainer): return MagicContainer([a + b for a, b in zip(self.elements, other.elements)]) else: raise TypeError(\\"Operand must be another MagicContainer\\") def __sub__(self, other): if isinstance(other, MagicContainer): return MagicContainer([a - b for a, b in zip(self.elements, other.elements)]) else: raise TypeError(\\"Operand must be another MagicContainer\\") def __mul__(self, other): if isinstance(other, MagicContainer): return MagicContainer([a * b for a, b in zip(self.elements, other.elements)]) else: raise TypeError(\\"Operand must be another MagicContainer\\")"},{"question":"# Question: Binary Data and Pretty Printing with Multithreading You have been tasked with creating a program that reads binary data from a file, processes the data to extract specific information, and prints this information in a readable format while performing these tasks in parallel using multiple threads. Detailed Requirements: 1. **Binary File Processing**: - Create a binary file containing records of users. Each record contains: - `user_id`: 4 bytes, unsigned integer. - `username_length`: 1 byte, unsigned integer. - `username`: variable length string based on `username_length`. Example: ``` user_id: 1234 (4 bytes) username_length: 5 (1 byte) username: \'Alice\' (5 bytes) ``` 2. **Data Extraction**: - Create a function `read_user_data(file_path)` that: - Reads the binary file. - Extracts and returns a list of tuples where each tuple corresponds to a user\'s data: `(user_id, username)`. 3. **Pretty Printing**: - Create a function `pretty_print_user_data(user_data, width=40)` that: - Takes the list of user data tuples. - Pretty prints the list in a formatted structure using the `pprint` module, ensuring the output fits within the specified width. 4. **Multithreading**: - Use the `threading` module to perform file reading and pretty printing in parallel. - Ensure the main program waits for both tasks to complete before exiting. Input: - Path to the binary file that needs to be read. Output: - Pretty printed user data. Example Usage: ```python # Create binary file with user data create_binary_user_file(\'users.bin\') # Read and pretty print data in parallel parallel_read_and_print(\'users.bin\') ``` Constraints: - The binary file contains at most 10,000 user records. Performance Requirements: - The program should be able to read and pretty print user data from the binary file in under 2 seconds for a file containing 10,000 records. Implementation Notes: - You can use `struct` module to handle binary data packing and unpacking. - Make sure to handle exceptions that might occur during file operations. - Pay attention to thread safety when accessing shared resources. Please implement the functions `create_binary_user_file`, `read_user_data`, `pretty_print_user_data`, and `parallel_read_and_print`.","solution":"import struct import threading import pprint def create_binary_user_file(file_path): user_data = [ (1234, \'Alice\'), (5678, \'Bob\'), (91011, \'Charlie\'), (121314, \'David\') ] with open(file_path, \'wb\') as f: for user_id, username in user_data: username_length = len(username) binary_data = struct.pack(\'I B\', user_id, username_length) + username.encode() f.write(binary_data) def read_user_data(file_path): users = [] try: with open(file_path, \'rb\') as f: while True: raw_user_id = f.read(4) if not raw_user_id: break user_id = struct.unpack(\'I\', raw_user_id)[0] username_length = struct.unpack(\'B\', f.read(1))[0] username = f.read(username_length).decode() users.append((user_id, username)) except Exception as e: print(f\\"Error reading file: {e}\\") return users def pretty_print_user_data(user_data, width=40): pprint.pprint(user_data, width=width) def parallel_read_and_print(file_path): user_data = [] read_thread = threading.Thread(target=lambda: user_data.extend(read_user_data(file_path))) print_thread = threading.Thread(target=lambda: pretty_print_user_data(user_data)) read_thread.start() read_thread.join() print_thread.start() print_thread.join() # Example usage create_binary_user_file(\'users.bin\') parallel_read_and_print(\'users.bin\')"},{"question":"Multi-threading Task # Objective Write a Python program using the `threading` module to simulate a multi-threaded system for processing data from multiple data sources. # Problem Statement You are required to create a program that reads data from three different files and processes each file in a separate thread. Each thread should read the content of its respective file, reverse the content string, and save the reversed content to a new file. # Function Specification Implement the following function: ```python def process_files(input_files: list, output_files: list): Given a list of input file names and a corresponding list of output file names, this function reads content from each input file in a separate thread, reverses the content, and writes the reversed content to the corresponding output file. :param input_files: List of names of input files. :param output_files: List of names of output files. :return: None ``` # Input - `input_files`: A list of names of input files (strings), e.g., `[\'file1.txt\', \'file2.txt\', \'file3.txt\']`. - `output_files`: A list of names of output files (strings), e.g., `[\'output1.txt\', \'output2.txt\', \'output3.txt\']`. # Output - The function should create output files with the reversed content from the corresponding input files. # Constraints - The length of `input_files` and `output_files` will always be the same. - The content in the input files will be plain text. # Example If `file1.txt`, `file2.txt`, and `file3.txt` contain `\\"abcd\\"`, `\\"efgh\\"`, and `\\"ijkl\\"` respectively, the corresponding output files should contain `\\"dcba\\"`, `\\"hgfe\\"`, and `\\"lkji\\"` respectively. # Notes - Ensure proper synchronization where necessary. - Handle any potential exceptions that may occur during file operations. - You may assume that the input files exist and are readable, and the program has permission to write to the output files. # Additional Information You may refer to the `threading` module documentation for details on how to create and manage threads in Python.","solution":"import threading def process_file(input_file, output_file): try: with open(input_file, \'r\') as f: content = f.read() reversed_content = content[::-1] with open(output_file, \'w\') as f: f.write(reversed_content) except Exception as e: print(f\\"Error processing {input_file}: {e}\\") def process_files(input_files: list, output_files: list): threads = [] for inp, out in zip(input_files, output_files): thread = threading.Thread(target=process_file, args=(inp, out)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"**Coding Assessment Question** # Objective You are tasked with creating a small utility that interacts with the NIS (Yellow Pages) service using the deprecated `nis` module in Python. Your task is to implement a function that performs specific operations using the NIS services and to handle any potential errors appropriately. # Problem Statement Write a Python function `nis_utilities(mapname: str, key: str) -> dict`, which performs the following operations: 1. Retrieves the system default NIS domain using the `nis.get_default_domain()` function. 2. Retrieves a dictionary of all entries in the specified `mapname` using `nis.cat(mapname)`. 3. Retrieves the value for the specified `key` in the specified `mapname` using `nis.match(key, mapname)`. 4. Creates and returns a dictionary with the following structure: ```python { \\"default_domain\\": <default_domain>, \\"map_entries\\": <dictionary from nis.cat(mapname)>, \\"matched_value\\": <value from nis.match(key, mapname)> } ``` # Constraints and Requirements - **Input:** - `mapname`: A string representing the NIS map name. - `key`: A string representing the key to look up in the NIS map. - **Output:** - A dictionary containing the default NIS domain, all entries in the specified map, and the matched value for the specified key. - **Error Handling:** - If any `nis.error` is raised during the operations, catch the exception and return a dictionary with the structure: ```python { \\"error\\": \\"An error message here\\" } ``` - **Performance:** - The function should handle operations efficiently even if the NIS map contains a large number of entries. # Example ```python def nis_utilities(mapname: str, key: str) -> dict: import nis try: default_domain = nis.get_default_domain() map_entries = nis.cat(mapname) matched_value = nis.match(key, mapname) return { \\"default_domain\\": default_domain, \\"map_entries\\": map_entries, \\"matched_value\\": matched_value } except nis.error as e: return { \\"error\\": str(e) } # Example usage: # Assume the NIS system has domain \\"example.com\\", a map named \\"passwd.byname\\", # and an entry matching key \\"jdoe\\" in this map. result = nis_utilities(\\"passwd.byname\\", \\"jdoe\\") print(result) ``` # Note - Make sure to test your function on a Unix system where the NIS service is configured and running. - Since `nis` is a deprecated module and only works on Unix systems, this code may not run in all environments.","solution":"def nis_utilities(mapname: str, key: str) -> dict: import nis try: default_domain = nis.get_default_domain() map_entries = nis.cat(mapname) matched_value = nis.match(key, mapname) return { \\"default_domain\\": default_domain, \\"map_entries\\": map_entries, \\"matched_value\\": matched_value } except nis.error as e: return { \\"error\\": str(e) }"},{"question":"In this task, you are required to create a function that analyzes the structure of a given Python module and outputs information about its classes and functions. Your function should use the `inspect` module to gather and print details about each class and function within the module. # Requirements: 1. **Function Signature**: ```python def analyze_module(module: str) -> None: ``` 2. **Input**: - `module` (str): The name of the module to be analyzed. 3. **Output**: - Print the following details for each class in the module: - Class name - Documentation string - List of methods and their corresponding documentation strings - Print the following details for each function in the module: - Function name - Documentation string - Function signature 4. **Constraints**: - Do not execute any method or function within the module. - Use only the `inspect` module for introspection. # Example: If the module `sample.py` contains: ```python This is a sample module for testing. class SampleClass: This is a sample class def method1(self): This is method1 pass def method2(self, x): This is method2 with parameter x pass def sample_function(a, b): This is a sample function. :param a: first parameter :param b: second parameter return a + b ``` Calling `analyze_module(\'sample\')` should print: ``` Class: SampleClass Documentation: This is a sample class Method: method1 Method Documentation: This is method1 Method: method2 Method Documentation: This is method2 with parameter x Function: sample_function Documentation: This is a sample function. Signature: (a, b) ``` # Points to consider: - Focus on clarity and completeness of the output. - Ensure proper error handling if the module does not exist or cannot be loaded. - Think about extensibility and maintainability of your code.","solution":"import inspect import importlib def analyze_module(module: str) -> None: Analyzes the given module and prints details about its classes and functions. :param module: The name of the module to be analyzed. try: mod = importlib.import_module(module) except ImportError: print(f\\"Could not import module \'{module}\'\\") return for name, obj in inspect.getmembers(mod): if inspect.isclass(obj): print(f\\"Class: {name}\\") print(f\\"Documentation: {inspect.getdoc(obj)}\\") for m_name, method in inspect.getmembers(obj, predicate=inspect.isfunction): print(f\\" Method: {m_name}\\") print(f\\" Method Documentation: {inspect.getdoc(method)}\\") print() elif inspect.isfunction(obj): print(f\\"Function: {name}\\") print(f\\"Documentation: {inspect.getdoc(obj)}\\") print(f\\"Signature: {inspect.signature(obj)}\\") print()"},{"question":"# **Email and MIME Handling in Python** **Introduction** You are to implement a functionality that processes an email message, parses it, modifies its content, and generates a new MIME document. This task will demonstrate your understanding of various components of the `email` module in Python 3.10. **Task Details** **1. Parse the Email Message:** - Write a function `parse_email` that takes a string `raw_email` as input, represents the email using `email.message.Message`, and returns the `Message` object. **2. Modify the Email Content:** - Write a function `modify_email_content` that: - Accepts a `Message` object and two strings `old_content`, `new_content`. - Searches for `old_content` within the email\'s payload and replaces it with `new_content`. - Returns the modified `Message` object. **3. Generate a New MIME Document:** - Write a function `generate_mime_document` that: - Accepts the modified `Message` object. - Generates and returns the new MIME formatted string of the `Message`. **Input and Output Formats** - **parse_email(`raw_email`)**: - **Input**: `raw_email` (str) - A string representing the raw email content. - **Output**: Returns a `Message` object parsed from the raw email string. - **modify_email_content(`message`, `old_content`, `new_content`)**: - **Input**: - `message` (`Message` object) - The parsed email message to be modified. - `old_content` (str) - The string content to be replaced. - `new_content` (str) - The string content to replace with. - **Output**: Returns the modified `Message` object. - **generate_mime_document(`message`)**: - **Input**: `message` (`Message` object) - The modified email message. - **Output**: Returns a string representing the MIME formatted email. **Constraints** - Consider email content to reside in plain text format for simplicity. - Do not modify the email headers. - Ensure any replaced content fits the MIME formatting standards. **Performance Requirements** - Efficient handling of large email bodies. - Minimal memory footprint, considering potential email attachments (though they won\'t be processed). **Example Use Case** ```python # Function definitions def parse_email(raw_email: str) -> email.message.Message: pass def modify_email_content(message: email.message.Message, old_content: str, new_content: str) -> email.message.Message: pass def generate_mime_document(message: email.message.Message) -> str: pass # Example usage raw_email = From: user@example.com To: user2@example.com Subject: Test email Hello, this is a test email. message = parse_email(raw_email) modified_message = modify_email_content(message, \\"test email\\", \\"sample email\\") mime_document = generate_mime_document(modified_message) print(mime_document) ``` Expected output should be a properly formatted MIME document with \\"sample email\\" instead of \\"test email\\".","solution":"import email from email.parser import Parser from email.message import Message def parse_email(raw_email: str) -> Message: Parse the raw email string and return a Message object. return Parser().parsestr(raw_email) def modify_email_content(message: Message, old_content: str, new_content: str) -> Message: Modify the email content by replacing old_content with new_content. if message.is_multipart(): for part in message.get_payload(): modify_email_content(part, old_content, new_content) else: payload = message.get_payload() if isinstance(payload, str): message.set_payload(payload.replace(old_content, new_content)) return message def generate_mime_document(message: Message) -> str: Generate and return the MIME formatted string of the Message. return message.as_string()"},{"question":"# Custom HTML Parser Implementation **Objective:** Create a custom HTML parser by subclassing the `HTMLParser` class from the `html.parser` module. Your parser should identify and extract specific information from the provided HTML content. **Task:** 1. Implement a subclass named `CustomHTMLParser` that processes HTML input to extract: - All image URLs (from `<img>` tags) - All hyperlinks (from `<a>` tags, including both the URL and the link text) - The title of the HTML document (from the `<title>` tag) 2. Define methods to appropriately handle start tags, end tags, and data: - `handle_starttag(self, tag, attrs)` - `handle_endtag(self, tag)` - `handle_data(self, data)` 3. The `CustomHTMLParser` should have methods to return the collected data: - `get_image_urls(self)` returns a list of all image URLs. - `get_hyperlinks(self)` returns a list of tuples, each containing the URL and the link text. - `get_title(self)` returns the text content of the `<title>` tag. **Input Format:** - A string containing HTML content. **Output Format:** - A list of image URLs. - A list of tuples with hyperlinks (URL, link text). - A string with the title of the document. **Constraints:** - The HTML input may contain nested tags and comments. - The methods should handle HTML tags and entities correctly. **Example:** ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.image_urls = [] self.hyperlinks = [] self.title = \\"\\" self.in_title = False self.current_link_text = \\"\\" self.current_link = \\"\\" def handle_starttag(self, tag, attrs): if tag == \\"img\\": for attr in attrs: if attr[0] == \\"src\\": self.image_urls.append(attr[1]) elif tag == \\"a\\": for attr in attrs: if attr[0] == \\"href\\": self.current_link = attr[1] elif tag == \\"title\\": self.in_title = True def handle_endtag(self, tag): if tag == \\"a\\" and self.current_link: self.hyperlinks.append((self.current_link, self.current_link_text.strip())) self.current_link_text = \\"\\" elif tag == \\"title\\": self.in_title = False def handle_data(self, data): if self.in_title: self.title += data.strip() if self.current_link: self.current_link_text += data def get_image_urls(self): return self.image_urls def get_hyperlinks(self): return self.hyperlinks def get_title(self): return self.title # Example usage html_content = \'\'\' <html> <head> <title>Sample Page</title> </head> <body> <h1>Welcome to the Sample Page</h1> <img src=\\"sample-image.jpg\\"> <a href=\\"https://example.com\\">Example link</a> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed(html_content) print(parser.get_image_urls()) # [\'sample-image.jpg\'] print(parser.get_hyperlinks()) # [(\'https://example.com\', \'Example link\')] print(parser.get_title()) # \'Sample Page\' ``` Your task is to implement the `CustomHTMLParser` class and its methods according to the above specification.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.image_urls = [] self.hyperlinks = [] self.title = \\"\\" self.in_title = False self.current_link_text = \\"\\" self.current_link = \\"\\" def handle_starttag(self, tag, attrs): if tag == \\"img\\": for attr in attrs: if attr[0] == \\"src\\": self.image_urls.append(attr[1]) elif tag == \\"a\\": for attr in attrs: if attr[0] == \\"href\\": self.current_link = attr[1] elif tag == \\"title\\": self.in_title = True def handle_endtag(self, tag): if tag == \\"a\\" and self.current_link: self.hyperlinks.append((self.current_link, self.current_link_text.strip())) self.current_link_text = \\"\\" self.current_link = \\"\\" elif tag == \\"title\\": self.in_title = False def handle_data(self, data): if self.in_title: self.title += data.strip() if self.current_link: self.current_link_text += data def get_image_urls(self): return self.image_urls def get_hyperlinks(self): return self.hyperlinks def get_title(self): return self.title"},{"question":"# Custom File Handling with Built-ins You are required to create a custom file handling class in Python that extends the functionality of the basic file operations. Your class should leverage Python\'s built-in file handling while also providing additional custom behavior. Implement a class named `FileHandler` with the following methods: Methods 1. `__init__(self, path: str, mode: str) -> None` - **Parameters**: - `path`: A string representing the file path. - `mode`: A string representing the mode in which the file should be opened (e.g., \'r\', \'w\', \'a\'). - **Description**: Initializes the `FileHandler` object and opens the file using the built-in `open()` method. 2. `read_content(self, count: int = -1) -> str` - **Parameters**: - `count`: An integer representing the number of characters to read from the file (default is -1, which means read all). - **Return**: A string containing the content read from the file. - **Description**: Reads the content from the file and returns it. It should read the content in its original form. 3. `read_content_upper(self, count: int = -1) -> str` - **Parameters**: - `count`: An integer representing the number of characters to read from the file (default is -1, which means read all). - **Return**: A string containing the content read from the file converted to uppercase. - **Description**: Reads the content from the file and returns it in uppercase form. 4. `write_content(self, content: str) -> None` - **Parameters**: - `content`: A string containing the content to write to the file. - **Return**: None - **Description**: Writes the provided content to the file in the specified mode. 5. `close(self) -> None` - **Return**: None - **Description**: Closes the file if it is open. Constraints 1. You must use the built-in `open()` function to open the file. 2. Ensure proper handling of file opening, reading, writing, and closing to avoid resource leaks. 3. The file should be opened and closed appropriately within the class methods to handle exceptions. Example Usage ```python # Initialize the custom file handler handler = FileHandler(\\"example.txt\\", \\"w\\") # Write content to the file handler.write_content(\\"Hello World!\\") # Close the file handler.close() # Re-open the custom file handler in read mode handler = FileHandler(\\"example.txt\\", \\"r\\") # Read content from the file print(handler.read_content()) # Output: Hello World! # Read content from the file in uppercase print(handler.read_content_upper()) # Output: HELLO WORLD! # Close the file handler.close() ```","solution":"class FileHandler: def __init__(self, path: str, mode: str) -> None: Initializes the FileHandler object and opens the file. self.path = path self.mode = mode self.file = open(self.path, self.mode) def read_content(self, count: int = -1) -> str: Reads content from the file. if self.file.closed: raise ValueError(\\"File is closed\\") return self.file.read(count) def read_content_upper(self, count: int = -1) -> str: Reads content from the file and converts it to uppercase. if self.file.closed: raise ValueError(\\"File is closed\\") return self.file.read(count).upper() def write_content(self, content: str) -> None: Writes content to the file. if self.file.closed: raise ValueError(\\"File is closed\\") self.file.write(content) def close(self) -> None: Closes the file. if not self.file.closed: self.file.close()"},{"question":"# Coding Assessment: Exception Handling in Python Objective To assess your understanding of error handling, exception management, and custom exceptions in Python. Problem Statement You are tasked with writing a function that processes a list of operations. Each operation is represented by a tuple containing a command and its arguments. Your function should handle exceptions appropriately, ensuring that the command is correctly executed or the error is reported without terminating the execution of other commands. Function Signature ```python def process_operations(operations: List[Tuple[str, Any]]) -> List[Union[str, Exception]]: ``` Input - `operations` (List[Tuple[str, Any]]): A list of tuples where each tuple consists of a command (a string) and its associated argument(s). The command can be one of the following: - `\\"add\\"`: Takes two integers and returns their sum. - `\\"subtract\\"`: Takes two integers and returns their difference. - `\\"multiply\\"`: Takes two integers and returns their product. - `\\"divide\\"`: Takes two integers and returns their quotient. If the second integer is zero, it should raise a `ZeroDivisionError`. - `\\"concatenate\\"`: Takes two strings and returns their concatenation. - `\\"unknown\\"`: An unknown command that should raise a `ValueError`. Output - A list where each element corresponds to the result of executing each operation. If an operation raises an exception, include the exception in the list instead. Example ```python operations = [ (\\"add\\", (1, 2)), (\\"subtract\\", (5, 3)), (\\"multiply\\", (2, 3)), (\\"divide\\", (10, 2)), (\\"divide\\", (10, 0)), (\\"concatenate\\", (\\"hello\\", \\"world\\")), (\\"unknown\\", ()), ] expected_output = [3, 2, 6, 5.0, ZeroDivisionError(\\"division by zero\\"), \\"helloworld\\", ValueError(\\"unknown command\\")] assert process_operations(operations) == expected_output ``` Constraints - You must handle exceptions using try-except blocks. - You need to ensure that the function continues processing the next operation even if any previous operation raises an exception. - You should use custom exception messages where appropriate. - The function must be efficient and handle a reasonable number of operations (up to 10^3 operations) within a short time frame. Tips - Use Python\'s built-in exceptions such as `ZeroDivisionError` and `ValueError` where appropriate. - Ensure readability and maintainability of your code with proper comments and coding standards. Good luck!","solution":"from typing import List, Tuple, Any, Union def process_operations(operations: List[Tuple[str, Any]]) -> List[Union[str, Exception]]: results = [] for operation in operations: command, args = operation try: if command == \\"add\\": result = args[0] + args[1] elif command == \\"subtract\\": result = args[0] - args[1] elif command == \\"multiply\\": result = args[0] * args[1] elif command == \\"divide\\": if args[1] == 0: raise ZeroDivisionError(\\"division by zero\\") result = args[0] / args[1] elif command == \\"concatenate\\": result = args[0] + args[1] else: raise ValueError(\\"unknown command\\") results.append(result) except Exception as e: results.append(e) return results"},{"question":"# Password Handling Utility **Objective:** Implement a secure password-handling utility in Python using the `crypt` module. This utility will provide functionality to hash passwords and verify them. # Requirements: 1. **Function Implementation:** - **`hash_password(plaintext: str, method=None) -> str`** - **Input:** A plaintext password and an optional hashing method (`crypt.METHOD_*`). - **Output:** A hashed version of the password using the specified method if provided, or the strongest method available otherwise. - **`verify_password(plaintext: str, hashed_password: str) -> bool`** - **Input:** A plaintext password and its hashed version. - **Output:** Returns `True` if the plaintext password matches the hashed password, `False` otherwise. 2. **Constraints:** - Use the `crypt.mksalt` method to generate salts as needed. - Ensure compatibility across platforms by handling potential unavailability of certain methods. - For `METHOD_SHA256`, `METHOD_SHA512`, and `METHOD_BLOWFISH`, allow specifying the number of rounds when generating salts. 3. **Performance Requirements:** - The solution should be efficient and must handle large input strings gracefully. - Aim for constant-time comparisons to prevent timing attacks. 4. **Documentation and Validation:** - Provide meaningful comments and docstrings for your functions. - Include validation for input parameters where necessary. # Example Usage: ```python import crypt from hmac import compare_digest as compare_hash def hash_password(plaintext: str, method=None, rounds=None) -> str: if method: salt = crypt.mksalt(method, rounds=rounds) else: salt = crypt.mksalt() return crypt.crypt(plaintext, salt) def verify_password(plaintext: str, hashed_password: str) -> bool: return compare_hash(crypt.crypt(plaintext, hashed_password), hashed_password) # Example usage hashed_pw = hash_password(\\"mysecretpassword\\", method=crypt.METHOD_SHA512, rounds=10000) print(\\"Hashed password: \\", hashed_pw) is_valid = verify_password(\\"mysecretpassword\\", hashed_pw) print(\\"Password verified: \\", is_valid) ``` Implement the functions `hash_password` and `verify_password` as specified and test their functionality with various passwords and hashing methods.","solution":"import crypt from hmac import compare_digest as compare_hash def hash_password(plaintext: str, method=None, rounds=None) -> str: Hashes the given plaintext password using the provided method and number of rounds. :param plaintext: The password to be hashed. :param method: The hashing method to be used (crypt.METHOD_*). Defaults to None. :param rounds: The number of rounds for hashing, applicable for certain methods. Defaults to None. :return: The hashed password. if method: salt = crypt.mksalt(method, rounds=rounds) else: salt = crypt.mksalt() return crypt.crypt(plaintext, salt) def verify_password(plaintext: str, hashed_password: str) -> bool: Verifies if the given plaintext password matches the hashed password. :param plaintext: The password to be verified. :param hashed_password: The hashed password to be compared against. :return: True if the passwords match, False otherwise. return compare_hash(crypt.crypt(plaintext, hashed_password), hashed_password)"},{"question":"# Kernel Ridge Regression Implementation and Optimization You are provided with a dataset that contains features and a target variable with added noise. Your task is to: 1. Implement a Kernel Ridge Regression model using the `KernelRidge` class from the `sklearn.kernel_ridge` module. 2. Optimize the hyperparameters (alpha and kernel) of the KRR model using grid-search to minimize the mean squared error on the validation dataset. 3. Compare the performance (fitting time and prediction time) with a Support Vector Regression (SVR) model from `sklearn.svm`. Input - `X_train`: A 2D numpy array or pandas DataFrame containing the training features. - `y_train`: A 1D numpy array or pandas Series containing the training target values. - `X_test`: A 2D numpy array or pandas DataFrame containing the test features. - `y_test`: A 1D numpy array or pandas Series containing the test target values. Output - `best_params_`: A dictionary containing the best hyperparameters found for the KRR model. - `krr_fit_time`: The time taken to fit the KRR model on the training data. - `krr_pred_time`: The time taken to predict the target values for the test data using the KRR model. - `krr_mse`: The mean squared error (MSE) of the predictions made by the KRR model on the test data. - `svr_fit_time`: The time taken to fit the SVR model on the training data. - `svr_pred_time`: The time taken to predict the target values for the test data using the SVR model. - `svr_mse`: The mean squared error (MSE) of the predictions made by the SVR model on the test data. Constraints 1. The grid-search for hyperparameters should consider at least the following: - `alpha`: `[1e-3, 1e-2, 1e-1, 1, 10]` - `kernel`: `[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]` 2. The performance requirements are to minimize the mean squared error of the predictions, while also recording the fitting and prediction times. Required Libraries ```python import numpy as np import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error import time ``` Implementation Write a function `kernel_ridge_regression_optimization` with the following signature: ```python def kernel_ridge_regression_optimization(X_train, y_train, X_test, y_test): # Hyperparameter grid for KernelRidge param_grid_krr = { \'alpha\': [1e-3, 1e-2, 1e-1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\', \'sigmoid\'] } # Initialize kernel ridge regressor and grid search krr = KernelRidge() grid_search_krr = GridSearchCV(krr, param_grid_krr, cv=5) # Fit the grid search start_fit = time.time() grid_search_krr.fit(X_train, y_train) end_fit = time.time() krr_fit_time = end_fit - start_fit # Best parameters best_params_ = grid_search_krr.best_params_ # Predict with the best KRR model best_krr_model = grid_search_krr.best_estimator_ start_pred = time.time() y_pred_krr = best_krr_model.predict(X_test) end_pred = time.time() krr_pred_time = end_pred - start_pred # Calculate MSE krr_mse = mean_squared_error(y_test, y_pred_krr) # Initialize and fit SVR svr = SVR() start_fit = time.time() svr.fit(X_train, y_train) end_fit = time.time() svr_fit_time = end_fit - start_fit # Predict with SVR model start_pred = time.time() y_pred_svr = svr.predict(X_test) end_pred = time.time() svr_pred_time = end_pred - start_pred # Calculate MSE svr_mse = mean_squared_error(y_test, y_pred_svr) return { \'best_params_\': best_params_, \'krr_fit_time\': krr_fit_time, \'krr_pred_time\': krr_pred_time, \'krr_mse\': krr_mse, \'svp_fit_time\': svr_fit_time, \'svr_pred_time\': svr_pred_time, \'svp_mse\': svr_mse } ``` Test your function with the input data and report the results.","solution":"import numpy as np import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error import time def kernel_ridge_regression_optimization(X_train, y_train, X_test, y_test): # Hyperparameter grid for KernelRidge param_grid_krr = { \'alpha\': [1e-3, 1e-2, 1e-1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\', \'sigmoid\'] } # Initialize kernel ridge regressor and grid search krr = KernelRidge() grid_search_krr = GridSearchCV(krr, param_grid_krr, cv=5) # Fit the grid search start_fit = time.time() grid_search_krr.fit(X_train, y_train) end_fit = time.time() krr_fit_time = end_fit - start_fit # Best parameters best_params_ = grid_search_krr.best_params_ # Predict with the best KRR model best_krr_model = grid_search_krr.best_estimator_ start_pred = time.time() y_pred_krr = best_krr_model.predict(X_test) end_pred = time.time() krr_pred_time = end_pred - start_pred # Calculate MSE krr_mse = mean_squared_error(y_test, y_pred_krr) # Initialize and fit SVR svr = SVR() start_fit = time.time() svr.fit(X_train, y_train) end_fit = time.time() svr_fit_time = end_fit - start_fit # Predict with SVR model start_pred = time.time() y_pred_svr = svr.predict(X_test) end_pred = time.time() svr_pred_time = end_pred - start_pred # Calculate MSE svr_mse = mean_squared_error(y_test, y_pred_svr) return { \'best_params_\': best_params_, \'krr_fit_time\': krr_fit_time, \'krr_pred_time\': krr_pred_time, \'krr_mse\': krr_mse, \'svr_fit_time\': svr_fit_time, \'svr_pred_time\': svr_pred_time, \'svr_mse\': svr_mse }"},{"question":"# Question: **Title:** Extracting Information from a Text Document Using Regular Expressions **Problem Statement:** You are provided with a text document containing multiple lines of semi-structured data. Your task is to extract specific pieces of information from this document using regular expressions in Python. **Input Format:** 1. A single string, `text`, that contains multiple lines of data. Each line represents a pseudo-log entry in the format: ``` [Date: YYYY-MM-DD] [Time: HH:MM:SS] [Level: DEBUG|INFO|WARNING|ERROR] Message: <log message> ``` 2. The extraction targets can be summarized as follows: - Extract all dates. - Extract all lines containing ERROR logs. - Replace all date-time entries with a placeholder `<DATE_TIME>`. **Output Format:** 1. A list of extracted dates (strings) in the format `YYYY-MM-DD`. 2. A list of log entries (strings) which contain the `ERROR` level. 3. A single string representing the modified text with dates and times replaced by `<DATE_TIME>`. **Function Signature:** ```python def extract_and_modify(text: str) -> (list, list, str): pass ``` **Example:** _Input:_ ```python text = [Date: 2023-03-21] [Time: 14:55:22] [Level: INFO] Message: System started. [Date: 2023-03-21] [Time: 15:04:03] [Level: ERROR] Message: Failure in system component. [Date: 2023-03-22] [Time: 09:15:12] [Level: DEBUG] Message: Initializing diagnostics. [Date: 2023-03-22] [Time: 10:20:45] [Level: WARNING] Message: Low memory warning. ``` _Output:_ ```python dates = [\\"2023-03-21\\", \\"2023-03-21\\", \\"2023-03-22\\", \\"2023-03-22\\"] error_logs = [ \\"[Date: 2023-03-21] [Time: 15:04:03] [Level: ERROR] Message: Failure in system component.\\" ] modified_text = [Date: <DATE_TIME>] [Time: <DATE_TIME>] [Level: INFO] Message: System started. [Date: <DATE_TIME>] [Time: <DATE_TIME>] [Level: ERROR] Message: Failure in system component. [Date: <DATE_TIME>] [Time: <DATE_TIME>] [Level: DEBUG] Message: Initializing diagnostics. [Date: <DATE_TIME>] [Time: <DATE_TIME>] [Level: WARNING] Message: Low memory warning. ``` **Constraints:** - Use the `re` module to solve the problem. - Handle both greedy and non-greedy matching where appropriate. - Assume the input will always follow the given format.","solution":"import re def extract_and_modify(text: str) -> (list, list, str): # Extract all dates date_pattern = r\'[Date: (d{4}-d{2}-d{2})]\' dates = re.findall(date_pattern, text) # Extract all lines containing ERROR logs error_pattern = r\'[Date: d{4}-d{2}-d{2}] [Time: d{2}:d{2}:d{2}] [Level: ERROR].*\' error_logs = re.findall(error_pattern, text) # Replace all date-time entries with <DATE_TIME> modified_text = re.sub(r\'[Date: d{4}-d{2}-d{2}] [Time: d{2}:d{2}:d{2}]\', \'[Date: <DATE_TIME>] [Time: <DATE_TIME>]\', text) return dates, error_logs, modified_text"},{"question":"**Problem Statement:** You are tasked with creating a Python function that handles slicing operations for a custom sequence class. This function will be tested for its ability to correctly create, interpret, and adjust slices of the sequence. # Part 1: Creating and Interpreting Slices You need to create a function `create_slice` that takes three parameters: `start`, `stop`, and `step`. These parameters might be `None`. Your function should create a slice object using these parameters. ```python def create_slice(start: int, stop: int, step: int): Create a slice object with given start, stop, and step. Parameters: - start (int or None): Start index of the slice - stop (int or None): Stop index of the slice - step (int or None): Step of the slice Returns: - slice: The created slice object Example: create_slice(1, 10, 2) # Should return slice(1, 10, 2) pass ``` # Part 2: Extracting Slice Indices Next, implement the function `extract_slice_indices` that takes a slice object and a length of the sequence, then returns the actual start, stop, and step values of the slice for that sequence. ```python def extract_slice_indices(slice_obj: slice, length: int): Extract start, stop, and step from a slice object for a given sequence length. Parameters: - slice_obj (slice): The slice object - length (int): The length of the sequence Returns: - tuple: A tuple containing (start, stop, step) Example: extract_slice_indices(slice(1, 10, 2), 15) # Should return (1, 10, 2) pass ``` # Part 3: Adjusting Slice Indices Implement `adjust_slice_indices` to adjust the indices of a given slice object to fit within the boundaries of a sequence with a given length. ```python def adjust_slice_indices(slice_obj: slice, length: int): Adjust the start, stop, and step indices of a slice object to fit within the bounds of a sequence of the specified length. Parameters: - slice_obj (slice): The slice object - length (int): The length of the sequence Returns: - tuple: A tuple containing (start, stop, step) adjusted for sequence length Example: adjust_slice_indices(slice(1, 100, 2), 15) # Should return (1, 15, 2) pass ``` # Constraints: - You may assume that the sequence length is a positive integer. - The `start`, `stop`, and `step` values in `create_slice` can be `None`. # Performance Requirements: - Your solution should efficiently handle sequences of length up to (10^6). Implement the three functions as described above. Ensure to handle edge cases appropriately.","solution":"def create_slice(start, stop, step): Create a slice object with given start, stop, and step. Parameters: - start: Start index of the slice (can be int or None) - stop: Stop index of the slice (can be int or None) - step: Step of the slice (can be int or None) Returns: - slice: The created slice object return slice(start, stop, step) def extract_slice_indices(slice_obj, length): Extract start, stop, and step from a slice object for a given sequence length. Parameters: - slice_obj: The slice object - length: The length of the sequence Returns: - tuple: A tuple containing (start, stop, step) return slice_obj.indices(length) def adjust_slice_indices(slice_obj, length): Adjust the start, stop, and step indices of a slice object to fit within the bounds of a sequence of the specified length. Parameters: - slice_obj: The slice object - length: The length of the sequence Returns: - tuple: A tuple containing (start, stop, step) adjusted for sequence length start, stop, step = slice_obj.indices(length) return (start, stop, step)"},{"question":"You are required to create a multifaceted bar plot using seaborn that visualizes the average body mass of penguins grouped by species and further divided by their sex. Follow the instructions and constraints carefully to complete this task. Requirements 1. Load the `penguins` dataset from seaborn. 2. Create a bar plot using the following parameters: - **x-axis**: Sex of penguins. - **y-axis**: Average body mass (`body_mass_g`). - **Column facet**: Species of penguins. 3. Customize the error bars to reflect the standard deviation. 4. Add appropriate labels for axes and title for the entire plot. 5. Ensure the plot is aesthetically appealing with: - `height=4` and `aspect=0.6` for the facets. - Custom colors for the bars. - Error bars with a line width of 1.5. 6. Display the plot. Input Dataset - You are not required to input any data as seaborn provides the `penguins` dataset. Expected Output A faceted bar plot with customizations as described above. Constraints - Use only seaborn and matplotlib for plotting. Code Template ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a faceted bar plot g = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", col=\\"species\\", height=4, aspect=0.6, errorbar=\\"sd\\", palette=\\"muted\\", linewidth=1.5, ) # Add consistent labels and a title g.set_axis_labels(\\"Sex\\", \\"Average Body Mass (g)\\") g.fig.suptitle(\\"Average Body Mass by Species and Sex\\", y=1.05) # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_bar_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a faceted bar plot g = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", col=\\"species\\", height=4, aspect=0.6, ci=\\"sd\\", palette=\\"muted\\", edgecolor=\\".6\\", linewidth=1.5, ) # Add consistent labels and a title g.set_axis_labels(\\"Sex\\", \\"Average Body Mass (g)\\") g.fig.suptitle(\\"Average Body Mass by Species and Sex\\", y=1.05) # Display the plot plt.show()"},{"question":"# Question You are tasked with developing a function to parse complex shell command strings into their respective components. Your function should handle quotes, escape characters, and common shell punctuation accurately, much like a Unix shell would. This will require a solid understanding of the `shlex` module and its features. Function Signature ```python def parse_shell_command(command: str, posix: bool = True, punctuation_chars: bool = False) -> list: ``` Input - `command` (str): A shell command string that may contain quotes, escape characters, and punctuation. - `posix` (bool): An optional parameter indicating whether to use POSIX parsing rules. Default is True. - `punctuation_chars` (bool or str): An optional parameter specifying whether to handle shell punctuation characters. If set to True, a default set of punctuation characters is used. Alternatively, a specific string of characters can be passed to determine which characters are treated as punctuation. Default is False. Output - A list of strings, where each string is a token parsed from the `command`. Each token should respect the parsing rules defined by the `shlex` module for `posix` and `punctuation_chars`. Constraints - The function must use the `shlex` module. - The function should gracefully handle edge cases, such as empty inputs, mismatched quotes, and escaped characters following the provided rules. - The function should not raise exceptions and should handle errors by returning an empty list. Examples ```python # Example 1 command = \'echo \\"Hello, World!\\"\' print(parse_shell_command(command)) # Expected output: [\'echo\', \'Hello, World!\'] # Example 2 command = \'ls -l; echo \\"Goodbye!\\" && rm -rf /tmp/*\' print(parse_shell_command(command, punctuation_chars=True)) # Expected output: [\'ls\', \'-l\', \';\', \'echo\', \'Goodbye!\', \'&&\', \'rm\', \'-rf\', \'/tmp/*\'] # Example 3 command = \'echo \\"It\'s a sunny day\\"\' print(parse_shell_command(command)) # Expected output: [\'echo\', \\"It\'s a sunny day\\"] # Example 4 command = \'cat file1.txt file2.txt > merged.txt\' print(parse_shell_command(command, posix=False)) # Expected output: [\'cat\', \'file1.txt\', \'file2.txt\', \'>\', \'merged.txt\'] ``` Implement the `parse_shell_command` function to meet the requirements specified.","solution":"import shlex def parse_shell_command(command: str, posix: bool = True, punctuation_chars: bool = False) -> list: Parses a complex shell command string into its respective components. Args: command (str): A shell command string that may contain quotes, escape characters, and punctuation. posix (bool): An optional parameter indicating whether to use POSIX parsing rules. Default is True. punctuation_chars (bool or str): An optional parameter specifying whether to handle shell punctuation characters. Default is False. Returns: list: A list of strings, where each string is a token parsed from the command. # Handle punctuation characters if punctuation_chars: if isinstance(punctuation_chars, str): lexer = shlex.shlex(command, posix=posix, punctuation_chars=punctuation_chars) else: lexer = shlex.shlex(command, posix=posix, punctuation_chars=True) else: lexer = shlex.shlex(command, posix=posix) lexer.whitespace_split = True try: parsed_command = list(lexer) except ValueError: # Handle errors like mismatched quotes by returning an empty list return [] return parsed_command"},{"question":"Dual-Version Compatible Function Objective You are required to write a Python function that is compatible with both Python 2.7 and Python 3.5+. This function will process a given input and return an appropriate output, demonstrating your understanding of differences between Python 2 and Python 3, and ensuring compatibility with both versions. Problem Statement Write a function `process_data(input_data)` that takes a single argument `input_data`. The `input_data` will be a string that can represent either text or binary data. The function should: 1. Detect whether the `input_data` is text or binary. 2. If it is text, convert it to uppercase. 3. If it is binary, calculate and return the sum of the byte values. 4. The function should handle both Python 2.7 and Python 3.5+. Input - `input_data`: A string. In Python 2, it can be a `str` or `unicode`. In Python 3, it can be a `str` or `bytes`. Output - If the input is text, return the uppercase version of the text (as a string). - If the input is binary, return the sum of the byte values (as an integer). Constraints 1. Do not use any external libraries or change the Python version. 2. Make sure your code runs without errors in both Python 2.7 and Python 3.5+. 3. Use feature detection methods for compatibility checks. Example ```python # In Python 2.7 print(process_data(\\"hello\\")) # Output: \\"HELLO\\" print(process_data(u\\"hello\\")) # Output: \\"HELLO\\" print(process_data(b\'x01x02x03\')) # Output: 6 # In Python 3.5+ print(process_data(\\"hello\\")) # Output: \\"HELLO\\" print(process_data(b\'x01x02x03\')) # Output: 6 ``` Notes - Pay attention to differences in string and binary handling between Python 2 and 3. - Make sure to handle edge cases, such as empty strings or binary data. ```python def process_data(input_data): # Your implementation here pass ```","solution":"def process_data(input_data): This function processes input_data and returns appropriate output. If input is text, returns its uppercase version. If input is binary, returns the sum of the bytes. Designed to be compatible with both Python 2.7 and Python 3.5+. try: # Python 3 basestring except NameError: basestring = str if isinstance(input_data, basestring) and not isinstance(input_data, bytes): return input_data.upper() elif isinstance(input_data, bytes): return sum(input_data) else: raise TypeError(\\"Invalid input type. Expected string or bytes.\\")"},{"question":"# Advanced Functional Programming Assignment with Python 3.10 **Objective:** Demonstrate your understanding of advanced functional programming concepts in Python 3.10 by implementing a function that processes a sequence of numbers using the \\"itertools,\\" \\"functools,\\" and \\"operator\\" modules. **Problem Statement:** Write a function `process_numbers(numbers: List[int]) -> List[int]` that performs the following operations on a list of integers: 1. Applies a transformation to each number using a given mathematical operation. 2. Filters out numbers that do not satisfy a specific condition. 3. Generates a running total of the remaining numbers. 4. Returns a list of tuples representing the original number, its transformed value, and its running total. **Requirements:** - Use the `map` function to apply a transformation to each number. The transformation should double each number. - Use the `filter` function to remove numbers that are less than 10 after the transformation. - Use the `itertools.accumulate` function to calculate the running total of the remaining numbers. - Use the `operator` module to perform the required arithmetic operations. **Input:** - A list of integers `numbers` such that (1 leq text{len(numbers)} leq 1000) and ( -1000 leq text{numbers[i]} leq 1000 ). **Output:** - A list of tuples where each tuple contains three elements: 1. The original number. 2. The transformed value (double the original number). 3. The running total up to that number after filtering. **Example:** ```python from itertools import accumulate def process_numbers(numbers): # Your implementation here # Example usage input_numbers = [1, 5, 10, 20, -3, 7] result = process_numbers(input_numbers) print(result) # Expected output: [(5, 10, 10), (10, 20, 30), (20, 40, 70), (7, 14, 84)] ``` # Guidelines: - You must use the `map`, `filter`, and `itertools.accumulate` functions. - You should use functions from the `operator` module where appropriate. - Your function should be efficient and work within the given constraints. Ensure your solution is well-documented and includes comments explaining the purpose of each major step in your implementation.","solution":"from itertools import accumulate from operator import mul from typing import List, Tuple def process_numbers(numbers: List[int]) -> List[Tuple[int, int, int]]: # Apply transformation to each number (double each number) transformed = list(map(lambda x: x * 2, numbers)) # Filter out numbers that do not satisfy the condition (less than 10 after transformation) filtered = list(filter(lambda x: x >= 10, transformed)) # Generate running total of the remaining numbers running_total = list(accumulate(filtered)) # Collect original numbers that are part of the remaining numbers result = [(numbers[i], filtered[j], running_total[j]) for i, num in enumerate(transformed) if num in filtered for j in range(len(filtered)) if num == filtered[j]] return result"},{"question":"**Objective:** Write a PyTorch script that performs distributed training of a simple neural network. The script must handle initialization of the distributed process group, maintain checkpoints, and restart from the most recent checkpoint in case of failure. The script should be robust and adhere to the instructions highlighted in the documentation. **Specifications:** 1. The neural network should be a simple feed-forward network. 2. The dataset for training can be generated using random tensors for simplicity. 3. Implement functions to load and save checkpoints. 4. Ensure the script initializes the distributed process group correctly, without manually passing `RANK`, `WORLD_SIZE`, `MASTER_ADDR`, and `MASTER_PORT`. 5. Use environmental variables to obtain the `LOCAL_RANK`. 6. The script should save a checkpoint at the end of each epoch. **Input:** You will not have specific inputs, but you need to ensure your script can be run with `torchrun` for distributed training. **Output:** The script should save checkpoints and log the training progress. Check that the checkpoint files exist and the training state (like the current epoch) is updated correctly during restarts. **Constraints:** 1. Your script must handle up to 4 distributed workers. 2. The dataset should be large enough to train for at least 5 epochs. 3. Ensure the script doesn\'t stop in case of worker failure and restarts from the last saved checkpoint. **Performance Requirements:** Your solution should demonstrate an understanding of distributed training, checkpointing, and process management in PyTorch. **Implementation Details:** ```python import os import sys import torch import torch.distributed as dist from torch.nn import functional as F from torch.optim import SGD from torch.utils.data import DataLoader, TensorDataset import argparse def parse_args(args): parser = argparse.ArgumentParser(description=\'Distributed Training Example\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\'Distributed backend\') parser.add_argument(\'--checkpoint_path\', type=str, required=True, help=\'Path to save/load checkpoints\') return parser.parse_args(args) def save_checkpoint(state, path): torch.save(state, path) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) else: return {\'epoch\': 0} def initialize_distributed(args): dist.init_process_group(backend=args.backend) local_rank = int(os.environ[\'LOCAL_RANK\']) torch.cuda.set_device(local_rank) class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = torch.nn.Linear(10, 10) def forward(self, x): return F.relu(self.fc(x)) def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) initialize_distributed(args) model = SimpleModel().cuda() model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[int(os.environ[\'LOCAL_RANK\'])]) optimizer = SGD(model.parameters(), lr=0.001) dataset = TensorDataset(torch.randn(1000, 10), torch.randn(1000, 10)) sampler = torch.utils.data.distributed.DistributedSampler(dataset) loader = DataLoader(dataset, batch_size=32, sampler=sampler) for epoch in range(state[\'epoch\'], 5): model.train() for data, target in loader: data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = model(data) loss = F.mse_loss(output, target) loss.backward() optimizer.step() state[\'epoch\'] = epoch + 1 save_checkpoint(state, args.checkpoint_path) if __name__ == \\"__main__\\": main() ``` Instructions: Write, test, and ensure the script above handles distributed training accurately and efficiently using PyTorch. Verifying on a multi-GPU setup would provide additional confidence in its correctness.","solution":"import os import sys import torch import torch.distributed as dist from torch.nn import functional as F from torch.optim import SGD from torch.utils.data import DataLoader, TensorDataset import argparse def parse_args(args): parser = argparse.ArgumentParser(description=\'Distributed Training Example\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\'Distributed backend\') parser.add_argument(\'--checkpoint_path\', type=str, required=True, help=\'Path to save/load checkpoints\') return parser.parse_args(args) def save_checkpoint(state, path): torch.save(state, path) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) else: return {\'epoch\': 0} def initialize_distributed(args): dist.init_process_group(backend=args.backend) local_rank = int(os.environ[\'LOCAL_RANK\']) torch.cuda.set_device(local_rank) class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = torch.nn.Linear(10, 10) def forward(self, x): return F.relu(self.fc(x)) def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) initialize_distributed(args) model = SimpleModel().cuda() model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[int(os.environ[\'LOCAL_RANK\'])]) optimizer = SGD(model.parameters(), lr=0.001) dataset = TensorDataset(torch.randn(1000, 10), torch.randn(1000, 10)) sampler = torch.utils.data.distributed.DistributedSampler(dataset) loader = DataLoader(dataset, batch_size=32, sampler=sampler) for epoch in range(state[\'epoch\'], 5): model.train() for data, target in loader: data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = model(data) loss = F.mse_loss(output, target) loss.backward() optimizer.step() state[\'epoch\'] = epoch + 1 save_checkpoint(state, args.checkpoint_path) if __name__ == \\"__main__\\": main()"},{"question":"You are required to demonstrate your understanding of shallow and deep copying by implementing a class that simulates a social network profile. The class should define its own copy behaviors for shallow and deep copies. # Requirements: 1. **Class Definition:** - Define a class `Profile` with the following attributes: - `username` (a string): the profile\'s username. - `posts` (a list): a list of strings where each string represents a post. - `followers` (a list): a list of `Profile` objects representing the profile\'s followers. 2. **Custom Copying Methods:** - Implement the `__copy__()` method to perform a shallow copy of the `Profile` object. - Implement the `__deepcopy__()` method to perform a deep copy of the `Profile` object. # Input and Output: - **Input:** - There is no direct input to your function. The copy operations will be tested by creating instances of the `Profile` class and invoking the `copy.copy()` and `copy.deepcopy()` functions on them. - **Output:** - The output should match the behavior of shallow and deep copying operations as defined by the `copy` module. # Constraints: - Ensure that the custom shallow copy does not create new objects for mutable elements like lists, but instead references the original objects. - Ensure that the custom deep copy creates new objects for all mutable elements, such as lists and nested Profile objects. # Example: ```python import copy class Profile: def __init__(self, username, posts, followers): self.username = username self.posts = posts self.followers = followers def __copy__(self): # Implement shallow copy logic pass def __deepcopy__(self, memo): # Implement deep copy logic pass # Example usage: # Create a profile profile1 = Profile(\\"user1\\", [\\"Post 1\\", \\"Post 2\\"], []) profile2 = Profile(\\"user2\\", [\\"Post 3\\"], [profile1]) profile1.followers.append(profile2) # Make shallow and deep copies shallow_copied_profile = copy.copy(profile1) deep_copied_profile = copy.deepcopy(profile1) ``` # Explanation: 1. **Shallow Copy:** - The `shallow_copied_profile` should reference the same `posts` and `followers` lists as `profile1`, but be a different object itself. 2. **Deep Copy:** - The `deep_copied_profile` should be completely independent from `profile1`, with entirely new `posts` and `followers` lists, as well as copies of all nested `Profile` objects. Implement the `Profile` class with the custom copy behavior and ensure that it correctly handles shallow and deep copying as per the constraints.","solution":"import copy class Profile: def __init__(self, username, posts, followers): self.username = username self.posts = posts self.followers = followers def __copy__(self): cls = self.__class__ result = cls.__new__(cls) result.username = self.username result.posts = self.posts # reference the same list result.followers = self.followers # reference the same list of followers return result def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result result.username = copy.deepcopy(self.username, memo) result.posts = copy.deepcopy(self.posts, memo) # create a new list result.followers = [copy.deepcopy(follower, memo) for follower in self.followers] # deepcopy each follower return result # Example usage: # Create a profile profile1 = Profile(\\"user1\\", [\\"Post 1\\", \\"Post 2\\"], []) profile2 = Profile(\\"user2\\", [\\"Post 3\\"], [profile1]) profile1.followers.append(profile2) # Make shallow and deep copies shallow_copied_profile = copy.copy(profile1) deep_copied_profile = copy.deepcopy(profile1)"},{"question":"# Partial Dependence and Individual Conditional Expectation Plot Assessment **Objective:** Assess the ability to create, understand, and interpret partial dependence and individual conditional expectation plots using scikit-learn. **Problem:** You are given a dataset containing various features describing houses and their corresponding prices. Your task is to build a machine learning model to predict house prices and create both Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots for key features in the dataset using the scikit-learn package. # Data Description: The dataset (`housing_data.csv`) has the following columns: - `OverallQual`: Overall material and finish quality (integer). - `GrLivArea`: Above grade (ground) living area in square feet (float). - `GarageCars`: Size of garage in car capacity (integer). - `FullBath`: Full bathrooms above grade (integer). - `SalePrice`: Sale price (float). # Requirements: 1. **Load and preprocess the data:** - Load the dataset from `housing_data.csv`. - Split the dataset into features `X` and target `y`. 2. **Train a regression model:** - Train a `GradientBoostingRegressor` model on the dataset. 3. **Create PDP and ICE plots:** - Create a one-way partial dependence plot for the feature `GrLivArea`. - Create a one-way ICE plot for the feature `GrLivArea`. - Create a two-way partial dependence plot for the features `OverallQual` and `GrLivArea`. # Constraints: - Use the convenient function `PartialDependenceDisplay.from_estimator` to create the plots. # Expected Input and Output: - **Input:** The script should read from the file `housing_data.csv` and should not take any additional input. - **Output:** The script should output the PDP and ICE plots as visualizations. # Performance Requirements: - The model training and plot creation should execute efficiently. # Solution Template: ```python import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def run(): # Load dataset df = pd.read_csv(\'housing_data.csv\') # Extract features and target X = df.drop(columns=[\'SalePrice\']) y = df[\'SalePrice\'] # Train Gradient Boosting Regressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Define features for PDP and ICE plots features = [\'GrLivArea\', \'OverallQual\', (\'GrLivArea\', \'OverallQual\')] # Create PDP for GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[0]]) plt.title(\'Partial Dependence Plot for GrLivArea\') plt.show() # Create ICE plot for GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[0]], kind=\'individual\') plt.title(\'Individual Conditional Expectation Plot for GrLivArea\') plt.show() # Create two-way PDP for OverallQual and GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[2]]) plt.title(\'Two-way Partial Dependence Plot for OverallQual and GrLivArea\') plt.show() if __name__ == \\"__main__\\": run() ``` **Notes:** 1. Ensure `housing_data.csv` contains the necessary columns as specified in the data description. 2. Visualize the PDP and ICE plots for analysis.","solution":"import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def run(): # Load dataset df = pd.read_csv(\'housing_data.csv\') # Extract features and target X = df.drop(columns=[\'SalePrice\']) y = df[\'SalePrice\'] # Train Gradient Boosting Regressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Define features for PDP and ICE plots features = [\'GrLivArea\', \'OverallQual\', (\'GrLivArea\', \'OverallQual\')] # Create PDP for GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[0]]) plt.title(\'Partial Dependence Plot for GrLivArea\') plt.show() # Create ICE plot for GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[0]], kind=\'individual\') plt.title(\'Individual Conditional Expectation Plot for GrLivArea\') plt.show() # Create two-way PDP for OverallQual and GrLivArea PartialDependenceDisplay.from_estimator(model, X, features=[features[2]]) plt.title(\'Two-way Partial Dependence Plot for OverallQual and GrLivArea\') plt.show() if __name__ == \\"__main__\\": run()"},{"question":"Coding Assessment Question # Objective: Implement a class that behaves like a specialized dictionary capable of managing nested scopes and contexts, similar to `ChainMap`, but with additional functionality to trace changes in the nested contexts for debugging and traceability. # Problem Statement: You need to implement a class `TraceableChainMap` that behaves like a `ChainMap` but also keeps a log of changes (insertions, updates, deletions) in the mappings. # Requirements: 1. **Class Definition**: `TraceableChainMap` should be a class inheriting from `collections.ChainMap`. 2. **Change Logging**: Implement methods that log every change made to any of the mappings. The log should keep track of operations in the form of a list of strings (for example: `[\\"INSERT:key1:value1\\", \\"UPDATE:key1:value2\\", \\"DELETE:key1\\"]`). 3. **Methods to Implement**: - `__setitem__(self, key, value)`: Insert or update the `key` with `value`. - `__delitem__(self, key)`: Delete the `key`. - `get_log(self)`: Return the log of changes. - Any other methods if required, to fulfill the above requirements. # Input and Output: - **Input**: Operations performed on the `TraceableChainMap` object, such as insertions, updates, and deletions. - **Output**: - The modified `TraceableChainMap` showing current state of mappings after operations. - The log of all changes performed. # Constraints: - You should use only the classes and methods provided by Python\'s standard library. - The class must maintain the performance characteristics of `ChainMap` and should not make any operation slower than O(log n). # Example: ```python # Create initial dictionaries baseline = {\'music\': \'bach\', \'art\': \'rembrandt\'} adjustments = {\'art\': \'van gogh\', \'opera\': \'carmen\'} # Initialize TraceableChainMap with these dictionaries tcm = TraceableChainMap(adjustments, baseline) # Perform operations and log changes tcm[\'music\'] = \'beethoven\' # Update an entry tcm[\'sculpture\'] = \'rodin\' # Insert a new entry del tcm[\'opera\'] # Delete an entry print(tcm.maps) # Output: [{\'art\': \'van gogh\', \'sculpture\': \'rodin\'}, {\'music\': \'beethoven\', \'art\': \'rembrandt\'}] print(tcm.get_log()) # Output: [\\"UPDATE:music:beethoven\\", \\"INSERT:sculpture:rodin\\", \\"DELETE:opera\\"] ``` # Notes: 1. The operations should modify only the first mapping unless specified otherwise. 2. The change log should reflect every operation that changes the state of any of the maps.","solution":"from collections import ChainMap class TraceableChainMap(ChainMap): A class that behaves like a ChainMap but also keeps a log of changes (insertions, updates, deletions) in the mappings. def __init__(self, *maps): super().__init__(*maps) self._log = [] def __setitem__(self, key, value): action = \\"INSERT\\" if key not in self.maps[0] else \\"UPDATE\\" self.maps[0][key] = value self._log.append(f\\"{action}:{key}:{value}\\") def __delitem__(self, key): if key in self.maps[0]: del self.maps[0][key] self._log.append(f\\"DELETE:{key}\\") else: raise KeyError(f\\"Key {key} not found in the first mapping\\") def get_log(self): return self._log"},{"question":"# MPS Device Memory Management in PyTorch In this assessment, you are required to implement a class `MPSManager` that facilitates memory management and random number operations for MPS devices in PyTorch. Class: `MPSManager` Your class should include the following methods: 1. **`__init__(self)`** - Initializes the MPSManager. Stores the number of MPS devices available. 2. **`get_memory_info(self)`** - Returns a dictionary containing: - `current_allocated_memory`: Current allocated memory in bytes on the current device. - `driver_allocated_memory`: Memory allocated by the driver for the current device in bytes. - `recommended_max_memory`: Recommended maximum memory that can be allocated on the current device in bytes. 3. **`set_seed(self, seed)`** - Sets the random seed for the current MPS device using `torch.mps.manual_seed`. 4. **`empty_device_cache(self)`** - Empties the cache of the current device using `torch.mps.empty_cache`. 5. **`profile_memory_usage(self, function_to_profile, *args, **kwargs)`** - Profiles memory usage of a given function. It should: - Start the profiler. - Execute the provided function with the given arguments. - Stop the profiler. - Return a dictionary with `current_allocated_memory` before and after the function execution. Input and Output Formats - `__init__(self)` - Input: None - Output: None - `get_memory_info(self)` - Input: None - Output: Dictionary with keys `current_allocated_memory`, `driver_allocated_memory`, `recommended_max_memory`, each mapping to their respective memory usage in bytes. - `set_seed(self, seed)` - Input: `seed` (int) - The seed value to set. - Output: None - `empty_device_cache(self)` - Input: None - Output: None - `profile_memory_usage(self, function_to_profile, *args, **kwargs)` - Input: - `function_to_profile` (callable) - The function you want to profile. - `*args`, `**kwargs` - Arguments to be passed to the function. - Output: Dictionary with keys `memory_before` and `memory_after`, each mapping to the `current_allocated_memory` in bytes before and after the function execution. Here\'s an example for your reference: ```python import torch.mps class MPSManager: def __init__(self): self.device_count = torch.mps.device_count() def get_memory_info(self): return { \'current_allocated_memory\': torch.mps.current_allocated_memory(), \'driver_allocated_memory\': torch.mps.driver_allocated_memory(), \'recommended_max_memory\': torch.mps.recommended_max_memory() } def set_seed(self, seed): torch.mps.manual_seed(seed) def empty_device_cache(self): torch.mps.empty_cache() def profile_memory_usage(self, function_to_profile, *args, **kwargs): torch.mps.profiler.start() memory_before = torch.mps.current_allocated_memory() function_to_profile(*args, **kwargs) torch.mps.profiler.stop() memory_after = torch.mps.current_allocated_memory() return { \'memory_before\': memory_before, \'memory_after\': memory_after } ``` In this task, make sure to handle any exceptions and edge cases appropriately. Write clean and efficient code making full use of the PyTorch MPS functionalities.","solution":"import torch class MPSManager: def __init__(self): self.device_count = torch.mps.device_count() if torch.has_mps else 0 def get_memory_info(self): if not torch.has_mps or self.device_count == 0: return None return { \'current_allocated_memory\': torch.mps.current_allocated_memory(), \'driver_allocated_memory\': torch.mps.driver_allocated_memory(), \'recommended_max_memory\': torch.mps.recommended_max_memory() } def set_seed(self, seed): if torch.has_mps and self.device_count > 0: torch.manual_seed(seed) torch.mps.manual_seed(seed) def empty_device_cache(self): if torch.has_mps and self.device_count > 0: torch.mps.empty_cache() def profile_memory_usage(self, function_to_profile, *args, **kwargs): if not torch.has_mps or self.device_count == 0: return None memory_before = torch.mps.current_allocated_memory() function_to_profile(*args, **kwargs) memory_after = torch.mps.current_allocated_memory() return { \'memory_before\': memory_before, \'memory_after\': memory_after }"},{"question":"# Question: Novelty Detection with Local Outlier Factor You are provided with a training dataset that is free from outliers and a testing dataset that may contain new observations which could be outliers. Your task is to implement novelty detection using scikit-learn\'s `LocalOutlierFactor`. # Requirements: 1. **Function Definition**: - Define a function `novelty_detection(train_data: np.ndarray, test_data: np.ndarray, n_neighbors: int) -> np.ndarray` that: - Takes in two numpy arrays: `train_data` (training dataset) and `test_data` (dataset for detection). - A parameter `n_neighbors` that defines the number of neighbors to use for calculating the Local Outlier Factor. 2. **Implementation**: - Instantiate the `LocalOutlierFactor` estimator with parameter `novelty=True`. - Fit the model using the training data. - Predict if the observations in the test data are inliers or outliers using `predict` method. 3. **Output**: - The function should return a numpy array with predictions for the test data, with `1` indicating inliers and `-1` indicating outliers. # Example: ```python import numpy as np from sklearn.neighbors import LocalOutlierFactor def novelty_detection(train_data: np.ndarray, test_data: np.ndarray, n_neighbors: int) -> np.ndarray: # Implement the model for novelty detection lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True) lof.fit(train_data) predictions = lof.predict(test_data) return predictions # Sample data train_data = np.array([[1, 1], [2, 2], [2, 3], [3, 3]]) test_data = np.array([[1, 1], [10, 10]]) # Run the function print(novelty_detection(train_data, test_data, n_neighbors=2)) ``` # Constraints: - The train and test datasets will be 2D numpy arrays with numeric values. - You can assume that there is no missing data in the datasets. # Notes: - Pay special attention to the constraints of `LocalOutlierFactor` in novelty detection mode, as detailed in the provided documentation. **This task is designed to test your understanding of novelty detection using scikit-learn\'s `LocalOutlierFactor`, proper implementation of the fitting and prediction methods, and handling of new data observations.**","solution":"import numpy as np from sklearn.neighbors import LocalOutlierFactor def novelty_detection(train_data: np.ndarray, test_data: np.ndarray, n_neighbors: int) -> np.ndarray: Detects outliers in test_data using Local Outlier Factor model fitted on train_data. Parameters: train_data (np.ndarray): Training dataset without outliers. test_data (np.ndarray): Dataset for predicting inliers and outliers. n_neighbors (int): Number of neighbors to use for the LOF calculation. Returns: np.ndarray: Array with predictions for the test data, where `1` indicates inlier and `-1` indicates outlier. lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True) lof.fit(train_data) predictions = lof.predict(test_data) return predictions"},{"question":"# Multi-layer Perceptron Classifier Implementation You are tasked with creating a mini solution to perform classification using the `MLPClassifier` from the `scikit-learn` library. This exercise will involve loading a dataset, training the model, and evaluating its performance. Requirements: 1. **Load and preprocess the data**: - Use the `load_iris` dataset from `sklearn.datasets`. - Normalize the dataset features using `StandardScaler`. 2. **Model Training**: - Initialize an `MLPClassifier` with the following parameters: - Hidden layer sizes: (10,) - Solver: \'adam\' - Maximum iterations: 300 - Random state: 42 - Train the classifier using the provided training data. 3. **Model Evaluation**: - Predict the class labels for the test set. - Measure the accuracy of the classifier. - Output the confusion matrix for a detailed error analysis. Implementation Details: - You must implement the `train_mlp_classifier` function which takes no input but follows these steps: - Load and preprocess the data. - Split the data into training and testing sets (80% training, 20% testing). - Train the `MLPClassifier`. - Evaluate and print the accuracy score and confusion matrix. Function Signature: ```python def train_mlp_classifier(): pass ``` Outputs: - Print the accuracy score of the model on the test set. - Print the confusion matrix for the test set predictions. Example: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def train_mlp_classifier(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Normalize the dataset scaler = StandardScaler().fit(X) X_scaled = scaler.transform(X) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Initialize and train the MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', max_iter=300, random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Output results print(f\'Accuracy: {accuracy:.2f}\') print(\'Confusion Matrix:\') print(conf_matrix) # Example usage train_mlp_classifier() ``` Note: Your function should execute similar to the example given above but without direct calling; it should be designed to follow the steps outlined in the requirements.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def train_mlp_classifier(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Normalize the dataset scaler = StandardScaler().fit(X) X_scaled = scaler.transform(X) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Initialize and train the MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', max_iter=300, random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Output results print(f\'Accuracy: {accuracy:.2f}\') print(\'Confusion Matrix:\') print(conf_matrix)"},{"question":"You are tasked with designing a plugin architecture for a file processing system. The system needs to support various types of file processors (e.g., CSV, JSON, XML), each implementing its own logic for reading and processing files. The main goals are to enforce a consistent interface for all file processors and allow the system to register new processors dynamically. Requirements 1. Define an abstract base class called `FileProcessor` that enforces the following interface: - `read(self, file_path: str) -> None`: An abstract method for reading a file given its path. - `process(self) -> None`: An abstract method for processing the data read from the file. 2. Implement two concrete file processors: - `CSVFileProcessor` that reads and processes CSV files. - `JSONFileProcessor` that reads and processes JSON files. 3. Create a registry for file processors using a dictionary to keep track of the registered processors by file type. 4. Implement a function `register_processor(file_type: str, processor: Type[FileProcessor])` to register processors dynamically. 5. Implement a function `get_processor(file_type: str) -> Optional[Type[FileProcessor]]` to retrieve a registered processor by file type. # Instructions 1. Define the `FileProcessor` abstract base class using the `abc` module. 2. Implement `CSVFileProcessor` and `JSONFileProcessor` classes that inherit from `FileProcessor` and implement the required methods. 3. Implement the `register_processor` and `get_processor` functions to manage the registry of file processors. 4. Demonstrate the registration and usage of `CSVFileProcessor` and `JSONFileProcessor`. # Example ```python from abc import ABC, abstractmethod from typing import Type, Optional class FileProcessor(ABC): @abstractmethod def read(self, file_path: str) -> None: pass @abstractmethod def process(self) -> None: pass class CSVFileProcessor(FileProcessor): def read(self, file_path: str) -> None: print(f\'Reading CSV file from {file_path}\') # Simulate reading of CSV file self.data = \'csv_data\' def process(self) -> None: print(f\'Processing data: {self.data}\') class JSONFileProcessor(FileProcessor): def read(self, file_path: str) -> None: print(f\'Reading JSON file from {file_path}\') # Simulate reading of JSON file self.data = \'json_data\' def process(self) -> None: print(f\'Processing data: {self.data}\') # Registry to manage processors processor_registry = {} def register_processor(file_type: str, processor: Type[FileProcessor]) -> None: processor_registry[file_type] = processor def get_processor(file_type: str) -> Optional[Type[FileProcessor]]: return processor_registry.get(file_type) # Register processors register_processor(\'csv\', CSVFileProcessor) register_processor(\'json\', JSONFileProcessor) # Demonstration processor_class = get_processor(\'csv\') if processor_class: processor = processor_class() processor.read(\'/path/to/file.csv\') processor.process() processor_class = get_processor(\'json\') if processor_class: processor = processor_class() processor.read(\'/path/to/file.json\') processor.process() ``` # Constraints - Each processor must implement both `read` and `process` methods. - The system should support dynamic registration of new processors without modifying the registry code. # Notes - Use the `abc` module to define the abstract base class. - Implement error handling as needed for file operations. - Ensure that the implemented processors conform to the interface defined by the abstract base class.","solution":"from abc import ABC, abstractmethod from typing import Type, Optional class FileProcessor(ABC): @abstractmethod def read(self, file_path: str) -> None: pass @abstractmethod def process(self) -> None: pass class CSVFileProcessor(FileProcessor): def read(self, file_path: str) -> None: print(f\'Reading CSV file from {file_path}\') # Simulate reading of CSV file self.data = \'csv_data\' def process(self) -> None: print(f\'Processing data: {self.data}\') class JSONFileProcessor(FileProcessor): def read(self, file_path: str) -> None: print(f\'Reading JSON file from {file_path}\') # Simulate reading of JSON file self.data = \'json_data\' def process(self) -> None: print(f\'Processing data: {self.data}\') # Registry to manage processors processor_registry = {} def register_processor(file_type: str, processor: Type[FileProcessor]) -> None: processor_registry[file_type] = processor def get_processor(file_type: str) -> Optional[Type[FileProcessor]]: return processor_registry.get(file_type) # Register processors register_processor(\'csv\', CSVFileProcessor) register_processor(\'json\', JSONFileProcessor) # Demonstration processor_class = get_processor(\'csv\') if processor_class: processor = processor_class() processor.read(\'/path/to/file.csv\') processor.process() processor_class = get_processor(\'json\') if processor_class: processor = processor_class() processor.read(\'/path/to/file.json\') processor.process()"},{"question":"# **Distributed Data Parallel (DDP) Training with Custom Optimization** **Objective:** Implement a PyTorch function to perform distributed training on a custom neural network model using `torch.nn.parallel.DistributedDataParallel` (DDP). Your implementation should demonstrate an understanding of DDP model construction, gradient synchronization, and optimizer step execution. **Detailed Description:** 1. **Function Signature:** ```python def train_model(rank, world_size, epochs, ddp_model_class, dataset_fn, loss_fn, optimizer_class, lr): Perform distributed training on the given model using DDP. Args: rank (int): Rank of the current process. world_size (int): Total number of processes. epochs (int): Number of training epochs. ddp_model_class (class): The model class to be used for training. dataset_fn (function): A function that returns the training dataset. loss_fn (torch.nn.Module): Loss function to be used during training. optimizer_class (torch.optim.Optimizer): Optimizer class to be used for training. lr (float): Learning rate for the optimizer. Returns: None ``` 2. **Input:** - `rank`: The rank of the current process. - `world_size`: The total number of processes. - `epochs`: The number of epochs for which the model should be trained. - `ddp_model_class`: The model class that needs to be wrapped and trained via DDP. - `dataset_fn`: A dataset function that returns the training dataset. - `loss_fn`: The loss function to be used for computing the loss during the training. - `optimizer_class`: The optimizer class to be used for optimizing the model parameters. - `lr`: The learning rate for the optimizer. 3. **Output:** - The function should not return anything. It should perform the training and update the model parameters. 4. **Constraints:** - You must use `torch.nn.parallel.DistributedDataParallel` for model parallelism. - Ensure proper synchronization and communication between processes using `torch.distributed` package. - Utilize the provided optimizer and learning rate for parameter updates. - You must handle the initialization of the process group and ensure its proper cleanup after the training. - Optional: Apply TorchDynamo DDPOptimizer for performance optimization. 5. **Example:** ```python import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, TensorDataset class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def create_dataset(): inputs = torch.randn(100, 10) targets = torch.randn(100, 10) return TensorDataset(inputs, targets) loss_function = nn.MSELoss() def train_model(rank, world_size, epochs, ddp_model_class, dataset_fn, loss_fn, optimizer_class, lr): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) dataset = dataset_fn() dataloader = DataLoader(dataset, batch_size=16, shuffle=True) model = ddp_model_class().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optimizer_class(ddp_model.parameters(), lr=lr) for epoch in range(epochs): for inputs, targets in dataloader: inputs = inputs.to(rank) targets = targets.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() dist.destroy_process_group() def main(): world_size = 2 epochs = 10 lr = 0.01 mp.spawn(train_model, args=(world_size, epochs, CustomModel, create_dataset, loss_function, optim.Adam, lr), nprocs=world_size, join=True) if __name__ == \\"__main__\\": os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" main() ``` In this example, we define a custom model, dataset creation function, and the training process using DDP. The `train_model` function sets up the process group, initializes the DDP model, and performs training with gradient synchronization across processes. **Notes:** - Ensure to handle the initialization and cleanup of the process group appropriately. - Make sure to test your function using a variety of model architectures and datasets to verify correctness. - Optionally, you can add support for DDP with TorchDynamo for improved performance.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, TensorDataset def train_model(rank, world_size, epochs, ddp_model_class, dataset_fn, loss_fn, optimizer_class, lr): Perform distributed training on the given model using DDP. Args: rank (int): Rank of the current process. world_size (int): Total number of processes. epochs (int): Number of training epochs. ddp_model_class (class): The model class to be used for training. dataset_fn (function): A function that returns the training dataset. loss_fn (torch.nn.Module): Loss function to be used during training. optimizer_class (torch.optim.Optimizer): Optimizer class to be used for training. lr (float): Learning rate for the optimizer. Returns: None dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(42) dataset = dataset_fn() dataloader = DataLoader(dataset, batch_size=16, shuffle=True) model = ddp_model_class().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optimizer_class(ddp_model.parameters(), lr=lr) for epoch in range(epochs): for inputs, targets in dataloader: inputs, targets = inputs.to(rank), targets.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() dist.destroy_process_group() class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def create_dataset(): inputs = torch.randn(100, 10) targets = torch.randn(100, 10) return TensorDataset(inputs, targets) def main(): world_size = 2 epochs = 2 lr = 0.01 mp.spawn(train_model, args=(world_size, epochs, CustomModel, create_dataset, nn.MSELoss(), optim.Adam, lr), nprocs=world_size, join=True) if __name__ == \\"__main__\\": os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" main()"},{"question":"# PyTorch Coding Assessment **Objective**: Demonstrate your understanding of PyTorch tensor views, their effects, and how to handle tensor contiguity efficiently. **Problem Statement**: Write a function `optimize_tensor_operations` that takes a single 3-dimensional tensor as input and performs the following operations: 1. Create a transposed view of dimensions `(1, 2, 0)`. 2. Slice the transposed view to extract every other element along the second dimension. 3. Check and ensure the resulting tensor is contiguous for optimal performance. 4. Return the final contiguous tensor. **Function Signature**: ```python import torch def optimize_tensor_operations(tensor: torch.Tensor) -> torch.Tensor: pass ``` **Input**: - `tensor` (torch.Tensor): A 3-dimensional tensor of size `(D1, D2, D3)` where each dimension size is between 5 and 100. **Output**: - Returns a contiguous tensor after performing the specified operations. **Constraints**: - The function should handle tensors of any dtype. - Ensure the operations are efficient and do not involve unnecessary data copies. **Example**: ```python >>> t = torch.rand(10, 20, 30) # Generate a random tensor >>> result = optimize_tensor_operations(t) >>> result.is_contiguous() True ``` **Notes**: - Ensure that the view operations do not create unnecessary copies of data. - The final tensor should retain the same values but might have different shapes and strides depending on the operations. **Hints**: - Use `tensor.transpose` for transposing dimensions. - Utilize slicing to extract elements efficiently. - Employ the `.contiguous()` method to enforce contiguity if needed. Good luck, and happy coding!","solution":"import torch def optimize_tensor_operations(tensor: torch.Tensor) -> torch.Tensor: # Step 1: Create a transposed view of dimensions `(1, 2, 0)` transposed_tensor = tensor.permute(1, 2, 0) # Step 2: Slice the transposed view to extract every other element along the second dimension sliced_tensor = transposed_tensor[:, ::2, :] # Step 3: Ensure the resulting tensor is contiguous contiguous_tensor = sliced_tensor.contiguous() # Step 4: Return the final contiguous tensor return contiguous_tensor"},{"question":"Objective You are tasked with implementing a custom floating-point arithmetic function that compensates for the precision errors associated with binary representation of decimal numbers. Your solution should use Python\'s `decimal` module to achieve higher precision arithmetic operations. Task Write a function `custom_sum` that takes a list of decimal numbers (as strings) and returns their precise sum. The function should leverage Python\'s `decimal` module to ensure the accuracy of the summation. Function Signature ```python def custom_sum(decimal_numbers: List[str]) -> str: ... ``` Input Format - `decimal_numbers`: A list of strings, where each string represents a decimal number. For example: [\\"0.1\\", \\"0.2\\", \\"0.3\\"] Output Format - Return the precise sum as a string. Constraints - The list `decimal_numbers` will have at least one element and at most 1000 elements. - Each decimal number in the list will have at most 20 decimal places. Example ```python assert custom_sum([\\"0.1\\", \\"0.2\\", \\"0.3\\"]) == \\"0.6\\" assert custom_sum([\\"1.0000000000000001\\", \\"2.0000000000000002\\"]) == \\"3.0000000000000003\\" assert custom_sum([\\"0.12345678901234567890\\", \\"0.87654321098765432110\\"]) == \\"1.00000000000000000000\\" ``` Explanation 1. The function should accurately sum the decimal numbers while avoiding the precision issues associated with floating-point arithmetic. 2. By using the `decimal` module and its `Decimal` class, the function should ensure that the summation remains precise. ```python from decimal import Decimal from typing import List def custom_sum(decimal_numbers: List[str]) -> str: total = Decimal(\'0.0\') for num in decimal_numbers: total += Decimal(num) return str(total) ``` The provided example problems should help students understand the expected behavior and verify the correctness of their implementation.","solution":"from decimal import Decimal from typing import List def custom_sum(decimal_numbers: List[str]) -> str: Returns the precise sum of a list of decimal numbers represented as strings. total = Decimal(\'0.0\') for num in decimal_numbers: total += Decimal(num) return str(total)"},{"question":"**Question**: Implement a function `calculate_timezone_offset`, which takes two parameters: a datetime string representing a timestamp in UTC and a string representing a time zone key. The function should return the time offset in hours between the given time zone and UTC at the specified timestamp considering any daylight saving transitions. # Function Signature: ```python def calculate_timezone_offset(utc_datetime: str, timezone_key: str) -> float: pass ``` # Input: - `utc_datetime` (str): A string representing a datetime in UTC formatted as \'%Y-%m-%d %H:%M:%S\'. - `timezone_key` (str): A string representing the IANA time zone key (e.g., \'America/Los_Angeles\'). # Output: - Returns a float representing the time offset in hours between the specified time zone and UTC. # Constraints: - You should use the `ZoneInfo` class from the `zoneinfo` module to handle the time zone functionality. - Assume that `utc_datetime` is always a valid datetime string in UTC and `timezone_key` is always a valid IANA time zone key. - You can assume the system has the necessary time zone data available either natively or via the `tzdata` package. # Example: ```python >>> calculate_timezone_offset(\'2021-03-14 10:00:00\', \'America/Los_Angeles\') -7 >>> calculate_timezone_offset(\'2021-11-07 10:00:00\', \'America/Los_Angeles\') -8 ``` # Explanation: - On March 14, 2021, at 10:00 UTC, it is 3:00 AM in Los Angeles time (PST - Pacific Standard Time), which is UTC-7 hours. - On November 7, 2021, at 10:00 UTC, it is 2:00 AM in Los Angeles time (PDT - Pacific Daylight Time), which is UTC-8 hours. # Hints: 1. You can use `datetime.strptime` to parse the `utc_datetime` string into a `datetime` object. 2. The `ZoneInfo` constructor can be used to create a time zone object using the `timezone_key`. 3. Use the `astimezone` method to convert a UTC datetime to the specified time zone. 4. Calculate the offset by comparing the UTC and local datetime objects.","solution":"from datetime import datetime from zoneinfo import ZoneInfo def calculate_timezone_offset(utc_datetime: str, timezone_key: str) -> float: Calculate the time offset in hours between the specified time zone and UTC at the given timestamp. :param utc_datetime: A string representing a datetime in UTC formatted as \'%Y-%m-%d %H:%M:%S\'. :param timezone_key: A string representing the IANA time zone key. :return: A float representing the offset in hours between the specified time zone and UTC. utc_dt = datetime.strptime(utc_datetime, \'%Y-%m-%d %H:%M:%S\').replace(tzinfo=ZoneInfo(\'UTC\')) local_dt = utc_dt.astimezone(ZoneInfo(timezone_key)) # Calculate offset in hours offset = (local_dt.utcoffset().total_seconds()) / 3600 return offset"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of the novelty detection mechanism using scikit-learn\'s `LocalOutlierFactor` and `OneClassSVM`. You will implement a function to train models on given training data, predict outliers in test data, and evaluate their performance. # Problem Statement: Implement a function `novelty_detection(train_data, test_data, contamination)` that takes in the following inputs: 1. `train_data`: A 2D list or numpy array of the training data (shape: n_samples x n_features) containing no outliers. 2. `test_data`: A 2D list or numpy array of the test data (shape: m_samples x n_features) which may contain outliers. 3. `contamination`: A float representing the proportion of outliers in the test data (range: 0 < contamination < 0.5). The function should: 1. Train both `LocalOutlierFactor` and `OneClassSVM` models on the given training data. 2. Predict outliers in the test data using both trained models. 3. Return a dictionary with prediction results and evaluation metrics. # Expected Output: The function should return a dictionary with the following keys: - `\'LOF_predictions\'`: A list of predictions (-1 for outlier, 1 for inlier) from Local Outlier Factor. - `\'OCSVM_predictions\'`: A list of predictions (-1 for outlier, 1 for inlier) from One-Class SVM. - `\'LOF_score_samples\'`: A list of outlier scores from Local Outlier Factor\'s `score_samples` method. - `\'OCSVM_decision_function\'`: A list of decision function values from One-Class SVM\'s `decision_function` method. - `\'LOF_outlier_ratio\'`: A float representing the ratio of detected outliers to total test samples using LOF. - `\'OCSVM_outlier_ratio\'`: A float representing the ratio of detected outliers to total test samples using One-Class SVM. # Constraints: - The contamination parameter is essential to set when instantiating the models. - Implement exception handling to manage common errors (such as incorrect input shapes). - Optimize your code for performance, considering typical data sizes used in anomaly detection. # Function Signature: ```python import numpy as np from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM def novelty_detection(train_data: np.ndarray, test_data: np.ndarray, contamination: float) -> dict: # Your implementation here pass ``` # Example: ```python train_data = [[0.1, 0.2], [0.1, 0.3], [0.2, 0.2], [0.2, 0.3], [0.3, 0.2], [0.3, 0.3]] test_data = [[0.1, 0.2], [1.0, 0.5], [0.2, 0.3], [0.2, 0.2], [50, 50], [0.3, 0.2]] contamination = 0.2 result = novelty_detection(np.array(train_data), np.array(test_data), contamination) print(result) ``` The provided input should yield results indicating which samples in the `test_data` are detected as outliers by each model, their corresponding scores, and outlier detection ratios.","solution":"import numpy as np from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM def novelty_detection(train_data: np.ndarray, test_data: np.ndarray, contamination: float) -> dict: Conduct novelty detection using Local Outlier Factor and One-Class SVM. Parameters: train_data (np.ndarray): Training data containing no outliers. test_data (np.ndarray): Test data that may contain outliers. contamination (float): Proportion of outliers in the test data. Returns: dict: A dictionary containing prediction results and evaluation metrics. # Instantiate the models lof = LocalOutlierFactor(novelty=True, contamination=contamination) ocsvm = OneClassSVM(nu=contamination, kernel=\\"rbf\\", gamma=\\"auto\\") # Train the models lof.fit(train_data) ocsvm.fit(train_data) # Make predictions lof_predictions = lof.predict(test_data) ocsvm_predictions = ocsvm.predict(test_data) # Obtain scores lof_scores = lof.decision_function(test_data) ocsvm_scores = ocsvm.decision_function(test_data) # Calculate outlier ratios lof_outliers_ratio = np.sum(lof_predictions == -1) / len(test_data) ocsvm_outliers_ratio = np.sum(ocsvm_predictions == -1) / len(test_data) return { \'LOF_predictions\': lof_predictions.tolist(), \'OCSVM_predictions\': ocsvm_predictions.tolist(), \'LOF_score_samples\': lof_scores.tolist(), \'OCSVM_decision_function\': ocsvm_scores.tolist(), \'LOF_outlier_ratio\': lof_outliers_ratio, \'OCSVM_outlier_ratio\': ocsvm_outliers_ratio }"},{"question":"You have been provided with the Titanic dataset, a famous dataset that contains information about the passengers on the Titanic. Your task is to create a detailed visualization using Seaborn\'s `objects` interface to display the distribution and survival details of the passengers based on various attributes. Requirements: 1. Load the Titanic dataset using `seaborn.load_dataset(\'titanic\')`. 2. Create a plot that achieves the following: - Visualizes the distribution of passengers\' age using a histogram. - Facets the histogram by the `sex` attribute to separate the data by gender. - Differentiates the passengers based on whether they survived or not by using the `alive` attribute (use different opacities for survived and not survived). - Adds a bar plot to show the count of passengers in different classes (`class`) and colors the bars by gender. Input: - None (The dataset is loaded within the function). Output: - A visualization as described in the requirements. Implementation: Write a function `titanic_visualization()` which: - Takes no parameters. - Loads the Titanic dataset. - Creates the required visualizations using Seaborn\'s `objects` interface. - Shows the generated plot. Example usage: ```python import seaborn.objects as so from seaborn import load_dataset def titanic_visualization(): # Your implementation here. titanic_visualization() ``` Constraints: - Use Seaborn version 0.11.2 or later. - Ensure your code runs efficiently and is well-documented. Note: Below is an example code demonstrating the loading and initial sorting of the dataset. ```python import seaborn.objects as so from seaborn import load_dataset def titanic_visualization(): titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Further implementation details go here. ``` Ensure that your final plot is clear and appropriately labeled, providing meaningful insights into the Titanic dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_visualization(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a figure to hold the subplots fig, ax = plt.subplots(2, 1, figsize=(10, 12)) # Histogram of passenger ages faceted by sex sns.histplot(data=titanic, x=\'age\', hue=\'alive\', multiple=\'stack\', kde=False, palette=\'dark\', element=\'step\', ax=ax[0]).set(title=\'Age Distribution by Survival\') ax[0].set(xlabel=\'Age\', ylabel=\'Count\') # Bar plot for passenger count by class and differentiated by gender sns.countplot(data=titanic, x=\'class\', hue=\'sex\', palette=\'viridis\', ax=ax[1]).set(title=\'Passenger Class Distribution by Gender\') ax[1].set(xlabel=\'Class\', ylabel=\'Count\') # Adjust layout to avoid overlap plt.tight_layout() # Display the plot plt.show()"},{"question":"# Pandas Styling Assessment Question Objective: You are given a dataset containing sales data for a company. Your task is to create a styled DataFrame that highlights certain cells, applies conditional formatting, and then exports the styled DataFrame to an HTML file. Instructions: 1. **DataFrame Construction**: - Construct a DataFrame `df` with the following data: ```python data = { \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Sales Q1\': [150, 200, 300, 280], \'Sales Q2\': [210, 180, 310, 320], \'Sales Q3\': [190, 220, 280, 300], \'Sales Q4\': [200, 210, 330, 310], } ``` 2. **Styler Application**: - Create a `Styler` object from the DataFrame. - Highlight the maximum sales value in each column. - Apply a background gradient to the sales columns to visually represent the sales data distribution. - Format the sales values to include a dollar sign and comma as a thousand separator (e.g., 1,000). 3. **Export to HTML**: - Export the styled DataFrame to an HTML file named `styled_sales.html`. Constraints: - You should only use the methods available in the `pandas.io.formats.style.Styler` class for styling. - Ensure your code is efficient and avoids redundancy. Input: Your function should not take any input. Instead, the DataFrame should be created within the function using the provided data. Output: The function should create a file `styled_sales.html` in the working directory containing the styled DataFrame. Function Signature: ```python def style_and_export_sales_data(): pass ``` # Example: After executing `style_and_export_sales_data()`, an HTML file `styled_sales.html` should be created with the DataFrame styled accordingly.","solution":"import pandas as pd def style_and_export_sales_data(): data = { \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Sales Q1\': [150, 200, 300, 280], \'Sales Q2\': [210, 180, 310, 320], \'Sales Q3\': [190, 220, 280, 300], \'Sales Q4\': [200, 210, 330, 310], } df = pd.DataFrame(data) # Create styler object styler = df.style # Highlight the maximum sales value in each column styler = styler.highlight_max(axis=0) # Apply a background gradient sales_columns = [\'Sales Q1\', \'Sales Q2\', \'Sales Q3\', \'Sales Q4\'] styler = styler.background_gradient(subset=sales_columns, cmap=\'viridis\') # Format the sales values styler = styler.format({ \'Sales Q1\': \'{:,.0f}\', \'Sales Q2\': \'{:,.0f}\', \'Sales Q3\': \'{:,.0f}\', \'Sales Q4\': \'{:,.0f}\', }) # Export to HTML styler.to_html(\'styled_sales.html\')"},{"question":"**Objective:** Evaluation of competence in the `ftplib` module, FTP operations, and error handling. # Problem Statement You are required to write a Python function that connects to an FTP server, changes to a specified directory, retrieves all files with a given extension, and saves them locally. Use the `ftplib` module to achieve this. # Function Signature ```python def fetch_files_from_ftp(server: str, username: str, password: str, directory: str, file_extension: str, local_path: str) -> None: pass ``` # Input - `server` (str): The address of the FTP server. Example: `\'ftp.us.debian.org\'`. - `username` (str): The username to log in to the FTP server. Example: `\'anonymous\'`. - `password` (str): The password to log in to the FTP server. Example: `\'anonymous@\'`. - `directory` (str): The directory on the FTP server to navigate to. Example: `\'debian\'`. - `file_extension` (str): The file extension to filter by. Example: `\'.txt\'`. - `local_path` (str): The local directory path to save the downloaded files. Example: `\'/home/user/downloads/\'`. # Output - None. The function will save the matching files to the given local directory. # Constraints - The function should handle any errors and exceptions gracefully, ensuring the application does not crash under normal error conditions like wrong login credentials, non-existent directory, etc. - The function should ensure the server connection is properly closed in all cases, whether the operation succeeds or fails. # Example Usage ```python fetch_files_from_ftp( server=\'ftp.us.debian.org\', username=\'anonymous\', password=\'anonymous@\', directory=\'debian\', file_extension=\'.txt\', local_path=\'/home/user/downloads/\' ) ``` # Notes - For retrieving the files, use the `retrbinary` method. - Ensure there is a check that filters files based on the `file_extension` before downloading. - Consider using the `os` module to handle local directory operations such as checking if the directory exists, creating directories, etc. **Additional Requirements** - **Error Handling:** Implement robust error handling that catches and logs appropriate exceptions: connection issues, login failures, file retrieval issues, and directory navigation errors. - **Documentation:** Provide comments and documentation within the code to explain key operations and decisions. This exercise tests the student\'s ability to: 1. Use the `ftplib` package to interact with an FTP server. 2. Handle different methods provided in the `ftplib` module. 3. Implement robust error handling. 4. Ensure the program handles file operations efficiently. 5. Write clean, well-documented code.","solution":"import ftplib import os def fetch_files_from_ftp(server: str, username: str, password: str, directory: str, file_extension: str, local_path: str) -> None: Connects to an FTP server, navigates to a specified directory, retrieves all files with a given extension, and saves them locally. ftp = ftplib.FTP() try: # Connect to the server ftp.connect(server) # Login to the server ftp.login(username, password) # Change to the specified directory ftp.cwd(directory) # Ensure the local path exists if not os.path.exists(local_path): os.makedirs(local_path) # Get a list of files in the directory files = ftp.nlst() for file in files: if file.endswith(file_extension): local_filepath = os.path.join(local_path, file) with open(local_filepath, \'wb\') as local_file: ftp.retrbinary(f\'RETR {file}\', local_file.write) except ftplib.all_errors as e: print(f\\"FTP error: {e}\\") finally: # Ensure the connection is closed properly ftp.quit()"},{"question":"# Question: Implementing Enhanced Interactive Command History Management You have been asked to implement a command history management system using the `readline` module. This system will enhance the default capabilities in two key areas: 1. **Persistent History Management**: This feature requires the system to load command history from a file upon startup and save the updated history back to the file upon exiting the interactive session. 2. **Enhanced Completer Function**: Implement a custom text completer that can be used to provide command suggestions based on a predefined list of commands. Your task is to implement these functionalities and demonstrate their usage in a Python script. Requirements: 1. **Persistent History Management**: - On startup, load history from a file named `.custom_history` located in the user\'s home directory. - Save the history back to the `.custom_history` file when the session ends. - Limit the history file length to 2000 entries. 2. **Enhanced Completer Function**: - Implement a custom completer function that suggests commands from the list [\'start\', \'stop\', \'restart\', \'status\', \'help\']. - Bind the completer function to the Tab key. # Detailed Instruction: 1. Create a function `initialize_history()` that will: - Load history from `.custom_history`. - Set up to save history back to `.custom_history` upon exit. 2. Create a function `custom_completer(text, state)` that: - Returns possible completions for the given `text` based on the predefined command list. 3. Register `custom_completer` with the readline module and bind it to the Tab key. 4. Provide an example showing how these functionalities work in an interactive Python session. Implementation Example: ```python import os import atexit import readline def initialize_history(histfile=os.path.expanduser(\\"~/.custom_history\\"), hist_len=2000): try: readline.read_history_file(histfile) except FileNotFoundError: pass atexit.register(lambda: readline.write_history_file(histfile)) atexit.register(lambda: readline.set_history_length(hist_len)) def custom_completer(text, state): commands = [\'start\', \'stop\', \'restart\', \'status\', \'help\'] options = [cmd for cmd in commands if cmd.startswith(text)] if state < len(options): return options[state] return None def setup_readline(): initialize_history() readline.parse_and_bind(\'tab: complete\') readline.set_completer(custom_completer) # Demonstration if __name__ == \\"__main__\\": setup_readline() while True: try: input_cmd = input(\\"Command: \\") print(f\\"Your command: {input_cmd}\\") except (KeyboardInterrupt, EOFError): print(\\"nExiting...\\") break ``` This example script initializes the history, sets up the custom completer, and runs an interactive loop that accepts commands and provides suggestions based on the predefined command list.","solution":"import os import atexit import readline def initialize_history(histfile=os.path.expanduser(\\"~/.custom_history\\"), hist_len=2000): Initialize command history by loading from the specified file, and set up saving history upon exit. Parameters: histfile (str): Path to the history file. hist_len (int): Maximum number of history entries to save. try: readline.read_history_file(histfile) except FileNotFoundError: pass atexit.register(lambda: readline.write_history_file(histfile)) atexit.register(lambda: readline.set_history_length(hist_len)) def custom_completer(text, state): Custom completer function to provide command suggestions. Parameters: text (str): The current input text. state (int): The index of the suggestion to return. Returns: str: The suggested command or None if no suggestion is available. commands = [\'start\', \'stop\', \'restart\', \'status\', \'help\'] options = [cmd for cmd in commands if cmd.startswith(text)] if state < len(options): return options[state] return None def setup_readline(): Setup readline with custom command history and completer function. initialize_history() readline.parse_and_bind(\'tab: complete\') readline.set_completer(custom_completer) # Demonstration if __name__ == \\"__main__\\": setup_readline() while True: try: input_cmd = input(\\"Command: \\") print(f\\"Your command: {input_cmd}\\") except (KeyboardInterrupt, EOFError): print(\\"nExiting...\\") break"},{"question":"You are asked to utilize the `py_compile` module to create a utility that compiles a list of Python source files into byte-code and generates a report summarizing the results of each compilation. # Task Write a function `compile_files(file_list, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=1)` that takes the following parameters: - `file_list`: A list of strings, where each string is a path to a Python source file that needs to be compiled. - `optimize`: An integer that controls the optimization level (default is -1). - `invalidation_mode`: A string that specifies the invalidation mode for the byte-code cache (default is \'TIMESTAMP\'). - `quiet`: An integer that controls the verbosity of error messages (default is 1). The function should compile each file in the `file_list` using the `py_compile.compile` method. It should return a dictionary where the keys are the file paths from `file_list` and the values are strings that indicate the result of the compilation. The possible results are: - \\"success\\" if the file was compiled without errors. - \\"file_error\\" if there was a `FileExistsError`. - \\"compile_error\\" if there was any other `PyCompileError`. # Constraints 1. If the file list is empty, the function should return an empty dictionary. 2. If an invalidation_mode other than \'TIMESTAMP\', \'CHECKED_HASH\', or \'UNCHECKED_HASH\' is provided, a `ValueError` should be raised. # Example ```python file_list = [\\"script1.py\\", \\"script2.py\\", \\"nonexistent.py\\"] result = compile_files(file_list, optimize=0, invalidation_mode=\'CHECKED_HASH\', quiet=0) print(result) ``` Possible output: ```python { \\"script1.py\\": \\"success\\", \\"script2.py\\": \\"success\\", \\"nonexistent.py\\": \\"file_error\\" } ``` # Notes - Use the `py_compile.PycInvalidationMode` class to handle the invalidation_modes. - Make sure to handle possible errors and return the appropriate result for each file.","solution":"import py_compile from py_compile import PycInvalidationMode def compile_files(file_list, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=1): if invalidation_mode not in [\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\']: raise ValueError(\\"Invalid invalidation_mode. Must be \'TIMESTAMP\', \'CHECKED_HASH\', or \'UNCHECKED_HASH\'\\") invalidation_mapping = { \'TIMESTAMP\': PycInvalidationMode.TIMESTAMP, \'CHECKED_HASH\': PycInvalidationMode.CHECKED_HASH, \'UNCHECKED_HASH\': PycInvalidationMode.UNCHECKED_HASH } results = {} for file in file_list: try: py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, invalidation_mode=invalidation_mapping[invalidation_mode], quiet=quiet) results[file] = \\"success\\" except FileNotFoundError: results[file] = \\"file_error\\" except py_compile.PyCompileError: results[file] = \\"compile_error\\" return results"},{"question":"**Question: Implement an Echo Server using the `selectors` module** Using the `selectors` module in Python, implement an echo server that can handle multiple client connections concurrently. The server should: 1. Accept incoming client connections. 2. Read data from connected clients. 3. Echo the received data back to the respective clients. 4. Handle client disconnections gracefully. # Requirements - You must use `selectors.DefaultSelector` for I/O event multiplexing. - Register the server socket to accept new connections. - For each client connection, register the socket to read incoming data. - Ensure non-blocking operations for both accepting connections and reading from clients. - Use appropriate event handling to write responses back to clients. # Input and Output Formats - **Input**: No direct input from the user, the server should run and handle connections as they come. - **Output**: The server should print meaningful log messages to the console for each significant event (e.g., new connection accepted, data received from a client, data sent back, client disconnection). # Constraints - The server should run indefinitely until manually stopped. - The port number the server listens on should be `12345`. # Example Here is an example structure of what the main loop of your server might look like: ```python import selectors import socket def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'accepted connection from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1024) # Should be ready if data: print(\'echoing\', repr(data), \'to\', conn) conn.send(data) # Hope it won\'t block else: print(\'closing connection\', conn) sel.unregister(conn) conn.close() sel = selectors.DefaultSelector() sock = socket.socket() sock.bind((\'localhost\', 12345)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) print(\'Server started and listening on port 12345\') while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) ``` # Additional Tasks 1. Modify the server to handle large messages that may be received in chunks. 2. Add error handling to manage exceptions that could occur during socket operations. Implement your solution in Python.","solution":"import selectors import socket import types sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() # Should be ready conn.setblocking(False) data = types.SimpleNamespace(addr=addr, inb=b\'\', outb=b\'\') events = selectors.EVENT_READ | selectors.EVENT_WRITE sel.register(conn, events, data=data) print(f\'Accepted connection from {addr}\') def service_connection(key, mask): sock = key.fileobj data = key.data if mask & selectors.EVENT_READ: recv_data = sock.recv(1024) # Should be ready if recv_data: data.inb += recv_data print(f\\"Received data from {data.addr}: {recv_data!r}\\") else: print(f\\"Closing connection to {data.addr}\\") sel.unregister(sock) sock.close() if mask & selectors.EVENT_WRITE and data.inb: sent = sock.send(data.inb) # Should be ready print(f\\"Echoing {data.inb!r} to {data.addr}\\") data.inb = data.inb[sent:] def start_server(): host, port = \'localhost\', 12345 lsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) lsock.bind((host, port)) lsock.listen() print(f\\"Listening on {(host, port)}\\") lsock.setblocking(False) sel.register(lsock, selectors.EVENT_READ, data=None) try: while True: events = sel.select(timeout=None) for key, mask in events: if key.data is None: accept(key.fileobj, mask) else: service_connection(key, mask) except KeyboardInterrupt: print(\\"Server stopped.\\") finally: sel.close() if __name__ == \\"__main__\\": start_server()"},{"question":"Objective You are required to write a Python function using the `grp` module to find the group with the maximum number of user members on a Unix system. Function Signature ```python def get_group_with_max_members() -> tuple: pass ``` Input - There is no direct input to the function since it will leverage the `grp` module to get all group entries. Output - The function should return a tuple representing the group entry with the maximum number of members. The tuple should be in the format: `(gr_name, gr_passwd, gr_gid, gr_mem)`. Requirements - Use the `grp.getgrall()` function to retrieve the list of all group entries. - Identify the group entry with the highest number of user members. - If there are multiple groups with the same maximum number of members, return any one of them. - Ensure to handle the possibility that the `grp` module functions might raise exceptions. Constraints - This function is to be executed in a Unix environment where the `grp` module is available. - Assume that there is at least one group entry returned by `grp.getgrall()`. Example Usage ```python result = get_group_with_max_members() print(result) # Example Output: (\'examplegroup\', \'\', 1001, [\'user1\', \'user2\', \'user3\']) ``` By implementing this function, you should demonstrate your understanding of the `grp` module, handling tuple-like group entries, and processing lists of entries to find specific information.","solution":"import grp def get_group_with_max_members() -> tuple: Returns the group entry with the maximum number of user members on a Unix system. groups = grp.getgrall() max_group = max(groups, key=lambda g: len(g.gr_mem)) return max_group"},{"question":"<|Analysis Begin|> The provided documentation describes two functions within the `torch.utils.dlpack` module: `from_dlpack` and `to_dlpack`. These functions facilitate data interchange between PyTorch tensors and other frameworks that support DLPack, a tensor format standard. - `from_dlpack`: Converts a DLPack tensor to a PyTorch tensor. - `to_dlpack`: Converts a PyTorch tensor to a DLPack tensor. This module is useful for interoperability between different deep learning frameworks, allowing seamless data exchange without the need for intermediate copying. Given the focus on `from_dlpack` and `to_dlpack`, we can design a question to assess students\' knowledge of these functions and their ability to work with tensor conversions, interoperability, and integration of PyTorch with other frameworks. <|Analysis End|> <|Question Begin|> **Question: Cross-Framework Tensor Interoperability with PyTorch** You are required to demonstrate your understanding of tensor interoperability using the `torch.utils.dlpack` module in PyTorch. Specifically, you will write functions to convert tensors back-and-forth between PyTorch and a hypothetical framework that uses DLPack for tensor representation. # Function 1: `convert_to_dlpack` **Input:** - A `torch.Tensor` object. **Output:** - A DLPack tensor (use the `to_dlpack` function to achieve this). **Constraints:** - Ensure the conversion handles a variety of tensor shapes and data types. # Function 2: `convert_from_dlpack` **Input:** - A DLPack tensor (assume it is generated by the hypothetical framework). **Output:** - A `torch.Tensor` object (use the `from_dlpack` function for this conversion). **Constraints:** - Ensure the converted PyTorch tensor correctly represents the original data from the DLPack tensor. # Function 3: `interoperability_test` **Input:** - A `torch.Tensor` object. **Output:** - A boolean indicating whether the tensor, after being converted to a DLPack tensor and back, remains the same as the original. **Steps:** 1. Convert the input tensor to a DLPack tensor using `convert_to_dlpack`. 2. Convert the DLPack tensor back to a PyTorch tensor using `convert_from_dlpack`. 3. Compare the original tensor and the twice-converted tensor for equality. Your task is to implement these three functions. The `interoperability_test` function will help verify the correctness of your conversion functions. # Example: ```python import torch # Implement the required functions here def convert_to_dlpack(tensor): pass def convert_from_dlpack(dlpack_tensor): pass def interoperability_test(tensor): pass # Example test original_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) assert interoperability_test(original_tensor) == True, \\"Test failed\\" print(\\"All tests passed.\\") ``` # Notes: - You do not need access to the underlying details of DLPack as they are abstracted by the `from_dlpack` and `to_dlpack` functions. - Make sure to handle edge cases such as empty tensors, tensors with different data types, and tensors of high dimensionality. Good luck!","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def convert_to_dlpack(tensor): Convert a PyTorch tensor to a DLPack tensor. return to_dlpack(tensor) def convert_from_dlpack(dlpack_tensor): Convert a DLPack tensor to a PyTorch tensor. return from_dlpack(dlpack_tensor) def interoperability_test(tensor): Test if the tensor converted to DLPack and back remains the same as the original. dlpack_tensor = convert_to_dlpack(tensor) converted_back_tensor = convert_from_dlpack(dlpack_tensor) return torch.equal(tensor, converted_back_tensor)"},{"question":"**Advanced Plotting with Seaborn** You are provided with a dataset containing information about restaurant tips. Your task is to use seaborn\'s `objects` module to generate a visually informative plot that accomplishes the following: 1. Load the `tips` dataset from seaborn. 2. Create a `Plot` object to show the relationship between `total_bill` and `tip`. 3. Add a scatter plot layer (`so.Dot`) for the `total_bill` and `tip` relationship with a 50% alpha transparency. 4. Add a linear polynomial fit line (`so.Line`, `so.PolyFit`) to the scatter plot to show the trend. 5. Create subplot facets based on the `day` variable. 6. Map the `sex` variable to different colors within each facet. 7. Ensure that the `pointsize` of the dots represents the `size` variable, and scales between 2 and 10. 8. Annotate the legend to show which plot elements correspond to `Male` and `Female`. The final plot should be facet by `day`, showing each day\'s tips separately with color distinction for `sex`, and should have transparent dots scaled by the party size. **Constraints:** - Use seaborn 0.11.1 or newer. - Clearly label all axes and provide a title for the plot. - The resulting figure should be displayed inline. **Input:** There are no inputs for this task as the dataset is loaded within the code. **Output:** An inline matplotlib plot will be displayed as the final output. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\") .add(so.Dot(alpha=0.5), pointsize=\\"size\\") .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .scale(pointsize=(2, 10)) .label(title=\\"Restaurant Tips by Total Bill and Day\\", x=\\"Total Bill\\", y=\\"Tip\\") ) p ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .add(so.Dot(alpha=0.5), pointsize=\\"size\\") .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .scale(pointsize=(2, 10)) .label(title=\\"Restaurant Tips by Total Bill and Day\\", x=\\"Total Bill\\", y=\\"Tip\\") ) # Display the plot p.show()"},{"question":"# File Monitoring and Command Execution You are required to write a Python script that monitors a specified file for changes and executes a given command whenever the file is modified. Function Signature ```python def monitor_file(filepath: str, command: str) -> None: ``` Input Parameters - `filepath`: A string representing the path to the file that needs to be monitored. - `command`: A string representing the command to be executed when the file is modified. Output The function does not return anything. It should run indefinitely until terminated, monitoring the given file for changes and executing the command whenever a modification is detected. Constraints - Assume the file path is valid and the file exists. - The command should be a valid system command. - The function should handle the file monitoring efficiently. Example Usage Here is an example of how your function should behave: ```python # Assume the file \'example.txt\' is being monitored. monitor_file(\'example.txt\', \'echo \\"File has been modified.\\"\') ``` This script should monitor `example.txt` and print \\"File has been modified.\\" to the console whenever the file contents are changed. Requirements Your solution must demonstrate: - File handling and monitoring using the `os` and `time` modules. - Execution of system commands using the `subprocess` module. - Efficient management of resources and processes. Notes You may use any function or class provided by the Python standard library to implement this behavior. Here\'s a structure to help you get started: ```python import os import time import subprocess def monitor_file(filepath: str, command: str) -> None: # Initial monitoring setup last_modified_time = os.path.getmtime(filepath) while True: current_modified_time = os.path.getmtime(filepath) if current_modified_time != last_modified_time: subprocess.run(command, shell=True) last_modified_time = current_modified_time time.sleep(1) # Example execution monitor_file(\'example.txt\', \'echo \\"File has been modified.\\"\') ```","solution":"import os import time import subprocess def monitor_file(filepath: str, command: str) -> None: Monitors the specified file for changes and executes the given command whenever the file is modified. Parameters: filepath (str): Path to the file to be monitored. command (str): Command to be executed when the file is modified. last_modified_time = os.path.getmtime(filepath) while True: current_modified_time = os.path.getmtime(filepath) if current_modified_time != last_modified_time: subprocess.run(command, shell=True) last_modified_time = current_modified_time time.sleep(1)"},{"question":"# Custom Numeric Class Implementation Background You are tasked with creating a custom numeric class in Python that mimics the behavior of standard Python arithmetic operations using a selection of functions from Python\'s C API (as described in the provided documentation). Objective Your custom numeric class should be able to handle the following operations: addition, subtraction, multiplication, true division, floor division, modulus, negation, and conversion to integer and float. Requirements - Implement a class called `CustomNumber` that accepts a single numeric value during initialization. - Implement the following methods using appropriate Python numeric expressions: - `__add__` for addition - `__sub__` for subtraction - `__mul__` for multiplication - `__truediv__` for true division - `__floordiv__` for floor division - `__mod__` for modulus - `__neg__` for negation - `__int__` for conversion to integer - `__float__` for conversion to float Constraints - Your code should handle edge cases, such as division by zero. - Assume the input values are always numeric. Input and Output - **Input:** Two instances of `CustomNumber` for any binary operation. - **Output:** A new instance of `CustomNumber` as a result of binary operations, or a native Python data type for unary operations like negation and conversions. Example ```python class CustomNumber: def __init__(self, value): # Initialize with a numeric value pass def __add__(self, other): # Implement addition pass def __sub__(self, other): # Implement subtraction pass def __mul__(self, other): # Implement multiplication pass def __truediv__(self, other): # Implement true division pass def __floordiv__(self, other): # Implement floor division pass def __mod__(self, other): # Implement modulus pass def __neg__(self): # Implement negation pass def __int__(self): # Implement conversion to int pass def __float__(self): # Implement conversion to float pass # Example usage a = CustomNumber(10) b = CustomNumber(3) print(a + b) # Should output an instance of CustomNumber representing 13 print(a - b) # Should output an instance of CustomNumber representing 7 print(a * b) # Should output an instance of CustomNumber representing 30 print(a / b) # Should output an instance of CustomNumber representing 3.3333... print(a // b) # Should output an instance of CustomNumber representing 3 print(a % b) # Should output an instance of CustomNumber representing 1 print(-a) # Should output an instance of CustomNumber representing -10 print(int(a)) # Should output 10 print(float(a)) # Should output 10.0 ``` Implement the `CustomNumber` class to complete the given operations.","solution":"class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): return CustomNumber(self.value + other.value) def __sub__(self, other): return CustomNumber(self.value - other.value) def __mul__(self, other): return CustomNumber(self.value * other.value) def __truediv__(self, other): if other.value == 0: raise ZeroDivisionError(\\"division by zero\\") return CustomNumber(self.value / other.value) def __floordiv__(self, other): if other.value == 0: raise ZeroDivisionError(\\"division by zero\\") return CustomNumber(self.value // other.value) def __mod__(self, other): if other.value == 0: raise ZeroDivisionError(\\"modulus by zero\\") return CustomNumber(self.value % other.value) def __neg__(self): return CustomNumber(-self.value) def __int__(self): return int(self.value) def __float__(self): return float(self.value) def __repr__(self): return f\\"CustomNumber({self.value})\\""},{"question":"Objective: Implement a Python class using the `selectors` module to create a simple echo server that: - Accepts multiple client connections. - Reads data from clients. - Echoes the received data back to the clients. Requirements: 1. Implement a class `EchoServer` with the following methods: - `__init__(self, host: str, port: int)`: Initializes the EchoServer to listen on the given host and port. - `start(self)`: Starts the server and handles client connections and I/O events. - `accept(self, sock, mask)`: Handles new client connections. - `read(self, conn, mask)`: Reads data from a client and echoes it back. - `close(self, conn)`: Unregisters and closes a client connection. 2. Use the `DefaultSelector` to manage I/O events. 3. Ensure non-blocking I/O operations are used. 4. Handle client disconnections gracefully. Input: - No direct input is required beyond initializing the EchoServer with a host and port. Output: - The server should print messages indicating accepted connections, echoed messages, and closed connections. Example Usage: ```python server = EchoServer(\'localhost\', 1234) server.start() ``` Implementation Constraints: - Only Python 3.4+ standard library should be used. ```python import selectors import socket class EchoServer: def __init__(self, host: str, port: int): self.sel = selectors.DefaultSelector() self.sock = socket.socket() self.sock.bind((host, port)) self.sock.listen(100) self.sock.setblocking(False) self.sel.register(self.sock, selectors.EVENT_READ, self.accept) def start(self): print(f\'Server started on {self.sock.getsockname()}\') while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) def accept(self, sock, mask): conn, addr = sock.accept() print(f\'Accepted {conn} from {addr}\') conn.setblocking(False) self.sel.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): data = conn.recv(1000) if data: print(f\'Echoing {repr(data)} to {conn}\') conn.send(data) else: print(f\'Closing {conn}\') self.close(conn) def close(self, conn): self.sel.unregister(conn) conn.close() # Example to create and start the server: # server = EchoServer(\'localhost\', 1234) # server.start() ``` Performance: - The implementation should handle multiple client connections smoothly and efficiently echo data back without blocking. This question assesses the understanding of the `selectors` module and requires implementing a real-world application (an echo server) using the provided abstractions for efficient I/O handling.","solution":"import selectors import socket class EchoServer: def __init__(self, host: str, port: int): self.sel = selectors.DefaultSelector() self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.bind((host, port)) self.sock.listen(100) self.sock.setblocking(False) self.sel.register(self.sock, selectors.EVENT_READ, self.accept) def start(self): print(f\'Server started on {self.sock.getsockname()}\') try: while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server stopped by KeyboardInterrupt\\") finally: self.sel.close() def accept(self, sock, mask): conn, addr = sock.accept() print(f\'Accepted {conn} from {addr}\') conn.setblocking(False) self.sel.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): data = conn.recv(1000) if data: print(f\'Echoing {repr(data)} to {conn}\') conn.send(data) else: print(f\'Closing {conn}\') self.close(conn) def close(self, conn): self.sel.unregister(conn) conn.close()"},{"question":"**Concurrency and Asynchronous Execution with concurrent.futures** You\'re tasked with processing a large list of integers to determine their primality and then summing the prime numbers asynchronously for efficient computation. The goal is to make use of both `ThreadPoolExecutor` and `ProcessPoolExecutor` to handle different parts of the task. # Task: 1. **Prime Number Check**: - Write a function `is_prime(n: int) -> bool` that checks if a given number is a prime number. 2. **Asynchronous Execution**: - Use `ThreadPoolExecutor` to asynchronously check if each number in the list is prime. Write a function `check_primes(numbers: List[int], max_workers: int) -> List[bool]` that takes a list of integers and the maximum number of worker threads, and returns a list of booleans indicating primality. 3. **Summing Primes**: - Use `ProcessPoolExecutor` to asynchronously sum the numbers that have been identified as prime. Write a function `sum_primes(prime_flags: List[bool], numbers: List[int], max_workers: int) -> int` that takes the list of boolean flags (indicating primality) and the original list of numbers, and returns the sum of prime numbers. # Requirements: 1. Ensure that `check_primes` and `sum_primes` are implemented efficiently and correctly handle input constraints. 2. The main execution function should manage the overall flow: - Check primality of the list of numbers using `ThreadPoolExecutor`. - Sum the prime numbers using `ProcessPoolExecutor`. # Example: Given the list of numbers `[10, 15, 3, 7, 11, 18, 29]`: ```python from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor def is_prime(n): if n < 2: return False if n == 2: return True if n % 2 == 0: return False for i in range(3, int(n**0.5) + 1, 2): if n % i == 0: return False return True def check_primes(numbers, max_workers): with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(is_prime, num): num for num in numbers} return [future.result() for future in concurrent.futures.as_completed(future_to_number)] def sum_primes(prime_flags, numbers, max_workers): primes = [num for flag, num in zip(prime_flags, numbers) if flag] with ProcessPoolExecutor(max_workers=max_workers) as executor: future = executor.submit(sum, primes) return future.result() def main(numbers, max_workers=4): prime_flags = check_primes(numbers, max_workers) sum_of_primes = sum_primes(prime_flags, numbers, max_workers) return sum_of_primes numbers = [10, 15, 3, 7, 11, 18, 29] print(main(numbers)) ``` Expected Output: ``` 50 # (3 + 7 + 11 + 29) ``` # Constraints: - The number of integers should be between 1 and 100,000. - Each integer should be a positive number less than 1,000,000. - Performance should be considered especially for large inputs. Implement the functions `is_prime`, `check_primes`, `sum_primes`, and integrate them in the `main` function to achieve the desired functionality.","solution":"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from typing import List def is_prime(n: int) -> bool: Returns True if n is a prime number, else False. if n < 2: return False if n == 2: return True if n % 2 == 0: return False for i in range(3, int(n**0.5) + 1, 2): if n % i == 0: return False return True def check_primes(numbers: List[int], max_workers: int) -> List[bool]: Checks the primality of each number in the list using multi-threading. Args: - numbers: List of integers to be checked for primality. - max_workers: Maximum number of threads to use. Returns: - A list of booleans indicating if each corresponding number is prime or not. with ThreadPoolExecutor(max_workers=max_workers) as executor: results = list(executor.map(is_prime, numbers)) return results def sum_primes(prime_flags: List[bool], numbers: List[int], max_workers: int) -> int: Sums the numbers that are identified as prime using multi-processing. Args: - prime_flags: List of booleans indicating primality of numbers. - numbers: Original list of integers. - max_workers: Maximum number of processes to use. Returns: - The sum of prime numbers. primes = [num for flag, num in zip(prime_flags, numbers) if flag] with ProcessPoolExecutor(max_workers=max_workers) as executor: future = executor.submit(sum, primes) return future.result() def main(numbers: List[int], max_workers: int = 4) -> int: Main function to check primes and sum them using concurrent execution. Args: - numbers: List of integers to be processed. - max_workers: Maximum number of workers for ThreadPoolExecutor and ProcessPoolExecutor. Returns: - The sum of prime numbers. prime_flags = check_primes(numbers, max_workers) sum_of_primes = sum_primes(prime_flags, numbers, max_workers) return sum_of_primes # Example usage if __name__ == \\"__main__\\": numbers = [10, 15, 3, 7, 11, 18, 29] print(main(numbers)) # Expected output: 50"},{"question":"# Pandas Indexing and Computation Assessment Objective You are tasked with implementing a function that processes a DataFrame index to perform several transformations and computations, demonstrating comprehension of pandas `Index` manipulations. Function Signature ```python import pandas as pd def process_dataframe_index(df: pd.DataFrame) -> dict: Process the DataFrame index to perform several operations and computations. Parameters: df (pd.DataFrame): Input DataFrame whose index needs to be processed. Returns: dict: A dictionary containing the results of various operations on the DataFrame index. pass ``` Instructions 1. **Check Index Properties**: - Determine if the index is monotonically increasing. - Confirm if the index is unique. - Check if the index contains any NaNs. 2. **Index Manipulations**: - Drop any duplicate indices. - Insert a new value in the index. - Rename the index. 3. **Computations and Conversions**: - Compute the minimum and maximum values of the index. - Convert the index to a list. - Convert the index to a numpy array. 4. **Outcome**: Collect the results of the above operations into a dictionary and return it. Example Usage ```python import pandas as pd # Example DataFrame data = {\'values\': [10, 20, 30, 40, 50]} index = pd.Index([1, 2, 3, 4, 5], name=\'original_index\') df = pd.DataFrame(data, index=index) result = process_dataframe_index(df) # Expected output assert result == { \'is_monotonic_increasing\': True, \'is_unique\': True, \'has_nans\': False, \'index_after_dropping_duplicates\': [1, 2, 3, 4, 5], \'index_after_insertion\': [1, 2, 3, 4, 5, 6], \'new_index_name\': \'processed_index\', \'index_min\': 1, \'index_max\': 6, \'index_as_list\': [1, 2, 3, 4, 5, 6], \'index_as_numpy\': np.array([1, 2, 3, 4, 5, 6]) } ``` Constraints - Assume the DataFrame index is a simple integer index for this task. - Ensure the function handles possible edge cases like empty DataFrame or index without duplicates. Tips - Utilize pandas built-in methods for index operations to maintain efficiency. - Make sure to handle the renaming of the index after manipulations. - Ensure the function is robust and can handle edge cases.","solution":"import pandas as pd import numpy as np def process_dataframe_index(df: pd.DataFrame) -> dict: Process the DataFrame index to perform several operations and computations. Parameters: df (pd.DataFrame): Input DataFrame whose index needs to be processed. Returns: dict: A dictionary containing the results of various operations on the DataFrame index. result = {} # Check Index Properties result[\'is_monotonic_increasing\'] = df.index.is_monotonic_increasing result[\'is_unique\'] = df.index.is_unique result[\'has_nans\'] = df.index.hasnans # Index Manipulations index_after_dropping_duplicates = df.index.drop_duplicates() index_with_insertion = index_after_dropping_duplicates.append(pd.Index([6])) index_with_insertion = index_with_insertion.rename(\'processed_index\') result[\'index_after_dropping_duplicates\'] = index_after_dropping_duplicates.tolist() result[\'index_after_insertion\'] = index_with_insertion.tolist() # Computations and Conversions result[\'new_index_name\'] = index_with_insertion.name result[\'index_min\'] = index_with_insertion.min() result[\'index_max\'] = index_with_insertion.max() result[\'index_as_list\'] = index_with_insertion.tolist() result[\'index_as_numpy\'] = index_with_insertion.to_numpy() return result"},{"question":"Linear Algebra Operations with PyTorch **Objective**: Demonstrate comprehension of fundamental and advanced concepts within the `torch.linalg` module by implementing a function that utilizes multiple linear algebra operations. Problem Statement You are required to implement a function `matrix_operations` that takes a matrix as input and performs a series of linear algebra operations using PyTorch. Specifically, your function should: 1. **Normalize the matrix**: Compute the Frobenius norm of the matrix and normalize the matrix by dividing each element by this norm. 2. **Perform QR Decomposition**: Decompose the normalized matrix into an orthogonal matrix `Q` and an upper triangular matrix `R` using QR decomposition. 3. **Compute Determinant**: Calculate the determinant of the upper triangular matrix `R`. 4. **Solve Linear System**: Given a vector `b`, solve the linear system `Ax = b` using the normalized matrix `A`, where `A` is the input matrix. # Input - `matrix` (`torch.Tensor`): A 2D tensor representing the matrix. Shape: (N, N). - `b` (`torch.Tensor`): A 1D tensor representing the vector. Shape: (N,). # Output - `norm_matrix` (`torch.Tensor`): The normalized matrix. - `Q` (`torch.Tensor`): The orthogonal matrix from QR decomposition. - `R` (`torch.Tensor`): The upper triangular matrix from QR decomposition. - `det_R` (`float`): The determinant of matrix `R`. - `x` (`torch.Tensor`): Solution to the linear system `Ax = b`. # Constraints - The input matrix will always be a square matrix (N x N) with N >= 2. - Matrices and vectors will contain float values. # Performance Requirements - Your implementation should efficiently use PyTorch\'s built-in functions. - Ensure that your solution is numerically stable. # Example ```python import torch matrix = torch.tensor([[2.0, -1.0, 0.0], [-1.0, 2.0, -1.0], [0.0, -1.0, 2.0]]) b = torch.tensor([1.0, 0.0, 1.0]) norm_matrix, Q, R, det_R, x = matrix_operations(matrix, b) print(\\"Normalized Matrix:n\\", norm_matrix) print(\\"Q matrix:n\\", Q) print(\\"R matrix:n\\", R) print(\\"Determinant of R:\\", det_R) print(\\"Solution to Ax = b:\\", x) ```","solution":"import torch def matrix_operations(matrix: torch.Tensor, b: torch.Tensor): Performs a series of linear algebra operations on the input matrix. Arguments: matrix -- 2D tensor, shape (N, N) b -- 1D tensor, shape (N,) Returns: norm_matrix -- The normalized matrix Q -- The orthogonal matrix from QR decomposition R -- The upper triangular matrix from QR decomposition det_R -- The determinant of matrix R x -- Solution to the linear system Ax = b # Step 1: Normalize the matrix using the Frobenius norm frobenius_norm = torch.norm(matrix, p=\'fro\') norm_matrix = matrix / frobenius_norm # Step 2: Perform QR Decomposition Q, R = torch.linalg.qr(norm_matrix) # Step 3: Compute Determinant of R det_R = torch.linalg.det(R) # Step 4: Solve Linear System Ax = b x = torch.linalg.solve(norm_matrix, b) return norm_matrix, Q, R, det_R, x"},{"question":"# Semi-Supervised Learning with scikit-learn Objective Your task is to implement a semi-supervised learning pipeline using the `LabelSpreading` model from scikit-learn. This will involve setting up the data, fitting the model, and evaluating its performance. Problem Statement You are given a dataset that contains both labeled and unlabeled data points. Your goal is to build a classifier that can leverage the unlabeled data to improve its performance. Requirements 1. **Data Loading and Preprocessing**: - Load the given dataset (it contains both labeled and unlabeled data points). - Separate the features (`X`) and labels (`y`). - Identify and mark the unlabeled data points in `y` with `-1`. 2. **Model Implementation**: - Implement a semi-supervised learning model using `LabelSpreading`. - Use the RBF kernel with an appropriate `gamma` value. 3. **Training and Evaluation**: - Fit the model to the entire dataset (including labeled and unlabeled data points). - Evaluate the model performance on a provided test set and print the accuracy score. Input Format - The dataset file is named `data.csv` and contains the following columns: - Features: `feature1`, `feature2`, ..., `featureN` - Label: `label` The `label` column contains -1 for unlabeled data points. - The gamma value for the RBF kernel is provided as a floating-point number: `gamma_value`. - Test dataset is provided as `test_data.csv` with the same structure as `data.csv`. Output Format - Print the accuracy score of the model on the test set. Constraints - You must use `LabelSpreading` from `sklearn.semi_supervised`. - Use the RBF kernel method. Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.semi_supervised import LabelSpreading from sklearn.metrics import accuracy_score # Load the dataset data = pd.read_csv(\'data.csv\') test_data = pd.read_csv(\'test_data.csv\') # Separate features and labels X = data.drop(columns=[\'label\']) y = data[\'label\'] # Implement the LabelSpreading model gamma_value = 0.1 model = LabelSpreading(kernel=\'rbf\', gamma=gamma_value) # Fit the model model.fit(X, y) # Test the model X_test = test_data.drop(columns=[\'label\']) y_test = test_data[\'label\'] # Predict and evaluate y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') ``` Use this template to guide your implementation. Make sure your solution is efficient and follows best practices in handling semi-supervised learning problems with scikit-learn.","solution":"import pandas as pd from sklearn.semi_supervised import LabelSpreading from sklearn.metrics import accuracy_score def semi_supervised_learning(train_file, test_file, gamma_value): # Load the dataset data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) # Separate features and labels X = data.drop(columns=[\'label\']) y = data[\'label\'] # Implement the LabelSpreading model model = LabelSpreading(kernel=\'rbf\', gamma=gamma_value) # Fit the model model.fit(X, y) # Test the model X_test = test_data.drop(columns=[\'label\']) y_test = test_data[\'label\'] # Predict and evaluate y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') return accuracy"},{"question":"# DLPack-PyTorch Tensor Conversion and Operations You are given access to a dataset in the form of DLPack tensors, and your task is to process this data using PyTorch. Specifically, you need to convert the DLPack tensors to PyTorch tensors, perform element-wise addition, and then convert the result back to a DLPack tensor. Please implement the following function: Function Signature ```python def process_dlpack_tensors(dlpack_tensor1, dlpack_tensor2): Converts two DLPack tensors to PyTorch tensors, adds them element-wise, and returns the result as a DLPack tensor. Parameters: dlpack_tensor1 (any): The first input tensor in DLPack format. dlpack_tensor2 (any): The second input tensor in DLPack format. Returns: any: The resulting tensor in DLPack format after element-wise addition. ``` Input - `dlpack_tensor1`: A tensor in DLPack format. - `dlpack_tensor2`: A tensor in DLPack format. Output - A tensor in DLPack format that represents the element-wise addition of the input tensors. Constraints - Assume both input tensors always have the same shape and data type. - The tensors can have any shape but must be convertible between DLPack and PyTorch formats without errors. Example ```python # Example tensors in PyTorch, for demonstration purposes: import torch x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) # Convert to DLPack format dlpack_x = torch.utils.dlpack.to_dlpack(x) dlpack_y = torch.utils.dlpack.to_dlpack(y) # Assume these are the dlpack_tensor1 and dlpack_tensor2 inputs result_dlpack = process_dlpack_tensors(dlpack_x, dlpack_y) # Convert result back to PyTorch tensor to view result_tensor = torch.utils.dlpack.from_dlpack(result_dlpack) print(result_tensor) # Output should be: # tensor([[ 6., 8.], # [10., 12.]]) ``` Note - Make sure to use the `torch.utils.dlpack.from_dlpack` and `torch.utils.dlpack.to_dlpack` functions for conversions. - You can assume that the required packages are already installed and do not need to handle installation within the function.","solution":"import torch def process_dlpack_tensors(dlpack_tensor1, dlpack_tensor2): Converts two DLPack tensors to PyTorch tensors, adds them element-wise, and returns the result as a DLPack tensor. Parameters: dlpack_tensor1 (any): The first input tensor in DLPack format. dlpack_tensor2 (any): The second input tensor in DLPack format. Returns: any: The resulting tensor in DLPack format after element-wise addition. # Convert DLPack tensors to PyTorch tensors tensor1 = torch.utils.dlpack.from_dlpack(dlpack_tensor1) tensor2 = torch.utils.dlpack.from_dlpack(dlpack_tensor2) # Perform element-wise addition result_tensor = tensor1 + tensor2 # Convert the result back to DLPack format result_dlpack_tensor = torch.utils.dlpack.to_dlpack(result_tensor) return result_dlpack_tensor"},{"question":"The dataset provided in this question represents tips received by waitstaff in a restaurant. Your task is to use the seaborn library to create an informative and complex plot that demonstrates your understanding of seaborn\'s advanced plotting capabilities. # Instructions 1. Load the dataset `tips` using `seaborn.load_dataset(\\"tips\\")`. 2. Create a plot that visualizes the relationship between the total bill and the tip, and include the following layers: - A scatter plot of total bill against tip. - A polynomial fit line that relates total bill to tip. 3. Enhance the plot to differentiate data points and polynomial fit lines by gender. 4. Add another layer to visualize the distribution of the total bill by day of the week using histograms. 5. Make sure your plot includes: - Colors that differentiate data groups (like gender or days). - Legends that indicate each plot element. - Proper labels for the x-axis and y-axis. # Implementation Requirements - You must use the new seaborn interface elements such as `so.Plot`, `so.Dot`, `so.Line`, `so.PolyFit`, `so.Bar`, etc. - Properly map and scale variables where appropriate. - Handle different data groups correctly and ensure the plot is informative and visually appealing. # Expected Input and Output Format - **Input:** No direct input needed as the dataset is included within seaborn. - **Output:** Rendered plots demonstrating the mentioned layers and configurations. # Constraints - Ensure your plot is readable and clearly differentiates between different data groups. - The use of seaborn\'s new plotting functionality is required. # Example Output The expected output should be a plot that visually differentiates between the different genders with scatter points and polynomial fit lines, and histograms showing total bill distributions by weekday. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Step 1: Base plot with scatter plot and polynomial fit plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") plot.add(so.Dot()) plot.add(so.Line(), so.PolyFit()) # Step 2: Add histograms for total bill distributions by day plot.add(so.Bar(), so.Hist(), col=\\"day\\", color=\\"day\\") # Step 3: Label the axes plot.label(x=\\"Total Bill\\", y=\\"Tip\\") # Display the plot plot ``` Create a code for the above task within a single cell to generate the visual plot as described.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_complex_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Step 1: Base plot with scatter plot and polynomial fit plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") plot.add(so.Dot()) plot.add(so.Line(), so.PolyFit()) # Step 2: Add histograms for total bill distributions by day plot.add(so.Bar(), so.Hist(), col=\\"day\\", color=\\"day\\") # Step 3: Label the axes plot.label(x=\\"Total Bill\\", y=\\"Tip\\") # Display the plot plot.show() # Example function call to create and display the plot create_complex_plot()"},{"question":"**Question: Comprehensive Database Interaction Using `sqlite3`** You\'re tasked with creating a mini application that manages a database of books using the `sqlite3` module. This application should allow for creating the database and table, inserting data, performing queries, editing entries, and handling potential errors. # Requirements: 1. **Create a SQLite Database**: Connect to a SQLite database named \\"books.db\\". This should create the database file if it doesn\'t already exist. 2. **Create a Table**: Create a table named \\"books\\" with the following columns: - `id` (INTEGER PRIMARY KEY) - `title` (TEXT, non-empty) - `author` (TEXT, non-empty) - `year` (INTEGER) - `rating` (REAL) 3. **Insert Data**: Insert the following sample data into the \\"books\\" table: - (The Great Gatsby, F. Scott Fitzgerald, 1925, 8.5) - (1984, George Orwell, 1949, 9.0) - (To Kill a Mockingbird, Harper Lee, 1960, 8.9) 4. **Retrieve Data**: Implement a function `fetch_books_by_author(author_name)` that retrieves and prints all books written by the specific author. The details should be printed in a readable format. 5. **Update Data**: Implement a function `update_book_rating(title, new_rating)` that updates the rating of a specific book identified by its title. 6. **Error Handling**: Implement proper error handling for database operations, including handling invalid inputs and possible operational errors. 7. **Transaction Management**: Ensure that your operations are properly managed within transactions, committing where appropriate and rolling back in case of errors. 8. **Custom Data Type Handling**: Implement a conversion for a custom Python type `Review` which includes a review text and a reviewer name. # Constraints: - The `title` and `author` fields should be non-empty. - If an attempted update target book title does not exist, print a message indicating so. # Expected Functions: 1. **setup_database()** - Initializes the database and the books table. 2. **insert_sample_data()** - Inserts the provided sample data into the books table. 3. **fetch_books_by_author(author_name: str)** - Fetches and prints books by the given author. 4. **update_book_rating(title: str, new_rating: float)** - Updates the rating for the given book title. 5. **Review Class**: Custom Python type with a suitable adapter and converter. # Example of Use: ```python setup_database() # Initializes the DB and creates tables insert_sample_data() # Inserts the provided sample data fetch_books_by_author(\'George Orwell\') # Retrieves and prints books by George Orwell update_book_rating(\'1984\', 9.5) # Updates the rating for \'1984\' ``` # Code Implementation: ```python import sqlite3 class Review: def __init__(self, text, reviewer): self.text = text self.reviewer = reviewer def __repr__(self): return f\'Review({self.text}, {self.reviewer})\' def adapt_review(review): return f\\"{review.text};{review.reviewer}\\".encode() def convert_review(s): text, reviewer = s.decode().split(\\";\\") return Review(text, reviewer) def setup_database(): con = sqlite3.connect(\\"books.db\\") con.execute(\\"PRAGMA foreign_keys = 1\\") # Enable foreign key support cur = con.cursor() cur.execute( CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, author TEXT NOT NULL, year INTEGER, rating REAL ) ) con.commit() con.close() def insert_sample_data(): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() data = [ (\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925, 8.5), (\\"1984\\", \\"George Orwell\\", 1949, 9.0), (\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, 8.9) ] cur.executemany(\\"INSERT INTO books (title, author, year, rating) VALUES (?, ?, ?, ?)\\", data) con.commit() con.close() def fetch_books_by_author(author_name): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"SELECT title, year, rating FROM books WHERE author = ?\\", (author_name,)) books = cur.fetchall() for book in books: print(f\\"Title: {book[0]}, Year: {book[1]}, Rating: {book[2]}\\") con.close() def update_book_rating(title, new_rating): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"UPDATE books SET rating = ? WHERE title = ?\\", (new_rating, title)) if cur.rowcount == 0: print(f\\"No book found with title {title}\\") con.commit() con.close() def setup_review_conversion(): sqlite3.register_adapter(Review, adapt_review) sqlite3.register_converter(\\"review\\", convert_review) # Example Usage if __name__ == \\"__main__\\": setup_database() insert_sample_data() setup_review_conversion() fetch_books_by_author(\'George Orwell\') update_book_rating(\'1984\', 9.5) # Test the Review type con = sqlite3.connect(\\"books.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\\"CREATE TABLE IF NOT EXISTS reviews (content review)\\") review = Review(\\"Great book!\\", \\"Alice\\") cur.execute(\\"INSERT INTO reviews (content) VALUES (?)\\", (review,)) cur.execute(\\"SELECT content FROM reviews\\") print(cur.fetchone()[0]) # Output should show Review instance con.commit() con.close() ``` # Explanation: 1. **setup_database**: Initializes the database and tables. 2. **insert_sample_data**: Inserts predefined sample data. 3. **fetch_books_by_author**: Fetches and prints books by a given author. 4. **update_book_rating**: Updates the rating of a book by its title. 5. **Review Class & Conversion**: Adapt custom Python type `Review` to SQLite format and vice versa.","solution":"import sqlite3 class Review: def __init__(self, text, reviewer): self.text = text self.reviewer = reviewer def __repr__(self): return f\'Review({self.text}, {self.reviewer})\' def adapt_review(review): return f\\"{review.text};{review.reviewer}\\".encode() def convert_review(s): text, reviewer = s.decode().split(\\";\\") return Review(text, reviewer) def setup_database(): con = sqlite3.connect(\\"books.db\\") con.execute(\\"PRAGMA foreign_keys = 1\\") # Enable foreign key support cur = con.cursor() cur.execute( CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, author TEXT NOT NULL, year INTEGER, rating REAL ) ) con.commit() con.close() def insert_sample_data(): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() data = [ (\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925, 8.5), (\\"1984\\", \\"George Orwell\\", 1949, 9.0), (\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, 8.9) ] cur.executemany(\\"INSERT INTO books (title, author, year, rating) VALUES (?, ?, ?, ?)\\", data) con.commit() con.close() def fetch_books_by_author(author_name): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"SELECT title, year, rating FROM books WHERE author = ?\\", (author_name,)) books = cur.fetchall() for book in books: print(f\\"Title: {book[0]}, Year: {book[1]}, Rating: {book[2]}\\") con.close() def update_book_rating(title, new_rating): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"UPDATE books SET rating = ? WHERE title = ?\\", (new_rating, title)) if cur.rowcount == 0: print(f\\"No book found with title {title}\\") con.commit() con.close() def setup_review_conversion(): sqlite3.register_adapter(Review, adapt_review) sqlite3.register_converter(\\"review\\", convert_review)"},{"question":"# Advanced Python Class Implementation Objective Design and implement a Python class that simulates a simple classroom seating arrangement. This class should incorporate fundamental and advanced concepts of Python’s class mechanism, including attributes, methods, inheritance, iterators, and generators. Problem Statement You need to create a class structure to manage the seating arrangement of students in a classroom. The classroom can add students, remove students, and iterate over the list of students in both regular and reverse order. Requirements: 1. **Class Definitions and Instances**: - Create a base class `Classroom`: - **Attributes**: - `class_capacity` (int): the maximum number of students allowed in the class. - `students` (list): a list to hold the names of the students. - **Methods**: - `__init__(self, class_capacity: int)`: Initialize the classroom with a given capacity and an empty student list. - `add_student(self, student_name: str)`: Add a student\'s name to the list if the capacity is not exceeded. - `remove_student(self, student_name: str)`: Remove a student\'s name from the list. - `__iter__(self)`: Return an iterator to loop over the student names in order. - `__next__(self)`: Iterator\'s next method to return the next student in the order. 2. **Inheritance and Overridden Methods**: - Create a derived class `ReversibleClassroom` that inherits from `Classroom`: - **Methods**: - `reversed(self)`: Returns an iterator to loop over the student names in reverse order. - Optionally override methods to show the distinctive functionality if necessary. 3. **Iterators**: - Implement iterators for both forward and reverse iteration using `__iter__()` and `__next__()` methods. 4. **Generator**: - Add a method `generate_seating_order` in `ReversibleClassroom` that yields student names in the current order. Implementation Details: - **Input**: - `add_student` and `remove_student` methods will take a string representing a student’s name. - **Output**: - Appropriate messages for adding and removing students. - Iterators that can print the list of students in the correct order. - **Constraints**: - Ensure that the number of students does not exceed the specified `class_capacity`. Example: ```python # Create a classroom with a capacity of 3 classroom = ReversibleClassroom(3) classroom.add_student(\\"Alice\\") classroom.add_student(\\"Bob\\") classroom.add_student(\\"Charlie\\") classroom.add_student(\\"Dave\\") # Should print a message that the class is full # Iterate over students in normal order for student in classroom: print(student) # Alice, Bob, Charlie # Remove a student and iterate again classroom.remove_student(\\"Bob\\") for student in classroom: print(student) # Alice, Charlie # Iterate over students in reverse order for student in classroom.reversed(): print(student) # Charlie, Alice # Using the generator to produce seating order for student in classroom.generate_seating_order(): print(student) # Alice, Charlie ``` Implement the `Classroom` and `ReversibleClassroom` classes with all the specified functionality.","solution":"class Classroom: def __init__(self, class_capacity: int): self.class_capacity = class_capacity self.students = [] self._position = 0 def add_student(self, student_name: str): if len(self.students) < self.class_capacity: self.students.append(student_name) print(f\\"{student_name} has been added.\\") else: print(f\\"Cannot add {student_name}. The class is full.\\") def remove_student(self, student_name: str): if student_name in self.students: self.students.remove(student_name) print(f\\"{student_name} has been removed.\\") else: print(f\\"{student_name} is not in the class.\\") def __iter__(self): self._position = 0 return self def __next__(self): if self._position < len(self.students): student = self.students[self._position] self._position += 1 return student else: raise StopIteration class ReversibleClassroom(Classroom): def reversed(self): return ReversibleIterator(self.students) def generate_seating_order(self): for student in self.students: yield student class ReversibleIterator: def __init__(self, students): self.students = students self._position = len(students) def __iter__(self): return self def __next__(self): if self._position > 0: self._position -= 1 return self.students[self._position] else: raise StopIteration"},{"question":"# Question: Implement a Custom Date Formatter Objective: Write a Python function `custom_date_formatter` that takes a `datetime.date` object and a format string, then returns a string representing the date in the specified format. Requirements: 1. **Function Signature**: ```python def custom_date_formatter(date_obj: datetime.date, format_str: str) -> str: ``` 2. **Inputs**: - `date_obj`: A `datetime.date` object representing a date. - `format_str`: A string specifying the format. The format should follow the same directives as supported by `strftime`. 3. **Outputs**: - A string representing the date in the specified format. 4. **Constraints**: - The function should not use `datetime.strftime` directly. - It should parse the format string and build the result using other methods and properties of the `datetime.date` class. 5. **Additional Notes**: - Handle common date format directives such as `%Y`, `%m`, `%d`, `%A`, `%B`, etc. - Raise a `ValueError` for unsupported format codes. Example: ```python from datetime import date d = date(2021, 3, 14) print(custom_date_formatter(d, \\"%A, %B %d, %Y\\")) # Output: \\"Sunday, March 14, 2021\\" print(custom_date_formatter(d, \\"%d/%m/%y\\")) # Output: \\"14/03/21\\" ``` Implementation Notes: - Consider using string methods and date object attributes to build the formatted string. - It might help to create a mapping of format codes to their corresponding date attribute representations.","solution":"import datetime def custom_date_formatter(date_obj: datetime.date, format_str: str) -> str: Formats a datetime.date object into a string based on provided format string. :param date_obj: datetime.date object :param format_str: string specifying the format :return: formatted date string format_directives = { \\"%Y\\": date_obj.year, \\"%m\\": f\\"{date_obj.month:02d}\\", \\"%d\\": f\\"{date_obj.day:02d}\\", \\"%A\\": date_obj.strftime(\\"%A\\"), \\"%B\\": date_obj.strftime(\\"%B\\"), \\"%y\\": f\\"{date_obj.year % 100:02d}\\", \\"%a\\": date_obj.strftime(\\"%a\\"), \\"%b\\": date_obj.strftime(\\"%b\\"), } result = format_str for key, value in format_directives.items(): result = result.replace(key, str(value)) # Validate for unsupported format codes if \'%\' in result: raise ValueError(\\"Unsupported format code in the format string\\") return result"},{"question":"Coding Assessment Question # Objective Implement a PyTorch model and perform dynamic quantization on it. This exercise will test your understanding of PyTorch\'s dynamic quantization technique and how to apply it to a model to improve its efficiency during inference. # Problem Statement 1. **Model Definition**: - Define a PyTorch module containing at least two linear layers. - Include ReLU activations between these layers. 2. **Dynamic Quantization**: - Convert the defined neural network to its quantized version using dynamic quantization. - Specifically quantize the `Linear` layers with `torch.qint8` data types. 3. **Model Inference**: - Demonstrate the model inference by running inference on both the original and quantized models. - Ensure the output of both models are shown. # Requirements 1. Implement the class `Net` that constructs the required model structure. 2. Implement a function `quantize_model` that: - Takes in an instance of the model. - Returns a dynamically quantized model instance. 3. Demonstrate performing inference with both the original and quantized models using a random input tensor of appropriate shape. # Constraints - Use `torch.nn` modules and functions only. - Models and functions should be capable of handling inputs in a batch. # Expected Input and Output Format Class Definition ```python class Net(torch.nn.Module): def __init__(self): # Define the network layers and structure here pass def forward(self, x): # Define the forward pass here pass ``` Quantization Function ```python def quantize_model(model): Quantize the input model using dynamic quantization. Args: - model (torch.nn.Module): The floating-point PyTorch model to be quantized. Returns: - quantized_model (torch.nn.Module): The dynamically quantized model. pass ``` Example Usage ```python import torch # Define and initialize the model model_fp32 = Net() # Random input tensor input_tensor = torch.randn(10, 4) # Quantize the model model_int8 = quantize_model(model_fp32) # Inference with the original model output_fp32 = model_fp32(input_tensor) print(\\"FP32 Model Output: \\", output_fp32) # Inference with the quantized model output_int8 = model_int8(input_tensor) print(\\"Quantized Model Output: \\", output_int8) ``` # Notes - Ensure that the model inference results (output tensors) are printed for both original and quantized models. - The outputs may not match exactly due to quantization errors, but they should be similar.","solution":"import torch import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(4, 16) self.relu = nn.ReLU() self.fc2 = nn.Linear(16, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def quantize_model(model): Quantize the input model using dynamic quantization. Args: - model (torch.nn.Module): The floating-point PyTorch model to be quantized. Returns: - quantized_model (torch.nn.Module): The dynamically quantized model. quantized_model = torch.quantization.quantize_dynamic( model, {torch.nn.Linear}, dtype=torch.qint8 ) return quantized_model"},{"question":"**Question: Implement a Custom Iterable and Iterator** You are required to implement a custom iterable and its corresponding iterator in Python. This will involve creating classes that follow the iterator protocol as specified in the provided documentation. Your classes must support both synchronous and asynchronous iteration. # Requirements 1. **CustomIterable Class**: This class should act as a container for your iterable collection. It should initialize with a list of items. 2. **CustomIterator Class**: This class should implement the standard iterator protocol. 3. **CustomAsyncIterator Class**: This class should implement the asynchronous iterator protocol. # Specifications **1. CustomIterable Class** - **Input**: A list of items - **Output**: An instance of CustomIterator when `__iter__` is called, and CustomAsyncIterator when `__aiter__` is called. **2. CustomIterator Class** - **Methods**: - `__init__(self, items: List[Any])`: Initialize with the list of items. - `__iter__(self)` : Returns self. - `__next__(self)`: Returns the next item from the list, raises `StopIteration` when the list is exhausted. **3. CustomAsyncIterator Class** - **Methods**: - `__init__(self, items: List[Any])`: Initialize with the list of items. - `__aiter__(self)`: Returns self. - `__anext__(self)`: Asynchronously returns the next item from the list, raises `StopAsyncIteration` when the list is exhausted. # Constraints - You must handle state and exceptions properly according to the iterator protocols. - Use `asyncio.sleep` for simulating asynchronous behavior in `CustomAsyncIterator`. # Example Usage ```python # Synchronous Iteration iterable = CustomIterable([1, 2, 3]) for item in iterable: print(item) # Output: 1, 2, 3 # Asynchronous Iteration import asyncio async def async_iter(): aiterable = CustomIterable([4, 5, 6]) async for item in aiterable: print(item) # Output: 4, 5, 6 asyncio.run(async_iter()) ``` The provided solution should follow the instructions to create a robust and well-defined iterable that demonstrates an understanding of both synchronous and asynchronous iteration protocols in Python.","solution":"from typing import List, Any import asyncio class CustomIterable: def __init__(self, items: List[Any]): self.items = items def __iter__(self): return CustomIterator(self.items) def __aiter__(self): return CustomAsyncIterator(self.items) class CustomIterator: def __init__(self, items: List[Any]): self._items = items self._index = 0 def __iter__(self): return self def __next__(self): if self._index >= len(self._items): raise StopIteration result = self._items[self._index] self._index += 1 return result class CustomAsyncIterator: def __init__(self, items: List[Any]): self._items = items self._index = 0 def __aiter__(self): return self async def __anext__(self): if self._index >= len(self._items): raise StopAsyncIteration await asyncio.sleep(0.1) # Simulate asynchronous behavior result = self._items[self._index] self._index += 1 return result"},{"question":"You are required to write a function `process_and_verify_tensors` that follows these steps: 1. Create a random tensor `A` of size (3, 3) with float32 data type using `torch.testing.make_tensor`. 2. Create another tensor `B` which is 2 times `A` (i.e., element-wise multiplication by 2). 3. Create a third tensor `C` which is the result of subtracting `A` from `B` (element-wise subtraction). 4. Verify that tensor `C` is equal to `A` using `torch.testing.assert_close` from the `torch.testing` module. **Function Signature:** ```python def process_and_verify_tensors() -> None: pass ``` # Constraints & Requirements - You are required to use `torch.testing.make_tensor` to create the initial tensor `A`. - Use `torch.testing.assert_close` to validate the result in tensor `C`. - Ensure that the function handles the required operations correctly without throwing any assertion errors. # Example The exact values in the tensors are random and thus will differ each time. However, the logic should follow this pattern: - If `A` = (begin{bmatrix} a & b & c d & e & f g & h & i end{bmatrix}) - Then `B` = (begin{bmatrix} 2a & 2b & 2c 2d & 2e & 2f 2g & 2h & 2i end{bmatrix}) - And `C` = (begin{bmatrix} a & b & c d & e & f g & h & i end{bmatrix}) Finally, `torch.testing.assert_close` should confirm `C` is close to `A`. **Notes:** - Ensure your function does not print anything. - Handle all necessary imports within your function.","solution":"import torch def process_and_verify_tensors() -> None: Function to create, manipulate, and verify tensors as specified. # Step 1: Create a random tensor A of size (3, 3) with float32 data type using torch.testing.make_tensor A = torch.testing.make_tensor((3, 3), dtype=torch.float32, device=\'cpu\') # Step 2: Create tensor B which is 2 times A (element-wise multiplication by 2) B = 2 * A # Step 3: Create tensor C which is the result of subtracting A from B (element-wise subtraction) C = B - A # Step 4: Verify that tensor C is equal to A using torch.testing.assert_close torch.testing.assert_close(C, A)"},{"question":"You are given a dataset of penguins with various measurements. Your task is to create a visual analysis using the seaborn library\'s `JointGrid` class to understand the relationship between different measurements. Follow the guidelines below to complete the task. Task 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a `JointGrid` instance to plot the relationship between `flipper_length_mm` and `body_mass_g` with the following specifications: - Use a scatter plot for the joint distribution. - Use histograms for the marginal distributions. - Color the points based on the `species` column. - Add vertical and horizontal reference lines at the medians of the respective axes. - Adjust the layout to ensure minimal space between plots and an appropriate ratio between the joint and marginal plots. Input The dataset is preloaded with the following columns: - `species` - `island` - `bill_length_mm` - `bill_depth_mm` - `flipper_length_mm` - `body_mass_g` - And others... Output A `JointGrid` visual displaying the described settings. Example Code Structure ```python import seaborn as sns # Load the dataset data = sns.load_dataset(\\"penguins\\") # Create the JointGrid instance with the required settings g = sns.JointGrid(data=data, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") # Plot using scatter and histplot g.plot(sns.scatterplot, sns.histplot) # Add reference lines at the median of flipper length and body mass median_flipper_length = data[\\"flipper_length_mm\\"].median() median_body_mass = data[\\"body_mass_g\\"].median() g.refline(x=median_flipper_length, y=median_body_mass) # Adjust grid size and layout g = sns.JointGrid(data=data, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", height=6, ratio=2, space=0.05) g.plot(sns.scatterplot, sns.histplot, alpha=.7) # Display the plot g.fig.suptitle(\\"JointGrid: Flipper Length vs Body Mass\\") g.fig.tight_layout() g.fig.subplots_adjust(top=0.95) ``` Constraints - You must use the seaborn library for plotting. - Ensure your code is well-commented and follows best practices in terms of readability and structure. - Follow the specified guidelines strictly to achieve the desired output.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_measurements(): # Load the dataset data = sns.load_dataset(\\"penguins\\") # Drop rows with missing values data = data.dropna(subset=[\\"flipper_length_mm\\", \\"body_mass_g\\", \\"species\\"]) # Create the JointGrid instance with the required settings g = sns.JointGrid(data=data, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", height=6, ratio=2, space=0.05) # Plot using scatter and histplot # Scatter plot for the joint distributions g.plot(sns.scatterplot, sns.histplot, alpha=0.7) # Add reference lines at the median of flipper length and body mass median_flipper_length = data[\\"flipper_length_mm\\"].median() median_body_mass = data[\\"body_mass_g\\"].median() g.refline(x=median_flipper_length, y=median_body_mass, linestyle=\'dashed\') # Add titles and adjust layout g.fig.suptitle(\\"JointGrid: Flipper Length vs Body Mass\\") g.fig.tight_layout() g.fig.subplots_adjust(top=0.95) # Show the plot plt.show() # Execute the plotting function plot_penguin_measurements()"},{"question":"Objective Demonstrate the practical application and understanding of asyncio in Python 3.10 by creating a simple concurrent task manager that includes network IO operations. Problem Statement You are required to create an asynchronous task manager that performs multiple tasks concurrently: 1. Fetch data from a set of provided URLs. 2. Process the fetched data. 3. Print the result of the processed data. You must use asyncio’s high-level APIs to achieve this. Specifications **Input:** - A list of URLs (strings). **Output:** - Print statements for each URL showing the fetched data and the processed result. **Details:** 1. Implement a function `fetch_data(url: str) -> str` that simulates fetching data from a URL by using `asyncio.sleep()`. 2. Implement a function `process_data(data: str) -> str` that simulates processing data by reversing the string and converting it to uppercase. 3. Implement a function `task_manager(urls: List[str]) -> None` that: - Creates tasks to fetch data concurrently from all URLs. - Waits for all data fetching tasks to complete. - Processes the fetched data concurrently. - Prints the fetched data and processed result for each URL. Constraints: - Use `asyncio.gather()` for running the fetch tasks concurrently. - Use `asyncio.gather()` for running the process tasks concurrently. - Simulate network fetch time in `fetch_data` by calling `asyncio.sleep(2)`. - Simulate processing time in `process_data` by calling `asyncio.sleep(1)`. Example: ```python import asyncio from typing import List async def fetch_data(url: str) -> str: await asyncio.sleep(2) # simulate network delay return f\\"Data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(1) # simulate processing delay return data[::-1].upper() async def task_manager(urls: List[str]) -> None: fetch_tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*fetch_tasks) process_tasks = [process_data(data) for data in results] processed_results = await asyncio.gather(*process_tasks) for url, fetched, processed in zip(urls, results, processed_results): print(f\\"Fetched from {url}: {fetched}\\") print(f\\"Processed: {processed}\\") # Example usage: # asyncio.run(task_manager([\\"http://example.com\\", \\"http://example.org\\"])) ``` When run with URLs `[\\"http://example.com\\", \\"http://example.org\\"]`, the expected output should be: ``` Fetched from http://example.com: Data from http://example.com Processed: MOC.ELPMAXE//:PTTH MORF ATAD Fetched from http://example.org: Data from http://example.org Processed: GRO.ELPMAXE//:PTTH MORF ATAD ``` Performance Requirements: Ensure that fetching and processing occur concurrently and asynchronously to minimize wait times.","solution":"import asyncio from typing import List async def fetch_data(url: str) -> str: await asyncio.sleep(2) # simulate network delay return f\\"Data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(1) # simulate processing delay return data[::-1].upper() async def task_manager(urls: List[str]) -> None: fetch_tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*fetch_tasks) process_tasks = [process_data(data) for data in results] processed_results = await asyncio.gather(*process_tasks) for url, fetched, processed in zip(urls, results, processed_results): print(f\\"Fetched from {url}: {fetched}\\") print(f\\"Processed: {processed}\\") # Example usage: # asyncio.run(task_manager([\\"http://example.com\\", \\"http://example.org\\"]))"},{"question":"**Coding Assessment Question** # Objective You are given a dataset representing information about users and their activities on a platform. Your task is to analyze the dataset to: 1. Report the memory usage of the DataFrame. 2. Adjust the DataFrame to handle missing values appropriately. 3. Ensure that the handling of missing values does not alter the original dtype of columns where possible. # Input Format You will be provided with a CSV file containing the following columns: - `user_id` (integer): Unique identifier for each user. - `age` (integer, can be missing): Age of the user. - `signup_date` (datetime, cannot be missing): Date when the user signed up. - `last_login` (datetime, can be missing): Date when the user last logged in. - `subscription_type` (string, can be missing): Type of subscription the user has. - `premium` (boolean, can be missing): Indicates if the user has premium status. # Expected Output Format You need to implement two functions: 1. **report_memory_usage(df: pd.DataFrame) -> pd.DataFrame:** - Input: A pandas DataFrame. - Output: A DataFrame with two columns: `\'Column\'` and `\'Memory Usage (bytes)\'`, listing each column and its memory usage in bytes. 2. **handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:** - Input: A pandas DataFrame. - Output: A DataFrame where: - Missing values in integer columns are filled with -1. - Missing values in datetime columns are filled with the date \'2000-01-01\'. - Missing values in string columns are filled with \'unknown\'. - Missing values in boolean columns are filled with `False`. # Constraints - Do not change the dtype of columns when handling missing values. - Ensure the resulting DataFrame has no missing values. # Example ```python import pandas as pd from io import StringIO data = user_id,age,signup_date,last_login,subscription_type,premium 1,25,2023-01-01,2023-01-10,Gold,True 2,,2023-03-15,,Silver, 3,33,2022-07-23,2022-12-01,,True 4,,2021-06-20,2021-09-15,Platinum,True 5,45,2020-02-28,,, df = pd.read_csv(StringIO(data), parse_dates=[\'signup_date\', \'last_login\']) # Report memory usage report_memory_usage(df) # Expected Output: # Column Memory Usage (bytes) # user_id 40 # age 40 # signup_date 40 # last_login 40 # subscription_type 40 # premium 40 # Handle missing values handle_missing_values(df) # Expected Output: # user_id age signup_date last_login subscription_type premium # 0 1 25 2023-01-01 2023-01-10 Gold True # 1 2 -1 2023-03-15 2000-01-01 Silver False # 2 3 33 2022-07-23 2022-12-01 unknown True # 3 4 -1 2021-06-20 2021-09-15 Platinum True # 4 5 45 2020-02-28 2000-01-01 unknown False ``` # Implementation ```python import pandas as pd import numpy as np def report_memory_usage(df: pd.DataFrame) -> pd.DataFrame: memory_usage = df.memory_usage(deep=True) memory_usage_df = pd.DataFrame({\'Column\': memory_usage.index, \'Memory Usage (bytes)\': memory_usage.values}) return memory_usage_df def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: df_filled = df.copy() df_filled[\'age\'] = df_filled[\'age\'].fillna(-1).astype(int) df_filled[\'last_login\'] = df_filled[\'last_login\'].fillna(pd.Timestamp(\'2000-01-01\')) df_filled[\'subscription_type\'] = df_filled[\'subscription_type\'].fillna(\'unknown\') df_filled[\'premium\'] = df_filled[\'premium\'].fillna(False) return df_filled ``` # Constraints - Do not use loops to fill missing values. - Use native pandas methods for handling missing values and reporting memory usage.","solution":"import pandas as pd import numpy as np def report_memory_usage(df: pd.DataFrame) -> pd.DataFrame: Reports memory usage of each column in the DataFrame. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with columns \\"Column\\" and \\"Memory Usage (bytes)\\". memory_usage = df.memory_usage(deep=True) memory_usage_df = pd.DataFrame({\'Column\': memory_usage.index, \'Memory Usage (bytes)\': memory_usage.values}) return memory_usage_df.reset_index(drop=True) def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: Handles missing values in the DataFrame. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with missing values handled. df_filled = df.copy() # Fill missing values in integer columns with -1 df_filled[\'age\'] = df_filled[\'age\'].fillna(-1).astype(int) # Fill missing values in datetime columns with the date \'2000-01-01\' df_filled[\'last_login\'] = df_filled[\'last_login\'].fillna(pd.Timestamp(\'2000-01-01\')) # Fill missing values in string columns with \'unknown\' df_filled[\'subscription_type\'] = df_filled[\'subscription_type\'].fillna(\'unknown\') # Fill missing values in boolean columns with False df_filled[\'premium\'] = df_filled[\'premium\'].fillna(False).astype(bool) return df_filled"},{"question":"# Advanced Python Assessment Question: System Audit and Memory Management **Objective:** Your task is to write a Python script that demonstrates the use of audit hooks and memory management using the `sys` module. **Problem Statement:** 1. Implement an auditing system using the functions `sys.addaudithook()` and `sys.audit()`. 2. Track memory allocations and deallocations using the `sys.getsizeof()` and `sys.getallocatedblocks()` functions. 3. Integrate the audit hooks to monitor and log memory allocations within the Python interpreter. **Requirements:** 1. **Audit Hook Implementation**: - Write a function `my_audit_hook(event, args)` that logs all triggered events with their arguments. You can use `print()` statements for logging. - Register this auditing hook using `sys.addaudithook()`. 2. **Memory Management**: - Implement a function `track_memory_usage()` that: - Prints the current number of memory blocks allocated using `sys.getallocatedblocks()`. - Returns the size of an object passed to it using `sys.getsizeof()`. 3. **Integration and Testing**: - Create a test function `test_audit_and_memory()` that: - Allocates different types of objects (e.g., list, dict, and str). - Uses `sys.audit()` to raise custom events related to memory allocations. - Calls `track_memory_usage()` for each object and prints the results. - The audit hook should log each memory allocation event, including the type and size of the allocated object. **Constraints:** - You must use only the `sys` module for auditing and memory management. - Ensure to handle potential exceptions within your auditing system gracefully. **Expected Output:** Your output should include logs from the audit hook, as well as memory usage statistics for different allocated objects. **Example:** ```python import sys def my_audit_hook(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") def track_memory_usage(obj): print(f\\"Allocated Blocks: {sys.getallocatedblocks()}\\") return sys.getsizeof(obj) def test_audit_and_memory(): sys.addaudithook(my_audit_hook) # Allocating different objects and auditing memory usage obj_list = [1, 2, 3] sys.audit(\\"memory.allocate\\", (\\"list\\",)) print(f\\"Size of List: {track_memory_usage(obj_list)} bytes\\") obj_dict = {\'key\': \'value\'} sys.audit(\\"memory.allocate\\", (\\"dict\\",)) print(f\\"Size of Dict: {track_memory_usage(obj_dict)} bytes\\") obj_str = \\"Hello, Python!\\" sys.audit(\\"memory.allocate\\", (\\"str\\",)) print(f\\"Size of Str: {track_memory_usage(obj_str)} bytes\\") test_audit_and_memory() ``` Ensure your implementation captures similar output structure and functionality.","solution":"import sys def my_audit_hook(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") def track_memory_usage(obj): print(f\\"Allocated Blocks: {sys.getallocatedblocks()}\\") return sys.getsizeof(obj) def test_audit_and_memory(): sys.addaudithook(my_audit_hook) # Allocating different objects and auditing memory usage obj_list = [1, 2, 3] sys.audit(\\"memory.allocate\\", (\\"list\\",)) print(f\\"Size of List: {track_memory_usage(obj_list)} bytes\\") obj_dict = {\'key\': \'value\'} sys.audit(\\"memory.allocate\\", (\\"dict\\",)) print(f\\"Size of Dict: {track_memory_usage(obj_dict)} bytes\\") obj_str = \\"Hello, Python!\\" sys.audit(\\"memory.allocate\\", (\\"str\\",)) print(f\\"Size of Str: {track_memory_usage(obj_str)} bytes\\") test_audit_and_memory()"},{"question":"# Custom File Handler Implementation in Python In this task, you will create a custom file handler class in Python, making use of the functions described in the documentation above. Your task is to create this class and demonstrate its usage through a few test cases. Specifications 1. **Class Name**: `CustomFileHandler` 2. **Initialization**: The constructor should accept a file descriptor (`fd`) and other optional parameters, including `mode`, `encoding`, `errors`, and `newline`. 3. **Methods**: - `readline(n=-1)`: Reads a line from the file. The parameter `n` specifies the number of bytes to read as described in `PyFile_GetLine`. - `write(data)`: Writes a string to the file. The `data` can be either a Python object or a string. - `close()`: Closes the file descriptor. 4. **Exception Handling**: Properly handle and raise appropriate Python exceptions when underlying C API functions fail. 5. **Usage Demonstration**: Include test cases demonstrating: - Opening a file and reading lines using `readline`. - Writing a string and an object to the file using `write`. - Properly closing the file after operations. Additional Constraints - Do not use Python\'s built-in file methods directly. Implement the functionality using the provided Python C API functions. - Ensure the class correctly cleans up resources, even in the case of exceptions. # Example Usage ```python # Assuming an existing file \'example.txt\' with some content. import os # Open a file descriptor fd = os.open(\'example.txt\', os.O_RDONLY) # Create a custom file handler instance handler = CustomFileHandler(fd) # Read lines from the file print(handler.readline()) print(handler.readline(10)) # Close the handler handler.close() # Open a file descriptor for writing fd = os.open(\'example.txt\', os.O_WRONLY | os.O_CREAT) # Create a custom file handler instance handler = CustomFileHandler(fd, mode=\'w\') # Write to the file handler.write(\\"Hello, World!\\") handler.write({\\"key\\": \\"value\\"}) # Ensure proper serialization or conversion # Close the handler handler.close() ``` Ensure your implementation correctly mimics this behavior while adhering to the constraints and using the provided C API functions.","solution":"import os import json class CustomFileHandler: def __init__(self, fd, mode=\'r\', encoding=None, errors=None, newline=None): self.fd = fd self.file = os.fdopen(fd, mode=mode, encoding=encoding, errors=errors, newline=newline) def readline(self, n=-1): return self.file.readline(n) def write(self, data): if isinstance(data, str): self.file.write(data) else: self.file.write(json.dumps(data)) def close(self): self.file.close()"},{"question":"# Asynchronous HTTP Server **Objective**: Implement an asynchronous HTTP server using Python\'s asyncio library, showcasing your understanding of event loops, task handling, and network operations. Requirements 1. **Create an HTTP Server**: Your server should be able to listen for incoming connections on a specified port and respond to HTTP GET requests. 2. **Asynchronous Handling**: Use asyncio\'s event loop and transport protocols to handle requests asynchronously. 3. **Graceful Shutdown**: Ensure the server can handle a graceful shutdown when a stop signal is received. 4. **Error Handling**: Properly handle errors and implement an appropriate error-handling mechanism. Function Signature ```python async def start_server(host: str, port: int) -> None: Starts an asynchronous HTTP server which handles GET requests. Args: - host (str): The host address on which the server will listen. - port (int): The port number on which the server will listen. Returns: - None class HTTPServerProtocol(asyncio.Protocol): A custom protocol to handle HTTP connections. def main(): Main function to start the server and handle shutdown. ``` Detailed Requirements 1. **HTTPServerProtocol**: - Should inherit from `asyncio.Protocol`. - Implement methods to handle connection creation, data reception (parse HTTP GET requests), and connection closure. - Respond with a simple HTTP 200 OK message along with a \\"Hello, world!\\" response body. 2. **start_server**: - Should set up the asyncio server, bind it to the specified `host` and `port`, and handle incoming connections using the `HTTPServerProtocol`. - Implement an asyncio event loop to run the server indefinitely until a stop signal is received. 3. **main**: - Initialize the asyncio event loop and call the `start_server`. - Implement a signal handler to gracefully shut down the server on receiving a termination signal. Example Start the server to listen on `localhost` at port `8080`: ```python if __name__ == \\"__main__\\": main() # The server should now be able to handle HTTP GET requests asynchronously on http://localhost:8080 ``` Your implementation should demonstrate a clear understanding of asyncio\'s event loop, task scheduling, and network operations using transports and protocols. Constraints - Ensure that you handle multiple simultaneous connections efficiently. - Provide error handling to deal with malformed requests and other potential issues. - Optimize the code to ensure minimal latency and resource usage during high traffic. **Hint**: Utilize methods like `asyncio.start_server`, and handle data asynchronously to create a robust and scalable solution.","solution":"import asyncio import signal class HTTPServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() # Check if it\'s a GET request if message.startswith(\'GET\'): # Prepare the response for a valid GET request response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/plainrn\\" \\"Content-Length: 13rn\\" \\"rn\\" \\"Hello, world!\\" ) self.transport.write(response.encode()) self.transport.close() async def start_server(host: str, port: int) -> None: loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HTTPServerProtocol(), host, port ) for s in (signal.SIGINT, signal.SIGTERM): loop.add_signal_handler(s, server.close) async with server: await server.serve_forever() def main(): host, port = \'localhost\', 8080 asyncio.run(start_server(host, port)) if __name__ == \\"__main__\\": main()"},{"question":"# Asyncio Networking and Task Management with Python 3.10 Objective Your task is to create an echo server and client using Python\'s `asyncio` module. The server will accept connections from clients, read incoming data, and echo back the same data to the client. Additionally, you should manage tasks and handle errors appropriately. Requirements 1. **Echo Server**: - The server should listen on `localhost` and port `8888`. - For each incoming connection, read data from the client. - Echo the received data back to the client. - Implement proper handling to ensure the server can run indefinitely and handle multiple clients simultaneously. - Cleanly handle client disconnects and errors without crashing the server. 2. **Echo Client**: - The client should connect to the server at `localhost` on port `8888`. - Send a message to the server and wait for a response. - Print the received data and close the connection. - Implement client with proper error handling and ensure it can reconnect to the server if the connection fails. Input and Output Formats - **Server Input**: None; the server listens and handles incoming connections and data. - **Client Input**: A string message to be sent to the server. - **Server Output**: None; the server echoes received data back to the client. - **Client Output**: Prints the echoed message received from the server. Constraints - Ensure you use `asyncio` paradigms including event loops, tasks, and network transports properly. - Implement both the server and client in a non-blocking manner. - Handle exceptions gracefully, ensuring the server and client can continue running despite issues. Performance Requirement - The server should be able to handle multiple clients concurrently without significant performance degradation. Example **Example usage of the client sending a message and receiving the response:** ```python # Run server.py in one terminal # Run client.py in another terminal # Output should be as follows for the client script # Client Input: \\"Hello, Server!\\" # Client Output: \\"Hello, Server!\\" ``` Server Code Skeleton ```python import asyncio async def handle_client(reader, writer): # Receive and echo back data pass async def start_server(): # Setup server and start listening pass def main(): loop = asyncio.get_event_loop() loop.run_until_complete(start_server()) loop.run_forever() if __name__ == \\"__main__\\": main() ``` Client Code Skeleton ```python import asyncio async def send_message(message): # Connect to server and send message pass def main(): message = \\"Hello, Server!\\" loop = asyncio.get_event_loop() loop.run_until_complete(send_message(message)) if __name__ == \\"__main__\\": main() ``` Complete the implementations for `handle_client`, `start_server`, and `send_message` functions by utilizing the asyncio methods covered in the documentation provided.","solution":"import asyncio async def handle_client(reader, writer): try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except asyncio.CancelledError: pass except Exception as e: print(f\\"Error: {e}\\") finally: writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'localhost\', 8888) async with server: await server.serve_forever() def main(): loop = asyncio.get_event_loop() try: loop.run_until_complete(start_server()) except KeyboardInterrupt: pass finally: loop.close() if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Implement a `CustomFormatter` class that leverages Python’s `Formatter` class to achieve a specific string formatting behavior. # Problem Statement: You are required to implement a `CustomFormatter` class. This class should extend the capabilities of Python’s `Formatter` class to meet the following specifications: 1. **Concatenate Fields**: If a special format specification `\\"+x\\"` is provided, concatenate the value of the field with its upper-case equivalent. 2. **Default Value**: If a field is missing or None, a default value of `\\"default\\"` should be used. 3. **Numeric Formats**: For numeric fields, if the format specification `\\"#0x\\"` is used, represent the number in hexadecimal format with the \\"0x\\" prefix. Class Definition ```python import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): # Override format_field() to handle custom format specifications def get_value(self, key, args, kwargs): # Override get_value() to handle default values ``` Method Declarations: - `format_field(self, value, format_spec)`: Override this method to handle the custom format specifications (`+x` and `#0x`). - `get_value(self, key, args, kwargs)`: Override this method to provide a default value of `\\"default\\"` for missing fields or None values. # Input Format: - The format string which may contain custom format specifications. - Variable number of positional and named arguments. # Output Format: - A formatted string according to the custom rules specified. # Constraints: - The input can contain alphanumeric characters, punctuation, and special format specifications. - Format specifications are not case-sensitive. # Examples: Example 1: ```python fmt = CustomFormatter() formatted_str = fmt.format(\\"This is a {test:+x}\\", test=\\"string\\") print(formatted_str) # Output: \\"This is a stringSTRING\\" ``` Example 2: ```python formatted_str = fmt.format(\\"Missing field value: {missing_field}\\", some_field=\\"non-default\\") print(formatted_str) # Output: \\"Missing field value: default\\" ``` Example 3: ```python formatted_str = fmt.format(\\"Hex Format: {number:#0x}\\", number=255) print(formatted_str) # Output: \\"Hex Format: 0xff\\" ``` # Note: 1. You are not required to handle nested replacement fields in this assessment. 2. The format specifications like `\\"#0x\\"` for hexadecimal representation and `\\"+x\\"` for concatenation should be strictly followed. Implement the `CustomFormatter` class as per the above guidelines to ensure it meets all specified behaviors.","solution":"import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): if format_spec == \\"+x\\": # Concatenate value with its upper-case equivalent return f\\"{value}{str(value).upper()}\\" elif format_spec == \\"#0x\\": # Format number in hexadecimal with \\"0x\\" prefix return f\\"{hex(value)}\\" else: # Default behavior return super().format_field(value, format_spec) def get_value(self, key, args, kwargs): # If the field is missing or None, use \\"default\\" if isinstance(key, int): value = args[key] if key < len(args) else \\"default\\" else: value = kwargs.get(key, \\"default\\") if value is None: return \\"default\\" return value"},{"question":"**Python Coding Assessment Question:** # Problem Statement: You are required to write a Python function that automates the process of creating built distributions for different formats using Distutils. The function should take module details from a setup configuration file and generate built distributions in specified formats. Additionally, it should verify the successful creation of the distributions. # Detailed Requirement: 1. **Input:** - `setup_cfg_path` (string): Path to the setup configuration file. - `dist_formats` (list of strings): List of distribution formats to generate (e.g., [\\"gztar\\", \\"zip\\", \\"rpm\\"]). 2. **Output:** - A dictionary in which keys are the distribution formats and values are boolean indicators of whether the build was successful (`True` if successful and `False` otherwise). 3. **Constraints:** - The function should handle errors gracefully and return `False` for any distribution format that fails to build. # Example: ```python def create_built_distributions(setup_cfg_path, dist_formats): Generate built distributions for the specified formats. Args: - setup_cfg_path (str): Path to the setup configuration file. - dist_formats (list of str): List of distribution formats. Returns: - dict: Dictionary with distribution formats as keys and boolean success indicators as values. # Your implementation here # Example usage: setup_cfg_path = \'path/to/setup.cfg\' dist_formats = [\'gztar\', \'zip\', \'rpm\'] result = create_built_distributions(setup_cfg_path, dist_formats) # Expected output: {\'gztar\': True, \'zip\': True, \'rpm\': False} (assuming .rpm build failed for some reason) ``` # Notes: 1. The setup configuration file contains all the necessary module details required for creating the distributions. 2. Use the Distutils commands (`bdist`, `bdist_dumb`, `bdist_rpm`, etc.) to generate the built distributions. 3. Ensure that the produced distributions are in the correct formats and store them in a `dist` directory. 4. Handle any exceptions that may occur during the build process and log the issues.","solution":"import os import subprocess import logging def create_built_distributions(setup_cfg_path, dist_formats): Generate built distributions for the specified formats. Args: - setup_cfg_path (str): Path to the setup configuration file. - dist_formats (list of str): List of distribution formats. Returns: - dict: Dictionary with distribution formats as keys and boolean success indicators as values. # Setup logging logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)s - %(levelname)s - %(message)s\') result = {} setup_dir = os.path.dirname(setup_cfg_path) for format in dist_formats: cmd = [\\"python\\", \\"setup.py\\", f\\"bdist_{format}\\", \\"--formats\\", format] logging.debug(f\\"Running command: {\' \'.join(cmd)}\\") try: subprocess.check_call(cmd, cwd=setup_dir) result[format] = True logging.debug(f\\"Successfully built distribution for format: {format}\\") except subprocess.CalledProcessError as e: logging.error(f\\"Failed to build distribution for format: {format}, error: {e}\\") result[format] = False return result"},{"question":"**Question:** You are provided with a dataset `flights` containing monthly airline passenger numbers from 1949 to 1960. Your goal is to create a multi-faceted visualization using seaborn to analyze this dataset. Specifically, you need to achieve the following: 1. Load the `flights` dataset using the `sns.load_dataset` function. 2. Create a line plot showing the number of passengers over time. 3. Add a rug plot along both axes to show the density of passenger numbers and the time points. 4. Color-code the rug plot to differentiate between years. 5. Increase the height of the rug plot to make it more prominent. 6. Ensure the rug plot lines are thinner and use alpha blending for better visualization. **Input:** No input required, as the dataset is loaded within the code. **Output:** A seaborn plot comprising the specified elements. **Constraints:** - Use the seaborn package for all visual elements. - Ensure the visualization is clear and informative. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset flights = sns.load_dataset(\\"flights\\") # Create the line plot sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") # Add rug plot along both axes with modifications sns.rugplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"year\\", height=0.1, lw=0.5, alpha=0.6) # Display the plot plt.show() ``` Your task is to complete and execute the above code to create the required visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_flights_data(): # Load dataset flights = sns.load_dataset(\\"flights\\") # Create the line plot plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", palette=\\"tab10\\") # Add rug plot along both axes with modifications sns.rugplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"year\\", height=0.1, lw=0.5, alpha=0.6) # Display the plot plt.title(\'Monthly Airline Passenger Numbers from 1949 to 1960\') plt.show()"},{"question":"You are given a high-dimensional dataset (`data.csv`), and your task is to perform unsupervised dimensionality reduction using different techniques from the scikit-learn library. You will need to apply Principal Component Analysis (PCA), Random Projections, and Feature Agglomeration, and then compare the variance ratios captured by each of these techniques. Data The CSV file `data.csv` has the following structure: - Rows represent individual samples. - Columns represent the different features. Tasks 1. Load the data from the CSV file. 2. Standardize the dataset to have zero mean and unit variance, since feature scaling can impact some dimensionality reduction techniques. 3. Apply **Principal Component Analysis (PCA)** to reduce the dataset to 2 principal components and calculate the explained variance ratio for each principal component. 4. Apply **Random Projections** to reduce the dataset to 2 dimensions and use the `Johnson-Lindenstrauss bound` to check if the projections meet the theoretical requirements. 5. Apply **Feature Agglomeration** to group together features into 2 clusters and transform the dataset accordingly. 6. Compare the variance ratios captured by PCA and the distances preserved by Random Projections. Constraints - You should utilize scikit-learn\'s PCA, random projection, and clustering classes. - Ensure that the dataset transformations and reduction steps are implemented efficiently. Output - Print the explained variance ratio for each component from PCA. - Print the results of the Johnson-Lindenstrauss bound verification for Random Projections. - Print the shape of the dataset after applying Feature Agglomeration. Code Template ```python import pandas as pd from sklearn.decomposition import PCA from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.preprocessing import StandardScaler # Load the data data = pd.read_csv(\'data.csv\') # Task 2: Standardize the dataset scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Task 3: Apply PCA pca = PCA(n_components=2) data_pca = pca.fit_transform(data_scaled) print(\\"PCA Explained Variance Ratio:\\", pca.explained_variance_ratio_) # Task 4: Apply Random Projections n_samples, n_features = data.shape jl_min_dim = johnson_lindenstrauss_min_dim(n_samples, eps=0.1) rp = SparseRandomProjection(n_components=2) data_rp = rp.fit_transform(data_scaled) print(\\"Johnson-Lindenstrauss Bound:\\", n_samples, \\"samples can be projected to\\", jl_min_dim, \\"dimensions\\") # Task 5: Apply Feature Agglomeration agglo = FeatureAgglomeration(n_clusters=2) data_agglo = agglo.fit_transform(data_scaled) print(\\"Dataset shape after Feature Agglomeration:\\", data_agglo.shape) # Task 6: Comparison (optional detailed analysis and comparison) ``` Ensure your solution is comprehensive and adheres to the provided structure. You can assume the availability of `data.csv` in the same directory as your script.","solution":"import pandas as pd from sklearn.decomposition import PCA from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(file_path): # Load the data data = pd.read_csv(file_path) # Standardize the dataset scaler = StandardScaler() data_scaled = scaler.fit_transform(data) return data, data_scaled def apply_pca(data_scaled): # Apply PCA pca = PCA(n_components=2) data_pca = pca.fit_transform(data_scaled) return pca.explained_variance_ratio_ def apply_random_projection(data_scaled, n_samples): # Apply Random Projections jl_min_dim = johnson_lindenstrauss_min_dim(n_samples, eps=0.1) rp = SparseRandomProjection(n_components=2) data_rp = rp.fit_transform(data_scaled) return jl_min_dim, data_rp.shape def apply_feature_agglomeration(data_scaled): # Apply Feature Agglomeration agglo = FeatureAgglomeration(n_clusters=2) data_agglo = agglo.fit_transform(data_scaled) return data_agglo.shape def dimensionality_reduction_comparison(file_path): # Load and preprocess data data, data_scaled = load_and_preprocess_data(file_path) n_samples, n_features = data.shape # Apply and print PCA results pca_variance_ratio = apply_pca(data_scaled) print(\\"PCA Explained Variance Ratio:\\", pca_variance_ratio) # Apply and print Random Projection results jl_min_dim, rp_shape = apply_random_projection(data_scaled, n_samples) print(\\"Johnson-Lindenstrauss Bound Verification: n_samples: {}, min_dim: {}\\".format(n_samples, jl_min_dim)) # Apply and print Feature Agglomeration results agglo_shape = apply_feature_agglomeration(data_scaled) print(\\"Dataset shape after Feature Agglomeration:\\", agglo_shape) return pca_variance_ratio, jl_min_dim, rp_shape, agglo_shape"},{"question":"# Python Coding Assessment: Unit Testing with `unittest` **Objective**: Assess your understanding and ability to use the `unittest` module for creating and running unit tests in Python. **Problem Statement**: You are tasked with creating a small library that handles basic mathematical operations and then writing unit tests to ensure its functionality. Specifically, you need to implement functions for addition, subtraction, multiplication, and division, and then verify their correctness using the `unittest` framework. **Instructions**: 1. **Library Implementation**: Create a Python module named `math_operations.py` containing the following functions: ```python def add(x, y): Return the sum of x and y. return x + y def subtract(x, y): Return the difference when y is subtracted from x. return x - y def multiply(x, y): Return the product of x and y. return x * y def divide(x, y): Return the division of x by y. Raises ValueError if y is zero. if y == 0: raise ValueError(\\"Cannot divide by zero.\\") return x / y ``` 2. **Testing**: Create a separate module named `test_math_operations.py` and write your unit tests using the `unittest` framework. Your tests should cover: - Correctness of each function (`add`, `subtract`, `multiply`, `divide`). - Edge cases such as division by zero. Here\'s an example of what your test file structure might look like: ```python import unittest from math_operations import add, subtract, multiply, divide class TestMathOperations(unittest.TestCase): def test_add(self): self.assertEqual(add(1, 2), 3) self.assertEqual(add(-1, 1), 0) def test_subtract(self): self.assertEqual(subtract(10, 5), 5) self.assertEqual(subtract(-1, 1), -2) def test_multiply(self): self.assertEqual(multiply(2, 3), 6) self.assertEqual(multiply(-1, 1), -1) def test_divide(self): self.assertEqual(divide(10, 2), 5) self.assertEqual(divide(-10, 2), -5) with self.assertRaises(ValueError): divide(10, 0) if __name__ == \'__main__\': unittest.main() ``` **Requirements**: - Implement `math_operations.py` as described. - Write comprehensive unit tests in `test_math_operations.py` covering all functions and edge cases. - Use `unittest` assertions to validate the results. - Ensure that your test suite can be run and will output a summary of results. **Submission**: Submit both `math_operations.py` and `test_math_operations.py` files. Your solution will be evaluated based on the correctness of the implementation, the comprehensiveness of your tests, and the adherence to best practices in unit testing.","solution":"def add(x, y): Return the sum of x and y. return x + y def subtract(x, y): Return the difference when y is subtracted from x. return x - y def multiply(x, y): Return the product of x and y. return x * y def divide(x, y): Return the division of x by y. Raises ValueError if y is zero. if y == 0: raise ValueError(\\"Cannot divide by zero.\\") return x / y"},{"question":"Advanced Seaborn Line Plotting **Objective:** Assess the student\'s ability to use Seaborn for creating and customizing line plots with different dataset formats. **Problem Statement:** You are provided with the `fmri` dataset from Seaborn\'s example datasets. This dataset contains signals recorded from participants\' brains during different mental conditions. You need to manipulate the data and create various line plots demonstrating your understanding of Seaborn\'s functionalities. **Requirements:** 1. Load the `fmri` dataset using `seaborn.load_dataset()`. 2. Create a line plot with the following specifications: - X-axis: `timepoint` - Y-axis: `signal` - Hue: `region` - Style: `event` - Set markers to `True` and dashes to `False`. - Display error bars as `se` (standard error) and extend them to two standard error widths. 3. Create a subplot with two line plots side by side, each for a different `region`: - The left plot should show `signals` for the `event` `stim` only. - The right plot should show `signals` for the `event` `cue` only. - Use `relplot` for creating the facet grid. 4. Customize the color palette for the plot in step 3: - Use a custom color palette with at least four distinct colors. 5. Save the final subplot from step 3 as a PNG file named `fmri_analysis.png`. **Input Format:** ```python # No input required. ``` **Output Format:** ```python # There is no need for a return statement. You only need to save the final plot as a PNG file named `fmri_analysis.png`. ``` **Performance Requirements:** - Ensure the code executes efficiently without unnecessary computations. **Constraints:** - You must use Seaborn for all plotting. - Use Pandas for any data manipulation. - The plot should be clear and well-labeled. **Example Output:** ```plaintext [The output would be a PNG file named `fmri_analysis.png` containing the subplot described in the requirements.] ``` **Hints:** - Refer to the Seaborn documentation for advanced plotting techniques and customization options. - Use the `FacetGrid` function within `relplot` to create the subplots. **Evaluation Criteria:** - Correctness: The code should correctly implement the specified plots. - Efficiency: The code should be optimized for performance. - Clarity: The plots should be well-labeled and clear. - Creativity: Effective use of Seaborn\'s customization features. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Step 2: Create the main line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, dashes=False, ci=\\"sd\\") plt.title(\'FMRI Signal Over Time by Region and Event\') plt.xlabel(\'Time (in seconds)\') plt.ylabel(\'Signal\') plt.legend(title=\'Region/Event\', loc=\'best\') plt.show() # Step 3 & 4: Create subplots with relplot and custom color palette palette = sns.color_palette(\\"husl\\", 4) g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", col=\\"event\\", kind=\\"line\\", palette=palette, col_order=[\\"stim\\", \\"cue\\"], facet_kws={\\"sharey\\": False}, ci=\\"sd\\" ) g.set_axis_labels(\\"Time (in seconds)\\", \\"Signal\\") g.set_titles(col_template=\\"{col_name} Event\\") plt.savefig(\\"fmri_analysis.png\\") plt.show()"},{"question":"# Python Coding Assessment: Working with ZIP Archives **Objective:** Demonstrate your understanding of the `zipfile` module in Python by implementing a function that performs specific operations on ZIP archives. **Function to Implement:** You need to implement a function called `process_zipfile` which takes three arguments: 1. `zip_filepath` (str): The path to the input ZIP file. 2. `output_dir` (str): The directory where extracted files should be saved. 3. `new_files` (dict): A dictionary where keys are the intended archive names within the ZIP file and values are paths to the files to be added. The function should: 1. **Extract** all files from the specified `zip_filepath` into the `output_dir`. 2. **List** all the files in the ZIP archive and print them to the standard output. 3. **Add** new files from the `new_files` dictionary into the same ZIP archive. If a file with the same name already exists in the archive, it should overwrite the old file. 4. **Return** a list of names of all files currently in the ZIP archive after modification. **Input:** - `zip_filepath` (str): Path to the ZIP file. - `output_dir` (str): Directory to extract the files. - `new_files` (dict): Dictionary containing new files to be added to the ZIP archive. **Output:** - A list of file names currently in the ZIP archive after modification. **Example:** ```python import os # Example usage: zip_filepath = \\"example.zip\\" output_dir = \\"extracted_files\\" new_files = { \\"new_file1.txt\\": \\"path/to/new_file1.txt\\", \\"new_file2.txt\\": \\"path/to/new_file2.txt\\" } # Call the function files_after_modification = process_zipfile(zip_filepath, output_dir, new_files) # Example Output print(files_after_modification) ``` **Constraints:** - You can assume the input ZIP file and directories exist and are properly accessible. - The function should handle potential exceptions (e.g., file not found, read/write errors) gracefully and provide appropriate error messages. - Ensure to use the `zipfile` module and its functionalities as specified in the documentation provided. **Performance Requirements:** - The solution should efficiently handle ZIP files up to several GB in size using the ZIP64 extensions. - The function should operate with minimal memory overhead. **Implementation Notes:** - Utilize context managers (`with` statement) for handling file operations to ensure resources are properly closed after use. - Pay attention to different modes of opening ZIP files (`\'r\'`, `\'w\'`, `\'a\'`, `\'x\'`). --- Your implementation should go below: ```python import os import zipfile def process_zipfile(zip_filepath, output_dir, new_files): try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) # Step 1: Extract all files from the ZIP archive with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: zip_ref.extractall(output_dir) # Step 2: List all files in the ZIP archive and print them with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: print(\\"Existing files in the ZIP archive:\\") zip_ref.printdir() file_list = zip_ref.namelist() # Step 3: Add new files to the ZIP archive with zipfile.ZipFile(zip_filepath, \'a\') as zip_ref: for arcname, filepath in new_files.items(): zip_ref.write(filepath, arcname) # Step 4: Return the list of names of files in the modified ZIP archive with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: updated_file_list = zip_ref.namelist() return updated_file_list except zipfile.BadZipFile: print(\\"Error: The provided file is not a valid ZIP file.\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return [] # Test the function with the provided example if __name__ == \\"__main__\\": zip_filepath = \\"example.zip\\" output_dir = \\"extracted_files\\" new_files = { \\"new_file1.txt\\": \\"path/to/new_file1.txt\\", \\"new_file2.txt\\": \\"path/to/new_file2.txt\\" } files_after_modification = process_zipfile(zip_filepath, output_dir, new_files) print(files_after_modification) ```","solution":"import os import zipfile def process_zipfile(zip_filepath, output_dir, new_files): try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) # Step 1: Extract all files from the ZIP archive with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: zip_ref.extractall(output_dir) # Step 2: List all files in the ZIP archive and print them with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: print(\\"Existing files in the ZIP archive:\\") zip_ref.printdir() file_list = zip_ref.namelist() # Step 3: Add new files to the ZIP archive with zipfile.ZipFile(zip_filepath, \'a\') as zip_ref: for arcname, filepath in new_files.items(): zip_ref.write(filepath, arcname) # Step 4: Return the list of names of files in the modified ZIP archive with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: updated_file_list = zip_ref.namelist() return updated_file_list except zipfile.BadZipFile: print(\\"Error: The provided file is not a valid ZIP file.\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"**Coding Assessment Question:** # Objective: Implement a function that takes a string of Python code and transforms all assignment statements to use augmented assignments where applicable. # Problem Statement: Write a function `transform_to_augmented_assignments(code: str) -> str` that modifies a given string of Python code to use augmented assignment (e.g., `a += 1` instead of `a = a + 1`) where feasible. Your implementation should use Python\'s `ast` module to parse the input code, modify the AST, and then convert it back to a string representation of the code. # Requirements: 1. The input will be a string containing valid Python code. 2. The function must identify patterns of the form `a = a + <expr>` and `a = a - <expr>` and transform them to `a += <expr>` and `a -= <expr>`, respectively. 3. The function should maintain the original formatting and comments of the code as closely as possible. # Constraints: - The input code string will not contain any syntax errors. - Only handle direct assignments like `a = a + 1`, and ignore complex patterns or assignments inside functions, classes, or other nested structures for simplicity. # Examples: ```python input_code_1 = a = a + 1 b = b - 2 c = c * 3 d = d / 4 output_code_1 = a += 1 b -= 2 c = c * 3 d = d / 4 input_code_2 = x = x + 3 y = y - 4 output_code_2 = x += 3 y -= 4 input_code_3 = m = n + 1 n = n + 2 output_code_3 = m = n + 1 n += 2 # Function Signature: ```python def transform_to_augmented_assignments(code: str) -> str: # Your code here pass ``` # Note: - You should use `ast.parse()` to parse the input code into an AST. - Use `NodeTransformer` to transform the AST. - Use `ast.unparse()` to convert the modified AST back to a string. - Consider all possible cases for valid augmented assignments. # Testing: Ensure your function passes the above examples and additional edge cases involving different types of expressions and assignments.","solution":"import ast import astor class AugmentedAssignmentTransformer(ast.NodeTransformer): def visit_Assign(self, node): # Only process simple assignments like a = a + b or a = a - b if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name): target = node.targets[0] value = node.value if isinstance(value, ast.BinOp) and isinstance(value.left, ast.Name): if target.id == value.left.id: if isinstance(value.op, ast.Add): return ast.AugAssign(target=target, op=ast.Add(), value=value.right) elif isinstance(value.op, ast.Sub): return ast.AugAssign(target=target, op=ast.Sub(), value=value.right) return node def transform_to_augmented_assignments(code: str) -> str: tree = ast.parse(code) transformer = AugmentedAssignmentTransformer() new_tree = transformer.visit(tree) ast.fix_missing_locations(new_tree) return astor.to_source(new_tree)"},{"question":"# Question: Implement a Neural Network with Random Initialization You are required to implement a fully connected neural network using PyTorch. The network will have one hidden layer. Specifically, you need to: 1. Initialize the weights of the network with random values. 2. Ensure that reproducibility is maintained by setting a random seed. Your task involves writing the following functions: - `initialize_weights`: This function initializes the weights of the neural network layers. - `forward`: This function performs a forward pass given an input tensor. # Function Details: 1. **initialize_weights(seed: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]**: - **Input**: An integer `seed` to ensure reproducibility. - **Output**: Four randomly initialized tensors: `w1`, `b1`, `w2`, `b2` representing the weights and biases of the two layers. 2. **forward(x: torch.Tensor, w1: torch.Tensor, b1: torch.Tensor, w2: torch.Tensor, b2: torch.Tensor) -> torch.Tensor**: - **Input**: An input tensor `x` of shape `(N, D_in)`, weight tensors `w1` and `w2`, and bias tensors `b1` and `b2`. - **Output**: An output tensor of shape `(N, D_out)` representing the final prediction. Notes: - The hidden layer uses the ReLU activation function. - The input dimension `D_in`, hidden layer dimension `H`, and output dimension `D_out` are given. - Use the random seed to ensure reproducibility for random number generation. # Example: ```python import torch D_in = 10 # Example input dimension H = 5 # Example hidden layer dimension D_out = 2 # Example output dimension # Initialize weights with a specific random seed seed = 42 w1, b1, w2, b2 = initialize_weights(seed) # Define an example input tensor x = torch.randn(3, D_in) # Perform a forward pass output = forward(x, w1, b1, w2, b2) print(output) ``` # Constraints: - Do not use any high-level libraries for neural network construction (e.g., `torch.nn`). - Ensure all random generations are compliant with the given seed value to maintain reproducibility. # Your Implementation Starts Here ```python import torch from typing import Tuple def initialize_weights(seed: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: torch.manual_seed(seed) D_in = 10 # Set your values accordingly H = 5 D_out = 2 # Initialize weights and biases with random values w1 = torch.randn(D_in, H) b1 = torch.randn(H) w2 = torch.randn(H, D_out) b2 = torch.randn(D_out) return w1, b1, w2, b2 def forward(x: torch.Tensor, w1: torch.Tensor, b1: torch.Tensor, w2: torch.Tensor, b2: torch.Tensor) -> torch.Tensor: # Hidden layer computation with ReLU activation h = torch.relu(x.mm(w1) + b1) # Output layer y = h.mm(w2) + b2 return y ``` # Explanation: - `initialize_weights`: Initializes weights and biases using the given random seed to maintain reproducibility. - `forward`: Implements the forward pass of the neural network, applying ReLU activation to the hidden layer and computing the final output.","solution":"import torch from typing import Tuple def initialize_weights(seed: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: torch.manual_seed(seed) D_in = 10 # Set your values accordingly H = 5 D_out = 2 # Initialize weights and biases with random values w1 = torch.randn(D_in, H) b1 = torch.randn(H) w2 = torch.randn(H, D_out) b2 = torch.randn(D_out) return w1, b1, w2, b2 def forward(x: torch.Tensor, w1: torch.Tensor, b1: torch.Tensor, w2: torch.Tensor, b2: torch.Tensor) -> torch.Tensor: # Hidden layer computation with ReLU activation h = torch.relu(x.mm(w1) + b1) # Output layer y = h.mm(w2) + b2 return y"},{"question":"**Question: Implement an HTTP Status Checker** You are tasked with developing a function that verifies if a series of HTTP status codes are successful or not, based on the `http.HTTPStatus` enumeration. Successful status codes are those in the range 200–299 (inclusive). # Function Signature ```python def check_http_statuses(status_codes: list) -> dict: Given a list of HTTP status codes, return a dictionary where the keys are the status codes and the values are boolean values indicating whether each status code represents a successful response. Args: status_codes (list of ints): A list of HTTP status codes (e.g., [200, 404, 503]). Returns: dict: A dictionary with status codes as keys and boolean values as values. True if the status code is successful (200-299), False otherwise. pass ``` # Input Example ```python status_codes = [200, 201, 204, 400, 500, 299] ``` # Output Example ```python { 200: True, 201: True, 204: True, 400: False, 500: False, 299: True } ``` # Constraints - All elements in the `status_codes` list are integers. - The `status_codes` list can have between 1 and 1000 elements. Using the `http.HTTPStatus` enum, determine if each status code is in the range of successful responses (200–299) or not. If the status code does not exist in `http.HTTPStatus`, consider it as a failure (False). **Hint:** You might want to use `hasattr` to check for the existence of a status code in `http.HTTPStatus`.","solution":"from http import HTTPStatus def check_http_statuses(status_codes: list) -> dict: Given a list of HTTP status codes, return a dictionary where the keys are the status codes and the values are boolean values indicating whether each status code represents a successful response. Args: status_codes (list of ints): A list of HTTP status codes (e.g., [200, 404, 503]). Returns: dict: A dictionary with status codes as keys and boolean values as values. True if the status code is successful (200-299), False otherwise. result = {} for code in status_codes: if 200 <= code <= 299: result[code] = True else: result[code] = False return result"},{"question":"# Mocking and Testing with `unittest.mock` **Problem Statement:** You are required to implement a utility function that interacts with an external data processing service. This service has a method that processes data asynchronously and returns the result. To ensure your function works correctly, you will write unit tests using the `unittest.mock` module to mock the external service interactions. # Requirements: 1. Implement a class `DataProcessor` with the method `process_data`: - Method `process_data`: - Takes an argument `data` (a list of integers). - Should interface with an external service class `ExternalService` that has an asynchronous method `async_process(data: List[int]) -> List[int]`. This method processes the data and returns a list of results. 2. Write unit tests ensuring: - `process_data` calls the `async_process` method with the correct arguments. - Handle the asynchronous nature of the method using proper mocking. - Test if `process_data` returns the correct results given mocked responses. # Implementation: Part 1: Class `DataProcessor` ```python class ExternalService: async def async_process(self, data): # Imagine this method interacts with an external service pass class DataProcessor: def __init__(self, service: ExternalService): self.service = service async def process_data(self, data): # Call the asynchronous process of the service result = await self.service.async_process(data) return result ``` Part 2: Unit Tests Implement the tests using the `unittest.mock` module in the Python standard library. ```python from unittest import IsolatedAsyncioTestCase from unittest.mock import AsyncMock, patch import asyncio class TestDataProcessor(IsolatedAsyncioTestCase): @patch(\\"path.to.module.ExternalService\\") def test_process_data(self, MockExternalService): # Given mock_service_instance = MockExternalService.return_value mock_service_instance.async_process = AsyncMock(return_value=[10, 20, 30]) processor = DataProcessor(mock_service_instance) # When data = [1, 2, 3] result = asyncio.run(processor.process_data(data)) # Then mock_service_instance.async_process.assert_called_once_with(data) self.assertEqual(result, [10, 20, 30]) if __name__ == \\"__main__\\": unittest.main() ``` # Guidelines: 1. **Input Specifications:** - An instance of `DataProcessor` with an instance of `ExternalService`. - List of integers for the `data` parameter in `process_data`. 2. **Output Specifications:** - A list of integers, which is the processed data returned from the asynchronous service. 3. **Constraints:** - You must use `unittest.mock` and `AsyncMock` for mocking. - Ensure the tests handle the asynchronous aspect correctly. Your task is to implement the `process_data` method correctly and write comprehensive unit tests using mocks to validate the implementation. # Evaluation Criteria: - Correct implementation of `process_data`. - Effective use of mocking to test asynchronous functions. - Assertions to verify the correct behavior and interactions. - Clean code and adherence to best practices for writing unit tests. Good luck!","solution":"from typing import List class ExternalService: async def async_process(self, data: List[int]) -> List[int]: # Imagine this method interacts with an external service pass class DataProcessor: def __init__(self, service: ExternalService): self.service = service async def process_data(self, data: List[int]) -> List[int]: # Call the asynchronous process of the service and return the result result = await self.service.async_process(data) return result"},{"question":"# Question: PyTorch Backend Optimization for Matrix Multiplication Background In this question, you will work with various PyTorch backend functionalities to control and optimize matrix multiplications on different hardware. Specifically, you will use `torch.backends.cuda` to toggle settings related to mixed-precision operations, and `torch.backends.cudnn` to manipulate convolution algorithms. The objective is to understand how these settings impact the performance and accuracy of matrix multiplications on GPUs. Task 1. **Matrix Multiplication Benchmark**: Implement a function `benchmark_matmul` that measures and returns the time taken to perform a large matrix multiplication using PyTorch. 2. **Toggle Mixed-Precision Settings**: Implement a function `toggle_mixed_precision` that can enable or disable the following settings: - Use of TensorFloat-32 tensor cores (`allow_tf32`) on Ampere GPUs. - Use of half-precision (FP16) accumulation (`allow_fp16_reduced_precision_reduction`). - Use of bfloat16 precision (`allow_bf16_reduced_precision_reduction`). 3. **Benchmark with Different Settings**: Implement a function `benchmark_with_settings` that uses `benchmark_matmul` to measure and print the performance and accuracy of the matrix multiplication under different mixed-precision settings applied by `toggle_mixed_precision`. Detailed Instructions 1. **Matrix Multiplication Benchmark**: ```python import torch import time def benchmark_matmul(size=2048): device = \'cuda\' if torch.cuda.is_available() else \'cpu\' a = torch.randn(size, size, device=device, dtype=torch.float32) b = torch.randn(size, size, device=device, dtype=torch.float32) start_time = time.time() c = torch.mm(a, b) torch.cuda.synchronize() # Wait for all kernels to finish end_time = time.time() return end_time - start_time ``` 2. **Toggle Mixed-Precision Settings**: ```python import torch def toggle_mixed_precision(tf32=None, fp16=None, bf16=None): if tf32 is not None: torch.backends.cuda.matmul.allow_tf32 = tf32 if fp16 is not None: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = fp16 if bf16 is not None: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = bf16 ``` 3. **Benchmark with Different Settings**: ```python def benchmark_with_settings(): settings_combinations = [ (True, True, True), (True, False, False), (False, True, False), (False, False, True), (False, False, False), ] for tf32, fp16, bf16 in settings_combinations: toggle_mixed_precision(tf32=tf32, fp16=fp16, bf16=bf16) time_taken = benchmark_matmul() print(f\\"TF32: {tf32}, FP16: {fp16}, BF16: {bf16} -> Time taken: {time_taken:.6f} seconds\\") ``` Expected Output When you run `benchmark_with_settings`, it should print the time taken for each combination of settings. For instance, on a system with a compatible GPU, the output might look like: ``` TF32: True, FP16: True, BF16: True -> Time taken: 0.123456 seconds TF32: True, FP16: False, BF16: False -> Time taken: 0.234567 seconds TF32: False, FP16: True, BF16: False -> Time taken: 0.345678 seconds TF32: False, FP16: False, BF16: True -> Time taken: 0.456789 seconds TF32: False, FP16: False, BF16: False -> Time taken: 0.567890 seconds ``` Constraints - Ensure that your code runs efficiently on both GPU (if available) and CPU. - Use only PyTorch functionalities for the task. - The test matrices for multiplication should be large enough (size >= 2048) to observe performance differences.","solution":"import torch import time def benchmark_matmul(size=2048): Measures and returns the time taken to perform a large matrix multiplication using PyTorch. Parameters: size (int): The size of the square matrices to multiply. Returns: float: Time taken to perform the matrix multiplication. device = \'cuda\' if torch.cuda.is_available() else \'cpu\' a = torch.randn(size, size, device=device, dtype=torch.float32) b = torch.randn(size, size, device=device, dtype=torch.float32) start_time = time.time() c = torch.mm(a, b) torch.cuda.synchronize() # Wait for all kernels to finish end_time = time.time() return end_time - start_time def toggle_mixed_precision(tf32=None, fp16=None, bf16=None): Enables or disables mixed-precision settings for matrix multiplication. Parameters: tf32 (bool or None): Enable/disable TensorFloat-32 tensor cores. fp16 (bool or None): Enable/disable half-precision (FP16) accumulation. bf16 (bool or None): Enable/disable bfloat16 precision. if tf32 is not None: torch.backends.cuda.matmul.allow_tf32 = tf32 if fp16 is not None: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = fp16 if bf16 is not None: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = bf16 def benchmark_with_settings(): Measures and prints the performance of matrix multiplication under different mixed-precision settings. settings_combinations = [ (True, True, True), (True, False, False), (False, True, False), (False, False, True), (False, False, False), ] for tf32, fp16, bf16 in settings_combinations: toggle_mixed_precision(tf32=tf32, fp16=fp16, bf16=bf16) time_taken = benchmark_matmul() print(f\\"TF32: {tf32}, FP16: {fp16}, BF16: {bf16} -> Time taken: {time_taken:.6f} seconds\\")"},{"question":"# Debugging Utility with `pdb` **Objective**: Implement a custom debugging utility using Python\'s `pdb` module. This utility should help in debugging a given Python function by setting breakpoints, stepping through the code, and evaluating expressions. **Task**: Write a Python function `debug_function(func, *args, **kwargs)` that: 1. Sets a breakpoint at the beginning of the function `func`. 2. Steps through each line of code in `func`. 3. Evaluates and prints the value of a specified expression each time a breakpoint is hit. 4. Continues execution until the end of the function. **Expected Input and Output**: - **Input**: - `func`: The function to be debugged. - `args`: Positional arguments to be passed to `func`. - `kwargs`: Keyword arguments to be passed to `func`. - `expression`: The expression to be evaluated and printed at each breakpoint. - **Output**: The function should print the evaluated expression at each breakpoint and return the result of `func`. **Constraints**: - You can assume that `func` is a valid Python function. - Do not modify the original function `func`. **Example Usage**: ```python def sample_function(x, y): z = x + y a = z * 2 b = a - y return b # Calling the debugging utility debug_function(sample_function, 3, 4, expression=\\"z\\") ``` **Expected Output**: ``` Evaluating expression \'z\': 7 Evaluating expression \'z\': 7 Returned value: 10 ``` **Performance Requirements**: - The debugging utility should efficiently set breakpoints and evaluate expressions without significantly impacting the performance of the function being debugged. **Note**: - Use the `pdb` module to implement the debugging functionalities. - Implement appropriate handling for cases where the specified expression is not defined in the current scope. # Solution Template: ```python import pdb def debug_function(func, *args, expression, **kwargs): def tracer(frame, event, arg): if event == \'line\': try: val = eval(expression, frame.f_globals, frame.f_locals) print(f\\"Evaluating expression \'{expression}\': {val}\\") except NameError: print(f\\"Evaluating expression \'{expression}\': Undefined in this scope\\") return tracer sys.settrace(tracer) result = func(*args, **kwargs) sys.settrace(None) print(f\\"Returned value: {result}\\") return result ``` **Hint**: You can use `sys.settrace` to set a global trace function that triggers on each line execution. This trace function can help evaluate expressions and print their values.","solution":"import sys import pdb def debug_function(func, *args, expression, **kwargs): def tracer(frame, event, arg): if event == \'line\': try: val = eval(expression, frame.f_globals, frame.f_locals) print(f\\"Evaluating expression \'{expression}\': {val}\\") except NameError: print(f\\"Evaluating expression \'{expression}\': Undefined in this scope\\") return tracer sys.settrace(tracer) try: result = func(*args, **kwargs) finally: sys.settrace(None) print(f\\"Returned value: {result}\\") return result"},{"question":"# Asynchronous Web Scraping with Error Handling You are tasked with designing a function that asynchronously fetches multiple web pages and processes the content. You will use the `concurrent.futures.ThreadPoolExecutor` for this purpose. The function should handle potential errors gracefully and keep track of both successes and failures. Specifications: 1. **Function Signature**: ```python def async_web_scraper(urls: List[str], timeout: float) -> Tuple[Dict[str, str], Dict[str, Exception]]: pass ``` 2. **Parameters**: - `urls` (List[str]): A list of URLs to fetch. - `timeout` (float): Timeout for each web request in seconds. 3. **Returns**: - A tuple containing two dictionaries: - The first dictionary maps URLs to their fetched content (as a string) for successful fetches. - The second dictionary maps URLs to the raised exceptions for failed fetches. 4. **Functionality**: - Use `ThreadPoolExecutor` to schedule and execute the fetching tasks asynchronously. - Each URL fetch should have a timeout defined by the `timeout` parameter. - Use `urllib.request` to perform the web requests. - Gracefully handle exceptions and include them in the error dictionary. 5. **Example**: ```python urls = [\\"http://www.foxnews.com/\\", \\"http://www.cnn.com/\\", \\"http://nonexistenturl.com/\\"] timeout = 5.0 success, errors = async_web_scraper(urls, timeout) print(\\"Successes:\\", success) print(\\"Errors:\\", errors) ``` Implementation Details: You will need the following imports: ```python import concurrent.futures import urllib.request from typing import List, Tuple, Dict ``` You can structure your solution with a helper function for fetching URLs, and handling exceptions in your main function to populate the success and error dictionaries. # Notes: - Be mindful of the executor\'s management of threads. - Ensure proper handling of the shutdown process to clean up resources. - Demonstrate the function with a variety of web pages, including some that are intentionally invalid to showcase error handling.","solution":"import concurrent.futures import urllib.request from typing import List, Tuple, Dict def fetch_url(url: str, timeout: float) -> str: Fetches the content of a URL with a specified timeout. :param url: URL to fetch :param timeout: Timeout in seconds :return: Content of the fetched URL :raises: Exception if the fetch fails with urllib.request.urlopen(url, timeout=timeout) as response: return response.read().decode(\'utf-8\') def async_web_scraper(urls: List[str], timeout: float) -> Tuple[Dict[str, str], Dict[str, Exception]]: Asynchronously fetches multiple web pages and processes the content. :param urls: List of URLs to fetch :param timeout: Timeout for each web request in seconds :return: A tuple containing two dictionaries: - The first dictionary maps URLs to their fetched content for successful fetches. - The second dictionary maps URLs to the exceptions raised for failed fetches. success = {} errors = {} with concurrent.futures.ThreadPoolExecutor() as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: success[url] = future.result() except Exception as e: errors[url] = e return success, errors"},{"question":"Problem Statement You are tasked with implementing a `TimezoneConverter` class in Python, which allows converting a naive datetime object to an aware datetime object for a given time zone, and vice versa. Additionally, you must provide utility methods to handle some common time operations such as formatting to a specific string format and computing the difference between two datetime objects. Requirements 1. Implement a `TimezoneConverter` class with the following methods: - `__init__(self, timezone_offset: float)`: Constructor that initializes the time zone offset in hours. - `naive_to_aware(self, naive_dt: datetime) -> datetime`: Converts a naive datetime to an aware datetime using the provided time zone offset. - `aware_to_naive(self, aware_dt: datetime) -> datetime`: Converts an aware datetime to a naive datetime by removing the time zone information. - `format_datetime(self, dt: datetime, format_str: str) -> str`: Formats the given datetime object to a string using the specified format. - `compute_difference(self, dt1: datetime, dt2: datetime) -> timedelta`: Computes the difference between two datetime objects. Input and Output Formats - Input: - `timezone_offset`: A float value representing the timezone offset in hours (e.g., for UTC+5:30, the offset would be 5.5). - `naive_dt`: A naive datetime object. - `aware_dt`: An aware datetime object. - `dt`: A datetime object (naive or aware). - `format_str`: A string representing the format for the datetime object. - `dt1`, `dt2`: Datetime objects (naive or aware). - Output: - `naive_to_aware`: An aware datetime object. - `aware_to_naive`: A naive datetime object. - `format_datetime`: A formatted string representing the datetime. - `compute_difference`: A `timedelta` object representing the duration between the two given datetime objects. Constraints - The `datetime` objects should adhere to basic constraints such as valid range for years, months, days, hours, minutes, seconds, and microseconds as described in the documentation. - Handle exceptions where necessary, such as invalid datetime inputs or unsupported formats. Example ```python from datetime import datetime # Initializing the TimezoneConverter for UTC+5:30 converter = TimezoneConverter(5.5) # Converting a naive datetime to an aware datetime naive_dt = datetime(2023, 10, 15, 12, 0) aware_dt = converter.naive_to_aware(naive_dt) # Converting the aware datetime back to a naive datetime naive_dt_converted = converter.aware_to_naive(aware_dt) # Formatting datetime objects formatted_date = converter.format_datetime(aware_dt, \\"%Y-%m-%d %H:%M:%S %Z%z\\") # Computing the difference between two datetime objects dt1 = datetime(2023, 10, 15, 12, 0) dt2 = datetime(2023, 10, 16, 12, 0) difference = converter.compute_difference(dt1, dt2) print(aware_dt) # 2023-10-15 17:30:00+05:30 print(naive_dt_converted) # 2023-10-15 12:00:00 print(formatted_date) # \'2023-10-15 17:30:00 \' print(difference) # 1 day, 0:00:00 ```","solution":"from datetime import datetime, timedelta, timezone class TimezoneConverter: def __init__(self, timezone_offset: float): Initializes the TimezoneConverter with a given timezone offset in hours. self.timezone_offset = timezone_offset self.timezone_info = timezone(timedelta(hours=timezone_offset)) def naive_to_aware(self, naive_dt: datetime) -> datetime: Converts a naive datetime to an aware datetime using the provided time zone offset. if naive_dt.tzinfo is not None: raise ValueError(\\"The input datetime object is already timezone-aware.\\") return naive_dt.replace(tzinfo=self.timezone_info) def aware_to_naive(self, aware_dt: datetime) -> datetime: Converts an aware datetime to a naive datetime by removing the time zone information. if aware_dt.tzinfo is None: raise ValueError(\\"The input datetime object is naive.\\") return aware_dt.replace(tzinfo=None) def format_datetime(self, dt: datetime, format_str: str) -> str: Formats the given datetime object to a string using the specified format. return dt.strftime(format_str) def compute_difference(self, dt1: datetime, dt2: datetime) -> timedelta: Computes the difference between two datetime objects. return dt2 - dt1"},{"question":"# Advanced PyTorch Multiprocessing Assessment Objective To assess your understanding of multiprocessing in PyTorch, specifically using `torch.multiprocessing` for sharing tensors and managing multiple subprocesses. Problem Statement You are required to implement a program that processes a large dataset using multiple subprocesses to parallelize the computation. The program should: 1. Initialize a large tensor on the CPU. 2. Divide this tensor into chunks and process each chunk in parallel using subprocesses. 3. Share the processed chunks across processes and aggregate the results. 4. Use the appropriate sharing strategies to handle CPU tensors efficiently. 5. Ensure proper management of the subprocesses and shared memory to avoid any leaks or deadlocks. Requirements 1. **Function**: `process_tensor(tensor: torch.Tensor, num_processes: int) -> torch.Tensor` - **Input**: - `tensor`: A large CPU tensor of arbitrary shape. - `num_processes`: Number of subprocesses to spawn for parallel processing. - **Output**: Aggregated tensor after processing all chunks. 2. **Constraints**: - Use the `file_system` sharing strategy for CPU tensors to avoid file descriptor limits. - Ensure that the producer process remains active until all subprocesses complete their tasks. - Follow best practices for sharing CUDA tensors if any are used in the subprocesses. 3. **Performance**: - The program should handle tensors of large dimensions efficiently. - Ensure proper cleanup of resources to avoid memory leaks. Example ```python import torch import torch.multiprocessing as mp def chunk_processing(chunk, queue): # Example processing function processed_chunk = chunk * 2 # Simple operation for illustration queue.put(processed_chunk) def process_tensor(tensor, num_processes): mp.set_sharing_strategy(\'file_system\') chunks = tensor.chunk(num_processes) queue = mp.Queue() processes = [] for chunk in chunks: p = mp.Process(target=chunk_processing, args=(chunk, queue)) p.start() processes.append(p) results = [queue.get() for _ in processes] for p in processes: p.join() return torch.cat(results) # Example usage if __name__ == \\"__main__\\": tensor = torch.randn(1000, 1000) # Large tensor num_processes = 4 result = process_tensor(tensor, num_processes) print(result.shape) ``` Notes - Ensure proper handling of shared memory and synchronization mechanisms such as events or queues. - Make sure the main process waits for all subprocesses to finish before exiting. - Handle any exceptions or errors in subprocesses and propagate them to the main process. Deliverables - Implement the `process_tensor` function. - Provide comments and documentation for your code. - Write tests to demonstrate that your implementation handles large tensors efficiently and follows the specified constraints.","solution":"import torch import torch.multiprocessing as mp def chunk_processing(chunk, queue): Example processing function to process a chunk of tensor. Multiply the chunk by 2 as a placeholder processing. processed_chunk = chunk * 2 queue.put(processed_chunk) def process_tensor(tensor, num_processes): Process a large tensor by dividing it into chunks and processing in parallel. Args: tensor (torch.Tensor): The input tensor to be processed. num_processes (int): Number of subprocesses to use for parallel processing. Returns: torch.Tensor: The aggregated tensor after processing all chunks. mp.set_sharing_strategy(\'file_system\') chunks = tensor.chunk(num_processes) queue = mp.Queue() processes = [] # Create and start subprocesses for chunk in chunks: p = mp.Process(target=chunk_processing, args=(chunk, queue)) p.start() processes.append(p) # Collect processed chunks results = [queue.get() for _ in processes] # Join subprocesses to ensure completion for p in processes: p.join() # Aggregate and return the processed chunks return torch.cat(results) # Example usage if __name__ == \\"__main__\\": tensor = torch.randn(1000, 1000) # A large tensor num_processes = 4 result = process_tensor(tensor, num_processes) print(result.shape)"},{"question":"Coding Assessment Question # Objective To assess your understanding of iterators in Python, including synchronous and asynchronous iteration protocols, and to evaluate your ability to implement custom iterators. # Problem Statement You are required to implement a custom synchronous iterator class and an asynchronous iterator class in Python. Your task is to: 1. Implement a synchronous iterator class `CustomIterator` that: - Takes an iterable (like a list) upon initialization. - Implements the iterator protocol methods `__iter__()` and `__next__()`. 2. Implement an asynchronous iterator class `AsyncCustomIterator` that: - Takes an iterable (like a list) upon initialization. - Implements the asynchronous iterator protocol methods `__aiter__()` and `__anext__()`. # Detailed Requirements Part 1: Synchronous Iterator Implement the class `CustomIterator` with the following: - **init**: Accepts an iterable and initializes the iterator. - **iter**: Returns the iterator object itself. - **next**: Returns the next item from the iterator. If there are no more items, raises `StopIteration`. ```python class CustomIterator: def __init__(self, iterable): Initialize with an iterable. self._data = iter(iterable) def __iter__(self): Return the iterator object itself. return self def __next__(self): Return the next item from the iterator. Raises StopIteration if no more items are left. try: return next(self._data) except StopIteration: raise ``` Part 2: Asynchronous Iterator Implement the class `AsyncCustomIterator` with the following: - **init**: Accepts an iterable and initializes the asynchronous iterator. - **aiter**: Returns the asynchronous iterator object itself. - **anext**: Asynchronously returns the next item from the iterator. If there are no more items, raises `StopAsyncIteration`. Use `await asyncio.sleep(0.1)` to simulate asynchronous behavior. ```python import asyncio class AsyncCustomIterator: def __init__(self, iterable): Initialize with an iterable. self._data = iter(iterable) async def __aiter__(self): Return the asynchronous iterator object itself. return self async def __anext__(self): Asynchronously returns the next item from the iterator. Raises StopAsyncIteration if no more items are left. await asyncio.sleep(0.1) # Simulate some asynchronous operation try: return next(self._data) except StopIteration: raise StopAsyncIteration ``` # Input and Output Formats Part 1: Synchronous Iterator - **Input**: An iterable (for example, `iterable = [1, 2, 3, 4, 5]`). - **Output**: An iterator object that iterates over the elements of the iterable. Part 2: Asynchronous Iterator - **Input**: An iterable (for example, `iterable = [1, 2, 3, 4, 5]`). - **Output**: An asynchronous iterator object that iterates over the elements of the iterable. # Constraints - The solution must only use standard Python libraries. - You must handle exceptions appropriately. Your implementation should demonstrate efficient use of the iterator and asynchronous iterator protocols, and handle edge cases where iterables are empty or exhausted. # Performance Requirements - Iteration should be handled in a time complexity proportional to the size of the iterable object. Good luck!","solution":"class CustomIterator: def __init__(self, iterable): Initialize with an iterable. self._data = iter(iterable) def __iter__(self): Return the iterator object itself. return self def __next__(self): Return the next item from the iterator. Raises StopIteration if no more items are left. try: return next(self._data) except StopIteration: raise import asyncio class AsyncCustomIterator: def __init__(self, iterable): Initialize with an iterable. self._data = iter(iterable) async def __aiter__(self): Return the asynchronous iterator object itself. return self async def __anext__(self): Asynchronously returns the next item from the iterator. Raises StopAsyncIteration if no more items are left. await asyncio.sleep(0.1) # Simulate some asynchronous operation try: return next(self._data) except StopIteration: raise StopAsyncIteration"},{"question":"**Objective:** Implement and work with custom `Policy` objects in the `email.policy` module. **Problem Statement:** You are provided with an email message stored in a text file (e.g., `email_message.txt`). Your task is to implement a Python function `process_email_with_policy` that: 1. Reads the email message from the file. 2. Uses a custom policy to parse the email message. 3. Modifies certain headers of the email message using the custom policy. 4. Writes the modified email message to a new file while ensuring compliance with email RFCs. **Function Signature:** ```python def process_email_with_policy(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): Path to the input text file containing the raw email message. - `output_file` (str): Path to the output text file where the modified email message should be written. **Output:** - The function should not return anything. It should write the modified email message directly to `output_file`. **Constraints:** - Use a custom policy that inherits from `EmailPolicy` and has the following attributes: - `max_line_length` set to 100. - `linesep` set to `rn`. - `raise_on_defect` set to `True`. - `utf8` set to `True`. **Requirements:** 1. Parse the email message using the custom policy. 2. Modify the `Subject` header to add a prefix \\"[MODIFIED] \\". 3. If the `To` header has more than one recipient, raise an exception. 4. Write the modified email message to the output file using the custom policy. **Example:** Suppose `email_message.txt` contains: ``` From: sender@example.com To: recipient@example.com Subject: Test email This is a test email. ``` Running `process_email_with_policy(\'email_message.txt\', \'modified_email_message.txt\')` should create `modified_email_message.txt` with the following content: ``` From: sender@example.com To: recipient@example.com Subject: [MODIFIED] Test email This is a test email. ``` **Implementation Details:** - Use `message_from_binary_file` from the `email` package to read the email message from the file. - Implement a custom policy class inheriting from `EmailPolicy`. - Use the custom policy to parse and modify the email message. - Use the `BytesGenerator` to write the modified email message to the output file. **Hints:** - Refer to the `Policy` class and its methods such as `clone()` to create and modify policies. - Use the `EmailMessage` class and its methods to work with email headers. - Handle any exceptions as specified (e.g., multiple recipients in the `To` header).","solution":"from email import policy from email.policy import EmailPolicy from email.parser import BytesParser from email.generator import BytesGenerator import os class CustomPolicy(EmailPolicy): def __init__(self): super().__init__( max_line_length=100, linesep=\'rn\', raise_on_defect=True, utf8=True ) def process_email_with_policy(input_file: str, output_file: str) -> None: custom_policy = CustomPolicy() with open(input_file, \'rb\') as f: msg = BytesParser(policy=custom_policy).parse(f) if msg[\'To\'] and len(msg[\'To\'].addresses) > 1: raise ValueError(\\"Multiple recipients found in \'To\' header.\\") if msg[\'Subject\']: msg.replace_header(\'Subject\', \'[MODIFIED] \' + msg[\'Subject\']) with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=custom_policy) generator.flatten(msg)"},{"question":"# Secure File Operations with `tempfile` # Question: You are developing an application that processes and stores user-generated reports. These reports are processed in various stages, each requiring temporary storage at different points in your pipeline. To ensure that your application handles temporary files securely and efficiently, you decide to use the `tempfile` module. Your task is to write a function `process_report` that takes a string `report_data`, processes it, and outputs a cleaned and summarized version of the report. The function should use temporary files for intermediate processing steps. Function Signature: ```python def process_report(report_data: str) -> str: # Your code here ``` Process Description: 1. **Initial Storage**: The raw report data should first be stored in a temporary file. 2. **Cleanup Step**: Read the content from the temporary file, clean up any non-alphanumeric characters except for spaces, and write the cleaned data to a new temporary file. 3. **Summarize Step**: Read the cleaned data, summarize it by extracting every word that appears more than once (case insensitive), and write the summarized data to another temporary file. 4. **Final Output**: Read the summarized data and return it as a string output from the function. Example: ```python input_data = \\"Report: The annual sales were up by 20%. However, costs increased by 10%.\\" output = process_report(input_data) print(output) # Expected: \\"REPORT THE BY\\" ``` Notes: - Use the `tempfile` module functions and/or classes appropriately to handle temporary files. - Ensure all temporary files are cleaned up (deleted) after use, regardless of whether an error occurs. - The function should handle exceptions and ensure that no temporary files are left behind. Constraints: - The content of `report_data` will not exceed 10,000 characters. - The function should be designed to run efficiently, using appropriate temporary storage solutions. Hints: - You might find the `NamedTemporaryFile` function useful to ensure that temporary files are correctly handled. - Considering using `with` statements to ensure files and directories are properly closed and cleaned up.","solution":"import tempfile import re def process_report(report_data: str) -> str: cleaned_data = \'\' summarized_data = \'\' # Step 1: Initial Storage with tempfile.NamedTemporaryFile(\'w+\', delete=False) as temp_initial: temp_initial.write(report_data) temp_initial.flush() # Step 2: Cleanup Step temp_initial.seek(0) raw_data = temp_initial.read() # Removing non-alphanumeric characters except for spaces cleaned_data = re.sub(r\'[^a-zA-Z0-9 ]\', \'\', raw_data) with tempfile.NamedTemporaryFile(\'w+\', delete=False) as temp_cleaned: temp_cleaned.write(cleaned_data) temp_cleaned.flush() # Step 3: Summarize Step temp_cleaned.seek(0) cleaned_data = temp_cleaned.read() # Extracting words that appear more than once (case insensitive) word_count = {} words = cleaned_data.split() for word in words: word_lower = word.lower() word_count[word_lower] = word_count.get(word_lower, 0) + 1 summarized_data = \' \'.join(sorted(set(word.upper() for word, count in word_count.items() if count > 1))) # Return the summarized data return summarized_data"},{"question":"# Question You are given multiple data fields that need to be encoded into XDR format and then decoded back to verify correctness. The data fields are outlined as follows: - An integer representing `user_id` - A string representing `username` - An array of floats representing `scores` (variable length) - A string representing `comments` (variable length) Your task is to write two functions: 1. **`pack_data(user_id, username, scores, comments) -> bytes`**: - **Input**: - `user_id` (integer): A user\'s ID. - `username` (string): A user\'s name. - `scores` (list of floats): A list of scores. - `comments` (string): A comment about the user. - **Output**: - A byte string representing the packed data in XDR format. 2. **`unpack_data(xdr_data: bytes) -> tuple`**: - **Input**: - `xdr_data` (bytes): A byte string containing the packed XDR data. - **Output**: - A tuple (`user_id`, `username`, `scores`, `comments`) representing the unpacked data, with the same structure as the inputs to the `pack_data` function. # Constraints - You must use `xdrlib` for packing and unpacking the data. - The `username` length will not exceed 100 characters. - The `comments` length will not exceed 256 characters. - The number of `scores` will not exceed 50. # Example ```python data = (12345, \\"john_doe\\", [78.5, 88.0, 92.3], \\"Excellent performance in the recent tests.\\") packed = pack_data(*data) print(packed) # This should print the XDR encoded byte string. unpacked = unpack_data(packed) print(unpacked) # This should print (12345, \\"john_doe\\", [78.5, 88.0, 92.3], \\"Excellent performance in the recent tests.\\") ``` Implement both functions ensuring that the data is packed and unpacked correctly.","solution":"import xdrlib def pack_data(user_id, username, scores, comments): Packs the given data into XDR format. Args: - user_id (int): A user\'s ID. - username (str): A user\'s name. - scores (list of floats): A list of scores. - comments (str): A comment about the user. Returns: - bytes: The packed data in XDR format. packer = xdrlib.Packer() packer.pack_int(user_id) packer.pack_string(username.encode(\'utf-8\')) packer.pack_array(scores, packer.pack_double) packer.pack_string(comments.encode(\'utf-8\')) return packer.get_buffer() def unpack_data(xdr_data): Unpacks the given XDR data into its original format. Args: - xdr_data (bytes): The packed XDR data. Returns: - tuple: The unpacked data in the format (user_id, username, scores, comments). unpacker = xdrlib.Unpacker(xdr_data) user_id = unpacker.unpack_int() username = unpacker.unpack_string().decode(\'utf-8\') scores = unpacker.unpack_array(unpacker.unpack_double) comments = unpacker.unpack_string().decode(\'utf-8\') return (user_id, username, scores, comments)"},{"question":"# Feature Selection with Scikit-learn You are given a dataset consisting of several features and labels. Your task is to implement and compare different feature selection techniques provided by the `sklearn.feature_selection` module. You will then use these methods to create a feature selection pipeline and evaluate the performance of the final models. # Task 1. Implement a function `select_features_variance_threshold` that takes a dataset `X` and a threshold `thresh` as input and returns a transformed dataset with low-variance features removed. ```python def select_features_variance_threshold(X, thresh): Select features based on variance threshold. Parameters: X (array-like): Feature dataset. thresh (float): Variance threshold. Returns: array-like: Transformed dataset with low-variance features removed. pass ``` 2. Implement a function `select_features_univariate` that takes a dataset `X`, labels `y`, a feature selection method, and the number `k` of top features to keep, and returns a transformed dataset. ```python def select_features_univariate(X, y, method, k): Select top k features based on a univariate feature selection method. Parameters: X (array-like): Feature dataset. y (array-like): Labels. method (str): Univariate selection method (\'chi2\', \'f_classif\', or \'mutual_info_classif\'). k (int): Number of top features to select. Returns: array-like: Transformed dataset with top k features. pass ``` 3. Implement a function `select_features_recursive` that takes a dataset `X`, labels `y`, an estimator, and the number `n_features_to_select` and returns a transformed dataset using recursive feature elimination. ```python def select_features_recursive(X, y, estimator, n_features_to_select): Select features based on recursive feature elimination. Parameters: X (array-like): Feature dataset. y (array-like): Labels. estimator (object): Estimator with `coef_` or `feature_importances_` attribute. n_features_to_select (int): The number of features to select. Returns: array-like: Transformed dataset with selected features. pass ``` 4. Create a function `create_pipeline` that takes the outputs from the above feature selection functions, a classifier, and returns a scikit-learn pipeline. Use the pipeline to fit and predict on a given dataset. ```python def create_pipeline(selector, classifier, X, y): Create a pipeline with a feature selector and a classifier. Parameters: selector (tuple): A tuple with the feature selector method and parameters. classifier (object): An instance of a classifier. X (array-like): Feature dataset. y (array-like): Labels. Returns: object: A trained scikit-learn pipeline. pass ``` # Constraints - The dataset `X` can be a NumPy array or a Pandas DataFrame. - For `select_features_univariate`, the `method` parameter can be one of `[\'chi2\', \'f_classif\', \'mutual_info_classif\']`. - The `estimator` in `select_features_recursive` can be any estimator from scikit-learn that provides `coef_` or `feature_importances_`. # Example ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC X, y = load_iris(return_X_y=True) # Select features using variance threshold X_var = select_features_variance_threshold(X, thresh=0.1) # Select top 3 features using f_classif X_uni = select_features_univariate(X, y, method=\'f_classif\', k=3) # Select top 2 features using RFE and Logistic Regression X_rfe = select_features_recursive(X, y, estimator=SVC(kernel=\\"linear\\"), n_features_to_select=2) # Create a pipeline and evaluate pipeline = create_pipeline((\'rfe\', {\'estimator\': LogisticRegression(), \'n_features_to_select\': 2}), LogisticRegression(), X, y) ``` # Notes - Ensure that your functions handle edge cases and invalid inputs gracefully. - Document each function with appropriate docstrings and comments. - Compare the performance of models trained with different feature selection methods and provide insights based on your comparisons.","solution":"from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif, mutual_info_classif, RFE from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_iris def select_features_variance_threshold(X, thresh): Select features based on variance threshold. Parameters: X (array-like): Feature dataset. thresh (float): Variance threshold. Returns: array-like: Transformed dataset with low-variance features removed. selector = VarianceThreshold(threshold=thresh) return selector.fit_transform(X) def select_features_univariate(X, y, method, k): Select top k features based on a univariate feature selection method. Parameters: X (array-like): Feature dataset. y (array-like): Labels. method (str): Univariate selection method (\'chi2\', \'f_classif\', or \'mutual_info_classif\'). k (int): Number of top features to select. Returns: array-like: Transformed dataset with top k features. if method == \'chi2\': selector = SelectKBest(chi2, k=k) elif method == \'f_classif\': selector = SelectKBest(f_classif, k=k) elif method == \'mutual_info_classif\': selector = SelectKBest(mutual_info_classif, k=k) else: raise ValueError(f\\"Unknown method: {method}\\") return selector.fit_transform(X, y) def select_features_recursive(X, y, estimator, n_features_to_select): Select features based on recursive feature elimination. Parameters: X (array-like): Feature dataset. y (array-like): Labels. estimator (object): Estimator with `coef_` or `feature_importances_` attribute. n_features_to_select (int): The number of features to select. Returns: array-like: Transformed dataset with selected features. selector = RFE(estimator, n_features_to_select=n_features_to_select) return selector.fit_transform(X, y) def create_pipeline(selector_param, classifier, X, y): Create a pipeline with a feature selector and a classifier. Parameters: selector_param (tuple): A tuple with the feature selector method and parameters. classifier (object): An instance of a classifier. X (array-like): Feature dataset. y (array-like): Labels. Returns: object: A trained scikit-learn pipeline. selector_method, selector_args = selector_param if selector_method == \'variance_threshold\': selector = VarianceThreshold(**selector_args) elif selector_method == \'univariate\': method = selector_args.pop(\'method\') k = selector_args.pop(\'k\') selector = SelectKBest(eval(method), k=k) # eval for method \'chi2\', \'f_classif\', etc. elif selector_method == \'rfe\': estimator = selector_args.pop(\'estimator\') n_features_to_select = selector_args.pop(\'n_features_to_select\') selector = RFE(estimator, n_features_to_select=n_features_to_select) else: raise ValueError(f\\"Unknown selector method: {selector_method}\\") pipeline = Pipeline([ (\'feature_selection\', selector), (\'classification\', classifier) ]) pipeline.fit(X, y) return pipeline"},{"question":"# Filename Pattern Filtering and Counting Problem Statement You are tasked with writing a Python function to filter a list of filenames based on a given pattern and to count the occurrences of each file extension in the filtered results. This will demonstrate your understanding of the `fnmatch` module and string manipulation. Function Signature ```python def filter_and_count_filenames(filenames: list, pattern: str) -> dict: Filters the given list of filenames based on the specified pattern and returns a dictionary with file extensions as keys and their counts as values. Parameters: filenames (list): A list of filenames (strings) to filter and count. pattern (str): The pattern to filter filenames. Returns: dict: A dictionary where keys are file extensions and values are the counts of those extensions in the filtered filenames. ``` Input - `filenames`: A list of strings representing filenames. Each filename will have an extension (e.g., \\"file.txt\\"). - `pattern`: A string pattern using Unix shell-style wildcards to filter the filenames (e.g., \\"*.txt\\", \\"?.py\\"). Output - A dictionary with file extensions (including the period) as keys and their counts as values in the filtered set of filenames. Constraints - Each filename in `filenames` will have only one period which separates the file name and extension. - The pattern will be a valid Unix shell-style wildcard. - Your implementation should make use of the `fnmatch` module for pattern matching. Example ```python filenames = [ \\"report.docx\\", \\"data.csv\\", \\"summary.txt\\", \\"analysis.py\\", \\"data_backup.csv\\", \\"code.py\\", \\"notes.txt\\", \\"archive.tar.gz\\" ] pattern = \\"*.py\\" result = filter_and_count_filenames(filenames, pattern) print(result) # Output: {\'.py\': 2} ``` In this example, applying the pattern `*.py` filters the filenames to `[\\"analysis.py\\", \\"code.py\\"]`. The resulting dictionary `{\'.py\': 2}` indicates that there are two files with the `.py` extension in the filtered result. Notes - You may use the `fnmatch.filter()` function to filter the filenames. - Ensure your function efficiently handles the list of filenames and counts the occurrences accurately. Evaluation - Correct implementation and usage of the `fnmatch` module. - Correct counting and dictionary manipulation. - Performance considerations for large lists of filenames.","solution":"import fnmatch def filter_and_count_filenames(filenames: list, pattern: str) -> dict: Filters the given list of filenames based on the specified pattern and returns a dictionary with file extensions as keys and their counts as values. Parameters: filenames (list): A list of filenames (strings) to filter and count. pattern (str): The pattern to filter filenames. Returns: dict: A dictionary where keys are file extensions and values are the counts of those extensions in the filtered filenames. filtered_filenames = fnmatch.filter(filenames, pattern) extension_count = {} for filename in filtered_filenames: _, extension = filename.rsplit(\'.\', 1) extension = \'.\' + extension if extension in extension_count: extension_count[extension] += 1 else: extension_count[extension] = 1 return extension_count"},{"question":"As part of this assessment, you are tasked with evaluating the similarity of two sets of generated tensors using PyTorch. You will need to implement a function that leverages PyTorch\'s `torch.ao.ns.fx.utils` module to compute various similarity metrics between corresponding tensors of the two sets. Function Signature ```python def evaluate_tensor_similarity(tensors_set_1, tensors_set_2): Given two lists of tensors of the same length, this function computes three similarity metrics for each pair of corresponding tensors in the lists. Args: tensors_set_1 (list of torch.Tensor): First list of tensors. tensors_set_2 (list of torch.Tensor): Second list of tensors. Returns: list of dict: A list containing a dictionary for each tensor pair, where each dictionary has the keys \'sqnr\', \'normalized_l2\', and \'cosine_similarity\', and their corresponding computed values. Raises: ValueError: If the input lists are not of the same length. pass ``` Implementation Requirements 1. **Input**: Two lists of tensors, `tensors_set_1` and `tensors_set_2`. Both lists will contain tensors of the same shape and the same length - this can be assumed. 2. **Output**: A list of dictionaries. Each dictionary corresponds to a pair of tensors, containing the similarity metrics \'sqnr\', \'normalized_l2\', and \'cosine_similarity\'. 3. **Constraints**: * You must raise a `ValueError` if the two input lists are not of the same length. * Utilize the functions from `torch.ao.ns.fx.utils` module to compute the metrics. 4. **Performance**: * Ensure the function handles varying tensor sizes and performs computations efficiently. Example ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Define the function def evaluate_tensor_similarity(tensors_set_1, tensors_set_2): if len(tensors_set_1) != len(tensors_set_2): raise ValueError(\\"The input lists must have the same length.\\") result = [] for t1, t2 in zip(tensors_set_1, tensors_set_2): metrics = { \\"sqnr\\": compute_sqnr(t1, t2), \\"normalized_l2\\": compute_normalized_l2_error(t1, t2), \\"cosine_similarity\\": compute_cosine_similarity(t1, t2) } result.append(metrics) return result # Create example tensors set_1 = [torch.randn(3, 3), torch.randn(3, 3)] set_2 = [torch.randn(3, 3), torch.randn(3, 3)] # Call the function with example data output = evaluate_tensor_similarity(set_1, set_2) print(output) ``` The provided example demonstrates how to use the utility functions to compute the required metrics for each pair of tensors from the given sets.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_tensor_similarity(tensors_set_1, tensors_set_2): Given two lists of tensors of the same length, this function computes three similarity metrics for each pair of corresponding tensors in the lists. Args: tensors_set_1 (list of torch.Tensor): First list of tensors. tensors_set_2 (list of torch.Tensor): Second list of tensors. Returns: list of dict: A list containing a dictionary for each tensor pair, where each dictionary has the keys \'sqnr\', \'normalized_l2\', and \'cosine_similarity\', and their corresponding computed values. Raises: ValueError: If the input lists are not of the same length. if len(tensors_set_1) != len(tensors_set_2): raise ValueError(\\"The input lists must have the same length.\\") result = [] for t1, t2 in zip(tensors_set_1, tensors_set_2): metrics = { \\"sqnr\\": compute_sqnr(t1, t2), \\"normalized_l2\\": compute_normalized_l2_error(t1, t2), \\"cosine_similarity\\": compute_cosine_similarity(t1, t2) } result.append(metrics) return result"},{"question":"# Question: Implement a Custom Sound File Type Identifier You are required to create a custom function that identifies the type of a sound file using the `sndhdr` module and extends its functionality by including additional details in the output. Specifications: 1. **Function Name:** `identify_sound_file` 2. **Input:** - `filename` (string): The path to the sound file you want to identify. 3. **Output:** - A dictionary with the following keys: - `filetype` (string): Type of the sound file (e.g., \'wav\', \'aiff\'). - `framerate` (int): The framerate (sampling rate) of the sound file or `0` if unknown. - `nchannels` (int): The number of channels in the sound file or `0` if unknown. - `nframes` (int): The number of frames in the sound file or `-1` if unknown. - `sampwidth` (string or int): The sample width in bits or `\'A\'` for A-LAW / `\'U\'` for u-LAW. - `duration` (float): Duration of the sound file in seconds, calculated as `(nframes / framerate)`. If framerate is `0`, set duration to `None`. 4. **Constraints:** - Use the `sndhdr.whathdr` function to determine the attributes of the sound file. - Handle cases where `sndhdr.whathdr` returns `None` by raising a `ValueError` with a message `\\"Cannot identify sound file.\\"` - Ensure that the `duration` field is calculated correctly and handled gracefully if framerate is `0`. 5. **Example:** ```python def identify_sound_file(filename): # Your implementation here # Example usage filename = \'example.wav\' result = identify_sound_file(filename) print(result) ``` If `sndhdr.whathdr(filename)` returns a namedtuple with the values `(\\"wav\\", 44100, 2, 176400, 16)`, then the output dictionary should be: ```python { \'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 176400, \'sampwidth\': 16, \'duration\': 4.0 } ``` If `sndhdr.whathdr(filename)` returns `None`, the function should raise: ```python ValueError: Cannot identify sound file. ``` Note: - This question tests your understanding of file handling, `namedtuple`, and basic audio file attributes computation. - You are expected to handle edge cases and ensure the program does not crash unexpectedly.","solution":"import sndhdr def identify_sound_file(filename): Identifies the type of a sound file and returns its details. Args: filename (str): The path to the sound file to identify. Returns: dict: A dictionary with keys \'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\' and \'duration\'. hdr = sndhdr.whathdr(filename) if hdr is None: raise ValueError(\\"Cannot identify sound file.\\") filetype, framerate, nchannels, nframes, sampwidth = hdr if framerate == 0: duration = None else: duration = nframes / framerate return { \'filetype\': filetype, \'framerate\': framerate, \'nchannels\': nchannels, \'nframes\': nframes, \'sampwidth\': sampwidth, \'duration\': duration }"},{"question":"**Objective:** Your task is to write a PyTorch code to perform the following operations leveraging `torch.xpu` functionality. This will involve managing multiple devices, streams, memory, and ensuring consistent random number generation across devices. **Question:** 1. Initialize the XPU devices. Ensure to check if XPUs are available. If not, the code should raise an appropriate error. 2. Create two streams for parallel computation on the devices. 3. Allocate memory for two large tensors (size: 10,000 x 10,000) on two separate devices. 4. Assign a specific manual seed for random number generation for consistency across runs. 5. Perform a simple matrix multiplication on each device using the respective streams. 6. Synchronize the devices to ensure the operations complete. 7. Retrieve and print the memory usage statistics for both devices. 8. Clear the cached memory on both devices at the end of the computation. **Constraints:** - You may assume a maximum of 2 available XPUs. - Ensure that the code handles scenarios where fewer devices are available gracefully. - The operations should be encapsulated in functions where applicable, which allows for modular testing and reusability. **Input:** No user input required. All configurations and parameters should be set within your code. **Output:** Print the memory allocated and reserved on each device. **Performance Requirements:** - The code should be efficient in managing device resources. - Avoid deep copying between devices; perform operations directly on allocated memory. **Helpful methods from `torch.xpu` to use:** - `init()` - `is_available()` - `device_count()` - `set_device()` - `Stream()` - `manual_seed()` - `memory_allocated()` - `memory_reserved()` - `synchronize()` - `empty_cache()` **Function Signatures:** ```python def initialize_xpu() -> int: # Initialize the XPU devices and return the number of devices available def create_streams(device_count: int): # Create and return streams for the devices def allocate_tensors(device_count: int): # Allocate and return tensors on the devices def set_random_seed(seed: int): # Set a manual seed for randomness def matrix_multiplication(tensor1, tensor2): # Perform matrix multiplication on the allocated tensors def synchronize_devices(): # Synchronize all devices def print_memory_statistics(device_count: int): # Print memory usage statistics for the devices def clear_memory(device_count: int): # Clear memory cache on all devices ``` Implement the above functions and integrate them in the main block to perform the complete task.","solution":"import torch def initialize_xpu(): if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") device_count = torch.xpu.device_count() if device_count < 2: raise RuntimeError(\\"Fewer than 2 XPUs available\\") torch.xpu.init() return device_count def create_streams(device_count): streams = [] for i in range(device_count): torch.xpu.set_device(i) streams.append(torch.xpu.Stream()) return streams def allocate_tensors(device_count): tensors = [] size = (10000, 10000) for i in range(device_count): torch.xpu.set_device(i) tensors.append(torch.rand(size, device=f\'xpu:{i}\')) return tensors def set_random_seed(seed): torch.manual_seed(seed) def matrix_multiplication(tensor1, tensor2): torch.xpu.set_device(tensor1.device) stream = torch.xpu.current_stream() with torch.xpu.stream(stream): result = torch.mm(tensor1, tensor2) torch.xpu.synchronize() return result def synchronize_devices(): for i in range(torch.xpu.device_count()): torch.xpu.set_device(i) torch.xpu.synchronize() def print_memory_statistics(device_count): for i in range(device_count): torch.xpu.set_device(i) allocated = torch.xpu.memory_allocated() reserved = torch.xpu.memory_reserved() print(f\'Device {i}: Allocated memory: {allocated}, Reserved memory: {reserved}\') def clear_memory(device_count): for i in range(device_count): torch.xpu.set_device(i) torch.xpu.empty_cache() if __name__ == \\"__main__\\": try: device_count = initialize_xpu() streams = create_streams(device_count) tensors = allocate_tensors(device_count) set_random_seed(42) results = [] for i in range(device_count): result = matrix_multiplication(tensors[i], tensors[i]) results.append(result) synchronize_devices() print_memory_statistics(device_count) clear_memory(device_count) except RuntimeError as e: print(e)"},{"question":"Use the seaborn library to create the following plots with specific requirements for dataset loading, plot types, and legend manipulation. Task 1: Load dataset and basic plot 1. Load the \\"penguins\\" dataset from seaborn\'s built-in datasets. 2. Create a histogram showing the distribution of `bill_length_mm` with different colors for each species. 3. Move the legend to the center right position. Task 2: Advanced legend manipulation 1. Create a histogram showing the distribution of `bill_length_mm` with hues based on species. 2. Move the legend to the upper left outside the plot area using `bbox_to_anchor`. Task 3: FacetGrid usage 1. Create a FacetGrid plot displaying histograms of `bill_length_mm` grouped by `species` and faceted by the `island` column using a 2 column wrap and a height of 3. 2. Move the legend to the upper left corner of each plot, ensuring there is no extra blank space on the right. Task 4: Fine-tuning the legend 1. Create a histogram showing the distribution of `bill_length_mm` with hues based on species. 2. Move the legend to the lower center, removing the title and frame, and arrange the legend items in 3 columns. # Input and Output Formats The plots should be generated within the script or Jupyter notebook and visualized. There are no specific input and output formats, but the visual output should meet the specified requirements. # Constraints - All plots must be generated using seaborn version 0.11.2 or later. - Ensure that the plots are clearly labeled and the legends do not overlap with the data. # Performance Requirements - Efficiently load the dataset and generate the plots with minimal computation overhead. **Example** ```python import seaborn as sns sns.set_theme() # Task 1 penguins = sns.load_dataset(\\"penguins\\") ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") # Task 2 ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax2, \\"upper left\\", bbox_to_anchor=(1, 1)) # Task 3 g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) # Task 4 ax4 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend( ax4, \\"lower center\\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False, ) ```","solution":"import seaborn as sns # Task 1: Load dataset and basic plot def task_1(): penguins = sns.load_dataset(\\"penguins\\") ax1 = sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax1.legend(loc=\'center right\') # Task 2: Advanced legend manipulation def task_2(): penguins = sns.load_dataset(\\"penguins\\") ax2 = sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax2.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) # Task 3: FacetGrid usage def task_3(): penguins = sns.load_dataset(\\"penguins\\") g = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\", col_wrap=2, height=3) g.map(sns.histplot, \\"bill_length_mm\\") g.add_legend() for ax in g.axes.flat: ax.legend(loc=\'upper left\', frameon=False) # Task 4: Fine-tuning the legend def task_4(): penguins = sns.load_dataset(\\"penguins\\") ax4 = sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") handles, labels = ax4.get_legend_handles_labels() ax4.legend( handles=handles, labels=labels, loc=\'lower center\', bbox_to_anchor=(0.5, -0.2), ncol=3, title=None, frameon=False )"},{"question":"Efficient Multiplexing of File Descriptors using `select` Module Objective Demonstrate your understanding of the `select` module by implementing a function that monitors multiple file descriptors and efficiently handles I/O events. Problem Statement You are required to implement a function `monitor_io(file_descriptors: List[int], timeout: float) -> Tuple[List[int], List[int], List[int]]` that monitors a list of file descriptors for reading, writing, and exceptional conditions using the `select.select()` function. Function Signature ```python def monitor_io(file_descriptors: List[int], timeout: float) -> Tuple[List[int], List[int], List[int]]: pass ``` Input - `file_descriptors`: A list of integers representing the file descriptors you need to monitor. You can assume that these file descriptors are valid and represent open files or sockets. - `timeout`: A float representing the maximum time (in seconds) the function should wait while monitoring the file descriptors. Output - The function should return a tuple of three lists: - List of file descriptors that are ready for reading. - List of file descriptors that are ready for writing. - List of file descriptors with exceptional conditions. Constraints: - The list of file descriptors will contain at most 1024 elements. - The timeout will be a positive floating-point number, or None to block indefinitely. Example ```python import os import socket # Example Setup sock1, sock2 = socket.socketpair() with open(\'somefile.txt\', \'w+\') as f: fds = [sock1.fileno(), f.fileno()] # Writing to sock2 and syncing the file to have data available for reading sock2.send(b\\"hello\\") f.write(\\"world\\") f.flush() # Function Call ready_to_read, ready_to_write, exceptionals = monitor_io(fds, 1.0) print(\\"Ready to Read:\\", ready_to_read) print(\\"Ready to Write:\\", ready_to_write) print(\\"Exceptional Conditions:\\", exceptionals) # Expected Output (actual values may vary based on exact timing and state): # Ready to Read: [sock1.fileno(), f.fileno()] # Ready to Write: [sock1.fileno()] # Exceptional Conditions: [] ``` Notes - Ensure your implementation handles platform-specific limitations described in the documentation. - You may use the constants and methods as described in the module documentation to handle different event types. - Consider edge cases, such as the file descriptors becoming unavailable during the monitoring period. Evaluation Criteria - Correct use of the `select.select()` function. - Accurate handling of the input file descriptors for reading, writing, and exceptional conditions. - Robustness and efficiency of the solution. - Adherence to the given function signature and specifications.","solution":"import select from typing import List, Tuple def monitor_io(file_descriptors: List[int], timeout: float) -> Tuple[List[int], List[int], List[int]]: Monitors a list of file descriptors for reading, writing, and exceptional conditions. Args: file_descriptors (List[int]): List of file descriptors to monitor. timeout (float): Maximum time (in seconds) to wait for an event. Returns: Tuple[List[int], List[int], List[int]]: A tuple containing three lists: - List of file descriptors ready for reading. - List of file descriptors ready for writing. - List of file descriptors with exceptional conditions. # We use the select.select() function to monitor the given file descriptors. # The same file descriptors list is passed for reading, writing, and exceptional conditions checks. readable, writable, exceptional = select.select(file_descriptors, file_descriptors, file_descriptors, timeout) return readable, writable, exceptional"},{"question":"# Python310 Advanced Coding Assessment: Objective: Implement a class that supports both the *tp_call* and vectorcall protocols, and demonstrate their usage to ensure compatibility and efficiency. Instructions: 1. **Class Implementation:** - Implement a Python class `CallableClass` that supports both the *tp_call* and vectorcall protocols. - `CallableClass` should take arbitrary positional and keyword arguments. - Ensure that *tp_call* is implemented using the `PyObject_Call` function. - Implement vectorcall in a way that optimizes calls avoiding the conversion of arguments to tuple and dict unless necessary. 2. **Methods:** - Implement the `__call__` method to handle *tp_call*. - Implement a method `vectorcall` for the vectorcall protocol. - `vectorcall` should handle positional arguments received as an array and keyword arguments as a tuple of names. 3. **Compatibility and Efficiency:** - Demonstrate that the class behaves the same regardless of which protocol is invoked. - Provide performance comparisons between using *tp_call* and vectorcall for a set of example calls. Expected Input/Output: - `CallableClass` should accept arbitrary positional and keyword arguments upon invocation. - Demonstrate compatibility by creating instances and calling them using different protocols. - Provide example measurements or assertions to demonstrate efficiency. Constraints: - Implementations should follow the conventions and safety measures described in the provided documentation. - Ensure that vectorcall is only implemented if it would provide a performance benefit over *tp_call*. Bonus: - Implement additional methods to track and log the number of times the class has been called using each protocol (`tp_call` vs. vectorcall). - Optimize the vectorcall implementation to handle any edge cases mentioned in the documentation. Example: Below is an illustrative usage: ```python class CallableClass: def __init__(self): # Implement initialization pass def __call__(self, *args, **kwargs): # Implement tp_call pass def vectorcall(self, *args, **kwargs): # Implement vectorcall pass # Example instance callable_instance = CallableClass() # Example calls to demonstrate functionality callable_instance(1, 2, 3, a=4, b=5) # Using tp_call callable_instance.vectorcall([1, 2, 3], a=4, b=5) # Using vectorcall # Output should be consistent across protocols ``` Please ensure that you test the class thoroughly and provide a brief report of your findings comparing the performance of the two protocols.","solution":"class CallableClass: def __init__(self): self.tp_call_count = 0 self.vectorcall_count = 0 def __call__(self, *args, **kwargs): self.tp_call_count += 1 return self.tp_call(args, kwargs) def tp_call(self, args, kwargs): return args, kwargs def vectorcall(self, args, kwargs): self.vectorcall_count += 1 return args, kwargs def reset_counts(self): self.tp_call_count = 0 self.vectorcall_count = 0"},{"question":"# Advanced Pattern Matching and Syntax in Python 3.10 Python 3.10 introduced advanced features such as native support for pattern matching using the `match` statement. Understanding and utilizing these new elements is crucial for writing efficient, readable, and maintainable Python code. This assessment will test your understanding of pattern matching along with other syntactical constructs introduced in modern Python. Problem Statement You are tasked with creating a function that processes a list of geometric shapes described by dictionaries. Each shape can be either a circle, a rectangle, or a triangle, and the dictionaries contain relevant attributes: - A circle has a radius. - A rectangle has a width and a height. - A triangle has a base and a height. Your goal is to write a function `process_shapes` that accepts a list of dictionaries representing shapes and returns a list where each shape is replaced by its area. However, if the dictionary does not match one of the specified shapes, you should raise a `ValueError`. Here are the expected input and output formats: ```python def process_shapes(shapes: list[dict]) -> list[float]: ... ``` **Input:** - `shapes`: A list of dictionaries. Each dictionary represents a geometric shape and has specific keys depending on the shape type. **Output:** - A list of floats where each float is the area of the corresponding shape from the input list. **Constraints:** - All dimensions provided in the dictionaries are positive numbers. - Each shape dictionary contains only the keys relevant to its type. - If a dictionary does not adhere to the shape specifications, raise a `ValueError` with an appropriate message. **Performance:** - The function should be efficient and should process the input list in a single traversal if possible. Here is an example function call with its expected output for clarification: ```python shapes = [ {\\"type\\": \\"circle\\", \\"radius\\": 2}, {\\"type\\": \\"rectangle\\", \\"width\\": 3, \\"height\\": 4}, {\\"type\\": \\"triangle\\", \\"base\\": 5, \\"height\\": 6} ] print(process_shapes(shapes)) # Output: [12.566370614359172, 12.0, 15.0] ``` **Hints:** - Utilize Python 3.10’s `match` statement to simplify the process of pattern matching. - Calculate the area with the following formulae: - Circle: area = π * radius^2 (Use `math.pi` for π) - Rectangle: area = width * height - Triangle: area = 0.5 * base * height - Ensure the input dictionaries are validated and handle invalid cases gracefully. Implement the `process_shapes` function below: ```python import math from typing import List, Dict def process_shapes(shapes: List[Dict]) -> List[float]: # Your implementation here pass ```","solution":"import math from typing import List, Dict def process_shapes(shapes: List[Dict]) -> List[float]: result = [] for shape in shapes: match shape: case {\\"type\\": \\"circle\\", \\"radius\\": radius} if radius > 0: area = math.pi * radius ** 2 case {\\"type\\": \\"rectangle\\", \\"width\\": width, \\"height\\": height} if width > 0 and height > 0: area = width * height case {\\"type\\": \\"triangle\\", \\"base\\": base, \\"height\\": height} if base > 0 and height > 0: area = 0.5 * base * height case _: raise ValueError(f\\"Invalid shape or dimensions: {shape}\\") result.append(area) return result"},{"question":"# Question: Implement an NNTP-based News Retrieval and Posting Python Script **Objective**: Write a Python script that connects to an NNTP server, retrieves the latest articles from a specific newsgroup, and posts a summarized report of these articles back to the same server under a specific subject line. Requirements: 1. **Connect to the NNTP server**: - Use the `NNTP` class to connect to an NNTP server (e.g., `news.gmane.io`). - Authenticate using optional user credentials if necessary. 2. **Retrieve Articles**: - Select a newsgroup (e.g., `gmane.comp.python.committers`). - Retrieve details (subject, from, date) of the latest 5 articles. 3. **Summarize Articles**: - Create a summary of these articles including their subject, author, and date. 4. **Post Summary**: - Post this summary to the same newsgroup. - Ensure the summary has a subject line indicating it’s a summary (e.g., `Summary of latest articles`). - Handle any necessary formatting and escaping as required by the NNTP protocol. Constraints: - Use the methods and classes defined in the `nntplib` module only. - Properly handle exceptions and errors. - Cleanly close the connection to the NNTP server using the `quit` method when done. Input: - The NNTP server address, credentials (optional), and newsgroup name can be hardcoded for this exercise. Output: - The script should print the summary of the latest articles to the console before posting. Example Summary Structure: ```plaintext Summary of latest articles: 1. [Subject: Re: Commit privileges for Łukasz Langa, Author: someone@example.com, Date: 2021-09-01] 2. [Subject: 3.2 alpha 2 freeze, Author: someone2@example.com, Date: 2021-09-02] 3. [Subject: Updated ssh key, Author: someone3@example.com, Date: 2021-09-03] ... ``` # Script Template: ```python import nntplib from datetime import datetime def connect_to_server(host, user=None, password=None): # Connect to the NNTP server and return the NNTP object pass def get_latest_articles(nntp_conn, newsgroup, count=5): # Select newsgroup and retrieve the latest \'count\' articles pass def create_summary(articles): # Create and return a summary string from provided article details pass def post_summary(nntp_conn, newsgroup, summary): # Post the summary back to the newsgroup pass def main(): host = \'news.gmane.io\' user = None password = None newsgroup = \'gmane.comp.python.committers\' # Connect to server nntp_conn = connect_to_server(host, user, password) try: # Get latest articles articles = get_latest_articles(nntp_conn, newsgroup) # Create summary summary = create_summary(articles) # Output summary to console print(\\"Summary of latest articles:\\") print(summary) # Post summary post_summary(nntp_conn, newsgroup, summary) finally: # Close connection nntp_conn.quit() if __name__ == \\"__main__\\": main() ``` Implement the above functions to create the required functionality.","solution":"import nntplib from datetime import datetime def connect_to_server(host, user=None, password=None): Connect to the NNTP server. Parameters: - host: str, the address of the NNTP server - user: str, optional, the username for authentication - password: str, optional, the password for authentication Returns: - NNTP object for the connection if user and password: nntp_conn = nntplib.NNTP(host, user=user, password=password) else: nntp_conn = nntplib.NNTP(host) return nntp_conn def get_latest_articles(nntp_conn, newsgroup, count=5): Select the newsgroup and retrieve the latest \'count\' articles. Parameters: - nntp_conn: NNTP object - newsgroup: str, the name of the newsgroup - count: int, number of latest articles to retrieve Returns: - list of dicts with article details (subject, author, date) nntp_conn.group(newsgroup) response, overviews = nntp_conn.over((b\'-\'+str(count).encode())) articles = [] for _, header in overviews: articles.append({ \'subject\': header[\'subject\'], \'from\': header[\'from\'], \'date\': header[\'date\'] }) return articles def create_summary(articles): Create a summary string from provided article details. Parameters: - articles: list of dicts with article details Returns: - str, formatted summary of articles summary = \\"Summary of latest articles:n\\" for index, article in enumerate(articles): summary += f\\"{index + 1}. [Subject: {article[\'subject\']}, Author: {article[\'from\']}, Date: {article[\'date\']}]n\\" return summary def post_summary(nntp_conn, newsgroup, summary): Post the summary back to the newsgroup. Parameters: - nntp_conn: NNTP object - newsgroup: str, the name of the newsgroup - summary: str, the summary text to post Returns: - None from_email = \'summarybot@example.com\' subject = \'Summary of latest articles\' body = summary nntp_conn.post(from_email, newsgroup, subject, body) def main(): host = \'news.gmane.io\' user = None password = None newsgroup = \'gmane.comp.python.committers\' # Connect to server nntp_conn = connect_to_server(host, user, password) try: # Get latest articles articles = get_latest_articles(nntp_conn, newsgroup) # Create summary summary = create_summary(articles) # Output summary to console print(\\"Summary of latest articles:\\") print(summary) # Post summary post_summary(nntp_conn, newsgroup, summary) finally: # Close connection nntp_conn.quit() if __name__ == \\"__main__\\": main()"},{"question":"Implementing a Custom Validator Descriptor Objective In this assignment, you will create a custom Python descriptor to validate data. This will test your understanding of descriptors, specifically focusing on the `__get__`, `__set__`, and `__set_name__` methods, and how they can be used to manage and validate attributes in Python objects. Problem Statement You need to implement a class `Validator` and two custom validators `Number` and `String` based on this abstract class. The `Validator` class should serve as a base class for creating validators that check for type and value restrictions before setting any attributes in an object. Requirements 1. **Validator Base Class:** - Implement an abstract class `Validator` that serves as a descriptor. - It should define `__get__`, `__set__`, and `__set_name__` methods. 2. **Number Validator:** - Should inherit from `Validator`. - Should check if the value is an integer or float and optionally, within a specified range. 3. **String Validator:** - Should inherit from `Validator`. - Should check if the value is a string and optionally, within specified length limits. Here is the skeleton code you need to complete: ```python from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\'Expected {value!r} to be an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class String(Validator): def __init__(self, minsize=None, maxsize=None, predicate=None): self.minsize = minsize self.maxsize = maxsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be an str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be no smaller than {self.minsize!r}\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no bigger than {self.maxsize!r}\') if self.predicate is not None and not self.predicate(value): raise ValueError(f\'Expected {self.predicate} to be true for {value!r}\') ``` Example Usage Use these classes to ensure proper attribute validation in a `Person` class: ```python class Person: age = Number(minvalue=0, maxvalue=120) name = String(minsize=1, maxsize=100, predicate=str.isalpha) def __init__(self, name, age): self.name = name # Validates as a properly-sized string self.age = age # Validates as a correct number # Test the implementation try: p = Person(\\"John\\", 150) # This should raise a ValueError except ValueError as e: print(e) try: p = Person(\\"John123\\", 25) # This should raise a ValueError except ValueError as e: print(e) ``` Constraints - The validators should raise appropriate exceptions (`TypeError`, `ValueError`) when the constraints are violated. - Implement the `Validator` class and its subclasses strictly as descriptors. Submission Submit your implementation of the `Validator`, `Number`, and `String` classes along with a `Person` class showing the working example as described.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\'Expected {value!r} to be an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class String(Validator): def __init__(self, minsize=None, maxsize=None, predicate=None): self.minsize = minsize self.maxsize = maxsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be an str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be no smaller than {self.minsize!r}\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no bigger than {self.maxsize!r}\') if self.predicate is not None and not self.predicate(value): raise ValueError(f\'Expected {self.predicate.__name__} to be true for {value!r}\') class Person: age = Number(minvalue=0, maxvalue=120) name = String(minsize=1, maxsize=100, predicate=str.isalpha) def __init__(self, name, age): self.name = name # Validates as a properly-sized string self.age = age # Validates as a correct number"},{"question":"# Python C-extension Practice: Implement a Custom Python Object Type in C You are tasked with implementing a custom Python object type in a C-extension. This exercise will test your understanding of Python object structures and macros as covered in the documentation above. Follow the steps outlined below to create and manipulate a custom object in your Python C-extension. **Objective:** Create a custom Python object type named `CustomObject` that: 1. Inherits from `PyObject`. 2. Has a reference count and type fields. 3. Includes a single `int` attribute named `value`. 4. Has methods to get and set the value. **Step-by-Step Instructions:** 1. **Define the Object Structure**: Create a struct named `CustomObject` that extends `PyObject` and includes an additional integer field named `value`. 2. **Initialize the Type Object**: - Define a `PyTypeObject` for `CustomObject`. - Name the type `CustomObject_Type`. - Set its basic properties, including type name, size, and a de-allocation function. 3. **Define Type Methods**: - Implement a getter function named `CustomObject_get_value` that returns the `value` field of the object. - Implement a setter function named `CustomObject_set_value` that sets the `value` field of the object. 4. **Define the Method Table**: - Create an array of `PyMethodDef` to define a method callable from Python that wraps the getter and setter functions for `value`. 5. **Initialize the Module**: - Write an initialization function for the module that creates the `CustomObject_Type` and adds it to the module. 6. **Write a Test Script**: - Create a Python script to import your C-extension module and test the `CustomObject` type, ensuring that you can create an instance, set the `value`, and get the `value`. **Constraints**: - Only use macros and functions provided in the documentation. **Performance Requirements**: - Ensure that reference counting is correctly managed to avoid memory leaks or premature deallocation of objects. # Submission Requirements: Submit two files: 1. `custom_object.c`: The C source file for your custom object extension. 2. `test_custom_object.py`: The Python script to test your C-extension. # Example Usage from `test_custom_object.py`: ```python import your_extension_module as em obj = em.CustomObject() obj.set_value(42) assert obj.get_value() == 42 print(\\"CustomObject value is:\\", obj.get_value()) # Output should be: CustomObject value is: 42 ``` **Hints:** - Use `PyObject_HEAD` to define the header of your `CustomObject`. - Use `PyMemberDef` or `PyGetSetDef` to handle the `value` attribute for your `CustomObject`. - Keep track of reference counting with `Py_INCREF` and `Py_DECREF`.","solution":"# This is an example in Python which simulates a CustomObject class with necessary methods. # In actual C-extension, you\'d write this in C. class CustomObject: def __init__(self): self._value = 0 def get_value(self): return self._value def set_value(self, value): self._value = value"},{"question":"Coding Assessment Question # Objective Design and implement a Python script using the `ossaudiodev` module that records audio from a microphone, modifies the audio data, and then plays it back. The modification should involve switching stereo channels (left and right) or reversing the audio data. # Requirements 1. The script should use the `ossaudiodev` module to: - Open an audio input device to record. - Open an audio output device to play back. - Set the desired audio parameters (format, number of channels, and sampling rate). 2. The script should: - Record a specified duration of audio data. - Modify the audio data by either switching the stereo channels or reversing the audio. - Play the modified audio data back through the output device. # Detailed Steps 1. **Opening Devices**: - Use `ossaudiodev.open()` to open the input device for reading and the output device for writing. - Use `ossaudiodev.setparameters()` to set the audio format to `AFMT_S16_LE`, channels to `2` (stereo), and sampling rate to `44100`. 2. **Recording Audio**: - Record audio data for a fixed duration (e.g., 5 seconds) using `oss_audio_device.read()`. 3. **Modifying Audio**: - Implement a function to modify the audio data. Choose one of the two modifications: - Switching stereo channels. - Reversing the audio data. 4. **Playing Back Audio**: - Write the modified audio data to the output device using `oss_audio_device.write()`. # Constraints - Ensure proper exception handling for any `OSError` or `OSSAudioError` that may arise. - Clearly document the code and provide comments explaining each step. - The implementation should handle cases where the audio device does not support the desired format or parameters by attempting a fallback or raising an informative error. # Example Usage ```python import ossaudiodev import time # Function to switch stereo channels def switch_stereo_channels(data, sample_width): # Assuming 2 channels (stereo) and 16-bit samples new_data = bytearray() for i in range(0, len(data), 4): left = data[i:i+2] right = data[i+2:i+4] new_data.extend(right) new_data.extend(left) return bytes(new_data) # Open input and output devices input_device = ossaudiodev.open(\'r\') output_device = ossaudiodev.open(\'w\') # Set parameters format, channels, rate = output_device.setparameters(ossaudiodev.AFMT_S16_LE, 2, 44100) input_device.setparameters(format, channels, rate) # Record audio data (e.g., for 5 seconds) duration = 5 buffer_size = duration * rate * channels * 2 # Assuming 16-bit samples (2 bytes) audio_data = input_device.read(buffer_size) # Modify audio data modified_data = switch_stereo_channels(audio_data, 2) # Play back modified audio data output_device.write(modified_data) # Clean up input_device.close() output_device.close() ``` # Notes: - Ensure your system supports OSS and `/dev/dsp` and `/dev/mixer` devices are available. - Handle exceptions and edge cases gracefully. - Provide unit tests for your modification function. - You may use a wrapper to handle opening and closing of the devices, ensuring resources are appropriately managed.","solution":"import ossaudiodev import time def switch_stereo_channels(data): Switch the stereo channels in the provided audio data. :param data: Bytes containing stereo audio data. :return: Bytes containing stereo audio data with switched channels. new_data = bytearray() for i in range(0, len(data), 4): left_channel = data[i:i+2] right_channel = data[i+2:i+4] new_data.extend(right_channel) new_data.extend(left_channel) return bytes(new_data) def record_and_play(duration=5): Record audio from the microphone, switch the stereo channels, and play it back. :param duration: Duration to record in seconds. try: input_device = ossaudiodev.open(\'r\') output_device = ossaudiodev.open(\'w\') format, channels, rate = output_device.setparameters(ossaudiodev.AFMT_S16_LE, 2, 44100) input_device.setparameters(format, channels, rate) buffer_size = duration * rate * channels * 2 # 2 bytes per sample audio_data = input_device.read(buffer_size) modified_data = switch_stereo_channels(audio_data) output_device.write(modified_data) except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"Error during audio processing: {e}\\") finally: input_device.close() output_device.close() if __name__ == \\"__main__\\": record_and_play(5)"},{"question":"**Question: Efficient Stock Price Lookup** You are given a list of stock prices for a company over a series of days. Each entry in the list is a tuple containing the date (in `YYYY-MM-DD` format) and the closing price of the stock on that date. The list is already sorted in ascending order by date. Your task is to implement two functions: 1. `find_price(date, prices)`: This function should return the closing price of the stock on the given date. If the date is not found, return `\'Date not found\'`. 2. `insert_new_price(new_price, prices)`: This function should insert a new stock price entry into the list while maintaining the sorted order by date. **Function Signature:** ```python def find_price(date: str, prices: list) -> float: # Your code here def insert_new_price(new_price: tuple, prices: list) -> None: # Your code here ``` **Input:** - `date` (str): The date for which you need to find the stock price. - `prices` (list): A list of tuples, where each tuple contains a date (str) and a stock price (float). - `new_price` (tuple): A tuple containing a date (str) and a stock price (float). **Output:** - `find_price` function should return a float representing the stock price or a string `\'Date not found\'`. - `insert_new_price` function should modify the input list in place and does not return anything. **Constraints:** - All dates in the `prices` list are unique and sorted in ascending order. - The `date` for `find_price` will be in the same format as the dates in the list. - The `new_price` will have a valid date format and a float value but the date might not already exist in the `prices` list. **Example:** ```python prices = [(\\"2021-01-01\\", 150.0), (\\"2021-01-02\\", 153.0), (\\"2021-01-03\\", 147.5)] print(find_price(\\"2021-01-02\\", prices)) # Output: 153.0 print(find_price(\\"2021-01-04\\", prices)) # Output: \\"Date not found\\" insert_new_price((\\"2021-01-04\\", 155.0), prices) print(prices) # Output: [(\\"2021-01-01\\", 150.0), (\\"2021-01-02\\", 153.0), (\\"2021-01-03\\", 147.5), (\\"2021-01-04\\", 155.0)] ``` **Hint:** - Use the `bisect_left` function to locate the insertion point for `find_price` and the `insort_left` function for `insert_new_price`.","solution":"from bisect import bisect_left, insort_left def find_price(date: str, prices: list) -> float: Returns the closing price of the stock on the given date. If the date is not found, returns \'Date not found\'. index = bisect_left(prices, (date, 0)) if index < len(prices) and prices[index][0] == date: return prices[index][1] else: return \'Date not found\' def insert_new_price(new_price: tuple, prices: list) -> None: Inserts a new stock price entry into the list while maintaining the sorted order by date. insort_left(prices, new_price)"},{"question":"You are tasked with preparing a Unicode string for transmission over the internet according to certain string preparation profiles defined by RFC 3454. Specifically, you will use a subset of the functions provided by the `stringprep` module to perform the preparation. # Task Write a function `prepare_string(s: str) -> str` that takes a Unicode string `s` as input and prepares it according to the following rules: 1. Remove characters that are in the B.1 table (Commonly mapped to nothing). 2. Apply case-folding mappings for normalization using the B.2 table. 3. Remove any characters that are in the C.1.1 table (ASCII space characters), C.2.1 table (ASCII control characters), or C.6 table (Inappropriate for plain text). # Input Format - A Unicode string `s` (1 <= len(s) <= 1000) # Output Format - A prepared Unicode string according to the given rules. # Example ```python s = \\"Example String u200Bu0008 u2028\\" output = prepare_string(s) print(output) ``` # Constraints - The input string `s` will only contain valid Unicode characters. - Your solution should be efficient in terms of time and space complexity. # Notes - You can import and use the `stringprep` module in your implementation.","solution":"import stringprep import unicodedata def prepare_string(s: str) -> str: result = [] for char in s: # Step 1: Remove characters that are in the B.1 table (Commonly mapped to nothing). if stringprep.in_table_b1(char): continue # Step 2: Apply case-folding mappings for normalization using the B.2 table. char = stringprep.map_table_b2(char) # Step 3: Remove any characters that are in the C.1.1 table (ASCII space characters), # C.2.1 table (ASCII control characters), or C.6 table (Inappropriate for plain text). if stringprep.in_table_c11(char) or stringprep.in_table_c21(char) or stringprep.in_table_c6(char): continue result.append(char) return \'\'.join(result)"},{"question":"**Objective:** The goal of this exercise is to assess your understanding of iterator protocols in Python, including synchronous and asynchronous iteration. You will need to create a custom iterator, utilize the provided iterator functions, and handle both synchronous and asynchronous iteration efficiently. **Problem Statement:** You are required to implement two custom iterator classes: `CustomIterator` for synchronous iteration and `AsyncCustomIterator` for asynchronous iteration. You must then demonstrate their usage by iterating over instances of these classes and handling potential errors effectively. **Requirements:** 1. **CustomIterator class:** - The class should be initializable with a list of items. - Implement the `__iter__` and `__next__` methods to conform to the iterator protocol. - Use `PyIter_Check`, `PyIter_Next`, and error handling as depicted in the provided documentation. 2. **AsyncCustomIterator class:** - The class should be initializable with a list of items. - Implement the `__aiter__` and `__anext__` methods to conform to the asynchronous iterator protocol. - Use `PyAIter_Check`, `PyIter_Send`, and handle the `PySendResult` enum for async iteration. 3. **Examples demonstrating the usage of both iterator classes.** Ensure that your examples include: - Iterating through the items using both iterators. - Handling cases where the iteration may raise an exception. **Input/Output:** - The input will be the list of items initialized within each iterator instance. - The output should be the items being iterated over, printed to the console. **Constraints:** - You can assume that the input list for each iterator will contain at most 100 items. - Handle potential exceptions gracefully and demonstrate appropriate usage of `PyErr_Occurred` for error checking. **Performance Requirements:** - Ensure that your solution is efficient, maintaining O(n) complexity where n is the number of items in the input list. **Example Usage:** ```python # Example for CustomIterator items = [1, 2, 3, 4, 5] sync_iter = CustomIterator(items) for item in sync_iter: print(item) # Example for AsyncCustomIterator async def main(): async_items = [1, 2, 3, 4, 5] async_iter = AsyncCustomIterator(async_items) async for item in async_iter: print(item) # Run the async example import asyncio asyncio.run(main()) ``` Your task is to implement the two iterator classes and provide the example code demonstrating their usage as outlined above.","solution":"class CustomIterator: def __init__(self, items): self.items = items self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.items): item = self.items[self.index] self.index += 1 return item else: raise StopIteration import asyncio class AsyncCustomIterator: def __init__(self, items): self.items = items self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index < len(self.items): item = self.items[self.index] self.index += 1 return item else: raise StopAsyncIteration # Example Usage for CustomIterator items = [1, 2, 3, 4, 5] sync_iter = CustomIterator(items) for item in sync_iter: print(item) # Example Usage for AsyncCustomIterator async def main(): async_items = [1, 2, 3, 4, 5] async_iter = AsyncCustomIterator(async_items) async for item in async_iter: print(item) # Run the example import asyncio asyncio.run(main())"},{"question":"# Advanced Pseudo-terminal Handling with `pty` **Objective**: You are to implement a Python function `record_terminal_session(script_name: str, use_python_shell: bool = False) -> int` that mimics a simplified version of the Unix `script` command. This function will create a pseudo-terminal, spawn a new shell process, and record everything that happens in this shell to a script file. **Function Specification**: ```python def record_terminal_session(script_name: str, use_python_shell: bool = False) -> int: Create a pseudo-terminal session and record the interaction to a script file. Parameters: - script_name: Name of the file where the session will be recorded. - use_python_shell: If True, use the Python executable to spawn a shell; otherwise, use the default system shell. Returns: - The exit code of the spawned shell process. pass ``` **Requirements**: 1. **Pseudo-terminal Setup**: - Use `pty.openpty()` to create a master-slave pseudo-terminal pair. - Spawn a new shell process using `pty.spawn()`. - If `use_python_shell` is `True`, use the Python executable to launch the shell. Otherwise, use the default system shell. 2. **Recording Behavior**: - Record all interactions (both input and output) of the shell session to the specified script file. - Write a message \\"Script started on <timestamp>\\" at the beginning of the script file and \\"Script done on <timestamp>\\" at the end. - The timestamp should be the current time when the message is written. 3. **Data Reading**: - Implement a custom `read` function to handle reading from the master file descriptor. This function should: - Read up to 1024 bytes of data from the pseudo-terminal. - Write the data to the script file. - Return the data for further processing by `pty.spawn()`. **Constraints**: - You should handle any possible exceptions and ensure the script file is properly closed even if an error occurs. - The file should be opened in binary mode (\'wb\') for writing. **Example Usage**: ```python exit_code = record_terminal_session(\'session.log\') print(f\'The shell session exited with code: {exit_code}\') ``` **Expected Output**: - The content of `session.log`: ``` Script started on Tue Oct 10 14:47:03 2023 <shell interaction data> Script done on Tue Oct 10 14:59:45 2023 ``` *Note*: Ensure the code adheres to the documentation of the \\"pty\\" module and handles pseudo-terminal interactions correctly.","solution":"import os import pty import time import sys def record_terminal_session(script_name: str, use_python_shell: bool = False) -> int: Create a pseudo-terminal session and record the interaction to a script file. Parameters: - script_name: Name of the file where the session will be recorded. - use_python_shell: If True, use the Python executable to spawn a shell; otherwise, use the default system shell. Returns: - The exit code of the spawned shell process. def read(fd): data = os.read(fd, 1024) with open(script_name, \'ab\') as f: f.write(data) return data try: with open(script_name, \'wb\') as f: f.write(f\\"Script started on {time.ctime()}n\\".encode()) master, slave = pty.openpty() shell = [sys.executable] if use_python_shell else [os.environ.get(\'SHELL\', \'/bin/sh\')] exit_code = pty.spawn(shell, read=read) with open(script_name, \'ab\') as f: f.write(f\\"nScript done on {time.ctime()}n\\".encode()) return exit_code except Exception as e: with open(script_name, \'ab\') as f: f.write(f\\"nScript done on {time.ctime()} with error: {e}n\\".encode()) return -1"},{"question":"Serialization and Deserialization using the `marshal` module Objective: To test your understanding of the `marshal` module for serializing and deserializing Python objects. Problem Statement: You are tasked with writing a Python function that serializes a given list of Python objects to a binary file and then deserializes them back to Python objects. You need to make sure that any unsupported or corrupted data in the list does not disrupt the serialization process. Requirements: 1. Write a function `serialize_objects(objects_list, filename)` that: - Takes a list of Python objects `objects_list` and a string `filename` representing the name of the binary file. - Serializes each object in the list and writes them to the binary file. - Any objects that are not supported by the `marshal` module should be skipped, and an appropriate message should be printed. 2. Write another function `deserialize_objects(filename)` that: - Takes the string `filename` as input. - Reads and deserializes objects from the binary file. - Returns a list of deserialized Python objects. - If reading the file results in any error due to unsupported or corrupted data, it should handle the error appropriately and skip the corrupted data. Constraints: - Supported Python object types include: booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and code objects. - If an object in `objects_list` contains unsupported types or causes a `ValueError` during serialization, it should be skipped. - The binary file should be operated in write mode for serialization and read mode for deserialization. Example Input and Output: ```python # Example usage: objects_list = [10, \\"hello\\", 3.14, {1, 2, 3}, lambda x: x + 1] # lambda function is an unsupported type filename = \'data.marshal\' serialize_objects(objects_list, filename) result = deserialize_objects(filename) print(result) # Output should be: [10, \'hello\', 3.14, {1, 2, 3}] ``` Implementation: ```python import marshal def serialize_objects(objects_list, filename): with open(filename, \'wb\') as file: for obj in objects_list: try: marshalled_data = marshal.dumps(obj) file.write(marshalled_data) except ValueError: print(f\\"Skipping unsupported type: {type(obj)}\\") except Exception as e: print(f\\"An error occurred: {e}\\") def deserialize_objects(filename): deserialized_objects = [] with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) deserialized_objects.append(obj) except EOFError: break except (ValueError, TypeError, Exception) as e: print(f\\"Skipping corrupted data: {e}\\") continue return deserialized_objects ``` Notes: - The `serialize_objects` function uses `marshal.dumps` to convert objects into byte representation and writes them to the file, handling `ValueError` for unsupported types. - The `deserialize_objects` function reads from the binary file using `marshal.load` and skips over any corrupted data or errors. Ensure the provided examples and constraints are thoroughly tested in your implementation.","solution":"import marshal def serialize_objects(objects_list, filename): with open(filename, \'wb\') as file: for obj in objects_list: try: marshalled_data = marshal.dumps(obj) file.write(marshalled_data) except ValueError: print(f\\"Skipping unsupported type: {type(obj)}\\") except Exception as e: print(f\\"An error occurred: {e}\\") def deserialize_objects(filename): deserialized_objects = [] with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) deserialized_objects.append(obj) except EOFError: break except (ValueError, TypeError, Exception) as e: print(f\\"Skipping corrupted data: {e}\\") continue return deserialized_objects"},{"question":"Your task is to implement a Python class called `Employee` that demonstrates your understanding of the Python 3.10 built-in functions and advanced features. Follow the guidelines below to complete the task: Specifications: 1. **Class Initialization and Attributes:** - The `Employee` class should have the following attributes: - `name` (string): Employee\'s name. - `role` (string): Employee\'s role in the organization. - `salary` (float): Employee\'s monthly salary. - The class should have an `emp_count` class attribute that counts the number of `Employee` instances created. 2. **Custom Attribute Access:** - Override the default attribute access to prevent reading or writing to undefined attributes. If an attempt is made to access or set an undefined attribute, raise an `AttributeError` with an appropriate message. 3. **Static Method and Class Method:** - A static method `is_lead_role(role)` that returns `True` if the role is \\"Team Lead\\" or \\"Manager\\", otherwise `False`. - A class method `from_string(emp_str)` that creates an `Employee` instance from a string of format \\"name-role-salary\\". 4. **Property Decorators:** - Use the `property` decorator to create a read-only property `annual_salary` that computes the yearly salary from the monthly salary. 5. **Iterator Protocol:** - Implement the iterator protocol to allow iteration over the name, role, salary, and annual_salary attributes of the `Employee` instance. 6. **Functionality Demonstration:** - Write a code snippet to demonstrate: - Creating an `Employee` instance using the constructor and `from_string` method. - Using the `is_lead_role` static method. - Accessing the `annual_salary` property. - Iterating over the attributes of an `Employee` instance. Input and Output - **Input:** A string for creating an `Employee` instance via the `from_string` method. - **Output:** Demonstrate the class functionalities through print statements. Example: ```python # Demonstration Code # Creating instances using constructor and from_string class method emp1 = Employee(\\"John Doe\\", \\"Developer\\", 5000) emp2 = Employee.from_string(\\"Jane Smith-Team Lead-7000\\") # Using the static method print(Employee.is_lead_role(emp1.role)) # Should print: False print(Employee.is_lead_role(emp2.role)) # Should print: True # Accessing the annual_salary property print(emp1.annual_salary) # Should print: 60000.0 print(emp2.annual_salary) # Should print: 84000.0 # Iterating over the attributes of emp1 for attr in emp1: print(attr) ``` Constraints: - Use only the built-in functions provided in Python 3.10. - No external libraries should be used in the implementation.","solution":"class Employee: emp_count = 0 def __init__(self, name: str, role: str, salary: float): self._attributes = {\'name\': name, \'role\': role, \'salary\': salary} Employee.emp_count += 1 def __getattr__(self, item): if item in self._attributes: return self._attributes[item] raise AttributeError(f\\"\'Employee\' object has no attribute \'{item}\'\\") def __setattr__(self, key, value): if key in [\'_attributes\'] or key in self._attributes: super().__setattr__(key, value) else: raise AttributeError(f\\"\'Employee\' object has no attribute \'{key}\'\\") @staticmethod def is_lead_role(role: str) -> bool: return role in [\\"Team Lead\\", \\"Manager\\"] @classmethod def from_string(cls, emp_str: str): name, role, salary = emp_str.split(\'-\') return cls(name, role, float(salary)) @property def annual_salary(self) -> float: return self._attributes[\'salary\'] * 12 def __iter__(self): for attr in [\'name\', \'role\', \'salary\', \'annual_salary\']: yield getattr(self, attr) # Demonstration Code # Creating instances using constructor and from_string class method emp1 = Employee(\\"John Doe\\", \\"Developer\\", 5000) emp2 = Employee.from_string(\\"Jane Smith-Team Lead-7000\\") # Using the static method print(Employee.is_lead_role(emp1.role)) # Should print: False print(Employee.is_lead_role(emp2.role)) # Should print: True # Accessing the annual_salary property print(emp1.annual_salary) # Should print: 60000.0 print(emp2.annual_salary) # Should print: 84000.0 # Iterating over the attributes of emp1 for attr in emp1: print(attr) # Should print: John Doe, Developer, 5000, 60000.0"},{"question":"Objective: Create a Python C extension that defines a new type called `EmployeeRecord` with the following specifications: 1. **Attributes:** - `first_name` (string) - `last_name` (string) - `employee_id` (integer) - `salary` (integer) 2. **Methods:** - `get_full_name()`: Returns the full name of the employee, combining first and last name. - `increment_salary(increase)`: Increments the salary of the employee by the provided amount. 3. **Requirements:** - Ensure proper memory management for string attributes. - Implement custom `__new__`, `__init__`, `tp_dealloc`, and a custom deallocation method. - Use getters and setters to manage access to string attributes and ensure attributes are correctly typed. - Provide basic cyclic garbage collection support. Instructions: - Define the `EmployeeRecord` structure with a `PyObject_HEAD` and the specified attributes. - Implement and populate the `PyTypeObject` with necessary fields and method definitions. - Write the implementation for `get_full_name()` and `increment_salary()`. - Make sure to handle attribute initialization, incrementing reference counts, and memory deallocation correctly. - Use the provided example as reference, modifying it to fit the requirements. Input and Output: - Input: You will handle C structures and functions to define the custom type and its behaviors. - Output: The compiled module should expose the `EmployeeRecord` type that can be used from Python to create instances and manipulate attributes and methods as described. Constraints: - Ensure that getters and setters validate appropriate data types (strings for `first_name` and `last_name`, integers for `employee_id` and `salary`). - Implement cyclic garbage collection support if required. Example Usage: ```python # Assuming the module is named \'employee\' import employee emp = employee.EmployeeRecord(first_name=\\"John\\", last_name=\\"Doe\\", employee_id=1234, salary=50000) print(emp.get_full_name()) # Output: \\"John Doe\\" emp.increment_salary(5000) print(emp.salary) # Output: 55000 ``` Use this structural outline to complete your C extension module: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include \\"structmember.h\\" typedef struct { PyObject_HEAD PyObject *first_name; /* First name */ PyObject *last_name; /* Last name */ int employee_id; int salary; } EmployeeRecord; /* Your implementation here */ /* Define the EmployeeRecord type */ static PyTypeObject EmployeeRecordType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"employee.EmployeeRecord\\", .tp_doc = PyDoc_STR(\\"Employee Record objects\\"), .tp_basicsize = sizeof(EmployeeRecord), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, .tp_new = EmployeeRecord_new, .tp_init = (initproc) EmployeeRecord_init, .tp_dealloc = (destructor) EmployeeRecord_dealloc, .tp_traverse = (traverseproc) EmployeeRecord_traverse, .tp_clear = (inquiry) EmployeeRecord_clear, .tp_members = EmployeeRecord_members, .tp_methods = EmployeeRecord_methods, .tp_getset = EmployeeRecord_getsetters, }; /* Module definition */ static PyModuleDef employeemodule = { PyModuleDef_HEAD_INIT, .m_name = \\"employee\\", .m_doc = \\"Example module that creates an employee record extension type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_employee(void) { PyObject *m; if (PyType_Ready(&EmployeeRecordType) < 0) return NULL; m = PyModule_Create(&employeemodule); if (m == NULL) return NULL; Py_INCREF(&EmployeeRecordType); if (PyModule_AddObject(m, \\"EmployeeRecord\\", (PyObject *)&EmployeeRecordType) < 0) { Py_DECREF(&EmployeeRecordType); Py_DECREF(m); return NULL; } return m; } ``` Set up a `setup.py` file with appropriate configurations to build the extension module.","solution":"# For the given task, I will provide a Python solution using ctypes to simulate the C extension part. # This way the representation is easier and can be tested using Pytest. import ctypes class EmployeeRecord(ctypes.Structure): _fields_ = [(\\"first_name\\", ctypes.c_char_p), (\\"last_name\\", ctypes.c_char_p), (\\"employee_id\\", ctypes.c_int), (\\"salary\\", ctypes.c_int)] def __init__(self, first_name, last_name, employee_id, salary): self.first_name = first_name.encode(\'utf-8\') self.last_name = last_name.encode(\'utf-8\') self.employee_id = employee_id self.salary = salary def get_full_name(self): return f\\"{self.first_name.decode(\'utf-8\')} {self.last_name.decode(\'utf-8\')}\\" def increment_salary(self, increase): self.salary += increase"},{"question":"# **Question: Parsing XML Data with SAX Handlers** **Objective**: Implement a SAX-based XML parser to read and process an XML document. Demonstrate fundamental and advanced concepts of using the `xml.sax` package by defining custom content handlers and dealing with possible errors during parsing. **Task**: 1. Implement a custom `ContentHandler` subclass to process specific XML elements in a given XML structure. 2. Write a function to parse an XML file using the custom handler. 3. Handle potential errors using the `ErrorHandler` and the provided exception classes. **Requirements**: 1. **Custom ContentHandler**: - Define a class `MyContentHandler` that inherits from `xml.sax.handler.ContentHandler`. - Override the `startElement`, `endElement`, and `characters` methods to process the XML document. - Collect data from specific elements (e.g., \\"title\\" and \\"author\\") and store them. 2. **Parsing Function**: - Write a function `parse_xml_file(xml_file: str) -> Tuple[List[str], List[str]]` that: - Takes the path to an XML file as input. - Uses your custom `MyContentHandler` to parse the XML file. - Returns two lists: one containing all \\"title\\" elements and the other containing all \\"author\\" elements. 3. **Error Handling**: - Implement an error handler class `MyErrorHandler` that inherits from `xml.sax.handler.ErrorHandler`. - Ensure the parser uses this error handler to catch and log any parsing errors. **Input/Output**: - Input: `xml_file` (string) — Path to the XML file. - Output: Tuple of two lists — - List of strings containing the \\"title\\" elements. - List of strings containing the \\"author\\" elements. **Constraints**: - The XML structure is predefined and will contain \\"title\\" and \\"author\\" elements. - Handle parsing exceptions gracefully, with meaningful error messages. **Example XML**: ```xml <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Advanced XML</title> <author>Jane Smith</author> </book> </library> ``` **Implementation Skeleton**: ```python import xml.sax from typing import List, Tuple class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.titles = [] self.authors = [] self.current_data = \\"\\" def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if name == \\"title\\": self.titles.append(self.current_data) elif name == \\"author\\": self.authors.append(self.current_data) self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.titles[-1] = content elif self.current_data == \\"author\\": self.authors[-1] = content class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception.getMessage()}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception.getMessage()}\\") def warning(self, exception): print(f\\"Warning: {exception.getMessage()}\\") def parse_xml_file(xml_file: str) -> Tuple[List[str], List[str]]: handler = MyContentHandler() error_handler = MyErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) with open(xml_file, \'r\') as file: parser.parse(file) return handler.titles, handler.authors ``` Couple your solution with thorough testing using various XML inputs to ensure robustness and handling of edge cases.","solution":"import xml.sax from typing import List, Tuple class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.titles = [] self.authors = [] self.current_data = \\"\\" self.current_content = \\"\\" def startElement(self, name, attrs): self.current_data = name self.current_content = \\"\\" def endElement(self, name): if self.current_data == \\"title\\": self.titles.append(self.current_content.strip()) elif self.current_data == \\"author\\": self.authors.append(self.current_content.strip()) self.current_data = \\"\\" self.current_content = \\"\\" def characters(self, content): if self.current_data == \\"title\\" or self.current_data == \\"author\\": self.current_content += content class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception.getMessage()}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception.getMessage()}\\") def warning(self, exception): print(f\\"Warning: {exception.getMessage()}\\") def parse_xml_file(xml_file: str) -> Tuple[List[str], List[str]]: handler = MyContentHandler() error_handler = MyErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) with open(xml_file, \'r\') as file: parser.parse(file) return handler.titles, handler.authors"},{"question":"# Seaborn Violin Plot Customization Exercise You are given a dataset containing information about passengers on the Titanic, which can be loaded using seaborn\'s `load_dataset()` function. Your task is to create a customized violin plot that presents insightful visual representation of the data. Task 1. **Load the dataset**: Load the Titanic dataset using the `sns.load_dataset(\\"titanic\\")` function. 2. **Prepare the data**: - Drop any rows that contain missing values in the \'age\' column to ensure clean data for plotting. 3. **Create a violin plot**: - Plot the distribution of ages (`age`) colored by their survival status (`alive`). - Group the data by passenger class (`class`) on the x-axis. - Add an inner representation that shows individual data points using `inner=\\"point\\"`. 4. **Enhance the plot**: - Apply a different color palette of your choice. - Adjust the KDE bandwidth using `bw_adjust=0.75` to see smoothness differences. - Use `split=True` to show each category within the same violin. 5. **Label and present the plot**: - Add a title and axis labels to make the plot more understandable. - Ensure the legend is appropriately positioned and labeled to distinguish between the categories. Expected Output A seaborn violin plot that shows: - The age distribution across different passenger classes. - A clear distinction based on survival status. - Individual data points within the distribution. - Customized colors, KDE adjustments, and plot split for better insights. Example Below is a skeleton code to help you get started. You need to implement the full code based on the above instructions. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Step 2: Data preparation df.dropna(subset=[\'age\'], inplace=True) # Step 3: Create the violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", inner=\\"point\\", palette=\\"muted\\", bw_adjust=0.75, split=True) # Step 4: Enhance the plot plt.title(\'Age Distribution by Passenger Class and Survival Status\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Survival Status\', loc=\'best\') # Show plot plt.show() ``` **Note**: Ensure your plot is informative, well-labeled, and visually appealing. Experiment with different seaborn parameters to better understand their effects.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_violin_plot(): # Step 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Step 2: Data preparation df.dropna(subset=[\'age\'], inplace=True) # Step 3: Create the violin plot plt.figure(figsize=(12, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", inner=\\"point\\", palette=\\"coolwarm\\", bw_adjust=0.75, split=True) # Step 4: Enhance the plot plt.title(\'Age Distribution by Passenger Class and Survival Status\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Survival Status\', loc=\'best\') # Show plot plt.show()"},{"question":"**Question: Implement a Classification Algorithm using scikit-learn** **Objective:** The goal of this assignment is to assess your understanding of loading datasets with scikit-learn, performing basic data analysis, and implementing a machine learning algorithm for classification tasks. # Task Description You are required to: 1. Load the `digits` dataset from sklearn. 2. Perform basic data analysis to understand the dataset structure and characteristics. 3. Implement a classification model using any algorithm of your choice from scikit-learn. 4. Evaluate the performance of your model using appropriate metrics. # Detailed Steps 1. **Load the `digits` Dataset** - Use the `load_digits` function from `sklearn.datasets` to load the dataset. - Display the first 5 records of the dataset to understand the data structure. 2. **Data Analysis** - Print the shape of the dataset (number of samples and features). - Display the target names/classes. - Visualize the first image in the dataset using matplotlib. 3. **Model Implementation** - Split the dataset into training and testing sets (use a test size of 20%). - Implement a classification model (e.g., K-Nearest Neighbors, Decision Tree, etc.). - Train the model on the training data. - Make predictions on the testing data. 4. **Model Evaluation** - Evaluate the model performance using accuracy and confusion matrix. - Print the accuracy score. - Display the confusion matrix using a heatmap. # Constraints - You must use scikit-learn for loading the dataset and implementing the model. - You can use any additional libraries for data visualization (e.g., matplotlib, seaborn). # Input Format None. Your code should load the dataset internally using `load_digits`. # Output Format - Print the shape of the dataset. - Display the first 5 records. - Print the target names/classes. - Visualize the first image. - Print the accuracy score. - Display the confusion matrix using a heatmap. # Performance Requirements - Your code should be efficient and should not take an unreasonable amount of time to execute (less than 5 minutes). # Example of Expected Output ``` Shape of dataset: (1797, 64) First 5 records: [array of first 5 records] Target names: [0 1 2 3 4 5 6 7 8 9] Visualizing first image: [display matplotlib image] Accuracy score: 0.95 Confusion Matrix: [heatmap of the confusion matrix] ```","solution":"import matplotlib.pyplot as plt import seaborn as sns from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score, confusion_matrix # 1. Load the `digits` Dataset digits = datasets.load_digits() # Display the first 5 records of the dataset to understand the data structure print(f\\"First 5 records:n{digits.data[:5]}\\") # 2. Perform basic data analysis # Print the shape of the dataset (number of samples and features) print(f\\"Shape of dataset: {digits.data.shape}\\") # Display the target names/classes print(f\\"Target names: {digits.target_names}\\") # Visualize the first image in the dataset using matplotlib plt.imshow(digits.images[0], cmap=\'gray\') plt.title(\\"First Image in the Digits Dataset\\") plt.show() # 3. Split the dataset into training and testing sets (use a test size of 20%) X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) # Implement a classification model (e.g., K-Nearest Neighbors) model = KNeighborsClassifier(n_neighbors=3) # Train the model on the training data model.fit(X_train, y_train) # Make predictions on the testing data y_pred = model.predict(X_test) # 4. Evaluate the model performance # Evaluate the model performance using accuracy and confusion matrix accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Print the accuracy score print(f\\"Accuracy score: {accuracy:.2f}\\") # Display the confusion matrix using a heatmap plt.figure(figsize=(10, 7)) sns.heatmap(conf_matrix, annot=True, fmt=\'d\', cmap=\'Blues\', xticklabels=digits.target_names, yticklabels=digits.target_names) plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.title(\\"Confusion Matrix Heatmap\\") plt.show()"},{"question":"**Objective:** Design and implement a system of geometric shapes using abstract base classes. Students are required to demonstrate their understanding of Python\'s `abc` module by creating abstract classes, abstract methods, and utilizing virtual subclass registration. **Problem Statement:** 1. **Define an Abstract Base Class `Shape`**: - Create an abstract base class `Shape` using the `ABC` helper class. - Declare two abstract methods: - `area(self)`: Should return the area of the shape. - `perimeter(self)`: Should return the perimeter of the shape. 2. **Create Concrete Subclasses**: - Implement two concrete subclasses `Circle` and `Rectangle` that inherit from `Shape`. - `Circle`: Should be initialized with a `radius`. - Implement `area(self)` to return the area of the circle. - Implement `perimeter(self)` to return the perimeter of the circle. - `Rectangle`: Should be initialized with `width` and `height`. - Implement `area(self)` to return the area of the rectangle. - Implement `perimeter(self)` to return the perimeter of the rectangle. 3. **Register a Virtual Subclass**: - Define a class `Square` that does not directly inherit from `Shape`, but should be considered a virtual subclass of `Rectangle`. - Implement `Square` to be initialized with a `side_length`, and have the following methods: - `area(self)`: Returns the area of the square. - `perimeter(self)`: Returns the perimeter of the square. - Register `Square` as a virtual subclass of `Rectangle`. 4. **Demonstrate Functionality**: - Create instances of `Circle`, `Rectangle`, and `Square`. - Verify that the `isinstance` and `issubclass` functions reflect the correct relationships, including the virtual subclass. **Constraints:** - Use Python 3.10 or higher. - Ensure all abstract methods are implemented in the concrete classes. **Input and Output**: - The exact input and output format is flexible and depends on the implementation details. The primary goal is to validate through code that the abstract and concrete class structures are properly followed. **Example**: ```python from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return 3.14 * (self.radius ** 2) def perimeter(self): return 2 * 3.14 * self.radius class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Square: def __init__(self, side_length): self.side_length = side_length def area(self): return self.side_length ** 2 def perimeter(self): return 4 * self.side_length Rectangle.register(Square) # Create instances shapes = [ Circle(5), Rectangle(4, 6), Square(3) ] # Verify relationships print(isinstance(shapes[0], Shape)) # True print(isinstance(shapes[1], Shape)) # True print(isinstance(shapes[2], Rectangle)) # True print(issubclass(Square, Shape)) # True as Square is a virtual subclass of Rectangle ``` **Your Task**: Implement the given abstract base class and concrete classes, register the virtual subclass, and demonstrate the functionality using the provided example structure.","solution":"from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return 3.14 * (self.radius ** 2) def perimeter(self): return 2 * 3.14 * self.radius class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Square: def __init__(self, side_length): self.side_length = side_length def area(self): return self.side_length ** 2 def perimeter(self): return 4 * self.side_length Rectangle.register(Square) # Create instances circle = Circle(5) rectangle = Rectangle(4, 6) square = Square(3) # Verify relationships is_instance_circle = isinstance(circle, Shape) # True is_instance_rectangle = isinstance(rectangle, Shape) # True is_instance_square_shape = isinstance(square, Shape) # True due to virtual subclassing is_instance_square_rectangle = isinstance(square, Rectangle) # True is_subclass_square_shape = issubclass(Square, Shape) # True as Square is a virtual subclass of Rectangle is_subclass_square_rectangle = issubclass(Square, Rectangle) # True"},{"question":"**Building a Concurrent TCP Echo Server** **Objective:** You are to design a basic TCP echo server using the `socketserver` module. The server should handle multiple simultaneous connections using threads. Each connected client should be able to send a string message to the server, and the server should respond by echoing back the same message. **Requirements:** 1. **Server Class:** Implement a threaded TCP server class that can handle multiple client connections concurrently. 2. **Request Handler:** Implement a request handler that processes incoming messages from clients and sends back the same message (echo). 3. **Concurrency:** Utilize the `socketserver.ThreadingMixIn` mixin class to ensure the server handles connections in separate threads. **Constraints:** - The server must listen on localhost and port 9999. - The server should be able to handle at least 10 simultaneous client connections. - Each message sent by the client will be a single line of text. - The server should run indefinitely until a CTRL-C interrupt is received. **Input:** - Messages from clients (e.g., \\"Hello, Server\\"). **Output:** - Echoed messages sent back to the clients (e.g., \\"Hello, Server\\"). - Print each client\'s message and the thread handling it on the server console. **Performance:** - The server should efficiently manage multiple connections and respond to clients without significant delay. **Implementation Details:** 1. **Server Setup:** - Define a class `ThreadedTCPServer` that inherits from `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. - Define the server\'s address (`HOST` as \\"localhost\\" and `PORT` as 9999). 2. **Request Handler:** - Define a class `EchoRequestHandler` inheriting from `socketserver.BaseRequestHandler`. - Implement the `handle` method to read data from the client, print the received message and thread name, and then send the data back to the client. 3. **Running the Server:** - Create an instance of `ThreadedTCPServer` with the specified server address and request handler. - Use a `with` statement to ensure the server is properly closed on exit. - Call the `serve_forever` method to start the server loop. **Example:** ```python import socketserver import threading class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): self.data = self.request.recv(1024).strip() print(\\"{} wrote: {}\\".format(self.client_address[0], self.data.decode(\'utf-8\'))) print(\\"Handled by thread:\\", threading.current_thread().name) self.request.sendall(self.data) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), EchoRequestHandler) as server: print(\\"Server started at {}:{}\\".format(HOST, PORT)) server.serve_forever() ``` **Instructions:** 1. Implement the `EchoRequestHandler` and `ThreadedTCPServer` classes. 2. Ensure the server handles multiple client connections concurrently using threads. 3. Test your server using a TCP client script or tools such as `telnet` or `nc`. Good luck!","solution":"import socketserver import threading class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): self.data = self.request.recv(1024).strip() print(\\"{} wrote: {}\\".format(self.client_address[0], self.data.decode(\'utf-8\'))) print(\\"Handled by thread:\\", threading.current_thread().name) self.request.sendall(self.data) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), EchoRequestHandler) as server: print(\\"Server started at {}:{}\\".format(HOST, PORT)) try: server.serve_forever() except KeyboardInterrupt: print(\\"nServer is shutting down.\\")"},{"question":"Objective You are required to implement a function that computes the Normalized L2 Error between two PyTorch tensors. The Normalized L2 Error is useful in comparing the similarity between tensors in a normalized manner, making it easier to interpret the error relative to the magnitude of the compared items. Function Signature ```python def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: ``` Inputs - `x` (torch.Tensor): A PyTorch tensor of any shape. - `y` (torch.Tensor): Another PyTorch tensor of the same shape as `x`. Outputs - A single floating-point number representing the normalized L2 error between tensors `x` and `y`. Constraints and Assumptions - You can assume that both tensors are non-empty and have the same shape. - You should use PyTorch operations to ensure efficient computation. Example ```python import torch x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([1.0, 3.0, 5.0]) error = compute_normalized_l2_error(x, y) print(f\\"Normalized L2 Error: {error}\\") ``` Expected output: ``` Normalized L2 Error: 0.564976 ``` Notes 1. The Normalized L2 Error between two tensors `x` and `y` is defined as: [ text{Normalized L2 Error} = sqrt{frac{sum (x_i - y_i)^2}{sum x_i^2}} ] 2. Please ensure that the result is a floating-point number with reasonable precision. Requirements - The solution should demonstrate efficient and accurate use of tensor operations in PyTorch. - The result should be properly normalized, making the error value interpretable.","solution":"import torch import math def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: Computes the normalized L2 error between two tensors. Args: x (torch.Tensor): A tensor of any shape. y (torch.Tensor): Another tensor of the same shape as `x`. Returns: float: The normalized L2 error between tensors `x` and `y`. numerator = torch.sum((x - y) ** 2) denominator = torch.sum(x ** 2) normalized_l2_error = torch.sqrt(numerator / denominator).item() return normalized_l2_error"},{"question":"Coding Assessment Question # Objective Design a utility that overrides the default behavior of file reading and writing using the provided `builtins` module. # Problem Statement You are to create a custom file handler class called `CustomFileHandler` that wraps the built-in `open` function. This handler should have the following features: - When reading from a file, it converts all text to lower case. - When writing to a file, it appends a timestamp and a custom message to the beginning of the file content to be written. # Requirements 1. **Input Format**: - For reading: A file path (string). - For writing: A file path (string) and content (string). 2. **Output Format**: - For reading: Lower-case content of the file (string). - For writing: File content prefixed with a timestamp and a custom message. 3. **Constraints**: - The class should internally use `builtins.open` to handle file operations. - The timestamp should be in the format `YYYY-MM-DD HH:MM:SS`. # Implementation Details 1. Create a class `CustomFileHandler` with methods: - `read_file(self, file_path)`: Reads the content of the file at `file_path` and returns it in lower case. - `write_file(self, file_path, content)`: Writes `content` to the file at `file_path` with the current timestamp and custom message prefixed. 2. The custom message to be written should be \\"Written by CustomFileHandler\\". 3. Use the `datetime` module to get the current timestamp. # Example ```python # Usage Example: handler = CustomFileHandler() # Writing to a file handler.write_file(\'example.txt\', \'Hello World!\') # Reading from a file content = handler.read_file(\'example.txt\') print(content) # Output will be the content of \'example.txt\' in lower case ``` # Constraints - Do not modify the default behavior of `builtins.open()` globally. - Ensure the class uses `builtins.open` only internally to handle the files. # Performance Requirements - Efficient file reading and writing is expected. - Avoid loading large files entirely into memory; handle them in chunks if necessary.","solution":"import builtins import datetime class CustomFileHandler: def read_file(self, file_path): with builtins.open(file_path, \'r\', encoding=\'utf-8\') as file: return file.read().lower() def write_file(self, file_path, content): timestamp = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") custom_message = \\"Written by CustomFileHandler\\" prefixed_content = f\\"{timestamp} - {custom_message}n{content}\\" with builtins.open(file_path, \'w\', encoding=\'utf-8\') as file: file.write(prefixed_content)"},{"question":"# K-Nearest Neighbors Classifier Implementation Using KDTree In this assignment, you are required to implement a K-Nearest Neighbors (KNN) classifier using the KDTree algorithm from the `sklearn.neighbors` module. Your task is to write a function that initializes the classifier, trains it on the given dataset, and evaluates its performance on a test dataset. Function Signature ```python def knn_classifier_with_kdtree(train_data, train_labels, test_data, test_labels, k, metric=\'euclidean\'): Function to implement K-Nearest Neighbors classifier using KDTree. Parameters: - train_data (np.array): A 2D numpy array of shape (n_train_samples, n_features) representing the training data. - train_labels (np.array): A 1D numpy array of shape (n_train_samples,) representing the training labels. - test_data (np.array): A 2D numpy array of shape (n_test_samples, n_features) representing the test data. - test_labels (np.array): A 1D numpy array of shape (n_test_samples,) representing the test labels. - k (int): The number of nearest neighbors to consider for classification. - metric (str): The distance metric to use for the KDTree. Default is \'euclidean\'. Returns: - accuracy (float): The classification accuracy of the KNN classifier on the test dataset. pass ``` Instructions 1. **Import the necessary libraries:** - Use `KDTree` from `sklearn.neighbors`. - You may also need other libraries such as `numpy` for handling arrays. 2. **Initialize the KDTree:** - Create a KDTree instance using the training data and the specified metric. 3. **Find the k-nearest neighbors:** - Use the KDTree to find the k-nearest neighbors for each point in the test dataset. 4. **Predict Labels:** - Predict the label of each test sample by majority voting on the labels of its k-nearest neighbors. 5. **Evaluate the classifier:** - Compute the classification accuracy by comparing the predicted labels with the true test labels. 6. **Arguments:** - `train_data`: A 2D numpy array of shape (n_train_samples, n_features). - `train_labels`: A 1D numpy array of shape (n_train_samples,). - `test_data`: A 2D numpy array of shape (n_test_samples, n_features). - `test_labels`: A 1D numpy array of shape (n_test_samples,). - `k`: Number of nearest neighbors to consider. - `metric`: Distance metric to use for KDTree (default is \'euclidean\'). 7. **Returns:** - The function should return the accuracy of the classifier on the test dataset. Example Usage ```python import numpy as np # Sample training data (4 samples, 2 features) train_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) train_labels = np.array([0, 0, 1, 1]) # Sample test data (2 samples, 2 features) test_data = np.array([[1.5, 2.5], [3.5, 4.5]]) test_labels = np.array([0, 1]) # Number of neighbors k = 3 # Call your function accuracy = knn_classifier_with_kdtree(train_data, train_labels, test_data, test_labels, k) print(f\\"Accuracy: {accuracy}\\") ``` Constraints - `1 <= k <= len(train_data)` - The `metric` must be one of the valid metrics for KDTree, which can be obtained by `KDTree.valid_metrics`. Notes - Focus on implementing the functionality as efficiently as possible. - Ensure proper handling of edge cases, such as ties in majority voting.","solution":"import numpy as np from sklearn.neighbors import KDTree def knn_classifier_with_kdtree(train_data, train_labels, test_data, test_labels, k, metric=\'euclidean\'): Function to implement K-Nearest Neighbors classifier using KDTree. Parameters: - train_data (np.array): A 2D numpy array of shape (n_train_samples, n_features) representing the training data. - train_labels (np.array): A 1D numpy array of shape (n_train_samples,) representing the training labels. - test_data (np.array): A 2D numpy array of shape (n_test_samples, n_features) representing the test data. - test_labels (np.array): A 1D numpy array of shape (n_test_samples,) representing the test labels. - k (int): The number of nearest neighbors to consider for classification. - metric (str): The distance metric to use for the KDTree. Default is \'euclidean\'. Returns: - accuracy (float): The classification accuracy of the KNN classifier on the test dataset. # Initialize KDTree with the training data and specified metric kdtree = KDTree(train_data, metric=metric) # Predict labels for test data predictions = [] for test_point in test_data: distances, indices = kdtree.query([test_point], k=k) nearest_labels = train_labels[indices[0]] # Majority voting unique, counts = np.unique(nearest_labels, return_counts=True) vote_result = unique[np.argmax(counts)] predictions.append(vote_result) # Calculate accuracy predictions = np.array(predictions) accuracy = np.sum(predictions == test_labels) / len(test_labels) return accuracy"},{"question":"# Problem: Concurrent Execution and Resource Management You are required to implement a Python function that processes a list of tasks using both threading and multiprocessing to optimize for both CPU-bound and IO-bound operations. The task list contains tuples where each tuple has the format `(task_type, data)`. The `task_type` can be either `\\"CPU\\"` or `\\"IO\\"` indicating whether the task is CPU-bound or IO-bound, respectively. The `data` is an integer representing the task\'s complexity. Your function should: 1. Use the `concurrent.futures.ThreadPoolExecutor` for IO-bound tasks. 2. Use the `concurrent.futures.ProcessPoolExecutor` for CPU-bound tasks. 3. Return a list of results where each result corresponds to the processed data of the task. # Input - A list of tuples `tasks` where each tuple is `(task_type, data)`. - `task_type`: a string that is either `\\"CPU\\"` or `\\"IO\\"`. - `data`: an integer representing the complexity of the task. # Output - A list of results where each result is an integer representing the processed data of the task after applying the appropriate processing. # Function Signature ```python from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor def process_tasks(tasks: list) -> list: pass ``` # Example ```python tasks = [(\\"CPU\\", 10), (\\"IO\\", 20), (\\"CPU\\", 5), (\\"IO\\", 15)] result = process_tasks(tasks) print(result) # Output will vary based on the task processing results ``` # Constraints 1. Each task should take a considerable amount of time to demonstrate the benefit of concurrent execution. 2. Use lock mechanisms to ensure thread-safe operations if shared resources are involved. 3. Assume that the IO operations can be simulated using `time.sleep()`. 4. Assume that the CPU operations can be simulated using a computationally intensive function. # Note - You are required to implement the logic for CPU-bound and IO-bound task processing within the `process_tasks` function. - Use appropriate concurrency primitives to manage and monitor the execution of tasks.","solution":"import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor # Simulate a CPU-bound task by performing a computationally intensive operation def cpu_bound_task(data): # Just a dummy function that performs some heavy computations result = 0 for i in range(1, data * 10000): result += i * i return result # Simulate an IO-bound task by sleeping for some time def io_bound_task(data): time.sleep(data / 10) # Simulate IO delay return data # Return data to represent completion def process_tasks(tasks): results = [] with ThreadPoolExecutor() as thread_pool, ProcessPoolExecutor() as process_pool: future_to_task = {} for task_type, data in tasks: if task_type == \\"CPU\\": future = process_pool.submit(cpu_bound_task, data) elif task_type == \\"IO\\": future = thread_pool.submit(io_bound_task, data) future_to_task[future] = (task_type, data) for future in future_to_task: results.append(future.result()) return results"},{"question":"# Question: PCA Application and Comparison You are given a dataset composed of several high-dimensional numerical features. Your task is to implement a function that applies Principal Component Analysis (PCA) to reduce the dimensionality of the dataset, comparing different settings for the PCA transformation. Function Signature ```python import numpy as np import pandas as pd from sklearn.decomposition import PCA def apply_pca_and_compare(dataset: pd.DataFrame, n_components_list: list, whiten_option: bool) -> dict: pass ``` Requirements 1. **Input**: - `dataset`: A Pandas DataFrame containing numerical features. - `n_components_list`: A list of integers specifying the number of components to keep for PCA. - `whiten_option`: A boolean indicating whether to apply whitening (`whiten=True`) or not (`whiten=False`). 2. **Output**: - A dictionary where each key is the number of components from `n_components_list` and the value is a dictionary containing the following: - The explained variance ratio for the PCA transformation. - The first record of the transformed dataset as a list. 3. **Functionality**: - For each number of components specified in `n_components_list`: - Apply PCA to the provided dataset using that specific number of components. - If `whiten_option` is true, apply the whitening option of PCA. - Capture the explained variance ratio and the first record of the transformed dataset. - Store these results in a dictionary and return it. Example: ```python # Sample dataset data = { \'A\': [1.0, 2.0, 3.0, 4.0], \'B\': [2.1, 3.2, 4.3, 5.5], \'C\': [1.3, 1.5, 1.7, 1.9], \'D\': [4.0, 2.0, 3.0, 1.0] } df = pd.DataFrame(data) n_components_list = [2, 3] whiten_option = False result = apply_pca_and_compare(df, n_components_list, whiten_option) print(result) ``` Constraints: - Use the `PCA` class from the `sklearn.decomposition` module. - Ensure that the function is efficient for moderate sizes of dataframes (few thousand rows and columns). Notes: - Consider checking the input types and validity within the function. - Test the function with different datasets and component settings to ensure its correctness.","solution":"import numpy as np import pandas as pd from sklearn.decomposition import PCA def apply_pca_and_compare(dataset: pd.DataFrame, n_components_list: list, whiten_option: bool) -> dict: results = {} for n_components in n_components_list: # Initialize PCA with the given number of components and whiten option. pca = PCA(n_components=n_components, whiten=whiten_option) # Fit PCA to the dataset and transform it pca_transformed = pca.fit_transform(dataset) # Capture the explained variance ratio explained_variance_ratio = pca.explained_variance_ratio_.tolist() # Capture the first record of the transformed dataset first_record = pca_transformed[0].tolist() # Store the results in the dictionary results[n_components] = { \'explained_variance_ratio\': explained_variance_ratio, \'first_record\': first_record } return results"},{"question":"Objective: You are assigned to analyze and visualize the distribution of passenger ages across different classes and sexes using the Titanic dataset. Your task is to demonstrate your understanding of Seaborn\'s `catplot` and customization options. Task: 1. Load the Titanic dataset from Seaborn\'s built-in datasets. 2. Create a violin plot to visualize the distribution of ages (`age` column) across passenger classes (`class` column) and sexes (`sex` column). Use `hue` to distinguish between sexes. 3. Further categorize the plot by creating subplots for each passenger class using the `col` parameter. 4. Use the following customizations: - Adjust bandwidth of the kernel density estimate using `bw_adjust=0.75`. - Remove the inner representation of the violin plot. - Set the figure height to 5 and the aspect ratio to 1. - Customize the plot with appropriate axis labels and titles. - Set the y-axis limit to range from 0 to 80. Input: None (the dataset is loaded directly within your code). Output: A Seaborn plot displayed with the specified visualizations and customizations. Constraints: - Use only Seaborn and pandas libraries for your solution. - Ensure that your final plot is clear and well-labeled. Sample Solution: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"titanic\\") # Create the violin plot with the specified customizations g = sns.catplot( data=df, x=\\"sex\\", y=\\"age\\", hue=\\"sex\\", col=\\"class\\", kind=\\"violin\\", bw_adjust=0.75, inner=None, height=5, aspect=1 ) # Customize the plot appearance g.set_axis_labels(\\"Sex\\", \\"Age\\") g.set_titles(\\"{col_name} Class\\") g.set(ylim=(0, 80)) # Ensure the plot is displayed plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_age_distribution(): Plots the distribution of passenger ages across different classes and sexes in the Titanic dataset. # Load the dataset df = sns.load_dataset(\\"titanic\\") # Create the violin plot with the specified customizations g = sns.catplot( data=df, x=\\"sex\\", y=\\"age\\", hue=\\"sex\\", col=\\"class\\", kind=\\"violin\\", bw_adjust=0.75, inner=None, height=5, aspect=1 ) # Customize the plot appearance g.set_axis_labels(\\"Sex\\", \\"Age\\") g.set_titles(\\"{col_name} Class\\") g.set(ylim=(0, 80)) # Ensure the plot is displayed plt.show()"},{"question":"# Seaborn Plot Context Customization **Objective:** Your task is to demonstrate your understanding of Seaborn\'s `plotting_context` function. You will need to implement a Python function that performs the following operations and generates relevant visualizations. **Problem Statement:** Implement a function called `customize_plot_context` that takes no parameters and performs the following: 1. **Retrieve Default Parameters:** - Get the current default plotting context parameters using `sns.plotting_context()` and print them. 2. **Switch to \'talk\' Context:** - Change the plotting context to the \\"talk\\" style using `sns.plotting_context(\\"talk\\")`. - Create and display a simple line plot with example data (`x = [\\"A\\", \\"B\\", \\"C\\"]` and `y = [1, 3, 2]`). 3. **Context Manager Usage:** - Use a context manager to temporarily switch to the \\"poster\\" style within a code block. - Inside this context, create and display another line plot with different example data (`x = [\\"D\\", \\"E\\", \\"F\\"]` and `y = [4, 6, 5]`). **Function Signature:** ```python def customize_plot_context(): pass ``` **Expected Output:** - The function should print the current default plotting context parameters. - The function should display a line plot in the \\"talk\\" style. - The function should display another line plot in the \\"poster\\" style within a context manager. **Constraints:** - You should only use the seaborn and matplotlib libraries for plotting. - Ensure your code is clean and well-documented. Here\'s an example usage and output of the function: ```python customize_plot_context() ``` - Expected printed output: A dictionary of the current default plotting context parameters. - Expected visual output: Two line plots, one in \\"talk\\" style and another in \\"poster\\" style. The code should be executable in a typical Python environment with seaborn and matplotlib installed. Make sure the plots are generated and displayed properly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot_context(): # Retrieve and print the current default plotting context parameters default_context = sns.plotting_context() print(\\"Default plotting context parameters:\\", default_context) # Switch to \'talk\' context and create a line plot sns.set_context(\\"talk\\") x_talk = [\\"A\\", \\"B\\", \\"C\\"] y_talk = [1, 3, 2] plt.figure() plt.plot(x_talk, y_talk) plt.title(\\"Talk Context Plot\\") plt.show() # Use a context manager to temporarily switch to \'poster\' context and create another line plot with sns.plotting_context(\\"poster\\"): x_poster = [\\"D\\", \\"E\\", \\"F\\"] y_poster = [4, 6, 5] plt.figure() plt.plot(x_poster, y_poster) plt.title(\\"Poster Context Plot\\") plt.show()"},{"question":"# Objective: Write a comprehensive function utilizing the `importlib.metadata` module to extract and summarize metadata from given packages and output a structured JSON string containing this data. # Task: Implement a function `summarize_package_metadata(package_names: list) -> str` that takes a list of package names and returns a JSON-formatted string summarizing vital metadata about these packages. The output JSON should include the package name, version, requirements, and the list of files installed by the package. # Constraints: 1. If a package from the input list is not installed, the corresponding entry in the summary should indicate that the package is missing. 2. The function should handle a reasonably large number of packages, say up to 50. 3. The function can assume internet access for querying package metadata if necessary. # Input: - `package_names: list` - A list containing the names of the packages (as strings) to query. # Output: - A JSON-formatted string summarizing the relevant metadata of each package. # Example: ```python import json def summarize_package_metadata(package_names: list) -> str: from importlib.metadata import version, metadata, requires, files summary = {} for package in package_names: try: pkg_info = {} pkg_info[\'version\'] = version(package) pkg_info[\'requires\'] = requires(package) pkg_info[\'files\'] = [str(f) for f in files(package)] summary[package] = pkg_info except Exception: summary[package] = \'Package not installed\' return json.dumps(summary, indent=4) # Example usage packages = [\\"pip\\", \\"setuptools\\", \\"non_existent_package\\"] print(summarize_package_metadata(packages)) ``` # Explanation: - The function begins by importing necessary functions from `importlib.metadata`. - It initializes an empty dictionary `summary` to collect metadata for each package. - For each package in the input list, it attempts to gather metadata (version, requirements, files). If successful, it stores these details in a dictionary keyed by the package name. If an exception is raised (e.g., the package is not installed), it records that the package is not installed. - Finally, it converts the dictionary to a JSON-formatted string and returns it. **Edge Cases to Consider:** - The input list may be empty. - Packages might not have complete metadata (e.g., no requirements). - Package names may include those with unusual characters or names. --- By solving this problem, students will demonstrate their ability to navigate and manipulate package metadata, showing an understanding of data collection, error handling, and JSON serialization in Python.","solution":"import json from importlib.metadata import version, metadata, requires, files def summarize_package_metadata(package_names: list) -> str: summary = {} for package in package_names: try: pkg_info = {} pkg_info[\'version\'] = version(package) pkg_info[\'requires\'] = requires(package) or [] pkg_info[\'files\'] = [str(f) for f in files(package)] or [] summary[package] = pkg_info except Exception: summary[package] = \'Package not installed\' return json.dumps(summary, indent=4)"},{"question":"**Special Mathematical Operations on Tensor Data using PyTorch** PyTorch provides a variety of special mathematical functions in the `torch.special` module, which can be used for complex numerical computations. In this task, you will demonstrate your understanding of these functions by implementing a custom function that performs specific operations on tensor data. # Task Write a function `special_operations(tensor: torch.Tensor) -> torch.Tensor` that takes a single 2-dimensional tensor as input and performs the following operations on each element: 1. Compute the natural logarithm of the complementary error function (using `torch.special.erfc`) for each element. 2. Apply the log-sum-exp trick across each row (using `torch.special.logsumexp`). 3. For each element, compute its exponential integral (using `torch.special.expm1`). 4. Return the resulting tensor after these operations. # Constraints - The input tensor will have dimensions (n x m), where 1 <= n, m <= 100. - The elements of the tensor will be real numbers between -10 and 10. # Input - `tensor` (torch.Tensor): A 2D tensor of shape (n, m) with real numbers between -10 and 10. # Output - A 2D tensor of shape (n, m) after performing the specified operations. # Example ```python import torch # Example tensor input_tensor = torch.tensor([ [0.5, 1.0, -1.0], [2.0, -3.0, 4.0] ]) # Call the function output_tensor = special_operations(input_tensor) print(output_tensor) ``` # Note Make sure your function uses `torch.special` for the required operations and handles PyTorch tensors correctly. Ensure that your code is optimized to handle the edge cases effectively.","solution":"import torch def special_operations(tensor: torch.Tensor) -> torch.Tensor: Perform special mathematical operations on the input tensor. Args: - tensor (torch.Tensor): A 2D tensor with real numbers between -10 and 10. Returns: - torch.Tensor: The resulting tensor after performing the specified operations. # Step 1: Compute the natural logarithm of the complementary error function for each element comp_error_func_log = torch.log(torch.special.erfc(tensor)) # Step 2: Apply the log-sum-exp trick across each row log_sum_exp_rows = torch.special.logsumexp(comp_error_func_log, dim=1, keepdim=True) # Subtract log_sum_exp_rows from comp_error_func_log to reset the log-scaled tensor to original scale before Step 3 centered_tensor = comp_error_func_log - log_sum_exp_rows # Step 3: Compute the exponential integral exp(x) - 1 for each element exp_integral = torch.special.expm1(centered_tensor) return exp_integral"},{"question":"Objective To assess the students\' understanding of creating a source distribution in Python, handling file operations, and customizing the manifest file using the provided MANIFEST.in guidelines. Problem Statement You are given a Python project directory containing several Python scripts, text files, and subdirectories. Implement a function `create_source_distribution` which generates a source distribution archive for this project. The function should allow specifying additional files to include via a MANIFEST.in file and should support creation of the distribution in multiple formats. Function Signature ```python def create_source_distribution(project_dir: str, manifest_template: str, output_dir: str, formats: str = \\"gztar\\"): pass ``` Input - `project_dir` (str): The path to the root directory of the Python project. - `manifest_template` (str): The path to the MANIFEST.in template file specifying additional files to include in the distribution. - `output_dir` (str): The path to the directory where the generated source distribution archives should be saved. - `formats` (str): A comma-separated string specifying the formats of the archive (default is \\"gztar\\"). Output - The function does not return a value but creates one or more source distribution archive(s) in the specified `output_dir`. Constraints - The `project_dir` must contain a valid `setup.py` file. - The function should support the formats: \\"zip\\", \\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", and \\"tar\\". - The `manifest_template` should conform to the specifications given in the document. Example Usage ```python create_source_distribution( project_dir=\\"/path/to/project\\", manifest_template=\\"/path/to/MANIFEST.in\\", output_dir=\\"/path/to/output\\", formats=\\"gztar,zip\\" ) ``` Notes - Ensure the `MANIFEST` file is generated or updated based on the provided `manifest_template`. - Use appropriate error handling for file operations. - You may use Python\'s `setuptools` library to facilitate the creation of the source distribution. Hints - You can use the `setuptools` module to invoke the `sdist` command programmatically. - Consider using temporary directories for intermediate files if needed.","solution":"import os import shutil import subprocess from setuptools import setup from setuptools.command.sdist import sdist def create_source_distribution(project_dir: str, manifest_template: str, output_dir: str, formats: str = \\"gztar\\"): # Ensure the provided project directory contains a valid setup.py setup_file = os.path.join(project_dir, \'setup.py\') if not os.path.isfile(setup_file): raise FileNotFoundError(f\\"No setup.py found in the specified project directory: {project_dir}\\") # Copy the MANIFEST.in template to the project directory manifest_dest = os.path.join(project_dir, \'MANIFEST.in\') shutil.copyfile(manifest_template, manifest_dest) # Change the current directory to the project directory current_dir = os.getcwd() os.chdir(project_dir) try: # Run the sdist command with the specified formats subprocess.run([\'python\', \'setup.py\', \'sdist\', \'--formats=\' + formats], check=True) # Move the created distribution archives to the output directory dist_dir = os.path.join(project_dir, \'dist\') for file_name in os.listdir(dist_dir): full_file_name = os.path.join(dist_dir, file_name) if os.path.isfile(full_file_name): shutil.move(full_file_name, output_dir) finally: # Revert to the original directory os.chdir(current_dir)"},{"question":"# PyTorch Distributed RPC Framework Assessment **Objective:** Design a distributed training setup using PyTorch\'s Distributed RPC Framework. Your task is to implement a system where one worker serves as a parameter server holding a shared model, while multiple trainer workers fetch parameters from the server, perform training steps, and update the model on the parameter server. **Requirements:** 1. Implement the necessary initialization for the distributed environment. 2. Define and implement a simple neural network model to be shared across workers. 3. Implement the following functions: - `parameter_server(rank, world_size)`: Initializes the parameter server which holds the model. - `trainer(rank, world_size)`: Fetches the model parameters from the parameter server, performs a dummy training step, and updates the parameters on the parameter server. 4. Use Remote Procedure Calls (RPC) to fetch and update model parameters. **Input and Output Format:** - The input will be the rank of the current process and the total number of processes (world_size). - There is no direct input/output for the functions as they are meant to be run in a distributed environment. **Constraints:** 1. Only CPU tensors should be used. 2. The model should be a simple feed-forward neural network. **Function Signatures:** ```python import torch import torch.nn as nn import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) global_model = None def parameter_server(rank, world_size): Initializes the parameter server and sets up RPC. Arguments: rank -- the rank of the current process world_size -- the total number of processes global global_model global_model = SimpleModel() options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=8, rpc_timeout=20) rpc.init_rpc(f\\"parameter_server{rank}\\", rank=rank, world_size=world_size, rpc_backend_options=options) rpc.shutdown() def trainer(rank, world_size): Fetches model parameters from the parameter server, performs a dummy training step, and updates the parameters on the parameter server. Arguments: rank -- the rank of the current process world_size -- the total number of processes options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=8, rpc_timeout=20) rpc.init_rpc(f\\"trainer{rank}\\", rank=rank, world_size=world_size, rpc_backend_options=options) # Fetch model parameters server_rref = rpc.remote(f\\"parameter_server0\\", lambda: global_model) model = server_rref.to_here() # Dummy training step dummy_input = torch.randn(10) output = model(dummy_input) loss = output.sum() loss.backward() # Update model parameters with torch.no_grad(): for param in model.parameters(): param -= 0.01 * param.grad # Send updated parameters back to the parameter server rpc.rpc_sync(f\\"parameter_server0\\", torch.distributed.rpc.RRef.to_here(), args=(list(model.parameters()),)) rpc.shutdown() # Example usage: if __name__ == \\"__main__\\": world_size = 3 # 1 parameter server + 2 trainers for i in range(world_size): if i == 0: parameter_server(i, world_size) else: trainer(i, world_size) ``` **Notes:** - Ensure that the `SimpleModel` class and the `global_model` variable are defined in a way that they can be accessed across different RPC calls. - Remember to handle the `rpc_shutdown` to ensure the RPC framework is properly terminated in each function. **Performance Considerations:** - Use appropriate synchronization mechanisms to avoid race conditions when updating model parameters. - Determine the optimal number of worker threads based on the system architecture and workload.","solution":"import torch import torch.nn as nn import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) # Simple linear layer def forward(self, x): return self.fc(x) global_model = None def parameter_server(rank, world_size): Initializes the parameter server which holds the model. Arguments: rank -- the rank of the current process world_size -- the total number of processes global global_model global_model = SimpleModel() options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=8, rpc_timeout=20) rpc.init_rpc(f\\"parameter_server{rank}\\", rank=rank, world_size=world_size, rpc_backend_options=options) # Wait until all training is done rpc.shutdown() def trainer(rank, world_size): Fetches model parameters from the parameter server, performs a dummy training step, and updates the parameters on the parameter server. Arguments: rank -- the rank of the current process world_size -- the total number of processes options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=8, rpc_timeout=20) rpc.init_rpc(f\\"trainer{rank}\\", rank=rank, world_size=world_size, rpc_backend_options=options) # Fetch model parameters server_rref = rpc.remote(f\\"parameter_server0\\", lambda: global_model) def get_model(): return global_model server_model = server_rref.rpc_sync().to_here() # Dummy training step dummy_input = torch.randn(1, 10) output = server_model(dummy_input) loss = output.sum() loss.backward() # Update model parameters with torch.no_grad(): for param in server_model.parameters(): param -= 0.01 * param.grad # No inbuilt method to send back updated parameters, creating a custom RPC update function def update_params(new_params): with torch.no_grad(): for param, new_param in zip(global_model.parameters(), new_params): param.copy_(new_param) rpc.rpc_sync(f\\"parameter_server0\\", update_params, args=(list(server_model.parameters()),)) # Shut down the RPC framework rpc.shutdown() # Example usage: if __name__ == \\"__main__\\": world_size = 3 # 1 parameter server + 2 trainers from multiprocessing import Process def init_process(rank, world_size, fn): fn(rank, world_size) processes = [] for rank in range(world_size): p = Process(target=init_process, args=(rank, world_size, parameter_server if rank == 0 else trainer)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"Working with Event Scheduler You are tasked with implementing a small program that schedules and manages events using Python\'s `sched` module. Write a Python class `EventManager` that utilizes the `sched.scheduler` to manage and execute events. Class Definition ```python class EventManager: def __init__(self): # Initialize your scheduler with appropriate time and delay functions pass def schedule_event(self, delay, priority, action, argument=(), kwargs={}): Schedules an event with a delay in seconds, priority, action, optional arguments and keyword arguments. Returns the event object. pass def schedule_event_at(self, event_time, priority, action, argument=(), kwargs={}): Schedules an event at an absolute time, priority, action, optional arguments and keyword arguments. Returns the event object. pass def cancel_event(self, event): Cancels a scheduled event. pass def run(self, blocking=True): Runs all scheduled events, optionally in a blocking manner. pass def list_events(self): Returns a list of all scheduled events in the order they will be executed. pass ``` Example Usage ```python import time def my_event(action_name): print(f\\"Executing {action_name} at time {time.time()}\\") # Create an instance of EventManager em = EventManager() # Schedule some events em.schedule_event(5, 2, my_event, argument=(\'Event 1\',)) em.schedule_event(3, 1, my_event, argument=(\'Event 2\',)) em.schedule_event_at(time.time() + 10, 1, my_event, argument=(\'Event 3\',)) # List scheduled events print(em.list_events()) # Run scheduled events em.run() ``` Constraints 1. Events with a lower priority number should execute first if they are scheduled for the same time. 2. The scheduler should be able to handle multiple threads safely. 3. Your implementation should handle cases where `action` or `delayfunc` raise exceptions without crashing. Expected Output 1. The `schedule_event` and `schedule_event_at` methods should return the event object that can be used to cancel the event. 2. The `cancel_event` method should remove the event from the schedule. 3. The `run` method should execute all events in the correct order based on their scheduled time and priority. 4. The `list_events` method should return a list of upcoming events with details as named tuples. Please implement the `EventManager` class with these methods and make sure they operate as described.","solution":"import sched import time from collections import namedtuple EventDetail = namedtuple(\\"EventDetail\\", [\\"time\\", \\"priority\\", \\"action\\", \\"argument\\", \\"kwargs\\"]) class EventManager: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.events = [] def schedule_event(self, delay, priority, action, argument=(), kwargs={}): event_time = time.time() + delay event = self.scheduler.enterabs(event_time, priority, action, argument, kwargs) self.events.append(EventDetail(event_time, priority, action, argument, kwargs)) return event def schedule_event_at(self, event_time, priority, action, argument=(), kwargs={}): event = self.scheduler.enterabs(event_time, priority, action, argument, kwargs) self.events.append(EventDetail(event_time, priority, action, argument, kwargs)) return event def cancel_event(self, event): self.scheduler.cancel(event) self.events = [e for e in self.events if e.action != event.action or e.time != event.time] def run(self, blocking=True): try: self.scheduler.run(blocking=blocking) except Exception as e: print(f\\"Exception occurred while running events: {e}\\") def list_events(self): return sorted(self.events, key=lambda e: (e.time, e.priority))"},{"question":"You are tasked with writing a Python function that takes a list of URLs, validates them, fetches the content from each URL, and performs some basic parsing and data extraction from the content. Specifically, you need to achieve the following: 1. **Validate the URLs:** Ensure that each URL is well-formed. 2. **Fetch the Content:** Make an HTTP GET request to each validated URL and retrieve the content. 3. **Handle Errors:** Appropriately handle scenarios where the URL is not reachable or results in an HTTP error. 4. **Parse URL Components:** Extract and print the scheme, netloc, path, parameters, query, and fragment of each valid URL. 5. **Extract Title Tag:** From the fetched content of each URL, extract and print the contents of the `<title>` tag found in the HTML. # Function Signature ```python def process_urls(url_list: List[str]) -> None: pass ``` # Input - `url_list`: A list of strings, where each string is a URL that needs to be processed (1 <= len(url_list) <= 100). # Output - For each URL, the function should print: - \\"Invalid URL\\" if the URL is not well-formed. - \\"URL Error: <error_message>\\" if there is an issue reaching the URL. - Scheme, netloc, path, parameters, query, and fragment of each valid URL. - Content of the `<title>` tag for each fetched HTML content. # Constraints - Your function should perform the network requests and handling synchronously. - You are not required to use advanced asynchronous programming techniques. - Assume that HTTP responses will be relatively small in size. # Example ```python urls = [ \\"https://www.example.com\\", \\"https://www.nonexistentwebsite.xyz\\", \\"htp://malformed.url\\", \\"https://www.github.com\\" ] process_urls(urls) ``` Expected output: ``` URL: https://www.example.com Scheme: https Netloc: www.example.com Path: / Params: Query: Fragment: Title: Example Domain URL Error: <HTTPError Message> Invalid URL URL: https://www.github.com Scheme: https Netloc: www.github.com Path: / Params: Query: Fragment: Title: GitHub: Where the world builds software · GitHub ``` # Notes - Be sure to handle various error scenarios gracefully to avoid your program crashing abruptly. - Use the `urllib` package appropriately for URL handling tasks.","solution":"import requests from urllib.parse import urlparse from bs4 import BeautifulSoup def process_urls(url_list): for url in url_list: parsed_url = urlparse(url) if not (parsed_url.scheme and parsed_url.netloc): print(\\"Invalid URL\\") continue try: response = requests.get(url) response.raise_for_status() except requests.RequestException as e: print(f\\"URL Error: {str(e)}\\") continue print(f\\"URL: {url}\\") print(f\\" Scheme: {parsed_url.scheme}\\") print(f\\" Netloc: {parsed_url.netloc}\\") print(f\\" Path: {parsed_url.path}\\") print(f\\" Params: {parsed_url.params}\\") print(f\\" Query: {parsed_url.query}\\") print(f\\" Fragment: {parsed_url.fragment}\\") soup = BeautifulSoup(response.content, \'html.parser\') title_tag = soup.title.string if soup.title else \\"No Title Found\\" print(f\\" Title: {title_tag}\\") print()"},{"question":"You are required to write a Python C extension that provides a Python function `filter_and_sort_even_numbers` as part of a module named `custom_list_ops`. This function will take a single argument, a Python list `lst`. The function should filter out the even numbers from the list, sort them in ascending order, and return a tuple containing these sorted even numbers. Function Signature ```python def filter_and_sort_even_numbers(lst: list) -> tuple ``` Input - `lst`: A Python list of integers (e.g., `[10, 3, 6, 2, 15]`). Output - Returns a tuple containing sorted even numbers (e.g., `(2, 6, 10)`). # Constraints 1. The input list `lst` can be empty. 2. The integers in `lst` are in the range of [-10**6, 10**6]. 3. The function should handle lists with up to 10^6 elements efficiently. # Performance Requirements The solution must be efficient with a time complexity of O(n log n), where n is the number of elements in the input list `lst`. # Example ```python # Example usage in Python after the extension module is compiled and imported import custom_list_ops result = custom_list_ops.filter_and_sort_even_numbers([10, 3, 6, 2, 15]) print(result) # Output: (2, 6, 10) ``` # Instructions 1. Implement the `filter_and_sort_even_numbers` function using the CPython API functions provided in the documentation above. 2. Make sure to handle cases where the list can be empty. 3. Include necessary error handling and input validation. 4. Write tests in Python to validate your C extension. 5. Provide a README file on how to compile and install the C extension module. Useful Functions from the Documentation - `PyList_New`, `PyList_Size`, `PyList_GetItem`, `PyList_SetItem`, `PyList_Sort`, `PyList_AsTuple`. Good luck!","solution":"from typing import List, Tuple def filter_and_sort_even_numbers(lst: List[int]) -> Tuple[int]: Filters and sorts even numbers from the given list in ascending order. Parameters: lst (List[int]): A list of integers to be filtered and sorted. Returns: Tuple[int]: A tuple containing the sorted even numbers. even_numbers = [num for num in lst if num % 2 == 0] even_numbers.sort() return tuple(even_numbers)"},{"question":"**Question: Implement Subsampling for Sparse COO Tensors in PyTorch** You are tasked with implementing a function to subsample a sparse COO tensor in PyTorch. Given a sparse COO tensor, write a function that randomly selects a subset of the non-zero entries based on a specified subsampling ratio. # Function Signature ```python def subsample_sparse_coo(coo_tensor: torch.sparse_coo_tensor, ratio: float) -> torch.sparse_coo_tensor: pass ``` # Input - `coo_tensor`: A sparse COO tensor (an instance of `torch.sparse_coo_tensor`). - `ratio`: A float representing the subsampling ratio. The value is between 0 and 1, where 1 means return the original tensor and 0 means return an empty tensor. # Output - A sparse COO tensor containing a random subset of the non-zero entries of the input tensor, according to the given ratio. # Constraints - You must ensure that the subsampled tensor maintains the sparse COO format. - The subsampling should be randomized. - The resulting sparse tensor should be coalesced (no duplicate indices). # Example ```python i = torch.tensor([[0, 1, 1], [2, 0, 2]]) v = torch.tensor([3, 4, 5], dtype=torch.float32) s = torch.sparse_coo_tensor(i, v, (2, 3)) ratio = 0.5 subsampled_s = subsample_sparse_coo(s, ratio) print(subsampled_s) # Possible output could be a tensor with fewer non-zero elements like: # tensor(indices=tensor([[0, 1], # [2, 2]]), # values=tensor([3., 5.]), # size=(2, 3), nnz=2, layout=torch.sparse_coo) ``` # Notes - You may use any PyTorch functions or methods, but ensure that the resulting tensor is in the sparse COO format. - Use the `.coalesce()` method to ensure the final output does not contain duplicate indices. - To randomly sample indices, consider using PyTorch\'s random number generation facilities. # Performance Considerations - The function should perform efficiently even for large sparse tensors. Efficient handling of indices and values is crucial. **Hint**: Pay special attention to correctly subsampling both the indices and values while maintaining the integrity of the COO representation (i.e., correctly paired indices and values).","solution":"import torch def subsample_sparse_coo(coo_tensor: torch.sparse_coo, ratio: float) -> torch.sparse_coo: Subsamples the given sparse COO tensor according to the given ratio. Args: coo_tensor (torch.sparse_coo_tensor): Input sparse COO tensor. ratio (float): Subsampling ratio. A float between 0 and 1. Returns: torch.sparse_coo: Subsampled sparse COO tensor. assert 0 <= ratio <= 1, \\"The ratio must be between 0 and 1.\\" if ratio == 1: # No subsampling needed, return the original tensor return coo_tensor elif ratio == 0: # Return an empty sparse tensor with the same shape empty_indices = coo_tensor._indices()[:, :0] # empty indices tensor empty_values = coo_tensor._values()[:0] # empty values tensor return torch.sparse_coo_tensor(empty_indices, empty_values, coo_tensor.size()) # Retrieve the indices and values of the sparse tensor indices = coo_tensor._indices() values = coo_tensor._values() # Calculate the number of non-zero elements to keep nnz = values.size(0) num_samples = int(nnz * ratio) # Generate random sample indices perm = torch.randperm(nnz) sampled_indices = perm[:num_samples] # Select the sampled elements subsampled_indices = indices[:, sampled_indices] subsampled_values = values[sampled_indices] # Create the new subsampled sparse tensor subsampled_tensor = torch.sparse_coo_tensor(subsampled_indices, subsampled_values, coo_tensor.size()).coalesce() return subsampled_tensor"},{"question":"Secure File Integrity Check with Personalized BLAKE2 Hashing You have been contracted to create a secure file integrity verification system that uses the BLAKE2 hashing algorithm. This system should allow users to generate personalized hash values for their files and verify the integrity of these files later on. Implement the following two functions: 1. **generate_personalized_hash(file_path: str, personalization_string: str) -> str:** - **Input:** - `file_path`: A string representing the path to the file. - `personalization_string`: A string that is used to personalize the hash value. - **Output:** - Returns a hexadecimal string representing the personalized hash value of the file\'s content. - **Constraints:** - The file must be read in binary mode. - The personalization string must be no longer than 16 bytes. 2. **verify_file_integrity(file_path: str, expected_hash: str, personalization_string: str) -> bool:** - **Input:** - `file_path`: A string representing the path to the file. - `expected_hash`: The expected hexadecimal hash string to compare against. - `personalization_string`: A string that was used to personalize the hash value. - **Output:** - Returns `True` if the file\'s hash matches the expected hash, `False` otherwise. # Requirements: - Use the `BLAKE2b` algorithm from the `hashlib` module. - Ensure that the personalized hashing uses the provided `personalization_string`. # Example: ```python # Example usage file_path = \'example.txt\' personalization_string = \'file_check\' # Generate personalized hash generated_hash = generate_personalized_hash(file_path, personalization_string) print(f\\"Generated Hash: {generated_hash}\\") # Verify the integrity of the file is_intact = verify_file_integrity(file_path, generated_hash, personalization_string) print(f\\"File Integrity Check: {is_intact}\\") ``` Assume the file `example.txt` contains the text `\\"Hello, world!\\"`. ```python # Example output Generated Hash: \'51f7b68a845fbefc5a23e8b2c8555b6ad47af8a93d1c6d912bdbf998d8bfa59e\' File Integrity Check: True ``` # Notes: - If the file changes, the integrity check should return `False`. - The `personalization_string` is an important part of the hashing process and can be used to ensure that different users can have different hashes for the same file content. # Implementation Hints: - Refer to the `hashlib` and `blake2b` documentation for implementing the personalization feature. - Ensure to handle file reading appropriately to avoid memory issues with large files.","solution":"import hashlib def generate_personalized_hash(file_path: str, personalization_string: str) -> str: Generates a personalized BLAKE2b hash for the given file. :param file_path: Path to the file :param personalization_string: Personalization string, must be no longer than 16 bytes :return: Hexadecimal string representing the personalized hash value of the file\'s content # Ensure that the personalization string is no longer than 16 bytes if len(personalization_string) > 16: raise ValueError(\\"Personalization string must be no longer than 16 bytes\\") # Create a BLAKE2b hash object with the personalization string blake2b_hash = hashlib.blake2b(person=personalization_string.encode(\'utf-8\')) # Read the file in binary mode and update the hash object with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\\"\\"): blake2b_hash.update(chunk) # Return the hexadecimal digest of the hash return blake2b_hash.hexdigest() def verify_file_integrity(file_path: str, expected_hash: str, personalization_string: str) -> bool: Verifies the file integrity by comparing the personalized hash to the expected hash. :param file_path: Path to the file :param expected_hash: Expected hexadecimal hash string to compare against :param personalization_string: Personalization string used for hashing :return: True if the file\'s hash matches the expected hash, False otherwise # Generate the personalized hash for the given file and personalization string actual_hash = generate_personalized_hash(file_path, personalization_string) # Compare the generated hash with the expected hash and return the result return actual_hash == expected_hash"},{"question":"**Problem Statement: Advanced Logging System with Custom Rotating Files** You have been tasked to implement a logging system for a multi-threaded web server application. The logging system must meet the following requirements: 1. Log messages from various parts of the application to both the console and rotating log files. 2. Log files should be rotated when they reach a size of 5MB. Up to 5 rotated log files must be kept. 3. Log messages should include the time, logger name, level, and message. 4. Implement a custom log message format that includes additional context such as the IP address and username of the requester. 5. Ensure log rotation happens smoothly without interruption in a multi-threaded setup. 6. Use a `QueueHandler` to ensure that logging operations do not block the main application threads. Write a Python function called `setup_logging` that configures the logging system to meet these requirements. Additionally, demonstrate its usage by setting up logging in a sample web server function that handles incoming requests. # Requirements: - **Function: `setup_logging()`** - *Arguments:* None - *Returns:* None - **Sample Function:** Demonstrate logging from a sample function. # Example Usage: ```python def setup_logging(): # Your implementation here pass def handle_request(ip: str, username: str): import logging logger = logging.getLogger(\\"web_server\\") # Log at different levels logger.debug(f\\"Debug message from {username}@{ip}\\") logger.info(f\\"Info message from {username}@{ip}\\") logger.warning(f\\"Warning message from {username}@{ip}\\") logger.error(f\\"Error message from {username}@{ip}\\") logger.critical(f\\"Critical message from {username}@{ip}\\") if __name__ == \\"__main__\\": setup_logging() handle_request(\\"192.168.1.1\\", \\"alice\\") handle_request(\\"192.168.1.2\\", \\"bob\\") ``` # Constraints: - Ensure that the setup_logging function works seamlessly in a multi-threaded environment. - Use `QueueListener` and `QueueHandler` to offload actual logging operations from the main threads. # Assessment Criteria: - Proper use of the Python logging module. - Implementation of rotating file handler. - Inclusion of additional context in log messages. - Demonstration of logging usage in a multi-threaded application setup.","solution":"import logging import logging.handlers from queue import Queue from threading import Thread class ContextFilter(logging.Filter): def filter(self, record): record.ip = getattr(record, \'ip\', \'N/A\') record.username = getattr(record, \'username\', \'N/A\') return True def setup_logging(): log_queue = Queue() queue_handler = logging.handlers.QueueHandler(log_queue) root_logger = logging.getLogger() root_logger.addHandler(queue_handler) root_logger.setLevel(logging.DEBUG) formatter = logging.Formatter(\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s - IP: %(ip)s - User: %(username)s\\") # Console Handler console_handler = logging.StreamHandler() console_handler.setFormatter(formatter) # Rotating File Handler file_handler = logging.handlers.RotatingFileHandler(\'web_server.log\', maxBytes=5 * 1024 * 1024, backupCount=5) file_handler.setFormatter(formatter) # Adding a context filter context_filter = ContextFilter() listeners = [console_handler, file_handler] for listener in listeners: listener.addFilter(context_filter) def listener_thread(): while True: record = log_queue.get() if record is None: # Sentinel to cleanly shutdown the logging thread break for listener in listeners: listener.handle(record) listener = Thread(target=listener_thread) listener.setDaemon(True) listener.start() return listener, log_queue def handle_request(ip: str, username: str): logger = logging.getLogger(\\"web_server\\") extra = {\'ip\': ip, \'username\': username} logger.debug(\\"Debug message from handler\\", extra=extra) logger.info(\\"Info message from handler\\", extra=extra) logger.warning(\\"Warning message from handler\\", extra=extra) logger.error(\\"Error message from handler\\", extra=extra) logger.critical(\\"Critical message from handler\\", extra=extra) if __name__ == \\"__main__\\": listener, log_queue = setup_logging() handle_request(\\"192.168.1.1\\", \\"alice\\") handle_request(\\"192.168.1.2\\", \\"bob\\") # Ensure the logging thread properly shuts down log_queue.put(None) listener.join()"},{"question":"# Python Coding Assessment Question **Problem Statement:** Your task is to create both a TCP echo client and a TCP echo server using asyncio streams in Python. They should communicate such that the client sends a message to the server, the server echoes it back, and the client receives the message and prints it. Requirements: 1. **TCP Echo Server:** - Use `asyncio.start_server` to start the server. - The server should handle each incoming connection, read a message from the client, print the received message, echo the message back to the client, and then close the connection. 2. **TCP Echo Client:** - Use `asyncio.open_connection` to connect to the server. - The client should send a message to the server, await a response, print the echoed message, and then close the connection. Function Definitions: - **TCP Echo Server:** - `async def tcp_echo_server(host: str, port: int):` where `host` is the server address and `port` is the port number. - Inside this function, write a handler function to manage client connections. - **TCP Echo Client:** - `async def tcp_echo_client(host: str, port: int, message: str):` where `host` is the server address, `port` is the port number, and `message` is the message to send. Input and Output: - Your solution should include the main function to run both the server and the client. - The client should send the message \\"Hello, World!\\" to the server. - The server should echo this message back, and the client should print it. Constraints: - Use `async` and `await` for handling asynchronous operations. - Ensure proper handling of connections by using appropriate methods for reading, writing, and closing streams. - Assume that the client and server run on the same machine (i.e., use `\'127.0.0.1\'` as the host). Example: ```python import asyncio async def handle_echo(reader, writer): ... # Implement your handler here ... async def tcp_echo_server(host: str, port: int): server = await asyncio.start_server(handle_echo, host, port) async with server: await server.serve_forever() async def tcp_echo_client(host: str, port: int, message: str): reader, writer = await asyncio.open_connection(host, port) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') writer.close() await writer.wait_closed() async def main(): server_task = asyncio.create_task(tcp_echo_server(\'127.0.0.1\', 8888)) await asyncio.sleep(1) # Give the server a second to start await tcp_echo_client(\'127.0.0.1\', 8888, \'Hello, World!\') server_task.cancel() asyncio.run(main()) ``` Your task is to implement the missing parts of the `handle_echo`, `tcp_echo_server`, and `tcp_echo_client` functions following the above instructions.","solution":"import asyncio async def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") print(f\\"Send: {message!r}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def tcp_echo_server(host: str, port: int): server = await asyncio.start_server(handle_echo, host, port) async with server: await server.serve_forever() async def tcp_echo_client(host: str, port: int, message: str): reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') writer.close() await writer.wait_closed() async def main(): server_task = asyncio.create_task(tcp_echo_server(\'127.0.0.1\', 8888)) await asyncio.sleep(1) # Give the server a second to start await tcp_echo_client(\'127.0.0.1\', 8888, \'Hello, World!\') server_task.cancel() asyncio.run(main())"},{"question":"# Python Coding Assessment Objective Demonstrate your understanding of the `pwd` module by writing Python functions that interact with Unix password database entries. Question You are required to write a function in Python that retrieves and formats user information from the Unix password database. Specifically, the function should be able to find users based on their user ID and names and output a summary of information for each user. Requirements: 1. **Function Name:** `get_user_info` 2. **Inputs:** One of the following arguments: - `uid` (integer) representing the numerical user ID. - `name` (string) representing the login name of the user. 3. **Output:** A dictionary with the following structure: ```python { \'login_name\': <login_name>, \'uid\': <uid>, \'gid\': <gid>, \'user_name_or_comment\': <user_name_or_comment>, \'home_directory\': <home_directory>, \'command_interpreter\': <command_interpreter> } ``` 4. **Constraints:** - If the input is a user ID that does not exist or a name that is not found, the function should return `None`. - Use the `pwd` module\'s functions `getpwuid` and `getpwnam` for retrieving the user information. Example Usage: ```python def get_user_info(identifier): import pwd # Your code here # Example calls: print(get_user_info(0)) # Example for UID # Expected output # { # \'login_name\': \'root\', # \'uid\': 0, # \'gid\': 0, # \'user_name_or_comment\': \'System Administrator\', # \'home_directory\': \'/root\', # \'command_interpreter\': \'/bin/bash\' # } print(get_user_info(\'root\')) # Example for username # Expected output # { # \'login_name\': \'root\', # \'uid\': 0, # \'gid\': 0, # \'user_name_or_comment\': \'System Administrator\', # \'home_directory\': \'/root\', # \'command_interpreter\': \'/bin/bash\' # } print(get_user_info(999999)) # Example for non-existing UID # Expected output: None print(get_user_info(\'nonexistentuser\')) # Example for non-existing username # Expected output: None ``` Notes: - The function should handle both user IDs and user names correctly. - Ensure that the function returns `None` for non-existing users without throwing an exception. - Assume the platform is Unix-based.","solution":"import pwd def get_user_info(identifier): try: if isinstance(identifier, int): user_info = pwd.getpwuid(identifier) elif isinstance(identifier, str): user_info = pwd.getpwnam(identifier) else: return None except KeyError: return None return { \'login_name\': user_info.pw_name, \'uid\': user_info.pw_uid, \'gid\': user_info.pw_gid, \'user_name_or_comment\': user_info.pw_gecos, \'home_directory\': user_info.pw_dir, \'command_interpreter\': user_info.pw_shell }"},{"question":"**Question Title: Manipulating and Analyzing Time-Series Data with DateOffsets in Pandas** **Objective:** Write a function that can manipulate a time-series DataFrame by applying various `DateOffset` objects and create a summary based on specific offset conditions. **Problem Statement:** Given a time-series DataFrame `df` with a DateTime index representing daily stock prices, implement a function `analyze_stock_data` that performs the following operations: 1. Add a new column `IsMonthEnd` to `df` which is True if the date is the end of the month, otherwise False. 2. Add a new column `IsMonthStart` to `df` which is True if the date is the start of the month, otherwise False. 3. Create a new DataFrame `business_days` containing only the rows where the date is a business day (excluding weekends and custom holidays specified). 4. Add a column `NextQuarterEnd` to `df` which contains the date of the next quarter end. 5. Return a summary DataFrame with the following stats for the original DataFrame: - Total number of business days - Total number of month end days - Total number of month start days - Date of the next quarter end from the last date in `df` **Input:** - `df` : A pandas DataFrame with DateTime index. The DataFrame has a column `StockPrice` representing the daily stock price. - `custom_holidays`: A list of dates that are considered holidays. **Output:** - A tuple with the modified DataFrame `df` and the summary DataFrame `summary_df`. ```python def analyze_stock_data(df: pd.DataFrame, custom_holidays: list) -> tuple[pd.DataFrame, pd.DataFrame]: pass ``` **Constraints:** - The DataFrame `df` contains at least one year\'s worth of daily data. - The list `custom_holidays` contains valid dates that fall within the range of `df`. **Example:** ```python import pandas as pd # Sample DataFrame dates = pd.date_range(start=\'2021-01-01\', end=\'2021-12-31\') data = {\'StockPrice\': [i + (i % 5) for i in range(len(dates))]} df = pd.DataFrame(data, index=dates) # Custom holidays custom_holidays = [\'2021-01-01\', \'2021-12-25\'] # Function call new_df, summary_df = analyze_stock_data(df, custom_holidays) print(new_df.head()) print(summary_df) ``` This function tests students\' ability to: - Utilize various `DateOffsets` like `MonthEnd`, `MonthBegin`, `BusinessDay`, and `QuarterEnd`. - Add and manipulate columns based on offset calculations. - Filter DataFrames based on custom business criteria. - Summarize time-series data effectively.","solution":"import pandas as pd from pandas.tseries.offsets import BDay, MonthEnd, MonthBegin, QuarterEnd def analyze_stock_data(df: pd.DataFrame, custom_holidays: list) -> tuple[pd.DataFrame, pd.DataFrame]: # Add \'IsMonthEnd\' column df[\'IsMonthEnd\'] = df.index.is_month_end # Add \'IsMonthStart\' column df[\'IsMonthStart\'] = df.index.is_month_start # Create \'business_days\' DataFrame excluding weekends and custom holidays business_days = df[df.index.to_series().apply(lambda x: x.weekday() < 5 and x not in custom_holidays)] # Add \'NextQuarterEnd\' column df[\'NextQuarterEnd\'] = df.index + QuarterEnd() # Create summary DataFrame summary_data = { \'Total Business Days\': [len(business_days)], \'Total Month End Days\': [df[\'IsMonthEnd\'].sum()], \'Total Month Start Days\': [df[\'IsMonthStart\'].sum()], \'Date of Next Quarter End from Last Date\': [df[\'NextQuarterEnd\'].iloc[-1].date()] } summary_df = pd.DataFrame(summary_data) return df, summary_df"},{"question":"# SVM Classification and Evaluation using Scikit-learn **Objective:** Implement a Python function that uses Support Vector Machines (SVM) from scikit-learn to classify a given dataset. Your function should evaluate the model with different kernel types and hyperparameters, comparing their performance. **Task:** Write a function `svm_comparison(X_train, X_test, y_train, y_test, kernels, C_values, gamma_values)` that takes in the following inputs: - `X_train`: Training features, a numpy array of shape `(n_samples_train, n_features)`. - `X_test`: Testing features, a numpy array of shape `(n_samples_test, n_features)`. - `y_train`: Training labels, a numpy array of shape `(n_samples_train,)`. - `y_test`: Testing labels, a numpy array of shape `(n_samples_test,)`. - `kernels`: A list of kernel types to test (e.g., [\'linear\', \'rbf\', \'poly\']). - `C_values`: A list of `C` parameter values to test (e.g., [0.1, 1, 10]). - `gamma_values`: A list of `gamma` parameter values to test (e.g., [0.01, 0.1, 1]). The function should: 1. Train an SVM model for each combination of the provided `kernel`, `C`, and `gamma` values. 2. Evaluate the performance of each model on the test set using accuracy as the metric. 3. Return a list of dictionaries, each containing: - `kernel`: The kernel type used. - `C`: The `C` parameter value used. - `gamma`: The `gamma` parameter value used (Note: only applicable for non-linear kernels). - `accuracy`: The accuracy on the test set. **Constraints:** - The `gamma` parameter is only relevant for non-linear kernels (\'rbf\', \'poly\', \'sigmoid\'). **Example Usage:** ```python X_train = [[0, 0], [1, 1], [2, 2], [3, 3]] X_test = [[1, 0], [2, 1]] y_train = [0, 1, 1, 0] y_test = [0, 1] kernels = [\'linear\', \'rbf\'] C_values = [0.1, 1] gamma_values = [0.01, 0.1] results = svm_comparison(X_train, X_test, y_train, y_test, kernels, C_values, gamma_values) for res in results: print(res) ``` **Expected Output:** A list of dictionaries with each dictionary containing: - The kernel type used. - The C parameter value used. - The gamma parameter value used (if applicable). - The accuracy of the model on the test set. Example: ```python [ {\'kernel\': \'linear\', \'C\': 0.1, \'accuracy\': 0.5}, {\'kernel\': \'linear\', \'C\': 1, \'accuracy\': 0.5}, {\'kernel\': \'rbf\', \'C\': 0.1, \'gamma\': 0.01, \'accuracy\': 0.5}, {\'kernel\': \'rbf\', \'C\': 0.1, \'gamma\': 0.1, \'accuracy\': 0.5}, {\'kernel\': \'rbf\', \'C\': 1, \'gamma\': 0.01, \'accuracy\': 0.5}, {\'kernel\': \'rbf\', \'C\': 1, \'gamma\': 0.1, \'accuracy\': 0.5} ] ``` Use scikit-learn\'s `SVC` class and methods `fit`, and `predict` to implement this function.","solution":"from sklearn.svm import SVC from sklearn.metrics import accuracy_score def svm_comparison(X_train, X_test, y_train, y_test, kernels, C_values, gamma_values): results = [] for kernel in kernels: for C in C_values: if kernel == \'linear\': model = SVC(kernel=kernel, C=C) model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results.append({\'kernel\': kernel, \'C\': C, \'accuracy\': accuracy}) else: for gamma in gamma_values: model = SVC(kernel=kernel, C=C, gamma=gamma) model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results.append({\'kernel\': kernel, \'C\': C, \'gamma\': gamma, \'accuracy\': accuracy}) return results"},{"question":"<|Analysis Begin|> The documentation provides detailed information about various linear models available in the `scikit-learn` library, including their mathematical formulations and use cases. Key models include Ordinary Least Squares, Ridge Regression, Lasso, Logistic Regression, and more. Each of these models has specific purposes and constraints, and the documentation explains how to use them, the underlying mathematical principles, and their complexities. In this context, a good coding assessment question could revolve around the implementation and comparison of different linear models for a given regression problem. The problem could involve creating functions to fit models, make predictions, and evaluate their performances with appropriate metrics, reflecting the students\' understanding of the models and their practical application in scikit-learn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: To assess your understanding of different linear models in scikit-learn, you will implement a function that fits multiple linear models to a dataset, makes predictions, and evaluates their performance. Problem: You are given a dataset containing features `X` and a target variable `y`. Your task is to implement a Python function using scikit-learn to perform the following steps: 1. Split the dataset into training and testing sets. 2. Fit three different linear models to the training data: - Ordinary Least Squares (using `LinearRegression`). - Ridge Regression (using `Ridge`). - Lasso Regression (using `Lasso`). 3. Make predictions on the testing set using each of the fitted models. 4. Evaluate the performance of each model using Mean Squared Error (MSE). 5. Return a dictionary containing the MSE for each model. Function Signature: ```python def evaluate_linear_models(X: np.ndarray, y: np.ndarray) -> dict: Fits multiple linear models to the dataset and evaluates their performance. Args: X (np.ndarray): The feature matrix with shape (n_samples, n_features). y (np.ndarray): The target vector with shape (n_samples,). Returns: dict: A dictionary containing the Mean Squared Error for each model: {\'OLS\': float, \'Ridge\': float, \'Lasso\': float} pass ``` Constraints: - Use an 80-20 split for the training and testing sets. - Use a Ridge alpha value of 1.0. - Use a Lasso alpha value of 0.1. Example: ```python import numpy as np # Example data X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = np.dot(X, np.array([1, 2])) + 3 # Calling the function results = evaluate_linear_models(X, y) # Example output print(results) # Output: {\'OLS\': 0.0, \'Ridge\': 0.0, \'Lasso\': some_value} ``` The example above is provided to help you understand the expected input and output format. Ensure your solution meets these specifications and handles different datasets correctly. You can assume that the `scikit-learn` library is installed and available for import.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error def evaluate_linear_models(X: np.ndarray, y: np.ndarray) -> dict: Fits multiple linear models to the dataset and evaluates their performance. Args: X (np.ndarray): The feature matrix with shape (n_samples, n_features). y (np.ndarray): The target vector with shape (n_samples,). Returns: dict: A dictionary containing the Mean Squared Error for each model: {\'OLS\': float, \'Ridge\': float, \'Lasso\': float} # Split the data into training and testing sets using 80-20 split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models ols = LinearRegression() ridge = Ridge(alpha=1.0) lasso = Lasso(alpha=0.1) # Fit models ols.fit(X_train, y_train) ridge.fit(X_train, y_train) lasso.fit(X_train, y_train) # Make predictions y_pred_ols = ols.predict(X_test) y_pred_ridge = ridge.predict(X_test) y_pred_lasso = lasso.predict(X_test) # Calculate Mean Squared Errors mse_ols = mean_squared_error(y_test, y_pred_ols) mse_ridge = mean_squared_error(y_test, y_pred_ridge) mse_lasso = mean_squared_error(y_test, y_pred_lasso) return {\'OLS\': mse_ols, \'Ridge\': mse_ridge, \'Lasso\': mse_lasso}"},{"question":"**Coding Assessment Question: PyTorch Tensor Testing** # Objective In this exercise, you will need to use PyTorch\'s `torch.testing` module to write a function that validates certain properties of tensors. This problem will assess your ability to use `assert_close`, `make_tensor`, and `assert_allclose` functions for tensor validation. # Problem Statement Implement a function `validate_tensors()` that generates tensors and verifies their properties using `torch.testing` functions. Your function should: 1. Create two random tensors `a` and `b` of size 3x3, each containing floating point numbers between -5 and 5. 2. Create a third tensor `c` which is expected to be the sum of `a` and `b`. 3. Verify if `a + b` is close to `c` using `assert_close`. 4. Create a tensor `d` which is slightly different from `c` by adding a small noise (epsilon) to each element of `c`. 5. Verify that `c` and `d` are close within a defined tolerance using `assert_allclose`. # Function Signature ```python import torch def validate_tensors(epsilon: float = 1e-2) -> None: pass ``` # Inputs - `epsilon`: A small value to be added as noise to each element of tensor `c` to produce tensor `d`. Default value is `1e-2`. # Constraints - The random tensors should be generated with a floating-point value between -5 and 5. - Ensure the epsilon value is small enough to test the closeness effectively. # Output - Your function should not return any value but should raise an assertion error if any of the conditions fail. # Example Here is how your function might be used: ```python validate_tensors(epsilon=1e-3) ``` # Notes - Use the `make_tensor` function to generate tensors `a` and `b`. - Ensure to use `assert_close` to check the sum of `a` and `b` against `c`. - Use `assert_allclose` to check that `c` and `d` are close within the specified tolerance. # Additional Information - Make sure to handle any potential exceptions or errors and ensure the function is robust. - The function should not produce any print output if all assertions are met.","solution":"import torch import torch.testing def validate_tensors(epsilon: float = 1e-2) -> None: Generates two random tensors a and b, sums them to get c, and then validates the properties of these tensors using torch.testing functions. Parameters: epsilon (float): A small value added as noise to each element of tensor c to generate tensor d # Create two random tensors a and b of size 3x3 with values between -5 and 5 a = torch.rand((3, 3)) * 10 - 5 b = torch.rand((3, 3)) * 10 - 5 # Calculate tensor c as the sum of a and b c = a + b # Verify that a + b is close to c using assert_close torch.testing.assert_close(a + b, c, msg=\\"Tensors a+b and c are not close\\") # Create tensor d by adding small noise (epsilon) to each element of c d = c + epsilon * torch.rand_like(c) # Verify that tensors c and d are close within a defined tolerance using assert_allclose torch.testing.assert_allclose(c, d, rtol=0.0, atol=epsilon, msg=\\"Tensors c and d are not within tolerance\\")"},{"question":"Objective: Create a program that performs the following tasks: 1. Fetch two different real-world datasets using the `sklearn.datasets.fetch_XX` functions. 2. Perform a basic preprocessing step on these datasets. 3. Train and evaluate a simple machine learning model on one of the datasets. Tasks: 1. **Fetching Datasets**: - Fetch the `California Housing` dataset. - Fetch the `Newsgroups` dataset (you can choose any subset of categories you desire). 2. **Preprocessing**: - For the `California Housing` dataset, perform a scaling on the features. - For the `Newsgroups` dataset, vectorize the text data. 3. **Model Training and Evaluation**: - Train a linear regression model on the `California Housing` dataset to predict housing prices. - Evaluate the model\'s performance using Mean Squared Error (MSE) as the evaluation metric. Requirements: - Use `sklearn.datasets.fetch_california_housing` for fetching the California Housing data. - Use `sklearn.datasets.fetch_20newsgroups` for fetching the Newsgroups data. - Use `sklearn.preprocessing.StandardScaler` for scaling features of the California Housing dataset. - Use `sklearn.feature_extraction.text.TfidfVectorizer` for vectorizing Newsgroups data. - Use `sklearn.linear_model.LinearRegression` for the linear regression model. - Use `sklearn.metrics.mean_squared_error` for evaluating the regression model\'s performance. Input: There is no input to be provided by the user. Your code should fetch the datasets and proceed with the specified tasks. Output: - Print the shape of the datasets after loading. - Print the first 5 rows of the scaled California Housing data. - Print the shape of the vectorized Newsgroups data. - Print the MSE of the linear regression model on the California Housing dataset. Constraints: - You are only allowed to use libraries under the `sklearn.datasets` and `sklearn` packages. Example: Here is a high-level structure of the expected implementation (you need to fill in the actual code): ```python from sklearn.datasets import fetch_california_housing, fetch_20newsgroups from sklearn.preprocessing import StandardScaler from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # Fetch datasets california_housing = fetch_california_housing() newsgroups = fetch_20newsgroups(subset=\'train\', categories=[\'alt.atheism\', \'soc.religion.christian\']) # Print dataset shapes print(california_housing.data.shape, california_housing.target.shape) print(len(newsgroups.data), len(newsgroups.target)) # Preprocess California Housing data scaler = StandardScaler() scaled_features = scaler.fit_transform(california_housing.data) print(scaled_features[:5]) # Preprocess Newsgroups data vectorizer = TfidfVectorizer() vectorized_data = vectorizer.fit_transform(newsgroups.data) print(vectorized_data.shape) # Train and evaluate a linear regression model model = LinearRegression() model.fit(scaled_features, california_housing.target) predictions = model.predict(scaled_features) mse = mean_squared_error(california_housing.target, predictions) print(f\'Mean Squared Error: {mse}\') ``` Implement the full code to demonstrate your understanding of the `sklearn.datasets` utilities and basic machine learning workflows.","solution":"from sklearn.datasets import fetch_california_housing, fetch_20newsgroups from sklearn.preprocessing import StandardScaler from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def perform_data_operations(): # Fetch datasets california_housing = fetch_california_housing() newsgroups = fetch_20newsgroups(subset=\'train\', categories=[\'alt.atheism\', \'soc.religion.christian\']) # Print dataset shapes print(\\"California Housing data shape:\\", california_housing.data.shape, \\"Target shape:\\", california_housing.target.shape) print(\\"Newsgroups data length:\\", len(newsgroups.data), \\"Target length:\\", len(newsgroups.target)) # Preprocess California Housing data scaler = StandardScaler() scaled_features = scaler.fit_transform(california_housing.data) print(\\"First 5 rows of scaled California Housing data:\\", scaled_features[:5]) # Preprocess Newsgroups data vectorizer = TfidfVectorizer() vectorized_data = vectorizer.fit_transform(newsgroups.data) print(\\"Vectorized Newsgroups data shape:\\", vectorized_data.shape) # Train and evaluate a linear regression model model = LinearRegression() model.fit(scaled_features, california_housing.target) predictions = model.predict(scaled_features) mse = mean_squared_error(california_housing.target, predictions) print(f\'Mean Squared Error: {mse}\') return scaled_features, vectorized_data, mse"},{"question":"**Coding Assessment Question** # Objective To assess the understanding of PyTorch GPU operations and how specific conditions can affect performance optimizations. # Question You are given an input tensor and required to perform a series of operations ensuring each step satisfies certain conditions to take advantage of potential performance improvements described in the provided documentation. Implement the function `optimize_tensor_operations`. # Function Signature ```python def optimize_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: Perform operations on the input tensor such that the following conditions are met: 1. cudnn is enabled 2. input data is on the GPU 3. input data has dtype torch.float16 4. V100 GPU is used 5. input data is not in PackedSequence format Args: input_tensor (torch.Tensor): The input tensor to be processed. Returns: torch.Tensor: The resultant tensor after performing required operations under the specified conditions. pass ``` # Example Usage ```python import torch # Example input tensor input_tensor = torch.randn(10, 10) # Using a GPU available with V100, assuming the environment is setup correctly result_tensor = optimize_tensor_operations(input_tensor) print(result_tensor) ``` # Constraints and Requirements 1. Ensure `cudnn` is enabled. You may use: ```python torch.backends.cudnn.enabled = True ``` 2. Transfer the input tensor to GPU. 3. Convert the input tensor to `torch.float16` dtype. 4. You must ensure that the computation is running on a V100 GPU (although this cannot be programmatically enforced, set comments or checks to assert the environment). 5. Input tensor should **not** be in `PackedSequence` format. # Hints - Check if the GPU is available using `torch.cuda.is_available()`. - Use `.cuda()` to transfer the tensor to GPU. - Use `.half()` to convert tensor to `torch.float16`. # Notes - This question assumes that the environment has a configured V100 GPU accessible. - Only the tensor operations code within the function should focus on ensuring the mentioned conditions are met. # Performance Considerations - Ensure to leverage GPU computation capabilities to optimize the performance under the described conditions.","solution":"import torch def optimize_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: Perform operations on the input tensor such that the following conditions are met: 1. cudnn is enabled 2. input data is on the GPU 3. input data has dtype torch.float16 4. V100 GPU is used 5. input data is not in PackedSequence format Args: input_tensor (torch.Tensor): The input tensor to be processed. Returns: torch.Tensor: The resultant tensor after performing required operations under the specified conditions. # Enable cudnn torch.backends.cudnn.enabled = True # Verify GPU availability if not torch.cuda.is_available(): raise EnvironmentError(\\"CUDA is not available. Ensure you\'re running on a GPU-enabled environment.\\") # Transfer tensor to GPU and convert to float16 processed_tensor = input_tensor.cuda().half() # Ensure the input tensor is not in PackedSequence format # This check is conceptual since PackedSequence would be a different object type if isinstance(processed_tensor, torch.nn.utils.rnn.PackedSequence): raise TypeError(\\"Input tensor should not be in PackedSequence format.\\") return processed_tensor"},{"question":"# Advanced Python Protocols: Implementing a Custom Data Structure Objective Your task is to implement a custom data structure in Python, which supports specific functionalities using Python\'s Abstract Objects Layer. You need to demonstrate your understanding of the Sequence Protocol and Iterator Protocol. Requirements 1. **Class Implementation**: Implement a class called `CustomList`, which mimics the behavior of Python\'s built-in list. 2. **Sequence Protocol Compliance**: Ensure your class supports the following operations: - Indexing (`obj[index]`) - Slicing (`obj[start:stop:step]`) - Length (`len(obj)`) - Iteration Implement appropriate methods (__getitem__, __setitem__, __delitem__, __len__) to support these operations. 3. **Iterator Protocol Compliance**: Your class must support iteration (i.e., it should work with loops and comprehensions). Implement the `__iter__` method and an iterator class `CustomIterator` that supports the iterator protocol (`__iter__` and `__next__` methods). Constraints - Do not use Python’s built-in list data structure to directly store elements. Use another internal mechanism (e.g., a dictionary with numeric keys, or an array module). - Ensure the class handles exceptions properly (e.g., IndexError for out-of-bound indices). Input - Various operations like indexing, slicing, length checking, and iteration might be performed on instances of `CustomList`. Output - Correct behavior following the sequence and iterator protocol. Example ```python clist = CustomList([1, 2, 3, 4, 5]) # Indexing print(clist[0]) # Output: 1 # Slicing print(clist[1:4]) # Output: [2, 3, 4] # Length print(len(clist)) # Output: 5 # Iteration for item in clist: print(item) # Output: 1 2 3 4 5 ``` Implementation Notes To get you started, here is a template for your implementation: ```python class CustomIterator: def __init__(self, custom_list): self._custom_list = custom_list self._index = 0 def __iter__(self): return self def __next__(self): if self._index < len(self._custom_list): result = self._custom_list[self._index] self._index += 1 return result else: raise StopIteration class CustomList: def __init__(self, initial_data=None): self._data = {} if initial_data: for i, value in enumerate(initial_data): self._data[i] = value def __getitem__(self, index): if isinstance(index, slice): return [self._data[i] for i in range(*index.indices(len(self)))] if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") return self._data[index] def __setitem__(self, index, value): if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") self._data[index] = value def __delitem__(self, index): if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") del self._data[index] # Reindex remaining items for i in range(index, len(self._data)): self._data[i] = self._data.pop(i+1) def __len__(self): return len(self._data) def __iter__(self): return CustomIterator(self) # You can test your implementation by creating an instance of CustomList and performing various operations. ```","solution":"class CustomIterator: def __init__(self, custom_list): self._custom_list = custom_list self._index = 0 def __iter__(self): return self def __next__(self): if self._index < len(self._custom_list): result = self._custom_list[self._index] self._index += 1 return result else: raise StopIteration class CustomList: def __init__(self, initial_data=None): self._data = {} if initial_data: for i, value in enumerate(initial_data): self._data[i] = value def __getitem__(self, index): if isinstance(index, slice): return [self._data[i] for i in range(*index.indices(len(self)))] if index < 0: index += len(self._data) if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") return self._data[index] def __setitem__(self, index, value): if index < 0: index += len(self._data) if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") self._data[index] = value def __delitem__(self, index): if index < 0: index += len(self._data) if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") del self._data[index] # Reindex remaining items for i in range(index, len(self._data)): self._data[i] = self._data.pop(i+1, None) def __len__(self): return len(self._data) def __iter__(self): return CustomIterator(self)"},{"question":"# Advanced Enum Utilization in Python You are required to create a comprehensive inventory management system for a fictional company. Use the `Enum` capabilities in Python to classify and manage different types of products and their statuses in the inventory. Your task involves creating and manipulating several enums to ensure robust management and querying capabilities. Task Description 1. **Create the Enums:** - Define an `Enum` class `Category` with the following categories of products: `ELECTRONICS`, `FURNITURE`, `CLOTHING`, `FOOD`. - Define an `Enum` class `Status` using `auto()` to represent the status of a product: `AVAILABLE`, `OUT_OF_STOCK`, `DISCONTINUED`. 2. **Create a Product Class:** - Define a `Product` class with the following properties: `name` (string), `category` (instance of `Category`), `status` (instance of `Status`), and `price` (float). - Implement appropriate `__init__` and `__str__` methods for this class. 3. **Inventory Management Functions:** - Create a function `add_product(inventory, name, category, status, price)` that adds a new `Product` to the inventory (a list). - Create a function `update_status(inventory, name, new_status)` to update the status of a product given its name. - Create a function `get_available_products(inventory)` that returns a list of names of all products that are `AVAILABLE`. - Create a function `get_products_by_category(inventory, category)` that returns a list of products belonging to a given category. 4. **Constraints and Validation:** - Ensure that the `category` and `status` passed to the `Product` class and functions are valid members of the respective `Enum` classes. - Ensure that the `price` is a positive value. - Raise appropriate errors for invalid inputs. 5. **Efficiency Considerations:** - The methods should be optimized to handle an inventory list with up to 10,000 products efficiently. Expected Input and Output Formats: - `add_product(inventory, name, category, status, price)`: - Input: (`list`, `str`, `Category`, `Status`, `float`) - Output: (`None`) - `update_status(inventory, name, new_status)`: - Input: (`list`, `str`, `Status`) - Output: (`None`) - `get_available_products(inventory)`: - Input: (`list`) - Output: (`list` of `str`) - `get_products_by_category(inventory, category)`: - Input: (`list`, `Category`) - Output: (`list` of `Product`) Make sure to test your implementation using the given format and ensure it handles edge cases seamlessly.","solution":"from enum import Enum, auto class Category(Enum): ELECTRONICS = auto() FURNITURE = auto() CLOTHING = auto() FOOD = auto() class Status(Enum): AVAILABLE = auto() OUT_OF_STOCK = auto() DISCONTINUED = auto() class Product: def __init__(self, name, category, status, price): if not isinstance(category, Category): raise ValueError(\\"Invalid category\\") if not isinstance(status, Status): raise ValueError(\\"Invalid status\\") if price <= 0: raise ValueError(\\"Price must be a positive value\\") self.name = name self.category = category self.status = status self.price = price def __str__(self): return f\\"{self.name} - {self.category.name} - {self.status.name} - {self.price}\\" def add_product(inventory, name, category, status, price): product = Product(name, category, status, price) inventory.append(product) def update_status(inventory, name, new_status): if not isinstance(new_status, Status): raise ValueError(\\"Invalid status\\") for product in inventory: if product.name == name: product.status = new_status return raise ValueError(f\\"Product with name {name} not found\\") def get_available_products(inventory): return [product.name for product in inventory if product.status == Status.AVAILABLE] def get_products_by_category(inventory, category): if not isinstance(category, Category): raise ValueError(\\"Invalid category\\") return [product for product in inventory if product.category == category]"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},q=["disabled"],L={key:0},O={key:1};function N(n,e,l,m,i,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>i.searchQuery=o),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",L,"See more"))],8,q)):d("",!0)])}const M=p(z,[["render",N],["__scopeId","data-v-34bc197f"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/57.md","filePath":"chatai/57.md"}'),j={name:"chatai/57.md"},H=Object.assign(j,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,H as default};
