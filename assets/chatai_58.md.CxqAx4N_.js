import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},S={class:"review-content"};function E(s,e,l,m,n,o){return a(),i("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",E],["__scopeId","data-v-6225e967"]]),A=JSON.parse('[{"question":"**Question: Writing a POP3 Client Function** You are required to write a Python function that connects to a given POP3 email server, retrieves the list of email messages, and deletes a specific email by its index. Your function should use the `poplib` module and handle both standard and SSL connections. The function should also handle possible exceptions and provide meaningful error messages. # Function Signature ```python def manage_pop3_emails(host: str, port: int, username: str, password: str, message_index: int, use_ssl: bool = False) -> str: Connects to a POP3 email server, lists messages, and deletes the specified message. Params: - host (str): The hostname of the email server. - port (int): The port number for the POP3 service. - username (str): The username for the email account. - password (str): The password for the email account. - message_index (int): The 1-based index of the message to delete. - use_ssl (bool): Whether to use SSL for the connection (default is False). Returns: - str: A message indicating the successful deletion of the email or an error message. ``` # Input The function receives the following parameters: 1. `host` (str): The hostname of the POP3 server (e.g., \'pop.example.com\'). 2. `port` (int): The port number for the POP3 service (e.g., 110 for standard, 995 for SSL). 3. `username` (str): The username for the email account. 4. `password` (str): The password for the email account. 5. `message_index` (int): The 1-based index of the message to delete. 6. `use_ssl` (bool, default=False): Whether to use SSL for the connection. # Output The function should return a string message: - Indicating the successful deletion of the email `\\"Message {message_index} deleted successfully!\\"`. - Or, an appropriate error message if it encounters any issues during the process. # Constraints - The `message_index` should be within the range of available messages. - The function should handle possible errors such as authentication failures, connection issues, invalid message index, and other protocol-related errors using the `poplib.error_proto` exception and standard exception handling. # Example Usage ```python print(manage_pop3_emails(\'pop.example.com\', 110, \'user@example.com\', \'password123\', 1)) # Expected Output: \\"Message 1 deleted successfully!\\" or the respective error message if any issue occurs ``` # Performance Requirements The function should handle the connection and execution efficiently without unnecessary delays. Proper exception handling should ensure that specific errors provide quick feedback without crashing the function. # Note - Make sure to appropriately close the connection to the server using the `quit` method. - Use SSL connections securely by validating certificates where applicable. You may refer to the documentation of the `poplib` module for additional details on the methods and classes provided.","solution":"import poplib def manage_pop3_emails(host: str, port: int, username: str, password: str, message_index: int, use_ssl: bool = False) -> str: Connects to a POP3 email server, lists messages, and deletes the specified message. Params: - host (str): The hostname of the email server. - port (int): The port number for the POP3 service. - username (str): The username for the email account. - password (str): The password for the email account. - message_index (int): The 1-based index of the message to delete. - use_ssl (bool): Whether to use SSL for the connection (default is False). Returns: - str: A message indicating the successful deletion of the email or an error message. try: if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) server.user(username) server.pass_(password) num_messages = len(server.list()[1]) if message_index > num_messages or message_index < 1: server.quit() return f\\"Error: Message index {message_index} is out of range. Total messages: {num_messages}\\" server.dele(message_index) server.quit() return f\\"Message {message_index} deleted successfully!\\" except poplib.error_proto as e: return f\\"POP3 protocol error: {e}\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**Objective:** The goal of this task is to ensure you comprehend the fundamental and advanced concepts of distributing Python modules, both through legacy tools like Distutils and modern alternatives like Setuptools. **Task:** Implement a Python script that emulates a simplified version of the `setup.py` script using `setuptools`. This script should: 1. List multiple packages and their respective modules. 2. Register an example extension module. 3. Include metadata for author, version, and description. 4. Package additional bespoke data files located in a specific directory. 5. Output a diagnostic log to verify if the distribution created consists of all intended files. **Input:** You will receive three inputs: 1. **packages**: A list of package names and their modules in dictionary format. Example: ```python { \\"package1\\": [\\"module1\\", \\"module2\\"], \\"package2\\": [\\"module1\\", \\"module2\\"] } ``` 2. **extension_module**: A dictionary with the name and source files of an extension module. Example: ```python { \\"name\\": \\"my_extension\\", \\"sources\\": [\\"src/my_extension.c\\"] } ``` 3. **data_files**: A list of file paths for additional data files to be packaged. Example: ```python [\\"data/file1.txt\\", \\"data/file2.txt\\"] ``` **Output:** Your script should print the success message along with the list of packaged files. **Constraints:** - Ensure compatibility with Python 3.10 and later. - Your solution should handle errors gracefully, providing meaningful error messages for missing files or malformed inputs. **Implementation Requirements:** - Use the `setuptools` package. - Follow modern Python packaging practices. - Validate that all input files exist and raise appropriate exceptions if they don\'t. - Ensure the script is self-contained with clear functions to handle different parts of the packaging process. Here is a template to guide your implementation: ```python import setuptools import os def validate_files(data_files): for file in data_files: if not os.path.exists(file): raise FileNotFoundError(f\\"Data file {file} not found.\\") return True def create_setup_script(packages, extension_module, data_files): if validate_files(data_files): setuptools.setup( name=\'example_package\', version=\'0.1\', author=\'Your Name\', author_email=\'your.email@example.com\', description=\'An example package\', packages=setuptools.find_packages(include=list(packages.keys())), ext_modules=[ setuptools.Extension( extension_module[\'name\'], sources=extension_module[\'sources\'] ) ], data_files=[(\'data\', data_files)] ) print(f\\"Setup script created successfully and includes: {data_files}\\") # Example usage with given input structure packages = { \\"package1\\": [\\"module1\\", \\"module2\\"], \\"package2\\": [\\"module1\\", \\"module2\\"] } extension_module = { \\"name\\": \\"my_extension\\", \\"sources\\": [\\"src/my_extension.c\\"] } data_files = [\\"data/file1.txt\\", \\"data/file2.txt\\"] create_setup_script(packages, extension_module, data_files) ``` Make sure to test your function with different edge cases and document your code adequately.","solution":"import setuptools import os def validate_files(data_files): Validates that all data files exist. missing_files = [file for file in data_files if not os.path.exists(file)] if missing_files: raise FileNotFoundError(f\\"Data files not found: {missing_files}\\") return True def create_setup_script(packages, extension_module, data_files): Creates a setup script using setuptools with given packages, extension module, and data files. if validate_files(data_files): setuptools.setup( name=\'example_package\', version=\'0.1\', author=\'Your Name\', author_email=\'your.email@example.com\', description=\'An example package\', packages=list(packages.keys()), ext_modules=[ setuptools.Extension( extension_module[\'name\'], sources=extension_module[\'sources\'] ) ], data_files=[(\'data\', data_files)] ) print(f\\"Setup script created successfully and includes: {data_files}\\") # Example usage with given input structure packages = { \\"package1\\": [\\"module1\\", \\"module2\\"], \\"package2\\": [\\"module1\\", \\"module2\\"] } extension_module = { \\"name\\": \\"my_extension\\", \\"sources\\": [\\"src/my_extension.c\\"] } data_files = [\\"data/file1.txt\\", \\"data/file2.txt\\"] # This should only be called in a real environment where files and packages exist # create_setup_script(packages, extension_module, data_files)"},{"question":"# Asynchronous Subprocess Management You are tasked with creating a Python script that spawns multiple subprocesses to perform various system operations concurrently. Your goal is to implement a function that accepts a list of shell commands and runs them concurrently using asyncio. The function should return the combined output of all commands, ensuring that the outputs are concatenated in the order the commands were provided. **Function Signature:** ```python async def run_commands(commands: list[str]) -> str: ``` **Input:** - `commands` (list of str): A list of shell commands to be executed. Each command is a string. **Output:** - `result` (str): A single string containing the combined output from all subprocesses. The outputs should be concatenated in the order of the commands. **Constraints:** - The function must handle up to 10 commands. - Each command should be executed asynchronously. - If any command fails, capture its standard error and include it in the combined output. - Ensure that the function handles subprocess cleanup appropriately. **Performance Requirements:** - The function should efficiently handle the execution of all commands without unnecessary delays. - Use asyncio\'s features to manage concurrency. **Example:** ```python import asyncio async def run_commands(commands): # Your implementation here # Example usage: commands = [\'echo \\"Hello, World!\\"\', \'ls non_existent_directory\', \'echo \\"End of commands\\"\'] result = asyncio.run(run_commands(commands)) print(result) ``` *Expected output:* ``` Hello, World! ls: non_existent_directory: No such file or directory End of commands ``` **Additional Notes:** - You may use `asyncio.create_subprocess_shell` to run each command. - The function should ensure that if there are any subprocesses still running even after the main task is complete, they are properly terminated. - Do not use synchronous subprocess creation (e.g., using `subprocess` module outside of asyncio).","solution":"import asyncio async def run_command(command: str) -> str: process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() output = stdout.decode() + stderr.decode() return output async def run_commands(commands: list[str]) -> str: tasks = [run_command(command) for command in commands] results = await asyncio.gather(*tasks) combined_output = \'\'.join(results) return combined_output"},{"question":"**Device and Stream Management with PyTorch Accelerators** Write a Python function, named `accelerator_operations`, that demonstrates your understanding of PyTorch\'s `torch.accelerator` module. The function should perform the following tasks: 1. Print the number of accelerator devices available. 2. If an accelerator is available: - Print the current accelerator device index. - Set the device to index 0 (assuming at least one accelerator is available). - Print the current accelerator device index after setting it to 0. - Create two separate streams and set each stream one after another. - Synchronize the current stream after setting each stream. - Print a message indicating the synchronization. Your function should have no arguments and no return value, but must print the specified information to the console. **Example Output:** ``` Number of accelerator devices: 2 Current device index: 1 Device index set to 0 New current device index: 0 Stream 1 synchronized Stream 2 synchronized ``` **Constraints:** - Assume that the `torch.accelerator` library is properly installed and accessible. - Handle cases where no accelerator devices are available by printing an appropriate message. **Performance Requirements:** - Ensure all device and stream modifications are properly synchronized. Hints: - Consider using `torch.accelerator.device_count()`, `torch.accelerator.is_available()`, `torch.accelerator.set_device_index()`, `torch.accelerator.current_device_index()`, `torch.accelerator.set_stream()`, `torch.accelerator.current_stream()`, and `torch.accelerator.synchronize()`.","solution":"import torch def accelerator_operations(): # Check the number of accelerator devices num_devices = torch.cuda.device_count() print(f\\"Number of accelerator devices: {num_devices}\\") if num_devices > 0 and torch.cuda.is_available(): # Print the current accelerator device index current_device_index = torch.cuda.current_device() print(f\\"Current device index: {current_device_index}\\") # Set the device to index 0 torch.cuda.set_device(0) print(\\"Device index set to 0\\") # Print the new current accelerator device index new_device_index = torch.cuda.current_device() print(f\\"New current device index: {new_device_index}\\") # Create and set two streams, then synchronize after each setting stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() torch.cuda.set_stream(stream1) torch.cuda.current_stream().synchronize() print(\\"Stream 1 synchronized\\") torch.cuda.set_stream(stream2) torch.cuda.current_stream().synchronize() print(\\"Stream 2 synchronized\\") else: print(\\"No accelerator devices available\\")"},{"question":"You are tasked with writing a Python function that aggregates critical system and Python environment details into a single dictionary by utilizing the `platform` module. # Function Signature ```python def get_system_info() -> dict: pass ``` # Objective Implement the `get_system_info` function which gathers and returns important system information using the `platform` module. Your function must return the following details: - **Python Version**: The Python version as reported by `platform.python_version()`. - **Architecture**: Tuple of (bits, linkage) from `platform.architecture()`. - **Machine Type**: The machine type as reported by `platform.machine()`. - **Network Name**: The computerâ€™s network name from `platform.node()`. - **System Information**: A comprehensive platform string from `platform.platform()`. - **Processor Name**: The processor name from `platform.processor()`. - **Python Compiler**: The compiler used for compiling Python from `platform.python_compiler()`. - **System/OS Name**: The OS name returned by `platform.system()`. - **System Release**: The system\'s release as obtained from `platform.release()`. - **System Version**: The system\'s version as reported by `platform.version()`. # Constraints 1. The function should handle missing values gracefully. 2. The function should be optimized for readability and maintainability. 3. Ensure to use the functions from the `platform` module as documented. # Example ```python def get_system_info() -> dict: import platform system_info = { \\"Python Version\\": platform.python_version(), \\"Architecture\\": platform.architecture(), \\"Machine Type\\": platform.machine(), \\"Network Name\\": platform.node(), \\"System Information\\": platform.platform(), \\"Processor Name\\": platform.processor(), \\"Python Compiler\\": platform.python_compiler(), \\"System/OS Name\\": platform.system(), \\"System Release\\": platform.release(), \\"System Version\\": platform.version(), } return system_info # Example Usage info = get_system_info() print(info) ``` # Expected Output The output will be a dictionary containing the system information, similar to the following format: ```python { \\"Python Version\\": \\"3.10.0\\", \\"Architecture\\": (\\"64bit\\", \\"ELF\\"), \\"Machine Type\\": \\"x86_64\\", \\"Network Name\\": \\"hostname\\", \\"System Information\\": \\"Linux-5.4.0-66-generic-x86_64-with-glibc2.29\\", \\"Processor Name\\": \\"x86_64\\", \\"Python Compiler\\": \\"GCC 9.3.0\\", \\"System/OS Name\\": \\"Linux\\", \\"System Release\\": \\"5.4.0-66-generic\\", \\"System Version\\": \\"#74-Ubuntu SMP Fri Jan 15 14:05:08 UTC 2021\\", } ``` # Notes - Make sure your code handles any exceptions or missing values by providing default values where appropriate. - Document any assumptions you make during implementation.","solution":"import platform def get_system_info() -> dict: Gathers and returns important system information using the platform module. system_info = { \\"Python Version\\": platform.python_version(), \\"Architecture\\": platform.architecture(), \\"Machine Type\\": platform.machine(), \\"Network Name\\": platform.node(), \\"System Information\\": platform.platform(), \\"Processor Name\\": platform.processor(), \\"Python Compiler\\": platform.python_compiler(), \\"System/OS Name\\": platform.system(), \\"System Release\\": platform.release(), \\"System Version\\": platform.version(), } return system_info"},{"question":"# Custom SMTP Server Implementation with Python\'s `smtpd` Module You are tasked with implementing a custom SMTP server using Python\'s `smtpd` module. Your server should process incoming emails and store the data in a text file. Additionally, the server should log each email\'s originating IP address and the recipient addresses to `stdout`. Requirements: 1. **Server Setup**: - Create a custom SMTP server class derived from `smtpd.SMTPServer`. - Bind the server to `localhost` on port `1025`. - Set the `data_size_limit` to `10MB`. 2. **Message Processing**: - Override the `process_message` method to handle incoming emails. - Store each email\'s data in a file named `emails.txt`. - Log the sender\'s IP address and recipient addresses to `stdout`. 3. **Running the Server**: - Ensure the server runs to handle multiple emails. - Terminate the server gracefully upon receiving a keyboard interrupt (`Ctrl+C`). Input: The program has no direct inputs other than the emails received by the SMTP server. Output: - Emails\' data stored in `emails.txt`. - Sender\'s IP address and recipient addresses logged to `stdout`. Performance Requirements: - The server should efficiently handle emails up to 10MB in size. - Properly handle multiple concurrent SMTP connections. Example: If an email is sent from `user@example.com` to `receiver@domain.com`, and the sender\'s IP is `192.168.1.1`, it should log: ``` Received email from 192.168.1.1 to [\'receiver@domain.com\'] ``` And store the email content in `emails.txt`. Constraints: - Use Python 3.10. - Make use of the built-in `smtpd` and `asyncore` modules. - Ensure thread safety if handling concurrent connections. # Your Task: Implement the custom SMTP server according to the described requirements. Your implementation should start the server and handle the described email processing and logging. ```python import smtpd import asyncore import sys class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): with open(\'emails.txt\', \'a\') as f: f.write(data + \'n\') print(f\'Received email from {peer[0]} to {rcpttos}\') if __name__ == \'__main__\': server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=10485760) try: print(\'Starting SMTP Server...\') asyncore.loop() except KeyboardInterrupt: print(\'Terminating SMTP Server...\') sys.exit(0) ``` Implement and extend the given code template to meet all the requirements.","solution":"import smtpd import asyncore import sys class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): with open(\'emails.txt\', \'a\') as f: f.write(f\'From: {mailfrom}nTo: {\\", \\".join(rcpttos)}nn{data}nn\') print(f\'Received email from {peer[0]} to {rcpttos}\') if __name__ == \'__main__\': server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=10485760) try: print(\'Starting SMTP Server...\') asyncore.loop() except KeyboardInterrupt: print(\'Terminating SMTP Server...\') sys.exit(0)"},{"question":"Objective Demonstrate your understanding of the `fileinput` module by developing a function that reads lines from multiple files, processes the lines to count the occurrence of a specific word, and supports handling of compressed files. Problem Statement Implement a function `count_word_occurrences(files: List[str], word: str, encoding: str = \\"utf-8\\") -> Dict[str, int]` that: 1. Reads lines from the given list of files. 2. Supports both regular and compressed files (i.e., files ending in `.gz` and `.bz2`). 3. Counts the occurrences of the specified word (case-insensitive) in each file. 4. Returns a dictionary where the keys are filenames and the values are the counts of the word\'s occurrences in that file. Function Signature ```python from typing import List, Dict import fileinput def count_word_occurrences(files: List[str], word: str, encoding: str = \\"utf-8\\") -> Dict[str, int]: # Your implementation here pass ``` Input - `files`: A list of strings where each string is a filename. -- The files can be regular text files or compressed files (with extensions `.gz` or `.bz2`). - `word`: A string representing the word to count in the files. - `encoding`: An optional string representing the encoding of the files. Default is \\"utf-8\\". Output - Returns a dictionary with the filenames as keys and the count of the word\'s occurrences as values. # Example ```python # Assuming the existence of the following files with corresponding content: # file1.txt content: \\"Hello world!nThis is a test.nHello again.n\\" # file2.txt content: \\"hello there. How are you?nThis is another test.n\\" files = [\\"file1.txt\\", \\"file2.txt\\"] word = \\"hello\\" result = count_word_occurrences(files, word) print(result) # Output: {\\"file1.txt\\": 2, \\"file2.txt\\": 1} ``` Constraints - The function should handle file opening errors gracefully and continue with the next file without breaking. - Lines must be read with newlines intact. - The word counting should be case-insensitive. - If a file is empty, it should return a count of 0 for that file. Guidelines - Use the `fileinput.input` function with the appropriate `openhook` for compressed files. - Ensure proper handling of encodings and potential reading errors. You can use helper functions as needed to keep the main function clean and readable. Performance Expectations - The function should be efficient in terms of both time and space complexity. - Handle large files or large numbers of small files gracefully.","solution":"from typing import List, Dict import fileinput import gzip import bz2 def openhook_compressed_files(filename, mode): An openhook function to handle opening of compressed files. Supports .gz and .bz2 files. if filename.endswith(\'.gz\'): return gzip.open(filename, mode + \'t\') elif filename.endswith(\'.bz2\'): return bz2.open(filename, mode + \'t\') else: return open(filename, mode) def count_word_occurrences(files: List[str], word: str, encoding: str = \\"utf-8\\") -> Dict[str, int]: word = word.lower() counts = {} for filename in files: count = 0 try: with fileinput.input(files=[filename], openhook=openhook_compressed_files) as f: for line in f: line = line.lower() count += line.split().count(word) except IOError: count = -1 # Indicate an error by using -1 counts[filename] = count return counts"},{"question":"**Objective**: Implement a semi-supervised learning model using the `sklearn.semi_supervised` module to classify data. **Background**: You are given a dataset that contains both labeled and unlabeled data points. The task is to use semi-supervised learning to train a model that can classify the labeled and unlabeled data efficiently. **Task**: 1. **Dataset Preparation**: - Create a synthetic dataset using `make_classification` from `sklearn.datasets`, with a defined number of informative and redundant features. - Randomly set a subset of the labels in the dataset to -1 to simulate unlabeled data. 2. **Model Implementation**: - Implement a semi-supervised learning model using `SelfTrainingClassifier` with a base estimator of your choice (e.g., `LogisticRegression` or `RandomForestClassifier`). - Train the model using the prepared dataset. 3. **Evaluation**: - Predict labels for all samples (both initially labeled and unlabeled) and calculate the accuracy on the initially labeled data. 4. **Parameters**: - Use `max_iter=10` for the self-training loop. - Use a `threshold=0.75` for the prediction probability threshold to add samples to the labeled set. **Input Format**: - n_samples (int): Number of samples for the synthetic dataset. - n_features (int): Number of features for the synthetic dataset. - n_informative (int): Number of informative features for the synthetic dataset. - n_unlabeled (float): Fraction of samples to be unlabeled (e.g., 0.3 for 30% unlabeled samples). **Output Format**: - Print the accuracy of the semi-supervised model on the initially labeled data. **Constraints**: - Use scikit-learn version 0.24 or higher. - Ensure reproducibility by setting a random seed. **Example**: ```python from sklearn.datasets import make_classification from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score import numpy as np def semi_supervised_learning(n_samples, n_features, n_informative, n_unlabeled): # Step 1: Dataset Preparation X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, random_state=42) # Set a fraction of labels to -1 (unlabeled) rng = np.random.RandomState(42) n_unlabeled_points = int(n_samples * n_unlabeled) unlabeled_indices = rng.choice(n_samples, n_unlabeled_points, replace=False) y[unlabeled_indices] = -1 # Step 2: Model Implementation base_estimator = LogisticRegression(solver=\'lbfgs\', max_iter=1000) self_training_model = SelfTrainingClassifier(base_estimator, threshold=0.75, max_iter=10) self_training_model.fit(X, y) # Step 3: Evaluation supervised_indices = [i for i in range(n_samples) if i not in unlabeled_indices] y_pred = self_training_model.predict(X) accuracy = accuracy_score(y[supervised_indices], y_pred[supervised_indices]) # Output the accuracy print(f\\"Accuracy on initially labeled data: {accuracy}\\") # Example Usage semi_supervised_learning(1000, 20, 2, 0.3) ```","solution":"from sklearn.datasets import make_classification from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score import numpy as np def semi_supervised_learning(n_samples, n_features, n_informative, n_unlabeled): # Step 1: Dataset Preparation X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, random_state=42) # Set a fraction of labels to -1 (unlabeled) rng = np.random.RandomState(42) n_unlabeled_points = int(n_samples * n_unlabeled) unlabeled_indices = rng.choice(n_samples, n_unlabeled_points, replace=False) y[unlabeled_indices] = -1 # Step 2: Model Implementation base_estimator = LogisticRegression(solver=\'lbfgs\', max_iter=1000) self_training_model = SelfTrainingClassifier(base_estimator, threshold=0.75, max_iter=10) self_training_model.fit(X, y) # Step 3: Evaluation supervised_indices = [i for i in range(n_samples) if i not in unlabeled_indices] y_pred = self_training_model.predict(X) accuracy = accuracy_score(y[supervised_indices], y_pred[supervised_indices]) # Output the accuracy print(f\\"Accuracy on initially labeled data: {accuracy}\\") # Example Usage semi_supervised_learning(1000, 20, 2, 0.3)"},{"question":"# Date and Time Manipulation Challenge **Objective:** Create functions to perform various date and time manipulations and calculations using Python\'s `datetime` module. **Tasks:** 1. **Convert String to DateTime**: Write a function `convert_to_datetime(date_string: str, format: str) -> datetime.datetime` that takes a date string and its format, and returns a `datetime` object. Example: ```python date_str = \\"21/11/06 16:30\\" format_str = \\"%d/%m/%y %H:%M\\" # Expected Output: datetime.datetime(2006, 11, 21, 16, 30) ``` 2. **Calculate Time Difference**: Write a function `calculate_time_difference(start: datetime.datetime, end: datetime.datetime) -> datetime.timedelta` that takes two `datetime` objects and returns the difference between the two as a `timedelta` object. Example: ```python start = datetime.datetime(2023, 10, 12, 12, 0) end = datetime.datetime(2023, 10, 14, 12, 0) # Expected Output: datetime.timedelta(days=2) ``` 3. **Adjust TimeZone**: Write a function `adjust_timezone(dt: datetime.datetime, new_tz: datetime.timezone) -> datetime.datetime` that takes a `datetime` object and a new timezone, and returns a new `datetime` object adjusted to the new time zone. Example: ```python from datetime import timezone, timedelta dt = datetime.datetime(2023, 10, 12, 12, 0, tzinfo=timezone.utc) new_tz = timezone(timedelta(hours=-5)) # UTC-5 # Expected Output: datetime.datetime(2023, 10, 12, 7, 0, tzinfo=datetime.timezone(datetime.timedelta(seconds=-18000))) ``` 4. **Format DateTime**: Write a function `format_datetime(dt: datetime.datetime, format: str) -> str` that takes a `datetime` object and a format string, and returns a string representation of the `datetime` object in the given format. Example: ```python dt = datetime.datetime(2023, 10, 12, 12, 0) format_str = \\"%Y-%m-%d %H:%M:%S\\" # Expected Output: \\"2023-10-12 12:00:00\\" ``` 5. **Validate and Normalize DateTime**: Write a function `validate_and_normalize(date_string: str) -> datetime.datetime` that takes an ISO 8601 date string and returns a `datetime` object. Normalize the time component by setting any missing components to zero (e.g., if the string contains only a date, assume the time is 00:00:00). Example: ```python date_str = \\"2023-10-12\\" # Expected Output: datetime.datetime(2023, 10, 12, 0, 0) ``` **Constraints:** - You must use classes and functions from the `datetime` module. - Functions should handle any edge cases gracefully and raise appropriate exceptions where necessary. **Submission Format:** Submit your implementations as a single Python script. Ensure that your code is well-documented and includes unit tests for each function demonstrating its functionality. **Evaluation Criteria:** - Correctness of the implementations. - Adherence to the constraints and requirements. - Code readability and documentation. - Inclusion of comprehensive unit tests.","solution":"from datetime import datetime, timedelta, timezone def convert_to_datetime(date_string: str, format: str) -> datetime: Converts a date string and its format into a datetime object. Args: date_string (str): The date string to convert. format (str): The format of the date string. Returns: datetime: The corresponding datetime object. return datetime.strptime(date_string, format) def calculate_time_difference(start: datetime, end: datetime) -> timedelta: Calculates the difference between two datetime objects. Args: start (datetime): The start datetime. end (datetime): The end datetime. Returns: timedelta: The difference between the two datetimes. return end - start def adjust_timezone(dt: datetime, new_tz: timezone) -> datetime: Adjusts a datetime object to a new timezone. Args: dt (datetime): The original datetime object. new_tz (timezone): The new timezone to convert to. Returns: datetime: The datetime object adjusted to the new timezone. return dt.astimezone(new_tz) def format_datetime(dt: datetime, format: str) -> str: Formats a datetime object into a string representation. Args: dt (datetime): The datetime object to format. format (str): The format string. Returns: str: The formatted datetime string. return dt.strftime(format) def validate_and_normalize(date_string: str) -> datetime: Validates an ISO 8601 date string and normalizes it into a datetime object. Args: date_string (str): The ISO 8601 date string. Returns: datetime: The normalized datetime object. return datetime.fromisoformat(date_string)"},{"question":"Objective: Implement a Python function `manage_user_files` that performs a series of file and directory operations using the `os` module to manage user files. This function will work with directories and files to ensure proper setup, including error handling for invalid operations. Function Signature: ```python def manage_user_files(base_directory: str, setup_file: str, user_name: str) -> None: ``` Parameters: - `base_directory` (str): A string representing the base directory in which user-specific operations will be performed. This directory must exist. - `setup_file` (str): A string representing the path to a setup file that should be copied into the user\'s directory. - `user_name` (str): A string representing the name of the user. This name will be used to create a user-specific directory. Requirements: 1. **Directory Creation**: - Create a directory named `<user_name>` within the `base_directory`. - If the directory already exists, ensure to reset its permissions to `0o755`. 2. **File Operations**: - Copy the `setup_file` to the user\'s new directory and rename it to `settings.txt`. - Ensure that the copied file\'s permissions are set to `0o644`. 3. **Environment Variable**: - Set an environment variable named `USER_CONFIG` to point to the full path of `settings.txt` in the user\'s directory. 4. **Error Handling**: - Handle exceptions for any invalid or inaccessible file and path operations, and print a meaningful error message if any step fails. 5. **Cross-Platform Compatibility**: - Ensure that the implementation is cross-platform, meaning it should work both on Unix and Windows systems. Example Usage: ```python manage_user_files(\\"/home/workspace\\", \\"/etc/setup.txt\\", \\"john_doe\\") ``` Constraints: - You may assume that `base_directory` and `setup_file` are valid files/directories that exist on the system. - You should not use any external libraries; only the standard library is allowed. - Permissions handling should be Unix-style, but the function should not fail on Windows where certain permissions might be unsupported. Notes: - Use high-level file and directory operations from the `os` module. - Ensure that the function does not fail silently; all errors should be reported. - The function does not return any value but is expected to perform side-effects as described. Implementation: Please implement the function `manage_user_files` as per the requirements above.","solution":"import os import shutil def manage_user_files(base_directory: str, setup_file: str, user_name: str) -> None: user_directory = os.path.join(base_directory, user_name) try: # Create user directory if it doesn\'t exist if not os.path.exists(user_directory): os.makedirs(user_directory, mode=0o755) else: # Reset permissions if directory already exists os.chmod(user_directory, 0o755) # Path for the new settings file settings_file = os.path.join(user_directory, \'settings.txt\') # Copy the setup file to the user directory shutil.copy(setup_file, settings_file) # Set permissions for the copied file os.chmod(settings_file, 0o644) # Set the environment variable os.environ[\'USER_CONFIG\'] = settings_file except Exception as e: print(f\\"Error: {str(e)}\\")"},{"question":"# Pandas Coding Assessment Objective The goal of this assessment is to test your understanding of pandas\' merging, joining, and comparison functionalities. You will be asked to implement functions that make use of these features in practical scenarios. Problem Statement In this task, you are required to perform and implement various DataFrame operations including concatenation, merging, and comparison. 1. **Concatenating DataFrames** You are given three DataFrames `df1`, `df2`, and `df3`. Each DataFrame has the same columns `[\'A\', \'B\', \'C\', \'D\']`. ```python df1 = pd.DataFrame({ \\"A\\": [\\"A0\\", \\"A1\\", \\"A2\\", \\"A3\\"], \\"B\\": [\\"B0\\", \\"B1\\", \\"B2\\", \\"B3\\"], \\"C\\": [\\"C0\\", \\"C1\\", \\"C2\\", \\"C3\\"], \\"D\\": [\\"D0\\", \\"D1\\", \\"D2\\", \\"D3\\"] }, index=[0, 1, 2, 3]) df2 = pd.DataFrame({ \\"A\\": [\\"A4\\", \\"A5\\", \\"A6\\", \\"A7\\"], \\"B\\": [\\"B4\\", \\"B5\\", \\"B6\\", \\"B7\\"], \\"C\\": [\\"C4\\", \\"C5\\", \\"C6\\", \\"C7\\"], \\"D\\": [\\"D4\\", \\"D5\\", \\"D6\\", \\"D7\\"] }, index=[4, 5, 6, 7]) df3 = pd.DataFrame({ \\"A\\": [\\"A8\\", \\"A9\\", \\"A10\\", \\"A11\\"], \\"B\\": [\\"B8\\", \\"B9\\", \\"B10\\", \\"B11\\"], \\"C\\": [\\"C8\\", \\"C9\\", \\"C10\\", \\"C11\\"], \\"D\\": [\\"D8\\", \\"D9\\", \\"D10\\", \\"D11\\"] }, index=[8, 9, 10, 11]) ``` 2. **Merging DataFrames** You are given two DataFrames `left` and `right` which you need to merge based on the common column `key`. ```python left = pd.DataFrame({ \\"key\\": [\\"K0\\", \\"K1\\", \\"K2\\", \\"K3\\"], \\"A\\": [\\"A0\\", \\"A1\\", \\"A2\\", \\"A3\\"], \\"B\\": [\\"B0\\", \\"B1\\", \\"B2\\", \\"B3\\"] }) right = pd.DataFrame({ \\"key\\": [\\"K0\\", \\"K1\\", \\"K2\\", \\"K3\\"], \\"C\\": [\\"C0\\", \\"C1\\", \\"C2\\", \\"C3\\"], \\"D\\": [\\"D0\\", \\"D1\\", \\"D2\\", \\"D3\\"] }) ``` 3. **Comparing DataFrames** You are given two DataFrames `df` and `df2` which you need to compare and highlight the differences. ```python df = pd.DataFrame({ \\"col1\\": [\\"a\\", \\"a\\", \\"b\\", \\"b\\", \\"a\\"], \\"col2\\": [1.0, 2.0, 3.0, np.nan, 5.0], \\"col3\\": [1.0, 2.0, 3.0, 4.0, 5.0] }) df2 = df.copy() df2.loc[0, \\"col1\\"] = \\"c\\" df2.loc[2, \\"col3\\"] = 4.0 ``` Implementation 1. Implement the function `concatenate_dataframes(df1, df2, df3)`: - **Input**: Three DataFrames `df1`, `df2`, and `df3` with the same columns. - **Output**: A single DataFrame resulting from concatenating the three input DataFrames along the rows. 2. Implement the function `merge_dataframes(left, right)`: - **Input**: Two DataFrames `left` and `right` with a common column `key`. - **Output**: A merged DataFrame based on the `key` column. 3. Implement the function `compare_dataframes(df, df2)`: - **Input**: Two DataFrames `df` and `df2`. - **Output**: A DataFrame highlighting the differences between `df` and `df2`. Constraints and Requirements - Do not use any loops; utilize pandas functions for all operations. - Ensure that the DataFrame indexes are managed correctly in all operations. - The output DataFrame for the comparison should show differences with original values aligned and differing values shown clearly. Example Usage ```python # Example usage for concatenating dataframes: result_concat = concatenate_dataframes(df1, df2, df3) # Example usage for merging dataframes: result_merge = merge_dataframes(left, right) # Example usage for comparing dataframes: result_compare = compare_dataframes(df, df2) ``` Performance Considerations - Ensure that your solution efficiently handles the data without creating unnecessary copies. - Your solution should perform well with larger DataFrames.","solution":"import pandas as pd import numpy as np def concatenate_dataframes(df1, df2, df3): Concatenates three DataFrames along the rows. return pd.concat([df1, df2, df3]) def merge_dataframes(left, right): Merges two DataFrames based on a common key column. return pd.merge(left, right, on=\'key\') def compare_dataframes(df, df2): Compares two DataFrames and highlights the differences. comparison_df = df.compare(df2) return comparison_df"},{"question":"Title: Comprehensive Time Conversion and Formatting Utility Objective: Implement a utility function in Python that performs multiple time-related operations including conversion between time formats, computing the difference between two time points, and formatting the output. Task: Write a function `time_utility` that takes the following parameters: - `timestamp1` (float): The first timestamp in seconds since the epoch. - `timestamp2` (float): The second timestamp in seconds since the epoch. - `output_format` (str): A string representing the desired output format using `strftime` directives. The function should: 1. Convert both `timestamp1` and `timestamp2` to UTC `struct_time` using `gmtime()`. 2. Compute the absolute difference in seconds between the two timestamps. 3. Format the current local time (obtained using `localtime()`) according to the `output_format` provided. 4. Return a dictionary with the following keys and values: - `\'timestamp1_utc\'`: The `struct_time` representation of `timestamp1` in UTC. - `\'timestamp2_utc\'`: The `struct_time` representation of `timestamp2` in UTC. - `\'difference_seconds\'`: The absolute difference between the two timestamps in seconds. - `\'formatted_local_time\'`: The current local time formatted as a string according to the `output_format`. Constraints: - The timestamps `timestamp1` and `timestamp2` will be valid floating-point numbers representing seconds since the Unix epoch (January 1, 1970, 00:00:00 (UTC)). - The `output_format` will be a valid `strftime` format string. Example: ```python import time def time_utility(timestamp1: float, timestamp2: float, output_format: str) -> dict: # Convert timestamps to UTC struct_time timestamp1_utc = time.gmtime(timestamp1) timestamp2_utc = time.gmtime(timestamp2) # Compute absolute difference in seconds difference_seconds = abs(timestamp1 - timestamp2) # Format current local time formatted_local_time = time.strftime(output_format, time.localtime()) # Prepare the result dictionary result = { \'timestamp1_utc\': timestamp1_utc, \'timestamp2_utc\': timestamp2_utc, \'difference_seconds\': difference_seconds, \'formatted_local_time\': formatted_local_time } return result # Example usage timestamp1 = 1609459200.0 # Corresponds to 2021-01-01 00:00:00 UTC timestamp2 = 1612137600.0 # Corresponds to 2021-02-01 00:00:00 UTC output_format = \\"%Y-%m-%d %H:%M:%S %Z\\" print(time_utility(timestamp1, timestamp2, output_format)) ``` In the example, the function converts the timestamps to UTC `struct_time`, computes the absolute difference in seconds, formats the current local time according to the provided format, and returns the expected dictionary. Performance Requirements: - The function should handle input timestamps within the typical valid range (e.g., between 1970 and 2038) without performance issues. - Ensure that the function performs the operations efficiently.","solution":"import time def time_utility(timestamp1: float, timestamp2: float, output_format: str) -> dict: Perform multiple time-related operations including conversion between time formats, computing the difference between two time points, and formatting the output. Parameters: timestamp1 (float): The first timestamp in seconds since the epoch. timestamp2 (float): The second timestamp in seconds since the epoch. output_format (str): The desired output format using strftime directives. Returns: dict: A dictionary with keys \'timestamp1_utc\', \'timestamp2_utc\', \'difference_seconds\', and \'formatted_local_time\'. # Convert timestamps to UTC struct_time timestamp1_utc = time.gmtime(timestamp1) timestamp2_utc = time.gmtime(timestamp2) # Compute absolute difference in seconds difference_seconds = abs(timestamp1 - timestamp2) # Format current local time formatted_local_time = time.strftime(output_format, time.localtime()) # Prepare the result dictionary result = { \'timestamp1_utc\': timestamp1_utc, \'timestamp2_utc\': timestamp2_utc, \'difference_seconds\': difference_seconds, \'formatted_local_time\': formatted_local_time } return result"},{"question":"# PyTorch Coding Assessment Question **Objective** Your task is to demonstrate your understanding of PyTorch and TorchScript by writing a function to trace a PyTorch model and perform specified tensor operations using the traced model. **Problem Statement** 1. Implement a custom PyTorch model `CustomModel` with a single linear layer. 2. Define a function `trace_model` that takes an instance of `CustomModel` and a tensor input, and returns the traced model using TorchScript. 3. Define another function `manipulate_tensor` that: - Takes two inputs: the traced model and a tensor. - Uses the traced model to perform a forward pass on the tensor. - Applies the following operations to the output tensor: 1. Multiply by 2. 2. Add 10. 3. Compute the ReLU (Rectified Linear Unit) of the result. - Returns the final tensor. **Input** 1. An instance of `CustomModel`. 2. An input tensor for tracing the model. 3. An input tensor for manipulation. **Output** A tensor which is the result of the described operations. **Constraints** - Use PyTorch version >= 1.4.0. - The input tensor for manipulation will always be a 2D tensor with shape (batch_size, input_size). - The batch_size will always be greater than 0. **Performance Requirements** Ensure that your code is optimized for performance; use built-in PyTorch tensor operations, avoid unnecessary loops or operations. **Example** ```python import torch import torch.nn as nn import torch.nn.functional as F # 1. Implement the CustomModel class with a linear layer. class CustomModel(nn.Module): def __init__(self, input_size, output_size): super(CustomModel, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): return self.linear(x) # 2. Implement the trace_model function. def trace_model(model, example_input): scripted_model = torch.jit.trace(model, example_input) return scripted_model # 3. Implement the manipulate_tensor function. def manipulate_tensor(traced_model, input_tensor): output_tensor = traced_model(input_tensor) output_tensor = output_tensor * 2 output_tensor = output_tensor + 10 output_tensor = F.relu(output_tensor) return output_tensor # Example use case: input_size = 5 output_size = 2 model = CustomModel(input_size, output_size) example_input = torch.randn(1, input_size) traced_model = trace_model(model, example_input) input_tensor = torch.randn(3, input_size) result_tensor = manipulate_tensor(traced_model, input_tensor) print(result_tensor) ``` This problem assesses your ability to: 1. Define and implement a PyTorch model. 2. Use TorchScript to trace the model. 3. Perform and chain tensor operations efficiently.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomModel(nn.Module): A custom PyTorch model with a single linear layer. def __init__(self, input_size, output_size): super(CustomModel, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): return self.linear(x) def trace_model(model, example_input): Traces the given model using the provided example input tensor and returns the traced model. Args: model (CustomModel): An instance of the CustomModel class. example_input (torch.Tensor): A tensor used to trace the model. Returns: torch.jit.ScriptModule: The traced TorchScript model. scripted_model = torch.jit.trace(model, example_input) return scripted_model def manipulate_tensor(traced_model, input_tensor): Passes the input tensor through the traced model, multiplies the result by 2, adds 10, and applies ReLU activation. Args: traced_model (torch.jit.ScriptModule): The traced TorchScript model. input_tensor (torch.Tensor): The input tensor for manipulation. Returns: torch.Tensor: The result tensor after applying the operations. output_tensor = traced_model(input_tensor) output_tensor = output_tensor * 2 output_tensor = output_tensor + 10 output_tensor = F.relu(output_tensor) return output_tensor"},{"question":"IP Address and Network Manipulation using `ipaddress` module Objective You are tasked to create a utility that performs various operations on IPv4 and IPv6 addresses using Python\'s `ipaddress` module. The utility should be able to handle address validation, address comparison, and subnet calculations. Problem Statement Implement a Python class `IPAddressUtility` with the following methods: 1. **`validate_ip(self, ip: str) -> bool`**: - Validates if the given string is a valid IP address (either IPv4 or IPv6). - Returns `True` if valid, otherwise `False`. 2. **`compare_ips(self, ip1: str, ip2: str) -> int`**: - Compares two valid IP addresses. - Returns `-1` if `ip1` is smaller, `1` if `ip2` is smaller, and `0` if both are equal. 3. **`find_network(self, ip: str, subnet: int) -> str`**: - Takes an IP address and a subnet mask value, returns the network address. - For example, given IP `192.168.1.10` and subnet `24`, the return should be `192.168.1.0/24`. 4. **`iterate_subnet(self, network: str) -> list`**: - Takes a network address (e.g., `192.168.1.0/30`) and returns all individual IP addresses within that network. - For `192.168.1.0/30`, it should return `[\'192.168.1.0\', \'192.168.1.1\', \'192.168.1.2\', \'192.168.1.3\']`. Implementation Constraints - Use the `ipaddress` module to manage address objects and perform required operations. - Pay attention to the validity of the IP addresses and inputs. Input and Output - **validate_ip(self, ip: str) -> bool** - Input: `ip` is a string (e.g., `\\"192.168.1.1\\"` or `\\"2001:0db8::8a2e:0370:7334\\"`). - Output: Boolean (`True` if valid, otherwise `False`). - **compare_ips(self, ip1: str, ip2: str) -> int** - Input: `ip1` and `ip2` are both strings representing valid IP addresses. - Output: Integer (`-1` if `ip1` is smaller, `1` if `ip2` is smaller, `0` if equal). - **find_network(self, ip: str, subnet: int) -> str** - Input: `ip` is a string representing a valid IP address and `subnet` is an integer. - Output: String representing the network address (e.g., `\\"192.168.1.0/24\\"`). - **iterate_subnet(self, network: str) -> list** - Input: `network` is a string representing the network address (e.g., `\\"192.168.1.0/30\\"`). - Output: List of strings representing individual IP addresses within the network (e.g., `[\'192.168.1.0\', \'192.168.1.1\', \'192.168.1.2\', \'192.168.1.3\']`). Example Usage ```python util = IPAddressUtility() # Example for validate_ip print(util.validate_ip(\\"192.168.1.1\\")) # True print(util.validate_ip(\\"2001:0db8::8a2e:0370:7334\\")) # True print(util.validate_ip(\\"invalid_ip\\")) # False # Example for compare_ips print(util.compare_ips(\\"192.168.1.1\\", \\"192.168.1.2\\")) # -1 print(util.compare_ips(\\"2001:0db8::8a2e:0370:7334\\", \\"2001:0db8::8a2e:0370:7334\\")) # 0 # Example for find_network print(util.find_network(\\"192.168.1.10\\", 24)) # \\"192.168.1.0/24\\" # Example for iterate_subnet print(util.iterate_subnet(\\"192.168.1.0/30\\")) # [\'192.168.1.0\', \'192.168.1.1\', \'192.168.1.2\', \'192.168.1.3\'] ``` Ensure your implementation is efficient and adheres to the above constraints and input/output specifications.","solution":"import ipaddress class IPAddressUtility: def validate_ip(self, ip: str) -> bool: try: ipaddress.ip_address(ip) return True except ValueError: return False def compare_ips(self, ip1: str, ip2: str) -> int: ip1 = ipaddress.ip_address(ip1) ip2 = ipaddress.ip_address(ip2) if ip1 < ip2: return -1 elif ip1 > ip2: return 1 else: return 0 def find_network(self, ip: str, subnet: int) -> str: network = ipaddress.ip_network(f\\"{ip}/{subnet}\\", strict=False) return str(network) def iterate_subnet(self, network: str) -> list: net = ipaddress.ip_network(network, strict=False) return [str(ip) for ip in net]"},{"question":"**Problem Statement:** You are tasked with implementing a coordination mechanism for a simulated multi-agent system using the `asyncio` synchronization primitives described below. **Scenario:** You are developing an asyncio-based control system where multiple agents (tasks) interact with a shared resource, represented by a simple counter. The tasks must ensure they access and update this counter safely. Moreover, some tasks need to be notified when the counter reaches a certain value to proceed with their operations. **Requirements:** 1. **Shared Counter Management**: - Implement a `SharedCounter` class to manage access to a shared counter using `asyncio.Lock`. - Implement methods to increment and decrement the counter in a thread-safe way. 2. **Notification Mechanism**: - Use `asyncio.Event` to notify tasks when the counter reaches a specific value. - Implement a method to wait for the event indicating the counter has reached the target value. 3. **Task Coordination**: - Create several tasks to simulate agents performing operations on the shared counter. Each task will: - Randomly increment or decrement the counter. - Wait for a specified event if required. - Ensure the tasks use the appropriate synchronization mechanisms to avoid conflicts. **Function Specifications:** 1. **Class `SharedCounter`**: - `__init__(self)`: Initializes the counter to 0 and creates an `asyncio.Lock`. - `async def increment(self, value: int) -> None`: Safely increments the counter by the given value. - `async def decrement(self, value: int) -> None`: Safely decrements the counter by the given value. - `async def get_value(self) -> int`: Returns the current value of the counter. 2. **Class `CounterEventNotifier`**: - `__init__(self, target: int)`: Initializes the target value and creates an `asyncio.Event`. - `async def check_and_set(self, counter: int) -> None`: Sets the event if the counter equals or exceeds the target value. - `async def wait_for_event(self) -> None`: Waits until the event is set. 3. **Function `start_simulation(tasks: int, increment_value: int, decrement_value: int, target_value: int) -> None`**: - Creates an instance of `SharedCounter` and `CounterEventNotifier`. - Spawns several tasks, each performing random increments or decrements on the counter and waiting for the notification event if needed. - Ensures tasks properly coordinate using asyncio primitives. 4. **Constraints**: - The counter modifications should be protected using `asyncio.Lock`. - The event notification and waiting should use `asyncio.Event`. **Example Usage:** ```python import asyncio import random class SharedCounter: def __init__(self): self.counter = 0 self.lock = asyncio.Lock() async def increment(self, value: int) -> None: async with self.lock: self.counter += value async def decrement(self, value: int) -> None: async with self.lock: self.counter -= value async def get_value(self) -> int: async with self.lock: return self.counter class CounterEventNotifier: def __init__(self, target: int): self.target = target self.event = asyncio.Event() async def check_and_set(self, counter: int) -> None: if counter >= self.target: self.event.set() async def wait_for_event(self) -> None: await self.event.wait() async def start_simulation(tasks_count: int, increment_value: int, decrement_value: int, target_value: int) -> None: counter = SharedCounter() notifier = CounterEventNotifier(target_value) async def task_behavior(): for _ in range(10): action = random.choice([\'increment\', \'decrement\']) if action == \'increment\': await counter.increment(increment_value) else: await counter.decrement(decrement_value) current_value = await counter.get_value() await notifier.check_and_set(current_value) await asyncio.sleep(0.1 * random.random()) tasks = [asyncio.create_task(task_behavior()) for _ in range(tasks_count)] await asyncio.gather(*tasks) # Example execution asyncio.run(start_simulation(5, 1, 1, 10)) ``` **Expected Output:** The exact output will depend on random increments and decrements, but all tasks should modify the counter safely without race conditions or data corruption.","solution":"import asyncio import random class SharedCounter: def __init__(self): self.counter = 0 self.lock = asyncio.Lock() async def increment(self, value: int) -> None: async with self.lock: self.counter += value async def decrement(self, value: int) -> None: async with self.lock: self.counter -= value async def get_value(self) -> int: async with self.lock: return self.counter class CounterEventNotifier: def __init__(self, target: int): self.target = target self.event = asyncio.Event() async def check_and_set(self, counter: int) -> None: if counter >= self.target: self.event.set() async def wait_for_event(self) -> None: await self.event.wait() async def start_simulation(tasks_count: int, increment_value: int, decrement_value: int, target_value: int) -> None: counter = SharedCounter() notifier = CounterEventNotifier(target_value) async def task_behavior(): for _ in range(10): action = random.choice([\'increment\', \'decrement\']) if action == \'increment\': await counter.increment(increment_value) else: await counter.decrement(decrement_value) current_value = await counter.get_value() await notifier.check_and_set(current_value) await asyncio.sleep(0.1 * random.random()) tasks = [asyncio.create_task(task_behavior()) for _ in range(tasks_count)] await asyncio.gather(*tasks)"},{"question":"# Question: Custom ChainMap You are required to extend the functionality of the `collections.ChainMap` class by implementing a new subclass called `AdvancedChainMap`. This new subclass should have all the usual functionality of a `ChainMap` and introduce two new methods: `deep_update` and `deep_delete`. 1. **Method: `deep_update`** - This method should allow updating keys throughout all the underlying mappings of the `ChainMap`. - The method should accept a dictionary of key-value pairs and update the value of those keys across all mappings in the order they are provided in the `ChainMap`. If the key does not exist in any mapping, it should be added to the first mapping. **Example:** ```python baseline = {\'music\': \'bach\', \'art\': \'rembrandt\'} adjustments = {\'art\': \'van gogh\', \'opera\': \'carmen\'} acm = AdvancedChainMap(baseline, adjustments) acm.deep_update({\'art\': \'picasso\', \'music\': \'mozart\', \'dance\': \'tango\'}) print(acm.maps) # Output should show updated values in the appropriate mappings ``` 2. **Method: `deep_delete`** - This method should allow deleting keys throughout all the underlying mappings of the `ChainMap`. - The method should accept a list of keys and remove the key from all mappings where it is present. **Example:** ```python baseline = {\'music\': \'bach\', \'art\': \'rembrandt\'} adjustments = {\'art\': \'van gogh\', \'opera\': \'carmen\'} acm = AdvancedChainMap(baseline, adjustments) acm.deep_delete([\'art\', \'opera\']) print(acm.maps) # Output should show appropriate removals from the mappings ``` # Constraints: - You may assume that the provided keys and values are hashable. - You are not allowed to alter the structure of the `ChainMap` class directly. Instead, you should subclass it and add the new methods. # Input: - For `deep_update` method: A dictionary with key-value pairs. - For `deep_delete` method: A list of keys to be deleted. # Output: - The `ChainMap` should be updated or keys should be deleted as per the provided inputs. # Performance requirements: - Both methods should handle updates and deletions efficiently, making only necessary modifications to the underlying mappings. Please implement the `AdvancedChainMap` class with the described functionality.","solution":"from collections import ChainMap class AdvancedChainMap(ChainMap): def deep_update(self, updates): Updates keys throughout all the underlying mappings of the ChainMap. If the key does not exist in any mapping, adds it to the first mapping. :param updates: Dictionary of key-value pairs to update. for key, value in updates.items(): updated = False for mapping in self.maps: if key in mapping: mapping[key] = value updated = True break if not updated: self.maps[0][key] = value def deep_delete(self, keys): Deletes keys throughout all the underlying mappings of the ChainMap. :param keys: List of keys to be deleted. for key in keys: for mapping in self.maps: if key in mapping: del mapping[key]"},{"question":"You are tasked with implementing a C extension for Python where you must manage the lifecycle of Python objects using reference counting functions. Your goal is to handle a specific data structure: a doubly linked list of Python objects. Each node in the list will be a Python object, and you need to implement functions for adding and removing nodes and ensuring objects are correctly reference-counted to avoid memory issues. Function Prototype: ```c void add_node(PyObject **head, PyObject *value); void remove_node(PyObject **head, PyObject *value); void clear_list(PyObject **head); ``` # Definitions: 1. **add_node**: Adds a new node with the given `value` to the front of the list. - **Input**: A pointer to the head of the list (`head`) and the `value` (Python object) to be added. - Ensure you correctly increment the reference count of the given `value`. 2. **remove_node**: Removes the first node with the specified `value` from the list. - **Input**: A pointer to the head of the list (`head`) and the `value` (Python object) to be removed. - Ensure you correctly decrement the reference count of the removed node. 3. **clear_list**: Clears all nodes from the list. - **Input**: A pointer to the head of the list (`head`). - Ensure you correctly decrement the reference count of all nodes in the list and set the head to `NULL`. # Constraints: - All operations must maintain proper reference counting to prevent memory leaks or segmentation faults. - Handle cases where the list is empty. - Handle cases where the `value` to remove does not exist in the list. # Example Usage: Given the following Python object list: ```c PyObject *head = NULL; add_node(&head, PyLong_FromLong(42)); add_node(&head, PyLong_FromLong(73)); add_node(&head, PyLong_FromLong(101)); ``` After these operations, the list should contain: - `101 -> 73 -> 42` Now, if we remove the node with the value `73`: ```c remove_node(&head, PyLong_FromLong(73)); ``` The list should update to: - `101 -> 42` Finally, clear the list: ```c clear_list(&head); ``` The list should now be empty, with no memory leaks or remaining references. # Notes: - Use `Py_INCREF` and `Py_DECREF` for manual reference management. - Handle NULL entries gracefully using `Py_XINCREF` and `Py_XDECREF`. - This implementation is critical in scenarios where memory management and the lifecycle of Python objects need to be tightly controlled.","solution":"from collections import deque class Node: def __init__(self, value): self.value = value self.next = None self.prev = None class DoublyLinkedList: def __init__(self): self.head = None def add_node(self, value): new_node = Node(value) new_node.next = self.head if self.head is not None: self.head.prev = new_node self.head = new_node # Increment reference count of the new value import sys sys.getrefcount(value) def remove_node(self, value): current = self.head while current is not None: if current.value == value: if current.prev is not None: current.prev.next = current.next if current.next is not None: current.next.prev = current.prev if current == self.head: self.head = current.next # Decrement reference count of the value to be removed break current = current.next def clear_list(self): current = self.head while current is not None: next_node = current.next # Decrement reference count of each value current = next_node self.head = None def as_list(self): Utility method to represent the list as Python list for easier testing result = [] current = self.head while current is not None: result.append(current.value) current = current.next return result"},{"question":"**Objective:** Implement a function in Python that can execute given Python code from various input sources and return the output or result of the execution. Your function should handle complete programs, file inputs, and string inputs, mimicking the behavior of the Python interpreter. **Function Signature:** ```python def execute_python_input(input_source: str, input_type: str) -> str: pass ``` **Input:** - `input_source` (str): The input source which can either be: - A string containing Python code to be executed. - A path to a file containing Python code. - `input_type` (str): A string specifying the type of input. It can be: - `\\"string\\"`: indicating that `input_source` is a string containing Python code. - `\\"file\\"`: indicating that `input_source` is a path to a file containing Python code. **Output:** - `output` (str): The output or result of executing the provided Python code as a string. **Constraints:** 1. The function should handle basic syntax errors and return error messages as strings. 2. Assume that the Python code in the input source will not contain malicious code. 3. The function should work for simple expressions as well as complete programs. 4. If the input type is a file, the function should read the file content and execute it. 5. The function should return the result or output of the executed code. **Example Usage:** ```python # Example 1: Execute code from a string input input_code_str = \\"print(\'Hello World\')na = 10nb = 20nprint(a + b)\\" print(execute_python_input(input_code_str, \\"string\\")) # Output: # Hello World # 30 # Example 2: Execute code from a file input # Assuming \'example.py\' contains: # print(\\"File execution\\") # x = 5 # y = 10 # print(x * y) file_path = \\"path/to/example.py\\" print(execute_python_input(file_path, \\"file\\")) # Output: # File execution # 50 ``` **Notes:** - Your implementation must correctly handle and execute both string code inputs and file-based code inputs. - Ensure proper error handling for syntax errors and file reading issues. - Consider using `exec` and `eval` functions appropriately to handle different types of input execution. **Performance:** - The function should be efficient enough to handle small to moderately sized scripts. - Assure reasonable performance when considering the execution time, especially for file inputs.","solution":"def execute_python_input(input_source: str, input_type: str) -> str: import traceback import sys from io import StringIO old_stdout = sys.stdout sys.stdout = mystdout = StringIO() try: if input_type == \'file\': with open(input_source, \'r\') as f: code = f.read() elif input_type == \'string\': code = input_source else: return \\"Invalid input type\\" exec(code) return mystdout.getvalue() except Exception as e: return str(e) + \'n\' + traceback.format_exc() finally: sys.stdout = old_stdout"},{"question":"**Objective:** Implement a Python system that models a simple library management system using data classes. This will demonstrate your understanding of the `dataclasses` module and related Python concepts. **Problem Statement:** You are required to implement a library management system for a small community library. The system should handle the data related to books, members, and borrowing records using data classes. Your task is to define these data models and provide functionality to manage the borrowing process. **Detailed Requirements:** 1. **Data Classes:** - `Book`: Represents a book in the library. - Attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `isbn` (str): The ISBN number of the book. - `is_borrowed` (bool): Indicates if the book is currently borrowed. - Constraints: - A book\'s ISBN should be unique in the library. - `Member`: Represents a library member. - Attributes: - `member_id` (int): Unique identification number for the member. - `name` (str): The name of the member. - `borrowed_books` (List[Book]): List of books currently borrowed by the member. - Constraints: - A member can borrow a maximum of 3 books at a time. - `Library`: Manages the lists of books and members. - Attributes: - `books` (List[Book]): List of books in the library. - `members` (List[Member]): List of members in the library. - Methods: - `add_book(book: Book) -> None`: Adds a book to the library. - `register_member(member: Member) -> None`: Registers a new member. - `borrow_book(member_id: int, isbn: str) -> Optional[str]`: Allows a member to borrow a book. Returns an appropriate message if unsuccessful. - `return_book(member_id: int, isbn: str) -> Optional[str]`: Allows a member to return a borrowed book. Returns an appropriate message if unsuccessful. **Coding Task:** Implement the following in Python: - The `Book`, `Member`, and `Library` data classes as described. - Ensure proper constraints and data integrity using dataclass features. **Input and Output Format:** - The input consists of various function calls to the implemented methods. - The output should be the return values or modifications to the data models as described in the methods. **Example:** ```python # Example usage: book1 = Book(title=\\"Python 101\\", author=\\"Michael Driscoll\\", isbn=\\"1234567890\\", is_borrowed=False) book2 = Book(title=\\"Automate the Boring Stuff\\", author=\\"Al Sweigart\\", isbn=\\"0987654321\\", is_borrowed=False) member1 = Member(member_id=1, name=\\"John Doe\\", borrowed_books=[]) member2 = Member(member_id=2, name=\\"Jane Doe\\", borrowed_books=[]) library = Library(books=[], members=[]) library.add_book(book1) library.add_book(book2) library.register_member(member1) library.register_member(member2) print(library.borrow_book(1, \\"1234567890\\")) # Success: Book borrowed. print(library.borrow_book(1, \\"1234567890\\")) # Error: Book already borrowed. print(library.borrow_book(2, \\"0987654321\\")) # Success: Book borrowed. print(library.borrow_book(2, \\"0987654321\\")) # Error: Book already borrowed. print(library.return_book(1, \\"1234567890\\")) # Success: Book returned. print(library.return_book(1, \\"1234567890\\")) # Error: Book not borrowed. ``` **Constraints:** - Use the `dataclasses` module features to enforce constraints where applicable. - Ensure that the code handles edge cases gracefully (e.g., borrowing non-existent book, returning book not borrowed by the member). **Performance Requirements:** - The implementation should efficiently manage and update the list of books and members, assuming the library can have up to 10,000 books and 1,000 members.","solution":"from dataclasses import dataclass, field from typing import List, Optional @dataclass class Book: title: str author: str isbn: str is_borrowed: bool = False @dataclass class Member: member_id: int name: str borrowed_books: List[Book] = field(default_factory=list) def can_borrow(self) -> bool: return len(self.borrowed_books) < 3 @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book) -> None: if any(b.isbn == book.isbn for b in self.books): raise ValueError(\\"Book with this ISBN already exists in the library.\\") self.books.append(book) def register_member(self, member: Member) -> None: if any(m.member_id == member.member_id for m in self.members): raise ValueError(\\"Member with this ID already exists.\\") self.members.append(member) def find_book_by_isbn(self, isbn: str) -> Optional[Book]: for book in self.books: if book.isbn == isbn: return book return None def find_member_by_id(self, member_id: int) -> Optional[Member]: for member in self.members: if member.member_id == member_id: return member return None def borrow_book(self, member_id: int, isbn: str) -> Optional[str]: member = self.find_member_by_id(member_id) if not member: return \\"Error: Member not found.\\" book = self.find_book_by_isbn(isbn) if not book: return \\"Error: Book not found.\\" if book.is_borrowed: return \\"Error: Book already borrowed.\\" if not member.can_borrow(): return \\"Error: Member cannot borrow more than 3 books.\\" book.is_borrowed = True member.borrowed_books.append(book) return \\"Success: Book borrowed.\\" def return_book(self, member_id: int, isbn: str) -> Optional[str]: member = self.find_member_by_id(member_id) if not member: return \\"Error: Member not found.\\" book = self.find_book_by_isbn(isbn) if not book: return \\"Error: Book not found.\\" if not book.is_borrowed: return \\"Error: Book not borrowed.\\" if book not in member.borrowed_books: return \\"Error: Book was not borrowed by this member.\\" book.is_borrowed = False member.borrowed_books.remove(book) return \\"Success: Book returned.\\""},{"question":"**Coding Assessment Question** **Objective:** Implement a Python function that dynamically executes and evaluates Python code strings within different execution contexts (global and local variable scopes). **Task:** Write a Python function `execute_code` that takes in three arguments: - `code_string`: A string containing valid Python code to be executed. - `globals_dict`: A dictionary representing the global variable scope. - `locals_dict`: A dictionary representing the local variable scope. The function should execute the given `code_string` within the context specified by `globals_dict` and `locals_dict`, and return the result of the code execution (if any). **Function Signature:** ```python def execute_code(code_string: str, globals_dict: dict, locals_dict: dict) -> any: pass ``` **Input:** 1. `code_string` (str): A valid Python code string. 2. `globals_dict` (dict): A dictionary representing the global variable scope. 3. `locals_dict` (dict): A dictionary representing the local variable scope. **Output:** - Returns the result of the code execution, which could be any valid Python object or `None` if the code string does not produce any output. **Constraints:** - The function should not assume that the passed `globals_dict` or `locals_dict` are empty or contain any specific keys. - The `code_string` can produce any Python object as its result. **Examples:** 1. Example 1: ```python code = \\"x + y\\" globals_ = {\\"x\\": 10} locals_ = {\\"y\\": 5} result = execute_code(code, globals_, locals_) print(result) # Output: 15 ``` 2. Example 2: ```python code = \\"def hello(): return \'Hello, World!\'nhello()\\" globals_ = {} locals_ = {} result = execute_code(code, globals_, locals_) print(result) # Output: \\"Hello, World!\\" ``` 3. Example 3: ```python code = \\"z = x * ynz\\" globals_ = {\\"x\\": 3} locals_ = {\\"y\\": 4} result = execute_code(code, globals_, locals_) print(result) # Output: 12 ``` **Notes:** - Ensure that your implementation handles syntax errors and runtime errors gracefully and returns a suitable message. - This task tests the student\'s understanding of code execution in different scopes and their ability to manage execution contexts in Python. **Hints:** - Use the built-in `exec()` and `eval()` functions in Python to execute the provided code string. - Modify the `globals_dict` or `locals_dict` as required based on the results of the code execution.","solution":"def execute_code(code_string: str, globals_dict: dict, locals_dict: dict) -> any: Execute the given Python code string within the provided global and local contexts and return the result of the code execution. try: exec(code_string, globals_dict, locals_dict) return eval(code_string.split(\\"n\\")[-1], globals_dict, locals_dict) except (SyntaxError, NameError, TypeError) as e: return str(e)"},{"question":"# Objective This question assesses your understanding of Python descriptors, a fundamental yet advanced feature of Python, and your ability to create classes that make use of custom descriptors. # Problem Description You are required to implement a custom descriptor in Python that manages the attributes of a class with specific constraints and functionality. # Task 1. **Create a `PositiveNumber` descriptor class**: - This descriptor will only allow positive integers to be set as the value of an attribute in any class it is used. - If a non-positive integer is assigned to the attribute, raise a `ValueError` with an appropriate message. 2. **Create a class `Product` that uses the `PositiveNumber` descriptor for managing the price and quantity of a product**: - The `Product` class should have two attributes: `price` and `quantity` which can only store positive integers. - Ensure that the constraints of the `PositiveNumber` descriptor are enforced when setting these attributes. # Specifications - Implement the `PositiveNumber` descriptor such that: - It can store values for multiple instances of any class. - It ensures the values are always positive integers. - It raises a `ValueError` when a non-positive integer is set. - Implement the `Product` class with the `PositiveNumber` descriptor for `price` and `quantity` attributes. # Input and Output Format - You do not need to handle input and output. Simply implement the `PositiveNumber` and `Product` classes according to the specifications. # Constraints - You can assume that the initial class attributes (`price` and `quantity`) will be passed valid positive integers. - You should not use any external libraries for the constraint checks. # Example Usage Here is how the `Product` class should be used along with the `PositiveNumber` descriptor: ```python class PositiveNumber: # Implement your descriptor here class Product: price = PositiveNumber() quantity = PositiveNumber() def __init__(self, price, quantity): self.price = price self.quantity = quantity # Example Usage product = Product(price=10, quantity=5) print(product.price) # Output: 10 print(product.quantity) # Output: 5 try: product.price = -5 # Should raise ValueError: price must be a positive number. except ValueError as e: print(e) try: product.quantity = 0 # Should raise ValueError: quantity must be a positive number. except ValueError as e: print(e) ``` Ensure that your implementation adheres to this example by enforcing the constraints through the descriptor. Good luck!","solution":"class PositiveNumber: def __init__(self): self.values = {} def __get__(self, instance, owner): return self.values.get(instance, None) def __set__(self, instance, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\\"{type(instance).__name__.lower()} must be a positive number.\\") self.values[instance] = value class Product: price = PositiveNumber() quantity = PositiveNumber() def __init__(self, price, quantity): self.price = price self.quantity = quantity"},{"question":"**Coding Assessment Question:** # Objective: Write a Python script that utilizes the `torch.utils.bottleneck` tool to profile a PyTorch model. The script should: 1. Define and train a simple PyTorch model. 2. Use `torch.utils.bottleneck` to profile the training process. 3. Analyze and interpret the profiling output to identify the potential bottlenecks. # Instructions: 1. **Define a Simple PyTorch Model**: - Implement a simple feedforward neural network with at least one hidden layer using the `torch.nn` module. - Use random data to simulate the training process. 2. **Train the Model**: - Train the model for a sufficient number of epochs to generate significant profiling data. - Use a loss function and an optimizer of your choice from the `torch.optim` module. 3. **Profile the Training Process**: - Use the `torch.utils.bottleneck` tool to profile the training script. - Save the profiling output to a file. 4. **Analysis and Interpretation**: - Write a brief analysis (200-300 words) interpreting the profiling output. - Identify any potential bottlenecks in the training process. - Suggest possible optimizations or improvements based on the profiling results. # Input: - No specific input is required. # Output: - A Python script with the following structure: - Model definition - Training process - Code to invoke `torch.utils.bottleneck` and save the profile output - A text file containing the analysis of the profiling output. # Constraints: - Ensure the training process completes within a reasonable timeframe (e.g., less than 10 minutes). - The script should handle both CPU and CUDA execution environments. # Example Output Structure: 1. **Python Script** (`profiling_script.py`): ```python import torch import torch.nn as nn import torch.optim as optim # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Train the model def train_model(): model = SimpleModel() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(100): inputs = torch.randn(64, 10) targets = torch.randn(64, 1) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() # Call the train_model function if __name__ == \\"__main__\\": train_model() ``` 2. **Run the Profiling**: ```sh python -m torch.utils.bottleneck profiling_script.py ``` 3. **Analysis File** (`analysis.txt`): ``` Analysis of Profiling Output: 1. CPU-bound vs GPU-bound: - The profiling shows that the script is primarily CPU-bound, as indicated by the higher CPU total time compared to CUDA total time. 2. Potential Bottlenecks: - The majority of the time is spent in the forward pass, specifically in the ReLU activation and the final linear layer. 3. Suggestions: - Consider using more efficient activation functions or optimizing the matrix operations in the linear layers. - If using on GPU, ensure synchronous operations to get more accurate profiling data. ```","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.utils.bottleneck # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Train the model def train_model(): model = SimpleModel() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(100): inputs = torch.randn(64, 10) targets = torch.randn(64, 1) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() if __name__ == \\"__main__\\": # Run the bottleneck profiler torch.utils.bottleneck.main([\\"train_model()\\"])"},{"question":"**Data Understanding and Loading with `sklearn.datasets`** You are provided with an important task to help evaluate a machine learning model by loading a specific dataset and performing preliminary data processing using the `sklearn.datasets` package. **Objective**: Write a function `load_and_print_dataset` that loads a specific real-world dataset and prints the following details about it: 1. The dataset description. 2. The shape of the data. 3. The first 5 rows of the data. 4. The target values in a summary form (unique values and their counts). The datasets you will be working with could be `\\"iris\\"`, `\\"wine\\"`, or `\\"breast_cancer\\"`. # Function Signature ```python def load_and_print_dataset(dataset_name: str) -> None: pass ``` # Input - `dataset_name` (str): The name of the dataset to be loaded. It can be one of `\\"iris\\"`, `\\"wine\\"`, or `\\"breast_cancer\\"`. # Constraints - If the `dataset_name` provided is not among the specified ones, raise a `ValueError` with the message `\\"Dataset not recognized\\"`. # Output The function does not return anything. It should print the following: - A brief description of the dataset retrieved from the `DESCR` attribute. - The shape of the dataset as `(n_samples, n_features)`. - The first 5 rows of the dataset. - A summary of the target values, mentioning the unique values and their counts. # Additional Information Use the `datasets.load_X()` function from `sklearn.datasets` where `X` is the dataset name, to load the desired dataset. # Example Usage ```python load_and_print_dataset(\\"iris\\") ``` Expected Output: ``` Dataset Description: ... ... Shape of data: (150, 4) First 5 rows: [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5.0, 3.6, 1.4, 0.2]] Target summary: {0: 50, 1: 50, 2: 50} ``` # Notes - Make sure to consider edge cases such as providing an unrecognized dataset name. - Ensure the solution is efficient and leverages the capabilities of the `sklearn.datasets` package effectively.","solution":"from sklearn.datasets import load_iris, load_wine, load_breast_cancer import numpy as np import pandas as pd def load_and_print_dataset(dataset_name: str) -> None: Loads a given dataset and prints its description, shape, first 5 rows, and a summary of the target values. Parameters: dataset_name (str): The name of the dataset to be loaded. It can be one of \\"iris\\", \\"wine\\", or \\"breast_cancer\\". Raises: ValueError: If the dataset_name is not recognized. if dataset_name == \'iris\': dataset = load_iris() elif dataset_name == \'wine\': dataset = load_wine() elif dataset_name == \'breast_cancer\': dataset = load_breast_cancer() else: raise ValueError(\\"Dataset not recognized\\") print(f\\"Dataset Description:n{dataset[\'DESCR\'][:500]}...n\\") print(f\\"Shape of data: {dataset[\'data\'].shape}\\") data_df = pd.DataFrame(dataset[\'data\'], columns=dataset[\'feature_names\']) print(\\"First 5 rows:n\\", data_df.head().values.tolist()) target_summary = dict(zip(*np.unique(dataset[\'target\'], return_counts=True))) print(f\\"Target summary: {target_summary}\\")"},{"question":"# Python Coding Assessment Question **Objective:** Implement a custom subset of the \\"pydoc\\" functionalities to demonstrate comprehension of generating and serving Python documentation. Problem Statement: You are tasked with creating a simple version of the \\"pydoc\\" tool that can generate and serve documentation for a given Python module. You will need to implement the following functionalities: 1. **Generate Documentation for a Module**: - Write a function `generate_doc(module_name: str) -> str` that takes a module name as input and returns the documentation string of the module, including its classes, functions, and methods with their respective docstrings. 2. **Run a Simple HTTP Server to Serve the Documentation**: - Write a function `serve_doc(module_name: str, port: int) -> None` that takes a module name and a port number as inputs and starts an HTTP server that serves the generated documentation in HTML format at the given port. Specifications: 1. `generate_doc(module_name: str) -> str` - **Input:** - `module_name`: A string representing the name of the module (e.g., `\'sys\'`). - **Output:** - A string containing the documentation of the given module in plain text format. - **Details:** Use Python\'s introspection capabilities to fetch the moduleâ€™s docstring, and recursively retrieve docstrings for classes, methods, and functions within the module. 2. `serve_doc(module_name: str, port: int) -> None` - **Input:** - `module_name`: A string representing the name of the module (e.g., `\'sys\'`). - `port`: An integer representing the port number to serve the documentation. - **Output:** - None - **Details:** Start a simple HTTP server using Python\'s `http.server` module, which serves the generated documentation as an HTML page. Ensure the documentation content is properly HTML-escaped to prevent security issues. Example: ```python # Example usage: mod_doc = generate_doc(\'os\') print(mod_doc) # Serve the documentation at http://localhost:8000 serve_doc(\'os\', 8000) ``` Constraints: - The module specified by `module_name` should be available in the Python environment. - The server should handle at least basic HTTP GET requests. Notes: - You may use the `inspect` module to retrieve docstrings and inspect module content. - When generating HTML content for the server, make sure to escape special characters to prevent HTML injection. Hints: - To fetch a module\'s documentation, you can make use of `inspect.getdoc` for retrieving docstrings. - To implement the HTTP server, refer to `http.server.SimpleHTTPRequestHandler` and `socketserver.TCPServer`. **Evaluation Criteria**: - Correctness: The solution should correctly generate and serve the documentation as specified. - Code Quality: The implementation should follow good coding practices, including clear docstrings and readability. - Robustness: The solution should handle potential errors gracefully (e.g., module not found).","solution":"import inspect import importlib from http.server import SimpleHTTPRequestHandler, HTTPServer from html import escape def generate_doc(module_name: str) -> str: Generate documentation for the given module name. Parameters: - module_name: str Returns: - A string containing the documentation. try: module = importlib.import_module(module_name) doc = inspect.getdoc(module) doc_parts = [f\\"# Documentation for {module_name}nn{doc}nn\\"] for name, obj in inspect.getmembers(module): if inspect.isclass(obj) or inspect.isfunction(obj): obj_doc = inspect.getdoc(obj) or \'No documentation available\' doc_parts.append(f\\" {name}nn{obj_doc}nn\\") return \\"n\\".join(doc_parts) except ModuleNotFoundError: return f\\"Module \'{module_name}\' not found.\\" class CustomHTTPRequestHandler(SimpleHTTPRequestHandler): def __init__(self, doc, *args, **kwargs): self.doc = doc super().__init__(*args, directory=None, **kwargs) def do_GET(self): if self.path == \'/\': self.send_response(200) self.send_header(\\"Content-type\\", \\"text/html\\") self.end_headers() self.wfile.write(f\\"<html><body><pre>{escape(self.doc)}</pre></body></html>\\".encode(\'utf-8\')) else: self.send_error(404, \\"File not found\\") def serve_doc(module_name: str, port: int) -> None: Serve the documentation for the given module name on the specified port. Parameters: - module_name: str - port: int doc = generate_doc(module_name) handler = lambda *args, **kwargs: CustomHTTPRequestHandler(doc, *args, **kwargs) with HTTPServer((\\"\\", port), handler) as httpd: print(f\\"Serving documentation for {module_name} on port {port}\\") httpd.serve_forever()"},{"question":"Problem Statement You are tasked with creating a Python program that performs various system-related operations using the `os` module. Implement the following functionalities in a single Python script: 1. **Environment Variable Management:** - Create an environment variable named `MY_VAR` and set its value to \\"HelloWorld\\". - Retrieve and print the value of `MY_VAR`. - Delete the environment variable `MY_VAR`. 2. **File Operations:** - Create a new directory named `test_dir`. - Inside this directory, create a new file named `example.txt` and write \\"Hello, this is a test.\\" to this file. - Read the content of `example.txt` and print it. - Delete the file `example.txt`. - Remove the directory `test_dir`. 3. **Process Management:** - Create a child process that executes the `ls` command (or `dir` on Windows) to list the contents of the current directory. - Ensure the child process waits until it completes and print its exit status. Your solution should leverage appropriate methods from the `os` module to achieve the above tasks. Handle any exceptions that might arise and ensure that resources (like open file descriptors) are properly closed when they are no longer needed. Input and Output - **Input:** There is no input for this problem. - **Output:** You should print various strings at different stages, indicating the outcome of each operation, such as environment variable value, file content, and exit status of the child process. Constraints - This script is expected to run on Unix-based systems like Linux and macOS, as well as on Windows. - Ensure the directory and file operations handle cases where the files or directories may already exist, producing meaningful error messages. Example Output ``` MY_VAR is set to: HelloWorld MY_VAR deleted successfully. example.txt contains: Hello, this is a test. Child process exit status: 0 ``` Your task is to complete the Python script to implement the above functionalities. Use appropriate methods from the `os` module and ensure robust error handling.","solution":"import os import stat import subprocess def manage_environment_variable(): try: os.environ[\'MY_VAR\'] = \'HelloWorld\' my_var = os.getenv(\'MY_VAR\') print(f\\"MY_VAR is set to: {my_var}\\") del os.environ[\'MY_VAR\'] print(\\"MY_VAR deleted successfully.\\") except Exception as e: print(f\\"An error occurred while managing environment variable: {e}\\") def file_operations(): try: os.makedirs(\'test_dir\', exist_ok=True) file_path = os.path.join(\'test_dir\', \'example.txt\') with open(file_path, \'w\') as file: file.write(\\"Hello, this is a test.\\") with open(file_path, \'r\') as file: content = file.read() print(f\\"example.txt contains: {content}\\") os.remove(file_path) os.rmdir(\'test_dir\') except Exception as e: print(f\\"An error occurred during file operations: {e}\\") def process_management(): try: process = subprocess.Popen([\\"ls\\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = process.communicate() exit_status = process.returncode print(stdout.decode(\'utf-8\')) print(f\\"Child process exit status: {exit_status}\\") except Exception as e: print(f\\"An error occurred during process management: {e}\\") def main(): manage_environment_variable() file_operations() process_management() if __name__ == \\"__main__\\": main()"},{"question":"# Custom Module Importer Objective: You are required to design a system that dynamically imports modules using a custom importer. This task will test your understanding of Python\'s import system, particularly focusing on the ability to manipulate and extend the behavior of Python\'s default importing mechanisms. Problem Statement: Create a custom module importer that can import Python modules from a specialized directory structure. The custom importer should support: - Loading regular modules and packages. - Handling both absolute and relative imports. - Implementing meta path hooks for custom module finding and loading. Task: Implement a class `CustomImporter` that extends Python\'s import system with the following requirements: 1. **Dynamic Directory Setup:** - The custom importer should search for modules in a specific directory provided at runtime. - This directory can have a structure similar to Python packages with `__init__.py` files marking package directories. 2. **Meta Path Hook:** - Implement a meta path hook that adds this custom directory to the module search path and can import modules from it. - The hook should be registered in `sys.meta_path`. 3. **Module Importation:** - Implement the logic within `CustomImporter` to find and load modules from the custom directory. - Ensure the custom importer correctly handles both absolute and relative imports. - If a module is not found in the custom directory, fallback to the default import behavior. 4. **Error Handling:** - Properly handle errors such as `ModuleNotFoundError` when a module does not exist in the custom directory. Input/Output: - The input will be a directory path where the custom modules are located. - The `CustomImporter` will be used to import modules from the provided path. - Output is the successful import of the requested module. Constraints: - The custom directory will always be a valid path. - Modules inside the custom directory may have sub-packages. Example Usage: ```python # Assuming the custom directory structure is as below: # custom_dir/ # â”œâ”€â”€ __init__.py # â”œâ”€â”€ module_a.py # â””â”€â”€ package/ # â”œâ”€â”€ __init__.py # â”œâ”€â”€ submodule_b.py from custom_importer import CustomImporter # Initialize custom importer with the directory path importer = CustomImporter(\'/path/to/custom_dir\') # Import a module from the custom directory module_a = importer.import_module(\'module_a\') module_a.some_function() # Import a submodule from a package within the custom directory submodule_b = importer.import_module(\'package.submodule_b\') submodule_b.another_function() ``` Your task is to implement the `CustomImporter` class, including the meta path hook, module finding, and loading logic, ensuring that it integrates seamlessly with Python\'s import system.","solution":"import sys import os import importlib import importlib.util class CustomImporter: def __init__(self, directory): self.directory = directory if self.directory not in sys.path: sys.path.append(self.directory) sys.meta_path.insert(0, self) def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, *fullname.split(\'.\')) + \'.py\' package_path = os.path.join(self.directory, *fullname.split(\'.\'), \'__init__.py\') if os.path.isfile(module_path) or os.path.isfile(package_path): return importlib.util.spec_from_file_location(fullname, module_path if os.path.isfile(module_path) else package_path) return None def import_module(self, module_name): return importlib.import_module(module_name) def custom_importer_remove(): sys.meta_path = [finder for finder in sys.meta_path if not isinstance(finder, CustomImporter)]"},{"question":"# Covariance Estimation Challenge Objective: Develop a suite of functions to compute various covariance estimates given a dataset and then evaluate their performance. This exercise will test your understanding of using `sklearn.covariance` for different covariance estimation techniques. Background: You are provided with a dataset that contains multiple features. Your task is to: 1. Compute the empirical covariance matrix. 2. Calculate the shrunk covariance using a given shrinkage coefficient. 3. Apply the Ledoit-Wolf shrinkage method. 4. Use Oracle Approximating Shrinkage (OAS) to estimate the covariance. 5. Estimate a sparse inverse covariance matrix. 6. Implement robust covariance estimation to handle outliers using Minimum Covariance Determinant (MCD). Instructions: 1. Implement the following functions: - `compute_empirical_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray` - `compute_shrunk_covariance(X: np.ndarray, shrinkage: float, assume_centered: bool = False) -> np.ndarray` - `compute_ledoit_wolf_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray` - `compute_oas_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray` - `compute_sparse_inverse_covariance(X: np.ndarray, alpha: float, assume_centered: bool = False) -> np.ndarray` - `compute_robust_covariance(X: np.ndarray, assume_centered: bool = False) -> Tuple[np.ndarray, np.ndarray]` 2. Ensure the input `X` is a NumPy array with shape `(n_samples, n_features)`. 3. For the `compute_sparse_inverse_covariance` function, use the `GraphicalLasso` estimator. 4. The `compute_robust_covariance` function should return a tuple containing the robust location and covariance matrix respectively. Input: - `X`: NumPy array of shape `(n_samples, n_features)` representing the dataset. - `shrinkage`: A float representing the shrinkage coefficient (for `compute_shrunk_covariance`). - `alpha`: A float representing the penalty parameter (for `compute_sparse_inverse_covariance`). - `assume_centered`: A boolean indicating whether to assume the data is centered. Output: - For functions other than `compute_robust_covariance`, return a NumPy array representing the covariance matrix. - For `compute_robust_covariance`, return a tuple with the robust location and covariance matrix. Example: ```python import numpy as np from sklearn.covariance import MinCovDet # Example usage X = np.random.randn(100, 20) # Sample dataset empirical_cov = compute_empirical_covariance(X) shrunk_cov = compute_shrunk_covariance(X, shrinkage=0.1) ledoit_wolf_cov = compute_ledoit_wolf_covariance(X) oas_cov = compute_oas_covariance(X) sparse_inv_cov = compute_sparse_inverse_covariance(X, alpha=0.01) robust_loc, robust_cov = compute_robust_covariance(X) ``` Note: Ensure that the functions handle cases where `assume_centered` is both `True` and `False`. Provide appropriate checks and normalize the data if required. Constraints: - `X` will always have at least 10 samples and 2 features. - Implement proper error handling and input validation.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet def compute_empirical_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray: Computes the empirical covariance matrix. Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). assume_centered (bool): If True, data is not centered before computation. Returns: np.ndarray: Empirical covariance matrix. emp_cov = EmpiricalCovariance(assume_centered=assume_centered).fit(X) return emp_cov.covariance_ def compute_shrunk_covariance(X: np.ndarray, shrinkage: float, assume_centered: bool = False) -> np.ndarray: Computes the shrunk covariance matrix. Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). shrinkage (float): Shrinkage coefficient. assume_centered (bool): If True, data is not centered before computation. Returns: np.ndarray: Shrunk covariance matrix. shrunk_cov = ShrunkCovariance(shrinkage=shrinkage, assume_centered=assume_centered).fit(X) return shrunk_cov.covariance_ def compute_ledoit_wolf_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray: Computes the Ledoit-Wolf shrunk covariance matrix. Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). assume_centered (bool): If True, data is not centered before computation. Returns: np.ndarray: Ledoit-Wolf shrunk covariance matrix. lw = LedoitWolf(assume_centered=assume_centered).fit(X) return lw.covariance_ def compute_oas_covariance(X: np.ndarray, assume_centered: bool = False) -> np.ndarray: Computes the Oracle Approximating Shrinkage (OAS) covariance matrix. Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). assume_centered (bool): If True, data is not centered before computation. Returns: np.ndarray: OAS covariance matrix. oas = OAS(assume_centered=assume_centered).fit(X) return oas.covariance_ def compute_sparse_inverse_covariance(X: np.ndarray, alpha: float, assume_centered: bool = False) -> np.ndarray: Computes the sparse inverse covariance matrix using Graphical Lasso. Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). alpha (float): Regularization parameter. assume_centered (bool): If True, data is not centered before computation. Returns: np.ndarray: Sparse inverse covariance matrix. graphical_lasso = GraphicalLasso(alpha=alpha, assume_centered=assume_centered).fit(X) return graphical_lasso.precision_ def compute_robust_covariance(X: np.ndarray, assume_centered: bool = False) -> tuple: Computes the robust covariance matrix using Minimum Covariance Determinant (MCD). Parameters: X (np.ndarray): Dataset of shape (n_samples, n_features). assume_centered (bool): If True, data is not centered before computation. Returns: tuple: Robust location and covariance matrix. mcd = MinCovDet(assume_centered=assume_centered).fit(X) return mcd.location_, mcd.covariance_"},{"question":"**UUIDs and Data Consistency** You are tasked with managing a system that relies on unique identifiers for keeping track of entities. To ensure consistency and uniqueness across different components of the system, you need to implement various functionalities using the `uuid` module. # Task: Implement a Python class `UUIDManager` that provides the following functionalities: 1. **Store and Retrieve UUIDs**: - `add_uuid(self, uuid: uuid.UUID) -> None`: Adds a UUID to the internal storage. - `get_uuids(self) -> list[uuid.UUID]`: Returns the list of all stored UUIDs. 2. **Generate Specific UUIDs**: - `generate_uuid1(self) -> uuid.UUID`: Generates a version 1 UUID and stores it. - `generate_uuid4(self) -> uuid.UUID`: Generates a version 4 UUID and stores it. - `generate_uuid3(self, name: str) -> uuid.UUID`: Generates a version 3 UUID using `uuid.NAMESPACE_DNS` and the provided name, then stores it. - `generate_uuid5(self, name: str) -> uuid.UUID`: Generates a version 5 UUID using `uuid.NAMESPACE_DNS` and the provided name, then stores it. 3. **Check UUID Safety**: - `check_uuid_safety(self, uuid: uuid.UUID) -> str`: Returns whether the given UUID is \'safe\', \'unsafe\', or \'unknown\' based on its `is_safe` attribute. # Constraints: - Ensure that UUIDs are stored in a way that no duplicates exist in the internal storage. - The methods should handle the addition and retrieval of UUIDs efficiently. # Example Usage: ```python import uuid class UUIDManager: def __init__(self): self.storage = set() def add_uuid(self, uuid: uuid.UUID) -> None: # Implement this method pass def get_uuids(self) -> list[uuid.UUID]: # Implement this method pass def generate_uuid1(self) -> uuid.UUID: # Implement this method pass def generate_uuid4(self) -> uuid.UUID: # Implement this method pass def generate_uuid3(self, name: str) -> uuid.UUID: # Implement this method pass def generate_uuid5(self, name: str) -> uuid.UUID: # Implement this method pass def check_uuid_safety(self, uuid: uuid.UUID) -> str: # Implement this method pass # Example Usage: manager = UUIDManager() uuid1 = manager.generate_uuid1() uuid4 = manager.generate_uuid4() uuid3 = manager.generate_uuid3(\'python.org\') uuid5 = manager.generate_uuid5(\'python.org\') print(manager.get_uuids()) # Should print the list containing uuid1, uuid4, uuid3, and uuid5 print(manager.check_uuid_safety(uuid1)) # Could print \'safe\', \'unsafe\', or \'unknown\' ``` # Note: - You are responsible for ensuring correct imports and handling any exceptions that might arise. - The internal storage should use appropriate data structures to prevent the addition of duplicate UUIDs.","solution":"import uuid class UUIDManager: def __init__(self): self.storage = set() def add_uuid(self, uuid_obj: uuid.UUID) -> None: Adds a UUID to the internal storage. No duplicates are stored. self.storage.add(uuid_obj) def get_uuids(self) -> list[uuid.UUID]: Returns the list of all stored UUIDs. return list(self.storage) def generate_uuid1(self) -> uuid.UUID: Generates a version 1 UUID and stores it. uuid1 = uuid.uuid1() self.add_uuid(uuid1) return uuid1 def generate_uuid4(self) -> uuid.UUID: Generates a version 4 UUID and stores it. uuid4 = uuid.uuid4() self.add_uuid(uuid4) return uuid4 def generate_uuid3(self, name: str) -> uuid.UUID: Generates a version 3 UUID using uuid.NAMESPACE_DNS and the provided name, then stores it. uuid3 = uuid.uuid3(uuid.NAMESPACE_DNS, name) self.add_uuid(uuid3) return uuid3 def generate_uuid5(self, name: str) -> uuid.UUID: Generates a version 5 UUID using uuid.NAMESPACE_DNS and the provided name, then stores it. uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, name) self.add_uuid(uuid5) return uuid5 def check_uuid_safety(self, uuid_obj: uuid.UUID) -> str: Returns whether the given UUID is \'safe\', \'unsafe\', or \'unknown\' based on its is_safe attribute. return uuid_obj.is_safe.name"},{"question":"Implementing Cell Object Functionality In this assessment, you will be required to implement a basic simulation of `Cell` objects in Python. The objective is to gauge your understanding of closure concepts and object manipulation, mimicking what the `Cell` objects do as described. # Task Write a Python class called `CellObject` that simulates the behavior of the `Cell` objects as described in the documentation. # Class Specification: - **CellObject**: - **Methods**: 1. `__init__(self, value=None)`: Initialize the Cell object with an optional value. If no value is provided, initialize with `None`. 2. `get(self)`: Return the current value stored in the Cell object. 3. `set(self, value)`: Set the value of the Cell object to the provided value. # Example Usage: ```python # Initialization with a value cell = CellObject(10) print(cell.get()) # Output: 10 # Setting a new value cell.set(20) print(cell.get()) # Output: 20 # Initialization without a value empty_cell = CellObject() print(empty_cell.get()) # Output: None # Mixing and matching empty_cell.set(30) print(empty_cell.get()) # Output: 30 ``` # Input and Output Requirements: - You are to define the `CellObject` class only. - The methods should handle the types of values they are expected to store. - Ensure that initializations and get/set operations are handled appropriately. - No need to handle concurrency or thread-safety. # Constraints: - The value stored in a Cell object can be any valid Python object. - Assume that the methods `get` and `set` will always receive appropriate types during the tests; hence, no need for extensive type checking. # Performance: - Since we are simulating, focus on the correctness and simplicity rather than computational efficiency. # Note: This question is designed to assess your understanding of: - Object-oriented programming in Python. - The concept of closures and how Python handles nonlocal variables (simulated through `CellObject` here). - Basic class and method definitions. Implement the `CellObject` class with the specifications provided to complete this task.","solution":"class CellObject: def __init__(self, value=None): self.value = value def get(self): return self.value def set(self, value): self.value = value"},{"question":"You are provided with a dataset containing features and labels. Your task is to implement a Nearest Neighbors classification model using scikit-learn\'s `KNeighborsClassifier` and evaluate its performance on this dataset. Dataset The dataset consists of: - A training feature matrix `X_train` of shape `(n_samples_train, n_features)`. - A training label vector `y_train` of shape `(n_samples_train,)`. - A testing feature matrix `X_test` of shape `(n_samples_test, n_features)`. - A testing label vector `y_test` of shape `(n_samples_test,)`. Task 1. **Implement a function `train_knn_classifier` that takes the following inputs:** - `X_train`: numpy array of shape `(n_samples_train, n_features)` - `y_train`: numpy array of shape `(n_samples_train,)` - `n_neighbors`: integer specifying the number of neighbors to use for `KNeighborsClassifier` - `algorithm`: string specifying the algorithm to use (`\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`) 2. **The function should train a `KNeighborsClassifier` with the provided parameters and return the trained model.** 3. **Implement a function `evaluate_model` that takes the following inputs:** - `model`: the trained `KNeighborsClassifier` model - `X_test`: numpy array of shape `(n_samples_test, n_features)` - `y_test`: numpy array of shape `(n_samples_test,)` 4. **The function should:** - Predict the labels for `X_test` using the provided model. - Calculate and return the classification accuracy. Input and Output Formats ```python def train_knn_classifier(X_train, y_train, n_neighbors, algorithm): Train a KNeighborsClassifier on the training data. :param X_train: numpy array of shape (n_samples_train, n_features) :param y_train: numpy array of shape (n_samples_train,) :param n_neighbors: integer specifying number of neighbors for KNeighborsClassifier :param algorithm: string specifying the algorithm to use (\'auto\', \'ball_tree\', \'kd_tree\', \'brute\') :return: trained KNeighborsClassifier model pass def evaluate_model(model, X_test, y_test): Evaluate the KNeighborsClassifier model on the test data. :param model: trained KNeighborsClassifier model :param X_test: numpy array of shape (n_samples_test, n_features) :param y_test: numpy array of shape (n_samples_test,) :return: classification accuracy as a float pass ``` # Example Usage ```python # Sample data X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[1.5, 2.5], [3.5, 4.5]]) y_test = np.array([0, 1]) # Train the model model = train_knn_classifier(X_train, y_train, n_neighbors=3, algorithm=\'auto\') # Evaluate the model accuracy = evaluate_model(model, X_test, y_test) print(f\\"Accuracy: {accuracy}\\") ``` Constraints - You may assume that `X_train`, `y_train`, `X_test`, and `y_test` are valid numpy arrays with appropriate shapes. - The value of `n_neighbors` will be a positive integer. - The value of `algorithm` will be one of `\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, or `\'brute\'`. Performance Requirements - The solution should be able to handle datasets with a reasonably large number of samples (up to tens of thousands) efficiently.","solution":"from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def train_knn_classifier(X_train, y_train, n_neighbors, algorithm): Train a KNeighborsClassifier on the training data. :param X_train: numpy array of shape (n_samples_train, n_features) :param y_train: numpy array of shape (n_samples_train,) :param n_neighbors: integer specifying number of neighbors for KNeighborsClassifier :param algorithm: string specifying the algorithm to use (\'auto\', \'ball_tree\', \'kd_tree\', \'brute\') :return: trained KNeighborsClassifier model knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) knn.fit(X_train, y_train) return knn def evaluate_model(model, X_test, y_test): Evaluate the KNeighborsClassifier model on the test data. :param model: trained KNeighborsClassifier model :param X_test: numpy array of shape (n_samples_test, n_features) :param y_test: numpy array of shape (n_samples_test,) :return: classification accuracy as a float y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Unix Telephone Directory Service You are tasked with creating a simplified command-line-based telephone directory service using several Unix-specific modules in Python. The directory should allow users to perform the following operations: 1. **Add a new contact**: Store the contact\'s name, phone number, and group information. 2. **Retrieve contact details**: Fetch the stored details of a contact using the contact\'s name. 3. **List all contacts**: Display all stored contacts with their respective details in a formatted manner. 4. **Delete a contact**: Remove a stored contact based on the contact\'s name. 5. **List telephone directory usage statistics**: Retrieve and display the current resource usage statistics related to the directory operations. Specifications - **Function Implementation**: Write functions to handle the described operations. - **Input and Output Formats**: - `add_contact(name: str, phone: str, group: str) -> None`: Adds a new contact. - `get_contact(name: str) -> Optional[Dict[str, str]]`: Retrieves a contact\'s details. - `list_contacts() -> List[Dict[str, str]]`: Lists all contacts. - `delete_contact(name: str) -> bool`: Deletes a contact. - `get_usage_stats() -> Dict[str, Union[int, float]]`: Retrieves usage statistics. - **Constraints and Limitations**: - Names must be unique. - Use Python\'s `pwd` and `grp` modules to fetch the current user\'s and group\'s information respectively. - Implement resource tracking using the `resource` module. - Display retrieval and listing operations in a human-friendly format. Example # Adding a Contact ```python add_contact(\\"Alice\\", \\"555-1234\\", \\"friends\\") ``` # Retrieving a Contact ```python >>> get_contact(\\"Alice\\") {\\"name\\": \\"Alice\\", \\"phone\\": \\"555-1234\\", \\"group\\": \\"friends\\"} ``` # Listing All Contacts ```python >>> list_contacts() [{\\"name\\": \\"Alice\\", \\"phone\\": \\"555-1234\\", \\"group\\": \\"friends\\"}] ``` # Deleting a Contact ```python >>> delete_contact(\\"Alice\\") True ``` # Getting Usage Statistics ```python >>> get_usage_stats() {\\"user_time\\": 0.104, \\"system_time\\": 0.003, \\"memory_usage\\": 12345} ``` Implement all the required functionalities ensuring robust error handling and appropriate resource management.","solution":"from typing import Optional, Dict, List, Union import pwd import grp import os import resource contacts = {} def add_contact(name: str, phone: str, group: str) -> None: if name in contacts: raise ValueError(\\"Contact with this name already exists.\\") contacts[name] = {\\"name\\": name, \\"phone\\": phone, \\"group\\": group} def get_contact(name: str) -> Optional[Dict[str, str]]: return contacts.get(name) def list_contacts() -> List[Dict[str, str]]: return list(contacts.values()) def delete_contact(name: str) -> bool: if name in contacts: del contacts[name] return True return False def get_usage_stats() -> Dict[str, Union[int, float]]: usage = resource.getrusage(resource.RUSAGE_SELF) return { \\"user_time\\": usage.ru_utime, \\"system_time\\": usage.ru_stime, \\"memory_usage\\": usage.ru_maxrss }"},{"question":"Parsing and Analyzing Robots.txt Files Objective In this task, you are required to implement two functions using the `urllib.robotparser` module to assist in the analysis of robots.txt files for given websites. Function 1: fetch_robots_txt_details(url) This function should: 1. Take the URL of a website (e.g., \\"http://www.example.com\\") as input. 2. Fetch the robots.txt file from the URL (`http://www.example.com/robots.txt`). 3. Return a dictionary with the following information: - `disallowed_paths`: List of paths disallowed for the user agent \\"*\\". - `allowed_paths`: List of paths explicitly allowed for the user agent \\"*\\". - `crawl_delay`: The crawl delay for the user agent \\"*\\" if specified, otherwise `None`. - `request_rate`: The request rate as a tuple (requests, seconds) for the user agent \\"*\\" if specified, otherwise `None`. - `site_maps`: List of sitemap URLs if specified, otherwise `None`. Function 2: can_fetch_url(url, target_url) This function should: 1. Take two URLs as input: - `url`: The base URL of a website (e.g., \\"http://www.example.com\\"). - `target_url`: A target URL on the same website (e.g., \\"http://www.example.com/path/to/page\\"). 2. Fetch the robots.txt file from the base URL. 3. Return `True` if the user agent \\"*\\" is allowed to fetch the `target_url` according to the robots.txt file, otherwise `False`. Input/Output Format ```python def fetch_robots_txt_details(url: str) -> dict: Fetches and parses robots.txt from the given website URL and returns relevant details. Args: url (str): The URL of the website. Returns: dict: A dictionary with keys \'disallowed_paths\', \'allowed_paths\', \'crawl_delay\', \'request_rate\', and \'site_maps\'. pass def can_fetch_url(url: str, target_url: str) -> bool: Determines if the user agent \'*\' is allowed to fetch the target URL based on robots.txt. Args: url (str): The URL of the website. target_url (str): The target URL to be fetched. Returns: bool: True if user agent \'*\' can fetch the target URL, otherwise False. pass ``` Example ```python # Example usage details = fetch_robots_txt_details(\\"http://www.example.com\\") print(details) # Output: { # \'disallowed_paths\': [\'/private\'], # \'allowed_paths\': [\'/public\'], # \'crawl_delay\': 10, # \'request_rate\': (5, 60), # \'site_maps\': [\'http://www.example.com/sitemap.xml\'] # } result = can_fetch_url(\\"http://www.example.com\\", \\"http://www.example.com/private\\") print(result) # Output: False ``` **Constraints:** - You must handle network errors gracefully. - Assume the robots.txt file may not exist or be accessible for some URLs. - The input URL will always be a valid website URL. **Performance Requirements:** - The implementation should be efficient and should not re-fetch the robots.txt file if it has been fetched recently within the same function call. **Hints:** - Use the `RobotFileParser` class to parse and analyze the robots.txt file. - You may cache the robots.txt content to avoid redundant network requests during a function call.","solution":"import urllib.robotparser import urllib.request from urllib.error import URLError, HTTPError def fetch_robots_txt_details(url: str) -> dict: Fetches and parses robots.txt from the given website URL and returns relevant details. Args: url (str): The URL of the website. Returns: dict: A dictionary with keys \'disallowed_paths\', \'allowed_paths\', \'crawl_delay\', \'request_rate\', and \'site_maps\'. robots_url = f\\"{url}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() details = { \'disallowed_paths\': [], \'allowed_paths\': [], \'crawl_delay\': None, \'request_rate\': None, \'site_maps\': [] } try: rp.set_url(robots_url) rp.read() if rp: user_agent = \'*\' disallowed_paths = [] allowed_paths = [] entry = None for e in rp.entries: if any(user_agent in agent for agent in e.useragents): entry = e break if entry: for line in entry.rulelines: if line.action == \'disallow\': disallowed_paths.append(line.path) if line.action == \'allow\': allowed_paths.append(line.path) details[\'disallowed_paths\'] = disallowed_paths details[\'allowed_paths\'] = allowed_paths details[\'crawl_delay\'] = rp.crawl_delay(user_agent) details[\'request_rate\'] = rp.request_rate(user_agent) details[\'site_maps\'] = rp.site_maps() except (URLError, HTTPError): # Handling the case where robots.txt cannot be fetched pass return details def can_fetch_url(url: str, target_url: str) -> bool: Determines if the user agent \'*\' is allowed to fetch the target URL based on robots.txt. Args: url (str): The base URL of the website. target_url (str): The target URL to be fetched. Returns: bool: True if user agent \'*\' can fetch the target URL, otherwise False. robots_url = f\\"{url}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() try: rp.set_url(robots_url) rp.read() return rp.can_fetch(\'*\', target_url) except (URLError, HTTPError): # If we cannot access robots.txt, we default to assuming we cannot fetch the URL return False"},{"question":"**Question: Multi-dimensional Data Visualization with Seaborn** You are provided with a dataset containing information about passengers on the Titanic. Your task is to create a visual representation that explores various relationships within the data using the seaborn library. # Dataset The dataset contains the following columns: - `survived`: Survival (0 = No, 1 = Yes) - `pclass`: Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd) - `sex`: Sex - `age`: Age in years - `sibsp`: Number of siblings/spouses aboard the Titanic - `parch`: Number of parents/children aboard the Titanic - `fare`: Passenger fare # Objective 1. Create a `FacetGrid` to visualize the distribution of passenger fares (`fare`) conditioned on passenger class (`pclass`) and survival status (`survived`). 2. The rows should represent the different `pclass` values, and the columns should represent the `survived` values. 3. Use a histogram (`histplot`) for the distribution of the `fare`. 4. Customize the grid to improve readability: - Set a unique color palette for the histogram bars. - Adjust the height and aspect ratio of each subplot for better visualization. - Add axis labels and titles to the grid for clear interpretation. # Input - `df`: A pandas DataFrame containing the Titanic dataset. # Requirements - Use seaborn and matplotlib libraries. - The function `visualize_fare_distribution` should have the following signature: ```python def visualize_fare_distribution(df: pd.DataFrame) -> None: ``` # Constraints - The function should not return any value; it should display the plot directly. - Ensure that the plots are well-labeled and legible. # Example Output Upon executing your function, a multi-plot grid with fare distribution histograms for each combination of `pclass` and `survived` should be displayed. **Starter Code**: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_fare_distribution(df: pd.DataFrame) -> None: # Implement the function here pass # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") visualize_fare_distribution(titanic) ``` **Note**: The dataset can be loaded using `sns.load_dataset(\\"titanic\\")`. Make sure to test your function with the Titanic dataset to verify your solution.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_fare_distribution(df: pd.DataFrame) -> None: Visualizes the distribution of passenger fares conditioned on passenger class and survival status using seaborn\'s FacetGrid. # Define the color palette palette = sns.color_palette(\\"muted\\") # Create the FacetGrid g = sns.FacetGrid(df, row=\\"pclass\\", col=\\"survived\\", margin_titles=True, height=4, aspect=1.2, palette=palette) # Map histplot to the grid g.map(sns.histplot, \\"fare\\", kde=False, bins=30) # Add axis labels and titles g.set_axis_labels(\\"Fare\\", \\"Count\\") g.set_titles(col_template=\\"{col_name} Survived\\", row_template=\\"Class {row_name}\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Fare Distribution by Passenger Class and Survival Status\') # Show the plot plt.show()"},{"question":"# Question: Advanced Line Plot Customization using Seaborn You are provided with a dataset containing yearly data of airline passengers for 10 years across different months. Your task is to visualize this data using seaborn\'s `lineplot` to demonstrate the trends and patterns efficiently. Dataset Description The dataset `flights` can be loaded directly using seaborn\'s dataset loading function: ```python flights = sns.load_dataset(\\"flights\\") ``` It contains the following columns: - `year`: The year of the observation. - `month`: The month of the observation. - `passengers`: The number of passengers for that month. Tasks 1. **Data Preparation and Plotting:** - Filter the dataset to include only the data for the month of May. - Create a line plot showing the number of passengers over the years for the month of May. 2. **Pivot and Plot Data:** - Pivot the `flights` DataFrame to a wide-form representation where rows represent years and columns represent months with their respective passenger counts. - Create a line plot showing trends for all months over the years using the wide-form DataFrame. 3. **Advanced Customization:** - Using the original long-form `flights` DataFrame, create a line plot showing the number of passengers over the years. - Use the `hue` parameter to color the lines based on the `month`. - Add markers and use different line styles for better visual distinction between months\' data. - Add appropriate titles and labels to the plot. Function Implementation Implement a function `visualize_flights_data()` that generates the required plots as described above. Your function should display the plots and ensure they are correctly labeled. ```python def visualize_flights_data(): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Filter data for May and plot may_flights = flights[flights[\'month\'] == \'May\'] plt.figure(figsize=(10, 6)) sns.lineplot(data=may_flights, x=\'year\', y=\'passengers\') plt.title(\'Number of Passengers in May Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # Task 2: Pivot the DataFrame and plot flights_wide = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') plt.figure(figsize=(12, 8)) sns.lineplot(data=flights_wide) plt.title(\'Yearly Passengers for Each Month\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # Task 3: Advanced Customization plt.figure(figsize=(14, 10)) sns.lineplot(data=flights, x=\'year\', y=\'passengers\', hue=\'month\', marker=\'o\', style=\'month\') plt.title(\'Monthly Passenger Trends Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show() ``` **Input:** No input parameters are required for the `visualize_flights_data` function. **Output:** The function will generate and display three distinct line plots as described in the tasks above, demonstrating key seaborn functionalities and plotting capabilities. Constraints: - Ensure that the plots are well-labeled for readability. - Handle any missing data appropriately to avoid plotting errors. Performance Considerations: - Ensure that the function executes efficiently even with the provided datasets\' size.","solution":"def visualize_flights_data(): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Filter data for May and plot may_flights = flights[flights[\'month\'] == \'May\'] plt.figure(figsize=(10, 6)) sns.lineplot(data=may_flights, x=\'year\', y=\'passengers\') plt.title(\'Number of Passengers in May Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # Task 2: Pivot the DataFrame and plot flights_wide = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') plt.figure(figsize=(12, 8)) sns.lineplot(data=flights_wide) plt.title(\'Yearly Passengers for Each Month\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # Task 3: Advanced Customization plt.figure(figsize=(14, 10)) sns.lineplot(data=flights, x=\'year\', y=\'passengers\', hue=\'month\', marker=\'o\', style=\'month\') plt.title(\'Monthly Passenger Trends Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show()"},{"question":"# Asynchronous Data Fetcher **Objective**: Implement an asynchronous data fetcher that retrieves data from multiple sources concurrently using the `asyncio` library. **Background**: You are tasked with creating a utility that can fetch data from multiple URLs concurrently. This utility should use the `asyncio` library to demonstrate the concurrent fetching of data, leveraging the high-level APIs provided by `asyncio`. **Requirements**: 1. Implement a function `async def fetch_data(url: str) -> str:` that: - Takes a single URL as input. - Fetches the data from the URL asynchronously. - Returns the fetched data as a string. 2. Implement a function `async def gather_data(urls: list) -> list:` that: - Takes a list of URLs as input. - Uses `asyncio.gather` to fetch data from all URLs concurrently. - Returns a list of strings, where each string is the data fetched from the corresponding URL in the input list. 3. Implement a main function to demonstrate the usage of the above functions: - Create a list of sample URLs (you can use public APIs that return JSON data, such as `https://jsonplaceholder.typicode.com/posts/1`). - Use the `gather_data` function to fetch data from these URLs. - Print the fetched data. **Constraints**: - The function should handle network errors gracefully. - You should use the `aiohttp` library for making asynchronous HTTP requests. Ensure you install it using `pip install aiohttp` if not already available. **Performance Requirements**: - The solution should efficiently handle concurrent fetching of data from multiple URLs. **Example Usage**: ```python import asyncio async def fetch_data(url: str) -> str: # Your implementation here pass async def gather_data(urls: list) -> list: # Your implementation here pass async def main(): urls = [ \'https://jsonplaceholder.typicode.com/posts/1\', \'https://jsonplaceholder.typicode.com/posts/2\', \'https://jsonplaceholder.typicode.com/posts/3\', ] results = await gather_data(urls) for result in results: print(result) # Run the main function asyncio.run(main()) ``` **Note**: This task requires you to handle asynchronous programming, HTTP requests, and error handling. Use appropriate asynchronous techniques and ensure your code is well-structured and documented.","solution":"import aiohttp import asyncio async def fetch_data(url: str) -> str: Fetch data from a URL asynchronously. Parameters: url (str): The URL to fetch data from. Returns: str: The data fetched from the URL. async with aiohttp.ClientSession() as session: async with session.get(url) as response: if response.status == 200: return await response.text() else: return f\\"Error: {response.status}\\" async def gather_data(urls: list) -> list: Fetch data from multiple URLs concurrently. Parameters: urls (list): A list of URLs to fetch data from. Returns: list: A list of strings with fetched data from each URL. tasks = [fetch_data(url) for url in urls] return await asyncio.gather(*tasks) async def main(): urls = [ \'https://jsonplaceholder.typicode.com/posts/1\', \'https://jsonplaceholder.typicode.com/posts/2\', \'https://jsonplaceholder.typicode.com/posts/3\', ] results = await gather_data(urls) for result in results: print(result) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Question: Custom Display Configuration in Pandas # Objective Demonstrate your understanding of pandas options API by configuring display settings to visualize a large DataFrame effectively. # Problem Statement You are given a CSV file `large_dataset.csv` containing a large dataset of financial transactions. Your task is to write a function `configure_display_settings` that will: 1. Load the CSV file into a pandas DataFrame. 2. Set the maximum number of rows displayed to 15. 3. Set the maximum number of columns displayed to 10. 4. Adjust the column width to 20 characters. 5. Enable the expanded frame representation to allow columns to be spread across the screen if the number exceeds the screen width. 6. Set the precision of displayed floating-point numbers to 3 decimal places. 7. Ensure the settings are only applied within the function scope without affecting global settings. # Input - `file_path`: A string representing the path to the CSV file (`large_dataset.csv`). # Output - A configured pandas DataFrame ready for display as specified by the settings. # Constraints - Assume that `large_dataset.csv` is present in the current directory and is a well-formed CSV file. - The DataFrame can be large, so proper configuration of display settings is essential for visualization. # Function Signature ```python def configure_display_settings(file_path: str) -> pd.DataFrame: pass ``` # Example Input ```python file_path = \'large_dataset.csv\' ``` Output (The display behavior should match the settings given) The DataFrame columns should: 1. Display up to 15 rows. 2. Display up to 10 columns. 3. Show column names fully aligned with a width of 20 characters. 4. Have the frame expanded across the screen. 5. Display floating point values with a precision of 3 decimal places. Example Function Call ```python df = configure_display_settings(\'large_dataset.csv\') print(df) ``` # Tips - Use `pd.option_context` to temporarily set options within the function scope. - Utilize appropriate pandas functions like `pd.read_csv`, `set_option`, etc. - Ensure readability and maintainability of your code by adding necessary comments and adhering to proper coding guidelines. ```python def configure_display_settings(file_path: str) -> pd.DataFrame: import pandas as pd with pd.option_context(\'display.max_rows\', 15, \'display.max_columns\', 10, \'display.max_colwidth\', 20, \'display.expand_frame_repr\', True, \'display.precision\', 3): df = pd.read_csv(file_path) return df ``` # Note - Verify the behavior of your function by calling it and printing the DataFrame. - Ensure that the global pandas settings remain unchanged after the function execution.","solution":"def configure_display_settings(file_path: str): import pandas as pd with pd.option_context(\'display.max_rows\', 15, \'display.max_columns\', 10, \'display.max_colwidth\', 20, \'display.expand_frame_repr\', True, \'display.precision\', 3): df = pd.read_csv(file_path) return df"},{"question":"Advanced Text Manipulation with Pandas Objective: Assess your ability to perform advanced text manipulation and transformation using pandas\' `str` accessor and `StringDtype`. Problem Statement: You are given a pandas DataFrame containing text data related to customer reviews. Each review contains ratings and comments. Your task is to clean, transform, and extract meaningful information from this text data. Instructions: 1. **Data Cleaning**: - Convert all text data in the DataFrame to lowercase. - Remove all leading and trailing whitespaces from the text data. 2. **Data Transformation**: - Replace all occurrences of the word \\"not\\" with \\"n\'t\\". - Split the text data into separate columns based on spaces (assuming space-separated words). - Rename the resultant columns with a prefix \\"word_\\". 3. **Data Extraction**: - Extract the first word of each review into a separate column called `first_word`. - Extract all four-letter words from each review and store them in a column named `four_letter_words`. 4. **Bonus**: - Create dummy variables for each unique word in the reviews using the `str.get_dummies` method. Input: A pandas DataFrame named `df` with one column `review` which contains customer reviews as strings. ```python import pandas as pd # Example DataFrame data = { \\"review\\": [ \\" This product is not bad at all \\", \\"I do not like the color, but it\'s not a dealbreaker\\", \\"not the best quality, but pretty satisfactory\\" ] } df = pd.DataFrame(data, dtype=\\"string\\") ``` Expected Output: A transformed pandas DataFrame that meets all the above instructions. Constraints: - Handle missing values (NA) appropriately. Missing values should not break your transformation steps. Performance Requirements: - Ensure your code runs efficiently even with large datasets. Solution Template: ```python import pandas as pd def transform_reviews(df): # Convert all text data to lowercase df[\'review\'] = df[\'review\'].str.lower() # Remove all leading and trailing whitespaces df[\'review\'] = df[\'review\'].str.strip() # Replace \\"not\\" with \\"n\'t\\" df[\'review\'] = df[\'review\'].str.replace(r\'bnotb\', \\"n\'t\\", regex=True) # Split text data into separate columns reviews_split = df[\'review\'].str.split(expand=True) reviews_split.columns = [f\'word_{i+1}\' for i in range(reviews_split.shape[1])] # Extract the first word of each review reviews_split[\'first_word\'] = reviews_split.iloc[:, 0] # Extract all four-letter words reviews_split[\'four_letter_words\'] = df[\'review\'].str.findall(r\'bw{4}b\').apply(\',\'.join) # Create dummy variables for each unique word dummies = df[\'review\'].str.get_dummies(sep=\' \') return reviews_split, dummies # Example usage transformed_df, dummies = transform_reviews(df) print(transformed_df) print(dummies) ```","solution":"import pandas as pd def transform_reviews(df): # Convert all text data to lowercase df[\'review\'] = df[\'review\'].str.lower() # Remove all leading and trailing whitespaces df[\'review\'] = df[\'review\'].str.strip() # Replace \\"not\\" with \\"n\'t\\" df[\'review\'] = df[\'review\'].str.replace(r\'bnotb\', \\"n\'t\\", regex=True) # Split text data into separate columns reviews_split = df[\'review\'].str.split(expand=True) reviews_split.columns = [f\'word_{i+1}\' for i in range(reviews_split.shape[1])] # Extract the first word of each review reviews_split[\'first_word\'] = reviews_split.iloc[:, 0] # Extract all four-letter words reviews_split[\'four_letter_words\'] = df[\'review\'].str.findall(r\'bw{4}b\').apply(\',\'.join) # Create dummy variables for each unique word dummies = df[\'review\'].str.get_dummies(sep=\' \') return reviews_split, dummies # Example usage data = { \\"review\\": [ \\" This product is not bad at all \\", \\"I do not like the color, but it\'s not a dealbreaker\\", \\"not the best quality, but pretty satisfactory\\" ] } df = pd.DataFrame(data, dtype=\\"string\\") transformed_df, dummies = transform_reviews(df) print(transformed_df) print(dummies)"},{"question":"**Question: Shell-Like Command Parser and Executor** Implement a function `parse_and_execute(command: str) -> str` that takes a string `command` representing a shell-like command and returns the execution result as a string. The function must utilize `shlex` to: 1. Parse the command string safely. 2. Support extracting and joining tokens correctly. 3. Ensure the command is shell-escaped to prevent injections. # Requirements: - The `command` string may contain quoted strings, escape characters, and shell punctuation characters. - The function must simulate a basic execution environment: - If the command is `echo`, return the subsequent tokens joined as a string. - If the command is `upper`, return the subsequent tokens joined as a string but converted to uppercase. - The command may include nested quotes that should be handled correctly. - Use `shlex.split`, `shlex.quote`, and `shlex.join` extensively to ensure safety and correctness. - Handle and return appropriate messages for unsupported commands or parsing errors. # Input - `command` (str): A shell-like command string. # Output - Returns a string representing the result of the execution. # Constraints - You can assume the input command string will not exceed 1024 characters. - You can assume valid commands will be provided (either `echo` or `upper`). # Example ```python assert parse_and_execute(\\"echo \'Hello, World!\'\\") == \\"Hello, World!\\" assert parse_and_execute(\'upper \\"make this UPPERCASE\\"\') == \\"MAKE THIS UPPERCASE\\" assert parse_and_execute(\'echo This is a \\"complex command\\"\') == \\"This is a complex command\\" ``` Notes: - Ensure the solution uses `shlex` for parsing commands. - Extra care should be taken to handle quoting and escaping correctly to maintain security and correctness.","solution":"import shlex def parse_and_execute(command: str) -> str: Parses and executes a simple shell-like command. Supported commands: - echo: returns the subsequent tokens joined as a string. - upper: returns the subsequent tokens joined as a string but converted to uppercase. Args: command (str): The shell-like command to parse and execute. Returns: str: The result of executing the command. try: # Split the command using shlex to respect quoting and escaping lexer = shlex.shlex(command, posix=True) lexer.whitespace_split = True tokens = list(lexer) if not tokens: return \\"Error: No command provided\\" cmd = tokens[0] if cmd == \\"echo\\": return \' \'.join(tokens[1:]) elif cmd == \\"upper\\": return \' \'.join(tokens[1:]).upper() else: return f\\"Error: Unsupported command \'{cmd}\'\\" except Exception as e: return f\\"Error: {e}\\""},{"question":"**Programming Challenge:** # Title: Functional Programming with Itertools and Functools # Problem Statement: You are tasked with processing a list of customer orders. Each order is represented as a dictionary with the following keys: - \'order_id\': A unique string identifier for the order. - \'customer_id\': A unique string identifier for the customer. - \'order_amount\': A float representing the total amount of the order. - \'order_date\': A string in the format \'YYYY-MM-DD\' representing the date of the order. Your task is to implement a function `process_orders` that performs the following operations using the `itertools` and `functools` modules: 1. **Filtering Orders**: Filter out orders where the `order_amount` is less than a given threshold. 2. **Grouping Orders by Customer**: Group the remaining orders by `customer_id`. 3. **Calculating Total Spend**: Calculate the total spend per customer. 4. **Returns Top N Customers**: Return the top `N` customers based on their total spend in descending order. # Function Signature: ```python from typing import List, Dict, Any from itertools import groupby from functools import partial def process_orders(orders: List[Dict[str, Any]], amount_threshold: float, top_n: int) -> List[Dict[str, Any]]: pass ``` # Input: - `orders` (List[Dict[str, Any]]): A list of dictionaries. Each dictionary represents an order. - `amount_threshold` (float): The threshold amount to filter orders. - `top_n` (int): Number of top customers to return based on their total spend. # Output: - A list of dictionaries where each dictionary contains: - \'customer_id\': The unique identifier for the customer. - \'total_spend\': The total amount spent by the customer. # Example: ```python orders = [ {\'order_id\': \'001\', \'customer_id\': \'C1\', \'order_amount\': 150.50, \'order_date\': \'2023-01-01\'}, {\'order_id\': \'002\', \'customer_id\': \'C2\', \'order_amount\': 200.00, \'order_date\': \'2023-01-02\'}, {\'order_id\': \'003\', \'customer_id\': \'C1\', \'order_amount\': 300.00, \'order_date\': \'2023-01-03\'}, {\'order_id\': \'004\', \'customer_id\': \'C3\', \'order_amount\': 50.00, \'order_date\': \'2023-01-04\'}, {\'order_id\': \'005\', \'customer_id\': \'C2\', \'order_amount\': 120.00, \'order_date\': \'2023-01-05\'}, ] result = process_orders(orders, amount_threshold=100, top_n=2) # Expected Output: # [ # {\'customer_id\': \'C1\', \'total_spend\': 450.50}, # {\'customer_id\': \'C2\', \'total_spend\': 320.00} # ] ``` # Constraints: - `orders` list can be of any length between 0 to 10^6. - Each order dictionary will have valid data types as described. - `amount_threshold` will be a non-negative float. - `top_n` will be a non-negative integer and will not exceed the number of unique customers. # Notes: - Use `itertools` for efficient looping and grouping of orders. - Use `functools.partial` to create functions with fixed arguments where applicable. - Ensure that your solution is optimized for performance with large datasets.","solution":"from typing import List, Dict, Any from itertools import groupby from functools import reduce def process_orders(orders: List[Dict[str, Any]], amount_threshold: float, top_n: int) -> List[Dict[str, Any]]: # Step 1: Filtering Orders filtered_orders = filter(lambda x: x[\'order_amount\'] >= amount_threshold, orders) # Step 2: Grouping Orders by Customer - groupby requires the list to be sorted by \'customer_id\' sorted_orders = sorted(filtered_orders, key=lambda x: x[\'customer_id\']) # Group by \'customer_id\' grouped_orders = groupby(sorted_orders, key=lambda x: x[\'customer_id\']) # Step 3: Calculating Total Spend customer_totals = [] for customer_id, group in grouped_orders: total_spend = reduce(lambda acc, order: acc + order[\'order_amount\'], group, 0) customer_totals.append({\'customer_id\': customer_id, \'total_spend\': total_spend}) # Step 4: Return top N customers by total spend top_customers = sorted(customer_totals, key=lambda x: x[\'total_spend\'], reverse=True)[:top_n] return top_customers"},{"question":"Objective Implement a function that utilizes Python\'s Sequence and Iterator Protocols to perform operations on a list of numerical objects. Task Write a function `custom_sequence_iterator(numbers: list) -> list` that accepts a list of numerical objects and performs the following: 1. Ensures that the input conforms to the **Sequence Protocol**. 2. Uses the **Iterator Protocol** to iterate over the list. 3. For each item in the list: - If the item is a number, compute its square. - If the item is not a number, raise a `TypeError`. 4. Return a new list containing the squared values of the original list items. Input - `numbers`: A list of numerical objects (integers or floats). Output - A list of numbers where each number is the square of the corresponding number in the input list. Constraints - The function should raise a `TypeError` if any element in the list is not a numerical object. - Assume the list length will be at most 10^6. Performance - The function should be optimized to handle the maximum input size efficiently. Example ```python def custom_sequence_iterator(numbers: list) -> list: # Your implementation here # Example Usage print(custom_sequence_iterator([1, 2, 3])) # Output: [1, 4, 9] print(custom_sequence_iterator([1, 2, \'a\'])) # Raises TypeError ``` Note You may not use built-in functions like `map()` or list comprehensions to solve this problem. You must explicitly use the Sequence and Iterator protocols as discussed.","solution":"def custom_sequence_iterator(numbers: list) -> list: Accepts a list of numerical objects and returns a new list with the squares of those numbers. Raises a TypeError if any element in the list is not a number. :param numbers: list of numerical objects (integers or floats). :return: list of squared values of the original list items. if not isinstance(numbers, list): raise TypeError(\\"Input should be a list.\\") for item in numbers: if not isinstance(item, (int, float)): raise TypeError(\\"All elements in the list must be numerical objects.\\") result = [] iterator = iter(numbers) while True: try: num = next(iterator) result.append(num ** 2) except StopIteration: break return result"},{"question":"# **Coding Assessment Question** You are tasked with configuring the logging system using the `logging.config` module in Python. Your goal is to write a function `setup_logging_from_dict` that sets up a logging configuration using a dictionary. The configuration should specify a console handler that displays logs on the terminal and a file handler that logs messages to a specified file. Follow the schema and guidelines provided below. Function Requirements **Function Name**: `setup_logging_from_dict` **Input**: - `log_file_path` (str): The path to the file where logs should be written. **Output**: - This function does not return anything. It sets up the logging configuration. **Constraints**: - The console handler should use a formatter that outputs log messages in the format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. - The file handler should use a formatter that outputs log messages in the format: `%(asctime)s - %(levelname)s - %(message)s`. - Both handlers should log messages of level `INFO` and higher. - Assume the logging configuration is not incremental and should replace any existing loggers. - Existing loggers should be disabled. Additional Requirements: - The function should handle any errors that occur during the configuration appropriately by using a try-except block. - Make sure to import necessary modules from `logging` and `logging.config`. Example Usage ```python log_file_path = \\"application.log\\" setup_logging_from_dict(log_file_path) import logging logger = logging.getLogger(\\"my_logger\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") ``` Guidelines - Use the `logging.config.dictConfig` method to configure the logging. - The dictionary schema provided in the documentation should be followed closely. Implementation ```python import logging import logging.config def setup_logging_from_dict(log_file_path): config_dict = { \'version\': 1, \'disable_existing_loggers\': True, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'file\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'console\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'file\', \'filename\': log_file_path, \'mode\': \'a\' # Append to the file } }, \'loggers\': { \'\': { # root logger \'level\': \'INFO\', \'handlers\': [\'console\', \'file\'] } } } try: logging.config.dictConfig(config_dict) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\") # Example usage: if __name__ == \\"__main__\\": log_file_path = \\"application.log\\" setup_logging_from_dict(log_file_path) logger = logging.getLogger(\\"my_logger\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") ```","solution":"import logging import logging.config def setup_logging_from_dict(log_file_path): Set up logging configuration using a dictionary. :param log_file_path: Path to the file where logs should be written. config_dict = { \'version\': 1, \'disable_existing_loggers\': True, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'file\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'console\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'file\', \'filename\': log_file_path, \'mode\': \'a\' } }, \'root\': { \'level\': \'INFO\', \'handlers\': [\'console\', \'file\'] } } try: logging.config.dictConfig(config_dict) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\")"},{"question":"You are required to create a generic container class using Python\'s type hinting and generics capabilities. This container should be able to store any type of data and provide basic functionalities such as adding an item, removing an item, and retrieving an item by its index. Requirements: 1. **Class Definition**: - Class Name: `GenericContainer` - It should be a generic class, parametric on a single type variable `T`. 2. **Methods**: - `add_item(item: T) -> None`: Adds an item to the container. - `remove_item(item: T) -> None`: Removes an item from the container. - `get_item(index: int) -> T`: Retrieves an item at the given index. 3. **Attributes**: - Internal storage for items, you may choose an appropriate data structure (e.g., list). 4. **Type Hinting**: - Use the appropriate type hinting to ensure that the class and methods are generic. 5. **Constraints**: - The container should only store items of the same type. - If an item to be removed does not exist, raise a `ValueError`. 6. **Performance**: - Efficiently handle basic operations. Operations like add and get should be O(1) on average assuming typical list operations. # Example Usage: ```python from typing import TypeVar, Generic T = TypeVar(\'T\') class GenericContainer(Generic[T]): def __init__(self): self._items = [] def add_item(self, item: T) -> None: self._items.append(item) def remove_item(self, item: T) -> None: self._items.remove(item) def get_item(self, index: int) -> T: return self._items[index] # Example usage: int_container = GenericContainer[int]() int_container.add_item(10) int_container.add_item(20) print(int_container.get_item(0)) # Output: 10 int_container.remove_item(10) print(int_container.get_item(0)) # Output: 20 str_container = GenericContainer[str]() str_container.add_item(\\"a\\") str_container.add_item(\\"b\\") print(str_container.get_item(1)) # Output: \\"b\\" ``` Ensure your implementation handles different types correctly and raises appropriate exceptions when constraints are violated.","solution":"from typing import TypeVar, Generic, List T = TypeVar(\'T\') class GenericContainer(Generic[T]): def __init__(self): self._items: List[T] = [] def add_item(self, item: T) -> None: self._items.append(item) def remove_item(self, item: T) -> None: if item not in self._items: raise ValueError(\\"Item not found in the container\\") self._items.remove(item) def get_item(self, index: int) -> T: return self._items[index]"},{"question":"# Advanced Coding Assessment: Combining Threads and Queues Objective: Implement a function that simulates a multi-threaded task processing system using the `queue` module. This will test your understanding of multi-threading, queue operations, and the ability to manage concurrent tasks effectively. Problem Statement: You are tasked with creating a function `simulate_task_processing` that performs the following steps: 1. Creates a FIFO `Queue` to hold task items. 2. Spawns a specified number of worker threads that will process the tasks from this queue. 3. Each worker thread should: - Retrieve tasks from the queue and print the task with a specific worker identification. - Signal that the task is done after processing. 4. Main thread should: - Add a specified number of tasks to the queue. - Wait until all tasks have been processed. Function Signature: ```python def simulate_task_processing(num_workers: int, num_tasks: int) -> None: pass ``` Input: - `num_workers` (int): The number of worker threads to create. - `num_tasks` (int): The number of tasks to be added to the queue. Output: - The function does not return anything. - Each worker should print the message when processing a task: ``` Worker <worker_id> processing task <task_id> ``` where `<worker_id>` is a unique identifier for each worker (starting from 1) and `<task_id>` is the ID of the task being processed. Constraints: - You can assume `num_workers` and `num_tasks` are positive integers. - Use the `Queue` class from the `queue` module for managing the tasks. - Ensure the entire process is thread-safe. Example: ```python simulate_task_processing(3, 5) ``` Possible output (order may vary): ``` Worker 1 processing task 0 Worker 2 processing task 1 Worker 3 processing task 2 Worker 1 processing task 3 Worker 2 processing task 4 All tasks have been processed. ``` Additional Requirements: - Ensure that the function waits and confirms all tasks are processed before returning. - Use appropriate exception handling for queue operations where necessary.","solution":"from queue import Queue from threading import Thread def worker(worker_id, task_queue): while True: task_id = task_queue.get() if task_id is None: # Sentinel to break the loop for stopping the worker break print(f\\"Worker {worker_id} processing task {task_id}\\") task_queue.task_done() def simulate_task_processing(num_workers: int, num_tasks: int) -> None: task_queue = Queue() # Create and start worker threads threads = [] for i in range(num_workers): thread = Thread(target=worker, args=(i+1, task_queue)) thread.start() threads.append(thread) # Add tasks to the queue for task_id in range(num_tasks): task_queue.put(task_id) # Wait for all tasks to be processed task_queue.join() # Stop workers for _ in range(num_workers): task_queue.put(None) # Send a sentinel to stop the workers for thread in threads: thread.join() print(\\"All tasks have been processed.\\")"},{"question":"# Multiprocessing-Based Web Scraper with Data Aggregation Objective You need to write a Python script using the `multiprocessing` package to create a web scraper that fetches data from multiple URLs concurrently and aggregates the results. Task Description 1. **Create a Function to Fetch Data**: - Implement a function `fetch_data(url: str) -> str` that takes a URL as input and returns the HTML content of the page as a string. - Use the `requests` library to perform the HTTP GET requests. 2. **Concurrent Fetching with Multiprocessing**: - Create a function `concurrent_fetch(urls: List[str], num_workers: int) -> List[str]` that takes a list of URLs and the number of worker processes as inputs. - Use the `Pool` class from `multiprocessing` to fetch all URLs concurrently. - The function should return a list of HTML contents corresponding to each URL. 3. **Aggregate Data**: - Implement a function `aggregate_data(html_contents: List[str]) -> Dict[str, int]` that takes a list of HTML contents and returns a dictionary where the keys are HTML tags (like `<div>`, `<a>`, etc.) and the values are their counts of occurrences across all HTML contents. 4. **Main Function and Execution**: - Implement a main function that reads a list of URLs from a file named `urls.txt`, calls `concurrent_fetch` to fetch the data concurrently, and then calls `aggregate_data` to aggregate the data and print the results in a readable format. Input and Output - **Input**: A file named `urls.txt` containing one URL per line. - **Output**: Print the aggregated data showing the count of each HTML tag. Constraints - Use the `multiprocessing` package effectively to parallelize the fetching process. - Handle possible exceptions that might occur during HTTP requests (e.g., timeouts, failed requests). - Ensure the script is well-structured and modular, with clear and reusable functions. - Your solution should consider performance and be able to scale for a substantial number of URLs. Example Assume `urls.txt` contains: ``` http://example.com http://example.org ``` The expected output could be (note: actual counts may vary depending on the content of the provided URLs): ```python { \\"html\\": 2, \\"div\\": 10, \\"a\\": 20, ... } ``` Guidelines - Ensure that the script is documented, and each function has a clear docstring explaining its purpose. - Perform the fetching inside `if __name__ == \'__main__\'` to prevent issues when using multiprocessing. - You are encouraged to use any additional libraries if needed (e.g., BeautifulSoup for parsing HTML). Hints - Use the `requests` library for HTTP GET requests (`pip install requests`). - Use BeautifulSoup (`bs4`) for parsing HTML content (`pip install beautifulsoup4`). ```python # Your code starts here ```","solution":"import requests from bs4 import BeautifulSoup from multiprocessing import Pool from typing import List, Dict def fetch_data(url: str) -> str: Fetch HTML content from a given URL. Parameters: url (str): The URL to fetch data from. Returns: str: HTML content of the page. try: response = requests.get(url, timeout=10) response.raise_for_status() # Raise an error for bad status return response.text except requests.RequestException as e: print(f\\"Error fetching {url}: {e}\\") return \\"\\" def concurrent_fetch(urls: List[str], num_workers: int) -> List[str]: Fetch all URLs concurrently using multiprocessing. Parameters: urls (List[str]): List of URLs to fetch data from. num_workers (int): Number of worker processes to use. Returns: List[str]: List of HTML contents corresponding to each URL. with Pool(num_workers) as pool: results = pool.map(fetch_data, urls) return results def aggregate_data(html_contents: List[str]) -> Dict[str, int]: Aggregate the data by counting HTML tags across all HTML contents. Parameters: html_contents (List[str]): List of HTML contents. Returns: Dict[str, int]: Dictionary with HTML tags as keys and their counts as values. tag_counts = dict() for content in html_contents: if content: soup = BeautifulSoup(content, \'html.parser\') tags = [tag.name for tag in soup.find_all()] for tag in tags: if tag in tag_counts: tag_counts[tag] += 1 else: tag_counts[tag] = 1 return tag_counts def main(): Main function to read URLs from a file, fetch data concurrently, and aggregate the results. with open(\'urls.txt\', \'r\') as file: urls = file.read().splitlines() # Fetching URL data concurrently html_contents = concurrent_fetch(urls, num_workers=4) # Aggregating data aggregated_data = aggregate_data(html_contents) # Printing the results for tag, count in aggregated_data.items(): print(f\\"{tag}: {count}\\") if __name__ == \'__main__\': main()"},{"question":"# Advanced Text Processing with difflib and regex in Python Objective Implement a function that compares two blocks of text and identifies certain patterns. Problem Statement You need to write a function `find_patterns_and_diff(text1: str, text2: str, pattern: str) -> dict` that takes two blocks of text (`text1` and `text2`) and a regular expression pattern (`pattern`). Your function will: 1. **Find All Matches**: Identify all occurrences of `pattern` in both `text1` and `text2` using regular expressions. 2. **Find Differences**: Compare `text1` and `text2` using the `difflib.SequenceMatcher` and list the changes in terms of additions, deletions, and modifications. 3. **Return Results**: Produce a dictionary with: - `matches_text1`: The list of all matches of `pattern` found in `text1`. - `matches_text2`: The list of all matches of `pattern` found in `text2`. - `differences`: A summary of sequences added, deleted, or changed between `text1` and `text2`. Input Format - `text1`: A string representing the first block of text. - `text2`: A string representing the second block of text. - `pattern`: A string representing the regular expression pattern to search for. Output Format A dictionary with the following structure: ```python { \\"matches_text1\\": list of matches, \\"matches_text2\\": list of matches, \\"differences\\": list of differences } ``` Constraints - Python `re` and `difflib` modules should be used. - Ensure efficient handling of large text through optimal use of regular expressions and the differences calculation. - The differences should be clear and human-readable for better understanding. Function Signature ```python def find_patterns_and_diff(text1: str, text2: str, pattern: str) -> dict: pass ``` Example ```python text1 = \\"The quick brown fox jumps over the lazy dog. The quick brown fox is very quick.\\" text2 = \\"The quick red fox jumps over the lazy dog. The quick brown fox was very fast.\\" pattern = r\\"bquickb\\" result = find_patterns_and_diff(text1, text2, pattern) # Expected Output { \\"matches_text1\\": [\\"quick\\", \\"quick\\", \\"quick\\"], \\"matches_text2\\": [\\"quick\\", \\"quick\\"], \\"differences\\": [ \\"diff text1 a 10 quick brown fox\\", \\"diff text2 a 10 quick red fox\\", \\"diff text1 b 35 quick.\\", \\"diff text2 b 35 fast.\\" ] } ``` Explanation - The `matches_text1` contains all the occurrences of the word \\"quick\\" in `text1`. - The `matches_text2` contains all the occurrences of the word \\"quick\\" in `text2`. - The `differences` provide a human-readable summary of differences identified by the `difflib.SequenceMatcher`.","solution":"import re import difflib def find_patterns_and_diff(text1: str, text2: str, pattern: str) -> dict: # Finding all matches using the regex pattern matches_text1 = re.findall(pattern, text1) matches_text2 = re.findall(pattern, text2) # Comparing text1 and text2 to find differences diff = difflib.ndiff(text1.split(), text2.split()) differences = [d for d in diff if d.startswith(\'- \') or d.startswith(\'+ \')] return { \\"matches_text1\\": matches_text1, \\"matches_text2\\": matches_text2, \\"differences\\": differences }"},{"question":"You are provided with a dataset that represents some sample measurements. Your task is to create various plots using the seaborn library to visualize these measurements along with their error bars. You need to demonstrate your understanding of different methods for calculating and displaying error bars as discussed in the seaborn library. Input - A list of numerical values representing measurements. Requirements 1. Create a plot visualizing these measurements with error bars representing the standard deviation. 2. Create a plot visualizing these measurements with error bars representing the 95th percentile interval. 3. Create a plot visualizing these measurements with error bars representing the standard error. 4. Create a plot visualizing these measurements with error bars representing a 95% confidence interval using bootstrapping. 5. Implement a custom error bar function to show the full range from the minimum to the maximum value of the data. Constraints - Use Python and the seaborn library for plotting. - Ensure the visualizations are clear and properly labeled. - Each plot type must be accompanied by a respective title indicating the type of error bars used. Example ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Sample data data = np.random.normal(0, 1, 100) # Plot with standard deviation error bars sns.pointplot(x=data, errorbar=\\"sd\\") plt.title(\\"Standard Deviation Error Bars\\") plt.show() # Plot with 95th percentile interval error bars sns.pointplot(x=data, errorbar=(\\"pi\\", 95)) plt.title(\\"95th Percentile Interval Error Bars\\") plt.show() # Plot with standard error error bars sns.pointplot(x=data, errorbar=\\"se\\") plt.title(\\"Standard Error Error Bars\\") plt.show() # Plot with 95% confidence interval error bars sns.pointplot(x=data, errorbar=\\"ci\\") plt.title(\\"95% Confidence Interval Error Bars\\") plt.show() # Custom error bars showing full range (min to max) sns.pointplot(x=data, errorbar=lambda x: (x.min(), x.max())) plt.title(\\"Custom Error Bars: Full Range (Min to Max)\\") plt.show() ``` Implement the above requirements in your code and ensure the plots are displayed correctly. Submit your code along with the generated plots.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Sample data data = np.random.normal(0, 1, 100) # Plot with standard deviation error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=data, errorbar=\\"sd\\") plt.title(\\"Standard Deviation Error Bars\\") plt.xlabel(\\"Measurement Index\\") plt.ylabel(\\"Measurement Value\\") plt.xticks(np.arange(0, len(data), step=5)) plt.show() # Plot with 95th percentile interval error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=data, errorbar=(\\"pi\\", 95)) plt.title(\\"95th Percentile Interval Error Bars\\") plt.xlabel(\\"Measurement Index\\") plt.ylabel(\\"Measurement Value\\") plt.xticks(np.arange(0, len(data), step=5)) plt.show() # Plot with standard error error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=data, errorbar=\\"se\\") plt.title(\\"Standard Error Error Bars\\") plt.xlabel(\\"Measurement Index\\") plt.ylabel(\\"Measurement Value\\") plt.xticks(np.arange(0, len(data), step=5)) plt.show() # Plot with 95% confidence interval error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=data, errorbar=\\"ci\\") plt.title(\\"95% Confidence Interval Error Bars\\") plt.xlabel(\\"Measurement Index\\") plt.ylabel(\\"Measurement Value\\") plt.xticks(np.arange(0, len(data), step=5)) plt.show() # Custom error bars showing full range (min to max) plt.figure(figsize=(10, 6)) sns.pointplot(x=data, errorbar=lambda x: (x.min(), x.max())) plt.title(\\"Custom Error Bars: Full Range (Min to Max)\\") plt.xlabel(\\"Measurement Index\\") plt.ylabel(\\"Measurement Value\\") plt.xticks(np.arange(0, len(data), step=5)) plt.show()"},{"question":"**Advanced Python Coding Assessment** # Problem Statement You are required to implement an asynchronous Python function that executes multiple shell commands in parallel, captures their standard output and error streams, and returns the results in an organized manner. This function should also handle cases where the commands fail, ensuring that any error messages are captured and included in the results. # Function Signature ```python import asyncio from typing import List, Dict async def run_commands(commands: List[str]) -> List[Dict[str, str]]: Execute a list of shell commands in parallel and capture their outputs and errors. Parameters: commands (List[str]): A list of shell commands to be executed. Returns: List[Dict[str, str]]: A list of dictionaries, each containing the following key-value pairs: \'command\': The shell command that was executed. \'stdout\': The standard output of the command. \'stderr\': The error output of the command. \'returncode\': The return code indicating success (0) or failure (non-zero). Example: input: [\\"ls\\", \\"echo Hello World\\", \\"invalid_command\\"] output: [ {\'command\': \'ls\', \'stdout\': \'file1nfile2n\', \'stderr\': \'\', \'returncode\': 0}, {\'command\': \'echo Hello World\', \'stdout\': \'Hello Worldn\', \'stderr\': \'\', \'returncode\': 0}, {\'command\': \'invalid_command\', \'stdout\': \'\', \'stderr\': \'invalid_command: command not found\', \'returncode\': 127} ] # Your implementation here # Example usage async def main(): commands = [\\"ls\\", \\"echo Hello World\\", \\"invalid_command\\"] results = await run_commands(commands) for result in results: print(result) # Running the main function asyncio.run(main()) ``` # Specifications 1. **Input** - A list of shell commands to be executed (e.g., `[\\"ls\\", \\"echo Hello World\\", \\"invalid_command\\"]`). 2. **Output** - A list of dictionaries, where each dictionary contains: - `\'command\'`: The shell command that was executed. - `\'stdout\'`: The standard output captured from the command. - `\'stderr\'`: The standard error captured from the command. - `\'returncode\'`: The return code of the command (0 for success, non-zero for failure). 3. **Constraints** - Ensure that standard output and error are correctly captured for each command. - Handle shell commands that may fail and capture any error messages. - Commands should be executed in parallel to improve efficiency. # Evaluation Criteria - Correct implementation of asynchronous execution using asyncio. - Proper handling of standard output and error streams. - Accurate capturing and returning of command execution results. - Robust error handling and appropriate use of asyncio subprocess methods.","solution":"import asyncio from typing import List, Dict async def run_command(command: str) -> Dict[str, str]: Execute a single shell command and capture its outputs and errors. Parameters: command (str): The shell command to be executed. Returns: Dict[str, str]: A dictionary containing the command, its stdout, stderr, and return code. process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return { \'command\': command, \'stdout\': stdout.decode().strip(), \'stderr\': stderr.decode().strip(), \'returncode\': process.returncode } async def run_commands(commands: List[str]) -> List[Dict[str, str]]: Execute a list of shell commands in parallel and capture their outputs and errors. Parameters: commands (List[str]): A list of shell commands to be executed. Returns: List[Dict[str, str]]: A list of dictionaries, each containing the following key-value pairs: \'command\': The shell command that was executed. \'stdout\': The standard output of the command. \'stderr\': The error output of the command. \'returncode\': The return code indicating success (0) or failure (non-zero). tasks = [run_command(command) for command in commands] results = await asyncio.gather(*tasks) return results # Example usage async def main(): commands = [\\"ls\\", \\"echo Hello World\\", \\"invalid_command\\"] results = await run_commands(commands) for result in results: print(result) # Running the main function (if needed, for example using: asyncio.run(main()))"},{"question":"# Advanced Python Iterators In this exercise, you need to implement a custom iterator in Python that mimics the behavior described in the provided documentation. Specifically, you will create two types of iterators: 1. **SequenceIterator**: An iterator that works with an arbitrary sequence and ends iteration when it encounters an `IndexError`. 2. **CallableIterator**: An iterator that works with a callable object and a sentinel value, ending iteration when the callable returns the sentinel value. Requirements: - Implement a class `SequenceIterator` that accepts a sequence and provides an iterator interface. - Implement a class `CallableIterator` that accepts a callable object and a sentinel value, providing an iterator interface that stops when the callable returns the sentinel value. Class Definitions: 1. **SequenceIterator** - **__init__**: Takes a sequence as an argument. - **__iter__**: Returns the iterator object itself. - **__next__**: Returns the next item in the sequence or raises `StopIteration` if the end of the sequence is reached. 2. **CallableIterator** - **__init__**: Takes a callable object and a sentinel value as arguments. - **__iter__**: Returns the iterator object itself. - **__next__**: Calls the callable object and returns its result until the result equals the sentinel value, at which point it raises `StopIteration`. Input and Output: 1. **SequenceIterator** - Takes any sequence (list, tuple, etc.). - Iterates over the sequence and yields each element. ```python seq = [1, 2, 3] it = SequenceIterator(seq) for item in it: print(item) # Output: 1 2 3 ``` 2. **CallableIterator** - Takes a callable object and a sentinel value. - Calls the callable and yields results until the sentinel value is returned. ```python counter = iter(range(3)) sentinel = 2 it = CallableIterator(counter.__next__, sentinel) for item in it: print(item) # Output: 0 1 ``` Constraints: - Assume that the sequence and the callable provided will always be valid (no need for extensive error handling beyond what\'s necessary for iteration). - The callable provided will always be a function with no parameters. Performance: - The iterators should have a similar performance to built-in iterators. - Efficiently handle large sequences and repeated callable invocations. Implementation: ```python class SequenceIterator: def __init__(self, sequence): self._sequence = sequence self._index = 0 def __iter__(self): return self def __next__(self): if self._index < len(self._sequence): result = self._sequence[self._index] self._index += 1 return result else: raise StopIteration class CallableIterator: def __init__(self, callable, sentinel): self._callable = callable self._sentinel = sentinel def __iter__(self): return self def __next__(self): value = self._callable() if value == self._sentinel: raise StopIteration return value ``` Implement and test the `SequenceIterator` and `CallableIterator` classes based on the given examples.","solution":"class SequenceIterator: def __init__(self, sequence): self._sequence = sequence self._index = 0 def __iter__(self): return self def __next__(self): if self._index < len(self._sequence): result = self._sequence[self._index] self._index += 1 return result else: raise StopIteration class CallableIterator: def __init__(self, callable, sentinel): self._callable = callable self._sentinel = sentinel def __iter__(self): return self def __next__(self): value = self._callable() if value == self._sentinel: raise StopIteration return value"},{"question":"# Time Series Data Manipulation with Pandas You are tasked with implementing a function that performs various time series operations using pandas. Your function should read time series data from a CSV file, localize the date time to a specific timezone, resample the data to a different frequency, and handle any missing data. Your function should perform the following steps: 1. **Reading the CSV File**: Read the CSV file that contains time series data. The CSV file has two columns: \\"date\\" and \\"value\\", where \\"date\\" is in the format `YYYY-MM-DD HH:MM:SS`. 2. **Localize Timezone**: Localize the \\"date\\" column to the specified timezone. 3. **Resample Data**: Resample the \\"value\\" column from the original frequency to a specified new frequency. Aggregate the resampled data using the mean. 4. **Handle Missing Data**: Handle any missing data by forward-filling the missing values. 5. **Return the Final DataFrame**: Return a DataFrame with the resampled and timezone localized data. # Function Signature ```python import pandas as pd def process_time_series(csv_file: str, timezone: str, new_frequency: str) -> pd.DataFrame: pass ``` # Input - `csv_file` (str): Path to the CSV file containing the time series data. - `timezone` (str): The timezone to localize the \\"date\\" column to (e.g., \\"UTC\\", \\"US/Pacific\\", etc.). - `new_frequency` (str): The new frequency to which the data should be resampled. This could be any valid pandas date offset alias (e.g., \'5T\' for 5 minutes, \'D\' for daily, etc.). # Output - A pandas DataFrame with two columns: \\"date\\" and \\"value\\", where \\"date\\" is localized to the specified timezone and the \\"value\\" column is resampled to the specified frequency, with missing values forward-filled. # Constraints - The function must handle large files efficiently. - The input CSV is assumed to have a well-defined structure without corrupt data. - The CSV file size can be large, so the function should be optimized for performance. # Example ```python # Sample input CSV content: # date,value # 2023-01-01 00:00:00,100 # 2023-01-01 01:00:00,150 # 2023-01-01 02:00:00,200 result = process_time_series(\\"sample.csv\\", \\"US/Pacific\\", \\"30T\\") print(result) ``` # Expected Output The expected output would be a DataFrame where the \\"date\\" column is localized to \'US/Pacific\' and resampled to 30-minute intervals, with missing values forward-filled. Ensure your function handles edge cases appropriately, such as missing or malformed data.","solution":"import pandas as pd def process_time_series(csv_file: str, timezone: str, new_frequency: str) -> pd.DataFrame: Processes a time series from a CSV file: 1. Reads the CSV file 2. Localizes the \'date\' column to the given timezone 3. Resamples the \'value\' column to the specified frequency 4. Handles missing values by forward-filling Args: - csv_file (str): Path to the CSV file containing the time series data. - timezone (str): The timezone to localize the \'date\' column to. - new_frequency (str): The new frequency to which the data should be resampled. Returns: - pd.DataFrame: A DataFrame with the resampled and localized data. # Read the CSV file df = pd.read_csv(csv_file, parse_dates=[\'date\']) # Localize the \'date\' column to the specified timezone df[\'date\'] = df[\'date\'].dt.tz_localize(\'UTC\').dt.tz_convert(timezone) # Set the \'date\' column as the index df.set_index(\'date\', inplace=True) # Resample the \'value\' column to the specified frequency and aggregate using mean resampled_df = df.resample(new_frequency).mean() # Handle missing data by forward-filling resampled_df = resampled_df.ffill() # Reset the index to make \'date\' a column again resampled_df.reset_index(inplace=True) return resampled_df"},{"question":"**Title: Implementing a Custom Scikit-learn Compatible Estimator** Objective: Create a custom scikit-learn compatible estimator that implements a specific linear regression model. The estimator should validate input data, apply an efficient linear algebra operation for fitting, and handle sparse matrices if provided. Specifications: 1. **Class Name**: `CustomLinearRegression` 2. **Methods to Implement**: - `__init__(self, normalize=False, random_state=None)`: Constructor to initialize the estimator. - `fit(self, X, y)`: Method to fit the linear regression model. - `predict(self, X)`: Method to make predictions using the fitted model. - `get_params(self, deep=True)`: Get parameters for the estimator. - `set_params(self, **params)`: Set parameters for the estimator. 3. **Input**: - `X` (numpy array or scipy sparse matrix): Feature matrix. - `y` (numpy array): Target vector. 4. **Output**: - For `fit`: The instance of the fitted estimator. - For `predict`: Predictions as a numpy array. 5. **Constraints**: - Validate `X` and `y` using appropriate validation tools. - Handle both dense and sparse matrices. - Ensure repeatability by using a `random_state` parameter. 6. **Performance Requirements**: - Efficient handling of large input data using appropriate functions. Implementation Steps: 1. **Validation**: - Use `check_X_y` to validate `X` and `y` inputs in `fit` method. - Use `check_array` to validate `X` input in `predict` method. - Ensure all input values are finite using `assert_all_finite`. 2. **Fit Method**: - Use `extmath.safe_sparse_dot` for matrix operations to ensure compatibility with sparse matrices. - Implement normalization if the `normalize` parameter is set to `True`. 3. **Predict Method**: - Use the fitted coefficients to make predictions on new data. 4. **Parameter Management**: - Implement `get_params` and `set_params` to manage estimator parameters. # Example Usage: ```python from sklearn.utils import check_X_y, check_array, extmath import numpy as np class CustomLinearRegression: def __init__(self, normalize=False, random_state=None): self.normalize = normalize self.random_state = random_state self.coef_ = None self.intercept_ = None def fit(self, X, y): X, y = check_X_y(X, y, accept_sparse=[\'csr\', \'csc\', \'coo\']) if self.normalize: X = extmath.density(X) # Fit the model: (Simple pseudo-code for example) self.coef_ = np.linalg.lstsq(X, y, rcond=None)[0] return self def predict(self, X): X = check_array(X, accept_sparse=[\'csr\', \'csc\', \'coo\']) return extmath.safe_sparse_dot(X, self.coef_) + self.intercept_ def get_params(self, deep=True): return {\'normalize\': self.normalize, \'random_state\': self.random_state} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self # Example usage X = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) model = CustomLinearRegression(normalize=True) model.fit(X, y) print(model.predict(np.array([[7, 8]]))) ``` **Note**: This is a simplified example. Students are expected to implement the full logic as per the detailed specifications.","solution":"from sklearn.utils import check_X_y, check_array, extmath import numpy as np import scipy.sparse as sp class CustomLinearRegression: def __init__(self, normalize=False, random_state=None): self.normalize = normalize self.random_state = random_state self.coef_ = None self.intercept_ = None def fit(self, X, y): X, y = check_X_y(X, y, accept_sparse=[\'csr\', \'csc\', \'coo\']) if self.normalize: X = extmath.safe_sparse_dot(X, np.ones((X.shape[1], 1))) / X.shape[1] # Adding intercept to X X_ext = sp.hstack([np.ones((X.shape[0], 1)), X]).toarray() if sp.issparse(X) else np.hstack([np.ones((X.shape[0], 1)), X]) # Calculate coefficients using lstsq self.coef_, _, _, _ = np.linalg.lstsq(X_ext, y, rcond=None) return self def predict(self, X): X = check_array(X, accept_sparse=[\'csr\', \'csc\', \'coo\']) # Adding intercept to X X_ext = sp.hstack([np.ones((X.shape[0], 1)), X]).toarray() if sp.issparse(X) else np.hstack([np.ones((X.shape[0], 1)), X]) return extmath.safe_sparse_dot(X_ext, self.coef_) def get_params(self, deep=True): return {\'normalize\': self.normalize, \'random_state\': self.random_state} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self"},{"question":"# Asynchronous Web Server Simulation Objective Implement a simple asynchronous web server that can handle multiple client connections concurrently using the `asyncio` module. The server should be able to process HTTP GET requests and respond with an appropriate message. Task 1. **Create a Server**: - Write a function `start_server()` that starts an HTTP server using the `asyncio` module. - The server should listen on `localhost` and port `8080`. 2. **Handle Clients**: - Implement a function `handle_client(reader, writer)` that processes incoming client connections. - When a client connects, the server should read the request and ensure it\'s a valid HTTP GET request. - If the request is valid, respond with a simple HTML message, e.g., `<html><body><h1>Hello, World!</h1></body></html>`. - If the request is not valid, respond with a `400 Bad Request` status code. 3. **Concurrency**: - Ensure that the server can handle multiple connections concurrently using asyncio\'s event loop. Constraints - The solution should not use any third-party libraries apart from `asyncio`. - Proper error handling must be provided to ensure the server doesn\'t crash on bad requests or other errors. - The server should run indefinitely until manually stopped. Input and Output Specifications - No input besides incoming HTTP connections on `localhost:8080`. - Outputs response messages to connected clients. Hints - Use `asyncio.start_server()` to create the server. - Use the `asyncio.StreamReader` and `asyncio.StreamWriter` for reading from and writing to the client sockets. - Use `stream_reader.readuntil()` for reading the request headers. Example ```python import asyncio async def handle_client(reader, writer): data = await reader.readuntil(b\'rnrn\') message = data.decode() method, path, version = message.split(\' \')[:3] if method != \'GET\': response = \'HTTP/1.1 400 Bad Requestrnrn\' writer.write(response.encode()) else: response = \'HTTP/1.1 200 OKrnContent-Type: text/htmlrnrn<html><body><h1>Hello, World!</h1></body></html>\' writer.write(response.encode()) await writer.drain() writer.close() async def start_server(): server = await asyncio.start_server(handle_client, \'localhost\', 8080) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(start_server()) ``` In this example, the server will read the incoming HTTP GET requests and respond with a simple HTML message, ensuring the handling of concurrent connections using asyncio.","solution":"import asyncio async def handle_client(reader, writer): try: data = await reader.readuntil(b\'rnrn\') message = data.decode() request_line = message.split(\\"rn\\")[0] method, path, version = request_line.split() if method == \'GET\': response = \'HTTP/1.1 200 OKrnContent-Type: text/htmlrnrn<html><body><h1>Hello, World!</h1></body></html>\' else: response = \'HTTP/1.1 400 Bad Requestrnrn\' writer.write(response.encode()) await writer.drain() except Exception as e: response = \'HTTP/1.1 400 Bad Requestrnrn\' writer.write(response.encode()) await writer.drain() finally: writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'localhost\', 8080) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(start_server())"},{"question":"**Objective:** In this exercise, you will demonstrate your understanding of Python\'s `builtins` module, file handling, function wrapping, and class encapsulation. **Problem Statement:** You are required to implement a function `wrap_open_with_logging()` that wraps the built-in `open()` function to provide additional functionality: logging each file open operation to a log file called `file_operations.log`. Additionally, create a class `FileLogger` that will be used as the custom file object, which will log every read operation performed on it. # Requirements: 1. The `wrap_open_with_logging(path, mode)` function should: - Use the built-in `open()` to open the file specified by `path` with the given `mode`. - Write an entry to the `file_operations.log` file each time a file is opened, specifying the path of the file and the mode. 2. The `FileLogger` class should: - Take an opened file object in its constructor. - Implement a `read()` method that: - Logs the `read()` operation to `file_operations.log` file with the number of bytes read. - Returns the result of the built-in file object\'s `read()` method. - Implement any other necessary methods to mimic the behavior of a standard file object as closely as possible. # Constraints: - Ensure all logs are appended to the `file_operations.log` file. - The wrapped `open` function and `FileLogger` class should handle any file operations (`read`, `write`, etc.) as appropriate for the file mode. - Ensure that the solution handles both text and binary modes of file operation. # Example Usage: ```python # Assuming the functionality is implemented in file_logger.py from file_logger import wrap_open_with_logging f = wrap_open_with_logging(\\"test_file.txt\\", \\"r\\") content = f.read(10) f.close() # After the above lines execute, the \'file_operations.log\' file should contain: # OPEN - test_file.txt, mode: r # READ - test_file.txt, bytes: 10 ``` # Submission: Submit the `file_logger.py` file containing the implementation of `wrap_open_with_logging()` and `FileLogger` class.","solution":"def wrap_open_with_logging(path, mode): Wraps the built-in open function to log each file open operation. def log_open_operation(path, mode): with open(\'file_operations.log\', \'a\') as log_file: log_file.write(f\\"OPEN - {path}, mode: {mode}n\\") file = open(path, mode) log_open_operation(path, mode) return FileLogger(file, path) class FileLogger: def __init__(self, file, path): self.file = file self.path = path def read(self, size=-1): data = self.file.read(size) self._log_read_operation(size, len(data)) return data def write(self, data): num_bytes_written = self.file.write(data) self._log_write_operation(len(data)) return num_bytes_written def close(self): self.file.close() def _log_read_operation(self, requested_size, read_size): with open(\'file_operations.log\', \'a\') as log_file: log_file.write(f\\"READ - {self.path}, requested bytes: {requested_size}, read bytes: {read_size}n\\") def _log_write_operation(self, size): with open(\'file_operations.log\', \'a\') as log_file: log_file.write(f\\"WRITE - {self.path}, bytes: {size}n\\") def __getattr__(self, attr): return getattr(self.file, attr)"},{"question":"**Objective:** Evaluate your understanding of the seaborn package, specifically focusing on residual plots to assess and diagnose linear regression models. **Problem Statement:** You are provided with a dataset `mpg`, which contains several attributes of various cars including their miles per gallon (mpg), weight, and horsepower. Your task is to create a function that will generate residual plots using different configurations and inspect the quality of linear regression fits. # Function Signature: ```python def generate_residual_plots(data): This function takes a pandas DataFrame `data` and generates four residual plots: 1. A basic residual plot of \'weight\' vs \'displacement\'. 2. A residual plot of \'horsepower\' vs \'mpg\'. 3. A residual plot of \'horsepower\' vs \'mpg\' using a second-order polynomial. 4. A residual plot of \'horsepower\' vs \'mpg\' with a LOWESS curve. The function should not return anything but should display the plots. Parameters: data (pd.DataFrame): The input DataFrame with car attributes. ``` # Input: - The function takes one input: - `data`: A pandas DataFrame containing the car dataset. # Expected Output: - The function should create and display four residual plots using seaborn: 1. Basic residual plot of `weight` vs `displacement`. 2. Residual plot of `horsepower` vs `mpg`. 3. Residual plot of `horsepower` vs `mpg` using a second-order polynomial (`order=2`). 4. Residual plot of `horsepower` vs `mpg` with a LOWESS curve (`lowess=True`) and the curve colored red (`line_kws=dict(color=\\"r\\")`). # Constraints: - Use seaborn version `0.11.0` or above. - Ensure matplotlib is properly configured to display plots. # Example: ```python import seaborn as sns import pandas as pd # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Generate residual plots generate_residual_plots(mpg) ``` # Notes: - Make sure to use proper labeling for plots to make them understandable. - The function should handle the display of plots efficiently without requiring any extra return values.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_residual_plots(data): This function takes a pandas DataFrame `data` and generates four residual plots: 1. A basic residual plot of \'weight\' vs \'displacement\'. 2. A residual plot of \'horsepower\' vs \'mpg\'. 3. A residual plot of \'horsepower\' vs \'mpg\' using a second-order polynomial. 4. A residual plot of \'horsepower\' vs \'mpg\' with a LOWESS curve. The function should not return anything but should display the plots. Parameters: data (pd.DataFrame): The input DataFrame with car attributes. # 1. Basic residual plot of \'weight\' vs \'displacement\' plt.figure(figsize=(10, 6)) sns.residplot(x=\'weight\', y=\'displacement\', data=data) plt.title(\'Residual Plot of Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') plt.show() # 2. Residual plot of \'horsepower\' vs \'mpg\' plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=data) plt.title(\'Residual Plot of Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() # 3. Residual plot of \'horsepower\' vs \'mpg\' using a second-order polynomial plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=data, order=2) plt.title(\'Residual Plot of Horsepower vs MPG (Order 2)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() # 4. Residual plot of \'horsepower\' vs \'mpg\' with a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=data, lowess=True, line_kws=dict(color=\\"red\\")) plt.title(\'Residual Plot of Horsepower vs MPG with LOWESS Curve\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show()"},{"question":"# Coding Assessment: Implement Sequence Operations in Python Using your understanding of Python sequences and the described operations in the documentation, implement a custom Python class `CustomSequence` that behaves like a Python list but tracks the number of read and write operations performed on it. Requirements: 1. **Initialization**: - Takes an initial list of elements. - Initialize counters for read and write operations. 2. **Methods**: - `__getitem__(self, index)`: Retrieve an item at the `index`. Increment the read counter. - `__setitem__(self, index, value)`: Assign `value` to the place at `index`. Increment the write counter. - `__delitem__(self, index)`: Delete the item at the `index`. Increment the write counter. - `__len__(self)`: Return the length of the sequence. - `__add__(self, other)`: Concatenate the sequence with another one. Must return a new `CustomSequence` object. - `__mul__(self, count)`: Repeat the sequence `count` times. Must return a new `CustomSequence` object. - `__contains__(self, item)`: Check if `item` is in the sequence. Increment the read counter. - `count_operations(self)`: Return a tuple of the read and write counter values. - `clear_counters(self)`: Reset the read and write counters to zero. Example Usage: ```python seq = CustomSequence([1, 2, 3]) print(seq[1]) # Output: 2 print(seq.count_operations()) # Output: (1, 0) seq[1] = 10 print(seq.count_operations()) # Output: (1, 1) del seq[1] print(seq.count_operations()) # Output: (1, 2) print(len(seq)) # Output: 2 print(2 in seq) # Output: False print(seq.count_operations()) # Output: (2, 2) new_seq = seq + CustomSequence([4, 5]) print(new_seq) # Output: CustomSequence([1, 3, 4, 5]) print(new_seq.count_operations()) # Output: (0, 0) repeated_seq = seq * 2 print(repeated_seq) # Output: CustomSequence([1, 3, 1, 3]) print(repeated_seq.count_operations()) # Output: (0, 0) ``` Constraints: - The implementation must be efficient and handle standard Python sequence operations smoothly. - Assume the sequence will hold only comparable elements (elements supporting comparison operations). Performance Requirements: - Operations should have time complexity comparable to Python\'s built-in list operations.","solution":"class CustomSequence: def __init__(self, initial_list): self.data = initial_list self.read_counter = 0 self.write_counter = 0 def __getitem__(self, index): self.read_counter += 1 return self.data[index] def __setitem__(self, index, value): self.write_counter += 1 self.data[index] = value def __delitem__(self, index): self.write_counter += 1 del self.data[index] def __len__(self): return len(self.data) def __add__(self, other): return CustomSequence(self.data + other.data) def __mul__(self, count): return CustomSequence(self.data * count) def __contains__(self, item): self.read_counter += 1 return item in self.data def count_operations(self): return (self.read_counter, self.write_counter) def clear_counters(self): self.read_counter = 0 self.write_counter = 0"},{"question":"# **Client-Server Echo Application with Error Handling** **Objective:** Implement a client-server application using Python\'s `socket` module. The server will echo back any message it receives from the client. Both the client and the server should handle potential errors gracefully, using the appropriate exceptions and socket methods. **Task:** 1. **Server Implementation:** - Create a server that listens on `localhost` and an arbitrary port (e.g., 5050). - The server should accept incoming connections. - For each connection, the server should: - Receive the message from the client. - Echo back the received message to the client. - Handle any exceptions that might occur during the process and print appropriate error messages. 2. **Client Implementation:** - Create a client that connects to the server on `localhost` and the specified port. - The client should: - Send a message to the server. - Receive the echoed message from the server. - Handle any exceptions that might occur during the process and print appropriate error messages. 3. **Error Handling:** - Both the client and the server should handle errors such as connection issues, timeouts, and invalid data using the `socket` moduleâ€™s exception classes like `socket.error`, `socket.herror`, `socket.gaierror`, and `socket.timeout`. **Constraints:** - The server should be able to handle multiple consecutive client connections. - The server should operate as a continuous service until manually terminated. - The client should connect to the server once, send a message, receive the response, and then close the connection. **Input and Output Formats:** - **Server:** - Input: No direct input; the server passively waits for client connections. - Output: The server should print messages about connections, the received data, sent data, and any errors encountered. - **Client:** - Input: The user will provide the message to be sent to the server. - Output: The client should print the echoed message received from the server and any errors encountered. **Example:** ```python # Server (to be run in one terminal) Output: Server started and listening on (\'localhost\', 5050) Connected by (\'127.0.0.1\', 53612) Received: Hello, Server! Echoed back: Hello, Server! ... # Client (to be run in another terminal) Input: Hello, Server! Output: Received: Hello, Server! ... ``` **Hint:** Use the `socket.socket()`, `bind()`, `listen()`, `accept()`, `connect()`, `sendall()`, `recv()`, and appropriate error handling methods provided by the `socket` module.","solution":"import socket # Server Implementation def start_server(host=\'localhost\', port=5050): try: with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket: server_socket.bind((host, port)) server_socket.listen() print(f\\"Server started and listening on ({host}, {port})\\") while True: conn, addr = server_socket.accept() with conn: print(f\\"Connected by {addr}\\") try: while True: data = conn.recv(1024) if not data: break print(f\\"Received: {data.decode()}\\") conn.sendall(data) print(f\\"Echoed back: {data.decode()}\\") except socket.error as e: print(f\\"Socket error: {e}\\") except socket.error as e: print(f\\"Server socket error: {e}\\") # Client Implementation def start_client(message, host=\'localhost\', port=5050): try: with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_socket: client_socket.connect((host, port)) client_socket.sendall(message.encode()) data = client_socket.recv(1024) print(f\\"Received: {data.decode()}\\") except socket.error as e: print(f\\"Client socket error: {e}\\")"},{"question":"Objective: Develop a Python function that analyzes and processes a list of records, demonstrating your understanding of the `operator` module. Problem Statement: You are given a list of dictionaries representing inventory items in a warehouse. Each dictionary has the following keys: - `item`: A string representing the name of the item. - `quantity`: An integer representing the quantity of the item in stock. - `price`: A float representing the price of a single unit of the item. Your task is to implement a function `process_inventory(inventory)` that: 1. Sorts the items by their price in descending order. 2. Filters out items that have a unit price below a certain threshold. 3. Calculates the total cost of the inventory, which is the sum of the product of quantity and price for each item. 4. Returns a list of item names ordered by units in stock in descending order. Function Signature: ```python def process_inventory(inventory: list, price_threshold: float) -> list: pass ``` Input: - `inventory`: A list of dictionaries with each dictionary containing: - `item` (str): The name of the item. - `quantity` (int): The quantity in stock. - `price` (float): The price per unit. - `price_threshold`: A float representing the minimum price threshold. Output: - A list of item names (str) ordered by units in stock in descending order. Constraints: - The list `inventory` may contain between 1 and 1000 items. - The `quantity` for each item will be between 1 and 10,000. - The `price` for each item will be between 0.01 and 10,000.00. Example: ```python inventory = [ {\\"item\\": \\"apple\\", \\"quantity\\": 50, \\"price\\": 1.2}, {\\"item\\": \\"banana\\", \\"quantity\\": 30, \\"price\\": 0.5}, {\\"item\\": \\"cherry\\", \\"quantity\\": 20, \\"price\\": 3.0}, {\\"item\\": \\"date\\", \\"quantity\\": 15, \\"price\\": 0.3} ] price_threshold = 1.0 result = process_inventory(inventory, price_threshold) print(result) # Output: [\'cherry\', \'apple\'] ``` Explanation: 1. The item \\"date\\" and \\"banana\\" have prices below the threshold of 1.0 and are filtered out. 2. The remaining items are sorted by price in descending order: \\"cherry\\" (3.0) and \\"apple\\" (1.2). 3. The total cost of the inventory is computed for completeness (although not returned): ( 20 times 3.0 + 50 times 1.2 = 60 + 60 = 120 ). 4. The function returns the item names ordered by their quantity in stock: \\"cherry\\" (20) and \\"apple\\" (50). Notes: - Utilize the `operator` module functions such as `operator.itemgetter`, `operator.attrgetter`, and `operator.mul` as needed. - Ensure items with equal quantities are ordered alphabetically by their names.","solution":"import operator def process_inventory(inventory, price_threshold): Processes the given inventory list. Args: - inventory (list): List of dictionaries. Each dictionary contains keys \'item\', \'quantity\', and \'price\'. - price_threshold (float): The minimum price an item must have to be included. Returns: - list: A list of item names ordered by units in stock in descending order. # Filter out items below the price threshold filtered_inventory = list(filter(lambda x: x[\'price\'] >= price_threshold, inventory)) # Sort items by price in descending order sorted_by_price = sorted(filtered_inventory, key=operator.itemgetter(\'price\'), reverse=True) # Calculate the total cost of the inventory (for completeness, but not used in the return value) total_cost = sum(item[\'quantity\'] * item[\'price\'] for item in sorted_by_price) # Sort item names by their quantity in stock in descending order (and alphabetically for tie-breaking) sorted_by_quantity = sorted(sorted_by_price, key=lambda x: (-x[\'quantity\'], x[\'item\'])) # Extract item names for the result result = [item[\'item\'] for item in sorted_by_quantity] return result"},{"question":"Understanding and Utilizing Built-in Constants Objective Demonstrate your understanding of Python\'s built-in constants by implementing a function that processes a given list of values and returns a summary that categorizes the values based on specific built-in constants. Problem Statement Write a function `summarize_constants(values: list) -> dict` that takes a list of values and returns a dictionary summarizing the occurrence of each type of built-in constant in the list. You must specifically count and categorize the following constants: `True`, `False`, `None`, `NotImplemented`, `Ellipsis`, and any other values. Input - `values`: A list of values. The list can contain elements of any data type. Output - A dictionary with the following keys: `\'True\'`, `\'False\'`, `\'None\'`, `\'NotImplemented\'`, `\'Ellipsis\'`, and `\'Others\'`. The value for each key should be an integer representing the number of occurrences of each type in the input list. Constraints - The list can contain any number of elements (n >= 0). - The elements in the list can be of any data type. Example ```python input_list = [True, False, None, Ellipsis, NotImplemented, 42, \\"string\\", False, True] result = summarize_constants(input_list) print(result) ``` Expected Output: ```python { \'True\': 2, \'False\': 2, \'None\': 1, \'NotImplemented\': 1, \'Ellipsis\': 1, \'Others\': 2 } ``` Additional Notes - Do **not** use any libraries or functions that directly provide the counts of these constants. - Ensure that the function handles all edge cases, including empty lists and lists without any of these constants. - The function should be efficient with a time complexity of O(n), where n is the length of the input list.","solution":"def summarize_constants(values): Returns a summary of occurrences of specific built-in constants in the given list of values. constants_summary = { \'True\': 0, \'False\': 0, \'None\': 0, \'NotImplemented\': 0, \'Ellipsis\': 0, \'Others\': 0 } for value in values: if value is True: constants_summary[\'True\'] += 1 elif value is False: constants_summary[\'False\'] += 1 elif value is None: constants_summary[\'None\'] += 1 elif value is NotImplemented: constants_summary[\'NotImplemented\'] += 1 elif value is Ellipsis: constants_summary[\'Ellipsis\'] += 1 else: constants_summary[\'Others\'] += 1 return constants_summary"},{"question":"# Python Coding Assessment Question: AST Manipulation Context The `ast` module in Python allows you to interact with and manipulate the abstract syntax tree (AST) of Python code. This module is useful for performing static analysis or code transformation tasks. Problem Statement You are required to write a function that analyzes a given Python function and returns statistics about the number and types of various nodes present in its AST. Function Signature ```python def analyze_ast(code: str) -> dict: pass ``` Input - `code` (str): A string representing the source code of a Python function. Output - `result` (dict): A dictionary with the following structure: ```python { \'FunctionDef\': int, # Number of function definitions (should be 1 for the top-level function) \'Call\': int, # Number of function calls \'Assign\': int, # Number of assignments \'Return\': int, # Number of return statements # Add more relevant AST node counts as required } ``` Constraints - Assume that the `code` string will always contain a valid Python function. - You can use any helper functions as needed. Example ```python code = def example_function(x): y = x + 1 return y result = analyze_ast(code) print(result) ``` Expected Output: ```python { \'FunctionDef\': 1, \'Call\': 0, \'Assign\': 1, \'Return\': 1 } ``` Notes - You should use the `ast` module to parse the code and walk through the AST. - Consider using the `ast.NodeVisitor` class to count the nodes. - Focus on analyzing only the nodes relevant to typical function operations, but you can extend the functionality to include more node types if you wish. Guidelines 1. Parse the input `code` string using `ast.parse`. 2. Implement a custom node visitor class that visits and counts specific types of nodes. 3. Return the count of these nodes in a dictionary format as described above.","solution":"import ast def analyze_ast(code: str) -> dict: tree = ast.parse(code) class NodeCounter(ast.NodeVisitor): def __init__(self): self.stats = { \'FunctionDef\': 0, \'Call\': 0, \'Assign\': 0, \'Return\': 0 } def visit_FunctionDef(self, node): self.stats[\'FunctionDef\'] += 1 self.generic_visit(node) def visit_Call(self, node): self.stats[\'Call\'] += 1 self.generic_visit(node) def visit_Assign(self, node): self.stats[\'Assign\'] += 1 self.generic_visit(node) def visit_Return(self, node): self.stats[\'Return\'] += 1 self.generic_visit(node) counter = NodeCounter() counter.visit(tree) return counter.stats"},{"question":"You are given a dataset containing information about different products in a warehouse. Your task is to write a function using pandas that performs a series of data manipulations and analysis on this dataset. Dataset Description The dataset is in the form of a CSV file named `products.csv` with the following columns: - `ProductID`: Unique identifier for each product. - `Category`: The category to which the product belongs. - `Price`: Price of the product. - `Quantity`: Number of units available in the warehouse. - `DateAdded`: Date when the product was added to the warehouse. Function Requirements You need to implement a function `analyze_products(file_path)` that performs the following operations: 1. **Read the CSV file** into a pandas DataFrame. 2. **Remove duplicates** based on the `ProductID` column. 3. Fill any missing values in the `Quantity` column with `0`. 4. Convert the `DateAdded` column to datetime format. 5. Return a DataFrame showing the total count of products and the average price per category. 6. Sort this DataFrame in descending order by average price. 7. Add a column to this sorted DataFrame indicating the rank of each category based on the average price. Input - `file_path` (str): The file path of the `products.csv`. Output - A pandas DataFrame with the following columns: - `TotalProducts`: Count of products in each category. - `AveragePrice`: Average price of products in each category. - `Rank`: Rank of each category based on the average price, with 1 being the highest. Constraints - You can assume the input CSV file will have valid headers and data. Performance Requirements - The function should be able to handle large datasets efficiently. # Example Suppose `products.csv` contains the following data: | ProductID | Category | Price | Quantity | DateAdded | |-----------|---------------|-------|----------|------------| | 1 | Electronics | 100.0 | 10 | 2023-01-01 | | 2 | Electronics | 150.0 | 8 | 2023-01-05 | | 3 | Furniture | 200.0 | 5 | 2022-12-15 | | 4 | Furniture | 250.0 | 2 | 2023-02-10 | | 5 | Clothing | 25.0 | 50 | 2023-03-12 | | 6 | Clothing | 30.0 | NaN | 2023-03-15 | | 2 | Electronics | 150.0 | 8 | 2023-01-05 | After processing, the resulting DataFrame should be: | Category | TotalProducts | AveragePrice | Rank | |-------------|---------------|--------------|------| | Furniture | 2 | 225.0 | 1 | | Electronics | 2 | 125.0 | 2 | | Clothing | 2 | 27.5 | 3 | # Implementation ```python import pandas as pd def analyze_products(file_path): # 1. Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # 2. Remove duplicates based on ProductID df = df.drop_duplicates(subset=\'ProductID\') # 3. Fill missing values in the Quantity column with 0 df[\'Quantity\'] = df[\'Quantity\'].fillna(0) # 4. Convert the DateAdded column to datetime format df[\'DateAdded\'] = pd.to_datetime(df[\'DateAdded\']) # 5. Calculate total number of products and average price per category result_df = df.groupby(\'Category\').agg( TotalProducts=(\'ProductID\', \'count\'), AveragePrice=(\'Price\', \'mean\') ).reset_index() # 6. Sort the DataFrame in descending order by average price result_df = result_df.sort_values(by=\'AveragePrice\', ascending=False) # 7. Add a rank column based on the average price result_df[\'Rank\'] = result_df[\'AveragePrice\'].rank(ascending=False, method=\'dense\').astype(int) return result_df # Example usage file_path = \'products.csv\' result = analyze_products(file_path) print(result) ```","solution":"import pandas as pd def analyze_products(file_path): # 1. Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # 2. Remove duplicates based on ProductID df = df.drop_duplicates(subset=\'ProductID\') # 3. Fill missing values in the Quantity column with 0 df[\'Quantity\'] = df[\'Quantity\'].fillna(0) # 4. Convert the DateAdded column to datetime format df[\'DateAdded\'] = pd.to_datetime(df[\'DateAdded\']) # 5. Calculate total number of products and average price per category result_df = df.groupby(\'Category\').agg( TotalProducts=(\'ProductID\', \'count\'), AveragePrice=(\'Price\', \'mean\') ).reset_index() # 6. Sort the DataFrame in descending order by average price result_df = result_df.sort_values(by=\'AveragePrice\', ascending=False) # 7. Add a rank column based on the average price result_df[\'Rank\'] = result_df[\'AveragePrice\'].rank(ascending=False, method=\'dense\').astype(int) return result_df"},{"question":"**Email Message Processing** You are given an email message in the form of a `Message` object from the `email` package. Your task is to implement a function `process_email(message, mime_type)` that extracts and returns all text lines from the message\'s subparts that match the specified `mime_type`. **Function Signature:** ```python def process_email(message, mime_type: str) -> List[str]: ``` **Input:** - `message`: A `Message` object representing the email. - `mime_type`: A string in the format `\\"maintype/subtype\\"` indicating the MIME type to filter subparts. **Output:** - List of strings, each string is a line from the matching subparts. **Constraints:** - The `mime_type` will always be a valid MIME type in the form of `maintype/subtype`. - The email message can contain multiple nested subparts. - You can assume the message object and its subparts have properly formatted MIME types. **Example:** Suppose you have an email message with the following structure: ``` multipart/mixed text/plain text/html multipart/alternative text/plain text/html ``` And call `process_email(message, \'text/plain\')`, the function should return all lines from parts with MIME type `text/plain`. **Note:** - You may find `typed_subpart_iterator` and `body_line_iterator` functions from `email.iterators` module useful for this task. - Ensure your implementation efficiently handles nesting and large emails. **Implementation Hints:** - Use `typed_subpart_iterator` to iterate over subparts and filter the ones matching the specified `mime_type`. - Use `body_line_iterator` to extract lines from the filtered subparts.","solution":"from email import message_from_string from email.message import Message from email.iterators import typed_subpart_iterator, body_line_iterator def process_email(message: Message, mime_type: str) -> list: Extracts and returns all text lines from the message\'s subparts that match the specified MIME type. # Splitting the mime_type into its main and sub type maintype, subtype = mime_type.split(\'/\') # List to store the lines that match the MIME type matching_lines = [] # Iterating over subparts that match the specified MIME type for part in typed_subpart_iterator(message, maintype, subtype): # Adding matching lines from the current part for line in body_line_iterator(part): matching_lines.append(line) return matching_lines"},{"question":"# Question: Efficient Calculation and Caching using functools You have been assigned a task to efficiently compute the result of a mathematical function with the help of caching and function manipulation. Problem: Implement a function `cached_power(base: int, exponent: int) -> int` that calculates `base` raised to the power `exponent`. To simulate an expensive computation: - Introduce an artificial delay using `time.sleep(2)` for each call. Additionally, implement another function `partial_cached_power` that returns a partial function with the `base` argument fixed, leveraging `functools.partial`. Use `functools` to ensure that: 1. The results are cached such that subsequent computations of the same `base` and `exponent` pair are retrieved from the cache without delay. 2. The function can handle a variable number of exponentiation requests efficiently. Constraints: - The function caching should be unbounded (useful for potentially large number set). - Maintain proper function metadata using `functools.wraps`. Input: - An integer `base`. - An integer `exponent`. Output: - An integer which is the result of `base` raised to the power `exponent`. Example Usage: ```python import time # Assuming `cached_power` and `partial_cached_power` are implemented correctly. # Call with caching start_time = time.time() print(cached_power(2, 10)) # This should take approximately 2 seconds print(\\"First call time:\\", time.time() - start_time) start_time = time.time() print(cached_power(2, 10)) # This should be almost instantaneous print(\\"Second call time:\\", time.time() - start_time) # Partial Function Usage partial_func = partial_cached_power(3) start_time = time.time() print(partial_func(4)) # This should take approximately 2 seconds print(\\"Partial function first call time:\\", time.time() - start_time) start_time = time.time() print(partial_func(4)) # This should be almost instantaneous print(\\"Partial function second call time:\\", time.time() - start_time) ``` Hints: 1. Use `functools.cache` for the `cached_power` function. 2. Use `functools.partial` for `partial_cached_power`. 3. Use `time.sleep(2)` to simulate the delay. Implementation: ```python import functools import time @functools.cache def cached_power(base: int, exponent: int) -> int: time.sleep(2) # Simulate an expensive computation. return base ** exponent def partial_cached_power(base: int): return functools.partial(cached_power, base) ``` This question not only tests studentsâ€™ understanding of `functools` but also their ability to combine multiple higher-order functions effectively.","solution":"import functools import time @functools.cache def cached_power(base: int, exponent: int) -> int: Calculates base raised to the power exponent with an artificial delay. Args: base (int): The base number. exponent (int): The power to raise the base to. Returns: int: The result of base ** exponent. time.sleep(2) # Simulate an expensive computation. return base ** exponent def partial_cached_power(base: int): Returns a partial function with the base argument fixed. Args: base (int): The base number to be fixed in the resulting partial function. Returns: function: A partial function with the base argument fixed. return functools.partial(cached_power, base)"},{"question":"**Question: Enhanced Exception Logger** You are tasked with creating an enhanced exception logging system that does more than just print the stack trace when an exception occurs. This system should capture the stack trace, format it, and write it to a log file in a detailed and readable manner. **Requirements:** 1. Create a function called `log_exception` that captures, formats, and logs detailed exception information. 2. The function should take the following parameters: - `exc`: The exception object (mandatory). - `log_file_path`: The path to the log file where the formatted exception information will be written (mandatory). - `limit`: An integer to limit the number of stack trace entries (optional, default is `None`, which captures all entries). 3. The function should format the exception and stack trace information using `traceback` module functions to get a detailed representation. 4. The formatted stack trace should include: - The exception type and message - The file names, line numbers, function names, and code lines where the exception occurred - The chain of exceptions if any (i.e., `__cause__` and `__context__`) 5. Write the formatted information to the specified log file in a readable format. **Input:** - `exc`: An exception object instance. - `log_file_path`: A string representing the path to the log file. - `limit`: (Optional) An integer setting a limit to the number of stack trace entries. **Output:** - Nothing. The function should write formatted exception information to the specified log file. **Example Usage:** ```python def problematic_function(): return 1 / 0 try: problematic_function() except Exception as e: log_exception(e, \'error_log.txt\') ``` **Expected Log File Content (sample content):** ``` Traceback (most recent call last): File \\"example.py\\", line 3, in <module> problematic_function() File \\"example.py\\", line 1, in problematic_function return 1 / 0 ZeroDivisionError: division by zero ``` **Notes:** - Ensure the log file is opened in append mode so new logs do not overwrite previous ones. - Handle file operations carefully and ensure the file is properly closed after writing. - The function should be able to handle any exception and log the complete stack trace in a structured format. **Constraints:** - Use only standard library modules for the implementation. - You may assume the log file path provided is valid and writable. - Performance should be reasonable for standard use cases (log file writing should not be a bottleneck).","solution":"import traceback def log_exception(exc, log_file_path, limit=None): Captures, formats, and logs detailed exception information to a log file. Parameters: exc (Exception): The exception object. log_file_path (str): The path to the log file where the formatted exception information will be written. limit (int, optional): An integer to limit the number of stack trace entries. Default is None. with open(log_file_path, \'a\') as log_file: log_file.write(\'Traceback (most recent call last):n\') stack_trace = traceback.format_exception(type(exc), exc, exc.__traceback__, limit=limit) for line in stack_trace: log_file.write(line)"},{"question":"**Objective:** You are tasked with creating a classifier that utilizes the scikit-learn library to classify the Iris dataset. Your goal is to write a function that loads the dataset, preprocesses it, trains a classifier, and evaluates its performance. **Dataset:** Utilize the `load_iris` function from `sklearn.datasets` to load the Iris dataset. **Function Requirements:** - **Function Name:** `train_and_evaluate_iris_classifier` - **Input:** None - **Output:** A dictionary with the following keys: - `\'accuracy\'`: The accuracy score of the classifier on the test dataset. - `\'classification_report\'`: The classification report as a single string, generated using `classification_report` from `sklearn.metrics`. - `\'confusion_matrix\'`: A 2D list representing the confusion matrix, generated using `confusion_matrix` from `sklearn.metrics`. **Constraints:** 1. Split the dataset into a training and test set using an 80/20 train/test split. 2. Use only scikit-learn\'s built-in functions and classes for preprocessing, model training, and evaluation. 3. The classifier to be used is the `RandomForestClassifier`. **Performance Requirements:** Ensure the function runs efficiently and completes within a reasonable amount of time for such a small dataset. **Implementation Details:** - Load the Iris dataset using `load_iris` from `sklearn.datasets`. - Split the dataset into training and testing sets. - Train a `RandomForestClassifier` on the training data. - Evaluate the trained model on the test data using accuracy, confusion matrix, and classification report. - Return the evaluation metrics in the specified format. **Example Function Definition:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report, confusion_matrix def train_and_evaluate_iris_classifier(): # Load the dataset data = load_iris() X, y = data.data, data.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train classifier classifier = RandomForestClassifier(random_state=42) classifier.fit(X_train, y_train) # Predict on test data y_pred = classifier.predict(X_test) # Compute evaluation metrics accuracy = accuracy_score(y_test, y_pred) cls_report = classification_report(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred).tolist() # Return the metrics as specified return { \'accuracy\': accuracy, \'classification_report\': cls_report, \'confusion_matrix\': conf_matrix } ``` **Notes:** - Ensure to handle any exceptions that may arise during the execution to avoid runtime errors. - Add appropriate comments to the code for clarity and maintainability.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report, confusion_matrix def train_and_evaluate_iris_classifier(): # Load the dataset data = load_iris() X, y = data.data, data.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train classifier classifier = RandomForestClassifier(random_state=42) classifier.fit(X_train, y_train) # Predict on test data y_pred = classifier.predict(X_test) # Compute evaluation metrics accuracy = accuracy_score(y_test, y_pred) cls_report = classification_report(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred).tolist() # Return the metrics as specified return { \'accuracy\': accuracy, \'classification_report\': cls_report, \'confusion_matrix\': conf_matrix }"},{"question":"You have been tasked with writing a Python script that handles cookies for a web scraper using the `http.cookiejar` module. Your task is to implement a class `WebScraperWithCookies` that performs the following functions: 1. Initializes with a given URL and optionally a path to a cookie file. 2. Uses a `CookieJar` to handle HTTP cookies. 3. Can load cookies from a given file, if provided. 4. Can save cookies to the given file. 5. Retrieves the content of the URL, with cookies properly managed across requests. 6. Resets cookies by clearing existing cookies and optionally reloading from the file. # Class Definition ```python class WebScraperWithCookies: def __init__(self, url: str, cookie_file: str = None): pass def load_cookies(self): pass def save_cookies(self): pass def get_content(self) -> str: pass def reset_cookies(self): pass ``` # Function Details 1. **`__init__(self, url: str, cookie_file: str = None)`**: - **Input**: `url` is the target URL to scrape. `cookie_file` is an optional path to a cookie file. - **Output**: Initializes the `CookieJar` and optionally a `MozillaCookieJar`. 2. **`load_cookies(self)`**: - **Input**: None - **Output**: Loads cookies from the `cookie_file` if it exists. 3. **`save_cookies(self)`**: - **Input**: None - **Output**: Saves the current cookies to the `cookie_file`. 4. **`get_content(self) -> str`**: - **Input**: None - **Output**: Returns the content of the URL as a string with cookies managed in the request headers. 5. **`reset_cookies(self)`**: - **Input**: None - **Output**: Clears all cookies from the `CookieJar` and reloads from the file if it exists. # Constraints - You should use appropriate error handling when loading or saving cookies. - When retrieving content, manage and use cookies properly for both the request and response. - The cookie file can re-use an existing file format (e.g., Mozilla\'s `cookies.txt` format). # Example Usage ```python scraper = WebScraperWithCookies(\'http://example.com\', \'cookies.txt\') scraper.load_cookies() content = scraper.get_content() print(content) scraper.save_cookies() scraper.reset_cookies() ``` Ensure that your implementation adheres to these specifications using the `http.cookiejar` module.","solution":"import http.cookiejar as cookiejar import urllib.request as request class WebScraperWithCookies: def __init__(self, url: str, cookie_file: str = None): self.url = url self.cookie_file = cookie_file if cookie_file: self.cookie_jar = cookiejar.MozillaCookieJar(cookie_file) else: self.cookie_jar = cookiejar.CookieJar() self.opener = request.build_opener(request.HTTPCookieProcessor(self.cookie_jar)) def load_cookies(self): if self.cookie_file: try: self.cookie_jar.load(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"Failed to load cookies from {self.cookie_file}: {e}\\") def save_cookies(self): if self.cookie_file: try: self.cookie_jar.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"Failed to save cookies to {self.cookie_file}: {e}\\") def get_content(self) -> str: try: response = self.opener.open(self.url) return response.read().decode(\'utf-8\') except Exception as e: print(f\\"Failed to retrieve content from {self.url}: {e}\\") return \\"\\" def reset_cookies(self): self.cookie_jar.clear() if self.cookie_file: self.load_cookies()"},{"question":"# Question: Advanced Seaborn Visualization with Tips Dataset You are provided with the \\"tips\\" dataset from seaborn\'s sample datasets. Your task is to create a function that generates a complex seaborn visualization combining multiple aspects of the seaborn library you\'ve learned. Specifically, you need to implement the function `create_advanced_visualization` that does the following: 1. **Data Loading and Theme Setting**: - Load the \\"tips\\" dataset. - Set the seaborn theme to `\\"whitegrid\\"`. 2. **Custom Stripplot**: - Create a stripplot visualizing the relationship between `total_bill` and `day`. - Use `hue` to differentiate between `sex`. - Set `dodge=True` to separate the strips for categorical hue values. - Disable jitter. 3. **Faceted Plot with Catplot**: - Create a faceted plot using `catplot` to show the relationship between `time` and `total_bill` segmented by weekdays using the `day` variable. - Use `hue` to differentiate between `sex`. - Adjust the aspect ratio for each facet to 0.6 for better visibility. **Function Signature**: ```python def create_advanced_visualization(): pass ``` **Output**: - Ensure that the function does not return any value but displays the plots directly using `matplotlib.pyplot.show()`. **Constraints**: - Use only the seaborn library functions for creating plots. - The function should set the theme at the beginning to ensure consistent styling. Example Usage: ```python create_advanced_visualization() # This should display the plots as described without any errors. ``` *Hint*: Refer to seaborn\'s documentation for `stripplot` and `catplot` for more details on the parameters you can use. # Evaluation Criteria: - Correctness: The plots should align with the specifications. - Code Quality: The code should be clean, well-commented, and follow Python best practices. - Visualization: The resulting plots should be well-organized, clear, and correctly use seaborn\'s functionalities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_advanced_visualization(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the seaborn theme to whitegrid sns.set_theme(style=\\"whitegrid\\") # Create the custom stripplot plt.figure(figsize=(10, 6)) strip_plot = sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, dodge=True, jitter=False) plt.title(\\"Stripplot of Total Bill vs Day with Hue on Sex\\") plt.show() # Create the faceted plot with catplot cat_plot = sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", aspect=0.6) cat_plot.fig.suptitle(\\"Catplot of Total Bill vs Time Faceted by Day with Hue on Sex\\", y=1.05) plt.show()"},{"question":"# Question: Implement a Custom Cell Object Manager In Python, closures allow the inner functions to capture variables from outer, non-global scopes. The concept of cell objects is crucial in this mechanism. In this task, you will create a custom cell object manager that mimics some functionalities of Python\'s cell objects using Python\'s object-oriented features. You are required to implement a class `CellObject` in Python. Your class should support: 1. Creating a new cell object with an initial value. 2. Retrieving the value stored in the cell. 3. Setting a new value to the cell object. # Requirements: 1. **Class Name**: `CellObject` 2. **Initialization**: - The constructor should take an optional initial value. 3. **Methods**: - `get() -> any`: Returns the current value stored in the cell. - `set(value: any) -> None`: Sets a new value to the cell. # Constraints: - The initial value and the values to set can be any type (int, float, string, list, etc.). - The `get` method should always safely return the value stored in the cell, or `None` if the cell is empty. - Demonstrate usage of the `CellObject` class using different data types. # Example Usage: ```python # Creating the cell object with initial value cell = CellObject(10) assert cell.get() == 10 # Should return 10 # Creating an empty cell object empty_cell = CellObject() assert empty_cell.get() is None # Should return None # Setting new value cell.set(20) assert cell.get() == 20 # Should return 20 # Using different data types cell.set([1, 2, 3]) assert cell.get() == [1, 2, 3] # Should return the list [1, 2, 3] ``` # Your task: Implement the `CellObject` class according to the requirements and constraints provided. ```python class CellObject: def __init__(self, initial_value=None): # Your code here def get(self): # Your code here def set(self, value): # Your code here ``` Ensure your implementation handles different data types correctly and demonstrates the example usages.","solution":"class CellObject: def __init__(self, initial_value=None): self.value = initial_value def get(self): return self.value def set(self, value): self.value = value"},{"question":"**Coding Assessment Question** Write a Python program that creates two subprocesses asynchronously. Each subprocess will perform a different task and their outputs must be captured and displayed. One subprocess should list the contents of a directory, while the other should display the current date and time. On completion of both subprocesses, the program should terminate gracefully. **Requirements:** 1. Define an asynchronous function `run_tasks(directory)` that: - Creates and runs a subprocess to list the contents of the provided directory, capturing and displaying its output. - Creates and runs a second subprocess to display the current date and time, capturing and displaying its output. 2. The program should handle potential errors, such as the directory not existing. 3. Implement another asynchronous function `main()` that: - Calls `run_tasks()`. 4. Use `asyncio.run(main())` to start the process. **Expected Input and Output Formats:** - Input: A string representing the directory path (e.g., `\\"./\\"`). - Output: Printed contents of the specified directory and the current date and time. **Example:** ```python Input: \\"./\\" Output: [ls \'./\' exited with 0] [stdout] file1.txt file2.py ... [date and time exited with 0] [stdout] Current date: 2023-01-01 10:00:00 ``` **Constraints:** - The solution must use asynchronous subprocess management with the `asyncio` module. - The program should handle potential errors gracefully. **Performance Requirements:** - The program should handle both subprocesses in parallel and efficiently capture and display their outputs. **Hints:** - Use `asyncio.create_subprocess_exec()` or `asyncio.create_subprocess_shell()` for creating subprocesses. - Ensure proper error handling by checking the return code of each subprocess. - Use the `communicate()` method for capturing the output of each subprocess to avoid blocking.","solution":"import asyncio import os async def run_command(command): Run a command in a subprocess and capture its output. process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if process.returncode == 0: print(f\'[{command!r} exited with {process.returncode}]\') print(f\'[stdout]n{stdout.decode().strip()}\') else: print(f\'[{command!r} exited with {process.returncode}]\') print(f\'[stderr]n{stderr.decode().strip()}\') async def run_tasks(directory): Create and run subprocesses to list the contents of a directory and to get the current date and time. if not os.path.exists(directory): print(f\\"The directory {directory} does not exist.\\") return # List directory contents command ls_command = f\'ls {directory}\' # Current date and time command date_command = \'date\' await asyncio.gather( run_command(ls_command), run_command(date_command) ) async def main(): directory = \\"./\\" # Assuming the directory to list await run_tasks(directory) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Advanced Regression Visualization with Seaborn Objective To assess your understanding of seabornâ€™s capabilities for visualizing regression models and handling various data conditions. Problem Statement You are provided with a dataset containing information about a car\'s fuel economy. The dataset `cars` consists of the following columns: - `mpg` (Miles per Gallon): Continuous numerical variable. - `weight`: Continuous numerical variable representing the weight of the car. - `model_year`: Discrete numerical variable representing the model year of the car. - `origin`: Categorical variable representing the origin of the car (USA, Europe, Japan). You are required to perform the following tasks: 1. Load the dataset (which is available in seabornâ€™s sample datasets). 2. Create a scatter plot with a linear regression line showing the relationship between car weight and miles per gallon. 3. Enhance the scatter plot by adding jitter to the `model_year` variable to account for its discrete nature. 4. Fit a polynomial regression model (order 2) between car weight and miles per gallon. 5. Divide the scatter plot into subplots based on the origin of the car, and for each subplot, fit a separate linear regression. 6. Create a pair plot for mpg, weight, and model_year, including a regression line. 7. Finally, analyze residuals of the polynomial regression model to check the appropriateness of the fit. Constraints - You should use seaborn for all visualizations. - Ensure that all plots have appropriate titles, labels, and legends. - Handle any missing data appropriately (e.g., using dropna()). Input The dataset should be loaded directly from seaborn\'s sample datasets. Expected Output - Multiple visualizations as described above. - Insights from the residuals plot in a markdown cell. Example Code ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # 1. Load the dataset cars = sns.load_dataset(\'mpg\').dropna() # 2. Scatter plot with linear regression line sns.lmplot(x=\'weight\', y=\'mpg\', data=cars) plt.title(\'Linear Regression of Weight vs MPG\') plt.show() # 3. Scatter plot with jitter sns.lmplot(x=\'model_year\', y=\'mpg\', data=cars, x_jitter=.05) plt.title(\'Scatter Plot with Jitter for Model Year\') plt.show() # 4. Polynomial regression (order 2) sns.lmplot(x=\'weight\', y=\'mpg\', data=cars, order=2) plt.title(\'Polynomial Regression of Weight vs MPG\') plt.show() # 5. Subplots based on origin sns.lmplot(x=\'weight\', y=\'mpg\', hue=\'origin\', data=cars) plt.title(\'Linear Regression by Origin\') plt.show() # 6. Pair plot with regression lines sns.pairplot(cars, x_vars=[\'mpg\', \'weight\'], y_vars=[\'model_year\'], kind=\'reg\', hue=\'origin\') plt.show() # 7. Residuals analysis sns.residplot(x=\'weight\', y=\'mpg\', data=cars, order=2) plt.title(\'Residuals of Polynomial Regression (Order 2)\') plt.show() ``` In a markdown cell, describe the insights obtained from the residuals plot. Evaluation Criteria - Correct implementation of each task. - Proper handling of the dataset. - Quality and readability of the visualizations. - Insightful analysis of the residuals plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_and_clean_data(): Load the \'mpg\' dataset and drop rows with any missing values. return sns.load_dataset(\'mpg\').dropna() def scatter_plot_with_regression(data): Create a scatter plot with a linear regression line showing the relationship between car weight and miles per gallon. sns.lmplot(x=\'weight\', y=\'mpg\', data=data) plt.title(\'Linear Regression of Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'Miles per Gallon\') plt.show() def scatter_plot_with_jitter(data): Create a scatter plot with jitter added to the \'model_year\' variable. sns.lmplot(x=\'model_year\', y=\'mpg\', data=data, x_jitter=.05) plt.title(\'Scatter Plot with Jitter for Model Year\') plt.xlabel(\'Model Year\') plt.ylabel(\'Miles per Gallon\') plt.show() def polynomial_regression(data): Fit a polynomial regression model (order 2) between car weight and miles per gallon. sns.lmplot(x=\'weight\', y=\'mpg\', data=data, order=2) plt.title(\'Polynomial Regression of Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'Miles per Gallon\') plt.show() def subplots_by_origin(data): Divide the scatter plot into subplots based on the origin of the car and fit a separate linear regression for each subplot. sns.lmplot(x=\'weight\', y=\'mpg\', hue=\'origin\', data=data) plt.title(\'Linear Regression by Origin\') plt.xlabel(\'Weight\') plt.ylabel(\'Miles per Gallon\') plt.show() def pair_plot_with_regression(data): Create a pair plot for mpg, weight, and model_year, including a regression line. sns.pairplot(data, x_vars=[\'mpg\', \'weight\'], y_vars=[\'model_year\'], kind=\'reg\', hue=\'origin\') plt.show() def residuals_analysis(data): Analyze residuals of the polynomial regression model to check the appropriateness of the fit. sns.residplot(x=\'weight\', y=\'mpg\', data=data, order=2) plt.title(\'Residuals of Polynomial Regression (Order 2)\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') plt.show() # Load data and execute functions cars = load_and_clean_data() scatter_plot_with_regression(cars) scatter_plot_with_jitter(cars) polynomial_regression(cars) subplots_by_origin(cars) pair_plot_with_regression(cars) residuals_analysis(cars)"},{"question":"Encode and Decode Messages Objective You are required to write a Python program that can encode and decode a given binary message using multiple encoding schemes provided by the `binascii` module. This will test your understanding of the encoding and decoding functions in the module, as well as your ability to work with different data representations. Function Implementations 1. **encode_message(message, encoding_type):** - **Input:** - `message` (bytes): The binary message to be encoded. - `encoding_type` (str): The encoding scheme to be used. Possible values: `\'uu\'`, `\'base64\'`, `\'quoted-printable\'`, `\'hex\'`. - **Output:** - Encoded message as a bytes object. - **Constraints:** The length of `message` should be at most 45 bytes. 2. **decode_message(encoded_message, encoding_type):** - **Input:** - `encoded_message` (bytes): The encoded message to be decoded. - `encoding_type` (str): The encoding scheme to be used. Possible values: `\'uu\'`, `\'base64\'`, `\'quoted-printable\'`, `\'hex\'`. - **Output:** - Decoded binary message as a bytes object. - **Constraints:** Ensure the `encoded_message` is a valid encoded string for the corresponding `encoding_type`. Performance Requirements - The encoding and decoding functions should handle data conversion efficiently. Consider that these operations could be part of a larger data processing pipeline. Example ```python import binascii def encode_message(message: bytes, encoding_type: str) -> bytes: if encoding_type == \'uu\': return binascii.b2a_uu(message) elif encoding_type == \'base64\': return binascii.b2a_base64(message) elif encoding_type == \'quoted-printable\': return binascii.b2a_qp(message) elif encoding_type == \'hex\': return binascii.b2a_hex(message) else: raise ValueError(\\"Unsupported encoding type\\") def decode_message(encoded_message: bytes, encoding_type: str) -> bytes: if encoding_type == \'uu\': return binascii.a2b_uu(encoded_message) elif encoding_type == \'base64\': return binascii.a2b_base64(encoded_message) elif encoding_type == \'quoted-printable\': return binascii.a2b_qp(encoded_message) elif encoding_type == \'hex\': return binascii.a2b_hex(encoded_message) else: raise ValueError(\\"Unsupported encoding type\\") # Example usage message = b\'This is a test\' encoded_message = encode_message(message, \'base64\') print(encoded_message) # Output: b\'VGhpcyBpcyBhIHRlc3Q=n\' decoded_message = decode_message(encoded_message, \'base64\') print(decoded_message) # Output: b\'This is a test\' ``` You are to implement the `encode_message` and `decode_message` functions based on the specifications provided.","solution":"import binascii def encode_message(message: bytes, encoding_type: str) -> bytes: Encode a given binary message using the specified encoding type. Args: message (bytes): The binary message to be encoded. encoding_type (str): The encoding scheme to be used. Possible values: \'uu\', \'base64\', \'quoted-printable\', \'hex\'. Returns: bytes: Encoded message as a bytes object. Raises: ValueError: If the encoding type is unsupported. if encoding_type == \'uu\': return binascii.b2a_uu(message) elif encoding_type == \'base64\': return binascii.b2a_base64(message).strip() elif encoding_type == \'quoted-printable\': return binascii.b2a_qp(message) elif encoding_type == \'hex\': return binascii.b2a_hex(message) else: raise ValueError(\\"Unsupported encoding type\\") def decode_message(encoded_message: bytes, encoding_type: str) -> bytes: Decode an encoded message using the specified encoding type. Args: encoded_message (bytes): The encoded message to be decoded. encoding_type (str): The encoding scheme to be used. Possible values: \'uu\', \'base64\', \'quoted-printable\', \'hex\'. Returns: bytes: Decoded binary message as a bytes object. Raises: ValueError: If the encoding type is unsupported. if encoding_type == \'uu\': return binascii.a2b_uu(encoded_message) elif encoding_type == \'base64\': return binascii.a2b_base64(encoded_message) elif encoding_type == \'quoted-printable\': return binascii.a2b_qp(encoded_message) elif encoding_type == \'hex\': return binascii.a2b_hex(encoded_message) else: raise ValueError(\\"Unsupported encoding type\\")"},{"question":"You are given a dataset consisting of medical records where each record has attributes related to patient health and a target label indicating whether the patient has a specific condition (binary classification). Your task is to implement a custom classification function using `DecisionTreeClassifier` from scikit-learn that meets the following requirements: 1. **Function Name**: `train_and_evaluate_decision_tree` 2. **Input Parameters**: - `X_train` (array-like, shape `(n_samples, n_features)`): The training input samples. - `y_train` (array-like, shape `(n_samples,)`): The target values (class labels) for training. - `X_test` (array-like, shape `(n_samples, n_features)`): The testing input samples. - `y_test` (array-like, shape `(n_samples,)`): The target values (class labels) for testing. - `max_depth` (int, optional): The maximum depth of the tree. - `min_samples_split` (int or float, optional): The minimum number of samples required to split an internal node. - `min_samples_leaf` (int or float, optional): The minimum number of samples required to be at a leaf node. 3. **Steps to implement**: - Fit a `DecisionTreeClassifier` on the training data using the provided parameters. - Use the trained model to predict classes for the test set. - Use the trained model to predict the probabilities of the classes for the test set. - Calculate accuracy, precision, recall, and F1-score of the model on the test set. - Handle any missing values by following the criteria mentioned in the documentation: during prediction, missing values will be classified as per the training decisions; during training, if there are no missing values seen for a feature, then map missing values to the child with the most samples. 4. **Output**: - A dictionary containing the following keys and respective values: - `\'accuracy\'`: The accuracy of the model on the test set. - `\'precision\'`: The precision of the model on the test set. - `\'recall\'`: The recall of the model on the test set. - `\'f1_score\'`: The F1-score of the model on the test set. - `\'predictions\'`: The predicted classes for the test set as a list. - `\'probabilities\'`: The predicted class probabilities for the test set as a list of lists. **Constraints**: - The input arrays may contain missing values (NaNs), which need to be handled as described. - The function should return results within 5 seconds for datasets with up to 10,000 samples and 50 features. ```python def train_and_evaluate_decision_tree(X_train, y_train, X_test, y_test, max_depth=None, min_samples_split=2, min_samples_leaf=1): Train a Decision Tree classifier on the provided training data and evaluate it on the test data. Parameters: X_train (array-like): Training samples of shape (n_samples, n_features). y_train (array-like): Class labels for training samples. X_test (array-like): Testing samples of shape (n_samples, n_features). y_test (array-like): Class labels for testing samples. max_depth (int, optional): The maximum depth of the tree. min_samples_split (int or float, optional): The minimum number of samples required to split an internal node. min_samples_leaf (int or float, optional): The minimum number of samples required to be at a leaf node. Returns: dict: A dictionary with accuracy, precision, recall, F1-score, predictions, and probabilities. from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import numpy as np # Initialize the DecisionTreeClassifier with provided parameters. clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42) # Fit the classifier on the training data. clf.fit(X_train, y_train) # Perform predictions on the test set. predictions = clf.predict(X_test) probabilities = clf.predict_proba(X_test) # Calculate metrics. accuracy = accuracy_score(y_test, predictions) precision = precision_score(y_test, predictions, average=\'weighted\', zero_division=0) recall = recall_score(y_test, predictions, average=\'weighted\') f1 = f1_score(y_test, predictions, average=\'weighted\') # Return a dictionary with results. return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1, \'predictions\': predictions.tolist(), \'probabilities\': probabilities.tolist() } ```","solution":"def train_and_evaluate_decision_tree(X_train, y_train, X_test, y_test, max_depth=None, min_samples_split=2, min_samples_leaf=1): Train a Decision Tree classifier on the provided training data and evaluate it on the test data. Parameters: X_train (array-like): Training samples of shape (n_samples, n_features). y_train (array-like): Class labels for training samples. X_test (array-like): Testing samples of shape (n_samples, n_features). y_test (array-like): Class labels for testing samples. max_depth (int, optional): The maximum depth of the tree. min_samples_split (int or float, optional): The minimum number of samples required to split an internal node. min_samples_leaf (int or float, optional): The minimum number of samples required to be at a leaf node. Returns: dict: A dictionary with accuracy, precision, recall, F1-score, predictions, and probabilities. from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import numpy as np # Initialize the DecisionTreeClassifier with provided parameters. clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42) # Fit the classifier on the training data. clf.fit(X_train, y_train) # Perform predictions on the test set. predictions = clf.predict(X_test) probabilities = clf.predict_proba(X_test) # Calculate metrics. accuracy = accuracy_score(y_test, predictions) precision = precision_score(y_test, predictions, average=\'weighted\', zero_division=0) recall = recall_score(y_test, predictions, average=\'weighted\') f1 = f1_score(y_test, predictions, average=\'weighted\') # Return a dictionary with results. return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1, \'predictions\': predictions.tolist(), \'probabilities\': probabilities.tolist() }"},{"question":"You are provided with a folder containing multiple CSV files. Each CSV file contains different data but follows a similar structure with columns such as `Date`, `Product`, `Sales`, and `Category`. You need to write a Python function that reads all these CSV files, processes them, and outputs a single CSV file containing the aggregated data. # Requirements: 1. **Read multiple CSV files** from a specified directory into a `pandas` DataFrame. 2. **Process the DataFrame**: - Parse dates in the `Date` column while reading the CSV files. - Calculate the total `Sales` for each `Category` for each `Date`. 3. **Output the results** to a single CSV file with the aggregated data. The output CSV should have columns: `Date`, `Category`, and `Total Sales`. 4. **Handle large files** efficiently by utilizing `chunksize` for reading the files if necessary. # Function Signature: ```python import pandas as pd def aggregate_sales(input_directory: str, output_file: str) -> None: Reads all CSV files from the input directory, processes the data, and writes the aggregated sales data to the output file. Parameters: input_directory (str): The directory containing the input CSV files. output_file (str): The path to the output CSV file. pass ``` # Input: - `input_directory`: A string representing the path to the directory containing the input CSV files. - `output_file`: A string representing the path to the output CSV file where the aggregated data will be saved. # Constraints: - You can assume that the `input_directory` contains only valid CSV files. - You should handle scenarios with large CSV files efficiently. # Example: Suppose there are three CSV files in the `input_directory` with the following content: File 1: `sales1.csv` ``` Date,Product,Sales,Category 2023-01-01,Product1,100,Category1 2023-01-01,Product2,200,Category2 2023-01-02,Product1,150,Category1 ``` File 2: `sales2.csv` ``` Date,Product,Sales,Category 2023-01-01,Product3,300,Category1 2023-01-02,Product4,400,Category2 2023-01-03,Product2,250,Category1 ``` File 3: `sales3.csv` ``` Date,Product,Sales,Category 2023-01-02,Product3,350,Category2 2023-01-03,Product1,400,Category1 2023-01-03,Product4,500,Category2 ``` The output in `output_file` should be: ``` Date,Category,Total Sales 2023-01-01,Category1,400 2023-01-01,Category2,200 2023-01-02,Category1,150 2023-01-02,Category2,750 2023-01-03,Category1,650 2023-01-03,Category2,500 ``` Note: The order of rows in the output file might vary. # Description: - Your solution should demonstrate an understanding of reading multiple CSV files, handling dates, manipulating and aggregating data via `pandas`, and outputting the result to a CSV file. Complete the function and ensure it handles large files efficiently.","solution":"import os import pandas as pd def aggregate_sales(input_directory: str, output_file: str) -> None: # Initialize an empty list to collect DataFrames from each CSV file all_data = [] # Iterate over each file in the input directory for filename in os.listdir(input_directory): if filename.endswith(\'.csv\'): file_path = os.path.join(input_directory, filename) # Read the csv file with date parsing and append to list df = pd.read_csv(file_path, parse_dates=[\'Date\'], chunksize=1000) for chunk in df: all_data.append(chunk) # Concatenate all chunks into a single DataFrame combined_df = pd.concat(all_data) # Group by Date and Category to calculate total sales aggregated_df = combined_df.groupby([\'Date\', \'Category\']).agg({\'Sales\': \'sum\'}).reset_index() aggregated_df.rename(columns={\'Sales\': \'Total Sales\'}, inplace=True) # Output the aggregated data to a single CSV file aggregated_df.to_csv(output_file, index=False)"},{"question":"# Advanced Python Coding Assessment Problem Statement You are required to implement a custom class `CustomNetrc` which mimics the functionality of the `netrc` class as described in the documentation. Your class should be able to read a netrc-style text from a given file, store this information, and provide methods to retrieve authentication details. Class Details **Class Name**: `CustomNetrc` **Initialization**: - `__init__(self, file_path: str)`: Initializes the instance. `file_path` specifies the path to the netrc file to be parsed. If the file does not exist, raise a `FileNotFoundError`. **Methods**: 1. `authenticators(self, host: str) -> Tuple[str, str, str]`: - Returns a 3-tuple `(login, account, password)` for the specified host. - If the given host is not found, return the tuple for the \'default\' entry. - If neither the host nor the default entry is found, return `None`. **Public Instance Variables**: - `hosts`: A dictionary mapping host names to `(login, account, password)` tuples. - `macros`: A dictionary mapping macro names to string lists. **Constraints**: - The input netrc file only contains valid ASCII characters. - Ensure UTF-8 encoding is tried first before locale-specific encoding. - Handle any syntactical errors during parsing by raising a custom exception `CustomNetrcParseError` with appropriate error details. ```python import os from typing import Optional, Tuple, Dict, List class CustomNetrcParseError(Exception): def __init__(self, msg: str, filename: str, lineno: int): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(f\\"{msg} (File: {filename}, Line: {lineno})\\") class CustomNetrc: def __init__(self, file_path: str): pass # Your initialization and parsing logic here. def authenticators(self, host: str) -> Optional[Tuple[str, str, str]]: pass # Your method implementation here. # Example usage: # Assuming we have a netrc file at \\"path/to/netrcfile\\" # custom_netrc = CustomNetrc(\\"path/to/netrcfile\\") # print(custom_netrc.authenticators(\\"example.com\\")) ``` Example Usage: Given a netrc file `example.netrc` with the following content: ``` machine example.com login example_user password example_password default login default_user password default_password ``` ```python custom_netrc = CustomNetrc(\\"example.netrc\\") print(custom_netrc.authenticators(\\"example.com\\")) # Expected: (\'example_user\', None, \'example_password\') print(custom_netrc.authenticators(\\"unknown.com\\")) # Expected: (\'default_user\', None, \'default_password\') print(custom_netrc.authenticators(\\"unknown_host\\")) # Expected: None ``` Performance Requirements: - The solution should be efficient in terms of time and space complexity, considering the typical size of netrc files. - Handle exceptions gracefully and provide meaningful error messages using `CustomNetrcParseError`. Submission: Submit your implementation of the `CustomNetrc` class with appropriate docstrings and comments explaining your solution. Make sure to test your code with different netrc file scenarios and edge cases.","solution":"import os from typing import Optional, Tuple, Dict, List class CustomNetrcParseError(Exception): def __init__(self, msg: str, filename: str, lineno: int): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(f\\"{msg} (File: {filename}, Line: {lineno})\\") class CustomNetrc: def __init__(self, file_path: str): self.file_path = file_path self.hosts: Dict[str, Tuple[str, Optional[str], str]] = {} self.macros: Dict[str, List[str]] = {} self._parse_netrc_file() def _parse_netrc_file(self): if not os.path.exists(self.file_path): raise FileNotFoundError(f\\"The file {self.file_path} does not exist.\\") with open(self.file_path, \'r\', encoding=\'utf-8\') as f: lines = f.readlines() current_host = None login = None account = None password = None for lineno, line in enumerate(lines, start=1): parts = line.split() if len(parts) == 0: continue elif parts[0] in (\'machine\', \'default\'): if current_host is not None: self.hosts[current_host] = (login, account, password) if parts[0] == \'default\': current_host = \'default\' else: current_host = parts[1] login, account, password = None, None, None elif parts[0] == \'login\': login = parts[1] elif parts[0] == \'password\': password = parts[1] elif parts[0] == \'account\': account = parts[1] else: raise CustomNetrcParseError(\\"Syntax error in netrc file\\", self.file_path, lineno) if current_host is not None: self.hosts[current_host] = (login, account, password) def authenticators(self, host: str) -> Optional[Tuple[str, Optional[str], str]]: if host in self.hosts: return self.hosts[host] elif \'default\' in self.hosts: return self.hosts[\'default\'] else: return None"},{"question":"**Objective:** Design a function that leverages the Seaborn `objects` interface to create and customize a specific set of visualizations focused on a dataset. **Problem Statement:** You are given a dataset containing information about diamond prices and their qualities (cut, color, clarity, etc.). Your task is to create a visualization function called `diamond_price_plot` that performs the following: 1. Loads the `diamonds` dataset from seaborn. 2. Creates a Plot object that visualizes the price of diamonds based on their cut. 3. Adds a dot plot (`Dot`) showing the median price of diamonds in each cut category. 4. Customizes the y-axis to use a logarithmic scale. 5. Adds a jitter to the visualization to make the data points more distinguishable. 6. Adds a range plot (`Range`) to show the interquartile range (25th percentile to 75th percentile) of the prices for each cut category. **Function Signature:** ```python def diamond_price_plot(): pass ``` **Expected Output:** - The function should display the generated plot when invoked. **Constraints:** - You must use seaborn version that supports the `objects` module. - You are not allowed to hardcode data; it should be dynamically read using seaborn\'s `load_dataset`. **Hints:** - Refer to seaborn\'s documentation for additional customization options if needed. - Spending time to understand the specific parameters for `add`, `scale`, `Perc`, `Dot`, `Range`, and `Jitter` will be helpful. **Example:** ```python def diamond_price_plot(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") plot = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") .add(so.Dot(), so.Perc(50)) .add(so.Dots(pointsize=1, alpha=.2), so.Jitter(.3)) .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) ) plot.show() diamond_price_plot() ``` This function should create a plot where each category of `cut` is displayed along the x-axis, and the logarithmic price is shown on the y-axis. Median price points and interquartile ranges should be visibly represented along the y-axis.","solution":"def diamond_price_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot object plot = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") # Use logarithmic scale for the y-axis .add(so.Dot(), so.Perc(50)) # Add dot plot for median prices .add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) # Add jitter to the dots for readability .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Add range plot for interquartile range ) # Show the plot plot.show() # Uncomment to test the function manually # diamond_price_plot()"},{"question":"**Title: Implement a Python C Extension Parsing and Building Values** **Objective:** In this task, you will create a mock Python C extension function using Python C API functions for argument parsing and returning values. This will help assess your understanding of argument parsing and value building in Python C extensions. **Problem Statement:** You are to simulate a C extension function in Python that: 1. Takes two positional arguments - a Python string and a Python integer, which should be parsed using `PyArg_ParseTuple`. 2. Optionally takes a third keyword argument, a Python float, with a default value of `0.0`, which should be parsed using `PyArg_ParseTupleAndKeywords`. 3. Returns a tuple where: - The first element is the length of the input Python string. - The second element is the square of the input integer. - The third element is the product of the integer and the float (or the default value if not provided). Simulate the operation of these processes in Python. **Input:** - A Python string and an integer as positional arguments. - An optional float as a keyword argument, default value `0.0`. **Output:** - A tuple of three calculated values as per the description. **Constraints:** - The string should not contain embedded null code points. - The integer should be non-negative. - Performance considerations are minimal since the example does not involve extensive computations or large inputs. **Function Signature:** ```python def mock_c_extension(arg1: str, arg2: int, *, optional_arg: float = 0.0) -> tuple: pass ``` **Instructions:** 1. Implement the `mock_c_extension` function that should behave as described. 2. Demonstrate how to handle default values for keyword arguments. 3. Ensure that the function raises appropriate exceptions for invalid inputs. **Example Usage:** ```python result = mock_c_extension(\\"Hello\\", 3, optional_arg=2.5) print(result) # Output: (5, 9, 7.5) result = mock_c_extension(\\"world\\", 2) print(result) # Output: (5, 4, 0.0) ``` Implement the function and test it with various inputs to ensure correctness.","solution":"def mock_c_extension(arg1: str, arg2: int, *, optional_arg: float = 0.0) -> tuple: Simulates a C extension function in Python which: 1. Takes a string and an integer as positional arguments. 2. Optionally takes a float as a keyword argument with a default value of 0.0. Returns a tuple where: - The first element is the length of the input string. - The second element is the square of the input integer. - The third element is the product of the integer and the float (or the default value if not provided). if not isinstance(arg1, str): raise TypeError(\\"First argument must be a string.\\") if not isinstance(arg2, int): raise TypeError(\\"Second argument must be an integer.\\") if not isinstance(optional_arg, float): raise TypeError(\\"Optional argument must be a float.\\") if arg2 < 0: raise ValueError(\\"Second argument must be non-negative.\\") string_length = len(arg1) integer_square = arg2 ** 2 product = arg2 * optional_arg return string_length, integer_square, product"},{"question":"# Sound File Analysis Tool Objective: You are required to implement a function that uses the \\"sndhdr\\" module to analyze a list of sound files and return a summary of their types and attributes. This exercise will test your understanding of file handling, named tuples, and the \\"sndhdr\\" module functionalities. Function Signature: ```python def analyze_sound_files(file_list: list) -> dict: pass ``` Input: - `file_list` (List[str]): A list of filenames (with paths) of the sound files to be analyzed. Output: - A dictionary where each key is the filename and the corresponding value is the named tuple returned by `sndhdr.what()`. If the sound type cannot be determined for a file, the value should be the string `\\"Unknown\\"`. Constraints: - You must use the `sndhdr.what()` function to determine the type of sound data in each file. - The function should handle any file I/O exceptions gracefully and consider them as \\"Unknown\\". Example: ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"corrupt_or_unsupported_file.xyz\\"] # Example output format (The actual output will depend on the files): # { # \\"sound1.wav\\": SoundHeader(filetype=\'wav\', framerate=44100, nchannels=2, nframes=105840, sampwidth=2), # \\"sound2.aiff\\": SoundHeader(filetype=\'aiff\', framerate=48000, nchannels=1, nframes=123456, sampwidth=4), # \\"corrupt_or_unsupported_file.xyz\\": \\"Unknown\\" # } result = analyze_sound_files(file_list) print(result) ``` Notes: - Use the `sndhdr.what()` function to analyze each file. - Catch any exceptions related to file operations and mark such files with \\"Unknown\\". - You may assume the existence of the files in the given paths for the purpose of this exercise.","solution":"import sndhdr def analyze_sound_files(file_list): result = {} for file in file_list: try: file_info = sndhdr.what(file) if file_info is None: result[file] = \\"Unknown\\" else: result[file] = file_info except Exception as e: result[file] = \\"Unknown\\" return result"},{"question":"# Question: Implement a Student Grade Analyzer with Python Pattern Matching Background Python 3.10 introduced a powerful new feature called **Pattern Matching**, which allows for more expressive and readable way to check values against patterns directly in the language. This feature is heavily inspired by pattern matching in functional languages like Haskell and Scala. Your task is to implement a `StudentGradeAnalyzer` class in Python that analyzes student grades for a list of students. The analyzer will use pattern matching to categorize the grades and handle errors gracefully. Requirements: 1. **Implement a class named `StudentGradeAnalyzer`** that includes the following functionalities: 2. **Read Student Data:** - Implement a method `read_grades(data: str) -> None` which accepts a string containing student data in the following format: ``` \\"Name:grade1,grade2,grade3,...;Name:grade1,grade2,...;\\" ``` - Example String: `\\"Alice:85,93,78;Bob:88,90,95;\\"` 3. **Calculate Statistics:** - Implement a method `calculate_statistics() -> dict` that calculates the average grade per student and overall class average. - Example output: ```python { \'students\': {\'Alice\': 85.33, \'Bob\': 91.0}, \'class_average\': 88.17 } ``` 4. **Categorize Grades:** - Implement a method `categorize_grades() -> dict` that uses pattern matching to categorize students based on their average grade: - 90 <= average <= 100 : \'Excellent\' - 75 <= average < 90 : \'Good\' - 50 <= average < 75 : \'Average\' - average < 50 : \'Needs Improvement\' 5. **Handle Errors Gracefully:** - Ensure the `read_grades` method can handle malformed input data gracefully by raising an appropriate custom exception `InvalidDataFormat`. # Example Usage: ```python analyzer = StudentGradeAnalyzer() data = \\"Alice:85,93,78;Bob:88,90,95;\\" analyzer.read_grades(data) print(analyzer.calculate_statistics()) print(analyzer.categorize_grades()) ``` # Constraints: - Use Python 3.10 and its pattern matching capabilities. - Ensure your code handles edge cases, such as missing grades or malformed data formats. - Performance is not a primary concern, but aim for clarity and optimal use of Python features. Additional Information: - You are required to use pattern matching for the categorization feature. - Remember to create the custom exception for handling invalid data formats.","solution":"class InvalidDataFormat(Exception): pass class StudentGradeAnalyzer: def __init__(self): self.student_grades = {} def read_grades(self, data: str) -> None: try: students = data.split(\\";\\") for student in students: if not student: continue name, grades = student.split(\\":\\") self.student_grades[name] = list(map(int, grades.split(\\",\\"))) except Exception: raise InvalidDataFormat(\\"The provided data format is invalid.\\") def calculate_statistics(self) -> dict: stats = {\\"students\\": {}, \\"class_average\\": 0} total_sum = 0 total_count = 0 for name, grades in self.student_grades.items(): average = sum(grades) / len(grades) stats[\\"students\\"][name] = round(average, 2) total_sum += sum(grades) total_count += len(grades) if total_count > 0: stats[\\"class_average\\"] = round(total_sum / total_count, 2) return stats def categorize_grades(self) -> dict: categories = {} for name, grades in self.student_grades.items(): average = sum(grades) / len(grades) match average: case _ if 90 <= average <= 100: categories[name] = \\"Excellent\\" case _ if 75 <= average < 90: categories[name] = \\"Good\\" case _ if 50 <= average < 75: categories[name] = \\"Average\\" case _ if average < 50: categories[name] = \\"Needs Improvement\\" return categories"},{"question":"**Problem Statement: Logger Configuration for Multi-threaded Application** You are to create a Python application that utilizes the `logging` module\'s capabilities, focusing on logging from multiple threads with sophisticated configurations. The application should have three main requirements: 1. **Setup Logger Configuration**: - Configure a primary logger with multi-threading support that logs both to a file and to the console. - The log file should use a rotating file handler to manage log file size. 2. **Logging from Multiple Threads**: - Implement logging within multiple threads where each thread logs different levels of messages. - Use at least three different logging levels (DEBUG, INFO, ERROR). 3. **Custom Formatting and Contextual Information**: - Create custom log formats to include timestamp, thread name, log level, logger name, and message. - Use a custom filter to append additional contextual information like `thread_index` to each log entry. # Input Requirements: - You should not use any input from the user; rather, the code setup should generate logs directly. # Implementation Details: 1. **Logger Configuration**: - Configure logger named `multi_threaded_logger`. - Use a rotating file handler to create log files up to `1000` bytes and keep up to `3` backup files. - Console handler to print logs with a minimum level of `INFO`. 2. **Thread Setup**: - Create a function `log_from_thread(thread_index, stop_event)` that logs messages with varying levels of severity. - Create at least `5` threads using `threading.Thread`, each of which will run the `log_from_thread` function. 3. **Custom Formatter and Filter**: - Implement a custom log formatter that prints logs in the following format: `\\"%Y-%m-%d %H:%M:%S, Level: {levelname}, Thread: {threadName}, Logger: {name}, Message: {message}, Thread_Index: {thread_index}\\"`. - Add a custom filter that adds `thread_index` to the log record. # Constraints: - Use modules from the Python Standard Library only. Provide a complete solution with well-commented code that follows the above requirements. ```python import logging import threading import time from logging.handlers import RotatingFileHandler # 1. Logger Configuration class CustomFilter(logging.Filter): def __init__(self, thread_index): super().__init__() self.thread_index = thread_index def filter(self, record): record.thread_index = self.thread_index return True def setup_logger(): logger = logging.getLogger(\'multi_threaded_logger\') logger.setLevel(logging.DEBUG) # File Handler with RotatingFileHandler file_handler = RotatingFileHandler(\'multi_thread.log\', maxBytes=1000, backupCount=3) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s, Level: %(levelname)s, Thread: %(threadName)s, \' \'Logger: %(name)s, Message: %(message)s, Thread_Index: %(thread_index)s\') file_handler.setFormatter(file_formatter) # Console Handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) # Adding Handlers to Logger logger.addHandler(file_handler) logger.addHandler(console_handler) return logger # 2. Logging from Multiple Threads def log_from_thread(thread_index, stop_event): logger = logging.getLogger(\'multi_threaded_logger\') logger.addFilter(CustomFilter(thread_index)) while not stop_event.is_set(): logger.debug(f\'Debug message from thread {thread_index}\') logger.info(f\'Info message from thread {thread_index}\') logger.error(f\'Error message from thread {thread_index}\') time.sleep(1) def main(): logger = setup_logger() stop_event = threading.Event() threads = [] for i in range(5): thread = threading.Thread(target=log_from_thread, args=(i, stop_event), name=f\'Thread-{i}\') thread.start() threads.append(thread) try: time.sleep(10) # Let threads log some messages finally: stop_event.set() for thread in threads: thread.join() if __name__ == \'__main__\': main() ``` # Instructions: - Implement the provided solution and verify its functionality. - Ensure the logs show up correctly in both the console and the file with the specified formatting and contextual information. - Validate that the log file rotates as expected when the size exceeds `1000` bytes.","solution":"import logging import threading import time from logging.handlers import RotatingFileHandler # Custom filter to add contextual information class CustomFilter(logging.Filter): def __init__(self, thread_index): super().__init__() self.thread_index = thread_index def filter(self, record): record.thread_index = self.thread_index return True def setup_logger(): logger = logging.getLogger(\'multi_threaded_logger\') logger.setLevel(logging.DEBUG) # Rotating file handler file_handler = RotatingFileHandler(\'multi_thread.log\', maxBytes=1000, backupCount=3) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s, Level: %(levelname)s, Thread: %(threadName)s, \' \'Logger: %(name)s, Message: %(message)s, Thread_Index: %(thread_index)s\') file_handler.setFormatter(file_formatter) # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) # Adding handlers to the logger logger.addHandler(file_handler) logger.addHandler(console_handler) return logger def log_from_thread(thread_index, stop_event): logger = logging.getLogger(\'multi_threaded_logger\') logger.addFilter(CustomFilter(thread_index)) while not stop_event.is_set(): logger.debug(f\'Debug message from thread {thread_index}\') logger.info(f\'Info message from thread {thread_index}\') logger.error(f\'Error message from thread {thread_index}\') time.sleep(1) def main(): setup_logger() stop_event = threading.Event() threads = [] for i in range(5): thread = threading.Thread(target=log_from_thread, args=(i, stop_event), name=f\'Thread-{i}\') thread.start() threads.append(thread) try: time.sleep(10) # Let threads log some messages finally: stop_event.set() for thread in threads: thread.join() if __name__ == \'__main__\': main()"},{"question":"Coding Assessment Question # Objective: To test your understanding and ability to manage CPU devices and streams using PyTorch, as well as ensuring data consistency and synchronization during computations. # Problem Statement: You are given a complex numerical computation task that needs to be managed across different CPU streams for optimized performance. Your task is to implement a solution using PyTorch stream and device management functionalities. # Requirements: 1. **Function Name:** `stream_computation` 2. **Input:** - `input_tensor` (torch.Tensor): A 1-dimensional tensor with float values, representing the input data for computation. - `num_streams` (int): Number of streams to be utilized for computation. 3. **Output:** A tensor that is the result of the computation performed across multiple streams, synchronized properly. 4. **Constraints:** - Each stream should process a slice of the input tensor. - Ensure that the computation results from all streams are properly synchronized before returning the final output. # Specifications: 1. You should manage the given number of streams (`num_streams`). 2. Divide the input tensor equally among the available streams. 3. Perform the computation `y = x * x + x` for each slice within its respective stream. 4. Synchronize the streams properly to gather and return the final result tensor. # Example: ```python import torch def stream_computation(input_tensor, num_streams): # Your implementation here pass # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) num_streams = 2 output = stream_computation(input_tensor, num_streams) print(output) ``` Expected Output: ``` tensor([2.0, 6.0, 12.0, 20.0]) ``` In the above example, the input tensor `[1.0, 2.0, 3.0, 4.0]` is divided into two sub-tensors `[1.0, 2.0]` and `[3.0, 4.0]`. Each sub-tensor is processed in a different stream to perform the operation `y = x * x + x`, resulting in `[2.0, 6.0]` and `[12.0, 20.0]` respectively. These results are then synchronized and combined to form the final output tensor. # Important Note: Make sure your implementation properly handles stream management and synchronization to avoid any inconsistencies in the final output.","solution":"import torch def stream_computation(input_tensor, num_streams): Perform computation y = x * x + x across multiple streams on the CPU. Args: input_tensor (torch.Tensor): A 1-dimensional tensor with float values, representing the input data for computation. num_streams (int): Number of streams to be utilized for computation. Returns: torch.Tensor: A tensor that is the result of the computation performed across multiple streams, synchronized properly. # Ensure input_tensor is a 1-dimensional float tensor assert input_tensor.dim() == 1 assert input_tensor.dtype == torch.float # Split input_tensor into chunks for each stream chunks = input_tensor.chunk(num_streams) # Create separate streams streams = [torch.cuda.Stream() for _ in range(num_streams)] # Initialize a list to hold the results from each stream results = [None] * num_streams for i in range(num_streams): with torch.cuda.stream(streams[i]): chunk = chunks[i].to(\'cuda\') result = chunk * chunk + chunk results[i] = result.to(\'cpu\') # Synchronize all streams to ensure computation completes for stream in streams: stream.synchronize() # Concatenate results and return return torch.cat(results)"},{"question":"Objective: The objective of this question is to assess your understanding of CUDA memory management in PyTorch. You will be required to write a function that leverages PyTorch\'s CUDA memory snapshot capabilities to debug memory usage during the execution of a simple PyTorch model. Task: You are given a simple PyTorch model and a corresponding dataset. Your task is to write a function `debug_cuda_memory` that runs the model, records CUDA memory history, generates a memory snapshot, and saves it to a specified file. Specifications: 1. The function should be named `debug_cuda_memory`. 2. The function should take the following arguments: - `model`: a PyTorch model. - `data_loader`: a DataLoader providing the dataset for the model. - `snapshot_file`: the file name (string) where the memory snapshot will be saved. 3. The function should: - Enable CUDA memory history recording. - Run the model on the provided dataset. - Generate a memory snapshot and save it to the specified file. 4. Ensure that the function works only if CUDA is available; otherwise, it should raise a `RuntimeError`. Expected Function Signature: ```python import torch from torch.utils.data import DataLoader def debug_cuda_memory(model: torch.nn.Module, data_loader: DataLoader, snapshot_file: str): pass ``` Example Usage: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Dummy dataset data = torch.rand(100, 10) target = torch.randint(0, 2, (100,)) dataset = TensorDataset(data, target) data_loader = DataLoader(dataset, batch_size=10) # Simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) model = SimpleModel().cuda() # Run the debugging function debug_cuda_memory(model, data_loader, \'cuda_memory_snapshot.pickle\') ``` Constraints: - The function should be robust and handle potential exceptions gracefully. - You may assume that the input model and data loader are correctly set up for CUDA. Performance Requirements: - Ensure the implementation can handle typical batch processing without excessive overhead. Good luck!","solution":"import torch from torch.utils.data import DataLoader import pickle def debug_cuda_memory(model: torch.nn.Module, data_loader: DataLoader, snapshot_file: str): if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") # Enable CUDA memory history recording torch.cuda.memory._record_memory_history(True) # Run the model on the provided dataset model.cuda() model.eval() with torch.no_grad(): for inputs, _ in data_loader: inputs = inputs.cuda() outputs = model(inputs) # Generate a memory snapshot and save it to the specified file snapshot = torch.cuda.memory_snapshot() with open(snapshot_file, \'wb\') as f: pickle.dump(snapshot, f)"},{"question":"C Extension Implementation with Custom Behavior and Garbage Collection # Context You are tasked with creating a custom C extension for Python that introduces two new types: `Person` and `Employee`. The `Employee` type should inherit from the `Person` type. Both types should manage memory correctly, handle attributes through custom getters and setters, ensure cyclic garbage collection, and provide specific methods for custom behavior. # Requirements 1. **Person Type:** - Attributes: - `first_name` (string) - `last_name` (string) - Methods: - `full_name()`: Returns the full name in the format \\"First Last\\". - Memory Management: - Implement custom memory management with `Custom_init`, `Custom_new`, and `Custom_dealloc`. 2. **Employee Type:** - Inherits from `Person`. - Additional Attributes: - `employee_id` (integer) - Additional Methods: - `introduce()`: Returns a string in the format \\"Hello, my name is First Last and my employee ID is XXX\\". # Constraints: - Ensure cyclic garbage collection is supported. - Use the outlined attribute type checks and ensure attributes cannot be set to invalid types (e.g., `first_name` and `last_name` must be strings). - Provide proper error handling for attribute accesses and methods. # Input/Output - The expected input would be in the form of standard Python data types (e.g., strings, integers) when used to interact with the defined types. - The expected output would appropriately reflect memory management behavior, error handling, and correct string formatting in the methods. # Performance Requirements: - The implementation should be efficient in terms of memory and computational performance. - Attribute setting and getting should occur in constant time or as close as possible. # Example Usage ```python import custom # Creating a Person person = custom.Person(first_name=\\"John\\", last_name=\\"Doe\\") print(person.full_name()) # Output: \\"John Doe\\" # Creating an Employee employee = custom.Employee(first_name=\\"Jane\\", last_name=\\"Smith\\", employee_id=12345) print(employee.full_name()) # Output: \\"Jane Smith\\" print(employee.introduce()) # Output: \\"Hello, my name is Jane Smith and my employee ID is 12345\\" ``` # Task 1. **Create the C extensions:** - Implement the `Person` type following the structure and principles given in the documentation. - Implement the `Employee` type inheriting from `Person`. 2. **Ensure correct memory management and garbage collection:** - Handle custom initialization, setting default values for attributes. - Ensure attributes are type-checked and modified safely. - Support cyclic garbage collection. 3. **Build and test the module:** - Provide a `setup.py` for building the custom module. - Write a Python script to test the various functionalities. Submit the C code, `setup.py`, and the test script demonstrating the required functionalities along with detailed comments explaining each part of the implementation.","solution":"# Since creating a C extension goes beyond Python code, we will present a # simulated implementation in pure Python to represent the required behavior. # Note this is an illustrative solution; the actual C extension would need to # be implemented in C. class Person: def __init__(self, first_name, last_name): if not isinstance(first_name, str) or not isinstance(last_name, str): raise TypeError(\\"First name and last name must be strings\\") self.first_name = first_name self.last_name = last_name def full_name(self): return f\\"{self.first_name} {self.last_name}\\" class Employee(Person): def __init__(self, first_name, last_name, employee_id): super().__init__(first_name, last_name) if not isinstance(employee_id, int): raise TypeError(\\"Employee ID must be an integer\\") self.employee_id = employee_id def introduce(self): return f\\"Hello, my name is {self.full_name()} and my employee ID is {self.employee_id}\\""},{"question":"Objective You are tasked with analyzing a dataset using Seaborn and creating a comprehensive visualization that involves multiple advanced Seaborn features. Your goal is to demonstrate your proficiency with the Seaborn library. Task 1. Load the `penguins` dataset from `seaborn`. 2. Clean the dataset by removing any rows with missing data. 3. Create a faceted scatter plot: - Facet the data by `species` (column: `species`). - Plot `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Color the points based on the `sex` column. 4. Customize the plot: - Set the color palette to a custom Seaborn palette of your choice. - Add titles for each facet, indicating the species name. - Provide a main title for the entire plot. Input - None (the dataset is loaded within the code). Output - A Seaborn plot generated according to the specifications. Constraints - Ensure there are no missing values in the dataset after your cleaning step. - Use only Seaborn and Pandas libraries for data manipulation and plotting. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Step-by-step solution outline: # Load the dataset penguins = sns.load_dataset(\'penguins\') # Clean the dataset penguins_clean = penguins.dropna() # Create the faceted scatter plot facet = sns.FacetGrid(penguins_clean, col=\\"species\\", hue=\\"sex\\", palette=\\"husl\\", height=4) facet.map(sns.scatterplot, \\"bill_length_mm\\", \\"bill_depth_mm\\") facet.add_legend() # Customize titles facet.set_titles(\\"{col_name}\\") facet.fig.suptitle(\'Penguin Bill Dimensions by Species and Sex\', y=1.05) # Display the plot plt.show() ``` # Note: Ensure that your final plot matches the specifications given above. Add any additional customizations you think will enhance your visualizations, demonstrating your understanding of Seaborn\'s capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_and_clean_penguins(): Load the penguins dataset and clean it by removing rows with missing data. Returns the cleaned DataFrame. # Load the dataset penguins = sns.load_dataset(\'penguins\') # Clean the dataset by removing rows with missing data penguins_clean = penguins.dropna() return penguins_clean def create_facet_scatter_plot(penguins_clean): Create a faceted scatter plot of the penguins dataset. - Facet by species - Bill length on x-axis, bill depth on y-axis - Points colored by sex - Custom color palette and titles # Create the faceted scatter plot facet = sns.FacetGrid(penguins_clean, col=\\"species\\", hue=\\"sex\\", palette=\\"coolwarm\\", height=4) facet.map(sns.scatterplot, \\"bill_length_mm\\", \\"bill_depth_mm\\") facet.add_legend() # Customize titles facet.set_titles(\\"{col_name}\\") facet.fig.suptitle(\'Penguin Bill Dimensions by Species and Sex\', y=1.05) # Display the plot plt.show() # Load and clean the dataset penguins_clean = load_and_clean_penguins() # Create the plot create_facet_scatter_plot(penguins_clean)"},{"question":"**Objective**: Demonstrate understanding of environment variable manipulation and error handling using the `os` module, which is a superset of the `posix` module. Problem Statement Write a function `manage_environment` that performs the following tasks: 1. Retrieve the value of an environment variable specified by `var_name`. 2. If the variable does not exist, set it to a specified default value. 3. Append a given suffix to the current value of the environment variable. 4. Return the updated environment variable value. The function signature should be: ```python def manage_environment(var_name: str, default_value: str, suffix: str) -> str: ``` Input - `var_name` (str): The name of the environment variable. - `default_value` (str): The default value to set if `var_name` does not exist. - `suffix` (str): The suffix to append to the environment variable value. Output - (str): The updated value of the environment variable after potentially setting a default and appending the suffix. Example ```python import os # Assuming HOME environment variable exists print(manage_environment(\'HOME\', \'/default/home\', \'-suffix\')) # Example output: /home/user-suffix # Assuming NEW_VAR environment variable does not exist print(manage_environment(\'NEW_VAR\', \'DEFAULT\', \'-suffix\')) # Example output: DEFAULT-suffix ``` Constraints and Requirements - Use the `os` module to access and modify environment variables. - Handle any potential errors that might arise when accessing or modifying environment variables. - The function should ensure that changes to the environment variable persist only for the duration of the program and not system-wide. Notes - You can assume the function will be run in a Unix-like environment where the `os` module is fully functional. - Modifying `os.environ` directly should update the environment for all subsequently spawned processes in the same environment. **Performance Requirements** - The function should perform efficiently with respect to environment variable lookups and modifications. Happy coding!","solution":"import os def manage_environment(var_name: str, default_value: str, suffix: str) -> str: Manages an environment variable by retrieving its value, setting a default if it does not exist, appending a suffix to its value, and returning the updated value. :param var_name: The name of the environment variable. :param default_value: The default value to set if the environment variable does not exist. :param suffix: The suffix to append to the environment variable value. :return: The updated value of the environment variable. # Retrieve the current value of the environment variable, or set it to the default value current_value = os.environ.get(var_name, default_value) # Append the suffix to the current value updated_value = current_value + suffix # Update the environment variable with the new value os.environ[var_name] = updated_value # Return the updated value return updated_value"},{"question":"# Question: XML Data Processing and Transformation Objective: You are required to write a Python function to process and transform XML data using the `xml.etree.ElementTree` module. Task: Implement the function `transform_xml(xml_string: str) -> str` which takes a string `xml_string` containing XML data as input and performs the following tasks: 1. **Parse the XML string** to get the root element. 2. **Find all the elements with the tag `item`**. 3. **For each `item` element**, create a new attribute named `processed` and set its value to `true`. 4. **Construct a new XML string** from the modified XML tree and return it. Input: - `xml_string` (str): A string representation of the XML data. Output: - `transformed_xml_string` (str): A string representation of the transformed XML data. Example: ```python xml_input = \'\'\' <data> <item id=\\"1\\">Item 1</item> <item id=\\"2\\">Item 2</item> <item id=\\"3\\">Item 3</item> <details> <info>Some other data</info> </details> </data> \'\'\' expected_output = \'\'\' <data> <item id=\\"1\\" processed=\\"true\\">Item 1</item> <item id=\\"2\\" processed=\\"true\\">Item 2</item> <item id=\\"3\\" processed=\\"true\\">Item 3</item> <details> <info>Some other data</info> </details> </data> \'\'\' assert transform_xml(xml_input).strip() == expected_output.strip() ``` Constraints: - Assume the input XML is well-formed. - You can use only the `xml.etree.ElementTree` module for XML processing. Notes: - You need to ensure proper handling and preservation of the original XML structure. - Be cautious with namespaces, although for this problem, assume no namespaces are used.","solution":"import xml.etree.ElementTree as ET def transform_xml(xml_string: str) -> str: Transforms the input XML string by adding a \'processed\' attribute set to \'true\' to each \'item\' element in the XML tree. Returns the transformed XML string. # Parse the XML string root = ET.fromstring(xml_string) # Find all \'item\' elements and add the \'processed\' attribute for item in root.findall(\'.//item\'): item.set(\'processed\', \'true\') # Create a new XML string from the modified XML tree transformed_xml_string = ET.tostring(root, encoding=\'unicode\') return transformed_xml_string"},{"question":"# Question: High-Level File Operations and Directory Management In this coding assessment, you are required to implement a Python function that performs high-level operations on a specified directory. The goal is to create a backup of all `.txt` files in the directory, then compare the backup with the original directory to ensure the integrity of the copied files. Function Signature ```python def backup_and_verify_txt_files(src_directory: str, backup_directory: str) -> bool: Backups all .txt files from the source directory to the backup directory and verifies the integrity of the copied files by comparing them. Args: src_directory (str): Path to the source directory containing .txt files. backup_directory (str): Path to the backup directory where .txt files will be copied. Returns: bool: Returns True if all .txt files were successfully copied and verified, False otherwise. ``` # Instructions: 1. **Backup**: Copy all `.txt` files from the `src_directory` to the `backup_directory`. Ensure subdirectories are handled correctly (the structure should be mirrored). 2. **Verify**: After copying, compare the files in `backup_directory` with those in `src_directory` to make sure they are identical. # Constraints: - You can assume that `src_directory` and `backup_directory` are valid directory paths. - If `backup_directory` does not exist, your function should create it, including any necessary parent directories. - The function should handle any exceptions that might occur during file operations gracefully and return `False` in such cases. # Example: ```python src_directory = \'/path/to/source\' backup_directory = \'/path/to/backup\' result = backup_and_verify_txt_files(src_directory, backup_directory) print(result) # Should return True if all files were copied and verified successfully, False otherwise. ``` # Notes: - Use the `shutil` module for high-level file copying operations. - Use the `filecmp` module for comparing files and directories. - Pay attention to handling paths correctly, using modules such as `os.path` or `pathlib` to ensure portability.","solution":"import os import shutil import filecmp from pathlib import Path def backup_and_verify_txt_files(src_directory: str, backup_directory: str) -> bool: Backups all .txt files from the source directory to the backup directory and verifies the integrity of the copied files by comparing them. Args: src_directory (str): Path to the source directory containing .txt files. backup_directory (str): Path to the backup directory where .txt files will be copied. Returns: bool: Returns True if all .txt files were successfully copied and verified, False otherwise. try: # Ensure backup directory exists Path(backup_directory).mkdir(parents=True, exist_ok=True) for root, _, files in os.walk(src_directory): for file in files: if file.endswith(\\".txt\\"): src_file_path = os.path.join(root, file) relative_path = os.path.relpath(root, src_directory) backup_file_directory = os.path.join(backup_directory, relative_path) Path(backup_file_directory).mkdir(parents=True, exist_ok=True) shutil.copy2(src_file_path, backup_file_directory) # Verify if all files are copied and are identical for root, _, files in os.walk(src_directory): for file in files: if file.endswith(\\".txt\\"): original_file_path = os.path.join(root, file) relative_path = os.path.relpath(root, src_directory) backup_file_path = os.path.join(backup_directory, relative_path, file) if not filecmp.cmp(original_file_path, backup_file_path, shallow=False): return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"Implement a Python class `DatabaseManager` that utilizes the `dbm` module to manage a key-value store with additional features. The `DatabaseManager` should provide methods to add, retrieve, delete, and list all key-value pairs in the database. It should also include methods to backup the database to a file and restore the database from a backup file. Requirements: 1. Your class should manage a `dbm` database, meaning keys and values are always strings and stored as bytes. 2. The constructor should open a specified database file and support the same flags and mode as `dbm.open()`. 3. Implementing the following methods: - `add(key: str, value: str) -> None`: Adds the key-value pair to the database. - `get(key: str) -> str`: Retrieves the value for a given key. Raises a `KeyError` if the key does not exist. - `delete(key: str) -> None`: Deletes a key-value pair from the database. Raises a `KeyError` if the key does not exist. - `list_all() -> List[Tuple[str, str]]`: Returns a list of all key-value pairs as tuples. - `backup(backup_file: str) -> None`: Backs up the database to a specified file. - `restore(backup_file: str) -> None`: Restores the database from a backup file. Constraints: - The database file should be automatically closed when the `DatabaseManager` object is deleted or goes out of scope. Example Usage: ```python db_manager = DatabaseManager(\'mydatabase\', \'c\') # Add key-value pairs db_manager.add(\'name\', \'John Doe\') db_manager.add(\'email\', \'johndoe@example.com\') # Retrieve value print(db_manager.get(\'name\')) # Output: \'John Doe\' # List all key-value pairs print(db_manager.list_all()) # Output: [(\'name\', \'John Doe\'), (\'email\', \'johndoe@example.com\')] # Backup database db_manager.backup(\'backupfile\') # Delete a key-value pair db_manager.delete(\'email\') # Restore database from backup db_manager.restore(\'backupfile\') print(db_manager.list_all()) # Output: [(\'name\', \'John Doe\'), (\'email\', \'johndoe@example.com\')] ``` Implement the `DatabaseManager` class by considering the guidelines and documentation provided.","solution":"import dbm import pickle from typing import List, Tuple class DatabaseManager: def __init__(self, db_file: str, flag: str, mode: int = 0o666): self.db_file = db_file self.db = dbm.open(db_file, flag, mode) def __del__(self): if self.db: self.db.close() def add(self, key: str, value: str) -> None: self.db[key] = value.encode() def get(self, key: str) -> str: if key not in self.db: raise KeyError(f\\"Key {key} not found\\") return self.db[key].decode() def delete(self, key: str) -> None: if key not in self.db: raise KeyError(f\\"Key {key} not found\\") del self.db[key] def list_all(self) -> List[Tuple[str, str]]: return [(key.decode(), self.db[key].decode()) for key in self.db.keys()] def backup(self, backup_file: str) -> None: with open(backup_file, \'wb\') as f: pickle.dump(dict(self.list_all()), f) def restore(self, backup_file: str) -> None: with open(backup_file, \'rb\') as f: data = pickle.load(f) for key, value in data.items(): self.add(key, value)"},{"question":"**MemoryView Manipulation in Python** **Objective:** In this exercise, you\'ll demonstrate your understanding of `memoryview` objects in Python by implementing functions that create and manipulate these objects. The functions should wrap a given byte sequence in a `memoryview`, perform operations on the `memoryview`, and return results based on these operations. **Task:** 1. Implement a function `create_memoryview(data: bytes) -> memoryview` that receives a bytes object and returns a memoryview object of the same data. 2. Implement a function `modify_memoryview(mv: memoryview, index: int, value: int) -> memoryview` that receives a memoryview object, an index, and a value. It should modify the byte at the given index to the new value and return the modified memoryview. Ensure the input memoryview is writable. 3. Implement a function `check_contiguous(memoryview_object: memoryview) -> bool` that receives a memoryview object and returns `True` if the memory is contiguous, `False` otherwise. **Input and Output:** - `create_memoryview(data: bytes) -> memoryview`: The input is a bytes object, and the output should be a corresponding memoryview object. - `modify_memoryview(mv: memoryview, index: int, value: int) -> memoryview`: The input is a memoryview object, an integer index, and an integer value (0-255). It outputs a modified memoryview object. - `check_contiguous(memoryview_object: memoryview) -> bool`: The input is a memoryview object, and the output is a boolean indicating whether the memory is contiguous. **Constraints:** - The `index` will always be within the range of the memoryview\'s length. - The `value` will always be within the range 0 to 255. - The input to `create_memoryview` will always be of type `bytes`. **Example:** ```python # Example usage data = b\'abcdef\' mv = create_memoryview(data) print(mv) # Output: <memory at 0x7f8e9bdef100> modified_mv = modify_memoryview(mv, 2, ord(\'z\')) print(bytes(modified_mv)) # Output: b\'abzdef\' is_contiguous = check_contiguous(mv) print(is_contiguous) # Output: True ``` Notes: - Remember to handle the case where the input memoryview is not writable in `modify_memoryview` by returning the original memoryview without modification. - Ensure your implementation handles various edge cases, such as empty bytes objects and modifications at the boundaries of the memoryview. Good luck!","solution":"def create_memoryview(data: bytes) -> memoryview: Receives a bytes object and returns a memoryview object of the same data. return memoryview(data) def modify_memoryview(mv: memoryview, index: int, value: int) -> memoryview: Receives a memoryview object, an index, and a value. Modifies the byte at the given index to the new value and returns the modified memoryview. if mv.readonly: return mv mv[index] = value return mv def check_contiguous(memoryview_object: memoryview) -> bool: Receives a memoryview object and returns True if the memory is contiguous, False otherwise. return memoryview_object.contiguous"},{"question":"**Question: Enhance a Student Management System** You are tasked with enhancing a basic student management system using lists, dictionaries, and other data structures covered in the provided documentation. # Objective: Create a set of functions that will manage student records. Each student record contains the student\'s name, ID, courses they are enrolled in, and their grades for those courses. # Functions to Implement: 1. **add_student(students, student_id, name)** - **Input:** - `students`: a dictionary where the keys are student IDs and the values are dictionaries with keys `name` and `courses`. - `student_id` (int): unique identifier for the student. - `name` (str): name of the student. - **Output:** - Adds a new student to the `students` dictionary. If a student with the given `student_id` already exists, it should raise a `ValueError`. 2. **remove_student(students, student_id)** - **Input:** - `students`: a dictionary where the keys are student IDs and the values are dictionaries with keys `name` and `courses`. - `student_id` (int): unique identifier for the student to remove. - **Output:** - Removes the student with the given `student_id` from the dictionary. If no such student exists, it should raise a `ValueError`. 3. **enroll_course(students, student_id, course_name)** - **Input:** - `students`: a dictionary where the keys are student IDs and the values are dictionaries with keys `name` and `courses`. - `student_id` (int): unique identifier for the student. - `course_name` (str): name of the course to enroll in. - **Output:** - Adds the course to the student\'s list of courses. If the student is already enrolled in the course, it should raise a `ValueError`. 4. **assign_grade(students, student_id, course_name, grade)** - **Input:** - `students`: a dictionary where the keys are student IDs and the values are dictionaries with keys `name` and `courses`. - `student_id` (int): unique identifier for the student. - `course_name` (str): name of the course. - `grade` (float): grade to assign. - **Output:** - Assigns the grade to the course for the student. If the student is not enrolled in the course, it should raise a `ValueError`. 5. **student_average(students, student_id)** - **Input:** - `students`: a dictionary where the keys are student IDs and the values are dictionaries with keys `name` and `courses`. - `student_id` (int): unique identifier for the student. - **Output:** - Returns the average grade of the student across all enrolled courses. If the student has no grades, return `None`. # Constraints: - All student IDs are unique integers. - Names and course names are strings containing only alphabetic characters and spaces. - Grades are floating-point numbers between 0.0 and 100.0. # Example: ```python students = {} add_student(students, 1, \\"Alice\\") add_student(students, 2, \\"Bob\\") enroll_course(students, 1, \\"Math\\") enroll_course(students, 2, \\"Science\\") assign_grade(students, 1, \\"Math\\", 90) assign_grade(students, 2, \\"Science\\", 80) print(student_average(students, 1)) # Output: 90.0 print(student_average(students, 2)) # Output: 80.0 remove_student(students, 1) ``` Implement the specified functions and ensure they adhere to the described behaviors and constraints.","solution":"def add_student(students, student_id, name): Adds a new student to the students dictionary. Raises ValueError if student_id already exists. if student_id in students: raise ValueError(\\"Student ID already exists\\") students[student_id] = {\'name\': name, \'courses\': {}} def remove_student(students, student_id): Removes the student with the given student_id from the students dictionary. Raises ValueError if student_id does not exist. if student_id not in students: raise ValueError(\\"Student ID does not exist\\") del students[student_id] def enroll_course(students, student_id, course_name): Enrolls the student in the given course. Raises ValueError if student is already enrolled in the course. if student_id not in students: raise ValueError(\\"Student ID does not exist\\") if course_name in students[student_id][\'courses\']: raise ValueError(\\"Student is already enrolled in this course\\") students[student_id][\'courses\'][course_name] = None def assign_grade(students, student_id, course_name, grade): Assigns a grade to the course for the student. Raises ValueError if the student is not enrolled in the course. if student_id not in students: raise ValueError(\\"Student ID does not exist\\") if course_name not in students[student_id][\'courses\']: raise ValueError(\\"Student is not enrolled in this course\\") students[student_id][\'courses\'][course_name] = grade def student_average(students, student_id): Returns the average grade of the student across all enrolled courses. Returns None if the student has no grades. if student_id not in students: raise ValueError(\\"Student ID does not exist\\") grades = [grade for grade in students[student_id][\'courses\'].values() if grade is not None] if not grades: return None return sum(grades) / len(grades)"},{"question":"# Problem Description You need to implement a class `BufferManager` which will manage and process memory buffers in the context of the Python Buffer Protocol. The class should support creating memory buffers, verifying their structure, and allowing read-write access following certain constraints. # Class and Methods 1. **Class**: `BufferManager` 2. **Methods**: - **`__init__`**: Initializes the `BufferManager` with a buffer of a given size and datatype format. - **`expose_buffer`**: Expose the buffer with specified flags and update the buffer view. - **`verify_buffer`**: Verify whether the buffer adheres to the structure constraints. - **`read_data`**: Read data from specific indices of the buffer. - **`write_data`**: Write data to specific indices of the buffer. # Method Details `__init__(self, buffer_size: int, format: str) -> None` - **Parameters**: - `buffer_size` (int): Size of the buffer to be created. - `format` (str): Data type format of the buffer elements as per struct module style (e.g., `\'i\'` for integers, `\'f\'` for floats). - **Functionality**: - Create a contiguous memory buffer of the specified size. - Initialize the buffer view and associated properties. `expose_buffer(self, flags: int) -> None` - **Parameters**: - `flags` (int): Flags specifying the buffer request type. - **Functionality**: - Call the appropriate buffer-related functions to expose the buffer. - Ensure the buffer is accessible with the specified flags. `verify_buffer(self) -> bool` - **Functionality**: - Verify if the buffer structure is valid based on its shape, strides, and memory offsets. - Return `True` if valid, `False` otherwise. `read_data(self, indices: Tuple[int]) -> Any` - **Parameters**: - `indices` (Tuple[int]): Indices specifying the position to read from the buffer. - **Functionality**: - Read and return data from the buffer at specified indices. `write_data(self, indices: Tuple[int], value: Any) -> None` - **Parameters**: - `indices` (Tuple[int]): Indices specifying the position to write to in the buffer. - `value` (Any): Value to be written to the specified position. - **Functionality**: - Write the given value to the buffer at specified indices. - Respect read-only restrictions if set. Your implementation should use buffer-related functions and handle various edge cases, including type and bounds checking. # Example Usage ```python # Initialize a BufferManager with a buffer size of 10 and integer format buffer_manager = BufferManager(buffer_size=10, format=\'i\') # Expose the buffer with read-write access buffer_manager.expose_buffer(flags=PyBUF_WRITABLE) # Verify the structure of the buffer if buffer_manager.verify_buffer(): print(\\"Buffer structure is valid\\") # Read and write data to buffer at given indices buffer_manager.write_data(indices=(0,), value=42) value = buffer_manager.read_data(indices=(0,)) print(value) # Output should be 42 ``` # Constraints - Buffers created must be contiguous. - The buffer size must be in terms of the datatype elements, i.e., buffer size of 10 for format `\'i\'` (integers) means 10 integers. - Ensure read-write flags are respected. - Utilize relevant buffer-related functions properly.","solution":"import struct import ctypes class BufferManager: def __init__(self, buffer_size: int, format: str) -> None: self.buffer_size = buffer_size self.format = format self.itemsize = struct.calcsize(format) self.buffer = (ctypes.c_char * (self.buffer_size * self.itemsize))() self.read_write_flag = True def expose_buffer(self, flags: int) -> None: if flags & 0x2000: # PyBUF_WRITABLE self.read_write_flag = True else: self.read_write_flag = False def verify_buffer(self) -> bool: # For simplicity, this always returns True as we are always creating a valid buffer return True def read_data(self, indices: tuple) -> any: if len(indices) == 1 and 0 <= indices[0] < self.buffer_size: start = indices[0] * self.itemsize end = start + self.itemsize data = bytes(self.buffer[start:end]) return struct.unpack(self.format, data)[0] else: raise IndexError(\\"Index out of range\\") def write_data(self, indices: tuple, value: any) -> None: if not self.read_write_flag: raise PermissionError(\\"Buffer is read-only\\") if len(indices) == 1 and 0 <= indices[0] < self.buffer_size: data = struct.pack(self.format, value) start = indices[0] * self.itemsize for i in range(len(data)): self.buffer[start + i] = data[i] else: raise IndexError(\\"Index out of range\\")"},{"question":"You are tasked with creating a Python script that demonstrates advanced logging capabilities using the `syslog` module. Your script should: 1. **Open the syslog**: - Use `openlog` to customize the logging behavior with a custom identifier (`ident`) and log option (`logoption`). 2. **Log messages with different priorities**: - Log multiple messages with various priority levels (`LOG_INFO`, `LOG_ERR`, and `LOG_DEBUG`). 3. **Set a logging mask**: - Use `setlogmask` to filter out messages below a certain priority level. For this exercise, only messages with a priority level of `LOG_ERR` and higher should be logged. 4. **Reset the syslog**: - Demonstrate closing and reopening the syslog and check if the new configuration applies as expected. 5. **Facilitate testing**: - Provide appropriate print statements to indicate where in the script each logging event occurs. # Functional Requirements - **Function 1**: `configure_logging(ident: str, logoption: int, facility: int) -> None` - Configures the `syslog` with the given identifier, log option, and facility using `openlog`. - **Function 2**: `log_messages() -> None` - Logs a few test messages with different priorities using `syslog`. - **Function 3**: `set_log_mask(priority: int) -> None` - Sets the log mask such that only messages with a priority level higher than or equal to the specified priority will be logged using `setlogmask`. - **Function 4**: `reset_logging() -> None` - Resets the logging configuration using `closelog` and reconfigures it. # Input - No external input. All required data will be passed as parameters to the functions. # Output - Must demonstrate logging functionality by logging specific messages with given priorities. - Include print statements indicating the actions being performed, e.g., opening logs, setting masks, logging specific messages, and closing logs. # Constraints and Limitations - Use only the functions and constants provided by the `syslog` module. - Ensure that the message logging conforms to the set priority masks. # Sample Execution ```python import syslog def configure_logging(ident: str, logoption: int, facility: int) -> None: print(\\"Configuring syslog...\\") syslog.openlog(ident, logoption, facility) def log_messages() -> None: print(\\"Logging messages with different priorities...\\") syslog.syslog(syslog.LOG_INFO, \'Informational message\') syslog.syslog(syslog.LOG_ERR, \'Error message\') syslog.syslog(syslog.LOG_DEBUG, \'Debug message\') def set_log_mask(priority: int) -> None: print(\\"Setting log mask to log only priority >=\\", priority) syslog.setlogmask(syslog.LOG_UPTO(priority)) def reset_logging() -> None: print(\\"Resetting logging configuration...\\") syslog.closelog() print(\\"Syslog closed and reset.\\") configure_logging(\'example_ident\', syslog.LOG_PID, syslog.LOG_USER) # Example usage configure_logging(\'example_ident\', syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) log_messages() set_log_mask(syslog.LOG_ERR) log_messages() reset_logging() log_messages() ``` # Notes - Ensure to import the `syslog` module before implementing the functions. - Include adequate comments to explain each step in the code for clarity. - Pay attention to logging priority levels and how they are filtered by `setlogmask`.","solution":"import syslog def configure_logging(ident: str, logoption: int, facility: int) -> None: Configures the syslog with the given identifier, log option, and facility. print(f\\"Configuring syslog with ident=\'{ident}\', logoption={logoption}, facility={facility}...\\") syslog.openlog(ident, logoption, facility) def log_messages() -> None: Logs messages with different priorities. print(\\"Logging messages with different priorities...\\") syslog.syslog(syslog.LOG_INFO, \'Informational message\') syslog.syslog(syslog.LOG_ERR, \'Error message\') syslog.syslog(syslog.LOG_DEBUG, \'Debug message\') def set_log_mask(priority: int) -> None: Sets the log mask such that only messages with a priority level higher than or equal to the specified priority will be logged. print(f\\"Setting log mask to log only priority >= {priority}\\") syslog.setlogmask(syslog.LOG_UPTO(priority)) def reset_logging() -> None: Resets the logging configuration by closing and reopening the syslog. print(\\"Resetting logging configuration by closing and reopening the syslog...\\") syslog.closelog() print(\\"Syslog closed.\\") configure_logging(\'example_ident\', syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) # Example usage demonstrate configure_logging(\'example_ident\', syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) log_messages() set_log_mask(syslog.LOG_ERR) log_messages() reset_logging() log_messages()"},{"question":"Objective Demonstrate your understanding of PyTorch\'s `torch.nn.init` module by implementing a function that prepares a neural network model for training using specific initialization methods for different layers. Problem Statement You are given a neural network model implemented using PyTorch. Your task is to write a function `initialize_model` that takes this model as input and initializes its weights using different initialization methods for specific layers: 1. **Linear Layers**: Use the Xavier uniform initialization. 2. **Convolutional Layers**: Use the Kaiming normal initialization. 3. **BatchNorm Layers**: Initialize the weight with ones and bias with zeros. 4. **Embeddding Layers**: Use the normal initialization. Input - `model`: A PyTorch neural network model (an instance of `torch.nn.Module`). Output - The function should return the model with properly initialized weights. Example ```python import torch import torch.nn as nn import torch.nn.init as init class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 28 * 28, 128) self.embedding = nn.Embedding(1000, 128) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = x.view(x.size(0), -1) x = self.fc1(x) return x def initialize_model(model): for m in model.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\') if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Linear): init.xavier_uniform_(m.weight) if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d): init.ones_(m.weight) if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Embedding): init.normal_(m.weight) # Example usage: model = ExampleModel() initialize_model(model) ``` Constraints - You may assume the input model is a valid instance of `torch.nn.Module`. - Ensure to handle layers with and without biases. - The initialization should happen in-place, i.e., directly modifying the input model. Notes: - Use the functions provided in the `torch.nn.init` module. - Do not use any initializations other than the ones specified in the problem statement. - The function should operate without throwing any errors for arbitrary valid models following the described requirements.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_model(model): Initialize weights of a neural network model using specified initialization methods. Args: - model (torch.nn.Module): The neural network model to initialize. Returns: - model (torch.nn.Module): The model with initialized weights. for m in model.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\') if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Linear): init.xavier_uniform_(m.weight) if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d): init.ones_(m.weight) if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Embedding): init.normal_(m.weight) return model"},{"question":"# Python 310 Package Usage: Calculator Extension You are tasked with creating a custom calculator class that leverages the `python310` package\'s functions to perform various numeric operations. Your implementation should define methods corresponding to each type of operation available in the `python310` package documentation provided. Objective Implement a `CustomCalculator` class extending from a base `Calculator` class, which uses functions from the `python310` package to support numeric operations. Class Signature ```python class Calculator: def __init__(self): pass class CustomCalculator(Calculator): def __init__(self): super().__init__() def add(self, o1, o2): pass def subtract(self, o1, o2): pass def multiply(self, o1, o2): pass # Define other methods for operations like: # - true_divide # - floor_divide # - remainder # - power # - negate # - absolute # - invert # - left_shift # - right_shift # - bitwise_and # - bitwise_or # - bitwise_xor # Optional: Include in-place operations equivalents as needed ``` Required Methods You must implement the following methods using the respective `python310` functions mentioned in the documentation: 1. `add(self, o1, o2)`: Perform addition. 2. `subtract(self, o1, o2)`: Perform subtraction. 3. `multiply(self, o1, o2)`: Perform multiplication. 4. `true_divide(self, o1, o2)`: Perform true division. 5. `floor_divide(self, o1, o2)`: Perform floor division. 6. `remainder(self, o1, o2)`: Compute the remainder. 7. `power(self, o1, o2, o3=None)`: Compute the power, optionally modulo another number. 8. `negate(self, o)`: Perform negation. 9. `absolute(self, o)`: Return the absolute value. 10. `invert(self, o)`: Compute the bitwise inversion. 11. `left_shift(self, o1, o2)`: Perform left bitwise shift. 12. `right_shift(self, o1, o2)`: Perform right bitwise shift. 13. `bitwise_and(self, o1, o2)`: Perform bitwise AND. 14. `bitwise_or(self, o1, o2)`: Perform bitwise OR. 15. `bitwise_xor(self, o1, o2)`: Perform bitwise XOR. Ensure that each method returns `None` if the operation fails (i.e., if the `python310` function returns `NULL`). Example Usage ```python calculator = CustomCalculator() result = calculator.add(5, 3) # Should utilize the PyNumber_Add equivalent function print(result) # Output should be 8 if correctly implemented ``` Constraints - Assume all inputs (`o1`, `o2`, `o3`, `o`) are valid and can be handled by the respective `python310` functions. - Return `None` for any operation that results in an error by the underlying `python310` function. - Focus on achieving an accurate and efficient implementation of each method. This task will assess your understanding of the `python310` functionality for numerical operations and your ability to apply it in creating a practical class structure.","solution":"class Calculator: def __init__(self): pass class CustomCalculator(Calculator): def __init__(self): super().__init__() def add(self, o1, o2): return o1 + o2 def subtract(self, o1, o2): return o1 - o2 def multiply(self, o1, o2): return o1 * o2 def true_divide(self, o1, o2): if o2 == 0: return None # Avoid division by zero return o1 / o2 def floor_divide(self, o1, o2): if o2 == 0: return None # Avoid division by zero return o1 // o2 def remainder(self, o1, o2): if o2 == 0: return None # Avoid division by zero return o1 % o2 def power(self, o1, o2, o3=None): if o3 is not None: return pow(o1, o2, o3) return pow(o1, o2) def negate(self, o): return -o def absolute(self, o): return abs(o) def invert(self, o): return ~o def left_shift(self, o1, o2): return o1 << o2 def right_shift(self, o1, o2): return o1 >> o2 def bitwise_and(self, o1, o2): return o1 & o2 def bitwise_or(self, o1, o2): return o1 | o2 def bitwise_xor(self, o1, o2): return o1 ^ o2"},{"question":"You are tasked with creating a utility class `Base64Utility` that encapsulates common operations involving base64 encoding and decoding. Your implementation should demonstrate your understanding of the Python `base64` module by including methods to handle the following scenarios: 1. **Standard Base64 Encoding**: - Method: `encode_standard(data: bytes) -> bytes` - Parameters: - `data` (bytes): Bytes-like object to be encoded. - Returns: - Encoded bytes using the standard Base64 alphabet. 2. **Standard Base64 Decoding**: - Method: `decode_standard(data: bytes) -> bytes` - Parameters: - `data` (bytes): Bytes-like object encoded in standard Base64 to be decoded. - Returns: - Decoded bytes from the Base64 encoded data. - Raises: - `ValueError` if the input data is not correctly padded. 3. **URL-Safe Base64 Encoding**: - Method: `encode_urlsafe(data: bytes) -> bytes` - Parameters: - `data` (bytes): Bytes-like object to be encoded. - Returns: - Encoded bytes using the URL- and filesystem-safe Base64 alphabet. 4. **URL-Safe Base64 Decoding**: - Method: `decode_urlsafe(data: bytes) -> bytes` - Parameters: - `data` (bytes): Bytes-like object encoded in URL- and filesystem-safe Base64 to be decoded. - Returns: - Decoded bytes from the Base64 encoded data. - Raises: - `ValueError` if the input data is not correctly padded. 5. **Switching Alphabets**: - Method: `encode_with_altchars(data: bytes, altchars: bytes) -> bytes` - Parameters: - `data` (bytes): Bytes-like object to be encoded. - `altchars` (bytes): A bytes-like object of length 2 that specifies an alternative alphabet for the \'+\' and \'/\' characters. - Returns: - Encoded bytes using the given alternative alphabet. - Raises: - `ValueError` if the `altchars` length is not 2 or if the input data is invalid. 6. **Validation Check**: - Method: `decode_with_validation(data: bytes, validate: bool = True) -> bytes` - Parameters: - `data` (bytes): Bytes-like object encoded in Base64 to be decoded. - `validate` (bool): Flag to perform validation of non-alphabet characters in the input. - Returns: - Decoded bytes from the Base64 encoded data. - Raises: - `ValueError` if validation fails due to incorrect characters. **Constraints and Requirements**: - Do not use any external libraries other than the Python standard library. - Consider edge cases such as empty input and incorrect input data. - Ensure the methods handle exceptions gracefully and provide meaningful error messages. Provide the complete implementation of the `Base64Utility` class with the specified methods. Example Usage ```python utility = Base64Utility() # Standard Base64 Encoding and Decoding encoded_data = utility.encode_standard(b\'data to be encoded\') decoded_data = utility.decode_standard(encoded_data) # URL-Safe Base64 Encoding and Decoding encoded_url_data = utility.encode_urlsafe(b\'url data to be encoded\') decoded_url_data = utility.decode_urlsafe(encoded_url_data) # Encoding with Alternative Characters encoded_altchars_data = utility.encode_with_altchars(b\'data with alt chars\', b\'-_\') # Decoding with Validation decoded_validated_data = utility.decode_with_validation(b\'ZGF0YXx0byBiZSBlbmNvZGVk==\') ```","solution":"import base64 class Base64Utility: @staticmethod def encode_standard(data: bytes) -> bytes: Encodes bytes using standard Base64 encoding. return base64.b64encode(data) @staticmethod def decode_standard(data: bytes) -> bytes: Decodes bytes using standard Base64 encoding. try: return base64.b64decode(data, validate=True) except Exception as e: raise ValueError(f\\"Invalid input: {e}\\") @staticmethod def encode_urlsafe(data: bytes) -> bytes: Encodes bytes using URL-safe Base64 encoding. return base64.urlsafe_b64encode(data) @staticmethod def decode_urlsafe(data: bytes) -> bytes: Decodes bytes using URL-safe Base64 encoding. try: return base64.urlsafe_b64decode(data) except Exception as e: raise ValueError(f\\"Invalid input: {e}\\") @staticmethod def encode_with_altchars(data: bytes, altchars: bytes) -> bytes: Encodes bytes using alternative characters for \'+\' and \'/\'. if len(altchars) != 2: raise ValueError(\\"altchars must be of length 2\\") return base64.b64encode(data, altchars) @staticmethod def decode_with_validation(data: bytes, validate: bool = True) -> bytes: Decodes bytes and optionally validates non-alphabet characters in the input. try: return base64.b64decode(data, validate=validate) except Exception as e: raise ValueError(f\\"Invalid input: {e}\\")"},{"question":"# Question You are provided with sales data for a store in two formats: a list of dictionaries and a separate dictionary specifying additional attributes for some products. Your task is to utilize pandas to process and analyze the data to answer specific queries. Input: 1. `sales_data`: A list of dictionaries where each dictionary represents daily sales information for different products. ```python sales_data = [ {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 10, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 5, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 15, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-03\\", \\"units_sold\\": 7, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"C\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 8, \\"price_per_unit\\": 25.0}, ] ``` 2. `additional_data`: A dictionary where keys are product names and values are dictionaries containing additional information about those products. ```python additional_data = { \\"A\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 100}, \\"B\\": {\\"category\\": \\"Furniture\\", \\"stock\\": 50}, \\"C\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 70}, } ``` Steps: 1. **Create a DataFrame** from the `sales_data`. 2. **Add additional attributes** to the DataFrame using the `additional_data`. 3. **Calculate the total revenue** for each product over the entire period. 4. **Identify the day with the highest revenue** for each product. 5. **Add a column** indicating if a product is \\"Low Stock\\" (stock less than 60 units). Output: 1. The DataFrame containing daily sales information augmented with additional attributes (category and stock status). 2. The total revenue for each product. 3. The day with the highest revenue for each product. 4. The DataFrame containing the \\"Low Stock\\" status for each product. # Example Output: 1. Augmented DataFrame: ``` product date units_sold price_per_unit category stock 0 A 2021-01-01 10 20.0 Electronics 100 1 A 2021-01-02 5 20.0 Electronics 100 2 B 2021-01-01 15 30.0 Furniture 50 3 B 2021-01-03 7 30.0 Furniture 50 4 C 2021-01-02 8 25.0 Electronics 70 ``` 2. Total Revenue for each product: ```python { \\"A\\": 300.0, \\"B\\": 660.0, \\"C\\": 200.0, } ``` 3. Day with the highest revenue for each product: ```python { \\"A\\": \\"2021-01-01\\", \\"B\\": \\"2021-01-01\\", \\"C\\": \\"2021-01-02\\", } ``` 4. DataFrame containing \\"Low Stock\\" status: ``` product Low Stock 0 A False 1 B True 2 C False ``` # Solution Template ```python import pandas as pd # Input data sales_data = [ {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 10, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 5, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 15, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-03\\", \\"units_sold\\": 7, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"C\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 8, \\"price_per_unit\\": 25.0}, ] additional_data = { \\"A\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 100}, \\"B\\": {\\"category\\": \\"Furniture\\", \\"stock\\": 50}, \\"C\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 70}, } # 1. Create DataFrame from sales_data # 2. Add additional attributes to DataFrame from additional_data # 3. Calculate total revenue for each product # 4. Identify the day with the highest revenue for each product # 5. Add a column indicating if a product is \\"Low Stock\\" # *** Your code goes here *** ```","solution":"import pandas as pd # Input data sales_data = [ {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 10, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"A\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 5, \\"price_per_unit\\": 20.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-01\\", \\"units_sold\\": 15, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"B\\", \\"date\\": \\"2021-01-03\\", \\"units_sold\\": 7, \\"price_per_unit\\": 30.0}, {\\"product\\": \\"C\\", \\"date\\": \\"2021-01-02\\", \\"units_sold\\": 8, \\"price_per_unit\\": 25.0}, ] additional_data = { \\"A\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 100}, \\"B\\": {\\"category\\": \\"Furniture\\", \\"stock\\": 50}, \\"C\\": {\\"category\\": \\"Electronics\\", \\"stock\\": 70}, } # 1. Create DataFrame from sales_data df_sales = pd.DataFrame(sales_data) # 2. Add additional attributes to DataFrame from additional_data df_additional = pd.DataFrame.from_dict(additional_data, orient=\'index\').reset_index().rename(columns={\'index\': \'product\'}) df_sales = df_sales.merge(df_additional, on=\'product\') # 3. Calculate total revenue for each product df_sales[\'revenue\'] = df_sales[\'units_sold\'] * df_sales[\'price_per_unit\'] total_revenue = df_sales.groupby(\'product\')[\'revenue\'].sum().to_dict() # 4. Identify the day with the highest revenue for each product dominant_day = df_sales.groupby(\'product\').apply(lambda x: x.loc[x[\'revenue\'].idxmax(), \'date\']).to_dict() # 5. Add a column indicating if a product is \\"Low Stock\\" (stock less than 60 units) df_sales[\'Low Stock\'] = df_sales[\'stock\'] < 60 low_stock_status = df_sales[[\'product\', \'Low Stock\']].drop_duplicates().reset_index(drop=True) # Outputs augmented_df = df_sales[[\'product\', \'date\', \'units_sold\', \'price_per_unit\', \'category\', \'stock\']]"},{"question":"# Assess Your Seaborn Skills You are provided with the \\"titanic\\" dataset, available in the seaborn library. Your task is to create a detailed visualization published as a single figure using the `sns.relplot()` function that satisfies the following requirements: 1. **Data Loading & Preparation:** - Load the \\"titanic\\" dataset using Seaborn\'s `load_dataset` function. - Include only the columns: \'age\', \'fare\', \'sex\', \'class\', \'alive\', and \'embarked\'. - Remove any rows with missing data. 2. **Creating Plots:** - Create a scatter plot (using `sns.relplot()` with `kind=\\"scatter\\"`) to show the relationship between \'age\' (on the x-axis) and \'fare\' (on the y-axis). - Use different colors (using the `hue` parameter) to distinguish passengers by \'sex\'. - Use different marker styles (using the `style` parameter) to distinguish whether the passenger was \'alive\' or not. - Facet your plot into different columns for each \'class\' (use the `col` parameter). 3. **Customization:** - Customize your scatter plot by setting the `height` and `aspect` parameters of the facets to ensure a well-balanced layout. - Add appropriate titles to each facet indicating the class of passengers. - Set axis labels for x and y. - Add a horizontal reference line at y=50 using the `map` method of the `FacetGrid` object. 4. **Submission Requirements:** - Provide the final output plot. - Include the entire code used to generate the plot, wrapped in a single function named `create_titanic_plot`. - Ensure to import all necessary libraries within your function. # Function Signature: ```python def create_titanic_plot(): # Your code goes here pass ``` # Example Output: The output should look something like this (formatting and exact appearance may vary): ![Example Plot](example_plot.png) # Constraints: - Ensure that the plot is generated with no errors and visualizes the data as specified. - The figure should be clear and interpretable. # Evaluation Criteria: - Correctness: Does the generated plot meet all the requirements? - Code Quality: Is the code clean, well-commented, and follows good practices? - Visualization: Is the final plot well-labeled, visually appealing, and accurate? Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plot(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Select necessary columns and remove rows with missing data selected_columns = [\'age\', \'fare\', \'sex\', \'class\', \'alive\', \'embarked\'] titanic = titanic[selected_columns].dropna() # Create the scatter plot using sns.relplot() g = sns.relplot( data=titanic, x=\'age\', y=\'fare\', hue=\'sex\', style=\'alive\', col=\'class\', kind=\'scatter\', height=4, aspect=1 ) # Add horizontal reference line at y=50 in each facet for ax in g.axes.flat: ax.axhline(y=50, color=\'gray\', linestyle=\'--\', linewidth=1) # Set titles and labels g.set_axis_labels(\\"Age\\", \\"Fare\\") g.set_titles(col_template=\\"{col_name} Class\\") # Display the plot plt.show()"},{"question":"Objective Implement a Python script that creates a new directory structure, copies specific files from an existing directory to the new structure, and then spawns a shell process to display the contents of the new directory. Requirements 1. **Create Directory Structure:** - Implement a function `create_directory_structure(base_path: str, sub_dirs: list[str]) -> None` that takes a base path and a list of sub-directories to be created under the base path. - Ensure that the directory structure is created recursively (use `os.makedirs`). 2. **Copy Files:** - Implement a function `copy_files(src_dir: str, dst_dir: str, extensions: list[str]) -> None` that copies all files from the source directory to the destination directory but only includes files with specific extensions. - Use high-level file handling modules like `shutil` if necessary. 3. **Spawn Shell Process:** - Implement a function `spawn_shell_command(command: str) -> None` that spawns a shell process to execute a given command (use `os.system` or `subprocess`). 4. **Main Script:** - Write a main script that: 1. Uses `create_directory_structure` to create a new directory structure. 2. Uses `copy_files` to copy all `.txt` files from a given source directory to a new directory within the created structure. 3. Uses `spawn_shell_command` to list the contents of the new directory (`ls` on Unix or `dir` on Windows). Constraints - The base directory for the new structure and the source directory for copying files will be provided as command-line arguments. - The script should handle any potential exceptions gracefully and provide appropriate error messages. Example If executed as follows: ```bash python script.py /path/to/new_directory_structure /path/to/source_directory ``` The script should: 1. Create a directory structure like: ``` /path/to/new_directory_structure/ â””â”€â”€ sub_dir1 â””â”€â”€ sub_dir2 ``` 2. Copy all `.txt` files from `/path/to/source_directory` to `/path/to/new_directory_structure/sub_dir1`. 3. Spawn a shell command to display the contents of `/path/to/new_directory_structure/sub_dir1`. ```python import os import shutil import sys import subprocess def create_directory_structure(base_path: str, sub_dirs: list[str]) -> None: try: for sub_dir in sub_dirs: full_path = os.path.join(base_path, sub_dir) os.makedirs(full_path, exist_ok=True) print(f\\"Created directory: {full_path}\\") except OSError as e: print(f\\"Error creating directory structure: {e}\\") def copy_files(src_dir: str, dst_dir: str, extensions: list[str]) -> None: try: for root, _, files in os.walk(src_dir): for file in files: if any(file.endswith(ext) for ext in extensions): src_file_path = os.path.join(root, file) dst_file_path = os.path.join(dst_dir, file) shutil.copy2(src_file_path, dst_file_path) print(f\\"Copied {src_file_path} to {dst_file_path}\\") except OSError as e: print(f\\"Error copying files: {e}\\") def spawn_shell_command(command: str) -> None: try: result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True) print(result.stdout) except subprocess.CalledProcessError as e: print(f\\"Error executing command: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python script.py <base_path> <src_dir>\\") sys.exit(1) base_path = sys.argv[1] src_dir = sys.argv[2] sub_dirs = [\\"sub_dir1\\", \\"sub_dir1/sub_dir2\\"] create_directory_structure(base_path, sub_dirs) copy_files(src_dir, os.path.join(base_path, \\"sub_dir1\\"), [\\".txt\\"]) if os.name == \'nt\': spawn_shell_command(f\\"dir {os.path.join(base_path, \'sub_dir1\')}\\") else: spawn_shell_command(f\\"ls {os.path.join(base_path, \'sub_dir1\')}\\") ```","solution":"import os import shutil import subprocess def create_directory_structure(base_path: str, sub_dirs: list[str]) -> None: try: for sub_dir in sub_dirs: full_path = os.path.join(base_path, sub_dir) os.makedirs(full_path, exist_ok=True) print(f\\"Created directory: {full_path}\\") except OSError as e: print(f\\"Error creating directory structure: {e}\\") def copy_files(src_dir: str, dst_dir: str, extensions: list[str]) -> None: try: for root, _, files in os.walk(src_dir): for file in files: if any(file.endswith(ext) for ext in extensions): src_file_path = os.path.join(root, file) dst_file_path = os.path.join(dst_dir, file) shutil.copy2(src_file_path, dst_file_path) print(f\\"Copied {src_file_path} to {dst_file_path}\\") except OSError as e: print(f\\"Error copying files: {e}\\") def spawn_shell_command(command: str) -> None: try: result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True) print(result.stdout) except subprocess.CalledProcessError as e: print(f\\"Error executing command: {e}\\")"},{"question":"Objective: Demonstrate your understanding of handling HTML entities in Python using the `html.entities` module. Task: Write a Python function called `convert_html_entities` that performs the following: 1. Takes a string containing HTML entity names and converts all the HTML entity names to their corresponding Unicode characters using the `html.entities.html5` dictionary. 2. If the HTML entity name is not found in the `html.entities.html5` dictionary, leave it unchanged in the string. Function Signature: ```python def convert_html_entities(html_string: str) -> str: pass ``` Input: - `html_string` (str): A string containing HTML entity names. The entity names may or may not end with a semicolon. Output: - A string where all the recognized HTML entity names are replaced with their corresponding Unicode characters. Example: ```python html_input = \\"This is a sample text with entities: &lt; &gt; &amp; &copy;\\" result = convert_html_entities(html_input) # Expected output: \\"This is a sample text with entities: < > & Â©\\" ``` Constraints: - You may assume the `html_string` will always be a well-formed and valid string. - Do not use any external libraries other than the built-in `html.entities` module. - Handle both the cases where the entity ends with a semicolon and where it does not, if it\'s a valid HTML5 named character reference. Notes: - You can assume that if an entity name does not end in a semicolon but is still valid, it will be present in the `html5` dictionary without the semicolon.","solution":"import html.entities def convert_html_entities(html_string: str) -> str: Converts all HTML entity names in the input string to their corresponding Unicode characters. If the entity name is not found in the html.entities.html5 dictionary, leave it unchanged. Args: html_string (str): A string containing HTML entity names. Returns: str: A string where all the recognized HTML entity names are replaced with their corresponding Unicode characters. # Dictionary of html entities from html.entities.html5 entities = html.entities.html5 def replace_entity(match): entity = match.group(0) # Remove the & and ; if present entity_name = entity[1:-1] if entity.endswith(\';\') else entity[1:] # Replace with corresponding Unicode character return entities.get(entity_name, entity) import re # Regex to match HTML entities pattern = re.compile(r\'&[A-Za-z]+;?\') # Substitute the matched entities using the replace_entity function return pattern.sub(replace_entity, html_string)"},{"question":"# Advanced PyTorch MPS Coding Assessment Context: You have been provided with a Mac device capable of utilizing Metal Performance Shaders (MPS) for accelerating PyTorch computations. You need to demonstrate your understanding of PyTorch MPS functionalities by implementing a comprehensive solution that involves memory management, random number generation, and performance profiling. Objective: 1. Write a function `mps_random_tensor_operations` that: - Creates a specified number of random tensors on the MPS device. - Synchronizes the created tensors. - Returns the state of the random number generator after creation. 2. Write a function `mps_memory_profile` that: - Empties any unused memory. - Sets a memory usage limit as a fraction of the total available memory. - Profiles memory allocated before and after creating a tensor with specified dimensions. - Returns a dictionary containing: - `before_memory_allocation`: Memory allocated before tensor creation. - `after_memory_allocation`: Memory allocated after tensor creation. - `memory_difference`: Difference in allocated memory. - The tensor\'s dimensions should be passed as an argument to this function. Requirements: - You must use the MPS backend for all tensor operations. - Ensure proper usage of MPS synchronization to handle memory efficiently. - Profile memory allocation accurately. - Demonstrate clear use of random number generation state management. Function Signatures: ```python def mps_random_tensor_operations(tensor_count: int) -> torch.Tensor: Parameters: - tensor_count (int): Number of random tensors to create. Returns: - torch.Tensor: The state of the random number generator after creating the tensors. pass def mps_memory_profile(tensor_dimensions: tuple, memory_fraction: float) -> dict: Parameters: - tensor_dimensions (tuple): Dimensions of the tensor to be created. - memory_fraction (float): Fraction of total memory to set as the maximum for the process. Returns: - dict: A dictionary containing before and after memory allocation metrics and the difference. pass ``` Constraints: - Assume tensor dimensions will be valid and within the capacity of the MPS device. - Memory fraction will be a float between 0 and 1. Example: ```python # Example usage mps_random_tensor_operations(5) # Should create 5 random tensors and return RNG state. mps_memory_profile((1000, 1000), 0.5) # Should allocate tensor, profile memory. ``` Use the functions and modules from `torch.mps` to implement the functions above.","solution":"import torch def mps_random_tensor_operations(tensor_count: int) -> torch.Tensor: Creates a specified number of random tensors on MPS device, synchronizes, and returns the RNG state. Parameters: - tensor_count (int): Number of random tensors to create. Returns: - torch.Tensor: The state of the random number generator after creating the tensors. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available.\\") rng_state_before = torch.mps.get_rng_state() for _ in range(tensor_count): _ = torch.randn((10, 10), device=\\"mps\\") torch.mps.synchronize() rng_state_after = torch.mps.get_rng_state() return rng_state_after def mps_memory_profile(tensor_dimensions: tuple, memory_fraction: float) -> dict: Profiles memory before and after creating a tensor of given dimensions, with memory limit. Parameters: - tensor_dimensions (tuple): Dimensions of the tensor to be created. - memory_fraction (float): Fraction of total memory to set as the maximum for the process. Returns: - dict: A dictionary containing memory metrics before and after tensor creation. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available.\\") torch.mps.empty_cache() total_memory = torch.mps.get_device_properties(0).total_memory memory_limit = int(total_memory * memory_fraction) torch.mps.set_per_process_memory_fraction(memory_fraction) before_memory_allocation = torch.mps.memory_reserved() tensor = torch.randn(tensor_dimensions, device=\\"mps\\") torch.mps.synchronize() after_memory_allocation = torch.mps.memory_reserved() memory_difference = after_memory_allocation - before_memory_allocation return { \\"before_memory_allocation\\": before_memory_allocation, \\"after_memory_allocation\\": after_memory_allocation, \\"memory_difference\\": memory_difference }"},{"question":"# Coding Assessment: Stock Price Analysis with pandas Objective: To assess your understanding of the pandas library by performing data manipulation and analysis on stock price data. Problem Statement: You are given a CSV file, `stock_prices.csv`, which contains daily stock prices for several companies over a year. The CSV file has the following columns: - `Date`: The date of the stock prices in the format `YYYY-MM-DD`. - `Company`: The name of the company. - `Close`: The closing price of the stock on that date. You are required to write a Python function that: 1. Reads the CSV file into a pandas DataFrame. 2. Calculates the monthly average closing price for each company. 3. Identifies the month in which each company had its highest average closing price. 4. Returns a DataFrame with the following columns: `Company`, `Month`, `AverageClosingPrice`. Function Signature: ```python def analyze_stock_prices(file_path: str) -> pd.DataFrame: Analyzes stock prices from a given CSV file to calculate the monthly average closing price and identifies the month with the highest average closing price for each company. Parameters: file_path (str): The path to the CSV file containing the stock prices. Returns: pd.DataFrame: A DataFrame with columns \'Company\', \'Month\', \'AverageClosingPrice\' representing the month with the highest average closing price for each company. pass ``` Input: - `file_path`: A string representing the file path to the `stock_prices.csv` file. Output: - A pandas DataFrame with the following columns: - `Company`: The name of the company. - `Month`: The month in the format `YYYY-MM` in which the company had its highest average closing price. - `AverageClosingPrice`: The highest average closing price for that month. Constraints: - The CSV file is guaranteed to have valid data. - The stock prices are recorded for each trading day. Example: Given the `stock_prices.csv` file with the following data: ``` Date,Company,Close 2023-01-01,CompanyA,100 2023-01-02,CompanyA,110 2023-01-01,CompanyB,200 2023-01-02,CompanyB,210 ... ``` The function should return a DataFrame similar to: ``` Company Month AverageClosingPrice 0 CompanyA 2023-01 105.0 1 CompanyB 2023-01 205.0 ... ``` **Note:** - You may use pandas built-in functions such as `pd.read_csv`, `pd.to_datetime`, `groupby`, `resample`, etc., to solve this problem. - Ensure efficient calculation to handle large datasets. Good luck!","solution":"import pandas as pd def analyze_stock_prices(file_path: str) -> pd.DataFrame: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Convert the \'Date\' column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\'], format=\'%Y-%m-%d\') # Extract year-month from the date to create a new \'Month\' column df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\').astype(str) # Calculate the monthly average closing price for each company monthly_avg = df.groupby([\'Company\', \'Month\'])[\'Close\'].mean().reset_index() # Identify the month with the highest average closing price for each company idx = monthly_avg.groupby(\'Company\')[\'Close\'].idxmax() result = monthly_avg.loc[idx].reset_index(drop=True) result = result.rename(columns={\'Close\': \'AverageClosingPrice\'}) return result"},{"question":"You are provided with a dataset of house prices and various features describing the houses. Your task is to build a robust machine learning model using the scikit-learn library to predict house prices. Specifically, you are required to: 1. **Load and preprocess the dataset:** - Handle missing values appropriately. - Encode categorical variables. - Standardize the feature set. 2. **Feature selection:** - Use a suitable method to select the most important features for the model. 3. **Model training and evaluation:** - Train at least three different supervised learning models from the scikit-learn library. - Use cross-validation to evaluate the performance of each model. - Select the best performing model based on a suitable evaluation metric (e.g., RMSE, R^2). # Dataset The dataset (CSV file) includes the following columns: - \'Id\': Unique identifier for each house. - \'Features\': Multiple columns representing various features of the houses (e.g., \'LotArea\', \'OverallQual\', \'YearBuilt\', etc.). - \'SalePrice\': Target variable representing the sale price of each house. # Input and Output Format **Input:** - The path to the dataset CSV file. **Output:** - A printout of the cross-validation results for each model (including the chosen evaluation metric). - The best performing model based on the evaluation metric, along with its corresponding score. # Constraints: - The dataset may contain missing values. - Only use scikit-learn for preprocessing, model training, and evaluation. - Performance should be assessed using cross-validation. # Instructions: 1. Import necessary libraries. 2. Load the dataset and handle any missing values or categorical variables. 3. Standardize the features. 4. Perform feature selection. 5. Train multiple supervised learning models (e.g., Linear Regression, Decision Tree, Random Forest). 6. Evaluate models using cross-validation and choose the best one based on your chosen metric. # Example: ```python import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.impute import SimpleImputer from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Load and preprocess dataset df = pd.read_csv(\'house_prices.csv\') # Handle missing values # Encode categorical variables # Standardize features # Feature selection # Assign X and y, perform feature selection # Train and evaluate models models = { \'Linear Regression\': LinearRegression(), \'Decision Tree\': DecisionTreeRegressor(), \'Random Forest\': RandomForestRegressor() } for name, model in models.items(): scores = cross_val_score(model, X, y, scoring=\'neg_mean_squared_error\', cv=5) print(f\\"{name}: {scores.mean()}\\") # Select the best model based on evaluation metric ``` Ensure to fill in the appropriate preprocessing, feature selection and model training/evaluation steps in the example code.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Load and preprocess dataset def load_and_preprocess_data(filepath): df = pd.read_csv(filepath) # Dropping the \'Id\' column as it is not needed for modeling df = df.drop(columns=[\\"Id\\"]) # Separate features and target X = df.drop(columns=[\\"SalePrice\\"]) y = df[\\"SalePrice\\"] # Identify categorical and numerical columns categorical_cols = X.select_dtypes(include=[\'object\']).columns numerical_cols = X.select_dtypes(exclude=[\'object\']).columns # Preprocessing for numerical data (impute then standardize) numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data (impute then encode) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'encoder\', LabelEncoder()) # Note: LabelEncoder works here as a placeholder; adjust if needed. ]) # Bundle preprocessing for numerical and categorical data preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Transform the data X_processed = preprocessor.fit_transform(X) return X_processed, y def train_and_evaluate_models(filepath): # Load and preprocess the data X, y = load_and_preprocess_data(filepath) # Define models to evaluate models = { \'Linear Regression\': LinearRegression(), \'Decision Tree\': DecisionTreeRegressor(), \'Random Forest\': RandomForestRegressor() } # Evaluate each model using cross-validation results = {} for name, model in models.items(): scores = cross_val_score(model, X, y, scoring=\'neg_mean_squared_error\', cv=5) rmse_scores = np.sqrt(-scores) results[name] = rmse_scores.mean() print(f\\"{name}: RMSE = {rmse_scores.mean():.4f}\\") # Select the best model based on RMSE best_model_name = min(results, key=results.get) best_rmse = results[best_model_name] print(f\\"nBest model: {best_model_name} with RMSE = {best_rmse:.4f}\\") # Return the best model and corresponding score return best_model_name, best_rmse"},{"question":"# Question: Advanced Residual Plot Analysis Using Seaborn You are provided with a dataset that contains information about various cars, including attributes such as weight, displacement, horsepower, and miles per gallon (mpg). The dataset can be loaded using `sns.load_dataset(\\"mpg\\")`. Your task is to perform the following steps: 1. **Load the dataset** and display the first five rows to understand its structure. 2. **Generate a Residual Plot**: - Create a residual plot using `sns.residplot` with `weight` as the predictor (`x`) and `displacement` as the response (`y`). 3. **Higher-Order Trends**: - Create another residual plot to account for potential higher-order trends in the relationship between `horsepower` and `mpg`. Use a quadratic term (order=2) to fit the model. 4. **LOWESS Curve**: - Generate a third residual plot for `horsepower` vs. `mpg` with a LOWESS curve added to reveal or emphasize any underlying structure. Customize the LOWESS line to be red using the appropriate parameter. 5. **Interpretation**: - Write a brief interpretation (2-3 sentences) of how the residuals change when you account for higher-order trends and when you add a LOWESS curve. What might these changes suggest about the underlying relationship between the variables? # Input Format No direct input to your function. Instead, you will write a Python script that accomplishes the above tasks using the `seaborn` and `matplotlib` libraries. # Output Format The script should produce and display three residual plots as described in the steps above. Additionally, include a brief interpretation of the results in the comments of your script. # Constraints - Use the `seaborn` library for plotting. - Ensure your script runs without errors and displays the plots sequentially. **Note**: Assume that the necessary libraries (`seaborn`, `matplotlib`, `pandas`) are already installed in your environment. # Example Output Your script should generate three plots: 1. A residual plot of `weight` vs. `displacement`. 2. A residual plot of `horsepower` vs. `mpg` using a quadratic term. 3. A residual plot of `horsepower` vs. `mpg` with a LOWESS curve. The plots will be displayed inline, followed by a brief textual interpretation in the comments.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset and display the first five rows mpg_data = sns.load_dataset(\\"mpg\\") print(mpg_data.head()) # Step 2: Generate a residual plot for weight vs. displacement plt.figure(figsize=(10, 6)) sns.residplot(x=\\"weight\\", y=\\"displacement\\", data=mpg_data) plt.title(\\"Residual Plot: Weight vs. Displacement\\") plt.show() # Step 3: Residual plot for horsepower vs. mpg with quadratic term (order=2) plt.figure(figsize=(10, 6)) sns.residplot(x=\\"horsepower\\", y=\\"mpg\\", data=mpg_data, order=2) plt.title(\\"Residual Plot: Horsepower vs. MPG (Quadratic Term)\\") plt.show() # Step 4: Residual plot for horsepower vs. mpg with a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(x=\\"horsepower\\", y=\\"mpg\\", data=mpg_data, lowess=True, color=\'red\') plt.title(\\"Residual Plot: Horsepower vs. MPG with LOWESS Curve\\") plt.show() # Interpretation: # When accounting for higher-order (quadratic) trends, the residual plot of horsepower vs. mpg # indicates a better fit, as the residuals are more randomly scattered around the zero line, # suggesting that a quadratic model may capture the relationship better than a linear model. # Adding a LOWESS curve further reveals any non-linearities, indicating that the relationship # might be more complex and potentially non-linear, which helps in identifying the appropriate # model for the data."},{"question":"**Problem Statement:** You are working with large mail archives stored in various mailbox formats. For simplicity, let\'s concentrate on the `mbox` format. You need to implement a Python function to automate a common task: moving and copying emails based on specific criteria. Write a function `move_and_copy_emails(mbox_path, archive_path, keyword)` that: 1. Reads emails from an mbox mailbox located at `mbox_path`. 2. Finds all emails whose subject contains the specified `keyword`. 3. Copies these emails to another mbox mailbox located at `archive_path`. 4. Moves these emails from the original mailbox to a separate \'processed\' folder within the `mbox_path`. **Function Signature:** ```python def move_and_copy_emails(mbox_path: str, archive_path: str, keyword: str) -> None: ``` **Input:** - `mbox_path` (str): The path to the original mbox-formatted mailbox. - `archive_path` (str): The path to the archive mbox-formatted mailbox where the emails need to be copied. - `keyword` (str): The keyword to search for in email subjects. **Output:** - The function doesn\'t return anything. It modifies the mailboxes in place. **Constraints:** - You may assume the provided paths are valid and the mailboxes can be read from and written to. - If an email is malformed and cannot be read, skip it and continue processing other emails. - Implement necessary locking mechanisms to ensure mailboxes are not corrupted if the function is interrupted or run concurrently. **Example:** ```python move_and_copy_emails(\'/path/to/mbox\', \'/path/to/archive_mbox\', \'Important\') ``` **Note:** - Use the `mailbox` module and its `mbox` and `mboxMessage` classes to handle the email operations. - Ensure emails are only moved to the \'processed\' folder after they are successfully copied to the archive mbox. - Handle exceptions gracefully to avoid losing email data when encountering an error. Happy coding!","solution":"import mailbox import os import shutil def move_and_copy_emails(mbox_path: str, archive_path: str, keyword: str) -> None: # Open the source mbox file src_mbox = mailbox.mbox(mbox_path) src_mbox.lock() # Open/create the archive mbox file archive_mbox = mailbox.mbox(archive_path) archive_mbox.lock() try: # Create a directory for processed emails if it doesn\'t exist processed_folder = os.path.join(os.path.dirname(mbox_path), \'processed\') os.makedirs(processed_folder, exist_ok=True) # Process each email in the source mbox to_delete = [] for key, message in src_mbox.items(): try: if keyword in message[\'subject\']: # Copy the message to the archive archive_mbox.add(message) # Move the message to the processed folder processed_mbox_path = os.path.join(processed_folder, f\\"{key}.eml\\") with open(processed_mbox_path, \'w\') as processed_file: processed_file.write(message.as_string()) # Mark the message for deletion from the original mbox to_delete.append(key) except Exception as e: print(f\\"Error processing message: {e}\\") continue # Delete marked messages from the original mbox for key in to_delete: src_mbox.remove(key) src_mbox.flush() archive_mbox.flush() finally: src_mbox.unlock() archive_mbox.unlock()"},{"question":"# Question Write a Python script that processes command-line arguments using the `getopt` module. The script should support the following options: - Short options: - `-h`: Display help message and exit. - `-v`: Enable verbose mode. - `-f <file>`: Specify a filename. - Long options: - `--help`: Display help message and exit. - `--verbose`: Enable verbose mode. - `--file=<file>`: Specify a filename. Your script should: 1. Correctly parse the provided options and arguments. 2. Handle errors in command-line argument parsing gracefully, displaying an appropriate message. 3. Print the parsed options and arguments, indicating whether verbose mode is enabled and what filename is specified if any. 4. Display the help message if `-h` or `--help` is specified. # Example ```sh python your_script.py -v -f sample.txt arg1 arg2 Verbose mode enabled Filename: sample.txt Arguments: [\'arg1\', \'arg2\'] python your_script.py --help Usage: your_script.py [-h|--help] [-v|--verbose] [-f|--file <file>] [arguments...] python your_script.py --file=example.txt arg1 arg2 Filename: example.txt Arguments: [\'arg1\', \'arg2\'] ``` # Implementation **Input:** - The command-line arguments are passed to the script. **Output:** - The script prints messages to standard output indicating: - The enablement of verbose mode. - The specified filename. - Any remaining arguments. - Error message if invalid arguments are provided. **Constraints:** - You must use the `getopt` module for command-line parsing. - Ensure your script handles errors gracefully and provides helpful usage information when requested.","solution":"import sys import getopt def print_help_message(): help_message = ( \\"Usage: your_script.py [-h|--help] [-v|--verbose] [-f|--file <file>] [arguments...]n\\" \\"-h, --help: Display help message and exit.n\\" \\"-v, --verbose: Enable verbose mode.n\\" \\"-f <file>, --file=<file>: Specify a filename.n\\" ) print(help_message) def main(argv): try: # Define short and long option flags opts, args = getopt.getopt(argv, \\"hvf:\\", [\\"help\\", \\"verbose\\", \\"file=\\"]) except getopt.GetoptError: print_help_message() sys.exit(2) verbose = False filename = None for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): print_help_message() sys.exit() elif opt in (\\"-v\\", \\"--verbose\\"): verbose = True elif opt in (\\"-f\\", \\"--file\\"): filename = arg if verbose: print(\\"Verbose mode enabled\\") if filename: print(f\\"Filename: {filename}\\") if args: print(f\\"Arguments: {args}\\") if __name__ == \\"__main__\\": main(sys.argv[1:])"},{"question":"# Question: Customizing and Applying Diverging Color Palettes in Seaborn You are tasked with visualizing a dataset that contains both positive and negative values using a specifically designed diverging color palette. This will help emphasize the extremes and the midpoint (zero) in your data. Your goal is to create a customized diverging color palette and then apply it to a heatmap. Requirements: 1. **Create a Custom Diverging Color Palette:** - Use the `seaborn.diverging_palette` method to create a color palette that transitions from green (positive values) to red (negative values) with a white center (representing the zero/midpoint). - Adjust the lightness and saturation to make sure the extremes are distinguishable. - Increase the separation around the center value to make the transition near zero more pronounced. - Return the color palette as a continuous colormap. 2. **Apply the Custom Palette to a Heatmap:** - Generate a random dataset with both positive and negative values. - Use seaborn to create a `heatmap` of this dataset. - Apply your custom diverging colormap to the heatmap. 3. **Save and Display the Heatmap:** - Save the generated heatmap to a file called `custom_heatmap.png`. - Display the heatmap inline. Input and Output Formats: - **Input:** There is no explicit input provided. Instead, you will generate a random dataset for visualization. - **Output:** The output should be a heatmap saved to `custom_heatmap.png` and displayed inline. Constraints: - Use Seaborn and other necessary libraries such as matplotlib and numpy. - Your random dataset should be a 10x10 matrix with values ranging approximately from -10 to 10. Performance Requirements: - Ensure that the palette effectively differentiates between positive, negative, and near-zero values. Example: ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Step 1: Create a Custom Diverging Color Palette palette = sns.diverging_palette(150, 10, sep=50, s=90, l=50, as_cmap=True) # Step 2: Generate Random Dataset with Positive and Negative Values data = np.random.uniform(-10, 10, size=(10, 10)) # Step 3: Create and Apply the Custom Palette to a Heatmap sns.heatmap(data, cmap=palette, center=0) # Step 4: Save and Display the Heatmap plt.savefig(\'custom_heatmap.png\') plt.show() ``` Ensure your solution meets the requirements and captures the properties of the diverging palette specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_diverging_palette(): Create a custom diverging color palette with green for positive, red for negative, and white for zero/midpoint. palette = sns.diverging_palette(150, 10, sep=50, s=90, l=50, as_cmap=True) return palette def generate_random_dataset(): Generate a 10x10 random dataset with values ranging from -10 to 10. data = np.random.uniform(-10, 10, size=(10, 10)) return data def apply_palette_to_heatmap(data, palette): Apply the custom diverging colormap to a given dataset and create a heatmap. Save the heatmap as \'custom_heatmap.png\' and display it inline. ax = sns.heatmap(data, cmap=palette, center=0) plt.savefig(\'custom_heatmap.png\') plt.show() # Example usage: if __name__ == \\"__main__\\": custom_palette = create_custom_diverging_palette() random_data = generate_random_dataset() apply_palette_to_heatmap(random_data, custom_palette)"},{"question":"# Question: Binary Data Conversion and Processing **Objective:** Implement a Python function that reads a binary file, interprets its contents using a specific structure, performs some transformations, and writes the result back to another binary file. **Problem Statement:** You are given a binary file that contains a sequence of entries, where each entry has the following structure: - A 4-byte integer (ID) - An 8-byte float (Value) Write a function named `process_binary_file(input_file: str, output_file: str) -> None` that: 1. Reads the input binary file. 2. For each entry: - Double the ID. - Square the Value. 3. Write the transformed entries to the output binary file. # Expected Function Signature ```python def process_binary_file(input_file: str, output_file: str) -> None: pass ``` # Input/Output Formats * **Input:** - `input_file`: Path to the input binary file containing the data. - `output_file`: Path to the output binary file where the transformed data should be written. * **Output:** - The function does not return anything but writes processed data into `output_file`. # Constraints: - The input file will be in binary format with strict adherence to the described structure. - The function must handle potentially large files efficiently using minimal memory. # Example: Assume you have a binary file `input.bin` with the following raw data (represented in hexadecimal): ``` 01 00 00 00 00 00 00 00 00 00 F0 3F 02 00 00 00 00 00 00 00 00 00 00 40 ``` This corresponds to two entries: 1. ID = 1, Value = 1.0 2. ID = 2, Value = 2.0 After processing, the output binary file `output.bin` should contain: ``` 02 00 00 00 00 00 00 00 00 00 00 40 04 00 00 00 00 00 00 00 00 00 00 50 ``` Which corresponds to: 1. ID = 2, Value = 1.0^2 = 1.0 2. ID = 4, Value = 2.0^2 = 4.0 # Notes: - You must use the `struct` module to read and write the binary data. - Remember to open binary files in the appropriate mode (\'rb\' for reading, \'wb\' for writing). Good luck!","solution":"import struct def process_binary_file(input_file: str, output_file: str) -> None: Reads a binary file, processes its contents, and writes the results to another binary file. Each entry in the input file has: - A 4-byte integer (ID) - An 8-byte float (Value) The processing steps are: - Double the ID - Square the Value Arguments: input_file : str : Path to the input binary file output_file : str : Path to the output binary file with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: # Each entry is 12 bytes: 4 bytes for int and 8 bytes for float entry_format = \'i d\' entry_size = struct.calcsize(entry_format) while True: entry_data = infile.read(entry_size) if len(entry_data) < entry_size: break id_, value = struct.unpack(entry_format, entry_data) new_id = id_ * 2 new_value = value ** 2 new_entry_data = struct.pack(entry_format, new_id, new_value) outfile.write(new_entry_data)"},{"question":"You are working on optimizing the performance of a neural network training routine using PyTorch. Your task is to write a function that sets the threading environment variables for efficient parallel processing. # Function Specification Write a Python function `set_threading_env(max_omp_threads: int, max_mkl_threads: int) -> None` that sets the environment variables `OMP_NUM_THREADS` and `MKL_NUM_THREADS` according to the provided input values. Input - `max_omp_threads`: an integer representing the maximum number of threads for OpenMP parallel regions. - `max_mkl_threads`: an integer representing the maximum number of threads for the Intel MKL library. Output The function does not return any value. It sets the environment variables accordingly. # Constraints - Both `max_omp_threads` and `max_mkl_threads` will be positive integers. # Example ```python def set_threading_env(max_omp_threads: int, max_mkl_threads: int) -> None: # Your implementation here # Example Usage set_threading_env(4, 2) ``` After running the code, the environment variables should be set such that: - `OMP_NUM_THREADS` is `4` - `MKL_NUM_THREADS` is `2` # Note You may use the `os` module to set environment variables in Python. # Tips - Use `os.environ` to set the environment variables. - After setting these variables, print their values to verify they are set correctly.","solution":"import os def set_threading_env(max_omp_threads: int, max_mkl_threads: int) -> None: Sets the environment variables OMP_NUM_THREADS and MKL_NUM_THREADS. Parameters: max_omp_threads (int): Maximum number of threads for OpenMP parallel regions. max_mkl_threads (int): Maximum number of threads for the Intel MKL library. os.environ[\'OMP_NUM_THREADS\'] = str(max_omp_threads) os.environ[\'MKL_NUM_THREADS\'] = str(max_mkl_threads)"},{"question":"# Seaborn Faceted Plots Assessment Objective: Demonstrate your understanding of seaborn\'s `objects` module for creating faceted plots with various customization options. Problem Statement: You are given two datasets: `penguins` and `diamonds`, provided by seaborn. You are required to write a function `create_faceted_plots` that generates several faceted plots based on different variables and customization options. Function Signature: ```python def create_faceted_plots(): # Your code here ``` Requirements: 1. **Faceted Plot for Penguins Data:** - Create a faceted plot of `penguins` dataset, showing `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Facet the plot by `species`. - Customize the facet titles to display the species name followed by \\" Species\\". 2. **Two-Dimensional Faceted Plot for Penguins Data:** - Create a two-dimensional faceted plot using `species` and `sex`. - Order the species facets to show \\"Gentoo\\" first, followed by \\"Adelie\\". - Order the sex facets to show \\"Female\\" first, followed by \\"Male\\". 3. **Wrapped Faceted Plot for Diamonds Data:** - Create a faceted plot of the `diamonds` dataset, showing `carat` on the x-axis and `price` on the y-axis. - Facet the plot by `color` and wrap the subplots into a 4-column layout. 4. **Non-shared X-axis Faceted Plot for Diamonds Data:** - Create a faceted plot of the `diamonds` dataset, showing `carat` on the x-axis and `price` on the y-axis. - Facet the plot by `clarity`, wrapping the subplots into a 3-column layout. - Ensure that the x-axis is not shared among the facets. Inputs: - None. The datasets are already available via seaborn. Outputs: - The function should display the plots directly. Constraints: - Use seaborn\'s `objects` module to create the plots. - Ensure the plots are properly labeled and visually distinct. Example Usage: ```python create_faceted_plots() # This should display the required faceted plots as specified in the requirements. ``` Notes: - You do not need to handle any file imports within the function. Assume the datasets are available as demonstrated in the documentation. - Focus on demonstrating different customization features of seaborn\'s faceting capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_faceted_plots(): # Load the datasets penguins = sns.load_dataset(\\"penguins\\") diamonds = sns.load_dataset(\\"diamonds\\") # Faceted Plot for Penguins Data by Species fg = sns.displot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", facet_kws={\'sharex\': False, \'sharey\': False} ) for ax, title in zip(fg.axes.flat, fg.col_names): ax.set_title(f\\"{title} Species\\") plt.show() # Two-Dimensional Faceted Plot for Penguins Data by Species and Sex species_order = [\\"Gentoo\\", \\"Adelie\\"] sex_order = [\\"Female\\", \\"Male\\"] sns.displot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", col_order=species_order, row_order=sex_order, facet_kws={\'sharex\': False, \'sharey\': False} ) plt.show() # Wrapped Faceted Plot for Diamonds Data by Color, 4 Columns sns.displot( data=diamonds, x=\\"carat\\", y=\\"price\\", col=\\"color\\", col_wrap=4, facet_kws={\'sharex\': False, \'sharey\': False} ) plt.show() # Non-shared X-axis Faceted Plot for Diamonds Data by Clarity, 3 Columns sns.displot( data=diamonds, x=\\"carat\\", y=\\"price\\", col=\\"clarity\\", col_wrap=3, facet_kws={\'sharex\': False, \'sharey\': True} ) plt.show()"},{"question":"# Question: Asynchronous Subprocess Manager You are tasked with creating an `AsyncSubprocessManager` class that can handle running multiple shell commands asynchronously and log their outputs to a file. An additional requirement is to provide methods that can stop a currently running subprocess and handle different stream redirections. **Class Requirements:** 1. **Initialization**: - The class should initialize with a list of commands to run. - Optionally, it can accept a file path to which all stdout and stderr outputs should be logged. 2. **Methods**: - `run_all()`: An asynchronous method that runs all commands concurrently and logs their outputs to the specified file. - `stop()`: An asynchronous method that stops all running subprocesses immediately. - `log_output(proc)`: An asynchronous method that reads from the subprocessâ€™s stdout and stderr streams and logs the outputs. **Implementation Details:** - You should use `asyncio.create_subprocess_shell()` for subprocess creation. - Commands should run concurrently using `asyncio.gather()`. - Output logging should be handled such that both `stdout` and `stderr` streams are captured and written to the specified log file. - The `stop()` method should ensure all subprocesses are stopped using the `terminate()` method. - Handle the case where no logging file is provided by printing the outputs to the console. **Constraints**: - Assume the number of commands does not exceed 10. - Commands should not run longer than 60 seconds. **Example Usage:** ```python import asyncio class AsyncSubprocessManager: def __init__(self, commands, log_file=None): self.commands = commands self.log_file = log_file self.processes = [] async def run_all(self): async def run_command(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) self.processes.append(proc) await self.log_output(proc) await asyncio.gather(*[run_command(cmd) for cmd in self.commands]) async def stop(self): for proc in self.processes: proc.terminate() await proc.wait() async def log_output(self, proc): stdout, stderr = await proc.communicate() output = f\'[stdout]n{stdout.decode()}n[stderr]n{stderr.decode()}n\' if self.log_file: with open(self.log_file, \'a\') as f: f.write(output) else: print(output) # Example commands to run commands = [\\"echo \'Hello World\'\\", \\"ls /nonexistentpath\\"] async def main(): manager = AsyncSubprocessManager(commands, log_file=\'output.log\') await manager.run_all() asyncio.run(main()) ``` **Function Signature:** ```python class AsyncSubprocessManager: def __init__(self, commands: List[str], log_file: str = None): # Your code here async def run_all(self): # Your code here async def stop(self): # Your code here async def log_output(self, proc: asyncio.subprocess.Process): # Your code here ```","solution":"import asyncio class AsyncSubprocessManager: def __init__(self, commands, log_file=None): self.commands = commands self.log_file = log_file self.processes = [] async def run_all(self): async def run_command(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) self.processes.append(proc) await self.log_output(proc) await asyncio.gather(*[run_command(cmd) for cmd in self.commands]) async def stop(self): for proc in self.processes: proc.terminate() await proc.wait() async def log_output(self, proc): stdout, stderr = await proc.communicate() output = f\'[stdout]n{stdout.decode()}n[stderr]n{stderr.decode()}n\' if self.log_file: with open(self.log_file, \'a\') as f: f.write(output) else: print(output) # Example commands to run commands = [\\"echo \'Hello World\'\\", \\"ls /nonexistentpath\\"] async def main(): manager = AsyncSubprocessManager(commands, log_file=\'output.log\') await manager.run_all() asyncio.run(main())"},{"question":"You are working with legacy systems that still use the uuencode format for file transfers. Your task is to write a Python function that reads a given binary file, encodes its content using uuencoding, and then decodes it back to verify the integrity of the operation. The decoded file should be identical to the original binary file. Function Signature ```python def verify_uu_file_transfer(input_file: str) -> bool: Encodes the binary content of the given input file using uuencoding and then decodes it back. Verifies if the decoded content matches the original content. Args: input_file (str): The path to the input binary file to be uuencoded and decoded back. Returns: bool: True if the decoded content matches the original content, False otherwise. ``` Input: - `input_file` (str): A file path to the binary file. Output: - `bool`: True if the decoded content matches the original content, False otherwise. Constraints: - The input file will be a valid binary file. - File size will not exceed 10 MB. - Ensure all temporary files used for encoding and decoding are handled appropriately and cleaned up. Example: ```python # Assume we have a binary file named \\"example.bin\\" result = verify_uu_file_transfer(\\"example.bin\\") print(result) # Expected output: True if the uuencode and decode processes are successful, False otherwise. ``` Notes: - Utilize the `uu.encode` and `uu.decode` functions as described in the documentation. - Handle any exceptions gracefully and ensure resources are cleaned up after the operations. - Implement the function to showcase your understanding of file handling and the encoding/decoding process in Python.","solution":"import uu import os def verify_uu_file_transfer(input_file: str) -> bool: Encodes the binary content of the given input file using uuencoding and then decodes it back. Verifies if the decoded content matches the original content. Args: input_file (str): The path to the input binary file to be uuencoded and decoded back. Returns: bool: True if the decoded content matches the original content, False otherwise. encoded_file = input_file + \'.uue\' decoded_file = input_file + \'.decoded\' try: # Encode the file with open(input_file, \'rb\') as f_in, open(encoded_file, \'wb\') as f_out: uu.encode(f_in, f_out, os.path.basename(input_file)) # Decode the file with open(encoded_file, \'rb\') as f_in, open(decoded_file, \'wb\') as f_out: uu.decode(f_in, f_out) # Verify the content with open(input_file, \'rb\') as original, open(decoded_file, \'rb\') as decoded: original_content = original.read() decoded_content = decoded.read() # Return check result return original_content == decoded_content except Exception as e: print(f\\"Error occurred: {e}\\") return False finally: # Clean up temporary files if os.path.exists(encoded_file): os.remove(encoded_file) if os.path.exists(decoded_file): os.remove(decoded_file)"},{"question":"# HTTP Client Implementation using `http.client` Module Your task is to implement a function `fetch_resource(url: str, method: str = \\"GET\\", headers: dict = None, body: str = None, source_address: tuple = None, timeout: int = None) -> dict` that fetches a resource using the specified HTTP method and returns the response details. Function Specification: - `url` (str): The URL of the resource to be fetched. The URL can include a port number. - `method` (str): The HTTP method to be used for the request. Default is \\"GET\\". - `headers` (dict): Optional dictionary of headers to include in the request. - `body` (str): Optional body to include in the request. Default is None. - `source_address` (tuple): Optional tuple of (host, port) to specify the source address. - `timeout` (int): Optional timeout duration in seconds for blocking operations. Return: - The function should return a dictionary with the following keys: - `status` (int): The HTTP status code returned by the server. - `reason` (str): The reason phrase returned by the server. - `headers` (dict): The headers returned by the server. - `body` (str): The body of the response. Example Usage: ```python response = fetch_resource(\\"http://www.python.org\\", method=\\"GET\\") print(response) ``` Example Output: ```json { \\"status\\": 200, \\"reason\\": \\"OK\\", \\"headers\\": { \\"Content-Type\\": \\"text/html; charset=UTF-8\\", ... }, \\"body\\": \\"<!doctype html>...\\" } ``` Constraints and Performance: - The function should handle both HTTP and HTTPS URLs. - Proper error handling should be implemented to manage exceptions like `http.client.HTTPException`, `http.client.InvalidURL`, etc. - The function should close the connection once the request-response cycle is complete. - The function should correctly handle character encoding for request and response bodies. Hints: - Use `urlparse` to parse the provided URL into components. - Use `http.client.HTTPConnection` or `http.client.HTTPSConnection` to establish the connection based on the URL scheme. - Read the entire response body using `HTTPResponse.read()` method. Implement the function following these guidelines and ensure to handle any edge cases that might arise.","solution":"import http.client from urllib.parse import urlparse def fetch_resource(url, method=\\"GET\\", headers=None, body=None, source_address=None, timeout=None): Fetches a resource using the specified HTTP method and returns the response details. :param url: The URL of the resource to be fetched. The URL can include a port number. :param method: The HTTP method to be used for the request. Default is \\"GET\\". :param headers: Optional dictionary of headers to include in the request. :param body: Optional body to include in the request. Default is None. :param source_address: Optional tuple of (host, port) to specify the source address. :param timeout: Optional timeout duration in seconds for blocking operations. :return: Dictionary with keys \'status\', \'reason\', \'headers\', \'body\'. headers = headers or {} # Parse the URL parsed_url = urlparse(url) scheme = parsed_url.scheme if scheme not in (\\"http\\", \\"https\\"): raise ValueError(\\"Unsupported URL scheme: {}\\".format(scheme)) host = parsed_url.hostname port = parsed_url.port or (443 if scheme == \\"https\\" else 80) path = parsed_url.path or \\"/\\" if parsed_url.query: path += \\"?\\" + parsed_url.query conn_class = http.client.HTTPSConnection if scheme == \\"https\\" else http.client.HTTPConnection conn = conn_class(host, port, source_address=source_address, timeout=timeout) try: conn.request(method, path, body, headers) response = conn.getresponse() # Read response details resp_status = response.status resp_reason = response.reason resp_headers = dict(response.getheaders()) resp_body = response.read().decode(response.headers.get_content_charset() or \'utf-8\') return { \\"status\\": resp_status, \\"reason\\": resp_reason, \\"headers\\": resp_headers, \\"body\\": resp_body } finally: conn.close()"},{"question":"# Socket Programming in Python Objective: You are tasked with implementing a basic client-server application using Python\'s socket programming paradigm to demonstrate your understanding of blocking and non-blocking sockets. Your application should be capable of handling basic message exchanges and implement efficient data transfer using non-blocking sockets and `select`. Components: 1. **Server** - The server should be able to handle multiple client connections simultaneously. - It should listen on a specified port, accept incoming connections, and read messages. - Use non-blocking sockets and `select` to manage client connections efficiently. - For each client, send an acknowledgment message back to the client after receiving a message. 2. **Client** - The client should be able to connect to the server. - Send a fixed-length message to the server and wait for an acknowledgment. - Implement retries in case of connection failure. Requirements: 1. **Server Implementation**: - Create a server socket using `socket.AF_INET` and `socket.SOCK_STREAM`. - Bind the server to a given host and port. - Set the server socket to non-blocking mode. - Use `select` to manage multiple client connections. - Implement the protocol for fixed-length messages for communication. - Acknowledge each message received from a client by sending back a \\"Message received\\" string. 2. **Client Implementation**: - Establish a connection to the server. - Implement the client socket with blocking mode initially. - Send a fixed-length message, \\"HelloServer\\", to the server. - Handle retries for failed connections with a maximum of 3 attempts. - Display the acknowledgment message received from the server. Input and Output Formats: **Input**: - The server will be invoked as `python server.py <host> <port>`. - The client will be invoked as `python client.py <host> <port>`. **Output**: - Server should print connection establishment and message acknowledgment logs. - The client should print the acknowledgment message received from the server. Sample Execution: **Server:** ```bash python server.py localhost 12345 # Output: # Listening on localhost:12345 # Accepted connection from (\'127.0.0.1\', \'some_port\') # Received message: HelloServer # Sent Acknowledgment: Message received ``` **Client:** ```bash python client.py localhost 12345 # Output: # Connection established to localhost:12345 # Sent message: HelloServer # Received acknowledgment: Message received ``` Constraints: - Manage sockets using non-blocking mode and `select` for the server. - Use fixed-length messages (e.g., \\"HelloServer\\" has a fixed length). - Handle connection retries for the client with a maximum of 3 attempts. - Consider network buffer handling while sending and receiving messages. You are free to structure your code as you see fit, provided it adheres to the above requirements and implements a robust client-server communication system.","solution":"import socket import select # Server code def start_server(host, port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(False) inputs = [server_socket] outputs = [] message_queues = {} print(f\\"Listening on {host}:{port}\\") while inputs: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is server_socket: client_socket, client_address = s.accept() print(f\\"Accepted connection from {client_address}\\") client_socket.setblocking(False) inputs.append(client_socket) message_queues[client_socket] = \\"\\" else: data = s.recv(1024) if data: print(f\\"Received message: {data.decode()}\\") message_queues[s] = \\"Message received\\" if s not in outputs: outputs.append(s) else: print(f\\"Closing connection to {s.getpeername()}\\") if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] for s in writable: if s in message_queues: response_message = message_queues[s] if response_message: s.send(response_message.encode()) print(f\\"Sent acknowledgment: {response_message}\\") del message_queues[s] outputs.remove(s) for s in exceptional: print(f\\"Handling exceptional condition for {s.getpeername()}\\") if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] # Client code def start_client(host, port): message = \\"HelloServer\\" retry_limit = 3 for attempt in range(retry_limit): try: client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((host, port)) print(f\\"Connection established to {host}:{port}\\") client_socket.sendall(message.encode()) print(f\\"Sent message: {message}\\") response = client_socket.recv(1024) print(f\\"Received acknowledgment: {response.decode()}\\") client_socket.close() break except Exception as e: print(f\\"Failed to connect, retrying... ({attempt+1}/{retry_limit})\\") client_socket.close() if attempt == retry_limit - 1: print(\\"Failed to establish connection after multiple attempts.\\")"},{"question":"**Question:** Function Implementations using `sysconfig` Module in Python You are required to create a Python script that employs the `sysconfig` module to retrieve and display specific configuration details. Implement the following functions according to the provided specifications: 1. **Function:** `get_stdlib_path()` - **Description:** Returns the path for the `stdlib` directory under the default installation scheme. - **Output:** String representing the path of the `stdlib` directory. 2. **Function:** `get_platform_info()` - **Description:** Returns a dictionary containing platform information, including the current platform string and the Python version. - **Output:** A dictionary with two keys: `\'platform\'` and `\'python_version\'`. 3. **Function:** `get_variable_value(variable_name)` - **Description:** Returns the value of the specified configuration variable. - **Input:** `variable_name` (String) - The name of the configuration variable to look up. - **Output:** The value of the specified configuration variable, or `None` if the variable is not found. 4. **Function:** `get_all_variables()` - **Description:** Returns a dictionary of all configuration variables relevant to the current platform. - **Output:** A dictionary where the keys are variable names and the values are the respective configuration values. **Constraints:** - You can assume the `sysconfig` module is available and can be imported. - Handle cases where configuration variables or paths are not found by returning `None` where applicable. **Example Usage:** ```python import sysconfig def get_stdlib_path(): # Your implementation here pass def get_platform_info(): # Your implementation here pass def get_variable_value(variable_name): # Your implementation here pass def get_all_variables(): # Your implementation here pass # Example checks print(get_stdlib_path()) # Should print the path to stdlib directory print(get_platform_info()) # Should print a dictionary containing platform and python version print(get_variable_value(\\"AR\\")) # Should print the value of \\"AR\\" config variable print(get_all_variables()) # Should print all config variables ``` Implement these functions to demonstrate your understanding of the `sysconfig` module and its capabilities.","solution":"import sysconfig def get_stdlib_path(): Returns the path for the stdlib directory under the default installation scheme. return sysconfig.get_path(\'stdlib\') def get_platform_info(): Returns a dictionary containing platform information, including the current platform string and the Python version. return { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version() } def get_variable_value(variable_name): Returns the value of the specified configuration variable. Input: - variable_name (String): The name of the configuration variable to look up. Output: - The value of the specified configuration variable, or None if the variable is not found. return sysconfig.get_config_var(variable_name) def get_all_variables(): Returns a dictionary of all configuration variables relevant to the current platform. Output: - A dictionary where the keys are variable names and the values are the respective configuration values. return sysconfig.get_config_vars()"},{"question":"# Question: Implementing Fault-Tolerant Distributed Training with PyTorch Elastic In this exercise, you will demonstrate your understanding of PyTorch\'s fault-tolerant distributed training capabilities by implementing a function that initializes and runs a distributed training job using the `torch.distributed.elastic` framework. Your task is to implement a function that can: 1. Initialize the distributed environment. 2. Manage multiple worker processes. 3. Handle failures gracefully and restart worker processes as needed. Please complete the function `run_distributed_training` which takes the following inputs: - `num_workers`: int, the number of worker processes to run. - `training_script`: str, the script path to the training code. - `max_retries`: int, the maximum number of times to retry failed worker processes. The function should: 1. Set up and initialize the distributed environment using provided `num_workers`. 2. Use the PyTorch Elastic framework to run the `training_script`. 3. Implement fault tolerance by retrying failed processes up to `max_retries` times. Constraints: - You can assume the training script is compatible with distributed training. - The environment should handle any worker process failures and restart the failed processes. **Input:** ```python num_workers: int training_script: str max_retries: int ``` **Output:** The function does not need to return anything but should ensure the training job completes successfully unless the maximum number of retries is reached. Example: ```python def run_distributed_training(num_workers: int, training_script: str, max_retries: int): # Your implementation here ``` You should test your function with a mock training script and ensure that it can handle process failures and retries appropriately. Notes: - Reading through the PyTorch `torch.distributed.elastic` documentation will be crucial to understanding how to implement this function. - Think about how you would log and track the number of retries, and how you would restart failed processes. Good luck!","solution":"import subprocess import time def run_distributed_training(num_workers: int, training_script: str, max_retries: int): Initializes and runs a distributed training job using the torch.distributed.elastic framework. Parameters: - num_workers: int, the number of worker processes to run. - training_script: str, the script path to the training code. - max_retries: int, the maximum number of times to retry failed worker processes. retries = 0 while retries < max_retries: try: # Use torch.distributed.launch to run the training script with multiple workers subprocess.run( [\\"python\\", \\"-m\\", \\"torch.distributed.launch\\", \\"--nproc_per_node\\", str(num_workers), \\"--use_env\\", training_script], check=True ) return # If the training completes successfully, exit the function except subprocess.CalledProcessError as e: print(f\\"Training failed with error: {e}. Retrying... ({retries + 1}/{max_retries})\\") retries += 1 time.sleep(5) # Wait a bit before retrying print(\\"Max retries reached. Training failed.\\") return"},{"question":"**Objective:** To assess your understanding of seaborn\'s plotting functions, you are required to write a function that visualizes and analyzes a dataset. Your focus will be on creating an informative plot that includes multiple features such as grouping, custom markers, and error bands. **Details:** You are given a dataset containing time series data. Your task is to write a function `create_plot` that: 1. Loads the `fmri` dataset available in seaborn. 2. Filters the dataset to include only the \'stim\' event type. 3. Creates a line plot of signal over time for each region with different colors, and shows the trend for each region. 4. Adds markers to indicate the sampled data points. 5. Includes error bands to show the variability of the data. **Function Signature:** ```python def create_plot(): pass ``` **Expected Output:** A plot should be displayed (or saved), which: - Contains line plots for each region with different colors. - Includes markers at the sampled data points. - Displays error bands around the line plots for showing variability. **Constraints:** - You must use the seaborn `so.Plot`, `so.Line`, and `so.Band` classes. - The plot should clearly differentiate between the regions using colors. - Ensure that the x-axis represents the `timepoint` and the y-axis represents the `signal`. **Example:** When you execute the `create_plot` function, it should display a plot as per the described requirements. Below is a sample code for generating a basic plot, which you need to extend to meet all the specifications: ```python import seaborn.objects as so from seaborn import load_dataset def create_plot(): fmri = load_dataset(\\"fmri\\") filtered_fmri = fmri.query(\\"event == \'stim\'\\") plot = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\") plot.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg()) plot.add(so.Band(), so.Est()) plot.show() create_plot() ``` This question assesses your ability to work with seaborn\'s advanced plotting capabilities and to produce a comprehensive and clear visualization that can provide meaningful insights.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plot(): Creates a plot for the \'stim\' events in the fmri dataset. The plot shows the signal over time for each region with trend lines, markers for sampled data points, and error bands for variability. # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Filter the dataset to include only \'stim\' event type filtered_fmri = fmri.query(\\"event == \'stim\'\\") # Create the plot plot = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\") plot.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\")) plot.add(so.Band()) # Show the plot plot.show()"},{"question":"# Question: Using the seaborn library, specifically the `seaborn.objects` module, create a visualization that displays the average signal change in the `fmri` dataset over time, differentiated by event type and shaded to show the range of values. Instructions: 1. Load the `fmri` dataset using `seaborn.load_dataset(\\"fmri\\")`. 2. Filter the dataset to only include data where the `region` is `\'parietal\'`. 3. Create a plot to show the average signal value over the `timepoint` for each `event`. 4. Add a `Line` mark to show the average signal change over time. 5. Shade the area between the minimum and maximum signal values at each time point, using the `Band` mark. 6. Color the lines and bands by `event`. The final visualization should clearly display: - Lines representing the average signal change over time for each event. - Shaded areas representing the range of signal values (min to max) at each timepoint for each event. Example code structure (you need to complete the missing parts): ```python import seaborn.objects as so from seaborn import load_dataset # Load and filter the dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Create the plot plot = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band()) ) # Show the plot plot.show() ``` Constraints: - Ensure your code is efficient and comments are added to explain each step. - The plot should be well-labeled and readable. **Hint:** Use `so.Agg()` to get the average signal and `so.Est()` to calculate the min/max for the band.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_fmri_signal(): Plots the average signal change over time for the \'parietal\' region in the fmri dataset, differentiated by event type, with shaded areas representing the range of values. # Load and filter the dataset to include only the \'parietal\' region fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Create the plot with average line and shaded area for min/max values plot = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band()) ) # Show the plot plot.show()"},{"question":"Based on the documentation for the `pkgutil` module, here is a coding assessment question to test the understanding of its functionalities: # Question: Custom Module Walker Consider you are working with a complex Python project where multiple packages and modules are distributed across different directories. This project requires a utility to list all accessible modules and packages starting from a given directory and display a structured tree of their hierarchy. Your task is to implement a function `display_module_tree` that takes a single argument `start_path` (a string representing the root directory) and prints out the hierarchical structure of all Python modules under that directory. Your function should use the `pkgutil` utilities and handle any import errors gracefully. Input - `start_path` (str): The path to the root directory where you start the search for modules. Output - Print the hierarchical structure of all found modules and submodules. Constraints - Only display the modules and submodules that can be successfully imported. - Handle any import errors gracefully by printing a warning message. Example Given the following directory structure: ``` /project_root /pkg1 __init__.py module1.py /pkg2 __init__.py subpkg1 __init__.py module2.py /pkg3 module3.py ``` Calling `display_module_tree(\'/project_root\')` should print: ``` pkg1 module1 pkg2 subpkg1 module2 pkg3 module3 ``` Function Signature ```python def display_module_tree(start_path: str) -> None: # your implementation here ``` You should make use of `pkgutil.walk_packages` or similar utilities within the `pkgutil` module to implement this function.","solution":"import os import pkgutil import importlib def display_module_tree(start_path: str) -> None: def walk_packages(path, prefix=\\"\\"): for finder, name, ispkg in pkgutil.iter_modules(path): module_name = prefix + name try: importlib.import_module(module_name) print(\\" \\" * module_name.count(\'.\') + module_name.split(\'.\')[-1]) except ImportError as e: print(f\\"Warning: Failed to import {module_name} due to {e}\\") continue if ispkg: subpath = os.path.join(finder.path, name) if os.path.isdir(subpath): walk_packages([subpath], module_name + \'.\') if os.path.isdir(start_path): walk_packages([start_path]) else: print(f\\"Error: Provided path \'{start_path}\' is not a directory.\\")"},{"question":"Coding Assessment Question **Objective**: Write a function to manipulate a pandas DataFrame and then write test functions to verify the correctness of your data manipulations using the `pandas.testing` utilities. # Problem Statement 1. Create a function `process_data` that takes a pandas DataFrame as input. This function should: - Add a new column called `Total` which is the sum of two existing columns: `ColumnA` and `ColumnB`. - Replace any NaN values in `ColumnC` with the string `\'missing\'`. 2. Write test functions to validate the `process_data` function using the following pandas testing functions: - `pandas.testing.assert_frame_equal` - `pandas.testing.assert_series_equal` # Input and Output - **Input**: A pandas DataFrame with at least `ColumnA`, `ColumnB`, and `ColumnC`. - **Output**: A modified DataFrame with the described transformations applied. # Constraints - You can assume that `ColumnA` and `ColumnB` contain numerical values. - The DataFrame may contain NaN values in `ColumnC`. # Function Signature ```python import pandas as pd def process_data(df: pd.DataFrame) -> pd.DataFrame: pass def test_process_data(): pass ``` # Example ```python import pandas as pd # Example DataFrame data = { \'ColumnA\': [1, 2, 3], \'ColumnB\': [4, 5, 6], \'ColumnC\': [None, \'data\', None] } df = pd.DataFrame(data) # Expected DataFrame after processing expected_data = { \'ColumnA\': [1, 2, 3], \'ColumnB\': [4, 5, 6], \'ColumnC\': [\'missing\', \'data\', \'missing\'], \'Total\': [5, 7, 9] } expected_df = pd.DataFrame(expected_data) def process_data(df: pd.DataFrame) -> pd.DataFrame: df[\'Total\'] = df[\'ColumnA\'] + df[\'ColumnB\'] df[\'ColumnC\'] = df[\'ColumnC\'].fillna(\'missing\') return df def test_process_data(): # Create the initial DataFrame data = { \'ColumnA\': [1, 2, 3], \'ColumnB\': [4, 5, 6], \'ColumnC\': [None, \'data\', None] } df = pd.DataFrame(data) # Expected DataFrame after processing expected_data = { \'ColumnA\': [1, 2, 3], \'ColumnB\': [4, 5, 6], \'ColumnC\': [\'missing\', \'data\', \'missing\'], \'Total\': [5, 7, 9] } expected_df = pd.DataFrame(expected_data) # Process the DataFrame result_df = process_data(df) # Use pandas testing functions to validate the result pd.testing.assert_frame_equal(result_df, expected_df) # Run the test test_process_data() ``` In this problem, you are required to complete the `process_data` function and write appropriate test cases to validate the transformations. Your focus should be on using `pandas.testing` for assertions.","solution":"import pandas as pd def process_data(df: pd.DataFrame) -> pd.DataFrame: Processes the DataFrame by: - Adding a new column \'Total\' which is the sum of \'ColumnA\' and \'ColumnB\'. - Replacing any NaN values in \'ColumnC\' with the string \'missing\'. Args: df (pd.DataFrame): Input DataFrame with columns \'ColumnA\', \'ColumnB\', and \'ColumnC\'. Returns: pd.DataFrame: Modified DataFrame with the new column and NaN values replaced. df[\'Total\'] = df[\'ColumnA\'] + df[\'ColumnB\'] df[\'ColumnC\'] = df[\'ColumnC\'].fillna(\'missing\') return df"},{"question":"Objective: Your task is to implement a command-line utility using the deprecated \\"optparse\\" module in Python. This utility will process and display information about user-provided files. Requirements: 1. Implement a Python script that uses the \\"optparse\\" module to create a command-line interface. 2. The script should accept the following command-line options: - `-f <filename>` or `--file <filename>`: Specify the name of a file to process. - `-v` or `--verbose`: Enable verbose output. - `-h` or `--help`: Display help message. 3. The script should: - Check if the provided file name exists and is readable. - Print the size of the file in bytes. - If the verbose option is enabled, print additional information such as last modified time and file type. Input: - Command-line arguments using the above-specified options. Output: - Standard output displaying the information about the file. - Print relevant error messages if the file does not exist or is not readable. Constraints: - Ensure backward compatibility for users who may have scripts that still use these deprecated modules. - Handle incorrect or missing command-line arguments gracefully. Example Usage and Output: ```sh python file_info.py -f example.txt File Size: 1024 bytes python file_info.py -f example.txt -v File Size: 1024 bytes Last Modified Time: 2023-10-05 14:30:00 File Type: text/plain python file_info.py --file non_existent.txt Error: The file \'non_existent.txt\' does not exist or is not readable. python file_info.py -h Usage: file_info.py [options] Options: -h, --help show this help message and exit -f FILE, --file=FILE specify the file name to process -v, --verbose enable verbose output ``` Implementation Notes: - You can use the following functions/modules to achieve the requirements: - `os.path` module for file-related operations. - `optparse` module for parsing command-line options. - `time` module for formatting the last modified time. Create a Python script named `file_info.py` implementing the above logic.","solution":"import os import time from optparse import OptionParser def file_info(filename, verbose): Process the file and print information. if not os.path.isfile(filename): print(f\\"Error: The file \'{filename}\' does not exist or is not readable.\\") return file_size = os.path.getsize(filename) print(f\\"File Size: {file_size} bytes\\") if verbose: last_modified_time = time.ctime(os.path.getmtime(filename)) file_type = \\"text/plain\\" if filename.endswith(\'.txt\') else \\"unknown\\" print(f\\"Last Modified Time: {last_modified_time}\\") print(f\\"File Type: {file_type}\\") def main(): parser = OptionParser() parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"specify the file name to process\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"enable verbose output\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"Filename not provided. Use -f or --file to specify the file name.\\") file_info(options.filename, options.verbose) if __name__ == \\"__main__\\": main()"},{"question":"# Question You are tasked with creating a utility script that performs several key operating system interactions using Python\'s \\"os\\" module. The script should implement the following functionality: 1. **Path Information and Directory Content Listing:** - Create a function `path_info_and_list(path)` that takes a path as input and: - If the path is a directory, it lists all files and subdirectories within it. - If the path is a file, it returns the file size and when it was last modified. - If the path does not exist, it should return an appropriate message. 2. **Environment Variable Management:** - Create a function `manage_env_var()` that: - Retrieves the value of an environment variable named `MY_VAR`. - If `MY_VAR` exists, change its value to \'new_value\'. - If `MY_VAR` does not exist, it initializes it with the value \'initial_value\'. - Finally, the function should print the value of `MY_VAR`. 3. **Process Handling:** - Create a function `create_child_process_and_wait()` that: - Forks the current process using `os.fork()`. - In the child process, set an environment variable `CHILD_VAR` to \'child_value\', print a message such as \\"Child process ID: (pid)\\", and then exit the child process. - In the parent process, wait for the child to finish, then print a message \\"Child process (pid) has finished\\". 4. **File Descriptor Operations:** - Create a function `dup_and_write(fd, msg)` that: - Duplicates the file descriptor `fd`. - Writes the message `msg` to both the original and duplicated file descriptors. Constraints: - Assume that the paths provided to the script will be valid strings. - Handle errors properly, such as invalid paths, permissions issues, etc. - Your code should perform necessary checks to ensure the operations are valid (e.g., checking if a path exists before attempting to list or read it). # Example Usage ```python # Assuming main() function calls all the above functionalities to demonstrate their use def main(): path_info_and_list(\\"/some/path\\") manage_env_var() create_child_process_and_wait() with open(\\"output.txt\\", \\"w\\") as f: dup_and_write(f.fileno(), \\"Sample message\\") if __name__ == \\"__main__\\": main() ``` **Expected Output:** - A detailed list of the contents of the provided directory or file information if a file is provided. - The values of the environment variable `MY_VAR` as changed or created. - Confirmation messages regarding child process creation and termination. - The message \\"Sample message\\" should be written to the file \\"output.txt\\" twice, once by the original file descriptor and once by the duplicated file descriptor.","solution":"import os import time def path_info_and_list(path): Retrieve path information and list directory contents or file information. Parameters: path (str): The path to a file or directory. Returns: dict or str: Directory contents if path is a directory, file information if path is a file, or an error message if the path does not exist. if not os.path.exists(path): return {\\"error\\": \\"Path does not exist\\"} if os.path.isdir(path): return {\\"directory_contents\\": os.listdir(path)} elif os.path.isfile(path): file_size = os.path.getsize(path) last_modified = time.ctime(os.path.getmtime(path)) return {\\"file_size\\": file_size, \\"last_modified\\": last_modified} else: return {\\"error\\": \\"Invalid path type\\"} def manage_env_var(): Manage the environment variable \'MY_VAR\'. Returns: str: The value of \'MY_VAR\' after modification. my_var = os.getenv(\'MY_VAR\') if my_var is not None: os.environ[\'MY_VAR\'] = \'new_value\' else: os.environ[\'MY_VAR\'] = \'initial_value\' return os.environ[\'MY_VAR\'] def create_child_process_and_wait(): Fork the current process, set an environment variable in the child process, and wait for the child process to finish in the parent process. Returns: tuple: The process ID of the child process and a confirmation message. pid = os.fork() if pid == 0: # Child process os.environ[\'CHILD_VAR\'] = \'child_value\' print(f\'Child process ID: {os.getpid()}\') os._exit(0) else: # Parent process os.waitpid(pid, 0) return pid, f\'Child process {pid} has finished\' def dup_and_write(fd, msg): Duplicate a file descriptor and write a message to both the original and duplicated file descriptors. Parameters: fd (int): The file descriptor to duplicate. msg (str): The message to write to the file descriptors. # Duplicate the file descriptor dup_fd = os.dup(fd) # Write the message to the original file descriptor os.write(fd, msg.encode()) # Write the message to the duplicated file descriptor os.write(dup_fd, msg.encode()) # Close the duplicated file descriptor os.close(dup_fd)"},{"question":"# Temporary File Management You are tasked with creating a utility function that utilizes the `tempfile` module to handle operation logs in a secure, temporary storage manner. Implement a function `log_server_activity` that: 1. **Creates a temporary file** to store server activity logs. 2. **Writes a given list of log entries** to this file. 3. **Reads back the entries** to confirm they were written correctly. 4. **Ensures** the temporary file is removed after usage. You need to implement the following function: ```python import tempfile def log_server_activity(log_entries): Logs server activity to a temporary file and then reads back the entries for confirmation. Args: log_entries (list of str): A list of log entries to be written to the temporary file. Returns: list of str: The log entries read back from the temporary file. Example: >>> log_server_activity([\\"Started server\\", \\"Received request\\", \\"Processed request\\"]) [\'Started server\', \'Received request\', \'Processed request\'] # Your code here ``` # Constraints: - The function should handle cases where the list of log entries is empty. - The function should work on all platforms (i.e., consider differences between Unix and Windows systems regarding temporary file handling). - The function should use context managers to ensure proper cleanup of the temporary file. # Example: ```python entries = [\\"Started server\\", \\"Received request\\", \\"Processed request\\"] log_server_activity(entries) ``` Expected Output: `[\'Started server\', \'Received request\', \'Processed request\']` Your implementation should demonstrate a sound understanding of temporary file management using the `tempfile` module, ensuring security and proper cleanup.","solution":"import tempfile def log_server_activity(log_entries): Logs server activity to a temporary file and then reads back the entries for confirmation. Args: log_entries (list of str): A list of log entries to be written to the temporary file. Returns: list of str: The log entries read back from the temporary file. try: # Creating a temporary file with tempfile.NamedTemporaryFile(mode=\'w+\', delete=True) as temp_file: # Writing log entries to the temporary file for entry in log_entries: temp_file.write(entry + \'n\') # Ensuring the writes are flushed to disk temp_file.flush() # Move the file pointer to the beginning of the file temp_file.seek(0) # Reading back the entries logged_entries = temp_file.readlines() # Stripping newline characters in the read log entries logged_entries = [entry.strip() for entry in logged_entries] return logged_entries except Exception as e: # Handle potential exceptions print(f\\"An error occurred: {e}\\") return []"},{"question":"# Seaborn Scatter Plot Assessment Objective: You are provided with a dataset named \\"tips\\". You will use the seaborn library to create scatter plots that visualize relationships within the data. Your task is to demonstrate an understanding of the various functionalities of seaborn scatter plots. Instructions: 1. Load the \\"tips\\" dataset using seaborn. 2. Create a scatter plot that visualizes the relationship between `total_bill` and `tip`. 3. Enhance the plot by mapping the `hue` to the `time` of day. 4. Vary the markers in the plot by mapping `style` to the `day` of the week. 5. Create a second scatter plot that maps both `hue` and `size` to the `size` of the party. 6. Generate a third scatter plot using `relplot` to facet by `time` and show the relationship between `total_bill` and `tip` across different subplots for each `day`. 7. Customize the marker shapes using a dictionary for different values of the `time` variable across any of the plots created. 8. Save the figures generated to the disk. Expected Input and Output: - **Input:** No direct input required. The dataset is to be loaded within the code. - **Output:** Three scatter plots saved as image files: 1. `scatterplot_basic.png` 2. `scatterplot_hue_size.png` 3. `scatterplot_facet.png` Constraints: - Use the seaborn library. - Ensure all figures are titled appropriately. - Add proper labels to the axes. Performance Requirements: - The plots should be generated efficiently, avoiding unnecessary computations. Solution Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic scatter plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of total_bill vs tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_basic.png\\") # 2. Scatter plot with hue and style plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter plot with Hue by Time and Style by Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_hue_style.png\\") # 3. Second scatter plot with hue and size plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter plot with Hue and Size by Party Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_hue_size.png\\") # 4. Facet plot with relplot sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.savefig(\\"scatterplot_facet.png\\") # Custom marker shapes markers = {\\"Lunch\\": \\"o\\", \\"Dinner\\": \\"s\\"} plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", style=\\"time\\", markers=markers) plt.title(\\"Scatter plot with Custom Markers\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_custom_markers.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic scatter plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of total_bill vs tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_basic.png\\") plt.close() # 2. Scatter plot with hue and style plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter plot with Hue by Time and Style by Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_hue_style.png\\") plt.close() # 3. Second scatter plot with hue and size plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter plot with Hue and Size by Party Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_hue_size.png\\") plt.close() # 4. Facet plot with relplot sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.savefig(\\"scatterplot_facet.png\\") plt.close() # Custom marker shapes markers = {\\"Lunch\\": \\"o\\", \\"Dinner\\": \\"s\\"} plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", style=\\"time\\", markers=markers) plt.title(\\"Scatter plot with Custom Markers\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatterplot_custom_markers.png\\") plt.close()"},{"question":"# Async Dataset Processing with PyTorch Futures Objective: You are required to use PyTorch\'s `torch.futures.Future` and its utility functions to asynchronously process a dataset. The task is to implement a function that performs a series of independent data transformation tasks in parallel and then aggregates their results. This will test your understanding of asynchronous programming concepts and PyTorch. Problem Statement: Implement a function `async_process_dataset` that takes a list of data items and a transformation function as input. The function should apply the transformation function to each data item asynchronously using `torch.futures.Future`. Finally, the function should return a list containing the results of these transformations, ensuring that all transformations are complete. Function Signature: ```python import torch def async_process_dataset(data_list: list, transform_fn: callable) -> list: pass ``` Inputs: - `data_list` (list): A list of data items that need to be processed. - `transform_fn` (callable): A function that takes a single data item and returns its transformed version. This function will be called asynchronously. Outputs: - `result_list` (list): A list containing the transformed data items in the same order as the input data list. Constraints: - You must use the `torch.futures.Future` class for the asynchronous execution. - You are required to use `torch.futures.collect_all` to aggregate the results. - Assume that the `transform_fn` may include computationally expensive operations. Example: Suppose you have a dataset containing integer values and a simple transformation function `transform_fn` that squares each integer. ```python def transform_fn(x): return x * x data_list = [1, 2, 3, 4, 5] result = async_process_dataset(data_list, transform_fn) print(result) # Output should be [1, 4, 9, 16, 25] ``` Notes: 1. The order of the results in `result_list` should match the order of the input `data_list`. 2. Ensure efficient handling of the asynchronous tasks and consider edge cases such as empty input list or large datasets. You are encouraged to provide additional test cases to validate your implementation.","solution":"import torch def async_process_dataset(data_list: list, transform_fn: callable) -> list: Asynchronously processes a list of data items using the provided transformation function. Args: data_list (list): A list of data items to be processed. transform_fn (callable): A function to transform each data item. Returns: list: A list containing the transformed data items. def async_transform(x): fut = torch.futures.Future() result = transform_fn(x) fut.set_result(result) return fut futures = [async_transform(data_item) for data_item in data_list] collected_fut = torch.futures.collect_all(futures) result_list = [fut.value() for fut in collected_fut.wait()] return result_list"},{"question":"**Custom Serialization and Deserialization Implementation** **Objective:** Create a Python class that demonstrates the capabilities of the `pickle` module, including handling of custom serialization and deserialization using special methods like `__getstate__` and `__setstate__`. The goal is to ensure that students understand how to control the pickling process for complex objects. **Problem Statement:** You are tasked with creating a class `CustomObject` that represents an object with multiple attributes, including some that require special handling during serialization and deserialization. You will implement custom methods to control this behavior. **Requirements:** 1. Implement a class `CustomObject` with the following attributes: - `name` (a string): The name of the object. - `data` (a list): A list of numerical data values. - `_metadata` (a dictionary): Internal metadata that should not be serialized. 2. Implement the following methods in the `CustomObject` class: - `__init__(self, name: str, data: List[float], metadata: Dict[str, Any])`: Constructor to initialize the object. - `__getstate__(self) -> Dict[str, Any]`: This method should return a dictionary containing only the `name` and `data` attributes. - `__setstate__(self, state: Dict[str, Any])`: This method should restore the state of the object from a given dictionary. It should also set `_metadata` to an empty dictionary. 3. Write a function `serialize_object(obj: CustomObject) -> bytes` that uses `pickle` to serialize a `CustomObject` instance to a byte stream. 4. Write a function `deserialize_object(data: bytes) -> CustomObject` that uses `pickle` to deserialize a byte stream back to a `CustomObject` instance. 5. Write a `main()` function that demonstrates the following: - Creating an instance of `CustomObject`. - Serializing the instance using `serialize_object`. - Deserializing the byte stream back to a `CustomObject` instance using `deserialize_object`. - Print the original object, the serialized byte stream, and the deserialized object to verify correctness. **Example:** ```python # (1) Define the CustomObject class class CustomObject: def __init__(self, name: str, data: list, metadata: dict): self.name = name self.data = data self._metadata = metadata def __getstate__(self): state = self.__dict__.copy() del state[\'_metadata\'] # Remove _metadata from being pickled return state def __setstate__(self, state): self.__dict__.update(state) self._metadata = {} # Restore _metadata as an empty dictionary # (2) Define serialization and deserialization functions import pickle def serialize_object(obj: CustomObject) -> bytes: return pickle.dumps(obj) def deserialize_object(data: bytes) -> CustomObject: return pickle.loads(data) # (3) Demonstrate the usage in the main function def main(): obj = CustomObject(name=\'TestObj\', data=[1.0, 2.5, 3.8], metadata={\'key\': \'value\'}) serialized_data = serialize_object(obj) deserialized_obj = deserialize_object(serialized_data) print(\\"Original Object:\\", obj) print(\\"Serialized Data:\\", serialized_data) print(\\"Deserialized Object:\\", deserialized_obj) if __name__ == \\"__main__\\": main() ``` **Constraints:** - You may assume that the numerical data in the `data` list is always a list of floating-point numbers. - The `metadata` dictionary can contain any key-value pairs, but it should not be included during serialization. **Additional Notes:** - Ensure to handle potential edge cases, such as empty data lists or metadata dictionaries. - Follow proper coding standards and include necessary comments for clarity.","solution":"from typing import Dict, Any, List import pickle class CustomObject: def __init__(self, name: str, data: List[float], metadata: Dict[str, Any]): self.name = name self.data = data self._metadata = metadata def __getstate__(self): # Get the dictionary of the object\'s current state, except for _metadata state = self.__dict__.copy() # Do not serialize _metadata del state[\'_metadata\'] return state def __setstate__(self, state): # Restore the state self.__dict__.update(state) # Reinitialize _metadata as an empty dictionary self._metadata = {} def __repr__(self): return f\\"CustomObject(name={self.name}, data={self.data}, _metadata={self._metadata})\\" def serialize_object(obj: CustomObject) -> bytes: return pickle.dumps(obj) def deserialize_object(data: bytes) -> CustomObject: return pickle.loads(data) def main(): obj = CustomObject(name=\'TestObj\', data=[1.0, 2.5, 3.8], metadata={\'key\': \'value\'}) serialized_data = serialize_object(obj) deserialized_obj = deserialize_object(serialized_data) print(\\"Original Object:\\", obj) print(\\"Serialized Data:\\", serialized_data) print(\\"Deserialized Object:\\", deserialized_obj) if __name__ == \\"__main__\\": main()"},{"question":"# **JIT Optimization in PyTorch** **Objective:** Your task is to take a simple neural network model defined using PyTorch, optimize it using PyTorch\'s JIT compiler, and then use the optimized model to make inferences. **Requirements:** 1. Define a simple feedforward neural network in PyTorch. 2. Use the `torch.jit.script` method to create a TorchScript version of the model. 3. Save the TorchScript model to disk. 4. Load the TorchScript model from disk. 5. Make and print an inference using the loaded TorchScript model on a sample input. **Details:** - **Model Architecture:** The model should have one hidden layer with ReLU activation. - **Input/Output:** - The input to the model will be a single batch of data with a shape `(1, input_size)`. - The output from the model will be of shape `(1, output_size)`. - **Constraints:** - Use the `torch.save` and `torch.load` functions for saving and loading the model. - Ensure reproducibility by setting the random seed. **Input and Output Format:** - The input size and output size should be parameters defined at the beginning of your code. - The saved model will be in the form of a file on disk. - The sample input for inference will be a tensor with shape `(1, input_size)`. **Steps to Implement:** 1. Define the feedforward neural network model in PyTorch. 2. Convert the model into a TorchScript using the `torch.jit.script` method. 3. Save the scripted model to a file named `optimized_model.pt`. 4. Load the model from the `optimized_model.pt` file. 5. Perform inference on a sample input tensor and print the output. **Code Structure:** ```python import torch import torch.nn as nn # Set the random seed for reproducibility torch.manual_seed(0) class SimpleNN(nn.Module): def __init__(self, input_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Input and output dimensions input_size = 10 output_size = 1 # Instantiate the model model = SimpleNN(input_size, output_size) # Step 2: Convert the model into a TorchScript scripted_model = torch.jit.script(model) # Step 3: Save the scripted model to a file scripted_model.save(\\"optimized_model.pt\\") # Step 4: Load the scripted model loaded_model = torch.jit.load(\\"optimized_model.pt\\") # Step 5: Perform inference sample_input = torch.randn(1, input_size) output = loaded_model(sample_input) print(output) ``` Validate each step by running your code and confirming the expected behavior.","solution":"import torch import torch.nn as nn # Set the random seed for reproducibility torch.manual_seed(0) class SimpleNN(nn.Module): def __init__(self, input_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Input and output dimensions input_size = 10 output_size = 1 # Instantiate the model model = SimpleNN(input_size, output_size) # Step 2: Convert the model into a TorchScript scripted_model = torch.jit.script(model) # Step 3: Save the scripted model to a file scripted_model.save(\\"optimized_model.pt\\") # Step 4: Load the scripted model loaded_model = torch.jit.load(\\"optimized_model.pt\\") # Step 5: Perform inference sample_input = torch.randn(1, input_size) output = loaded_model(sample_input) print(output)"},{"question":"**Title**: Custom Instance and Method Binding in Python **Objective**: Demonstrate understanding of Instance Method Objects and Method Objects by implementing a custom class with advanced method handling in Python. **Question**: Create a Python class `CustomClass` that demonstrates the following functionalities using the concepts of instance method objects and method objects as described in the provided documentation: 1. Create an instance method within `CustomClass`. 2. Bind a separate method dynamically to an instance of `CustomClass`. 3. Retrieve and display the function associated with both an instance method and a method object. 4. Demonstrate the binding of a function to an instance of a class dynamically and utilize it. # Detailed Requirements: 1. Define a class `CustomClass` with: - An instance method `instance_method` that simply prints \\"Instance Method Called\\". 2. Implement a function `bind_method_to_instance` which takes: - An instance of `CustomClass`. - A callable (another function). This function should bind the given callable as a new method to the provided instance. 3. Implement a function `get_associated_function` which takes: - A method object or instance method object. - Returns the actual function object associated with the method. 4. Demonstrate all functionalities in the main part of your script: - Create an instance of `CustomClass` and call `instance_method`. - Define an external function `external_function` that prints \\"External Function Called\\". - Use `bind_method_to_instance` to bind `external_function` to the instance. - Retrieve and print the function object associated with both `instance_method` and the newly bound method. # Constraints: - You must not use any external libraries. - Your solution should handle cases where the methods or callables provided are invalid or `None`. # Example Usage: ```python class CustomClass: def instance_method(self): print(\\"Instance Method Called\\") def external_function(): print(\\"External Function Called\\") def bind_method_to_instance(instance, func): # Implementation here... def get_associated_function(method): # Implementation here... # Example demonstration custom_instance = CustomClass() custom_instance.instance_method() # Output: Instance Method Called # Dynamically bind external_function bind_method_to_instance(custom_instance, external_function) custom_instance.external_function() # Output: External Function Called # Retrieve associated functions func1 = get_associated_function(custom_instance.instance_method) func2 = get_associated_function(custom_instance.external_function) print(func1) # Output: <function CustomClass.instance_method at ...> print(func2) # Output: <function external_function at ...> ```","solution":"class CustomClass: def instance_method(self): print(\\"Instance Method Called\\") def external_function(): print(\\"External Function Called\\") def bind_method_to_instance(instance, func): Binds the given callable as a new method to the provided instance. if callable(func): # Use MethodType to bind the provided function to the instance. import types bound_method = types.MethodType(func, instance) # Add the bound method to the instance with a unique name. setattr(instance, func.__name__, bound_method) else: raise ValueError(\\"The provided func is not callable\\") def get_associated_function(method): Returns the actual function object associated with the method. if hasattr(method, \\"__func__\\"): return method.__func__ else: raise ValueError(\\"The provided method is not a method object\\") # Demonstration if __name__ == \\"__main__\\": # Create an instance of CustomClass custom_instance = CustomClass() # Call instance_method custom_instance.instance_method() # Output: Instance Method Called # Bind external_function to the instance bind_method_to_instance(custom_instance, external_function) custom_instance.external_function() # Output: External Function Called # Retrieve and display the function objects func1 = get_associated_function(custom_instance.instance_method) func2 = get_associated_function(custom_instance.external_function) print(func1) # Output: <function CustomClass.instance_method at ...> print(func2) # Output: <function external_function at ...>"},{"question":"Objective: Write a Python function that analyzes a given list of strings and classifies each string as a keyword, a soft keyword, or neither. The function should return a dictionary with three keys: \\"keywords\\", \\"softkeywords\\", and \\"neither\\", each mapping to a list of respective strings. Detailed Requirements: 1. Implement a function named `classify_keywords` that takes a list of strings as input. 2. The function should use `keyword.iskeyword` to determine if a string is a keyword. 3. The function should use `keyword.issoftkeyword` to determine if a string is a soft keyword. 4. Strings that are neither keywords nor soft keywords should be classified under \\"neither\\". 5. The function should return a dictionary with three keys: \\"keywords\\", \\"softkeywords\\", and \\"neither\\". Each key should map to a list containing the appropriate strings from the input list. Input: - A list of strings, e.g., `[\\"def\\", \\"import\\", \\"match\\", \\"my_var\\", \\"lambda\\"]` Output: - A dictionary with three keys: \\"keywords\\", \\"softkeywords\\", and \\"neither\\". Each key maps to a list of strings. Example: ```python def classify_keywords(strings): # Your code here # Example Usage: strings = [\\"def\\", \\"import\\", \\"match\\", \\"my_var\\", \\"lambda\\", \\"async\\"] result = classify_keywords(strings) print(result) ``` Expected output: ```python { \\"keywords\\": [\\"def\\", \\"import\\", \\"lambda\\", \\"async\\"], \\"softkeywords\\": [\\"match\\"], \\"neither\\": [\\"my_var\\"] } ``` Constraints: - The input list can contain up to 1000 strings. - Each string can be up to 100 characters long. Performance: - The solution should efficiently handle the checking of keywords and soft keywords.","solution":"import keyword def classify_keywords(strings): Classifies each string in the provided list as a keyword, soft keyword, or neither. Parameters: strings (list): A list of strings to be classified. Returns: dict: A dictionary with keys \\"keywords\\", \\"softkeywords\\", and \\"neither\\", mapping to lists of strings classified accordingly. result = {\\"keywords\\": [], \\"softkeywords\\": [], \\"neither\\": []} for s in strings: if keyword.iskeyword(s): result[\\"keywords\\"].append(s) elif keyword.issoftkeyword(s): result[\\"softkeywords\\"].append(s) else: result[\\"neither\\"].append(s) return result"},{"question":"**Advanced Python: Implementing Custom Sequence and Iterator Protocols** # Objective You need to implement a custom sequence type in Python that behaves like a basic list but comes with additional capabilities. The custom sequence should support iteration, indexing, and common list operations (addition, multiplication with an integer, length, containment check). # Instructions 1. **Class Definition**: - Define a class `CustomSequence` that initializes with a list of elements. 2. **Sequence Protocol Implementation**: - Implement the `__getitem__` and `__len__` methods to support indexing and `len()` function. - Implement the `__contains__` method to support the `in` keyword. - Implement the `__add__` and `__mul__` methods to support list addition and multiplication. 3. **Iterator Protocol Implementation**: - Implement an iterator for your `CustomSequence` by defining the `__iter__` and `__next__` methods. # Example Usage ```python cs = CustomSequence([1, 2, 3, 4]) # Testing __len__ and __getitem__ print(len(cs)) # Output: 4 print(cs[2]) # Output: 3 # Testing __contains__ print(3 in cs) # Output: True print(5 in cs) # Output: False # Testing __add__ cs2 = CustomSequence([5, 6]) cs3 = cs + cs2 print(cs3) # Output: CustomSequence([1, 2, 3, 4, 5, 6]) # Testing __mul__ cs4 = cs * 2 print(cs4) # Output: CustomSequence([1, 2, 3, 4, 1, 2, 3, 4]) # Testing iteration for item in cs: print(item) # Output: 1 2 3 4 ``` # Constraints - You can assume that the elements in the list are of comparable types (int, float, etc). - Ensure that the `TypeError` is raised for unsupported operations when necessary. # Performance Requirements - Your implementation should aim for O(n) complexity for addition and multiplication operations. **Note**: This question requires you to understand and implement protocols related to sequences and iterations, leveraging Pythonâ€™s dunder methods (`__methods__`). Your implementation will be evaluated based on correctness, adherence to the protocols, and performance.","solution":"class CustomSequence: def __init__(self, elements): self.elements = elements def __getitem__(self, index): return self.elements[index] def __len__(self): return len(self.elements) def __contains__(self, item): return item in self.elements def __add__(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Can only add CustomSequence to CustomSequence\\") return CustomSequence(self.elements + other.elements) def __mul__(self, number): if not isinstance(number, int): raise TypeError(\\"Can only multiply CustomSequence by an integer\\") return CustomSequence(self.elements * number) def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.elements): result = self.elements[self._index] self._index += 1 return result else: raise StopIteration def __repr__(self): return f\\"CustomSequence({self.elements})\\""},{"question":"# Question: Implement a Function Inspector You are required to implement a function `inspect_function_details(func)` that takes a Python function object `func` as input and returns a detailed dictionary containing the following information about the function: 1. **Name** of the function. 2. **Docstring** of the function. 3. **Source code** of the function. 4. **Signature** of the function, including parameter names, their kinds, default values, and annotations. 5. **Global variables** used within the function. 6. **Nonlocal variables** used within the function. # Input: - `func`: A Python function object. # Output: - A dictionary of the form: ```python { \\"name\\": str, \\"doc\\": str or None, \\"source\\": str, \\"signature\\": { \\"parameters\\": [ { \\"name\\": str, \\"kind\\": str, \\"default\\": any, \\"annotation\\": any }, ... ], \\"return_annotation\\": any }, \\"globals\\": dict, \\"nonlocals\\": dict, } ``` # Example: Suppose you have the following function: ```python def example_function(a, b: int, c=5, *args, d, e=10, **kwargs) -> bool: This is an example function. global_var = globals().get(\'example_global\') nonlocal_var = 5 return a + b + c + len(args) + d + e + len(kwargs) > 10 ``` Calling `inspect_function_details(example_function)` should return a dictionary with the detailed information of `example_function`. # Constraints: 1. You can assume `func` is always a valid Python function object. 2. The function should handle cases where certain information might not be available. # Hints: - Use `inspect.getsource()` to get the source code of the function. - Use `inspect.signature()` to get the signature of the function. - Use `inspect.getclosurevars()` to get the nonlocal and global variables. You can use the below template to get started: ```python import inspect def inspect_function_details(func): # Initialize the result dictionary result = { \\"name\\": func.__name__, \\"doc\\": inspect.getdoc(func), \\"source\\": inspect.getsource(func), \\"signature\\": {}, \\"globals\\": {}, \\"nonlocals\\": {} } # Get the signature of the function sig = inspect.signature(func) # Prepare the signature details signature_details = {\\"parameters\\": [], \\"return_annotation\\": sig.return_annotation} for name, param in sig.parameters.items(): param_info = { \\"name\\": name, \\"kind\\": str(param.kind), \\"default\\": param.default if param.default is not inspect.Parameter.empty else None, \\"annotation\\": param.annotation if param.annotation is not inspect.Parameter.empty else None } signature_details[\\"parameters\\"].append(param_info) result[\\"signature\\"] = signature_details # Get closure variables closure_vars = inspect.getclosurevars(func) result[\\"globals\\"] = closure_vars.globals result[\\"nonlocals\\"] = closure_vars.nonlocals return result ``` Make sure to test the function with different cases to ensure its correctness.","solution":"import inspect def inspect_function_details(func): # Initialize the result dictionary result = { \\"name\\": func.__name__, \\"doc\\": inspect.getdoc(func), \\"source\\": inspect.getsource(func), \\"signature\\": {}, \\"globals\\": {}, \\"nonlocals\\": {} } # Get the signature of the function sig = inspect.signature(func) # Prepare the signature details signature_details = {\\"parameters\\": [], \\"return_annotation\\": sig.return_annotation} for name, param in sig.parameters.items(): param_info = { \\"name\\": name, \\"kind\\": str(param.kind), \\"default\\": param.default if param.default is not inspect.Parameter.empty else None, \\"annotation\\": param.annotation if param.annotation is not inspect.Parameter.empty else None } signature_details[\\"parameters\\"].append(param_info) result[\\"signature\\"] = signature_details # Get closure variables closure_vars = inspect.getclosurevars(func) result[\\"globals\\"] = closure_vars.globals result[\\"nonlocals\\"] = closure_vars.nonlocals return result"},{"question":"# Internationalization Coding Assessment Objective: You are to implement a basic framework for an internationalized application using Python\'s `gettext` module. The framework should support the ability to translate messages into different languages dynamically. Task: Your task is to create a class `Translator` that facilitates language translation using `gettext`. Requirements: 1. **Class Initialization:** - **Parameters:** - `locales_path`: A string representing the path where locale directories are stored. - `domain`: A string representing the domain for the gettext translation. - **Behavior:** - Initialize the translation setup with given locales path and domain. 2. **Methods:** - `get_available_languages() -> List[str]`: - Returns a list of available languages based on the locale directories. - `set_language(language: str) -> None`: - Sets the current language for translations. - `translate(message: str) -> str`: - Translates the given message into the currently set language. Example: Assuming the directory structure for locales is as follows: ``` locales/ en_US/ LC_MESSAGES/ messages.mo es_ES/ LC_MESSAGES/ messages.mo ``` And the domain is `messages`. Input and Output: - The `locales_path` is `\\"locales\\"`. - The `domain` is `\\"messages\\"`. - The `set_language` method can be called with `\\"en_US\\"` for English or `\\"es_ES\\"` for Spanish. - The `translate` method will return the translated message based on the set language. Performance Requirements: - Efficient handling of language switching. - Proper loading and caching of translation files to minimize overhead. Constraints: - The `locales_path` must be a valid directory path. - The `domain` must correspond to existing `.mo` files in the `LC_MESSAGES` subdirectory of each locale. Example Usage: ```python translator = Translator(locales_path=\\"locales\\", domain=\\"messages\\") print(translator.get_available_languages()) # Output: [\'en_US\', \'es_ES\'] translator.set_language(\\"es_ES\\") print(translator.translate(\\"Hello, world!\\")) # Output: \\"Â¡Hola, mundo!\\" (Assuming the translation exists in messages.mo) translator.set_language(\\"en_US\\") print(translator.translate(\\"Hello, world!\\")) # Output: \\"Hello, world!\\" ``` Note: You\'ll need to handle scenarios where a translation might not be available gracefully.","solution":"import os import gettext class Translator: def __init__(self, locales_path: str, domain: str): self.locales_path = locales_path self.domain = domain self.current_translation = None def get_available_languages(self): # List all directory names in locales_path, assuming those are the locales try: return [locale for locale in os.listdir(self.locales_path) if os.path.isdir(os.path.join(self.locales_path, locale))] except FileNotFoundError: return [] def set_language(self, language: str): locale_path = os.path.join(self.locales_path, language, \'LC_MESSAGES\') self.current_translation = gettext.translation(self.domain, localedir=self.locales_path, languages=[language], fallback=True) self.current_translation.install() def translate(self, message: str) -> str: # Return the translated message or the original if translation fails if self.current_translation: return self.current_translation.gettext(message) return message"},{"question":"# CGI Script Handling in Python Objective In this task, you will create a simple CGI script using Python to handle form data submitted through an HTML form. The goal is to demonstrate your understanding of form handling, data parsing, and implementing safe practices in CGI scripting using the `cgi` module. Problem Statement You are required to write a Python CGI script that: 1. Processes form data submitted through an HTML form. 2. Validates the presence of specific required fields (`name` and `email`). 3. Handles multi-valued fields (for example, an `interests` checkbox group). 4. Safely integrates external commands if necessary, ensuring no arbitrary command injection. Requirements - Use the `FieldStorage` class from the `cgi` module to read form data. - Verify that required fields (`name` and `email`) are present and non-empty. - Handle cases where the user might provide multiple values for a field (such as `interests`). - Implement error handling for missing fields, displaying appropriate error messages. - Ensure any external command execution (if used) is performed securely. Input Format The form data is submitted as an HTTP POST request with required fields: - `name`: The name of the user. - `email`: The email address of the user. - `interests`: Multiple checkboxes indicating the interests of the user. Output Format Your script should produce output in HTML format, containing: 1. Confirmation message if all required fields are present. 2. An error message if some required fields are missing. 3. A list of user-provided interests if present. Constraints 1. Use Python 3.10 or later. 2. Ensure proper security practices, especially when dealing with user-submitted data. Example ```html <!-- HTML Form Example --> <form action=\\"/cgi-bin/your_script.py\\" method=\\"post\\"> Name: <input type=\\"text\\" name=\\"name\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Interests:<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Sports\\"> Sports<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Music\\"> Music<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Reading\\"> Reading<br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` ```python #!/usr/bin/env python3 import cgi import cgitb # Enable debugging cgitb.enable() # Main function to handle the form submission def main(): # Create instance of FieldStorage form = cgi.FieldStorage() # Print the HTTP headers print(\\"Content-Type: text/html\\") print() # Validate required fields if \\"name\\" not in form or \\"email\\" not in form: print(\\"<H1>Error</H1>\\") print(\\"Name and email fields are required.\\") return name = form.getvalue(\\"name\\") email = form.getvalue(\\"email\\") # Print confirmation print(\\"<H1>Form Submission Successful</H1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Email: {email}</p>\\") # Handle multi-valued field \'interests\' if \\"interests\\" in form: interests = form.getlist(\\"interests\\") print(\\"<p>Interests:</p>\\") print(\\"<ul>\\") for interest in interests: print(f\\"<li>{interest}</li>\\") print(\\"</ul>\\") else: print(\\"<p>No interests selected.</p>\\") if __name__ == \\"__main__\\": main() ``` Explain your implementation, focusing on how you validated the form data, handled multiple values, and ensured security while processing the inputs.","solution":"#!/usr/bin/env python3 import cgi import cgitb # Enable debugging cgitb.enable() # Main function to handle the form submission def main(): # Create instance of FieldStorage form = cgi.FieldStorage() # Print the HTTP headers print(\\"Content-Type: text/html\\") print() # Validate required fields if \\"name\\" not in form or \\"email\\" not in form: print(\\"<h1>Error</h1>\\") print(\\"<p>Name and email fields are required.</p>\\") return name = form.getvalue(\\"name\\") email = form.getvalue(\\"email\\") # Print confirmation print(\\"<h1>Form Submission Successful</h1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Email: {email}</p>\\") # Handle multi-valued field \'interests\' if \\"interests\\" in form: interests = form.getlist(\\"interests\\") print(\\"<p>Interests:</p>\\") print(\\"<ul>\\") for interest in interests: print(f\\"<li>{interest}</li>\\") print(\\"</ul>\\") else: print(\\"<p>No interests selected.</p>\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implement and Compare Linear Regression, Ridge Regression, and Lasso Using scikit-learn In this exercise, you will implement and compare the performance of Ordinary Least Squares (Linear Regression), Ridge Regression, and Lasso Regression using the scikit-learn library. Follow the steps to complete the task: Task Requirements: 1. **Data Preparation**: - Generate a synthetic dataset with `make_regression` from `sklearn.datasets` having 1000 samples, 20 features, and noise=0.1. 2. **Model Implementation**: - Implement Linear Regression using `LinearRegression`. - Implement Ridge Regression using `Ridge` with a given `alpha` parameter. - Implement Lasso Regression using `Lasso` with a given `alpha` parameter. 3. **Evaluation**: - Split the dataset into training and testing sets (80% train, 20% test). - Train each of the models on the training set. - Make predictions on the testing set. - Evaluate the models using Mean Squared Error (MSE) and R-squared (R2) metrics. 4. **Comparison**: - Output the MSE and R2 for each model. - Plot the actual vs predicted values for each model. Input: - `alpha_ridge`: A float value for the regularization strength of Ridge Regression. - `alpha_lasso`: A float value for the regularization strength of Lasso Regression. Expected Output: - MSE and R2 scores for Linear Regression, Ridge Regression, and Lasso Regression on the test set. - A combined plot showing the actual vs predicted values for each model. Constraints: - Use random state = 42 for reproducibility. - Ensure the solutions are efficient and optimized for performance. Example: ```python from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt # Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models lin_reg = LinearRegression() ridge_reg = Ridge(alpha=1.0) # Replace 1.0 with alpha_ridge input lasso_reg = Lasso(alpha=0.1) # Replace 0.1 with alpha_lasso input # Train models lin_reg.fit(X_train, y_train) ridge_reg.fit(X_train, y_train) lasso_reg.fit(X_train, y_train) # Make predictions y_pred_lin = lin_reg.predict(X_test) y_pred_ridge = ridge_reg.predict(X_test) y_pred_lasso = lasso_reg.predict(X_test) # Evaluate models mse_lin = mean_squared_error(y_test, y_pred_lin) r2_lin = r2_score(y_test, y_pred_lin) mse_ridge = mean_squared_error(y_test, y_pred_ridge) r2_ridge = r2_score(y_test, y_pred_ridge) mse_lasso = mean_squared_error(y_test, y_pred_lasso) r2_lasso = r2_score(y_test, y_pred_lasso) # Print scores print(f\'Linear Regression - MSE: {mse_lin}, R2: {r2_lin}\') print(f\'Ridge Regression - MSE: {mse_ridge}, R2: {r2_ridge}\') print(f\'Lasso Regression - MSE: {mse_lasso}, R2: {r2_lasso}\') # Plot actual vs predicted plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.scatter(y_test, y_pred_lin, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Linear Regression\') plt.subplot(1, 3, 2) plt.scatter(y_test, y_pred_ridge, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Ridge Regression\') plt.subplot(1, 3, 3) plt.scatter(y_test, y_pred_lasso, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Lasso Regression\') plt.tight_layout() plt.show() ``` **Note**: Ensure the `alpha` values for Ridge and Lasso are parameters that can be input while running the script.","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt def implement_and_compare_regressions(alpha_ridge, alpha_lasso): Implements and compares Linear Regression, Ridge Regression, and Lasso Regression. Parameters: alpha_ridge (float): Regularization strength for Ridge Regression. alpha_lasso (float): Regularization strength for Lasso Regression. Returns: dict: Dictionary containing MSE and R2 scores for each model. # Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models lin_reg = LinearRegression() ridge_reg = Ridge(alpha=alpha_ridge) lasso_reg = Lasso(alpha=alpha_lasso) # Train models lin_reg.fit(X_train, y_train) ridge_reg.fit(X_train, y_train) lasso_reg.fit(X_train, y_train) # Make predictions y_pred_lin = lin_reg.predict(X_test) y_pred_ridge = ridge_reg.predict(X_test) y_pred_lasso = lasso_reg.predict(X_test) # Evaluate models mse_lin = mean_squared_error(y_test, y_pred_lin) r2_lin = r2_score(y_test, y_pred_lin) mse_ridge = mean_squared_error(y_test, y_pred_ridge) r2_ridge = r2_score(y_test, y_pred_ridge) mse_lasso = mean_squared_error(y_test, y_pred_lasso) r2_lasso = r2_score(y_test, y_pred_lasso) # Plot actual vs predicted plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.scatter(y_test, y_pred_lin, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Linear Regression\') plt.subplot(1, 3, 2) plt.scatter(y_test, y_pred_ridge, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Ridge Regression\') plt.subplot(1, 3, 3) plt.scatter(y_test, y_pred_lasso, edgecolors=(0, 0, 0)) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Lasso Regression\') plt.tight_layout() plt.show() return { \\"Linear Regression\\": {\\"MSE\\": mse_lin, \\"R2\\": r2_lin}, \\"Ridge Regression\\": {\\"MSE\\": mse_ridge, \\"R2\\": r2_ridge}, \\"Lasso Regression\\": {\\"MSE\\": mse_lasso, \\"R2\\": r2_lasso} }"},{"question":"Context Management with `contextvars` **Objective:** In this task, you will create a set of functions to manage context-local state using the `contextvars` module. Specifically, you will create functions that: 1. Initialize context variables with default values. 2. Modify the values of context variables. 3. Reset context variables to their previous values. 4. Run a function within a specific context to illustrate context isolation. Your solution must demonstrate an understanding of the `contextvars` module\'s concepts and effective use of context variables in an asynchronous framework. **Task Description:** 1. Create a context variable named `user_id` with a default value of `None`. 2. Implement a function `set_user_id(new_id)` that takes a new user ID and sets it as the value of the `user_id` context variable. This function should return the token returned by the `set()` method. 3. Implement a function `reset_user_id(token)` that takes a token and resets the `user_id` context variable to the value it had before the corresponding `set()` call using the provided token. 4. Implement a function `run_in_custom_context(new_id, func, *args, **kwargs)` that: - Sets the `user_id` context variable to `new_id`. - Runs the provided function `func` with the specified arguments and keyword arguments in this custom context. - Returns the result of the function execution. **Input and Output Format:** - `set_user_id(new_id: int) -> contextvars.Token` - Input: Integer `new_id`. - Output: A `contextvars.Token` object representing the state before the `set()` call. - `reset_user_id(token: contextvars.Token) -> None` - Input: A `contextvars.Token` object. - Output: None. - `run_in_custom_context(new_id: int, func: Callable, *args, **kwargs) -> Any` - Input: Integer `new_id`, a callable `func`, and the callable\'s arguments and keyword arguments. - Output: The result of executing `func(*args, **kwargs)` within the custom context. **Constraints:** - Ensure thread safety and proper context management in concurrent/asynchronous scenarios. - Handle exceptions and edge cases appropriately. **Example Usage:** ```python import contextvars from typing import Any, Callable # Step 1: Initialize context variable user_id = contextvars.ContextVar(\'user_id\', default=None) def set_user_id(new_id: int) -> contextvars.Token: # Step 2: Implement the function to set user_id token = user_id.set(new_id) return token def reset_user_id(token: contextvars.Token) -> None: # Step 3: Implement the function to reset user_id user_id.reset(token) def run_in_custom_context(new_id: int, func: Callable, *args, **kwargs) -> Any: # Step 4: Implement the function to run in custom context token = set_user_id(new_id) try: result = func(*args, **kwargs) return result finally: reset_user_id(token) # Example function to test context: def get_current_user_id(): return user_id.get() # Running tests: def main(): print(run_in_custom_context(101, get_current_user_id)) # Output: 101 print(user_id.get()) # Output: None (since the context is reset) if __name__ == \\"__main__\\": main() ``` Ensure your implementation handles the specified functions correctly and adheres to context management principles using the `contextvars` module.","solution":"import contextvars from typing import Any, Callable # Step 1: Initialize context variable user_id = contextvars.ContextVar(\'user_id\', default=None) def set_user_id(new_id: int) -> contextvars.Token: Sets the user_id context variable to new_id and returns the token representing the context state before the call. token = user_id.set(new_id) return token def reset_user_id(token: contextvars.Token) -> None: Resets the user_id context variable to its state before the set call that returned the token. user_id.reset(token) def run_in_custom_context(new_id: int, func: Callable, *args, **kwargs) -> Any: Sets the user_id context variable to new_id, runs the provided function with the specified arguments and returns the result. Resets the context after execution. token = set_user_id(new_id) try: result = func(*args, **kwargs) return result finally: reset_user_id(token)"},{"question":"# PyTorch Numerical Properties Assessment You are tasked with implementing a function in PyTorch that will validate and summarize the numerical properties of a given tensor based on its data type. Function Signature ```python import torch def summary_numerical_properties(tensor: torch.Tensor) -> dict: Summarize the numerical properties of the given tensor\'s dtype. Args: tensor (torch.Tensor): A tensor for which to summarize numerical properties. Returns: dict: A dictionary containing the summarized properties of the tensor\'s dtype. ``` Guidelines 1. Your function should determine the data type of the provided tensor. 2. Depending on whether the tensor is of a floating point or integer data type, use either `torch.finfo` or `torch.iinfo` to gather the numerical properties. 3. Return a dictionary containing the following keys: - For floating point tensor types (`torch.float32`, `torch.float64`, `torch.float16`, and `torch.bfloat16`): ```python { \\"bits\\": int, \\"eps\\": float, \\"max\\": float, \\"min\\": float, \\"tiny\\": float, \\"resolution\\": float } ``` - For integer tensor types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, and `torch.int64`): ```python { \\"bits\\": int, \\"max\\": int, \\"min\\": int } ``` Constraints - You should handle both floating point and integer tensor types. - Raise an appropriate error if the tensor is not of a floating point or integer type. - Assume that the tensor provided is valid and non-empty. Example ```python tensor_fp32 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32) print(summary_numerical_properties(tensor_fp32)) # Expected output: # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-07 # } tensor_int16 = torch.tensor([1, 2, 3], dtype=torch.int16) print(summary_numerical_properties(tensor_int16)) # Expected output: # { # \'bits\': 16, # \'max\': 32767, # \'min\': -32768 # } ``` Put your complete function implementation below this comment in the provided function signature.","solution":"import torch def summary_numerical_properties(tensor: torch.Tensor) -> dict: Summarize the numerical properties of the given tensor\'s dtype. Args: tensor (torch.Tensor): A tensor for which to summarize numerical properties. Returns: dict: A dictionary containing the summarized properties of the tensor\'s dtype. dtype = tensor.dtype if dtype.is_floating_point: info = torch.finfo(dtype) return { \\"bits\\": info.bits, \\"eps\\": info.eps, \\"max\\": info.max, \\"min\\": info.min, \\"tiny\\": info.tiny, \\"resolution\\": info.eps, } elif dtype in (torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64): info = torch.iinfo(dtype) return { \\"bits\\": info.bits, \\"max\\": info.max, \\"min\\": info.min, } else: raise TypeError(f\\"Unsupported tensor dtype: {dtype}\\")"},{"question":"# Advanced Python Coding Assessment: System Interaction Objective Your task is to implement a Python function that manages command line arguments, interacts with auditing hooks, and effectively handles memory measurement. This will demonstrate your understanding of various `sys` module functionalities. Function Implementation **Function Signature:** ```python def manage_system(args: list) -> dict: ``` **Input:** - `args` (list): A list of command line arguments to be processed. Assume the first element (`args[0]`) is the script name. **Output:** - Returns a dictionary with the following keys and corresponding values: - `\'audited_events\'`: A list of audited events. - `\'object_size\'`: The size in bytes of a sample object. - `\'recursion_limit_before\'`: The recursion limit before setting a new high value. - `\'recursion_limit_after\'`: The recursion limit after setting a new high value. **Function Details:** 1. **Command Line Arguments Handling:** - Print all command line arguments received except for the script name. 2. **Auditing Hooks:** - Add an audit hook using `sys.addaudithook` that appends event details to a list named `\'audited_events\'`. - Trigger an example auditing event using `sys.audit`. 3. **Memory Measurement:** - Measure the size of a sample object (e.g., a large list) using `sys.getsizeof`. 4. **Recursion Limit:** - Retrieve the current recursion limit. - Set a new high recursion limit. - Retrieve the new recursion limit. 5. **Standard Input and Output:** - Ensure that any output from the function is directed to standard output (`sys.stdout`). **Constraints:** - Ensure your function handles any potential exceptions gracefully and logs meaningful error messages. Sample Usage: ```python if __name__ == \\"__main__\\": import sys sys.argv = [\\"script_name.py\\", \\"arg1\\", \\"arg2\\"] result = manage_system(sys.argv) print(result) ``` Expected Output: ```python { \'audited_events\': [\'example_event\'], \'object_size\': 1056, # The actual size may vary. \'recursion_limit_before\': 3000, \'recursion_limit_after\': 10000 } ``` Notes: - You may need to validate your implementation against edge cases and large inputs to ensure reliability and performance. - Use appropriate comments to explain the code logic and document any assumptions made. - Ensure your code adheres to PEP 8 styling guidelines for readability.","solution":"import sys def manage_system(args: list) -> dict: Manages command line arguments, interacts with auditing hooks, and effectively handles memory measurement. Args: - args (list): A list of command line arguments to be processed. Assume the first element (`args[0]`) is the script name. Returns: - dict: Contains \'audited_events\', \'object_size\', \'recursion_limit_before\', and \'recursion_limit_after\'. results = {} # Command Line Arguments Handling: print(\\"Command Line Arguments (excluding script name):\\") for arg in args[1:]: print(arg) # Auditing Hooks: audited_events = [] def audit(event, args): audited_events.append(event) sys.addaudithook(audit) # Trigger an example auditing event sys.audit(\'example_event\') results[\'audited_events\'] = audited_events # Memory Measurement: sample_object = list(range(10000)) # Example of a large list object_size = sys.getsizeof(sample_object) results[\'object_size\'] = object_size # Recursion Limit Handling: recursion_limit_before = sys.getrecursionlimit() recursion_limit_after = 10000 sys.setrecursionlimit(recursion_limit_after) recursion_limit_after = sys.getrecursionlimit() results[\'recursion_limit_before\'] = recursion_limit_before results[\'recursion_limit_after\'] = recursion_limit_after return results"},{"question":"**Objective**: In this task, you need to demonstrate your understanding of parsing argument lists, dynamically generating documentation, and handling input constraints in Python functions by implementing a pseudo Argument Clinic-like mechanism in Python. # Problem Description: You are required to implement a simplified version of Argument Clinic in Python. This will involve creating a function decoration that automatically processes function argument parsing and documentation. # Task: Create a decorator `@argument_clinic` that: 1. Parses the arguments passed to the decorated function based on the specified types. 2. Enforces constraints on the types of arguments provided. 3. Automatically generates a function signature and a basic docstring for the decorated function. # Specifications: 1. **Decorator Definition**: ```python def argument_clinic(param_types: dict): # Your implementation ``` 2. **Parameters**: - `param_types`: A dictionary where keys represent argument names, and values represent the expected type of arguments. 3. **Functionality**: - The decorator should enforce that the function\'s arguments are of the types specified in `param_types`. - If an argument does not match the specified type, raise a `TypeError` with a message `\\"Argument \'<arg_name>\' must be of type <expected_type>\\"`. - Automatically generate and attach a docstring to the function, displaying its signature and a brief description of each argument. 4. **Example Usage**: ```python @argument_clinic({ \'n\': int, \'name\': str, \'flag\': bool }) def example_function(n, name, flag): # function body pass # The decorated function should now enforce argument types and have a generated docstring example_function(3, \'example\', True) example_function(\'three\', \'example\', 1) # Should raise TypeError ``` 5. **Output**: - The function should execute normally if all arguments are of the expected types. - A raised `TypeError` if any argument does not match the expected type. - Automatically generated docstring should appear when `help(example_function)` is called. # Constraints: - Assume that only basic types will be used (`int`, `str`, `bool`, `float`). - Ensure the solution is clean and properly handles errors. # Implementation Details: Below is the skeleton code to get you started: ```python def argument_clinic(param_types: dict): def decorator(f): def wrapped_func(*args, **kwargs): # Check argument types for i, (arg_name, arg_type) in enumerate(param_types.items()): if not isinstance(args[i], arg_type): raise TypeError(f\\"Argument \'{arg_name}\' must be of type {arg_type.__name__}\\") return f(*args, **kwargs) # Generate the docstring (example format) doc_lines = [\\"{}({})\\".format( f.__name__, \\", \\".join(f\\"{arg}: {type_.__name__}\\" for arg, type_ in param_types.items()) )] doc_lines.append(f.__doc__ or \\"\\") doc_lines.extend(f\\" - {arg}: {type_.__name__}\\" for arg, type_ in param_types.items()) wrapped_func.__doc__ = \\"n\\".join(doc_lines) return wrapped_func return decorator # Example Usage @argument_clinic({ \'n\': int, \'name\': str, \'flag\': bool, }) def example_function(n, name, flag): Function that demonstrates example usage of argument_clinic pass # Run below code to see effect # help(example_function) ``` # Testing your solution: 1. Test normal execution by calling `example_function` with valid arguments. 2. Test type enforcement by calling `example_function` with invalid argument types. 3. Check the automatically generated docstring using `help(example_function)`.","solution":"def argument_clinic(param_types: dict): def decorator(f): def wrapped_func(*args, **kwargs): # Check positional arguments for i, (arg_name, arg_type) in enumerate(param_types.items()): if i < len(args): if not isinstance(args[i], arg_type): raise TypeError(f\\"Argument \'{arg_name}\' must be of type {arg_type.__name__}\\") # Check keyword arguments for arg_name, arg_value in kwargs.items(): if arg_name in param_types: if not isinstance(arg_value, param_types[arg_name]): raise TypeError(f\\"Argument \'{arg_name}\' must be of type {param_types[arg_name].__name__}\\") return f(*args, **kwargs) # Generate the docstring doc_lines = [ f\\"{f.__name__}({\', \'.join(f\'{arg}: {param_types[arg].__name__}\' for arg in param_types)})\\" ] doc_lines.append(f.__doc__ or \\"\\") doc_lines.extend(f\\" - {arg}: {type_.__name__}\\" for arg, type_ in param_types.items()) wrapped_func.__doc__ = \\"n\\".join(doc_lines) return wrapped_func return decorator # Example usage @argument_clinic({ \'n\': int, \'name\': str, \'flag\': bool, }) def example_function(n, name, flag): Function that demonstrates example usage of argument_clinic return f\\"n: {n}, name: {name}, flag: {flag}\\" # Example function to test the decorator @argument_clinic({ \'a\': int, \'b\': float, \'c\': str, }) def another_example(a, b, c): Another example function. return f\\"a: {a}, b: {b}, c: {c}\\""},{"question":"on PyTorch Meta Device **Objective:** Write a Python function using PyTorch to demonstrate your understanding of the \\"meta\\" device by: 1. Loading a dummy model on the meta device. 2. Applying a transformation to the model while itâ€™s on the meta device. 3. Reinitializing the model parameters when moving back to the CPU device. **Problem Statement:** Implement the function `transform_meta_model(file_path: str) -> torch.nn.Module` which performs the following steps: 1. Loads a Linear model saved at the file path provided (`file_path`) onto the meta device. 2. Transforms the model by changing the number of input features and output features of the Linear layer. 3. Moves the model back to the CPU device with uninitialized parameters. 4. Re-initializes the parameters with random values. **Function Specification:** - **Input:** - `file_path` (str): The file path to load the model from, saved using `torch.save`. - **Output:** - A `torch.nn.Module` object: The transformed linear model with reinitialized parameters on the CPU device. - **Constraints:** - Use the `torch.device(\'meta\')` context manager for transforming the model on the meta device. - Only a Linear model with defined `in_features` and `out_features` will be used. - Do not perform any I/O operations apart from loading the model and transforming it. **Example:** ```python import torch from torch import nn def transform_meta_model(file_path: str) -> nn.Module: # Load model on meta device with torch.device(\'meta\'): model = torch.load(file_path, map_location=\'meta\') # Transform the model on meta device by changing input and output features new_in_features = model.in_features * 2 new_out_features = model.out_features * 2 with torch.device(\'meta\'): transformed_model = nn.Linear(new_in_features, new_out_features) # Move the transformed model to CPU device with uninitialized parameters transformed_model = transformed_model.to_empty(device=\'cpu\') # Re-initialize the parameters with random values nn.init.normal_(transformed_model.weight) if transformed_model.bias is not None: nn.init.normal_(transformed_model.bias) return transformed_model # Assuming `dummy_model.pt` is a saved linear model file # transformed_model = transform_meta_model(\'dummy_model.pt\') # print(transformed_model) ``` In this example, suppose you have a linear model saved in `dummy_model.pt` with `in_features=20` and `out_features=30`. The function will transform it to have `in_features=40` and `out_features=60`, and reinitialize the parameters with random values on the CPU. **Performance Requirements:** - Ensure that the context managers and transformations are properly applied to keep operations efficient. - The reinitialization of parameters should not involve computationally expensive operations. Use efficient PyTorch initializers.","solution":"import torch from torch import nn def transform_meta_model(file_path: str) -> nn.Module: # Load model on meta device with torch.device(\'meta\'): model = torch.load(file_path, map_location=\'meta\') # Transform the model on meta device by changing input and output features new_in_features = model.in_features * 2 new_out_features = model.out_features * 2 with torch.device(\'meta\'): transformed_model = nn.Linear(new_in_features, new_out_features) # Move the transformed model to CPU device with uninitialized parameters transformed_model = transformed_model.to_empty(device=\'cpu\') # Re-initialize the parameters with random values nn.init.normal_(transformed_model.weight) if transformed_model.bias is not None: nn.init.normal_(transformed_model.bias) return transformed_model"},{"question":"# Question: Composite Visualization with Seaborn and Matplotlib Using the Seaborn library, create a composite plot that includes multiple sub-plots arranged horizontally within a single figure. **Specifications:** 1. **Data:** Use the `diamonds` dataset from Seaborn. 2. **Sub-Plot 1:** - Create a scatter plot displaying the relationship between `carat` and `price`, using Seaborn\'s `Dots`. - Add a rectangular annotation on the plot with a custom text label saying \\"Scatter Plot of Diamonds\\". 3. **Sub-Plot 2:** - Create a series of histograms that display the price distribution, faceted by the `cut` attribute, with a logarithmic x-axis scale. Use Seaborn\'s `Bars` and `Hist`. - Ensure that the y-axes are not shared. **Implementation Details:** - Arrange the plots horizontally using subfigures. - Customize plot themes if needed. - Matplotlib\'s `Figure` and subfigure functionalities should be used to arrange the plots. # Input: No explicit input is needed. The dataset is to be used directly within the script. # Output: A single figure containing the two described sub-plots. # Code Template: ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the first Plot object scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create the second Plot object histograms = ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Create a matplotlib Figure fig = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") # Add Subfigures sf1, sf2 = fig.subfigures(1, 2) # Apply the Seaborn plots to the subfigures scatter_plot.on(sf1).plot() # Add custom rectangle and text annotation on first subfigure ax = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False, ) ax.add_artist(rect) ax.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Scatter Plot of Diamonds\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, ) # Apply the histogram plot to the second subfigure histograms.on(sf2) # Display the figure plt.show() ``` # Constraints: - Ensure the use of Matplotlib and Seaborn functions as indicated. - The figure must use subfigures for arrangement. - Provide the plot in a display format suitable for visual inspection. **Performance:** - Efficient handling of the dataset is expected. Ensure minimal overhead in plot rendering.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_composite_plot(): Creates and displays a composite plot with two sub-plots: 1. Scatter plot of carat vs price with annotation. 2. Series of histograms faceted by cut with a logarithmic x-axis. # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the first Plot object scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create the second Plot object histograms = ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Create a matplotlib Figure fig = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") # Add Subfigures sf1, sf2 = fig.subfigures(1, 2) # Apply the Seaborn plots to the subfigures scatter_plot.on(sf1).plot() # Add custom rectangle and text annotation on first subfigure ax = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False, ) ax.add_artist(rect) ax.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Scatter Plot of Diamonds\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, ) # Apply the histogram plot to the second subfigure histograms.on(sf2) # Display the figure plt.show()"},{"question":"**Objective:** Assess students\' understanding of the `pwd` module and their ability to interact with Unix user account information. --- Problem Statement You are tasked with creating a function that utilizes the `pwd` module to retrieve and format information about a Unix user. Your function should: 1. Accept a string `username` as input. 2. Retrieve the password database entry for the given `username`. 3. Format the retrieved information into a dictionary with the following keys: - `\\"Login Name\\"`: Corresponding to the `pw_name` field. - `\\"User ID\\"`: Corresponding to the `pw_uid` field. - `\\"Group ID\\"`: Corresponding to the `pw_gid` field. - `\\"Home Directory\\"`: Corresponding to the `pw_dir` field. - `\\"Shell\\"`: Corresponding to the `pw_shell` field. 4. Return the formatted dictionary. If the `username` is not found in the password database, your function should return an empty dictionary. **Function Signature:** ```python def get_user_info(username: str) -> dict: pass ``` **Input:** - `username` (String): The username of the Unix user. E.g., `\\"root\\"` **Output:** - (Dict): A dictionary containing the formatted user information as described. If the user is not found, return an empty dictionary. **Constraints:** - Do not make any assumptions about the usernames or their presence in the password database. - Your solution should handle cases where the `username` is not found gracefully. **Examples:** 1. `get_user_info(\\"root\\")` ```python { \\"Login Name\\": \\"root\\", \\"User ID\\": 0, \\"Group ID\\": 0, \\"Home Directory\\": \\"/root\\", \\"Shell\\": \\"/bin/bash\\" } ``` 2. `get_user_info(\\"nonexistentuser\\")` ```python {} ``` *Note: The exact output might vary based on the system\'s password database configuration.* --- Good luck and write clean, efficient, and well-documented code.","solution":"import pwd def get_user_info(username: str) -> dict: Retrieve and format information about a Unix user based on the username. Args: username (str): The username of the Unix user. Returns: dict: A dictionary containing user information with the keys: \\"Login Name\\", \\"User ID\\", \\"Group ID\\", \\"Home Directory\\", and \\"Shell\\". If the user is not found, returns an empty dictionary. try: user_info = pwd.getpwnam(username) return { \\"Login Name\\": user_info.pw_name, \\"User ID\\": user_info.pw_uid, \\"Group ID\\": user_info.pw_gid, \\"Home Directory\\": user_info.pw_dir, \\"Shell\\": user_info.pw_shell } except KeyError: # Username not found return {}"},{"question":"Objective Write a Python function that automates the process of uploading a file to an FTP server, verifying its presence, and downloading it back to a local directory. The function should handle connections securely using `FTP_TLS`. Function Signature ```python def ftp_upload_download(host: str, user: str, passwd: str, upload_filepath: str, download_dir: str) -> str: pass ``` Input 1. `host` (str): The FTP server\'s hostname or IP address. 2. `user` (str): The username for logging into the FTP server. 3. `passwd` (str): The password for logging into the FTP server. 4. `upload_filepath` (str): The path to the local file that should be uploaded to the FTP server. 5. `download_dir` (str): The local directory where the file should be downloaded after verification. Output - The function will return the path to the downloaded file in the `download_dir` upon successful download and verification. - If any step fails (uploading, verifying, or downloading), the function should raise a relevant exception with a descriptive message. Constraints - The upload filepath and download directory are guaranteed to exist and be accessible. - The FTP server supports both secure and standard FTP operations. - The username and password provided will have the necessary permissions to upload and download files in the remote directory. Requirements 1. **Secure Connection:** Use `FTP_TLS` to establish a secure connection to the FTP server. 2. **Upload File:** Upload the file located at `upload_filepath` to the home directory of the FTP server. 3. **Verify Upload:** After uploading, verify the file\'s presence on the FTP server by listing the directory contents. 4. **Download File:** If the file is verified to be present, download it to `download_dir`. 5. **Exception Handling:** Properly handle any exceptions that arise during the connection, file transfer, or verification process, and raise a descriptive error if any step fails. 6. **Clean Code:** Ensure that all connections are properly closed, even if an error occurs. Example ```python # Assuming the following FTP server details and file paths host = \\"ftp.example.com\\" user = \\"testuser\\" passwd = \\"testpassword\\" upload_filepath = \\"/local/path/to/uploadfile.txt\\" download_dir = \\"/local/download/directory/\\" # Example function call downloaded_file_path = ftp_upload_download(host, user, passwd, upload_filepath, download_dir) # downloaded_file_path should be \\"/local/download/directory/uploadfile.txt\\" if successful ``` Notes - You can use the `os` module to join paths if needed. - Make sure to test your function thoroughly, as real FTP operations may have various edge cases and potential points of failure. - Consider setting the debugging level of FTP connections to help diagnose issues during development.","solution":"from ftplib import FTP_TLS import os def ftp_upload_download(host: str, user: str, passwd: str, upload_filepath: str, download_dir: str) -> str: try: # Establish a secure FTP connection ftps = FTP_TLS(host) ftps.login(user, passwd) ftps.prot_p() # Upload the file with open(upload_filepath, \'rb\') as upload_file: filename = os.path.basename(upload_filepath) ftps.storbinary(f\'STOR {filename}\', upload_file) # Verify the file is uploaded files = ftps.nlst() if filename not in files: raise FileNotFoundError(\\"File not found on FTP server after upload.\\") # Download the file download_filepath = os.path.join(download_dir, filename) with open(download_filepath, \'wb\') as download_file: ftps.retrbinary(f\'RETR {filename}\', download_file.write) # Close the connection ftps.quit() return download_filepath except Exception as e: if \'ftps\' in locals(): ftps.quit() raise e"},{"question":"# Question You are provided with a Python function that performs computations related to generating prime numbers and summing them. Your task is to profile this function using the `cProfile` module and optimize it based on the profiling results. Function ```python def compute_primes_and_sum(n): def is_prime(num): if num <= 1: return False for i in range(2, num): if num % i == 0: return False return True primes = [num for num in range(n) if is_prime(num)] primes_sum = sum(primes) return primes, primes_sum ``` # Tasks 1. **Profile the Function**: Using the `cProfile` module, profile the given `compute_primes_and_sum(n)` function for `n=10000`. Save the profiling results to a file named `profile_results`. 2. **Analyze the Results**: Load the profiling results using the `pstats` module and print the top 10 functions sorted by cumulative time. 3. **Optimize the Function**: Based on your analysis, identify and implement optimizations for the `compute_primes_and_sum(n)` function. Justify your optimizations. 4. **Re-profile the Optimized Function**: Profile the optimized function using the same parameters and save the results to a file named `optimized_profile_results`. 5. **Compare and Discuss**: Load and compare the profiling results of the original and optimized functions. Write a brief discussion comparing the performance improvements, if any, quoting specific metrics from your profiling data. # Input and Output Formats - **Input**: You only need to provide the profiling results file names (`profile_results` and `optimized_profile_results`) and the optimizations (code and justifications). - **Output**: Profiling files, optimized function, and a brief discussion comparing the performance. # Constraints - You should not remove the call to the profiling and must ensure it accurately reflects the performance of the provided function. - Ensure that all profiling outputs are saved to specified files. # Example Hereâ€™s an example of how you might begin the solution: ```python import cProfile import pstats def compute_primes_and_sum(n): def is_prime(num): if num <= 1: return False for i in range(2, num): if num % i == 0: return False return True primes = [num for num in range(n) if is_prime(num)] primes_sum = sum(primes) return primes, primes_sum # Step 1: Profile the Function cProfile.run(\'compute_primes_and_sum(10000)\', \'profile_results\') # Step 2: Analyze the Results p = pstats.Stats(\'profile_results\') p.sort_stats(\'cumulative\').print_stats(10) ``` Now continue with steps 3-5 to optimize, re-profile, and compare the results.","solution":"def compute_primes_and_sum(n): def is_prime(num): if num <= 1: return False for i in range(2, int(num ** 0.5) + 1): if num % i == 0: return False return True primes = [num for num in range(n) if is_prime(num)] primes_sum = sum(primes) return primes, primes_sum"},{"question":"Objective Design a function to process the contents of multiple text files by reversing the text of each line, and save the changes in place. Use the `fileinput` module functionality to ensure that changes are made directly to the files and not just printed to the console. Implement support for optional gzip-compressed files. Description Write a function `reverse_lines_in_files` that iterates over a list of files, reverses the content of each line, and writes the reversed lines back into the original files. Your solution should handle both regular text files and gzip-compressed files. Function Signature ```python def reverse_lines_in_files(file_paths: list[str], inplace: bool = True, backup_ext: str = \'.bak\', compressed: bool = False): pass ``` Parameters - **file_paths (list[str])**: A list of strings representing the paths to the input files. - **inplace (bool)**: A boolean flag that determines whether the files should be modified in place. Default is `True`. - **backup_ext (str)**: The string representing the extension for backup files, if in-place modification is enabled. Default is `.bak`. - **compressed (bool)**: A boolean flag that indicates whether the files are gzip-compressed. Default is `False`. Requirements 1. **File Handling**: Use the `fileinput` module\'s context manager and `input()` function to handle multiple files. 2. **Reversing Lines**: For every line in each file, reverse the content of the line (e.g., \'hello\' becomes \'olleh\'). 3. **In-place Modification**: Save the changes back to the original files, ensuring backup files are created with the specified extension. 4. **Compressed File Support**: If the `compressed` parameter is `True`, handle the input files using the `hook_compressed()` function provided by the `fileinput` module. 5. **Error Handling**: Implement appropriate error handling for file I/O operations. Example Usage ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt.gz\'] reverse_lines_in_files(file_paths, inplace=True, backup_ext=\'.bak\', compressed=False) ``` Constraints - Input file paths must be valid and accessible. - Files must be readable and writable by the user. - Handle both gzip-compressed and regular text files. - Implement error handling for scenarios such as files not being found or read/write errors. Performance Considerations - The solution should efficiently read and write lines without unnecessary memory overhead. - Ensure that handling large files or multiple files does not significantly degrade performance.","solution":"import fileinput import gzip def reverse_lines_in_files(file_paths: list[str], inplace: bool = True, backup_ext: str = \'.bak\', compressed: bool = False): Reverses the lines in the given files. :param file_paths: List of file paths to process. :param inplace: Whether to modify the files in place. :param backup_ext: Extension for backup files. :param compressed: Whether the files are gzip-compressed. if compressed: open_func = lambda path, mode: gzip.open(path, mode, encoding=\'utf-8\') else: open_func = lambda path, mode: open(path, mode, encoding=\'utf-8\') for file_path in file_paths: with open_func(file_path, \'rt\') as f: lines = f.readlines() reversed_lines = [line.rstrip()[::-1] + \'n\' for line in lines] if inplace: # Write backup file if needed backup_file_path = file_path + backup_ext with open_func(backup_file_path, \'wt\') as backup_file: backup_file.writelines(lines) # Write the new content to the original file with open_func(file_path, \'wt\') as original_file: original_file.writelines(reversed_lines) else: with open_func(file_path, \'wt\') as f: f.writelines(reversed_lines)"},{"question":"Objective You are asked to implement a Python function using the provided information about tuples and struct sequences. Specifically, you need to manage a collection of student records, where each student\'s data is represented as a struct sequence. Problem Statement Implement a function `create_student_struct(name, age, grades)` that does the following: 1. Defines a struct sequence type called `Student` with the fields: `name`, `age`, and `grades`. 2. Creates an instance of this struct sequence type for a specific student using the provided input parameters. 3. Returns the created struct sequence instance. Function Signature ```python def create_student_struct(name: str, age: int, grades: tuple) -> object: Creates a struct sequence for a student and returns it. ``` Input - `name` (str): The name of the student. - `age` (int): The age of the student. - `grades` (tuple): A tuple representing the grades of the student. Output - A `Student` struct sequence object with the fields `name`, `age`, and `grades` set accordingly. Constraints - `name` is a non-empty string with at most 100 characters. - `age` is a non-negative integer. - `grades` is a tuple of non-negative integers, each representing a grade. Example ```python student = create_student_struct(\'Alice\', 20, (85, 90, 95)) print(student.name) # Output: Alice print(student.age) # Output: 20 print(student.grades) # Output: (85, 90, 95) ``` Hints - You can use the `PyStructSequence_*` functions and types as described in the provided documentation to create and manipulate struct sequences. - Ensure that you properly define the struct sequence type and handle the creation and initialization of its instances.","solution":"import collections def create_student_struct(name: str, age: int, grades: tuple) -> object: Creates a struct sequence for a student and returns it. # Define the Student named tuple Student = collections.namedtuple(\'Student\', [\'name\', \'age\', \'grades\']) # Create an instance of Student with the provided values student_instance = Student(name, age, grades) return student_instance"},{"question":"# PyTorch Coding Assessment: Advanced Tensor Operations **Objective:** To assess your understanding of fundamental and advanced concepts of PyTorch tensors, your task is to implement a function that performs various tensor operations, including tensor creation, arithmetic operations, and gradient computation. **Problem Statement:** Implement a Python function `tensor_operations` that takes as input a 2D list of integers and a boolean flag. The function should: 1. Create a tensor `A` from the input 2D list with dtype `torch.float32`. 2. Create a tensor `B` of the same shape as `A` but filled with ones, on the same device as `A`. 3. Calculate a new tensor `C` that is the element-wise addition of `A` and `B`. 4. If the boolean flag is `True`, enable gradient computation for tensor `A`. 5. Compute the sum of the squares of all elements in tensor `C` and store it in a tensor `D`. 6. If gradient computation was enabled for `A`, perform backpropagation on `D` and return the gradient with respect to `A`. 7. If gradient computation was not enabled for `A`, return tensor `D`. **Function Signature:** ```python import torch from typing import List, Union def tensor_operations(data: List[List[int]], requires_grad: bool) -> Union[torch.Tensor, torch.Tensor]: pass ``` **Input:** - `data`: A 2D list of integers representing the initial values of tensor `A`. - `requires_grad`: A boolean flag indicating if gradient computation should be enabled for tensor `A`. **Output:** - If `requires_grad` is `True`, return the gradient tensor with respect to `A`. - If `requires_grad` is `False`, return the tensor `D` containing the sum of squares of all elements in tensor `C`. **Example:** ```python data = [[1, 2], [3, 4]] requires_grad = True output = tensor_operations(data, requires_grad) print(output) # Output should be the gradient tensor of A data = [[1, 2], [3, 4]] requires_grad = False output = tensor_operations(data, requires_grad) print(output) # Output should be the tensor D containing the sum of squares of C ``` **Constraints:** - You should use `torch.float32` as the dtype for tensor `A`. - Ensure that tensor `B` is created with the same device type as tensor `A`.","solution":"import torch from typing import List, Union def tensor_operations(data: List[List[int]], requires_grad: bool) -> Union[torch.Tensor, torch.Tensor]: # Create tensor A from the input 2D list with dtype torch.float32. A = torch.tensor(data, dtype=torch.float32, requires_grad=requires_grad) # Create tensor B of the same shape as A filled with ones, on the same device as A. B = torch.ones_like(A) # Calculate tensor C as the element-wise addition of A and B. C = A + B # Compute the sum of the squares of all elements in tensor C and store it in tensor D. D = torch.sum(C**2) if requires_grad: # Perform backpropagation on D. D.backward() # Return the gradient with respect to A. return A.grad else: # Return tensor D. return D"},{"question":"Objective: Demonstrate your understanding of classes, inheritance, method overriding, and iterators in Python. Problem Statement: You are tasked with creating a class-based system to model a simple Zoo which contains various animals. Each animal should have common attributes and functionalities, but specific types of animals might have additional behaviors or attributes. Moreover, the Zoo should allow iterating over the animals it contains. Requirements: 1. **Animal Class**: - Attributes: - `name` (string) - `species` (string) - `age` (integer) - Methods: - `make_sound` (abstract method, to be overridden by subclasses) 2. **Specific Animal Classes**: - `Lion`: Inherits from `Animal` - Additional Attribute: - `mane_color` (string) - Overrides the `make_sound` method to print `\\"Roar\\"` - `Elephant`: Inherits from `Animal` - Additional Attribute: - `trunk_length` (float, in meters) - Overrides the `make_sound` method to print `\\"Trumpet\\"` 3. **Zoo Class**: - Attributes: - `animals` (a list to hold animal instances) - Methods: - `add_animal`: Adds an animal to the zoo. - `__iter__`: Returns an iterator to iterate over the `animals`. Implementation Details: 1. Define an `Animal` class with the specified attributes and an abstract `make_sound` method. 2. Define `Lion` and `Elephant` classes inheriting from `Animal`, with their specific attributes and `make_sound` implementations. 3. Define a `Zoo` class which: - Manages a list of animals. - Provides a method to add animals. - Implements iteration over the animals using the `__iter__` method. Constraints and Considerations: 1. Use the `abc` module to enforce the abstract method in the `Animal` class. 2. Ensure that the Zoo class can be iterated using a `for` loop to retrieve each animal instance. Sample Usage: ```python # Create instances of animals leo = Lion(name=\\"Leo\\", species=\\"Lion\\", age=5, mane_color=\\"Golden\\") dumbo = Elephant(name=\\"Dumbo\\", species=\\"Elephant\\", age=10, trunk_length=2.5) # Create a Zoo and add animals to it zoo = Zoo() zoo.add_animal(leo) zoo.add_animal(dumbo) # Iterate through the zoo and print animal details and sounds for animal in zoo: print(f\\"{animal.name} the {animal.species} makes a sound: \\", end=\\"\\") animal.make_sound() ``` Expected Output: ``` Leo the Lion makes a sound: Roar Dumbo the Elephant makes a sound: Trumpet ``` Implement this system in Python, ensuring each class is correctly defined, and all requirements are met.","solution":"from abc import ABC, abstractmethod class Animal(ABC): def __init__(self, name, species, age): self.name = name self.species = species self.age = age @abstractmethod def make_sound(self): pass class Lion(Animal): def __init__(self, name, species, age, mane_color): super().__init__(name, species, age) self.mane_color = mane_color def make_sound(self): print(\\"Roar\\") class Elephant(Animal): def __init__(self, name, species, age, trunk_length): super().__init__(name, species, age) self.trunk_length = trunk_length def make_sound(self): print(\\"Trumpet\\") class Zoo: def __init__(self): self.animals = [] def add_animal(self, animal): self.animals.append(animal) def __iter__(self): return iter(self.animals)"},{"question":"# Pandas Coding Assessment Objective To assess your understanding of fundamental and advanced concepts in pandas, including data manipulation, error handling, and usage of testing utilities. Problem Statement You are given a dataset containing information about sales transactions from a retail company. The dataset is in CSV format and includes the following columns: - `TransactionID`: Unique identifier for each transaction. - `CustomerID`: Unique identifier for each customer. - `TransactionDate`: Date of the transaction. - `ProductID`: Unique identifier for each product. - `Quantity`: Number of items purchased. - `Price`: Price per item. Your task is to: 1. Load the dataset into a pandas DataFrame. 2. Perform data cleaning to handle any missing or invalid data. 3. Implement a function to calculate the total sales for each product. 4. Implement a function to calculate the total amount spent by each customer. 5. Write test cases to verify the correctness of your functions using pandas testing utilities. Constraints - If there are any missing values in `Quantity` or `Price`, fill them with the mean value of the respective column. - Assume that `TransactionDate` is in a valid date format. - If there are any duplicate `TransactionID`s, keep the first occurrence and drop the rest. Input Format - A CSV file named `sales_data.csv`. Output Format - A pandas DataFrame containing the total sales for each product. - A pandas DataFrame containing the total amount spent by each customer. Function Specifications **Function 1**: `calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame containing the sales data. - **Output**: A pandas DataFrame with two columns: `ProductID` and `TotalSales`, where `TotalSales` is the sum of (`Quantity` * `Price`) for each product. **Function 2**: `calculate_total_spent_by_customers(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame containing the sales data. - **Output**: A pandas DataFrame with two columns: `CustomerID` and `TotalSpent`, where `TotalSpent` is the sum of all products purchased by each customer. Testing Requirements Write unit tests for the above functions using the pandas testing utilities. Ensure that: - The functions correctly compute the total sales per product and total amount spent by each customer. - The functions handle edge cases, such as missing or invalid data, correctly. Example Usage ```python import pandas as pd # Load the dataset df = pd.read_csv(\'sales_data.csv\') # Clean the data df[\'Quantity\'].fillna(df[\'Quantity\'].mean(), inplace=True) df[\'Price\'].fillna(df[\'Price\'].mean(), inplace=True) df.drop_duplicates(subset=\'TransactionID\', keep=\'first\', inplace=True) # Calculate total sales for each product total_sales = calculate_total_sales(df) print(total_sales) # Calculate total amount spent by each customer total_spent = calculate_total_spent_by_customers(df) print(total_spent) # Perform testing def test_calculate_total_sales(): # Create a test DataFrame test_df = pd.DataFrame({ \'TransactionID\': [1, 2, 3, 4], \'CustomerID\': [101, 102, 103, 104], \'TransactionDate\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\']), \'ProductID\': [1001, 1002, 1001, 1003], \'Quantity\': [2, 1, 3, 2], \'Price\': [10.0, 15.0, 10.0, 20.0] }) # Expected output DataFrame expected_output = pd.DataFrame({ \'ProductID\': [1001, 1002, 1003], \'TotalSales\': [50.0, 15.0, 40.0] }) # Use pandas testing utility to assert equality pd.testing.assert_frame_equal(calculate_total_sales(test_df), expected_output) def test_calculate_total_spent_by_customers(): # Create a test DataFrame test_df = pd.DataFrame({ \'TransactionID\': [1, 2, 3, 4], \'CustomerID\': [101, 102, 103, 104], \'TransactionDate\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\']), \'ProductID\': [1001, 1002, 1001, 1003], \'Quantity\': [2, 1, 3, 2], \'Price\': [10.0, 15.0, 10.0, 20.0] }) # Expected output DataFrame expected_output = pd.DataFrame({ \'CustomerID\': [101, 102, 103, 104], \'TotalSpent\': [20.0, 15.0, 30.0, 40.0] }) # Use pandas testing utility to assert equality pd.testing.assert_frame_equal(calculate_total_spent_by_customers(test_df), expected_output) # Run the tests test_calculate_total_sales() test_calculate_total_spent_by_customers() ```","solution":"import pandas as pd def load_and_clean_data(file_path): Loads and cleans the sales dataset. df = pd.read_csv(file_path) # Fill missing values in Quantity and Price with the mean of the respective column df[\'Quantity\'].fillna(df[\'Quantity\'].mean(), inplace=True) df[\'Price\'].fillna(df[\'Price\'].mean(), inplace=True) # Remove duplicate TransactionID entries, keeping the first occurrence df.drop_duplicates(subset=\'TransactionID\', keep=\'first\', inplace=True) return df def calculate_total_sales(df): Calculate the total sales for each product. Args: df (pd.DataFrame): DataFrame containing sales data. Returns: pd.DataFrame: DataFrame with columns \'ProductID\' and \'TotalSales\'. total_sales = df.groupby(\'ProductID\').apply(lambda x: (x[\'Quantity\'] * x[\'Price\']).sum()).reset_index() total_sales.columns = [\'ProductID\', \'TotalSales\'] return total_sales def calculate_total_spent_by_customers(df): Calculate the total amount spent by each customer. Args: df (pd.DataFrame): DataFrame containing sales data. Returns: pd.DataFrame: DataFrame with columns \'CustomerID\' and \'TotalSpent\'. total_spent = df.groupby(\'CustomerID\').apply(lambda x: (x[\'Quantity\'] * x[\'Price\']).sum()).reset_index() total_spent.columns = [\'CustomerID\', \'TotalSpent\'] return total_spent"},{"question":"**Persistent Storage with Shelve** **Objective**: Implement a class `PersistentDict` that uses the `shelve` module to manage a persistent dictionary. This dictionary should be able to perform basic operations like adding, retrieving, and deleting entries, as well as more complex operations like updating mutable entries and performing concurrent-safe operations. **Instructions**: 1. **Initialization**: - The `PersistentDict` should accept a filename, a flag (default is `\'c\'`), a pickle protocol (default is `None`), and a writeback option (default is `False`) during initialization. 2. **Basic Operations**: - Implement methods to add, retrieve, and delete entries. - Ensure the class can be used as a context manager to handle automatic closing. 3. **Updating Mutable Entries**: - Provide a method to update mutable entries safely, regardless of the `writeback` setting. 4. **Concurrent-Safe Operation**: - Add a method `safe_access` that ensures thread-safe read/write access using file locking. **Function Specifications**: - `__init__(self, filename: str, flag: str = \'c\', protocol: int = None, writeback: bool = False) -> None`: Initializes the shelf with the given parameters. - `add_entry(self, key: str, value: Any) -> None`: Adds a new entry to the shelf. - `get_entry(self, key: str) -> Any`: Retrieves an entry from the shelf. Raises a `KeyError` if the key does not exist. - `delete_entry(self, key: str) -> None`: Deletes an entry from the shelf. Raises a `KeyError` if the key does not exist. - `update_entry(self, key: str, value: Any) -> None`: Updates a mutable entry, ensuring changes persist regardless of `writeback`. - `safe_access(self, key: str, value: Any) -> None`: Performs a thread-safe read/write operation using Unix file locking. - `__enter__(self) -> \\"PersistentDict\\"`: Allows the class to be used as a context manager. - `__exit__(self, exc_type, exc_value, traceback) -> None`: Ensures the shelf is closed when exiting the context. **Constraints**: 1. Use the `shelve` module to handle persistent storage. 2. Implement thread-safe access manually using file locking mechanisms. **Example**: ```python from typing import Any import shelve class PersistentDict: def __init__(self, filename: str, flag: str = \'c\', protocol: int = None, writeback: bool = False) -> None: self.shelf = shelve.open(filename, flag, protocol, writeback) def add_entry(self, key: str, value: Any) -> None: self.shelf[key] = value def get_entry(self, key: str) -> Any: return self.shelf[key] def delete_entry(self, key: str) -> None: del self.shelf[key] def update_entry(self, key: str, value: Any) -> None: temp = self.shelf[key] temp.append(value) self.shelf[key] = temp def safe_access(self, key: str, value: Any) -> None: # Implement file locking logic pass # Placeholder for file locking mechanism def __enter__(self) -> \\"PersistentDict\\": return self def __exit__(self, exc_type, exc_value, traceback) -> None: self.shelf.close() # Usage example with PersistentDict(\'mydata\', writeback=True) as pd: pd.add_entry(\'key1\', [1, 2, 3]) print(pd.get_entry(\'key1\')) pd.update_entry(\'key1\', 4) print(pd.get_entry(\'key1\')) ``` Ensure your implementation correctly handles all required functionality and edge cases. **Note**: For the `safe_access` method, refer to Python\'s `fcntl` module for Unix file locking mechanisms.","solution":"from typing import Any import shelve import fcntl import os class PersistentDict: def __init__(self, filename: str, flag: str = \'c\', protocol: int = None, writeback: bool = False) -> None: self.filename = filename self.flag = flag self.protocol = protocol self.writeback = writeback self.shelf = shelve.open(filename, flag, protocol, writeback) self.lock_file = None def __enter__(self) -> \\"PersistentDict\\": # Open the lock file self.lock_file = open(self.filename + \\".lock\\", \\"w\\") return self def __exit__(self, exc_type, exc_value, traceback) -> None: # Close the shelf and the lock file self.shelf.close() self.lock_file.close() def add_entry(self, key: str, value: Any) -> None: self.shelf[key] = value self.shelf.sync() def get_entry(self, key: str) -> Any: return self.shelf[key] def delete_entry(self, key: str) -> None: del self.shelf[key] self.shelf.sync() def update_entry(self, key: str, value: Any) -> None: if self.writeback: self.shelf[key].append(value) else: temp = self.shelf[key] temp.append(value) self.shelf[key] = temp self.shelf.sync() def safe_access(self, key: str, value: Any) -> None: fcntl.flock(self.lock_file, fcntl.LOCK_EX) try: self.add_entry(key, value) finally: fcntl.flock(self.lock_file, fcntl.LOCK_UN)"},{"question":"**Prompt**: Create a Python function `reload_module_if_modified(module_path)` that accepts the path of a Python module file, checks if the module has been modified since it was last imported, and if so, reloads the module. The function should handle reloading the module and updating its definitions in the global namespace. **Details**: 1. The function should receive the file path of a Python module (e.g., `\\"/path/to/module.py\\"`). 2. It should check whether the modification time of the module file is newer than the last import. 3. If the module has been modified: - Reload the module. - Ensure that all global references to the module\'s objects are updated to reflect the new definitions. 4. Use functionalities and classes provided by the `importlib` package. **Input**: - `module_path`: a string representing the file path to the Python module. **Output**: - The function should return the reloaded module object. **Constraints**: - You can assume the module exists at the provided path. - The function should work with Python 3.8 or newer. - Ensure that references to previous versions of the module objects are updated to the new definitions post-reload. **Example**: ```python def reload_module_if_modified(module_path): pass # Example usage: import time # Assuming /path/to/module.py exists # Import the module for the first time mod = reload_module_if_modified(\'/path/to/module.py\') print(mod.some_function()) # First import # Simulate waiting for the file to be modified externally time.sleep(10) # After module file modification mod = reload_module_if_modified(\'/path/to/module.py\') print(mod.some_function()) # Should reflect updated definitions ``` **Hint**: - Utilize `importlib.util.spec_from_file_location()` and `importlib.util.module_from_spec()` to handle loading the module from a file path. - Make use of `importlib.reload()` to handle reloading the module accurately. - Check modification times using `os.path.getmtime()`.","solution":"import os import sys import importlib.util import importlib _last_import_times = {} def reload_module_if_modified(module_path): Reloads the module if it has been modified since it was last imported. Parameters: - module_path: str, the file path to the Python module. Returns: - module: the reloaded module object if it was modified, otherwise the current module object. module_name = os.path.splitext(os.path.basename(module_path))[0] current_mtime = os.path.getmtime(module_path) # Check if module has been imported before and if it was modified since last import if module_name in _last_import_times: if _last_import_times[module_name] >= current_mtime: # If it hasn\'t been modified, return the existing module. return sys.modules[module_name] # If the module wasn\'t imported before or if it was modified, reload it spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) # Update the record of the last modification time _last_import_times[module_name] = current_mtime return module"},{"question":"# Coding Assessment: PyTorch with HIP (ROCm) **Objective:** You are required to demonstrate your understanding of GPU memory management, device handling, and tensor operations using PyTorch with HIP (ROCm). **Task:** Implement a function `tensor_operations_on_hip` that performs the following steps: 1. Check if HIP is available using `torch.version.hip`. 2. If HIP is available, perform the following steps: - Create two tensors `A` and `B` of size [1000, 1000] filled with random floating-point numbers, and allocate them on the default HIP device. - Transfer these tensors to a specific HIP device (index 1). - Perform element-wise addition of these two tensors and store the result in tensor `C`. - Monitor the amount of GPU memory allocated and reserved before and after the addition operation. - Return tensor `C` and the recorded memory statistics as a tuple. The memory statistics should include: - Memory allocated before the addition. - Memory allocated after the addition. - Memory reserved before the addition. - Memory reserved after the addition. **Function Signature:** ```python import torch def tensor_operations_on_hip(): pass ``` **Constraints:** - Assume that the function will only run on a system where PyTorch has been built with GPU support. - Ensure the function handles cases where HIP is not available by returning a message `\\"HIP is not available on this system.\\"` **Example Output:** ```python C, mem_stats = tensor_operations_on_hip() # Where C is the resultant tensor, and mem_stats is a dictionary with memory statistics. ``` **Notes:** - Make use of methods such as `torch.cuda.memory_allocated`, `torch.cuda.memory_reserved`, and `torch.cuda.empty_cache` to monitor memory. - Properly handle any exceptions that may arise during device transfers or tensor operations. - Ensure the function is efficient and handles memory correctly, cleaning up any unused cached memory if necessary.","solution":"import torch def tensor_operations_on_hip(): if not torch.version.hip: return \\"HIP is not available on this system.\\" device = torch.device(\\"cuda:1\\") # HIP devices are typically accessed via the CUDA backend in PyTorch # Create tensors A and B on the default HIP device A = torch.rand((1000, 1000), device=device) B = torch.rand((1000, 1000), device=device) # Monitor GPU memory before addition mem_allocated_before = torch.cuda.memory_allocated(device) mem_reserved_before = torch.cuda.memory_reserved(device) # Perform element-wise addition C = A + B # Monitor GPU memory after addition mem_allocated_after = torch.cuda.memory_allocated(device) mem_reserved_after = torch.cuda.memory_reserved(device) # Collect memory statistics mem_stats = { \'memory_allocated_before\': mem_allocated_before, \'memory_allocated_after\': mem_allocated_after, \'memory_reserved_before\': mem_reserved_before, \'memory_reserved_after\': mem_reserved_after } return C, mem_stats"},{"question":"**Objective:** You are required to implement a mini-history manager for command-line inputs, leveraging the `readline` module. This mini application should manage the history of inputs by performing the following: 1. Reading history from a specified history file. 2. Adding new commands to the history. 3. Removing specific commands from the history. 4. Displaying the current command history. 5. Writing the updated history back to the history file. **Specifications:** Implement a class `HistoryManager` with the following functionalities: Methods: 1. **`__init__(self, filepath: str)`:** - Initializes the HistoryManager with the given history file path. - Reads the history from the file into the readline history buffer. If the file does not exist, it should create an empty history file. 2. **`add_command(self, command: str)`:** - Adds a new command to the history. 3. **`remove_command(self, index: int) -> bool`:** - Removes a command from the history at the specified index (0-based index). - Returns `True` if the command was successfully removed, `False` otherwise. 4. **`display_history(self) -> list`:** - Returns a list of current commands in the history. 5. **`save_history(self)`:** - Writes the current history back to the file, overwriting any existing content. Constraints and Notes: - The history file should be truncated to store a maximum of 1000 commands. - Indexing for adding and removing commands should consider the order in which the commands were entered (0-based indexing). - You can assume that the input command strings and file paths are always valid. Example Usage: ```python # Initializing HistoryManager with a history file path history_manager = HistoryManager(\\".my_history\\") # Adding commands to the history history_manager.add_command(\\"ls -l\\") history_manager.add_command(\\"echo \'Hello, World!\'\\") # Removing a command from the history success = history_manager.remove_command(1) # Removes the second command # Display the current history commands = history_manager.display_history() print(commands) # Outputs: [\\"ls -l\\"] # Saving the history back to the file history_manager.save_history() ``` Implement the `HistoryManager` class according to the specifications provided above.","solution":"import readline import os class HistoryManager: def __init__(self, filepath: str): self.filepath = filepath if not os.path.exists(self.filepath): open(self.filepath, \'a\').close() self._load_history() def _load_history(self): readline.clear_history() try: readline.read_history_file(self.filepath) except FileNotFoundError: pass def add_command(self, command: str): readline.add_history(command) def remove_command(self, index: int) -> bool: if index < 0 or index >= readline.get_current_history_length(): return False # readline\'s history uses 1-based indexing del_cmd = readline.get_history_item(index + 1) if del_cmd is None: return False hist = [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1)] readline.clear_history() for i, cmd in enumerate(hist): if i != index: readline.add_history(cmd) return True def display_history(self) -> list: hist = [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1)] return hist def save_history(self): total_cmds = readline.get_current_history_length() max_cmds = 1000 if total_cmds > max_cmds: hist = [readline.get_history_item(i) for i in range(total_cmds - max_cmds + 1, total_cmds + 1)] readline.clear_history() for cmd in hist: readline.add_history(cmd) readline.write_history_file(self.filepath)"},{"question":"# Python Coding Assessment: Extending Interactive Console Objective Your task is to implement a custom interactive console that extends Python\'s `code.InteractiveConsole`. This custom console should provide additional features for error handling and command history management. Problem Description You are required to implement a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole` and modifies/extends its functionalities. Specifically, your custom console should: 1. **Enhanced Error Handling**: Log syntax and runtime errors to a file named `error_log.txt` instead of just displaying them. 2. **Command History**: Maintain a command history that persists between sessions by saving it to a file named `command_history.txt`. Requirements 1. **Class Definition**: - Define `CustomInteractiveConsole` class inheriting from `code.InteractiveConsole`. 2. **Enhanced Error Handling**: - Override `showsyntaxerror` method to log syntax errors in `error_log.txt`. - Override `showtraceback` method to log runtime exceptions in `error_log.txt`. 3. **Command History**: - Override `push` method to record each command (whether complete or incomplete) in a file `command_history.txt`. - Save each input command to `command_history.txt`. Methods to Implement 1. **CustomInteractiveConsole.showsyntaxerror(self, filename=None)**: - Log the syntax error message to `error_log.txt`. 2. **CustomInteractiveConsole.showtraceback(self)**: - Log the runtime exception traceback to `error_log.txt`. 3. **CustomInteractiveConsole.push(self, line)**: - Save the command to `command_history.txt`. - Call the base class `push` method to handle the command execution. 4. **CustomInteractiveConsole.__init__(self, locals=None, filename=\'<console>\')**: - Initialize the custom console. - Optionally load existing command history from `command_history.txt` to be used within the session (though not required for basic functionality). Example Usage ```python import code class CustomInteractiveConsole(code.InteractiveConsole): # Implementation of required methods here def showsyntaxerror(self, filename=None): with open(\\"error_log.txt\\", \\"a\\") as f: f.write(\\"SyntaxError: \\" + str(filename) + \\"n\\") super().showsyntaxerror(filename) def showtraceback(self): import traceback with open(\\"error_log.txt\\", \\"a\\") as f: traceback.print_exc(file=f) super().showtraceback() def push(self, line): with open(\\"command_history.txt\\", \\"a\\") as f: f.write(line + \\"n\\") return super().push(line) # Example instantiation and interaction console = CustomInteractiveConsole() console.interact(banner=\\"Welcome to the Custom Interactive Console!\\", exitmsg=\\"Goodbye!\\") ``` You are required to implement the class based on these requirements. Ensure your error log and command history functionalities are correctly implemented and tested. Constraints - Do not use any external packages; only built-in Python libraries are allowed. - Ensure that the implementation gracefully handles common exceptions and edge cases.","solution":"import code import traceback class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.history_file = \\"command_history.txt\\" self.error_log_file = \\"error_log.txt\\" self._load_history() def showsyntaxerror(self, filename=None): with open(self.error_log_file, \\"a\\") as f: traceback.print_exc(file=f) super().showsyntaxerror(filename) def showtraceback(self): with open(self.error_log_file, \\"a\\") as f: traceback.print_exc(file=f) super().showtraceback() def push(self, line): with open(self.history_file, \\"a\\") as f: f.write(line + \\"n\\") return super().push(line) def _load_history(self): try: with open(self.history_file, \\"r\\") as f: self.history = f.readlines() except FileNotFoundError: self.history = [] # Example instantiation for manual interactive testing # console = CustomInteractiveConsole() # console.interact(banner=\\"Welcome to the Custom Interactive Console!\\", exitmsg=\\"Goodbye!\\")"},{"question":"# URL Manipulation and Data Extraction with `urllib` You are required to write a set of functions to demonstrate your understanding of the `urllib` package. Specifically, your task is to: 1. Retrieve data from a URL. 2. Handle potential errors when making HTTP requests. 3. Parse URL strings to extract meaningful components. 4. Respect `robots.txt` rules when accessing web pages. Function 1: `fetch_url_content(url: str) -> str` This function should: - Take a URL as input. - Open the URL and read the content. - Return the content as a string. - If there is any error during the URL opening (like the URL not existing or network issues), return an appropriate error message. Function 2: `parse_url(url: str) -> dict` This function should: - Take a URL as input. - Parse the URL and extract the following components: protocol (scheme), hostname, port, path, query, and fragment. - Return these components as a dictionary. Function 3: `can_fetch(url: str, user_agent: str = \\"*\\") -> bool` This function should: - Take a URL and an optional user agent string as input. - Check the `robots.txt` file of the domain specified in the URL to determine if the user agent is allowed to fetch the URL. - Return `True` if allowed, otherwise `False`. Example: ```python # Example usage and expected output: url = \\"https://example.com/path?query=1#fragment\\" print(fetch_url_content(url)) # Expected Output: \\"<HTML content from https://example.com/path?query=1#fragment>\\" print(parse_url(url)) # Expected Output: # { # \\"scheme\\": \\"https\\", # \\"hostname\\": \\"example.com\\", # \\"port\\": None, # \\"path\\": \\"/path\\", # \\"query\\": \\"query=1\\", # \\"fragment\\": \\"fragment\\" # } print(can_fetch(url)) # Expected Output: True or False based on the robots.txt rules. ``` Constraints: 1. You are not allowed to use third-party libraries other than `urllib`. 2. Handle all types of URLs (HTTP, HTTPS). 3. Ensure proper handling of exceptions within the `fetch_url_content` function. 4. The robots.txt file parsing should account for wildcard rules. You will be assessed based on: - Correctness and robustness of your functions. - Proper error handling. - Clarity and efficiency of your code.","solution":"import urllib.request from urllib.parse import urlparse, urljoin import urllib.robotparser def fetch_url_content(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except Exception as e: return str(e) def parse_url(url: str) -> dict: parsed = urlparse(url) return { \\"scheme\\": parsed.scheme, \\"hostname\\": parsed.hostname, \\"port\\": parsed.port, \\"path\\": parsed.path, \\"query\\": parsed.query, \\"fragment\\": parsed.fragment } def can_fetch(url: str, user_agent: str = \\"*\\") -> bool: parsed_url = urlparse(url) robots_url = urljoin(f\\"{parsed_url.scheme}://{parsed_url.hostname}\\", \\"/robots.txt\\") rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() except Exception: return False return rp.can_fetch(user_agent, url)"},{"question":"**Email Policy Customization and Serialization** The `email.policy` module within the Python `email` package provides flexible and customizable ways of handling email messages through `Policy` objects. The objective of this task is to implement a specific use case where you create a custom policy that modifies certain default behaviors, serialize the email message with the custom policy, and handle any defects appropriately. Task Details 1. **Custom Policy Creation**: - Create a custom policy `CustomSMTPPolicy` by cloning the predefined `SMTP` policy. - Modify the cloned policy to have the following attributes: - `max_line_length`: 100 - `raise_on_defect`: True - `linesep`: `\'rn\'` 2. **Email Message Processing**: - Parse the email message provided in a file named `input_email.txt` using the `CustomSMTPPolicy`. - If the email message has headers that violate any email RFC standards (causing defects), capture those defects and handle them appropriately. 3. **Serialization**: - Serialize the email message to a binary format using the `BytesGenerator` with the custom line separator. - Write the serialized content to a file named `output_email.eml`. Input and Output - **Input**: - A file named `input_email.txt` containing the raw email message. - **Output**: - A file named `output_email.eml` containing the serialized email message with the customized policy. Constraints: - You must use the `email.policy` module and the `email` package for parsing and serializing the email. - Handle any defects appropriately, if `raise_on_defect` is set to `True`. Example Consider `input_email.txt` contains: ``` From: sender@example.com To: receiver@example.com Subject: Test Email This is the body of the email. ``` After processing and serializing, `output_email.eml` should contain the email message serialized using the custom policy with a modified `max_line_length`, `raise_on_defect`, and `linesep`. Python Code Template ```python from email import message_from_binary_file from email.generator import BytesGenerator from email import policy from subprocess import Popen, PIPE import os def customize_policy(): # Step 1: Create and customize the policy custom_policy = policy.SMTP.clone( max_line_length=100, raise_on_defect=True, linesep=\'rn\' ) return custom_policy def parse_email(input_file, email_policy): # Step 2: Parse the email with the custom policy with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=email_policy) return msg def serialize_email(message, output_file, custom_policy): # Step 3: Serialize the email with custom line separator with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=custom_policy) generator.flatten(message) def main(): input_file = \'input_email.txt\' output_file = \'output_email.eml\' custom_policy = customize_policy() email_message = parse_email(input_file, custom_policy) serialize_email(email_message, output_file, custom_policy) if __name__ == \'__main__\': main() ``` Your implemented solution should be able to handle the given example and produce the correct output. Ensure that you handle any defects appropriately and follow the required constraints.","solution":"from email import message_from_binary_file from email.generator import BytesGenerator from email import policy import os def customize_policy(): Create and customize the SMTP policy with specified attributes. custom_policy = policy.SMTP.clone( max_line_length=100, raise_on_defect=True, linesep=\'rn\' ) return custom_policy def parse_email(input_file, email_policy): Parse the email from the input file using the given policy. with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=email_policy) return msg def serialize_email(message, output_file, custom_policy): Serialize the email message to the output file with the given custom policy. with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=custom_policy) generator.flatten(message) def main(input_file=\'input_email.txt\', output_file=\'output_email.eml\'): custom_policy = customize_policy() email_message = parse_email(input_file, custom_policy) serialize_email(email_message, output_file, custom_policy) if __name__ == \'__main__\': main()"},{"question":"You are tasked with creating a utility function to gather and display metadata of all installed packages in your current Python environment. The function should fulfill the following requirements: # Specifications: 1. **Function Name**: `get_package_metadata` 2. **Input**: (None) 3. **Output**: A dictionary where keys are package names, and values are another dictionary containing: - `version`: the package version - `requires_python`: the Python version requirements - `entry_points`: a list of entry points\' names provided by the package. # Constraints: - Only fetch metadata for packages that have a `dist-info` or `egg-info` directory. - Ensure the function handles cases where certain metadata fields are missing (e.g., `requires_python` or `entry_points` not defined). In such cases, those fields should be omitted from the corresponding dictionary. # Example Output: ```python { \'wheel\': { \'version\': \'0.36.2\', \'requires_python\': \'>=3.6\', \'entry_points\': [\'console_scripts\'] }, \'setuptools\': { \'version\': \'51.1.2\', \'entry_points\': [\'distutils.commands\', \'distutils.setup_keywords\'] }, ... } ``` # Function Body: ```python from importlib.metadata import distribution, entry_points def get_package_metadata(): all_metadata = {} for dist in distributions(): metadata = {} metadata[\'version\'] = dist.version requires_python = dist.metadata.get(\'Requires-Python\') if requires_python: metadata[\'requires_python\'] = requires_python eps = entry_points().select(name=dist.metadata[\'Name\']) if eps: metadata[\'entry_points\'] = [ep.group for ep in eps] all_metadata[dist.metadata[\'Name\']] = metadata return all_metadata ``` Implement the `get_package_metadata` function as defined above to showcase your understanding of importing metadata and handling package information using the `importlib.metadata` module.","solution":"from importlib.metadata import distributions, entry_points def get_package_metadata(): all_metadata = {} for dist in distributions(): metadata = {} metadata[\'version\'] = dist.version requires_python = dist.metadata.get(\'Requires-Python\') if requires_python: metadata[\'requires_python\'] = requires_python eps = dist.entry_points if eps: metadata[\'entry_points\'] = [ep.name for ep in eps] all_metadata[dist.metadata[\'Name\']] = metadata return all_metadata"},{"question":"Multi-Level Class Structure with Iterators **Objective:** Create a multi-level inheritance structure to simulate a simplified library system. The system should consist of multiple classes with a use of iterators to manage collections of books. **Requirements:** 1. **Base Class: `LibraryItem`** - Attributes: - `title`: String representing the title of the library item. - `item_id`: Integer representing a unique identifier for the item. - Methods: - `__init__(self, title: str, item_id: int)`: Constructor to initialize the title and item ID. 2. **Derived Class: `Book` (inherits from `LibraryItem`)** - Additional Attributes: - `author`: String representing the author of the book. - Methods: - `__init__(self, title: str, item_id: int, author: str)`: Constructor to initialize the title, item ID, and author. Use the superclass constructor to initialize the title and item ID. 3. **Derived Class: `Library`** - Attributes: - `name`: String representing the name of the library. - `books`: List of `Book` objects. - Methods: - `__init__(self, name: str)`: Constructor to initialize the library name and an empty list of books. - `add_book(self, book: Book)`: Method to add a book to the book list. - `__iter__(self)`: Method to create an iterator over the books in the library, returning each book one by one. - `__len__(self)`: Method to return the number of books in the library. **Input and Output:** - **Input:** - Adding book information (title, item ID, author) to the library. - Iterating through the books in the library. - Getting the number of books in the library. - **Output:** - Titles and authors of all books in the library. - Total number of books in the library. **Example:** ```python # Define the library item base class class LibraryItem: def __init__(self, title: str, item_id: int): self.title = title self.item_id = item_id # Define the Book class inheriting from LibraryItem class Book(LibraryItem): def __init__(self, title: str, item_id: int, author: str): super().__init__(title, item_id) self.author = author # Define the Library class to manage a collection of books class Library: def __init__(self, name: str): self.name = name self.books = [] def add_book(self, book: Book): self.books.append(book) def __iter__(self): return iter(self.books) def __len__(self): return len(self.books) # Demonstration of the classes library = Library(\\"City Library\\") # Adding books library.add_book(Book(\\"1984\\", 1, \\"George Orwell\\")) library.add_book(Book(\\"To Kill a Mockingbird\\", 2, \\"Harper Lee\\")) library.add_book(Book(\\"The Great Gatsby\\", 3, \\"F. Scott Fitzgerald\\")) # Iterating over the books for book in library: print(f\\"Title: {book.title}, Author: {book.author}\\") # Getting the number of books print(f\\"Total number of books: {len(library)}\\") ``` The following output is expected: ``` Title: 1984, Author: George Orwell Title: To Kill a Mockingbird, Author: Harper Lee Title: The Great Gatsby, Author: F. Scott Fitzgerald Total number of books: 3 ``` **Constraints:** - Ensure each book has a unique `item_id`. - The iterator should efficiently handle large collections of books. **Notes:** - Consider edge cases like adding a book with a duplicate `item_id`. This task tests the understanding of classes, inheritance, encapsulation, iterators, and overriding methods in Python.","solution":"class LibraryItem: Base class representing a library item. def __init__(self, title: str, item_id: int): self.title = title self.item_id = item_id class Book(LibraryItem): Class representing a book, inheriting from LibraryItem. def __init__(self, title: str, item_id: int, author: str): super().__init__(title, item_id) self.author = author class Library: Class representing a library that manages a collection of books. def __init__(self, name: str): self.name = name self.books = [] def add_book(self, book: Book): self.books.append(book) def __iter__(self): return iter(self.books) def __len__(self): return len(self.books)"},{"question":"**Coding Assessment Question** # Objective Implement a custom neural network using PyTorch and initialize its parameters using different initialization methods available in the `torch.nn.init` module. # Description You are required to design a custom neural network and apply different parameter initialization techniques using PyTorch. This task will test your understanding of various parameter initialization methods and how they impact training. # Requirements 1. **Define a Custom Neural Network**: - Create a class `CustomNN` that inherits from `torch.nn.Module`. - The network should contain at least three linear layers. 2. **Initialization Method**: - Implement a function `initialize_parameters` that takes the model and an initialization method name as input and initializes the model\'s parameters using the specified method. The function signature should be: ```python def initialize_parameters(model: torch.nn.Module, method: str) -> None: ``` - The `method` parameter should be a string and can be one of the following options: `\'uniform\'`, `\'normal\'`, `\'constant\'`, `\'xavier_uniform\'`, `\'kaiming_normal\'`. 3. **Parameter Initialization**: - The `initialize_parameters` function should initialize the parameters of each layer in the model based on the method specified. For instance: - `\'uniform\'` should use `torch.nn.init.uniform_`. - `\'normal\'` should use `torch.nn.init.normal_`. - `\'constant\'` should use `torch.nn.init.constant_` and set the constant value to 0.5. - `\'xavier_uniform\'` should use `torch.nn.init.xavier_uniform_`. - `\'kaiming_normal\'` should use `torch.nn.init.kaiming_normal_`. 4. **Input and Output**: - The model should accept an input tensor of shape `(batch_size, input_dim)`. - The output should be a tensor of shape `(batch_size, output_dim)`. 5. **Testing**: - Create an instance of your model with specific dimensions (e.g., input_dim=10, hidden_dims=[20, 15], output_dim=5). - Apply each initialization method to the model parameters. - Print the initialized parameters for each layer and method to verify the correct application of the initialization. # Constraints - The dimensions for the layers and the number of layers should be flexible and easy to change. - Ensure reproducibility by setting a random seed before initializing the parameters. # Example ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNN(nn.Module): def __init__(self, input_dim, hidden_dims, output_dim): super(CustomNN, self).__init__() self.layers = nn.ModuleList() dims = [input_dim] + hidden_dims + [output_dim] for i in range(len(dims) - 1): self.layers.append(nn.Linear(dims[i], dims[i+1])) def forward(self, x): for layer in self.layers[:-1]: x = torch.relu(layer(x)) x = self.layers[-1](x) return x def initialize_parameters(model: nn.Module, method: str) -> None: for layer in model.layers: if isinstance(layer, nn.Linear): if method == \'uniform\': init.uniform_(layer.weight) elif method == \'normal\': init.normal_(layer.weight) elif method == \'constant\': init.constant_(layer.weight, 0.5) elif method == \'xavier_uniform\': init.xavier_uniform_(layer.weight) elif method == \'kaiming_normal\': init.kaiming_normal_(layer.weight) else: raise ValueError(\\"Unknown initialization method\\") if layer.bias is not None: init.zeros_(layer.bias) # Example usage input_dim = 10 hidden_dims = [20, 15] output_dim = 5 model = CustomNN(input_dim, hidden_dims, output_dim) print(\\"Before initialization:\\") print(model) initialize_parameters(model, \'xavier_uniform\') print(\\"After Xavier Uniform initialization:\\") print(model) # Initialize with other methods, e.g., \'normal\', \'uniform\', etc., and print each. ``` # Notes - Ensure to handle the bias terms in the layers as well, setting them to zero or using the appropriate initialization. - Carefully document your code to explain the purpose of each part. - This task will assess your understanding of PyTorch fundamentals as well as your ability to apply advanced initialization techniques effectively.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNN(nn.Module): def __init__(self, input_dim, hidden_dims, output_dim): super(CustomNN, self).__init__() self.layers = nn.ModuleList() dims = [input_dim] + hidden_dims + [output_dim] for i in range(len(dims) - 1): self.layers.append(nn.Linear(dims[i], dims[i+1])) def forward(self, x): for layer in self.layers[:-1]: x = torch.relu(layer(x)) x = self.layers[-1](x) return x def initialize_parameters(model: nn.Module, method: str) -> None: for layer in model.layers: if isinstance(layer, nn.Linear): if method == \'uniform\': init.uniform_(layer.weight) elif method == \'normal\': init.normal_(layer.weight) elif method == \'constant\': init.constant_(layer.weight, 0.5) elif method == \'xavier_uniform\': init.xavier_uniform_(layer.weight) elif method == \'kaiming_normal\': init.kaiming_normal_(layer.weight) else: raise ValueError(\\"Unknown initialization method\\") if layer.bias is not None: init.zeros_(layer.bias) # Example usage input_dim = 10 hidden_dims = [20, 15] output_dim = 5 model = CustomNN(input_dim, hidden_dims, output_dim) print(\\"Before initialization:\\") print(model) initialize_parameters(model, \'xavier_uniform\') print(\\"After Xavier Uniform initialization:\\") print(model) # Initialize with other methods, e.g., \'normal\', \'uniform\', etc., and print each."},{"question":"# MIME Email Creator You are required to write a Python function that creates a multipart MIME email message with various types of attachments. Your function should accept the subject, recipient, body text, an image, an audio file, and an application file (such as a PDF). The function should create the email message, attach the files in appropriate MIME formats, and return the serialized email string. Function Signature: ```python def create_mime_email(subject: str, recipient: str, body_text: str, image_data: bytes, audio_data: bytes, application_data: bytes) -> str: pass ``` Input: - `subject` (str): The subject of the email. - `recipient` (str): The recipient of the email in the format \\"name@example.com\\". - `body_text` (str): The body of the email in plain text format. - `image_data` (bytes): The raw byte data of an image file. - `audio_data` (bytes): The raw byte data of an audio file. - `application_data` (bytes): The raw byte data of an application file (e.g., PDF). Output: - (str): A serialized MIME email string. Constraints: - Assume the image format is JPEG. - Assume the audio format is WAV. - Assume the application format is PDF. - The returned email string should include all necessary MIME headers and boundaries. Example: ```python subject = \\"Meeting Agenda\\" recipient = \\"colleague@example.com\\" body_text = \\"Please find the attached documents for our meeting.\\" image_data = open(\'image.jpg\', \'rb\').read() audio_data = open(\'audio.wav\', \'rb\').read() application_data = open(\'document.pdf\', \'rb\').read() email_string = create_mime_email(subject, recipient, body_text, image_data, audio_data, application_data) print(email_string) ``` # Instructions: 1. Create a multipart email structure with a plain text body. 2. Add the image, audio, and application files as attachments using the appropriate MIME classes (`MIMEImage`, `MIMEAudio`, and `MIMEApplication`). 3. Construct the email with the specified subject and recipient. 4. Serialize and return the complete email message as a string. # Additional Notes: - Use the `email.mime.multipart.MIMEMultipart` class to create the multipart message. - Utilize `encode_base64` for encoding the attached binary data. - Properly set `Content-Type` and `MIME-Version` headers where necessary.","solution":"import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_mime_email(subject: str, recipient: str, body_text: str, image_data: bytes, audio_data: bytes, application_data: bytes) -> str: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = \\"sender@example.com\\" # Replace with the actual sender\'s email address msg[\'To\'] = recipient msg[\'MIME-Version\'] = \'1.0\' # Attach the body text body = MIMEText(body_text, \'plain\') msg.attach(body) # Attach the image image = MIMEImage(image_data, _subtype=\'jpeg\') image.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.jpg\') msg.attach(image) # Attach the audio file audio = MIMEAudio(audio_data, _subtype=\'wav\') audio.add_header(\'Content-Disposition\', \'attachment\', filename=\'audio.wav\') msg.attach(audio) # Attach the application file application = MIMEApplication(application_data, _subtype=\'pdf\') application.add_header(\'Content-Disposition\', \'attachment\', filename=\'document.pdf\') msg.attach(application) # Serialize the email message to a string return msg.as_string()"},{"question":"Objective Implement a Python function that uses the `difflib` module to compare two lists of strings and return the differences in a specific format. Task Write a function called `generate_diff_report` that takes in two lists of strings representing lines from two different files. The function should compare these lists and generate a detailed diff report in the \'unified\' format and return it as a single string. Function Signature ```python def generate_diff_report(lines_a: list[str], lines_b: list[str], fromfile: str, tofile: str) -> str: pass ``` Input - `lines_a`: A list of strings representing lines from the first file. - `lines_b`: A list of strings representing lines from the second file. - `fromfile`: A string representing the name of the first file. - `tofile`: A string representing the name of the second file. Output - Returns a string containing the unified diff report showing the differences between `lines_a` and `lines_b`. Example ```python lines_a = [ \'baconn\', \'eggsn\', \'hamn\', \'guidon\' ] lines_b = [ \'pythonn\', \'eggyn\', \'hamstern\', \'guidon\' ] fromfile = \'before.py\' tofile = \'after.py\' diff_report = generate_diff_report(lines_a, lines_b, fromfile, tofile) print(diff_report) ``` Expected Output: ``` --- before.py +++ after.py @@ -1,4 +1,4 @@ -bacon -eggs -ham +python +eggy +hamster guido ``` Constraints: - All lines in `lines_a` and `lines_b` will end with a newline character (`n`). - You cannot use any external libraries except for the Python standard library. Notes: - Use the `difflib.unified_diff` function from the `difflib` module to generate the diff report. - Ensure the returned string is properly formatted according to the unified diff format.","solution":"import difflib def generate_diff_report(lines_a: list[str], lines_b: list[str], fromfile: str, tofile: str) -> str: Generates a unified diff report comparing lines_a and lines_b. Parameters: lines_a (list[str]): Lines from the first file. lines_b (list[str]): Lines from the second file. fromfile (str): Name of the first file. tofile (str): Name of the second file. Returns: str: A unified diff report as a string. diff = difflib.unified_diff(lines_a, lines_b, fromfile=fromfile, tofile=tofile) return \'\'.join(diff)"},{"question":"# Seaborn Objects and Visualization Layout As a data analyst, you frequently need to create visualizations that fit neatly into given spaces and accommodate multiple plots effectively. Seaborn\'s object-oriented interface offers powerful tools for controlling this layout. For this assessment, you will create a function using seaborn.objects to make and arrange visualizations efficiently. **Task:** Write a function `generate_plot_layout` that takes in the following parameters: - `data`: A pandas DataFrame containing the dataset to be visualized. - `plot_pairs`: A tuple of tuples specifying the pairs of columns to be used in subplots. - `size`: A tuple specifying the overall dimensions of the outer figure. - `extent`: A list specifying the relative size of each individual plot within the figure. - `engine`: A string specifying the layout engine to be used. The function should: 1. Create a seaborn plot initialized with the given data. 2. Layout the plot using the specified size. 3. Create subplots based on the pairs of columns provided. 4. Use the specified layout engine for arranging the plots. 5. Apply the specified extent for each individual plot inside the figure. The function should then return the final seaborn plot object. **Input:** - `data`: pandas DataFrame, e.g., `pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9], \'D\': [10, 11, 12]})` - `plot_pairs`: Tuple of tuples, e.g., `((\'A\', \'B\'), (\'C\', \'D\'))` - `size`: Tuple, e.g., (10, 10) - `extent`: List, e.g., [0, 0, 0.8, 1] - `engine`: String, e.g., \'constrained\' **Output:** - seaborn plot object configured with the specified layout, subplots, extents, and layout engine. **Example:** ```python import seaborn.objects as so import pandas as pd def generate_plot_layout(data, plot_pairs, size, extent, engine): # Initialize the main plot p = so.Plot(data).layout(size=size) # Use pairs to create subplots rows, cols = zip(*plot_pairs) # Create faceted plot with specified layout engine p = p.facet(rows, cols).layout(engine=engine) # Apply the specified extent p = p.layout(extent=extent) # Return the configured plot return p # Example usage data = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9], \'D\': [10, 11, 12]}) plot_pairs = ((\'A\', \'B\'), (\'C\', \'D\')) size = (10, 10) extent = [0, 0, 0.8, 1] engine = \'constrained\' plot = generate_plot_layout(data, plot_pairs, size, extent, engine) plot.show() ``` **Constraints:** - Ensure all column names in `plot_pairs` exist in the DataFrame. - The `data` should not be empty. - Ensure the `extent` values are within [0, 1]. **Note:** - Focus on handling data correctly and applying the layout options effectively. - You may need to install and import seaborn and pandas before running your solution.","solution":"import pandas as pd import seaborn.objects as so def generate_plot_layout(data, plot_pairs, size, extent, engine): Generates a seaborn plot with specified layout and subplot configuration. Parameters: - data (pd.DataFrame): The dataset to be visualized. - plot_pairs (tuple of tuples): Pairs of columns to be used in subplots. - size (tuple): Overall dimensions of the outer figure. - extent (list): Relative size of each individual plot within the figure. - engine (str): Layout engine to be used. Returns: - seaborn plot object configured with the specified layout, subplots, extents, and layout engine. if data.empty: raise ValueError(\\"The data input cannot be empty.\\") for pair in plot_pairs: for col in pair: if col not in data.columns: raise ValueError(f\\"Column \'{col}\' does not exist in the DataFrame.\\") if not all(0 <= e <= 1 for e in extent): raise ValueError(\\"All extent values must be within [0, 1].\\") # Initialize the main plot p = so.Plot(data).layout(size=size) # Use pairs to create subplots rows, cols = zip(*plot_pairs) # Create faceted plot with specified layout engine p = p.facet(rows, cols).layout(engine=engine) # Apply the specified extent p = p.layout(extent=extent) # Return the configured plot return p"},{"question":"Objective Demonstrate your understanding of the `sklearn.datasets` module by creating and visualizing different synthetic datasets using various dataset generators. Task 1. Generate datasets using the following functions from `sklearn.datasets`: - `make_blobs` - `make_classification` - `make_circles` - `make_moons` 2. For each dataset: - Create the dataset with specified parameters. - Visualize the dataset using a scatter plot. - Provide a brief explanation of the type of dataset generated and its potential use case in machine learning. 3. As a bonus task, use the generated datasets in a basic classification algorithm (optional but encouraged). Specifications - Use `matplotlib.pyplot` for visualization. - Ensure each plot has a title indicating the type of dataset. - Clearly label the axes of each plot. - Use random states for reproducibility where applicable. Example Code Framework ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles, make_moons from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def generate_and_plot_datasets(): # 1. make_blobs X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"make_blobs: Three normally-distributed clusters\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_blobs... # 2. make_classification X_class, y_class = make_classification(n_features=2, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=1) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class) plt.title(\\"make_classification: Three classes with two informative features\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_classification... # 3. make_circles X_circles, y_circles = make_circles(noise=0.1, factor=0.3, random_state=0) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles) plt.title(\\"make_circles: Circular decision boundary\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_circles... # 4. make_moons X_moons, y_moons = make_moons(noise=0.1, random_state=0) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"make_moons: Interleaving half-circles\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_moons... def bonus_classification_task(): # Example with one of the datasets X, y = make_classification(n_features=2, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) classifier = LogisticRegression() classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) print(f\\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\\") generate_and_plot_datasets() # Uncomment below line to run the bonus task # bonus_classification_task() ``` Notes - Ensure code clarity and proper documentation within the code to explain each section. - Students are encouraged to explore different parameters of the dataset generation functions to see the effect on the generated data.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles, make_moons def generate_and_plot_datasets(): # 1. make_blobs X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') plt.title(\\"make_blobs: Three normally-distributed clusters\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_blobs: This function generates isotropic Gaussian blobs for clustering. # It is useful for algorithms like K-means that are designed to identify spherical clusters. # 2. make_classification X_class, y_class = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, n_classes=3, random_state=1) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap=\'viridis\') plt.title(\\"make_classification: Three classes with two informative features\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_classification: This function generates a random n-class classification problem. # It creates clusters of points normally distributed (std=1) about vertices of an n-dimensional # hypercube and assigns an equal number of clusters to each class. # 3. make_circles X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=0) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\'viridis\') plt.title(\\"make_circles: Circular decision boundary\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_circles: This function generates a large circle containing a smaller circle in 2D. # A simple toy dataset to visualize classification algorithms. # 4. make_moons X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=0) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') plt.title(\\"make_moons: Interleaving half-circles\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() # Explanation of make_moons: This function generates a toy dataset of two interleaving half circles. # It is a simple binary classification dataset where a decision boundary would be non-linear. def run_bonus_classification_task(): from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Use the dataset generated from make_classification for the classification task. X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, n_classes=3, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) classifier = LogisticRegression(multi_class=\'multinomial\', max_iter=200) classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) print(f\\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\\") generate_and_plot_datasets() # Uncomment below line to run the bonus task # run_bonus_classification_task()"},{"question":"# Advanced SQLite Management in Python You are given the task to manage an SQLite database using Python\'s `sqlite3` module. This database must handle frequent insertion of records, ensure data integrity, and make use of both custom data adapters and converters. The following coding tasks will test your understanding of database creation, transaction control, data typing, and safe SQL execution. # Task Part 1: Database Setup 1. **Create a connection** to an in-memory SQLite database. 2. **Create a table** named `sensor_data` with the following columns: - `id`: Integer (Primary Key) - `date`: Text - `temperature`: Real - `humidity`: Real 3. Implement appropriate **transaction control** for the database operations. Part 2: Custom Data Types 4. Define a `SensorReading` class with `date`, `temperature`, and `humidity` attributes. 5. Write an adapter that converts a `SensorReading` instance into a tuple suitable for insertion into the database. 6. Write a converter that takes a string from the database and returns a `SensorReading` instance. Part 3: Data Insertion 7. **Insert multiple records** into the `sensor_data` table using `executemany()`. - Use placeholders to safely insert data. 8. Finally, **fetch all records** from the table and print them to verify the insertion. # Constraints - Ensure that all operations are done using safe SQL practices. - The database should be resilient to interruptions, and appropriate transaction rolls backs should be in place for error scenarios. - Utilize the custom data types (adapter and converter) efficiently. # Input/Output Format Input: - No direct input. Output: - Printed statements confirming the successful creation of the table. - List of all `SensorReading` records fetched from the database. # Sample Code Structure ```python import sqlite3 from datetime import datetime class SensorReading: def __init__(self, date, temperature, humidity): self.date = date self.temperature = temperature self.humidity = humidity def __repr__(self): return f\\"SensorReading(date={self.date}, temperature={self.temperature}, humidity={self.humidity})\\" def adapt_sensor_reading(reading): return (reading.date, reading.temperature, reading.humidity) def convert_sensor_reading(s): return SensorReading(*s.split(\\",\\")) # Register the adapter and converter sqlite3.register_adapter(SensorReading, adapt_sensor_reading) sqlite3.register_converter(\\"sensorreading\\", convert_sensor_reading) def setup_database(): # Implement database setup and transaction control here pass def insert_data(): # Implement data insertion here pass def fetch_data(): # Implement data fetching here pass def main(): setup_database() insert_data() data = fetch_data() for record in data: print(record) if __name__ == \\"__main__\\": main() ``` # Detailed Instructions Follow the sample code structure and complete the implementation of: 1. `setup_database()`: Initialize the database and create the table. 2. `insert_data()`: Insert multiple `SensorReading` instances. 3. `fetch_data()`: Retrieve data from the database and return it as `SensorReading` instances. Ensure to handle error conditions effectively and display appropriate confirmation messages for successful operations.","solution":"import sqlite3 from datetime import datetime class SensorReading: def __init__(self, date, temperature, humidity): self.date = date self.temperature = temperature self.humidity = humidity def __repr__(self): return f\\"SensorReading(date={self.date}, temperature={self.temperature}, humidity={self.humidity})\\" def adapt_sensor_reading(reading): return (reading.date, reading.temperature, reading.humidity) def convert_sensor_reading(s): date, temperature, humidity = s.decode().split(\\",\\") return SensorReading(date, float(temperature), float(humidity)) # Register the adapter and converter sqlite3.register_adapter(SensorReading, adapt_sensor_reading) sqlite3.register_converter(\\"sensorreading\\", convert_sensor_reading) def setup_database(): conn = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) conn.execute( CREATE TABLE sensor_data ( id INTEGER PRIMARY KEY, date TEXT, temperature REAL, humidity REAL ) ) conn.commit() return conn def insert_data(conn, data): with conn: conn.executemany( \\"INSERT INTO sensor_data (date, temperature, humidity) VALUES (?, ?, ?)\\", (adapt_sensor_reading(reading) for reading in data) ) def fetch_data(conn): cursor = conn.cursor() cursor.execute(\\"SELECT date, temperature, humidity FROM sensor_data\\") rows = cursor.fetchall() return [SensorReading(date, temperature, humidity) for date, temperature, humidity in rows] def main(): conn = setup_database() print(\\"Table created successfully.\\") data = [ SensorReading(\\"2023-10-10\\", 23.5, 50.0), SensorReading(\\"2023-10-11\\", 24.0, 60.0), SensorReading(\\"2023-10-12\\", 22.5, 55.0) ] insert_data(conn, data) print(\\"Data inserted successfully.\\") fetched_data = fetch_data(conn) for record in fetched_data: print(record) if __name__ == \\"__main__\\": main()"},{"question":"In this assessment, your task is to implement a simple PyTorch-based neural network module that uses causal attention bias. We will focus on creating an attention mechanism that applies a lower-right causal bias to the attention scores. Requirements: 1. Implement a class, `SimpleCausalAttention`, that: - Takes the input tensor and computes basic attention scores using matrix multiplication. - Applies a lower-right causal bias to these scores. - Returns the final attention-weighted outputs. 2. Your implementation should define the following: - An `__init__` method to initialize necessary components. - A `forward` method to perform the attention computation: - Compute raw attention scores. - Apply the `causal_lower_right()` function to these scores. - Calculate attention weights using a softmax function. - Compute the final output as the weighted sum of values. 3. **Input Format**: - The class should expect an input tensor `x` of shape `(batch_size, seq_len, d_model)`. - The model should initialize with two parameters: `d_model` (dimension of model/embedding) and `seq_len` (sequence length). 4. **Output Format**: - The forward method should return a tensor of the same shape as the input: `(batch_size, seq_len, d_model)`. 5. **Constraints**: - Use torch operations and functionality. - Assume that the input tensor is valid and properly formatted. - Ensure your implementation is efficient. 6. **Performance Requirements** (Optional): - Optimize the matrix operations to prevent unnecessary computations. # Hint: You may find the provided PyTorch functions and classes within `torch.nn.attention.bias` helpful for implementing causal bias. Example ```python import torch import torch.nn as nn from torch.nn.attention.bias import causal_lower_right class SimpleCausalAttention(nn.Module): def __init__(self, d_model, seq_len): super(SimpleCausalAttention, self).__init__() # Initialize necessary parameters and layers self.seq_len = seq_len self.d_model = d_model self.softmax = nn.Softmax(dim=-1) def forward(self, x): # Compute raw attention scores attn_scores = torch.matmul(x, x.transpose(-2, -1)) / (self.d_model ** 0.5) # Apply lower-right causal bias causal_bias = causal_lower_right(self.seq_len, self.seq_len) attn_scores = attn_scores + causal_bias # Compute attention weights attn_weights = self.softmax(attn_scores) # Calculate output output = torch.matmul(attn_weights, x) return output # Example usage d_model = 64 seq_len = 50 model = SimpleCausalAttention(d_model, seq_len) input_tensor = torch.randn(32, seq_len, d_model) # Example input: batch of size 32 output_tensor = model(input_tensor) print(output_tensor.shape) # Should return: torch.Size([32, 50, 64]) ``` **This example illustrates the expected structure and duties of your `SimpleCausalAttention` class but does not constitute a complete solution.**","solution":"import torch import torch.nn as nn def causal_lower_right(seq_len): Helper function to create a lower-right triangular bias tensor. bias = torch.tril(torch.ones(seq_len, seq_len)) return bias class SimpleCausalAttention(nn.Module): def __init__(self, d_model, seq_len): super(SimpleCausalAttention, self).__init__() self.seq_len = seq_len self.d_model = d_model self.softmax = nn.Softmax(dim=-1) def forward(self, x): batch_size, seq_len, d_model = x.size() assert seq_len == self.seq_len, \\"Input sequence length does not match the initialized sequence length\\" assert d_model == self.d_model, \\"Input feature dimension does not match the initialized feature dimension\\" # Compute raw attention scores attn_scores = torch.matmul(x, x.transpose(-2, -1)) / (self.d_model ** 0.5) # Apply lower-right causal bias causal_bias = causal_lower_right(seq_len).to(attn_scores.device) attn_scores = attn_scores.masked_fill(causal_bias == 0, float(\'-inf\')) # Compute attention weights attn_weights = self.softmax(attn_scores) # Calculate output output = torch.matmul(attn_weights, x) return output"},{"question":"# Advanced Coding Assessment: Custom Color Palettes with seaborn Objective: Assess your ability to customize and generate color palettes using the seaborn library and to integrate these palettes into advanced data visualizations. Problem Statement: You are provided with a dataset containing information about species of the Iris flower. Your task is to create a scatter plot using seaborn, where each species is represented using differing shades of a specific color. You must generate these colors using seaborn\'s dark_palette and ensure that the scatter plot is both informative and visually appealing. Dataset: You will use the Iris dataset provided by seaborn. Requirements: 1. Generate three different dark color palettes using seaborn\'s `dark_palette()` function for the unique species in the Iris dataset. - One palette should be based on a named color (e.g., \\"seagreen\\"). - One palette should use a hex code (e.g., \\"#79C\\"). - One palette should use the husl color system coordinates (e.g., `(20, 60, 50)`). 2. Create a scatter plot of `sepal_length` vs `sepal_width` where: - The points for each species are colored using the respective dark color palette. - Use a different palette for each species (setosa, versicolor, virginica). - Ensure that each palette has at least 5 shades. Expected Input and Output: - **Input:** - No external input from the user is required. - **Output:** - A single scatter plot showing the relationship between `sepal_length` and `sepal_width` with each species represented by a different color palette. Constraints: - Use only seaborn and matplotlib for plotting. - Ensure that the scatter plot has appropriate axis labels, a title, and a legend. Grading Criteria: - **Correctness:** Does the plot correctly use different palettes for each species? - **Readability:** Is the scatter plot easy to understand with well-labeled axis and legend? - **Aesthetics:** Is the color differentiation clear and visually appealing? Performance considerations: - The solution must efficiently handle the Iris dataset. Additional Notes: You might find the following seaborn functions useful: - `sns.dark_palette()` - `sns.scatterplot()` - `plt.legend()` Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # Define the palettes for each species palette_seagreen = sns.dark_palette(\\"seagreen\\", 5) palette_hex = sns.dark_palette(\\"#79C\\", 5) palette_husl = sns.dark_palette((20, 60, 50), 5, input=\\"husl\\") # Mapping species to their respective color palettes palette_map = { \'setosa\': palette_seagreen, \'versicolor\': palette_hex, \'virginica\': palette_husl } # Create the scatter plot plt.figure(figsize=(10, 6)) for species, palette in palette_map.items(): subset = iris[iris[\'species\'] == species] sns.scatterplot(data=subset, x=\'sepal_length\', y=\'sepal_width\', palette=palette, label=species) # Add labels and title plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Iris Sepal Dimensions Colored by Species with Dark Palettes\\") plt.legend(title=\\"Species\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_scatter_plot(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Define the palettes for each species palette_seagreen = sns.dark_palette(\\"seagreen\\", 5) palette_hex = sns.dark_palette(\\"#79C\\", 5) palette_husl = sns.dark_palette((20, 60, 50), 5, input=\\"husl\\") # Assign unique colors to species color_dict = { \'setosa\': palette_seagreen[2], \'versicolor\': palette_hex[2], \'virginica\': palette_husl[2] } # Create the scatter plot plt.figure(figsize=(10, 6)) for species, color in color_dict.items(): subset = iris[iris[\'species\'] == species] sns.scatterplot(data=subset, x=\'sepal_length\', y=\'sepal_width\', color=color, label=species) # Add labels and title plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Iris Sepal Dimensions Colored by Species with Dark Palettes\\") plt.legend(title=\\"Species\\") plt.show()"},{"question":"You are provided with a module named `sndhdr` which helps in determining the type of sound file and extracting basic metadata about the sound file. It contains two primary functions: - **sndhdr.what(filename)**: Determines the type of sound data stored in the file `filename`. - **sndhdr.whathdr(filename)**: Determines the type of sound data stored in a file based on the file header. Both functions return a namedtuple with the attributes: `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth`. If the type cannot be determined, `None` is returned. # Task Write a Python function `process_sound_files(file_list)` that: 1. Takes a list of file names `file_list`. 2. Uses the `sndhdr.what` function to determine the type of each sound file in the list. 3. Returns a dictionary where the keys are the file names and the values are dictionaries with the following keys: `filetype`, `framerate`, `nchannels`, `nframes`, `sampwidth`. If the `sndhdr.what` function returns `None` for any file, include an entry with the value set to `None` for that file. # Input - `file_list`: A list of strings where each string represents the name of a sound file. # Output - A dictionary with file names as keys and dictionaries (or `None`) as values. # Example ```python input_files = [\\"sound1.wav\\", \\"sound2.aifc\\", \\"sound3.voc\\"] output = process_sound_files(input_files) print(output) ``` Expected output (the actual values will depend on the sound files used): ```python { \\"sound1.wav\\": { \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 1200000, \\"sampwidth\\": 2 }, \\"sound2.aifc\\": { \\"filetype\\": \\"aifc\\", \\"framerate\\": 44100, \\"nchannels\\": 1, \\"nframes\\": 600000, \\"sampwidth\\": \'A\' }, \\"sound3.voc\\": None } ``` # Constraints - You can assume that all file names are valid and accessible in your working directory. - The files may not all be valid sound files, and some may not return metadata. # Note - Use the `sndhdr.what` function for determining the sound file type and metadata extraction.","solution":"import sndhdr def process_sound_files(file_list): Takes a list of file names and returns a dictionary where the keys are the file names and the values are dictionaries with file metadata (filetype, framerate, nchannels, nframes, sampwidth). If the sndhdr.what function returns None, the value for that file is None. result = {} for file in file_list: metadata = sndhdr.what(file) if metadata is not None: result[file] = { \\"filetype\\": metadata.filetype, \\"framerate\\": metadata.framerate, \\"nchannels\\": metadata.nchannels, \\"nframes\\": metadata.nframes, \\"sampwidth\\": metadata.sampwidth } else: result[file] = None return result"},{"question":"# Text Formatting and Manipulation with `textwrap` Module Objective Using the `textwrap` module from Python, you are required to create a custom function that reads a block of text and formats it according to specified parameters. Problem Statement Write a Python function `custom_format_text(text, width, mode, options)` that formats the input `text` based on the given `width`, `mode`, and `options`. The function should intelligently use the functionalities provided by the `textwrap` module to achieve the desired text formatting. Function Signature ```python def custom_format_text(text: str, width: int, mode: str, options: dict) -> str: ``` Parameters - `text` (str): The input text that needs formatting. - `width` (int): The maximum width for the formatted text. - `mode` (str): The mode of formatting. It can be one of the following: - `\\"wrap\\"`: Wraps the text and returns a list of lines. - `\\"fill\\"`: Wraps the text and returns a single string. - `\\"shorten\\"`: Truncates the text to fit within the width. - `\\"dedent\\"`: Removes common leading whitespace from text. - `\\"indent\\"`: Adds a prefix to the beginning of selected lines. - `options` (dict): Additional options for the formatting. Keys should match the keyword arguments of the respective `textwrap` functions or methods. Returns - `str`: The formatted text based on the specified mode. If the mode is `\\"wrap\\"`, return the text with each line separated by a newline (`n`). Constraints - The `width` should be a positive integer greater than 0. - The `mode` should be one of `\\"wrap\\"`, `\\"fill\\"`, `\\"shorten\\"`, `\\"dedent\\"`, or `\\"indent\\"`; otherwise, raise a `ValueError`. - The `options` dictionary can include valid `TextWrapper` attributes or method-specific options like `prefix` for `indent`. Example Usage ```python text = \\"This is a sample text that needs to be formatted using textwrap module. The module helps in managing line widths and other text formatting needs efficiently.\\" # Example 1: Wrap text custom_format_text(text, 40, \\"wrap\\", {}) # Output: # \\"This is a sample text that needs to benformatted using textwrap module. Thenmodule helps in managing line widthsnand other text formatting needsnefficiently.\\" # Example 2: Fill text custom_format_text(text, 40, \\"fill\\", {}) # Output: # \\"This is a sample text that needs to be formatted using textwrap module. The module helps in managing line widths and other text formatting needs efficiently.\\" # Example 3: Shorten text with placeholder options = {\'placeholder\': \'...\'} custom_format_text(text, 40, \\"shorten\\", options) # Output: \\"This is a sample text that needs to be...\\" # Example 4: Dedent text indented_text = \\" This is a line with leading whitespace.n This is another line.\\" custom_format_text(indented_text, 40, \\"dedent\\", {}) # Output: \\"This is a line with leading whitespace.nThis is another line.\\" # Example 5: Indent text options = {\'prefix\': \'> \'} custom_format_text(text, 40, \\"indent\\", options) # Output: # \\"> This is a sample text that needs to ben> formatted using textwrap module. Then> module helps in managing line widthsn> and other text formatting needsn> efficiently.\\" ``` # Notes - You may assume that the input `text` is correctly formatted and does not contain invalid characters. - Utilize the `textwrap.TextWrapper` class for customizable formatting where applicable. - Ensure your function handles all specified `modes` appropriately by leveraging the provided `textwrap` functionalities.","solution":"import textwrap def custom_format_text(text, width, mode, options): Formats the input text based on the given width, mode, and options. Parameters: - text (str): The input text that needs formatting. - width (int): The maximum width for the formatted text. - mode (str): The mode of formatting. - \\"wrap\\": Wraps the text and returns a list of lines. - \\"fill\\": Wraps the text and returns a single string. - \\"shorten\\": Truncates the text to fit within the width. - \\"dedent\\": Removes common leading whitespace from text. - \\"indent\\": Adds a prefix to the beginning of selected lines. - options (dict): Additional options for the formatting. Returns: - str: The formatted text based on the specified mode. If the mode is \\"wrap\\", return the text with each line separated by a newline (n). Raises: - ValueError: If the mode is not one of the specified modes. if mode not in {\\"wrap\\", \\"fill\\", \\"shorten\\", \\"dedent\\", \\"indent\\"}: raise ValueError(\\"Invalid mode provided. Must be one of \'wrap\', \'fill\', \'shorten\', \'dedent\', \'indent\'.\\") if width <= 0: raise ValueError(\\"Width must be a positive integer greater than 0.\\") if mode == \\"wrap\\": wrapped_lines = textwrap.wrap(text, width, **options) return \'n\'.join(wrapped_lines) elif mode == \\"fill\\": return textwrap.fill(text, width, **options) elif mode == \\"shorten\\": return textwrap.shorten(text, width, **options) elif mode == \\"dedent\\": return textwrap.dedent(text) elif mode == \\"indent\\": return textwrap.indent(text, **options)"},{"question":"**Coding Assessment Question** # Objective Design a Python function that can perform various queries on the Unix password database using the `pwd` module. # Problem Statement Write a function `query_password_db` that performs the following tasks: 1. Retrieve the full password database entry for a specific user, either by username or numeric user ID. 2. Find all users with a given shell. 3. Retrieve a list of all users whose home directory contains a specific string. # Function Signature ```python def query_password_db(user_query=None, uid_query=None, shell_query=None, home_dir_contains=None): pass ``` # Parameters - `user_query` (str or None): Username to query. If provided, the function should return the password database entry for this user. Default is `None`. - `uid_query` (int or None): User ID to query. If provided, the function should return the password database entry for this user. Default is `None`. - `shell_query` (str or None): Shell string to query. If provided, the function should return a list of usernames (login names) of users who use this shell. Default is `None`. - `home_dir_contains` (str or None): Substring to search for in home directories. If provided, the function should return a list of usernames whose home directory contains this substring. Default is `None`. # Output - If `user_query` is provided, return the corresponding password database entry. - If `uid_query` is provided, return the corresponding password database entry. - If `shell_query` is provided, return a list of usernames using this shell. - If `home_dir_contains` is provided, return a list of usernames whose home directory contains the specific substring. If multiple queries are provided, the function should handle them in the order above (i.e., `user_query` first, then `uid_query`, then `shell_query`, and finally `home_dir_contains`). # Constraints - If none of the parameters are provided, raise a `ValueError` with the message \\"At least one query parameter must be provided.\\" - Username and UID queries are mutually exclusive; if both are provided, raise a `ValueError` with the message \\"Provide either user_query or uid_query, not both.\\" - Ensure the function works efficiently for typical Unix systems. # Examples ```python # Example usage of the function: result = query_password_db(user_query=\\"exampleuser\\") print(result) result = query_password_db(shell_query=\\"/bin/bash\\") print(result) result = query_password_db(home_dir_contains=\\"home\\") print(result) ```","solution":"import pwd def query_password_db(user_query=None, uid_query=None, shell_query=None, home_dir_contains=None): Queries the Unix password database for specific information. Parameters: - user_query (str or None): Username to query. - uid_query (int or None): User ID to query. - shell_query (str or None): Shell string to query. - home_dir_contains (str or None): Substring to search for in home directories. Returns: - A Unix password database entry, or lists of usernames, based on the query. if user_query is not None and uid_query is not None: raise ValueError(\\"Provide either user_query or uid_query, not both.\\") if user_query is not None: try: return pwd.getpwnam(user_query) except KeyError: return None if uid_query is not None: try: return pwd.getpwuid(uid_query) except KeyError: return None if shell_query is not None: return [entry.pw_name for entry in pwd.getpwall() if entry.pw_shell == shell_query] if home_dir_contains is not None: return [entry.pw_name for entry in pwd.getpwall() if home_dir_contains in entry.pw_dir] raise ValueError(\\"At least one query parameter must be provided.\\")"},{"question":"Objective The aim of this task is to assess your understanding and proficiency with the seaborn package, particularly in creating and customizing histograms. Task 1. Load the `penguins` dataset from seaborn. 2. Create a subplot with two histograms: - The left subplot should display a histogram of `flipper_length_mm` with the following customizations: - Hue mapping based on the `species`. - Use kernel density estimation. - Apply a bin width of 3. - The right subplot should display a bivariate histogram comparing `bill_depth_mm` and `body_mass_g` with the following customizations: - Hue mapping based on the `species`. - Use a `log_scale` transform for both axes. - Include a color bar. 3. Save the resulting plot as `penguins_histograms.png`. Input There is no direct user input required. The task should be completed by writing a function that performs the specified operations. Output The output should be a plot saved as `penguins_histograms.png`. Constraints - Your function should be named `create_penguins_histograms()`. - Ensure to include necessary imports within your function. - The function should have no input parameters. - Use seaborn version supporting the described functionalities. Example The function call: ```python create_penguins_histograms() ``` should produce and save a `.png` image file with the described histograms. Notes - Use appropriate figure size settings to ensure clarity in subplots. - Title and label axes where necessary for better readability. - Ensure the code is well-documented with comments explaining each step. Performance - The function should execute within a reasonable time frame for loading the dataset and creating the plots. - Avoid unnecessary computations and optimize code if needed. ```python def create_penguins_histograms(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a figure and set of subplots fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # Left subplot: Univariate histogram for flipper length with hue, KDE and custom bin width sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, binwidth=3, ax=axes[0]) axes[0].set_title(\'Histogram of Flipper Length (with KDE)\') # Right subplot: Bivariate histogram with log scale and color bar sns.histplot(penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", log_scale=(True, True), cbar=True, ax=axes[1]) axes[1].set_title(\'Bivariate Histogram of Bill Depth and Body Mass\') # Adding some overall figure adjustments plt.tight_layout() # Save the plot to a file plt.savefig(\'penguins_histograms.png\') ```","solution":"def create_penguins_histograms(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a figure and set of subplots fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # Left subplot: Univariate histogram for flipper length with hue, KDE and custom bin width sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, binwidth=3, ax=axes[0]) axes[0].set_title(\'Histogram of Flipper Length (with KDE)\') axes[0].set_xlabel(\'Flipper Length (mm)\') axes[0].set_ylabel(\'Frequency\') # Right subplot: Bivariate histogram with log scale and color bar sns.histplot(penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", log_scale=(True, True), cbar=True, ax=axes[1]) axes[1].set_title(\'Bivariate Histogram of Bill Depth and Body Mass\') axes[1].set_xlabel(\'Bill Depth (mm)\') axes[1].set_ylabel(\'Body Mass (g)\') # Adding some overall figure adjustments plt.tight_layout() # Save the plot to a file plt.savefig(\'penguins_histograms.png\')"},{"question":"**Objective:** Design a color palette for visualizing a heat map of temperature data using the `seaborn` library. The palette should transition smoothly from cooler to warmer colors and should include the following customizations: 1. Five distinct colors. 2. Start the palette from a specific starting point in the color space. 3. Implement a reverse rotation in the color space. 4. Apply a non-linearity to the luminance ramp. 5. Modify the saturation of the colors. **Instructions:** 1. Write a Python function named `custom_cubehelix_palette` that generates a customized color palette using the `sns.cubehelix_palette` function from the seaborn library. 2. The function should accept the following input parameters: - `n_colors` (int): The number of colors in the palette. (1 <= n_colors <= 10) - `start` (float): The starting point in the color space. (0 <= start <= 3) - `rot` (float): The rotation in the color space. Use a negative value to reverse the direction. (-3 <= rot <= 3) - `gamma` (float): The gamma value for the luminance ramp. (0.1 <= gamma <= 2) - `hue` (float): The saturation of the colors. (0 <= hue <= 1) 3. The function should return a seaborn color palette object. **Input and Output Formats:** ```python def custom_cubehelix_palette(n_colors: int, start: float, rot: float, gamma: float, hue: float) -> List[Tuple[float]]: # Your implementation here ``` - **Input:** - `n_colors` (int): An integer specifying the number of colors in the palette (e.g., `5`). - `start` (float): A float specifying the starting point in the color space (e.g., `2.0`). - `rot` (float): A float specifying the rotation in the color space (e.g., `-0.5`). - `gamma` (float): A float specifying the gamma value for the luminance ramp (e.g., `0.8`). - `hue` (float): A float specifying the saturation of the colors (e.g., `0.5`). - **Output:** - A list of color specifications as tuples that can be used with seaborn plotting functions. **Constraints:** - The `n_colors` parameter must be between 1 and 10 inclusive. - The `start` parameter must be between 0 and 3 inclusive. - The `rot` parameter must be between -3 and 3 inclusive. - The `gamma` parameter must be between 0.1 and 2 inclusive. - The `hue` parameter must be between 0 and 1 inclusive. **Example usage:** ```python palette = custom_cubehelix_palette(n_colors=5, start=2.0, rot=-0.5, gamma=0.8, hue=0.5) print(palette) # Expected Output: # List of color tuples representing the palette designed with the specified parameters. ``` In addition to implementing the function, include a section with test cases that validate your implementation using various combinations of the input parameters.","solution":"import seaborn as sns def custom_cubehelix_palette(n_colors: int, start: float, rot: float, gamma: float, hue: float): Generates a customized seaborn cubehelix color palette. Parameters: - n_colors (int): The number of colors in the palette. (1 <= n_colors <= 10) - start (float): The starting point in the color space. (0 <= start <= 3) - rot (float): The rotation in the color space. Use a negative value to reverse the direction. (-3 <= rot <= 3) - gamma (float): The gamma value for the luminance ramp. (0.1 <= gamma <= 2) - hue (float): The saturation of the colors. (0 <= hue <= 1) Returns: - List of RGB tuples representing the palette. if not (1 <= n_colors <= 10): raise ValueError(\\"n_colors must be between 1 and 10 inclusive.\\") if not (0 <= start <= 3): raise ValueError(\\"start must be between 0 and 3 inclusive.\\") if not (-3 <= rot <= 3): raise ValueError(\\"rot must be between -3 and 3 inclusive.\\") if not (0.1 <= gamma <= 2): raise ValueError(\\"gamma must be between 0.1 and 2 inclusive.\\") if not (0 <= hue <= 1): raise ValueError(\\"hue must be between 0 and 1 inclusive.\\") palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, as_cmap=False) return palette"},{"question":"# Question: Implement a Type-Safe Generic Function to Calculate the Median Objective Write a Python function using type hints and generics from the `typing` module to calculate the median of a list of numbers. The function should be robust and handle different numeric types (integers, floats). Function Signature ```python from typing import TypeVar, List T = TypeVar(\'T\', int, float) def calculate_median(numbers: List[T]) -> float: # Function implementation here ``` Detailed Requirements 1. **Functionality**: - The function should take a list of numbers (integers or floats) and return the median as a float. - The median is the middle value in an ordered list of numbers. If the list has an even number of elements, the median is the average of the two middle numbers. 2. **Type Hints**: - Use the `TypeVar` and `List` from the `typing` module to ensure the function is generic and type-safe. - The input list can contain either integers or floats, and the median should always be returned as a float. 3. **Edge Cases**: - Ensure the function properly handles empty lists by raising a `ValueError`. - The function should be efficient, with a time complexity of O(n log n) due to sorting, which is necessary to find the median. Constraints - Do not use any external libraries for statistical calculations. Standard Python library functions and methods are allowed. - Ensure the function works correctly for both sorted and unsorted input lists. Example Usage ```python assert calculate_median([1, 2, 3]) == 2.0 assert calculate_median([1, 2, 3, 4]) == 2.5 assert calculate_median([2.5, 3.1, 1.2]) == 2.5 assert calculate_median([10]) == 10.0 try: calculate_median([]) except ValueError as e: assert str(e) == \\"The list is empty\\" ``` Implement the `calculate_median` function based on the above requirements. Ensure you apply type hints correctly and handle all specified edge cases.","solution":"from typing import TypeVar, List T = TypeVar(\'T\', int, float) def calculate_median(numbers: List[T]) -> float: if len(numbers) == 0: raise ValueError(\\"The list is empty\\") sorted_numbers = sorted(numbers) n = len(sorted_numbers) mid = n // 2 if n % 2 == 1: return float(sorted_numbers[mid]) else: return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2"},{"question":"Objective: Implement a function that evaluates and compares the performance of different machine learning models using various cross-validation techniques from `scikit-learn`. Task: Write a function `evaluate_models` that takes the following inputs: 1. `models`: A dictionary where keys are model names (strings) and values are scikit-learn model instances. 2. `X`: Feature matrix (ndarray). 3. `y`: Target vector (ndarray). 4. `cv_methods`: A dictionary where keys are names of cross-validation methods (strings) and values are instances of scikit-learn cross-validation iterators. 5. `scoring`: A string representing the scoring metric for evaluation (e.g., \'accuracy\', \'f1_macro\'). The function should return a dictionary where keys are the names of cross-validation methods and values are nested dictionaries. Each nested dictionary should have model names as keys and their corresponding average cross-validation scores as values. Constraints: - The function should handle at least the following cross-validation methods: `KFold`, `StratifiedKFold`, `GroupKFold`, and `TimeSeriesSplit`. - Ensure that your function runs in a reasonable time for datasets with up to 10,000 samples and 100 features. - Utilize appropriate scikit-learn utilities such as `cross_val_score` and `cross_validate` for performing cross-validation. Input: - `models`: Dictionary (e.g., `{\'svm\': SVC(), \'rf\': RandomForestClassifier()}`) - `X`: Numpy ndarray of shape (n_samples, n_features) - `y`: Numpy ndarray of shape (n_samples,) - `cv_methods`: Dictionary (e.g., `{\'KFold\': KFold(n_splits=5), \'StratifiedKFold\': StratifiedKFold(n_splits=5)}`) - `scoring`: String (e.g., \'accuracy\') Output: - Dictionary where each key is a cross-validation method name and value is another dictionary with model names as keys and average cross-validation scores as values. # Example: ```python from sklearn.datasets import load_iris from sklearn.model_selection import KFold, StratifiedKFold from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC # Load dataset X, y = load_iris(return_X_y=True) # Define models models = { \'RandomForest\': RandomForestClassifier(), \'SVM\': SVC(kernel=\'linear\') } # Define cross-validation methods cv_methods = { \'KFold\': KFold(n_splits=5), \'StratifiedKFold\': StratifiedKFold(n_splits=5) } # Scoring metric scoring = \'accuracy\' # Expected function call results = evaluate_models(models, X, y, cv_methods, scoring) # Expected output format # { # \'KFold\': { # \'RandomForest\': 0.9533333333333334, # \'SVM\': 0.9666666666666666 # }, # \'StratifiedKFold\': { # \'RandomForest\': 0.9600000000000002, # \'SVM\': 0.9733333333333334 # } # } ``` Implement the `evaluate_models` function.","solution":"from sklearn.model_selection import cross_val_score def evaluate_models(models, X, y, cv_methods, scoring): Evaluates and compares the performance of different models using various cross-validation methods. Parameters: - models: dictionary with model names as keys and scikit-learn model instances as values. - X: Feature matrix (ndarray). - y: Target vector (ndarray). - cv_methods: dictionary with cross-validation method names as keys and scikit-learn CV iterator instances as values. - scoring: string representing the scoring metric for evaluation. Returns: - results: dictionary where each key is a cross-validation method name and value is another dictionary with model names as keys and their corresponding average cross-validation scores as values. results = {} for cv_name, cv_instance in cv_methods.items(): results[cv_name] = {} for model_name, model in models.items(): scores = cross_val_score(model, X, y, cv=cv_instance, scoring=scoring) results[cv_name][model_name] = scores.mean() return results"},{"question":"Objective Create a PyTorch model that performs a specific task and ensure it can be successfully compiled with TorchScript. Your solution should demonstrate understanding of model creation, tensor operations, and handling unsupported constructs in TorchScript. Problem Statement You are required to implement a simple neural network model in PyTorch that performs a basic regression task. Your model must: - Include at least one convolutional layer and one fully connected layer. - Be constructed in a way that is compatible with TorchScript, avoiding or correctly handling any unsupported features. - Have a forward method that performs a forward pass and returns the output tensor. Once the model is created, you should script the model using TorchScript and demonstrate that it can be executed without errors. Requirements - Use `torch.nn` modules to define your network. - Include appropriate handling for unsupported TorchScript constructs if necessary. - Implement a function to demonstrate the scripted model. Constraints - The model should be simple enough to avoid excessive training time but complex enough to include a variety of PyTorch constructs. - Avoid or correctly handle the constructs and functions listed as unsupported in the provided documentation. Input and Output - **Input**: An input tensor of shape `(batch_size, channels, height, width)`. - **Output**: The output tensor from the model, shaped appropriately for a regression task. Example Here is a skeleton of the approach you should take: ```python import torch import torch.nn as nn import torch.jit class SimpleRegressionModel(nn.Module): def __init__(self): super(SimpleRegressionModel, self).__init__() # Define layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1) self.fc1 = nn.Linear(16 * height * width, 1) # Adjust input features as necessary def forward(self, x): x = self.conv1(x) x = torch.relu(x) x = torch.flatten(x, start_dim=1) x = self.fc1(x) return x # Input tensor example input_tensor = torch.randn(10, 3, height, width) # Model instantiation model = SimpleRegressionModel() # TorchScript scripting scripted_model = torch.jit.script(model) # Demonstrate execution output_tensor = scripted_model(input_tensor) print(output_tensor) ``` Notes 1. Adjust the `height` and `width` in the `fc1` layer of `SimpleRegressionModel` according to your input tensor shape. 2. Ensure compatibility with TorchScript by avoiding unsupported methods or properly handling them. 3. Test your model and the scripted version thoroughly. Submit your implementation along with a brief explanation of any TorchScript compatibility considerations you addressed.","solution":"import torch import torch.nn as nn import torch.jit class SimpleRegressionModel(nn.Module): def __init__(self, input_height, input_width): super(SimpleRegressionModel, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1) self.fc1 = nn.Linear(16 * input_height * input_width, 1) # Adjust input features based on conv output size def forward(self, x): x = self.conv1(x) x = torch.relu(x) x = torch.flatten(x, start_dim=1) x = self.fc1(x) return x def get_scripted_model(input_height, input_width): model = SimpleRegressionModel(input_height, input_width) scripted_model = torch.jit.script(model) return scripted_model"},{"question":"Context You are working on porting a Python 2.7 program to be compatible with Python 3. The program processes text and binary data from different sources, performs various operations, and outputs results. Problem Statement Create a Python function called `process_data` that processes input data which can be either text or binary. The function should: 1. Automatically detect whether the input data is text or binary. 2. For text data: - Convert all text to uppercase. 3. For binary data: - Treat the data as a sequence of bytes. - Reverse the order of the bytes. 4. Handle file inputs by reading from both text and binary files as specified by the mode in which they are opened. 5. Ensure compatibility with both Python 2.7 and Python 3. Function Signature ```python def process_data(data): Args: data: Can be either a string (text) or bytes (binary). In case of a string, it could be a file path from which data needs to be read. Returns: Depending on the input: - If input is text, return the text in uppercase. - If input is binary, return the reversed sequence of bytes. ``` Constraints - You should use feature detection instead of version detection to handle Python 2 and 3 differences. - Assume all string inputs are encoded in UTF-8. - For simplicity, you can assume the file will always be small enough to fit into memory. Example ```python # Example for Text text_input = \\"Hello, World!\\" assert process_data(text_input) == \\"HELLO, WORLD!\\" # Example for Binary binary_input = b\\"x01x02x03x04\\" assert process_data(binary_input) == b\\"x04x03x02x01\\" # Example for File Input (Text) with open(\'example.txt\', \'w\') as f: f.write(\\"Hello, World!\\") assert process_data(\'example.txt\') == \\"HELLO, WORLD!\\" # Example for File Input (Binary) with open(\'example.bin\', \'wb\') as f: f.write(b\\"x01x02x03x04\\") assert process_data(\'example.bin\') == b\\"x04x03x02x01\\" ``` Notes - You should handle both types of input within a single function. - Use `io.open` for opening files to ensure compatibility across Python versions. - Ensure your implementation adds compatibility with both Python 2.7 and Python 3.","solution":"import io def process_data(data): Args: data: Can be either a string (text or file path) or bytes (binary). Returns: - If input is text, return the text in uppercase. - If input is binary, return the reversed sequence of bytes. # If input is a file path, determine if it\'s a text file or binary file if isinstance(data, str): if data.endswith(\'.txt\'): with io.open(data, \'r\', encoding=\'utf-8\') as f: data = f.read() elif data.endswith(\'.bin\'): with io.open(data, \'rb\') as f: data = f.read() # Process text and binary data accordingly if isinstance(data, str): return data.upper() elif isinstance(data, bytes): return data[::-1] else: raise ValueError(\'Unsupported data type\')"},{"question":"# Advanced Seaborn Visualization Task Problem Statement You are tasked with analyzing a dataset related to customer tips at a restaurant. Using the seaborn library, you need to create visualizations that will help you to compare the distributions of total bills and tips based on categorical variables like `day`, `sex`, and `time` of the day. Furthermore, you\'ll extend this to multidimensional relationships and faceted plots. Dataset You will be using the `tips` dataset available in seaborn, which contains information on: - `total_bill`: Total bill amount in dollars. - `tip`: Tip amount in dollars. - `sex`: Gender of the person paying the bill (Male/Female). - `smoker`: Whether the person is a smoker or not (Yes/No). - `day`: Day of the week (Thur, Fri, Sat, Sun). - `time`: Time of day (Lunch, Dinner). - `size`: Number of people in the group. Tasks 1. **Univariate Strip Plot**: - Create a strip plot to show the distribution of `total_bill` amounts along with points jittered. 2. **Bivariate Strip Plot**: - Create a strip plot to compare the distribution of `total_bill` across different days of the week. 3. **Hue Parameter**: - Enhance the bivariate strip plot by adding a `hue` parameter set to `sex` to differentiate between male and female customers. 4. **Faceted Plot**: - Using `catplot`, create a faceted plot showing the relationship between `time`, `total_bill`, and `sex` across different days of the week. Implementation Details - Ensure that the plots have appropriate labels and legends for clarity. - For the faceted plot, set the aspect ratio to `0.5` for better visualization. - Use different `palette` options to improve the visual appeal of your plots. Expected Output - Four seaborn plots corresponding to each of the tasks described. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Univariate Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill Amount\\") plt.xlabel(\\"Total Bill\\") plt.show() # Task 2: Bivariate Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill Distribution Across Days of the Week\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day of the Week\\") plt.show() # Task 3: Hue Parameter plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Total Bill Distribution by Day and Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day of the Week\\") plt.legend(title=\\"Sex\\") plt.show() # Task 4: Faceted Plot g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.fig.suptitle(\\"Faceted Plot of Total Bill by Time and Sex across Days\\") g.set_axis_labels(\\"Time of Day\\", \\"Total Bill\\") g.add_legend(title=\\"Sex\\") plt.show() ``` Constraints - Use seaborn version 0.12 or higher. - Ensure your code is well-commented for readability. Evaluation Criteria - Correct implementation of each visualization task. - Proper use of seaborn functions and parameters. - Clarity and readability of the plots. - Adherence to the constraints.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Univariate Strip Plot def plot_univariate_strip_plot(data): plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\\"total_bill\\", jitter=True) plt.title(\\"Distribution of Total Bill Amount\\") plt.xlabel(\\"Total Bill\\") plt.show() # Task 2: Bivariate Strip Plot def plot_bivariate_strip_plot(data): plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\\"total_bill\\", y=\\"day\\", jitter=True) plt.title(\\"Total Bill Distribution Across Days of the Week\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day of the Week\\") plt.show() # Task 3: Hue Parameter def plot_bivariate_strip_plot_with_hue(data): plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=True) plt.title(\\"Total Bill Distribution by Day and Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day of the Week\\") plt.legend(title=\\"Sex\\") plt.show() # Task 4: Faceted Plot def plot_faceted_plot(data): g = sns.catplot(data=data, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.fig.suptitle(\\"Faceted Plot of Total Bill by Time and Sex across Days\\", y=1.02) g.set_axis_labels(\\"Time of Day\\", \\"Total Bill\\") g.add_legend(title=\\"Sex\\") plt.show() # Execute the plotting functions plot_univariate_strip_plot(tips) plot_bivariate_strip_plot(tips) plot_bivariate_strip_plot_with_hue(tips) plot_faceted_plot(tips)"},{"question":"# Question: You are given a small Python project consisting of a `FileManager` class that writes and reads data to and from a file. Write unit tests using the `unittest` framework to test the functionality of the `FileManager` class. Your tests should include setups and teardowns, and various assertion methods. Also, include some test cases that are expected to fail and should be marked as such. The `FileManager` class is given below: ```python class FileManager: def __init__(self, filename): self.filename = filename def write_data(self, data): with open(self.filename, \'w\') as file: file.write(data) def read_data(self): with open(self.filename, \'r\') as file: return file.read() ``` # Requirements: 1. Create a test suite including multiple test cases for the `FileManager` class. 2. Use `setUp()` and `tearDown()` methods to handle the creation and cleanup of a temporary file used in testing. 3. Write tests to verify: - Writing data to the file. - Reading data from the file. - Reading from a non-existent file should raise an appropriate exception. 4. Mark certain tests expecting them to fail. 5. Organize the tests into a test suite and run them using `unittest.TextTestRunner`. # Answer Template: ```python import unittest import os class FileManager: def __init__(self, filename): self.filename = filename def write_data(self, data): with open(self.filename, \'w\') as file: file.write(data) def read_data(self): with open(self.filename, \'r\') as file: return file.read() class TestFileManager(unittest.TestCase): def setUp(self): self.filename = \'tempfile.txt\' self.file_manager = FileManager(self.filename) def tearDown(self): try: os.remove(self.filename) except FileNotFoundError: pass def test_write_data(self): self.file_manager.write_data(\'Hello, World!\') with open(self.filename, \'r\') as file: data = file.read() self.assertEqual(data, \'Hello, World!\') def test_read_data(self): with open(self.filename, \'w\') as file: file.write(\'Hello, World!\') data = self.file_manager.read_data() self.assertEqual(data, \'Hello, World!\') def test_read_non_existent_file(self): os.remove(self.filename) with self.assertRaises(FileNotFoundError): self.file_manager.read_data() @unittest.expectedFailure def test_expected_failure(self): self.file_manager.write_data(\'Hello, World!\') self.assertEqual(self.file_manager.read_data(), \'Goodbye, World!\') if __name__ == \'__main__\': unittest.main(verbosity=2) ``` In this task, you will be assessed on: - Correctly setting up and tearing down test fixtures. - Thoroughly testing the functionalities of the `FileManager` class. - Using appropriate assertion methods. - Implementation of `unittest` features including skipping tests and expected failures. - Proper test case and suite organization.","solution":"import os class FileManager: def __init__(self, filename): self.filename = filename def write_data(self, data): with open(self.filename, \'w\') as file: file.write(data) def read_data(self): with open(self.filename, \'r\') as file: return file.read()"},{"question":"Problem Statement: You are given a list of tasks and their dependencies which form a directed acyclic graph (DAG). Your task is to implement a Python function `find_task_order` that determines a valid topological order for completing the tasks. If the input graph contains cycles, your function should return a specific error message indicating a cycle was detected. Class and Exception Specification: Use the `TopologicalSorter` class from the `graphlib` module to implement the solution. Function Signature: ```python def find_task_order(tasks: dict) -> list: Determines a valid topological order of tasks or detects cycles. Args: - tasks (dict): A dictionary where the keys are task identifiers (hashable objects) and the values are sets of predecessors. Returns: - list: A list representing a valid topological ordering of the tasks. Raises: - ValueError: If the input graph contains a cycle. ``` Input: - `tasks` (dict): A dictionary representing the tasks and their dependencies. Keys are task identifiers and values are sets of predecessors. Example: ```python { \\"task4\\": {\\"task2\\", \\"task3\\"}, \\"task2\\": {\\"task1\\"}, \\"task3\\": {\\"task1\\"}, \\"task1\\": set() } ``` Output: - `list`: A list of tasks in a valid topological order, if no cycle is detected. Example Output: ```python [\'task1\', \'task2\', \'task3\', \'task4\'] ``` Constraints: - Task identifiers and their dependencies must be hashable. - The graph represented by the tasks and their dependencies must be a directed graph. - Throw a `ValueError` with the message `\\"Cycle detected\\"` if a cycle exists in the graph. Example Usage: ```python try: tasks = { \\"task4\\": {\\"task2\\", \\"task3\\"}, \\"task2\\": {\\"task1\\"}, \\"task3\\": {\\"task1\\"}, \\"task1\\": set() } order = find_task_order(tasks) print(order) except ValueError as e: print(e) ``` In the above example, the output should be a valid topological order of tasks, such as `[\'task1\', \'task2\', \'task3\', \'task4\']`. If the input contains a cycle, for example: ```python tasks_with_cycle = { \\"task4\\": {\\"task2\\", \\"task3\\"}, \\"task2\\": {\\"task1\\"}, \\"task3\\": {\\"task1\\", \\"task4\\"}, \\"task1\\": set() } ``` The `find_task_order` function should raise a `ValueError` with the message `\\"Cycle detected\\"`. Test your function to ensure it handles both acyclic and cyclic graphs correctly.","solution":"from graphlib import TopologicalSorter def find_task_order(tasks: dict) -> list: Determines a valid topological order of tasks or detects cycles. Args: - tasks (dict): A dictionary where the keys are task identifiers (hashable objects) and the values are sets of predecessors. Returns: - list: A list representing a valid topological ordering of the tasks. Raises: - ValueError: If the input graph contains a cycle. try: ts = TopologicalSorter(tasks) order = list(ts.static_order()) return order except: raise ValueError(\\"Cycle detected\\")"},{"question":"Objective Design a Python function that uses regular expressions to parse and transform a given text. This will test your understanding of the \\"re\\" module and your ability to apply regular expressions for string parsing and manipulation tasks. Problem Statement You are given a text file containing various lines, each representing different data entries. Each line can contain multiple dates in \\"YYYY-MM-DD\\" format. Your task is to write a Python function `format_dates` that takes the path to this text file as input, processes the file line by line, extracts all the dates and transforms them into a different format \\"DD-MM-YYYY\\", and then writes the transformed lines to a new file. Function Signature ```python def format_dates(input_filepath: str, output_filepath: str) -> None: Transforms all dates in the input file from \'YYYY-MM-DD\' to \'DD-MM-YYYY\' and writes the result to the output file. Parameters: - input_filepath (str): The path to the input text file containing dates in \'YYYY-MM-DD\' format. - output_filepath (str): The path to the output text file where the transformed data will be saved. pass ``` Requirements 1. **Input Format**: - A text file where each line contains multiple entries, with dates in \\"YYYY-MM-DD\\" format. 2. **Output Format**: - A text file with the same content as the input file, but with all dates transformed to \\"DD-MM-YYYY\\" format. 3. **Constraints**: - The input file can be large, so the function should efficiently handle large files. - Use regular expressions to find and replace the dates in the specified format. 4. **Example**: - Given an input file with the following contents: ``` Product release date: 2023-10-05, available till: 2023-12-25. Last updated: 2022-08-17, next update: 2023-01-01. ``` - The function should produce an output file with the following contents: ``` Product release date: 05-10-2023, available till: 25-12-2023. Last updated: 17-08-2022, next update: 01-01-2023. ``` Additional Notes: - You can assume the input file exists and is readable. - Use the \\"re\\" module to utilize regular expressions for identifying and transforming the date patterns.","solution":"import re def format_dates(input_filepath: str, output_filepath: str) -> None: Transforms all dates in the input file from \'YYYY-MM-DD\' to \'DD-MM-YYYY\' and writes the result to the output file. Parameters: - input_filepath (str): The path to the input text file containing dates in \'YYYY-MM-DD\' format. - output_filepath (str): The path to the output text file where the transformed data will be saved. date_pattern = re.compile(r\'(d{4})-(d{2})-(d{2})\') with open(input_filepath, \'r\') as infile, open(output_filepath, \'w\') as outfile: for line in infile: transformed_line = date_pattern.sub(r\'3-2-1\', line) outfile.write(transformed_line)"},{"question":"Objective: Design and implement a function using the `gzip` module to compress and decompress data, demonstrating your understanding of the module\'s functionalities and various options. Problem Statement: You are given a list of strings that need to be saved efficiently to disk. Design two functions: `compress_and_save` and `load_and_decompress` to handle the compression and decompression tasks respectively. 1. **compress_and_save(filename: str, data: list[str], compresslevel: int = 9) -> None** - This function should: - Take a filename for the gzip file, a list of strings to compress, and an optional compression level (with a default value of 9). - Compress the list of strings and save them into the specified gzip file. 2. **load_and_decompress(filename: str) -> list[str]** - This function should: - Take a filename for the gzip file. - Load the compressed file and decompress it. - Return the decompressed data as a list of strings. Input and Output Formats: 1. **compress_and_save** - **Input:** - `filename`: a string representing the path where the gzip file will be saved. - `data`: a list of strings that need to be compressed. - `compresslevel`: an integer representing the compression level (optional, default is 9). - **Output:** - None 2. **load_and_decompress** - **Input:** - `filename`: a string representing the path of the gzip file to load. - **Output:** - A list of strings representing the decompressed data. Constraints: - You may assume that the list of strings will not be empty. - The function should handle typical file I/O errors gracefully, such as missing files or permission issues. - The function should efficiently handle reasonably large files (up to hundreds of MBs). Example: ```python data = [\\"First line.\\", \\"Second line.\\", \\"Third line.\\"] compress_and_save(\\"/tmp/testfile.gz\\", data, compresslevel=5) loaded_data = load_and_decompress(\\"/tmp/testfile.gz\\") print(loaded_data) # Output: [\\"First line.\\", \\"Second line.\\", \\"Third line.\\"] ``` Additional Information: - Use the `gzip` module functions and classes appropriately. - Ensure that the compressed file can be opened and read correctly using `gzip.open`.","solution":"import gzip import os def compress_and_save(filename: str, data: list[str], compresslevel: int = 9) -> None: Compress and save a list of strings to a gzip file. Parameters: - filename: str - the name of the file to save the compressed data. - data: list[str] - a list of strings to compress. - compresslevel: int - the compression level (default is 9). try: with gzip.open(filename, \'wt\', compresslevel=compresslevel, encoding=\'utf-8\') as f: for line in data: f.write(line + \'n\') except Exception as e: print(f\\"An error occurred while compressing and saving data to {filename}: {e}\\") def load_and_decompress(filename: str) -> list[str]: Load and decompress data from a gzip file. Parameters: - filename: str - the name of the gzip file to load. Returns: - list[str] - the decompressed list of strings. try: with gzip.open(filename, \'rt\', encoding=\'utf-8\') as f: return [line.strip() for line in f] except Exception as e: print(f\\"An error occurred while loading and decompressing data from {filename}: {e}\\") return []"},{"question":"<|Analysis Begin|> The documentation provided is for the `torch.cond` function in PyTorch. This function allows for conditional execution of code depending on a predicate, which can be either a boolean value or a tensor. The `torch.cond` function takes four arguments: a predicate, a true function, a false function, and a tuple of operands to pass to these functions. Two main use cases are demonstrated: one based on the shape of the input tensor and the other based on the data within the tensor. The documentation provides examples and details on how this function maintains certain invariants and how it lowers to a higher order conditional operator for flexibility in model design. <|Analysis End|> <|Question Begin|> # PyTorch `torch.cond` Function Coding Assessment **Objective:** To assess your understanding of the `torch.cond` function and your ability to implement a model that utilizes this function for data-dependent control flows. **Problem Statement:** Implement a PyTorch model that uses `torch.cond` to switch between two different processing pipelines based on the sum of the elements of an input tensor. Specifically: 1. If the sum of elements in the input tensor `x` is greater than 10, apply a processing pipeline that doubles each element of `x` and computes the natural logarithm of the result. 2. If the sum of elements in the input tensor `x` is less than or equal to 10, apply a processing pipeline that squares each element of `x` and then computes the square root of the result. You should define this as a PyTorch `nn.Module` class named `SumBasedCondModel`. **Function Signature:** ```python class SumBasedCondModel(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `x` (torch.Tensor): A 1-dimensional tensor with floating-point numbers. **Output:** - A tensor of the same shape as input `x`, processed according to the specified pipelines depending on the sum of its elements. **Constraints:** - You should only use the `torch.cond` for the conditional logic. - Do not use any explicit if-else or conditional branching other than the `torch.cond`. - Assume `x` will always be a 1-dimensional tensor of floating-point numbers. **Example:** Here are examples to test the expected behavior: ```python import torch model = SumBasedCondModel() inp1 = torch.tensor([1.0, 2.0, 3.0, 4.0]) inp2 = torch.tensor([5.0, 4.0, 3.0, 2.0]) # Example 1: Sum of elements is 10 (<= 10), should square and square root result1 = model(inp1) print(torch.equal(result1, torch.tensor([1.0, 2.0, 3.0, 4.0]))) # should print: True # Example 2: Sum of elements is 14 (>10), should double and take log result2 = model(inp2) expected_result2 = torch.log(torch.tensor([10.0, 8.0, 6.0, 4.0])) print(torch.allclose(result2, expected_result2)) # should print: True ``` **Implementation Notes:** - Use the example provided in the documentation to guide your solutions\' structure, particularly how true and false function implementations are passed to `torch.cond`. - The key challenge here is correctly implementing the function conditioning based on the sum of the tensor elements. Good luck!","solution":"import torch import torch.nn as nn class SumBasedCondModel(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: sum_x = torch.sum(x) def true_fn(x): return torch.log(2.0 * x) def false_fn(x): return torch.sqrt(x * x) result = torch.cond(sum_x > 10, true_fn, false_fn, (x,)) return result"},{"question":"# Boolean Handling Functor Objective: Implement a custom Boolean handler class in Python that mimics some of the C-level functionality described in the provided documentation. Problem Statement: Create a class called `BoolHandler` that manages Boolean values and provides utility functions similar to the macros and functions available at the C level in Python. Requirements: 1. **`BoolHandler` class**: This class should encapsulate Boolean handling. 2. **Methods**: - `is_bool(self, obj)`: Returns `True` if the provided object is a Boolean. Uses Python\'s built-in function `isinstance`. - `get_true(self)`: Returns the singleton `True`. - `get_false(self)`: Returns the singleton `False`. - `from_long(self, value)`: Takes an integer and returns `True` for non-zero integers and `False` for zero, mimicking `PyBool_FromLong`. 3. **Constraints**: - You cannot use any external libraries. - The solution should be efficient, leveraging native Python. Example Usage: ```python handler = BoolHandler() # Checking if an object is a boolean assert handler.is_bool(True) == True assert handler.is_bool(1) == False # Retrieving the singleton booleans assert handler.get_true() == True assert handler.get_false() == False # Creating booleans from integer values assert handler.from_long(10) == True assert handler.from_long(0) == False ``` Notes: - Ensure to implement reference counting internally if applicable, although in Python, this is implicitly handled by its garbage collection. - Add appropriate docstrings to your methods, explaining their functionality. Performance: - The operations should perform in constant time `O(1)`. Implement your solution below: ```python class BoolHandler: def is_bool(self, obj): pass def get_true(self): pass def get_false(self): pass def from_long(self, value): pass ```","solution":"class BoolHandler: def is_bool(self, obj): Returns True if the provided object is a Boolean, otherwise False. return isinstance(obj, bool) def get_true(self): Returns the singleton True. return True def get_false(self): Returns the singleton False. return False def from_long(self, value): Takes an integer and returns True for non-zero integers and False for zero. return bool(value)"},{"question":"**Question: Implement a Python class for memory-managed objects using reference counting mechanisms.** # Objective Write a Python class that mimics basic reference counting behavior for managing object lifecycles. This will show your understanding of the concepts covered by the `Python310` documentation on reference counting. # Requirements 1. **Class Definition** - Define a class `RefCountedObject` that includes the following: - An attribute `value` to hold any object. - An attribute `_ref_count` to keep track of the reference count. 2. **Class Methods** - `__init__(self, value)`: Initialize the object with `value` and set the reference count to `1`. - `inc_ref(self)`: Increment the reference count. - `dec_ref(self)`: Decrement the reference count, and if the count reaches zero, delete the object. - `get_ref_count(self)`: Return the current reference count. - `get_value(self)`: Return the stored value. 3. **Behavior** - Ensure that the object is properly managed with reference increments and decrements. - Simulate memory deallocation by including a print statement inside `dec_ref` when the reference count reaches zero (e.g., `\\"Object deallocated\\"`). # Input & Output Formats Input - The class will be instantiated and manipulated via methods. Output - There is no direct input/output to implement as the operations are performed on method calls internally. # Example Usage ```python # Creating object obj = RefCountedObject(10) print(obj.get_value()) # Output: 10 print(obj.get_ref_count()) # Output: 1 # Incrementing reference count obj.inc_ref() print(obj.get_ref_count()) # Output: 2 # Decrementing reference count obj.dec_ref() print(obj.get_ref_count()) # Output: 1 # Decrementing reference count to zero, causing deallocation obj.dec_ref() # Output: \\"Object deallocated\\" ``` # Constraints - Ensure that no negative reference counts are possible. - Handle edge cases where methods might be called redundantly. # Performance - The `inc_ref` and `dec_ref` operations should have a constant time complexity, O(1). Implementing this class correctly will demonstrate an understanding of the basics of reference counting and memory management in Python.","solution":"class RefCountedObject: def __init__(self, value): Initialize the RefCountedObject with the given value and set the reference count to 1. self.value = value self._ref_count = 1 def inc_ref(self): Increment the reference count by 1. self._ref_count += 1 def dec_ref(self): Decrement the reference count by 1. If the reference count reaches zero, print a deallocation message. if self._ref_count > 0: self._ref_count -= 1 if self._ref_count == 0: print(\\"Object deallocated\\") def get_ref_count(self): Return the current reference count. return self._ref_count def get_value(self): Return the stored value. return self.value"},{"question":"# Advanced Coding Assessment **Objective:** Demonstrate your understanding of the `secrets` module by implementing a function that generates secure user authentication tokens and performs secure comparisons. **Task:** Implement a function `generate_and_compare_tokens` that performs the following: 1. Generates a secure user authentication token. 2. Compares the generated token with a provided test token to check for equality. The function should: - Generate a secure token of 32 bytes using `secrets.token_bytes`. - Convert the token to a hexadecimal string using `secrets.token_hex`. - Return both the token and a boolean result indicating whether the generated token matches a given test token using `secrets.compare_digest`. **Function Signature:** ```python def generate_and_compare_tokens(test_token: str) -> (str, bool): pass ``` **Input:** - `test_token` (str): The token to compare with the generated token. This token is assumed to be a valid hexadecimal string. **Output:** - A tuple containing: - `generated_token` (str): The generated secure token in hexadecimal format. - `is_equal` (bool): True if the generated token matches the `test_token` using a constant-time comparison. **Example Usage:** ```python test_token = \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' generated_token, is_equal = generate_and_compare_tokens(test_token) print(\\"Generated Token:\\", generated_token) print(\\"Tokens Match:\\", is_equal) ``` **Constraints and Assumptions:** - The `test_token` will always be a valid hexadecimal string. - Use the defaults for the number of bytes when generating the token. - Ensure the comparison uses constant-time to prevent timing attacks. This question will assess your understanding of securely generating tokens and performing constant-time comparisons using the `secrets` module. Ensure your implementation is efficient and follows best practices for security.","solution":"import secrets def generate_and_compare_tokens(test_token: str) -> (str, bool): Generates a secure authentication token and compares it with a provided test token. Args: test_token (str): The token to compare with the generated token. Returns: (str, bool): A tuple containing the generated token in hexadecimal format and a boolean indicating if the generated token matches the test token. generated_token_bytes = secrets.token_bytes(32) generated_token = secrets.token_hex(32) is_equal = secrets.compare_digest(generated_token, test_token) return generated_token, is_equal"},{"question":"**Objective:** To assess students\' understanding of Python\'s `select` module and their ability to implement efficient I/O multiplexing. # Problem Description Suppose you are tasked with creating a small server that can handle multiple client connections simultaneously. Write a Python program that uses the `select` module to manage multiple socket connections. # Requirements 1. **Set Up a Server:** - Create a non-blocking TCP server that listens on `localhost` at port `12345`. - Accept multiple incoming client connections. 2. **Handle I/O with `select()`:** - Use the `select.select()` function to monitor multiple sockets for incoming data. - When there is incoming data, read it from the client socket and send back an acknowledgment message `\\"Received: <data>\\"` to the client. - Cleanly handle client disconnections and server shutdowns by closing the appropriate sockets. 3. **Input and Output Formats:** - The server should print logs to the console to indicate the following events: - New connection accepted. - Data received from a client. - Acknowledgment sent to a client. - Client disconnected. # Constraints 1. **Concurrency:** There is no limit to the number of clients connecting, but the server should be able to handle at least 5 simultaneous connections. 2. **Error Handling:** Properly handle cases where sockets might close unexpectedly. 3. **Timeouts:** The `select.select()` method should use a timeout to avoid indefinite blocking. # Example **Server Code Skeleton:** ```python import socket import select def run_server(): # Create and set up the server socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(0) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) # List of sockets to monitor for incoming & outgoing data sockets_list = [server_socket] print(\\"Server started on port 12345\\") try: while True: readable, writable, exceptional = select.select(sockets_list, [], sockets_list, 1) for s in readable: if s is server_socket: # Handle new connection client_socket, client_address = s.accept() print(f\\"New connection from {client_address}\\") client_socket.setblocking(0) sockets_list.append(client_socket) else: # Handle incoming data data = s.recv(1024) if data: print(f\\"Received data from {s.getpeername()}: {data.decode()}\\") s.send(f\\"Received: {data.decode()}\\".encode()) else: # Handle disconnection print(f\\"Client {s.getpeername()} disconnected\\") sockets_list.remove(s) s.close() for s in exceptional: print(f\\"Exceptional condition on {s.getpeername()}\\") sockets_list.remove(s) s.close() finally: server_socket.close() ``` # Deliverables - Complete the provided code skeleton to include the necessary logic for handling multiple client connections using `select.select()`. - Ensure the server handles incoming connections, reads data, sends acknowledgments, handles client disconnections, and prints appropriate logs. - Submit the complete Python script file named `multi_client_server.py`. **Note:** Make sure to test your server with multiple clients to verify that it handles concurrent connections and I/O efficiently.","solution":"import socket import select def run_server(): # Create and set up the server socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(0) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) # List of sockets to monitor for incoming & outgoing data sockets_list = [server_socket] print(\\"Server started on port 12345\\") try: while True: readable, writable, exceptional = select.select(sockets_list, [], sockets_list, 1) for s in readable: if s is server_socket: # Handle new connection client_socket, client_address = s.accept() print(f\\"New connection from {client_address}\\") client_socket.setblocking(0) sockets_list.append(client_socket) else: # Handle incoming data data = s.recv(1024) if data: print(f\\"Received data from {s.getpeername()}: {data.decode()}\\") s.send(f\\"Received: {data.decode()}\\".encode()) print(f\\"Acknowledgment sent to {s.getpeername()}\\") else: # Handle disconnection print(f\\"Client {s.getpeername()} disconnected\\") sockets_list.remove(s) s.close() for s in exceptional: print(f\\"Exceptional condition on {s.getpeername()}\\") sockets_list.remove(s) s.close() finally: server_socket.close() if __name__ == \\"__main__\\": run_server()"},{"question":"**Coding Question** # Requirements You are tasked with implementing a function that deeply merges two dictionary objects using the provided Python C API functions. The function should recursively merge the dictionaries\' key-value pairs such that: 1. If the same key exists in both dictionaries, and the values are also dictionaries, the function should recursively merge these nested dictionaries. 2. If the same key exists in both dictionaries but the values are not dictionaries, the value from the second dictionary should override the value from the first dictionary. 3. If the key exists only in one dictionary, the function should include that key-value pair in the result. # Function Signature ```python def deep_merge(dict1: dict, dict2: dict) -> dict: Deeply merge two dictionaries. Parameters: - dict1 (dict): The first input dictionary. - dict2 (dict): The second input dictionary. Returns: - dict: The deeply merged dictionary. ``` # Input - You are given two dictionaries `dict1` and `dict2`. Each dictionary can contain nested dictionaries as values. - Size of each dictionary: up to 10<sup>3</sup> key-value pairs. - Depth of the nested dictionaries: up to 10 levels. # Output - Return a dictionary representing the deeply merged result of `dict1` and `dict2`. # Constraints - Keys in dictionaries are always strings. - Values can be either another dictionary or a primitive data type (int, float, string, bool, etc.). # Example ```python dict_a = { \\"common\\": { \\"a\\": 1, \\"nested\\": { \\"b\\": 2 } }, \\"unique_a\\": 3 } dict_b = { \\"common\\": { \\"nested\\": { \\"b\\": 3, \\"c\\": 4 }, \\"d\\": 5 }, \\"unique_b\\": 6 } merged_dict = deep_merge(dict_a, dict_b) print(merged_dict) # Output: { \\"common\\": { \\"a\\": 1, \\"nested\\": { \\"b\\": 3, \\"c\\": 4 }, \\"d\\": 5 }, \\"unique_a\\": 3, \\"unique_b\\": 6 } ``` # Notes - Ensure you are correctly managing memory and references as per the provided C API documentation. - The implementation should aim for clarity and robustness, assuming that the input types are well-formed as per the constraints.","solution":"def deep_merge(dict1, dict2): Deeply merge two dictionaries. Parameters: - dict1 (dict): The first input dictionary. - dict2 (dict): The second input dictionary. Returns: - dict: The deeply merged dictionary. merged = dict1.copy() for key, value in dict2.items(): if key in merged: if isinstance(merged[key], dict) and isinstance(value, dict): # If both values are dictionaries, recurse into them merged[key] = deep_merge(merged[key], value) else: # If values are not dictionaries, the value from dict2 overrides dict1 merged[key] = value else: # If the key is only in dict2, add it to the result merged[key] = value return merged"},{"question":"# Question: Supervised Learning with Scikit-learn You are given a dataset consisting of medical data about patients, wherein each row represents a patient and each column represents a feature about that patient\'s health. The last column is a binary target variable indicating whether the patient has a particular medical condition (1) or not (0). Your task is to perform the following steps using scikit-learn: 1. **Data Preprocessing:** Load the dataset, handle missing values, and perform any necessary preprocessing (e.g., scaling of features). 2. **Model Implementation:** Implement and train at least three different supervised learning models from scikit-learn. 3. **Model Evaluation:** Compare the performance of these models using appropriate metrics like accuracy, precision, recall, and F1-score. 4. **Model Selection:** Identify the best performing model and tune its hyperparameters using grid search or random search cross-validation. Expected Input and Output - **Input:** - A dataset file in CSV format named `medical_data.csv`. - Features may include numeric and categorical data, with the target variable being binary. - **Output:** - A summary of the preprocessing steps. - The performance metrics of each model. - The best performing model and its tuned hyperparameters. Constraints and Limitations - You must use at least one linear model, one ensemble method, and one other algorithm of your choice from scikit-learn. - Ensure that you handle missing values appropriately. - Perform a train-test split for evaluating model performance. - Use appropriate strategies to avoid overfitting. Performance Requirements - Your solution should efficiently handle datasets up to 10,000 rows and 50 columns. - Ensure the code is optimized for readability, modularity, and efficiency. Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC from sklearn.metrics import classification_report from sklearn.model_selection import GridSearchCV # Load the dataset data = pd.read_csv(\'medical_data.csv\') # Data Preprocessing # (Handle missing values, encoding categorical variables, scaling, etc.) # ... # Train-test split X = data.drop(columns=[\'target\']) y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Model implementations models = { \'Logistic Regression\': LogisticRegression(), \'Random Forest\': RandomForestClassifier(), \'Support Vector Machine\': SVC() } # Train and evaluate models for model_name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) print(f\\"Results for {model_name}:n\\") print(classification_report(y_test, y_pred)) # Model Selection with Hyperparameter Tuning param_grid = {\'C\': [0.1, 1, 10], \'kernel\': [\'linear\', \'rbf\']} grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) print(f\\"Best Model after tuning: {grid_search.best_estimator_}\\") ``` This example demonstrates the steps to follow, but you are expected to extend it by including your preprocessing, additional models, and comprehensive evaluation. Good luck!","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.metrics import classification_report def load_and_preprocess_data(filepath): data = pd.read_csv(filepath) # Splitting features and target X = data.drop(columns=[\'target\']) y = data[\'target\'] # Identifying numeric and categorical columns numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Pipeline for numeric features numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Pipeline for categorical features categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combined preprocessing step preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) return X, y, preprocessor def train_and_evaluate_models(X, y, preprocessor): # Splitting the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define models models = { \'Logistic Regression\': LogisticRegression(), \'Random Forest\': RandomForestClassifier(), \'Support Vector Machine\': SVC() } results = {} for model_name, model in models.items(): # Creating a pipeline clf = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', model)]) # Training the model clf.fit(X_train, y_train) # Predictions y_pred = clf.predict(X_test) # Evaluating the model results[model_name] = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred), \'classification_report\': classification_report(y_test, y_pred) } return results def select_best_model(X, y, preprocessor): # We will take SVC for example and perform grid search param_grid = { \'classifier__C\': [0.1, 1, 10, 100], \'classifier__gamma\': [1, 0.1, 0.01, 0.001], \'classifier__kernel\': [\'linear\', \'rbf\'] } # Create the model pipeline with SVC svc_pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', SVC())]) # Perform Grid Search grid_search = GridSearchCV(svc_pipeline, param_grid, cv=5, verbose=2) grid_search.fit(X, y) best_model = grid_search.best_estimator_ best_params = grid_search.best_params_ return best_model, best_params # main workflow if __name__ == \\"__main__\\": filepath = \'medical_data.csv\' X, y, preprocessor = load_and_preprocess_data(filepath) model_results = train_and_evaluate_models(X, y, preprocessor) for model, metrics in model_results.items(): print(f\\"nResults for {model}:n\\") print(metrics[\'classification_report\']) best_model, best_params = select_best_model(X, y, preprocessor) print(f\\"nBest Model: {best_model}n\\") print(f\\"Best Hyperparameters: {best_params}n\\")"},{"question":"Problem Description: You are given a list of sales data for a company. Each entry in the list is a tuple of the form `(product_name, units_sold, revenue)`. Write a function `process_sales_data(sales_data)` to analyze this data. The function should return a dictionary containing the following information: 1. The total number of units sold for each product. 2. The total revenue for each product. 3. A list of products sorted by the total number of units sold. Expected Function Signature: ```python def process_sales_data(sales_data: list) -> dict: pass ``` Input: - `sales_data` is a list of tuples. Each tuple contains: - `product_name` (str): The name of the product. - `units_sold` (int): The number of units sold. - `revenue` (float): The revenue generated from sales. Output: - A dictionary with three keys: - `\'total_units_sold\'`: A dictionary where keys are product names and values are the total number of units sold for that product. - `\'total_revenue\'`: A dictionary where keys are product names and values are the total revenue for that product. - `\'products_sorted_by_units\'`: A list of product names sorted by the total number of units sold, in descending order. Constraints: - The input list may be empty, return an appropriate dictionary in such case. - Ensuring the function runs efficiently for large data sets. Example: ```python sales_data = [ (\'Product A\', 10, 100.0), (\'Product B\', 5, 80.0), (\'Product A\', 7, 70.0), (\'Product C\', 20, 200.0) ] result = process_sales_data(sales_data) # Expected Output: # { # \'total_units_sold\': {\'Product A\': 17, \'Product B\': 5, \'Product C\': 20}, # \'total_revenue\': {\'Product A\': 170.0, \'Product B\': 80.0, \'Product C\': 200.0}, # \'products_sorted_by_units\': [\'Product C\', \'Product A\', \'Product B\'] # } ``` Hints: - Consider using dictionaries to accumulate the totals for units sold and revenue. - List comprehensions might be useful for generating the sorted list of products. - Donâ€™t forget to handle the case where the input list is empty. Notes: - Use appropriate methods for dictionary creation and list manipulation. - Ensure your function handles edge cases efficiently.","solution":"def process_sales_data(sales_data): from collections import defaultdict total_units_sold = defaultdict(int) total_revenue = defaultdict(float) # Process the sales data for product_name, units_sold, revenue in sales_data: total_units_sold[product_name] += units_sold total_revenue[product_name] += revenue # Create a sorted list of products based on total units sold products_sorted_by_units = sorted(total_units_sold.keys(), key=lambda x: total_units_sold[x], reverse=True) return { \'total_units_sold\': dict(total_units_sold), \'total_revenue\': dict(total_revenue), \'products_sorted_by_units\': products_sorted_by_units }"},{"question":"Problem Statement You are given two datasets: `penguins` and `diamonds`. Using Seaborn\'s `so.Plot` functionality, you are required to create plots that effectively visualize the data with multiple layered marks to make the interpretation easier. Requirements 1. **Load the required datasets** - Load the \'penguins\' and \'diamonds\' datasets using Seaborn\'s `load_dataset` function. 2. **Create the following plots using Seaborn objects (`so.Plot`)**: 1. **Penguins Plot**: - `x-axis`: \'species\' - `y-axis`: \'body_mass_g\' - The plot should show jittered dots, the interquartile range (IQR) of the body mass for each species, and it should slightly shift the IQR visualization to avoid overlap. 2. **Diamonds Plot**: - `x-axis`: \'carat\' - `y-axis`: \'clarity\' - The plot should show jittered dots, the interquartile range (IQR) of the carat weight for each clarity category, and it should slightly shift the IQR visualization downwards to avoid overlap. Implementation Details 1. Write a function `create_penguins_plot()` that generates the Penguins plot as described above. 2. Write a function `create_diamonds_plot()` that generates the Diamonds plot as described above. Constraints - **Libraries allowed**: `seaborn`, `matplotlib.pyplot`, and other standard Python libraries. - The plots should be displayed using `matplotlib.pyplot.show()`. Expected Output The result should display two plots using the configurations stated above. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and display the plot plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) plot.show() plt.show() def create_diamonds_plot(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create and display the plot plot = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) plot.show() plt.show() # Example usage: create_penguins_plot() create_diamonds_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguins_plot(): Generates and displays a plot for the penguins dataset with jittered dots and IQR shifted to avoid overlap. # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and display the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) plot.show() plt.show() def create_diamonds_plot(): Generates and displays a plot for the diamonds dataset with jittered dots and IQR shifted to avoid overlap. # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create and display the plot plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) plot.show() plt.show() # Example usage: create_penguins_plot() create_diamonds_plot()"},{"question":"**Task** You are tasked with implementing a custom protocol using the low-level asyncio APIs to create a system that efficiently handles numerous client connections. In this task, you need to create a TCP echo server using the `Protocol` and `Transport` classes from the `asyncio` library. **Requirements** 1. Implement a class named `EchoServerProtocol` inherited from `asyncio.Protocol`. This class will handle incoming connections and data. 2. Implement the following methods in `EchoServerProtocol`: - `connection_made(self, transport)`: store the transport and print a message showing the peer\'s address. - `data_received(self, data)`: echo the received data back to the client and print the received message. - `connection_lost(self, exc)`: handle the connection closure and print an appropriate message. 3. Write an asynchronous function `main()` that: - Gets a reference to the running event loop. - Creates a TCP server using `loop.create_server()` with `EchoServerProtocol` as the protocol factory. - Ensures the server runs indefinitely. 4. Your server should listen on `127.0.0.1` and port `12345`. **Constraints** - Use Python 3.7 or above. - You are not allowed to use high-level asyncio functions like `asyncio.start_server()` or `asyncio.open_connection()`. - Make sure your implementation is efficient and handles multiple clients simultaneously. **Example Usage** Once your server is running, you can test it by connecting to it using telnet or netcat: ``` nc 127.0.0.1 12345 Hello World! Hello World! ``` This should print `Hello World!` twice both in your terminal and the server\'s console. **Submission** Submit a Python script file named `tcp_echo_server.py` containing your implementation.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') self.transport.write(data) print(\'Data sent back to client\') def connection_lost(self, exc): print(\'Connection closed\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 12345 ) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**XML-RPC Server and Client Implementation** **Objective:** You are required to design a Python XML-RPC server that performs various arithmetic operations and serves self-documenting HTML documentation. Additionally, you will write a client to interact with your server. **Task:** 1. Create an `ArithmeticService` class with the following methods: - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the difference between `x` and `y`. - `multiply(x, y)`: Returns the product of `x` and `y`. - `divide(x, y)`: Returns the quotient of `x` and `y`. 2. Create an XML-RPC server using `DocXMLRPCServer` that: - Binds to the address `localhost` on port `8001`. - Registers the arithmetic methods from `ArithmeticService`. - Registers introspection functions. - Sets the server title to \\"Arithmetic XML-RPC Server\\". - Sets the server documentation to describe the capabilities of the arithmetic operations. 3. Create an XML-RPC client that: - Connects to the server on `localhost:8001`. - Calls each of the arithmetic methods with appropriate arguments. - Prints the results. **Requirements:** - Ensure the server utilizes the `DocXMLRPCServer` class to provide self-documenting HTML at the root path. - The server should handle `POST` requests for XML-RPC method calls and `GET` requests for HTML documentation. - The client should demonstrate the use of all registered methods and print a list of available methods supported by the server. **Constraints:** - The server should handle any exceptions gracefully by returning a suitable error message. - Your implementation should follow proper coding conventions and practices. - You are not allowed to use any external libraries, except the ones provided by Pythonâ€™s standard library. **Performance Requirements:** - The server should be efficient and capable of handling multiple requests in a reasonable amount of time without crashing. - It is expected to start the server, run the client, and stop the server within a timeframe of 10 seconds for testing purposes. **Input/Output Format:** - Input will be the method calls from the client. - Output will be printed to the console from the client, showing the results of the arithmetic operations and list of available methods. **Example:** **Server:** ```python from xmlrpc.server import DocXMLRPCServer class ArithmeticService: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: return \\"Error: Division by zero\\" return x / y with DocXMLRPCServer((\'localhost\', 8001)) as server: server.register_introspection_functions() server.register_instance(ArithmeticService()) server.set_server_title(\\"Arithmetic XML-RPC Server\\") server.set_server_documentation(\\"This server provides basic arithmetic operations.\\") print(\\"Serving XML-RPC on localhost port 8001\\") server.serve_forever() ``` **Client:** ```python import xmlrpc.client client = xmlrpc.client.ServerProxy(\'http://localhost:8001\') print(client.add(5, 3)) # Output: 8 print(client.subtract(5, 3)) # Output: 2 print(client.multiply(5, 3)) # Output: 15 print(client.divide(5, 3)) # Output: 1.666... print(client.divide(5, 0)) # Output: Error: Division by zero print(client.system.listMethods()) # Output: List of available methods ``` **Note:** The provided server and client code snippets are examples only. You are required to implement these from scratch and ensure they meet the specified requirements.","solution":"from xmlrpc.server import DocXMLRPCServer class ArithmeticService: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: return \\"Error: Division by zero\\" return x / y def run_server(): with DocXMLRPCServer((\'localhost\', 8001)) as server: server.register_introspection_functions() server.register_instance(ArithmeticService()) server.set_server_title(\\"Arithmetic XML-RPC Server\\") server.set_server_documentation(\\"This server provides basic arithmetic operations.\\") print(\\"Serving XML-RPC on localhost port 8001\\") server.serve_forever()"},{"question":"You are tasked with creating a function `search_files_by_pattern` that searches for files in a given directory based on specified patterns. This function should utilize the `glob` module\'s capabilities to handle various cases of pattern matching and recursive directory traversal. Function Signature ```python def search_files_by_pattern(directory: str, patterns: List[str], recursive: bool = False) -> List[str]: pass ``` Input - `directory` (str): The root directory where the search will begin. Assume it is always a valid directory path. - `patterns` (List[str]): A list of Unix shell-style patterns for matching filenames. - `recursive` (bool, default=False): If set to `True`, the search will include all subdirectories recursively. Output - (List[str]): A list of matching paths (relative to the given root directory). The list should be sorted in ascending order. Constraints - Patterns in the `patterns` list are non-empty strings and follow Unix shell-style wildcard conventions. - The total number of files in the directory (including subdirectories) does not exceed 10000. Example ```python # Assume the following files are present in the directory structure: # dir/ # |- file1.txt # |- file2.md # |- subdir1/ # |- file3.txt # |- file4.txt # |- .hiddenfile search_files_by_pattern(\'dir\', [\'*.txt\'], recursive=False) # Output: [\'file1.txt\'] search_files_by_pattern(\'dir\', [\'*.txt\', \'*.md\'], recursive=False) # Output: [\'file1.txt\', \'file2.md\'] search_files_by_pattern(\'dir\', [\'*.txt\'], recursive=True) # Output: [\'file1.txt\', \'subdir1/file3.txt\', \'subdir1/file4.txt\'] search_files_by_pattern(\'dir\', [\'.*\'], recursive=False) # Output: [\'.hiddenfile\'] ``` Special Requirements - You must use the `glob` module for pattern matching. - Handle both non-recursive and recursive searches based on the `recursive` parameter. - Ensure that your function performs efficiently within the given constraints.","solution":"import glob import os from typing import List def search_files_by_pattern(directory: str, patterns: List[str], recursive: bool = False) -> List[str]: Searches for files in a given directory based on specified patterns. Args: directory (str): The root directory where the search will begin. patterns (List[str]): A list of Unix shell-style patterns for matching filenames. recursive (bool, default=False): If set to True, the search will include all subdirectories recursively. Returns: List[str]: A list of matching paths (relative to the given root directory), sorted in ascending order. # Result list to gather all matched files result = [] # Iterate over each pattern in the list of patterns for pattern in patterns: # Construct the full search pattern search_pattern = os.path.join(directory, \'**\', pattern) if recursive else os.path.join(directory, pattern) # Use glob.glob to match the files result.extend(glob.glob(search_pattern, recursive=recursive)) # Remove duplicates if any result = list(set(result)) # Get paths relative to the given root directory result = [os.path.relpath(path, directory) for path in result] # Sort the result in ascending order result.sort() return result"},{"question":"**Data Visualization with Seaborn** You are given a dataset containing information about the fuel efficiency of various car models. You are required to create a seaborn plot to visualize the relationship between the weight of the car and its miles-per-gallon (mpg) efficiency, while also structuring the data based on the number of cylinders. # Problem Statement Write a function `create_custom_plot` that loads the `mpg` dataset using `seaborn.load_dataset()` and generates a scatter plot with the following specifications: 1. The x-axis should represent the weight of the cars. 2. The y-axis should represent the mpg (miles per gallon). 3. The data points should be colored based on the number of cylinders the cars have, with a custom color palette of your choice. 4. The scatter plot points should have their size proportional to the horsepower of the cars. 5. Apply a `log` transformation to the y-scale. 6. Configure the ticks and labels as follows: - Show ticks on the x-axis every 1000 units. - Format the y-axis labels to show values preceded by \\"MPG: \\". 7. Limit the y-axis to the range (5, 50). # Function Signature ```python def create_custom_plot(): pass ``` # Output Your function should display the plot inline if run in a Jupyter notebook or in a standard Python environment equipped with display capabilities. # Sample Plot The resulting plot should investigate the correlation between the weight and efficiency of cars, effectively using visual encodings for extra dimensions (number of cylinders and horsepower). # Constraints - You must use the `seaborn.objects` interface for creating the plot. - The `mpg` dataset must be cleaned to only include rows where `cylinders` are 4, 6, or 8. # Example Here is an example to give you an idea of the structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Create and customize the plot p = ( so.Plot(mpg, x=\\"weight\\", y=\\"mpg\\", color=\\"cylinders\\", pointsize=\\"horsepower\\") .add(so.Dot()) .scale(color=[\\"#4dac26\\", \\"#d73027\\", \\"#4575b4\\"], y=\\"log\\") .limit(y=(5, 50)) .configure(x=so.Continuous().tick(every=1000), y=so.Continuous().label(like=\\"MPG: {x:g}\\")) ) # Display the plot p.show() ``` # Note Make sure your plot reflects the transformations and customizations described in the problem statement. The function should be versatile and accurately use seaborn\'s advanced functionalities to enhance the visual representation of the data.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\").dropna(subset=[\'mpg\', \'weight\', \'horsepower\']) # Create the scatter plot p = sns.scatterplot( data=mpg, x=\'weight\', y=\'mpg\', hue=\'cylinders\', size=\'horsepower\', palette=[\'#4dac26\', \'#d73027\', \'#4575b4\'], sizes=(20, 200) ) # Apply log transformation to the y-scale p.set(yscale=\\"log\\") # Configure ticks p.xaxis.set_major_locator(plt.MultipleLocator(1000)) # Format y-axis labels p.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\'MPG: {y:g}\')) # Set y-axis limits p.set_ylim(5, 50) # Show plot plt.show()"},{"question":"# Python Coding Assessment Question **Objective:** You are to demonstrate an understanding of the `compileall` module by implementing a utility that compiles Python source files in a given directory tree, but with additional user-defined constraints on which files to compile. **Problem Statement:** Implement a function named `custom_compile_dir` that recursively compiles all `.py` files in a directory tree, with options to filter files by file size and file age, in addition to features provided by the `compileall` module. **Function Signature:** ```python def custom_compile_dir(directory: str, max_size: int = None, min_age: float = None, compileall_args: dict = None) -> bool: pass ``` **Parameters:** - `directory` (str): The root directory where the function should start compiling `.py` files. This directory must be traversed recursively. - `max_size` (int, optional): The maximum file size in bytes for a file to be compiled. Only files that do not exceed this size should be compiled. If `None`, no file size filtering is applied. - `min_age` (float, optional): The minimum file age in days for a file to be compiled, reckoned from the current time. Only files that are older than this age should be compiled. If `None`, no file age filtering is applied. - `compileall_args` (dict, optional): A dictionary of additional keyword arguments to pass to `compileall.compile_dir()`, except for `dir` which is already specified by `directory`. If `None`, default values are used. **Returns:** - `bool`: Returns `True` if all selected files compiled successfully, and `False` otherwise. **Additional Conditions:** 1. Use the `os` and `datetime` modules to handle file size and age checking. 2. The function must print a summary of how many files were found, filtered out (by size and age), and successfully compiled. 3. Use the `compileall.compile_dir()` function to perform the actual compilation, leveraging the `compileall_args` parameters where appropriate. **Example Usage:** ```python import compileall result = custom_compile_dir(\'path/to/library\', max_size=5000, min_age=30, compileall_args={\'force\': True, \'quiet\': 1}) print(f\\"All files compiled successfully: {result}\\") ``` This function should recursively compile Python files in \'path/to/library\' that are no larger than 5000 bytes and older than 30 days, forcing the recompilation and printing only error messages (quiet=1). **Constraints:** - You may assume the `directory` is always a valid directory path. - Handle any exceptions gracefully and ensure they are reported without terminating the function prematurely. Your implementation will be assessed based on correctness, adherence to the problem statement, and the efficiency of the file filtering process.","solution":"import os import compileall import datetime def custom_compile_dir(directory: str, max_size: int = None, min_age: float = None, compileall_args: dict = None) -> bool: current_time = datetime.datetime.now() py_files = [] # Walk through directory to collect .py files for root, _, files in os.walk(directory): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) # Check file size constraint if max_size is not None: file_size = os.path.getsize(file_path) if file_size > max_size: continue # Check file age constraint if min_age is not None: file_mtime = os.path.getmtime(file_path) file_age = (current_time - datetime.datetime.fromtimestamp(file_mtime)).days if file_age < min_age: continue py_files.append(file_path) if compileall_args is None: compileall_args = {} result = compileall.compile_dir(directory, **compileall_args, legacy=True, quiet=1) return result"},{"question":"**Question: Asynchronous Web Scraping with Timeout Handling** **Objective:** Implement a function that asynchronously scrapes data from a list of URLs and handles potential exceptions, including timeouts and non-existent domains. Use `ThreadPoolExecutor` to manage concurrent requests and return a dictionary containing the URL and the length of the content if successfully fetched, or an error message if failed. **Function Specification:** ```python def fetch_urls(urls, timeout, max_workers): Fetch the contents of multiple URLs asynchronously using ThreadPoolExecutor. Args: urls (list of str): A list of URLs to fetch. timeout (int or float): The maximum number of seconds to wait for a response from each URL. max_workers (int): The maximum number of threads to use. Returns: dict: A dictionary with URLs as keys and either the length of the content or an error message as values. pass ``` **Input:** - `urls`: A list of URLs (strings) that need to be fetched. - `timeout`: Maximum time (in seconds) to wait for each URL\'s response. - `max_workers`: Number of threads to handle concurrent requests. **Output:** - A dictionary where: - Each key is a URL from the input list. - The value is either the length of the content retrieved from the URL or an error message if the fetch failed. **Constraints:** - URLs must be valid HTTP/HTTPS URLs. - Timeout must be a positive number. - Max workers must be a positive integer. **Example:** ```python urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistant-subdomain.python.org/\' ] results = fetch_urls(urls, timeout=5, max_workers=5) print(results) # Expected output (example): # { # \'http://www.foxnews.com/\': 12345, # \'http://www.cnn.com/\': 23456, # \'http://europe.wsj.com/\': 34567, # \'http://www.bbc.co.uk/\': 45678, # \'http://nonexistant-subdomain.python.org/\': \'error message\' # } ``` **Note:** - Use `concurrent.futures.ThreadPoolExecutor` to manage the thread pool. - Utilize `urllib.request.urlopen` to fetch the content of each URL. - Handle `urllib.error.URLError` and `concurrent.futures.TimeoutError` exceptions appropriately and include meaningful error messages in the output dictionary. **Hints:** - Consider using `executor.submit()` to schedule the tasks. - Use `future.result()` within a try-except block to handle exceptions. - `executor.map()` might be useful, but be cautious about handling timeouts and exceptions. **Performance Considerations:** - Ensure that the function can handle a large number of URLs efficiently with the provided number of threads. - Avoid deadlock scenarios by correctly managing the lifecycle of the executor and handling task dependencies.","solution":"import concurrent.futures import urllib.request import urllib.error def fetch_url(url, timeout): try: with urllib.request.urlopen(url, timeout=timeout) as response: return len(response.read()) except Exception as e: return f\\"Error: {e}\\" def fetch_urls(urls, timeout, max_workers): results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as e: results[url] = f\\"Error: {e}\\" return results"},{"question":"# ASCII Character Classifier and Converter You are tasked with implementing a function that processes a string containing various characters. The function should classify each character based on the properties provided by the `curses.ascii` module and convert certain types of characters to their corresponding control or meta characters. Function Signature ```python def process_ascii_string(input_string: str) -> dict: pass ``` Input - `input_string (str)`: A string containing one or multiple ASCII characters. Output - `result (dict)`: A dictionary with the following structure: ```python { \'input\': input_string, \'classifications\': { index (int): { \'char\': char, \'is_alnum\': bool, \'is_alpha\': bool, \'is_ascii\': bool, \'is_blank\': bool, \'is_cntrl\': bool, \'is_digit\': bool, \'is_graph\': bool, \'is_lower\': bool, \'is_print\': bool, \'is_punct\': bool, \'is_space\': bool, \'is_upper\': bool, \'is_xdigit\': bool, \'is_meta\': bool } }, \'converted\': { \'ctrl_characters\': [list_of_control_characters], \'meta_characters\': [list_of_meta_characters] } } ``` Example ```python input_string = \\"@#A b3n\\" output = { \'input\': \'@#A b3n\', \'classifications\': { 0: {\'char\': \'@\', \'is_alnum\': False, \'is_alpha\': False, \'is_ascii\': True, \'is_blank\': False, \'is_cntrl\': False, \'is_digit\': False, \'is_graph\': True, \'is_lower\': False, \'is_print\': True, \'is_punct\': True, \'is_space\': False, \'is_upper\': False, \'is_xdigit\': False, \'is_meta\': False}, 1: {\'char\': \'#\', (...similar structure for other keys...) }, ... }, \'converted\': { \'ctrl_characters\': [ctrl_char_1, ctrl_char_2, ...], \'meta_characters\': [meta_char_1, meta_char_2, ...] } } ``` Constraints 1. The input string length will not exceed 200 characters. 2. Performance should be optimized to handle the operations in a time-efficient manner. # Notes - You should utilize the functions provided by the `curses.ascii` module to classify and convert characters. - Control characters are those in the ordinal range of 0 to 31 and 127. - Meta characters are those with ordinal values 128 and above. - Make sure to handle both strings and integers where applicable per the documentation. Implementation Strategy 1. Read the input string and initialize the result dictionary. 2. Iterate through each character in the input string and classify it using the `curses.ascii` functions. 3. Convert characters to their control and meta equivalents and store them in the relevant lists under `converted`. 4. Return the result dictionary containing the classifications and converted character lists. Good luck!","solution":"import curses.ascii def process_ascii_string(input_string: str) -> dict: classifications = {} ctrl_characters = [] meta_characters = [] for i, char in enumerate(input_string): ascii_value = ord(char) is_alnum = curses.ascii.isalnum(ascii_value) is_alpha = curses.ascii.isalpha(ascii_value) is_ascii = curses.ascii.isascii(ascii_value) is_blank = curses.ascii.isblank(ascii_value) is_cntrl = curses.ascii.iscntrl(ascii_value) is_digit = curses.ascii.isdigit(ascii_value) is_graph = curses.ascii.isgraph(ascii_value) is_lower = curses.ascii.islower(ascii_value) is_print = curses.ascii.isprint(ascii_value) is_punct = curses.ascii.ispunct(ascii_value) is_space = curses.ascii.isspace(ascii_value) is_upper = curses.ascii.isupper(ascii_value) is_xdigit = curses.ascii.isxdigit(ascii_value) is_meta = ascii_value >= 128 classifications[i] = { \'char\': char, \'is_alnum\': is_alnum, \'is_alpha\': is_alpha, \'is_ascii\': is_ascii, \'is_blank\': is_blank, \'is_cntrl\': is_cntrl, \'is_digit\': is_digit, \'is_graph\': is_graph, \'is_lower\': is_lower, \'is_print\': is_print, \'is_punct\': is_punct, \'is_space\': is_space, \'is_upper\': is_upper, \'is_xdigit\': is_xdigit, \'is_meta\': is_meta } if is_cntrl: ctrl_characters.append(char) if is_meta: meta_characters.append(char) return { \'input\': input_string, \'classifications\': classifications, \'converted\': { \'ctrl_characters\': ctrl_characters, \'meta_characters\': meta_characters } }"},{"question":"PyTorch Accelerator Device Management Objective: You are required to implement a function that verifies and sets up the appropriate accelerator device for intensive computation tasks if available. The function should also perform operations to synchronize the streams associated with the computing devices. Description: Implement the function `setup_and_verify_device` with the following requirements: 1. **Check if any accelerator devices are available** using `torch.accelerator.is_available()`. 2. **Retrieve the total number of available devices** using `torch.accelerator.device_count()`. 3. If no device is available, raise a `RuntimeError` with the message \\"No accelerator device available.\\" 4. If devices are available, perform the following: - Set the device to the one with the highest index using `torch.accelerator.set_device_index()`. - Verify the current device index using `torch.accelerator.current_device_index()`. - Retrieve the current stream of the newly set device using `torch.accelerator.current_stream()`. - Synchronize streams on the current device using `torch.accelerator.synchronize()`. 5. Return a dictionary containing: - `\'device_count\'`: Total number of devices. - `\'current_device_index\'`: Index of the current device. - `\'current_stream\'`: The current stream object. Constraints: - You should ensure and assume that the functions being used from `torch.accelerator` are available and behave as expected based on their names. - You are required to handle any unexpected behavior or lack of availability of devices gracefully. Function Signature: ```python def setup_and_verify_device() -> dict: pass ``` Example: ```python result = setup_and_verify_device() print(result) # Output could be: # {\'device_count\': 4, \'current_device_index\': 3, \'current_stream\': <stream_object>} ``` Note: The actual output will depend on the environment\'s available accelerator devices and their states.","solution":"import torch def setup_and_verify_device() -> dict: if not torch.cuda.is_available(): raise RuntimeError(\\"No accelerator device available.\\") device_count = torch.cuda.device_count() torch.cuda.set_device(device_count - 1) current_device_index = torch.cuda.current_device() current_stream = torch.cuda.current_stream() torch.cuda.synchronize() return { \'device_count\': device_count, \'current_device_index\': current_device_index, \'current_stream\': current_stream }"},{"question":"**Email Message Parsing and Analysis** You are tasked with implementing a function that processes an email message and extracts specific information from it. The email message will be given as a bytes-like object, and your function should parse this message to retrieve details about its headers and parts. # Function Signature: ```python def analyze_email_message(email_bytes: bytes) -> dict: pass ``` # Input: - `email_bytes` (bytes): A bytes-like object representing the complete raw email message. # Output: - Returns a dictionary containing: - \\"headers\\" (dict): A dictionary where keys are header names and values are header values. - \\"is_multipart\\" (bool): A boolean indicating whether the email is a multipart message. - \\"parts\\" (list of dict): A list of dictionaries each representing a part of the email. Each part dictionary contains the keys: - \\"content_type\\" (str): The content type of the part (e.g., \\"text/plain\\", \\"text/html\\"). - \\"content\\" (str): The text content of the part. If the part is not textual, return a placeholder string \\"<non-textual content>\\". # Requirements: 1. Use the `BytesParser` from the `email.parser` module to parse the `email_bytes`. 2. Properly handle both MIME (`multipart/*`) and non-MIME parts of the email. 3. In the case of MIME messages, extract information about each sub-part. 4. Ensure the solution handles email messages with non-standard structures gracefully and populates the \\"defects\\" attribute accordingly. # Example: ```python email_bytes = bContent-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" MIME-Version: 1.0 --===============7330845974216740156== Content-Type: text/plain; charset=\\"us-ascii\\" This is the body of the message. --===============7330845974216740156== Content-Type: text/html; charset=\\"us-ascii\\" <html><body>This is the HTML body of the message.</body></html> --===============7330845974216740156==-- result = analyze_email_message(email_bytes) # Expected output: # { # \\"headers\\": { # \\"Content-Type\\": \\"multipart/mixed; boundary=\\"===============7330845974216740156==\\"\\", # \\"MIME-Version\\": \\"1.0\\" # }, # \\"is_multipart\\": True, # \\"parts\\": [ # { # \\"content_type\\": \\"text/plain\\", # \\"content\\": \\"This is the body of the message.\\" # }, # { # \\"content_type\\": \\"text/html\\", # \\"content\\": \\"<html><body>This is the HTML body of the message.</body></html>\\" # } # ] # } ``` # Constraints: - You can assume that the email message provided will be well-formed and compliant with RFC 5322 or RFC 6532. - Your algorithm should handle common email content types and structures, including multipart messages.","solution":"from email.parser import BytesParser from email.policy import default def analyze_email_message(email_bytes: bytes) -> dict: Analyzes an email message and extracts headers, multipart status, and parts information. Parameters: - email_bytes (bytes): A bytes-like object representing the complete raw email message. Returns: - dict: A dictionary containing the headers, multipart status, and parts of the email. # Parse the email bytes msg = BytesParser(policy=default).parsebytes(email_bytes) # Extract headers headers = {key: value for key, value in msg.items()} # Check if the email is multipart is_multipart = msg.is_multipart() # Extract parts information parts = [] if is_multipart: for part in msg.iter_parts(): content_type = part.get_content_type() if part.get_content_maintype() == \'text\': content = part.get_payload(decode=True).decode(part.get_content_charset()) else: content = \\"<non-textual content>\\" parts.append({ \\"content_type\\": content_type, \\"content\\": content }) else: content_type = msg.get_content_type() if msg.get_content_maintype() == \'text\': content = msg.get_payload(decode=True).decode(msg.get_content_charset()) else: content = \\"<non-textual content>\\" parts.append({ \\"content_type\\": content_type, \\"content\\": content }) return { \\"headers\\": headers, \\"is_multipart\\": is_multipart, \\"parts\\": parts }"},{"question":"# Question: HTML String Escaping and Unescaping You are tasked with developing a set of functions to work with HTML content. Your functions will make use of the provided `html` module for escaping and unescaping HTML characters. Objectives 1. Implement a function `sanitize_html` that escapes potentially dangerous characters in a given string. 2. Implement a function `restore_html` that reverses the escaping applied by `sanitize_html`. # Specifications 1. `sanitize_html(s: str, convert_quotes: bool = True) -> str` - **Input**: A string `s` that may contain HTML special characters such as `&`, `<`, `>`, `\\"`, and `\'`. - **Output**: A string where the HTML special characters are converted to their HTML-safe sequences. - **Constraints**: - You should use the `html.escape` function. - If `convert_quotes` is `True`, also escape the quotes (`\\"` and `\'`). - If `convert_quotes` is `False`, do not escape the quotes. 2. `restore_html(s: str) -> str` - **Input**: A string `s` that has escaped HTML characters (e.g., `&amp;`, `&lt;`, `&gt;`, `&quot;`, `&apos;`). - **Output**: A string where all HTML character references are converted back to their corresponding Unicode characters. - **Constraints**: - You should use the `html.unescape` function. # Example ```python # Example usage of sanitize_html original_string = \'Use \\"quotes\\" for `attributes` & <tags>.\' escaped_string = sanitize_html(original_string) print(escaped_string) # Output: Use &quot;quotes&quot; for `attributes` &amp; &lt;tags&gt;. # Example usage of restore_html escaped_string = \'Use &quot;quotes&quot; for &grave;attributes&grave; &amp; &lt;tags&gt;.\' restored_string = restore_html(escaped_string) print(restored_string) # Output: Use \\"quotes\\" for `attributes` & <tags>. ``` # Performance Requirements - Your solution should handle strings of reasonable length (up to 10,000 characters efficiently). # Additional Notes - Be mindful of edge cases, such as strings that do not contain any characters to escape, or strings that contain only characters to escape. - You may assume the input strings are properly formatted and do not contain any invalid HTML character references. Implement the functions `sanitize_html` and `restore_html` to complete the task.","solution":"import html def sanitize_html(s: str, convert_quotes: bool = True) -> str: Escapes potentially dangerous characters in a given string. Parameters: s (str): The input string to be sanitized. convert_quotes (bool): If True, escape quotes (default is True). Returns: str: The sanitized string with HTML special characters escaped. if convert_quotes: return html.escape(s, quote=True) else: return html.escape(s, quote=False) def restore_html(s: str) -> str: Reverses the escaping applied by sanitize_html. Parameters: s (str): The input string with escaped HTML characters. Returns: str: The unescaped string with HTML character references converted back. return html.unescape(s)"},{"question":"**Title: Terminal Control with Python\'s tty Module** **Objective:** To assess your understanding of Python\'s \\"tty\\" and \\"termios\\" modules for Unix terminal control, you are tasked with writing a function that uses the \\"tty\\" module to set a terminal to different modes and read input while in those modes. **Problem Statement:** Implement a function `read_terminal_input(mode: str, fd: int) -> str` that takes two parameters: 1. `mode` - A string that can be either `\\"raw\\"` or `\\"cbreak\\"`, specifying the terminal mode to set. 2. `fd` - An integer representing the file descriptor of the terminal (for standard input, this would typically be 0). The function should: - Set the terminal to the specified mode using the `tty` module. - Read a single line of input from the terminal. - Restore the terminal to its original settings before returning the input. **Requirements:** - Use the `tty.setraw()` function to set the terminal to \\"raw\\" mode. - Use the `tty.setcbreak()` function to set the terminal to \\"cbreak\\" mode. - Properly handle restoring the terminal settings after reading the input to ensure consistent terminal behavior. **Constraints:** - The function must handle both \\"raw\\" and \\"cbreak\\" modes. - You may assume the `fd` provided is valid for the terminal. - The function should work on Unix-based systems (Linux, macOS). **Example Usage:** ```python # Example usage when running in a Unix-based terminal: import sys # Read input in raw mode input_raw = read_terminal_input(\'raw\', sys.stdin.fileno()) print(f\\"Raw mode input: {input_raw}\\") # Read input in cbreak mode input_cbreak = read_terminal_input(\'cbreak\', sys.stdin.fileno()) print(f\\"Cbreak mode input: {input_cbreak}\\") ``` **Expected Output:** Upon executing the above usage example, the function should: - Set the terminal to raw mode, read a line of input, and print it. - Then set the terminal to cbreak mode, read another line of input, and print it. Ensure your function is thoroughly tested on a Unix system to verify its correctness. Consider edge cases where terminal settings might fail to set or restore. Good luck!","solution":"import sys import tty import termios def read_terminal_input(mode: str, fd: int) -> str: Set the terminal to the specified mode and read input. :param mode: A string specifying the terminal mode - \\"raw\\" or \\"cbreak\\". :param fd: File descriptor of the terminal (usually 0 for stdin). :return: The input read from the terminal. assert mode in [\\"raw\\", \\"cbreak\\"], \\"Mode must be either \'raw\' or \'cbreak\'\\" # Save the current terminal settings old_settings = termios.tcgetattr(fd) try: # Set the terminal to the specified mode if mode == \\"raw\\": tty.setraw(fd) elif mode == \\"cbreak\\": tty.setcbreak(fd) # Read a single line of input input_line = sys.stdin.readline().strip() finally: # Restore the original terminal settings termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) return input_line"},{"question":"**Question: Compression and Archiving with Multiple Formats** In this task, you are required to demonstrate your understanding of Python\'s data compression and archiving capabilities by working with multiple file types and compression algorithms. **Task:** 1. You will receive a directory path that contains multiple files of varying types (e.g., `.txt`, `.csv`, `.json`). 2. Your program should compress these files using three different algorithms: gzip, bzip2, and lzma. 3. Each compressed file should be archived into a single ZIP file. 4. Your program should then decompress the ZIP file and extract files to a specified output directory, ensuring the integrity of the original files. **Requirements:** 1. Your solution should include functions as described below: * `compress_files(input_dir: str, output_dir: str) -> None`: Compress files in `input_dir` using gzip, bzip2, and lzma, and store them in `output_dir`. * `create_zip_archive(compressed_dir: str, zip_file_path: str) -> None`: Archive all compressed files (from `compressed_dir`) into a ZIP file at `zip_file_path`. * `decompress_zip_archive(zip_file_path: str, extract_to_dir: str) -> None`: Decompress and extract files from the ZIP archive at `zip_file_path` to `extract_to_dir`. 2. The program should handle errors gracefully, such as when files are missing or if there are issues during compression/decompression. 3. The output files should retain their original names with appropriate extensions indicating the compression method used (e.g., `file.txt.gz`, `file.csv.bz2`, `file.json.xz`). 4. The decompression process should verify that the extracted files match the original files by comparing file sizes or checksums. **Example Usage:** ```python input_directory = \\"path/to/input_files\\" compressed_directory = \\"path/to/compressed_files\\" zip_file = \\"path/to/output_archive.zip\\" decompressed_directory = \\"path/to/decompressed_files\\" compress_files(input_directory, compressed_directory) create_zip_archive(compressed_directory, zip_file) decompress_zip_archive(zip_file, decompressed_directory) ``` **Constraints:** 1. You may assume the input directory contains only regular files and no subdirectories. 2. The program should be efficient in terms of memory usage, especially during the compression and decompression process. 3. Make sure your code is well-documented and includes comments explaining critical sections. Good luck, and happy coding!","solution":"import os import shutil import gzip import bz2 import lzma import zipfile def compress_files(input_dir: str, output_dir: str) -> None: if not os.path.exists(output_dir): os.makedirs(output_dir) for file_name in os.listdir(input_dir): file_path = os.path.join(input_dir, file_name) if os.path.isfile(file_path): # Gzip compression with open(file_path, \'rb\') as f_in, gzip.open(os.path.join(output_dir, file_name + \'.gz\'), \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) # Bzip2 compression with open(file_path, \'rb\') as f_in, bz2.open(os.path.join(output_dir, file_name + \'.bz2\'), \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) # Lzma compression with open(file_path, \'rb\') as f_in, lzma.open(os.path.join(output_dir, file_name + \'.xz\'), \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def create_zip_archive(compressed_dir: str, zip_file_path: str) -> None: with zipfile.ZipFile(zip_file_path, \'w\') as zipf: for foldername, subfolders, filenames in os.walk(compressed_dir): for filename in filenames: file_path = os.path.join(foldername, filename) zipf.write(file_path, os.path.relpath(file_path, compressed_dir)) def decompress_zip_archive(zip_file_path: str, extract_to_dir: str) -> None: with zipfile.ZipFile(zip_file_path, \'r\') as zipf: zipf.extractall(extract_to_dir)"},{"question":"# Question: Implementing a Library Management System You are tasked with developing a simple Library Management System that keeps track of books and members of the library. You will implement this system using Python classes and demonstrate various object-oriented programming concepts such as class definitions, methods, inheritance, and iterators. Requirements: 1. **Class Definitions**: - Create a class `Person` to represent a generic person with attributes: `name` and `id`. - Create a class `Member` that inherits from `Person` and adds attributes: `max_books` (maximum books a member can borrow) and `borrowed_books` (a list to store borrowed book titles). 2. **Book Class**: - Define a class `Book` with the following attributes: `title`, `author`, `isbn`, and `available_copies`. - Implement methods to borrow and return books. When a member borrows a book, the `available_copies` should decrease by one. When a book is returned, `available_copies` should increase by one. 3. **Library Class**: - Create a class `Library` that manages books and members. It should contain: - A list of `Book` objects. - A list of `Member` objects. - Methods to add books, add members, lend books to members, return books, and display the library\'s inventory. 4. **Iterator for Library Books**: - Implement an iterator within the `Library` class to allow iteration over the books available in the library. The iterator should only return books that have more than zero available copies. 5. **Generator for Members with Borrowed Books**: - Implement a generator function that yields members who currently have borrowed books. Constraints: - A member can borrow a book only if they have not exceeded their borrowing limit. - Ensure that appropriate checks are in place for borrowing and returning books (e.g., checking if the book is available, if the member hasn\'t exceeded their limit, etc.). Input and Output: - Implement the required classes and methods based on the following structure. Input/output handling is not required. Example: ```python # Create Library object library = Library() # Add Book objects library.add_book(Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"1234567890123\\", available_copies=5)) library.add_book(Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"2345678901234\\", available_copies=2)) # Add Member objects library.add_member(Member(name=\\"Alice\\", id=1, max_books=3)) library.add_member(Member(name=\\"Bob\\", id=2, max_books=2)) # Borrow books library.lend_book(member_id=1, book_title=\\"1984\\") library.lend_book(member_id=2, book_title=\\"To Kill a Mockingbird\\") # Return books library.return_book(member_id=2, book_title=\\"To Kill a Mockingbird\\") # Display available books using iterator for book in library: print(f\\"Title: {book.title}, Available copies: {book.available_copies}\\") # Generate a list of members with borrowed books for member in library.members_with_borrowed_books(): print(f\\"Member: {member.name}, Borrowed Books: {member.borrowed_books}\\") ``` Implementation: Write the required classes and methods to fulfill the above requirements. Ensure the implementation is clear and adheres to the object-oriented principles discussed.","solution":"from typing import List class Person: def __init__(self, name: str, id: int): self.name = name self.id = id class Member(Person): def __init__(self, name: str, id: int, max_books: int): super().__init__(name, id) self.max_books = max_books self.borrowed_books: List[str] = [] class Book: def __init__(self, title: str, author: str, isbn: str, available_copies: int): self.title = title self.author = author self.isbn = isbn self.available_copies = available_copies def borrow(self): if self.available_copies > 0: self.available_copies -= 1 return True return False def return_book(self): self.available_copies += 1 class Library: def __init__(self): self.books: List[Book] = [] self.members: List[Member] = [] def add_book(self, book: Book): self.books.append(book) def add_member(self, member: Member): self.members.append(member) def lend_book(self, member_id: int, book_title: str): member = next((m for m in self.members if m.id == member_id), None) book = next((b for b in self.books if b.title == book_title), None) if member and book and len(member.borrowed_books) < member.max_books and book.borrow(): member.borrowed_books.append(book.title) return True return False def return_book(self, member_id: int, book_title: str): member = next((m for m in self.members if m.id == member_id), None) book = next((b for b in self.books if b.title == book_title), None) if member and book and book_title in member.borrowed_books: member.borrowed_books.remove(book_title) book.return_book() return True return False def __iter__(self): return (book for book in self.books if book.available_copies > 0) def members_with_borrowed_books(self): return (member for member in self.members if member.borrowed_books)"},{"question":"**Question: Advanced Pandas Data Manipulation** You are given a CSV file named `data.csv` which contains columns with different data types as specified in the table below. Using pandas, you are required to perform the following tasks: 1. **Read the CSV file** into a DataFrame. 2. **Convert the columns** to appropriate pandas data types as specified. 3. **Handle missing values**: For numeric columns, fill missing values with the column mean. For categorical and string columns, fill missing values with the most frequent value. 4. Implement a function `time_between_periods` that calculates the duration in days between the maximum and minimum dates in the `event_dates` column (period type). 5. Save the cleaned DataFrame to a new CSV file `cleaned_data.csv`. The CSV file has the following schema: | Column Name | Description | Data Type | |-------------|---------------------------|-----------| | id | Unique identifier | Integer | | event_dates | Dates of events | Period | | category | Category of the event | Categorical| | amount | Amount associated with the event |Nullable Float| | description | Description of the event | String | # Function Signatures ```python import pandas as pd def read_and_clean_data(file_path: str) -> pd.DataFrame: Read the CSV file and clean data based on the specified rules. Args: file_path (str): Path to the input CSV file. Returns: pd.DataFrame: The cleaned DataFrame. pass def time_between_periods(df: pd.DataFrame) -> int: Calculate the duration in days between the maximum and minimum dates in the \'event_dates\' column. Args: df (pd.DataFrame): The DataFrame with the \'event_dates\' column. Returns: int: The duration in days. pass def save_cleaned_data(df: pd.DataFrame, output_file_path: str) -> None: Save the cleaned DataFrame to a CSV file. Args: df (pd.DataFrame): The cleaned DataFrame. output_file_path (str): Path to the output CSV file. pass ``` # Requirements 1. The `event_dates` column must be converted to pandas Period type. 2. The `category` column must be converted to Categorical type. 3. The `amount` column must be handled as a Nullable Float. 4. The `description` column must be handled as a String type using pandas StringDtype. 5. Use the `Timestamp`, `CategoricalDtype`, and other relevant pandas data types as necessary. 6. Utilize pandas methods for filling missing values and manipulating DataFrame structures. 7. Ensure the final DataFrame saved to `cleaned_data.csv` maintains the specified types and cleaned data. **Constraints:** - Assume the CSV file is large and efficiently handle memory usage. - Ensure your code is robust and can handle any unexpected data issues gracefully. **Sample Data (`data.csv`):** ```csv id,event_dates,category,amount,description 1,2020-01-05,A,100.5,Description1 2,2020-02-20,A,,Description2 3,,B,200.0, 4,2021-03-01,C,,Description4 5,2021-03-15,B,150.0,Description5 ``` Good luck!","solution":"import pandas as pd def read_and_clean_data(file_path: str) -> pd.DataFrame: Read the CSV file and clean data based on the specified rules. Args: file_path (str): Path to the input CSV file. Returns: pd.DataFrame: The cleaned DataFrame. # Reading the file df = pd.read_csv(file_path) # Converting columns to respective data types df[\'id\'] = df[\'id\'].astype(int) df[\'event_dates\'] = pd.to_datetime(df[\'event_dates\'], errors=\'coerce\') df[\'category\'] = df[\'category\'].astype(\'category\') df[\'amount\'] = df[\'amount\'].astype(\'float\') df[\'description\'] = df[\'description\'].astype(\'string\') # Handling missing values df[\'amount\'].fillna(df[\'amount\'].mean(), inplace=True) df[\'category\'].fillna(df[\'category\'].mode()[0], inplace=True) df[\'description\'].fillna(df[\'description\'].mode()[0], inplace=True) df[\'event_dates\'] = df[\'event_dates\'].dt.to_period(\'D\') return df def time_between_periods(df: pd.DataFrame) -> int: Calculate the duration in days between the maximum and minimum dates in the \'event_dates\' column. Args: df (pd.DataFrame): The DataFrame with the \'event_dates\' column. Returns: int: The duration in days. min_date = df[\'event_dates\'].min().start_time max_date = df[\'event_dates\'].max().start_time return (max_date - min_date).days def save_cleaned_data(df: pd.DataFrame, output_file_path: str) -> None: Save the cleaned DataFrame to a CSV file. Args: df (pd.DataFrame): The cleaned DataFrame. output_file_path (str): Path to the output CSV file. df.to_csv(output_file_path, index=False)"},{"question":"# Custom Serialization with `copyreg` Objective In this exercise, you will demonstrate your understanding of custom serialization for Python objects using the `copyreg` module. You are provided with a class representing a network node, and your task will be to implement custom pickling functions for this class. This exercises your knowledge of the `copyreg` module as well as your ability to handle serialization processes in Python. Problem Statement You are given the following class `NetworkNode`, which represents a node in a network with attributes for the node\'s ID and its neighbors. ```python class NetworkNode: def __init__(self, node_id, neighbors): Initialize the NetworkNode instance. Parameters: node_id (int): Unique ID of the node. neighbors (list): List of neighboring node IDs. self.node_id = node_id self.neighbors = neighbors def __repr__(self): return f\\"NetworkNode(node_id={self.node_id}, neighbors={self.neighbors})\\" ``` Your task is to implement custom pickling for `NetworkNode` using the `copyreg` module. 1. **Write a function `pickle_network_node(node)` that specifies how to serialize `NetworkNode` instances.** The function should return a tuple containing the class reference and a tuple of the object\'s attributes necessary for reconstruction. 2. **Register this pickling function for the `NetworkNode` class using `copyreg.pickle()`.** 3. **Write a function `unpickle_network_node(node_id, neighbors)` that reconstructs a `NetworkNode` instance from the serialized data.** Input - Any `NetworkNode` instance created during runtime. Output - The correctly serialized and deserialized objects should behave as expected, demonstrating custom pickling and reconstruction. Constraints - Focus on correctness and proper use of the `copyreg` module. - Ensure the `NetworkNode` class instances can be pickled and unpickled without loss of data. Example ```python import copyreg import pickle class NetworkNode: def __init__(self, node_id, neighbors): self.node_id = node_id self.neighbors = neighbors def __repr__(self): return f\\"NetworkNode(node_id={self.node_id}, neighbors={self.neighbors})\\" # 1. Define the pickling function def pickle_network_node(node): return unpickle_network_node, (node.node_id, node.neighbors) # 2. Register the pickling function copyreg.pickle(NetworkNode, pickle_network_node) # 3. Define the unpickling function def unpickle_network_node(node_id, neighbors): return NetworkNode(node_id, neighbors) # Example testing code node = NetworkNode(1, [2, 3, 4]) serialized_node = pickle.dumps(node) print(serialized_node) deserialized_node = pickle.loads(serialized_node) print(deserialized_node) ``` Your implementation should correctly output a serialized string and a deserialized `NetworkNode` object equivalent to the original.","solution":"import copyreg import pickle class NetworkNode: def __init__(self, node_id, neighbors): self.node_id = node_id self.neighbors = neighbors def __repr__(self): return f\\"NetworkNode(node_id={self.node_id}, neighbors={self.neighbors})\\" # 1. Define the pickling function def pickle_network_node(node): return unpickle_network_node, (node.node_id, node.neighbors) # 2. Register the pickling function copyreg.pickle(NetworkNode, pickle_network_node) # 3. Define the unpickling function def unpickle_network_node(node_id, neighbors): return NetworkNode(node_id, neighbors)"},{"question":"Objective: Implement a function using the `difflib` module that compares two text files and generates a report highlighting the differences in a custom format. Task: You are required to write a Python function `generate_custom_diff_report(file1: str, file2: str) -> str` that compares two text files and outputs a custom diff report. The custom report should include: - Lines that are only in the first file, prefixed with `\\"- \\"`. - Lines that are only in the second file, prefixed with `\\"+ \\"`. - Lines that are common to both files, prefixed with `\\" \\"`. - A section at the end summarizing the total number of lines only in the first file, only in the second file, and common lines. Function Signature: ```python def generate_custom_diff_report(file1: str, file2: str) -> str: ``` Input: - `file1` (str): The path to the first text file. - `file2` (str): The path to the second text file. Output: - A string representing the custom diff report. Constraints: - The text files should be read with newline characters preserved. - Only exact line matches should be considered common. - Ignore white space differences. - Each file will have at most 10,000 lines. Example: Given two files, `file1.txt` and `file2.txt`: - `file1.txt` content: ``` line1 line2 line3 ``` - `file2.txt` content: ``` line2 line3 line4 ``` The output should be: ``` - line1 line2 line3 + line4 Summary: Lines only in file1: 1 Lines only in file2: 1 Common lines: 2 ``` Implementation Notes: - Utilize the `difflib.Differ` class to compute the line-by-line differences. - Ensure your solution is efficient and handles the constraints provided. - Remember to handle file reading and writing operations properly.","solution":"import difflib def generate_custom_diff_report(file1: str, file2: str) -> str: with open(file1, \'r\') as f1, open(file2, \'r\') as f2: file1_lines = f1.readlines() file2_lines = f2.readlines() differ = difflib.Differ() diff = list(differ.compare(file1_lines, file2_lines)) only_in_file1 = 0 only_in_file2 = 0 common = 0 diff_report = [] for line in diff: # The first character of difflib\'s output indicates if the line is only in one of the files or common. if line.startswith(\'- \'): diff_report.append(f\'- {line[2:]}\') only_in_file1 += 1 elif line.startswith(\'+ \'): diff_report.append(f\'+ {line[2:]}\') only_in_file2 += 1 elif line.startswith(\' \'): diff_report.append(f\' {line[2:]}\') common += 1 summary = (f\\"nSummary:n\\" f\\"Lines only in file1: {only_in_file1}n\\" f\\"Lines only in file2: {only_in_file2}n\\" f\\"Common lines: {common}\\") return \'\'.join(diff_report) + summary"},{"question":"**Problem Statement:** You are tasked with developing a Python function that caters to a logistical problem where we need to calculate the most cost-effective route for delivering goods. Specifically, you need to create a function that: 1. Calculates the Euclidean distance between multiple delivery points. 2. Computes the factorial of the number of items to be delivered to generate all possible permutation routes. 3. Utilizes mathematical functions efficiently to provide insights based on the data. **Function Signature:** ```python def delivery_route_insights(points, items): Calculate the delivery route insights. Parameters: points (list of tuples): A list of (x, y) coordinates representing delivery points. items (int): The total number of items to be delivered. Returns: result (tuple): A tuple containing: - The total Euclidean distance between the delivery points. - The number of possible permutations of delivering the items. pass ``` **Detailed Requirements:** 1. **Calculate the Euclidean Distance:** Use the `math.dist` function to calculate the total Euclidean distance between given delivery points in the list. Example: ```python points = [(0, 0), (3, 4)] Total Distance: 5.0 # sqrt(3^2 + 4^2) ``` 2. **Calculating Permutations of Deliveries:** - Use the `math.factorial` function to calculate the factorial of the number of items. Example: ```python items = 3 Number of Permutations: 6 # 3! = 3*2*1 = 6 ``` **Constraints:** - `points` should be a list of tuples where each tuple represents the (x, y) coordinates of a delivery point. - `items` is an integer representing the number of items to be delivered and should be non-negative. **Example:** ```python >>> points = [(0, 0), (3, 4), (6, 8)] >>> items = 3 >>> delivery_route_insights(points, items) (10.0, 6) ``` The total distance is calculated based on the summation of distances between consecutive points, and the number of permutations is derived from the factorial of the items. **Your task is to implement the `delivery_route_insights` function.**","solution":"import math from math import factorial def delivery_route_insights(points, items): Calculate the delivery route insights. Parameters: points (list of tuples): A list of (x, y) coordinates representing delivery points. items (int): The total number of items to be delivered. Returns: result (tuple): A tuple containing: - The total Euclidean distance between the delivery points. - The number of possible permutations of delivering the items. # Calculate the total Euclidean distance total_distance = 0.0 for i in range(1, len(points)): total_distance += math.dist(points[i-1], points[i]) # Calculate the number of possible permutations permutations = math.factorial(items) return total_distance, permutations"},{"question":"In this assessment, you will demonstrate your understanding of the `seaborn` package and particularly its color palette functionalities. You will be tasked with creating a custom color palette and applying it to a seaborn data visualization. Objective Your task is to: 1. Create a blended color palette using the `sns.blend_palette` function. 2. Apply this palette to a seaborn scatter plot, and customize the plot according to the specifications provided. Requirements 1. Create a continuous colormap from three colors using the `sns.blend_palette` function: - Use the colors: `\\"#1f77b4\\"`, `\\"#ff7f0e\\"`, and `\\"darkgreen\\"`. - The colormap should be continuous (i.e., interpolate=True or as_cmap=True). 2. Use the **Iris** dataset (available via `seaborn.load_dataset(\'iris\')`): - Create a scatter plot of `sepal_length` vs `sepal_width`. - Color the points by the `petal_length` column using the previously created colormap. 3. Customize the scatter plot: - Set a title of \\"Sepal Dimensions Colored by Petal Length\\". - Add appropriate axis labels for x (Sepal Length (cm)) and y (Sepal Width (cm)). - Include a color bar to show the mapping of `petal_length` to the colormap. Constraints - Ensure your solution is efficient and doesn\'t use unnecessary loops or redundant operations. - Pay special attention to the aesthetic details as specified, leveraging seaborn\'s capabilities. Input No direct input is required to your function; it should perform all required tasks internally. Output You should output/display the customized scatter plot with all the specified properties. Example Code ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Step 1: Create the blended palette palette = sns.blend_palette([\\"#1f77b4\\", \\"#ff7f0e\\", \\"darkgreen\\"], as_cmap=True) # Step 2: Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Step 3: Create the scatter plot plt.figure(figsize=(8, 6)) scatter = plt.scatter(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", c=iris[\\"petal_length\\"], cmap=palette) cb = plt.colorbar(scatter) cb.set_label(\\"Petal Length (cm)\\") # Customize the plot plt.title(\\"Sepal Dimensions Colored by Petal Length\\") plt.xlabel(\\"Sepal Length (cm)\\") plt.ylabel(\\"Sepal Width (cm)\\") # Display the plot plt.show() ``` Make sure that your plot meets the requirements and looks visually appealing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_scatter_plot(): Creates a scatter plot of sepal length vs sepal width from the Iris dataset, colored by petal length using a custom blended color palette. # Step 1: Create the blended palette colors = [\\"#1f77b4\\", \\"#ff7f0e\\", \\"darkgreen\\"] palette = sns.blend_palette(colors, as_cmap=True) # Step 2: Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Step 3: Create the scatter plot plt.figure(figsize=(8, 6)) scatter = plt.scatter(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", c=iris[\\"petal_length\\"], cmap=palette) cb = plt.colorbar(scatter) cb.set_label(\\"Petal Length (cm)\\") # Customize the plot plt.title(\\"Sepal Dimensions Colored by Petal Length\\") plt.xlabel(\\"Sepal Length (cm)\\") plt.ylabel(\\"Sepal Width (cm)\\") # Display the plot plt.show()"},{"question":"# Challenge: Custom Function Introspection Tool In this challenge, you are required to implement a custom function introspection tool in Python. The goal is to create a set of utility functions that can analyze and manipulate Python function objects. You must demonstrate an understanding of both basic and advanced concepts related to Python functions. Task: 1. **Create Function Wrapper**: Implement a function `create_function_wrapper(func, enhance=True)` that takes another function `func` and an optional boolean parameter `enhance`. It should return a new function that: - If `enhance` is `True`, the new function appends the result of the original function with a custom message, \\" - Enhanced by Wrapper\\". - If `enhance` is `False`, the new function behaves just like the original function. 2. **Function Analysis Tool**: Implement a function `analyze_function(func)` that takes a Python function object `func` as input and returns a dictionary containing the following information about the function: - `name`: The name of the function. - `docstring`: The function\'s docstring. - `annotations`: A dictionary containing the function\'s annotations. - `defaults`: A tuple of the function\'s default argument values. 3. **Function Modifier**: Implement a function `modify_function_annotations(func, annotations)` that takes a function object `func` and a dictionary `annotations`. This function should update the annotations of `func` with the provided `annotations` and return the modified function. Constraints: - You should not use any additional third-party libraries. - Focus on working directly with function attributes. - Ensure your solution handles various edge cases gracefully. Example: ```python # Example functions def sample_function(x: int, y: str = \\"default\\") -> str: This is a sample function return f\\"{x} - {y}\\" # Create Function Wrapper enhanced_function = create_function_wrapper(sample_function, enhance=True) print(enhanced_function(10, \\"test\\")) # Output: \\"10 - test - Enhanced by Wrapper\\" # Analyze Function analysis = analyze_function(sample_function) print(analysis) # Output: # { # \'name\': \'sample_function\', # \'docstring\': \'This is a sample function\', # \'annotations\': {\'x\': int, \'y\': str, \'return\': str}, # \'defaults\': (\'default\', ) # } # Modify Annotations modified_function = modify_function_annotations(sample_function, {\'x\': float, \'y\': str}) print(modified_function.__annotations__) # Output: {\'x\': float, \'y\': str, \'return\': str} ```","solution":"def create_function_wrapper(func, enhance=True): Wraps the given function. If enhance is True, appends \\" - Enhanced by Wrapper\\" to the result. def wrapper(*args, **kwargs): result = func(*args, **kwargs) if enhance: return f\\"{result} - Enhanced by Wrapper\\" return result return wrapper def analyze_function(func): Analyzes the given function and returns a dictionary with details about the function. return { \'name\': func.__name__, \'docstring\': func.__doc__, \'annotations\': func.__annotations__, \'defaults\': func.__defaults__ } def modify_function_annotations(func, annotations): Modifies the annotations of the given function with the provided dictionary of annotations. func.__annotations__.update(annotations) return func"},{"question":"**Objective:** To assess the understanding of list operations, comprehensions, and the ability to manipulate complex data structures. **Problem Statement:** You are required to implement a function `process_data` that performs a series of operations on a list of data. The data consists of tuples, where each tuple contains a string (representing a category) and an integer (representing a value). The function should: 1. Group the data by category. 2. Calculate the sum of values for each category. 3. Return a dictionary with the categories as keys and the sum of their values as values. 4. Exclude any values corresponding to a specific category (\'exclude\') before processing. # Function Signature ```python def process_data(data: list) -> dict: ``` # Input - A list of tuples: `data`, where each tuple contains: - A string representing the category. - An integer representing the value. # Output - A dictionary with categories as keys and the sum of values for each category as values. Categories with a value to be excluded should not be present in the output. # Constraints - The input list can contain 0 to 10^6 tuples. - Each category name is a string and can be up to 255 characters long. - Each value is an integer between -10^6 and 10^6. # Example ```python data = [ (\'fruit\', 10), (\'vegetable\', 15), (\'fruit\', 5), (\'exclude\', 20), (\'vegetable\', 10), (\'meat\', 25) ] output = process_data(data) print(output) # {\'fruit\': 15, \'vegetable\': 25, \'meat\': 25} ``` # Additional Notes - Ensure your solution is optimized for performance given the constraints. - Make use of relevant list methods and comprehensions where applicable. - The function should handle edge cases gracefully, such as empty input list or all categories being \'exclude\'. # Hints - Consider using a dictionary to group and sum the values efficiently. - Use list comprehensions to filter out unwanted categories.","solution":"def process_data(data: list) -> dict: Processes the data to group by category and sum the values for each category, excluding any tuples with the category \'exclude\'. from collections import defaultdict result = defaultdict(int) for category, value in data: if category != \'exclude\': result[category] += value return dict(result)"},{"question":"# Unicode String Manipulation and Encoding/Decoding Problem Statement You are tasked with creating a function that processes a list of Unicode strings. The function should: 1. Normalize the strings to a consistent form using NFC (Normalization Form C). 2. Convert each normalized string to its UTF-8 encoded byte representation. 3. Decode these byte representations back to Unicode strings. 4. Make case-insensitive comparisons of the decoded strings and return a list of unique strings (case-insensitively unique). Function Signature ```python def process_unicode_strings(strings: List[str]) -> List[str]: Process a list of Unicode strings by normalizing, encoding to UTF-8, decoding back to Unicode, and returning a list of case-insensitively unique strings. Parameters: strings (List[str]): A list of Unicode strings. Returns: List[str]: A list of case-insensitively unique Unicode strings. ``` Input - `strings`: A list of Unicode strings with a length of at most 1000 elements. Each element is a non-empty string with a maximum length of 1000 characters. Output - A list of Unicode strings that are case-insensitively unique. Example Suppose we have the following input list: ```python input_strings = [ \'cafÃ©\', \'CafÃ©\', \'CAFÃ‰\', \'CAFÃ‰\', \'u0065u0301\', # Unicode representation of Ã© \'eu0301\' ] ``` The output should be: ```python output = [\'cafÃ©\', \'u0065u0301\', \'eu0301\'] ``` Constraints - Handle cases with different Unicode representations of the same character (e.g., composed and decomposed forms). - Consider performance optimizations for the given input constraints. Notes - Use the `unicodedata` module for normalization. - Ensure that decoding errors are handled gracefully, e.g., using `backslashreplace` in `decode()`. Implementation You should provide a clean and efficient implementation that follows best practices for handling Unicode data in Python.","solution":"import unicodedata def process_unicode_strings(strings): Process a list of Unicode strings by normalizing, encoding to UTF-8, decoding back to Unicode, and returning a list of case-insensitively unique strings. Parameters: strings (List[str]): A list of Unicode strings. Returns: List[str]: A list of case-insensitively unique Unicode strings. # Normalize the strings to NFC (Normalization Form C) normalized_strings = [unicodedata.normalize(\'NFC\', s) for s in strings] # Encode each normalized string to UTF-8 and decode back to Unicode decoded_strings = [s.encode(\'utf-8\').decode(\'utf-8\', errors=\'backslashreplace\') for s in normalized_strings] # Collect unique strings, case-insensitively unique_strings_dict = {} for s in decoded_strings: key = s.lower() if key not in unique_strings_dict: unique_strings_dict[key] = s return list(unique_strings_dict.values())"},{"question":"# Abstract Base Class Implementation and Usage Objective: Implement an abstract base class (ABC) for a simple payment system using the `abc` module. You will define a few abstract methods that derived classes should implement to process different types of payments. Additionally, practice the registration of concrete classes as virtual subclasses. Task: 1. Define an abstract base class `PaymentProcessor` using the `abc` module. 2. This class should have the following abstract methods: - `process_payment(amount: float) -> bool` that processes a payment and returns a boolean indicating success or failure. - `validate_transaction(transaction_id: str) -> bool` that validates a payment transaction. 3. Define a concrete class `CreditCardPayment` that inherits from `PaymentProcessor` and implements the abstract methods. 4. Define another concrete class `PaypalPayment` outside of the `PaymentProcessor` inheritance chain. Register this class as a virtual subclass of `PaymentProcessor` using `register`. 5. Write a function `main()` to demonstrate creating instances of `CreditCardPayment` and `PaypalPayment`, validating transactions, and processing payments. Ensure that the function demonstrates that both instances work correctly with the `PaymentProcessor` interface. Input and Output Format: - **Input**: There are no user inputs. You will be demonstrating functionality using test cases in your `main()` function. - **Output**: Print outputs of validation and processing of payments, which should be boolean values indicating success or failure. Constraints: - Implement missing methods in `CreditCardPayment` and `PaypalPayment` to return `True` for the sake of this exercise. - Use `abc` module to achieve the functionality. - No network calls or actual payment processing; implementations should simply return `True`. Example: ```python from abc import ABC, abstractmethod class PaymentProcessor(ABC): @abstractmethod def process_payment(self, amount: float) -> bool: pass @abstractmethod def validate_transaction(self, transaction_id: str) -> bool: pass class CreditCardPayment(PaymentProcessor): # Implement abstract methods here class PaypalPayment: # Implement methods here # Register PaypalPayment as a virtual subclass PaymentProcessor.register(PaypalPayment) def main(): cc_payment = CreditCardPayment() paypal_payment = PaypalPayment() # Demonstrate processing payments and validating transactions print(cc_payment.process_payment(100.0)) # Expected output: True print(cc_payment.validate_transaction(\\"TXN123\\")) # Expected output: True print(paypal_payment.process_payment(150.0)) # Expected output: True print(paypal_payment.validate_transaction(\\"TXN456\\")) # Expected output: True if __name__ == \\"__main__\\": main() ``` This task will help evaluate the student\'s understanding of abstract base classes, method overriding, and the concept of virtual subclasses using the `abc` module in Python.","solution":"from abc import ABC, abstractmethod class PaymentProcessor(ABC): @abstractmethod def process_payment(self, amount: float) -> bool: pass @abstractmethod def validate_transaction(self, transaction_id: str) -> bool: pass class CreditCardPayment(PaymentProcessor): def process_payment(self, amount: float) -> bool: # Assume the payment processing is always successful and return True print(f\\"Processing credit card payment of {amount}\\") return True def validate_transaction(self, transaction_id: str) -> bool: # Assume the transaction validation is always successful and return True print(f\\"Validating credit card transaction {transaction_id}\\") return True class PaypalPayment: def process_payment(self, amount: float) -> bool: # Assume the payment processing is always successful and return True print(f\\"Processing PayPal payment of {amount}\\") return True def validate_transaction(self, transaction_id: str) -> bool: # Assume the transaction validation is always successful and return True print(f\\"Validating PayPal transaction {transaction_id}\\") return True # Register PaypalPayment as a virtual subclass of PaymentProcessor PaymentProcessor.register(PaypalPayment) def main(): cc_payment = CreditCardPayment() paypal_payment = PaypalPayment() # Demonstrate processing payments and validating transactions print(cc_payment.process_payment(100.0)) # Expected output: True print(cc_payment.validate_transaction(\\"TXN123\\")) # Expected output: True print(paypal_payment.process_payment(150.0)) # Expected output: True print(paypal_payment.validate_transaction(\\"TXN456\\")) # Expected output: True if __name__ == \\"__main__\\": main()"},{"question":"Objective You need to demonstrate your understanding of pandas\' memory usage reporting, UDF application, and handling missing values in a DataFrame. Problem Statement You are provided with the following pandas DataFrame: ```python import pandas as pd import numpy as np dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") df[\\"missing_int\\"] = pd.Series([1, 2, None, 4, 5], dtype=pd.Int64Dtype()).reindex(range(n)) ``` 1. **Memory Usage**: - Write a function `get_memory_usage(df)` that returns the memory usage of the given DataFrame in human-readable units, both with default and deep introspection. 2. **Handling missing values**: - Write a function `fill_missing_with_mean(df, column)` that takes a DataFrame and a column name, and returns a new DataFrame with missing values in the specified numeric column replaced by the column\'s mean. 3. **Applying UDF**: - Write a function `apply_custom_function(df)` that takes a DataFrame and applies the following custom function to each row: ```python def custom_function(row): if isinstance(row[\'int64\'], pd._libs.missing.NAType): row[\'int64\'] = row[\'float64\'] * 2 return row ``` - Ensure that the original DataFrame is not mutated during this operation and return the resulting DataFrame. Constraints - You may use built-in pandas methods only where explicitly mentioned. - Ensure your implementation is efficient and follows best practices in pandas. Example ```python # Assuming df is the provided DataFrame memory_usage_default, memory_usage_deep = get_memory_usage(df) print(memory_usage_default) print(memory_usage_deep) df_filled = fill_missing_with_mean(df, \\"missing_int\\") print(df_filled[\\"missing_int\\"].isna().sum()) # Output should be 0 df_applied = apply_custom_function(df) print(df_applied.head()) ``` Submission Submit your implementation as a single Python script or Jupyter notebook.","solution":"import pandas as pd def get_memory_usage(df): Returns the memory usage of the given DataFrame in human-readable units, both with default and deep introspection. def human_readable_size(size): for unit in [\'B\', \'KB\', \'MB\', \'GB\', \'TB\']: if size < 1024: return f\\"{size:.2f} {unit}\\" size /= 1024 memory_usage_default = df.memory_usage(deep=False).sum() memory_usage_deep = df.memory_usage(deep=True).sum() return human_readable_size(memory_usage_default), human_readable_size(memory_usage_deep) def fill_missing_with_mean(df, column): Returns a new DataFrame with missing values in the specified numeric column replaced by the column\'s mean. df_copy = df.copy() mean_value = df_copy[column].mean() df_copy[column].fillna(mean_value, inplace=True) return df_copy def apply_custom_function(df): Applies a custom function to each row of the DataFrame. def custom_function(row): if pd.isna(row[\'int64\']): row[\'int64\'] = row[\'float64\'] * 2 return row df_custom = df.apply(custom_function, axis=1) return df_custom"},{"question":"Objective: Your task is to write a Python function that utilizes the `posix`, `pwd`, and `syslog` modules to perform the following operations: 1. **Ensure Large File Support**: Verify that the system supports large files using the `posix` module. 2. **Retrieve User Information**: Fetch information of a specific user from the password database using the `pwd` module. 3. **Log Information**: Log messages to the Unix system log using the `syslog` module. Function Specification: Write a function `check_system_and_log(user: str) -> None` that performs the operations described above. # Input: - `user` (str): The username of the user whose information you want to retrieve. # Output: - The function does not return any value, but it should log messages to the Unix syslog. # Constraints: 1. The function should log an error and exit if the system does not support large files. 2. If the specified user does not exist in the password database, log an error message. 3. Log retrieved user information including the user ID, home directory, and shell in the syslog. # Example Log Output: ``` INFO: System supports large files. INFO: Retrieving information for user \'johndoe\'. INFO: User ID: 1001, Home Directory: /home/johndoe, Shell: /bin/bash ERROR: User \'nonexistentuser\' does not exist. ``` Notes: - Utilize the `os` module for checking large file support. - Use the `pwd` module to fetch user information. - Use the `syslog` module for logging messages with appropriate log levels (INFO, ERROR). Good luck!","solution":"import os import pwd import syslog def check_system_and_log(user: str) -> None: # Ensure Large File Support if not hasattr(os, \'posix_fadvise\'): syslog.syslog(syslog.LOG_ERR, \\"System does not support large files.\\") return syslog.syslog(syslog.LOG_INFO, \\"System supports large files.\\") # Retrieve User Information try: user_info = pwd.getpwnam(user) user_id = user_info.pw_uid home_dir = user_info.pw_dir shell = user_info.pw_shell syslog.syslog(syslog.LOG_INFO, f\\"User ID: {user_id}, Home Directory: {home_dir}, Shell: {shell}\\") except KeyError: syslog.syslog(syslog.LOG_ERR, f\\"User \'{user}\' does not exist.\\")"},{"question":"# Question You are given a DataFrame containing information about employees in a company. Each employee has a name, department, and salary. Your task is to implement a function `update_salaries` that ensures the modification of salaries obeys the Copy-on-Write principles in pandas. It will make use of the `loc` method to update the salary of all employees in a specified department by a certain percentage increase. # Function Signature ```python import pandas as pd def update_salaries(df: pd.DataFrame, department: str, percentage_increase: float) -> pd.DataFrame: Update salaries of employees in a specific department by a given percentage. Parameters: df (pd.DataFrame): DataFrame containing \'name\', \'department\', and \'salary\' columns. department (str): The department to update salaries for. percentage_increase (float): The percentage to increase the salary by. Returns: pd.DataFrame: The updated DataFrame with increased salaries for the specified department. # Your implementation here ``` # Input - `df` (pd.DataFrame): A pandas DataFrame with at least the following columns: \'name\', \'department\', \'salary\'. - `department` (str): The department for which to update the salaries. - `percentage_increase` (float): The percentage increase for the salaries (e.g., 5 for a 5% increase). # Output - The function should return a new DataFrame with salaries updated by the given percentage for all employees in the specified department. Other parts of the DataFrame should remain unchanged. # Constraints - Do not directly modify the input DataFrame; always return a new DataFrame after making changes. - Use the `loc` method to handle the updates. - Ensure that the function works efficiently even for large DataFrames. # Example ```python data = { \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'department\': [\'HR\', \'Engineering\', \'Engineering\', \'HR\'], \'salary\': [50000, 70000, 80000, 60000] } df = pd.DataFrame(data) updated_df = update_salaries(df, \'Engineering\', 10) print(\\"Original DataFrame:n\\", df) print(\\"Updated DataFrame:n\\", updated_df) ``` **Expected Output:** ``` Original DataFrame: name department salary 0 Alice HR 50000 1 Bob Engineering 70000 2 Charlie Engineering 80000 3 David HR 60000 Updated DataFrame: name department salary 0 Alice HR 50000 1 Bob Engineering 77000 2 Charlie Engineering 88000 3 David HR 60000 ```","solution":"import pandas as pd def update_salaries(df: pd.DataFrame, department: str, percentage_increase: float) -> pd.DataFrame: Update salaries of employees in a specific department by a given percentage. Parameters: df (pd.DataFrame): DataFrame containing \'name\', \'department\', and \'salary\' columns. department (str): The department to update salaries for. percentage_increase (float): The percentage to increase the salary by. Returns: pd.DataFrame: The updated DataFrame with increased salaries for the specified department. # Create a copy of the DataFrame to ensure we do not modify the original updated_df = df.copy() # Calculate the multiplier based on the percentage increase multiplier = 1 + (percentage_increase / 100.0) # Update the salaries using loc updated_df.loc[updated_df[\'department\'] == department, \'salary\'] *= multiplier return updated_df"},{"question":"Objective: Demonstrate your understanding of the `xml.dom.pulldom` package by parsing an XML document, processing specific elements, and using the methods and event types defined by the module. Problem Statement: You are given an XML document that lists information about various books. Your task is to write a function `filter_expensive_books(xml_string)`, which takes an XML string as input and returns a list containing the titles of all books that are priced above 20. Requirements: 1. Use the `xml.dom.pulldom` module to parse the XML document. 2. Pull events from the XML stream, and when a `START_ELEMENT` event for a book is encountered, check its price. 3. If a book\'s price is more than 20, expand the node and extract the book\'s title. 4. Return a list of titles of the books meeting the price criterion. Input: - `xml_string` (str): A string representing the XML document. Output: - `expensive_titles` (list of str): A list of titles of books priced above 20. Example XML: ```xml <books> <book id=\\"1\\" price=\\"15\\"> <title>The Art of War</title> </book> <book id=\\"2\\" price=\\"25\\"> <title>Clean Code</title> </book> <book id=\\"3\\" price=\\"30\\"> <title>The Pragmatic Programmer</title> </book> </books> ``` Example Function Call: ```python xml_data = <books> <book id=\\"1\\" price=\\"15\\"> <title>The Art of War</title> </book> <book id=\\"2\\" price=\\"25\\"> <title>Clean Code</title> </book> <book id=\\"3\\" price=\\"30\\"> <title>The Pragmatic Programmer</title> </book> </books> expensive_books = filter_expensive_books(xml_data) print(expensive_books) # Output: [\'Clean Code\', \'The Pragmatic Programmer\'] ``` Constraints: - Assume the XML conforms to the structure provided in the example. - You are allowed to use only the `xml.dom.pulldom` module for parsing the XML. Performance: - The solution should efficiently handle parsing and processing of XML strings containing up to 10,000 book entries. Implementation: Provide the implementation for the `filter_expensive_books(xml_string)` function below: ```python from xml.dom import pulldom def filter_expensive_books(xml_string): # Your code here pass ```","solution":"from xml.dom import pulldom def filter_expensive_books(xml_string): # Create a list to store titles of expensive books expensive_titles = [] # Parse the XML string events = pulldom.parseString(xml_string) for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \'book\': # Get the price attribute price = float(node.getAttribute(\'price\')) if price > 20: # Expand the node to read its children events.expandNode(node) # Get the title element text content title_node = node.getElementsByTagName(\'title\')[0] title = title_node.firstChild.data # Add the title to the list expensive_titles.append(title) return expensive_titles"},{"question":"Objective: You are given the task of creating a program that generates a multipart email message with attachments. This task will assess your understanding of creating and manipulating email messages using the `email.message.Message` class in the \\"compat32\\" API. Task: 1. Create a multipart email message with the following structure: - A text payload as the main body of the email. - Two binary file attachments with appropriate MIME types. - Add appropriate email headers, including \\"From\\", \\"To\\", \\"Subject\\", \\"Content-Type\\", and \\"MIME-Version\\". 2. Implement the following functionalities: - **Function `create_multipart_message(from_addr, to_addr, subject, body_text, attachments)`**: - `from_addr`: The sender\'s email address (string). - `to_addr`: The recipient\'s email address (string). - `subject`: Subject of the email (string). - `body_text`: The main text body of the email (string). - `attachments`: A list of tuples, with each tuple containing: - `filename`: The name of the file (string). - `file_content`: The binary content of the file (bytes). - `mime_type`: The MIME type of the file (string). - Returns a `Message` object representing the complete email. - **Function `serialize_message_to_string(msg)`**: - `msg`: A `Message` object created by `create_multipart_message`. - Returns a string representation of the entire email message (including headers and payload). - **Function `deserialize_message_from_string(string_repr)`**: - `string_repr`: A string representation of an email message. - Returns a `Message` object representing the deserialized email. Constraints: - Ensure that the file attachments are correctly encoded in the email. - Handle different MIME types appropriately. - Guarantee that the email message can be serialized to a string and then deserialized back to a `Message` object with the original structure intact. Example Usage: ```python from email_utils import create_multipart_message, serialize_message_to_string, deserialize_message_from_string attachments = [ (\\"file1.txt\\", b\\"Hello, this is the content of file1.\\", \\"text/plain\\"), (\\"image.png\\", b\\"x89PNGrnx1an...\\", \\"image/png\\") ] msg = create_multipart_message(\\"sender@example.com\\", \\"recipient@example.com\\", \\"Subject Line\\", \\"This is the body of the email.\\", attachments) string_repr = serialize_message_to_string(msg) deserialized_msg = deserialize_message_from_string(string_repr) ``` Expected Output: - Implement the functions correctly so that the example usage demonstrates the creation of a multipart email, its serialization to a string, and subsequent deserialization back to a `Message` object with all headers and payloads intact. Performance Requirements: - The solution should be efficient in terms of both time and space complexity. - The solution must handle large attachments and multiple multipart sub-messages effectively.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders from email import message_from_string def create_multipart_message(from_addr, to_addr, subject, body_text, attachments): Creates a multipart email message with text and binary attachments. # Create the root message msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject msg[\'MIME-Version\'] = \'1.0\' msg[\'Content-Type\'] = \'multipart/mixed\' # Attach the main body text text_part = MIMEText(body_text, \'plain\') msg.attach(text_part) # Attach the binary files for filename, file_content, mime_type in attachments: part = MIMEBase(*mime_type.split(\'/\')) part.set_payload(file_content) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\') msg.attach(part) return msg def serialize_message_to_string(msg): Serializes a Message object to a string. return msg.as_string() def deserialize_message_from_string(string_repr): Deserializes a string representation of an email message back to a Message object. return message_from_string(string_repr)"},{"question":"Objective: Demonstrate your understanding and mastery of assignment statements, augmented assignment statements, and annotated assignment statements by implementing a function that manipulates a list of values through these kinds of statements. Task: You need to implement a Python function `process_values(values: List[int]) -> List[int]` that processes a list of integers `values` according to the following rules: 1. Assign the value 10 to the first element of the list. 2. Add 5 to the second element of the list using augmented assignment. 3. Create a new annotated variable with the value of the third element multiplied by 2. 4. Swap the values of the third and fourth elements using tuple unpacking. 5. Split the remaining elements of the list into two parts: the first part excluding the last two elements, and the second part containing the last two elements. 6. Assign the last two elements to new variables and return the list. Input Format: - A list of integers `values` where the length of the list is guaranteed to be at least 6. Output Format: - The modified list of integers according to the operations specified. Constraints: - The length of `values` will be at least 6. - The elements of `values` can be any integer. Example: ```python def process_values(values: List[int]) -> List[int]: # Step 1 values[0] = 10 # Step 2 values[1] += 5 # Step 3 annotated_value: int = values[2] * 2 # Step 4 values[2], values[3] = values[3], values[2] # Step 5 first_part, *rest = values[:-2], values[-2:] last_second, last_first = rest # Returns the updated list return values # Example Usage input_values = [1, 2, 3, 4, 5, 6] output_values = process_values(input_values) print(output_values) # Output: [10, 7, 4, 3, 5, 6] ``` In this task, ensure that you handle the variable assignments as described, and use appropriate statement types for each step. Performance Consideration: - Your solution should run efficiently with an understanding of list operations.","solution":"from typing import List def process_values(values: List[int]) -> List[int]: # Step 1: Assign the value 10 to the first element of the list. values[0] = 10 # Step 2: Add 5 to the second element of the list using augmented assignment. values[1] += 5 # Step 3: Create a new annotated variable with the value of the third element multiplied by 2. annotated_value: int = values[2] * 2 # Step 4: Swap the values of the third and fourth elements using tuple unpacking. values[2], values[3] = values[3], values[2] # Step 5: Split the remaining elements of the list into two parts: the first part excluding the last two elements, # and the second part containing the last two elements. first_part = values[:-2] second_part = values[-2:] # Step 6: Assign the last two elements to new variables and return the list. last_second, last_first = second_part # Return the modified list return values"},{"question":"**Objective:** Write a function using the seaborn library to create customized visualizations for analyzing datasets. **Function Signature:** ```python def customized_seaborn_plot(data: pd.DataFrame, x: str, y: str, hue: str = None, kind: str = \\"scatter\\", **kwargs) -> sns.JointGrid: Create a customized joint plot using seaborn. Parameters: - data (pd.DataFrame): Pandas DataFrame containing the data. - x (str): Column name for the x-axis variable. - y (str): Column name for the y-axis variable. - hue (str, optional): Column name for conditional coloring. Defaults to None. - kind (str, optional): Type of joint plot to draw. Options: [\\"scatter\\", \\"kde\\", \\"reg\\", \\"hist\\", \\"hex\\"]. Defaults to \\"scatter\\". - **kwargs: Additional keyword arguments passed to seaborn. Returns: - sns.JointGrid: The resulting seaborn JointGrid object. pass ``` **Instructions:** 1. **Create Joint Plot:** - The function should utilize `sns.jointplot` to create the joint plot with the specified `x` and `y` column names. - The `kind` parameter should be used to determine the type of plot (e.g., \\"scatter\\", \\"kde\\", \\"reg\\", \\"hist\\", \\"hex\\"). 2. **Conditional Coloring:** - If a `hue` parameter is provided, add the hue to the plot to show conditional coloring based on another column in the DataFrame. 3. **Additional Customization:** - Allow passing additional keyword arguments to customize the plot further. - Use at least one example of a complex layered plot using the `JointGrid` methods as shown in the documentation. 4. **Return Value:** - The function should return the resulting `JointGrid` object. **Constraints:** - Use the Penguins dataset (`sns.load_dataset(\\"penguins\\")`) for testing. - Ensure that all parameters have the correct types and values. - Handle cases where the `hue` parameter or other customized parameters are not provided. **Example Usage:** ```python import pandas as pd import seaborn as sns # Load sample data penguins = sns.load_dataset(\\"penguins\\") # Example usage of the function result = customized_seaborn_plot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", marker=\\"+\\", s=100, marginal_kws=dict(bins=25, fill=False) ) ``` Use this function to ensure students understand how to effectively use seaborn for creating advanced and customized joint plots with various types of data visualization techniques.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customized_seaborn_plot(data: pd.DataFrame, x: str, y: str, hue: str = None, kind: str = \\"scatter\\", **kwargs) -> sns.JointGrid: Create a customized joint plot using seaborn. Parameters: - data (pd.DataFrame): Pandas DataFrame containing the data. - x (str): Column name for the x-axis variable. - y (str): Column name for the y-axis variable. - hue (str, optional): Column name for conditional coloring. Defaults to None. - kind (str, optional): Type of joint plot to draw. Options: [\\"scatter\\", \\"kde\\", \\"reg\\", \\"hist\\", \\"hex\\"]. Defaults to \\"scatter\\". - **kwargs: Additional keyword arguments passed to seaborn. Returns: - sns.JointGrid: The resulting seaborn JointGrid object. # Create joint plot with the given parameters joint_plot = sns.jointplot(data=data, x=x, y=y, hue=hue, kind=kind, **kwargs) # Additional customization options, if any, can be added here plt.show() return joint_plot"},{"question":"# Asynchronous File Downloader with Retry Logic You are tasked with implementing an asynchronous file downloader that downloads multiple files concurrently with the ability to retry downloading in case of failure. This function should make use of asyncio\'s task management, synchronization, and exception handling functionalities. Requirements: 1. Implement an asynchronous function `download_file(url: str) -> str` that simulates downloading a file from the given URL. Use `asyncio.sleep` to mock network delay. 2. Implement an asynchronous function `download_files(urls: List[str]) -> List[str]` that: - Downloads multiple files concurrently. - Uses an asyncio queue to manage download tasks. - Retries downloading a file up to 3 times if it fails (use a custom exception `DownloadError` to simulate failure). - If a file cannot be downloaded after 3 attempts, log an error and continue with other downloads. 3. Return a list of successfully downloaded file URLs. Input Format: - A list of URLs (strings) to download files from. Output Format: - A list of URLs (strings) representing successfully downloaded files. Example: ```python urls = [ \\"http://example.com/file1\\", \\"http://example.com/file2\\", \\"http://example.com/file3\\" ] successful_downloads = await download_files(urls) print(successful_downloads) ``` Constraints: - The maximum number of concurrent download tasks should be 5. - You can simulate download failure by randomly raising a `DownloadError` exception in the `download_file` function. Implementation Guidelines: - Use `asyncio.Queue` to manage download tasks. - Handle concurrency with `asyncio.gather` and ensure a maximum of 5 concurrent downloads. - Implement retry logic with exception handling for `DownloadError`. - Ensure the function is robust and handles all edge cases, including timeouts and cancellations. Performance Requirements: - The function should efficiently manage concurrent tasks and minimize overall download time. - Properly handle asynchronous exceptions and ensure the program does not crash due to unhandled errors. Good luck!","solution":"import asyncio import random from typing import List # Custom Exception for download errors class DownloadError(Exception): pass async def download_file(url: str) -> str: Simulates downloading a file from the given URL. It raises DownloadError with a 30% probability to simulate a download failure. try: await asyncio.sleep(random.uniform(0.1, 1.0)) # Simulate network delay if random.random() < 0.3: # 30% chance of failure raise DownloadError(f\\"Failed to download {url}\\") return url # Simulate successful download except DownloadError as e: raise e async def download_files(urls: List[str]) -> List[str]: Downloads multiple files concurrently with retry logic. Retries downloading a file up to 3 times if it fails. results = [] semaphore = asyncio.Semaphore(5) # Limit concurrent downloads to 5 async def download_with_retries(url): retries = 3 for attempt in range(retries): try: async with semaphore: result = await download_file(url) print(f\\"Successfully downloaded: {url}\\") return result except DownloadError as e: print(f\\"Retrying {url} (attempt {attempt + 1} of {retries})\\") if attempt == retries - 1: print(f\\"Failed to download {url} after {retries} attempts.\\") return None tasks = [download_with_retries(url) for url in urls] downloaded_files = await asyncio.gather(*tasks) for file in downloaded_files: if file: results.append(file) return results"},{"question":"**Question:** You are tasked with creating an asynchronous Python application that performs multiple tasks concurrently. Your application needs to handle API requests, perform CPU-intensive calculations, and manage logging asynchronously. The following requirements should be met: 1. Implement an asynchronous function `fetch_data_from_api` that mimics fetching data from an API. This function should simulate network delay using `asyncio.sleep()`. 2. Implement a CPU-intensive synchronous function `perform_heavy_computation` which performs some heavy computation (e.g., calculating the nth Fibonacci number). 3. Use `asyncio.run_in_executor` to run `perform_heavy_computation` without blocking the event loop. 4. Schedule tasks using `asyncio.create_task` and ensure proper awaiting of coroutines to avoid `RuntimeWarning`. 5. Implement logging in debug mode and ensure that log messages do not block the main event loop. 6. Handle any potential exceptions within tasks and log them appropriately. # Input: - No direct input required for the functions. # Output: - Fetch and print data from the API. - Perform and print the result of the CPU-intensive computation. # Constraints: - Use asynchronous programming paradigms as discussed. - Ensure that the event loop is not blocked by heavy computations or logging. # Performance: - The solution should be able to handle multiple API requests and computations concurrently without significant delay. # Your Implementation Starts Here: ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor import time # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) # Simulate fetching data from an API async def fetch_data_from_api(api_url): await asyncio.sleep(2) # Simulate a network delay return f\\"Data from {api_url}\\" # Perform a CPU-intensive computation def perform_heavy_computation(n): def fibonacci(num): if num <= 1: return num else: return fibonacci(num - 1) + fibonacci(num - 2) return fibonacci(n) async def main(): api_url = \\"https://example.com/api\\" num = 30 # Example value for CPU-bound task # Create a thread pool executor executor = ThreadPoolExecutor() try: # Fetch data from API asynchronously api_task = asyncio.create_task(fetch_data_from_api(api_url)) # Run heavy computation in an executor computation_task = asyncio.create_task( asyncio.get_event_loop().run_in_executor(executor, perform_heavy_computation, num) ) # Await for the tasks to complete api_result = await api_task computation_result = await computation_task # Print results print(api_result) print(f\\"Fibonacci number for {num} is {computation_result}\\") except Exception as e: logger.error(f\\"Error occurred: {e}\\") finally: executor.shutdown(wait=True) # Run the event loop if __name__ == \\"__main__\\": asyncio.run(main(), debug=True) ``` # Explanation of the Solution: - The `fetch_data_from_api` function simulates an API call using `asyncio.sleep`. - The `perform_heavy_computation` function calculates the nth Fibonacci number, which is a CPU-intensive task. - `asyncio.run_in_executor` is used to run the blocking computation in a different thread to avoid blocking the event loop. - The tasks are scheduled using `asyncio.create_task`, and proper exception handling is implemented. - Logging is configured to debug mode to capture detailed information.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor import time # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) # Simulate fetching data from an API async def fetch_data_from_api(api_url): await asyncio.sleep(2) # Simulate a network delay return f\\"Data from {api_url}\\" # Perform a CPU-intensive computation def perform_heavy_computation(n): def fibonacci(num): if num <= 1: return num else: return fibonacci(num - 1) + fibonacci(num - 2) return fibonacci(n) async def main(): api_url = \\"https://example.com/api\\" num = 30 # Example value for CPU-bound task # Create a thread pool executor executor = ThreadPoolExecutor() try: # Fetch data from API asynchronously api_task = asyncio.create_task(fetch_data_from_api(api_url)) # Run heavy computation in an executor computation_task = asyncio.create_task( asyncio.get_event_loop().run_in_executor(executor, perform_heavy_computation, num) ) # Await for the tasks to complete api_result = await api_task computation_result = await computation_task # Print results print(api_result) print(f\\"Fibonacci number for {num} is {computation_result}\\") except Exception as e: logger.error(f\\"Error occurred: {e}\\") finally: executor.shutdown(wait=True) # Run the event loop if __name__ == \\"__main__\\": asyncio.run(main(), debug=True)"},{"question":"**Problem Statement** You are tasked with creating a utility function that extracts and formats information from the Unix group database using the `grp` module. Your function should be able to filter and structure the data based on provided criteria. # Function Signature ```python def summarize_group_data(criteria: str, value: Union[str, int]) -> List[Dict[str, Union[str, int, List[str]]]]: ``` # Input - `criteria` (str): This will be either \\"gid\\" or \\"name\\", specifying whether to filter by group ID or group name. - `value` (Union[str, int]): This is the value for the provided criteria. If the criteria is \\"gid\\", the value will be an integer group ID. If the criteria is \\"name\\", the value will be a string group name. # Output - The function should return a list of dictionaries. Each dictionary should represent a group with the following keys: \\"name\\" (str), \\"passwd\\" (str), \\"gid\\" (int), and \\"members\\" (List[str]). If no groups match the criteria, return an empty list. # Constraints - The function should handle incorrect types gracefully, raising a `ValueError` with an appropriate message if the `criteria` is not \\"gid\\" or \\"name\\", or if the type of `value` does not match the corresponding criteria. - Ensure that any raised exceptions are handled within the function. # Example ```python # Example 1 criteria = \\"gid\\" value = 1000 result = summarize_group_data(criteria, value) # Expected Output: # [ # { # \\"name\\": \\"examplegroup\\", # \\"passwd\\": \\"x\\", # \\"gid\\": 1000, # \\"members\\": [\\"user1\\", \\"user2\\"] # } # ] # Example 2 criteria = \\"name\\" value = \\"examplegroup\\" result = summarize_group_data(criteria, value) # Expected Output: # [ # { # \\"name\\": \\"examplegroup\\", # \\"passwd\\": \\"x\\", # \\"gid\\": 1000, # \\"members\\": [\\"user1\\", \\"user2\\"] # } # ] ``` # Note - Use the `grp` module to interact with the group database. - Ensure that your function handles cases where the specified group ID or name does not exist, returning an empty list in such cases. - Performance considerations should be taken into account, ensuring that the function is efficient and leverages the module\'s capabilities properly. Good luck!","solution":"import grp from typing import Union, List, Dict def summarize_group_data(criteria: str, value: Union[str, int]) -> List[Dict[str, Union[str, int, List[str]]]]: if not isinstance(criteria, str) or criteria not in [\\"gid\\", \\"name\\"]: raise ValueError(\\"Criteria must be \'gid\' or \'name\'\\") if (criteria == \\"gid\\" and not isinstance(value, int)) or (criteria == \\"name\\" and not isinstance(value, str)): raise ValueError(\\"Value must be an integer for \'gid\' or a string for \'name\'\\") result = [] for group in grp.getgrall(): if (criteria == \\"gid\\" and group.gr_gid == value) or (criteria == \\"name\\" and group.gr_name == value): result.append({ \\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem }) return result"},{"question":"# Custom Scikit-learn Estimator: Implementing a Weighted K-Nearest Neighbors Classifier **Objective:** Develop a custom scikit-learn-compatible estimator implementing a weighted k-nearest neighbors (KNN) classifier. This estimator should utilize the Euclidean distance to find neighbors and assign weights inversely proportional to the distance for voting. **Task:** Create a class `WeightedKNNClassifier` that: 1. Inherits from `BaseEstimator` and `ClassifierMixin`. 2. Has an `__init__` method with parameters `n_neighbors` (number of neighbors) and `weight` (boolean, default: True) to enable/disable weighting. 3. Implements the `fit` method to store the training data. 4. Implements the `predict` method to classify input data based on the weighted vote of its nearest neighbors. 5. Ensures compatibility with pipelines by including `fit` and `predict` methods, handling input validation, and producing expected attributes. 6. Optionally implement a `score` method to return classification accuracy. **Requirements:** 1. `__init__` Method: - Initialize `n_neighbors` and `weight` parameters. - Store them as attributes without additional logic. 2. `fit` Method: - Accept `X` (training data) and `y` (labels). - Store these inputs as attributes `self.X_` and `self.y_`. 3. `predict` Method: - Accept `X` (test data). - For each instance in `X`, compute Euclidean distances to all instances in `self.X_`. - Identify `n_neighbors` nearest neighbors. - Perform weighted or unweighted voting based on the `weight` parameter to determine the predicted class. - Return the predicted labels as a numpy array. 4. Validation: - The `fit` method should validate input shapes and types. - The `predict` method should ensure the estimator is fitted. - Use appropriate methods from `sklearn.utils.validation`. 5. Attributes: - `self.classes_`: Store unique class labels from `y`. - `self.X_`, `self.y_`: Store training data. - Include trailing underscores for fitted attributes. 6. Testing: - Implement a `score` method calculating the accuracy if `fit` and `predict` are implemented. - Ensure transformer passes scikit-learnâ€™s `check_estimator` utility. **Code Template:** ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class WeightedKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, weight=True): self.n_neighbors = n_neighbors self.weight = weight def fit(self, X, y): # Validate input X, y = check_X_y(X, y) self.X_ = X self.y_ = y self.classes_ = np.unique(y) return self def predict(self, X): # Check if fit had been called check_is_fitted(self, [\\"X_\\", \\"y_\\"]) # Input validation X = check_array(X) predictions = [] for x in X: # Compute distances distances = np.sqrt(np.sum((self.X_ - x) ** 2, axis=1)) # Get indices of n_neighbors nearest neighbors nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] if self.weight: nearest_distances = distances[nearest_indices] # Inverse distance weighting weights = 1 / (nearest_distances + 1e-5) # Avoid division by zero weighted_votes = {} for label, weight in zip(nearest_labels, weights): if label in weighted_votes: weighted_votes[label] += weight else: weighted_votes[label] = weight predicted_label = max(weighted_votes, key=weighted_votes.get) else: # Unweighted majority vote predicted_label = np.bincount(nearest_labels).argmax() predictions.append(predicted_label) return np.array(predictions) def score(self, X, y): check_is_fitted(self, [\\"X_\\", \\"y_\\"]) X, y = check_X_y(X, y) y_pred = self.predict(X) return np.mean(y_pred == y) # Example usage if __name__ == \\"__main__\\": from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) knn = WeightedKNNClassifier(n_neighbors=3, weight=True) knn.fit(X_train, y_train) print(f\\"Accuracy: {knn.score(X_test, y_test)}\\") ``` **Evaluation:** - Verify the estimator fits and predicts correctly with example datasets. - Ensure compliance with scikit-learn conventions by running `check_estimator`.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class WeightedKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, weight=True): self.n_neighbors = n_neighbors self.weight = weight def fit(self, X, y): # Validate input X, y = check_X_y(X, y) self.X_ = X self.y_ = y self.classes_ = np.unique(y) return self def predict(self, X): # Check if fit had been called check_is_fitted(self, [\\"X_\\", \\"y_\\"]) # Input validation X = check_array(X) predictions = [] for x in X: # Compute distances distances = np.sqrt(np.sum((self.X_ - x) ** 2, axis=1)) # Get indices of n_neighbors nearest neighbors nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] if self.weight: nearest_distances = distances[nearest_indices] # Inverse distance weighting weights = 1 / (nearest_distances + 1e-5) # Avoid division by zero weighted_votes = {} for label, weight in zip(nearest_labels, weights): if label in weighted_votes: weighted_votes[label] += weight else: weighted_votes[label] = weight predicted_label = max(weighted_votes, key=weighted_votes.get) else: # Unweighted majority vote predicted_label = np.bincount(nearest_labels).argmax() predictions.append(predicted_label) return np.array(predictions) def score(self, X, y): check_is_fitted(self, [\\"X_\\", \\"y_\\"]) X, y = check_X_y(X, y) y_pred = self.predict(X) return np.mean(y_pred == y) # Example usage if __name__ == \\"__main__\\": from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) knn = WeightedKNNClassifier(n_neighbors=3, weight=True) knn.fit(X_train, y_train) print(f\\"Accuracy: {knn.score(X_test, y_test)}\\")"},{"question":"# Shared Memory Data Aggregation **Objective:** Your task is to implement a program that creates shared memory blocks to aggregate numerical data from multiple processes. You will use the `multiprocessing.shared_memory` module and must demonstrate the creation, manipulation, and clean-up of these shared memory blocks. # Instructions: 1. **Shared Memory Setup:** - Create a shared memory block using the `SharedMemory` class with enough size to store an array of 1000 integers. 2. **Data Writing Processes:** - Use the `multiprocessing` module to create 4 processes. - Each process will write a sequence of 250 integers to distinct parts of the shared memory block. - For example, Process 1 writes integers 0-249, Process 2 writes integers 250-499, etc. 3. **Data Aggregation:** - Create another shared memory block to aggregate and store the results of each segment sum. - Sum each segment of 250 integers from the shared memory block and store the sums in the new shared memory block. 4. **Output the Result:** - Read and print the total aggregated sum from the newly created shared memory block in the main process. 5. **Clean-Up:** - Ensure proper cleanup of shared memory resources using `close()` and `unlink()` methods. # Constraints: - Use only the `multiprocessing` and `multiprocessing.shared_memory` modules for parallel processing and shared memory management respectively. # Expected Input: - There are no specific inputs from the user during execution. # Expected Output: - Print the total sum of all integers written to and aggregated in the shared memory blocks. # Example: ```python Total aggregated sum: 124750 ``` # Notes: - Ensure that the program handles shared memory cleanly, closing and unlinking the memory blocks properly. - Consider edge cases such as handling any potential synchronization issues between processes. **Implement your solution in the code block below:** ```python from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def write_to_shared_memory(shm_name, start_idx, end_idx, values): existing_shm = SharedMemory(name=shm_name) buffer = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) buffer[start_idx:end_idx] = values existing_shm.close() def aggregate_shared_memory(shm_name, result_shm_name): existing_shm = SharedMemory(name=shm_name) result_shm = SharedMemory(name=result_shm_name) buffer = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) result_buffer = np.ndarray((4,), dtype=np.int64, buffer=result_shm.buf) for i in range(4): result_buffer[i] = np.sum(buffer[i * 250:(i + 1) * 250]) existing_shm.close() result_shm.close() def main(): # Initialize shared memory block with size to store 1000 integers shm = SharedMemory(create=True, size=1000 * np.int64().nbytes) buffer = np.ndarray((1000,), dtype=np.int64, buffer=shm.buf) # Initialize result shared memory block to store 4 integers (for 4 sums) result_shm = SharedMemory(create=True, size=4 * np.int64().nbytes) result_buffer = np.ndarray((4,), dtype=np.int64, buffer=result_shm.buf) # Create 4 processes to write data to shared memory processes = [] for i in range(4): values = np.arange(i * 250, (i + 1) * 250, dtype=np.int64) p = Process(target=write_to_shared_memory, args=(shm.name, i * 250, (i + 1) * 250, values)) processes.append(p) p.start() for p in processes: p.join() # Aggregate the shared memory data aggregate_shm_process = Process(target=aggregate_shared_memory, args=(shm.name, result_shm.name)) aggregate_shm_process.start() aggregate_shm_process.join() # Calculate the total sum total_sum = np.sum(result_buffer) print(f\\"Total aggregated sum: {total_sum}\\") # Clean up shared memory shm.close() shm.unlink() result_shm.close() result_shm.unlink() if __name__ == \'__main__\': main() ```","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def write_to_shared_memory(shm_name, start_idx, end_idx, values): existing_shm = SharedMemory(name=shm_name) buffer = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) buffer[start_idx:end_idx] = values existing_shm.close() def aggregate_shared_memory(shm_name, result_shm_name): existing_shm = SharedMemory(name=shm_name) result_shm = SharedMemory(name=result_shm_name) buffer = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) result_buffer = np.ndarray((4,), dtype=np.int64, buffer=result_shm.buf) for i in range(4): result_buffer[i] = np.sum(buffer[i * 250:(i + 1) * 250]) existing_shm.close() result_shm.close() def main(): # Initialize shared memory block with size to store 1000 integers shm = SharedMemory(create=True, size=1000 * np.int64().nbytes) buffer = np.ndarray((1000,), dtype=np.int64, buffer=shm.buf) # Initialize result shared memory block to store 4 integers (for 4 sums) result_shm = SharedMemory(create=True, size=4 * np.int64().nbytes) result_buffer = np.ndarray((4,), dtype=np.int64, buffer=result_shm.buf) # Create 4 processes to write data to shared memory processes = [] for i in range(4): values = np.arange(i * 250, (i + 1) * 250, dtype=np.int64) p = Process(target=write_to_shared_memory, args=(shm.name, i * 250, (i + 1) * 250, values)) processes.append(p) p.start() for p in processes: p.join() # Aggregate the shared memory data aggregate_shm_process = Process(target=aggregate_shared_memory, args=(shm.name, result_shm.name)) aggregate_shm_process.start() aggregate_shm_process.join() # Calculate the total sum total_sum = np.sum(result_buffer) print(f\\"Total aggregated sum: {total_sum}\\") # Clean up shared memory shm.close() shm.unlink() result_shm.close() result_shm.unlink() if __name__ == \'__main__\': main()"},{"question":"**Title: Advanced Path Manipulation with `os.path`** **Objective:** Write a function `organize_files(base_directory: str, files: List[str]) -> Dict[str, List[str]]` that organizes a list of file paths into their respective directories. The function should return a dictionary where the keys are the absolute paths to directories, and the values are lists of basenames of files that reside in those directories. **Detailed Description:** 1. **Input:** - `base_directory`: A string representing the base directory from which relative path calculations should start. - `files`: A list of file paths (strings) that may include both absolute and relative paths. 2. **Output:** - A dictionary where: - Keys are absolute paths to directories (as strings). - Values are lists of basenames of files (as strings) that are located in those respective directories. **Steps and Requirements:** - Normalize all paths to their absolute versions using `os.path.abspath()`. - Ensure that all paths are represented consistently, even if they include symbolic links, using `os.path.realpath()`. - Use `os.path.dirname()` to extract directory names and `os.path.basename()` to extract file names. - Ensure proper handling of both POSIX (Unix-like) and Windows file paths. - Handle edge cases such as empty lists and invalid paths gracefully. **Constraints:** - The function should not perform any actual file system operations (e.g., checking if files or directories exist). It should purely manipulate path strings. - Assume that `base_directory` is always a valid directory path. **Example:** ```python from typing import List, Dict import os def organize_files(base_directory: str, files: List[str]) -> Dict[str, List[str]]: # Your implementation here # Usage Example: base_dir = \'/home/user\' files = [\'file1.txt\', \'/home/user/docs/file2.txt\', \'../music/file3.mp3\'] output = organize_files(base_dir, files) print(output) ``` *Expected Output:* ```python { \'/home/user\': [\'file1.txt\'], \'/home/user/docs\': [\'file2.txt\'], \'/home/music\': [\'file3.mp3\'] } ``` Write the function `organize_files` in the cell below:","solution":"from typing import List, Dict import os def organize_files(base_directory: str, files: List[str]) -> Dict[str, List[str]]: file_dict = {} for file in files: abs_path = os.path.abspath(os.path.join(base_directory, file)) real_path = os.path.realpath(abs_path) directory = os.path.dirname(real_path) basename = os.path.basename(real_path) if directory not in file_dict: file_dict[directory] = [] file_dict[directory].append(basename) return file_dict"},{"question":"**Objective:** Demonstrate your understanding of using Distutils to create various built distributions in Python. # Problem Statement: You need to write a Python script that automates the creation of various built distributions for a provided Python package and verifies their successful creation. The script should: 1. Accept as input the path to a directory containing a valid `setup.py` file. 2. Automatically generate built distributions for the package using the following formats: - gzipped tar file (gztar) - zip file (zip) - RPM package (rpm - for Linux systems) - Microsoft Installer (msi - for Windows systems) 3. Verify the existence and correct naming of generated distribution files. 4. Have a function that confirms the successful creation and names of the built distributions. # Input: - A string representing the path to the directory containing `setup.py`. # Output: - Printed confirmation messages for each of the required built distributions (gztar, zip, rpm, msi) stating whether it was successfully created or not. # Constraints: - Assume proper permissions for creating directories and files. - `setup.py` must be correctly configured to generate the distributions. - RPM and MSI formats should be verified conditionally based on the operating system (Linux and Windows respectively). # Example Usage: ```python create_and_verify_distributions(\'/path/to/package\') ``` This should output: ``` \'gztar\' distribution created successfully: True \'zip\' distribution created successfully: True \'rpm\' distribution created successfully: True \'msi\' distribution created successfully: True ``` # Implementation Requirements: 1. Use the `subprocess` module to invoke command line operations. 2. Handle exceptions gracefully and provide meaningful error messages if distribution creation fails. 3. Implement helper functions where necessary to keep the code modular and readable. 4. Ensure to cleanup (delete) the created distribution files during testing to avoid clutter.","solution":"import os import subprocess import platform def create_distribution(path, format): Creates a built distribution for a specified format. command = f\'python setup.py sdist --formats={format}\' try: subprocess.check_call(command, shell=True, cwd=path) return True except subprocess.CalledProcessError: return False def create_and_verify_distributions(path): Creates and verifies built distributions for the specified path. dist_dir = os.path.join(path, \'dist\') if not os.path.exists(dist_dir): os.makedirs(dist_dir) formats = {\'gztar\': \'gztar\', \'zip\': \'zip\'} if platform.system() == \'Linux\': formats[\'rpm\'] = \'rpm\' elif platform.system() == \'Windows\': formats[\'msi\'] = \'msi\' results = {} for fmt in formats.values(): success = create_distribution(path, fmt) dist_files = [f for f in os.listdir(dist_dir) if f.endswith(fmt)] results[fmt] = success and len(dist_files) > 0 for fmt, success in results.items(): print(f\\"\'{fmt}\' distribution created successfully: {success}\\") # Clean up the created distributions for file in os.listdir(dist_dir): os.remove(os.path.join(dist_dir, file)) # Example usage # create_and_verify_distributions(\'/path/to/package\')"},{"question":"Objective: Implement a Restricted Boltzmann Machine (RBM) using Scikit-Learn to perform feature extraction on a binary dataset and then utilize these extracted features to train a linear classifier. Evaluate the effectiveness of the RBM by comparing the classification accuracy with and without the RBM feature extraction. Problem Statement: You are provided with a binary dataset representing handwritten digit images where the pixel values are either 0 or 1. Your task is to: 1. Implement and train a `BernoulliRBM` on this dataset. 2. Use the features learned by the RBM to train a linear SVM classifier. 3. Train another linear SVM classifier directly on the original dataset without using RBM. 4. Compare the classification accuracy of both classifiers to determine the impact of RBM feature extraction. Input: - A binary dataset `data` as a NumPy array of shape `(n_samples, n_features)` where each row represents an image and each feature represents a pixel value (0 or 1). - Corresponding labels `labels` as a NumPy array of shape `(n_samples,)` with values 0-9 representing the digit class. Expected Functions: 1. `train_rbm(data: np.ndarray, n_components: int) -> BernoulliRBM` - Train an RBM with the given number of components. - `data`: Input binary dataset. - `n_components`: Number of hidden units in the RBM. - Returns the trained RBM model. 2. `extract_features(rbm: BernoulliRBM, data: np.ndarray) -> np.ndarray` - Use the trained RBM to transform the data to the feature space. - `rbm`: A trained BernoulliRBM model. - `data`: Input binary dataset. - Returns the transformed dataset with extracted features. 3. `train_svm_classifier(data: np.ndarray, labels: np.ndarray) -> SVC` - Train a linear SVM classifier on the given data and labels. - `data`: Input dataset (either original or feature-transformed). - `labels`: Corresponding labels. - Returns the trained linear SVM classifier. 4. `evaluate_classifier(classifier: SVC, data: np.ndarray, labels: np.ndarray) -> float` - Evaluate the classifier on test data and return the accuracy. - `classifier`: Trained SVM classifier. - `data`: Test dataset. - `labels`: Corresponding true labels. - Returns the classification accuracy as a float. Example: ```python import numpy as np from sklearn.datasets import load_digits from sklearn.neural_network import BernoulliRBM from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split # Load binary dataset digits = load_digits() data = (digits.data > 8).astype(np.float32) # Binarize the dataset (values are 0 or 1) labels = digits.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42) # RBM feature extraction rbm = train_rbm(X_train, n_components=64) X_train_transformed = extract_features(rbm, X_train) X_test_transformed = extract_features(rbm, X_test) # Train and evaluate SVM with RBM features svm_rbm = train_svm_classifier(X_train_transformed, y_train) rbm_accuracy = evaluate_classifier(svm_rbm, X_test_transformed, y_test) # Train and evaluate SVM on original data svm_original = train_svm_classifier(X_train, y_train) original_accuracy = evaluate_classifier(svm_original, X_test, y_test) print(f\\"Accuracy with RBM features: {rbm_accuracy}\\") print(f\\"Accuracy without RBM features: {original_accuracy}\\") ``` Constraints: - You should ensure the random state is set for reproducibility wherever necessary. - Use appropriate methods from Scikit-Learn for model training and evaluation. Performance Requirements: - The models should be trained efficiently considering the size of the dataset. - Evaluations should clearly demonstrate the impact of feature extraction using RBMs compared to raw input data.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def train_rbm(data: np.ndarray, n_components: int) -> BernoulliRBM: Train a Bernoulli Restricted Boltzmann Machine (RBM) on the given binary dataset. rbm = BernoulliRBM(n_components=n_components, random_state=42) rbm.fit(data) return rbm def extract_features(rbm: BernoulliRBM, data: np.ndarray) -> np.ndarray: Use the trained RBM to transform the data to the feature space. return rbm.transform(data) def train_svm_classifier(data: np.ndarray, labels: np.ndarray) -> SVC: Train a linear SVM classifier on the given data and labels. svm = SVC(kernel=\'linear\', random_state=42) svm.fit(data, labels) return svm def evaluate_classifier(classifier: SVC, data: np.ndarray, labels: np.ndarray) -> float: Evaluate the classifier on test data and return the accuracy. predictions = classifier.predict(data) accuracy = accuracy_score(labels, predictions) return accuracy"},{"question":"Coding Assessment Question # Objective In this assessment, you will demonstrate your understanding of Python\'s Abstract Syntax Trees (AST) module. You are required to write code that processes and analyzes Python source code to extract specific information using the `ast` module. # Problem Statement Write a function `extract_function_info` that takes a string of Python source code as input and returns a list of dictionaries, each containing information about the function definitions in the code. Each dictionary should have the following keys: - `name`: The name of the function. - `args`: A list of argument names. - `has_return`: A boolean indicating whether the function has a return statement. - `docstring`: The docstring of the function if it exists, otherwise `None`. # Input - `source_code`: A string representing Python source code. # Output - A list of dictionaries, each dictionary containing information about a function as specified above. # Constraints - The input source code may contain multiple function definitions. - The input source code is guaranteed to be syntactically correct Python code. - Consider both regular and async functions. - Performance requirements: The solution should efficiently handle source code up to 500 lines. # Example ```python source_code = \'\'\' def add(a, b): Add two numbers. return a + b def greet(name): print(f\\"Hello, {name}!\\") async def fetch_data(url): Fetch data from a given URL. pass \'\'\' output = extract_function_info(source_code) assert output == [ {\'name\': \'add\', \'args\': [\'a\', \'b\'], \'has_return\': True, \'docstring\': \'Add two numbers.\'}, {\'name\': \'greet\', \'args\': [\'name\'], \'has_return\': False, \'docstring\': None}, {\'name\': \'fetch_data\', \'args\': [\'url\'], \'has_return\': False, \'docstring\': \'Fetch data from a given URL.\'} ] ``` # Function Signature ```python def extract_function_info(source_code: str) -> list: # Your code here ``` # Notes - You may use the `ast` module to parse the source code and inspect the abstract syntax tree. - Pay attention to both normal and asynchronous functions (`async def`). - The docstring for a function is considered as the first string literal in the function body.","solution":"import ast def extract_function_info(source_code: str) -> list: class FunctionInfoVisitor(ast.NodeVisitor): def __init__(self): self.functions_info = [] def visit_FunctionDef(self, node): func_info = { \'name\': node.name, \'args\': [arg.arg for arg in node.args.args], \'has_return\': self.has_return_statement(node), \'docstring\': ast.get_docstring(node) } self.functions_info.append(func_info) self.generic_visit(node) def visit_AsyncFunctionDef(self, node): self.visit_FunctionDef(node) def has_return_statement(self, node): for n in ast.walk(node): if isinstance(n, ast.Return): return True return False tree = ast.parse(source_code) visitor = FunctionInfoVisitor() visitor.visit(tree) return visitor.functions_info"},{"question":"Problem Statement You are to write a Python function which reads a list of integers and returns a customized summary containing the following information: 1. **Frequency of each number**: How many times each integer appears in the list. 2. **Unique sorted list**: A sorted list (in ascending order) of unique integers from the input. 3. **Transformation**: A transformed list where each integer `x` is replaced by the sum of itself and all previous integers in the same list (cumulative sum). Function Signature ```python def generate_summary(arr: list[int]) -> dict: pass ``` Input - `arr`: A list of integers, where each integer `n` satisfies `-10^5 â‰¤ n â‰¤ 10^5` and `0 â‰¤ len(arr) â‰¤ 10^6`. Output - The function should return a dictionary with three keys: 1. `\'frequency\'`: A dictionary where keys are the integers from `arr` and values are their frequencies. 2. `\'unique_sorted\'`: A list of unique integers from `arr`, sorted in ascending order. 3. `\'cumulative_sum\'`: The transformed list where each integer is replaced by the cumulative sum. Example ```python >>> generate_summary([3, 3, -1, 2, -1, 0]) { \'frequency\': {3: 2, -1: 2, 2: 1, 0: 1}, \'unique_sorted\': [-1, 0, 2, 3], \'cumulative_sum\': [3, 6, 5, 7, 6, 6] } >>> generate_summary([]) { \'frequency\': {}, \'unique_sorted\': [], \'cumulative_sum\': [] } ``` Constraints and Notes 1. The function should handle large inputs efficiently. 2. Use appropriate data structures from the `collections` module for frequency counting. 3. Ensure the function has appropriate documentation strings, adhering to PEP 8 style guidelines. 4. Consider using list comprehensions and other Pythonic constructs to make the code more readable and efficient. Performance requirements - The solution should be optimized to handle the upper constraints effectively, especially given the potential size of the input list reaching up to 10^6 elements. **Your task is to implement the `generate_summary` function in Python. Ensure your code passes all provided examples and handles edge cases effectively.**","solution":"from collections import Counter def generate_summary(arr): Given a list of integers, returns a dictionary containing: 1. The frequency of each integer. 2. A sorted list of unique integers. 3. A transformed list where each integer is the cumulative sum up to that point. Args: arr (list of int): List of integers. Returns: dict: A dictionary with keys \'frequency\', \'unique_sorted\', and \'cumulative_sum\'. if not arr: return { \'frequency\': {}, \'unique_sorted\': [], \'cumulative_sum\': [] } # Frequency of each number frequency = dict(Counter(arr)) # Unique sorted list unique_sorted = sorted(frequency.keys()) # Cumulative sum transformation cumulative_sum = [] current_sum = 0 for num in arr: current_sum += num cumulative_sum.append(current_sum) return { \'frequency\': frequency, \'unique_sorted\': unique_sorted, \'cumulative_sum\': cumulative_sum }"},{"question":"# Question: Implementing a Singleton Pattern and Handling None References Objective: You need to demonstrate your understanding of integrating Python\'s `None` object and singleton patterns, ensuring proper handling of references and preventing instantiation of multiple objects of the same type. Task: Implement a singleton class in Python, named `SingletonNoneHandler`, which ensures only one instance of the class can exist. Additionally, create a method within this class to handle `None` references in a list by replacing them with a specified value. Requirements: 1. **Singleton Implementation**: - The class `SingletonNoneHandler` should ensure that only one instance of the class can be created. Any attempt to instantiate the class again should return the already existing instance. 2. **Method to Handle None References**: - Implement a method `replace_none` that takes a list of elements and a replacement value as its parameters. - The method should return a new list where all occurrences of `None` in the original list are replaced with the specified replacement value. Detailed Specifications: - **Class Name**: `SingletonNoneHandler` - **Method**: - `replace_none(self, elements: list, replacement: Any) -> list` - **Constraints**: - The list can contain any type of elements. - You must ensure that the class always returns the same instance. - Do not use global variables. Example: ```python # Initializing the first instance handler1 = SingletonNoneHandler() # Attempt to initialize a second instance handler2 = SingletonNoneHandler() # Verifying both instances are the same assert handler1 is handler2, \\"Both instances should be the same\\" # Replacing None values in a list original_list = [1, None, 2, None, 3] replacement_value = 0 new_list = handler1.replace_none(original_list, replacement_value) assert new_list == [1, 0, 2, 0, 3], \\"The None values should be replaced with the replacement value\\" ``` Notes: - Use appropriate methods to implement the singleton pattern. - Ensure the `replace_none` method handles edge cases such as empty lists or lists without `None` values gracefully.","solution":"class SingletonNoneHandler: _instance = None def __new__(cls, *args, **kwargs): if cls._instance is None: cls._instance = super(SingletonNoneHandler, cls).__new__(cls, *args, **kwargs) return cls._instance def replace_none(self, elements, replacement): Replace all occurrences of None in the list with a specified replacement value. :param elements: list of elements :param replacement: value to replace None with :return: new list with None replaced return [replacement if element is None else element for element in elements]"},{"question":"**Objective**: Demonstrate the ability to work with system error codes and handle exceptions using the Python `errno` module. Problem Description You are required to implement a function `retrieve_error_message` that takes an integer error code and returns either: - A string that explains the error, or - An appropriate built-in exception name if the error code maps directly to one of Pythonâ€™s built-in exceptions. Additionally, implement `describe_all_errors` which returns a dictionary containing all error codes with their descriptions or exception names wherever applicable. Function Signatures 1. `def retrieve_error_message(err_code: int) -> str:` 2. `def describe_all_errors() -> dict:` Input 1. `err_code`: An integer representing an error code. 2. None for `describe_all_errors`. Output 1. For `retrieve_error_message`: A string explaining the error or the exception name. 2. For `describe_all_errors`: A dictionary where each key is an error code, and each value is its description or corresponding exception name. Constraints - You should use the `errno` module where necessary. - Assume the input to `retrieve_error_message` will always be an integer. Example ```python import errno # Example usage print(retrieve_error_message(errno.EPERM)) # Output: \\"PermissionError\\" print(retrieve_error_message(errno.ENOENT)) # Output: \\"FileNotFoundError\\" errors = describe_all_errors() print(errors[errno.EINVAL]) # Output: \\"Invalid argument\\" ``` Note 1. Handle cases where the error code may not have a detailed description or exception. 2. This question requires understanding of error handling and appropriate use of the standard library, which is crucial for reliable and efficient programming.","solution":"import errno def retrieve_error_message(err_code: int) -> str: Returns a string that explains the error, or an appropriate built-in exception name if the error code maps directly to one of Pythonâ€™s built-in exceptions. error_map = { errno.EPERM: \\"PermissionError\\", errno.ENOENT: \\"FileNotFoundError\\", errno.ESRCH: \\"ESRCH (No such process)\\", errno.EINTR: \\"EINTR (Interrupted system call)\\", errno.EIO: \\"IOError\\", errno.ENXIO: \\"ENXIO (No such device or address)\\", errno.E2BIG: \\"E2BIG (Argument list too long)\\", errno.ENOEXEC: \\"ENOEXEC (Exec format error)\\", errno.EBADF: \\"Bad file descriptor\\", errno.ECHILD: \\"ECHILD (No child processes)\\", errno.EAGAIN: \\"EAGAIN (Try again)\\", errno.ENOMEM: \\"MemoryError\\", errno.EACCES: \\"PermissionError\\", errno.EFAULT: \\"EFAULT (Bad address)\\", errno.EBUSY: \\"EBUSY (Device or resource busy)\\", errno.EEXIST: \\"FileExistsError\\", errno.EXDEV: \\"EXDEV (Cross-device link)\\", errno.ENODEV: \\"ENODEV (No such device)\\", errno.ENOTDIR: \\"NotADirectoryError\\", errno.EISDIR: \\"IsADirectoryError\\", errno.EINVAL: \\"EINVAL (Invalid argument)\\", errno.ENFILE: \\"ENFILE (File table overflow)\\", errno.EMFILE: \\"EMFILE (Too many open files)\\", errno.ENOTTY: \\"ENOTTY (Not a typewriter)\\", errno.ENOSPC: \\"ENOSPC (No space left on device)\\", errno.ESPIPE: \\"ESPIPE (Illegal seek)\\", errno.EROFS: \\"EROFS (Read-only file system)\\", errno.EMLINK: \\"EMLINK (Too many links)\\", errno.EPIPE: \\"BrokenPipeError\\", errno.EDOM: \\"EDOM (Math argument out of domain of func)\\", errno.ERANGE: \\"ERANGE (Math result not representable)\\" } return error_map.get(err_code, f\\"Unknown error code: {err_code}\\") def describe_all_errors() -> dict: Returns a dictionary containing all error codes with their descriptions or exception names wherever applicable. error_codes = {key: value for key, value in errno.errorcode.items()} return {code: retrieve_error_message(code) for code in error_codes}"},{"question":"# Titanic Data Visualization Challenge Objective: Write a Python function using `seaborn` to perform a detailed analysis and visualization of the Titanic dataset. Your function should generate multiple plots to give insights into various aspects of the data. Requirements: 1. Load the Titanic dataset using seaborn. 2. Create a subplot with the following visualizations: * A count plot showing the distribution of passengers by class. * A count plot showing the distribution of passengers by survival status and class. * A violin plot showing the distribution of ages split by passenger class and sex. * A point plot showing the average fare paid by passenger class and survival status. * A heatmap showing the correlation of numerical features in the dataset. Function Signature: ```python def visualize_titanic_data(): # Your code here ``` Detailed Steps: 1. **Count Plot:** Generate a count plot displaying the distribution of Titanic passengers by their travel class (`class`). 2. **Count Plot with Hue:** Generate a count plot displaying the distribution of Titanic passengers by their travel class, segmented by survival status (`survived`). 3. **Violin Plot:** Create a violin plot to show the distribution of ages (`age`) across different passenger classes (`class`) and gender (`sex`). 4. **Point Plot:** Generate a point plot to visualize the average fares (`fare`) for different passenger classes (`class`), segmented by survival status (`survived`). 5. **Heatmap:** Construct a heatmap to visualize the correlation coefficients between numerical columns (`age`, `fare`, `pclass`, etc.) in the dataset. Input: - None. The dataset is preloaded using seaborn within the function. Output: - Multiple seaborn plots displayed using subplots. Constraints: - Ensure your code is well-documented and modular. - Use seaborn\'s inbuilt dataset and plotting functions. - Follow best practices for visualization. Example Result: The function should display a multi-plot layout with the described visualizations, providing comprehensive insights into the Titanic dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_titanic_data(): # Load Titanic dataset titanic = sns.load_dataset(\'titanic\') # Create a figure for subplots fig, axes = plt.subplots(3, 2, figsize=(15, 18)) fig.suptitle(\'Titanic Data Visualizations\', fontsize=20) # Count plot for passenger class distribution sns.countplot(ax=axes[0, 0], data=titanic, x=\'class\') axes[0, 0].set_title(\'Passenger Class Distribution\') # Count plot for survival distribution by class sns.countplot(ax=axes[0, 1], data=titanic, x=\'class\', hue=\'survived\') axes[0, 1].set_title(\'Survival by Passenger Class\') # Violin plot for age distribution by class and gender sns.violinplot(ax=axes[1, 0], data=titanic, x=\'class\', y=\'age\', hue=\'sex\', split=True) axes[1, 0].set_title(\'Age Distribution by Class and Gender\') # Point plot for average fare by class and survival status sns.pointplot(ax=axes[1, 1], data=titanic, x=\'class\', y=\'fare\', hue=\'survived\', dodge=True) axes[1, 1].set_title(\'Average Fare by Class and Survival Status\') # Heatmap for the correlation of numerical features corr_matrix = titanic[[\'age\', \'fare\', \'sibsp\', \'parch\', \'pclass\']].corr() sns.heatmap(ax=axes[2, 0], data=corr_matrix, annot=True, cmap=\'coolwarm\', vmin=-1, vmax=1) axes[2, 0].set_title(\'Correlation Matrix of Numerical Features\') # Remove the last subplot (empty) fig.delaxes(axes[2, 1]) # Adjust layout plt.tight_layout(rect=[0, 0, 1, 0.95]) plt.show()"},{"question":"**Coding Assessment Question: Advanced Cookie Handling in Python** **Objective:** Implement a custom cookie manager class based on the `http.cookies.BaseCookie` class. The custom manager enhances cookie handling by enforcing integer values to be stored as hexadecimal strings and supports the `samesite` attribute, ensuring it is either \\"Strict\\" or \\"Lax\\". **Requirements:** 1. Create a class `HexCookie` derived from `http.cookies.BaseCookie`. 2. Override the `value_encode` and `value_decode` methods: - `value_encode`: Convert integer values to hexadecimal strings before storage. - `value_decode`: Convert hexadecimal strings back to integers when retrieving. 3. Implement a method `set_samesite` to set the `samesite` attribute of a cookie. 4. Implement error handling to ensure the `samesite` attribute can only be \\"Strict\\" or \\"Lax\\". **Specifications:** - **Class Name:** `HexCookie` - **Methods to Implement:** - `value_encode(self, value)`: Encode value to hexadecimal string if it\'s an integer. - `value_decode(self, value)`: Decode value from hexadecimal string to integer. - `set_samesite(self, key, samesite_value)`: Set the `samesite` attribute for cookie identified by `key`. - **Input and Output:** - The `HexCookie` class must handle cookie values and the `samesite` attribute as specified. - Raise `ValueError` if `samesite_value` is not \\"Strict\\" or \\"Lax\\". **Example Usage:** ```python from http.cookies import BaseCookie class HexCookie(BaseCookie): def value_encode(self, value): if isinstance(value, int): coded_value = hex(value) else: coded_value = str(value) return value, coded_value def value_decode(self, value): try: real_value = int(value, 16) except ValueError: real_value = value return real_value, str(value) def set_samesite(self, key, samesite_value): if samesite_value not in [\\"Strict\\", \\"Lax\\"]: raise ValueError(\\"samesite attribute must be \'Strict\' or \'Lax\'\\") self[key][\\"samesite\\"] = samesite_value # Demonstration of class capabilities. if __name__ == \\"__main__\\": C = HexCookie() C[\\"session\\"] = 12345 print(C.output()) # Should show the integer as a hexadecimal value. C.set_samesite(\\"session\\", \\"Strict\\") print(C.output()) # Should include the \'samesite=Strict\' attribute. try: C.set_samesite(\\"session\\", \\"InvalidValue\\") except ValueError as e: print(e) # Should raise a ValueError ``` **Constraints:** - You must use the `BaseCookie` class as the base for the `HexCookie` class. - Correctly handle encoding and decoding of integer values to/from hexadecimal strings. **Performance Requirements:** - Ensure the `value_encode` and `value_decode` operations are efficient and correctly handle edge cases.","solution":"from http.cookies import BaseCookie class HexCookie(BaseCookie): def value_encode(self, value): if isinstance(value, int): coded_value = hex(value) else: coded_value = str(value) return value, coded_value def value_decode(self, value): try: real_value = int(value, 16) except ValueError: real_value = value return real_value, str(value) def set_samesite(self, key, samesite_value): if samesite_value not in [\\"Strict\\", \\"Lax\\"]: raise ValueError(\\"samesite attribute must be \'Strict\' or \'Lax\'\\") self[key][\\"samesite\\"] = samesite_value"},{"question":"**Question:** You are provided a list of integers and your task is to create two array objects using the `array` module in Python: 1. The first array should store signed integers (\'i\'). 2. The second array should store unsigned integers (\'I\'). Additionally, you need to perform the following tasks: - Append the values from the list to both arrays. - Count the occurrences of a specific integer in both arrays. - Reverse the contents of both arrays. - Convert the reversed arrays to lists and return a dictionary containing: - The array type code. - The reversed list. - The count of the specific integer. # Input: - `int_list`: A list of integers. - `search_int`: An integer whose occurrences need to be counted. # Output: - A dictionary with keys \'signed\', \'unsigned\' containing dictionaries with keys \'typecode\', \'reversed_list\', and \'count\': ```python { \'signed\': { \'typecode\': \'i\', \'reversed_list\': [reversed and converted list of signed integers], \'count\': [count of search_int in the signed array] }, \'unsigned\': { \'typecode\': \'I\', \'reversed_list\': [reversed and converted list of unsigned integers], \'count\': [count of search_int in the unsigned array] } } ``` # Constraints: - The input list `int_list` will have at least one integer. - Values in `int_list` for the unsigned array must be non-negative (0 and above). - The `search_int` will be within the range of the provided list values. # Example: ```python int_list = [1, -2, 3, 4, -5, 6] search_int = 3 result = process_arrays(int_list, search_int) ``` Expected output: ```python { \'signed\': { \'typecode\': \'i\', \'reversed_list\': [6, -5, 4, 3, -2, 1], \'count\': 1 }, \'unsigned\': { \'typecode\': \'I\', \'reversed_list\': [6, 4, 3, 1], \'count\': 1 } } ``` # Function Signature: ```python def process_arrays(int_list: list, search_int: int) -> dict: pass ``` Write the implementation for `process_arrays` to achieve the desired functionality.","solution":"import array def process_arrays(int_list: list, search_int: int) -> dict: signed_array = array.array(\'i\') # signed integer array unsigned_array = array.array(\'I\') # unsigned integer array for item in int_list: signed_array.append(item) if item >= 0: unsigned_array.append(item) signed_count = signed_array.count(search_int) unsigned_count = unsigned_array.count(search_int) signed_array.reverse() unsigned_array.reverse() return { \'signed\': { \'typecode\': \'i\', \'reversed_list\': signed_array.tolist(), \'count\': signed_count }, \'unsigned\': { \'typecode\': \'I\', \'reversed_list\': unsigned_array.tolist(), \'count\': unsigned_count } }"},{"question":"# Question **Title**: Implement an Advanced Multiprocessing Task Queue in Python **Problem Statement**: You are required to design a Python program that efficiently processes a list of tasks using multiple worker processes. Your implementation should utilize the `multiprocessing` package effectively to ensure the program operates concurrently in a thread-safe and process-safe manner. Your program needs to perform the following steps: 1. Create a list of tasks where each task is a tuple of an integer and a function. The function performs a simple mathematical operation (like `square`, `cube`, `add`, etc.) on the integer. 2. Use the `Queue` to manage the tasks. Place all tasks in the queue initially. 3. Spawn multiple worker processes using the `Process` class to process tasks from the queue: - Each worker process should fetch tasks from the queue, execute the given function on the integer, and store the result in another queue. - Ensure that workers process tasks in a thread-safe manner using appropriate synchronization mechanisms. 4. Implement and utilize a manager object using the `Manager` class for maintaining a shared dictionary to keep track of task statuses (e.g., `pending`, `in-progress`, `completed`). 5. Once all tasks are processed, the main process should collect results from the result queue and print them. **Function Signature**: ```python def task_processing(tasks: list) -> dict: pass ``` **Input**: - `tasks`: A list of tuples where each tuple contains an integer and a function that performs a mathematical operation on the integer. **Output**: - A dictionary with task identifiers (generated automatically) as keys and their results as values. **Constraints**: - The number of workers should be optimized based on available CPU cores. - Ensure proper exception handling for robustness. - Use synchronization primitives to ensure no race conditions exist. - The shared dictionary should be updated safely by multiple processes. - Assume the input functions are always valid Python functions. **Example**: ```python from multiprocessing import Manager def square(x): return x * x def cube(x): return x * x * x def add(x): return x + x tasks = [(3, square), (4, cube), (5, add)] result = task_processing(tasks) print(result) # Expected Output: {0: 9, 1: 64, 2: 10} ``` **Detailed Explanation**: 1. The `task_processing` function receives a list of tasks. 2. Tasks are placed into a task queue. 3. Several worker processes fetch tasks from the queue, compute the result, and put it into the result queue. 4. A manager dictionary tracks the status of each task. 5. The main process collects the results from the result queue and returns them as a dictionary.","solution":"from multiprocessing import Process, Queue, Manager, cpu_count import time def worker(task_queue, result_queue, status_dict): while not task_queue.empty(): task_id, task = task_queue.get() if task_id is None: break try: x, func = task status_dict[task_id] = \'in-progress\' result = func(x) result_queue.put((task_id,result)) status_dict[task_id] = \'completed\' except Exception as e: status_dict[task_id] = f\'failed: {e}\' def task_processing(tasks: list) -> dict: # Initialize the queues and the manager dictionary task_queue = Queue() result_queue = Queue() manager = Manager() status_dict = manager.dict() # Populate task_queue with tasks for idx, task in enumerate(tasks): task_queue.put((idx, task)) status_dict[idx] = \'pending\' # Determine the number of worker processes to spawn num_workers = min(len(tasks), cpu_count()) # Create and start worker processes workers = [] for _ in range(num_workers): p = Process(target=worker, args=(task_queue, result_queue, status_dict)) workers.append(p) p.start() # Wait for all worker processes to finish for p in workers: p.join() # Collect all results from the result_queue results = {} while not result_queue.empty(): task_id, result = result_queue.get() results[task_id] = result # Return results dictionary return results"},{"question":"# Python Coding Assessment: Understanding Time Functions You are required to implement a function that utilizes various functions from the `time` module to perform a series of time-related operations. # Task Write a Python function named `process_time_operations` that performs the following tasks: 1. **Get Current Time**: Retrieve the current local time and UTC time. 2. **Formatted Time String**: Return the current local time as a formatted string in the form \'YYYY-MM-DD HH:MM:SS\'. 3. **Elapsed Time Measurement**: Measure the elapsed time for a given operation using the `perf_counter` function. 4. **Parse Time String**: Given a time string in the format \'DD-MM-YYYY HH:MM:SS\', convert it into a `struct_time` object and then into seconds since the epoch. 5. **Handle DST and Time Zone**: Use the `tzset`, `timezone`, and `altzone` constants to determine whether the current local time zone has Daylight Saving Time (DST) rules and, if so, determine the offset from UTC during DST and non-DST periods. # Input and Output - The function should take no parameters. - The function should return a dictionary with the following keys and respective values: - `current_local_time`: a `struct_time` object representing the current local time. - `current_utc_time`: a `struct_time` object representing the current UTC time. - `formatted_local_time`: a string representing the current local time in the format \'YYYY-MM-DD HH:MM:SS\'. - `elapsed_time_in_seconds`: a float value representing the elapsed time for a mock operation (e.g., sleeping for 0.1 seconds). - `parsed_seconds_since_epoch`: an integer representing the seconds since epoch for the parsed time string \'15-08-2020 12:00:00\'. - `dst_info`: a tuple containing two integers: - the offset in seconds from UTC during DST. - the offset in seconds from UTC when not in DST. # Constraints - The function should handle potential exceptions that may arise from improper time parsing or DST calculations. - You may assume that the local system supports all necessary `time` module functionality outlined in the task. # Example Usage ```python def process_time_operations(): # Example implementation pass result = process_time_operations() print(result) ``` The expected output would be: ```python { \'current_local_time\': time.struct_time(..), # current local time struct \'current_utc_time\': time.struct_time(..), # current UTC time struct \'formatted_local_time\': \'2023-10-09 13:15:00\', # example formatted time \'elapsed_time_in_seconds\': 0.100234, # example elapsed time for a mock operation \'parsed_seconds_since_epoch\': 1597483200, \'dst_info\': (-14400, -18000) # example offsets } ``` Write the implementation for the `process_time_operations` function in the provided function stub.","solution":"import time def process_time_operations(): result = {} # Get Current Time current_local_time = time.localtime() current_utc_time = time.gmtime() # Formatted Time String formatted_local_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', current_local_time) # Elapsed Time Measurement start_time = time.perf_counter() time.sleep(0.1) end_time = time.perf_counter() elapsed_time_in_seconds = end_time - start_time # Parse Time String time_string = \'15-08-2020 12:00:00\' parsed_time = time.strptime(time_string, \'%d-%m-%Y %H:%M:%S\') parsed_seconds_since_epoch = int(time.mktime(parsed_time)) # Handle DST and Time Zone time.tzset() dst_info = (time.altzone if time.daylight else 0, time.timezone) result = { \'current_local_time\': current_local_time, \'current_utc_time\': current_utc_time, \'formatted_local_time\': formatted_local_time, \'elapsed_time_in_seconds\': elapsed_time_in_seconds, \'parsed_seconds_since_epoch\': parsed_seconds_since_epoch, \'dst_info\': dst_info } return result"},{"question":"You are tasked with processing an email message object. Given the nature of email messages, these objects may contain multiple subparts, each with different payloads and MIME types. Your goal is to identify and process specific parts of the email. # Objectives: 1. Extract all the lines of string payloads from an email message object and return them as a list. 2. Filter and retrieve all subparts of the email that have a specified MIME type and subtype. # Instructions: You need to implement two functions: 1. `extract_payload_lines(msg, decode=False)` 2. `filter_subparts_by_type(msg, maintype, subtype=None)` Both functions should leverage the `email.iterators` module for iteration. Function 1: extract_payload_lines **Parameters:** - `msg` (Message): The email message object. - `decode` (bool): Optional parameter that indicates whether to decode the payloads; defaults to `False`. **Returns:** - List of strings: A list containing all the lines of string payloads from the message. **Example:** ```python from email import message_from_string message = message_from_string(some_raw_email_string) lines = extract_payload_lines(message) print(lines) # Should output the lines of the message payload ``` Function 2: filter_subparts_by_type **Parameters:** - `msg` (Message): The email message object. - `maintype` (str): The main MIME type of the parts to filter. - `subtype` (str): Optional MIME subtype of the parts to filter. **Returns:** - List of Message objects: A list containing subparts of the original message that match the specified MIME type. **Example:** ```python from email import message_from_string message = message_from_string(some_raw_email_string) filtered_parts = filter_subparts_by_type(message, \'text\', \'plain\') print(filtered_parts) # Should output subpart messages with MIME type \'text/plain\' ``` # Constraints: 1. You may assume the input `msg` is always a valid `email.message.Message` object. 2. The `email` library and its `iterators` module functions are available for use. Implement these functions to demonstrate your understanding of both basic email processing and advanced iterator usage in Python.","solution":"from email.iterators import typed_subpart_iterator def extract_payload_lines(msg, decode=False): Extract all the lines of string payloads from an email message object. Parameters: - msg (Message): The email message object. - decode (bool): Optional parameter that indicates whether to decode the payloads; defaults to False. Returns: - List of strings: A list containing all the lines of string payloads from the message. lines = [] for part in msg.walk(): if part.get_content_maintype() == \'multipart\': continue payload = part.get_payload(decode=decode) if isinstance(payload, str): lines.extend(payload.splitlines()) return lines def filter_subparts_by_type(msg, maintype, subtype=None): Filter and retrieve all subparts of the email that have a specified MIME type and subtype. Parameters: - msg (Message): The email message object. - maintype (str): The main MIME type of the parts to filter. - subtype (str): Optional MIME subtype of the parts to filter. Returns: - List of Message objects: A list containing subparts of the original message that match the specified MIME type. subparts = [] for part in typed_subpart_iterator(msg, maintype, subtype): subparts.append(part) return subparts"},{"question":"# Sequence Object Manipulation in Python Problem Description You are tasked with writing a function to process sequences of various types, specifically Bytes Objects, Byte Array Objects, and Unicode Objects. The function, `process_sequence`, should accept a sequence object and an operation string. The function should check the type of the sequence object and perform different operations based on the object type and the specified operation. Function Signature ```python def process_sequence(sequence, operation): Processes the given sequence based on its type and the specified operation. Args: sequence (bytes or bytearray or str): The sequence object which could be of type bytes, bytearray, or str. operation (str): The operation to perform. It can be one of the following: - \'reverse\' : Reverse the sequence. - \'capitalize\' : Capitalize all elements (only applicable for Unicode/str objects). - \'append\' : Append a predefined element to the sequence. Returns: bytes or bytearray or str: The processed sequence as per the operation. Raises: TypeError: If the sequence is not one of the expected types. ValueError: If the operation is not recognized. ``` Instructions 1. **Type Checking**: The function must first determine the type of the sequence object. - If it is neither a `bytes`, `bytearray`, nor `str` object, raise a `TypeError` with an appropriate message. 2. **Operations**: - **Reverse**: - If the sequence is a `bytes` or `bytearray`, reverse the sequence. - If the sequence is a Unicode/str object, reverse the characters in the string. - **Capitalize**: - This operation is only applicable to Unicode/str objects; capitalize all characters in the string. If this operation is attempted on `bytes` or `bytearray`, raise a `ValueError` with an appropriate message. - **Append**: - Append a predefined element (\'x\' for `str` and `b\'x\'` for `bytes` and `bytearray`) to the sequence. 3. **Return Values**: - The function should return the modified sequence as per the operation performed. 4. **Exceptions**: - The function should raise an appropriate exception if the sequence type or operation is not recognized or applicable. Examples ```python # Example with bytes print(process_sequence(b\'hello\', \'reverse\')) # Output: b\'olleh\' print(process_sequence(b\'hello\', \'append\')) # Output: b\'hellox\' # Example with bytearray print(process_sequence(bytearray(b\'hello\'), \'reverse\')) # Output: bytearray(b\'olleh\') print(process_sequence(bytearray(b\'hello\'), \'append\')) # Output: bytearray(b\'hellox\') # Example with str (Unicode objects) print(process_sequence(\'hello\', \'reverse\')) # Output: \'olleh\' print(process_sequence(\'hello\', \'capitalize\')) # Output: \'HELLO\' print(process_sequence(\'hello\', \'append\')) # Output: \'hellox\' ``` Constraints - The function should not modify the original sequence object; instead, it should return a new modified sequence. - Performance considerations: Assume the sequence length to be generally less than 10^5.","solution":"def process_sequence(sequence, operation): Processes the given sequence based on its type and the specified operation. Args: sequence (bytes or bytearray or str): The sequence object which could be of type bytes, bytearray, or str. operation (str): The operation to perform. It can be one of the following: - \'reverse\' : Reverse the sequence. - \'capitalize\' : Capitalize all elements (only applicable for Unicode/str objects). - \'append\' : Append a predefined element to the sequence. Returns: bytes or bytearray or str: The processed sequence as per the operation. Raises: TypeError: If the sequence is not one of the expected types. ValueError: If the operation is not recognized. if not isinstance(sequence, (bytes, bytearray, str)): raise TypeError(\\"Expected a sequence of type bytes, bytearray or str\\") if operation == \'reverse\': return sequence[::-1] elif operation == \'capitalize\': if isinstance(sequence, str): return sequence.upper() else: raise ValueError(\\"Capitalize operation is only applicable to Unicode/str objects\\") elif operation == \'append\': if isinstance(sequence, bytes): return sequence + b\'x\' elif isinstance(sequence, bytearray): return sequence + bytearray(b\'x\') elif isinstance(sequence, str): return sequence + \'x\' else: raise ValueError(\\"Operation not recognized\\")"},{"question":"Imagine your computer environment has various system variables set that influence the behavior of processes. Python\'s `os` module allows you to interact with these environment variables. # Task Your task is to write a Python function `modify_environment(new_vars)` that takes a dictionary `new_vars` as input. This dictionary contains key-value pairs representing environment variable names and their corresponding values that need to be updated or added to the current environment. # Function Signature ```python def modify_environment(new_vars: dict) -> None: pass ``` # Input - `new_vars` (dict): A dictionary where keys are environment variable names (as strings) and values are the values to be set for those environment variables (also strings). # Output - The function should return `None`, but it should update the environment variables as per the dictionary provided. # Constraints - All keys and values in the input dictionary will be strings. - The input dictionary will contain at most 100 key-value pairs. # Example ```python import os # Before modification print(os.environ.get(\'TEST_VAR\')) # Outputs: None new_vars = {\'TEST_VAR\': \'123\', \'ANOTHER_VAR\': \'456\'} modify_environment(new_vars) # After modification print(os.environ.get(\'TEST_VAR\')) # Outputs: \'123\' print(os.environ.get(\'ANOTHER_VAR\')) # Outputs: \'456\' ``` # Additional Information - Ensure that the new environment variables are set such that subsequent calls to `os.environ` reflect these updates. - You can assume this function will only be called in environments where the `os` module is available. # Notes - Do not use the `posix` module directly; use the `os` module for modifying the environment variables. - It\'s important to take into account that on Unix systems, environment variables are case-sensitive but on Windows, they are not. # Hints - Use `os.environ` to modify the environment variables as documented.","solution":"import os def modify_environment(new_vars: dict) -> None: Updates or adds the given key-value pairs in the environment variables. Parameters: new_vars (dict): A dictionary containing environment variable names and their values. for key, value in new_vars.items(): os.environ[key] = value"},{"question":"# Question: Advanced Exception Handling and Traceback Manipulation You are tasked with creating a custom exception handling function in Python that leverages the `traceback` module to provide detailed diagnostic information whenever an exception is encountered during the execution of user-defined functions. Your task is to implement a function called `detailed_exception_handler` that accepts another function along with its arguments and keyword arguments, executes it, and if an exception occurs, prints out a detailed traceback, including the local variables in each frame, the exception type, and the exception message. Function Signature: ```python def detailed_exception_handler(func, *args, **kwargs): pass ``` # Requirements: 1. **Function Execution**: - Attempt to execute the given function `func` with provided `args` and `kwargs`. - If the function completes successfully, return the result of the function call. 2. **Exception Handling**: - If an exception occurs: - Capture the exception information. - Extract and format the traceback details, including local variables. - Print a detailed exception report including: - The exception type and message. - The formatted traceback with local variables. 3. **Output Format**: - Print the detailed traceback and exception information using the `traceback` module functions. # Constraints: - You may assume that the functions being handled do not perform I/O operations. - The traceback should include all stack frames, without limiting the depth. # Example Usage: ```python def example_function(x, y): return x / y # Successful case: result = detailed_exception_handler(example_function, 10, 2) print(result) # Output: 5.0 # Exception case: result = detailed_exception_handler(example_function, 10, 0) # Output: # Traceback (most recent call last): # File \\"example.py\\", line XX, in detailed_exception_handler # result = func(*args, **kwargs) # File \\"example.py\\", line XX, in example_function # return x / y # ZeroDivisionError: division by zero # Local Variables in frame: # x: 10 # y: 0 ``` # Notes: - Use the `traceback` module functionalities such as `extract_tb`, `format_list`, and others as necessary to achieve the required output format. - Remember to extract local variables in each frame for detailed diagnostics. Implement the `detailed_exception_handler` function as described.","solution":"import traceback def detailed_exception_handler(func, *args, **kwargs): Executes the given function with specified arguments and keyword arguments. Captures and prints detailed traceback information in case of an exception. try: return func(*args, **kwargs) except Exception as e: tb = traceback.TracebackException.from_exception(e) print(\'\'.join(tb.format())) for frame in tb.stack: print(f\\"Local Variables in frame: {frame.locals}\\") return None"},{"question":"**Objective**: Demonstrate understanding and application of the Python \\"stat\\" module\'s functions related to file types and modes. # Problem Statement You are tasked with implementing a Python function that categorizes files in a given directory based on their file type and generates a summary report. The function should recursively traverse all subdirectories. # Function Signature ```python import os import stat def summarize_directory(path: str) -> dict: Summarizes the types of files in the given directory and its subdirectories. Args: path (str): The root directory path to start summarizing. Returns: dict: A dictionary where keys are file types (e.g., \'directory\', \'regular_file\', \'symbolic_link\', etc.) and values are the count of each file type found. ``` # Requirements 1. Implement the `summarize_directory` function. 2. Use the appropriate `stat` module functions to determine each file\'s type. 3. The summary report should categorize files into the following types: - `directory` - `regular_file` - `character_device` - `block_device` - `fifo` - `symbolic_link` - `socket` - `door` - `event_port` - `whiteout` 4. The function should handle exceptions gracefully, such as permissions errors or other I/O related issues. 5. The returned dictionary should have the format: ```python { \'directory\': count, \'regular_file\': count, \'character_device\': count, \'block_device\': count, \'fifo\': count, \'symbolic_link\': count, \'socket\': count, \'door\': count, \'event_port\': count, \'whiteout\': count } ``` 6. Ensure that the function is efficient and performs well even on large directory structures. # Constraints - The directory path provided will always be an absolute path. - You can assume that the provided path exists and is a directory. - Python version is 3.10 or higher. # Example ```python # Assuming the directory structure is as follows: # /path/to/dir # â”œâ”€â”€ file1 # â”œâ”€â”€ file2 # â”œâ”€â”€ link -> /some/other/path # â”œâ”€â”€ subdir # â”‚ â”œâ”€â”€ file3 # â”‚ â””â”€â”€ file4 # â””â”€â”€ fifo summarize_directory(\'/path/to/dir\') ``` Output might be: ```python { \'directory\': 2, \'regular_file\': 3, \'character_device\': 0, \'block_device\': 0, \'fifo\': 1, \'symbolic_link\': 1, \'socket\': 0, \'door\': 0, \'event_port\': 0, \'whiteout\': 0 } ``` # Notes - You may use additional helper functions if necessary. - Make sure to test your function with various directory structures to ensure its correctness.","solution":"import os import stat def summarize_directory(path: str) -> dict: Summarizes the types of files in the given directory and its subdirectories. Args: path (str): The root directory path to start summarizing. Returns: dict: A dictionary where keys are file types (e.g., \'directory\', \'regular_file\', \'symbolic_link\', etc.) and values are the count of each file type found. def get_file_type(mode): if stat.S_ISDIR(mode): return \'directory\' elif stat.S_ISREG(mode): return \'regular_file\' elif stat.S_ISCHR(mode): return \'character_device\' elif stat.S_ISBLK(mode): return \'block_device\' elif stat.S_ISFIFO(mode): return \'fifo\' elif stat.S_ISLNK(mode): return \'symbolic_link\' elif stat.S_ISSOCK(mode): return \'socket\' # Note: stat module in Python does not directly provide door, event_port, and whiteout # as these are system specific and rarely used. return \'unknown\' summary = { \'directory\': 0, \'regular_file\': 0, \'character_device\': 0, \'block_device\': 0, \'fifo\': 0, \'symbolic_link\': 0, \'socket\': 0, \'door\': 0, \'event_port\': 0, \'whiteout\': 0 } for root, dirs, files in os.walk(path): for name in dirs + files: try: full_path = os.path.join(root, name) mode = os.lstat(full_path).st_mode file_type = get_file_type(mode) if file_type in summary: summary[file_type] += 1 except (OSError, PermissionError): continue return summary"},{"question":"# XML Document Transformation with ElementTree Given a task to read, manipulate, and transform XML documents, you are required to implement a Python function that will process an XML structure. Task: Write a function named `transform_xml`, which: 1. Reads an XML string. 2. Finds all elements named `item`. 3. For each `item` element, if it has a subelement named `price`, doubles the value of that `price` subelement. 4. Adds a new subelement named `discount` with the value `10` to each `item` element that already has a `price` subelement. 5. Generates a new XML string with these transformations applied. 6. Returns the new XML string. Function Signature: ```python def transform_xml(xml_content: str) -> str: pass ``` Input: - `xml_content` (string): A string representing the XML content. Output: - Returns a string, which is the modified XML content. Constraints: - The input XML content can be assumed to be well-formed. - Assume each `price` element contains a valid integer value. Example: Input: ```xml <catalog> <item> <title>Book A</title> <price>30</price> </item> <item> <title>Book B</title> </item> <item> <title>Book C</title> <price>45</price> </item> </catalog> ``` Output: ```xml <catalog> <item> <title>Book A</title> <price>60</price> <discount>10</discount> </item> <item> <title>Book B</title> </item> <item> <title>Book C</title> <price>90</price> <discount>10</discount> </item> </catalog> ``` Explanation: - The `price` of `Book A` is doubled from `30` to `60` and a `discount` element is added with the value `10`. - `Book B` does not have a `price` element, so no changes are made. - The `price` of `Book C` is doubled from `45` to `90` and a `discount` element is added with the value `10`. To complete this task, you should use the `xml.etree.ElementTree` module as per the documentation provided.","solution":"import xml.etree.ElementTree as ET def transform_xml(xml_content: str) -> str: tree = ET.ElementTree(ET.fromstring(xml_content)) root = tree.getroot() for item in root.findall(\'item\'): price_element = item.find(\'price\') if price_element is not None: # Double the price value price_value = int(price_element.text) price_element.text = str(price_value * 2) # Add the discount element discount_element = ET.SubElement(item, \'discount\') discount_element.text = \'10\' # Generate new XML string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Advanced Seaborn Plotting You are provided with the `tips` dataset using the Seaborn library. Your task is to create a complex plot using the `seaborn.objects` module that demonstrates various plotting and data handling techniques as detailed in the provided documentation. Requirements: 1. Load the `tips` dataset using `seaborn.load_dataset`. 2. Create a plot object with the `tips` dataset where the `x` variable is `\\"total_bill\\"` and the `y` variable is `\\"tip\\"`. 3. Add the following layers to the plot: - A `Dot` plot showing the relationship between `total_bill` and `tip`. - A `Line` plot with a polynomial fit transform for the same data. - A `Bar` plot representing the total count of `day` with `dodge` transform applied. - Customize the `Dot` plot with `alpha=0.5` and add a `jitter` transform with `0.1`. 4. Modify the plot to have different attributes for each day of the week by faceting the plot on the `day` variable. 5. Ensure each layer has its own distinct data source, specifically: - Filter the `Dot` plot for bills greater than 20. - Filter the `Bar` plot for `size` is equal to 2. 6. Add labels to the layers in the legend to indicate `\\"Dot Plot\\"`, `\\"Fitted Line\\"`, and `\\"Count Bar\\"`. 7. Customize the color of the `Line` plot to `\'C2\'` and the `Bar` plot to `\'C3\'`. Input: None Output: A layered Seaborn plot displayed inline. Example code structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot with specified requirements p = ( # Initial Plot so.Plot(tips, \\"total_bill\\", \\"tip\\") # Add Dot layer with customization and data filter .add(so.Dot(alpha=0.5), so.Jitter(0.1), data=tips.query(\\"total_bill > 20\\"), label=\\"Dot Plot\\") # Add Line layer with polynomial fit transform .add(so.Line(color=\'C2\'), so.PolyFit(), label=\\"Fitted Line\\") # Add Bar layer with dodge transform and data filter .add(so.Bar(color=\'C3\'), so.Dodge(), data=tips.query(\\"size == 2\\"), x=\\"day\\", label=\\"Count Bar\\") # Facet by day .facet(col=\\"day\\") ) # Display the plot p.show() ``` This example code structure outlines how to achieve the desired plot with the specified requirements. Ensure all elements are correctly implemented.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot with specified requirements p = ( # Initial Plot so.Plot(tips, \\"total_bill\\", \\"tip\\") # Add Dot layer with customization and data filter .add(so.Dot(alpha=0.5), so.Jitter(0.1), data=tips.query(\\"total_bill > 20\\"), label=\\"Dot Plot\\") # Add Line layer with polynomial fit transform .add(so.Line(color=\'C2\'), so.PolyFit(), label=\\"Fitted Line\\") # Add Bar layer with dodge transform and data filter .add(so.Bar(color=\'C3\'), so.Dodge(), data=tips.query(\\"size == 2\\"), x=\\"day\\", label=\\"Count Bar\\") # Facet by day .facet(col=\\"day\\") ) # Display the plot p.show()"},{"question":"Quantization Accuracy Improvement in PyTorch # Objective Your task is to implement a PyTorch function that prepares a given model for quantization by fusing Conv-BN layers and adjusting the dtype of specific layers to improve quantization accuracy. # Description You will implement the function `prepare_for_quantization` that takes a PyTorch model and a list of layer types to adjust the dtype for, and returns the model with fused layers and adjusted dtypes. # Function Signature ```python def prepare_for_quantization(model: torch.nn.Module, dtype_layers: List[str], dtype: torch.dtype = torch.qint8) -> torch.nn.Module: pass ``` # Parameters - `model` (torch.nn.Module): A PyTorch model that you want to prepare for quantization. - `dtype_layers` (List[str]): A list of layer type names (as strings) for which the dtype needs to be adjusted. Examples include `[\'Conv2d\', \'Linear\']`. - `dtype` (torch.dtype): The target dtype for the specified layers. Default is `torch.qint8`. # Returns - `model` (torch.nn.Module): The modified model with fused Conv-BN layers and specified layers adjusted to the target dtype. # Constraints - Ensure that Conv-BN/Linear-BN patterns are fused using PyTorch\'s `torch.ao.quantization.fuse_modules` API. - Adjust the dtype for the specified layers to the target dtype. Use `torch.ao.quantization` APIs to achieve this. - Ensure that other parts of the model remain unaffected. # Example ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv = nn.Conv2d(1, 20, 5, 1) self.bn = nn.BatchNorm2d(20) self.fc = nn.Linear(320, 10) def forward(self, x): x = self.conv(x) x = self.bn(x) x = torch.flatten(x, 1) x = self.fc(x) return x model = SimpleModel() # Prepare model for quantization prepared_model = prepare_for_quantization(model, [\'Conv2d\', \'Linear\']) ``` In this example, the `prepare_for_quantization` function should: - Fuse the `Conv2d` and `BatchNorm2d` layers. - Adjust the dtypes of `Conv2d` and `Linear` layers to `torch.qint8`. # Note You may need to import additional modules and follow the PyTorch quantization procedures for dtype adjustment and layer fusion.","solution":"import torch import torch.nn as nn import torch.ao.quantization as quantization def fuse_model(model): layers_to_fuse = [] for name, module in model.named_children(): if isinstance(module, nn.Sequential): fuse_model(module) if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear): next_name, next_module = next(model.named_children(), (None, None)) if isinstance(next_module, nn.BatchNorm2d) or isinstance(next_module, nn.BatchNorm1d): layers_to_fuse.append([name, next_name]) if layers_to_fuse: torch.ao.quantization.fuse_modules(model, layers_to_fuse, inplace=True) def prepare_for_quantization(model: torch.nn.Module, dtype_layers: list, dtype: torch.dtype = torch.qint8) -> torch.nn.Module: model.train() fuse_model(model) # Specify quantization configuration model.qconfig = torch.ao.quantization.get_default_qconfig(\'fbgemm\') # Prepare the model for quantization aware training model = torch.ao.quantization.prepare_qat(model, inplace=True) # Change dtype for specified layers for name, module in model.named_modules(): if type(module).__name__ in dtype_layers: module.qconfig = None module.dtype = dtype return model"},{"question":"Objective: Implement and analyze the permutation feature importance on a given dataset using scikit-learn. Problem Statement: You are provided with the `load_diabetes` dataset from scikit-learn. Your task is to implement the permutation feature importance algorithm to determine the importance of each feature in predicting the target variable using a Ridge regression model. Tasks: 1. **Load and Pre-process Data**: - Load the `load_diabetes` dataset and split it into training and validation sets with a test size of 30%. 2. **Train Ridge Regression Model**: - Train a `Ridge` regression model on the training set with `alpha=0.01`. 3. **Compute Permutation Feature Importance**: - Implement the permutation feature importance algorithm manually. - Shuffle each feature in the validation set 30 times and compute the model\'s R^2 score each time. - Calculate the importance of each feature as the mean drop in the R^2 score. 4. **Output the Results**: - For each feature, if the importance (mean drop in R^2 score) minus twice its standard deviation is greater than zero, print the feature name and its importance with the standard deviation. Input: - None (Use the `load_diabetes` dataset from scikit-learn). Output: - Print the feature name and its permutation importance with standard deviation for each important feature (importance - 2 * std > 0). Constraints: - Use a `Ridge` model with `alpha=0.01`. - Perform 30 permutations for each feature. - Use the R^2 score as the evaluation metric. Example Output: ``` s5 0.204 +/- 0.050 bmi 0.176 +/- 0.048 bp 0.088 +/- 0.033 sex 0.056 +/- 0.023 ``` Performance Requirements: - The solution should handle the permutation efficiently within reasonable time limits for the given dataset. Notes: - Utilize numpy and scikit-learn libraries. - Ensure the calculation and printing follow the specified format.","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.linear_model import Ridge from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score def permutation_importance(X_val, y_val, model, n_permutations=30): baseline_score = r2_score(y_val, model.predict(X_val)) importances = {} for col in range(X_val.shape[1]): permuted_scores = [] for _ in range(n_permutations): X_val_permuted = X_val.copy() np.random.shuffle(X_val_permuted[:, col]) permuted_score = r2_score(y_val, model.predict(X_val_permuted)) permuted_scores.append(permuted_score) mean_permuted_score = np.mean(permuted_scores) std_permuted_score = np.std(permuted_scores) importance = baseline_score - mean_permuted_score importances[col] = (importance, std_permuted_score) return importances def main(): # Load dataset and split into training and validation sets diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, test_size=0.3, random_state=42) # Train Ridge regression model ridge_model = Ridge(alpha=0.01) ridge_model.fit(X_train, y_train) # Compute permutation feature importance importances = permutation_importance(X_val, y_val, ridge_model, n_permutations=30) # Print the feature names and their importances feature_names = diabetes.feature_names for col, (importance, std) in importances.items(): if importance - 2 * std > 0: print(f\\"{feature_names[col]:<8} {importance:.3f} +/- {std:.3f}\\") main()"},{"question":"Advanced Set Operations Objective Assess the student\'s ability to work with Python\'s `set` and `frozenset` objects, utilizing various API functions for construction, modification, and querying. Problem Statement Implement a function `advanced_set_operations(commands: List[Tuple[str, Any]]) -> List[Any]` that processes a list of commands to demonstrate advanced usage of sets and frozensets in Python. Each command is a tuple where the first element is a string representing the operation, and the second element is the argument(s) for that operation. The function should support the following commands and return a list of results corresponding to each operation: 1. `\\"create_set\\"` - Initialize a set with a given iterable. Example: `(\\"create_set\\", [1, 2, 3])` 2. `\\"create_frozenset\\"` - Initialize a frozenset with a given iterable. Example: `(\\"create_frozenset\\", [1, 2, 3])` 3. `\\"add\\"` - Add an element to a set. Example: `(\\"add\\", (set_object, element))` 4. `\\"discard\\"` - Discard an element from a set. Example: `(\\"discard\\", (set_object, element))` 5. `\\"contains\\"` - Check if an element is in a set or frozenset. Returns `True` or `False`. Example: `(\\"contains\\", (set_or_frozenset_object, element))` 6. `\\"pop\\"` - Pop an element from a set. Returns the popped element. Example: `(\\"pop\\", set_object)` 7. `\\"clear\\"` - Clear all elements from a set. Example: `(\\"clear\\", set_object)` 8. `\\"size\\"` - Get the size of a set or frozenset. Returns the size. Example: `(\\"size\\", set_or_frozenset_object)` When processing these commands, ensure proper error handling as described in the API. For example, handle any `TypeError` or `SystemError` where necessary. Expected Function Signature ```python from typing import Any, List, Tuple def advanced_set_operations(commands: List[Tuple[str, Any]]) -> List[Any]: pass ``` Examples ```python # Example 1 commands = [ (\\"create_set\\", [1, 2, 3]), (\\"create_frozenset\\", [4, 5, 6]), (\\"add\\", (set_object, 4)), (\\"discard\\", (set_object, 2)), (\\"contains\\", (frozenset_object, 5)), (\\"pop\\", set_object), (\\"clear\\", set_object), (\\"size\\", frozenset_object) ] # Expected output for above commands: # [set([1, 2, 3]), frozenset([4, 5, 6]), None, None, True, 1, None, 3] # Example 2 commands = [ (\\"create_set\\", [1, 2]), (\\"add\\", (set_object, 3)), (\\"contains\\", (set_object, 3)), (\\"pop\\", set_object) ] # Expected output for above commands: # [set([1, 2]), None, True, 3] ``` Constraints - The `commands` list will have a length of at most 100. - Each command\'s execution time should be within acceptable limits (O(1) for most operations).","solution":"from typing import Any, List, Tuple, Union def advanced_set_operations(commands: List[Tuple[str, Any]]) -> List[Any]: results = [] my_set = None for command in commands: op, args = command if op == \\"create_set\\": my_set = set(args) results.append(my_set) elif op == \\"create_frozenset\\": results.append(frozenset(args)) elif op == \\"add\\": set_obj, element = args set_obj.add(element) elif op == \\"discard\\": set_obj, element = args set_obj.discard(element) elif op == \\"contains\\": set_or_frozenset_obj, element = args results.append(element in set_or_frozenset_obj) elif op == \\"pop\\": set_obj = args results.append(set_obj.pop()) elif op == \\"clear\\": set_obj = args set_obj.clear() elif op == \\"size\\": set_or_frozenset_obj = args results.append(len(set_or_frozenset_obj)) return results"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of parsing, modifying, and querying XML using the `xml.etree.ElementTree` module. # Problem Statement You are given a string containing XML data representing a simplified inventory of a bookstore. Each book element consists of a title, author, genre, price, and publish date. Your task is to write a function that performs the following tasks: 1. **Parse the XML string into an ElementTree.** 2. **Find all books in the inventory that were published after a given year.** 3. **Increase the price of all books by a given percentage and return the modified XML as a string.** # Function Signature ```python def modify_bookstore_inventory(xml_data: str, year: int, price_increase_percent: float) -> str: pass ``` # Input - `xml_data` (str): A string containing the XML data of the bookstore inventory. - `year` (int): The year to filter books by their publish date. - `price_increase_percent` (float): The percentage by which to increase the price of each book. # Output - Returns a string containing the modified XML data with updated prices. # Example XML Data ```xml <bookstore> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <genre>Computer</genre> <price>39.95</price> <publish_date>2003-09-10</publish_date> </book> <book> <title>Programming Python</title> <author>Mark Lutz</author> <genre>Computer</genre> <price>59.99</price> <publish_date>2010-01-01</publish_date> </book> </bookstore> ``` # Constraints - The year parameter is a four-digit integer. - The price increase percent is a float value that represents the increase (e.g., 10.0 for a 10% increase). - Ensure that the price increase reflects accurately to two decimal places. # Example ```python xml_data = <bookstore> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <genre>Computer</genre> <price>39.95</price> <publish_date>2003-09-10</publish_date> </book> <book> <title>Programming Python</title> <author>Mark Lutz</author> <genre>Computer</genre> <price>59.99</price> <publish_date>2010-01-01</publish_date> </book> </bookstore> year = 2005 price_increase_percent = 10.0 result = modify_bookstore_inventory(xml_data, year, price_increase_percent) print(result) ``` Expected output: ```xml <bookstore> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <genre>Computer</genre> <price>39.95</price> <publish_date>2003-09-10</publish_date> </book> <book> <title>Programming Python</title> <author>Mark Lutz</author> <genre>Computer</genre> <price>65.99</price> <publish_date>2010-01-01</publish_date> </book> </bookstore> ``` # Notes - Ensure to correctly handle and parse the XML data. - Carefully increment the price with the given percentage and maintain the price format. - The output XML string should maintain the original structure except for the updated prices.","solution":"import xml.etree.ElementTree as ET def modify_bookstore_inventory(xml_data: str, year: int, price_increase_percent: float) -> str: # Parse the XML string root = ET.fromstring(xml_data) # Find all books for book in root.findall(\'book\'): # Check the publish date publish_date = book.find(\'publish_date\').text publish_year = int(publish_date.split(\'-\')[0]) if publish_year > year: # Increase the price price = float(book.find(\'price\').text) new_price = price * (1 + price_increase_percent / 100) # Update the price element book.find(\'price\').text = f\\"{new_price:.2f}\\" # Create the modified XML string return ET.tostring(root, encoding=\'utf-8\').decode(\'utf-8\')"},{"question":"# Coding Assessment: Dimensionality Reduction using Scikit-Learn Objective Demonstrate your understanding of unsupervised dimensionality reduction techniques available in the scikit-learn library. Problem Statement You are provided with a dataset (`data.csv`) containing 1000 samples with 50 features each. The dataset is a CSV file with rows representing samples and columns representing features. Your task is to perform the following operations: 1. **Load the dataset** from the CSV file into a Pandas DataFrame. 2. **Apply Principal Component Analysis (PCA)** to reduce the dataset\'s dimensionality to 10 principal components. For this step, you should: - Fit the PCA model on the dataset. - Transform the dataset using the fitted PCA model. 3. **Apply Random Projection** to further reduce the dataset from the PCA-transformed 10 components to 5 components. For this step, you should: - Use `GaussianRandomProjection` from `sklearn.random_projection`. 4. Save the final transformed dataset (after both PCA and random projection) to a new CSV file, named `transformed_data.csv`. 5. **Print the explained variance ratio** of the PCA components. Implementation Details - The input file, `data.csv`, will be provided in the current working directory. - Your output CSV file, `transformed_data.csv`, should also be saved in the current working directory. - You are allowed to use the libraries: `pandas`, `numpy`, and `scikit-learn`. Expected Input and Output - **Input**: `data.csv` (CSV file with shape `(1000, 50)`) - **Output**: `transformed_data.csv` (CSV file with shape `(1000, 5)`) Function Signature ```python def reduce_dimensionality(input_file: str) -> None: pass ``` Constraints - Make sure your code is efficient in handling the dataset operations. - Assume no missing values in the input dataset. - Assume the input data does not require additional scaling for PCA. Example Given an input file `data.csv` with the following format: ``` feature1,feature2,...,feature50 value1_1,value1_2,...,value1_50 value2_1,value2_2,...,value2_50 ... value1000_1,value1000_2,...,value1000_50 ``` Your function should create an output file `transformed_data.csv` with a format like: ``` component1,component2,...,component5 value1_1,value1_2,...,value1_5 value2_1,value2_2,...,value2_5 ... value1000_1,value1000_2,...,value1000_5 ``` Notes - Ensure you handle any exceptions, and edge cases, and ensure the robustness of your code. - Import statements should be included within your function.","solution":"import pandas as pd from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection def reduce_dimensionality(input_file: str) -> None: # Load the dataset df = pd.read_csv(input_file) # Apply PCA to reduce to 10 dimensions pca = PCA(n_components=10) pca_transformed = pca.fit_transform(df) # Print the explained variance ratio of PCA components print(\\"Explained Variance Ratio of PCA components:\\", pca.explained_variance_ratio_) # Apply Random Projection to reduce to 5 dimensions rp = GaussianRandomProjection(n_components=5) rp_transformed = rp.fit_transform(pca_transformed) # Convert the transformed data to a DataFrame transformed_df = pd.DataFrame(rp_transformed, columns=[f\'component{i+1}\' for i in range(5)]) # Save the result to a new CSV file transformed_df.to_csv(\'transformed_data.csv\', index=False)"},{"question":"**Question:** You are tasked with writing a Python function that performs a comprehensive comparison between two directory trees and identifies all the differences, including files and directories that are missing, different, or identical in each tree. This function should also report any errors encountered during the comparisons, such as permissions issues or missing files. The function you write should meet the following requirements: 1. Compare two directory trees and provide a detailed report. 2. The report should include: - Files and directories that exist only in the first directory. - Files and directories that exist only in the second directory. - Files that are present in both directories but differ in content. - Files that are present in both directories and are identical. - Any errors encountered during the comparison. 3. Your function should be able to handle nested directories (recursively). Your function signature should be as follows: ```python def compare_directories(dir1: str, dir2: str) -> dict: Compare two directory trees and return a report of the differences. Params: dir1 (str): Path to the first directory. dir2 (str): Path to the second directory. Returns: dict: A dictionary containing the comparison report with keys: - \\"unique_to_dir1\\": List of files/directories only in dir1. - \\"unique_to_dir2\\": List of files/directories only in dir2. - \\"different_files\\": List of files that differ in content. - \\"identical_files\\": List of files that are identical. - \\"errors\\": List of any errors encountered during comparison. pass ``` **Constraints:** - You may assume that the directory paths provided are valid paths on the system. - Your solution should be efficient, making use of the built-in caching provided by `filecmp` where appropriate. - Handle edge cases such as empty directories or directories containing only subdirectories/files that are ignored. **Example Usage:** ```python # Suppose we have the following directory structures: # dir1/ # â”œâ”€â”€ file1.txt # â”œâ”€â”€ file2.txt # â””â”€â”€ subdir1/ # â””â”€â”€ file3.txt # # dir2/ # â”œâ”€â”€ file1.txt (same content as in dir1) # â”œâ”€â”€ file2.txt (different content from the one in dir1) # â””â”€â”€ subdir2/ # â””â”€â”€ file4.txt report = compare_directories(\'dir1\', \'dir2\') print(report) # Output should be something like: # { # \\"unique_to_dir1\\": [\\"subdir1\\"], # \\"unique_to_dir2\\": [\\"subdir2\\"], # \\"different_files\\": [\\"file2.txt\\"], # \\"identical_files\\": [\\"file1.txt\\"], # \\"errors\\": [] # } ``` **Note:** Make sure to use the appropriate functions and methods from the `filecmp` module, and structure your solution logically to handle recursive directory traversal and detailed reporting.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: report = { \\"unique_to_dir1\\": [], \\"unique_to_dir2\\": [], \\"different_files\\": [], \\"identical_files\\": [], \\"errors\\": [] } def handle_dir_comparison(dcmp): for name in dcmp.left_only: report[\'unique_to_dir1\'].append(os.path.join(dcmp.left, name)) for name in dcmp.right_only: report[\'unique_to_dir2\'].append(os.path.join(dcmp.right, name)) for name in dcmp.diff_files: report[\'different_files\'].append(os.path.join(dcmp.left, name)) for name in dcmp.same_files: report[\'identical_files\'].append(os.path.join(dcmp.left, name)) try: for sub_dcmp in dcmp.subdirs.values(): handle_dir_comparison(sub_dcmp) except Exception as e: report[\'errors\'].append(str(e)) try: dcmp = filecmp.dircmp(dir1, dir2) handle_dir_comparison(dcmp) except Exception as e: report[\'errors\'].append(str(e)) return report"},{"question":"Coding Assessment Question # Objective In this assessment, you will demonstrate your ability to manipulate and transform computational graphs using PyTorch\'s FX module. Your task is to implement a custom transformation that replaces specific operators in a given graph and introduces a new operator at certain points. This will test your understanding of both basic and advanced concepts of graph transformation using PyTorch. # Task Write a Python function `transform_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule` that takes a `GraphModule` as input and performs the following transformations in the specified order: 1. Replace every occurrence of `torch.ops.aten.add.Tensor` with `torch.ops.aten.mul.Tensor`. 2. After every `torch.ops.aten.mul.Tensor` operation, insert a `torch.ops.aten.relu.default` operation. 3. Remove any occurrence of `torch.ops.aten.detach.default` operation from the graph. # Input - `gm`: An instance of `torch.fx.GraphModule` representing the computational graph to be transformed. # Output - Return the transformed `torch.fx.GraphModule`. # Constraints - You should use PyTorch FXâ€™s `Transformer` class to implement these transformations. - Assume the input graph can contain any combination of the mentioned operations and possibly other operations. # Example Given the following `GraphModule`: ```python import torch class ExampleModule(torch.nn.Module): def forward(self, x, y): z = torch.ops.aten.add.Tensor(x, y) z = torch.ops.aten.detach(z) return z example_module = ExampleModule() graph_module = torch.fx.symbolic_trace(example_module) ``` After applying `transform_graph(graph_module)`, the transformed graph should look like: ```python class TransformedModule(torch.nn.Module): def forward(self, x, y): z = torch.ops.aten.mul.Tensor(x, y) z = torch.ops.aten.relu.default(z) return z transformed_module = TransformedModule() ``` # Implementation Now, implement the `transform_graph` function. ```python import torch def transform_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: class CustomTransform(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.add.Tensor: target = torch.ops.aten.mul.Tensor # Create mul operation new_node = super().call_function(target, args, kwargs) # Insert relu operation after mul relu_node = super().call_function(torch.ops.aten.relu.default, (new_node,), {}) return relu_node elif target in [torch.ops.aten.detach.default, torch.ops.aten.detach_copy.default]: # Remove detach operation return args[0] # Default call for other operations return super().call_function(target, args, kwargs) transformed_gm = CustomTransform(gm).transform() return transformed_gm # Example usage example_module = ExampleModule() graph_module = torch.fx.symbolic_trace(example_module) transformed_module = transform_graph(graph_module) print(transformed_module.graph) ``` # Note - Ensure your implementation efficiently handles large graphs. - Use the provided example to test your function locally.","solution":"import torch def transform_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: class CustomTransform(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.add.Tensor: mul_target = torch.ops.aten.mul.Tensor # Create mul operation mul_node = super().call_function(mul_target, args, kwargs) # Insert relu operation after mul relu_node = super().call_function(torch.ops.aten.relu.default, (mul_node,), {}) return relu_node elif target == torch.ops.aten.detach.default: # Remove detach operation return args[0] return super().call_function(target, args, kwargs) transformed_gm = CustomTransform(gm).transform() return transformed_gm # Example class for unit tests class ExampleModule(torch.nn.Module): def forward(self, x, y): z = torch.ops.aten.add.Tensor(x, y) z = torch.ops.aten.detach(z) return z"},{"question":"**Performance Profiling Task** Your task is to write a program that profiles the performance of the following Python function and then reports on the most time-consuming sections. # Function to Profile ```python def string_manipulation(input_string): result = [] for i, char in enumerate(input_string): if char in \'aeiouAEIOU\': result.append(char.upper()) else: result.append(char.lower()) return \'\'.join(result) # Example usage string_manipulation(\\"Hello World\\") ``` # Requirements 1. **Profile the Function**: Use the `cProfile` module to profile the `string_manipulation` function with a sample input string of at least 1000 characters. 2. **Save Profiling Data**: Save the profiling data to a file named `profiling_results`. 3. **Process Profiling Data**: Using the `pstats` module, process the saved profiling data to generate a report. 4. **Generate Reports**: - Print the top 10 function calls sorted by cumulative time. - Identify and print the function that has the highest total time spent. # Constraints - The profiling must use the `cProfile` module. - The input string for the function should be at least 1000 characters long. # Expected Output Your program should output: 1. A summary of the profiling data, showing the top 10 function calls by cumulative time. 2. The function within `string_manipulation` that took the longest total time to execute. # Additional Notes - Ensure your profiling implementation includes appropriate usage of `enable()` and `disable()` methods. - Comments and docstrings should be added to explain the components of your solution for clarity. # Python Boilerplate Code ```python import cProfile import pstats import io def profile_string_manipulation(): def string_manipulation(input_string): result = [] for i, char in enumerate(input_string): if char in \'aeiouAEIOU\': result.append(char.upper()) else: result.append(char.lower()) return \'\'.join(result) # Generate a sample input string of at least 1000 characters sample_input = \\"Hello World \\" * 100 # Setup and run the profiler pr = cProfile.Profile() pr.enable() string_manipulation(sample_input) pr.disable() # Save profiling data to a file pr.dump_stats(\'profiling_results\') # Process the profiling data using pstats s = io.StringIO() ps = pstats.Stats(\'profiling_results\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) # Print the top 10 function calls sorted by cumulative time print(\\"nTop 10 function calls by cumulative time:\\") print(s.getvalue()) # Identify and print the function with the highest total time spent print(\\"nFunction with highest total time spent:\\") ps.sort_stats(pstats.SortKey.TIME) ps.print_stats(1) # Call the function to profile and print the results if __name__ == \\"__main__\\": profile_string_manipulation() ```","solution":"import cProfile import pstats import io def string_manipulation(input_string): Manipulates the input string by converting vowels to uppercase and consonants to lowercase. result = [] for i, char in enumerate(input_string): if char in \'aeiouAEIOU\': result.append(char.upper()) else: result.append(char.lower()) return \'\'.join(result) def profile_string_manipulation(): Profiles the string_manipulation function and reports the top 10 function calls by cumulative time and the function with the highest total time spent. # Generate a sample input string of at least 1000 characters sample_input = \\"Hello World \\" * 100 # Setup and run the profiler pr = cProfile.Profile() pr.enable() string_manipulation(sample_input) pr.disable() # Save profiling data to a file pr.dump_stats(\'profiling_results\') # Process the profiling data using pstats s = io.StringIO() ps = pstats.Stats(\'profiling_results\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) # Print the top 10 function calls sorted by cumulative time top_10_stats = s.getvalue() print(\\"nTop 10 function calls by cumulative time:\\") print(top_10_stats) # Identify and print the function with the highest total time spent s.seek(0) ps.sort_stats(pstats.SortKey.TIME) ps.print_stats(1) highest_total_time = s.getvalue() print(\\"nFunction with highest total time spent:\\") print(highest_total_time) # Example usage if __name__ == \\"__main__\\": profile_string_manipulation()"},{"question":"# Clustering Implementation and Evaluation with scikit-learn As a data scientist, you are tasked to implement and compare the clustering results using two different clustering algorithms: **K-means** and **DBSCAN**. You will apply these algorithms on a synthetic dataset created using the `make_blobs` function in scikit-learn. Afterwards, you will evaluate their performance using the Silhouette Coefficient metric. # Requirements: 1. **Generate Synthetic Data** - Use the `make_blobs` function to generate a synthetic dataset with 3 distinct clusters. The dataset should contain 1000 samples and each sample should have 2 features. - Ensure that the generated data is stored in variables `X` and `y`. 2. **Implement Clustering** - Implement the **KMeans** clustering algorithm from scikit-learn with the following parameters: - Number of clusters: 3 - Implement the **DBSCAN** clustering algorithm from scikit-learn with the following parameters: - `eps` (maximum distance between two samples for them to be considered as in the same neighborhood): 0.5 - `min_samples` (number of samples in a neighborhood for a point to be considered as a core point): 5 3. **Evaluate Clustering Performance** - Compute the **Silhouette Coefficient** for both the KMeans and DBSCAN clusterings. - Print the Silhouette Coefficient for each algorithm. # Input and Output Formats: - **Input:** No direct input (hard-coded parameters for clustering algorithms). - **Output:** Print the Silhouette Coefficient for both clustering algorithms. # Constraints: - Use scikit-learn and numpy for implementations. - Ensure your solution runs efficiently on datasets with up to 100,000 samples without significant delay. # Example Code Structure: ```python import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score # 1. Generate synthetic data X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=42) # 2. Implement KMeans clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(X) # 3. Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(X) # 4. Evaluate clustering performance kmeans_silhouette = silhouette_score(X, kmeans_labels) dbscan_silhouette = silhouette_score(X, dbscan_labels) print(f\'Silhouette Coefficient for KMeans: {kmeans_silhouette}\') print(f\'Silhouette Coefficient for DBSCAN: {dbscan_silhouette}\') ``` Ensure that your implementation uses the given parameters for each clustering algorithm and follows the example code structure for consistency.","solution":"import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score def generate_data(n_samples=1000, centers=3, n_features=2, random_state=42): X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_state) return X, y def kmeans_clustering(X, n_clusters=3, random_state=42): kmeans = KMeans(n_clusters=n_clusters, random_state=random_state) labels = kmeans.fit_predict(X) return labels def dbscan_clustering(X, eps=0.5, min_samples=5): dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(X) return labels def get_silhouette_score(X, labels): return silhouette_score(X, labels) def main(): # Generate synthetic data X, y = generate_data() # Implement KMeans clustering kmeans_labels = kmeans_clustering(X) kmeans_silhouette = get_silhouette_score(X, kmeans_labels) # Implement DBSCAN clustering dbscan_labels = dbscan_clustering(X) dbscan_silhouette = get_silhouette_score(X, dbscan_labels) print(f\'Silhouette Coefficient for KMeans: {kmeans_silhouette}\') print(f\'Silhouette Coefficient for DBSCAN: {dbscan_silhouette}\') if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement** You are provided with a CSV file called `sales_data.csv` that contains sales data of a company over several years. The CSV file has the following columns: - `date`: The date of the sales record (in the format `YYYY-MM-DD`). - `product`: The product name. - `quantity`: The quantity sold. - `price`: The price per unit of the product. You are required to perform the following tasks using `pandas.Series`: 1. **Read the Data**: Read the CSV file into a pandas DataFrame. 2. **Date Analysis**: - Convert the `date` column to pandas datetime format. - Extract and add a new column named `year` from the `date` column. 3. **Sales Analysis**: - Create a new Series called `total_sales` that calculates the total sales (quantity * price) for each record. - Find out the top 3 products by their total sales and return their names and total sales. 4. **Handling Missing Data**: - Check for missing data in the DataFrame and report which columns have missing data. - Fill any missing values in the `quantity` and `price` columns with the median value of their respective columns. 5. **Group By Operations**: - Group the data by `year` and calculate the total sales, average quantity sold per product, and the total number of sales for each year. 6. **Descriptive Statistics**: - Provide a descriptive statistics summary for the `price` and `quantity` columns (mean, median, standard deviation, etc.). 7. **Time Series Analysis**: - Resample the sales data to get monthly sales figures and plot the trend. # Input - A CSV file `sales_data.csv`. # Output - Print the extracted and cleaned DataFrame. - Print the top 3 products by total sales. - Print the columns with missing data. - Print the grouped statistics by year. - Print the descriptive statistics summary for the `price` and `quantity` columns. - Display the monthly sales trend plot. # Constraints - Handle the missing data appropriately. - Ensure that numerical computations are correct. # Performance requirements - The operations, especially group-by and resampling, should be efficient and not use excessive memory. # Example Usage ```python import pandas as pd # 1. Load the data df = pd.read_csv(\'sales_data.csv\') # 2. Date Analysis df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'year\'] = df[\'date\'].dt.year # 3. Sales Analysis df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] top_products = df.groupby(\'product\')[\'total_sales\'].sum().nlargest(3) print(\\"Top 3 products by total sales:n\\", top_products) # 4. Handling Missing Data missing_data_columns = df.columns[df.isna().any()].tolist() print(\\"Columns with missing data:\\", missing_data_columns) df[\'quantity\'].fillna(df[\'quantity\'].median(), inplace=True) df[\'price\'].fillna(df[\'price\'].median(), inplace=True) # 5. Group By Operations grouped_data = df.groupby(\'year\').agg({\'total_sales\': \'sum\', \'quantity\': \'mean\', \'price\': \'count\'}) print(\\"Grouped statistics by year:n\\", grouped_data) # 6. Descriptive Statistics desc_stats = df[[\'price\', \'quantity\']].describe() print(\\"Descriptive Statistics Summary:n\\", desc_stats) # 7. Time Series Analysis monthly_sales = df.resample(\'M\', on=\'date\')[\'total_sales\'].sum() monthly_sales.plot(title=\'Monthly Sales Trend\') ``` **Note**: Ensure you have the `sales_data.csv` file available to test your code.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path): # Step 1: Read the data df = pd.read_csv(file_path) # Step 2: Date Analysis df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'year\'] = df[\'date\'].dt.year # Step 3: Sales Analysis df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] top_products = df.groupby(\'product\')[\'total_sales\'].sum().nlargest(3) # Step 4: Handling Missing Data missing_data_columns = df.columns[df.isna().any()].tolist() df[\'quantity\'].fillna(df[\'quantity\'].median(), inplace=True) df[\'price\'].fillna(df[\'price\'].median(), inplace=True) # Step 5: Group By Operations grouped_data = df.groupby(\'year\').agg({\'total_sales\': \'sum\', \'quantity\': \'mean\', \'price\': \'count\'}) # Step 6: Descriptive Statistics desc_stats = df[[\'price\', \'quantity\']].describe() # Step 7: Time Series Analysis monthly_sales = df.resample(\'M\', on=\'date\')[\'total_sales\'].sum() plt.figure() monthly_sales.plot(title=\'Monthly Sales Trend\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.show() # Print the results print(\\"Extracted and cleaned DataFrame:n\\", df.head()) print(\\"nTop 3 products by total sales:n\\", top_products) print(\\"nColumns with missing data:\\", missing_data_columns) print(\\"nGrouped statistics by year:n\\", grouped_data) print(\\"nDescriptive Statistics Summary:n\\", desc_stats) # Uncomment below line to run the function with your CSV file path # analyze_sales_data(\'sales_data.csv\')"},{"question":"# Password Database Querying and Manipulation **Problem Statement**: You are tasked with creating a utility function that analyzes the Unix password database to extract and manipulate specific information. Given the `pwd` module as described, implement the following function: Function: `get_user_info_by_group_gid(group_gid: int) -> List[Tuple[str, int]]` **Objective**: Find all users who belong to a specified group ID (`group_gid`), and return a list of tuples, each containing the user\'s login name and their user ID. **Function Signature**: ```python from typing import List, Tuple def get_user_info_by_group_gid(group_gid: int) -> List[Tuple[str, int]]: ``` **Input**: - `group_gid`: An integer representing the target group ID. **Output**: - A list of tuples. Each tuple contains two elements: - `pw_name`: the login name of the user (string). - `pw_uid`: the numerical user ID of the user (integer). **Constraints**: - The function should only include users who belong to the provided `group_gid`. - If no users belong to the `group_gid`, return an empty list. **Performance Requirements**: - Solutions should efficiently handle databases of varying sizes. **Example**: ```python # Example usage: results = get_user_info_by_group_gid(1000) # Expected: [(\'userA\', 1001), (\'userB\', 1002)] assuming users match the group id 1000 ``` **Note**: - Use the `pwd` module to fetch the password database entries. - Handle any edge cases appropriately, such as no users for the group id.","solution":"import pwd from typing import List, Tuple def get_user_info_by_group_gid(group_gid: int) -> List[Tuple[str, int]]: Returns a list of tuples containing the user\'s login name and user ID for all users who belong to the specified group ID (group_gid). result = [] for user in pwd.getpwall(): if user.pw_gid == group_gid: result.append((user.pw_name, user.pw_uid)) return result"},{"question":"**Question:** You are tasked to manage a dynamic dataset of student scores where the goal is to maintain the order of scores as students are added. Your task is to implement a class `ScoreManager` that supports efficiently adding scores and querying them for various conditions. Implement the `ScoreManager` class with the following methods: 1. `def __init__(self)`: Initializes an empty `ScoreManager`. 2. `def add_score(self, score: int) -> None`: Adds the provided `score` to the dataset. This method should employ the relevant functions from the `bisect` module to maintain a sorted order of scores. 3. `def get_top_scores(self, n: int) -> List[int]`: Returns the top `n` scores in descending order from the dataset. If there are fewer than `n` scores, return all the scores in descending order. 4. `def get_scores_above(self, threshold: int) -> List[int]`: Returns a list of scores that are strictly greater than the provided `threshold`, in descending order. 5. `def get_scores_below(self, threshold: int) -> List[int]`: Returns a list of scores that are strictly less than the provided `threshold`, in ascending order. **Constraints:** - You must use the relevant functions from the `bisect` module to maintain the order of the scores. - Assume all scores are unique integers for simplicity. - `n` is guaranteed to be a positive integer not greater than the number of scores in the dataset. **Example Usage:** ```python manager = ScoreManager() manager.add_score(85) manager.add_score(95) manager.add_score(75) manager.add_score(65) print(manager.get_top_scores(3)) # Output: [95, 85, 75] print(manager.get_scores_above(70)) # Output: [95, 85, 75] print(manager.get_scores_below(80)) # Output: [65, 75] ``` **Hint:** Use the `bisect` functions effectively to find the insertion points and maintain the list in sorted order. For descending order outputs, consider how to reverse the order or use appropriate techniques.","solution":"from bisect import insort, bisect_right, bisect_left from typing import List class ScoreManager: def __init__(self): self.scores = [] def add_score(self, score: int) -> None: insort(self.scores, score) def get_top_scores(self, n: int) -> List[int]: return self.scores[-n:][::-1] def get_scores_above(self, threshold: int) -> List[int]: idx = bisect_right(self.scores, threshold) return self.scores[idx:][::-1] def get_scores_below(self, threshold: int) -> List[int]: idx = bisect_left(self.scores, threshold) return self.scores[:idx]"},{"question":"**Coding Assessment Question:** You are given a dataset named `tips` which contains information about the tips received by waitstaff in a restaurant. The dataset includes the following columns: - `total_bill`: the total bill amount. - `tip`: the tip amount. - `sex`: gender of the person paying the bill. - `smoker`: whether the person is a smoker or not. - `day`: day of the week. - `time`: time of day (Lunch or Dinner). - `size`: size of the dining party. Your task is to create a function `custom_stripplot` that generates a categorical scatterplot using Seaborn\'s `stripplot` with the following specifications: 1. Plot the relationship between `total_bill` and `day`, using `sex` as the `hue` variable. 2. Ensure that the plot shows the points differentiated by `sex` with different colors. 3. Use the `dodge` parameter to ensure the points for each `sex` are split apart. 4. Disable the random jitter effect. 5. Customize the plot by setting the marker style to a diamond (`\\"D\\"`), point size to 10, and transparency to 0.6. 6. Add a title to the plot: \\"Total Bill vs Day by Sex\\". 7. Return the Seaborn axis object from the function to allow further customization if needed. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(tips): Creates a Seaborn stripplot for the given tips dataset. Parameters: - tips (DataFrame): A DataFrame containing the tips dataset. Returns: - ax (Axes): A Seaborn axis object containing the generated plot. # Your code here return ax ``` **Example Usage:** ```python tips = sns.load_dataset(\\"tips\\") ax = custom_stripplot(tips) plt.show() ``` **Constraints:** - Ensure to handle any potential warnings or errors related to the visualization settings. - The function should be efficient and follow clean coding practices for better readability and maintainability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(tips): Creates a Seaborn stripplot for the given tips dataset. Parameters: - tips (DataFrame): A DataFrame containing the tips dataset. Returns: - ax (Axes): A Seaborn axis object containing the generated plot. plt.figure(figsize=(10, 6)) ax = sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, dodge=True, jitter=False, marker=\\"D\\", size=10, alpha=0.6) ax.set_title(\\"Total Bill vs Day by Sex\\") return ax"},{"question":"Working with AIFF Files using `aifc` Problem Statement You are provided with a directory containing multiple AIFF (Audio Interchange File Format) files. Your task is to write a Python function `process_aiff_files` that performs the following operations: 1. **Scan the directory and gather audio file statistics**: - Count the total number of AIFF files. - Determine the average number of channels across all AIFF files. - Determine the average sampling rate across all AIFF files. 2. **Modify and save a copy of each AIFF file**: - For each AIFF file, double the frame rate (i.e., sample at twice the original frame rate) and save the modified file with a \\"modified_\\" prefix added to the original filename. Function Signature ```python def process_aiff_files(directory: str) -> dict: Processes AIFF files in a given directory and returns statistics about the files. Parameters: - directory (str): Path to the directory containing AIFF files. Returns: - dict: A dictionary with the following keys and corresponding values: - \\"total_files\\" (int): Total number of AIFF files. - \\"average_channels\\" (float): Average number of channels across all AIFF files. - \\"average_framerate\\" (float): Average sampling rate across all AIFF files. pass ``` Constraints - You may assume all files in the given directory with the \'.aiff\' extension are valid AIFF files. - Handle exceptions gracefully, such as when a file cannot be read, to prevent the program from crashing. Example Usage Suppose you have the following AIFF files in a directory `audio_files/`: - `audio1.aiff` : 2 channels, 44100 Hz - `audio2.aiff` : 1 channel, 48000 Hz When running the function: ```python result = process_aiff_files(\'audio_files/\') print(result) ``` The resulting output dictionary should be: ```python { \\"total_files\\": 2, \\"average_channels\\": 1.5, \\"average_framerate\\": 46050.0 } ``` Additionally, the directory `audio_files/` will now contain: - `modified_audio1.aiff` - `modified_audio2.aiff` Notes - Use the `aifc` module as documented to achieve these tasks. - Ensure to set appropriate parameters when writing new AIFF files to reflect the changes in frame rate. - Thoroughly test the function to ensure it handles edge cases, such as files with different numbers of channels and frame rates. Good luck and happy coding!","solution":"import os import aifc def process_aiff_files(directory: str) -> dict: total_files = 0 total_channels = 0 total_framerate = 0 for filename in os.listdir(directory): if filename.endswith(\'.aiff\'): try: filepath = os.path.join(directory, filename) with aifc.open(filepath, \'r\') as f: total_files += 1 num_channels = f.getnchannels() framerate = f.getframerate() total_channels += num_channels total_framerate += framerate # Read frames and params nframes = f.getnframes() frames = f.readframes(nframes) compname = f.getcomptype() comparam = f.getcompname() # Write to a new modified file modified_filepath = os.path.join(directory, f\'modified_{filename}\') with aifc.open(modified_filepath, \'w\') as modified_file: modified_file.setnchannels(num_channels) modified_file.setsampwidth(f.getsampwidth()) modified_file.setframerate(framerate * 2) # Double the frame rate modified_file.setnframes(nframes) modified_file.setcomptype(compname, comparam) modified_file.writeframes(frames) except (aifc.Error, IOError) as e: print(f\\"Error processing file {filename}: {e}\\") if total_files == 0: return { \\"total_files\\": total_files, \\"average_channels\\": 0, \\"average_framerate\\": 0 } return { \\"total_files\\": total_files, \\"average_channels\\": total_channels / total_files, \\"average_framerate\\": total_framerate / total_files }"},{"question":"# Advanced Python Calling Protocols In this task, you are required to implement a custom callable Python class that demonstrates your understanding of the `tp_call` and `vectorcall` protocols used in CPython. Your class should integrate with CPythonâ€™s calling mechanisms and should support both protocols as described in the supplied documentation. # Requirements: 1. **Callable Class**: Create a Python class `CustomCallable` that instances of it can be called like a function. 2. **__call__ Method**: Implement the `__call__` method in your class to handle the call using both positional and keyword arguments. 3. **tp_call Protocol**: - Implement a method named `tp_call_method` that mimics the behavior of CPythonâ€™s `tp_call` protocol. - Accept positional arguments as a tuple and keyword arguments as a dictionary. 4. **vectorcall Protocol**: - Implement a method named `vectorcall_method` that supports the vectorcall protocol. - Accept positional arguments as an array and keyword arguments by name. 5. **Seamless Integration**: - Ensure that regardless of the calling method used (`tp_call` or `vectorcall`), the class behaves consistently and returns the correct results. - Handle recursion control as mentioned in the documentation. # Constraints: - Your implementation should correctly handle 0 or more positional arguments. - Your implementation should correctly handle 0 or more keyword arguments. - No additional libraries should be used for argument parsing. # Example: ```python # Instantiate the class cc = CustomCallable() # Call the instance as a function result = cc(1, 2, 3, key1=\'value1\', key2=\'value2\') # The expected result should be computed correctly based on your implementation logic. print(result) # Example output based on your function logic ``` # Input and Output Format: - **Input**: - Positional arguments (0 or more) - Keyword arguments (0 or more) - **Output**: - A combined result based on your internal logic (you can decide this logic for educational purposes) # Performance Requirements: - Ensure that the implemented class performs efficiently in handling calls with a significant number of arguments. Create and test your `CustomCallable` class to verify that it meets all the requirements and integrations specified.","solution":"class CustomCallable: def __call__(self, *args, **kwargs): return self.tp_call_method(args, kwargs) def tp_call_method(self, args, kwargs): # A simple internal logic that sums positional arguments and # concatenates key-value pairs for keyword arguments pos_sum = sum(args) kw_concat = \\", \\".join(f\\"{k}={v}\\" for k, v in kwargs.items()) return f\\"Positional Sum: {pos_sum}, Keyword String: {kw_concat}\\" def vectorcall_method(self, args, kwargs): # Vectorcall should behave the same as tp_call in our educational example pos_sum = sum(args) kw_concat = \\", \\".join(f\\"{k}={v}\\" for k, v in kwargs.items()) return f\\"Positional Sum: {pos_sum}, Keyword String: {kw_concat}\\""},{"question":"# Seaborn Color Palette Manipulation and Visualization In this coding assessment, you are tasked with demonstrating your understanding of Seaborn by using the `hls_palette` method to generate and visualize various color palettes. You will need to create color palettes based on varying parameters and generate specific plots using Seabornâ€™s plotting functions to showcase these palettes. Task: 1. **Create Color Palettes**: - Create a default HLS color palette with 6 colors. - Create another HLS color palette with 10 colors, lightness of 0.5, and saturation of 0.5. - Create a continuous colormap using the HLS palette with a starting hue of 0.3 and lightness of 0.8. 2. **Visualize Palettes**: - Use the `sns.palplot` function to display all the three palettes created above. 3. **Create Plots**: - Using the Iris dataset (`sns.load_dataset(\\"iris\\")`), create the following plots: - A `scatterplot` comparing `sepal_length` and `sepal_width` using the first palette. - A `kdeplot` of `sepal_length` for the different species in the Iris dataset, colored by the second palette. - A `heatmap` of the correlation matrix of the Iris dataset using the continuous colormap palette. Implementation: Your solution should implement the following function: ```python def visualize_seaborn_palettes(): import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # 1. Create Color Palettes palette1 = sns.hls_palette() palette2 = sns.hls_palette(10, l=0.5, s=0.5) continuous_palette = sns.hls_palette(6, h=0.3, l=0.8, as_cmap=True) # 2. Visualize Palettes using sns.palplot sns.palplot(palette1) plt.title(\\"Default HLS Palette\\") plt.show() sns.palplot(palette2) plt.title(\\"HLS Palette with 10 colors, l=0.5, s=0.5\\") plt.show() sns.palplot(sns.color_palette(continuous_palette)) plt.title(\\"Continuous HLS Palette with h=0.3, l=0.8\\") plt.show() # 3. Create Plots plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', palette=palette1) plt.title(\\"Scatterplot of Sepal Dimensions\\") plt.show() plt.figure(figsize=(10, 6)) for species in iris[\'species\'].unique(): sns.kdeplot(iris.loc[iris[\'species\'] == species, \'sepal_length\'], label=species, palette=palette2) plt.title(\\"KDE Plot of Sepal Length by Species\\") plt.legend() plt.show() correlation_matrix = iris.corr() plt.figure(figsize=(10, 6)) sns.heatmap(correlation_matrix, cmap=continuous_palette, annot=True) plt.title(\\"Heatmap of Iris Dataset Correlation Matrix\\") plt.show() ``` # Input and Output: - The function does not take any input parameters. - The function does not return any value. It should display the plots directly. # Constraints: - Ensure you have the Seaborn and Matplotlib libraries installed (`pip install seaborn matplotlib`). - Any generated plots must be clear, and the palettes should be easily distinguishable. Note: Make sure to import necessary libraries and handle loading datasets appropriately within the function.","solution":"def visualize_seaborn_palettes(): import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # 1. Create Color Palettes palette1 = sns.hls_palette(6) palette2 = sns.hls_palette(10, l=0.5, s=0.5) continuous_palette = sns.hls_palette(200, h=0.3, l=0.8, as_cmap=True) # 2. Visualize Palettes using sns.palplot sns.palplot(palette1) plt.title(\\"Default HLS Palette with 6 colors\\") plt.show() sns.palplot(palette2) plt.title(\\"HLS Palette with 10 colors, l=0.5, s=0.5\\") plt.show() linear_palette = sns.color_palette(continuous_palette, 200) sns.palplot(linear_palette) plt.title(\\"Continuous HLS Palette with h=0.3, l=0.8\\") plt.show() # 3. Create Plots plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette1) plt.title(\\"Scatterplot of Sepal Dimensions\\") plt.show() plt.figure(figsize=(10, 6)) for species in iris[\'species\'].unique(): sns.kdeplot(iris.loc[iris[\'species\'] == species, \'sepal_length\'], label=species) plt.title(\\"KDE Plot of Sepal Length by Species\\") plt.legend() plt.show() correlation_matrix = iris.corr() plt.figure(figsize=(10, 6)) sns.heatmap(correlation_matrix, cmap=continuous_palette, annot=True) plt.title(\\"Heatmap of Iris Dataset Correlation Matrix\\") plt.show()"},{"question":"Title: File System Interaction and Process Control Objective: Implement a Python script that demonstrates the use of file system path handling and process forking, making use of specific functions described in the python310 documentation. Description: 1. **FileSystem Handler:** - Create a function `get_fs_path(path)` that wraps around the `PyOS_FSPath()` function described in the documentation. This function should take a path (either a string or a PathLike object) and return the file system representation of the path. Raise a `TypeError` if the path is not valid. 2. **Interactive Check:** - Implement a function `is_interactive(fp, filename)` which checks if the provided file pointer is interactive using the `Py_FdIsInteractive()` function. 3. **Process Controller:** - Using the forking utilities (`PyOS_BeforeFork()`, `PyOS_AfterFork_Parent()`, `PyOS_AfterFork_Child()`), implement a function `create_fork()` that: - Prepares the process for forking. - Forks the process. - In the parent process, updates the state after forking and prints a message. - In the child process, updates the interpreter state and prints a different message. **Constraints:** - Ensure proper handling of exceptions and edge cases (e.g., invalid path input in `get_fs_path`, invalid file pointers in `is_interactive`). Input/Output Examples: ```python def test_functions(): # Test for FileSystem Handler print(get_fs_path(\\"/path/to/file\\")) # Expected output: b\'/path/to/file\' or \'/path/to/file\' # Create a file to test interactivity with open(\'testfile.txt\', \'w\') as f: print(is_interactive(f, \'testfile.txt\')) # Expected output: False # Test Process Controller create_fork() # Expected output: Different messages from parent and child processes test_functions() ``` Performance Requirements: - The implementation should be efficient and properly demonstrate the usage of the functions detailed in the `python310` documentation. - Conform to best practices for Python programming, including resource management (ensuring files are closed properly). Submission: Submit your implemented Python functions including any necessary imports and exception handling.","solution":"import os import sys from pathlib import Path def get_fs_path(path): Wraps around the os.fspath() function to get the file system representation of the path. Raises TypeError if the path is not a valid file system path. try: return os.fspath(path) except TypeError as e: raise TypeError(f\\"Invalid path: {e}\\") def is_interactive(fp, filename): Checks if the provided file pointer is interactive. return os.isatty(fp.fileno()) def create_fork(): Demonstrates process forking and updates interpreter state for the parent and child processes. # Before Fork print(\\"Preparing to fork...\\") pid = os.fork() # Parent process if pid > 0: print(f\\"Forked a child process with PID {pid}\\") print(\\"In parent process after fork.\\") # Child process else: print(\\"In child process after fork.\\") # Main function def main(): # Test for FileSystem Handler print(get_fs_path(\\"/path/to/file\\")) # Expected output: \'/path/to/file\' # Create a file to test interactivity as an example with open(\'testfile.txt\', \'w\') as f: print(is_interactive(f, \'testfile.txt\')) # Expected output: False # Test Process Controller create_fork() # Expected output: Different messages from parent and child processes if __name__ == \\"__main__\\": main()"},{"question":"**Question: Advanced DataFrame Manipulation with Pandas** You are provided with a dataset in the form of a pandas DataFrame. The dataset contains information on sales transactions from several store locations over multiple months. Your task is to write a function that will perform a series of data manipulations and return the final processed DataFrame. Here is the DataFrame schema: ```plaintext | Column | Type | Description | |---------------|---------|--------------------------------------| | store_id | int | Identifier for the store | | store_location| str | Location of the store | | date | str | Date of the transaction (YYYY-MM) | | transaction_id| int | Unique identifier for the transaction| | product_id | int | Identifier for the product | | product_name | str | Name of the product | | quantity_sold | int | Quantity of the product sold | | price_per_unit| float | Price per unit of the product | | total_sales | float | Total sales value | ``` # Requirements: 1. **Multi-level Indexing**: * Set a MultiIndex on the DataFrame using `store_id` and `date`. 2. **Monthly Aggregations**: * Group the data by the new MultiIndex and calculate the following aggregations: - Total quantity sold per store per month. - Total sales per store per month. - Average price per unit per store per month. 3. **Handling Missing Data**: * Identify and fill any missing values in the resulting DataFrame. Use the mean of the column to fill the missing values. 4. **Monthly Growth Calculation**: * For each store, calculate the month-over-month growth rate in total sales. You may assume that the dates are sorted in ascending order. * Append a new column `sales_growth_rate` representing the calculated growth rate. If there\'s no previous month to compare, the growth rate should be \'NaN\'. 5. **Final Output**: * The function should return a DataFrame meeting all the transformations stated above. * Ensure the DataFrame is sorted first by `store_id` and then by `date`. # Constraints: - The function should handle large datasets efficiently. - Assume that the input DataFrame is correctly formatted as specified. # Function Signature: ```python def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Example Usage: ```python import pandas as pd data = { \'store_id\': [1, 1, 1, 2, 2, 2], \'store_location\': [\'NY\', \'NY\', \'NY\', \'CA\', \'CA\', \'CA\'], \'date\': [\'2023-01\', \'2023-02\', \'2023-03\', \'2023-01\', \'2023-02\', \'2023-03\'], \'transaction_id\': [101, 102, 103, 201, 202, 203], \'product_id\': [301, 301, 301, 302, 302, 302], \'product_name\': [\'Widget\', \'Widget\', \'Widget\', \'Gadget\', \'Gadget\', \'Gadget\'], \'quantity_sold\': [100, 150, 200, 80, 90, 85], \'price_per_unit\': [10.0, 10.0, 10.0, 20.0, 20.0, 20.0], \'total_sales\': [1000.0, 1500.0, 2000.0, 1600.0, 1800.0, 1700.0] } df = pd.DataFrame(data) result_df = process_sales_data(df) print(result_df) ``` Your implementation should produce a DataFrame that accurately reflects the processed data as specified.","solution":"import pandas as pd import numpy as np def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Set MultiIndex using store_id and date df.set_index([\'store_id\', \'date\'], inplace=True) # Aggregate data aggregated_df = df.groupby(level=[\'store_id\', \'date\']).agg( total_quantity_sold=(\'quantity_sold\', \'sum\'), total_sales=(\'total_sales\', \'sum\'), average_price_per_unit=(\'price_per_unit\', \'mean\') ) # Fill missing values with the mean of the column aggregated_df.fillna(aggregated_df.mean(), inplace=True) # Calculate month-over-month sales growth rate aggregated_df[\'sales_growth_rate\'] = aggregated_df.groupby(level=\'store_id\')[\'total_sales\'].pct_change() # Ensure DataFrame is sorted by store_id and date aggregated_df.sort_index(inplace=True) return aggregated_df"},{"question":"**Objective:** You are tasked with creating and parsing property list (\\".plist\\") files using Python\'s `plistlib` module. You need to demonstrate the ability to handle both XML and Binary formats and to properly manage any exceptions that may arise during these operations. **Problem Statement:** Write a Python function called `process_plist(data: dict, file_path: str, fmt: str) -> dict` that performs the following tasks: 1. **Writes** the given dictionary `data` into a property list file at the location specified by `file_path` in the format specified by `fmt`. 2. **Reads** the newly created property list file and returns the dictionary content. 3. If the format or the data type is not supported, it should raise an appropriate exception. # Function Signature ```python def process_plist(data: dict, file_path: str, fmt: str) -> dict: pass ``` # Input - `data`: A dictionary containing keys and values of varying types including strings, integers, floats, booleans, lists, and other dictionaries. - `file_path`: A string representing the file path where the property list will be saved. - `fmt`: A string indicating the format of the property list. It can either be `\'xml\'` or `\'binary\'`. # Output - The function returns a dictionary that represents the content read from the newly created property list file. # Constraints - Only string keys are allowed in the dictionary. - The function should raise `TypeError` if any key in the dictionary is not a string. - The function should raise `ValueError` if the format specified is not either `xml` or `binary`. # Examples ```python import datetime data = { \'name\': \'Alice\', \'age\': 30, \'balance\': 100.50, \'is_active\': True, \'address\': { \'street\': \'123 Apple St\', \'city\': \'Wonderland\' }, \'joined_date\': datetime.datetime(2020, 5, 17), \'phone_numbers\': [\'123-456-7890\', \'987-654-3210\'] } # Example usage: try: # Process the example data, save it to \'example.plist\' in XML format result = process_plist(data, \'example.plist\', \'xml\') print(result) except Exception as e: print(f\'Error: {e}\') ``` # Notes - Ensure to handle potential exceptions that may arise during the writing and reading of the plist files. - Utilize the `plistlib` module effectively to achieve the desired functionality.","solution":"import plistlib from typing import Dict def process_plist(data: Dict, file_path: str, fmt: str) -> Dict: Processes property list data: writes it to a file and then reads it back. :param data: A dictionary containing keys and values of varying types. :param file_path: A string representing the file path where the property list will be saved. :param fmt: A string indicating the format of the property list. It can either be \'xml\' or \'binary\'. :return: The dictionary content read from the newly created property list file. :raises TypeError: If any key in the data dictionary is not a string. :raises ValueError: If the format specified is not either \'xml\' or \'binary\'. # Validate all keys in the dictionary are strings for key in data: if not isinstance(key, str): raise TypeError(f\\"All keys must be strings. Found key of type {type(key)}\\") # Verify the specified format if fmt not in (\'xml\', \'binary\'): raise ValueError(\\"Format must be either \'xml\' or \'binary\'\\") # Determine the plist format plist_format = plistlib.FMT_XML if fmt == \'xml\' else plistlib.FMT_BINARY # Write the data to a plist file with open(file_path, \'wb\') as f: plistlib.dump(data, f, fmt=plist_format) # Read the data back from the plist file with open(file_path, \'rb\') as f: loaded_data = plistlib.load(f) return loaded_data"},{"question":"**Question: Configuration File Processing with Python\'s `configparser` Module** You are provided with a configuration file named `settings.ini` containing the following content: ``` [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [bitbucket.org] User = hg [topsecret.server.com] Port = 50022 ForwardX11 = no ``` Write a Python function named `parse_config(file_path)` that: 1. Loads the configuration from the given file. 2. Retrieves and returns the following information as a dictionary: - The value of `Compression` in the `DEFAULT` section. - The `User` value in the `bitbucket.org` section. - The `Port` value in the `topsecret.server.com` section. If it is not defined, return the default value `22`. # Function Signature: ```python def parse_config(file_path: str) -> dict: pass ``` # Input: - `file_path` (str): The file path to the `settings.ini` configuration file. # Output: - Returns a dictionary with the following keys and their corresponding values: - `\\"Compression\\"`: the value of `Compression` in the `DEFAULT` section. - `\\"User\\"`: the `User` value in the `bitbucket.org` section. - `\\"Port\\"`: the `Port` value in the `topsecret.server.com` section, or `22` if not defined. # Constraints: - You may assume the file at `file_path` exists and is correctly formatted as shown above. # Example: ```python parse_config(\'settings.ini\') ``` Expected output: ```python { \\"Compression\\": \\"yes\\", \\"User\\": \\"hg\\", \\"Port\\": \\"50022\\" } ``` # Notes: - Pay attention to handling sections and default values correctly, as per the requirements.","solution":"import configparser def parse_config(file_path: str) -> dict: Parses the given configuration file and retrieves specified values. Args: file_path (str): The path to the configuration file. Returns: dict: A dictionary containing specified values from the configuration file. config = configparser.ConfigParser() config.read(file_path) result = { \\"Compression\\": config.get(\'DEFAULT\', \'Compression\'), \\"User\\": config.get(\'bitbucket.org\', \'User\'), \\"Port\\": config.get(\'topsecret.server.com\', \'Port\', fallback=\'22\') } return result"},{"question":"Objective: This question aims to evaluate your understanding of creating and customizing color palettes in Seaborn and applying them to a data visualization. Problem Statement: You are given a dataset containing information about different species of flowers. Your task is to: 1. Load and preprocess the dataset. 2. Create a customized light color palette using Seaborn. 3. Apply the customized palette to a Seaborn plot and visualize the data. Requirements: 1. **Load the Dataset:** - You are given the following dataset: ``` flower_species.csv ``` The dataset contains the following columns: - `Species`: Name of the flower species. - `SepalLength`: Length of the sepal. - `SepalWidth`: Width of the sepal. - `PetalLength`: Length of the petal. - `PetalWidth`: Width of the petal. 2. **Create a Custom Light Color Palette:** - Create a light color palette using Seaborn\'s `sns.light_palette` function. - The color palette should transition from light gray to a specified color of your choice (use a hex code). - The palette should consist of 10 discrete colors. 3. **Plot the Data:** - Create a scatter plot using Seaborn\'s `sns.scatterplot` function. - The x-axis should represent `SepalLength` and the y-axis should represent `SepalWidth`. - Use the `Species` column to color the data points using the custom light color palette you created. Constraints: - The dataset contains no missing values; you do not need to perform any data cleaning. - You must use the color palette created in step 2 to color the data points in the scatter plot. Input Format: - The input is a CSV file named `flower_species.csv` with columns `Species`, `SepalLength`, `SepalWidth`, `PetalLength`, and `PetalWidth`. Output Format: - A scatter plot displaying the relationship between `SepalLength` and `SepalWidth`, colored by `Species` using the custom light color palette. Example: Given the CSV file content: ``` Species,SepalLength,SepalWidth,PetalLength,PetalWidth Iris-setosa,5.1,3.5,1.4,0.2 Iris-versicolor,7.0,3.2,4.7,1.4 Iris-virginica,6.3,3.3,6.0,2.5 ... ``` The output should be a scatter plot with `SepalLength` on the x-axis, `SepalWidth` on the y-axis, and points colored by `Species` using your custom light color palette. Function Signature: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flower_data(csv_file: str, hex_color: str): # Load the data df = pd.read_csv(csv_file) # Generate the custom light color palette palette = sns.light_palette(hex_color, 10) # Create the scatter plot sns.scatterplot(data=df, x=\\"SepalLength\\", y=\\"SepalWidth\\", hue=\\"Species\\", palette=palette) plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flower_data(csv_file: str, hex_color: str): Loads the flower species data, creates a custom light color palette, and generates a scatter plot visualizing SepalLength vs SepalWidth colored by Species. Parameters: csv_file (str): The path to the CSV file containing the data. hex_color (str): The hex code of the base color for the light color palette. # Load the data df = pd.read_csv(csv_file) # Generate the custom light color palette palette = sns.light_palette(hex_color, 10) # Create the scatter plot sns.scatterplot(data=df, x=\\"SepalLength\\", y=\\"SepalWidth\\", hue=\\"Species\\", palette=palette) plt.show()"},{"question":"**Objective:** To demonstrate your understanding of the `sched` module in Python, your task is to implement a custom task scheduler. This scheduler should handle various time-critical tasks and should be able to manage the execution order and priorities effectively. **Task Description:** 1. **Initialize a Scheduler:** - Use `sched.scheduler()` to initialize a scheduler with appropriate `timefunc` and `delayfunc`. 2. **Implement a function `schedule_tasks` that:** - Takes a list of tasks where each task is a dictionary with keys `time`, `priority`, `action`, `args`, and `kwargs`. - Schedules each task using either `enter` or `enterabs`, depending on the task\'s time value (absolute or relative time). 3. **Implement a function `execute_all_tasks` that:** - Runs all the scheduled tasks using the scheduler\'s `run` method. 4. **Implement a function `cancel_task` that:** - Cancels a specific task that was previously scheduled. 5. **Example usage:** Hereâ€™s an example of tasks that should be handled by your scheduler: ```python tasks = [ {\'time\': 5, \'priority\': 1, \'action\': print, \'args\': (\'Task 1\',), \'kwargs\': {}}, {\'time\': 10, \'priority\': 2, \'action\': print, \'args\': (\'Task 2\',), \'kwargs\': {}}, {\'time\': 15, \'priority\': 3, \'action\': print, \'args\': (\'Task 3\',), \'kwargs\': {}} ] ``` **Constraints:** - Assume the time is given in seconds. - Use the `time.monotonic` function for `timefunc`. - Use the `time.sleep` function for `delayfunc`. **Performance Requirements:** - Ensure the scheduler handles tasks with overlapping times appropriately and adheres to the priority rules. **Input/Output Format:** - `schedule_tasks`: - **Input:** List of task dictionaries. - **Output:** None (tasks should be scheduled internally). - `execute_all_tasks`: - **Input:** None. - **Output:** None (tasks should be executed internally). - `cancel_task`: - **Input:** Task dictionary to be canceled. - **Output:** None (specified task should be canceled). **Complete Implementation:** Write a complete implementation that efficiently manages and executes the scheduler as described. ```python import sched import time class CustomScheduler: def __init__(self): self.scheduler = sched.scheduler(time.monotonic, time.sleep) self.events = {} # To keep track of events for easy cancellation def schedule_tasks(self, tasks): for task in tasks: if task[\'time\'] > time.monotonic(): event = self.scheduler.enterabs(task[\'time\'], task[\'priority\'], task[\'action\'], task[\'args\'], task[\'kwargs\']) else: event = self.scheduler.enter(task[\'time\'], task[\'priority\'], task[\'action\'], task[\'args\'], task[\'kwargs\']) self.events[task[\'args\']] = event def execute_all_tasks(self): self.scheduler.run() def cancel_task(self, task): if task[\'args\'] in self.events: self.scheduler.cancel(self.events.pop(task[\'args\'])) # Example Usage if __name__ == \\"__main__\\": scheduler = CustomScheduler() tasks = [ {\'time\': time.monotonic() + 5, \'priority\': 1, \'action\': print, \'args\': (\'Task 1\',), \'kwargs\': {}}, {\'time\': time.monotonic() + 10, \'priority\': 2, \'action\': print, \'args\': (\'Task 2\',), \'kwargs\': {}}, {\'time\': time.monotonic() + 15, \'priority\': 3, \'action\': print, \'args\': (\'Task 3\',), \'kwargs\': {}} ] scheduler.schedule_tasks(tasks) # To cancel a task # scheduler.cancel_task(tasks[1]) scheduler.execute_all_tasks() ``` **Note:** - Ensure to test the implementation with various scenarios, including cancelling events and scheduling with different priority levels.","solution":"import sched import time class CustomScheduler: def __init__(self): self.scheduler = sched.scheduler(time.monotonic, time.sleep) self.events = {} def schedule_tasks(self, tasks): for task in tasks: if \'time\' in task: event = self.scheduler.enterabs(task[\'time\'], task[\'priority\'], task[\'action\'], task[\'args\'], task[\'kwargs\']) else: event = self.scheduler.enter(task[\'delay\'], task[\'priority\'], task[\'action\'], task[\'args\'], task[\'kwargs\']) self.events[task[\'args\']] = event def execute_all_tasks(self): self.scheduler.run() def cancel_task(self, task): if task[\'args\'] in self.events: self.scheduler.cancel(self.events.pop(task[\'args\']))"},{"question":"# PyTorch CPU Device and Stream Management This question assesses your understanding of CPU device and stream management using the PyTorch `torch.cpu` module. Task Implement a function `cpu_device_info` that: 1. Checks if the CPU is available. 2. Retrieves and returns the current CPU device. 3. Synchronizes the CPU and all streams. 4. Returns a dictionary with the following information: - `cpu_available`: Boolean indicating if the CPU is available. - `current_device`: The current CPU device. Additionally, you need to implement a second function `manage_stream` that: 1. Creates a new CPU stream. 2. Sets this stream as the current stream. 3. Returns the details of the new stream in a dictionary which includes: - `stream_id`: The identifier of the stream. - `in_context`: Whether the stream is currently set as the active context. Input/Output Format # `cpu_device_info` - **Input:** No input required. - **Output:** A dictionary with the following structure: ```python { \\"cpu_available\\": bool, \\"current_device\\": Device } ``` # `manage_stream` - **Input:** No input required. - **Output:** A dictionary with the following structure: ```python { \\"stream_id\\": Stream, \\"in_context\\": bool } ``` Constraints - Use the functionalities from `torch.cpu` module. - Ensure synchronization is done properly before returning any information. - The stream should be created and set as the current stream correctly. Example ```python def cpu_device_info(): pass def manage_stream(): pass # Example Usage cpu_info = cpu_device_info() print(cpu_info) # Output might be: # { # \\"cpu_available\\": True, # \\"current_device\\": <torch.cpu.Device object at 0x...> # } stream_info = manage_stream() print(stream_info) # Output might be: # { # \\"stream_id\\": <torch.cpu.Stream object at 0x...>, # \\"in_context\\": True # } ``` Implement the `cpu_device_info` and `manage_stream` functions according to the specifications given.","solution":"import torch def cpu_device_info(): Returns information about the CPU device. # Checking if the CPU is available cpu_available = torch.device(\\"cpu\\") is not None # Retrieving the current CPU device current_device = torch.device(\\"cpu\\") # Synchronizing the CPU device and all streams torch.cuda.synchronize() return { \\"cpu_available\\": cpu_available, \\"current_device\\": current_device } def manage_stream(): Creates a new CPU stream and sets it as the current stream. # Creating a new CPU stream new_stream = torch.cpu.Stream() # Setting this stream as the current stream with torch.cpu.stream(new_stream): in_context = torch.cpu.current_stream() == new_stream return { \\"stream_id\\": new_stream, \\"in_context\\": in_context }"},{"question":"# Memory Management and Profiling in PyTorch Using MPS Context You are working on optimizing a machine learning model that runs on Apple\'s Metal Performance Shaders (MPS) using PyTorch. To better understand and optimize memory usage and performance, you need to manage memory allocation and profile the model execution. Task 1. **Memory Management**: - Create a function `initialize_mps_memory` that sets the per-process memory fraction for the MPS backend to 0.5. - This function should also print the current allocated memory and the driver allocated memory. 2. **Random Number Initialization**: - Create a function `initialize_random_seed` that sets a manual random seed (e.g., `42`) for reproducibility when using MPS. 3. **Profiling**: - Create a function `profile_model_execution` that: - Starts MPS profiling. - Executes a given function (simulating a model\'s forward pass). - Stops MPS profiling. - Returns a boolean indicating whether Metal capture is enabled during the profile. 4. **Synchronization**: - After profiling, ensure synchronization to make sure all operations have finished. Implementation Please implement the required functions according to the specifications above. You may assume that the MPS device is available and initialized properly. # Example Usage ```python def dummy_model(): # Simulate a model\'s forward pass pass initialize_mps_memory() initialize_random_seed() profiler_enabled = profile_model_execution(dummy_model) print(f\\"Is Metal profiling capture enabled? {profiler_enabled}\\") ``` # Constraints and Notes - Use PyTorch\'s `torch.mps` module and its functions. - Ensure your code is compatible with PyTorch\'s MPS backend. - You can import additional necessary modules from PyTorch as needed. - The execution of the model function is simulated in `dummy_model` for demonstration purposes. # Expected Output ```plaintext Current allocated memory: <some_value> bytes Driver allocated memory: <some_value> bytes Is Metal profiling capture enabled? True/False ```","solution":"import torch def initialize_mps_memory(): Sets the per-process memory fraction for the MPS backend to 0.5. Prints the current allocated memory and the driver allocated memory. try: torch.mps.set_per_process_memory_fraction(0.5, device=torch.device(\'mps\')) allocated_mem = torch.mps.memory_allocated() driver_allocated_mem = torch.mps.memory_reserved() print(f\\"Current allocated memory: {allocated_mem} bytes\\") print(f\\"Driver allocated memory: {driver_allocated_mem} bytes\\") except Exception as e: print(f\\"Error in initializing MPS memory: {e}\\") def initialize_random_seed(seed=42): Sets a manual random seed for reproducibility when using MPS. try: torch.manual_seed(seed) if torch.backends.mps.is_available(): torch.mps.manual_seed(seed) print(f\\"Random seed set to: {seed}\\") except Exception as e: print(f\\"Error in setting random seed: {e}\\") def profile_model_execution(model_func): Profiles the model execution. Starts MPS profiling, executes the given function, stops MPS profiling. Returns a boolean indicating whether Metal capture is enabled during the profile. try: if not torch.backends.mps.is_available(): print(\\"MPS backend not available.\\") return False torch.mps.profiler_start() model_func() torch.mps.profiler_stop() torch.mps.synchronize() is_capture_enabled = torch.mps.metal_capture_is_enabled() return is_capture_enabled except Exception as e: print(f\\"Error in profiling model execution: {e}\\") return False"},{"question":"**Objective:** Demonstrate comprehension of PyTorch tensor manipulation, model building, and TorchScript compilation. You are required to implement a simple neural network model using PyTorch and then convert it to a TorchScript model. # Part 1: Neural Network Implementation 1. Implement a simple feedforward neural network with one hidden layer using PyTorch. 2. The network should accept an input of size 4 and output a single value. 3. Use ReLU activation function for the hidden layer and a sigmoid activation function for the output layer. # Part 2: TorchScript Conversion 1. Convert the PyTorch neural network model to a TorchScript model. 2. Write a function that exports this model to a specified file path. # Expected Input and Output Formats 1. **Neural Network Initialization** ```python class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() # Your code here def forward(self, x): # Your code here ``` 2. **TorchScript Conversion Function** ```python def convert_to_torchscript(model, file_path): Converts the given PyTorch model to TorchScript and saves it to the specified file path. Parameters: model (nn.Module): The PyTorch model to be converted file_path (str): The file path where the TorchScript model will be saved Returns: None # Your code here ``` # Constraints and Limitations - You should use the latest version of PyTorch. - Ensure that your TorchScript model can be loaded and run without issues. # Performance Requirements - The model conversion function should handle the conversion process efficiently. - The exported model file should be checked for correctness by loading and running a sample tensor through it. **Example Usage:** ```python # Instantiate the neural network model = SimpleNN() # Convert the model to TorchScript and save it convert_to_torchscript(model, \'simple_nn.pt\') # Load the TorchScript model and perform a sample inference scripted_model = torch.jit.load(\'simple_nn.pt\') sample_input = torch.randn(1, 4) output = scripted_model(sample_input) print(output) ``` # Notes - Make sure to handle any exceptions that might occur during the conversion process. - You can use torch.jit.trace or torch.jit.script for model conversion, depending on your implementation requirements.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(4, 10) self.output = nn.Linear(10, 1) self.relu = nn.ReLU() self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.relu(self.hidden(x)) x = self.sigmoid(self.output(x)) return x def convert_to_torchscript(model, file_path): Converts the given PyTorch model to TorchScript and saves it to the specified file path. Parameters: model (nn.Module): The PyTorch model to be converted file_path (str): The file path where the TorchScript model will be saved Returns: None try: scripted_model = torch.jit.script(model) scripted_model.save(file_path) except Exception as e: print(f\\"An error occurred during the TorchScript conversion: {e}\\")"},{"question":"You are provided with a dataset containing features and target values. Use scikit-learnâ€™s linear model modules to build a comprehensive regression analysis. Write a Python script that accomplishes the following tasks: 1. Load the dataset from a CSV file. 2. Preprocess the data by handling missing values and scaling the features. 3. Implement and fit the following regression models: - `LinearRegression` - `Ridge` (with cross-validation to find the best regularization parameter) - `Lasso` (with cross-validation to find the best alpha parameter) - `ElasticNet` (with cross-validation to find the best alpha and l1_ratio parameters) - `BayesianRidge` 4. For each model: - Fit the model to the training data. - Evaluate and display the modelâ€™s performance using Mean Squared Error (MSE) on the test data. 5. Compare the results and determine which model performs the best on the dataset. Input Format: - A CSV file named `data.csv` containing the dataset with features and target values. Output Format: - The Mean Squared Error (MSE) for each model. - The best performing model on the test data. Constraints: - Ensure all necessary imports from scikit-learn. - Assume the target column in the dataset is named `target`. Here is a starting template to help you get started: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import (LinearRegression, RidgeCV, LassoCV, ElasticNetCV, BayesianRidge) from sklearn.metrics import mean_squared_error # 1. Load the dataset data = pd.read_csv(\\"data.csv\\") X = data.drop(columns=[\\"target\\"]) y = data[\\"target\\"] # 2. Preprocess the data # Handle missing values X.fillna(X.mean(), inplace=True) # Scale the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # 3. Implement and fit the models models = { \\"LinearRegression\\": LinearRegression(), \\"Ridge\\": RidgeCV(alphas=[0.1, 1.0, 10.0]), \\"Lasso\\": LassoCV(alphas=[0.1, 0.5, 1.0]), \\"ElasticNet\\": ElasticNetCV(alphas=[0.1, 0.5, 1.0], l1_ratio=[0.1, 0.5, 0.9]), \\"BayesianRidge\\": BayesianRidge() } # 4. Fit the models and evaluate their performance mse_results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) mse_results[name] = mse # Display the MSE results for comparison for model_name, mse in mse_results.items(): print(f\\"{model_name}: MSE = {mse}\\") # 5. Determine the best performing model best_model = min(mse_results, key=mse_results.get) print(f\\"The best performing model is {best_model} with MSE = {mse_results[best_model]}\\") ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, BayesianRidge from sklearn.metrics import mean_squared_error def regression_analysis(csv_path): # 1. Load the dataset data = pd.read_csv(csv_path) X = data.drop(columns=[\\"target\\"]) y = data[\\"target\\"] # 2. Preprocess the data # Handle missing values X.fillna(X.mean(), inplace=True) # Scale the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # 3. Implement and fit the models models = { \\"LinearRegression\\": LinearRegression(), \\"Ridge\\": RidgeCV(alphas=[0.1, 1.0, 10.0]), \\"Lasso\\": LassoCV(alphas=[0.1, 0.5, 1.0]), \\"ElasticNet\\": ElasticNetCV(alphas=[0.1, 0.5, 1.0], l1_ratio=[0.1, 0.5, 0.9]), \\"BayesianRidge\\": BayesianRidge() } # 4. Fit the models and evaluate their performance mse_results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) mse_results[name] = mse # Display the MSE results for comparison for model_name, mse in mse_results.items(): print(f\\"{model_name}: MSE = {mse}\\") # 5. Determine the best performing model best_model = min(mse_results, key=mse_results.get) print(f\\"The best performing model is {best_model} with MSE = {mse_results[best_model]}\\") return mse_results, best_model"},{"question":"# Clustering of a Dataset Using scikit-learn You are provided with a dataset, `data.csv`, containing data points with features in columns. Your task is to: 1. Implement a function to read the data from `data.csv`. 2. Implement a function to perform clustering using the K-Means algorithm. 3. Implement another function to perform clustering using the DBSCAN algorithm. 4. Evaluate and compare the performance of these two clustering methods using appropriate metrics. Function Definitions 1. **read_data(file_path)** - **Input**: `file_path` (str) - The path to the CSV file containing the dataset. - **Output**: A tuple (X, y) where X is a numpy array of shape `(n_samples, n_features)` containing the features, and y is a numpy array containing the labels (if available, otherwise `None`). 2. **kmeans_clustering(X, n_clusters)** - **Input**: - `X` (numpy array) - An array of shape `(n_samples, n_features)` containing the features. - `n_clusters` (int) - The number of clusters to form. - **Output**: A numpy array of shape `(n_samples,)` containing the cluster labels for each point. 3. **dbscan_clustering(X, eps, min_samples)** - **Input**: - `X` (numpy array) - An array of shape `(n_samples, n_features)` containing the features. - `eps` (float) - The maximum distance between two samples for one to be considered as in the neighborhood of the other. - `min_samples` (int) - The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. - **Output**: A numpy array of shape `(n_samples,)` containing the cluster labels for each point. 4. **evaluate_clustering(X, labels)** - **Input**: - `X` (numpy array) - An array of shape `(n_samples, n_features)` containing the features. - `labels` (numpy array) - A numpy array of shape `(n_samples,)` containing the cluster labels for each point. - **Output**: A dictionary containing various clustering performance metrics such as silhouette score, Davies-Bouldin index, and Calinski-Harabasz index. Evaluation Criteria - Correctness and completeness of function implementations. - Proper use of scikit-learn\'s clustering and evaluation modules. - Clear and concise code with appropriate comments. ```python import numpy as np import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score def read_data(file_path): Read the data from a CSV file. # Implement this function pass def kmeans_clustering(X, n_clusters): Perform K-means clustering. # Implement this function pass def dbscan_clustering(X, eps, min_samples): Perform DBSCAN clustering. # Implement this function pass def evaluate_clustering(X, labels): Evaluate clustering performance. # Implement this function pass # Example usage X, y = read_data(\'data.csv\') kmeans_labels = kmeans_clustering(X, 3) dbscan_labels = dbscan_clustering(X, 0.5, 5) kmeans_metrics = evaluate_clustering(X, kmeans_labels) dbscan_metrics = evaluate_clustering(X, dbscan_labels) print(\\"K-Means Metrics:\\", kmeans_metrics) print(\\"DBSCAN Metrics:\\", dbscan_metrics) ``` **Note:** Ensure `data.csv` is in the current directory or provide the correct path to the file.","solution":"import numpy as np import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score def read_data(file_path): Read the data from a CSV file. data = pd.read_csv(file_path) if \'label\' in data.columns: X = data.drop(columns=[\'label\']).values y = data[\'label\'].values else: X = data.values y = None return X, y def kmeans_clustering(X, n_clusters): Perform K-means clustering. kmeans = KMeans(n_clusters=n_clusters, random_state=0) kmeans.fit(X) return kmeans.labels_ def dbscan_clustering(X, eps, min_samples): Perform DBSCAN clustering. dbscan = DBSCAN(eps=eps, min_samples=min_samples) dbscan.fit(X) return dbscan.labels_ def evaluate_clustering(X, labels): Evaluate clustering performance. metrics = {} if len(set(labels)) > 1: # More than one cluster metrics[\'silhouette_score\'] = silhouette_score(X, labels) metrics[\'davies_bouldin_score\'] = davies_bouldin_score(X, labels) metrics[\'calinski_harabasz_score\'] = calinski_harabasz_score(X, labels) else: metrics[\'silhouette_score\'] = None metrics[\'davies_bouldin_score\'] = None metrics[\'calinski_harabasz_score\'] = None return metrics"},{"question":"# Question: Implementing Custom Signal Handlers and Timer You are required to write a Python program that performs the following tasks: 1. **Set up a signal handler** for the `SIGALRM` signal. This handler should print \\"Received SIGALRM. Exiting the program!\\" and then terminate the program using an appropriate function/method. 2. **Schedule a timer** to send a `SIGALRM` signal after 10 seconds. 3. **Simulate a long-running task** by having your program calculate the sum of the first 1,000,000,000 integers. 4. The program should catch the signal, trigger the handler, and terminate before the sum calculation completes, using the scheduled alarm. # Implementation Details 1. Define a signal handler function `handle_sigalrm(signum, frame)` that is called when the `SIGALRM` signal is received. This function should: - Print \\"Received SIGALRM. Exiting the program!\\" - Terminate the program gracefully. 2. Use the `signal.signal()` function to set the `handle_sigalrm` as the handler for `SIGALRM`. 3. Use the `signal.alarm()` function to schedule an alarm for 10 seconds. 4. Implement a long-running task that simulates heavy computation by calculating the sum of numbers from 1 to 1,000,000,000. 5. Ensure the long-running task should be interrupted by `SIGALRM` before it completes, demonstrating the correct setup and handling of the signal. # Function Signature ```python import signal def handle_sigalrm(signum: int, frame): pass def setup_signal_handler(): pass def schedule_alarm(seconds: int): pass def long_running_task(): pass def main(): setup_signal_handler() schedule_alarm(10) long_running_task() if __name__ == \\"__main__\\": main() ``` # Constraints - The alarm should be set only once, and the program should terminate on receiving the `SIGALRM` signal. - The handler should print the specified message and exit the program. - The specified `long_running_task()` should demonstrate that a long computation can be interrupted by signals. # Example Output When you run the program, you should see: ``` Received SIGALRM. Exiting the program! ``` (indicating that the signal handler successfully caught the `SIGALRM` signal and terminated the program).","solution":"import signal import sys def handle_sigalrm(signum, frame): Signal handler for SIGALRM. Prints a message and exits the program. print(\\"Received SIGALRM. Exiting the program!\\") sys.exit(0) def setup_signal_handler(): Sets up the signal handler for SIGALRM. signal.signal(signal.SIGALRM, handle_sigalrm) def schedule_alarm(seconds): Schedules an alarm to send SIGALRM after a given number of seconds. signal.alarm(seconds) def long_running_task(): Simulates a long-running task by calculating the sum of the first 1,000,000,000 integers. This task is expected to be interrupted by the SIGALRM signal. total = 0 for i in range(1, 1000000001): total += i return total def main(): Main function to set up the signal handler, schedule the alarm, and start the long-running task. setup_signal_handler() schedule_alarm(10) long_running_task() if __name__ == \\"__main__\\": main()"},{"question":"# Python Development Mode and Resource Management You are provided with a script that reads and processes a text file specified as a command-line argument. The current implementation, however, has some resource management issues that are only detected when Python Development Mode is enabled. The existing script is as follows: ```python import sys def process_file(): fp = open(sys.argv[1]) lines = fp.readlines() process_lines(lines) print(\\"Processing complete\\") def process_lines(lines): # Placeholder for line processing code pass if __name__ == \\"__main__\\": process_file() ``` # Task 1. **Identify Resource Management Issues**: Run the script in Python Development Mode to identify any resource management issues. ```bash python3 -X dev script.py <your_text_file.txt> ``` 2. **Fix the Issues**: Modify the script to properly manage resources and eliminate any warnings. # Requirements: - Ensure that all resources are explicitly closed to avoid any `ResourceWarning`. - Use best practices for opening and closing files to ensure the script runs without warnings in both normal and development modes. - Your solution should not generate any warnings when run in Python Development Mode. # Constraints: - You may only modify the `process_file` and `process_lines` functions. - The script should be able to handle large files efficiently. The modified script should be self-contained and retain the same overall structure. Hereâ€™s a guideline on how your updated code should look: ```python import sys def process_file(): # Your modified code here def process_lines(lines): # Placeholder for line processing code pass if __name__ == \\"__main__\\": process_file() ``` # Example: Before Fix: ```bash python3 -X dev script.py README.txt 269 script.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name=\'README.txt\' mode=\'r\' encoding=\'UTF-8\'> process_file() ResourceWarning: Enable tracemalloc to get the object allocation traceback ``` After Fix: ```bash python3 -X dev script.py README.txt 269 ``` Submit your modified script as the solution for this question.","solution":"import sys def process_file(): # Open the file using a with statement to ensure it is closed properly with open(sys.argv[1]) as fp: lines = fp.readlines() process_lines(lines) print(\\"Processing complete\\") def process_lines(lines): # Placeholder for line processing code pass if __name__ == \\"__main__\\": process_file()"},{"question":"**Question:** Write a Python program that demonstrates the use of the `marshal` module to serialize and deserialize Python objects. The program should meet the following requirements: 1. Define a function `serialize_objects` that takes in a list of Python objects and a file path and writes the objects to the file using the `marshal.dump` function. Ensure the file is opened in binary write mode. 2. Define a function `deserialize_objects` that takes in a file path, reads the objects using the `marshal.load` function, and returns them in a list. Ensure the file is opened in binary read mode. 3. Handle cases where unsupported types are included in the object list. If an unsupported type is encountered, print a warning message and skip serialization of that particular object. 4. Handle any exceptions that may arise during serialization and deserialization, such as I/O errors or marshal errors. **Expected input and output format:** - The `serialize_objects` function takes two arguments: - `objects` (a list of Python objects to be serialized) - `file_path` (a string representing the path to the file where the objects will be saved) - The `deserialize_objects` function takes one argument: - `file_path` (a string representing the path to the file from which the objects will be read) - The function `serialize_objects` should not return any value but should perform the serialization operation. - The function `deserialize_objects` should return a list of deserialized objects. **Example Usage:** ```python objects = [42, \'hello\', [1, 2, 3], {\'key\': \'value\'}, lambda x: x + 1] # Serialize objects to a file serialize_objects(objects, \'objects.marsh\') # Deserialize objects from the file deserialized_objects = deserialize_objects(\'objects.marsh\') print(deserialized_objects) ``` **Constraints:** - Ensure the file is opened in the correct binary mode during both serialization and deserialization. - Skip unsupported types and notify the user in a manner appropriate for the scenario. **Performance requirements:** - The solution should efficiently handle serialization and deserialization of large lists of supported object types. - Properly handle edge cases including empty lists, and unsupported object types. **Solution:** ```python import marshal def serialize_objects(objects, file_path): with open(file_path, \'wb\') as f: for obj in objects: try: marshal.dump(obj, f) except ValueError as e: print(f\\"Warning: Skipping unsupported type {type(obj)} - {e}\\") def deserialize_objects(file_path): deserialized = [] try: with open(file_path, \'rb\') as f: while True: try: deserialized.append(marshal.load(f)) except EOFError: break except (OSError, ValueError, TypeError) as e: print(f\\"Error reading file: {e}\\") return deserialized # Example Usage objects = [42, \'hello\', [1, 2, 3], {\'key\': \'value\'}, lambda x: x + 1] serialize_objects(objects, \'objects.marsh\') deserialized_objects = deserialize_objects(\'objects.marsh\') print(deserialized_objects) ```","solution":"import marshal def serialize_objects(objects, file_path): Serializes a list of Python objects to a file using the marshal module. Unsupported types are skipped with a warning. :param objects: List of Python objects to serialize. :param file_path: Path to the file where the objects will be saved. with open(file_path, \'wb\') as f: for obj in objects: try: marshal.dump(obj, f) except (ValueError, TypeError) as e: print(f\\"Warning: Skipping unsupported type {type(obj)} - {e}\\") def deserialize_objects(file_path): Deserializes Python objects from a file using the marshal module. :param file_path: Path to the file from which the objects will be read. :return: List of deserialized Python objects. deserialized = [] try: with open(file_path, \'rb\') as f: while True: try: deserialized.append(marshal.load(f)) except EOFError: break except (OSError, ValueError, TypeError) as e: print(f\\"Error reading file: {e}\\") return deserialized"},{"question":"Objective: Using the seaborn library, you are required to create a detailed residual plot analysis on a provided dataset to demonstrate your understanding of regression diagnostics and data visualization. Question: 1. Load the \\"mpg\\" dataset using seaborn. 2. Create a residual plot showing the relationship between \\"weight\\" and \\"displacement\\". 3. Create a residual plot showing the relationship between \\"horsepower\\" and \\"mpg\\" and identify any non-linear trends by adjusting the order of the polynomial fit to 2. 4. Overlay a LOWESS curve on the residual plot from step 3 to highlight any structures in the residuals. Specific Requirements: - The dataset should be loaded using `sns.load_dataset`. - Each plot should be well-labeled, including titles, x-axis, and y-axis labels. - All plots should be presented in a single figure with an appropriate layout to compare the results easily. - Use comments in your code to explain each step. Expected Output: The final output should be a single figure containing the three described residual plots. The plots should provide insights into the fit of the regression models and highlight any violations of linear regression assumptions. Constraints: - Ensure that all necessary libraries are imported. - Use clear and descriptive variable names. - The code should be efficient and well-structured. Performance Requirements: - Your solution should be able to generate the plots within a reasonable time frame (less than 10 seconds for all plots). - Ensure that your plots are visually appealing and properly formatted. Input and Output Formats: - **Input**: No explicit input is required other than the loading of the \\"mpg\\" dataset within the code. - **Output**: Display of the single figure containing the three residual plots. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the \\"mpg\\" dataset mpg = sns.load_dataset(\\"mpg\\") # Set the theme for seaborn sns.set_theme() # Step 2: Create residual plot for \\"weight\\" vs \\"displacement\\" plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Displacement Residuals\\") # Step 3: Create residual plot for \\"horsepower\\" vs \\"mpg\\" with polynomial fit of order 2 plt.subplot(1, 3, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot: Horsepower vs MPG (Order 2)\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG Residuals\\") # Step 4: Create residual plot for \\"horsepower\\" vs \\"mpg\\" with a LOWESS curve plt.subplot(1, 3, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot: Horsepower vs MPG with LOWESS\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG Residuals\\") # Display the plots plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): # Step 1: Load the \\"mpg\\" dataset mpg = sns.load_dataset(\\"mpg\\") # Set the theme for seaborn sns.set_theme() # Step 2: Create residual plot for \\"weight\\" vs \\"displacement\\" plt.figure(figsize=(18, 6)) plt.subplot(1, 3, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Displacement Residuals\\") # Step 3: Create residual plot for \\"horsepower\\" vs \\"mpg\\" with polynomial fit of order 2 plt.subplot(1, 3, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot: Horsepower vs MPG (Order 2)\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG Residuals\\") # Step 4: Create residual plot for \\"horsepower\\" vs \\"mpg\\" with a LOWESS curve plt.subplot(1, 3, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot: Horsepower vs MPG with LOWESS\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG Residuals\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"**Question:** You are tasked with visualizing a dataset using seaborn and customizing the plot\'s limits and appearance based on specific requirements. Your goal is to write a function `customize_plot` that will read a dataset and generate a plot with precise customizations. # Function Signature ```python def customize_plot(data: dict, x_col: str, y_col: str, x_limits: tuple, y_limits: tuple, invert_axes: bool) -> so.Plot: pass ``` # Inputs - `data`: A dictionary containing lists of values for each column. For example: `{\'x\': [1, 2, 3], \'y\': [1, 3, 2]}`. - `x_col`: The column name in the `data` dictionary to be used for the x-axis values. - `y_col`: The column name in the `data` dictionary to be used for the y-axis values. - `x_limits`: A tuple specifying the limits of the x-axis as `(min, max)`. Either `min` or `max` can be `None` to maintain default values. - `y_limits`: A tuple specifying the limits of the y-axis as `(min, max)`. Either `min` or `max` can be `None` to maintain default values. - `invert_axes`: A boolean value indicating whether to invert the axes. # Output - Return a seaborn plot object (`so.Plot`) customized according to the given inputs. # Constraints - You must use the `seaborn.objects` module for plotting. - The function should be robust and handle edge cases, such as non-existent columns or improper limit values. - If `invert_axes` is `True`, both the x and y axes should be inverted. # Example ```python data = {\'x\': [1, 2, 3], \'y\': [1, 3, 2]} x_col = \'x\' y_col = \'y\' x_limits = (0, 4) y_limits = (-1, 6) invert_axes = False plot = customize_plot(data, x_col, y_col, x_limits, y_limits, invert_axes) plot.show() ``` This example will generate a plot with x-axis limits from 0 to 4 and y-axis limits from -1 to 6, without inverting the axes. **Notes:** - You can assume the data dictionary will always contain numerical lists for the column values. - Ensure the function checks and handles the scenario where specified columns do not exist in the data.","solution":"import seaborn.objects as so import pandas as pd def customize_plot(data: dict, x_col: str, y_col: str, x_limits: tuple, y_limits: tuple, invert_axes: bool) -> so.Plot: Generates a customized seaborn plot based on the provided specifications. :param data: A dictionary containing lists of values for each column. :param x_col: The column name in the data dictionary to be used for the x-axis values. :param y_col: The column name in the data dictionary to be used for the y-axis values. :param x_limits: A tuple specifying the limits of the x-axis as (min, max). :param y_limits: A tuple specifying the limits of the y-axis as (min, max). :param invert_axes: A boolean value indicating whether to invert the axes. :return: A seaborn plot object customized according to the given inputs. # Convert the data dictionary to a pandas DataFrame df = pd.DataFrame(data) # Ensure specified columns exist in the DataFrame if x_col not in df.columns or y_col not in df.columns: raise ValueError(f\\"Specified columns {x_col} or {y_col} do not exist in the data\\") # Create the initial plot object p = so.Plot(df, x=x_col, y=y_col).add(so.Line()) # Set x and y axis limits if specified if x_limits[0] is not None or x_limits[1] is not None: p = p.limit(x=[x_limits[0], x_limits[1]]) if y_limits[0] is not None or y_limits[1] is not None: p = p.limit(y=[y_limits[0], y_limits[1]]) # Invert axes if required if invert_axes: p = p.scale(x=\'reverse\', y=\'reverse\') return p"},{"question":"**Question: Loading and Analyzing Real-World Datasets** In this coding assessment, you are required to demonstrate your ability to work with real-world datasets using the `scikit-learn` (sklearn) library. Your task is to implement a function that fetches a specified real-world dataset, extracts relevant information, and performs basic analysis. **Function Specifications**: - **Function Name**: `fetch_and_analyze_dataset` - **Input Parameters**: - `dataset_name` (str): The name of the dataset to fetch. For this task, you can use any of the following datasets: `\'california_housing\'`, `\'iris\'`, `\'diabetes\'`. - `return_X_y` (bool): Fetch the dataset as a tuple of `(data, target)` if True. - **Output**: - If `return_X_y` is `True`, return a tuple `(X, y)` where: - `X` (numpy.ndarray): The feature data array of shape `(n_samples, n_features)`. - `y` (numpy.ndarray): The target values array of shape `(n_samples,)`. - If `return_X_y` is `False`, return a dictionary with the following keys: - `data` (numpy.ndarray): The feature data array. - `target` (numpy.ndarray): The target values array. - `feature_names` (list): List of feature names. - `target_names` (list): List of target names. If the dataset does not provide target names, return an empty list. - `DESCR` (str): Description of the dataset. - `data_shape` (tuple): Shape of the data array. **Constraints**: - The function should handle invalid dataset names by raising a `ValueError` with the message `\\"Invalid dataset name provided.\\"`. - You are only allowed to use the `sklearn.datasets` module to load the datasets. **Example Usage**: ```python # Example 1: Using the return_X_y parameter X, y = fetch_and_analyze_dataset(\'iris\', return_X_y=True) print(X.shape) # Output: (150, 4) print(y.shape) # Output: (150,) # Example 2: Fetching the full dataset information dataset_info = fetch_and_analyze_dataset(\'diabetes\', return_X_y=False) print(dataset_info[\'data\'].shape) # Output: (442, 10) print(dataset_info[\'target\'].shape) # Output: (442,) print(dataset_info[\'feature_names\']) # Output: list of feature names print(dataset_info[\'target_names\']) # Output: [] print(dataset_info[\'DESCR\']) # Output: Description of the dataset print(dataset_info[\'data_shape\']) # Output: (442, 10) ``` **Note**: 1. Make sure to add appropriate docstrings to your function. 2. Ensure your code is robust and handles edge cases.","solution":"from sklearn.datasets import fetch_california_housing, load_iris, load_diabetes def fetch_and_analyze_dataset(dataset_name, return_X_y=False): Fetches a specified real-world dataset, extracts relevant information, and performs basic analysis. Parameters: - dataset_name (str): The name of the dataset to fetch. It can be \'california_housing\', \'iris\', or \'diabetes\'. - return_X_y (bool): Fetch the dataset as a tuple of (data, target) if True. Returns: - If return_X_y is True: tuple: (X, y) - X (numpy.ndarray): The feature data array of shape (n_samples, n_features). - y (numpy.ndarray): The target values array of shape (n_samples,). - If return_X_y is False: dict: containing dataset information with keys: - \'data\' (numpy.ndarray): The feature data array. - \'target\' (numpy.ndarray): The target values array. - \'feature_names\' (list): List of feature names. - \'target_names\' (list): List of target names or an empty list if not available. - \'DESCR\' (str): Description of the dataset. - \'data_shape\' (tuple): Shape of the data array. Raises: - ValueError: If an invalid dataset name is provided. if dataset_name not in [\'california_housing\', \'iris\', \'diabetes\']: raise ValueError(\\"Invalid dataset name provided.\\") if dataset_name == \'california_housing\': data = fetch_california_housing() elif dataset_name == \'iris\': data = load_iris() elif dataset_name == \'diabetes\': data = load_diabetes() if return_X_y: return data.data, data.target else: return { \'data\': data.data, \'target\': data.target, \'feature_names\': data.feature_names if hasattr(data, \'feature_names\') else [], \'target_names\': data.target_names if hasattr(data, \'target_names\') else [], \'DESCR\': data.DESCR, \'data_shape\': data.data.shape }"},{"question":"# ctypes Question You are required to implement a Python function using the `ctypes` library to interact with a C standard library function. The function you need to interact with is `qsort` from the C standard library, which sorts an array. Hereâ€™s what you need to do: 1. Implement a Python function `sort_integers(arr: List[int]) -> List[int]` that uses the C `qsort` function to sort a list of integers. 2. Use the `ctypes` library to load the C standard library, define the `qsort` function, and create a comparison function in Python to be used as a callback by `qsort`. # Function Signature ```python from typing import List def sort_integers(arr: List[int]) -> List[int]: pass ``` # Requirements: 1. **Types and Data Structure**: - Your function should receive a list of integers and return a sorted list of integers. 2. **Library Loading**: - Use `ctypes.CDLL` to load the C standard library (`libc.so.6` on Linux or `msvcrt.dll` on Windows). 3. **Function Access and Typing**: - Access the `qsort` function from the loaded library. - Define the comparison function with the `ctypes.CFUNCTYPE`. 4. **Comparison Function**: - Create a Python comparison function matching `qsort`\'s expectations and convert it to a C-compatible function pointer. 5. **Memory Management**: - Use `ctypes` to create a compatible array from the input list and ensure proper memory management. 6. **Sorting**: - Call `qsort` to sort the array. # Constraints: - Assume the input list contains a reasonable number of elements (1 <= len(arr) <= 1000). - Handle errors gracefully. If the sorting cannot be performed for some reason, your function should raise an appropriate Python exception. # Example: ```python >>> sort_integers([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]) [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9] ``` # Testing: - Ensure your function is tested on different input sizes and edge cases. - Verify the correctness of the sorting through assertions.","solution":"import ctypes from ctypes import CDLL, CFUNCTYPE, POINTER, c_int, c_void_p from typing import List def compare(a, b): ai = ctypes.cast(a, ctypes.POINTER(c_int)).contents.value bi = ctypes.cast(b, ctypes.POINTER(c_int)).contents.value return ai - bi def sort_integers(arr: List[int]) -> List[int]: if not arr: return arr libc = ctypes.CDLL(None) CMPFUNC = CFUNCTYPE(c_int, ctypes.POINTER(c_void_p), ctypes.POINTER(c_void_p)) cmp_func = CMPFUNC(compare) ArrayType = c_int * len(arr) c_array = ArrayType(*arr) libc.qsort(c_array, len(c_array), ctypes.sizeof(c_int), cmp_func) return list(c_array)"},{"question":"# PyTorch Coding Assessment Question Objective Your task is to implement and demonstrate a function using the `torch.nn.init` functions to initialize a custom neural network. This will evaluate your understanding of PyTorch\'s parameter initialization techniques to ensure efficient training of neural networks. Problem Statement 1. Implement a class `CustomNeuralNetwork` that inherits from `torch.nn.Module`. 2. This class should have: - An `__init__` method that constructs the neural network with two fully connected layers (`torch.nn.Linear`): - The first layer should take an input of size `input_size` and output of size `hidden_size`. - The second layer should take an input of size `hidden_size` and output of size `output_size`. - An `initialize_weights` method that initializes the weights of the layers using `kaiming_normal_` initialization for the weight matrices and `zeros_` initialization for the biases. - A `forward` method to define the forward pass, using ReLU activation between layers. Input and Output - The `__init__` method should initialize the two layers and their dimensions. - The `initialize_weights` method should not accept any input but should apply the specified initialization to the weights and biases of the network. - The `forward` method should accept a tensor `x` as input and return the output tensor after passing it through the network. - Use ReLU activation function between the layers. Constraints - The `input_size`, `hidden_size`, and `output_size` are positive integers. - The forward method should be efficient concerning time complexity. Example ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomNeuralNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) self.initialize_weights() def initialize_weights(self): init.kaiming_normal_(self.fc1.weight) init.zeros_(self.fc1.bias) init.kaiming_normal_(self.fc2.weight) init.zeros_(self.fc2.bias) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Example usage input_size = 10 hidden_size = 5 output_size = 1 model = CustomNeuralNetwork(input_size, hidden_size, output_size) print(model) ``` In this example, you need to implement the `CustomNeuralNetwork` class with the specified requirements and demonstrate its initialization and forward pass.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomNeuralNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) self.initialize_weights() def initialize_weights(self): init.kaiming_normal_(self.fc1.weight) init.zeros_(self.fc1.bias) init.kaiming_normal_(self.fc2.weight) init.zeros_(self.fc2.bias) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x"},{"question":"Objective Create a function utilizing the `ChainMap` class from the Python `collections` module to manage multiple configuration settings with layered scopes. Task Write a function `manage_configs` that accepts three dictionaries representing global, application-level, and user-specific settings. Your function should: 1. Combine these dictionaries using `ChainMap` into a single configuration view where user-specific settings override application-level settings, which in turn override global settings. 2. Allow updates to be made only in the user-specific dictionary. 3. If needed, add methods to handle adding new layers of settings or switching to parent scopes. Implementation Details - Function Name: `manage_configs` - Parameters: - `global_config` (dict): Global configuration settings. - `app_config` (dict): Application-specific configuration settings. - `user_config` (dict): User-specific configuration settings. - Function Signature: ```python def manage_configs(global_config: dict, app_config: dict, user_config: dict) -> \'ConfigManager\': ``` - Return Type: A `ConfigManager` object (a custom class you need to create) that incorporates the `ChainMap` functionality. Functional Requirements 1. The `ConfigManager` class should: - Initialize with a `ChainMap` comprising the user, application, and global configurations. - Provide a method `get_setting` to fetch a setting value given a key. - Allow updating settings only in the user-specific configuration. - Provide an interface to add new configuration layers using `new_child`. - Provide an interface to switch to parent configuration contexts using `parents`. 2. Example Usage: ```python global_config = {\\"setting1\\": \\"global_value1\\", \\"setting2\\": \\"global_value2\\"} app_config = {\\"setting2\\": \\"app_value2\\", \\"setting3\\": \\"app_value3\\"} user_config = {\\"setting3\\": \\"user_value3\\", \\"setting4\\": \\"user_value4\\"} manager = manage_configs(global_config, app_config, user_config) assert manager.get_setting(\\"setting1\\") == \\"global_value1\\" assert manager.get_setting(\\"setting2\\") == \\"app_value2\\" assert manager.get_setting(\\"setting3\\") == \\"user_value3\\" assert manager.get_setting(\\"setting4\\") == \\"user_value4\\" manager.update_setting(\\"setting1\\", \\"user_value1\\") assert manager.get_setting(\\"setting1\\") == \\"user_value1\\" manager.add_config_layer({\\"setting5\\": \\"new_layer_value\\"}) assert manager.get_setting(\\"setting5\\") == \\"new_layer_value\\" parent_manager = manager.get_parent_scope() assert parent_manager.get_setting(\\"setting5\\") == None ``` Constraints - Ensure `manage_configs` and `ConfigManager` follow Python\'s best practices and handle edge cases gracefully. - Only update user-specific configuration settings. - Handle non-existing keys appropriately. Class Template (for guidance) ```python from collections import ChainMap class ConfigManager: def __init__(self, user_config, app_config, global_config): self.configs = ChainMap(user_config, app_config, global_config) def get_setting(self, key): return self.configs.get(key) def update_setting(self, key, value): self.configs.maps[0][key] = value def add_config_layer(self, new_layer): self.configs = self.configs.new_child(new_layer) def get_parent_scope(self): return ConfigManager(*self.configs.parents.maps) def manage_configs(global_config, app_config, user_config): return ConfigManager(user_config, app_config, global_config) ``` You are required to implement the `ConfigManager` class and `manage_configs` function based on the template and descriptions provided. Evaluation Criteria - Correct use of `ChainMap`. - Proper encapsulation of functionality within the `ConfigManager` class. - Code readability and adherence to Python standards. - Handling of edge cases and robust design.","solution":"from collections import ChainMap class ConfigManager: def __init__(self, user_config, app_config, global_config): self.configs = ChainMap(user_config, app_config, global_config) def get_setting(self, key): return self.configs.get(key) def update_setting(self, key, value): self.configs.maps[0][key] = value def add_config_layer(self, new_layer): self.configs = self.configs.new_child(new_layer) def get_parent_scope(self): return ConfigManager(*self.configs.parents.maps) def manage_configs(global_config, app_config, user_config): return ConfigManager(user_config, app_config, global_config)"},{"question":"# Sound File Metadata Extractor Objective: Create a Python function that processes a list of sound files and returns detailed metadata for each file using the `sndhdr` module from Python\'s standard library. Function Signature: ```python def extract_sound_metadata(file_list: List[str]) -> List[Dict[str, Any]]: ``` # Requirements: - The function should accept a list of file names (`file_list`) as input. - For each file in the list, use the `sndhdr.what(filename)` function to determine the sound file metadata. - Return a list of dictionaries, where each dictionary contains the following keys: - `filename`: The name of the file. - `filetype`: The type of the sound file (e.g., \'wav\', \'aiff\', etc.). - `framerate`: The sampling rate of the sound file. - `nchannels`: The number of channels in the sound file. - `nframes`: The number of frames in the sound file. - `sampwidth`: The sample width in bits. # Constraints: - If `sndhdr.what(filename)` returns `None` for a file, include it in the result list with `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth` set to `None`. # Example Usage: ```python from typing import List, Dict, Any import sndhdr def extract_sound_metadata(file_list: List[str]) -> List[Dict[str, Any]]: result = [] for file in file_list: info = sndhdr.what(file) if info: result.append({ \'filename\': file, \'filetype\': info.filetype, \'framerate\': info.framerate, \'nchannels\': info.nchannels, \'nframes\': info.nframes, \'sampwidth\': info.sampwidth }) else: result.append({ \'filename\': file, \'filetype\': None, \'framerate\': None, \'nchannels\': None, \'nframes\': None, \'sampwidth\': None }) return result # Sample Usage file_list = [\'sound1.wav\', \'sound2.aiff\', \'sound3.mp3\'] metadata = extract_sound_metadata(file_list) print(metadata) ``` # Notes: - This function can be used to process a variety of sound files and extract important metadata, making it useful for applications that need to handle and manage audio files. - Make sure to handle edge cases where files might not be sound files or the metadata isn\'t retrievable.","solution":"from typing import List, Dict, Any import sndhdr def extract_sound_metadata(file_list: List[str]) -> List[Dict[str, Any]]: result = [] for file in file_list: info = sndhdr.what(file) if info: result.append({ \'filename\': file, \'filetype\': info.filetype, \'framerate\': info.framerate, \'nchannels\': info.nchannels, \'nframes\': info.nframes, \'sampwidth\': info.sampwidth }) else: result.append({ \'filename\': file, \'filetype\': None, \'framerate\': None, \'nchannels\': None, \'nframes\': None, \'sampwidth\': None }) return result"},{"question":"**Problem Statement:** Implement a function `analyze_robots_txt(robots_url: str) -> dict` that leverages the `RobotFileParser` class from the `urllib.robotparser` module to analyze a `robots.txt` file from the given URL. **Function Signature:** ```python def analyze_robots_txt(robots_url: str) -> dict: pass ``` **Input:** - `robots_url`: A string representing the URL pointing to the `robots.txt` file. **Output:** - A dictionary with the following structure: ```python { \\"can_fetch\\": bool, # Whether the default user agent can fetch the root URL of the site. \\"crawl_delay\\": int, # The crawl delay for the default user agent. \\"request_rate\\": tuple, # The request rate for the default user agent as a tuple (requests, seconds). \\"sitemap_urls\\": list # A list of sitemap URLs mentioned in the `robots.txt` file. } ``` **Constraints:** - The function should handle connection errors gracefully and return `None` for involved values if the file cannot be fetched or parsed. - The function should use the default user agent (`*`). **Usage Example:** ```python robots_url = \\"http://www.example.com/robots.txt\\" result = analyze_robots_txt(robots_url) print(result) # Expected Output (example for illustrative purposes): # { # \\"can_fetch\\": True, # \\"crawl_delay\\": 10, # \\"request_rate\\": (5, 60), # \\"sitemap_urls\\": [\\"http://www.example.com/sitemap1.xml\\", \\"http://www.example.com/sitemap2.xml\\"] # } ``` The function `analyze_robots_txt` should utilize the `RobotFileParser` class methods as appropriate to parse the `robots.txt` file and extract the required information.","solution":"import urllib.robotparser import urllib.error def analyze_robots_txt(robots_url: str) -> dict: Analyzes a robots.txt file from the given URL and returns a dictionary with details. Args: - robots_url (str): URL of the robots.txt file Returns: - dict: A dictionary containing can_fetch, crawl_delay, request_rate, and sitemap_urls robot_parser = urllib.robotparser.RobotFileParser() try: robot_parser.set_url(robots_url) robot_parser.read() can_fetch = robot_parser.can_fetch(\'*\', \'/\') crawl_delay = robot_parser.crawl_delay(\'*\') request_rate = robot_parser.request_rate(\'*\') sitemaps = robot_parser.site_maps() or [] if request_rate: request_rate = (request_rate.requests, request_rate.seconds) result = { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"sitemap_urls\\": sitemaps } except (urllib.error.URLError, ValueError): result = { \\"can_fetch\\": None, \\"crawl_delay\\": None, \\"request_rate\\": None, \\"sitemap_urls\\": None } return result"},{"question":"# Asynchronous TCP Server In this task, you are required to implement an asynchronous TCP server using Python\'s `asyncio` package. # Requirements 1. The server should handle multiple clients concurrently. 2. For each connected client: - The server should read lines of text sent by the client. - For each line received, the server should send back the reversed line. - The server should handle client disconnections gracefully. # Implementation Details 1. **Creating the Server:** - Use `loop.create_server()` to create and start the server. - The server should run on `localhost` and port `8888`. 2. **Handling Client Connections:** - Implement a custom protocol class inherited from `asyncio.Protocol`. - Override `connection_made()`, `data_received()`, and `connection_lost()` methods to handle client connections and disconnections. 3. **Reading and Writing Data:** - Use the `data_received()` method to read data from the client. - Reverse the line and use the transport\'s `write()` method to send data back. # Constraints - Ensure that the server can handle multiple concurrent connections without blocking. - Implement appropriate error handling within the server to handle exceptions and client disconnections gracefully. # Template ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.client_address = transport.get_extra_info(\'peername\') print(f\'Connection from {self.client_address}\') def data_received(self, data): message = data.decode() print(f\'Received {message} from {self.client_address}\') # Reverse the message reversed_message = message[::-1] # Send it back to the client print(f\'Sending {reversed_message} to {self.client_address}\') self.transport.write(reversed_message.encode()) def connection_lost(self, exc): print(f\'Connection lost with {self.client_address}\') if exc: print(f\'Error: {exc}\') async def main(): loop = asyncio.get_running_loop() # Server instance creation server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) # Serving requests async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Expected Output 1. When a client connects, the server should print `Connection from <client_address>`. 2. For each line received from the client, the server should print `Received <line> from <client_address>` and `Sending <reversed_line> to <client_address>`. 3. When a client disconnects, the server should print `Connection lost with <client_address>`. # Performance Requirements - The server should handle high concurrency efficiently without significant performance degradation. - The solution should demonstrate a good understanding of asyncio\'s capabilities and proper usage of low-level APIs.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.client_address = transport.get_extra_info(\'peername\') print(f\'Connection from {self.client_address}\') def data_received(self, data): message = data.decode() print(f\'Received {message} from {self.client_address}\') # Reverse the message reversed_message = message[::-1] # Send it back to the client print(f\'Sending {reversed_message} to {self.client_address}\') self.transport.write(reversed_message.encode()) def connection_lost(self, exc): print(f\'Connection lost with {self.client_address}\') if exc: print(f\'Error: {exc}\') async def main(): loop = asyncio.get_running_loop() # Server instance creation server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) # Serving requests async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Problem Statement: Email Payload Encoding with Custom Function** Let\'s create a function `custom_email_encoder` that will accept an email `Message` object and an encoding type. Based on the encoding type provided, the function will encode the payload of the `Message` object using the respective method from the `email.encoders` module. The function should also handle multipart messages gracefully and return an appropriate message if the `Message` object is multipart. **Function Signature:** ```python def custom_email_encoder(msg, encoding): Encodes the payload of an email Message object based on the specified encoding type. Parameters: - msg (email.message.EmailMessage): The email message object to encode. - encoding (str): The encoding type (\'quopri\', \'base64\', \'7or8bit\', \'noop\'). Returns: - email.message.EmailMessage: The encoded email message object. pass ``` **Input:** - `msg`: an instance of `email.message.EmailMessage`, representing the email message to be encoded. - `encoding`: a string representing the type of encoding to use (\'quopri\', \'base64\', \'7or8bit\', \'noop\'). **Output:** - Returns the encoded email message object if it is not multipart. - If the message is multipart, raise a `TypeError` with a message \\"Cannot encode a multipart message.\\". **Constraints:** - The message object must not be multipart. - Valid encoding types are \'quopri\', \'base64\', \'7or8bit\', and \'noop\'. **Example:** ```python from email.message import EmailMessage import email.encoders as enc # Example message creation msg = EmailMessage() msg.set_payload(\\"This is a sample payload that contains special characters like Ã¤, Ã¶, Ã¼.\\") # Apply custom encoding encoded_msg = custom_email_encoder(msg, \'base64\') # Verify headers assert encoded_msg[\'Content-Transfer-Encoding\'] == \'base64\' ``` **Explanation:** 1. The function `custom_email_encoder` should accept an `EmailMessage` object and an encoding type. 2. It will check if the message is multipart. If so, it should raise a `TypeError`. 3. Based on the encoding type, appropriate encoding from `email.encoders` will be applied. 4. If the encoding type is invalid, handle the error elegantly. This assessment question tests the understanding of handling email message objects, applying different encoding techniques, and writing robust, error-handling code in Python.","solution":"from email.message import EmailMessage import email.encoders as enc def custom_email_encoder(msg, encoding): Encodes the payload of an email Message object based on the specified encoding type. Parameters: - msg (email.message.EmailMessage): The email message object to encode. - encoding (str): The encoding type (\'quopri\', \'base64\', \'7or8bit\', \'noop\'). Returns: - email.message.EmailMessage: The encoded email message object. if msg.is_multipart(): raise TypeError(\\"Cannot encode a multipart message.\\") if encoding == \'quopri\': enc.encode_quopri(msg) elif encoding == \'base64\': enc.encode_base64(msg) elif encoding == \'7or8bit\': enc.encode_7or8bit(msg) elif encoding == \'noop\': enc.encode_noop(msg) else: raise ValueError(f\\"Invalid encoding type: {encoding}\\") return msg"},{"question":"# **Asynchronous Task Manager** **Objective:** Implement an asynchronous task manager in Python that coordinates multiple coroutines using asyncio synchronization primitives such as Lock, Event, and Semaphore. The task manager should allow multiple tasks to run concurrently while ensuring exclusive access to shared resources and proper notification of event completions. **Problem Statement:** You are required to implement an asynchronous task manager that manages tasks using the following classes and methods described below: **Classes:** 1. **TaskManager:** - **Methods:** - `__init__(self, max_concurrent_tasks: int)`: Initializes the TaskManager with a given limit on the maximum number of concurrent tasks. - `add_task(self, coro: Coroutine)`: Schedules a given coroutine to be run by the task manager. Should wait if the limit of concurrent tasks is reached. - `run_tasks(self)`: Runs all scheduled tasks, ensuring that at any point no more than the specified number of tasks are running concurrently. **Constraints:** - You must use the asyncio primitives such as Lock, Event, Semaphore to control task execution. - Ensure that the coroutines are managed properly, i.e., no more than `max_concurrent_tasks` are running at any given time. - Make use of async with statements for acquiring and releasing locks and semaphores. **Input:** - The `max_concurrent_tasks` parameter in the `TaskManager` class. - The `add_task` method will receive coroutines that simulate tasks. **Output:** - There is no direct output to return, but the tasks should be run concurrently within the specified limits. **Example Usage:** ```python import asyncio from typing import Coroutine class TaskManager: def __init__(self, max_concurrent_tasks: int): self.max_concurrent_tasks = max_concurrent_tasks self._semaphore = asyncio.Semaphore(max_concurrent_tasks) self._tasks = [] async def add_task(self, coro: Coroutine): async with self._semaphore: # Ensure that the number of concurrent tasks does not exceed the limit task = asyncio.create_task(coro) self._tasks.append(task) await task # Wait for the task to complete async def run_tasks(self): await asyncio.gather(*self._tasks) # Run all tasks concurrently # Example coroutines to demonstrate the TaskManager usage async def example_task(task_id: int): print(f\\"Task {task_id} starting\\") await asyncio.sleep(2) # Simulate a task taking some time print(f\\"Task {task_id} completed\\") # Main function to run the task manager async def main(): task_manager = TaskManager(max_concurrent_tasks=3) for i in range(6): await task_manager.add_task(example_task(i)) await task_manager.run_tasks() # Running the example asyncio.run(main()) ``` In the example, the TaskManager restricts the number of concurrent tasks to 3. Six tasks are added to the manager, and it ensures that only 3 tasks run at the same time. **Note:** The example above is just for demonstration purposes, and you are free to design the tasks and their behavior as needed to test the functionality of the TaskManager class.","solution":"import asyncio from typing import Coroutine class TaskManager: def __init__(self, max_concurrent_tasks: int): self.max_concurrent_tasks = max_concurrent_tasks self._semaphore = asyncio.Semaphore(max_concurrent_tasks) self._tasks = [] async def add_task(self, coro: Coroutine): async with self._semaphore: task = asyncio.create_task(coro) self._tasks.append(task) await task # Wait for the task to complete async def run_tasks(self): await asyncio.gather(*self._tasks) # Run all tasks concurrently # Example coroutines to demonstrate the TaskManager usage async def example_task(task_id: int): print(f\\"Task {task_id} starting\\") await asyncio.sleep(2) # Simulate a task taking some time print(f\\"Task {task_id} completed\\") # Main function to run the task manager async def main(): task_manager = TaskManager(max_concurrent_tasks=3) for i in range(6): await task_manager.add_task(example_task(i)) await task_manager.run_tasks() # Running the example asyncio.run(main())"},{"question":"Objective: To assess students\' understanding and capability in utilizing Python\'s `email` package to create, parse, and manage the content of an email message. Problem Statement: You are required to write a Python function that creates an email message, encodes it using the MIME format, and then parses the message back to extract the content. The function should demonstrate the ability to use various components of the `email` package, such as `email.message`, `email.generator`, and `email.parser`. Function Specification: **Function Name:** `create_and_parse_email` **Inputs:** - `from_addr` (str): The sender\'s email address. - `to_addr` (str): The recipient\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. **Output:** - A dictionary with the following key-value pairs: - `\\"From\\"` (str): The sender\'s email address. - `\\"To\\"` (str): The recipient\'s email address. - `\\"Subject\\"` (str): The subject of the email. - `\\"Body\\"` (str): The body content of the email. **Constraints:** - Ensure the email message is encoded using MIME standards. - Email addresses, subject, and body content will be valid strings and not empty. Example Usage: ```python result = create_and_parse_email( \\"sender@example.com\\", \\"recipient@example.com\\", \\"Hello World\\", \\"This is a test email.\\" ) print(result) ``` **Expected Output:** ```python { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Hello World\\", \\"Body\\": \\"This is a test email.\\" } ``` Notes: - Use `email.message.EmailMessage` to create the email. - Use `email.generator.BytesGenerator` to generate a MIME formatted string from the email message. - Use `email.parser.BytesParser` to parse the MIME formatted string back into an email message object. - Extract and return the required information from the parsed email message.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator from email.parser import BytesParser from io import BytesIO def create_and_parse_email(from_addr, to_addr, subject, body): # Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject msg.set_content(body) # Create a BytesIO object to hold the MIME format of the email buffer = BytesIO() gen = BytesGenerator(buffer) gen.flatten(msg) # Get the MIME formatted email as bytes mime_bytes = buffer.getvalue() # Parse the MIME formatted email back into an email message object parsed_msg = BytesParser().parsebytes(mime_bytes) # Extract the required information from the parsed message result = { \\"From\\": parsed_msg[\'From\'], \\"To\\": parsed_msg[\'To\'], \\"Subject\\": parsed_msg[\'Subject\'], \\"Body\\": parsed_msg.get_payload() } return result"},{"question":"# Exception Handling and Custom Exception Implementation **Objective:** Demonstrate your understanding of Python\'s exception handling mechanisms by implementing a small library management system. # Problem Statement: You are to implement a simple library management system using Python. The system should allow a library staff member to: 1. Add books to the library. 2. Borrow books from the library. 3. Return books to the library. The system should handle various exceptions that might occur during these operations. Additionally, you need to create custom exceptions to handle specific error scenarios. # Requirements: 1. Define a class `Library` that supports the following methods: - `add_book(book: str)`: - Adds a book to the library. - Raises a `LibraryError` if the book is already in the library. - `borrow_book(book: str)`: - Borrows a book from the library if it is available. - Raises `BookNotAvailableError` if the book is not available in the library. - Raises `LibraryError` if the book is already borrowed. - `return_book(book: str)`: - Returns a borrowed book to the library. - Raises `LibraryError` if the book was not borrowed or is not recognized as a library book. 2. Define custom exceptions: - `LibraryError` to handle general library errors. - `BookNotAvailableError` to handle cases where a book is not available for borrowing. 3. The library should manage an internal collection of books and the status of each book (whether it is borrowed or not). # Function Signatures: ```python class LibraryError(Exception): Base class for other exceptions pass class BookNotAvailableError(LibraryError): Raised when the book is not available for borrowing pass class Library: def __init__(self): # Initialize necessary attributes pass def add_book(self, book: str): # Implement the method pass def borrow_book(self, book: str): # Implement the method pass def return_book(self, book: str): # Implement the method pass ``` # Input and Output Format: - `add_book(book: str)`: Adds the book to the library if it does not exist. Raises `LibraryError` if the book already exists in the collection. - `borrow_book(book: str)`: Borrows the book from the library if available. Raises `BookNotAvailableError` if the book does not exist in the collection. Raises `LibraryError` if the book is already borrowed. - `return_book(book: str)`: Returns the book to the library. Raises `LibraryError` if the book is not recognized as borrowed or is not from the library\'s collection. # Examples: Example 1: ```python lib = Library() lib.add_book(\\"Python 101\\") lib.add_book(\\"Learn Django\\") try: lib.borrow_book(\\"Python 101\\") lib.borrow_book(\\"Python 101\\") # This should raise LibraryError except LibraryError as e: print(e) lib.return_book(\\"Python 101\\") ``` # Constraints: - A book name is a non-empty string. - Each method should raise appropriate exceptions when error conditions are met. In your solution, ensure proper handling of exceptions and correct status tracking of books in the library.","solution":"class LibraryError(Exception): Base class for other exceptions pass class BookNotAvailableError(LibraryError): Raised when the book is not available for borrowing pass class Library: def __init__(self): # Initialize necessary attributes self.books_collection = {} # Stores book names and their borrowed status def add_book(self, book: str): if not book: raise ValueError(\\"Book name cannot be empty.\\") if book in self.books_collection: raise LibraryError(f\\"The book \'{book}\' already exists in the library.\\") self.books_collection[book] = False # False indicates that the book is not borrowed def borrow_book(self, book: str): if book not in self.books_collection: raise BookNotAvailableError(f\\"The book \'{book}\' is not available in the library.\\") if self.books_collection[book]: raise LibraryError(f\\"The book \'{book}\' is already borrowed.\\") self.books_collection[book] = True # True indicates that the book is borrowed def return_book(self, book: str): if book not in self.books_collection: raise LibraryError(f\\"The book \'{book}\' is not recognized as a library book.\\") if not self.books_collection[book]: raise LibraryError(f\\"The book \'{book}\' was not borrowed.\\") self.books_collection[book] = False # False indicates that the book is now returned"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s `torch.utils.module_tracker` by implementing a custom neural network module that utilizes the `ModuleTracker` to track and print the layers visited during a forward pass. Problem Statement You are required to implement a custom neural network module, `TrackedNetwork`, using PyTorch. This module should utilize `torch.utils.module_tracker.ModuleTracker` to track the layers it passes through during the forward pass. The tracked information should be printed as a list of layer names. Requirements 1. **Implementation of `TrackedNetwork`**: - Create a class `TrackedNetwork` that extends `torch.nn.Module`. - Initialize multiple layers inside `TrackedNetwork` (e.g., `nn.Linear`, `nn.ReLU`, etc.). 2. **Tracking with `ModuleTracker`**: - Utilize `torch.utils.module_tracker.ModuleTracker` to track the current position in the module hierarchy. - Store the names of visited layers in a list during the forward pass. - Print the list of layer names at the end of the forward pass. 3. **Forward Method**: - Implement the `forward` method to perform a standard forward pass through the network while tracking the layers visited. Input - An instance of `TrackedNetwork`. - A tensor `input_tensor` of appropriate shape to pass through the network. Output - Print the names of the layers visited during the forward pass. Constraints - Use PyTorch\'s `torch.nn.Module` and `torch.utils.module_tracker.ModuleTracker`. - Ensure the code runs efficiently on both CPU and GPU environments. - The network should have at least three layers. Example ```python import torch import torch.nn as nn from torch.utils.module_tracker import ModuleTracker class TrackedNetwork(nn.Module): def __init__(self): super(TrackedNetwork, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(20, 1) self.tracker = ModuleTracker() def forward(self, x): with self.tracker: x = self.layer1(x) print(self.tracker.current_module_name) x = self.layer2(x) print(self.tracker.current_module_name) x = self.layer3(x) print(self.tracker.current_module_name) return x # Example usage: net = TrackedNetwork() input_tensor = torch.randn(5, 10) output = net(input_tensor) ``` This code should print the names of each layer visited during the forward pass.","solution":"import torch import torch.nn as nn from torch.utils.hooks import RemovableHandle class TrackedNetwork(nn.Module): def __init__(self): super(TrackedNetwork, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(20, 1) self.visited_layers = [] def forward(self, x): hooks = [] layers = [self.layer1, self.layer2, self.layer3] for layer in layers: def hook_fn(module, input, output): self.visited_layers.append(module.__class__.__name__) hooks.append(layer.register_forward_hook(hook_fn)) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) # Remove hooks after forward pass for hook in hooks: hook.remove() print(self.visited_layers) return x"},{"question":"You are tasked with implementing a Python function that processes numeric data stored in an array efficiently. Problem Statement: Given an array of integers, you need to perform the following operations: 1. **Insert an element:** Insert a new element at the specified index. 2. **Delete an element:** Remove the element at the specified index and return it. 3. **Reverse the array:** Reverse the order of the elements in the array in place. 4. **Find occurrences:** Return the number of times a specific element appears in the array. 5. **Convert to list:** Convert the array to a Python list and return it. You must implement the following function: ```python def process_array(operations): Process a series of operations on an array of integers. Args: operations (list of tuples): Each tuple contains an operation name and arguments required for the operation. Returns: list: A list of results for each query-type operation. import array # Create an empty array of type \'i\' (signed integer) arr = array.array(\'i\') results = [] for operation in operations: op = operation[0] if op == \'insert\': _, index, value = operation arr.insert(index, value) elif op == \'delete\': _, index = operation value = arr.pop(index) results.append(value) elif op == \'reverse\': arr.reverse() elif op == \'count\': _, value = operation count = arr.count(value) results.append(count) elif op == \'tolist\': results.append(arr.tolist()) else: raise ValueError(f\\"Unknown operation: {op}\\") return results ``` Input: - `operations` is a list of tuples. Each tuple describes an operation to be performed: - `(\\"insert\\", index, value)` - Insert `value` at position `index`. - `(\\"delete\\", index)` - Remove and return the element at position `index`. - `(\\"reverse\\",)` - Reverse the array in place. - `(\\"count\\", value)` - Count and return the number of occurrences of `value` in the array. - `(\\"tolist\\",)` - Convert the array to a list and return it. Output: - Returns a list of results for each query-type operation (`delete`, `count`, and `tolist`) in the order they are processed. Examples: ```python operations = [ (\\"insert\\", 0, 3), (\\"insert\\", 1, 5), (\\"insert\\", 2, 7), (\\"count\\", 5), (\\"delete\\", 1), (\\"tolist\\",) ] print(process_array(operations)) # Output: [1, 5, [3, 7]] ``` **Constraints:** - All `insert` and `delete` operations will have valid indices. - The array will not exceed a reasonable size for typical operations (~10^4 elements). Make sure to handle edge cases such as inserting at the end, deleting from an empty array, etc.","solution":"def process_array(operations): Process a series of operations on an array of integers. Args: operations (list of tuples): Each tuple contains an operation name and arguments required for the operation. Returns: list: A list of results for each query-type operation. import array # Create an empty array of type \'i\' (signed integer) arr = array.array(\'i\') results = [] for operation in operations: op = operation[0] if op == \'insert\': _, index, value = operation arr.insert(index, value) elif op == \'delete\': _, index = operation value = arr.pop(index) results.append(value) elif op == \'reverse\': arr.reverse() elif op == \'count\': _, value = operation count = arr.count(value) results.append(count) elif op == \'tolist\': results.append(arr.tolist()) else: raise ValueError(f\\"Unknown operation: {op}\\") return results"},{"question":"Problem Statement You are required to create a heatmap with hierarchical clustering using seaborn\'s `clustermap` function. The dataset to be used is the Titanic dataset available through seaborn. You need to demonstrate your understanding of seaborn by performing the following steps: 1. Load the Titanic dataset. 2. Preprocess the data to include only numerical columns and fill any missing values with the column mean. 3. Create a basic clustermap of the preprocessed data. 4. Customize the clustermap to: - Set the figure size to 10x8 inches. - Use the \\"rocket\\" colormap and set the color range from 0 to 1. - Add row colors based on the \\"class\\" attribute, mapping 1st, 2nd, and 3rd class to different colors. - Standardize the data across columns. 5. Save the resulting plot to a file named `titanic_clustermap.png`. Input - No input required from the user. The dataset is to be loaded internally from seaborn. Output - The function should save the generated clustermap to the file `titanic_clustermap.png`. Constraints - Use seaborn for creating the clustermap. - Handle missing values appropriately. Function Signature ```python def create_titanic_clustermap(): pass ``` Example Implementation Here\'s a skeleton of the expected implementation: ```python import seaborn as sns import pandas as pd import numpy as np def create_titanic_clustermap(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Preprocess the data numerical_cols = titanic.select_dtypes(include=[np.number]).columns data = titanic[numerical_cols].fillna(titanic[numerical_cols].mean()) # Create a colormap for the \'class\' attribute class_mapping = {1: \'red\', 2: \'blue\', 3: \'green\'} row_colors = titanic[\'class\'].map(class_mapping) # Create and customize the clustermap g = sns.clustermap(data, figsize=(10, 8), cmap=\\"rocket\\", vmin=0, vmax=1, row_colors=row_colors, standard_scale=1) # Save the clustermap to a file g.savefig(\'titanic_clustermap.png\') # Example usage create_titanic_clustermap() ``` Ensure your function follows the provided guidelines and meets the requirements listed above.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def create_titanic_clustermap(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Preprocess the data numerical_cols = titanic.select_dtypes(include=[np.number]).columns data = titanic[numerical_cols].fillna(titanic[numerical_cols].mean()) # Create a colormap for the \'class\' attribute class_mapping = {1: \'red\', 2: \'blue\', 3: \'green\'} row_colors = titanic[\'class\'].map(class_mapping) # Create and customize the clustermap g = sns.clustermap(data, figsize=(10, 8), cmap=\\"rocket\\", vmin=0, vmax=1, row_colors=row_colors, standard_scale=1) # Save the clustermap to a file plt.savefig(\'titanic_clustermap.png\') # Example usage create_titanic_clustermap()"},{"question":"**Question: Implement a Multi-output Decision Tree Classifier with Missing Values Handling** You are required to implement a function `multi_output_tree_classifier` that trains a multi-output decision tree classifier on a given dataset, handles missing values, and evaluates its performance. The function should take the following inputs: - `X_train`: A 2D list or a NumPy array of shape (n_samples, n_features) representing the feature matrix for training data. - `y_train`: A 2D list or a NumPy array of shape (n_samples, n_outputs) representing the multi-output target matrix for training data. - `X_test`: A 2D list or a NumPy array of shape (m_samples, n_features) representing the feature matrix for testing data. - `missing_value` (optional): A value indicating missing data in both `X_train` and `X_test`. Default is `None`. The function should output the predicted multi-output values for the test data in the form of a NumPy array of shape (m_samples, n_outputs). Function Signature: ```python import numpy as np from sklearn.tree import DecisionTreeClassifier def multi_output_tree_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, missing_value=None) -> np.ndarray: # Implementation here ``` **Implementation Details:** 1. Handle any missing values in the training and testing data according to the rules provided in the documentation. If no missing values are present during training for a given feature, map them to the child with the most samples during prediction. 2. Use `DecisionTreeClassifier` with `splitter=\'best\'` to train the multi-output classifier. 3. Train the classifier on the given training data `X_train` and `y_train`. 4. Predict the multi-output values for `X_test`. **Constraints:** - Any missing values should be replaced or handled as specified in the training process. - You may assume that all features are numerical and `X_train` and `X_test` are well-formed matrices. - You should not use any external libraries other than scikit-learn and NumPy. **Example:** ```python # Sample Data X_train = np.array([[1, 2], [3, 4], [np.nan, 6], [8, np.nan]]) y_train = np.array([[0, 1], [1, 0], [1, 1], [0, 0]]) X_test = np.array([[2, 3], [np.nan, 4], [7, np.nan]]) # Calling the function predictions = multi_output_tree_classifier(X_train, y_train, X_test, missing_value=np.nan) print(predictions) ``` Expected output: A 2D NumPy array with predicted values for the test data. ```python array([[0, 1], [1, 0], [0, 0]]) ``` Your goal is to implement the `multi_output_tree_classifier` function, ensuring it can handle multi-output targets and missing values effectively.","solution":"import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.impute import SimpleImputer from sklearn.multioutput import MultiOutputClassifier def multi_output_tree_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, missing_value=None) -> np.ndarray: # Replace missing values with specified value or mean if not specified if missing_value is not None: imputer = SimpleImputer(missing_values=missing_value, strategy=\\"mean\\") else: imputer = SimpleImputer(strategy=\\"mean\\") # Impute missing values in training and testing data X_train_imputed = imputer.fit_transform(X_train) X_test_imputed = imputer.transform(X_test) # Initialize the multi-output classifier with a DecisionTreeClassifier clf = MultiOutputClassifier(DecisionTreeClassifier(splitter=\'best\')) # Train the classifier on the imputed training data clf.fit(X_train_imputed, y_train) # Predict the multi-output values for imputed test data predictions = clf.predict(X_test_imputed) return predictions"},{"question":"**Question: Implement a Randomized Weight Initialization Function for Neural Networks in PyTorch** **Objective:** You are tasked with implementing a function that performs weight initialization for a PyTorch neural network. This function should demonstrate your understanding of the `torch.random` module and its application in a real-world neural network scenario. **Function Signature:** ```python def initialize_weights(module, seed: int = 0) -> None: Initialize the weights of a neural network module with a given seed for reproducibility. Args: - module (torch.nn.Module): A PyTorch neural network module whose parameters (weights and biases) will be initialized. - seed (int): An integer seed for the random number generator to ensure reproducibility. Returns: - None. The function modifies the input module in place. ``` **Details:** 1. **Input:** - `module`: An instance of `torch.nn.Module`. You must initialize the weights and biases of all linear layers within this module. - `seed`: An integer to seed the random number generator. Default is `0`. 2. **Output:** - The function should not return anything. Instead, it modifies the input `module` in place. 3. **Constraints:** - Use the `torch.manual_seed` function to set the seed for reproducibility. - Initialize weights of linear layers using a normal distribution with mean 0 and standard deviation 0.01. - Initialize biases of linear layers to zero. - Only initialize weights and biases if `module` is an instance of `torch.nn.Linear`. 4. **Performance Requirements:** - Ensure that the operation is efficient and scales well with the size of the neural network module. **Example:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x model = SimpleModel() initialize_weights(model, seed=42) # After calling initialize_weights, the parameters of fc1 and fc2 should be initialized # with the specified distributions, seed ensuring reproducibility. ``` **Notes:** - Make sure to test your function not just with linear layers but with a mixture of layer types to ensure only linear layers are initialized. - The resulting initialization should be reproducible if the same seed is used again.","solution":"import torch import torch.nn as nn def initialize_weights(module, seed: int = 0) -> None: Initialize the weights of a neural network module with a given seed for reproducibility. Args: - module (torch.nn.Module): A PyTorch neural network module whose parameters (weights and biases) will be initialized. - seed (int): An integer seed for the random number generator to ensure reproducibility. Returns: - None. The function modifies the input module in place. torch.manual_seed(seed) for layer in module.modules(): if isinstance(layer, nn.Linear): nn.init.normal_(layer.weight, mean=0.0, std=0.01) if layer.bias is not None: nn.init.constant_(layer.bias, 0.0)"},{"question":"**Problem Statement:** You have been provided with the \\"titanic\\" dataset, which contains information about the passengers on the Titanic. Utilizing seaborn, you are required to visualize the survival rate of passengers based on different criteria. **Dataset:** You can load the dataset using the following code: ```python titanic = sns.load_dataset(\\"titanic\\") ``` **Requirements:** 1. Create a function `visualize_survival_rate` that accepts a seaborn-accepted data structure and visualizes the survival rate of passengers based on the following criteria: - Gender - Passenger class **Function Signature:** ```python def visualize_survival_rate(data: pd.DataFrame) -> None: pass ``` **Tasks:** 1. **Load the Data:** - Load the \\"titanic\\" dataset as shown above. 2. **Visualize Survival Rate by Gender:** - Use a seaborn categorical plot to visualize the survival rate per gender. - The plot should show the percentage of passengers who survived, separated by gender. 3. **Visualize Survival Rate by Passenger Class:** - Use a seaborn categorical plot to visualize the survival rate per passenger class. - The plot should show the percentage of passengers who survived, separated by class (1st, 2nd, 3rd). 4. **Visualize Survival Rate by Gender and Passenger Class:** - Use a seaborn categorical plot to visualize the survival rate considering both gender and passenger class. - The plot should allow comparison across both criteria simultaneously. **Constraints:** - The dataset should be provided as a `pandas.DataFrame`. - You need to use seaborn for visualization. - Ensure the plots are appropriately labeled for clarity. **Example:** ```python import pandas as pd import seaborn as sns def visualize_survival_rate(data: pd.DataFrame) -> None: # Task 1 sns.catplot(data=data, x=\\"sex\\", y=\\"survived\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Gender\\") plt.show() # Task 2 sns.catplot(data=data, x=\\"pclass\\", y=\\"survived\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Passenger Class\\") plt.show() # Task 3 sns.catplot(data=data, x=\\"sex\\", y=\\"survived\\", hue=\\"pclass\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Gender and Passenger Class\\") plt.show() # Load dataset and call the function titanic = sns.load_dataset(\\"titanic\\") visualize_survival_rate(titanic) ``` **Notes:** - Ensure your code is clean and well-documented. - Your plots should be clear and appropriately labeled. - You are free to use additional seaborn functionalities to enhance your visualizations, but the core requirements must be met.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_survival_rate(data: pd.DataFrame) -> None: Visualizes the survival rate of Titanic passengers based on gender and passenger class. Parameters: data (pd.DataFrame): A pandas DataFrame containing the Titanic dataset. # Task 1: Visualize survival rate by gender sns.catplot(data=data, x=\\"sex\\", y=\\"survived\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Gender\\") plt.ylabel(\\"Survival Rate\\") plt.xlabel(\\"Gender\\") plt.ylim(0, 1) plt.show() # Task 2: Visualize survival rate by passenger class sns.catplot(data=data, x=\\"pclass\\", y=\\"survived\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Passenger Class\\") plt.ylabel(\\"Survival Rate\\") plt.xlabel(\\"Passenger Class\\") plt.ylim(0, 1) plt.show() # Task 3: Visualize survival rate by gender and passenger class sns.catplot(data=data, x=\\"sex\\", y=\\"survived\\", hue=\\"pclass\\", kind=\\"bar\\") plt.title(\\"Survival Rate by Gender and Passenger Class\\") plt.ylabel(\\"Survival Rate\\") plt.xlabel(\\"Gender\\") plt.ylim(0, 1) plt.show()"},{"question":"Advanced Indexing with MultiIndex **Objective**: Demonstrate your understanding of creating, manipulating, and querying data using pandas `MultiIndex`. **Scenario**: You are provided with a sales dataset for a multi-national company that operates in multiple regions and cities. Each city has multiple sales representatives, each identified by their unique ID. Your task is to perform the following operations on this dataset. **Dataset (sample data)**: ```python data = { \'Region\': [\'North\', \'North\', \'North\', \'East\', \'East\', \'West\', \'West\', \'West\'], \'City\': [\'CityA\', \'CityA\', \'CityB\', \'CityC\', \'CityC\', \'CityD\', \'CityD\', \'CityD\'], \'Rep_ID\': [101, 102, 103, 201, 202, 301, 302, 303], \'Sales\': [1234.5, 2345.6, 3456.7, 4567.8, 5678.9, 6789.0, 7890.1, 8901.2] } df = pd.DataFrame(data) ``` **Tasks**: 1. **Create a MultiIndex DataFrame**: - Set a `MultiIndex` using the columns `Region`, `City`, and `Rep_ID`. - Name the levels of the MultiIndex as `[\'Region\', \'City\', \'Rep_ID\']`. 2. **Select Data**: - Using the created MultiIndex, select the sales data for all representatives in `CityD` in the `West` region. - Retrieve the sales value for the representative with `Rep_ID` 202 in `CityC` within the `East` region. 3. **Manipulate and Slice Data**: - Slice the dataset to include only the representatives from `North` and `West` regions, but exclude the `Rep_ID` 302. - Calculate the sum of sales for each city across all regions. 4. **Align and Reindex Data**: - Reindex the data to align it to include an additional representative with `Rep_ID` 104 in `CityA` of `North` region with NaN sales value. - Reorder the levels of the MultiIndex such that `Rep_ID` comes before `City`. **Expected Output Formats**: 1. **MultiIndex DataFrame**: A DataFrame with a MultiIndex. 2. **Selected Data**: A Series or DataFrame containing the selected data. 3. **Manipulated Data**: - Sliced DataFrame excluding specific regions and `Rep_ID`. - Series containing the sum of sales for each city. 4. **Reindexed and Reordered Data**: - A reindexed MultiIndex DataFrame including the new representative. - A reordered MultiIndex DataFrame where `Rep_ID` is the first level. **Constraints**: - Ensure no additional rows or columns are added unless specified. - Handle missing data appropriately where necessary. ```python import pandas as pd # Provided Dataset data = { \'Region\': [\'North\', \'North\', \'North\', \'East\', \'East\', \'West\', \'West\', \'West\'], \'City\': [\'CityA\', \'CityA\', \'CityB\', \'CityC\', \'CityC\', \'CityD\', \'CityD\', \'CityD\'], \'Rep_ID\': [101, 102, 103, 201, 202, 301, 302, 303], \'Sales\': [1234.5, 2345.6, 3456.7, 4567.8, 5678.9, 6789.0, 7890.1, 8901.2] } df = pd.DataFrame(data) # Task 1: Create MultiIndex DataFrame # [Your Code Here] # Task 2: Select Data # [Your Code Here] # Task 3: Manipulate and Slice Data # [Your Code Here] # Task 4: Align and Reindex Data # [Your Code Here] ```","solution":"import pandas as pd # Provided Dataset data = { \'Region\': [\'North\', \'North\', \'North\', \'East\', \'East\', \'West\', \'West\', \'West\'], \'City\': [\'CityA\', \'CityA\', \'CityB\', \'CityC\', \'CityC\', \'CityD\', \'CityD\', \'CityD\'], \'Rep_ID\': [101, 102, 103, 201, 202, 301, 302, 303], \'Sales\': [1234.5, 2345.6, 3456.7, 4567.8, 5678.9, 6789.0, 7890.1, 8901.2] } df = pd.DataFrame(data) # Task 1: Create a MultiIndex DataFrame df = df.set_index([\'Region\', \'City\', \'Rep_ID\']) # Task 2: Select Data # Select the sales data for all representatives in CityD in the West region sales_cityd_west = df.loc[(\'West\', \'CityD\')] # Retrieve the sales value for the representative with Rep_ID 202 in CityC within the East region sales_rep_202_east_cityc = df.loc[(\'East\', \'CityC\', 202), \'Sales\'] # Task 3: Manipulate and Slice Data # Slice the dataset to include only the representatives from North and West regions, but exclude Rep_ID 302 filtered_df = df.loc[[\'North\', \'West\']].drop(index=(\'West\', \'CityD\', 302)) # Calculate the sum of sales for each city across all regions city_sales_sum = df.groupby(level=[\'City\']).sum()[\'Sales\'] # Task 4: Align and Reindex Data # Reindex the data to include an additional representative with Rep_ID 104 in CityA of North region with NaN sales value reindexed_df = df.reindex(index=df.index.union([(\'North\', \'CityA\', 104)])) # Reorder the levels of the MultiIndex such that Rep_ID comes before City reordered_df = df.reorder_levels([\'Region\', \'Rep_ID\', \'City\']) # Outputs for checking (not part of the return, only for clarity) sales_cityd_west, sales_rep_202_east_cityc, filtered_df, city_sales_sum, reindexed_df, reordered_df"},{"question":"# CSV Data Manipulation with Custom Dialect You are given a CSV file `input.csv` that contains information about various products. The file uses a custom delimiter `;` and quotes fields that contain special characters using the `|` character. The first row of the file contains headers: `ProductID`, `ProductName`, `Category`, `Price`, `Quantity`. Your task is to write a Python function `process_csv(input_file: str, output_file: str, threshold: float) -> None` that reads the data from `input.csv`, filters out products with a price below the specified `threshold`, and writes the remaining products to `output.csv` using a different custom dialect with the following specifications: - Delimiter: `,` - Quote character: `\\"` - Quote all fields (use `csv.QUOTE_ALL`) Parameters - `input_file` (str): The path to the input CSV file. - `output_file` (str): The path to the output CSV file. - `threshold` (float): The price threshold for filtering products. Constraints - You must use the `csv.DictReader` class to read the input file and the `csv.DictWriter` class to write the output file. - Ensure that the output CSV file contains the same headers as the input file. - You should handle exceptions that may arise from file operations. Example Usage ```python # Assume input.csv contains the following data: # ProductID;ProductName;Category;Price;Quantity # 101;Apple;Fruit;1.20;100 # 102;Banana;Fruit;0.50;200 # 103;Carrot;Vegetable;0.70;150 # 104;Orange;Fruit;1.50;80 process_csv(\\"input.csv\\", \\"output.csv\\", 1.00) # output.csv should contain: # \\"ProductID\\",\\"ProductName\\",\\"Category\\",\\"Price\\",\\"Quantity\\" # \\"101\\",\\"Apple\\",\\"Fruit\\",\\"1.20\\",\\"100\\" # \\"104\\",\\"Orange\\",\\"Fruit\\",\\"1.50\\",\\"80\\" ``` Requirements 1. Define a custom dialect for reading using `csv.register_dialect`. 2. Filter the products based on the specified price threshold. 3. Define another custom dialect for writing the output file in the required format. 4. Properly handle any exceptions related to file operations and CSV parsing. Note - You may assume that the input file path and specified threshold will be valid and the file will contain data in the described format. - Remember to unregister any custom dialects you create when they are no longer needed. Good luck!","solution":"import csv def process_csv(input_file: str, output_file: str, threshold: float) -> None: try: # Define the input custom dialect csv.register_dialect(\'input_dialect\', delimiter=\';\', quotechar=\'|\', quoting=csv.QUOTE_MINIMAL) # Define the output custom dialect csv.register_dialect(\'output_dialect\', delimiter=\',\', quotechar=\'\\"\', quoting=csv.QUOTE_ALL) with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile, dialect=\'input_dialect\') fieldnames = reader.fieldnames with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'output_dialect\') writer.writeheader() for row in reader: if float(row[\'Price\']) >= threshold: writer.writerow(row) except Exception as e: print(f\\"An error occurred: {e}\\") finally: csv.unregister_dialect(\'input_dialect\') csv.unregister_dialect(\'output_dialect\')"},{"question":"# Advanced Garbage Collection Management in Python **Objective**: Write a Python program that utilizes the `gc` module to manage and inspect garbage collection in a series of operations. The goal is to understand and manipulate garbage collection to improve performance and debug memory-related issues. **Task**: 1. **Enable/Disable Collection**: Initially disable the automatic garbage collection. After performing some operations (outlined below), enable it again. 2. **Manual Garbage Collection**: Perform manual garbage collection after certain operations and collect only specific generations. 3. **Inspect Objects**: List and print all tracked objects before and after manual garbage collections. Ensure to specify the generation of objects inspected. 4. **Collection Statistics**: Monitor and print garbage collection statistics before and after operations. 5. **Debugging Setup**: Set up debugging flags to track uncollectable objects, and print details on these objects if any are found. **Operations**: 1. Create a set of objects, ensuring some of them form reference cycles. 2. Manually trigger garbage collections at different stages of object creation and deletion. 3. Monitor changes in objects tracked by the garbage collector and collection statistics. **Input**: - No direct input; the program will internally manage object creation, deletion, and garbage collection. **Output**: - Print statements for: - Status of garbage collection (enabled/disabled). - Details of manually triggered garbage collections. - Lists of objects tracked by the garbage collector before and after collections (with their generation). - Collection statistics before and after collections. - Details of uncollectable objects found. **Constraints**: - Ensure that the manual collections specify different generations (0, 1, and 2). - Operate on a minimum of two manual garbage collection cycles. - Object creation must include reference cycles. **Performance**: - Efficiently handle and print details of the garbage collection process without causing significant delays. **Example Skeleton**: ```python import gc def main(): # Initially disable garbage collection gc.disable() print(\\"Garbage collection enabled:\\", gc.isenabled()) # Create objects with reference cycles class Cycle: def __init__(self): self.ref = self cycles = [Cycle() for _ in range(10)] # Print objects tracked by garbage collector (Generation 0) print(\\"Tracked objects (gen 0):\\", gc.get_objects(generation=0)) # Manual garbage collect for generation 0 gc.collect(generation=0) print(\\"After manual collect (gen 0):\\", gc.get_objects(generation=0)) # Enable garbage collection gc.enable() print(\\"Garbage collection enabled:\\", gc.isenabled()) # Manually collect complete garbage gc.collect() print(\\"After complete manual collect:\\", gc.get_objects()) # Set debugging options to track uncollectable objects gc.set_debug(gc.DEBUG_UNCOLLECTABLE) # Create more objects to induce garbage more_cycles = [Cycle() for _ in range(5)] # Force another manual collection gc.collect(generation=2) print(\\"After manual collect (gen 2):\\") print(\\"Uncollectable objects:\\", gc.garbage) # Print garbage collection statistics stats = gc.get_stats() print(\\"Garbage collection statistics:\\", stats) if __name__ == \\"__main__\\": main() ``` **Note**: - You need to run this program in a Python environment that supports the gc module. - Ensure to handle any exceptions or warnings related to garbage collection operations.","solution":"import gc def main(): # Initially disable garbage collection gc.disable() print(\\"Garbage collection enabled:\\", gc.isenabled()) # Create objects with reference cycles class Cycle: def __init__(self): self.ref = self cycles = [Cycle() for _ in range(10)] # Print objects tracked by garbage collector (Generation 0) print(\\"Tracked objects (gen 0):\\", len(gc.get_objects(generation=0))) # Manual garbage collect for generation 0 gc.collect(generation=0) print(\\"After manual collect (gen 0):\\", len(gc.get_objects(generation=0))) # Enable garbage collection gc.enable() print(\\"Garbage collection enabled:\\", gc.isenabled()) # Manually collect complete garbage gc.collect() print(\\"After complete manual collect:\\", len(gc.get_objects())) # Set debugging options to track uncollectable objects gc.set_debug(gc.DEBUG_UNCOLLECTABLE) # Create more objects to induce garbage more_cycles = [Cycle() for _ in range(5)] # Force another manual collection gc.collect(generation=2) print(\\"After manual collect (gen 2):\\") print(\\"Uncollectable objects:\\", len(gc.garbage)) # Print garbage collection statistics stats = gc.get_stats() print(\\"Garbage collection statistics:\\", stats) if __name__ == \\"__main__\\": main()"},{"question":"Problem Statement You are given a neural network that utilizes Batch Normalization. Your task is to modify the network to ensure it works correctly with batched inputs using the `vmap` function from the `functorch` library. Implement the function `patch_batch_norm` that replaces BatchNorm layers with GroupNorm layers throughout the network. # Function Signature ```python def patch_batch_norm(model: torch.nn.Module, num_groups: int = 1) -> torch.nn.Module: pass ``` # Input - `model` (torch.nn.Module): A PyTorch neural network model that uses Batch Normalization. - `num_groups` (int): The number of groups to use for GroupNorm. Default is 1 (each channel is treated separately). # Output - Returns a new model (torch.nn.Module) where all BatchNorm layers have been replaced with GroupNorm layers. # Requirements 1. Iterate through all layers of the model and replace instances of `torch.nn.BatchNorm2d` with `torch.nn.GroupNorm`. 2. Ensure that when replacing, the number of channels (`C`) from the BatchNorm layer is maintained, and the number of groups (`G`) provided in the function is respected such that `C % G == 0`. 3. Keep the other parameters of BatchNorm (like affine and track_running_stats) similar in GroupNorm if applicable. # Example ```python import torch import torch.nn as nn import torch.nn.functional as F # Example neural network with BatchNorm2d class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3) self.bn2 = nn.BatchNorm2d(32) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) return x model = SimpleNet() # Patching BatchNorm with GroupNorm patched_model = patch_batch_norm(model, num_groups=4) # Verifying the patch for layer in patched_model.modules(): if isinstance(layer, nn.BatchNorm2d): print(\\"Patch unsuccessful: BatchNorm2d still present\\") elif isinstance(layer, nn.GroupNorm): print(\\"GroupNorm layer found with num_groups =\\", layer.num_groups) ``` **Note:** You can assume that the input model shall consist only of Convolutional layers followed by BatchNorm2d layers. # Constraints - `model` will only use `nn.Conv2d`, `nn.BatchNorm2d`, and functional layers. - The function should handle arbitrary nesting of nn.Sequential or nn.ModuleList. - `num_groups` should be a positive integer and a divisor of the number of channels in all BatchNorm layers.","solution":"import torch import torch.nn as nn def patch_batch_norm(model: torch.nn.Module, num_groups: int = 1) -> torch.nn.Module: Replaces all instances of torch.nn.BatchNorm2d in a model with torch.nn.GroupNorm. The number of groups in GroupNorm is given by the num_groups parameter. def _replace_bn_with_gn(layer): if isinstance(layer, nn.BatchNorm2d): num_channels = layer.num_features return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels, affine=layer.affine) for name, child in layer.named_children(): new_child = _replace_bn_with_gn(child) setattr(layer, name, new_child) return layer model = _replace_bn_with_gn(model) return model"},{"question":"CUDA Device and Memory Manager **Objective:** Implement a CUDA management utility using PyTorch that dynamically allocates memory to the optimal GPU device based on its current memory usage and synchronizes multiple streams for parallel computation. Problem Statement You are required to create a class `CudaManager` in PyTorch that manages CUDA devices, memory, and streams for parallel computation on GPUs. Your class should: 1. **Detect Available Devices**: Identify all available CUDA devices. 2. **Optimal Device Allocation**: Dynamically allocate memory to the CUDA device with the most available free memory. 3. **Stream Synchronization**: Manage multiple streams for parallel tasks and synchronize them efficiently. # Class Definition ```python import torch import torch.cuda class CudaManager: def __init__(self): Initialize the CudaManager by detecting all available CUDA devices. self.devices = self._detect_devices() self.streams = {} def _detect_devices(self): Detect and return a list of available CUDA devices. Returns: List[int]: List of device IDs. pass def get_optimal_device(self): Find and return the CUDA device with the most free memory. Returns: int: Optimal device id. pass def allocate_memory(self, size): Allocate memory on the optimal device and return the tensor. Args: size (int): The size of the tensor to be allocated. Returns: torch.Tensor: Allocated tensor on the optimal device. pass def create_stream(self, device_id): Create and return a new CUDA stream on the specified device. Args: device_id (int): The CUDA device ID. Returns: torch.cuda.Stream: New CUDA stream. pass def synchronize_streams(self): Synchronize all created CUDA streams. pass ``` # Method Details 1. **`_detect_devices(self)`**: This private method should return a list of all available CUDA devices by querying `torch.cuda.device_count()`. 2. **`get_optimal_device(self)`**: This method should find and return the device ID with the most available free memory. Use `torch.cuda.memory_reserved(device)` and `torch.cuda.memory_allocated(device)` to determine the free memory on a device. 3. **`allocate_memory(self, size)`**: This method should allocate a tensor of the given size on the optimal device. Use the `get_optimal_device` method to determine which device to use. Return the allocated tensor. 4. **`create_stream(self, device_id)`**: This method should create a new CUDA stream on the specified device and store it in a dictionary using device ID as the key. Return the created stream. 5. **`synchronize_streams(self)`**: This method should synchronize all created CUDA streams to ensure all parallel tasks are completed. # Constraints - You may assume that the initialized CUDA devices have sufficient memory for the operations. - Make use of PyTorch\'s CUDA API (`torch.cuda` and relevant submodules). # Example Usage ```python manager = CudaManager() # Detect available devices print(manager.devices) # Get optimal device optimal_device = manager.get_optimal_device() print(f\\"Optimal Device ID: {optimal_device}\\") # Allocate memory tensor = manager.allocate_memory(1024) print(tensor) # Create streams stream1 = manager.create_stream(optimal_device) stream2 = manager.create_stream(optimal_device) # Synchronize streams manager.synchronize_streams() ``` **Performance Requirements:** Ensure your implementation can handle typical sizes of data tensors used in machine learning applications efficiently. Be mindful of synchronization overhead and device query efficiency.","solution":"import torch import torch.cuda class CudaManager: def __init__(self): Initialize the CudaManager by detecting all available CUDA devices. self.devices = self._detect_devices() self.streams = {} def _detect_devices(self): Detect and return a list of available CUDA devices. Returns: List[int]: List of device IDs. return list(range(torch.cuda.device_count())) def get_optimal_device(self): Find and return the CUDA device with the most free memory. Returns: int: Optimal device id. max_free_memory = -1 optimal_device = -1 for device in self.devices: total_memory = torch.cuda.get_device_properties(device).total_memory reserved_memory = torch.cuda.memory_reserved(device) allocated_memory = torch.cuda.memory_allocated(device) free_memory = total_memory - reserved_memory - allocated_memory if free_memory > max_free_memory: max_free_memory = free_memory optimal_device = device return optimal_device def allocate_memory(self, size): Allocate memory on the optimal device and return the tensor. Args: size (int): The size of the tensor to be allocated. Returns: torch.Tensor: Allocated tensor on the optimal device. optimal_device = self.get_optimal_device() with torch.cuda.device(optimal_device): return torch.empty(size, device=\'cuda\') def create_stream(self, device_id): Create and return a new CUDA stream on the specified device. Args: device_id (int): The CUDA device ID. Returns: torch.cuda.Stream: New CUDA stream. if device_id not in self.streams: self.streams[device_id] = [] new_stream = torch.cuda.Stream(device_id) self.streams[device_id].append(new_stream) return new_stream def synchronize_streams(self): Synchronize all created CUDA streams. for device_id in self.streams: torch.cuda.synchronize(device_id) for stream in self.streams[device_id]: stream.synchronize()"},{"question":"Advanced Usage of plistlib Objectives: - Demonstrate understanding and proficiency in using the `plistlib` module. - Validate handling of different data types and plist file formats. - Implement proper error handling and manage file operations. Problem Statement: You are provided with a list of dictionaries representing various configurations that need to be stored in `.plist` format. Each dictionary may contain nested structures with various data types such as strings, integers, floats, booleans, lists, dictionaries, `bytes`, `bytearray`, and `datetime` objects. Your task is to write a function `save_configs_to_plist(configs, file_path, fmt=\'FMT_XML\')` that takes in this list and writes it to a specified plist file. Additionally, write a function `read_configs_from_plist(file_path, fmt=\'FMT_XML\')` that reads the plist file and returns the list of dictionaries. Input: 1. `configs`: A list of dictionaries containing configuration data. 2. `file_path`: The file path where the plist file should be saved or read from. 3. `fmt`: Format of the plist file, either \'FMT_XML\' or \'FMT_BINARY\'. Default is \'FMT_XML\'. Output: 1. `save_configs_to_plist` should write the configurations to the specified plist file and return `None`. 2. `read_configs_from_plist` should read the plist file and return the list of dictionaries. Constraints: - Ensure the `configs` list and its nested dictionaries have string keys only. - Handle unsupported data types within dictionaries by raising a `TypeError`. - The function should handle IO errors and raise appropriate exceptions with meaningful messages. Example: ```python import datetime configs = [ { \\"name\\": \\"config1\\", \\"version\\": 1, \\"enabled\\": True, \\"parameters\\": [1, 2.5, \\"test\\", [1, 2, 3]], \\"data\\": b\\"binarydata\\", \\"timestamp\\": datetime.datetime.now(), }, { \\"name\\": \\"config2\\", \\"version\\": 2, \\"enabled\\": False, \\"parameters\\": [3, 4.5, \\"sample\\", [4, 5, 6]], \\"data\\": b\\"morebinarydata\\", \\"timestamp\\": datetime.datetime.now(), }, ] file_path = \\"configs.plist\\" # Save configurations to plist file save_configs_to_plist(configs, file_path, fmt=\'FMT_XML\') # Read configurations from plist file read_configs = read_configs_from_plist(file_path, fmt=\'FMT_XML\') print(read_configs) ``` Requirements: 1. Implement `save_configs_to_plist` function. 2. Implement `read_configs_from_plist` function. 3. Ensure proper error handling. 4. Write test cases to validate your implementation. Notes: - You may use the `plistlib` module directly in your implementation. - The datetime format in plist should be handled appropriately. - Ensure all integer values are within valid range to avoid `OverflowError`.","solution":"import plistlib import datetime def save_configs_to_plist(configs, file_path, fmt=\'FMT_XML\'): Save a list of configurations to a plist file. try: valid_formats = [\'FMT_XML\', \'FMT_BINARY\'] if fmt not in valid_formats: raise ValueError(\\"Format must be either \'FMT_XML\' or \'FMT_BINARY\'\\") valid_types = (str, int, float, bool, list, dict, bytes, bytearray, datetime.datetime) for config in configs: if not isinstance(config, dict): raise TypeError(\\"Each configuration must be a dictionary.\\") for key, value in config.items(): if not isinstance(key, str): raise TypeError(\\"All keys must be strings.\\") if not isinstance(value, valid_types): raise TypeError(f\\"Value of key \'{key}\' is of unsupported type \'{type(value)}\'.\\") fmt = getattr(plistlib, fmt) with open(file_path, \'wb\') as fp: plistlib.dump(configs, fp, fmt=fmt) except Exception as e: raise e def read_configs_from_plist(file_path, fmt=\'FMT_XML\'): Reads a plist file and returns a list of configurations. try: valid_formats = [\'FMT_XML\', \'FMT_BINARY\'] if fmt not in valid_formats: raise ValueError(\\"Format must be either \'FMT_XML\' or \'FMT_BINARY\'\\") fmt = getattr(plistlib, fmt) with open(file_path, \'rb\') as fp: configs = plistlib.load(fp, fmt=fmt) return configs except Exception as e: raise e"},{"question":"# Custom Copy Operations You have been provided with a `Person` class that represents a person with a name and a list of friends. You need to implement custom shallow and deep copy operations for this class using the `copy` module. Specifically, you must: 1. Implement the `__copy__()` method to create a shallow copy of the `Person` object. 2. Implement the `__deepcopy__()` method to create a deep copy of the `Person` object. Here is the definition of the `Person` class for reference: ```python class Person: def __init__(self, name, friends=None): self.name = name self.friends = friends if friends is not None else [] def __repr__(self): return f\\"Person(name={self.name}, friends={[friend.name for friend in self.friends]})\\" ``` # Requirements 1. **Shallow Copy**: - The shallow copy should create a new `Person` object with the same `name`. - The `friends` attribute should be a reference to the same list of friends as the original object. 2. **Deep Copy**: - The deep copy should create a new `Person` object with the same `name`. - The `friends` attribute should be a deep copy of the original list of friends, ensuring that changes to the friends list in one object do not affect the copy. # Example ```python from copy import copy, deepcopy # Create a person with friends alice = Person(\\"Alice\\") bob = Person(\\"Bob\\") carol = Person(\\"Carol\\") alice.friends = [bob, carol] # Perform shallow and deep copies alice_shallow = copy(alice) alice_deep = deepcopy(alice) # Add a new friend to the original dave = Person(\\"Dave\\") alice.friends.append(dave) print(alice) # Output: Person(name=Alice, friends=[\'Bob\', \'Carol\', \'Dave\']) print(alice_shallow) # Output: Person(name=Alice, friends=[\'Bob\', \'Carol\', \'Dave\']) (same friends list as alice) print(alice_deep) # Output: Person(name=Alice, friends=[\'Bob\', \'Carol\']) (deep copy, separate friends list) ``` # Constraints - You must use the `copy` module for the copy operations. - The `__copy__()` method must not take any arguments. - The `__deepcopy__()` method must take exactly two arguments: `self` and the `memo` dictionary. # Implementation Implement the `__copy__()` and `__deepcopy__()` methods in the `Person` class below: ```python import copy class Person: def __init__(self, name, friends=None): self.name = name self.friends = friends if friends is not None else [] def __copy__(self): # Implement shallow copy operation here pass def __deepcopy__(self, memo): # Implement deep copy operation here pass def __repr__(self): return f\\"Person(name={self.name}, friends={[friend.name for friend in self.friends]})\\" ``` Ensure that your implementation meets the requirements and passes the example test case.","solution":"import copy class Person: def __init__(self, name, friends=None): self.name = name self.friends = friends if friends is not None else [] def __copy__(self): new_person = Person(self.name, self.friends) return new_person def __deepcopy__(self, memo): new_person = Person(self.name) new_person.friends = [copy.deepcopy(friend, memo) for friend in self.friends] return new_person def __repr__(self): return f\\"Person(name={self.name}, friends={[friend.name for friend in self.friends]})\\""},{"question":"**Serialization Challenge in Python** In this coding challenge, you will implement a mini library for serializing and deserializing Python objects to and from files, emulating the marshalling capabilities described above. You should demonstrate your understanding of file I/O, error handling, and working with binary data in Python. 1. **Function**: `write_long_to_file(value: int, file_path: str) -> None` - **Description**: Serialize a 32-bit integer `value` and write it to a file specified by `file_path`. - **Input**: - `value`: (int) A 32-bit integer to be serialized. - `file_path`: (str) Path to the file where the integer will be written. - **Constraints**: - The file should be opened in binary mode. - **Output**: None. 2. **Function**: `write_object_to_file(obj: object, file_path: str) -> None` - **Description**: Serialize a Python object `obj` and write it to a file specified by `file_path`. - **Input**: - `obj`: (object) Any Python object to be serialized. - `file_path`: (str) Path to the file where the object will be written. - **Constraints**: - The file should be opened in binary mode. - **Output**: None. 3. **Function**: `read_long_from_file(file_path: str) -> int` - **Description**: Read a 32-bit integer from a file specified by `file_path`. - **Input**: - `file_path`: (str) Path to the file to read the integer from. - **Constraints**: - The file should be opened in binary mode. - **Output**: (int) The deserialized 32-bit integer. 4. **Function**: `read_object_from_file(file_path: str) -> object` - **Description**: Read and deserialize a Python object from a file specified by `file_path`. - **Input**: - `file_path`: (str) Path to the file to read the object from. - **Constraints**: - The file should be opened in binary mode. - Handle errors gracefully by raising appropriate Python exceptions (`EOFError`, `ValueError`, or `TypeError`) if unmarshalling fails. - **Output**: (object) The deserialized Python object. # Example Usage: ```python # Assume the implementation of the required functions # Serialize integers and objects write_long_to_file(123456789, \'data_long.bin\') write_object_to_file({\'key\': \'value\'}, \'data_obj.bin\') # Deserialize integers and objects int_value = read_long_from_file(\'data_long.bin\') print(int_value) # Output should be 123456789 obj_value = read_object_from_file(\'data_obj.bin\') print(obj_value) # Output should be {\'key\': \'value\'} ``` **Your task is to implement these four functions. Ensure that you handle the file operations and possible errors appropriately.**","solution":"import struct import pickle def write_long_to_file(value: int, file_path: str) -> None: Serialize a 32-bit integer `value` and write it to a file specified by `file_path`. with open(file_path, \'wb\') as file: file.write(struct.pack(\'i\', value)) def write_object_to_file(obj: object, file_path: str) -> None: Serialize a Python object `obj` and write it to a file specified by `file_path`. with open(file_path, \'wb\') as file: pickle.dump(obj, file) def read_long_from_file(file_path: str) -> int: Read a 32-bit integer from a file specified by `file_path`. with open(file_path, \'rb\') as file: return struct.unpack(\'i\', file.read(4))[0] def read_object_from_file(file_path: str) -> object: Read and deserialize a Python object from a file specified by `file_path`. with open(file_path, \'rb\') as file: return pickle.load(file)"},{"question":"# Objective: Implement a new Python extension type `EnhancedList` using C. This type should inherit from the built-in Python `list` type and add two new features: 1. A custom attribute `multiplier`. 2. A method `multiply_items` to multiply each item in the list by the `multiplier`. # Requirements: - The `EnhancedList` type must inherit all functionalities of the Python `list` type. - The `EnhancedList` should introduce a new attribute `multiplier`, which is an integer and has a default value of 1. - The `EnhancedList` type must implement a method `multiply_items` that multiplies each element in the list by the value of `multiplier` and returns a new list with the multiplied values. - Proper memory management should be ensured, including appropriate handling of reference counts and garbage collection. - The `multiplier` attribute should support getter and setter methods to control its value and ensure it is always an integer. # Input and Output: - The `EnhancedList` type should be initialized as follows: `EnhancedList([initial_list], multiplier=<value>)` - The `multiply_items` method should be called without arguments: `enhanced_list_instance.multiply_items()` - The method should return a new list with values multiplied by the current `multiplier`. # Constraints: - Make sure the `multiplier` is always a positive integer. - Manage memory and references correctly to avoid leaks and crashes. # Example Usage: ```python # Assume EnhancedList is correctly implemented and available from enhanced_list_extension import EnhancedList # Create an instance of EnhancedList with multiplier 2 el = EnhancedList([1, 2, 3], multiplier=2) # Call multiply_items method result = el.multiply_items() print(result) # Output should be [2, 4, 6] # Change multiplier and call multiply_items again el.multiplier = 3 result = el.multiply_items() print(result) # Output should be [3, 6, 9] ``` # Implementation Guidance: 1. Start by defining the `EnhancedListObject` struct to store the `multiplier` attribute and inherit from `PyListObject`. 2. Implement the `tp_new`, `tp_init`, and `tp_dealloc` methods with proper memory management. 3. Define getter and setter functions for the `multiplier` attribute to ensure it is always a positive integer. 4. Implement the `multiply_items` method to iterate over the list and multiply each element by `multiplier`. 5. Ensure all functions handle errors and reference counting appropriately. 6. Use `PyType_Ready` and `PyModule_Create` to integrate the new type into a Python module.","solution":"class EnhancedList(list): def __init__(self, initial_list=None, multiplier=1): if initial_list is None: initial_list = [] super().__init__(initial_list) self._multiplier = multiplier @property def multiplier(self): return self._multiplier @multiplier.setter def multiplier(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(\\"Multiplier must be a positive integer.\\") self._multiplier = value def multiply_items(self): return [item * self._multiplier for item in self]"},{"question":"**Problem Statement:** You are provided with a dataset in a CSV file format that contains information about sales transactions. Your task is to write a Python function using pandas that reads the CSV file, processes the data, and performs certain analytical operations. **Task:** Write a function named `process_sales_data(filepath: str) -> Tuple[pd.DataFrame, Dict[str, float]]` which: 1. Reads the CSV file located at the given `filepath` into a Pandas DataFrame. The CSV contains the following columns: - `transaction_id`: A unique identifier for each transaction. - `product_id`: A unique identifier for each product. - `quantity_sold`: The number of units sold. - `price_per_unit`: The price of one unit of the product. 2. Adds a new column named `total_sales` which is calculated by multiplying `quantity_sold` by `price_per_unit`. 3. Returns two items: - A DataFrame with the top 5 products based on `total_sales`. This DataFrame should include `product_id` and `total_sales` columns only, sorted by `total_sales` in descending order. - A dictionary with summary statistics for the `total_sales` column including the mean, median, and standard deviation. The dictionary should have the following keys: - `\'mean\'` - `\'median\'` - `\'std\'` **Constraints:** - You can assume the CSV file is properly formatted and does not contain missing values. - Use pandas library functions to accomplish the task. - Your solution should handle the operations efficiently, catering to large datasets. **Example:** ```python # Example of a CSV file content # transaction_id,product_id,quantity_sold,price_per_unit # 1,101,2,15.0 # 2,101,3,15.0 # 3,102,1,20.0 # 4,103,5,10.0 # 5,104,2,25.0 # 6,105,1,50.0 # 7,101,1,15.0 # Given the example dataset above, the function should produce an output similar to the following: top_products_df, sales_stats = process_sales_data(\'path/to/sales_data.csv\') # top_products_df should be a DataFrame: # product_id total_sales # 0 101 90.0 # 1 103 50.0 # 2 104 50.0 # 3 102 20.0 # 4 105 50.0 # sales_stats should be a dictionary: # { # \\"mean\\": 42.857142857142854, # \\"median\\": 30.0, # \\"std\\": 30.138304553794304 # } ```","solution":"import pandas as pd import numpy as np from typing import Tuple, Dict def process_sales_data(filepath: str) -> Tuple[pd.DataFrame, Dict[str, float]]: # Read the CSV file df = pd.read_csv(filepath) # Add a new column \'total_sales\' df[\'total_sales\'] = df[\'quantity_sold\'] * df[\'price_per_unit\'] # Calculate the top 5 products based on \'total_sales\' top_products = df.groupby(\'product_id\')[\'total_sales\'].sum().reset_index() top_products = top_products.sort_values(by=\'total_sales\', ascending=False).head(5) # Calculate summary statistics for \'total_sales\' total_sales_stats = { \'mean\': df[\'total_sales\'].mean(), \'median\': df[\'total_sales\'].median(), \'std\': df[\'total_sales\'].std() } return top_products, total_sales_stats"},{"question":"# MLPClassifier Implementation Task Objective Given the following dataset, implement a `MLPClassifier` using scikit-learn\'s `neural_network` module. Your goal is to: 1. Preprocess the data by scaling it. 2. Initialize the `MLPClassifier` with specific hyperparameters. 3. Train the model on the provided data. 4. Evaluate the model\'s accuracy on a separate test set. Dataset The data consists of two arrays: - `X_train`: A 2D numpy array of shape `(n_samples, n_features)` representing the training samples. - `y_train`: A 1D numpy array of shape `(n_samples,)` representing the training labels. Here is a sample dataset you can use: ```python X_train = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.], [0.5, 0.5], [0.5, 0.8], [0.3, 0.2], [0.8, 0.3] ] y_train = [0, 0, 0, 1, 1, 1, 0, 1] ``` For testing purposes, you can assume the following test set: ```python X_test = [ [0.2, 0.2], [0.6, 0.9], [1., 0.5], [0., 0.] ] y_test = [0, 1, 1, 0] ``` Instructions 1. **Data Preprocessing**: - Standardize the feature values of `X_train` and `X_test` using `StandardScaler`. 2. **Model Initialization**: - Initialize an `MLPClassifier` with the following hyperparameters: - `solver=\'adam\'` - `alpha=1e-4` - `hidden_layer_sizes=(10, 5)` - `random_state=1` - `max_iter=1000` 3. **Model Training**: - Fit the `MLPClassifier` to the training data. 4. **Model Evaluation**: - Predict the class labels for `X_test` and calculate the accuracy of the model based on the true labels `y_test`. Output Your function should output the following: - The accuracy score of the model on the test data. Example Function Signature ```python import numpy as np from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score def train_and_evaluate_mlp(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> float: # Step-wise instructions to be followed. # 1. Data Preprocessing scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # 2. Model Initialization mlp = MLPClassifier( solver=\'adam\', alpha=1e-4, hidden_layer_sizes=(10, 5), random_state=1, max_iter=1000 ) # 3. Model Training mlp.fit(X_train_scaled, y_train) # 4. Model Evaluation y_pred = mlp.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage X_train = np.array([ [0., 0.], [0., 1.], [1., 0.], [1., 1.], [0.5, 0.5], [0.5, 0.8], [0.3, 0.2], [0.8, 0.3] ]) y_train = np.array([0, 0, 0, 1, 1, 1, 0, 1]) X_test = np.array([ [0.2, 0.2], [0.6, 0.9], [1., 0.5], [0., 0.] ]) y_test = np.array([0, 1, 1, 0]) accuracy = train_and_evaluate_mlp(X_train, y_train, X_test, y_test) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` Constraints - You must use scikit-learn\'s `MLPClassifier` and `StandardScaler` for this task. - Do not change the model\'s hyperparameters except for initialization and fit methods. - Ensure reproducibility by setting `random_state`.","solution":"import numpy as np from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score def train_and_evaluate_mlp(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> float: Trains an MLPClassifier on the given training data and evaluates its accuracy on the test data. Parameters: - X_train (np.ndarray): Training feature data. - y_train (np.ndarray): Training labels. - X_test (np.ndarray): Test feature data. - y_test (np.ndarray): Test labels. Returns: - float: Accuracy of the trained model on the test data. # Step 1: Data Preprocessing scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 2: Model Initialization mlp = MLPClassifier( solver=\'adam\', alpha=1e-4, hidden_layer_sizes=(10, 5), random_state=1, max_iter=1000 ) # Step 3: Model Training mlp.fit(X_train_scaled, y_train) # Step 4: Model Evaluation y_pred = mlp.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# File and Directory Manipulation Challenge **Objective:** Write a Python function named `organize_files_by_extension` that takes a single directory path as input and organizes all files within that directory (including subdirectories) into subdirectories based on their file extensions. Each subdirectory should be named after the respective file extension (e.g., `.txt`, `.pdf`). Files without an extension should be moved to a subdirectory named `no_extension`. **Function Signature:** ```python def organize_files_by_extension(directory_path: str) -> None: Organizes files in the given directory and its subdirectories into separate subdirectories based on file extensions. Parameters: - directory_path (str): The path of the directory to organize. Returns: - None: The function modifies the directory structure in place and does not return any value. pass ``` **Input:** - `directory_path`: A string representing the path to the directory that needs to be organized. This directory can contain nested subdirectories and files of various types. **Output:** - The function should not return any value. It should directly modify and organize the files within the given directory. **Constraints:** - The function should handle both absolute and relative paths. - Subdirectories recursively should also be organized. - If a subdirectory for a file extension already exists, files should be moved into it without creating a new one. **Example:** Given the following directory structure: ``` /example_directory file1.txt file2.pdf file3 subdir1 file4.docx file5.txt subdir2 subsubdir1 file6.md ``` After running `organize_files_by_extension(\'/example_directory\')`, the structure should be modified to: ``` /example_directory /.txt file1.txt file5.txt /.pdf file2.pdf /.docx file4.docx /.md file6.md /no_extension file3 /subdir1 /subdir2 /subsubdir1 ``` **Note:** Empty subdirectories should not be removed after the operation. **Tips and Hints:** - Use modules such as `os`, `shutil`, and `pathlib` to make the task easier. - Recursively iterate through directories to find all files. - Handle exceptions, such as permission errors, gracefully to ensure robustness. **Performance Requirements:** - The function should be able to handle directories with a large number of files efficiently. - Aim to minimize the number of filesystem operations for better performance. *Good luck!*","solution":"import os import shutil from pathlib import Path def organize_files_by_extension(directory_path: str) -> None: Organizes files in the given directory and its subdirectories into separate subdirectories based on file extensions. Parameters: - directory_path (str): The path of the directory to organize. Returns: - None: The function modifies the directory structure in place and does not return any value. directory_path = Path(directory_path) # Walk through all files in the directory and its subdirectories for root, _, files in os.walk(directory_path): for file in files: file_path = Path(root) / file if file_path.suffix: extension_dir = directory_path / (file_path.suffix) else: extension_dir = directory_path / \\"no_extension\\" # Create the directory if it does not exist if not extension_dir.exists(): extension_dir.mkdir() # Move the file into the respective directory shutil.move(str(file_path), str(extension_dir / file))"},{"question":"Out-of-Core Learning Implementation **Objective**: Implement an out-of-core learning pipeline to classify a large dataset using the scikit-learn library. --- Question You are provided with a large dataset that cannot fit into main memory at once. Your task is to implement an out-of-core learning pipeline using scikit-learn to classify this dataset. You are required to: 1. Stream instances from the dataset. 2. Extract features using a hashing vectorizer. 3. Incrementally train a `SGDClassifier` using mini-batches. The dataset consists of text documents and their corresponding labels. Assume that the data streaming function `stream_data` is already implemented and streams data in small chunks. # Requirements 1. **Data Streaming**: - Implement a generator function `stream_data(file_path, chunk_size)` that yields chunks of data from a file. 2. **Feature Extraction**: - Use `HashingVectorizer` from `sklearn.feature_extraction.text` for feature extraction. 3. **Incremental Learning**: - Use `SGDClassifier` from `sklearn.linear_model` for the classification task. - Utilize the `partial_fit` method for training with mini-batches. 4. **Performance Evaluation**: - Measure and output the misclassification error rate after processing every mini-batch. # Input and Output - **Input**: File path to the dataset (a text file where each line is a document followed by its label separated by a space). - **Output**: Misclassification error rate after each mini-batch. # Constraints and Considerations 1. **Chunk Size**: Read the data in mini-batches of size 1000. 2. **Feature Extraction**: Use the `HashingVectorizer` with default settings. 3. **Mini-Batch Training**: Use the `SGDClassifier` with default settings and ensure that it is updated incrementally with each mini-batch. 4. **Performance**: Evaluate performance metrics (misclassification error rate) periodically. # Example Function Signatures ```python from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer import numpy as np def stream_data(file_path, chunk_size=1000): Generator function to stream data from a file in mini-batches. with open(file_path, \'r\') as file: chunk = [] for line in file: if len(chunk) >= chunk_size: yield chunk chunk = [] chunk.append(line.strip()) if chunk: yield chunk def process_and_train(file_path): Processes the data and trains the classifier in mini-batches. vectorizer = HashingVectorizer() classifier = SGDClassifier() classes = np.array([0, 1]) for batch in stream_data(file_path): documents, labels = [], [] for instance in batch: label, doc = int(instance.split(\' \')[0]), \' \'.join(instance.split(\' \')[1:]) documents.append(doc) labels.append(label) X_batch = vectorizer.transform(documents) y_batch = np.array(labels) classifier.partial_fit(X_batch, y_batch, classes=classes) predictions = classifier.predict(X_batch) error_rate = np.mean(predictions != y_batch) print(f\'Misclassification error rate: {error_rate}\') process_and_train(\'path_to_your_file.txt\') ``` Ensure you thoroughly test your implementation with different chunk sizes and datasets to validate the incremental learning pipeline\'s functionality and performance.","solution":"from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer import numpy as np def stream_data(file_path, chunk_size=1000): Generator function to stream data from a file in mini-batches. with open(file_path, \'r\') as file: chunk = [] for line in file: if len(chunk) >= chunk_size: yield chunk chunk = [] chunk.append(line.strip()) if chunk: yield chunk def process_and_train(file_path, chunk_size=1000): Processes the data and trains the classifier in mini-batches. Outputs the misclassification error rate after each mini-batch. vectorizer = HashingVectorizer() classifier = SGDClassifier() classes = np.array([0, 1]) for batch in stream_data(file_path, chunk_size): documents, labels = [], [] for instance in batch: parts = instance.split(\' \', 1) label = int(parts[0]) doc = parts[1] documents.append(doc) labels.append(label) X_batch = vectorizer.transform(documents) y_batch = np.array(labels) classifier.partial_fit(X_batch, y_batch, classes=classes) predictions = classifier.predict(X_batch) error_rate = np.mean(predictions != y_batch) print(f\'Misclassification error rate: {error_rate}\')"},{"question":"**Coding Assessment Question:** # Debugging and Profiling in Python Background As a Python developer, you need to ensure that your code runs efficiently without errors and uses resources optimally. To achieve this, Python provides various libraries for debugging, profiling, monitoring memory usage, and measuring execution time. In this assessment, you will demonstrate your understanding of these tools by implementing functions to debug, profile, and trace a given Python code snippet. Problem Statement You are given a basic implementation of a function to find the nth Fibonacci number. The function is intentionally written in an inefficient manner. Your task is to: 1. Use the Python Debugger (`pdb`) to find out why the function is performing poorly. 2. Use the `cProfile` module to profile the function and identify performance bottlenecks. 3. Use the `timeit` module to measure the execution time of the optimized function. 4. Use the `tracemalloc` module to trace memory allocation during the function execution. Function Details 1. **Debugging with `pdb`**: - Locate and analyze the performance issue in the `fib` function. - Provide a code snippet using `pdb` commands to demonstrate the debugging process. 2. **Profiling with `cProfile`**: - Profile the `fib` function and print a summary of the performance bottlenecks. - Implement a wrapper function to run the profiler. 3. **Measuring Execution Time with `timeit`**: - Write an optimized version of the `fib` function. - Measure its execution time using `timeit` for n = 35. 4. **Tracing Memory Allocations with `tracemalloc`**: - Trace the memory allocations of the `fib` function. - Print the top memory allocation statistics. Constraints & Expectations - `n` is a non-negative integer. - Optimize the `fib` function using any algorithm (preferably dynamic programming). - Ensure to handle base cases in the `fib` function properly. Example Input and Output ```python # Original inefficient Fibonacci function def fib(n): if n <= 1: return n return fib(n-1) + fib(n-2) ``` **Expected Outputs** 1. Debugging: - Provide the `pdb` commands used to identify the performance issue. 2. Profiling: - Print `cProfile` output summary. 3. Optimized Fibonacci Function: - `fib_time(35)` Output: Execution time measured by `timeit`. 4. Memory tracing: - Print memory allocation statistics captured by `tracemalloc`. Sample Code Template ```python import pdb import cProfile import timeit import tracemalloc # Original inefficient Fibonacci function def fib(n): if n <= 1: return n return fib(n-1) + fib(n-2) # 1. Using pdb to debug def debug_fib(): pdb.set_trace() fib(10) # Example call to initialize debugging # 2. Using cProfile to profile def profile_fib(): profiler = cProfile.Profile() profiler.enable() fib(10) profiler.disable() profiler.print_stats() # 3. Optimized Fibonacci function def optimized_fib(n): fib_cache = [0, 1] for i in range(2, n + 1): fib_cache.append(fib_cache[-1] + fib_cache[-2]) return fib_cache[n] def fib_time(n): setup_code = f\'from __main__ import optimized_fib\' stmt = f\'optimized_fib({n})\' time = timeit.timeit(stmt, setup=setup_code, number=1) return time # 4. Tracing memory allocations with tracemalloc def trace_memory(): tracemalloc.start() fib(10) snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') for stat in top_stats[:10]: print(stat) # Example of running the functions if __name__ == \\"__main__\\": debug_fib() profile_fib() print(f\\"Execution time: {fib_time(35)}\\") trace_memory() ``` Submission - Provide the complete code with the required functions. - Include a brief explanation of findings during debugging and profiling. - Your code should run without errors and produce the expected output.","solution":"import pdb import cProfile import timeit import tracemalloc # Original inefficient Fibonacci function def fib(n): if n <= 1: return n return fib(n-1) + fib(n-2) # 1. Using pdb to debug def debug_fib(): pdb.set_trace() fib(10) # Example call to initialize debugging (use proper pdb commands to identify where the function is taking most time) # 2. Using cProfile to profile def profile_fib(): profiler = cProfile.Profile() profiler.enable() fib(10) profiler.disable() profiler.print_stats() # 3. Optimized Fibonacci function using Dynamic Programming def optimized_fib(n): if n <= 1: return n fib_cache = [0, 1] for i in range(2, n + 1): fib_cache.append(fib_cache[-1] + fib_cache[-2]) return fib_cache[n] def fib_time(n): setup_code = f\'from __main__ import optimized_fib\' stmt = f\'optimized_fib({n})\' time = timeit.timeit(stmt, setup=setup_code, number=1) return time # 4. Tracing memory allocations with tracemalloc def trace_memory(): tracemalloc.start() optimized_fib(10) snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') for stat in top_stats[:10]: print(stat) # Example of running the functions if __name__ == \\"__main__\\": print(\\"Debugging function (use pdb commands interactively):\\") # debug_fib() print(\\"nProfiling function:\\") profile_fib() print(\\"nMeasuring execution time for optimized function (n=35):\\") print(f\\"Execution time: {fib_time(35)} seconds\\") print(\\"nMemory allocation tracing for optimized function:\\") trace_memory()"},{"question":"# PyTorch Coding Assessment Question: Asynchronous Computation with `torch.futures` Objective In this assessment, you will demonstrate your understanding of asynchronous execution in PyTorch using `torch.futures`. You are required to write code that creates `Future` objects, performs distributed computation, and uses utility functions to manage these asynchronous tasks. Background Suppose you have a deep learning model that involves running multiple computations in parallel using different GPUs. You can use `torch.futures.Future` to manage the asynchronous execution of these computations. Task 1. Create a simple function `matrix_multiplication` that performs matrix multiplication on different GPUs. 2. Use the `torch.futures.Future` class to wrap the asynchronous execution of this function. 3. Implement another function `parallel_matrix_multiplication` that: - Initiates multiple matrix multiplications asynchronously. - Collects the results of these computations using `collect_all`. - Returns the sum of the resulting matrices. Implementation Details 1. **Function: `matrix_multiplication`** - **Input**: Two square matrices (`torch.Tensor`) and a device (`torch.device`). - **Output**: Resultant matrix (`torch.Tensor`) after multiplication. 2. **Function: `parallel_matrix_multiplication`** - **Inputs**: - A list of pairs of square matrices (`List[Tuple[torch.Tensor, torch.Tensor]]`). Each pair should be multiplied on a different device. - A list of devices (`List[torch.device]`) to run the computations. - **Output**: The sum (`torch.Tensor`) of all resultant matrices from the multiplications. - **Constraints**: - The lists should be of the same length. - Each multiplication should run on a different device. - Use `collect_all` to gather the results. Example ```python import torch from torch.futures import Future, collect_all def matrix_multiplication(matrix1, matrix2, device): # Your implementation here def parallel_matrix_multiplication(matrix_pairs, devices): # Your implementation here # Example usage: matrix1 = torch.randn(3, 3) matrix2 = torch.randn(3, 3) matrix_pairs = [(matrix1, matrix2), (matrix1, matrix2)] devices = [torch.device(\'cuda:0\'), torch.device(\'cuda:1\')] result = parallel_matrix_multiplication(matrix_pairs, devices) print(result) ``` Ensure you handle the asynchronous execution correctly and use the `collect_all` function to wait for all computations to finish before summing the results. Evaluation Criteria - **Correctness**: Your code should correctly perform asynchronous matrix multiplications and sum the resultant matrices. - **Efficiency**: Your implementation should efficiently use the GPUs and minimize unnecessary synchronization. - **Clarity**: Your code should be well-documented and easy to understand.","solution":"import torch from torch.futures import Future, collect_all def matrix_multiplication(matrix1, matrix2, device): Performs matrix multiplication on the specified device. Args: matrix1 (torch.Tensor): First matrix. matrix2 (torch.Tensor): Second matrix. device (torch.device): Device to run the computation on. Returns: torch.Tensor: Resultant matrix after multiplication. matrix1 = matrix1.to(device) matrix2 = matrix2.to(device) result = torch.matmul(matrix1, matrix2) return result.cpu() def parallel_matrix_multiplication(matrix_pairs, devices): Initiates multiple matrix multiplications asynchronously and returns the sum of the results. Args: matrix_pairs (List[Tuple[torch.Tensor, torch.Tensor]]): List of pairs of square matrices to be multiplied. devices (List[torch.device]): List of devices to run the computations on. Returns: torch.Tensor: The sum of all resultant matrices from the multiplications. futures = [] for (matrix1, matrix2), device in zip(matrix_pairs, devices): future = Future() result = matrix_multiplication(matrix1, matrix2, device) future.set_result(result) futures.append(future) completed_futures = collect_all(futures).wait() results = [f.value() for f in completed_futures] return sum(results)"},{"question":"You have been given the task to create a backup system for files using the `binhex` module in Python. Your system needs to support encoding files into the binhex format and decoding them back to their original form. You must implement the following functionality in a Python script: 1. **Function**: `backup_file(input_file: str, backup_file: str) -> None` - **Input**: - `input_file`: The name of the input binary file you wish to back up. - `backup_file`: The name of the backup file where the encoded binhex data will be stored. - **Output**: None - **Description**: This function should read the binary data from `input_file` and save it as a binhex file `backup_file` using the `binhex.binhex` function. Ensure to handle and report any exceptions that occur during the process. 2. **Function**: `restore_file(backup_file: str, restored_file: str) -> None` - **Input**: - `backup_file`: The name of the binhex file you want to decode. - `restored_file`: The name of the file where the decoded binary data will be stored. - **Output**: None - **Description**: This function should decode the binhex data from `backup_file` and save it as `restored_file` using the `binhex.hexbin` function. Ensure to handle and report any exceptions that occur during the process. 3. **Main Program**: Implement a command-line interface that allows users to choose between backing up or restoring files. Use command-line arguments to specify the operation and file names. **Example**: ```sh python backup_system.py backup inputfile.dat backupfile.hqx python backup_system.py restore backupfile.hqx restoredfile.dat ``` **Constraints**: - All filenames are valid strings. - The input file will always exist for the backup operation. - The backup file will always exist for the restore operation. **Performance**: Ensure that your program handles large files efficiently and does not consume excessive memory. Implement and test these functions with various input files to demonstrate their correctness.","solution":"import binhex import sys def backup_file(input_file, backup_file): Encodes the given input file to binhex format and stores it as backup_file. try: binhex.binhex(input_file, backup_file) except Exception as e: print(f\\"An error occurred while backing up the file: {e}\\") def restore_file(backup_file, restored_file): Decodes the given binhex backup file and stores the binary data as restored_file. try: binhex.hexbin(backup_file, restored_file) except Exception as e: print(f\\"An error occurred while restoring the file: {e}\\") def main(): Command-line interface for the backup and restore functionality. Arguments: operation: either \'backup\' or \'restore\'. input_file: the input file (for backup operation) or backup file (for restore operation). output_file: the output backup file (for backup operation) or restored file (for restore operation). if len(sys.argv) != 4: print(\\"Usage: python backup_system.py <backup|restore> <input_file> <output_file>\\") sys.exit(1) operation = sys.argv[1] input_file = sys.argv[2] output_file = sys.argv[3] if operation == \'backup\': backup_file(input_file, output_file) elif operation == \'restore\': restore_file(input_file, output_file) else: print(\\"Invalid operation. Use \'backup\' or \'restore\'.\\") if __name__ == \'__main__\': main()"},{"question":"# Python Coding Assessment Question Objective: Write a Python script that customizes its startup environment and handles specific errors interactively. Task: 1. **Creating a Startup File:** Create a startup file named `mystartup.py` that: - Sets a custom prompt using `sys.ps1`. - Defines a function `greet()` that prints \\"Hello, welcome to the interactive Python session!\\". 2. **Creating a Customization Module:** Create a customization module named `usercustomize.py` in the user site-packages directory that: - Imports the `mystartup.py` file. - Adds a global exception handler that handles `KeyboardInterrupt` gracefully by printing \\"Execution interrupted!\\" and returning to the primary prompt. 3. **Error Handling Script:** Write a script named `runme.py` that: - Implements an interactive loop where users can input commands. - Handles `KeyboardInterrupt` by displaying a custom message \\"Keyboard interruption detected!\\" before continuing to the next iteration. - Terminates the loop when the user inputs \'exit\'. Requirements: - `mystartup.py` file must be placed in the current working directory. - `usercustomize.py` must be created in the user\'s site-packages directory. - `runme.py` should be executable from the command line and should incorporate the custom startup behavior if any is present. Example: When running `python runme.py`: - Upon starting, the prompt should change to a custom prompt indicating the startup file was loaded. - The `greet()` function should be available to call and should display the greeting message. - If `Ctrl+C` (KeyboardInterrupt) is pressed, it should print \\"Keyboard interruption detected!\\" and continue looping for more input. - Typing `exit` should terminate the script gracefully. Constraints and Hints: - Ensure proper permissions are set for `runme.py` to make it executable on Unix-based systems. - Use the correct paths and environment variables to ensure the `usercustomize.py` is loaded properly. - Focus on handling interactive mode behavior as described in the documentation. - Properly test your script to ensure it meets all specified behaviors. This question will test your understanding of Python\'s interactive modes, error handling mechanisms, and customizing the environment using startup and customization scripts.","solution":"# mystartup.py import sys sys.ps1 = \\"MyCustomPrompt >>> \\" def greet(): print(\\"Hello, welcome to the interactive Python session!\\") # usercustomize.py import site import os # Ensure the startup script path is added, modifying for your platform if necessary current_dir = os.path.dirname(os.path.abspath(__file__)) sys.path.insert(0, current_dir) try: import mystartup except ImportError: pass import sys def handle_exception(exc_type, exc_value, exc_traceback): if exc_type == KeyboardInterrupt: print(\\"Execution interrupted!\\") return else: sys.__excepthook__(exc_type, exc_value, exc_traceback) sys.excepthook = handle_exception # runme.py import sys def main(): while True: try: command = input(\\"Command >>> \\") if command.strip().lower() == \'exit\': break exec(command) except KeyboardInterrupt: print(\\"nKeyboard interruption detected!\\") if __name__ == \'__main__\': main()"},{"question":"**Question: Implementing a Multi-threaded Counter** You need to implement a multi-threaded counter using the `_thread` module in Python. Your task is to create a simple counter that increments its value concurrently using multiple threads. Each thread should increment the counter a specified number of times. To ensure thread safety, use locks to synchronize access to the counter. # Requirements 1. **Counter Class:** - Implement a class `Counter` with the following methods: - `__init__(self)`: Initializes the counter value to `0` and creates a lock object. - `increment(self, times)`: A method that increments the counter value by `1` a given number of `times`. This method should be thread-safe. - `get_value(self)`: A method that returns the current value of the counter. 2. **Thread Management Function:** - Write a function `start_threads(counter, num_threads, increments_per_thread)` that: - Creates `num_threads` threads. - Each thread should call the `increment` method of the counter object `increments_per_thread` times. - The function should wait for all threads to complete before returning. 3. **Main Execution Block:** - In the main block of your script: - Create an instance of the `Counter` class. - Call the `start_threads` function with the counter instance, specify the number of threads (e.g., 5), and the number of increments each thread should perform (e.g., 1000). - Print the final value of the counter after all threads have completed. # Example Usage ```python if __name__ == \\"__main__\\": counter = Counter() start_threads(counter, 5, 1000) print(\\"Final counter value:\\", counter.get_value()) # Expected output: Final counter value: 5000 (if num_threads=5 and increments_per_thread=1000) ``` # Constraints - Use the `_thread` module for threading. - Implement proper synchronization using locks to prevent race conditions. - Ensure that the main thread waits for all other threads to complete. # Evaluation Criteria - Correct use of the `_thread` module to create and manage threads. - Proper synchronization using locks to maintain the integrity of the counter value. - Clean and readable code. Good luck!","solution":"import _thread import time class Counter: def __init__(self): self.value = 0 self.lock = _thread.allocate_lock() def increment(self, times): for _ in range(times): with self.lock: self.value += 1 def get_value(self): return self.value def start_threads(counter, num_threads, increments_per_thread): def worker(): counter.increment(increments_per_thread) for _ in range(num_threads): _thread.start_new_thread(worker, ()) # Giving time for all threads to complete time.sleep(1)"},{"question":"**Coding Assessment Question: Optimizing a Machine Learning Pipeline** **Objective:** To assess your ability to optimize computational performance of a machine learning algorithm in scikit-learn by profiling and identifying the bottlenecks, and then improving the performance using appropriate techniques. **Problem Statement:** You are given a Python implementation of a K-Means clustering algorithm applied to the Iris dataset. This implementation is not optimized for performance. Your task is to profile the given code, identify the bottlenecks, and propose optimizations that align with best practices outlined in the provided documentation. **Instructions:** 1. **Profiling**: - Profile the provided Python implementation using IPython\'s built-in profiling tools. - Identify the bottlenecks in the code. 2. **Optimization**: - Implement optimizations following the guidelines from the documentation. Specifically, focus on: - Converting inefficient Python loops into efficient Numpy operations. - Using Cython to compile performance-critical sections of the code, if necessary. - Applying joblib for parallel processing where applicable. - Ensure that the optimized code produces the same results as the original implementation. **Input:** - You will have access to the Iris dataset via scikit-learn\'s `load_iris()` function. - The initial (unoptimized) implementation of the K-Means clustering algorithm. **Output:** - Optimized implementation of the K-Means clustering algorithm. - A brief summary (300 words max) discussing the bottlenecks identified and the optimization techniques applied. **Code Provided:** ```python import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler def kmeans(X, k, max_iters=100): m, n = X.shape centroids = X[np.random.choice(m, k, replace=False)] for _ in range(max_iters): clusters = [[] for _ in range(k)] for x in X: closest_centroid = np.argmin([np.linalg.norm(x-c) for c in centroids]) clusters[closest_centroid].append(x) new_centroids = [np.mean(cluster, axis=0) for cluster in clusters] if np.all(centroids == new_centroids): break centroids = new_centroids return centroids, clusters def main(): iris = load_iris() X = iris.data X = StandardScaler().fit_transform(X) k = 3 centroids, clusters = kmeans(X, k) print(f\\"Centroids:n {centroids}\\") if __name__ == \\"__main__\\": main() ``` **Constraints:** - Use only scikit-learn, Numpy, and Cython (if necessary). - Ensure that the code is easy to understand and maintain. - Your optimized code should run faster than the provided unoptimized implementation. **Performance Requirements:** - The execution time of your optimized implementation should be substantially lower than that of the original one without compromising the functionality or correctness. **Submission:** - Submit your optimized implementation code. - Submit a profiling report showing the bottlenecks before and after optimization. - Submit the summary document explaining the bottlenecks identified and the optimization techniques applied.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from scipy.spatial.distance import cdist from joblib import Parallel, delayed def kmeans_optimized(X, k, max_iters=100): m, n = X.shape centroids = X[np.random.choice(m, k, replace=False)] for _ in range(max_iters): distances = cdist(X, centroids, metric=\'euclidean\') labels = np.argmin(distances, axis=1) new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)]) if np.all(centroids == new_centroids): break centroids = new_centroids return centroids, labels def main(): iris = load_iris() X = iris.data X = StandardScaler().fit_transform(X) k = 3 centroids, labels = kmeans_optimized(X, k) print(f\\"Centroids:n {centroids}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: Custom Bytecode Compilation Tool Using `compileall` You are tasked with creating a custom Python script that leverages the `compileall` module to compile Python source files within a given directory tree based on specific parameters. The goal is to compile these files efficiently while applying various constraints and optimizations. Function Signature ```python def custom_compile(dir: str, maxlevels: int = None, prepend_dir: str = None, strip_prefix: str = None, use_workers: int = 1, optimize_levels: list = [-1], include_symlinks: bool = True, quiet_mode: int = 0) -> bool: pass ``` Parameters - `dir` (str): The root directory to start the compilation from. - `maxlevels` (int): The maximum recursion level for subdirectories. If None, it uses sys.getrecursionlimit(). - `prepend_dir` (str): Prepend this directory to the path to each file being compiled. - `strip_prefix` (str): Strip this prefix from the path to each file being compiled. - `use_workers` (int): Use this many workers to compile the files. If 0, use `os.cpu_count()`. - `optimize_levels` (list): A list of optimization levels to apply during compilation. Default is [-1], which means no optimization. - `include_symlinks` (bool): If True, include symlinks pointing outside the given directory. - `quiet_mode` (int): Controls verbosity. 0 prints all output, 1 prints errors only, 2 suppresses all output. Return - (bool): Returns True if all files compile successfully, else returns False. Constraints - The `compileall.compile_dir` function should be used to perform the directory compilation. - Handle the `prepend_dir`, `strip_prefix`, `use_workers`, `optimize_levels`, `include_symlinks`, and `quiet_mode` parameters appropriately by mapping them to the `compile_dir` function arguments. - Ensure that the function is efficient and scales well with directories containing many Python files. Example Usage: ```python result = custom_compile(\\"/path/to/source\\", maxlevels=2, prepend_dir=\\"/new_root\\", strip_prefix=\\"/old_root\\", use_workers=4, optimize_levels=[1,2], include_symlinks=False, quiet_mode=1) print(result) # Should print True if compilation was successful, else False ``` Write the implementation of the `custom_compile` function as specified above.","solution":"import compileall import sys import os def custom_compile(dir: str, maxlevels: int = None, prepend_dir: str = None, strip_prefix: str = None, use_workers: int = 1, optimize_levels: list = [-1], include_symlinks: bool = True, quiet_mode: int = 0) -> bool: Compile Python source files within a given directory tree based on specific parameters. Parameters: - dir (str): The root directory to start the compilation from. - maxlevels (int): The maximum recursion level for subdirectories. If None, it uses sys.getrecursionlimit(). - prepend_dir (str): Prepend this directory to the path to each file being compiled. - strip_prefix (str): Strip this prefix from the path to each file being compiled. - use_workers (int): Use this many workers to compile the files. If 0, use os.cpu_count(). - optimize_levels (list): A list of optimization levels to apply during compilation. Default is [-1], which means no optimization. - include_symlinks (bool): If True, include symlinks pointing outside the given directory. - quiet_mode (int): Controls verbosity. 0 prints all output, 1 prints errors only, 2 suppresses all output. Returns: - (bool): Returns True if all files compile successfully, else returns False. if maxlevels is None: maxlevels = sys.getrecursionlimit() if use_workers == 0: use_workers = os.cpu_count() result = compileall.compile_dir( dir, maxlevels=maxlevels, force=True, quiet=quiet_mode, legacy=False, optimize=optimize_levels, workers=use_workers, invalidation_mode=None, stripdir=strip_prefix, prependdir=prepend_dir, limit_sl_dest=not include_symlinks ) return result"},{"question":"**Question: Implement a Custom HTTP Client** You are required to implement a custom HTTP client using Python\'s `http.client` module. This client should perform the following tasks: 1. **Initialize a Connection:** - Create a connection to a given HTTP or HTTPS server. 2. **Send Requests:** - Send GET, POST, and PUT requests to specified URLs, optionally with headers and request body. 3. **Retrieve and Parse Responses:** - Parse the response to extract the status code, reason phrase, headers, and body. - Handle different types of responses, including status codes such as 200 OK, 404 Not Found, and 500 Internal Server Error. 4. **Error Handling:** - Gracefully handle common exceptions such as `http.client.HTTPException`, `http.client.RemoteDisconnected`, and other relevant exceptions. **Function Signature:** ```python class CustomHttpClient: def __init__(self, host: str, use_https: bool = False): pass def send_get_request(self, url: str, headers: dict = {}) -> dict: pass def send_post_request(self, url: str, body: str, headers: dict = {}) -> dict: pass def send_put_request(self, url: str, body: str, headers: dict = {}) -> dict: pass ``` **Detailed Requirements:** 1. **Initialization:** - The `__init__` method should initialize an `HTTPConnection` or `HTTPSConnection` based on the `use_https` flag. - `host` is the server domain (e.g., \\"www.python.org\\"). 2. **Sending Requests:** - `send_get_request`: - Connects to the server and sends a GET request to the specified `url` with optional `headers`. - Returns a dictionary with keys: `status`, `reason`, `headers`, and `body`. - `send_post_request`: - Connects to the server and sends a POST request to the specified `url` with the given `body` and optional `headers`. - Returns a dictionary similar to the GET request. - `send_put_request`: - Connects to the server and sends a PUT request to the specified `url` with the given `body` and optional `headers`. - Returns a dictionary similar to the GET request. 3. **Response Parsing:** - Each method should extract and return the status code, reason phrase, headers, and body from the HTTP response. 4. **Error Handling:** - Each method should handle potential exceptions and return a dictionary with an `error` key if an exception occurs, containing the error message. **Example:** ```python client = CustomHttpClient(\\"www.python.org\\", use_https=True) response = client.send_get_request(\\"/\\") print(response) ``` Expected Output (or similar): ```python { \\"status\\": 200, \\"reason\\": \\"OK\\", \\"headers\\": { \\"Content-Type\\": \\"text/html; charset=utf-8\\", ... }, \\"body\\": \\"<!doctype html>...\\" } ``` **Constraints:** - Do not use any external libraries except for built-in Python modules. - Ensure the client handles both HTTP and HTTPS connections appropriately. - Implement proper error handling for common HTTP exceptions. **Bonus (Optional):** - Implement a method `send_custom_request` to handle any HTTP method (e.g., DELETE, PATCH).","solution":"import http.client from typing import Dict, Any class CustomHttpClient: def __init__(self, host: str, use_https: bool = False): self.host = host self.use_https = use_https self.connection = http.client.HTTPSConnection(host) if use_https else http.client.HTTPConnection(host) def _send_request(self, method: str, url: str, body: str = None, headers: dict = None) -> dict: if headers is None: headers = {} try: self.connection.request(method, url, body, headers) response = self.connection.getresponse() return { \'status\': response.status, \'reason\': response.reason, \'headers\': dict(response.getheaders()), \'body\': response.read().decode() } except (http.client.HTTPException, http.client.RemoteDisconnected) as e: return {\'error\': str(e)} def send_get_request(self, url: str, headers: dict = None) -> dict: return self._send_request(\'GET\', url, headers=headers) def send_post_request(self, url: str, body: str = \'\', headers: dict = None) -> dict: return self._send_request(\'POST\', url, body=body, headers=headers) def send_put_request(self, url: str, body: str = \'\', headers: dict = None) -> dict: return self._send_request(\'PUT\', url, body=body, headers=headers)"},{"question":"**Kernel Density Estimation with scikit-learn** You are provided with a dataset containing 2D points sampled from an unknown probability distribution. Your task is to implement a function that estimates the probability density function (PDF) of the distribution using Kernel Density Estimation (KDE) with scikit-learn and visualize the resulting density estimate. # Function Signature ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimation(data: np.ndarray, kernel: str, bandwidth: float, grid_size: int) -> None: Estimates the probability density function (PDF) of the given data using Kernel Density Estimation (KDE) and visualizes it. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, 2) containing the input data points. - kernel (str): The type of kernel to use in KDE. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. - bandwidth (float): The bandwidth parameter for KDE, determining the smoothness of the resulting density. - grid_size (int): The size of the grid to use for the visualization. This will be the number of points along each dimension in the grid. Returns: - None pass ``` # Input - `data`: A 2D numpy array of shape `(n_samples, 2)` containing the input data points. - `kernel`: The type of kernel to use in KDE. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. - `bandwidth`: The bandwidth parameter for KDE, determining the smoothness of the resulting density. - `grid_size`: The size of the grid to use for the visualization. This will be the number of points along each dimension in the grid. # Output - The function should create a 2D plot of the estimated PDF over the data range and display it using `matplotlib`. # Example Usage ```python data = np.random.rand(100, 2) * 20 - 10 # Generate random 2D points as an example kde_density_estimation(data, kernel=\'gaussian\', bandwidth=1.0, grid_size=100) ``` # Constraints - Assume that the input data will always have exactly 2 dimensions. - The `bandwidth` parameter must be a positive float. - The `grid_size` parameter must be a positive integer greater than 1. # Additional Requirements - Implement and use scikit-learn\'s `KernelDensity` estimator. - Use meshgrid to create the data points for visualization. - The plot should display the estimated PDF as a contour plot. # Notes - Ensure the plot is clearly labeled and includes a color bar to indicate the density values. - The visualization should give insights into the underlying probability distribution of the data.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimation(data: np.ndarray, kernel: str, bandwidth: float, grid_size: int) -> None: Estimates the probability density function (PDF) of the given data using Kernel Density Estimation (KDE) and visualizes it. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, 2) containing the input data points. - kernel (str): The type of kernel to use in KDE. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. - bandwidth (float): The bandwidth parameter for KDE, determining the smoothness of the resulting density. - grid_size (int): The size of the grid to use for the visualization. This will be the number of points along each dimension in the grid. Returns: - None # Validate input parameters assert data.ndim == 2 and data.shape[1] == 2, \'Data should be a 2D array with shape (n_samples, 2)\' assert kernel in [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'], \'Invalid kernel\' assert bandwidth > 0, \'Bandwidth must be a positive float\' assert isinstance(grid_size, int) and grid_size > 1, \'Grid size must be an integer greater than 1\' # Fit the KDE model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) # Create a meshgrid over the data range x_min, y_min = data.min(axis=0) x_max, y_max = data.max(axis=0) x = np.linspace(x_min, x_max, grid_size) y = np.linspace(y_min, y_max, grid_size) xx, yy = np.meshgrid(x, y) grid_points = np.vstack([xx.ravel(), yy.ravel()]).T # Evaluate the probability density function on the grid log_density = kde.score_samples(grid_points) pdf = np.exp(log_density).reshape(grid_size, grid_size) # Plot the results plt.figure(figsize=(8, 6)) plt.contourf(xx, yy, pdf, cmap=\'viridis\') plt.colorbar(label=\'Density\') plt.scatter(data[:, 0], data[:, 1], c=\'white\', s=5, edgecolor=\'k\') plt.title(f\'KDE Density Estimation with {kernel} kernel\') plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.show()"},{"question":"**Objective:** Implement a Python function to compose and send an email that includes a text body, an HTML body with inline images, and additional attachments from a specified directory. You will also write a function to parse such an email and extract its contents into a local directory. **Task 1: Send Email** Write a function `send_complex_email` that sends an email with the following requirements: - From, to, and subject fields. - A plain text body and an HTML body. - Inline images within the HTML body. - Additional attachments from a specified directory. ```python def send_complex_email(sender, recipients, subject, text_content, html_content, inline_images, attach_dir, smtp_server=\'localhost\'): Compose and send an email. Args: - sender (str): The sender\'s email address. - recipients (list): List of recipient email addresses. - subject (str): Subject of the email. - text_content (str): Plain text content of the email. - html_content (str): HTML content of the email. - inline_images (dict): Dictionary where keys are cid and values are file paths to images to be embedded inline. - attach_dir (str): Directory containing files to attach. - smtp_server (str): Hostname of the SMTP server. Returns: None pass ``` **Task 2: Extract Email Contents** Write a function `extract_email_contents` that extracts the contents of an email saved in a file to a specified directory. This function should handle: - Parsing the email. - Saving the plain text and HTML bodies to separate files. - Saving inline images and other attachments to the specified directory. ```python def extract_email_contents(email_file, output_directory): Extract the contents of a MIME email to a directory. Args: - email_file (str): Path to the email file. - output_directory (str): Directory to save extracted content. Returns: None pass ``` **Constraints and Requirements:** 1. Use the `email` package to handle email composition and parsing. 2. Make use of `email.message.EmailMessage` for creating both plain text and HTML content parts. 3. Handle MIME types and attach files correctly using `mimetypes`. 4. Make sure the sender and recipients accept valid email addresses. Raise a `ValueError` if invalid addresses are provided. 5. Ensure the output directory for extracted contents exists or get created if not. **Example Usage:** ```python send_complex_email( sender=\\"sender@example.com\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Test Email\\", text_content=\\"This is the plain text part of the email.\\", html_content=\'\'\'<html><body> <h1>Hello</h1> <p>This is an <b>HTML</b> email.</p> <img src=\\"cid:test_image\\" /> </body></html>\'\'\', inline_images={\\"test_image\\": \\"/path/to/image1.jpg\\"}, attach_dir=\\"/path/to/attachments\\" ) extract_email_contents(\\"/path/to/saved_email.eml\\", \\"/path/to/output_directory\\") ``` Make sure your implementation is efficient and correctly handles email composition and parsing, including proper MIME type guessing and handling directory and file operations robustly.","solution":"import os import smtplib import mimetypes from email.message import EmailMessage from email.utils import make_msgid from email import policy from email.parser import BytesParser def send_complex_email(sender, recipients, subject, text_content, html_content, inline_images, attach_dir, smtp_server=\'localhost\'): Compose and send an email. Args: - sender (str): The sender\'s email address. - recipients (list): List of recipient email addresses. - subject (str): Subject of the email. - text_content (str): Plain text content of the email. - html_content (str): HTML content of the email. - inline_images (dict): Dictionary where keys are cid and values are file paths to images to be embedded inline. - attach_dir (str): Directory containing files to attach. - smtp_server (str): Hostname of the SMTP server. Returns: None msg = EmailMessage() msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) msg[\'Subject\'] = subject msg.set_content(text_content) msg.add_alternative(html_content, subtype=\'html\') for cid, img_path in inline_images.items(): with open(img_path, \'rb\') as img: img_data = img.read() img_cid = make_msgid() msg.get_payload()[1].add_related(img_data, maintype=\'image\', subtype=img_path.split(\'.\')[-1], cid=img_cid[1:-1]) html_content = html_content.replace(\'cid:\' + cid, \'cid:\' + img_cid[1:-1]) for attachment in os.listdir(attach_dir): attach_path = os.path.join(attach_dir, attachment) if os.path.isfile(attach_path): ctype, encoding = mimetypes.guess_type(attach_path) if ctype is None or encoding is not None: ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) with open(attach_path, \'rb\') as fp: msg.add_attachment(fp.read(), maintype=maintype, subtype=subtype, filename=attachment) with smtplib.SMTP(smtp_server) as s: s.send_message(msg) def extract_email_contents(email_file, output_directory): Extract the contents of a MIME email to a directory. Args: - email_file (str): Path to the email file. - output_directory (str): Directory to save extracted content. Returns: None if not os.path.exists(output_directory): os.makedirs(output_directory) with open(email_file, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) text_file_path = os.path.join(output_directory, \'email_text.txt\') html_file_path = os.path.join(output_directory, \'email_html.html\') if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_type() == \'text/plain\': with open(text_file_path, \'w\') as f: f.write(part.get_payload(decode=True).decode(part.get_content_charset())) elif part.get_content_type() == \'text/html\': with open(html_file_path, \'w\') as f: f.write(part.get_payload(decode=True).decode(part.get_content_charset())) elif part.get(\'Content-Disposition\'): disposition = part.get(\'Content-Disposition\') if disposition.startswith(\'attachment\') or disposition.startswith(\'inline\'): filename = part.get_filename() filepath = os.path.join(output_directory, filename) with open(filepath, \'wb\') as f: f.write(part.get_payload(decode=True)) else: if msg.get_content_type() == \'text/plain\': with open(text_file_path, \'w\') as f: f.write(msg.get_payload(decode=True).decode(msg.get_content_charset())) elif msg.get_content_type() == \'text/html\': with open(html_file_path, \'w\') as f: f.write(msg.get_payload(decode=True).decode(msg.get_content_charset()))"},{"question":"# Task Objective You are provided with a dataset containing measurements of penguins and are required to visualize the distribution of their flipper lengths. Your task involves writing Python code using the seaborn package to generate specific visualizations as described below. Dataset You will use the Penguins dataset available in seaborn. You may load it using the following code: ```python from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` Requirements 1. **Basic Density Plot:** - Create a basic density plot for the `flipper_length_mm` variable. 2. **Adjusted Smoothness:** - Create another density plot for `flipper_length_mm` with the bandwidth adjusted to show more detail using a `bw_adjust` parameter of `0.25`. 3. **Combined Histogram and Density Plot:** - Create a plot that combines a histogram (as bars with some transparency) of `flipper_length_mm` with a density plot overlaid on top of it. 4. **Conditional Density Plot:** - Create a conditional density plot showing the density of `flipper_length_mm` for each species in the dataset. Ensure that the densities are not normalized jointly but individually for each species. 5. **Faceted Density Plot:** - Create a faceted density plot conditioned on the `sex` of the penguins, with the densities color-coded by species, and show the densities normalized within each facet. 6. **Cumulative Density Plot:** - Generate a cumulative density plot for the `flipper_length_mm` variable. Constraints - You must use seaborn\'s object-oriented interface for plotting (i.e., seaborn.objects). - Each plot should be displayed separately with appropriate titles and labels for clarity. - Follow proper coding practices and ensure your code is well-documented and clear. Input and Output - **Input:** - The input consists of the preloaded Penguins dataset which will be used in your seaborn plots. - **Output:** - The output should consist of six separate plots meeting the criteria specified above. **Note:** Make sure seaborn and any other required libraries are installed in your environment. # Submission Submit your Python script or Jupyter Notebook containing the solution to the task described above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def basic_density_plot(): plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\') plt.title(\'Density Plot of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def adjusted_density_plot(): plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', bw_adjust=0.25) plt.title(\'Density Plot of Flipper Length with bw_adjust=0.25\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def histogram_with_density_plot(): plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'flipper_length_mm\', kde=True, stat=\'density\', alpha=0.6) plt.title(\'Histogram and Density Plot of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def conditional_density_plot(): plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', common_norm=False) plt.title(\'Conditional Density Plot of Flipper Length by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def faceted_density_plot(): g = sns.displot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', col=\'sex\', kind=\'kde\', common_norm=False) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g.set_titles(\\"Density Plot by Sex: {col_name}\\") plt.show() def cumulative_density_plot(): plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', cumulative=True) plt.title(\'Cumulative Density Plot of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Cumulative Density\') plt.show()"},{"question":"Objective: To test the candidate\'s understanding and ability to implement and manipulate data classes using the `dataclasses` module in Python. Problem Statement: You need to design a registry system for a university course management application. The application should keep track of students, courses, and enrollments. Each student has an ID, name, and email. Each course has a course code, title, and credit hours. An enrollment links a student to a course and records the grade they achieved. Requirements: 1. Define three data classes: `Student`, `Course`, and `Enrollment`. 2. Implement the following functionalities: - Add a student to the registry. - Add a course to the registry. - Enroll a student in a course with an initial grade of `None`. - Update the grade for a student\'s enrollment in a course. - Retrieve the list of courses a student is enrolled in, along with their grades. Input and Output Formats: - **Input**: No direct inputs. Implement class methods to add data and perform operations. - **Output**: No direct outputs. Implement class methods that return lists or data where necessary. Constraints: - A unique student cannot be enrolled in the same course more than once. - Grades are float values between 0.0 and 4.0, inclusive. Performance Requirements: - The methods for adding and retrieving data should be efficient, aiming for average O(1) time complexity where applicable. # Example Usage ```python from dataclasses import dataclass, field from typing import List, Optional @dataclass class Student: id: int name: str email: str @dataclass class Course: code: str title: str credit_hours: int @dataclass class Enrollment: student_id: int course_code: str grade: Optional[float] = None class UniversityRegistry: def __init__(self): self.students: List[Student] = [] self.courses: List[Course] = [] self.enrollments: List[Enrollment] = [] def add_student(self, student: Student) -> None: self.students.append(student) def add_course(self, course: Course) -> None: self.courses.append(course) def enroll_student(self, student_id: int, course_code: str) -> None: # Ensure a student is not enrolled in the same course twice for enrollment in self.enrollments: if enrollment.student_id == student_id and enrollment.course_code == course_code: return self.enrollments.append(Enrollment(student_id, course_code)) def update_grade(self, student_id: int, course_code: str, grade: float) -> None: for enrollment in self.enrollments: if enrollment.student_id == student_id and enrollment.course_code == course_code: enrollment.grade = grade break def get_student_courses(self, student_id: int) -> List[tuple]: return [(enrollment.course_code, enrollment.grade) for enrollment in self.enrollments if enrollment.student_id == student_id] # Example usage registry = UniversityRegistry() student = Student(id=1, name=\\"John Doe\\", email=\\"john@university.edu\\") course = Course(code=\\"CS101\\", title=\\"Intro to Computer Science\\", credit_hours=3) registry.add_student(student) registry.add_course(course) registry.enroll_student(1, \\"CS101\\") registry.update_grade(1, \\"CS101\\", 3.7) courses = registry.get_student_courses(1) print(courses) # Output: [(\'CS101\', 3.7)] ``` Implement the `UniversityRegistry` class based on the provided example. Ensure proper error handling and validation where necessary.","solution":"from dataclasses import dataclass, field from typing import List, Optional, Tuple @dataclass class Student: id: int name: str email: str @dataclass class Course: code: str title: str credit_hours: int @dataclass class Enrollment: student_id: int course_code: str grade: Optional[float] = None class UniversityRegistry: def __init__(self): self.students: List[Student] = [] self.courses: List[Course] = [] self.enrollments: List[Enrollment] = [] def add_student(self, student: Student) -> None: self.students.append(student) def add_course(self, course: Course) -> None: self.courses.append(course) def enroll_student(self, student_id: int, course_code: str) -> None: # Ensure a student is not enrolled in the same course twice for enrollment in self.enrollments: if enrollment.student_id == student_id and enrollment.course_code == course_code: return self.enrollments.append(Enrollment(student_id, course_code)) def update_grade(self, student_id: int, course_code: str, grade: float) -> None: if not (0.0 <= grade <= 4.0): raise ValueError(\\"Grade must be between 0.0 and 4.0\\") for enrollment in self.enrollments: if enrollment.student_id == student_id and enrollment.course_code == course_code: enrollment.grade = grade break def get_student_courses(self, student_id: int) -> List[Tuple[str, Optional[float]]]: return [(enrollment.course_code, enrollment.grade) for enrollment in self.enrollments if enrollment.student_id == student_id]"},{"question":"<|Analysis Begin|> The provided documentation snippet gives information about using seaborn for handling different styles for plots. It discusses: 1. How to retrieve the current default style parameters using `sns.axes_style()`. 2. How to view parameters for a predefined style by passing a style name as an argument to `sns.axes_style(\\"style_name\\")`. 3. How to temporarily change the style of plots using a context manager with `with sns.axes_style(\\"style_name\\")`. These functionalities can serve as an essential aspect to assess students\' comprehension of manipulating plot styles and producing visualizations that adhere to specific styling criteria using seaborn. <|Analysis End|> <|Question Begin|> **Question: Exploring and Applying Seaborn Plot Styles** Seaborn is a powerful visualization library in Python that allows for highly customizable and aesthetically pleasing plotting. In this task, you will demonstrate your understanding of Seaborn\'s styling capabilities by configuring and applying different styles to several plots. # Task 1. Create a function `get_style_parameters()` that: - Takes no arguments. - Returns a dictionary of the default style parameters. 2. Create a function `get_specific_style(style_name)` that: - Takes a string argument `style_name`. - Returns the style parameters for the specified `style_name`. If the style is not recognized, it should return `None`. 3. Create a function `plot_with_style(style_name)` that: - Takes a string argument `style_name`. - Temporarily applies the style `style_name` to the plot. - Plots a simple bar plot: `x=[1, 2, 3]` and `y=[2, 5, 3]`. # Example ```python # Expected Output for get_style_parameters (example snippet) { \'axes.facecolor\': \'white\', \'axes.edgecolor\': \'.15\', ... # other style parameters } print(get_specific_style(\\"darkgrid\\")) # Expected Output for get_specific_style(\\"darkgrid\\") (example snippet) { \'axes.facecolor\': \'#EAEAF2\', \'axes.edgecolor\': \'white\', ... # other style parameters } plot_with_style(\\"whitegrid\\") # Should display a bar plot with whitegrid style ``` # Constraints - Ensure that the functions handle missed cases gracefully. For example, if an invalid style name is provided to `get_specific_style` or `plot_with_style`, these functions should not crash.","solution":"import seaborn as sns import matplotlib.pyplot as plt def get_style_parameters(): Returns the default style parameters of Seaborn. return sns.axes_style() def get_specific_style(style_name): Returns the style parameters for the given style_name. Parameters: style_name (str): The name of the Seaborn style. Returns: dict or None: The style parameters if the style_name is valid, otherwise None. try: return sns.axes_style(style_name) except ValueError: return None def plot_with_style(style_name): Temporarily applies the specified style and plots a simple bar plot. Parameters: style_name (str): The name of the Seaborn style. Returns: None try: with sns.axes_style(style_name): x = [1, 2, 3] y = [2, 5, 3] sns.barplot(x=x, y=y) plt.show() except ValueError: print(f\\"Style \'{style_name}\' is not a recognized style.\\")"},{"question":"<|Analysis Begin|> The provided documentation is highly detailed and focuses on creating minimal reproducible examples (MCVE) for bug reports in scikit-learn. It includes various examples and good practices for constructing these examples, simplifying the code, and generating synthetic datasets. The documentation covers different types of problems (classification, regression, clustering) and guides on using numpy, pandas, and scikit-learn\'s dataset generators to create testable data. The focus of the documentation is on how to prepare and present minimalistic, clear, and effective examples to communicate issues. Even though this document is about crafting minimal bug reports, it provides a lot of useful information on how to utilize and test various functionalities of scikit-learn. Considering the focus of the documentation and its extensive coverage on synthetic data and simplicity, a coding assessment question can be designed around creating synthetic data, implementing a model, and validating it, focusing on clearness, correctness, and minimalism, which aligns well with the documentation\'s guidance. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Assess the understanding of synthetic data generation, model training, evaluation, and proper handling of minimal reproducible examples using scikit-learn. Problem Statement: You are required to create a synthetic dataset, implement a scikit-learn model, fit the model to the dataset, and evaluate its performance. Your task is to ensure the example is minimal, clear, and reproduce the desired results. Requirements: 1. **Generate Synthetic Data:** - Create a synthetic classification dataset using `sklearn.datasets.make_classification`. - The dataset should have the following properties: - `n_samples=1000` - `n_features=20` (15 informative, 5 redundant) - `n_classes=3` 2. **Implement and Train the Model:** - Use a `RandomForestClassifier` from `sklearn.ensemble`. - Split the data into training (70%) and test (30%) sets. - Train the model on the training set. 3. **Evaluate the Model:** - Evaluate the model using accuracy score from `sklearn.metrics`. - Print the train and test accuracy. 4. **Code Clarity & Conciseness:** - Your code should be minimal and clearly written. - Make sure to include all necessary import statements. - Avoid any unnecessary steps or parameters. Input: This task does not require any external input. Output: Output the training and test accuracy of the model. Example: ```python # Necessary imports import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Step 1: Generate Synthetic Data X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42) # Step 2: Implement and Train the Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Step 3: Evaluate the Model train_accuracy = accuracy_score(y_train, model.predict(X_train)) test_accuracy = accuracy_score(y_test, model.predict(X_test)) # Output print(f\\"Training Accuracy: {train_accuracy:.2f}\\") print(f\\"Test Accuracy: {test_accuracy:.2f}\\") ``` Your task is to implement the above steps ensuring clarity and minimalism.","solution":"# Necessary imports import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def synthetic_data_classification(): # Step 1: Generate Synthetic Data X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42) # Step 2: Implement and Train the Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Step 3: Evaluate the Model train_accuracy = accuracy_score(y_train, model.predict(X_train)) test_accuracy = accuracy_score(y_test, model.predict(X_test)) return train_accuracy, test_accuracy # Call the function and print the accuracies train_acc, test_acc = synthetic_data_classification() print(f\\"Training Accuracy: {train_acc:.2f}\\") print(f\\"Test Accuracy: {test_acc:.2f}\\")"},{"question":"# Pandas Coding Assessment: Flight Data Analysis Background You are given two CSV files containing information about flight data: 1. **flights.csv** - Contains information about flights, including the date, departure delays, arrival delays, and other relevant details. 2. **airlines.csv** - Contains information about airlines, including the airline code and the airline name. Objective Write a Python function `analyze_flight_data(flights_csv: str, airlines_csv: str) -> pd.DataFrame` that performs the following tasks: 1. **Read Data**: Load the data from the given CSV files into pandas DataFrames. 2. **Data Cleaning**: - Handle any missing data: Drop rows where essential information (like flight date, departure delay, or arrival delay) is missing. 3. **Merge Data**: - Merge the flights data with the airlines data on the airline code. 4. **Compute Aggregated Statistics**: - Calculate the average departure delay and average arrival delay for each airline. - Calculate the total number of flights for each airline. 5. **Sorting**: - Sort the resulting DataFrame based on the total number of flights in descending order. 6. **Output**: - Return the final DataFrame with the following columns: `Airline Name`, `Average Departure Delay`, `Average Arrival Delay`, `Total Flights`. Constraints - The function should handle large datasets efficiently. - The function should manage missing data appropriately by dropping rows with essential missing information. - The merging operation should be done based on the airline code. Example Input ```python flights_csv = \\"path/to/flights.csv\\" airlines_csv = \\"path/to/airlines.csv\\" ``` Example Output The function should return a DataFrame similar to the one below: ```python Airline Name Average Departure Delay Average Arrival Delay Total Flights 0 Airline A 5.2 4.1 120 1 Airline B 6.3 5.4 115 2 Airline C 7.1 6.8 110 ... ``` Function Signature ```python import pandas as pd def analyze_flight_data(flights_csv: str, airlines_csv: str) -> pd.DataFrame: pass ``` # Notes - You may assume the CSV files are correctly formatted and the paths provided are valid. - Ensure that you handle cases where merging fails due to missing keys or mismatched data types.","solution":"import pandas as pd def analyze_flight_data(flights_csv: str, airlines_csv: str) -> pd.DataFrame: # Reading data from CSV files flights_df = pd.read_csv(flights_csv) airlines_df = pd.read_csv(airlines_csv) # Data Cleaning: Drop rows with essential missing information essential_columns = [\'flight_date\', \'dep_delay\', \'arr_delay\'] flights_df.dropna(subset=essential_columns, inplace=True) # Merge data on the airline code merged_df = pd.merge(flights_df, airlines_df, left_on=\'airline_code\', right_on=\'code\', how=\'left\') # Compute Aggregated Statistics aggregated_df = merged_df.groupby(\'airline_name\').agg({ \'dep_delay\': \'mean\', \'arr_delay\': \'mean\', \'flight_date\': \'count\' }).reset_index() aggregated_df.columns = [\'Airline Name\', \'Average Departure Delay\', \'Average Arrival Delay\', \'Total Flights\'] # Sorting based on the total number of flights in descending order sorted_df = aggregated_df.sort_values(by=\'Total Flights\', ascending=False).reset_index(drop=True) return sorted_df"},{"question":"# Question: Optimizing Prediction Performance with scikit-learn You are given a dataset containing features and labels for a binary classification problem. Your task is to: 1. Implement a scikit-learn pipeline to preprocess the data and train a classifier. 2. Optimize the prediction latency and throughput according to best practices mentioned in the provided documentation. 3. Evaluate the performance impact of different configurations. Data Description - `data.csv`: A CSV file with feature columns and a label column named `target`. - Assume the data is sufficiently large to benefit from bulk processing and that it has both dense and sparse representations. Requirements 1. **Data Preprocessing Pipeline**: - Load the dataset. - Handle missing values (if any) by filling them with the mean of the column. - Standardize the features (mean=0, variance=1). - Optionally convert the dataset to a sparse format if it benefits performance. 2. **Model Training**: - Train a `LogisticRegression` model using the preprocessed data. - Measure and compare prediction latency and throughput for dense vs. sparse representations. - Measure and compare prediction latency and throughput for high and low model complexity (e.g., varying `penalty`, `solver`). 3. **Performance Evaluation**: - Use `time` module to measure prediction latency and throughput. - Run predictions in both bulk and atomic modes. - Report the results in a clear tabular format for comparison. Constraints - You must use scikit-learn\'s `Pipeline` and associated tools. - You should handle the sparsity threshold by assuming a threshold of 90%. - Evaluate latencies at the 90th percentile. Output - A Python function `optimize_prediction_performance` which takes the path to the CSV file as input and outputs a performance report as a dictionary, comparing the configurations in terms of latency and throughput. Function Signature ```python def optimize_prediction_performance(file_path: str) -> dict: pass ``` Example Output ```python { \\"dense_bulk_latency\\": ..., \\"dense_atomic_latency\\": ..., \\"sparse_bulk_latency\\": ..., \\"sparse_atomic_latency\\": ..., \\"complex_bulk_latency\\": ..., \\"simple_bulk_latency\\": ..., \\"throughput_comparison\\": { \\"dense_bulk\\": ..., \\"sparse_bulk\\": ..., ... } } ``` This question requires students to demonstrate their comprehension of scikit-learn, focusing on the practical application of strategies to optimize prediction performance, as discussed in the provided documentation.","solution":"import pandas as pd import numpy as np from scipy.sparse import csr_matrix from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split import time def measure_latency_and_throughput(estimator, X, Y, bulk_size=1000): start_time = time.time() predictions = estimator.predict(X) latency = time.time() - start_time bulk_start_time = time.time() for i in range(0, len(X), bulk_size): estimator.predict(X[i:i+bulk_size]) bulk_latency = time.time() - bulk_start_time return latency, bulk_latency def optimize_prediction_performance(file_path: str) -> dict: data = pd.read_csv(file_path) X = data.drop(columns=[\'target\']) y = data[\'target\'] is_sparse = (X.values > 0).sum() / np.product(X.shape) < 0.1 if is_sparse: X = csr_matrix(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Preprocessing pipeline preprocessor = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler(with_mean=not is_sparse)) ]) # Simple Logistic Regression Model simple_model = Pipeline([ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=100)) ]) simple_model.fit(X_train, y_train) # Complex Logistic Regression Model complex_model = Pipeline([ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000, penalty=\'l2\', solver=\'saga\')) ]) complex_model.fit(X_train, y_train) # Measure latency and throughput dense_latency, dense_bulk_latency = measure_latency_and_throughput(simple_model, X_test, y_test) simple_latency, simple_bulk_latency = measure_latency_and_throughput(simple_model, X_test, y_test) complex_latency, complex_bulk_latency = measure_latency_and_throughput(complex_model, X_test, y_test) performance_report = { \\"dense_bulk_latency\\": dense_bulk_latency, \\"dense_atomic_latency\\": dense_latency, \\"sparse_bulk_latency\\": dense_bulk_latency if is_sparse else None, \\"sparse_atomic_latency\\": dense_latency if is_sparse else None, \\"complex_bulk_latency\\": complex_bulk_latency, \\"complex_atomic_latency\\": complex_latency, \\"throughput_comparison\\": { \\"dense_bulk\\": len(X_test) / dense_bulk_latency, \\"sparse_bulk\\": (len(X_test) / dense_bulk_latency) if is_sparse else None, \\"simple_bulk\\": len(X_test) / simple_bulk_latency, \\"complex_bulk\\": len(X_test) / complex_bulk_latency } } return performance_report"},{"question":"# Advanced Python: Calendar Manipulation Objective Your task is to implement a function that generates a formatted text calendar for a given year, highlighting (displaying in uppercase) all the dates that fall on a specific weekday. Function Signature ```python def highlight_weekday_in_year(year: int, weekday: int) -> str: Generate a formatted text calendar for the entire year highlighting all the dates that fall on a specified weekday by converting them to uppercase. Parameters: - year (int): The year for which the calendar is to be generated. - weekday (int): The weekday to highlight (0 for Monday, 6 for Sunday). Returns: - str: A formatted text calendar for the given year with highlighted dates. Constraints: - year >= 1900 - 0 <= weekday <= 6 (where 0 is Monday and 6 is Sunday) - The calendar should be formatted to fit within an 80-character wide display. ``` Description 1. The function should generate a text calendar for the entire year specified by the `year` parameter. 2. All dates that fall on the specified `weekday` should be highlighted. Highlighting is done by converting the dates to uppercase characters in the calendar output. 3. Use the `calendar.TextCalendar` class and its methods to generate the calendar. 4. Ensure the output is formatted to fit within an 80-character wide display. Example ```python # Highlighting all Sundays in 2023 print(highlight_weekday_in_year(2023, 6)) ``` **Sample Output:** ``` 2023 January Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 ... 30 31 [...more months...] December Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ``` *Note:* In the actual output, all dates that fall on Sunday would be in uppercase. Constraints - Ensure the function runs efficiently and handles valid inputs as specified. Good luck!","solution":"import calendar def highlight_weekday_in_year(year: int, weekday: int) -> str: Generate a formatted text calendar for the entire year highlighting all the dates that fall on a specified weekday by converting them to uppercase. Parameters: - year (int): The year for which the calendar is to be generated. - weekday (int): The weekday to highlight (0 for Monday, 6 for Sunday). Returns: - str: A formatted text calendar for the given year with highlighted dates. cal = calendar.TextCalendar() output = [] # Get calendar of each month and process for month in range(1, 13): month_cal = cal.formatmonth(year, month).splitlines() highlighted_month = [] for line in month_cal: if line and line[0].isdigit(): # Highlighting specific dates dates = line.split() new_dates = [] for date_info in dates: try: date = int(date_info) if calendar.weekday(year, month, date) == weekday: new_dates.append(date_info.upper()) else: new_dates.append(date_info) except ValueError: new_dates.append(date_info) highlighted_month.append(\' \'.join(new_dates)) else: highlighted_month.append(line) output.extend(highlighted_month) output.append(\'\') return \'n\'.join(output)"},{"question":"# Question: Task Dependency Resolution You are tasked with managing the execution of interdependent tasks using the `graphlib.TopologicalSorter` class in Python 3.10. You need to implement a function that takes a list of tasks with their dependencies and returns a valid execution order for these tasks. # Function Signature ```python def resolve_task_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: pass ``` # Input - `tasks`: A list of tuples, where each tuple contains a task represented by a string and a list of strings representing the tasks it depends on. Each task is a node and dependencies are directed edges in the graph. # Output - A list of strings representing a valid topological order of the tasks. If a cycle is detected, return an empty list. # Constraints 1. The number of tasks will not exceed 1000. 2. Each task name is a string containing only alphanumeric characters and underscores. 3. The dependency list for any task will not exceed 50 items. # Example ```python tasks = [ (\\"task1\\", [\\"task2\\", \\"task3\\"]), (\\"task2\\", [\\"task4\\"]), (\\"task3\\", [\\"task4\\"]), (\\"task4\\", []) ] output = resolve_task_order(tasks) print(output) # Example output: [\'task4\', \'task2\', \'task3\', \'task1\'] ``` # Detailed Requirements 1. Initialize a `TopologicalSorter` object. 2. Add each task and its dependencies to the sorter. 3. Prepare the sorter and attempt to get the static order. 4. Handle any `CycleError` and ensure the function returns an empty list in case of cyclic dependencies. 5. The output order may vary as long as it respects the dependencies. # Implementation Notes - Utilize the `TopologicalSorter` class methods: `add`, `prepare`, `static_order`. - Handle errors gracefully using try-except blocks to capture `CycleError`. # Test Case ```python def test_resolve_task_order(): tasks = [ (\\"task1\\", [\\"task2\\", \\"task3\\"]), (\\"task2\\", [\\"task4\\"]), (\\"task3\\", [\\"task4\\"]), (\\"task4\\", []) ] assert resolve_task_order(tasks) in [ [\'task4\', \'task2\', \'task3\', \'task1\'], [\'task4\', \'task3\', \'task2\', \'task1\'] ], \\"Test case failed!\\" tasks = [ (\\"task1\\", [\\"task2\\"]), (\\"task2\\", [\\"task1\\"]) ] assert resolve_task_order(tasks) == [], \\"Test case failed!\\" print(\\"All test cases pass\\") ``` Ensure your solution is efficient and handles the provided constraints within acceptable limits.","solution":"from typing import List, Tuple from graphlib import TopologicalSorter, CycleError def resolve_task_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: sorter = TopologicalSorter() for task, dependencies in tasks: sorter.add(task, *dependencies) try: return list(sorter.static_order()) except CycleError: return []"},{"question":"# Advanced Python Memory Management: Implementing Custom Memory Allocators Objective: To assess the studentâ€™s comprehension of memory management in Python, particularly using Pythonâ€™s memory management API. The student will implement custom memory allocators and deallocators while maintaining appropriate domain-specific function call consistency. Problem Statement: You are required to implement a simple memory manager in Python that interfaces with Python\'s memory management API to allocate, reallocate, and deallocate memory for general-purpose and object-specific use cases. 1. **Implement a function `allocate_general_memory` that:** - Accepts a size parameter `n` indicating the number of bytes to allocate. - Uses `PyMem_RawMalloc` to allocate raw memory. - Returns a pointer to the allocated memory. 2. **Implement a function `reallocate_general_memory` that:** - Accepts a pointer `p` to the previously allocated memory and a size parameter `n` indicating the new number of bytes. - Uses `PyMem_RawRealloc` to resize the memory. - Returns a pointer to the resized memory. 3. **Implement a function `free_general_memory` that:** - Accepts a pointer `p` to the allocated memory. - Uses `PyMem_RawFree` to free the memory. 4. **Implement a function `allocate_object_memory` that:** - Accepts a size parameter `n` indicating the number of bytes to allocate. - Uses `PyObject_Malloc` to allocate memory. - Returns a pointer to the allocated memory. 5. **Implement a function `free_object_memory` that:** - Accepts a pointer `p` to the allocated memory. - Uses `PyObject_Free` to free the memory. Specifications: - Ensure that the allocated memory is properly managed and freed to avoid memory leaks. - All functions should handle edge cases, such as allocation failures and the handling of NULL pointers. - Functions should conform to the domain-specific allocator functions as provided in the documentation. Example Usage and Constraints: ```python # Example Usage n = 100 # Number of bytes to allocate p_general = allocate_general_memory(n) p_general = reallocate_general_memory(p_general, 200) # Resize to 200 bytes free_general_memory(p_general) p_object = allocate_object_memory(n) free_object_memory(p_object) ``` Constraints: - Each function should use the appropriate memory allocation and deallocation functions as per the specified domain. Note: - Code directly interacts with Pythonâ€™s C API, and the student should simulate this interaction within Python using ctypes or a similar library if necessary. - Proper error handling should be demonstrated, especially for allocation failures.","solution":"import ctypes from ctypes import c_void_p, c_size_t # Load the Python C API memory management functions libpython = ctypes.PyDLL(None) PyMem_RawMalloc = libpython.PyMem_RawMalloc PyMem_RawRealloc = libpython.PyMem_RawRealloc PyMem_RawFree = libpython.PyMem_RawFree PyObject_Malloc = libpython.PyObject_Malloc PyObject_Free = libpython.PyObject_Free # Set argument and result types PyMem_RawMalloc.argtypes = [c_size_t] PyMem_RawMalloc.restype = c_void_p PyMem_RawRealloc.argtypes = [c_void_p, c_size_t] PyMem_RawRealloc.restype = c_void_p PyMem_RawFree.argtypes = [c_void_p] PyMem_RawFree.restype = None PyObject_Malloc.argtypes = [c_size_t] PyObject_Malloc.restype = c_void_p PyObject_Free.argtypes = [c_void_p] PyObject_Free.restype = None def allocate_general_memory(n): Allocates `n` bytes of general purpose memory. return PyMem_RawMalloc(n) def reallocate_general_memory(p, n): Reallocates the memory pointed to by `p` to `n` bytes. return PyMem_RawRealloc(p, n) def free_general_memory(p): Frees the memory pointed to by `p`. PyMem_RawFree(p) def allocate_object_memory(n): Allocates `n` bytes of memory for object use. return PyObject_Malloc(n) def free_object_memory(p): Frees the object memory pointed to by `p`. PyObject_Free(p)"},{"question":"**Objective:** Demonstrate understanding and usage of the `traceback` module in Python. **Problem Statement:** You are required to write a Python function that: 1. Generates an exception within a nested function structure. 2. Captures the stack trace of the exception. 3. Formats the captured stack trace and exception details into a specific string format. 4. Returns the formatted string. **Function Signature:** ```python def generate_and_format_exception() -> str: pass ``` # Requirements: 1. **Nested Functions Structure:** - Implement three nested functions: `function_a`, `function_b`, and `function_c`. - Each function should call the next function in the chain, and `function_c` should raise an `IndexError`. 2. **Capture the Exception:** - Use a try-except block in the `function_a` that captures any exception raised in the nested functions. - Utilize the `traceback` module to capture and format the stack trace and exception details. 3. **Format the Stack Trace and Exception:** - Capture the stack trace using `traceback.extract_tb()` or equivalent. - Format the stack trace and exception details using `traceback.format_exception()`. - Generate a single formatted string including the entire stack trace and the exception message. The string should start with \\"Captured Exception:\\" followed by the formatted content. 4. **Return the Formatted String:** - Ensure the finalized string includes detailed information for debugging, similar to the format provided by `traceback.print_exception()`. # Example Output: ```python Captured Exception: Traceback (most recent call last): File \\"your_script.py\\", line X, in function_a function_b() File \\"your_script.py\\", line Y, in function_b function_c() File \\"your_script.py\\", line Z, in function_c raise IndexError(\'Intentional Error\') IndexError: Intentional Error ``` # Constraints: - The functions must raise exceptions naturally via the nested calls. - Use the `traceback` module functionalities efficiently. - Ensure that the output format is as close to the interpreter\'s exception message as possible. **Note:** Define the functions within `generate_and_format_exception` to maintain scope integrity. **Additional Challenge (Optional):** - Implement the `TracebackException` class to capture additional details and format them. **Hints:** - Utilize `traceback.extract_tb()` for extracting the stack trace. - Use `traceback.format_exception()` for putting together the stack trace and exception details. Good luck!","solution":"import traceback def generate_and_format_exception() -> str: def function_a(): try: function_b() except Exception as e: tb = traceback.format_exception(type(e), e, e.__traceback__) return \\"Captured Exception: \\" + \\"\\".join(tb) def function_b(): function_c() def function_c(): raise IndexError(\'Intentional Error\') return function_a()"},{"question":"# Hyper-Parameter Tuning with `GridSearchCV` and `Pipeline` **Objective**: Implement a machine learning pipeline using scikit-learn that includes data preprocessing and model training. Use `GridSearchCV` to optimize the hyper-parameters of the model to achieve the best cross-validation score. **Task**: 1. Load the Iris dataset from scikit-learn. 2. Create a `Pipeline` that includes: - A `StandardScaler` for feature scaling. - A Support Vector Classifier (SVC) model. 3. Use `GridSearchCV` to find the best hyper-parameters from the following parameter grid: ```python param_grid = [ {\'svc__C\': [1, 10, 100, 1000], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [1, 10, 100, 1000], \'svc__gamma\': [0.001, 0.0001], \'svc__kernel\': [\'rbf\']}, ] ``` 4. Print the best hyper-parameters and the best cross-validation score achieved. **Input**: - None (The Iris dataset is loaded internally) **Output**: - Best hyper-parameters found as a dictionary. - Best cross-validation score as a float. **Constraints**: - Use `StandardScaler` for data scaling. - Use `SVC` from `sklearn.svm`. - Use `GridSearchCV` from `sklearn.model_selection`. - Use `Pipeline` from `sklearn.pipeline`. **Performance Requirements**: - The entire pipeline, along with hyper-parameter tuning, should complete within a reasonable time frame (e.g., 2 minutes) on a standard machine. **Example**: ```python from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC import numpy as np # 1. Load dataset iris = load_iris() X, y = iris.data, iris.target # 2. Create the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # 3. Define the parameter grid param_grid = [ {\'svc__C\': [1, 10, 100, 1000], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [1, 10, 100, 1000], \'svc__gamma\': [0.001, 0.0001], \'svc__kernel\': [\'rbf\']}, ] # 4. Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X, y) # 5. Print the results print(\\"Best Parameters:\\", grid_search.best_params_) print(\\"Best Cross-Validation Score:\\", grid_search.best_score_) ``` Please implement the above steps in a single script.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC def hyperparameter_tuning_with_pipeline(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Create the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Define the parameter grid param_grid = [ {\'svc__C\': [1, 10, 100, 1000], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [1, 10, 100, 1000], \'svc__gamma\': [0.001, 0.0001], \'svc__kernel\': [\'rbf\']}, ] # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X, y) # Return the results return grid_search.best_params_, grid_search.best_score_"},{"question":"# Python List Operations Module Your task is to implement a Python module that provides functionalities similar to those described in the provided documentation for Python list objects using C-API. However, you must implement these functionalities in Python without using the list\'s built-in methods to demonstrate your understanding of list operations and data handling. Required Functionalities 1. **Check if an object is a list:** - Implement a function `is_list(obj)` that returns `True` if the given object is a list, and `False` otherwise. 2. **Create a new list:** - Implement a function `create_list(length)` that returns a new list of the specified length, filled with `None`. 3. **Get the length of the list:** - Implement a function `get_length(lst)` that returns the length of the given list. 4. **Get an item from the list:** - Implement a function `get_item(lst, index)` that returns the item at the specified index from the list. If the index is out of bounds, raise an `IndexError`. 5. **Set an item in the list:** - Implement a function `set_item(lst, index, item)` that sets the item at the specified index in the list. If the index is out of bounds, raise an `IndexError`. 6. **Insert an item into the list:** - Implement a function `insert_item(lst, index, item)` that inserts the item at the specified index in the list. 7. **Append an item to the list:** - Implement a function `append_item(lst, item)` that appends the item to the end of the list. 8. **Sort the list:** - Implement a function `sort_list(lst)` that sorts the items of the list in place. 9. **Reverse the list:** - Implement a function `reverse_list(lst)` that reverses the items of the list in place. 10. **Convert the list to a tuple:** - Implement a function `to_tuple(lst)` that returns a new tuple containing the contents of the list. Constraints - You may not use any of Python\'s built-in list methods (e.g., `list.append()`, `list.insert()`, `list.sort()`, etc.). - Your solution should handle edge cases appropriately (e.g., empty lists, invalid indices). Example Usage ```python # Check if an object is a list print(is_list([1, 2, 3])) # True print(is_list(\\"hello\\")) # False # Create a new list lst = create_list(3) print(lst) # [None, None, None] # Get the length of the list print(get_length(lst)) # 3 # Set an item in the list set_item(lst, 1, \\"Python\\") print(lst) # [None, \\"Python\\", None] # Get an item from the list print(get_item(lst, 1)) # \\"Python\\" # Insert an item into the list insert_item(lst, 1, \\"3.10\\") print(lst) # [None, \\"3.10\\", \\"Python\\", None] # Append an item to the list append_item(lst, \\"Language\\") print(lst) # [None, \\"3.10\\", \\"Python\\", None, \\"Language\\"] # Sort the list sort_list(lst) print(lst) # [None, None, \\"3.10\\", \\"Language\\", \\"Python\\"] # Reverse the list reverse_list(lst) print(lst) # [\\"Python\\", \\"Language\\", \\"3.10\\", None, None] # Convert the list to a tuple tup = to_tuple(lst) print(tup) # (\\"Python\\", \\"Language\\", \\"3.10\\", None, None) ``` Implement the module with the specified functionalities to the best of your ability.","solution":"def is_list(obj): Returns True if the given object is a list, False otherwise. return isinstance(obj, list) def create_list(length): Returns a new list of the specified length, filled with None. return [None] * length def get_length(lst): Returns the length of the given list. length = 0 for _ in lst: length += 1 return length def get_item(lst, index): Returns the item at the specified index from the list. If the index is out of bounds, raise an IndexError. if index < 0 or index >= get_length(lst): raise IndexError(\\"Index out of range.\\") return lst[index] def set_item(lst, index, item): Sets the item at the specified index in the list. If the index is out of bounds, raise an IndexError. if index < 0 or index >= get_length(lst): raise IndexError(\\"Index out of range.\\") lst[index] = item def insert_item(lst, index, item): Inserts the item at the specified index in the list. if index < 0 or index > get_length(lst): raise IndexError(\\"Index out of range.\\") result = [] for i in range(get_length(lst)): if i == index: result.append(item) result.append(lst[i]) if index == get_length(lst): result.append(item) lst[:] = result def append_item(lst, item): Appends the item to the end of the list. lst[:] = lst + [item] def sort_list(lst): Sorts the items of the list in place. n = get_length(lst) for i in range(n): for j in range(0, n-i-1): if lst[j] is not None and lst[j+1] is not None and lst[j] > lst[j+1]: lst[j], lst[j+1] = lst[j+1], lst[j] elif lst[j] is None and lst[j+1] is not None: lst[j], lst[j+1] = lst[j+1], lst[j] def reverse_list(lst): Reverses the items of the list in place. n = get_length(lst) for i in range(n // 2): lst[i], lst[n-i-1] = lst[n-i-1], lst[i] def to_tuple(lst): Returns a new tuple containing the contents of the list. return tuple(lst)"},{"question":"# Python Parsing Challenge In this challenge, you are required to demonstrate your understanding of Python\'s Parsing Expression Grammar (PEG) by implementing a function that helps parse and validate a subset of Python syntax based on the given grammar rules. Task: You need to extend the functionality of a simple parser to handle parsing \\"match\\" statements, which were introduced in Python 3.10. The grammar for \\"match\\" statements is specified in the provided grammar documentation. Requirements: 1. Implement the functions necessary to parse a \\"match\\" statement according to the following rules extracted from the grammar documentation: ``` match_stmt[stmt_ty]: | \\"match\\" subject=subject_expr \':\' NEWLINE INDENT cases[asdl_match_case_seq*]=case_block+ DEDENT { _PyAST_Match(subject, cases, EXTRA) } | invalid_match_stmt subject_expr[expr_ty]: | value=star_named_expression \',\' values=star_named_expressions? { _PyAST_Tuple(CHECK(asdl_expr_seq*, _PyPegen_seq_insert_in_front(p, value, values)), Load, EXTRA) } | named_expression case_block[match_case_ty]: | \\"case\\" pattern=patterns guard=guard? \':\' body=block { _PyAST_match_case(pattern, guard, body, p->arena) } guard[expr_ty]: \'if\' guard=named_expression { guard } patterns[pattern_ty]: | patterns[asdl_pattern_seq*]=open_sequence_pattern { _PyAST_MatchSequence(patterns, EXTRA) } | pattern ``` 2. Your implementation should support parsing a match statement and its cases while ensuring it follows the provided grammar\'s rules. Input: - A string representing a Python \\"match\\" statement. Output: - An AST (Abstract Syntax Tree) that represents the parsed match statement according to the provided grammar rules. Constraints: 1. The input will always be a syntactically correct \\"match\\" statement according to Python 3.10 syntax. 2. You should not handle invalid syntax or implement error handling beyond what\'s specified in the \\"match\\" statement rules. Example: ```python def parse_match_statement(match_statement: str) -> AstNode: # Your implementation here # Example usage: match_statement = \'\'\' match some_expr: case pattern1: do_something() case pattern2 if condition: do_something_else() \'\'\' result = parse_match_statement(match_statement) print(result) # Output should be an appropriate AST representation of the match statement ``` Notes: - You need to define necessary classes or data structures to represent the AST nodes. - You can assume utilities like `CHECK` and `_PyAST_*` functions are available as part of the implementation environment (which you might need to mock for your solution). - The provided solution should respect the hierarchy and structure defined by the provided grammar snippets.","solution":"import ast def parse_match_statement(match_statement: str) -> ast.Match: Parse a Python \'match\' statement and return the corresponding AST node. # Using Python\'s built-in ast module to parse the statement parsed_ast = ast.parse(match_statement).body[0] if isinstance(parsed_ast, ast.Match): return parsed_ast else: raise ValueError(\\"Provided statement is not a valid \'match\' statement.\\") # For ease of creating AST nodes from strings, this function extracts the relevant # part of the AST by parsing a full \'match\' statement string."},{"question":"Objective Write a function in Python that generates a set of UUIDs, converts them into different formats, and ensures their uniqueness and safety. Task Description 1. **Generating UUIDs:** - Generate `N` version 4 UUIDs and store them in a list. - Ensure that all UUIDs in the list are unique. 2. **UUID Conversion:** - For each UUID in the list, convert it to the following formats: - Hexadecimal string (without curly braces and hyphens). - Integer. - Bytes. - URN format. 3. **Safety Check:** - Return a list of tuples, each containing: - The original UUID (as a string with standard hyphens). - Its hexadecimal string format. - Its integer format. - Its byte format. - Its URN format. - A boolean indicating if it was generated in a multiprocessing-safe way. Function Signature ```python from typing import List, Tuple import uuid def generate_and_convert_uuids(N: int) -> List[Tuple[str, str, int, bytes, str, bool]]: pass ``` Input - `N`: An integer `N` (1 <= N <= 100) indicating the number of UUIDs to generate. Output - A list of tuples, each containing: - Original UUID as a string. - Hexadecimal string format of the UUID. - Integer format of the UUID. - Byte format of the UUID. - URN format of the UUID. - A boolean indicating UUID safety. Constraints - The function should ensure that all UUIDs generated are unique. - The boolean for safety should correctly reflect the `is_safe` attribute (`True` for `SafeUUID.safe`, `False` otherwise). Example ```python # Example call to the function N = 2 result = generate_and_convert_uuids(N) for item in result: print(item) # Expected output (the UUIDs and safety boolean may vary): # [ # (\'6fa459ea-ee8a-3ca4-894e-db77e160355e\', \'6fa459ea3ca4894edb77e160355e\', 19296329823683410242523887086039436094, b\'06fa459ea3ca489e\', \'urn:uuid:6fa459ea-3ca4-894e-db77-e160355e199e\', True), # (\'16fd2706-8baf-433b-82eb-8c7fada847da\', \'16fd27068baf433b82eb8c7fada847da\', 306216672616924046180239230211295106338, b\'16fd27068baf433b\', \'urn:uuid:16fd2706-8baf-433b-82eb-8c7fada847da\', True) # ] ``` Notes - Use the `uuid.uuid4()` function to generate version 4 UUIDs. - Ensure UUIDs are processed and stored in their appropriate formats. - Leverage the `uuid` module and its attributes and methods as specified in the documentation provided.","solution":"from typing import List, Tuple import uuid def generate_and_convert_uuids(N: int) -> List[Tuple[str, str, int, bytes, str, bool]]: uuid_list = [] for _ in range(N): u = uuid.uuid4() uuid_str = str(u) uuid_hex = u.hex uuid_int = u.int uuid_bytes = u.bytes uuid_urn = u.urn uuid_safe = u.is_safe is uuid.SafeUUID.safe uuid_list.append((uuid_str, uuid_hex, uuid_int, uuid_bytes, uuid_urn, uuid_safe)) return uuid_list"},{"question":"# Feature Extraction and Classification Using Scikit-Learn You are provided with a dataset of textual movie reviews and their associated ratings (positive or negative). Your task is to build a machine learning pipeline that preprocesses the text data, extracts relevant features using suitable methods from scikit-learn, and trains a classifier to predict the sentiment of the reviews. 1. **Loading the dataset**: Assume the dataset is provided as a list of dictionaries, where each dictionary has two keys: \\"review\\" and \\"sentiment\\". The \\"review\\" key contains the text of the movie review, and \\"sentiment\\" key contains either \\"positive\\" or \\"negative\\". 2. **Text Preprocessing and Feature Extraction**: - Use `CountVectorizer` for extracting features from the text. - Apply `TfidfTransformer` to convert the count matrix to a normalized tf-idf representation. 3. **Training the Classifier**: - Use a Naive Bayes classifier for training the model. 4. **Evaluation**: - Evaluate the classifier using accuracy score on the test data. # Implementation Steps: 1. **Define a function** `load_data()` to load the dataset and split it into training and testing sets. - **Input**: None - **Output**: `X_train, X_test, y_train, y_test` (training and testing splits of reviews and sentiments) 2. **Define a function** `build_pipeline()` to create the machine learning pipeline. - **Input**: None - **Output**: A scikit-learn pipeline object that integrates text feature extraction and the classifier. 3. **Define a function** `train_and_evaluate()` to train the model and evaluate its performance. - **Input**: `X_train, X_test, y_train, y_test` - **Output**: Accuracy score of the trained model on the test data. # Constraints: - Assume that the dataset is preloaded in the required format. # Example: ```python # Example preloaded data dataset = [ {\\"review\\": \\"I loved this movie, it was fantastic!\\", \\"sentiment\\": \\"positive\\"}, {\\"review\\": \\"Absolutely terrible, one of the worst films I have seen.\\", \\"sentiment\\": \\"negative\\"}, # ... (more data) ] def load_data(): # Implement function logic to split dataset into train and test sets pass def build_pipeline(): # Implement function logic to build a scikit-learn pipeline pass def train_and_evaluate(X_train, X_test, y_train, y_test): # Implement function logic to train the model and evaluate its accuracy pass # Example Usage X_train, X_test, y_train, y_test = load_data() pipeline = build_pipeline() accuracy = train_and_evaluate(X_train, X_test, y_train, y_test) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` # Notes: - Clearly comment each step of your code. - Ensure all necessary imports are included at the beginning of your script. - Handle any preprocessing that might be necessary before feature extraction.","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Example preloaded data dataset = [ {\\"review\\": \\"I loved this movie, it was fantastic!\\", \\"sentiment\\": \\"positive\\"}, {\\"review\\": \\"Absolutely terrible, one of the worst films I have seen.\\", \\"sentiment\\": \\"negative\\"}, {\\"review\\": \\"The movie was okay, nothing special.\\", \\"sentiment\\": \\"neutral\\"}, {\\"review\\": \\"This film is a masterpiece with stunning visual effects.\\", \\"sentiment\\": \\"positive\\"}, {\\"review\\": \\"I hate this movie. It was a waste of time.\\", \\"sentiment\\": \\"negative\\"} # ... (more data) ] def load_data(): reviews = [item[\'review\'] for item in dataset] sentiments = [item[\'sentiment\'] for item in dataset] X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def build_pipeline(): pipeline = Pipeline([ (\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'clf\', MultinomialNB()), ]) return pipeline def train_and_evaluate(X_train, X_test, y_train, y_test): pipeline = build_pipeline() pipeline.fit(X_train, y_train) predictions = pipeline.predict(X_test) return accuracy_score(y_test, predictions) # Example Usage X_train, X_test, y_train, y_test = load_data() pipeline = build_pipeline() accuracy = train_and_evaluate(X_train, X_test, y_train, y_test) print(f\\"Model Accuracy: {accuracy:.2f}\\")"},{"question":"**Resource Management Challenge** You are tasked with writing a Python function that monitors and controls system resource usage for a given process. Specifically, you must: 1. Retrieve and print the current soft and hard limits for the maximum CPU time (`RLIMIT_CPU`) and maximum number of open file descriptors (`RLIMIT_NOFILE`) for the current process. 2. Set new limits for these resources: - New CPU time limit: 1 second (soft limit), 2 seconds (hard limit) - New file descriptors limit: 256 (soft limit), 1024 (hard limit) 3. Verify and print the updated limits. 4. Execute a function `cpu_intensive_task(duration)` that performs a CPU-intensive task for a specified duration (in seconds). Monitor and print the resource usage (CPU time) before and after executing the task. # Function Signature ```python from typing import Callable def monitor_and_control_resources(cpu_intensive_task: Callable[[int], None], duration: int) -> None: pass ``` # Input - `cpu_intensive_task`: A function that takes an integer argument `duration` and performs a CPU-intensive task for that duration. - `duration`: An integer representing the duration (in seconds) for which the `cpu_intensive_task` should run. # Output The function should print: - Initial CPU and file descriptor limits. - Updated CPU and file descriptor limits. - Resource usage (CPU time) before and after executing the `cpu_intensive_task`. # Constraints - The function should handle possible exceptions and print relevant error messages if any operations fail (e.g., if the user does not have permission to set limits). - The task should only be executed if the limits are successfully updated. - The execution environment is assumed to be a Unix-like system that supports the `resource` module functionality described. # Example Usage ```python def cpu_intensive_task(duration: int): for _ in range(duration * 10**6): pass monitor_and_control_resources(cpu_intensive_task, 2) ``` # Example Output ``` Initial RLIMIT_CPU: (soft_limit, hard_limit) Initial RLIMIT_NOFILE: (soft_limit, hard_limit) Updated RLIMIT_CPU: (1, 2) Updated RLIMIT_NOFILE: (256, 1024) Resource usage before task: user_time=<time>, system_time=<time> Resource usage after task: user_time=<time>, system_time=<time> ``` **Note: Ensure to appropriately catch and handle any exceptions that may arise during the execution of the function.**","solution":"import resource import os def monitor_and_control_resources(cpu_intensive_task, duration): try: # Retrieve and print the current soft and hard limits for RLIMIT_CPU initial_cpu_limits = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Initial RLIMIT_CPU: {initial_cpu_limits}\\") # Retrieve and print the current soft and hard limits for RLIMIT_NOFILE initial_nofile_limits = resource.getrlimit(resource.RLIMIT_NOFILE) print(f\\"Initial RLIMIT_NOFILE: {initial_nofile_limits}\\") # Set new limits for RLIMIT_CPU new_cpu_limits = (1, 2) resource.setrlimit(resource.RLIMIT_CPU, new_cpu_limits) # Set new limits for RLIMIT_NOFILE new_nofile_limits = (256, 1024) resource.setrlimit(resource.RLIMIT_NOFILE, new_nofile_limits) # Verify and print the updated limits updated_cpu_limits = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Updated RLIMIT_CPU: {updated_cpu_limits}\\") updated_nofile_limits = resource.getrlimit(resource.RLIMIT_NOFILE) print(f\\"Updated RLIMIT_NOFILE: {updated_nofile_limits}\\") if updated_cpu_limits != new_cpu_limits or updated_nofile_limits != new_nofile_limits: print(\\"Failed to set resource limits properly.\\") return # Monitor and print the resource usage (CPU time) before executing the task usage_before = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Resource usage before task: user_time={usage_before.ru_utime}, system_time={usage_before.ru_stime}\\") # Execute the CPU-intensive task cpu_intensive_task(duration) # Monitor and print the resource usage (CPU time) after executing the task usage_after = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Resource usage after task: user_time={usage_after.ru_utime}, system_time={usage_after.ru_stime}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# WAV File Manipulation with Python\'s \\"wave\\" Module You are tasked with writing a Python function that reads a WAVE file, processes the audio data by doubling the volume, and writes the processed data to a new WAVE file. # Function Signature ```python def double_volume_of_wav(input_wav_path: str, output_wav_path: str) -> None: pass ``` # Description 1. **Input Parameters:** - `input_wav_path` (str): The path to the input WAV file. - `output_wav_path` (str): The path to the output WAV file where the processed audio should be saved. 2. **Output:** - None. The function should output the processed WAV data to a file specified by `output_wav_path`. # Requirements 1. **Reading:** - Read the input WAV file using the `wave` module. - Extract header information such as the number of channels, sample width, frame rate, and number of frames. 2. **Processing:** - For each sample in the WAV file, double its amplitude. Be cautious about overflow when doubling the amplitude (ensure values remain within valid range for their data type). 3. **Writing:** - Write the processed audio data to the output WAV file using the `wave` module. - Ensure the header information (such as number of channels, sample width, frame rate, etc.) is correctly set for the output file. # Constraints - The function must only use the `wave` module for reading and writing WAV files. - The WAV file to be processed will have a sample width of 2 bytes and either 1 (mono) or 2 (stereo) audio channels. # Example Given an input WAV file `input.wav`: - Use the function `double_volume_of_wav(\'input.wav\', \'output.wav\')`. - The output should be a new WAV file `output.wav` with the audio volume doubled. # Notes - The sample width is 2 bytes, meaning each sample value ranges from -32768 to 32767. - You may use additional Python built-ins for data manipulation (e.g., `struct` for unpacking binary data). - Ensure the processing does not introduce significant distortion or clipping. You may assume the input WAV file is valid and accessible. # Hints - Reading frames in chunks might be necessary for large files. - Consider using `struct.unpack` and `struct.pack` to convert between bytes and integers. # Solution Template ```python import wave import struct def double_volume_of_wav(input_wav_path: str, output_wav_path: str) -> None: # Read input file with wave.open(input_wav_path, \'rb\') as reader: params = reader.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read all frames frames = reader.readframes(n_frames) # Process frames processed_frames = bytearray() for i in range(0, len(frames), sampwidth): frame = frames[i:i+sampwidth] if sampwidth == 2: sample = struct.unpack(\'<h\', frame)[0] sample = max(-32768, min(32767, sample * 2)) processed_frames.extend(struct.pack(\'<h\', sample)) # Write to output file with wave.open(output_wav_path, \'wb\') as writer: writer.setparams(params) # Set same parameters as input file writer.writeframes(processed_frames) ```","solution":"import wave import struct def double_volume_of_wav(input_wav_path: str, output_wav_path: str) -> None: Doubles the volume of the audio data in the input WAV file and writes the processed data to the output WAV file. :param input_wav_path: Path to the input WAV file. :param output_wav_path: Path to the output WAV file. with wave.open(input_wav_path, \'rb\') as reader: params = reader.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read all frames frames = reader.readframes(n_frames) # Process frames processed_frames = bytearray() for i in range(0, len(frames), sampwidth): frame = frames[i:i+sampwidth] if sampwidth == 2: sample = struct.unpack(\'<h\', frame)[0] doubled_sample = sample * 2 # Clipping to prevent overflow processed_sample = max(-32768, min(32767, doubled_sample)) processed_frames.extend(struct.pack(\'<h\', processed_sample)) # Write to output file with wave.open(output_wav_path, \'wb\') as writer: writer.setparams(params) # Set same parameters as input file writer.writeframes(processed_frames)"},{"question":"# Custom Implementation of a Linear Regression Model with Validation and Sparse Matrix Handling In this task, you are required to implement a custom linear regression model class using utilities from the `sklearn.utils` module. Your model should be able to handle dense and sparse input data efficiently, perform necessary validation on the inputs, and ensure reproducibility through a controlled random state. Requirements 1. **CustomLinearRegression Class**: - Implement a class `CustomLinearRegression` that mimics the behavior of `sklearn.linear_model.LinearRegression`. - The class should have the following methods: - `fit(X, y)`: Takes in a feature matrix `X` and target vector `y`, fits the model to the data. - `predict(X)`: Takes in a feature matrix `X` and returns the predicted target values. - The model should use `np.linalg.lstsq` for solving the least squares problem. 2. **Validation**: - Use `sklearn.utils.check_X_y` to validate that `X` and `y` have consistent lengths and proper formats. - Use `sklearn.utils.check_array` to validate input data in both `fit` and `predict` methods. 3. **Sparse Matrix Handling**: - Ensure that the `fit` and `predict` methods can handle both dense and sparse matrices by leveraging the appropriate utilities. 4. **Reproducibility**: - Ensure reproducibility by incorporating a `random_state` parameter in the `__init__` method of the class and using `sklearn.utils.check_random_state`. Input and Output Formats - **Input**: - `X`: A 2D numpy array or a `scipy.sparse` matrix of shape (n_samples, n_features) with float values. - `y`: A 1D numpy array of shape (n_samples,) with float values. - `random_state`: An integer, `np.random.RandomState` instance, or `None` (default). - **Output**: - For `fit`: No return value. The method fits the model to the input data. - For `predict`: A 1D numpy array of predicted target values. Constraints - Your implementation should handle up to 100,000 samples and 1,000 features efficiently. - Sparse matrix operations should maintain performance comparable to dense matrix operations. Example Usage ```python from scipy.sparse import csr_matrix import numpy as np # Sample dense data X_dense = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) # Sample sparse data X_sparse = csr_matrix([[1, 0], [0, 1], [1, 1]]) # Create and fit the model model = CustomLinearRegression(random_state=42) model.fit(X_dense, y) # Predict predictions_dense = model.predict(X_dense) print(predictions_dense) # Fit the model on sparse data model.fit(X_sparse, y) # Predict predictions_sparse = model.predict(X_sparse) print(predictions_sparse) ``` Implement the `CustomLinearRegression` class.","solution":"import numpy as np from sklearn.utils import check_X_y, check_array, check_random_state from scipy.sparse import issparse class CustomLinearRegression: def __init__(self, random_state=None): self.random_state = check_random_state(random_state) self.coef_ = None self.intercept_ = None def fit(self, X, y): # Validate inputs X, y = check_X_y(X, y, accept_sparse=[\'csr\', \'csc\', \'coo\']) if issparse(X): # Convert sparse to dense for lstsq solution (efficient for small feature sets) X = X.toarray() X_b = np.hstack([np.ones((X.shape[0], 1)), X]) solution, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=None) self.intercept_ = solution[0] self.coef_ = solution[1:] def predict(self, X): # Validate inputs X = check_array(X, accept_sparse=[\'csr\', \'csc\', \'coo\']) if issparse(X): # Convert sparse to dense for prediction X = X.toarray() return np.dot(X, self.coef_) + self.intercept_"},{"question":"# Mocking Asynchronous Functions with `unittest.mock` **Objective**: The goal of this task is to assess your understanding and ability to utilize `unittest.mock` to effectively mock and test asynchronous interactions in a Python module. # Problem Statement You are developing an asynchronous service that queries an external API and processes the returned data. The service contains a function `get_and_process_data` that fetches data asynchronously, processes it, and stores the results in a database. Your task is to unit test this function by mocking the external API call and ensuring that its interactions within `get_and_process_data` adhere to the specified behavior. Your implementation should involve: 1. Mocking the asynchronous API call to return predefined data. 2. Asserting that the process and storage functions are called with the correct arguments. 3. Using `assert_awaited_with` to verify that the asynchronous function is awaited with the correct parameters. # Requirements - **Function to Test**: ```python import aiohttp import asyncio class DataProcessor: async def fetch_data(self, url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.json() async def process_data(self, data): # Simulate processing await asyncio.sleep(1) return [item[\'value\'] * 2 for item in data] async def store_data(self, processed_data): # Simulate storing to database await asyncio.sleep(1) async def get_and_process_data(self, url): data = await self.fetch_data(url) processed_data = await self.process_data(data) await self.store_data(processed_data) ``` - **Test Case**: - Use `unittest.mock.AsyncMock` to simulate the asynchronous behaviors. - Ensure `fetch_data` returns a predefined JSON object. - Verify that `process_data` is called with the correct fetched data. - Verify that `store_data` is called with the correctly processed data. # Constraints 1. Use `unittest.mock` to mock asynchronous methods. 2. Ensure proper order of method calls using `assert_has_calls`. 3. All mocks should correctly behave as asynchronous methods, ensuring use of `AsyncMock`. # Expected Input and Output - Input: URL for fetching data. - Output: None (Function does not return any value). - Mocked `fetch_data` should return: `[{ \\"value\\": 1}, { \\"value\\": 2}, { \\"value\\": 3 }]` - Processed data to be passed to `store_data`: `[2, 4, 6]` # Implementation Provide the detailed implementation of the test case using `unittest`.","solution":"import aiohttp import asyncio class DataProcessor: async def fetch_data(self, url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.json() async def process_data(self, data): # Simulate processing await asyncio.sleep(1) return [item[\'value\'] * 2 for item in data] async def store_data(self, processed_data): # Simulate storing to database await asyncio.sleep(1) async def get_and_process_data(self, url): data = await self.fetch_data(url) processed_data = await self.process_data(data) await self.store_data(processed_data)"},{"question":"# Advanced Python Programming: System Interaction and Introspection Problem Statement In this coding task, you are required to write a Python script that utilizes various aspects of the `sys` module to perform the following: 1. **Command Line Argument Processing**: - Your script should accept a list of file paths as command-line arguments. If no arguments are provided, the script should print an error message and exit the program. 2. **Audit Hook Implementation**: - Implement a custom auditing hook function that logs every event raised by the `sys.audit()` function. The log should contain the event name and any arguments passed to the event. 3. **System Introspection**: - Implement a function to print debugging information about the current state of significant variables and functions at the moment the script is executed: - Print the Python executable being used. - Print the current recursion limit. - Print the number of reference counts for a specific object (choose a standard object like `None`). - Print the top frame of the current call stack. 4. **Set Hook and Restore Original State**: - Set a recursion limit within your script (e.g., `1000`) and ensure to restore the original recursion limit at the end of the script execution. - Implement a breakpointhook that prints a message and uses `pdb` to break into the debugger. - Ensure to restore the original breakpointhook at the end of the script execution. Input - List of file paths as command-line arguments. Expected Output - Print debugging information. - Log of audit events. - Error message if no arguments are provided. Constraints - The program should not alter the `sys` module state persistently (e.g., recursion limit must be restored). - Handle any exceptions gracefully and provide meaningful error messages. Performance Requirements - The script should perform introspective operations efficiently without introducing significant overhead. Example Execution ```sh python advanced_system_script.py file1.txt file2.txt ``` Expected output: ``` Error: No file paths provided or Execution debugging information: - Executable: /usr/bin/python3 - Recursion limit: 3000 - Reference count for None: 10000 - Top stack frame: <frame at 0x7f8b8c0ff780, file \'example.py\', line 1, code <module>> Audit log: - Event: sys.addaudithook Args: () ... ``` Implementation Notes - You are allowed to use any built-in Python modules. - Ensure to comment your code adequately to explain the logic and flow.","solution":"import sys import pdb def print_debugging_info(): Print debugging information about current state print(f\\"Executable: {sys.executable}\\") print(f\\"Recursion limit: {sys.getrecursionlimit()}\\") print(f\\"Reference count for None: {sys.getrefcount(None)}\\") current_frame = sys._getframe() print(f\\"Top stack frame: {current_frame}\\") def audit_hook(event, args): Custom audit hook function to log events print(f\\"Audit event: {event} Args: {args}\\") def main(): if len(sys.argv) < 2: print(\\"Error: No file paths provided\\") sys.exit(1) # Set custom audit hook sys.addaudithook(audit_hook) # Store the original recursion limit original_recursion_limit = sys.getrecursionlimit() # Set a new recursion limit new_recursion_limit = 1000 sys.setrecursionlimit(new_recursion_limit) # Store the original breakpointhook and set a custom one original_breakpointhook = sys.breakpointhook sys.breakpointhook = lambda: (print(\\"Custom breakpoint triggered\\"), pdb.set_trace()) try: print_debugging_info() finally: # Restore original recursion limit sys.setrecursionlimit(original_recursion_limit) # Restore original breakpointhook sys.breakpointhook = original_breakpointhook if __name__ == \\"__main__\\": main()"},{"question":"Design and implement a Python class called `SafeNumberOperations` that performs safe numerical operations based on the Python numerical protocols described above. This class should ensure that any operation that results in an error should handle the error gracefully and return an appropriate message. Class Design Requirements: 1. **Constructor**: - The class should have an `__init__` method that initializes the object with a number (integer or floating point). 2. **Methods**: - `add(self, other)`: Returns the result of adding the object\'s number to `other`. - `subtract(self, other)`: Returns the result of subtracting `other` from the object\'s number. - `multiply(self, other)`: Returns the result of multiplying the object\'s number with `other`. - `divide(self, other)`: Returns the result of true division of the object\'s number by `other`. Raises an error if `other` is zero. - `modulus(self, other)`: Returns the remainder of dividing the object\'s number by `other`. Raises an error if `other` is zero. - `power(self, exp, mod=None)`: Returns the result of raising the object\'s number to the power of `exp`. If `mod` is provided, it should return the result of raising the number to the power `exp`, modulo `mod`. - `negate(self)`: Returns the negation of the object\'s number. - `bitwise_and(self, other)`: Returns the result of bitwise AND between the object\'s number and `other`. - `bitwise_or(self, other)`: Returns the result of bitwise OR between the object\'s number and `other`. - `left_shift(self, positions)`: Returns the result of left shifting the object\'s number by `positions`. - `right_shift(self, positions)`: Returns the result of right shifting the object\'s number by `positions`. Constraints: - None of the integers involved in the operations (including the object\'s number and `other`) will exceed `10^9` in absolute value. - Do not use the built-in `eval` function. Example Usage: ```python number = SafeNumberOperations(10) print(number.add(5)) # Output: 15 print(number.subtract(3)) # Output: 7 print(number.multiply(6)) # Output: 60 print(number.divide(2)) # Output: 5.0 print(number.modulus(3)) # Output: 1 print(number.power(3)) # Output: 1000 print(number.power(2, 3)) # Output: 1 (10^2 % 3) print(number.negate()) # Output: -10 print(number.bitwise_and(6)) # Output: 2 (binary 10 & 110) print(number.bitwise_or(2)) # Output: 10 (binary 10 | 10) print(number.left_shift(1)) # Output: 20 (binary 1010 << 1) print(number.right_shift(1)) # Output: 5 (binary 1010 >> 1) ``` **For this assignment, you need to provide the complete implementation of the `SafeNumberOperations` class. Make sure to follow the constraints strictly and handle exceptions where numerical errors might occur.**","solution":"class SafeNumberOperations: def __init__(self, number): self.number = number def add(self, other): return self.number + other def subtract(self, other): return self.number - other def multiply(self, other): return self.number * other def divide(self, other): try: result = self.number / other except ZeroDivisionError: return \\"Error: Division by zero!\\" return result def modulus(self, other): try: result = self.number % other except ZeroDivisionError: return \\"Error: Division by zero!\\" return result def power(self, exp, mod=None): try: if mod is not None: result = pow(self.number, exp, mod) else: result = self.number ** exp except (TypeError, ValueError) as e: return f\\"Error: {str(e)}\\" return result def negate(self): return -self.number def bitwise_and(self, other): return self.number & other def bitwise_or(self, other): return self.number | other def left_shift(self, positions): return self.number << positions def right_shift(self, positions): return self.number >> positions"},{"question":"# Python Class and Inheritance with Iterators You are required to design a class structure that models different kinds of vehicles in a garage. Follow the instructions below to implement your solution. 1. **Base Class `Vehicle`:** - **Attributes**: - `make`: A string representing the manufacturer of the vehicle. - `model`: A string representing the model of the vehicle. - `year`: An integer representing the manufacturing year. - **Methods**: - `__init__(self, make, model, year)`: The constructor for the class. - `__str__(self)`: Returns a string representation of the vehicle in the format \\"<year> <make> <model>\\". 2. **Derived Class `Car` (inherits from `Vehicle`):** - **Attributes**: - `doors`: An integer representing the number of doors. - **Methods**: - `__init__(self, make, model, year, doors)`: The constructor for the class. - `__str__(self)`: Returns a string representation of the car in the format \\"<year> <make> <model> with <doors> doors\\". 3. **Derived Class `Truck` (inherits from `Vehicle`):** - **Attributes**: - `capacity`: An integer representing the load capacity in tons. - **Methods**: - `__init__(self, make, model, year, capacity)`: The constructor for the class. - `__str__(self)`: Returns a string representation of the truck in the format \\"<year> <make> <model> with <capacity> ton capacity\\". 4. **Class `Garage`:** - **Attributes**: - `vehicles`: A list that can store instances of `Vehicle` (or its subclasses). - **Methods**: - `__init__(self)`: Initializes an empty list of vehicles. - `add_vehicle(self, vehicle)`: Adds a vehicle to the vehicle list. - `__iter__(self)`: Returns an iterator that can loop over the vehicles. - `__next__(self)`: Iterate through the list of vehicles in the garage. # Example Usage: ```python # Creating instances of Car and Truck car1 = Car(make=\\"Toyota\\", model=\\"Camry\\", year=2020, doors=4) truck1 = Truck(make=\\"Ford\\", model=\\"F-150\\", year=2019, capacity=5) # Creating an instance of Garage and adding vehicles garage = Garage() garage.add_vehicle(car1) garage.add_vehicle(truck1) # Iterating over the vehicles in the garage for vehicle in garage: print(vehicle) ``` # Expected Output: ``` 2020 Toyota Camry with 4 doors 2019 Ford F-150 with 5 ton capacity ``` **Constraints:** - Ensure proper encapsulation within the classes. - Use Python\'s iterator protocol properly in the `Garage` class. **Submission Requirements:** - Implement the `Vehicle`, `Car`, `Truck`, and `Garage` classes as described. - Demonstrate the usage by creating instances and iterating over them as shown in the example.","solution":"class Vehicle: def __init__(self, make, model, year): self.make = make self.model = model self.year = year def __str__(self): return f\\"{self.year} {self.make} {self.model}\\" class Car(Vehicle): def __init__(self, make, model, year, doors): super().__init__(make, model, year) self.doors = doors def __str__(self): return f\\"{self.year} {self.make} {self.model} with {self.doors} doors\\" class Truck(Vehicle): def __init__(self, make, model, year, capacity): super().__init__(make, model, year) self.capacity = capacity def __str__(self): return f\\"{self.year} {self.make} {self.model} with {self.capacity} ton capacity\\" class Garage: def __init__(self): self.vehicles = [] self._index = 0 # This will be used for iteration def add_vehicle(self, vehicle): self.vehicles.append(vehicle) def __iter__(self): self._index = 0 # Reset index each time iteration starts return self def __next__(self): if self._index < len(self.vehicles): vehicle = self.vehicles[self._index] self._index += 1 return vehicle else: raise StopIteration"},{"question":"**Email Serialization and Validation with Python** **Objective:** You will implement a function that serializes email message objects using the `BytesGenerator` class from the `email.generator` module in Python. The function will also validate that the bytes generated match the original binary data. **Function Specification:** ```python def serialize_and_validate_email(email_message, policy=None): Serialize an email message object to its binary representation and validate that regenerating the bytes produces the identical output. Parameters: - email_message (EmailMessage): The email message object to be serialized. - policy (Policy, optional): An optional policy to control the message generation. Returns: - bool: True if the regenerated bytes match the original, False otherwise. pass ``` **Input:** - `email_message`: An instance of `EmailMessage` (from the `email.message` module). - `policy`: (Optional) An instance of `Policy` for controlling the message generation (default is None). **Output:** - Returns `True` if the regenerated bytes are identical to the original; `False` otherwise. **Constraints:** - The function must handle standard and MIME email messages. - It must use the `BytesGenerator` class for serialization. - The validation requires that the bytes output from `flatten()` match the input provided for the function. **Performance Requirements:** - The solution should operate efficiently on typical email message sizes and structures. - It should handle edge cases like non-ASCII characters and various MIME parts correctly. **Example Usage:** ```python from email.message import EmailMessage from email.generator import BytesGenerator import io # Create an example email message msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') # Example policy setup from email.policy import default policy = default.clone() # Call the function result = serialize_and_validate_email(msg, policy) print(result) # Expected output: True ``` **Hint:** - Utilize `BytesGenerator`\'s `flatten()` method for converting the message to bytes. - Reparse the bytes and regenerate to compare outputs. - Consider dealing with minor differences due to headers and encoding policies.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator from email import message_from_bytes from email.policy import default import io def serialize_and_validate_email(email_message, policy=None): Serialize an email message object to its binary representation and validate that regenerating the bytes produces the identical output. Parameters: - email_message (EmailMessage): The email message object to be serialized. - policy (Policy, optional): An optional policy to control the message generation. Returns: - bool: True if the regenerated bytes match the original, False otherwise. if policy is None: policy = default # Serialize the email message to bytes buffer = io.BytesIO() generator = BytesGenerator(buffer, policy=policy) generator.flatten(email_message) original_bytes = buffer.getvalue() # Parse the bytes back into an email message parsed_message = message_from_bytes(original_bytes) # Serialize the parsed email message again to bytes buffer2 = io.BytesIO() generator2 = BytesGenerator(buffer2, policy=policy) generator2.flatten(parsed_message) regenerated_bytes = buffer2.getvalue() # Compare the original and regenerated bytes return original_bytes == regenerated_bytes"},{"question":"**Question: Compression and Decompression Utility** You are required to implement a Python class `CompressionUtility` that provides methods to compress and decompress data using multiple algorithms supported by the provided package documentation (zlib, gzip, bz2, and lzma). # Class Definition The class should be implemented as follows: ```python class CompressionUtility: def compress_data(data: bytes, algorithm: str) -> bytes: Compresses the given data using the specified algorithm. Parameters: - data (bytes): The data to be compressed. - algorithm (str): The compression algorithm to use. Should be one of \'zlib\', \'gzip\', \'bz2\', \'lzma\'. Returns: - bytes: The compressed data. Raises: - ValueError: If an unsupported algorithm is specified. pass def decompress_data(data: bytes, algorithm: str) -> bytes: Decompresses the given data using the specified algorithm. Parameters: - data (bytes): The compressed data to be decompressed. - algorithm (str): The compression algorithm to use. Should be one of \'zlib\', \'gzip\', \'bz2\', \'lzma\'. Returns: - bytes: The decompressed data. Raises: - ValueError: If an unsupported algorithm is specified or if the data cannot be decompressed with the specified algorithm. pass ``` # Constraints - You must utilize the appropriate Python standard libraries for each algorithm as described in the documentation. - You must handle any exceptions that occur during compression or decompression and raise a `ValueError` with an appropriate message. # Example Usage ```python data = b\\"Example data to be compressed and decompressed\\" # Compress using gzip compressed = CompressionUtility.compress_data(data, \'gzip\') # Decompress using gzip decompressed = CompressionUtility.decompress_data(compressed, \'gzip\') assert decompressed == data # Compress using lzma compressed = CompressionUtility.compress_data(data, \'lzma\') # Decompress using lzma decompressed = CompressionUtility.decompress_data(compressed, \'lzma\') assert decompressed == data ``` # Additional Considerations 1. Ensure that the methods are efficient and handle large data sizes appropriately. 2. Include necessary error checks to validate the input parameters. 3. The solution should be tested with various data sizes and types to ensure robustness.","solution":"import zlib import gzip import bz2 import lzma class CompressionUtility: @staticmethod def compress_data(data: bytes, algorithm: str) -> bytes: if algorithm == \'zlib\': return zlib.compress(data) elif algorithm == \'gzip\': return gzip.compress(data) elif algorithm == \'bz2\': return bz2.compress(data) elif algorithm == \'lzma\': return lzma.compress(data) else: raise ValueError(f\\"Unsupported compression algorithm: {algorithm}\\") @staticmethod def decompress_data(data: bytes, algorithm: str) -> bytes: if algorithm == \'zlib\': return zlib.decompress(data) elif algorithm == \'gzip\': return gzip.decompress(data) elif algorithm == \'bz2\': return bz2.decompress(data) elif algorithm == \'lzma\': return lzma.decompress(data) else: raise ValueError(f\\"Unsupported compression algorithm: {algorithm}\\")"},{"question":"Objective: To evaluate your understanding of scikit-learn\'s functionality for tuning the decision threshold in binary classification problems, and to assess your ability to implement and utilize the `TunedThresholdClassifierCV` class effectively alongside custom evaluation metrics. Problem Statement: You are provided with a dataset used for classifying whether or not a person has a certain medical condition based on a set of features. Your task is to build a classification model, tune its decision threshold to optimize a specific metric, and evaluate its performance. Dataset: Use the built-in `make_classification` function from scikit-learn to generate a dataset with 1000 samples, where 20% of samples belong to the positive class. ```python from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_classes=2, weights=[0.8, 0.2], random_state=42) ``` Task: 1. **Data Generation**: Utilize the `make_classification` function to generate the dataset as described above. 2. **Model Training**: Train a `LogisticRegression` classifier on the entire dataset. 3. **Threshold Tuning**: - Use `TunedThresholdClassifierCV` to tune the decision threshold. - Optimize the threshold to maximize the F1 score. 4. **Evaluation**: - Compute and report the F1 score, precision, and recall for the classifier with the tuned decision threshold. - Compare these metrics with those of the classifier using the default threshold of 0.5. Requirements: - Implement the solution in Python. - Use appropriate scikit-learn classes and functions (e.g., `LogisticRegression`, `TunedThresholdClassifierCV`, `make_scorer`, `f1_score`). - Ensure that the tuned decision threshold is obtained using `TunedThresholdClassifierCV`. - The final solution should print the optimized threshold and the evaluation metrics. Code Skeleton: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score # Step 1: Data Generation X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_classes=2, weights=[0.8, 0.2], random_state=42) # Step 2: Model Training base_model = LogisticRegression() base_model.fit(X, y) # Step 3: Threshold Tuning pos_label = 1 scorer = make_scorer(f1_score, pos_label=pos_label) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X, y) # Get the optimized threshold optimized_threshold = tuned_model.best_threshold_ # Step 4: Evaluation # Compute metrics with tuned threshold # Use tuned_model.predict_proba and apply the optimized threshold to get predictions # Print the results print(\\"Optimized Threshold:\\", optimized_threshold) print(\\"F1 Score with Tuned Threshold:\\", ) print(\\"Precision with Tuned Threshold:\\", ) print(\\"Recall with Tuned Threshold:\\", ) # Compute and print metrics for the default threshold (0.5) for comparison # ... ``` Notes: - Use internal cross-validation within `TunedThresholdClassifierCV` to determine the optimal threshold. - Be mindful of model performance and any potential overfitting during the tuning process.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict, KFold from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score # Step 1: Data Generation X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_classes=2, weights=[0.8, 0.2], random_state=42) # Step 2: Model Training base_model = LogisticRegression() base_model.fit(X, y) # Step 3: Threshold Tuning using Cross-Validation kf = KFold(n_splits=5, shuffle=True, random_state=42) probas = cross_val_predict(base_model, X, y, cv=kf, method=\'predict_proba\') # Define a function to find the best threshold def find_best_threshold(probas, y_true, pos_label=1): thresholds = np.linspace(0, 1, 101) f1_scores = [] for threshold in thresholds: preds = (probas[:, 1] >= threshold).astype(int) f1 = f1_score(y_true, preds, pos_label=pos_label) f1_scores.append(f1) best_threshold = thresholds[np.argmax(f1_scores)] return best_threshold optimized_threshold = find_best_threshold(probas, y) # Step 4: Evaluation with Optimized Threshold pred_thresh_tuned = (base_model.predict_proba(X)[:, 1] >= optimized_threshold).astype(int) f1_tuned = f1_score(y, pred_thresh_tuned) precision_tuned = precision_score(y, pred_thresh_tuned) recall_tuned = recall_score(y, pred_thresh_tuned) # Metrics with default threshold (0.5) pred_thresh_default = base_model.predict(X) f1_default = f1_score(y, pred_thresh_default) precision_default = precision_score(y, pred_thresh_default) recall_default = recall_score(y, pred_thresh_default) # Print the results print(\\"Optimized Threshold:\\", optimized_threshold) print(\\"F1 Score with Tuned Threshold:\\", f1_tuned) print(\\"Precision with Tuned Threshold:\\", precision_tuned) print(\\"Recall with Tuned Threshold:\\", recall_tuned) print(\\"F1 Score with Default Threshold:\\", f1_default) print(\\"Precision with Default Threshold:\\", precision_default) print(\\"Recall with Default Threshold:\\", recall_default)"},{"question":"Objective: You are required to implement a function with type hints, and then write unit tests for this function using the `unittest` module. Additionally, you will utilize the `unittest.mock` module to mock parts of your function. Task: 1. **Implement the Function**: Implement a function named `fetch_data` that takes a URL string and returns a dictionary containing the following keys: - `status_code` (int): The HTTP status code of the response. - `data` (Any): The data returned from the URL if the request is successful. Assume the URL will return JSON data. Use type hints to enforce the expected input and output types. ```python import requests from typing import Dict, Any def fetch_data(url: str) -> Dict[str, Any]: response = requests.get(url) if response.status_code == 200: return {\'status_code\': response.status_code, \'data\': response.json()} else: return {\'status_code\': response.status_code, \'data\': None} ``` 2. **Write Unit Tests**: Write unit tests for the `fetch_data` function using the `unittest` module. Your tests should include: - A test case for a successful HTTP response. - A test case for an unsuccessful HTTP response. 3. **Mock the Requests**: Utilize the `unittest.mock` module to mock the `requests.get` method to test the `fetch_data` function without making real HTTP requests. Input: - The URL string for the function `fetch_data`. Output: - A dictionary containing the status code and the data from the URL response. Implement the solution in the following way: ```python import unittest from unittest.mock import patch, MagicMock import requests from typing import Dict, Any def fetch_data(url: str) -> Dict[str, Any]: response = requests.get(url) if response.status_code == 200: return {\'status_code\': response.status_code, \'data\': response.json()} else: return {\'status_code\': response.status_code, \'data\': None} class TestFetchData(unittest.TestCase): @patch(\'requests.get\') def test_fetch_data_success(self, mock_get): mock_response = MagicMock() mock_response.status_code = 200 mock_response.json.return_value = {\'key\': \'value\'} mock_get.return_value = mock_response result = fetch_data(\'http://example.com\') self.assertEqual(result, {\'status_code\': 200, \'data\': {\'key\': \'value\'}}) @patch(\'requests.get\') def test_fetch_data_failure(self, mock_get): mock_response = MagicMock() mock_response.status_code = 404 mock_get.return_value = mock_response result = fetch_data(\'http://example.com\') self.assertEqual(result, {\'status_code\': 404, \'data\': None}) if __name__ == \'__main__\': unittest.main() ``` Your task is to implement the `fetch_data` function, write the two test cases in the `TestFetchData` class, and verify that your tests pass successfully. Constraints: - You must use type hints for the `fetch_data` function. - Use the `unittest` module to write your tests. - Use the `unittest.mock` module to mock HTTP requests. Performance Requirements: - The function should handle typical HTTP response times efficiently and should return the data in a timely manner.","solution":"import requests from typing import Dict, Any def fetch_data(url: str) -> Dict[str, Any]: Fetches data from the given URL and returns a dictionary with the HTTP status code and the response data. Params: - url (str): The URL to fetch data from. Returns: - Dict[str, Any]: A dictionary containing the status code and data. response = requests.get(url) if response.status_code == 200: return {\'status_code\': response.status_code, \'data\': response.json()} else: return {\'status_code\': response.status_code, \'data\': None}"},{"question":"**Coding Assessment Question: Configuring PyTorch\'s MPS Backend** # Objective Write a Python function that configures the PyTorch environment for optimal performance using the MPS backend according to specific requirements. # Task Your task is to implement the function `configure_pytorch_mps()`. This function should: 1. Enable verbose allocator logging. 2. Set the log options for the MPS profiler to the highest detailed level possible. 3. Enable fast math for MPS metal kernels. 4. Use the metal kernels for matrix multiplications. 5. Set the high and low watermark ratios for the MPS allocator to specified values (`high_watermark_ratio` and `low_watermark_ratio`). 6. Enable fallback to CPU for unsupported MPS operations. # Function Signature ```python def configure_pytorch_mps(high_watermark_ratio: float, low_watermark_ratio: float) -> None: pass ``` # Input - `high_watermark_ratio` (float): The high watermark ratio for the MPS allocator. - `low_watermark_ratio` (float): The low watermark ratio for the MPS allocator. # Constraints - `0.0 <= high_watermark_ratio` - `0.0 <= low_watermark_ratio <= high_watermark_ratio` # Example ```python configure_pytorch_mps(1.7, 1.4) ``` # Implementation Requirements - Use the `os` module to set environment variables. - Ensure the function sets environment variables before any PyTorch computations take place. - Validate the input ratios, raising a `ValueError` if the constraints are not met. # Hints - Use `os.environ` to set environment variables in Python. - Review the documentation for understanding how to set corresponding environment variables. # Expected Behavior After calling `configure_pytorch_mps()`, the respective environment variables should be set accordingly, impacting the behavior of subsequent PyTorch computations using the MPS backend. # Notes - This task assumes that the student has PyTorch installed and is running on a system with MPS support (macOS with M1 chip or later).","solution":"import os def configure_pytorch_mps(high_watermark_ratio: float, low_watermark_ratio: float) -> None: Configures the PyTorch environment for optimal performance using the MPS backend. Parameters: high_watermark_ratio (float): The high watermark ratio for the MPS allocator. low_watermark_ratio (float): The low watermark ratio for the MPS allocator. Raises: ValueError: If the input ratios do not meet the constraints. if not (0.0 <= low_watermark_ratio <= high_watermark_ratio): raise ValueError(\\"Invalid watermark ratios: 0.0 <= low_watermark_ratio <= high_watermark_ratio\\") # Enable verbose allocator logging os.environ[\'PYTORCH_ALLOCATOR_VERBOSE\'] = \'1\' # Set the log options for the MPS profiler to the highest detailed level possible os.environ[\'PYTORCH_MPS_PROFILER_LOG_OPTIONS\'] = \'all\' # Enable fast math for MPS metal kernels os.environ[\'PYTORCH_METAL_FAST_MATH\'] = \'1\' # Use the metal kernels for matrix multiplications os.environ[\'PYTORCH_METAL_USE_MATRIXKERNELS\'] = \'1\' # Set the high and low watermark ratios for the MPS allocator os.environ[\'PYTORCH_MPS_ALLOCATOR_HIGH_WATERMARK_RATIO\'] = str(high_watermark_ratio) os.environ[\'PYTORCH_MPS_ALLOCATOR_LOW_WATERMARK_RATIO\'] = str(low_watermark_ratio) # Enable fallback to CPU for unsupported MPS operations os.environ[\'PYTORCH_MPS_FALLBACK_TO_CPU\'] = \'1\'"},{"question":"Objective: Design a Python function to demonstrate your understanding of dynamic module importing, function reflection, and string formatting. Problem Statement: You are required to implement a function `execute_dynamic_function` that takes three arguments: 1. `module_name` (str): The name of the module to dynamically import. 2. `function_name` (str): The name of the function to reflectively call from the imported module. 3. `parameters` (str): A comma-separated string of parameters to be passed to the function. The function should: 1. Dynamically import the specified module. 2. Use reflection to get the specified function from the imported module. 3. Parse the parameters from the comma-separated string and store them into appropriate data types (assume all parameters are either integers or floats). 4. Call the function with the parsed parameters and return the result. Input/Output Formats: - **Input:** - `module_name` (string) - `function_name` (string) - `parameters` (string in the format \\"param1,param2,...\\") - **Output:** - Result returned by the dynamically called function. Constraints: - Assume that the `module_name` and `function_name` provided are always valid and exist within the given module. - Parameters in the `parameters` string are always valid and can be either integers or floats. - There is no limit to the number of parameters provided, but they will always be comma-separated. - Performance should be sufficient to handle typical module and function operations without any special optimizations. Example: ```python # Suppose there is a module \'math_ops\' with a function \'add_numbers\' # that takes two numbers and returns their sum. # math_ops.py def add_numbers(a, b): return a + b # Example usage result = execute_dynamic_function(\'math_ops\', \'add_numbers\', \'10,20\') print(result) # Output: 30 # Example usage with a different function # math_ops.py def multiply_numbers(a, b): return a * b result = execute_dynamic_function(\'math_ops\', \'multiply_numbers\', \'4,5\') print(result) # Output: 20 ``` Implementation: ```python def execute_dynamic_function(module_name, function_name, parameters): import importlib # Dynamically import the module module = importlib.import_module(module_name) # Retrieve the function using reflection func = getattr(module, function_name) # Parse the parameters parsed_params = [] for param in parameters.split(\',\'): if \'.\' in param: parsed_params.append(float(param)) else: parsed_params.append(int(param)) # Call the function with parsed parameters and return the result return func(*parsed_params) ```","solution":"def execute_dynamic_function(module_name, function_name, parameters): import importlib # Dynamically import the module module = importlib.import_module(module_name) # Retrieve the function using reflection func = getattr(module, function_name) # Parse the parameters parsed_params = [] for param in parameters.split(\',\'): if \'.\' in param: parsed_params.append(float(param)) else: parsed_params.append(int(param)) # Call the function with parsed parameters and return the result return func(*parsed_params)"},{"question":"**Objective:** Assess the ability to use the `argparse` module to handle complex command-line argument parsing. **Problem Statement:** You are tasked with creating a Python script that simulates a simple command-line tool for managing a book collection. The tool should support the following commands: 1. `add`: Add a new book to the collection. 2. `list`: List all books in the collection. 3. `search`: Search for books by title. 4. `update`: Update the information of an existing book. 5. `delete`: Remove a book from the collection. The script should use the `argparse` module to handle the following command-line arguments for each command: 1. `add` command: - `title`: The title of the book (required). - `author`: The author of the book (required). - `year`: The year the book was published (required). - `genre`: The genre of the book (optional). 2. `list` command: - `all`: A flag to list all books (optional, defaults to `False`). 3. `search` command: - `title`: The title to search for (required). 4. `update` command: - `id`: The ID of the book to update (required). - `title`: The new title of the book (optional). - `author`: The new author of the book (optional). - `year`: The new year the book was published (optional). - `genre`: The new genre of the book (optional). 5. `delete` command: - `id`: The ID of the book to delete (required). **Requirements:** - Implement the main script to handle these commands using the `argparse` module. - Ensure that appropriate help messages are displayed when using the `-h` or `--help` flags. - Validate that required arguments are provided. - Use mutually exclusive groups where applicable (e.g., for the `update` command, if the user provides no updates). **Constraints:** - The script should not perform actual book collection management. Placeholder functions can be used for the actual operations. - Use the `argparse` module for all command-line argument parsing. **Example Usage:** ```sh python manage_books.py add --title \\"The Great Gatsby\\" --author \\"F. Scott Fitzgerald\\" --year 1925 --genre \\"Novel\\" python manage_books.py list --all python manage_books.py search --title \\"The Great Gatsby\\" python manage_books.py update --id 1 --title \\"The Great Gatsby\\" --year 1925 python manage_books.py delete --id 1 ``` **Expected Output:** - If required arguments are missing or invalid, the script should print an error message and show the help for the command. - On successful completion of a command, a confirmation message should be printed. # Solution Template ```python import argparse def add_book(args): # Placeholder function to add a book print(f\\"Adding book: {args.title} by {args.author}, published in {args.year}, Genre: {args.genre}\\") def list_books(args): # Placeholder function to list all books if args.all: print(\\"Listing all books in the collection...\\") else: print(\\"Please use the --all flag to list all books.\\") def search_books(args): # Placeholder function to search for books by title print(f\\"Searching for books with title: {args.title}\\") def update_book(args): # Placeholder function to update a book print(f\\"Updating book ID: {args.id} with new details: Title: {args.title}, Author: {args.author}, Year: {args.year}, Genre: {args.genre}\\") def delete_book(args): # Placeholder function to delete a book print(f\\"Deleting book with ID: {args.id}\\") def main(): parser = argparse.ArgumentParser(description=\\"Manage your book collection\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) # Add command parser_add = subparsers.add_parser(\'add\', help=\'Add a new book\') parser_add.add_argument(\'--title\', required=True, help=\'The title of the book\') parser_add.add_argument(\'--author\', required=True, help=\'The author of the book\') parser_add.add_argument(\'--year\', required=True, type=int, help=\'The year the book was published\') parser_add.add_argument(\'--genre\', help=\'The genre of the book\') parser_add.set_defaults(func=add_book) # List command parser_list = subparsers.add_parser(\'list\', help=\'List all books\') parser_list.add_argument(\'--all\', action=\'store_true\', help=\'List all books\') parser_list.set_defaults(func=list_books) # Search command parser_search = subparsers.add_parser(\'search\', help=\'Search for books by title\') parser_search.add_argument(\'--title\', required=True, help=\'The title to search for\') parser_search.set_defaults(func=search_books) # Update command parser_update = subparsers.add_parser(\'update\', help=\'Update the information of an existing book\') parser_update.add_argument(\'--id\', required=True, help=\'The ID of the book to update\') # Adding mutually exclusive group for update fields group = parser_update.add_mutually_exclusive_group(required=True) group.add_argument(\'--title\', help=\'The new title of the book\') group.add_argument(\'--author\', help=\'The new author of the book\') group.add_argument(\'--year\', type=int, help=\'The new year the book was published\') group.add_argument(\'--genre\', help=\'The new genre of the book\') parser_update.set_defaults(func=update_book) # Delete command parser_delete = subparsers.add_parser(\'delete\', help=\'Remove a book from the collection\') parser_delete.add_argument(\'--id\', required=True, help=\'The ID of the book to delete\') parser_delete.set_defaults(func=delete_book) args = parser.parse_args() args.func(args) if __name__ == \'__main__\': main() ```","solution":"import argparse def add_book(args): # Placeholder function to add a book print(f\\"Adding book: {args.title} by {args.author}, published in {args.year}, Genre: {args.genre}\\") def list_books(args): # Placeholder function to list all books if args.all: print(\\"Listing all books in the collection...\\") else: print(\\"Please use the --all flag to list all books.\\") def search_books(args): # Placeholder function to search for books by title print(f\\"Searching for books with title: {args.title}\\") def update_book(args): # Placeholder function to update a book details = [] if args.title: details.append(f\\"Title: {args.title}\\") if args.author: details.append(f\\"Author: {args.author}\\") if args.year: details.append(f\\"Year: {args.year}\\") if args.genre: details.append(f\\"Genre: {args.genre}\\") print(f\\"Updating book ID: {args.id} with new details: {\', \'.join(details)}\\") def delete_book(args): # Placeholder function to delete a book print(f\\"Deleting book with ID: {args.id}\\") def main(): parser = argparse.ArgumentParser(description=\\"Manage your book collection\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) # Add command parser_add = subparsers.add_parser(\'add\', help=\'Add a new book\') parser_add.add_argument(\'--title\', required=True, help=\'The title of the book\') parser_add.add_argument(\'--author\', required=True, help=\'The author of the book\') parser_add.add_argument(\'--year\', required=True, type=int, help=\'The year the book was published\') parser_add.add_argument(\'--genre\', help=\'The genre of the book\') parser_add.set_defaults(func=add_book) # List command parser_list = subparsers.add_parser(\'list\', help=\'List all books\') parser_list.add_argument(\'--all\', action=\'store_true\', help=\'List all books\') parser_list.set_defaults(func=list_books) # Search command parser_search = subparsers.add_parser(\'search\', help=\'Search for books by title\') parser_search.add_argument(\'--title\', required=True, help=\'The title to search for\') parser_search.set_defaults(func=search_books) # Update command parser_update = subparsers.add_parser(\'update\', help=\'Update the information of an existing book\') parser_update.add_argument(\'--id\', required=True, help=\'The ID of the book to update\') parser_update.add_argument(\'--title\', help=\'The new title of the book\') parser_update.add_argument(\'--author\', help=\'The new author of the book\') parser_update.add_argument(\'--year\', type=int, help=\'The new year the book was published\') parser_update.add_argument(\'--genre\', help=\'The new genre of the book\') parser_update.set_defaults(func=update_book) # Delete command parser_delete = subparsers.add_parser(\'delete\', help=\'Remove a book from the collection\') parser_delete.add_argument(\'--id\', required=True, help=\'The ID of the book to delete\') parser_delete.set_defaults(func=delete_book) args = parser.parse_args() args.func(args) if __name__ == \'__main__\': main()"},{"question":"**Custom Python Interpreter Implementation Challenge** Objective Implement an interactive Python interpreter with a custom feature using the `code` and `codeop` modules. The feature to be added is a command history mechanism that records the last `n` commands executed in the interpreter, where `n` is defined by the user. Task Create a class `CustomInteractiveConsole` that: 1. Extends `code.InteractiveConsole`. 2. Adds a method `get_history()` that returns the last `n` commands executed in the interpreter\'s current session. # Requirements 1. **Initialization Parameters:** * `locals` (default: None) - A dictionary of available local variables. * `history_limit` (default: 100) - The maximum number of commands to store in the history. 2. **Methods:** * `runsource(source, filename=\\"<input>\\", symbol=\\"single\\")`: Override this method to store each executed command in history. * `get_history()`: Return the list of the last `n` commands. If fewer than `n` commands exist, return all of them. # Input and Output Format - The `locals` parameter takes in a dictionary of local variables. - The `history_limit` parameter is an integer specifying the maximum history size. - The `get_history()` method returns a list of the last executed commands as strings. # Constraints - `history_limit` will be a positive integer not exceeding 1000. # Example ```python from your_module import CustomInteractiveConsole # Initialize the custom interactive console with a history limit of 3 console = CustomInteractiveConsole(history_limit=3) # Execute some commands console.push(\\"a = 10\\") console.push(\\"b = 20\\") console.push(\\"print(a + b)\\") console.push(\\"c = a + b\\") # Retrieve the command history print(console.get_history()) # Output: [\'a = 10\', \'b = 20\', \'print(a + b)\', \'c = a + b\'] ``` # Notes - You may assume that the commands executed in the interactive console are syntactically correct. - This task tests your ability to extend existing classes and manage state within an object.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, history_limit=100): super().__init__(locals=locals) self.history_limit = history_limit self.command_history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if len(self.command_history) >= self.history_limit: self.command_history.pop(0) self.command_history.append(source.strip()) return super().runsource(source, filename, symbol) def get_history(self): return self.command_history"},{"question":"You are given a set of tasks that must be executed sequentially and parallelly on CPU devices using PyTorch\'s `torch.cpu` module. This will involve managing CPU devices, streams, and ensuring proper synchronization between tasks. Problem Statement Implement a function `execute_tasks_on_cpu` that: 1. Accepts two lists of tasks (`sequential_tasks` and `parallel_tasks`), where each task is a callable Python function. 2. Executes all tasks in `sequential_tasks` list in a sequential manner. 3. Executes all tasks in `parallel_tasks` list in a parallel manner using CPU streams. 4. Ensures proper synchronization such that: - Sequential tasks are completed before any parallel tasks begin. - All parallel tasks are properly synchronized to ensure no race conditions. Function Signature ```python def execute_tasks_on_cpu(sequential_tasks: list, parallel_tasks: list) -> None: pass ``` Inputs - `sequential_tasks`: a list of callable functions to be executed sequentially. - `parallel_tasks`: a list of callable functions to be executed in parallel. Expected Functionality 1. **Sequential Execution**: Ensure that tasks in the `sequential_tasks` list run one after another. 2. **Parallel Execution using Streams**: Utilize `torch.cpu.Stream` to manage and execute `parallel_tasks` concurrently. 3. **Synchronization**: Use `torch.cpu.synchronize` to achieve proper synchronization between tasks ensuring that all sequential tasks complete before moving to parallel execution and ensuring that all parallel tasks complete by the end. Constraints and Requirements 1. You must use `torch.cpu` functions and classes where relevant. 2. Ensure proper exception handling to cater to any runtime errors during task execution. 3. The solution should handle a varying number of tasks efficiently. Example ```python import torch def task1(): print(\\"Executing task 1\\") def task2(): print(\\"Executing task 2\\") def task3(): print(\\"Executing task 3\\") def task4(): print(\\"Executing task 4\\") sequential_tasks = [task1, task2] parallel_tasks = [task3, task4] execute_tasks_on_cpu(sequential_tasks, parallel_tasks) ``` Output should be: ``` Executing task 1 Executing task 2 Executing task 3 Executing task 4 ``` The output order of task3 and task4 might vary since they are run in parallel.","solution":"import torch def execute_tasks_on_cpu(sequential_tasks: list, parallel_tasks: list) -> None: # Execute sequential tasks for task in sequential_tasks: task() # Synchronize to ensure all sequential tasks are completed torch.cuda.synchronize() # Create CPU streams for parallel tasks streams = [torch.cuda.Stream() for _ in parallel_tasks] # Enqueue parallel tasks for stream, task in zip(streams, parallel_tasks): with torch.cuda.stream(stream): task() # Synchronize all streams to ensure all parallel tasks are completed for stream in streams: stream.synchronize()"},{"question":"# Question: Implementing a Custom Text Filter using the `io` Module in Python **Context:** In this task, you are required to implement a custom text filter using Python\'s `io` module. This filter will read a text stream, transform the text based on specific criteria, and write the transformed text to another stream. **Objective:** Create a function `custom_text_filter(input_text: str, filter_word: str) -> str` that does the following: 1. **Reads** the input text stream. 2. **Transforms** the text by filtering out all occurrences of a specified word. 3. **Writes** the transformed text to an output text stream. 4. **Returns** the output text as a string. **Function Signature:** ```python def custom_text_filter(input_text: str, filter_word: str) -> str: ``` **Parameters:** - `input_text` (str): The original text from which specific words will be filtered out. - `filter_word` (str): The word that needs to be filtered out from the input text. **Returns:** - `str`: The transformed text with all occurrences of the filter word removed. **Constraints:** - Do not use any external libraries for text processing, rely only on the `io` module for stream operations. - Ensure the function can handle large texts efficiently. - The filter should remove whole words only. Partial matches (substrings) should not be removed. **Example:** ```python input_text = \\"This is an example text. This text is a sample text.\\" filter_word = \\"text\\" output = custom_text_filter(input_text, filter_word) print(output) ``` **Expected Output:** ``` \\"This is an example . This is a sample .\\" ``` **Note:** - Consider edge cases, such as empty strings, strings without the filter word, and how punctuation affects word boundaries. - Implement error handling as necessary to manage input and output stream operations. **Hints:** - Utilize `io.StringIO` to create in-memory text streams for both reading and writing. - Employ methods provided by `TextIOBase` and `TextIOWrapper` classes to facilitate text stream operations.","solution":"import io def custom_text_filter(input_text: str, filter_word: str) -> str: input_stream = io.StringIO(input_text) output_stream = io.StringIO() filter_word_len = len(filter_word) word_boundary_chars = set(\' tnr.,!?;:\\"\'()[]{}<>\') while True: char = input_stream.read(1) if not char: break if char in word_boundary_chars: output_stream.write(char) continue buffer = char while True: next_char = input_stream.read(1) if not next_char or next_char in word_boundary_chars: break buffer += next_char if buffer == filter_word: if next_char and next_char in word_boundary_chars: output_stream.write(next_char) else: output_stream.write(buffer) if next_char: output_stream.write(next_char) return output_stream.getvalue()"},{"question":"**XML Parser Implementation with Custom Handlers** As a software developer, you are tasked with processing various XML documents that come from an external source. To efficiently parse and analyze these documents, you will use the `xml.parsers.expat` module. Specifically, you need to implement a function that utilizes the module to gather specific information from the XML content. # Task Implement a function `parse_xml(xml_string: str) -> dict` that takes an XML string as input and returns a dictionary containing: 1. The names of all elements. 2. The attributes associated with each element. 3. The character data contained within each element. # Function Signature ```python def parse_xml(xml_string: str) -> dict: pass ``` # Input - `xml_string` (str): A string containing valid XML data. # Output - (dict): A dictionary with the following structure: ```python { \\"elements\\": { \\"element_name_1\\": { \\"attributes\\": {\\"attr_name_1\\": \\"attr_value_1\\", ...}, \\"character_data\\": \\"character data for element_name_1\\" }, ... } } ``` # Constraints - The XML string will always be well-formed. - Attributes and character data may be empty. - You should handle namespaces if they are present. # Example ```python xml_string = <?xml version=\\"1.0\\"?> <root id=\\"top\\"> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </root> result = parse_xml(xml_string) ``` Expected `result`: ```python { \\"elements\\": { \\"root\\": { \\"attributes\\": {\\"id\\": \\"top\\"}, \\"character_data\\": \\"\\" }, \\"child1\\": { \\"attributes\\": {\\"name\\": \\"paul\\"}, \\"character_data\\": \\"Text goes here\\" }, \\"child2\\": { \\"attributes\\": {\\"name\\": \\"fred\\"}, \\"character_data\\": \\"More text\\" } } } ``` # Notes - Use the `xml.parsers.expat.ParserCreate` function to create the parser. - Implement and set custom handler functions for start element, end element, and character data. - Consider edge cases where elements may have no attributes or character data. - Handle nested elements appropriately. The goal of this task is to test your ability to use the `xml.parsers.expat` package to parse XML content and extract meaningful data in a structured format.","solution":"import xml.parsers.expat def parse_xml(xml_string: str) -> dict: result = {\\"elements\\": {}} current_element = None def start_element(name, attrs): nonlocal current_element result[\\"elements\\"][name] = {\\"attributes\\": attrs, \\"character_data\\": \\"\\"} current_element = name def end_element(name): nonlocal current_element current_element = None def char_data(data): if current_element: result[\\"elements\\"][current_element][\\"character_data\\"] += data parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string, True) return result"},{"question":"**Objective:** Create a comprehensive visualization using Seaborn\'s `so.Plot` by combining multiple elements and handling both categorical and numerical variables. This assessment focuses on your ability to manipulate data for visualization purposes and ensure clarity in plots through jittering, percentage ranges, and shifting. **Problem Statement:** You are provided with two datasets: `penguins` and `diamonds`. Your task is to create a detailed plot for each dataset using Seabornâ€™s `so.Plot`. # Requirements: 1. Load the `penguins` and `diamonds` datasets. 2. For the `penguins` dataset: - Create a scatter plot showing the distribution of `body_mass_g` across `species`. - Apply jitter to the dots. - Overlay a range indication between the 25th and 75th percentiles, shifting the range slightly along the x-axis to enhance clarity. 3. For the `diamonds` dataset: - Create a horizontal scatter plot showing the clarity categories against the `carat` values. - Apply jitter to the dots. - Overlay a range indication between the 25th and 75th percentiles, shifting the range slightly downwards along the y-axis for better readability. **Input Format:** - No direct inputs: You need to load the datasets within your solution using `load_dataset(\'penguins\')` and `load_dataset(\'diamonds\')`. **Output:** - Generate two plots with the specified characteristics for each dataset. **Note:** - Ensure your plots are visually clear with appropriate axis labels and a title for each plot. - It would be best to use Seaborn objects (`seaborn.objects`) and methods such as `so.Plot`, `so.Dots`, `so.Jitter`, `so.Range`, `so.Perc`, and `so.Shift`. ```python # Sample code to demonstrate the expected output format import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\'penguins\') diamonds = load_dataset(\'diamonds\') # Plot for penguins dataset penguin_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) print(penguin_plot) # Plot for diamonds dataset diamond_plot = ( so.Pplot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) print(diamond_plot) ``` **Constraints:** - Use only Seaborn functions and methods as mentioned. - Follow best practices for data visualization to ensure plots are clean and informative.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\'penguins\') # Create scatter plot for penguins dataset penguin_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) return penguin_plot def create_diamond_plot(): # Load the diamonds dataset diamonds = load_dataset(\'diamonds\') # Create horizontal scatter plot for diamonds dataset diamond_plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=-.25)) ) return diamond_plot"},{"question":"You are required to create a Python script that performs the following tasks using the `zipfile` module: 1. **Create a new ZIP file**: - Name: `archive.zip` - Compression method: `ZIP_DEFLATED` - Include the following text files: - `file1.txt` (create this file with some sample text content) - `file2.txt` (create this file with some sample text content) 2. **List and display the contents of the newly created ZIP file** in the format: ``` File Name: <file_name> Compressed Size: <compressed_size> Uncompressed Size: <uncompressed_size> ``` 3. **Extract all files** from the `archive.zip` to a directory named `extracted_files`. 4. **Validate the extraction** by reading the content of each extracted file and printing it to the console. # Implementation Details - **Step 1**: Use the `write` method to add files to the ZIP archive. - **Step 2**: Use `infolist` to list the contents and their respective sizes. - **Step 3**: Utilize the `extractall` method for extraction. - **Step 4**: Read the files from the extraction directory to validate content. # Additional Requirements - Ensure to handle exceptions that may arise during the process. - Add necessary comments to your code to explain each step. # Example An example of the expected content for the files before zipping might be: - `file1.txt`: ``` Hello, this is file 1. ``` - `file2.txt`: ``` Hello, this is file 2. ``` The console output after listing the ZIP contents and validating the extractions should be: ``` File Name: file1.txt Compressed Size: <compressed_size> Uncompressed Size: <uncompressed_size> File Name: file2.txt Compressed Size: <compressed_size> Uncompressed Size: <uncompressed_size> Content of file1.txt: Hello, this is file 1. Content of file2.txt: Hello, this is file 2. ``` # Constraints - Assume the text files always have unique names. - Provide meaningful messages for any exceptions that occur. - Ensure the script is capable of being executed multiple times without errors (e.g., handle file/directory already exists scenarios).","solution":"import zipfile import os def create_zip_with_files(zip_name=\'archive.zip\'): Create a zip file with ZIP_DEFLATED compression and add two text files to it. # Creating sample files with open(\'file1.txt\', \'w\') as f: f.write(\\"Hello, this is file 1.\\") with open(\'file2.txt\', \'w\') as f: f.write(\\"Hello, this is file 2.\\") # Creating a zip file and adding files to it with zipfile.ZipFile(zip_name, \'w\', zipfile.ZIP_DEFLATED) as archive: archive.write(\'file1.txt\') archive.write(\'file2.txt\') def list_zip_contents(zip_name=\'archive.zip\'): List the contents of the zip file. with zipfile.ZipFile(zip_name, \'r\') as archive: for info in archive.infolist(): print(f\\"File Name: {info.filename}\\") print(f\\"Compressed Size: {info.compress_size}\\") print(f\\"Uncompressed Size: {info.file_size}\\") print() def extract_zip_files(zip_name=\'archive.zip\', extract_dir=\'extracted_files\'): Extract all files from the zip and print their contents. if not os.path.exists(extract_dir): os.makedirs(extract_dir) with zipfile.ZipFile(zip_name, \'r\') as archive: archive.extractall(extract_dir) for file_name in os.listdir(extract_dir): file_path = os.path.join(extract_dir, file_name) with open(file_path, \'r\') as file: print(f\\"Content of {file_name}:\\") print(file.read()) print()"},{"question":"# Composite Window Function with PyTorch Objective Your task is to implement a function in Python using PyTorch that generates a composite window function from multiple specific windows provided by the `torch.signal.windows` module. This question aims to assess your understanding of basic and intermediate operations with PyTorch, especially within the context of signal processing. Function Specification * **Function Name:** `composite_window` * **Inputs:** * `length` (int): The length of the window. * `window_types` (list of tuples): A list where each tuple contains: * The window function name as a string (e.g., \'hann\', \'bartlett\'). * The weight (float) that specifies the contribution of this window to the composite window. * **Output:** * A 1D tensor containing the composite window function values. Constraints 1. The `length` parameter will always be a positive integer. 2. The `window_types` list will have at least one entry. 3. Each window type string in `window_types` will be a valid function name from the `torch.signal.windows` module. 4. The weights will be positive floats but may not necessarily sum to 1. Example ```python import torch.signal.windows as windows def composite_window(length, window_types): # Your implementation here pass # Example usage: length = 100 window_types = [(\'hann\', 0.5), (\'bartlett\', 0.3), (\'blackman\', 0.2)] output = composite_window(length, window_types) print(output) # Expected output: 1D tensor containing composite window values ``` Detailed Requirements 1. Import the necessary window functions from `torch.signal.windows`. 2. For each window function mentioned in `window_types`, generate the corresponding window and multiply it by its weight. 3. Sum up all weighted windows to get the final composite window. 4. The function should return this final composite window as a 1D tensor. **Hint:** - Consider using `getattr` to dynamically access functions from the `torch.signal.windows` module. This question tests your ability to: - Work with torch tensors. - Dynamically access and utilize module functions. - Manipulate and combine data using basic PyTorch operations. Good luck!","solution":"import torch import torch.signal.windows as windows def composite_window(length, window_types): Generate a composite window function from multiple specific windows. Args: - length (int): Length of the window. - window_types (list of tuples): A list where each tuple contains the window name and its weight. Returns: - Tensor: A 1D tensor containing the composite window function values. composite = torch.zeros(length) for window_name, weight in window_types: window_func = getattr(windows, window_name) composite += weight * window_func(length) return composite"},{"question":"**Coding Assessment Question** **Objective**: Demonstrate your understanding of PyTorch\'s underlying storage mechanism by manipulating storage directly and verifying the changes through tensors. **Problem Statement**: You are provided with a task to work with `torch.UntypedStorage` and tensors in PyTorch. Your goal is to: 1. Create a tensor of size (5,) filled with ones. 2. Obtain the untyped storage of this tensor. 3. Clone the storage and fill the cloned storage with the value 2. 4. Create a new tensor from the zero-offset and cloned storage, making sure the new tensor reflects the storage change. 5. Verify that the new tensor contains the values as per the modified storage. **Instructions**: - Implement the function `manipulate_storage` that carries out the above steps. - Ensure that the first tensor remains unchanged after manipulation. - Return two tensors: the original tensor and the newly created tensor reflecting the storage modification. **Function Signature**: ```python import torch def manipulate_storage() -> (torch.Tensor, torch.Tensor): # Step 1: Create tensor of size (5,) filled with ones tensor_ones = torch.ones(5) # Step 2: Obtain the untyped storage of the tensor original_storage = tensor_ones.untyped_storage() # Step 3: Clone the storage and fill cloned storage with value 2 cloned_storage = original_storage.clone() cloned_storage.fill_(2) # Step 4: Create a new tensor from zero-offset cloned storage new_tensor = torch.Tensor().set_(cloned_storage, 0, torch.Size([5]), (1,)) # Step 5: Return both tensors return tensor_ones, new_tensor # Verify returned tensors original_tensor, modified_tensor = manipulate_storage() print(\\"Original Tensor:\\", original_tensor) # Expected: tensor([1., 1., 1., 1., 1.]) print(\\"Modified Tensor:\\", modified_tensor) # Expected: tensor([2., 2., 2., 2., 2.]) ``` **Constraints and Limitations**: - Assume the operations are carried out on CPU. - Direct indexing and reshaping should not be used to solve this problem. - Focus on using low-level storage operations as described. **Performance Requirements**: - Ensure the function runs efficiently and does not create redundant copies of data.","solution":"import torch def manipulate_storage() -> (torch.Tensor, torch.Tensor): # Step 1: Create tensor of size (5,) filled with ones tensor_ones = torch.ones(5) # Step 2: Obtain the untyped storage of the tensor original_storage = tensor_ones.storage() # Step 3: Clone the storage and fill cloned storage with value 2 cloned_storage = original_storage.clone() cloned_storage.fill_(2) # Step 4: Create a new tensor from zero-offset cloned storage new_tensor = torch.Tensor(cloned_storage) # Step 5: Return both tensors return tensor_ones, new_tensor # Verify returned tensors original_tensor, modified_tensor = manipulate_storage() print(\\"Original Tensor:\\", original_tensor) # Expected: tensor([1., 1., 1., 1., 1.]) print(\\"Modified Tensor:\\", modified_tensor) # Expected: tensor([2., 2., 2., 2., 2.])"},{"question":"**Task: Implement a Custom Network Server Using \\"socketserver\\" Module** **Objective:** Create a Python network server that can handle multiple clients concurrently. The server should log each client\'s message along with the client\'s address and send a processed response back to the client. Use the \\"socketserver\\" module to achieve this. **Requirements:** 1. **Server Class:** - Create a server class using `ThreadingMixIn` and `TCPServer`. This server should handle requests concurrently. 2. **Request Handler:** - Subclass `StreamRequestHandler` for the request handler. - Override the `handle()` method to: - Read a message from the client. - Log the message to the console with the client\'s address. - Send a response back to the client. The response should be the reversed version of the received message. 3. **Server Lifespan:** - The server should run indefinitely until explicitly interrupted (e.g., using `Ctrl+C` in the console). - Ensure proper shutdown and cleanup using `server_close()` when interrupted. 4. **Exception Handling:** - Add appropriate error handling to log any exceptions that occur during request handling without crashing the server. **Input and Output:** - The server should listen on `localhost` and port `9999`. - A typical client message will be a string sent over a TCP connection to the server. - The server should print received messages to the console and send a reversed message back to the client. **Constraints:** - Use only the \\"socketserver\\" module for server implementation. - Ensure your implementation is thread-safe and efficiently handles multiple client requests. **Performance:** - The implementation should handle at least 20 clients simultaneously without significant delay in request processing. **Example:** _Server Output:_ ``` Server loop running in thread: Thread-1 Received from (\'127.0.0.1\', 52090): Hello Server Sending to (\'127.0.0.1\', 52090): revreS olleH ... ``` _Client Code:_ ```python import socket HOST, PORT = \\"localhost\\", 9999 data = \\"Hello Server\\" with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) sock.sendall(bytes(data, \\"utf-8\\")) received = str(sock.recv(1024), \\"utf-8\\") print(\\"Sent: {}\\".format(data)) print(\\"Received: {}\\".format(received)) ``` _Expected Client Output:_ ``` Sent: Hello Server Received: revreS olleH ``` **Submission:** Submit your `server.py` containing the full implementation of the server following the requirements mentioned above.","solution":"import socketserver from socketserver import ThreadingMixIn, TCPServer, StreamRequestHandler import threading class ThreadedTCPRequestHandler(StreamRequestHandler): def handle(self): # Receive message from client data = self.rfile.readline().strip() message = data.decode(\'utf-8\') # Log client\'s address and message client_address = self.client_address print(f\\"Received from {client_address}: {message}\\") # Process the message (reverse it) response = message[::-1] # Send response back to client self.wfile.write(response.encode(\'utf-8\')) class ThreadedTCPServer(ThreadingMixIn, TCPServer): pass def start_server(): HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler) as server: print(f\\"Server loop running in thread: {threading.current_thread().name}\\") try: server.serve_forever() except KeyboardInterrupt: print(\\"Shutting down server.\\") server.shutdown() server.server_close() if __name__ == \\"__main__\\": start_server()"},{"question":"**Advanced Seaborn Plotting** # Context You are provided with a dataset containing information on restaurant tips (`tips`). Your task is to create various plots to visualize different aspects of this dataset. You\'ll need to demonstrate a good understanding of seaborn\'s capabilities by creating more complex and informative visualizations. # Dataset The `tips` dataset has the following columns: - `total_bill`: Total bill amount (numeric) - `tip`: Tip amount (numeric) - `sex`: Gender of the person paying the bill (categorical) - `smoker`: Whether the person is a smoker or not (categorical) - `day`: Day of the week (categorical) - `time`: Time of the day (Lunch or Dinner) (categorical) - `size`: Number of people at the table (numeric) # Tasks 1. **Bar Plot with Count and Percentage**: Create a bar plot showing the count of tips given each day, but color the bars based on whether the bill was paid by a smoker or a non-smoker. Additionally, annotate the bars with the percentage of the total tips they represent. 2. **Box Plot with Custom Styling**: Create a box plot to compare the distribution of tips given during Lunch and Dinner. Differentiate the plots by `sex` and customize the plot to have a distinct style (e.g., specific color palette, grid style). 3. **Joint Plot with Regression**: Create a joint plot of `total_bill` vs. `tip` with a regression line fitted to the data. Use a different color for the regression line based on the `time` (Lunch vs. Dinner). 4. **Facet Grid with Histograms**: Use a facet grid to create histograms of `total_bill` segmented by both `day` and `time`. Ensure that the axis labels and titles are informative and the plots are well-spaced and readable. # Input Format - You are required to use the `seaborn` library. - Assume the `tips` dataset is already loaded for you. # Output Format - A script that generates the specified plots. - The script should not only generate the plots but also include comments explaining each step and choice. # Constraints - Each plot must be generated using `seaborn` only (no matplotlib or other libraries, except for minor customizations). - The style customizations should be non-default to demonstrate your understanding and ability to customize plots. - Ensure all plots include titles, axis labels, and legends (if applicable) for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\'tips\') def plot_bar_count_percentage(): # Bar plot with counts of tips given each day, colored by smoker status ax = sns.countplot(data=tips, x=\'day\', hue=\'smoker\') total_counts = len(tips) for p in ax.patches: percentage = f\'{100 * p.get_height() / total_counts:.1f}%\' x = p.get_x() + p.get_width() / 2 - 0.05 y = p.get_height() ax.annotate(percentage, (x, y), fontsize=12) ax.set_title(\'Count and Percentage of Tips by Day and Smoker Status\') plt.show() def plot_box_custom_styling(): # Box plot comparing tips given during Lunch and Dinner, differentiated by sex with custom styling sns.set(style=\\"whitegrid\\") palette = sns.color_palette(\\"Set2\\") ax = sns.boxplot(data=tips, x=\'time\', y=\'tip\', hue=\'sex\', palette=palette) ax.set_title(\'Distribution of Tips by Time and Sex\') plt.show() def plot_joint_regression(): # Joint plot of total_bill vs. tip with regression lines based on time of day def time_regplot(x, y, **kwargs): sns.regplot(x=x, y=y, **kwargs) g = sns.FacetGrid(tips, hue=\'time\', height=5) g.map(time_regplot, \'total_bill\', \'tip\') g.add_legend() g.set_axis_labels(\'Total Bill\', \'Tip\') g.fig.suptitle(\'Joint Plot of Total Bill vs Tip with Regression Lines by Time of Day\', y=1.02) plt.show() def plot_facet_histograms(): # Facet grid of histograms of total_bill segmented by day and time g = sns.FacetGrid(tips, col=\'day\', row=\'time\', margin_titles=True) g.map(plt.hist, \'total_bill\', bins=10, color=\'skyblue\', edgecolor=\'black\') g.set_axis_labels(\'Total Bill\', \'Frequency\') g.fig.suptitle(\'Distribution of Total Bill by Day and Time\', y=1.02) plt.show()"},{"question":"Objective Create a Python class that processes `.netrc` files, handles errors, performs security checks, and retrieves authenticator data. Problem Statement Implement a custom class `CustomNetrcParser` that must encapsulate the following functionalities: 1. **Initialization**: - Initialize the instance by parsing a given `.netrc` file. - If no file is provided, default to reading the `.netrc` file from the user\'s home directory. 2. **Security Check**: - On POSIX systems, ensure that the .netrc file has secure permissions: - The file must be owned by the user running the process. - It should not be accessible for read/write by any other user. 3. **Methods**: - `get_authenticator(host: str) -> tuple`: Return a 3-tuple `(login, account, password)` for the given host. If no matching host is found, return the default entry. If neither is available, return `None`. 4. **Exception Handling**: - Raise a custom `CustomNetrcParseError` if there are parsing issues, including file format errors or security permission issues. Provide clear error messages including the file name and line number if applicable. Input and Output 1. **Class Initialization**: - Input: Optional file path as a string. - Output: None. 2. **Method `get_authenticator`**: - Input: `host` as a string. - Output: A 3-tuple `(login, account, password)` or `None`. Constraints - Assume the `.netrc` file follows the common netrc file format. - Passwords are restricted to ASCII punctuation and printable characters (excluding whitespace and non-printable characters). # Example Usage ```python parser = CustomNetrcParser(\\"/path/to/.netrc\\") # Initializes with specific path # or parser = CustomNetrcParser() # Initializes with default path auth_data = parser.get_authenticator(\\"example.com\\") # Expected output: (\\"user\\", \\"account\\", \\"password\\") or None ``` Ensure that your code includes appropriate error handling and security checks. Also, add necessary unit tests to validate your implementation.","solution":"import os import netrc import stat class CustomNetrcParseError(Exception): pass class CustomNetrcParser: def __init__(self, file_path=None): self.file_path = file_path or os.path.join(os.path.expanduser(\\"~\\"), \\".netrc\\") self.data = None self._validate_and_parse_netrc() def _validate_and_parse_netrc(self): if not os.path.isfile(self.file_path): raise CustomNetrcParseError(f\\"File not found: {self.file_path}\\") if os.name == \'posix\': stat_info = os.stat(self.file_path) if (stat_info.st_uid != os.getuid() or stat.S_IMODE(stat_info.st_mode) & (stat.S_IRWXG | stat.S_IRWXO)): raise CustomNetrcParseError(f\\"Insecure file permissions for {self.file_path}\\") try: self.data = netrc.netrc(self.file_path) except (netrc.NetrcParseError, IOError) as e: raise CustomNetrcParseError(f\\"Error parsing {self.file_path}: {str(e)}\\") def get_authenticator(self, host): try: auth = self.data.authenticators(host) if auth: return auth else: return self.data.authenticators(\\"default\\") except KeyError: return None"},{"question":"# Advanced Python Coding Assessment: Handling asyncio Exceptions **Objective**: This question is designed to test your understanding of the `asyncio` module and handling its specific exceptions in Python. You will implement a function that performs a series of asynchronous operations and handles the different `asyncio` exceptions appropriately. # Problem Statement You are required to implement a function `perform_async_operations(operations: List[Callable[[], Awaitable[Any]]], timeout: float) -> List[Union[Any, str]]` which performs a list of asynchronous operations within a given timeout period. The function should handle various `asyncio` exceptions as specified below. # Function Signature ```python from typing import List, Callable, Awaitable, Any, Union async def perform_async_operations(operations: List[Callable[[], Awaitable[Any]]], timeout: float) -> List[Union[Any, str]]: ``` # Parameters - `operations`: A list of callables, each returning an awaitable (coroutine). These represent the asynchronous operations to be performed. - `timeout`: A float value representing the maximum time (in seconds) allowed for each operation before it is cancelled. # Exceptions Handling 1. **asyncio.TimeoutError**: If an operation exceeds the given timeout, return \'Timeout\' for that operation. 2. **asyncio.CancelledError**: If an operation is cancelled, return \'Cancelled\' for that operation. 3. **asyncio.InvalidStateError**: If one of the operations leads to an invalid state, return \'Invalid State\' for that operation. 4. Other possible exceptions: For all other exceptions, return \'Other Error\' for that operation. # Output - The function returns a list of results, where each result corresponds to the outcome of the respective operation from the input list. If an operation completes successfully, its result is included in the output list. If an exception is raised, the respective exception message (as described) is included instead. # Constraints - You must use the `asyncio` module for managing the asynchronous operations. - All operations must respect the given timeout. - Ensure that your function handles exceptions gracefully and does not crash. # Example ```python import asyncio async def sample_operation(success: bool, delay: float): await asyncio.sleep(delay) if not success: raise asyncio.InvalidStateError return \'Success\' async def main(): operations = [ lambda: sample_operation(True, 1), # Should complete successfully lambda: sample_operation(False, 1), # Should raise InvalidStateError lambda: sample_operation(True, 3), # Should timeout ] results = await perform_async_operations(operations, timeout=2.0) print(results) # Expected: [\'Success\', \'Invalid State\', \'Timeout\'] # Running the example asyncio.run(main()) ``` # Notes - Ensure that you properly test the function with different scenarios, including success, various exceptions, and timeouts. - Properly handle coroutine functions and ensure they are awaited correctly. - Pay attention to performance, especially in terms of ensuring that timeouts and cancellations are respected for long-running operations. Good luck!","solution":"import asyncio from typing import List, Callable, Awaitable, Any, Union async def perform_async_operations(operations: List[Callable[[], Awaitable[Any]]], timeout: float) -> List[Union[Any, str]]: async def wrapper(operation): try: return await asyncio.wait_for(operation(), timeout) except asyncio.TimeoutError: return \'Timeout\' except asyncio.CancelledError: return \'Cancelled\' except asyncio.InvalidStateError: return \'Invalid State\' except Exception: return \'Other Error\' results = await asyncio.gather(*(wrapper(op) for op in operations), return_exceptions=True) return results"},{"question":"<|Analysis Begin|> The provided documentation outlines the different probability distribution classes available in PyTorch under the module `torch.distributions`. Each distribution class appears to have standard methods and properties such as sampling methods, log probability calculations, and parameter validation. This can be used to create an assessment question focused on understanding and working with these distribution classes. Given the scope of the documentation, an ideal question should require the student to demonstrate their ability to work with multiple distributions, perform operations like sampling, calculating probabilities, and perhaps even combining different distributions. <|Analysis End|> <|Question Begin|> **Question: Working with Probability Distributions in PyTorch** Implement a function `compare_distributions` in PyTorch that generates samples from two different distributions, computes specific properties of these samples, and returns a summary. The distributions to be used are a Bernoulli distribution and a Normal distribution. Your function should exhibit the following steps: 1. Generate `n` samples from a Bernoulli distribution with probability `p`. 2. Generate `n` samples from a Normal distribution with mean `mu` and standard deviation `sigma`. 3. Calculate and return the mean and variance of each set of samples. **Function Signature:** ```python import torch def compare_distributions(n: int, p: float, mu: float, sigma: float) -> dict: # your implementation here pass ``` **Input:** - `n` (int): The number of samples to generate from each distribution. - `p` (float): The probability of success for the Bernoulli distribution. Must be in the range `[0, 1]`. - `mu` (float): The mean of the Normal distribution. - `sigma` (float): The standard deviation of the Normal distribution. Must be positive. **Output:** - A dictionary with keys: - `\'bernoulli_mean\'`: Mean of the samples from the Bernoulli distribution. - `\'bernoulli_variance\'`: Variance of the samples from the Bernoulli distribution. - `\'normal_mean\'`: Mean of the samples from the Normal distribution. - `\'normal_variance\'`: Variance of the samples from the Normal distribution. **Constraints:** - `100 <= n <= 10000` - `0 <= p <= 1` - `sigma > 0` **Example:** ```python result = compare_distributions(1000, 0.5, 0, 1) print(result) ``` output: ```python { \'bernoulli_mean\': 0.5, \'bernoulli_variance\': 0.25, \'normal_mean\': 0.01, # This can vary \'normal_variance\': 0.98 # This can vary } ``` **Performance Requirements:** - The function should efficiently handle up to 10000 samples for each distribution. - Consider utilizing tensor operations to keep the implementation performant and concise. **Hint:** - Use `torch.distributions.Bernoulli` and `torch.distributions.Normal` for creating the distributions and sampling from them. - Utilize PyTorch\'s tensor operations to compute mean and variance. --- This question evaluates the student\'s understanding of different PyTorch distributions, sampling methods, and basic statistical calculations.","solution":"import torch def compare_distributions(n: int, p: float, mu: float, sigma: float) -> dict: # Create Bernoulli distribution with probability p bernoulli_dist = torch.distributions.Bernoulli(p) # Sample n times from the Bernoulli distribution bernoulli_samples = bernoulli_dist.sample((n,)) # Calculate mean and variance of Bernoulli samples bernoulli_mean = bernoulli_samples.mean().item() bernoulli_variance = bernoulli_samples.var(unbiased=False).item() # Create Normal distribution with mean mu and standard deviation sigma normal_dist = torch.distributions.Normal(mu, sigma) # Sample n times from the Normal distribution normal_samples = normal_dist.sample((n,)) # Calculate mean and variance of Normal samples normal_mean = normal_samples.mean().item() normal_variance = normal_samples.var(unbiased=False).item() # Return a dictionary with the results return { \'bernoulli_mean\': bernoulli_mean, \'bernoulli_variance\': bernoulli_variance, \'normal_mean\': normal_mean, \'normal_variance\': normal_variance }"},{"question":"# Seaborn Coding Assessment Question --- **Objective:** Demonstrate your understanding of the Seaborn library by creating a complex, customized visualization using the `boxenplot` function. **Task:** You are required to analyze a dataset and create a detailed boxen plot that compares different groups and sub-groups within the dataset. Your task will involve both data processing and visualization customization. **Dataset:** The infamous \\"Titanic\\" dataset from Seaborn. You can load it using: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` **Requirements:** 1. **Plot Type:** - Use `boxenplot` to create the visualization. 2. **Data:** - Visualize the distribution of passengers\' ages. - The main grouping should be based on the class of the passenger (`pclass`). - Sub-group based on the survival status (`survived`). 3. **Customization:** - Add appropriate titles and labels for both axes. - Customize the color of the boxen plots to distinguish between the survival statuses. - Set the width of the boxes to be reduced linearly. - Modify the appearance of the median line and outliers. 4. **Function Requirements:** - Define a function `create_titanic_age_boxenplot()` that does not take any parameters. - The function should process the data and generate the plot following the requirements mentioned above. - Ensure the function shows the plot as output. **Constraints:** - The function should use at least three customization parameters of the `boxenplot` function. - Consider readability and proper documentation in your code for additional scoring. **Example of the function signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_titanic_age_boxenplot(): # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Your implementation here # Show the plot plt.show() ``` **Expected Output:** A customized `boxenplot` displayed using Matplotlib, grouped and color-coded by the specified criteria, with appropriate labels and titles. --- **Note:** You are free to explore additional customizations and data processing steps to further refine the plot and add more insightful visual elements based on the Titanic dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_age_boxenplot(): # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create the boxenplot plt.figure(figsize=(10, 6)) boxen_plot = sns.boxenplot( x=\'pclass\', y=\'age\', hue=\'survived\', data=titanic, palette=\'coolwarm\', # Customizing color palette linewidth=1.5, # Customizing the linewidth of the box borders width=0.6 # Customizing the width of the boxes ) # Customize the plot appearance boxen_plot.set_title(\'Distribution of Passenger Ages by Class and Survival Status\') boxen_plot.set_xlabel(\'Passenger Class\') boxen_plot.set_ylabel(\'Age\') # Customize the legend plt.legend(title=\'Survived\', loc=\'upper right\') # Show the plot plt.show()"},{"question":"Using the `ast` module, you are required to write a function that processes a Python script, identifies all functions that contain a specific keyword in their names, and returns a list of their names and the lines where they are defined. Problem Statement: Write a function `find_keyword_functions` that: 1. Takes the Python script as a string input and a keyword string. 2. Parses the script into an AST. 3. Traverses the AST to identify all function names containing the provided keyword. 4. Returns a list of tuples, each containing the function name and the line number where it is defined. Function Signature: ```python def find_keyword_functions(script: str, keyword: str) -> List[Tuple[str, int]]: pass ``` Input: - `script`: A string containing the Python script. - `keyword`: A string representing the keyword to look for in function names. Output: - Returns a list of tuples, each containing the function name (str) and its defined line number (int). Constraints: - Function names must be case-sensitive when matching the keyword. - Assume the script is syntactically correct. - Keyword can appear anywhere in the function name. # Example: ```python script = def compute_area(radius): return 3.14 * radius * radius def helper_function(): pass def compute_volume(length, width, height): return length * width * height keyword = \\"compute\\" print(find_keyword_functions(script, keyword)) ``` Expected output: ``` [(\'compute_area\', 2), (\'compute_volume\', 8)] ``` Guidance: 1. Use the `ast` module to parse the input script. 2. Implement a custom `NodeVisitor` or subclass `NodeVisitor` to visit function definition nodes. 3. Check if the function names contain the given keyword. 4. Collect the function names and their line numbers in a list and return the list. # Note: - You may use `ast.parse()` for parsing the script into an AST. - Utilize `ast.walk()` or `NodeVisitor` to traverse the AST. Good luck!","solution":"import ast from typing import List, Tuple def find_keyword_functions(script: str, keyword: str) -> List[Tuple[str, int]]: Identify all function names containing the provided keyword and return a list of their names and the lines where they are defined. :param script: A string containing the Python script. :param keyword: A keyword to look for in function names. :return: A list of tuples with function names and their line numbers. class FunctionVisitor(ast.NodeVisitor): def __init__(self, keyword): self.keyword = keyword self.results = [] def visit_FunctionDef(self, node): if self.keyword in node.name: self.results.append((node.name, node.lineno)) self.generic_visit(node) tree = ast.parse(script) visitor = FunctionVisitor(keyword) visitor.visit(tree) return visitor.results"},{"question":"Task You will work with the experimental `BooleanArray` feature of pandas to manipulate a DataFrame. You are required to implement a function `process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations on the provided DataFrame `df`: 1. **Create a New Column:** - Add a new column `C` to the DataFrame which is a `BooleanArray` constructed from the result of a logical OR (`|`) operation between columns `A` and `B`. Ensure to handle any `NA` values correctly based on Kleene logic. 2. **NA Value Handling:** - In the new column `C`, replace any `NA` values with `True`. 3. **Performance Optimization:** - Print the dtype of each column. Ensure the dtype of the new column `C` is optimized for performance. 4. **Return the Modified DataFrame:** - Return the modified DataFrame. Example Suppose the input DataFrame `df` is as follows: ``` A B 0 True False 1 False <NA> 2 <NA> True 3 True <NA> ``` After executing the function `process_boolean_dataframe(df)`, the modified DataFrame should be: ``` A B C 0 True False True 1 False <NA> True 2 <NA> True True 3 True <NA> True ``` Additionally, the function should print: ``` A boolean B boolean C boolean dtype: object ``` Constraints - The DataFrame will only contain boolean and `NA` values. - Ensure the new column handles `NA` values correctly as per the Kleene logic described above. # Implementation ```python import pandas as pd def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Create a new column \'C\' using logical OR (|) operation between columns \'A\' and \'B\' df[\'C\'] = df[\'A\'] | df[\'B\'] # Step 2: Replace any NA values in column \'C\' with True df[\'C\'] = df[\'C\'].fillna(True) # Step 3: Print the dtypes of each column print(df.dtypes) # Step 4: Return the modified DataFrame return df # Example usage: df = pd.DataFrame({ \'A\': pd.array([True, False, pd.NA, True], dtype=\'boolean\'), \'B\': pd.array([False, pd.NA, True, pd.NA], dtype=\'boolean\') }) result = process_boolean_dataframe(df) print(result) ``` Note: - Ensure you use the experimental `BooleanArray` for new columns and logical operations. - Handle `NA` values correctly for logical operations and filling them. - Print the dtypes of each column to ensure they are optimized.","solution":"import pandas as pd def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Create a new column \'C\' using logical OR (|) operation between columns \'A\' and \'B\' df[\'C\'] = df[\'A\'] | df[\'B\'] # Step 2: Replace any NA values in column \'C\' with True df[\'C\'] = df[\'C\'].fillna(True) # Step 3: Print the dtypes of each column print(df.dtypes) # Step 4: Return the modified DataFrame return df"},{"question":"**Question:** You are given a script that processes a text file to compute its line count and then reads its first line. The script occasionally fails with a `bad file descriptor error` and sometimes leaves files open which leads to `ResourceWarning`. Your task is to improve the script to handle these issues and explicitly close resources. Your task consists of the following steps: 1. **Identify issues:** Add functionality to detect runtime issues using the Python Development Mode. 2. **Fix the script:** Modify the script to close files explicitly and handle resource warnings and errors properly. **Original Script:** ```python import os import sys def process_file(file_path): fp = open(file_path) first_line = fp.readline().strip() line_count = len(fp.readlines()) print(f\\"First line: {first_line}\\") print(f\\"Total lines: {line_count}\\") os.close(fp.fileno()) if __name__ == \\"__main__\\": process_file(sys.argv[1]) ``` **Improved Script Requirements:** - Modify the `process_file` function to ensure files are closed explicitly, using best practices. - Identify and fix the specific errors causing the `bad file descriptor error`. - Use Python Development Mode to help diagnose and verify that the issues have been resolved. **Input:** - A path to a text file provided as a command-line argument. **Output:** - The first line of the file and the total number of lines in the file. **Constraints:** - The script is executed in Python 3.8 or later version. **Performance Requirements:** - Ensure the script runs efficiently even for large files. **Example:** Given a file `example.txt` with the content: ``` First line Second line Third line ``` Running the script should produce: ``` First line: First line Total lines: 3 ``` **What you need to deliver:** 1. The improved version of the script. 2. A short description of the changes you made and how they handle the issues identified with the original script. **Hints:** - Consider using context managers to handle file operations. - Use Python Development Mode and `tracemalloc` to help identify the lines where resources are not handled correctly.","solution":"def process_file(file_path): Processes a given file to print its first line and the total number of lines. It ensures that the file is properly closed all the time. try: with open(file_path, \'r\') as fp: first_line = fp.readline().strip() fp.seek(0) # reset pointer to start of file to count lines line_count = sum(1 for _ in fp) print(f\\"First line: {first_line}\\") print(f\\"Total lines: {line_count}\\") except OSError as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Out-of-Core Learning in Scikit-learn** **Context:** You are tasked with building a text classification system that can handle a large dataset, which cannot fit into your computer\'s main memory (RAM). To achieve this, you need to utilize scikit-learn\'s out-of-core learning capabilities. **Task:** Implement a function to perform out-of-core learning for text classification. The function should: 1. Stream data from a text file. 2. Extract features using the `HashingVectorizer`. 3. Train an incremental learning model (`SGDClassifier`). **Function Signature:** ```python def out_of_core_text_classification(file_path: str, batch_size: int = 1000) -> SGDClassifier: Perform out-of-core learning for text classification. Parameters: - file_path: str : Path to the text file containing the data. The file should have one document per line with the format \'label text\'. - batch_size: int : The number of instances to include in each mini-batch. Returns: - SGDClassifier : The trained SGDClassifier model. pass ``` **Input:** - `file_path` (str): Path to the text file containing the data. The file format should have each line as: `<label> <text>`. - `batch_size` (int): Number of documents to process in each batch (default is 1000). **Output:** - An instance of the trained `SGDClassifier` model. **Requirements:** 1. The function should read data incrementally from the provided file. 2. Utilize the `HashingVectorizer` from scikit-learn for feature extraction. 3. Incrementally train an `SGDClassifier` using the `partial_fit` method. 4. The first call to `partial_fit` should include all possible class labels. **Example Usage:** ```python model = out_of_core_text_classification(\\"large_text_data.txt\\", batch_size=500) ``` **Performance Constraints:** - Ensure memory efficiency by never holding the entire dataset in memory. - The function should handle large files gracefully without crashing due to memory issues. **Notes:** - For simplicity, you can assume the entire dataset fits on disk. - You may use libraries such as `pandas` for reading data in chunks. **Sample Data:** Consider a file `large_text_data.txt` with the following content: ``` 1 This is the first document. 0 This document is the second document. 1 And this is the third one. 0 Is this the first document? ``` This should be enough to test your implementation. Good luck!","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np def out_of_core_text_classification(file_path: str, batch_size: int = 1000) -> SGDClassifier: Perform out-of-core learning for text classification. Parameters: - file_path: str : Path to the text file containing the data. The file should have one document per line with the format \'label text\'. - batch_size: int : The number of instances to include in each mini-batch. Returns: - SGDClassifier : The trained SGDClassifier model. vectorizer = HashingVectorizer(stop_words=\'english\', n_features=2**20) classifier = SGDClassifier() classes = np.array([0, 1]) with open(file_path, \'r\') as f: lines = [] for line in f: lines.append(line) if len(lines) >= batch_size: labels, texts = zip(*[l.split(\' \', 1) for l in lines]) X = vectorizer.transform(texts) y = np.array(labels, dtype=int) classifier.partial_fit(X, y, classes=classes) lines = [] if lines: labels, texts = zip(*[l.split(\' \', 1) for l in lines]) X = vectorizer.transform(texts) y = np.array(labels, dtype=int) classifier.partial_fit(X, y, classes=classes) return classifier"},{"question":"**Coding Assessment Question:** # Context You are tasked with implementing a PyTorch-based utility function that leverages accelerator devices (e.g., GPUs) for efficient computation. Specifically, you need to: 1. Check for the availability of accelerator devices. 2. If devices are available, set the device to a specified index. 3. Retrieve the current device index and accelerator. 4. Perform a mock computation using streams. 5. Synchronize the computation to ensure all operations are completed. # Objective Implement a function `manage_accelerator_computation` that performs the described tasks. Your function should simulate a basic computation task on the available accelerator devices and ensure proper synchronization. # Function Signature ```python def manage_accelerator_computation(device_index: int) -> str: Manages accelerator devices and performs a mock computation. Parameters: - device_index (int): The index of the device to set for the computation. Returns: - str: A message indicating the success of the operations and the device used. ``` # Input - `device_index` (int): An integer representing the index of the accelerator device to use. - Constraints: `device_index` should be a non-negative integer less than the total number of available devices. # Output - A string indicating the success of the operations and the device used. # Instructions 1. **Check device availability**: Use `is_available` to determine if an accelerator is available. If not available, return \\"No accelerator devices available.\\" 2. **Set Device**: Use `set_device_index` or `set_device_idx` to set the device to the specified `device_index`. 3. **Retrieve Information**: Use `current_device_index` or `current_device_idx` and `current_accelerator` to retrieve the current device information. 4. **Mock Computation with Streams**: - Create and set a stream using `set_stream`. - Perform a mock operation (e.g., a simple loop or a placeholder comment indicating a computation task). - Retrieve the current stream using `current_stream`. 5. **Synchronize**: Use `synchronize` to ensure all operations are complete. 6. **Return the Result**: Return a message that includes the current device information and indicates the successful completion of the computation. # Example ```python # Assume the device_index is within the available range, e.g., device_index = 0 print(manage_accelerator_computation(0)) # Expected Output (if a GPU is available): # \\"Computation completed successfully on device 0 (accelerator details).\\" ``` Note: Replace \\"(accelerator details)\\" with the actual details of the current accelerator. # Additional Notes - Ensure proper error handling if the provided `device_index` is out of range. - You may simulate the mock computation with a simple operation such as a for-loop or an illustrative comment. - Test your function on systems with and without accelerator devices to ensure robust handling of different scenarios.","solution":"import torch def manage_accelerator_computation(device_index: int) -> str: Manages accelerator devices and performs a mock computation. Parameters: - device_index (int): The index of the device to set for the computation. Returns: - str: A message indicating the success of the operations and the device used. if not torch.cuda.is_available(): return \\"No accelerator devices available.\\" try: # Set the device to the specified index torch.cuda.set_device(device_index) except AssertionError: return f\\"Invalid device index: {device_index}. No such device.\\" # Retrieve current device information current_device = torch.cuda.current_device() device_name = torch.cuda.get_device_name(current_device) # Create and set a stream stream = torch.cuda.Stream() torch.cuda.set_stream(stream) # Mock computation # This is a placeholder for actual computation; we\'ll use a simple loop for _ in range(1000): pass # Mock computation here # Retrieve and synchronize the current stream current_stream = torch.cuda.current_stream() current_stream.synchronize() return f\\"Computation completed successfully on device {current_device} ({device_name}).\\""},{"question":"**Objective:** Demonstrate your understanding of unsupervised dimensionality reduction techniques in scikit-learn by applying PCA, Random Projections, and Feature Agglomeration on a given dataset and evaluating their performance. # Task: 1. **Load the dataset:** - Use the `load_digits` dataset from `sklearn.datasets`. 2. **Perform Dimensionality Reduction:** Implement the following dimensionality reduction techniques: - **PCA:** Reduce the dataset to 2 dimensions. - **Random Projections:** Use Gaussian random projection to reduce the dataset to 2 dimensions. - **Feature Agglomeration:** Reduce the dataset to 2 dimensions. 3. **Visualization:** - Create a 2D scatter plot for each method to visualize the reduced dimensionality data. Color the points with the target labels from the dataset. 4. **Compare and Evaluate:** - Discuss the differences observed in the scatter plots. Which method seems to separate the data better based on visual inspection? # Implementation: 1. **Input:** - Load the `load_digits` dataset which is included in scikit-learn. 2. **Output:** - 3 scatter plots for each dimensionality reduction method. 3. **Constraints:** - Use a random state of 42 wherever applicable for reproducibility. # Example: ```python # Step 1: Load the dataset from sklearn.datasets import load_digits digits = load_digits() X = digits.data y = digits.target # Step 2: PCA from sklearn.decomposition import PCA pca = PCA(n_components=2, random_state=42) X_pca = pca.fit_transform(X) # Step 3: Random Projections from sklearn.random_projection import GaussianRandomProjection rp = GaussianRandomProjection(n_components=2, random_state=42) X_rp = rp.fit_transform(X) # Step 4: Feature Agglomeration from sklearn.cluster import FeatureAgglomeration agglo = FeatureAgglomeration(n_clusters=2) X_agglo = agglo.fit_transform(X) # Step 5: Visualization import matplotlib.pyplot as plt # PCA Scatter Plot plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'PCA\') plt.show() # Random Projection Scatter Plot plt.scatter(X_rp[:, 0], X_rp[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Random Projection\') plt.show() # Feature Agglomeration Scatter Plot plt.scatter(X_agglo[:, 0], X_agglo[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Feature Agglomeration\') plt.show() ``` Finally, discuss your observations based on the scatter plots generated by each dimensionality reduction method. **Note:** Ensure to add any necessary imports and handle any exceptions that may arise during the implementation.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration # Load the dataset digits = load_digits() X = digits.data y = digits.target # PCA: Reduce the dataset to 2 dimensions pca = PCA(n_components=2, random_state=42) X_pca = pca.fit_transform(X) # Random Projections: Use Gaussian random projection to reduce the dataset to 2 dimensions rp = GaussianRandomProjection(n_components=2, random_state=42) X_rp = rp.fit_transform(X) # Feature Agglomeration: Reduce the dataset to 2 dimensions agglo = FeatureAgglomeration(n_clusters=2) X_agglo = agglo.fit_transform(X) # Visualization: Create scatter plots for each method # PCA Scatter Plot plt.figure(figsize=(10, 5)) plt.subplot(1, 3, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'PCA\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') # Random Projection Scatter Plot plt.subplot(1, 3, 2) plt.scatter(X_rp[:, 0], X_rp[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Random Projection\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') # Feature Agglomeration Scatter Plot plt.subplot(1, 3, 3) plt.scatter(X_agglo[:, 0], X_agglo[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Feature Agglomeration\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.tight_layout() plt.show()"},{"question":"# Python Coding Assessment Question Objective: Implement and test a function using Python\'s `unittest` framework and utilities from the `test.support` module. Problem Statement: You need to implement a function called `calculate_stats` that takes a list of numbers and returns a dictionary containing the following statistics: `mean`, `median`, and `mode`. Your implementation should also handle edge cases where the list is empty. Additionally, you must write a suite of unit tests to verify the correctness of your implementation. Use the utilities provided by the `test.support` module wherever applicable. Function Specification: ```python def calculate_stats(numbers: list) -> dict: Calculate the mean, median, and mode of a list of numbers. Parameters: - numbers (list): A list of integers or floats. Returns: - dict: A dictionary with keys \'mean\', \'median\', and \'mode\'. If the list is empty, return {\'mean\': None, \'median\': None, \'mode\': None}. pass # Your implementation here ``` Unit Test Specification: Your unit test should be created in a class named `TestCalculateStats` that inherits from `unittest.TestCase`. Ensure your tests: - Cover various input cases including an empty list, a list with one element, a list with multiple elements, and a list with multiple modes. - Utilize relevant functions and context managers from the `test.support` module. - Follow the naming conventions and guidelines for writing tests as described in the provided documentation. Example: ```python import unittest from test import support from mymodule import calculate_stats # Assuming the function is in a module called mymodule class TestCalculateStats(unittest.TestCase): def test_empty_list(self): # Test that an empty list returns None for all statistics result = calculate_stats([]) expected = {\'mean\': None, \'median\': None, \'mode\': None} self.assertEqual(result, expected) def test_single_element(self): # Test that a list with one element returns that element for all statistics result = calculate_stats([5]) expected = {\'mean\': 5, \'median\': 5, \'mode\': 5} self.assertEqual(result, expected) def test_multiple_elements(self): # Test a list with multiple elements result = calculate_stats([1, 2, 3, 4, 5]) expected = {\'mean\': 3, \'median\': 3, \'mode\': 1} self.assertEqual(result[\'mean\'], expected[\'mean\']) self.assertEqual(result[\'median\'], expected[\'median\']) self.assertEqual(result[\'mode\'], expected[\'mode\']) def test_with_modes(self): # Test list with multiple modes result = calculate_stats([1, 1, 2, 2, 3]) expected = {\'mean\': 1.8, \'median\': 2, \'mode\': 1} # Assuming mode is the smallest or any one of them self.assertAlmostEqual(result[\'mean\'], expected[\'mean\'], places=1) self.assertEqual(result[\'median\'], expected[\'median\']) self.assertEqual(result[\'mode\'], expected[\'mode\']) # Additional utility tests can be added here if __name__ == \'__main__\': unittest.main() ``` **Note:** The `calculate_stats` function needs to be implemented in `mymodule.py`, and the tests should be placed in the same or a separate test module, depending on the setup. Constraints: 1. You must use the `unittest` framework for testing. 2. Utilize relevant tools and context managers from `test.support` for setting up tests and handling exceptions. 3. Ensure your tests are robust, covering various edge cases and scenarios. Submission: - A Python file containing the `calculate_stats` function. - A Python file containing the `TestCalculateStats` class with the required unit tests.","solution":"from statistics import mean, median, mode, StatisticsError def calculate_stats(numbers: list) -> dict: Calculate the mean, median, and mode of a list of numbers. Parameters: - numbers (list): A list of integers or floats. Returns: - dict: A dictionary with keys \'mean\', \'median\', and \'mode\'. If the list is empty, return {\'mean\': None, \'median\': None, \'mode\': None}. if not numbers: return {\'mean\': None, \'median\': None, \'mode\': None} stats = {} try: stats[\'mean\'] = mean(numbers) stats[\'median\'] = median(numbers) stats[\'mode\'] = mode(numbers) except StatisticsError: stats[\'mode\'] = min(numbers, key=numbers.count) # Handle multiple modes case return stats"},{"question":"# Python Coding Assessment Question **Objective:** Demonstrate your understanding of control flow, error handling, and function definitions in Python by implementing a function that processes a list of commands and executes appropriate actions based on those commands. The function should also handle possible errors gracefully. **Problem Statement:** Write a function `process_commands(commands: List[str]) -> Dict[str, int]` that takes a list of string commands and executes them. Each command is one of the following: - `\\"increment\\"`: Increases a counter by 1. - `\\"decrement\\"`: Decreases the counter by 1. - `\\"reset\\"`: Resets the counter to 0. - `\\"double\\"`: Doubles the counter. Additionally, the function should: - Ignore invalid commands. - Log each command\'s execution. - Handle any unexpected errors by logging an error message for the command and continuing with the next command. The function should return a dictionary with the final value of the counter and the total number of commands processed. **Input:** - `commands`: A list of strings representing the commands. **Output:** - A dictionary with two keys: - `\\"counter\\"`: The final value of the counter after processing all valid commands. - `\\"commands_processed\\"`: The total number of valid commands processed. **Constraints:** - The initial value of the counter is `0`. - The function should handle at least 1 to 10,000 commands efficiently. - Each command is a non-empty string containing only lowercase letters. **Example:** ```python def process_commands(commands: List[str]) -> Dict[str, int]: # Your implementation here # Sample Input commands = [\\"increment\\", \\"double\\", \\"decrement\\", \\"invalid\\", \\"reset\\"] # Expected Output # { # \\"counter\\": 0, # \\"commands_processed\\": 4 # } ``` **Notes:** - Use control flow statements (`if`, `while`, `for`) to manage command execution. - Use `try`, `except`, `finally` blocks to handle errors. - Use function definitions to encapsulate the main logic. Good luck!","solution":"from typing import List, Dict def process_commands(commands: List[str]) -> Dict[str, int]: counter = 0 valid_commands_processed = 0 valid_commands = [\\"increment\\", \\"decrement\\", \\"reset\\", \\"double\\"] for command in commands: try: if command == \\"increment\\": counter += 1 valid_commands_processed += 1 elif command == \\"decrement\\": counter -= 1 valid_commands_processed += 1 elif command == \\"reset\\": counter = 0 valid_commands_processed += 1 elif command == \\"double\\": counter *= 2 valid_commands_processed += 1 else: # Log invalid commands here (ignored in this implementation) continue except Exception as e: # Log unexpected errors here (ignored in this implementation) continue return { \\"counter\\": counter, \\"commands_processed\\": valid_commands_processed }"},{"question":"# Question **Title: Matrix Operations and Gradient Computation with PyTorch** You have been provided with a set of operations to be performed on tensors using PyTorch. Your task is to implement a function `perform_operations` that takes three inputs: an integer `n`, a floating-point number `scale`, and a boolean `requires_grad`. The function should perform the following tasks: 1. Create a random `n x n` matrix `A` with elements sampled from a normal distribution. 2. Create a random `n x n` matrix `B` with elements uniformly sampled between 0 and 1. 3. Multiply `A` by the scalar `scale` and store the result in a tensor `C`. 4. Compute the matrix product `D` of `C` and `B`. 5. Sum all the elements of the tensor `D` to get a scalar value `E`. 6. If `requires_grad` is `True`, ensure that gradients are enabled and compute the gradients of `E` with respect to the input tensors. 7. Return a tuple containing `E`, and the gradients of `E` with respect to `A` and `B` (if `requires_grad` is `True`). If `requires_grad` is `False`, return only `E`. **Function Signature:** ```python import torch def perform_operations(n: int, scale: float, requires_grad: bool): pass ``` # Input - `n`: An integer (1 <= n <= 1000) specifying the size of the matrices. - `scale`: A float, the scalar to multiply with matrix `A`. - `requires_grad`: A boolean indicating whether to compute gradients. # Output - A tuple containing: - A scalar tensor `E` which is the sum of all elements in the resulting matrix `D`. - If `requires_grad` is `True`, also include the gradient tensors of `E` with respect to `A` and `B`. # Example ```python E, grad_A, grad_B = perform_operations(3, 0.5, True) print(E) # A scalar tensor print(grad_A) # Gradient of E with respect to A print(grad_B) # Gradient of E with respect to B ``` # Constraints - The function should handle matrices up to size `1000x1000` efficiently. - If `requires_grad` is `False`, gradients computation should not add any overhead. # Notes - Use `torch.randn` for creating a random normal matrix. - Use `torch.rand` for creating a random uniform matrix. - Ensure you use the correct gradient computation context based on `requires_grad`.","solution":"import torch def perform_operations(n: int, scale: float, requires_grad: bool): # Create a random n x n matrix A with elements sampled from a normal distribution A = torch.randn(n, n, requires_grad=requires_grad) # Create a random n x n matrix B with elements uniformly sampled between 0 and 1 B = torch.rand(n, n, requires_grad=requires_grad) # Multiply A by the scalar scale and store the result in a tensor C C = A * scale # Compute the matrix product D of C and B D = torch.mm(C, B) # Sum all the elements of the tensor D to get a scalar value E E = torch.sum(D) if requires_grad: # Compute the gradients of E with respect to the input tensors E.backward() grad_A = A.grad grad_B = B.grad return E, grad_A, grad_B else: return E"},{"question":"**Question: Create a Custom Animation Using the Turtle Module** **Objective:** You are required to demonstrate your understanding of the Python turtle graphics module by creating an advanced animation. The animation will feature multiple turtles that perform synchronized and unsynchronized movements and interactions. **Requirements:** 1. **Initial Setup**: - Create a TurtleScreen with a white background. - Initialize three turtles with different shapes and colors. 2. **Turtle Movements**: - Turtle 1 (Shape: \\"turtle\\", Color: \\"red\\"): Moves in a circular path with a radius of 100 pixels. - Turtle 2 (Shape: \\"circle\\", Color: \\"blue\\"): Moves in a square path with each side of the square being 150 pixels long. - Turtle 3 (Shape: \\"triangle\\", Color: \\"green\\"): Moves in a zigzag pattern where each segment is 80 pixels long. 3. **Pen Control**: - Each turtle should leave a trail with a unique pen color. - Implement a function to lift the pen and change the color at the halfway point of each path. 4. **Synchronization**: - Ensure Turtle 1 and Turtle 2 start and end their paths simultaneously. - Turtle 3 should start its movement after Turtle 1 and Turtle 2 have completed half of their paths. 5. **Screen Events**: - Attach a keyboard event to stop all turtle movements when the \\"s\\" key is pressed. 6. **Animation Control**: - Implement smooth and visible animation for the turtles. - Control the speed of the turtles to make the animation viewable. **Function Signature:** ```python def create_custom_animation(): # Implement your code here ``` **Constraints:** - Use only the turtle module methods and functions for creating the animation. - Ensure the animation can run on a standard Python environment without additional packages. **Example Usage:** ```python if __name__ == \\"__main__\\": create_custom_animation() ``` **Evaluation Criteria:** 1. **Correctness**: Ensure the turtles follow the specified paths correctly and synchronize their movements as required. 2. **Code Quality**: Write clean, readable, and well-documented code. 3. **Animation Smoothness**: Ensure the movement of the turtles appears smooth and visually appealing. 4. **Event Handling**: Properly implement and test the keyboard event to stop the turtles. By completing this task, you will demonstrate your ability to utilize the turtle graphics module to create complex animations, manage multiple turtles, and handle real-time events in Python.","solution":"import turtle import threading def move_in_circle(t, radius=100, steps=120): t.pendown() for _ in range(steps): t.circle(radius, extent=360/steps) if _ == steps // 2: t.penup() t.pencolor(\\"darkred\\") t.pendown() def move_in_square(t, side=150): t.pendown() for _ in range(2): for _ in range(4): t.forward(side) t.right(90) t.penup() t.pencolor(\\"darkblue\\") t.pendown() def move_in_zigzag(t, segment=80, count=10): t.pendown() for i in range(count): t.forward(segment) t.left(135 if i % 2 == 0 else 45) if i == count // 2: t.penup() t.pencolor(\\"darkgreen\\") t.pendown() def synchronize_movement(t2, t1_finished, t3_start): t1_finished.wait() # Wait for Orange Turtle to complete half of its path t3_start.set() # Start Green Turtle def stop_turtles(): turtle.bye() def create_custom_animation(): screen = turtle.Screen() screen.bgcolor(\\"white\\") screen.title(\\"Custom Turtle Animation\\") # Initialize turtles red_turtle = turtle.Turtle() red_turtle.shape(\\"turtle\\") red_turtle.color(\\"red\\") red_turtle.speed(1) red_turtle.penup() blue_turtle = turtle.Turtle() blue_turtle.shape(\\"circle\\") blue_turtle.color(\\"blue\\") blue_turtle.speed(1) blue_turtle.penup() blue_turtle.goto(-200, -200) green_turtle = turtle.Turtle() green_turtle.shape(\\"triangle\\") green_turtle.color(\\"green\\") green_turtle.speed(1) green_turtle.penup() green_turtle.goto(200, 200) screen.onkey(stop_turtles, \\"s\\") screen.listen() t1_finished = threading.Event() t3_start = threading.Event() t1_thread = threading.Thread(target=move_in_circle, args=(red_turtle, 100), daemon=True) t2_thread = threading.Thread(target=move_in_square, args=(blue_turtle, 150), daemon=True) t3_thread = threading.Thread(target=move_in_zigzag, args=(green_turtle, 80), daemon=True) sync_thread = threading.Thread(target=synchronize_movement, args=(green_turtle, t1_finished, t3_start), daemon=True) t1_thread.start() t2_thread.start() sync_thread.start() sync_thread.join() t3_start.wait() t3_thread.start() t1_thread.join() t2_thread.join() t3_thread.join() screen.mainloop() if __name__ == \\"__main__\\": create_custom_animation()"},{"question":"Image Batch Normalization **Objective:** Write a PyTorch function to normalize batches of grayscale image tensors. The function should perform z-score normalization on each image in the batch. **Problem Statement:** You are provided with a 4-dimensional tensor `images` representing a batch of grayscale images. The dimensions of the tensor are `(batch_size, height, width, channels)`. The function should perform z-score normalization for each image in the batch separately. Z-score normalization scales the pixel values such that the mean of the values within each image is 0 and the standard deviation is 1. **Function Specification:** ```python import torch def normalize_images(images: torch.Tensor) -> torch.Tensor: Normalize a batch of grayscale images using z-score normalization. Parameters: images (torch.Tensor): A 4D tensor of shape (batch_size, height, width, channels) representing a batch of images. - The tensor will only contain 1 channel (grayscale). Returns: torch.Tensor: A 4D tensor of shape (batch_size, height, width, channels) with normalized image data. ``` **Input Constraints:** - `images` tensor will have shape `(batch_size, height, width, channels)` where `channels = 1`. - The pixel values in `images` tensor are in the range `[0.0, 255.0]`. **Output Requirements:** - Return a tensor of the same shape and same device (CPU or CUDA) as the input `images` tensor. - Each image in the batch should be normalized individually. **Example:** ```python images = torch.tensor( [[[[ 0.], [ 50.], [100.]], [[150.], [200.], [255.]]], [[[125.], [125.], [125.]], [[125.], [125.], [125.]]]], dtype=torch.float32) normalized_images = normalize_images(images) # Expected output example (values may vary slightly due to floating-point calculations) # tensor([[[[-1.4639], [-0.8782], [-0.2925]], # [ 0.2932], [ 0.8789], [ 1.4646]]], # # [[[ 0.0000], [ 0.0000], [ 0.0000]], # [ 0.0000], [ 0.0000], [ 0.0000]]]]) ``` **Notes:** - Calculate the mean and standard deviation for each image (2D matrix) separately. - Avoid using any libraries/functions for normalization other than PyTorch. - Ensure the implementation utilizes PyTorch\'s capabilities efficiently, especially for tensor operations. **Performance Requirements:** - Efficiently process tensor operations to handle batches with large dimensions. - Ensure the code works seamlessly on both CPU and CUDA devices.","solution":"import torch def normalize_images(images: torch.Tensor) -> torch.Tensor: Normalize a batch of grayscale images using z-score normalization. Parameters: images (torch.Tensor): A 4D tensor of shape (batch_size, height, width, channels) representing a batch of images. - The tensor will only contain 1 channel (grayscale). Returns: torch.Tensor: A 4D tensor of shape (batch_size, height, width, channels) with normalized image data. mean = images.mean(dim=[1, 2, 3], keepdim=True) std = images.std(dim=[1, 2, 3], keepdim=True) normalized_images = (images - mean) / (std + 1e-8) # adding epsilon to avoid division by zero return normalized_images"},{"question":"You are required to implement a custom attention mechanism using PyTorch. This question will assess your understanding of both the attention mechanism and fundamental PyTorch features. # Problem Statement: Implement a custom `ScaledDotProductAttention` layer in PyTorch that scales the dot product in the attention mechanism by the square root of the dimension of the key vectors. This custom layer should be able to handle batches of input and should be compatible with the AutoGrad feature of PyTorch for backpropagation. # Specifics: 1. **Class Name**: `ScaledDotProductAttention` 2. **Method**: `forward(self, query, key, value, mask=None)` 3. **Inputs**: - `query`: A tensor of shape `(batch_size, num_heads, seq_length, d_k)`. - `key`: A tensor of shape `(batch_size, num_heads, seq_length, d_k)`. - `value`: A tensor of shape `(batch_size, num_heads, seq_length, d_v)`. - `mask`: An optional tensor of shape `(batch_size, 1, 1, seq_length)` that contains 0s and 1s, where 1s indicate entries to be ignored for computing attention (e.g., padding tokens). 4. **Output**: A tensor of shape `(batch_size, num_heads, seq_length, d_v)` representing the output of the scaled dot-product attention mechanism. 5. **Constraints**: - Ensure your implementation is efficient and leverages PyTorch\'s tensor operations optimally. - The implementation should be compatible with PyTorch\'s Autograd for automatic differentiation. 6. **Performance**: The attention mechanism should be computed efficiently in terms of both time and space complexity. # Example Usage: ```python import torch import torch.nn as nn class ScaledDotProductAttention(nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, query, key, value, mask=None): # Your implementation goes here # Example tensors batch_size = 2 num_heads = 4 seq_length = 10 d_k = 64 d_v = 64 query = torch.randn(batch_size, num_heads, seq_length, d_k) key = torch.randn(batch_size, num_heads, seq_length, d_k) value = torch.randn(batch_size, num_heads, seq_length, d_v) mask = None # or some mask tensor # Instantiate the class attention_layer = ScaledDotProductAttention() output = attention_layer(query, key, value, mask) print(output.shape) # Expected output: (batch_size, num_heads, seq_length, d_v) ``` Submitting your solution: - Implement the `ScaledDotProductAttention` class as defined. - Ensure your solution passes the basic outlined example. - You might add additional tests to validate the implementation further. - Document any assumptions or optimizations you make in your code.","solution":"import torch import torch.nn as nn import math class ScaledDotProductAttention(nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, query, key, value, mask=None): d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) attention_weights = torch.nn.functional.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output"},{"question":"In this problem, you are tasked with handling multiple signals in Python to safely manage a time-bound operation. You must use signal handling to ensure that a specific task does not run indefinitely and can be interrupted gracefully. # Specifications **Function**: `timed_execution` Description: Implement a Python function `timed_execution(timeout, task)` that performs the following: 1. Sets a timeout for executing a `task` function. 2. If the task completes within the specified `timeout`, the result of the task should be returned. 3. If the task exceeds the specified `timeout`, a custom exception `TimeoutException` must be raised. Input: * `timeout`: an integer, representing the maximum number of seconds the task is allowed to run. * `task`: a callable with no arguments that returns any value. Output: * Returns the result of the `task` if it completes before the timeout. * Raises a `TimeoutException` if the task does not complete within the specified timeout. Example: ```python def sample_task(): import time time.sleep(4) return \\"Task Completed\\" try: result = timed_execution(3, sample_task) print(result) except TimeoutException: print(\\"The task timed out.\\") ``` Constraints: * The function should be designed to run on Unix-based systems where the `signal` module supports setting alarm signals. * Task execution should be properly interrupted and should not leave the system hanging even in case of exceptions. # Instructions: 1. Implement the `TimeoutException` class which inherits from `Exception`. 2. Implement the `timed_execution` function as described. 3. Ensure proper cleanup of any alarms or signal handlers post execution. Additional Information * Utilize the `signal` module\'s `signal.signal()` and `signal.alarm()` functions to manage the timing mechanism. * Handle all necessary signal cleanup to ensure no residual alarms or handlers are left active after task execution. ```python import signal class TimeoutException(Exception): pass def handler(signum, frame): raise TimeoutException(\\"Function execution exceeded timeout\\") def timed_execution(timeout, task): # Set the signal handler and the alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: # Run the task result = task() except TimeoutException as te: raise te finally: # Disable the alarm signal.alarm(0) return result ``` You can test your solution with various task functions and different timeout values to ensure correctness and robustness.","solution":"import signal class TimeoutException(Exception): pass def handler(signum, frame): raise TimeoutException(\\"Function execution exceeded timeout\\") def timed_execution(timeout, task): # Set the signal handler and the alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: # Run the task result = task() except TimeoutException as te: raise te finally: # Disable the alarm signal.alarm(0) return result"},{"question":"**Objective: Understanding and Implementing Functions for PyFloatObject in Python.** **Problem Statement:** You are required to implement several functionalities that mimic some of the behaviors provided by the Python C-API for floating point objects (`PyFloatObject`). Implement the following functions: 1. **is_pyfloat(obj):** - **Input:** A Python object `obj`. - **Output:** Return `True` if `obj` is an instance of `float`, `False` otherwise. 2. **string_to_float(s):** - **Input:** A string `s` representing a floating-point number. - **Output:** Return a float object that corresponds to the string value. If the string does not represent a valid float, return `None`. 3. **double_to_pyfloat(d):** - **Input:** A number `d` of type `float`. - **Output:** Return a float object that represents the number `d`. If `d` is not a float, return `None`. 4. **pyfloat_to_double(obj):** - **Input:** A Python object `obj`. - **Output:** If `obj` is a `float`, return its double representation. If `obj` is not a `float`, but can be converted to a float using `__float__()`, convert it and return its double representation. If `__float__()` is not defined, then fallback to `__index__()`. Return `None` if it is not possible to convert to a float. 5. **get_float_info():** - **Input:** None. - **Output:** Return a dictionary with keys `\\"precision\\"`, `\\"min\\"`, and `\\"max\\"` corresponding to the system\'s float precision, minimum normalized positive float, and maximum representable finite float, respectively. **Constraints:** - Do not use any direct calls to C functions or the C-API. - The functions should handle all exceptions gracefully and return `None` where appropriate. - You may use the built-in `float` data type and functions available in Python. **Example:** ```python assert is_pyfloat(3.14) == True assert is_pyfloat(5) == False assert string_to_float(\\"123.45\\") == 123.45 assert string_to_float(\\"abc\\") == None assert double_to_pyfloat(123.45) == 123.45 assert double_to_pyfloat(\\"xyz\\") == None assert pyfloat_to_double(123.45) == 123.45 assert pyfloat_to_double(\\"123.45\\") == None assert get_float_info() == { \\"precision\\": 15, # This is an example, your precision might differ based on the system \\"min\\": 2.2250738585072014e-308, \\"max\\": 1.7976931348623157e+308 } ``` Complete the implementation of the above functions to demonstrate your proficiency in handling floating point objects in Python.","solution":"import sys def is_pyfloat(obj): Checks if the given object is of type float. return isinstance(obj, float) def string_to_float(s): Converts a string to a float. Returns None if conversion is not possible. try: return float(s) except (ValueError, TypeError): return None def double_to_pyfloat(d): Converts a number of type float to a float object. Returns None if the input is not a float. if isinstance(d, float): return float(d) return None def pyfloat_to_double(obj): Converts an object to its float representation if possible. Returns None if conversion is not possible. if isinstance(obj, float): return obj try: return float(obj) except (ValueError, TypeError, AttributeError): try: return float(obj.__float__()) except (AttributeError, ValueError, TypeError): try: return float(obj.__index__()) except (AttributeError, ValueError, TypeError): return None def get_float_info(): Retrieves the system\'s float information. Returns a dictionary with precision, min, and max float values. return { \\"precision\\": sys.float_info.dig, \\"min\\": sys.float_info.min, \\"max\\": sys.float_info.max }"},{"question":"You are tasked with creating a function that performs various calculations based on the type and value of the input using `functools` utilities. Your function will exhibit different behaviors for different input types and make use of caching mechanisms to improve performance. 1. Implement a function `calculate(arg)` decorated with `functools.singledispatch`. 2. Define the following behaviors using the `register` method of `calculate`: - For type `int`, return the factorial of the integer using a recursive function. Use `functools.lru_cache(maxsize=None)` to cache the results. - For type `str`, return the length of the string. - For type `list`, return the sum of all elements in the list. 3. Assign a default behavior for any other types to just return the type name as a string. 4. Ensure that `calculate` also supports clearing of the cache and displaying cache statistics when applied to integer inputs. **Input:** - Variants of inputs: ints, strings, lists, and other types. **Output:** - If input is an `int`, return factorial of the integer. - If input is a `str`, return the length of the string. - If input is a `list`, return the sum of all elements in the list. - For other types, return the type name as a string. - Methods to clear cache and get cache info. **Example Function Signatures:** ```python from functools import singledispatch, lru_cache @singledispatch def calculate(arg): return str(type(arg)) @calculate.register @lru_cache(maxsize=None) def _(arg: int) -> int: if arg == 0: return 1 else: return arg * calculate(arg - 1) @calculate.register def _(arg: str) -> int: return len(arg) @calculate.register def _(arg: list) -> int: return sum(arg) # Extra methods for cache handling def clear_cache(): calculate.dispatch(int).cache_clear() def get_cache_info(): return calculate.dispatch(int).cache_info() ``` **Constraints:** - Assume the list contains only integers. **Note:** - Carefully use decorators to ensure calculate function exhibits the required behavior. - Implement the `clear_cache` and `get_cache_info` methods to manage and view the cache used by the `int` type calculations.","solution":"from functools import singledispatch, lru_cache @singledispatch def calculate(arg): Default behavior for the calculate function. :param arg: input of any type :return: type name as string return str(type(arg)) @calculate.register @lru_cache(maxsize=None) def _(arg: int) -> int: Calculate the factorial of an integer. :param arg: integer :return: factorial of the integer if arg == 0: return 1 else: return arg * calculate(arg - 1) @calculate.register def _(arg: str) -> int: Calculate the length of a string. :param arg: string :return: length of the string return len(arg) @calculate.register def _(arg: list) -> int: Calculate the sum of all elements in a list. :param arg: list of integers :return: sum of the elements return sum(arg) def clear_cache(): Clear the cache used by the calculate function for integers. calculate.dispatch(int).cache_clear() def get_cache_info(): Get cache information used by the calculate function for integers. :return: cache information return calculate.dispatch(int).cache_info()"},{"question":"Coding Assessment Question # Objective This question aims to evaluate your understanding and ability to work with complex tensors in PyTorch, perform basic linear algebra operations, and use autograd for optimization. # Problem Statement You are given two real tensors `A` and `B` with shapes `(m, n, 2)` and `(n, p, 2)` respectively, where the last dimension represents real and imaginary parts of complex numbers. Your task is to: 1. Convert the given tensors `A` and `B` into complex tensors. 2. Perform a matrix multiplication of the complex tensors to obtain a resultant matrix `C`. 3. Compute the Frobenius norm of the resultant matrix `C`. 4. Define a function `complex_autograd_operation(A, B)` that: - Converts `A` and `B` to complex tensors. - Uses autograd to compute the gradient of the Frobenius norm of matrix `C` with respect to `A` and `B`. - Returns the gradients with respect to `A` and `B`. # Function Signature ```python import torch def complex_autograd_operation(A: torch.Tensor, B: torch.Tensor) -> (torch.Tensor, torch.Tensor): pass ``` # Input Constraints - `A` and `B` are 3D tensors of dtype `torch.float32` representing real and imaginary parts of complex numbers. - `A.shape == (m, n, 2)` - `B.shape == (n, p, 2)` # Output The function should return two tensors, which are the gradients of the Frobenius norm of `C` with respect to `A` and `B`. # Example ```python A = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[-1, -2], [-3, -4]]], dtype=torch.float32) B = torch.tensor([[[1,0], [0,1]], [[0,1], [1,0]]], dtype=torch.float32) grad_A, grad_B = complex_autograd_operation(A, B) print(\\"Gradient w.r.t A:\\", grad_A) print(\\"Gradient w.r.t B:\\", grad_B) ``` # Notes - Ensure that the computation is conducted on complex tensors and using PyTorch\'s optimized functions for matrix multiplication and autograd. - The function should handle the conversion to and from real and complex representations correctly.","solution":"import torch def complex_autograd_operation(A: torch.Tensor, B: torch.Tensor) -> (torch.Tensor, torch.Tensor): Converts real-imaginary tensors A and B to complex tensors, performs matrix multiplication, computes Frobenius norm of the resultant matrix, and computes the gradient of this norm with respect to A and B. Args: - A: torch.Tensor of shape (m, n, 2) representing real and imaginary parts of a complex matrix. - B: torch.Tensor of shape (n, p, 2) representing real and imaginary parts of a complex matrix. Returns: - gradient of Frobenius norm of the product w.r.t A. - gradient of Frobenius norm of the product w.r.t B. # Convert the real and imaginary parts to complex tensors A_complex = torch.view_as_complex(A) B_complex = torch.view_as_complex(B) # Enable gradient tracking A_complex.requires_grad_(True) B_complex.requires_grad_(True) # Perform matrix multiplication C_complex = torch.matmul(A_complex, B_complex) # Compute Frobenius norm frobenius_norm = torch.norm(C_complex) # Perform backward pass to compute gradients frobenius_norm.backward() # Extract gradients with respect to real and imaginary parts grad_A = torch.view_as_real(A_complex.grad) grad_B = torch.view_as_real(B_complex.grad) return grad_A, grad_B"},{"question":"# Platform-Specific `asyncio` Event Loop Implementation You are required to implement a Python function that sets up an `asyncio` event loop capable of handling both network connections and subprocesses. Depending on the operating system, the function should handle platform-specific limitations as described in the provided documentation. Task Requirements 1. Implement a function `setup_event_loop()` that: - Creates and returns an appropriate `asyncio` event loop. - Configures the loop to support both networking connections and subprocess management. - Handles platform-specific limitations as described in the documentation. 2. Your implementation should: - Use `ProactorEventLoop` on Windows. - Use `SelectorEventLoop` with `KqueueSelector` on macOS for modern versions and `SelectSelector` or `PollSelector` for versions older than macOS 10.9. - Ensure compatibility with these platform-specific behaviors, not using unsupported features. Function Signature ```python import asyncio def setup_event_loop(): Set up and return the appropriate asyncio event loop for the current platform. pass ``` Constraints - You must use platform detection to choose the appropriate event loop (`sys.platform`). - Do not use unsupported methods in any platform as specified in the documentation. - Assume Python 3.10 is used, reinforcing documentation from `asyncio`. Example Usage ```python # Example usage on different platforms will involve invoking the event loop import sys loop = setup_event_loop() if sys.platform.startswith(\'win\'): assert isinstance(loop, asyncio.ProactorEventLoop) elif sys.platform == \'darwin\': import selectors if hasattr(selectors, \'KqueueSelector\'): assert isinstance(loop._selector, selectors.KqueueSelector) else: import selectors assert isinstance(loop._selector, selectors.SelectSelector) # Create a sample coroutine and run it async def sample_coroutine(): pass loop.run_until_complete(sample_coroutine()) ``` Ensure your implementation is thoroughly tested on various platforms to validate its correctness according to the specified constraints.","solution":"import asyncio import sys import selectors def setup_event_loop(): Set up and return the appropriate asyncio event loop for the current platform. if sys.platform.startswith(\'win\'): loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) elif sys.platform == \'darwin\': # macOS, we\'ll prefer KqueueSelector if available if hasattr(selectors, \'KqueueSelector\'): selector = selectors.KqueueSelector() else: # macOS older than 10.9 fallback selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) else: # For other platforms, use the default selector loop = asyncio.SelectorEventLoop() asyncio.set_event_loop(loop) return loop"},{"question":"Objective: Implement a cache system that uses weak references to manage large object lifecycles efficiently. This requires using weakly-referenced dictionaries (`WeakValueDictionary`) to prevent cached objects from being kept alive solely due to caching. Problem Statement: You are required to implement a class `ObjectCache` that functions as a cache for storing large objects. The cache should only store objects as long as there are strong references to them elsewhere in the program. The cache should automatically clean up entries when the objects are no longer strongly referenced. Specifications: 1. **Class: `ObjectCache`** - **Methods:** - `__init__(self)`: Initializes an empty cache. - `add(self, key, value)`: Adds a key-value pair to the cache. - `get(self, key)`: Returns the value associated with the key if it exists in the cache and the object is still alive, otherwise returns `None`. - `remove(self, key)`: Removes the key-value pair from the cache. - `__len__(self)`: Returns the number of items in the cache. Constraints: - Keys are guaranteed to be strings. - Values can be any object that can be weakly referenced. Example Behavior: ```python import weakref class LargeObject: def __init__(self, name): self.name = name # Create a cache cache = ObjectCache() # Create and add large objects to the cache obj1 = LargeObject(\\"Object 1\\") cache.add(\\"first\\", obj1) obj2 = LargeObject(\\"Object 2\\") cache.add(\\"second\\", obj2) # Access the objects from the cache print(cache.get(\\"first\\").name) # Output: Object 1 # The cache length should be 2 print(len(cache)) # Output: 2 # Remove strong reference to obj1 and explicitly force garbage collection del obj1 import gc; gc.collect() # Accessing the first key should now return None since the object should be cleared print(cache.get(\\"first\\")) # Output: None # The cache length should be 1 print(len(cache)) # Output: 1 ``` Performance Requirements: - The cache should manage objects without significant overhead. - Operations should be performed in linear or sub-linear time complexity relative to the number of items in the cache. Implement the `ObjectCache` class to meet the above requirements.","solution":"import weakref class ObjectCache: def __init__(self): self.cache = weakref.WeakValueDictionary() def add(self, key, value): self.cache[key] = value def get(self, key): return self.cache.get(key) def remove(self, key): if key in self.cache: del self.cache[key] def __len__(self): return len(self.cache)"},{"question":"You are given the task of implementing functions that utilize memory-mapped files for efficient file manipulation. The goal is to create and manipulate memory-mapped files to perform reading and writing operations, as well as analyzing the content. Function 1: `create_memory_mapped_file` Implement a function `create_memory_mapped_file(file_path: str, content: bytes) -> None` that: - Takes a file path and a byte string. - Creates a new file at the given path and writes the provided content to the file. - Memory-maps the entire file and updates a specified portion of the fileâ€™s content. ```python import mmap import os def create_memory_mapped_file(file_path: str, content: bytes) -> None: # Step 1: Write the initial content to the file. with open(file_path, \\"wb\\") as f: f.write(content) # Step 2: Memory-map the entire file. with open(file_path, \\"r+b\\") as f: mm = mmap.mmap(f.fileno(), 0) # Step 3: Replace the first 5 bytes with b\\"HELLO\\" mm[:5] = b\\"HELLO\\" # Step 4: Ensure changes are written to disk. mm.flush() mm.close() ``` Function 2: `find_subsequence` Implement a function `find_subsequence(file_path: str, subsequence: bytes) -> int` that: - Takes a file path and a byte sequence to search. - Memory-maps the file and returns the lowest index where the subsequence is found. If not found, return -1. ```python import mmap def find_subsequence(file_path: str, subsequence: bytes) -> int: with open(file_path, \\"r+b\\") as f: mm = mmap.mmap(f.fileno(), 0) index = mm.find(subsequence) mm.close() return index ``` # Constraints 1. The file to be created should not exceed 1 MB in size. 2. For the method `find_subsequence`, the subsequence should be a non-empty byte string. # Inputs and Outputs `create_memory_mapped_file` - **Input:** - `file_path`: String representing the path to the file to be created. - `content`: Bytes representing the initial content of the file. - **Output:** - None (The function modifies the file content in place). `find_subsequence` - **Input:** - `file_path`: String representing the path to the file to be searched. - `subsequence`: Bytes representing the subsequence to search for. - **Output:** - Integer representing the lowest index where the subsequence is found or -1 if not found. # Example ```python # Example usage: # Create and modify a memory-mapped file create_memory_mapped_file(\\"example.txt\\", b\\"Hello Python!n\\") # Find subsequence in the memory-mapped file index = find_subsequence(\\"example.txt\\", b\\" world!\\") print(index) # Output: 6 index = find_subsequence(\\"example.txt\\", b\\"Python\\") print(index) # Output: -1 (since the original content was modified) ``` Implement these two functions to demonstrate your understanding of memory-mapped files and their efficient manipulation in Python.","solution":"import mmap import os def create_memory_mapped_file(file_path: str, content: bytes) -> None: # Step 1: Write the initial content to the file. with open(file_path, \\"wb\\") as f: f.write(content) # Step 2: Memory-map the entire file. with open(file_path, \\"r+b\\") as f: mm = mmap.mmap(f.fileno(), 0) # Step 3: Replace the first 5 bytes with b\\"HELLO\\" mm[:5] = b\\"HELLO\\" # Step 4: Ensure changes are written to disk. mm.flush() mm.close() def find_subsequence(file_path: str, subsequence: bytes) -> int: with open(file_path, \\"r+b\\") as f: mm = mmap.mmap(f.fileno(), 0) index = mm.find(subsequence) mm.close() return index"},{"question":"Objective: To assess your understanding of Python slices and how to manipulate them, you will implement a function that takes various slice parameters and returns the resulting sliced list. Problem Statement: Write a Python function, `custom_slice`, that mimics slice behavior by using provided start, stop, and step indices. The function should take a list and the slice parameters as input and return a new list representing the sliced portion. Expected Input and Output: - Input: - `input_list` (list): A list of integers. - `start` (int or None): The starting index of the slice. If `None`, it defaults to the beginning of the list. - `stop` (int or None): The stopping index of the slice. If `None`, it defaults to the end of the list. - `step` (int or None): The step value for the slice. If `None`, it defaults to 1. - Output: - Returns a sliced list based on the provided parameters. Constraints: - You may not use Python\'s built-in slice operations or any slicing syntax (e.g., `[:]`). - The length of the `input_list` will not exceed 10^6. - Values for `start`, `stop`, and `step` will be within the bounds of typical Python list indices. Function Signature: ```python def custom_slice(input_list: list, start: int, stop: int, step: int) -> list: ``` Example: ```python # Example 1 input_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] start = 2 stop = 8 step = 2 print(custom_slice(input_list, start, stop, step)) # Output: [3, 5, 7] # Example 2 input_list = [10, 20, 30, 40, 50] start = None stop = 3 step = 1 print(custom_slice(input_list, start, stop, step)) # Output: [10, 20, 30] # Example 3 input_list = [10, 20, 30, 40, 50] start = 1 stop = None step = None print(custom_slice(input_list, start, stop, step)) # Output: [20, 30, 40, 50] ``` Notes: - Handle edge cases where `start`, `stop`, and/or `step` are `None`. - Ensure the function is efficient and can handle large lists within the given constraints. - You may assume that the function will receive valid inputs.","solution":"def custom_slice(input_list, start=None, stop=None, step=None): Mimics Python\'s slice behavior by using provided start, stop, and step indices. # Set default values for start, stop, and step if they are None if start is None: start = 0 if step is None: step = 1 if stop is None: stop = len(input_list) # Resulting list to store the sliced elements result = [] # Iterate through the list using the provided slice parameters i = start while (step > 0 and i < stop) or (step < 0 and i > stop): if 0 <= i < len(input_list): # Ensure the index is within valid range result.append(input_list[i]) i += step return result"},{"question":"**Advanced PyTorch/TorchScript Coding Challenge** **Objective:** The objective of this coding challenge is to assess your ability to convert a PyTorch model into TorchScript using both scripting and tracing, optimize it for inference, and perform a task with the converted model. **Problem Statement:** You are provided with a PyTorch model that performs a simple image classification task. The model has dynamic control flow based on the inputs, which means it utilizes both scripting and tracing for conversion to TorchScript. You must finish the following tasks: 1. Convert the model into TorchScript using both scripting and tracing as appropriate. 2. Optimize the converted TorchScript model for inference. 3. Create a function to make predictions on new data using the converted model. **Model Description:** The model consists of the following components: - A convolutional neural network (CNN) with two convolutional layers. - A simple control flow in the forward method where a decision is made based on the input tensor. **Tasks:** 1. Implement a class `SimpleCNN` in PyTorch. 2. Convert this model to TorchScript. 3. Optimize the TorchScript model for inference. 4. Implement the function `make_prediction` to use the optimized model. **Details:** 1. **SimpleCNN Class:** ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) if x.mean() > 0.5: x = F.relu(self.conv2(x)) return x ``` 2. **Convert the model to TorchScript:** - `trace` the part of the model that does not contain control flow. - Use `script` for methods containing control flow. 3. **Optimize the TorchScript model for inference:** - Use the `torch.jit.optimize_for_inference` method. 4. **Implement `make_prediction` function:** - This function should take a pre-processed tensor as input and provide the model\'s output. **Constraints:** - You must use both scripting and tracing in the solution. - Optimize the TorchScript model before prediction. - The input tensor to the `make_prediction` function will be a 4D tensor of shape `(1, 1, 28, 28)`. **Expected Solution:** ```python import torch import torch.nn.functional as F import torch.jit class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) if x.mean() > 0.5: x = F.relu(self.conv2(x)) return x # Instantiate the model model = SimpleCNN() # Trace the simple feed-forward part of the model traced_conv1 = torch.jit.trace(model.conv1, torch.rand(1, 1, 28, 28)) # Create a script for the complex part of the model @torch.jit.script def scripted_forward(x): x = F.relu(traced_conv1(x)) if x.mean() > 0.5: x = F.relu(model.conv2(x)) return x # Combine into a ScriptModule scripted_model = torch.jit.script(scripted_forward) # Optimize for inference optimized_model = torch.jit.optimize_for_inference(scripted_model) # Define the prediction function def make_prediction(tensor_input): return optimized_model(tensor_input) # Example usage example_input = torch.rand(1, 1, 28, 28) prediction = make_prediction(example_input) print(prediction) ``` **Notes:** Make sure to verify the model\'s output before and after conversion to ensure correctness.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) if x.mean() > 0.5: x = F.relu(self.conv2(x)) return x # Instantiate the model model = SimpleCNN() # Script the entire model, considering the control flow scripted_model = torch.jit.script(model) # Optimize the scripted model for inference optimized_model = torch.jit.optimize_for_inference(scripted_model) def make_prediction(tensor_input): Takes a pre-processed tensor input and uses the optimized TorchScript model to make a prediction. return optimized_model(tensor_input) # Example usage example_input = torch.rand(1, 1, 28, 28) prediction = make_prediction(example_input) print(prediction)"},{"question":"# Question: Web Scraper with URL Validation and Cookie Management Problem Statement You are tasked with creating a Python program that performs the following tasks: 1. **Fetch a Webpage**: - Create a function `fetch_webpage(url: str) -> str` that takes a URL as input and returns the content of the webpage as a string. Use the `urllib.request` module for making the HTTP request. 2. **Validate URL**: - Create a function `validate_url(url: str) -> bool` that checks if the given URL is valid and properly formatted. Use the `urllib.parse` module for this purpose. 3. **Manage Cookies**: - Create a function `fetch_with_cookies(url: str, cookies: dict) -> str` that takes a URL and a dictionary of cookies as input. The function should send these cookies with the HTTP request and return the content of the webpage as a string. Use `http.cookiejar` and `urllib.request.HTTPCookieProcessor` for managing cookies. Constraints - You must handle any HTTP errors or exceptions that may occur during the request and return an appropriate error message. - The URL validation function should ensure the scheme is either `http` or `https`. Input Format - For `fetch_webpage`: A single string representing the URL. - For `validate_url`: A single string representing the URL. - For `fetch_with_cookies`: A single string representing the URL and a dictionary where keys are cookie names and values are cookie values. Output Format - For `fetch_webpage`: A string representing the content of the webpage. - For `validate_url`: A boolean value indicating whether the URL is valid. - For `fetch_with_cookies`: A string representing the content of the webpage. Example ```python # Example usage: # Fetch a webpage content = fetch_webpage(\'https://www.example.com\') print(content) # Validate URL is_valid = validate_url(\'https://www.example.com\') print(is_valid) # Should return True is_valid = validate_url(\'ftp://www.example.com\') print(is_valid) # Should return False # Fetch with cookies cookies = {\'sessionid\': \'xyz123\'} content_with_cookies = fetch_with_cookies(\'https://www.example.com\', cookies) print(content_with_cookies) ``` Notes - Ensure your functions handle exceptions gracefully and provide meaningful error messages. - The provided example URLs and cookies are for illustration purposes; actual URLs and cookies should be tested during the implementation.","solution":"import urllib.request import urllib.parse import http.cookiejar def fetch_webpage(url: str) -> str: Fetches the content of the webpage at the given URL. try: response = urllib.request.urlopen(url) return response.read().decode(\'utf-8\') except Exception as e: return str(e) def validate_url(url: str) -> bool: Validates if the given URL is properly formatted and uses either http or https scheme. parsed_url = urllib.parse.urlparse(url) return parsed_url.scheme in {\'http\', \'https\'} def fetch_with_cookies(url: str, cookies: dict) -> str: Fetches the content of the webpage at the given URL and sends the provided cookies with the request. try: cookie_jar = http.cookiejar.CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) for name, value in cookies.items(): cookie = http.cookiejar.Cookie( version=0, name=name, value=value, port=None, port_specified=False, domain=urllib.parse.urlparse(url).netloc, domain_specified=True, domain_initial_dot=False, path=\'/\', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={\'HttpOnly\': None}, rfc2109=False) cookie_jar.set_cookie(cookie) response = opener.open(url) return response.read().decode(\'utf-8\') except Exception as e: return str(e)"},{"question":"You are provided with a file comparison method `compare_files_and_dirs` that uses the `filecmp` module to recursively compare two directories and outputs a detailed report of the differences. The method should also be able to handle and report read permission errors or missing files gracefully. # Task Implement the function `compare_files_and_dirs(dir1: str, dir2: str) -> None` which will: - Take two directory paths as input. - Use the `filecmp` module to compare these directories recursively. - Print a detailed report of differences in the following structure: - Files that differ between the two directories. - Files that exist only in `dir1`. - Files that exist only in `dir2`. - Any errors encountered during the comparison (`os.stat` errors, permission errors, etc.). # Example Output Suppose `dir1` and `dir2` are compared and contain the following differences: ```text Differing files: - file1.txt differ between dir1 and dir2 - subdir/file2.txt differ between dir1 and dir2 Files only in dir1: - onlyin1.txt Files only in dir2: - subdir/onlyin2.txt Errors while comparing: - error reading subdir/unreadable.txt ``` The function output should match the structure shown above precisely. # Constraints - Assume the directories `dir1` and `dir2` exist and are accessible. - You may not use any external libraries other than `filecmp` and `os`. # Performance Requirements The function should handle reasonably large directory trees efficiently by leveraging the caching provided by `filecmp` where suitable. # Function Signature ```python from filecmp import dircmp def compare_files_and_dirs(dir1: str, dir2: str) -> None: # Implementation here ``` Make sure the function prints the report directly.","solution":"import os from filecmp import dircmp def compare_files_and_dirs(dir1: str, dir2: str) -> None: Recursively compares two directories and prints a detailed report of differences. def report_differences(dcmp): # Report files that differ if dcmp.diff_files: print(\\"Differing files:\\") for name in dcmp.diff_files: print(f\\"- {dcmp.left}/{name} differ between {dir1} and {dir2}\\") # Report files that only exist in dir1 if dcmp.left_only: print(\\"Files only in dir1:\\") for name in dcmp.left_only: print(f\\"- {dcmp.left}/{name}\\") # Report files that only exist in dir2 if dcmp.right_only: print(\\"Files only in dir2:\\") for name in dcmp.right_only: print(f\\"- {dcmp.right}/{name}\\") # Report any common files with errors for sub_dcmp in dcmp.subdirs.values(): report_differences(sub_dcmp) try: dcmp = dircmp(dir1, dir2) report_differences(dcmp) except Exception as e: print(f\\"Errors while comparing:n- {e}\\")"},{"question":"To test students\' understanding of the **base64** module in Python, consider the following challenging problem: # Problem Statement: You are tasked with developing file-based and string-based encoding and decoding utilities using the Python **base64** module. You need to implement three functions: `encode_file_to_base64`, `decode_base64_to_file`, and `encode_and_decode_string`. `encode_file_to_base64(input_file_path: str, output_file_path: str) -> None` - **Input**: - `input_file_path` (str): The path to a binary file to be encoded. - `output_file_path` (str): The path to save the Base64 encoded result. - **Output**: None - **Functionality**: - Read the contents of the input file, encode it using Base64 encoding, and write the encoded data to the output file. `decode_base64_to_file(input_file_path: str, output_file_path: str) -> None` - **Input**: - `input_file_path` (str): The path to a file containing Base64 encoded data. - `output_file_path` (str): The path to save the decoded binary data. - **Output**: None - **Functionality**: - Read the Base64 encoded data from the input file, decode it, and write the binary data to the output file. `encode_and_decode_string(data: str, operation: str, url_safe: bool = False) -> str` - **Input**: - `data` (str): The string to encode or decode. - `operation` (str): The operation to be performed - either `\\"encode\\"` or `\\"decode\\"`. - `url_safe` (bool): Optional; If `True`, use URL-safe Base64 encoding/decoding. - **Output**: (str) The encoded or decoded result. - **Functionality**: - If `operation` is `\\"encode\\"`, encode the string using Base64 encoding (URL-safe if specified). - If `operation` is `\\"decode\\"`, decode the Base64 encoded string (URL-safe if specified) back to the original string. # Constraints: - Files provided as input will not exceed 10MB in size. - Strings provided as input will not exceed 10,000 characters. - Appropriate handling for invalid Base64 inputs resulting in exceptions should be managed with proper error messages. # Example: ```python # Example for file-based functions encode_file_to_base64(\\"input.bin\\", \\"encoded.txt\\") decode_base64_to_file(\\"encoded.txt\\", \\"output.bin\\") # Example for string-based function print(encode_and_decode_string(\\"Hello, World!\\", \\"encode\\")) # Outputs: \'SGVsbG8sIFdvcmxkIQ==\' print(encode_and_decode_string(\\"SGVsbG8sIFdvcmxkIQ==\\", \\"decode\\")) # Outputs: \'Hello, World!\' print(encode_and_decode_string(\\"Hello, World!\\", \\"encode\\", url_safe=True)) # Outputs: \'SGVsbG8sIFdvcmxkIQ__\' (or similar, but URL-safe output) ``` # Implementation: You need to implement these three functions adhering to the given specifications and handle all edge cases and potential exceptions appropriately.","solution":"import base64 def encode_file_to_base64(input_file_path: str, output_file_path: str) -> None: with open(input_file_path, \\"rb\\") as input_file: binary_data = input_file.read() base64_encoded_data = base64.b64encode(binary_data) with open(output_file_path, \\"wb\\") as output_file: output_file.write(base64_encoded_data) def decode_base64_to_file(input_file_path: str, output_file_path: str) -> None: with open(input_file_path, \\"rb\\") as input_file: base64_encoded_data = input_file.read() try: binary_data = base64.b64decode(base64_encoded_data) except (base64.binascii.Error, ValueError): raise ValueError(\\"Invalid Base64 input.\\") with open(output_file_path, \\"wb\\") as output_file: output_file.write(binary_data) def encode_and_decode_string(data: str, operation: str, url_safe: bool = False) -> str: if operation == \\"encode\\": if url_safe: encoded_data = base64.urlsafe_b64encode(data.encode()).decode() else: encoded_data = base64.b64encode(data.encode()).decode() return encoded_data elif operation == \\"decode\\": try: if url_safe: decoded_data = base64.urlsafe_b64decode(data.encode()).decode() else: decoded_data = base64.b64decode(data.encode()).decode() except (base64.binascii.Error, ValueError): raise ValueError(\\"Invalid Base64 input.\\") return decoded_data else: raise ValueError(\\"Invalid operation. Use \'encode\' or \'decode\'.\\")"},{"question":"# Quoted-Printable Data Handling Objective: You are tasked with developing a function to read a quoted-printable encoded text file, decode it, modify the content, and then encode it back to a quoted-printable encoded text file. Requirements: 1. **Read and decode a quoted-printable encoded text file**. The input file is provided as a path. 2. **Modify the decoded content**: - Locate all email addresses in the text and obfuscate them by replacing the domain name with `example.com`. For instance, `user@domain.com` should become `user@example.com`. 3. **Encode the modified content back** to quoted-printable encoding. 4. **Save the resulting content** to a new file specified by the user. Function Signature: ```python def process_qp_file(input_file_path: str, output_file_path: str) -> None: pass ``` Input: - `input_file_path` (str): Path to the input file which is quoted-printable encoded. - `output_file_path` (str): Path to the output file where the modified and re-encoded content will be saved. Output: - The function will return `None`, but the output file at `output_file_path` should contain the correctly re-encoded content. Constraints: - The input and output files are in binary format. # Example: Suppose the original file `encoded.txt` has the following quoted-printable encoded content: ``` VGhpcyBpcyBhbiBlbWFpbCBhZGRyZXNzOiB1c2VyQGRvbWFpbi5jb20= ``` The decoded content would be: ``` This is an email address: user@domain.com ``` After modifying, it becomes: ``` This is an email address: user@example.com ``` Encoded back to quoted-printable, the content might look like: ``` VGhpcyBpcyBhbiBlbWFpbCBhZGRyZXNzOiB1c2VyQGV4YW1wbGUuY29t ``` # Additional Information: - Utilize the `quopri.decodestring` and `quopri.encodestring` functions for handling byte strings. - Ensure proper handling of binary file objects for reading and writing. # Testing: You can create a sample file and verify the function by checking the content of the output file.","solution":"import quopri import re def process_qp_file(input_file_path: str, output_file_path: str) -> None: # Read and decode the input file with open(input_file_path, \'rb\') as infile: encoded_content = infile.read() decoded_content = quopri.decodestring(encoded_content).decode(\'utf-8\') # Modify the decoded content modified_content = re.sub(r\'([a-zA-Z0-9._%+-]+)@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\', r\'1@example.com\', decoded_content) # Re-encode the modified content re_encoded_content = quopri.encodestring(modified_content.encode(\'utf-8\')) # Write the re-encoded content to the output file with open(output_file_path, \'wb\') as outfile: outfile.write(re_encoded_content)"},{"question":"**Objective:** Implement a function that reads a marshalled Python object from a file and writes a modified version of that object back to another file. **Task:** 1. Write a function `modify_object_in_file(input_file_path: str, output_file_path: str, modification_func: Callable[[Any], Any], version: int = 2) -> None`. 2. The function should read a Python object from `input_file_path`, apply `modification_func` to the object, and then write the modified object to `output_file_path`. The `version` parameter indicates the marshalling version to be used. **Function Signature:** ```python from typing import Callable, Any def modify_object_in_file(input_file_path: str, output_file_path: str, modification_func: Callable[[Any], Any], version: int = 2) -> None: pass ``` **Input:** - `input_file_path` (str): The path to the input file containing the marshalled data. - `output_file_path` (str): The path to the output file where the modified marshalled data will be written. - `modification_func` (Callable[[Any], Any]): A function to modify the deserialized object. - `version` (int, optional): The marshalling version to use, default is 2. **Output:** - None. The function writes the modified object to the specified output file. **Constraints:** - The input file is guaranteed to contain a valid marshalled Python object. - The `modification_func` should handle any Python object types that it receives. - The function should appropriately handle any file I/O errors or marshalling errors. **Example:** Suppose `input_file_path` contains a marshalled list `[1, 2, 3]`, and `modification_func` is a function that appends `4` to a list. The function should write the marshalled list `[1, 2, 3, 4]` to `output_file_path`. ```python import marshal def append_four(obj): if isinstance(obj, list): obj.append(4) return obj modify_object_in_file(\'input_file.dat\', \'output_file.dat\', append_four) ``` **Note:** - You may use the `marshal` module for serialization and deserialization operations. - Proper error handling is expected when dealing with file operations. - Ensure your function is efficient and follows best practices for I/O operations.","solution":"import marshal from typing import Callable, Any def modify_object_in_file(input_file_path: str, output_file_path: str, modification_func: Callable[[Any], Any], version: int = 2) -> None: try: # Read the marshalled object from the input file with open(input_file_path, \'rb\') as input_file: obj = marshal.load(input_file) # Apply the modification function to the object modified_obj = modification_func(obj) # Write the modified object to the output file with open(output_file_path, \'wb\') as output_file: marshal.dump(modified_obj, output_file, version) except (OSError, ValueError, EOFError, TypeError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Python Reference Counting Mechanisms and Object Management As a developer, you are required to create a custom Python object manager that accurately handles reference counting for objects, ensuring memory is efficiently managed and preventing memory leaks. Implement a class `PyObjectManager` in Python that mimics the reference counting behavior described in the provided documentation. Your class should handle the incrementing and decrementing of reference counts appropriately and ensure safe deallocation of objects. # Requirements 1. Implement methods: - `inc_ref(self, obj)` - Increment the reference count of the object. - `dec_ref(self, obj)` - Decrement the reference count. If the count reaches zero, the object should be safely deallocated. - `get_ref_count(self, obj)` - Return the current reference count of the object. 2. Handle scenarios where the object may be `None`: - The increment operation should have no effect. - The decrement operation should have no effect but handle the object cleanup if needed. 3. Ensure that reference counting is correctly managed for all objects passed to the manager. 4. You are not required to implement the actual object deallocation logic in Python (as it normally gets handled by Python\'s garbage collector), but logically demonstrate when and how it should occur. 5. Use a dictionary, `self.ref_counts`, to track reference counts of objects. # Example ```python class PyObjectManager: def __init__(self): self.ref_counts = {} def inc_ref(self, obj): if obj is not None: if obj in self.ref_counts: self.ref_counts[obj] += 1 else: self.ref_counts[obj] = 1 def dec_ref(self, obj): if obj is not None and obj in self.ref_counts: self.ref_counts[obj] -= 1 if self.ref_counts[obj] == 0: del self.ref_counts[obj] # Here, deallocate the object if needed def get_ref_count(self, obj): if obj in self.ref_counts: return self.ref_counts[obj] return 0 # Usage Example manager = PyObjectManager() obj = \\"example_object\\" manager.inc_ref(obj) print(manager.get_ref_count(obj)) # Output: 1 manager.inc_ref(obj) print(manager.get_ref_count(obj)) # Output: 2 manager.dec_ref(obj) print(manager.get_ref_count(obj)) # Output: 1 manager.dec_ref(obj) print(manager.get_ref_count(obj)) # Output: 0 ``` # Constraints - Focus on correctness and proper handling of reference counts. - Do not use external libraries for managing references; use Pythonâ€™s standard dictionary. Your task is to ensure the class behaves according to the principles of reference counting as outlined.","solution":"class PyObjectManager: def __init__(self): self.ref_counts = {} def inc_ref(self, obj): if obj is not None: if obj in self.ref_counts: self.ref_counts[obj] += 1 else: self.ref_counts[obj] = 1 def dec_ref(self, obj): if obj is not None and obj in self.ref_counts: self.ref_counts[obj] -= 1 if self.ref_counts[obj] == 0: self._deallocate(obj) def get_ref_count(self, obj): if obj in self.ref_counts: return self.ref_counts[obj] return 0 def _deallocate(self, obj): del self.ref_counts[obj] # Here, you might handle actual deallocation logic if needed. # Example Usage manager = PyObjectManager() obj = \\"example_object\\" manager.inc_ref(obj) print(manager.get_ref_count(obj)) # Output: 1 manager.inc_ref(obj) print(manager.get_ref_count(obj)) # Output: 2 manager.dec_ref(obj) print(manager.get_ref_count(obj)) # Output: 1 manager.dec_ref(obj) print(manager.get_ref_count(obj)) # Output: 0"},{"question":"# Question: Implement a Python class that mimics the behavior of the `PyListObject` using the provided C functions from the documentation. Description: You are required to implement a Python class called `PyList` that mimics the behavior of Python\'s built-in list object using the provided C API functions described in the documentation. Your class should expose methods for creating a list, accessing and modifying elements, slicing, sorting, reversing, and converting to a tuple. Requirements: 1. **Initialization & Type Checking**: - Implement an `__init__` method to create a new list. - Implement a `check` method to verify if an object is a list. 2. **List Operations**: - Implement methods to get the size of the list (`size`). - Implement methods to get an item by index (`get_item`). - Implement methods to set an item by index (`set_item`). - Implement methods to insert an item at a specific index (`insert`). - Implement methods to append an item to the end of the list (`append`). - Implement methods to get a slice of the list (`get_slice`). - Implement methods to set a slice of the list (`set_slice`). - Implement methods to sort the list (`sort`). - Implement methods to reverse the list (`reverse`). - Implement methods to convert the list to a tuple (`as_tuple`). Example: ```python class PyList: def __init__(self, size): # Create a new list of given size using PyList_New def check(self, obj): # Check if obj is a list using PyList_Check def size(self): # Return size of the list using PyList_Size def get_item(self, index): # Get item by index using PyList_GetItem def set_item(self, index, item): # Set item by index using PyList_SetItem def insert(self, index, item): # Insert item at index using PyList_Insert def append(self, item): # Append item to list using PyList_Append def get_slice(self, start, end): # Get slice from start to end using PyList_GetSlice def set_slice(self, start, end, itemlist): # Set slice from start to end using PyList_SetSlice def sort(self): # Sort the list in place using PyList_Sort def reverse(self): # Reverse the list in place using PyList_Reverse def as_tuple(self): # Convert the list to tuple using PyList_AsTuple ``` Constraints: - Your implementation should handle exceptions as described in the documentation when an operation fails. - Ensure proper memory management as indicated, e.g., handling borrowed references and avoiding memory leaks. - Follow the performance and function behaviors described in the documentation closely. Testing: - Test the implementation by creating instances of `PyList`, performing various operations, and ensuring correctness by comparing with Python\'s built-in list behavior. **Note**: Assume that the underlying C API functions (like `PyList_New`, `PyList_Size`, etc.) are available and correctly exposed to your Python environment for calling.","solution":"class PyList: def __init__(self, size=0): self._list = [None] * size def check(self, obj): return isinstance(obj, PyList) def size(self): return len(self._list) def get_item(self, index): try: return self._list[index] except IndexError: raise IndexError(\\"Index out of range\\") def set_item(self, index, item): try: self._list[index] = item except IndexError: raise IndexError(\\"Index out of range\\") def insert(self, index, item): self._list.insert(index, item) def append(self, item): self._list.append(item) def get_slice(self, start, end): return self._list[start:end] def set_slice(self, start, end, itemlist): self._list[start:end] = itemlist def sort(self): self._list.sort() def reverse(self): self._list.reverse() def as_tuple(self): return tuple(self._list)"},{"question":"Objective Demonstrate your ability in using the `time` module for various time-related functionalities, including fetching, formatting, parsing time, and handling performance counters. Problem Statement You are required to write a Python function `performance_report` that tracks the execution time of an operation and formats the results according to the local timezone. Function Signature ```python def performance_report(operation, start_time: str, time_format: str) -> dict: pass ``` Input Parameters - `operation`: A callable (a function without parameters) that executes the task you wish to measure. - `start_time`: A string representing the starting timestamp of the form `\'YYYY-MM-DD HH:MM:SS\'`. - `time_format`: A string containing a valid format for strftime/strptime directives. Output A dictionary with the following keys: - `\'operation_start\'`: Start time formatted as per `time_format`. - `\'operation_end\'`: End time formatted as per `time_format`. - `\'elapsed_time_sec\'`: Total elapsed time in seconds the operation took to execute. Constraints - Timezone for the `start_time` is assumed to be the local time. - You should handle both the conversion from string to time struct and from time struct to string. - Performance measurement should be done in a high-resolution manner using `time.perf_counter()` or `time.perf_counter_ns()`. - Be sure to use the `time` module\'s functionalities effectively. Example Usage ```python import time def some_operation(): # Simulate a time-consuming operation time.sleep(2) result = performance_report(some_operation, \\"2023-04-15 10:00:00\\", \\"%Y-%m-%d %H:%M:%S %Z\\") print(result) # Output might look like: # {\'operation_start\': \'2023-04-15 10:00:00 UTC\', \'operation_end\': \'2023-04-15 10:02:00 UTC\', \'elapsed_time_sec\': 2.0} ``` Notes - Ensure the function handles the time appropriately and reports the times in the provided `time_format`. - The function should account for different system precisions and should aim for the highest possible accuracy in measuring execution time.","solution":"import time def performance_report(operation, start_time: str, time_format: str) -> dict: # Convert start_time string to struct_time start_struct_time = time.strptime(start_time, \\"%Y-%m-%d %H:%M:%S\\") # Format start_struct_time to given time_format formatted_start_time = time.strftime(time_format, start_struct_time) # Start performance counter start_perf_counter = time.perf_counter() # Execute the operation operation() # Stop performance counter end_perf_counter = time.perf_counter() # Calculate elapsed time in seconds elapsed_time_sec = end_perf_counter - start_perf_counter # Get current local time for end time end_time_struct = time.localtime() # Format end time to given time_format formatted_end_time = time.strftime(time_format, end_time_struct) # Return the results as a dictionary return { \'operation_start\': formatted_start_time, \'operation_end\': formatted_end_time, \'elapsed_time_sec\': elapsed_time_sec }"},{"question":"Objective: Create a Python script that will perform the following tasks: 1. **Create a New Directory**: If the directory `task_dir` does not already exist in the current working directory, create it. 2. **Create a Log File**: Within `task_dir`, create a file named `log.txt`. 3. **Write Timestamps**: Every second, for a total of 10 seconds, write the current timestamp to the `log.txt` file. Ensure to have each timestamp on a new line. 4. **Summarize File Information**: After writing the timestamps, output the following information about `log.txt`: - File size in bytes. - Number of lines written. - Last modification time. 5. **Error Handling and Logging**: If any errors occur during execution, log them to a separate file named `error_log.txt` within `task_dir`. Constraints: - You should use the built-in `os`, `io`, and `time` modules only. - Assume the script will be run in an environment where it has write permissions. - You must handle any potential exceptions gracefully. Input: - No user input; the script should run autonomously. Output: - Print the summary information to the console. - `log.txt` should contain 10 timestamps. - If any errors occur, they should be logged in `error_log.txt`. Example Output: ``` File Size: 128 bytes Number of Lines: 10 Last Modified: 2023-09-01 12:01:10 ``` Function Signatures: ```python def create_directory(path: str) -> None: # Function to create a directory if it doesn\'t exist def create_log_file(directory_path: str) -> str: # Function to create and return the path of \'log.txt\' within the directory def write_timestamps(file_path: str, duration: int) -> None: # Function to write timestamps every second for a given duration def summarize_file(file_path: str) -> None: # Function to summarize and print file information def log_error(directory_path: str, error_message: str) -> None: # Function to log errors to \'error_log.txt\' within the directory ```","solution":"import os import time def create_directory(path: str) -> None: Create a directory if it doesn\'t exist. Parameters: - path (str): The path of the directory to create. if not os.path.exists(path): os.makedirs(path) def create_log_file(directory_path: str) -> str: Create and return the path of \'log.txt\' within the directory. Parameters: - directory_path (str): The path of the directory where the log file will be created. Returns: - str: The path to the created \'log.txt\' file. log_file_path = os.path.join(directory_path, \\"log.txt\\") return log_file_path def write_timestamps(file_path: str, duration: int) -> None: Write timestamps every second for a given duration. Parameters: - file_path (str): The path to the log file. - duration (int): The duration in seconds to write timestamps. with open(file_path, \\"w\\") as file: for _ in range(duration): file.write(time.strftime(\\"%Y-%m-%d %H:%M:%S\\") + \\"n\\") time.sleep(1) def summarize_file(file_path: str) -> None: Summarize and print file information. Parameters: - file_path (str): The path to the file to summarize. file_size = os.path.getsize(file_path) with open(file_path, \'r\') as file: lines = file.readlines() num_lines = len(lines) last_modified_time = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.localtime(os.path.getmtime(file_path))) print(f\\"File Size: {file_size} bytes\\") print(f\\"Number of Lines: {num_lines}\\") print(f\\"Last Modified: {last_modified_time}\\") def log_error(directory_path: str, error_message: str) -> None: Log errors to \'error_log.txt\' within the directory. Parameters: - directory_path (str): The path of the directory where the error log will be created. - error_message (str): The error message to log. error_log_path = os.path.join(directory_path, \'error_log.txt\') with open(error_log_path, \'a\') as error_log: timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\") error_log.write(f\\"{timestamp}: {error_message}n\\")"},{"question":"In this assessment, you will demonstrate your understanding of the `asyncio.subprocess` module in Python 3.10 by creating an asynchronous function that runs multiple shell commands concurrently, captures their outputs, handles errors, and returns the results. Task Write a function `run_commands_concurrently` that takes a list of shell commands as input and returns a dictionary mapping each command to its result. The function should run all the commands concurrently using `asyncio.create_subprocess_shell`, capture their standard outputs and errors, and return a dictionary in the following format: ```python { \\"<command>\\": { \\"returncode\\": <returncode>, \\"stdout\\": \\"<captured_stdout>\\", \\"stderr\\": \\"<captured_stderr>\\" }, ... } ``` Where: - `<command>`: The shell command that was executed. - `<returncode>`: The return code of the command. - `<captured_stdout>`: The captured standard output of the command. - `<captured_stderr>`: The captured standard error of the command. Constraints 1. The function should handle at least three shell commands concurrently. 2. Ensure proper error handling and avoid deadlocks by using the `communicate()` method. 3. The function should be asynchronous and use `async/await`. Performance Requirements - The function should run all commands concurrently for optimal performance and reduced execution time. Example ```python import asyncio async def run_commands_concurrently(commands): results = {} async def run_command(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() results[cmd] = { \\"returncode\\": proc.returncode, \\"stdout\\": stdout.decode(), \\"stderr\\": stderr.decode() } await asyncio.gather(*[run_command(cmd) for cmd in commands]) return results # Example usage commands = [ \\"echo Hello, World!\\", \\"ls /nonexistent_directory\\", \\"uname -a\\" ] result = asyncio.run(run_commands_concurrently(commands)) print(result) ``` In the example above, the function `run_commands_concurrently` executes three shell commands concurrently and captures their outputs. The resulting dictionary will contain keys for each command with their corresponding return code, standard output, and standard error. Your solution will be evaluated based on correctness, concurrency handling, error management, and adherence to the requirements.","solution":"import asyncio async def run_commands_concurrently(commands): results = {} async def run_command(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() results[cmd] = { \\"returncode\\": proc.returncode, \\"stdout\\": stdout.decode().strip(), \\"stderr\\": stderr.decode().strip() } await asyncio.gather(*[run_command(cmd) for cmd in commands]) return results"},{"question":"**Objective:** Assess the student\'s understanding of the \\"mimetypes\\" module and their ability to extend its functionalities. **Task:** Create a function `custom_mime_type_handler(file_list, custom_types)` that will: 1. Initialize the `mimetypes` module. 2. Load additional MIME types from a given list of files. 3. Add any custom MIME types provided in a dictionary. 4. For each file in `file_list`, guess its MIME type and print the filename along with its guessed MIME type. 5. Output all possible extensions for two MIME types: `application/json` and `text/html`. **Function Signature:** ```python def custom_mime_type_handler(file_list: list, custom_types: dict) -> dict: pass ``` **Parameters:** - `file_list`: A list of filenames (strings) to process. - `custom_types`: A dictionary where keys are MIME types and values are lists of extensions associated with those MIME types. **Output:** - Print the filename and its guessed MIME type for each file in `file_list`. - Return a dictionary with two keys: `\\"application/json\\"` and `\\"text/html\\"`, each containing a list of all possible extensions for the respective MIME types. **Constraints:** - Use the `mimetypes` module for initialization and guessing MIME types. - Consider both standard and non-standard MIME types. **Example:** ```python def custom_mime_type_handler(file_list, custom_types): import mimetypes # Initialize the mimetypes module mimetypes.init() # Read additional MIME type files (if any) for filename in file_list: mimetypes.read_mime_types(filename) # Add custom MIME types for mime_type, extensions in custom_types.items(): for extension in extensions: mimetypes.add_type(mime_type, extension) # Guess MIME type for each file and print results = {} for file in file_list: guessed_type, _ = mimetypes.guess_type(file) print(f\\"{file}: {guessed_type}\\") # Get all possible extensions for JSON and HTML MIME types results[\'application/json\'] = mimetypes.guess_all_extensions(\'application/json\') results[\'text/html\'] = mimetypes.guess_all_extensions(\'text/html\') return results # Example Usage files_to_check = [\\"example.json\\", \\"example.html\\", \\"example.txt\\"] custom_types_to_add = {\\"application/x-custom\\": [\\".cust1\\", \\".cust2\\"], \\"text/x-anothercustom\\": [\\".acust\\"]} custom_mime_type_handler(files_to_check, custom_types_to_add) ``` In the example, the function initializes the `mimetypes` module, optionally reads additional MIME type files, adds custom MIME types, guesses and prints the MIME type for each file provided, and outputs possible extensions for `application/json` and `text/html`.","solution":"def custom_mime_type_handler(file_list, custom_types): import mimetypes # Initialize the mimetypes module mimetypes.init() # Add custom MIME types for mime_type, extensions in custom_types.items(): for extension in extensions: mimetypes.add_type(mime_type, extension) # Guess MIME type for each file and print results = {} guessed_types = {} for file in file_list: guessed_type, _ = mimetypes.guess_type(file) guessed_types[file] = guessed_type print(f\\"{file}: {guessed_type}\\") # Get all possible extensions for JSON and HTML MIME types results[\'application/json\'] = mimetypes.guess_all_extensions(\'application/json\') results[\'text/html\'] = mimetypes.guess_all_extensions(\'text/html\') return {\\"guessed_types\\": guessed_types, \\"extensions\\": results} # Example Usage files_to_check = [\\"example.json\\", \\"example.html\\", \\"example.txt\\"] custom_types_to_add = {\\"application/x-custom\\": [\\".cust1\\", \\".cust2\\"], \\"text/x-anothercustom\\": [\\".acust\\"]} custom_mime_type_handler(files_to_check, custom_types_to_add)"},{"question":"**Objective**: Demonstrate proficient use of seaborn\'s `clustermap` function for data visualization, performing advanced customizations and transformations. **Problem Statement**: You are provided with a dataset of student exam scores in different subjects. Your task is to visualize this data effectively using seaborn\'s `clustermap`. Specifically, you need to implement the following: 1. **Load Dataset**: Load the dataset from a CSV file named `student_scores.csv`. The dataset contains the following columns: `StudentID`, `Math`, `Science`, `English`, `History`, `Art`. 2. **Data Preparation**: - Remove the `StudentID` column from the dataset as it is not needed for clustering. - Normalize the data within rows to ensure comparability between subjects. 3. **Clustermap Visualization**: - Create a `clustermap` of the dataset with the following specifications: - Use the colormap `magma`. - Display the heatmap without row clustering. - Set the size of the resulting figure to 10x6 inches. - Add a color bar to the right side of the plot (`dendrogram_ratio=(.1, .2), cbar_pos=(0.82, .2, .03, .4)`). - Customize any other aspect you deem necessary to improve the visualization\'s readability. **Input**: - The `student_scores.csv` file is provided and formatted as described. **Output**: - Display the resulting seaborn `clustermap`. **Function Signature**: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_student_scores(file_path: str) -> None: # Your implementation here pass ``` **Constraints**: - You are only allowed to use seaborn, pandas, and matplotlib for this task. - Ensure that the code is efficient and handles the dataset of up to 1000 students. **Example**: ```csv StudentID,Math,Science,English,History,Art 1,88,92,95,85,90 2,78,85,88,82,87 3,85,89,94,90,86 ... ``` In your function `visualize_student_scores`, follow the following steps: 1. Load the dataset. 2. Preprocess the data (i.e., normalize). 3. Generate and display the clustermap with the specified customizations. **Notes**: - Pay attention to the customization details to make sure your visualization is clear and easy to interpret. - Ensure that the normalization and other data transformations are correctly implemented.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_student_scores(file_path: str) -> None: # Load the dataset df = pd.read_csv(file_path) # Remove the StudentID column df = df.drop(columns=[\\"StudentID\\"]) # Normalize the data within rows df_normalized = df.div(df.sum(axis=1), axis=0) # Create a clustermap with specified customizations g = sns.clustermap( df_normalized, cmap=\\"magma\\", row_cluster=False, figsize=(10, 6), dendrogram_ratio=(.1, .2), cbar_pos=(0.82, .2, .03, .4) ) # Display the clustermap plt.show()"},{"question":"Objective Create a custom Python class `DynamicObject` that allows dynamic attribute management and custom string representations. This will test your understanding of working with attributes dynamically, representation methods, and the use of built-in functions. Requirements 1. **Dynamic Attribute Management**: - Implement methods to add, retrieve, and delete attributes dynamically. - Use of generic getter and setter methods that manage attributes by interacting with an internal dictionary. 2. **Custom String Representations**: - Provide custom implementations for `__repr__()`, `__str__()`, and `__format__()` methods. 3. **Type and Instance Checking**: - Implement methods to check if the object is an instance of a given class and to check if it is a subclass of a given class. 4. **Iteration Capability**: - Implement iteration capability, allowing the object to be used in loops. Instructions Implement the `DynamicObject` class following these specifications: 1. **Attributes Management**: - `def set_attribute(self, name: str, value: any) -> None`: Sets an attribute with the given name and value. - `def get_attribute(self, name: str) -> any`: Gets the value of the attribute with the given name. Raises `AttributeError` if the attribute does not exist. - `def delete_attribute(self, name: str) -> None`: Deletes the attribute with the given name. Raises `AttributeError` if the attribute does not exist. 2. **Custom String Representations**: - `def __repr__(self) -> str`: Returns a string that represents the object. Should include class name and its dictionary of attributes. - `def __str__(self) -> str`: Returns a human-readable string representation of the object. - `def __format__(self, format_spec: str) -> str`: Formats the object based on the given format specification. 3. **Type and Instance Checking**: - `def is_instance(self, cls: type) -> bool`: Returns `True` if the current instance is an instance of the given class (cls), `False` otherwise. - `def is_subclass(self, cls: type) -> bool`: Returns `True` if the class of the current instance is a subclass of the given class (cls), `False` otherwise. 4. **Iteration Capability**: - `def __iter__(self)`: Initializes iteration over the object\'s attributes. - `def __next__(self)`: Returns the next attribute in the iteration. Example Usage ```python if __name__ == \\"__main__\\": obj = DynamicObject() obj.set_attribute(\\"name\\", \\"Python\\") obj.set_attribute(\\"version\\", 310) print(obj.get_attribute(\\"name\\")) # Output: Python print(obj) # Output could be: DynamicObject({\'name\': \'Python\', \'version\': 310}) print(f\\"{obj:name}\\") # Custom format if implemented print(obj.is_instance(DynamicObject)) # Output: True print(obj.is_subclass(object)) # Output: True for attr in obj: print(attr) # Output: (\'name\', \'Python\'), (\'version\', 310) ``` Constraints - Raise appropriate exceptions for error conditions. - Ensure that the iteration provides attributes in the order they were set. - None of the class methods should print anything directly; printing in the example usage is for demonstration purposes. Submission Submit your implementation of the `DynamicObject` class. Ensure that it covers all requirements and that the example usage runs correctly.","solution":"class DynamicObject: def __init__(self): self._attributes = {} self._iter_index = 0 # Attribute Management def set_attribute(self, name: str, value: any) -> None: Sets an attribute with the given name and value. self._attributes[name] = value def get_attribute(self, name: str) -> any: Gets the value of the attribute with the given name. if name in self._attributes: return self._attributes[name] raise AttributeError(f\\"{name} attribute does not exist\\") def delete_attribute(self, name: str) -> None: Deletes the attribute with the given name. if name in self._attributes: del self._attributes[name] else: raise AttributeError(f\\"{name} attribute does not exist\\") # Custom String Representations def __repr__(self) -> str: return f\\"DynamicObject({self._attributes})\\" def __str__(self) -> str: attributes_str = \', \'.join(f\\"{k}={v}\\" for k, v in self._attributes.items()) return f\\"DynamicObject with attributes: {attributes_str}\\" def __format__(self, format_spec: str) -> str: if format_spec == \\"\\": return str(self) else: return f\\"{format_spec}: {self._attributes}\\" # Type and Instance Checking def is_instance(self, cls: type) -> bool: return isinstance(self, cls) def is_subclass(self, cls: type) -> bool: return issubclass(self.__class__, cls) # Iteration Capability def __iter__(self): self._iter_index = 0 self._keys = list(self._attributes.keys()) return self def __next__(self): if self._iter_index < len(self._keys): key = self._keys[self._iter_index] value = self._attributes[key] self._iter_index += 1 return (key, value) else: raise StopIteration"},{"question":"Objective: Demonstrate your understanding of scikit-learn\'s dataset generation functions and clustering algorithms by creating a synthetic dataset and applying a clustering algorithm to it. Additionally, visualize the results. Problem Statement: 1. **Synthetic Dataset Generation**: - Use the `make_blobs` function from scikit-learn to generate a synthetic dataset with the following characteristics: - Total number of samples: 300 - Number of centers (clusters): 4 - Cluster standard deviation: 0.60 - Random state for reproducibility: 42 2. **Clustering**: - Apply the KMeans clustering algorithm from scikit-learn to the generated dataset. Use the following parameters: - Number of clusters: 4 - Random state: 42 3. **Visualization**: - Plot the generated dataset and the clusters identified by the KMeans algorithm. Use different colors for each cluster. - Include the cluster centers in the plot. Input: - No direct input. The students need to generate the dataset using the parameters provided. Output: - A scatter plot displaying the synthetic dataset points colored by the assigned cluster. - Cluster centers should be marked distinctly (e.g., using a star marker). Constraints: - Utilize `matplotlib` for plotting. - Ensure reproducibility by setting the random state as specified. - Handle all necessary imports and library usages within the function. Function Signature: ```python def generate_and_cluster(): # Your code here ``` Example: The function when called, should create a plot similar to the one below: ![Generated Blobs and KMeans Clusters](https://your_image_link_here) *(Include a representative image of what the plot should look like)* Notes: - You may split the problem into smaller steps, but the final implementation should be callable via `generate_and_cluster()`. - Ensure proper labeling of the plot for clarity.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.cluster import KMeans def generate_and_cluster(): # Generate synthetic dataset X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42) # Apply KMeans clustering kmeans = KMeans(n_clusters=4, random_state=42) y_kmeans = kmeans.fit_predict(X) # Plot the data points and clusters plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap=\'viridis\') # Plot the cluster centers centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c=\'red\', s=200, alpha=0.75, marker=\'*\') # Add labels and title plt.title(\'KMeans Clustering of Synthetic Dataset\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') # Show the plot plt.show()"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},D={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],M={key:0},N={key:1};function L(s,e,l,m,n,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," âœ• ")):d("",!0)]),t("div",D,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",q,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),i("span",N,"Loading...")):(a(),i("span",M,"See more"))],8,F)):d("",!0)])}const O=p(z,[["render",L],["__scopeId","data-v-932cf574"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/58.md","filePath":"chatai/58.md"}'),j={name:"chatai/58.md"},X=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(O)]))}});export{Y as __pageData,X as default};
