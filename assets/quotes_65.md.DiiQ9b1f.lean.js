import{_ as p,o as a,c as i,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},E={class:"review"},C={class:"review-title"},S={class:"review-content"};function P(n,e,l,m,s,o){return a(),i("div",T,[t("div",E,[t("div",C,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const z=p(k,[["render",P],["__scopeId","data-v-c908ca2c"]]),I=JSON.parse('[{"question":"You are provided with a dataset containing information on the health expenditure of different countries over multiple years. Your task is to load, clean, and visualize this dataset using the seaborn library. 1. **Data Cleaning**: - Load the dataset `healthexp` using `seaborn.load_dataset()` method. - Pivot the table so that each row corresponds to a year and each column corresponds to a country with their respective health expenditure in USD. - Interpolate any missing values in the dataset. - Stack the data back into a long format and rename the expenditure column to `Spending_USD`. - Reset the index and sort the dataframe by country name. 2. **Data Visualization**: - Create a facet plot using `seaborn.objects` to visualize the health expenditure trends over the years for each country. Ensure that the plots are separated by country, and wrap the facets in a grid of 3 columns. - Customize the appearance of the area plot: - The color of the area plot should correspond to the country. - The edge of the area plot should be highlighted. - In addition, create another plot to show the cumulative health expenditure over the years for each country using a stacked area chart. # Implementation ```python import seaborn.objects as so from seaborn import load_dataset # Load and clean the dataset def load_and_clean_data(): healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) return healthexp # Function to create the facet plot def create_facet_plot(data): p = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgecolor=\\".5\\", edgewidth=2) return p # Function to create the stacked area plot def create_stacked_area_plot(data): p = ( so.Plot(data, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()) ) return p # Main function to execute the above processes def main(): data = load_and_clean_data() facet_plot = create_facet_plot(data) stacked_area_plot = create_stacked_area_plot(data) # You may want to display or save these plots # facet_plot.show() # stacked_area_plot.show() ``` # Constraints - Ensure the plot is well-labeled and visually distinct. - Use appropriate color mappings to distinguish between different countries. - The code should be efficient and handle large datasets gracefully. Output: - The final output should be two plots, one for the facet plot and one for the stacked area plot showing cumulative health expenditure.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and clean the dataset def load_and_clean_data(): healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) return healthexp # Function to create the facet plot def create_facet_plot(data): p = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgecolor=\\".5\\", edgewidth=2) return p # Function to create the stacked area plot def create_stacked_area_plot(data): p = ( so.Plot(data, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()) ) return p # Main function to execute the above processes def main(): data = load_and_clean_data() facet_plot = create_facet_plot(data) stacked_area_plot = create_stacked_area_plot(data) return facet_plot, stacked_area_plot"},{"question":"Objective: Implement a set of Python functions to manage and utilize custom codec encoders and decoders, including handling errors using provided error handling callbacks. Tasks: 1. **Register and Unregister Codecs:** - Write a function `register_codec(search_function: callable) -> bool` to register a new codec search function. - Write a function `unregister_codec(search_function: callable) -> bool` to unregister an existing codec search function. 2. **Encode and Decode Strings:** - Write a function `encode_string(data: str, encoding: str, errors: str = \'strict\') -> bytes` to encode a string using the specified encoding. - Write a function `decode_string(data: bytes, encoding: str, errors: str = \'strict\') -> str` to decode a string using the specified encoding. 3. **Error Handling:** - Write a function `register_error_handler(name: str, error_handler: callable) -> bool` to register a custom error handler. - Write a function `get_error_handler(name: str) -> callable` to retrieve a previously registered error handler. Input and Output Formats: - The `register_codec` and `unregister_codec` functions take a codec search function as input and return a boolean indicating success. - The `encode_string` function takes a string, encoding name, and (optionally) error handling method, returning the encoded bytes. - The `decode_string` function takes encoded bytes, encoding name, and (optionally) error handling method, returning the decoded string. - The `register_error_handler` function takes a name and error handler callable, returning a boolean indicating success. - The `get_error_handler` function takes a name and returns the registered error handler callable. Constraints: - Assume the search function and error handler callables are predefined and passed correctly. - Default error handling methods are \'strict\', \'ignore\', \'replace\', \'xmlcharrefreplace\', \'backslashreplace\', and \'namereplace\'. Example: ```python def search_function(encoding): if encoding == \'custom\': return (\'custom\', encode_function, decode_function, streamreader_function, streamwriter_function) return None def error_handler(exception): return (b\'?\', exception.start + 1) # Registering the codec assert register_codec(search_function) == True # Registering the error handler assert register_error_handler(\'custom_replace\', error_handler) == True # Encoding and decoding with the custom codec encoded = encode_string(\'example\', \'custom\', \'custom_replace\') decoded = decode_string(encoded, \'custom\', \'custom_replace\') print(decoded) # Should print \'example\' ``` Implement the functions as per the specifications and the example provided. Ensure to handle potential errors and edge cases smoothly.","solution":"import codecs from typing import Callable, Dict # A dictionary to store custom error handlers error_handlers: Dict[str, Callable] = {} def register_codec(search_function: Callable) -> bool: try: codecs.register(search_function) return True except Exception as e: return False def unregister_codec(search_function: Callable) -> bool: try: codecs.unregister(search_function) return True except Exception as e: return False def encode_string(data: str, encoding: str, errors: str = \'strict\') -> bytes: try: return data.encode(encoding, errors) except LookupError: # If encoding is not available raise ValueError(f\\"Encoding not found: {encoding}\\") def decode_string(data: bytes, encoding: str, errors: str = \'strict\') -> str: try: return data.decode(encoding, errors) except LookupError: # If encoding is not available raise ValueError(f\\"Encoding not found: {encoding}\\") def register_error_handler(name: str, error_handler: Callable) -> bool: if not callable(error_handler): return False error_handlers[name] = error_handler codecs.register_error(name, error_handler) return True def get_error_handler(name: str) -> Callable: return error_handlers.get(name, None)"},{"question":"# Advanced Bytes Manipulation in Python You are working on a Python project that involves advanced handling of bytes objects. To test your understanding of these concepts, you are tasked to implement a set of functions that will create, manipulate, and inspect bytes objects. Task 1. **create_bytes_from_string**: Implement a function that takes a string as input and returns a new bytes object containing the same data. Use the function `PyBytes_FromString`. ```python def create_bytes_from_string(s: str) -> bytes: Create a bytes object from a string. Args: s (str): The input string. Returns: bytes: The created bytes object. pass ``` 2. **concat_bytes_objects**: Implement a function that takes two bytes objects and concatenates them. Use the function `PyBytes_Concat`. ```python def concat_bytes_objects(b1: bytes, b2: bytes) -> bytes: Concatenate two bytes objects. Args: b1 (bytes): The first bytes object. b2 (bytes): The second bytes object. Returns: bytes: The concatenated bytes object. pass ``` 3. **resize_bytes_object**: Implement a function that takes a bytes object and a new size, and resizes the bytes object. Use the function `_PyBytes_Resize`. ```python def resize_bytes_object(b: bytes, new_size: int) -> bytes: Resize a bytes object to the specified new size. Args: b (bytes): The bytes object to resize. new_size (int): The new size of the bytes object. Returns: bytes: The resized bytes object, or raises an error if resizing fails. pass ``` 4. **bytes_object_info**: Implement a function that takes a bytes object and returns its size and content as a tuple. Use `PyBytes_Size` and `PyBytes_AsString`. ```python def bytes_object_info(b: bytes) -> tuple: Return the size and content of a bytes object. Args: b (bytes): The bytes object. Returns: tuple: A tuple containing the size of the bytes object and its content. pass ``` Constraints - For \'create_bytes_from_string\', assume the input string is always non-null. - For \'resize_bytes_object,\' the new size should be less than or equal to the length of the bytes object. - For \'concat_bytes_objects,\' both input parameters are guaranteed to be valid bytes objects (i.e., no type checking is required). Notes - You must raise appropriate exceptions when an error occurs as per the documentation. - You are not allowed to use any built-in Python functions or libraries that directly manipulate bytes objects; you must use the provided C functions. Example Usage ```python # Example usage after implementing the functions b1 = create_bytes_from_string(\\"Hello\\") b2 = create_bytes_from_string(\\"World\\") b_concatenated = concat_bytes_objects(b1, b2) b_resized = resize_bytes_object(b_concatenated, 8) size, content = bytes_object_info(b_resized) print(f\\"Bytes Object: {content}, Size: {size}\\") # Expected Output: Bytes Object: b\'HelloWor\', Size: 8 ```","solution":"def create_bytes_from_string(s: str) -> bytes: Create a bytes object from a string. Args: s (str): The input string. Returns: bytes: The created bytes object. return bytes(s, \'utf-8\') def concat_bytes_objects(b1: bytes, b2: bytes) -> bytes: Concatenate two bytes objects. Args: b1 (bytes): The first bytes object. b2 (bytes): The second bytes object. Returns: bytes: The concatenated bytes object. return b1 + b2 def resize_bytes_object(b: bytes, new_size: int) -> bytes: Resize a bytes object to the specified new size. Args: b (bytes): The bytes object to resize. new_size (int): The new size of the bytes object. Returns: bytes: The resized bytes object, or raises an error if resizing fails. if new_size > len(b): raise ValueError(\\"New size must be less than or equal to the length of the bytes object\\") return b[:new_size] def bytes_object_info(b: bytes) -> tuple: Return the size and content of a bytes object. Args: b (bytes): The bytes object. Returns: tuple: A tuple containing the size of the bytes object and its content. return (len(b), b)"},{"question":"You are tasked with creating a utility function in Python that performs specific operations on a sequence of data. The function `process_data` should meet the following requirements: # Function Specifications - **Function name**: `process_data` - **Parameters**: - `data_list` (List[int]): A list of integers provided as the input data. - **Returns**: - A list of integers where each integer has been processed according to the logic below. # Requirements 1. Use a `with` statement to handle file operations. 2. Use a `try` statement to handle potential exceptions that might occur during data processing. 3. Use a `for` loop to iterate over the input list and apply the processing logic. 4. Use a coroutine (function with `async def`) that performs an asynchronous I/O bound task as part of the processing logic. # Processing Logic 1. Write the input data to a file named \\"input_data.txt\\" using a context manager (with statement). 2. Read back the data from \\"input_data.txt\\" to ensure it was written correctly. 3. Define an asynchronous coroutine function `async double_value(value: int) -> int` that takes an integer and returns its double after simulating an I/O operation using `await asyncio.sleep(1)`. 4. For each integer in the input list, use the `async double_value` coroutine to double the value and collect these results in a new list. 5. Use the `try` block to handle any potential I/O errors and ensure proper cleanup using the `finally` clause. # Constraints - The length of `data_list` should not exceed 1000 elements. - Each element in `data_list` must be an integer between -1000 and 1000 inclusive. # Example ```python import asyncio async def process_data(data_list): # Your code here # Example usage: data = [1, 2, 3, 4, 5] # Run the coroutine using asyncio result = asyncio.run(process_data(data)) print(result) # Expected output: [2, 4, 6, 8, 10] ``` Implement the function `process_data` according to the above specifications and demonstrate its usage with the provided example.","solution":"import asyncio async def double_value(value: int) -> int: Asynchronously double the value after simulating an I/O bound operation. await asyncio.sleep(1) return value * 2 async def process_data(data_list): Process the input data list by doubling each value asynchronously. Args: - data_list (List[int]): a list of integers. Returns: - List[int]: a list of integers after processing. try: # Write the data to a file \\"input_data.txt\\" with open(\\"input_data.txt\\", \\"w\\") as f: for data in data_list: f.write(f\\"{data}n\\") # Read back the data to ensure it was written correctly with open(\\"input_data.txt\\", \\"r\\") as f: data_read = f.readlines() data_read = [int(line.strip()) for line in data_read] # Process the data using the async double_value function tasks = [double_value(value) for value in data_read] processed_data = await asyncio.gather(*tasks) return processed_data except Exception as e: print(f\\"An error occurred: {e}\\") finally: print(\\"Completed data processing.\\")"},{"question":"Task: Implement a script that uses the `runpy` module to dynamically execute a given Python module or script and return specific details about its execution environment. Instructions: 1. Create a function `execute_module(module_name, alter_sys=False)` with the following parameters: - `module_name`: A string representing the absolute name of the module or the path to the script to be executed. - If `module_name` is a module name (e.g., \'http.server\'), use `runpy.run_module`. - If `module_name` is a script path (e.g., \'path/to/script.py\'), use `runpy.run_path`. - `alter_sys`: A boolean indicating whether to allow `runpy.run_module` to alter the `sys` module. 2. The function should execute the specified module/script and return a dictionary containing the following details: - The value of `__name__` in the executed module/script. - The value of `__file__` in the executed module/script. - Any global variables defined in the executed module/script. 3. Ensure your function handles potential errors gracefully, such as when the specified module or script does not exist. Example: ```python def execute_module(module_name: str, alter_sys: bool = False) -> dict: # Your implementation here # Example module execution result = execute_module(\'http.server\', alter_sys=True) print(result) # Output may look like: # { # \'__name__\': \'http.server\', # \'__file__\': \'/usr/lib/python3.10/http/server.py\', # \'global_var_1\': \'value1\', # \'global_var_2\': \'value2\', # ... # } ``` Requirements: - Your solution should demonstrate an understanding of how `runpy.run_module` and `runpy.run_path` work. - You must manage global variables and return them correctly. - Handle exceptions that may occur during module/script execution. Constraints: - Assume that the provided module name or script path is valid and accessible. - Do not use any additional modules for executing the given task except for `runpy`. Happy coding!","solution":"import runpy def execute_module(module_name: str, alter_sys: bool = False) -> dict: result = {} try: if module_name.endswith(\'.py\'): execution_globals = runpy.run_path(module_name) else: execution_globals = runpy.run_module(module_name, alter_sys=alter_sys) result[\'__name__\'] = execution_globals.get(\'__name__\') result[\'__file__\'] = execution_globals.get(\'__file__\') result.update({k: v for k, v in execution_globals.items() if k not in (\'__name__\', \'__file__\')}) except Exception as e: result[\'error\'] = str(e) return result"},{"question":"# Assessment Question Summary You are required to implement a function that generates and saves a set of plots using the seaborn.objects package. This task will test your ability to work with seaborn for data visualization, including loading datasets, creating and customizing plots, and combining multiple plot elements. Function Description Implement the function `generate_plots(dowjones_file: str, fmri_file: str, output_dir: str) -> None` that performs the following tasks: 1. Load the datasets from the provided CSV files. 2. Create and save the following plots in the specified output directory: - A line plot of the Dow Jones dataset showing Date vs. Price. - A line plot of the Dow Jones dataset with Price on the x-axis and Date on the y-axis. - A line plot of the FMRI dataset (filtered for `region == \'parietal\'` and `event == \'stim\'`) with signals over time, grouped by subject. - A line plot of the FMRI dataset with timepoint vs. signal, with lines varying by region color and event linestyle. - An enhanced plot from the previous step, adding an error bar (using `so.Band`) and markers at sampled data points. Input Parameters - `dowjones_file (str)`: The path to the CSV file containing the Dow Jones dataset. - `fmri_file (str)`: The path to the CSV file containing the FMRI dataset. - `output_dir (str)`: The directory path where the plots should be saved. Output - The function should save the generated plots as PNG files in the specified output directory with filenames: - `dowjones_date_price.png` - `dowjones_price_date.png` - `fmri_parietal_stim.png` - `fmri_grouped.png` - `fmri_grouped_with_error.png` Requirements - Use `seaborn.objects` for creating the plots. - Ensure that each plot is saved with a resolution of 300 DPI. - Handle file I/O and possible exceptions that may occur (e.g., file not found, permission errors). Constraints - Assume the datasets are small enough to fit into memory. - Assume the datasets have no missing values. Example ```python generate_plots(\'dowjones.csv\', \'fmri.csv\', \'./plots\') ``` This function will read the datasets from `dowjones.csv` and `fmri.csv`, generate the specified plots, and write them to the `./plots` directory with the appropriate filenames.","solution":"import seaborn.objects as so import pandas as pd import os def generate_plots(dowjones_file: str, fmri_file: str, output_dir: str) -> None: try: # Load datasets dow_jones_df = pd.read_csv(dowjones_file) fmri_df = pd.read_csv(fmri_file) # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) # 1. Line plot of Dow Jones (Date vs. Price) plot1 = so.Plot(dow_jones_df, x=\\"Date\\", y=\\"Price\\").add(so.Line()) plot1.save(os.path.join(output_dir, \\"dowjones_date_price.png\\"), dpi=300) # 2. Line plot of Dow Jones (Price vs. Date) plot2 = so.Plot(dow_jones_df, x=\\"Price\\", y=\\"Date\\").add(so.Line()) plot2.save(os.path.join(output_dir, \\"dowjones_price_date.png\\"), dpi=300) # 3. Line plot of FMRI (filtered dataset) filtered_fmri_df = fmri_df[(fmri_df[\'region\'] == \'parietal\') & (fmri_df[\'event\'] == \'stim\')] plot3 = so.Plot(filtered_fmri_df, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\").add(so.Line()) plot3.save(os.path.join(output_dir, \\"fmri_parietal_stim.png\\"), dpi=300) # 4. Line plot of FMRI (grouped by region and event) plot4 = so.Plot(fmri_df, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\").add(so.Line()) plot4.save(os.path.join(output_dir, \\"fmri_grouped.png\\"), dpi=300) # 5. Enhanced FMRI plot with error bars and markers plot5 = ( so.Plot(fmri_df, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line()) .add(so.Band()) .add(so.Dot()) ) plot5.save(os.path.join(output_dir, \\"fmri_grouped_with_error.png\\"), dpi=300) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"<|Analysis Begin|> The provided documentation focuses on using the `seaborn.objects` module to create visualizations from a dataset. It demonstrates loading a dataset using the `load_dataset` function, and then creating a plot using `seaborn.objects.Plot` to visualize \'Spending_USD\' against \'Life_Expectancy\' with color-coded data points for different countries. The `Path` mark is used to plot trajectories through variable space without sorting the data, and various properties such as `marker`, `pointsize`, `linewidth`, and `fillcolor` can be customized. Key concepts covered: - Loading datasets. - Creating plots with `seaborn.objects.Plot`. - Using the `Path` mark to plot trajectories. - Customizing plot properties. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of using the `seaborn.objects` module to create customized visualizations. **Question:** You are provided with a dataset that includes various statistics for multiple countries over several years. Your task is to create a visualization that shows the trajectory of a specific statistical relationship for each country. The relationship to be visualized is between \'Healthcare Spending (in USD)\' and \'Life Expectancy (in years).\' You need to create a plot that distinguishes each country\'s data points with unique colors and includes specific customizations for the markers and lines. **Input:** - A dataset, `healthexp`, loaded with the following columns: - \'Country\' (string): Name of the country. - \'Year\' (int): Year of observation. - \'Spending_USD\' (float): Healthcare spending in USD. - \'Life_Expectancy\' (float): Life expectancy in years. **Output:** - A `seaborn` plot object displaying the trajectories of the \'Spending_USD\' vs. \'Life_Expectancy\' relationship for each country. Each country\'s trajectory should be uniquely colored, with circular markers at each data point. Additionally, the markers should have a white fill color, a point size of 3, and a line width of 1. **Constraints:** - Ensure all data points are plotted without sorting by the dataset. - The plot should include a legend to identify each country by color. **Performance Requirements:** - The plot should be generated efficiently and should render correctly for a dataset with up to 1000 observations. **Example Usage:** ```python import seaborn.objects as so from seaborn import load_dataset # Load and prepare the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=3, linewidth=1, fillcolor=\\"w\\")) p.show() ``` **Expected Output:** - A plot where different colored lines represent different countries, with circular markers at each data point, as described in the customization requirements. **Python Libraries Requirements:** - seaborn Ensure your solution includes proper data loading, plot creation, and customization as specified.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_health_exp_plot(): # Load and prepare the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=3, linewidth=1, fillcolor=\\"w\\")) return p"},{"question":"**Coding Assessment Question:** # Implementing a Custom Container Using Abstract Base Classes Objective: Design and implement a custom sequence container class using the `collections.abc` abstract base classes. Your implementation should demonstrate a thorough understanding of the `Sequence` ABC and its required methods. Requirements: 1. **Class Design**: Implement a class `CustomSequence` that behaves like a sequence (similar to a Python list) and inherits from `collections.abc.Sequence`. 2. **Mandatory Methods**: - `__getitem__(self, index)`: Retrieve an item at a specified index. - `__len__(self)`: Return the length of the sequence. 3. **Additional Methods**: - Implement the `__contains__`, `__iter__`, and `__reversed__` methods, which are mixin methods provided by `Sequence` through inheritance. 4. **Initialization**: - The class should initialize with an iterable, converting it into a list to store the elements. 5. **Constraints**: - The sequence should not allow modifications (i.e., it should be an immutable sequence). 6. **Performance**: - Your implementation of `__getitem__` and `__len__` must be efficient with `O(1)` time complexity. Input and Output: - **Initialization**: Accepts any iterable (e.g., list, tuple, string) and converts it to a list. - **Methods**: - `__getitem__(self, index)`: Takes an integer index and returns the corresponding element. - `__len__(self)`: Returns the number of elements in the sequence. Example Usage: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self._elements = list(iterable) def __getitem__(self, index): return self._elements[index] def __len__(self): return len(self._elements) # Example usage: seq = CustomSequence([1, 2, 3, 4, 5]) print(seq[2]) # Output: 3 print(len(seq)) # Output: 5 print(3 in seq) # Output: True print(list(seq)) # Output: [1, 2, 3, 4, 5] print(list(reversed(seq))) # Output: [5, 4, 3, 2, 1] ``` Notes: - Do not use any additional libraries apart from `collections.abc`. - Handle edge cases such as empty initialization and negative indexing. - Ensure your class passes the `isinstance` and `issubclass` checks for `Sequence`.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self._elements = list(iterable) def __getitem__(self, index): return self._elements[index] def __len__(self): return len(self._elements) def __contains__(self, item): return item in self._elements def __iter__(self): return iter(self._elements) def __reversed__(self): return reversed(self._elements)"},{"question":"**Title: Exploring the Behavior of Copy-on-Write in Pandas** Given the new Copy-on-Write (CoW) behavior introduced in pandas 3.0, you are to write functions that reflect an understanding of these changes. You will be working with DataFrames and are required to demonstrate how CoW impacts data manipulations. # Task 1: Identify and update a subset 1. **Function**: `update_subset` - **Input**: A pandas DataFrame `df`, a column name `col_name` to filter, and a dictionary `update_values` containing the filter condition and the corresponding values to set. - **Output**: A DataFrame after updating the specified subset. - **Constraints**: - You must use CoW-compliant methods to avoid any chained assignments or unintended side effects. ```python def update_subset(df, col_name, update_values): Update values in a DataFrame based on a condition in a specified column. Parameters: df (pd.DataFrame): The input DataFrame. col_name (str): The column name to apply the filter on. update_values (dict): A dictionary where keys are conditions (e.g., {\'>\': 5}) and values are the new values to set in `col_name`. Returns: pd.DataFrame: The DataFrame after applying the updates. pass ``` Example: ```python df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) update_values = {\\">\\": 5} # The result should use CoW practices to update rows where the \'bar\' column is greater than 5. updated_df = update_subset(df, \'foo\', update_values) print(updated_df) ``` # Task 2: Safe update with shared data 2. **Function**: `safely_increment_column` - **Input**: A pandas DataFrame `df`, a column name `col_name`, and an integer `increment_value`. - **Output**: A DataFrame where the specified column values are incremented by the provided value, ensuring no original data is accidentally updated due to shared references. - **Constraints**: - The function must ensure that any necessary copies are made to prevent shared data issues. - Performance should be considered; avoid unnecessary copies. ```python def safely_increment_column(df, col_name, increment_value): Increment the values of a specified column, ensuring no accidental modification of shared data structures using Copy-on-Write principles. Parameters: df (pd.DataFrame): The input DataFrame. col_name (str): The column to increment. increment_value (int): The value to add to each item in the specified column. Returns: pd.DataFrame: The DataFrame after applying the increment. pass ``` Example: ```python df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) incremented_df = safely_increment_column(df, \\"foo\\", 10) print(incremented_df) ``` # Task 3: Confirming CoW behavior 3. **Function**: `create_and_modify_copy` - **Input**: A pandas DataFrame `df` and a dict `updates` where keys are column names and values are lists of updates to perform. - **Output**: A tuple containing the original DataFrame and the modified DataFrame after performing updates, ensuring the original DataFrame remains unaffected. - **Constraints**: - Utilize the CoW principles to ensure the original DataFrame is unchanged. - Updates should be performed using pandas methods that comply with CoW. ```python def create_and_modify_copy(df, updates): Create a copy of the DataFrame, perform updates, and ensure the original DataFrame remains unchanged. Parameters: df (pd.DataFrame): The input DataFrame. updates (dict): A dictionary where keys are column names and values are lists with tuples of row index and new value. Returns: tuple: A tuple containing the original DataFrame and the modified DataFrame. pass ``` Example: ```python df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) updates = {\\"foo\\": [(0, 10), (1, 20)], \\"bar\\": [(0, 40)]} original_df, modified_df = create_and_modify_copy(df, updates) print(original_df) print(modified_df) ``` # Notes - Ensure to import pandas as pd in your solution. - Add any necessary error handling to ensure the functions handle invalid inputs gracefully. - Test your functions with various DataFrames to confirm they perform as expected under the CoW rules.","solution":"import pandas as pd def update_subset(df, col_name, update_values): Update values in a DataFrame based on a condition in a specified column. Parameters: df (pd.DataFrame): The input DataFrame. col_name (str): The column name to apply the filter on. update_values (dict): A dictionary where keys are conditions (e.g., {\'>\': 5}) and values are the new values to set in `col_name`. Returns: pd.DataFrame: The DataFrame after applying the updates. df = df.copy() # Ensure we are working on a copy to follow CoW guidelines for condition, new_value in update_values.items(): if condition == \\">\\": df.loc[df[col_name] > new_value, col_name] = new_value # Additional conditions can be added as needed return df def safely_increment_column(df, col_name, increment_value): Increment the values of a specified column, ensuring no accidental modification of shared data structures using Copy-on-Write principles. Parameters: df (pd.DataFrame): The input DataFrame. col_name (str): The column to increment. increment_value (int): The value to add to each item in the specified column. Returns: pd.DataFrame: The DataFrame after applying the increment. df = df.copy() # Ensure we are working on a copy to follow CoW guidelines df[col_name] += increment_value return df def create_and_modify_copy(df, updates): Create a copy of the DataFrame, perform updates, and ensure the original DataFrame remains unchanged. Parameters: df (pd.DataFrame): The input DataFrame. updates (dict): A dictionary where keys are column names and values are lists with tuples of row index and new value. Returns: tuple: A tuple containing the original DataFrame and the modified DataFrame. original_df = df.copy() # Make sure we retain a copy of the original to preserve it modified_df = df.copy() # Work on a copy of the original to apply updates for column, changes in updates.items(): for index, new_value in changes: modified_df.at[index, column] = new_value return original_df, modified_df"},{"question":"# Complex Data Visualization with Seaborn **Objective**: Assess the student\'s ability to utilize advanced features of Seaborn for complex data visualizations, particularly using figure-level functions and customization options. Problem Statement: You are provided with a dataset containing information about penguins. Your task is to create a multi-faceted data visualization that explores relationships among the variables in the dataset. Specifically, you need to create a combined visualization that consists of: 1. **A pair plot**: Showing relationships between different variables color-coded by species. 2. **A customized scatterplot**: Displaying the relationship between flipper length and bill length, including a regression line and custom styling. # Dataset: You should use the built-in penguins dataset available in Seaborn. # Requirements: 1. **Pair Plot**: - Create a pair plot for the variables (`flipper_length_mm`, `bill_length_mm`, `body_mass_g`), color-coded by species. 2. **Customized Scatterplot**: - Create a scatterplot displaying the relationship between `flipper_length_mm` and `bill_length_mm`, color-coded by species. - Add a regression line to the scatterplot. - Customize the scatterplot with the following: - Use a distinct marker for each species. - Include a legend outside of the plot. - Set custom axis labels (\\"Flipper Length (mm)\\" and \\"Bill Length (mm)\\"). # Input: No explicit input is needed as the dataset (`penguins`) should be loaded within the solution. # Expected Output: The expected output is a figure with: 1. A pair plot showing relations between `flipper_length_mm`, `bill_length_mm`, and `body_mass_g`. 2. A customized scatter plot with a regression line for `flipper_length_mm` vs `bill_length_mm`. # Implementation: Implement this in a function called `create_penguin_plots()`. The function should not take any parameters and should not return any values, but it should display the plots. ```python def create_penguin_plots(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Pair plot sns.pairplot(penguins, hue=\\"species\\", diag_kind=\\"kde\\") # Customized Scatterplot plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", style=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"]) sns.regplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", scatter=False, color=\'blue\') plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.legend(title=\'Species\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.tight_layout() plt.show() ``` # Constraints: - Ensure the plots are neat and readable. - Utilize Seaborn\'s capabilities to handle plot aesthetics.","solution":"def create_penguin_plots(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Pair plot sns.pairplot(penguins, vars=[\\"flipper_length_mm\\", \\"bill_length_mm\\", \\"body_mass_g\\"], hue=\\"species\\", diag_kind=\\"kde\\") # Customized Scatterplot plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", style=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"]) sns.regplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", scatter=False, color=\'blue\') plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.legend(title=\'Species\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.tight_layout() plt.show()"},{"question":"# Question: Network Data Retrieval and Filtering **Objective**: Demonstrate your understanding of socket programming, regular expressions, and functional programming in Python by implementing a function that retrieves data from a network socket and processes it. Problem Statement You are required to write a Python function `retrieve_and_filter_data(host: str, port: int, pattern: str) -> list`. This function will: 1. Establish a connection to a server specified by a `host` and `port`. 2. Retrieve data from the server until no more data is to be received. 3. Filter the retrieved data using a regular expression pattern provided in `pattern`. 4. Return a list of all matches found. Function Signature ```python def retrieve_and_filter_data(host: str, port: int, pattern: str) -> list: pass ``` Input 1. `host` (str): The server hostname or IP address. 2. `port` (int): The port number to connect to the server. 3. `pattern` (str): The regular expression pattern for filtering data. Output - Returns a list of strings, each representing a match found in the retrieved data according to the provided `pattern`. Constraints - The data will be received in chunks of up to 4096 bytes. - You may assume the server closes the connection once all data has been sent. Example Suppose the server at `example.com` on port `12345` sends the following data: ``` Hello, welcome to the example server. Your IP address is 123.45.67.89. Today\'s date is 2023-01-01. Goodbye! ``` And the regular expression pattern provided is `r\'d{3}.d{2}.d{2}.d{2}\'`. The function call `retrieve_and_filter_data(\'example.com\', 12345, r\'d{3}.d{2}.d{2}.d{2}\')` should return: ``` [\'123.45.67.89\'] ``` Notes 1. Ensure you handle socket connection errors gracefully. 2. Use built-in Python libraries such as `socket` for network connections and `re` for regular expression matching. 3. Consider edge cases such as partial matches at the boundaries of received data chunks. 4. Maintain modularity and readability of the code. Implementation Tips - Establishing a connection using `socket.connect`. - Reading data in a loop using `socket.recv`. - Using `re.findall` to filter the data. - Closing the socket connection properly. Good luck!","solution":"import socket import re def retrieve_and_filter_data(host: str, port: int, pattern: str) -> list: Establishes a connection to a server, retrieves data, filters it with a regular expression, and returns all matches. Parameters: host (str): The server hostname or IP address. port (int): The port number to connect to the server. pattern (str): The regular expression pattern for filtering data. Returns: list: A list of all matches found. try: # Initialize socket connection with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((host, port)) data = \\"\\" while True: chunk = s.recv(4096) if not chunk: break data += chunk.decode(\'utf-8\') # Use regular expression to filter data matches = re.findall(pattern, data) return matches except socket.error as err: print(f\\"Socket error: {err}\\") return [] except Exception as err: print(f\\"General error: {err}\\") return []"},{"question":"You are given a list of mathematical expressions in the form of strings. Your task is to write a function `evaluate_expressions(expressions: List[str], global_vars: Dict[str, Any]) -> List[Any]` that evaluates these expressions and returns a list of their results. The function should support variables, and you can assume that all variables needed to evaluate the expressions are provided in the `global_vars` dictionary. Additionally, the function should handle exceptions gracefully and return a custom error message for invalid expressions. Function Signature: ```python from typing import List, Dict, Any def evaluate_expressions(expressions: List[str], global_vars: Dict[str, Any]) -> List[Any]: # Your code here ``` Input: - `expressions`: A list of strings, where each string is a valid mathematical expression. Example: `[\\"a + b\\", \\"c * d / e\\", \\"f ** 2\\"]`. - `global_vars`: A dictionary where the keys are variable names, and the values are their corresponding values. Example: `{\\"a\\": 10, \\"b\\": 5, \\"c\\": 8, \\"d\\": 2, \\"e\\": 4, \\"f\\": 3}`. Output: - A list containing the results of the evaluated expressions in the same order as the input expressions. - If an expression cannot be evaluated, instead of causing the program to crash, it should return the custom error message `\\"Invalid expression\\"` for that particular expression. Constraints: - You may assume that all expressions only involve basic arithmetic operations (`+`, `-`, `*`, `/`, and `**`). - You may assume that `global_vars` provides all necessary variables for the expressions. Example: ```python expressions = [\\"a + b\\", \\"c * d / e\\", \\"f ** 2\\", \\"a + unknown_var\\"] global_vars = {\\"a\\": 10, \\"b\\": 5, \\"c\\": 8, \\"d\\": 2, \\"e\\": 4, \\"f\\": 3} output = evaluate_expressions(expressions, global_vars) # Expected output: [15, 4.0, 9, \\"Invalid expression\\"] ``` Notes: - Use the `eval()` function to evaluate the expressions. - Handle errors such as `NameError`, `SyntaxError`, or any other exceptions that may arise during the evaluation process and return `\\"Invalid expression\\"` for those cases.","solution":"from typing import List, Dict, Any def evaluate_expressions(expressions: List[str], global_vars: Dict[str, Any]) -> List[Any]: results = [] for expr in expressions: try: result = eval(expr, {}, global_vars) results.append(result) except Exception: results.append(\\"Invalid expression\\") return results"},{"question":"# Email MIME Composition and Manipulation You are tasked with designing a script that constructs and manipulates MIME objects using the `email.mime` module in Python. Specifically, the goal is to create a MIME email with various parts, including text, images, and an application attachment. Your task includes the following subtasks: 1. **Creating the Multi-Part MIME Structure**: - Create a MIME message with a subtype of `mixed`. - This main message should have two parts: - A text part that describes the email\'s purpose. - A multi-part alternative section containing: - A plain text version of the message. - An HTML version of the message. 2. **Adding Attachments**: - Attach an image to the email. - Attach a PDF file to the email. 3. **Custom Headers**: - Add suitable `Content-Type` and `MIME-Version` headers. - Ensure the appropriate encoding for the attachments. Function Definitions ```python def create_email(subject, sender, recipient): Creates a multi-part MIME email. Args: subject (str): The subject of the email. sender (str): The sender\'s email address. recipient (str): The recipient\'s email address. Returns: MIMEMultipart: The constructed email object. pass def attach_text_parts(container, plain_text, html_text): Attaches text parts (plain and HTML) to the provided container. Args: container (MIMEMultipart): The email object to which text parts are to be attached. plain_text (str): The plain text version of the message. html_text (str): The HTML version of the message. Returns: None pass def attach_file(container, file_path, mime_class, subtype): Attaches a file to a MIME container. Args: container (MIMEMultipart): The email object to which the file is to be attached. file_path (str): Path to the file to attach. mime_class (type): The specific MIME class to use for the attachment. subtype (str): The MIME subtype. Returns: None pass ``` Constraints: - Use `MIMEText` for text parts. - Use `MIMEImage` for image attachments. - Use `MIMEApplication` for applications like PDF. - Handle exceptions where necessary (e.g., invalid file paths). - The image file and PDF should be encoded properly. # Example Execution ```python subject = \\"Monthly Report\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" plain_text = \\"This is the plain text version of the email.\\" html_text = \\"<html><body><h1>This is the HTML version of the email.</h1></body></html>\\" image_path = \\"/path/to/image.png\\" pdf_path = \\"/path/to/report.pdf\\" email = create_email(subject, sender, recipient) attach_text_parts(email, plain_text, html_text) attach_file(email, image_path, MIMEImage, \\"png\\") attach_file(email, pdf_path, MIMEApplication, \\"pdf\\") print(email.as_string()) ``` Implement the given functions to satisfy the requirements and properly construct a MIME email object.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders import os def create_email(subject, sender, recipient): Creates a multi-part MIME email. Args: subject (str): The subject of the email. sender (str): The sender\'s email address. recipient (str): The recipient\'s email address. Returns: MIMEMultipart: The constructed email object. email = MIMEMultipart(\'mixed\') email[\'Subject\'] = subject email[\'From\'] = sender email[\'To\'] = recipient email.add_header(\'MIME-Version\', \'1.0\') return email def attach_text_parts(container, plain_text, html_text): Attaches text parts (plain and HTML) to the provided container. Args: container (MIMEMultipart): The email object to which text parts are to be attached. plain_text (str): The plain text version of the message. html_text (str): The HTML version of the message. Returns: None alternative_part = MIMEMultipart(\'alternative\') alternative_part.attach(MIMEText(plain_text, \'plain\')) alternative_part.attach(MIMEText(html_text, \'html\')) container.attach(alternative_part) def attach_file(container, file_path, mime_class, subtype): Attaches a file to a MIME container. Args: container (MIMEMultipart): The email object to which the file is to be attached. file_path (str): Path to the file to attach. mime_class (type): The specific MIME class to use for the attachment. subtype (str): The MIME subtype. Returns: None if not os.path.isfile(file_path): raise FileNotFoundError(f\\"The file at path {file_path} does not exist.\\") with open(file_path, \'rb\') as file: part = mime_class(file.read(), _subtype=subtype) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(file_path)}\\"\') encoders.encode_base64(part) container.attach(part)"},{"question":"# Creating Built Distributions with Distutils Distutils is a powerful tool for building and distributing Python modules. To assess your understanding of this tool, you will implement a function that programmatically generates built distributions for a given Python module. Task You need to write a function called `create_built_distribution` that does the following: 1. Takes the path to a module\'s setup script (`setup.py`) as input. 2. Generates a built distribution for the module in multiple formats as specified. 3. Supports generating built distributions for the following formats: `gzipped tar file (gztar)`, `zip file (zip)`, `RPM (rpm)`. 4. Provides an option for cross-compiling on Windows to generate a 64-bit installer from a 32-bit environment. Function Signature ```python def create_built_distribution(setup_script_path: str, formats: list, cross_compile: bool = False, cross_compile_target: str = \\"win-amd64\\") -> None: Generates built distributions for a given module. :param setup_script_path: str - The path to the module\'s setup.py script. :param formats: list - A list of desired formats for the built distribution (e.g., [\\"gztar\\", \\"zip\\", \\"rpm\\"]). :param cross_compile: bool - If True, enables cross-compilation (default: False). :param cross_compile_target: str - Target platform for cross-compilation (default: \\"win-amd64\\"). :return: None - Outputs built distributions to the appropriate directories. pass ``` Constraints - The function should use the `subprocess` module to call appropriate Distutils commands. - Ensure proper exception handling and informative error messages in case of issues (e.g., invalid format, missing setup script). - When cross-compiling, ensure that the operation is only carried out on Windows platforms with the necessary tools installed. - Document any assumptions made in your code as comments. Example Usage ```python # Example usage of the create_built_distribution function create_built_distribution(\\"/path/to/your/module/setup.py\\", [\\"gztar\\", \\"zip\\"], cross_compile=True) ``` Performance Requirements - The function should efficiently handle the creation of multiple distribution formats in a single run. - Cross-compiling should leverage available system resources without unnecessary overhead. Hints - The `subprocess` module will be crucial for executing shell commands from within your Python script. - Consult Python\'s Distutils documentation for details on the `bdist`, `bdist_rpm`, and other related commands. Good luck and happy coding!","solution":"import subprocess import platform def create_built_distribution(setup_script_path: str, formats: list, cross_compile: bool = False, cross_compile_target: str = \\"win-amd64\\") -> None: Generates built distributions for a given module. :param setup_script_path: str - The path to the module\'s setup.py script. :param formats: list - A list of desired formats for the built distribution (e.g., [\\"gztar\\", \\"zip\\", \\"rpm\\"]). :param cross_compile: bool - If True, enables cross-compilation (default: False). :param cross_compile_target: str - Target platform for cross-compilation (default: \\"win-amd64\\"). :return: None - Outputs built distributions to the appropriate directories. # Ensuring the formats provided are valid valid_formats = {\\"gztar\\", \\"zip\\", \\"rpm\\"} for fmt in formats: if fmt not in valid_formats: raise ValueError(f\\"Invalid format specified: {fmt}\\") # Initial command to run the setup script command = [\\"python\\", setup_script_path, \\"sdist\\"] # Adding specific formats for fmt in formats: command += [\\"bdist\\", \\"--formats=\\" + fmt] # Handling cross-compilation, specifically for Windows target if cross_compile: if platform.system() != \'Windows\': raise ValueError(\\"Cross-compiling is only supported on Windows.\\") command += [\\"bdist_wininst\\", \\"--plat-name\\", cross_compile_target] try: # Running the setup command using subprocess subprocess.run(command, check=True) except subprocess.CalledProcessError as e: print(f\\"An error occurred while creating built distribution: {e}\\")"},{"question":"# Comprehensive Warning Management In this exercise, you will demonstrate your understanding of the Python `warnings` module by completing the following tasks: 1. **Function Implementation**: - Implement the function `log_deprecation(function)` which issues a `DeprecationWarning` whenever the decorated function is called. - Implement the function `manage_warnings` that configures the global warning filter to: - Show all warnings by default. - Treat all `ResourceWarning` as errors. - Ignore all `ImportWarning`. 2. **Warning Capture**: - Write the function `capture_specific_warning(func)` that captures and returns any `RuntimeWarning` issued by the function `func` when it is called. Return `None` if no `RuntimeWarning` is issued. 3. **Temporary Warning Filter**: - Implement the function `suppress_warnings_temporarily(func, warning_category)` which temporarily suppresses all warnings of the specified `warning_category` when `func` is called. # Expected Input and Output 1. **log_deprecation**: - Input: A function. - Output: A decorated function that issues a `DeprecationWarning` when called. 2. **manage_warnings**: - Input: None. - Output: No return value. The function configures the global warning filter. 3. **capture_specific_warning**: - Input: A function `func`. - Output: The captured `RuntimeWarning` object or `None`. 4. **suppress_warnings_temporarily**: - Input: A function `func` and a warning category. - Output: No return value. Suppresses the specified warning category when `func` is called. # Constraints - Use the `warnings` module. - Ensure the code runs efficiently even for large codes that produce numerous warnings. # Example ```python import warnings @log_deprecation def deprecated_function(): print(\\"This function is deprecated.\\") def resource_usage_function(): warnings.warn(\\"Resource usage warning\\", ResourceWarning) def import_issue_function(): warnings.warn(\\"Import warning\\", ImportWarning) def runtime_issue_function(): warnings.warn(\\"Runtime warning\\", RuntimeWarning) # Task 1: Log Deprecation deprecated_function() # Should print \\"This function is deprecated.\\" and issue a DeprecationWarning # Task 2: Manage Warnings manage_warnings() resource_usage_function() # Should raise a ResourceWarning as an error import_issue_function() # Should not print anything as ImportWarning is ignored # Task 3: Capture Specific Warning captured_warning = capture_specific_warning(runtime_issue_function) print(captured_warning) # Should print the RuntimeWarning object details # Task 4: Suppress Warnings Temporarily suppress_warnings_temporarily(resource_usage_function, ResourceWarning) # Should not raise any ResourceWarning ```","solution":"import warnings from functools import wraps def log_deprecation(function): Decorator that issues a DeprecationWarning whenever the decorated function is called. @wraps(function) def wrapped(*args, **kwargs): warnings.warn(f\\"{function.__name__} is deprecated\\", DeprecationWarning, stacklevel=2) return function(*args, **kwargs) return wrapped def manage_warnings(): Configures the global warning filter. Show all warnings by default. Treat all ResourceWarning as errors. Ignore all ImportWarning. warnings.resetwarnings() warnings.simplefilter(\\"default\\") # Show all warnings by default warnings.simplefilter(\\"error\\", ResourceWarning) # Treat ResourceWarning as errors warnings.simplefilter(\\"ignore\\", ImportWarning) # Ignore ImportWarning def capture_specific_warning(func): Captures and returns any RuntimeWarning issued by the function func when it is called. Returns None if no RuntimeWarning is issued. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", RuntimeWarning) # Capture RuntimeWarnings func() for warning in w: if issubclass(warning.category, RuntimeWarning): return warning return None def suppress_warnings_temporarily(func, warning_category): Temporarily suppresses all warnings of the specified warning_category when func is called. with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", warning_category) func()"},{"question":"# Question: Implement a Custom Calendar Event Manager You are tasked with creating a simple calendar event manager in Python that leverages the `datetime` module to manage events. This event manager should support the following functionalities: 1. **Add Event**: This function should add an event with a specified name, start datetime, and end datetime. 2. **Remove Event**: This function should remove an event by its name. 3. **List Events**: This function should list all events, optionally filtered by a specific date. 4. **Get Event Duration**: This function should return the duration of a specific event in hours, minutes, and seconds. Your implementation should handle the following constraints and edge cases: - Event names are unique. - The start datetime should be before the end datetime. - The filter function should only list events that occur (partially or entirely) on the specified date. # Input and Output Formats - **add_event(name: str, start: str, end: str) -> None**: - `name`: `string`, the name of the event. - `start`: `string`, the start datetime in the format \\"YYYY-MM-DD HH:MM:SS\\". - `end`: `string`, the end datetime in the format \\"YYYY-MM-DD HH:MM:SS\\". - Output: `None`. - **remove_event(name: str) -> None**: - `name`: `string`, the name of the event to be removed. - Output: `None`. - **list_events(date: Optional[str] = None) -> List[str]**: - `date`: `str` or `None`, the date to filter events in the format \\"YYYY-MM-DD\\". If `None`, list all events. - Output: `List[string]`, where each string represents an event\'s name and its start and end datetimes in the format \\"EventName: YYYY-MM-DD HH:MM:SS to YYYY-MM-DD HH:MM:SS\\". - **get_event_duration(name: str) -> Tuple[int, int, int]**: - `name`: `string`, the name of the event to get the duration for. - Output: `Tuple[int, int, int]`, representing the hours, minutes, and seconds of the event\'s duration. # Example ```python # Creating an instance of the event manager manager = EventManager() # Adding events manager.add_event(\\"Conference\\", \\"2023-10-01 09:00:00\\", \\"2023-10-01 17:00:00\\") manager.add_event(\\"Meeting\\", \\"2023-10-02 14:00:00\\", \\"2023-10-02 15:00:00\\") # Listing all events print(manager.list_events()) # Output: [\\"Conference: 2023-10-01 09:00:00 to 2023-10-01 17:00:00\\", \\"Meeting: 2023-10-02 14:00:00 to 2023-10-02 15:00:00\\"] # Listing events for a specific date print(manager.list_events(\\"2023-10-01\\")) # Output: [\\"Conference: 2023-10-01 09:00:00 to 2023-10-01 17:00:00\\"] # Getting the duration of an event print(manager.get_event_duration(\\"Conference\\")) # Output: (8, 0, 0) # 8 hours, 0 minutes, 0 seconds # Removing an event manager.remove_event(\\"Conference\\") print(manager.list_events()) # Output: [\\"Meeting: 2023-10-02 14:00:00 to 2023-10-02 15:00:00\\"] ``` Implement the `EventManager` class with the described functionalities.","solution":"from datetime import datetime, timedelta from typing import List, Tuple, Optional class EventManager: def __init__(self): self.events = {} def add_event(self, name: str, start: str, end: str) -> None: Adds an event with the specified name, start datetime, and end datetime. start_dt = datetime.strptime(start, \\"%Y-%m-%d %H:%M:%S\\") end_dt = datetime.strptime(end, \\"%Y-%m-%d %H:%M:%S\\") if start_dt >= end_dt: raise ValueError(\\"The start datetime must be before the end datetime.\\") self.events[name] = (start_dt, end_dt) def remove_event(self, name: str) -> None: Removes the event with the specified name. if name in self.events: del self.events[name] def list_events(self, date: Optional[str] = None) -> List[str]: Lists all events, optionally filtered by a specific date. result = [] if date: filter_date = datetime.strptime(date, \\"%Y-%m-%d\\").date() for name, (start, end) in self.events.items(): if start.date() <= filter_date <= end.date(): result.append(f\\"{name}: {start.strftime(\'%Y-%m-%d %H:%M:%S\')} to {end.strftime(\'%Y-%m-%d %H:%M:%S\')}\\") else: for name, (start, end) in self.events.items(): result.append(f\\"{name}: {start.strftime(\'%Y-%m-%d %H:%M:%S\')} to {end.strftime(\'%Y-%m-%d %H:%M:%S\')}\\") return result def get_event_duration(self, name: str) -> Tuple[int, int, int]: Returns the duration of the specified event in hours, minutes, and seconds. if name not in self.events: raise ValueError(\\"Event not found.\\") start, end = self.events[name] duration = end - start hours, remainder = divmod(duration.total_seconds(), 3600) minutes, seconds = divmod(remainder, 60) return int(hours), int(minutes), int(seconds)"},{"question":"**Objective:** Implement a program that sets up and demonstrates the use of different types of queues provided by the `queue` module in Python. Specifically, you will be required to use `Queue`, `LifoQueue`, and `PriorityQueue` for a hypothetical task scheduling system that must handle tasks based on different queuing strategies. **Requirements:** 1. Create a `TaskScheduler` class that manages tasks using three types of queues: FIFO, LIFO, and Priority. 2. Implement methods to add tasks and retrieve tasks from each queue type. Ensure that: - Adding tasks to each queue uses the `put` method. - Retrieving tasks from each queue uses the `get` method. - Tasks to the priority queue should be added with a priority number. 3. Implement a method to simulate worker threads processing each queue type: - Each worker should retrieve a task, process it (print a message indicating the task\'s completion), and call `task_done`. - Continue until all tasks in the queue are processed. - Ensure thread-safe operations when accessing the queues. **Input:** - `add_task(queue_type, task, priority=None)`: Add a task to the specified queue type. `queue_type` is a string that can be \\"fifo\\", \\"lifo\\", or \\"priority\\". The priority is required for the priority queue. - `process_tasks(queue_type)`: Process all tasks in the specified queue type. **Output:** - Print statements indicating the task being processed and when it is finished. **Constraints:** - You may assume a maximum of 10 tasks for simplicity. - Use threading to simulate simultaneous task processing. **Sample Code:** ```python import queue import threading from dataclasses import dataclass, field from typing import Any @dataclass(order=True) class PrioritizedItem: priority: int item: Any=field(compare=False) class TaskScheduler: def __init__(self): self.fifo_queue = queue.Queue() self.lifo_queue = queue.LifoQueue() self.priority_queue = queue.PriorityQueue() def add_task(self, queue_type: str, task, priority: int = None): Add a task to the specified queue type. if queue_type == \'fifo\': self.fifo_queue.put(task) elif queue_type == \'lifo\': self.lifo_queue.put(task) elif queue_type == \'priority\': if priority is not None: self.priority_queue.put(PrioritizedItem(priority, task)) else: raise ValueError(\\"Priority must be specified for priority queue.\\") def process_tasks(self, queue_type: str): Simulate worker processing tasks in the specified queue. if queue_type == \'fifo\': q = self.fifo_queue elif queue_type == \'lifo\': q = self.lifo_queue elif queue_type == \'priority\': q = self.priority_queue def worker(): while not q.empty(): try: item = q.get_nowait() if isinstance(item, PrioritizedItem): item = item.item print(f\'Processing {item}\') print(f\'Finished {item}\') q.task_done() except queue.Empty: break threading.Thread(target=worker).start() # Example usage: scheduler = TaskScheduler() scheduler.add_task(\'fifo\', \'Task 1\') scheduler.add_task(\'lifo\', \'Task 2\') scheduler.add_task(\'priority\', \'Task 3\', priority=1) scheduler.add_task(\'priority\', \'Task 4\', priority=2) scheduler.process_tasks(\'fifo\') scheduler.process_tasks(\'lifo\') scheduler.process_tasks(\'priority\') ``` **Explanation:** - The `TaskScheduler` class initializes three different queue types. - `add_task` method adds tasks to the queues appropriately, with priority required for the priority queue. - `process_tasks` method processes tasks in a separate thread, ensuring tasks are retrieved and processed in the correct order for each queue type. Implement the `TaskScheduler` class based on the provided structure and ensure thread-safe operations when handling the queues.","solution":"import queue import threading from dataclasses import dataclass, field from typing import Any @dataclass(order=True) class PrioritizedItem: priority: int item: Any=field(compare=False) class TaskScheduler: def __init__(self): self.fifo_queue = queue.Queue() self.lifo_queue = queue.LifoQueue() self.priority_queue = queue.PriorityQueue() def add_task(self, queue_type: str, task, priority: int = None): Add a task to the specified queue type. if queue_type == \'fifo\': self.fifo_queue.put(task) elif queue_type == \'lifo\': self.lifo_queue.put(task) elif queue_type == \'priority\': if priority is not None: self.priority_queue.put(PrioritizedItem(priority, task)) else: raise ValueError(\\"Priority must be specified for priority queue.\\") def process_tasks(self, queue_type: str): Simulate worker processing tasks in the specified queue. if queue_type == \'fifo\': q = self.fifo_queue elif queue_type == \'lifo\': q = self.lifo_queue elif queue_type == \'priority\': q = self.priority_queue def worker(): while not q.empty(): try: item = q.get_nowait() if isinstance(item, PrioritizedItem): item = item.item print(f\'Processing {item}\') print(f\'Finished {item}\') q.task_done() except queue.Empty: break thread = threading.Thread(target=worker) thread.start() thread.join()"},{"question":"You are required to design a Python class `CustomMapping`, which mimics the behavior of Python\'s built-in dictionary. In this class, you\'ll need to provide methods that leverage the concepts of the mapping protocol specified in the documentation provided. The class should support typical dictionary operations such as item retrieval, assignment, deletion, and key, value, or item listing. Requirements 1. **Initialization** - The class should be initialized with an optional dictionary input. 2. **Methods to Implement:** - `__getitem__(self, key)`: This method should return the value associated with the key. If the key is not found, raise a `KeyError`. - `__setitem__(self, key, value)`: This method should assign the value to the specified key. - `__delitem__(self, key)`: This method should delete the key-value pair. If the key is not found, raise a `KeyError`. - `__contains__(self, key)`: This method should return `True` if the key is found in the mapping, and `False` otherwise. - `keys(self)`: This method should return a list of all the keys in the mapping. - `values(self)`: This method should return a list of all the values in the mapping. - `items(self)`: This method should return a list of tuples, where each tuple contains a key-value pair from the mapping. - `size(self)`: This method should return the number of key-value pairs in the mapping. Constraints - You may assume all keys are strings. - Your solution should handle standard dictionary operations efficiently. Example Usage ```python # Create an instance of CustomMapping cm = CustomMapping({\\"a\\": 1, \\"b\\": 2}) # Access items print(cm[\\"a\\"]) # Output: 1 # Set items cm[\\"c\\"] = 3 # Check if a key is in the mapping print(\\"b\\" in cm) # Output: True # Get values print(cm.values()) # Output: [1, 2, 3] # Delete items del cm[\\"b\\"] # Get the size print(cm.size()) # Output: 2 ``` Implementing this class properly will demonstrate a solid understanding of mapping protocols and their functionality in Python.","solution":"class CustomMapping: def __init__(self, initial=None): Initialize the CustomMapping with an optional dictionary input. self._data = initial if initial is not None else {} def __getitem__(self, key): Return the value associated with the key. Raise KeyError if the key is not found. if key in self._data: return self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def __setitem__(self, key, value): Assign the value to the specified key. self._data[key] = value def __delitem__(self, key): Delete the key-value pair. Raise KeyError if the key is not found. if key in self._data: del self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def __contains__(self, key): Return True if the key is found in the mapping, and False otherwise. return key in self._data def keys(self): Return a list of all the keys in the mapping. return list(self._data.keys()) def values(self): Return a list of all the values in the mapping. return list(self._data.values()) def items(self): Return a list of tuples, where each tuple contains a key-value pair from the mapping. return list(self._data.items()) def size(self): Return the number of key-value pairs in the mapping. return len(self._data)"},{"question":"Implement a function to compute the per-sample Hessian of a simple quadratic model prediction with respect to the input features. Requirements: 1. Define a simple quadratic model of the form `f(x) = x^2`. 2. Use the `vmap` function to batch the computation of per-sample Hessians. 3. Make use of the `hessian` function to compute the Hessian. # Function Signature ```python import torch from torch.func import vmap, hessian def per_sample_hessian(inputs: torch.Tensor) -> torch.Tensor: Compute the per-sample Hessian of the quadratic function f(x) = x^2. Parameters: - inputs: A 1D Tensor containing a batch of input samples. Returns: - A 2D Tensor where each row contains the Hessian of the corresponding input sample. def quadratic_model(x): return x ** 2 # Your code here return hessians ``` # Instructions: 1. The `inputs` parameter is a 1D Tensor containing a batch of input samples. 2. The function should return a 2D Tensor `hessians` where each row corresponds to the Hessian of each input sample in `inputs`. # Example: ```python inputs = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) result = per_sample_hessian(inputs) print(result) # Expected Output: tensor([[2.0], [2.0], [2.0]]) ``` # Constraints: - You should not use for-loops to iterate over the batch; leverage `vmap` for batching the operations. - Ensure efficient computation by leveraging PyTorch and its function transforms. - Assume `inputs` will always be a valid 1D tensor with gradient computation enabled.","solution":"import torch from torch.func import vmap, hessian def per_sample_hessian(inputs: torch.Tensor) -> torch.Tensor: Compute the per-sample Hessian of the quadratic function f(x) = x^2. Parameters: - inputs: A 1D Tensor containing a batch of input samples. Returns: - A 2D Tensor where each row contains the Hessian of the corresponding input sample. def quadratic_model(x): return x ** 2 # Using vmap to vectorize the hessian computation for each sample in the batch batched_hessian = vmap(hessian(quadratic_model)) # Compute per-sample Hessians hessians = batched_hessian(inputs) return hessians"},{"question":"# Objective To test the understanding of Python\'s `random` module, including its basic functions, sequence operations, and statistical distributions. # Problem Statement You are tasked with simulating the results of a simplified game show. The game show involves: 1. Selecting random contestants from the audience. 2. Performing random shuffles of contestant lists to determine the order of their performance. 3. Assigning random scores to each contestant from a given range using various statistical distributions. 4. Checking the reproducibility of results using seeds. # Detailed Requirements 1. **Random Contestant Selection**: - Implement a function `select_contestants(population, k)` that returns a list of `k` unique contestants chosen randomly from the `population` list. If the number of contestants to select (`k`) is greater than the population size, raise a `ValueError`. - **Input**: - `population`: List of contestant names (strings). - `k`: Number of contestants to select (integer). - **Output**: - List of `k` unique selected contestant names. 2. **Shuffle Contestants**: - Implement a function `shuffle_contestants(contestants)` that takes a list of contestant names and shuffles the order in-place. - **Input**: - `contestants`: List of contestant names (strings). - **Output**: - Shuffled list of contestant names. (The original list is modified.) 3. **Assign Random Scores**: - Implement a function `assign_scores(contestants, a, b)` that assigns random scores to each contestant using a uniform distribution between `a` and `b` (inclusive). - **Input**: - `contestants`: List of contestant names (strings). - `a`: Lower bound of score range (integer). - `b`: Upper bound of score range (integer). - **Output**: - Dictionary where keys are contestant names and values are their assigned scores (integers). 4. **Reproducible Randomness**: - Implement a function `generate_reproducible_results(seed, population, k, a, b)` that simulates the game show process in a reproducible manner. - This function should: - Initialize the random number generator with the given `seed`. - Select `k` unique contestants from the `population`. - Shuffle the selected contestants. - Assign random scores to each contestant. - **Input**: - `seed`: Seed value for random number generator (integer). - `population`: List of contestant names (strings). - `k`: Number of contestants to select (integer). - `a`: Lower bound of score range (integer). - `b`: Upper bound of score range (integer). - **Output**: - Dictionary where keys are contestant names and values are their assigned scores (integers). # Constraints - The population list will have a length between 1 and 10000. - The values of `k`, `a`, and `b` will be such that `1 <= k <= len(population)` and `a < b`. - All contestant names will be strings with a maximum length of 50 characters. # Performance Requirements - The implementation should handle the maximum input sizes efficiently. - Ensure that the random number generation and shuffling operations are performed using the functions from the `random` module described in the documentation. # Example ```python population = [\\"Alice\\", \\"Bob\\", \\"Carol\\", \\"Dave\\", \\"Eve\\"] k = 3 a = 0 b = 100 seed = 42 print(select_contestants(population, k)) # Possible output: [\'Alice\', \'Eve\', \'Carol\'] contestants = select_contestants(population, k) shuffle_contestants(contestants) print(contestants) # Possible output: [\'Carol\', \'Alice\', \'Eve\'] print(assign_scores(contestants, a, b)) # Possible output: {\'Carol\': 66, \'Alice\': 49, \'Eve\': 85} print(generate_reproducible_results(seed, population, k, a, b)) # Possible output: {\'Carol\': 66, \'Alice\': 49, \'Eve\': 85} ``` # Notes - Ensure to handle potential exceptions where the number of contestants requested exceeds the population size. - Utilize functions from the `random` module to implement shuffling and random number generation to match expected behaviors and constraints.","solution":"import random def select_contestants(population, k): Returns a list of k unique contestants chosen randomly from the population list. Raises a ValueError if k is greater than the population size. if k > len(population): raise ValueError(\\"Number of contestants to select cannot exceed the population size\\") return random.sample(population, k) def shuffle_contestants(contestants): Shuffles the order of contestants in-place. random.shuffle(contestants) def assign_scores(contestants, a, b): Assigns random scores to each contestant using a uniform distribution between a and b (inclusive). Returns a dictionary with contestant names as keys and their scores as values. return {contestant: random.randint(a, b) for contestant in contestants} def generate_reproducible_results(seed, population, k, a, b): Simulates the game show process in a reproducible manner using the given seed. Selects k unique contestants, shuffles them, and assigns random scores. Returns a dictionary with contestant names as keys and their scores as values. random.seed(seed) selected_contestants = select_contestants(population, k) shuffle_contestants(selected_contestants) return assign_scores(selected_contestants, a, b)"},{"question":"# PyTorch Distributed Training with NCCL Configuration **Objective**: You are asked to design a distributed training script using PyTorch that utilizes NCCL for communication between processes. The script should allow configuration of NCCL-related environment variables and run a simple distributed training loop. **Requirements**: 1. The script should accept a dictionary as input where keys represent NCCL environment variables and values represent their respective configurations. 2. Implement a basic neural network and a simple training loop. 3. Configure the NCCL environment variables as specified by the input dictionary. 4. The script should run the distributed training using NCCL as the backend. **Expected Input**: - `config`: A dictionary where keys are NCCL environment variables and values are their settings. Example: ```python config = { \\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\": \\"1\\", \\"TORCH_NCCL_HIGH_PRIORITY\\": \\"1\\", \\"TORCH_NCCL_ENABLE_TIMING\\": \\"1\\" } ``` **Output**: - The script should print log messages indicating the status of the distributed training process, including any debug or error messages triggered by the NCCL configurations. **Constraints**: - Ensure proper error handling and cleanup. - Assume you have at least two GPUs for the distributed setup. - The focus should be on integrating the NCCL configurations into the training process. **Function Signature**: ```python def distributed_training(config: dict): pass ``` # Example Function Usage: ```python def distributed_training(config): import os import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP import torch.nn as nn import torch.optim as optim # Set NCCL environment variables for key, value in config.items(): os.environ[key] = value # Helper function to handle distributed setup def setup(rank, world_size): dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) # A simple model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Training loop def train(rank, world_size): setup(rank, world_size) model = Net().cuda(rank) ddp_model = DDP(model, device_ids=[rank]) criterion = nn.MSELoss().cuda(rank) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): inputs = torch.randn(20, 10).cuda(rank) target = torch.randn(20, 10).cuda(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() def main(): world_size = 2 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` Implement the function `distributed_training(config: dict)` to meet the requirements outlined above.","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP import torch.nn as nn import torch.optim as optim def distributed_training(config: dict): Configures NCCL parameters and runs a distributed training example # Set NCCL environment variables for key, value in config.items(): os.environ[key] = value def setup(rank, world_size): dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) # A simple model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Training loop def train(rank, world_size): setup(rank, world_size) model = Net().cuda(rank) ddp_model = DDP(model, device_ids=[rank]) criterion = nn.MSELoss().cuda(rank) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): inputs = torch.randn(20, 10).cuda(rank) target = torch.randn(20, 10).cuda(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() def main(): world_size = 2 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"# Objective: Design a comprehensive cluster map using seaborn that incorporates multiple customization features to display complex data relationships in a visually informative manner. # Problem Statement: You are given a dataset `df` which contains numerical data with several features. Your task is to create a customized cluster map that meets the following criteria: 1. **Data Preprocessing:** - Assume `df` is a pandas DataFrame loaded from a CSV file. 2. **Cluster Map Requirements:** - Create a cluster map of `df` using seaborn. - Customize the figure size to be 10 inches in width and 8 inches in height. - Use `metric=\\"correlation\\"` and `method=\\"average\\"` for clustering. - Set the colormap to `coolwarm` and the color range limits (`vmin` and `vmax`) should be the minimum and maximum of `df` respectively. - Add colored labels to identify observations. Assume `df` has a categorical column named `category` that you will pop out before plotting. The `category` column should map unique values to different colors. # Constraints: - The input dataframe `df` will have at least 100 observations and 10 features. - The `category` column will have at most 5 unique values. # Input: - A pandas DataFrame `df` with at least 100 rows and 10 columns where one of the columns is a categorical column named `category`. # Output: - A displayed seaborn cluster map plot meeting the specifications. # Example: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Sample DataFrame (In reality, this would be read from a CSV file) data = { \'feature1\': [1.2, 3.4, 2.3, 4.5], \'feature2\': [3.4, 1.2, 4.5, 3.3], \'feature3\': [2.1, 3.4, 2.5, 2.2], \'category\': [\'A\', \'B\', \'A\', \'C\'] } df = pd.DataFrame(data) # Your solution here def create_custom_clustermap(df): sns.set_theme() # Pop the \'category\' column category = df.pop(\'category\') # Map the category column to colors lut = dict(zip(category.unique(), sns.color_palette(\\"husl\\", len(category.unique())))) row_colors = category.map(lut) # Create clustermap sns.clustermap( df, figsize=(10, 8), row_colors=row_colors, metric=\\"correlation\\", method=\\"average\\", cmap=\\"coolwarm\\", vmin=df.min().min(), vmax=df.max().max() ) plt.show() create_custom_clustermap(df) ``` You need to adapt this example to handle larger data and ensure it satisfies all requirements outlined.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_clustermap(df): sns.set_theme() # Pop the \'category\' column category = df.pop(\'category\') # Map the category column to colors lut = dict(zip(category.unique(), sns.color_palette(\\"husl\\", len(category.unique())))) row_colors = category.map(lut) # Create clustermap sns.clustermap( df, figsize=(10, 8), row_colors=row_colors, metric=\\"correlation\\", method=\\"average\\", cmap=\\"coolwarm\\", vmin=df.min().min(), vmax=df.max().max() ) plt.show()"},{"question":"# Secure Data Transmission and Storage In this task, you are required to implement two functions in Python to demonstrate secure data handling practices. Your implementation should consider the outlined security considerations and aim to mitigate potential vulnerabilities. Function 1: `secure_data_transmission(data: str) -> str` - **Objective**: Safely encode a string into base64 format. - **Input**: A single string `data` representing the data to be encoded. - **Output**: A base64 encoded string. Function 2: `secure_data_storage(data: str, filename: str) -> None` - **Objective**: Store the given string securely in a file. - **Input**: - `data`: The data string to be stored. - `filename`: The name of the file where the data should be stored. - **Output**: This function should not return any value, but the file should be created and saved with the encoded data. Constraints and Considerations - For `secure_data_transmission`, use the `base64` module, as it is generally safe for encoding and decoding in transmission but be mindful of its security considerations. - For storing data securely, ensure that the data is not directly in plain text. Consider using encoding or hashing for this purpose. - You should not use insecure algorithms or methods explicitly mentioned in the document, e.g., avoid `pickle` due to its risks. # Example Usage ```python # Example usage of secure_data_transmission encoded_string = secure_data_transmission(\\"sensitive data\\") print(encoded_string) # Should print base64 encoded string # Example usage of secure_data_storage secure_data_storage(encoded_string, \\"secure_data.txt\\") # On checking the file \'secure_data.txt\', it should contain the encoded data. ``` **Note**: Ensure your code is well-documented and follows best practices for secure programming.","solution":"import base64 def secure_data_transmission(data: str) -> str: Encodes the input string data into base64 format. Args: data (str): The data to be encoded Returns: str: Base64 encoded string # Convert string data to bytes bytes_data = data.encode(\'utf-8\') # Encode to base64 base64_encoded = base64.b64encode(bytes_data) # Convert encoded bytes back to string and return return base64_encoded.decode(\'utf-8\') def secure_data_storage(data: str, filename: str) -> None: Stores the given string securely encodes in a file. Args: data (str): The data string to be stored. filename (str): The name of the file where the data should be stored. Returns: None # Ensure data is stored in encoded format for additional security encoded_data = secure_data_transmission(data) # Writing encoded data to file with open(filename, \'w\') as file: file.write(encoded_data)"},{"question":"# PyTorch Coding Assessment: Numerical Comparison of Tensors **Context:** You are given access to a few utility functions from the `torch.ao.ns.fx.utils` module. These functions are used to compute various error metrics between tensors. Your task is to implement a more comprehensive tool that will utilize these functions to provide a summary of comparison metrics between two sets of tensors. **Functions Available:** 1. `torch.ao.ns.fx.utils.compute_sqnr(x, y)`: Computes the Signal to Quantization Noise Ratio (SQNR) between two tensors. 2. `torch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)`: Computes the normalized L2 error between two tensors. 3. `torch.ao.ns.fx.utils.compute_cosine_similarity(x, y)`: Computes the cosine similarity between two tensors. **Task:** Implement a function `tensor_comparison_summary(tensors_a, tensors_b)` that accepts two lists of tensors (`tensors_a` and `tensors_b`). Each tensor in `tensors_a` should be compared to the corresponding tensor in `tensors_b` (i.e., tensors at the same index in each list). For each tensor pair `(a, b)`, compute the following: - SQNR - Normalized L2 Error - Cosine Similarity The function should return a summary dictionary where the keys are the indices of the tensors and the values are dictionaries containing the computed metrics for each pair. **Function Signature:** ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def tensor_comparison_summary(tensors_a: [torch.Tensor], tensors_b: [torch.Tensor]) -> dict: pass ``` **Input:** - `tensors_a`: List of `n` tensors. - `tensors_b`: List of `n` tensors. Both lists will have the same length `n`, and tensors at each corresponding index will have the same shape. **Output:** - A dictionary summarizing the comparison metrics. For example: ```python { 0: { \\"SQNR\\": <value>, \\"Normalized_L2_Error\\": <value>, \\"Cosine_Similarity\\": <value> }, 1: { \\"SQNR\\": <value>, \\"Normalized_L2_Error\\": <value>, \\"Cosine_Similarity\\": <value> }, ... n-1: { \\"SQNR\\": <value>, \\"Normalized_L2_Error\\": <value>, \\"Cosine_Similarity\\": <value> } } ``` **Constraints:** - Lists `tensors_a` and `tensors_b` will not be empty and will contain at least one tensor each. - The tensors will be on the same device (either all on CPU or all on GPU). **Example:** ```python import torch # Example tensors tensors_a = [torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0, 6.0])] tensors_b = [torch.tensor([1.1, 1.9, 3.1]), torch.tensor([4.1, 4.9, 6.1])] result = tensor_comparison_summary(tensors_a, tensors_b) expected_result = { 0: { \\"SQNR\\": <computed_value>, \\"Normalized_L2_Error\\": <computed_value>, \\"Cosine_Similarity\\": <computed_value> }, 1: { \\"SQNR\\": <computed_value>, \\"Normalized_L2_Error\\": <computed_value>, \\"Cosine_Similarity\\": <computed_value> } } ``` **Note:** Replace `<computed_value>` with the actual computed values. **Important:** This question assumes you have the appropriate version of PyTorch installed and that these utility functions are available for import.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def tensor_comparison_summary(tensors_a, tensors_b): Computes SQNR, Normalized L2 Error, and Cosine Similarity for pairs of tensors. Args: tensors_a (list of torch.Tensor): List of tensors. tensors_b (list of torch.Tensor): List of tensors. Returns: dict: Dictionary of comparison metrics for each pair of tensors. summary = {} for idx, (a, b) in enumerate(zip(tensors_a, tensors_b)): summary[idx] = { \\"SQNR\\": compute_sqnr(a, b), \\"Normalized_L2_Error\\": compute_normalized_l2_error(a, b), \\"Cosine_Similarity\\": compute_cosine_similarity(a, b) } return summary"},{"question":"# SAX XML Parsing in Python You are required to write a Python function that parses XML data using the SAX (Simple API for XML) interface provided by the `xml.sax` package. Your function should read an XML file, process its contents according to specific rules, and handle any errors that occur during parsing. Requirements: 1. **Function Signature**: ```python def process_xml(file_name: str) -> dict: pass ``` 2. **Input**: - `file_name`: A string representing the path to an XML file. 3. **Output**: - A dictionary containing two keys: `\\"elements\\"` and `\\"errors\\"`. - `\\"elements\\"`: A list of strings representing the names of all XML elements in the order they were encountered. - `\\"errors\\"`: A list of error messages encountered during the parsing process. 4. **Functionality**: - Implement a SAX `ContentHandler` to handle the XML elements. Your handler should collect the names of the start elements and store them in the `\\"elements\\"` list. - Implement a SAX `ErrorHandler` to handle any parsing errors. Error messages should be stored in the `\\"errors\\"` list. - Use `xml.sax.make_parser` to create the parser and associate your custom handlers with it. - Parse the XML data using the parser\'s `parse` method. 5. **Constraints**: - The XML file to be parsed can be assumed to be well-formed, but may contain structural errors that the SAX parser should handle gracefully. - You should not enable processing of external entities due to security concerns. Example Usage: Assuming you have an XML file `sample.xml` with the following content: ```xml <root> <child>Text</child> <childWithError> </root> ``` Calling your function `process_xml(\\"sample.xml\\")` should return a dictionary similar to: ```python { \\"elements\\": [\\"root\\", \\"child\\", \\"childWithError\\"], \\"errors\\": [\\"<unknown>:4:13: mismatched tag\\"] } ``` Here, `\\"elements\\"` contains a list of all the element names in the order they were processed, and `\\"errors\\"` contains a list of error messages encountered during parsing. Performance Requirements: - Your function should handle XML data efficiently and be capable of parsing files up to 10MB in a reasonable timeframe. Use the SAX approach to demonstrate your proficiency in handling XML parsing in a secure and efficient manner. Ensure to handle all potential exceptions and errors gracefully.","solution":"import xml.sax class MyContentHandler(xml.sax.ContentHandler): def __init__(self): self.elements = [] def startElement(self, name, attrs): self.elements.append(name) class MyErrorHandler(xml.sax.ErrorHandler): def __init__(self): self.errors = [] def error(self, exception): self.errors.append(str(exception)) def fatalError(self, exception): self.errors.append(str(exception)) def warning(self, exception): self.errors.append(str(exception)) def process_xml(file_name: str) -> dict: parser = xml.sax.make_parser() parser.setFeature(xml.sax.handler.feature_external_ges, False) content_handler = MyContentHandler() error_handler = MyErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) try: parser.parse(file_name) except xml.sax.SAXParseException as e: error_handler.fatalError(e) return { \\"elements\\": content_handler.elements, \\"errors\\": error_handler.errors }"},{"question":"# Configuration File Parser Challenge Problem Statement You are tasked with writing a Python function that reads and processes a configuration file in INI format. The function should read specific sections and extract configurations into a dictionary. You should utilize the `configparser` module for this task. Requirements 1. Write a function `parse_config(file_path: str, sections: list) -> dict` that takes: - `file_path`: A string representing the path to the configuration file. - `sections`: A list of strings representing the sections of the INI file to read. 2. The function should return a dictionary where: - The keys are the section names. - The values are dictionaries containing key-value pairs of the configurations in each section. 3. Handle the following cases: - If a section does not exist, skip it and continue. - Allow for variable interpolation within the INI file using the default configparser behavior. Constraints - Do not use any other external libraries. - Assume the configuration file is well-formed. - You can make use of `configparser`\'s interpolation feature. Example Given an INI file `example.ini`: ```ini [database] user = dbuser password = dbpass host = 192.168.1.10 port = 5432 [server] hostname = localhost port = 8080 ``` And calling the function with: ```python sections = [\\"database\\", \\"server\\"] result = parse_config(\\"example.ini\\", sections) ``` The expected output would be: ```python { \\"database\\": { \\"user\\": \\"dbuser\\", \\"password\\": \\"dbpass\\", \\"host\\": \\"192.168.1.10\\", \\"port\\": \\"5432\\" }, \\"server\\": { \\"hostname\\": \\"localhost\\", \\"port\\": \\"8080\\" } } ``` If a section listed in `sections` does not exist in the config file, that section should be skipped and not appear in the output dictionary. Additional Information - The `configparser` module should be used for this task. - Handle any potential exceptions that arise from file handling or config parsing gracefully. Good luck!","solution":"import configparser def parse_config(file_path: str, sections: list) -> dict: Parses the given INI file and extracts specific sections into a dictionary. param file_path: str - The path to the configuration file. param sections: list - The list of sections to extract. return: dict - A dictionary of the specified sections and their key-value pairs. config = configparser.ConfigParser() config.read(file_path) result = {} for section in sections: if section in config: result[section] = dict(config[section]) return result # Example usage: # sections = [\\"database\\", \\"server\\"] # result = parse_config(\\"example.ini\\", sections)"},{"question":"You are required to demonstrate your understanding of the seaborn package, specifically focusing on creating and utilizing color palettes using the `sns.blend_palette` function. Your task is to create and display a visualization that meets the following requirements: Requirements: 1. **Data Loading:** - Load the `tips` dataset that comes pre-installed with seaborn using `sns.load_dataset(\\"tips\\")`. 2. **Palette Creation:** - Create three color palettes using the `sns.blend_palette` function: 1. A palette interpolating between two colors of your choice. 2. A palette interpolating between three colors of your choice. 3. A continuous colormap interpolating between at least three colors of your choice. 3. **Visualizations:** - Create three different plots using the `tips` dataset where each plot uses one of the palettes created in the previous step. Specifically: 1. A scatter plot using the palette with two colors. 2. A bar plot using the palette with three colors. 3. A heatmap using the continuous colormap. 4. **Customization:** - Customize the aesthetics of your plots by setting: - Titles for each plot. - Appropriate axis labels. - A theme of your choice using `sns.set_theme()`. Input: No input is required other than data loading and palette specification (within your code). Output: The output should be the three generated seaborn plots saved as PNG files: 1. `scatter_plot.png` 2. `bar_plot.png` 3. `heatmap.png` Constraints: - Ensure the colors used in the palettes are distinct and visually pleasant. - Pay attention to the clarity and readability of the plots. Example: Here is how you can start with the scatter plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme() # Create a palette interpolating between two colors (example) palette1 = sns.blend_palette([\\"#2ecc71\\", \\"#e74c3c\\"]) # Create scatter plot using the first palette sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", palette=palette1) # Customization plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatter_plot.png\\") plt.show() ``` Similarly, create a bar plot and heatmap using the other two palettes and save them as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme() # Create color palettes using sns.blend_palette palette1 = sns.blend_palette([\\"#2ecc71\\", \\"#e74c3c\\"]) # Interpolating between two colors palette2 = sns.blend_palette([\\"#3498db\\", \\"#e74c3c\\", \\"#9b59b6\\"]) # Interpolating between three colors palette3 = sns.blend_palette([\\"#1abc9c\\", \\"#f1c40f\\", \\"#e74c3c\\", \\"#8e44ad\\"], as_cmap=True) # Continuous colormap # Scatter plot using the first palette plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", palette=palette1) plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"scatter_plot.png\\") plt.show() # Bar plot using the second palette plt.figure(figsize=(10, 6)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", palette=palette2) plt.title(\\"Bar Plot of Total Bill by Day and Sex\\") plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\'Sex\') plt.savefig(\\"bar_plot.png\\") plt.show() # Heatmap using the third palette pivot_table = tips.pivot_table(index=\'time\', columns=\'day\', values=\'total_bill\', aggfunc=\'mean\') plt.figure(figsize=(10, 6)) sns.heatmap(pivot_table, cmap=palette3, annot=True) plt.title(\\"Heatmap of Average Total Bill by Day and Time\\") plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Time of Day\\") plt.savefig(\\"heatmap.png\\") plt.show()"},{"question":"# Question: Creating a Python Extension Using the Limited API **Objective:** Design a Python extension module using Python\'s C Limited API. The module should include functions to manipulate and analyze lists, strings, and numerical values. This will require the use of various functions and types provided by the Limited API. **Requirements:** 1. **Function Implementation:** Implement three functions within the extension: - `list_reverse(PyObject *self, PyObject *args)`: Reverses a list. - `string_upper(PyObject *self, PyObject *args)`: Converts a string to uppercase. - `sum_numbers(PyObject *self, PyObject *args)`: Sums a list of numbers. 2. **Input and Output Formats:** - `list_reverse`: - Input: A Python list of any objects. - Output: A Python list with elements in reverse order. - `string_upper`: - Input: A Python string. - Output: A Python string in uppercase. - `sum_numbers`: - Input: A Python list of numerical values. - Output: A single numerical value which is the sum of the input list. 3. **Constraints:** - You must define `Py_LIMITED_API` to the value of `PY_VERSION_HEX` for compatibility. - Ensure that the extension does not use any functions or macros outside the Limited API list provided in the documentation. 4. **Performance Requirements:** - The implementation should handle typical use cases efficiently. **Instructions:** 1. Create a C source file for the Python extension module. 2. Include the necessary headers and define `Py_LIMITED_API`. 3. Implement the functions using the provided Limited API. 4. Create the module definition and initialization functions. 5. Write a Python script to test the module for correctness. **Example Code for Reference:** ```c #define Py_LIMITED_API 0x030A0000 #include <Python.h> // Reverse a list static PyObject* list_reverse(PyObject* self, PyObject* args) { PyObject* input_list; if (!PyArg_ParseTuple(args, \\"O!\\", &PyList_Type, &input_list)) { return NULL; } PyObject* reversed_list = PyList_New(PyList_Size(input_list)); for (Py_ssize_t i = 0; i < PyList_Size(input_list); i++) { PyObject* item = PyList_GetItem(input_list, i); Py_INCREF(item); PyList_SetItem(reversed_list, PyList_Size(input_list) - 1 - i, item); } return reversed_list; } // Convert a string to uppercase static PyObject* string_upper(PyObject* self, PyObject* args) { const char* input_string; if (!PyArg_ParseTuple(args, \\"s\\", &input_string)) { return NULL; } PyObject* py_string = PyUnicode_FromString(input_string); return PyUnicode_ToUpper(py_string); } // Sum a list of numbers static PyObject* sum_numbers(PyObject* self, PyObject* args) { PyObject* input_list; if (!PyArg_ParseTuple(args, \\"O!\\", &PyList_Type, &input_list)) { return NULL; } double sum = 0.0; for (Py_ssize_t i = 0; i < PyList_Size(input_list); i++) { PyObject* item = PyList_GetItem(input_list, i); sum += PyFloat_AsDouble(item); } return PyFloat_FromDouble(sum); } // Module method definitions static PyMethodDef MyMethods[] = { {\\"list_reverse\\", list_reverse, METH_VARARGS, \\"Reverse a list\\"}, {\\"string_upper\\", string_upper, METH_VARARGS, \\"Convert string to uppercase\\"}, {\\"sum_numbers\\", sum_numbers, METH_VARARGS, \\"Sum a list of numbers\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, -1, MyMethods }; // Module initialization PyMODINIT_FUNC PyInit_mymodule(void) { return PyModule_Create(&mymodule); } ``` **Python Test Script:** ```python import mymodule def test_list_reverse(): assert mymodule.list_reverse([1, 2, 3]) == [3, 2, 1] def test_string_upper(): assert mymodule.string_upper(\\"hello\\") == \\"HELLO\\" def test_sum_numbers(): assert mymodule.sum_numbers([1, 2, 3.5]) == 6.5 test_list_reverse() test_string_upper() test_sum_numbers() print(\\"All tests passed!\\") ``` **Note:** Ensure you have a working C compiler and the required Python development tools to compile the extension module.","solution":"def list_reverse(lst): Reverses the given list. return lst[::-1] def string_upper(s): Converts the given string to uppercase. return s.upper() def sum_numbers(nums): Sums a list of numbers. return sum(nums)"},{"question":"You are tasked with implementing a function that performs a series of tensor operations and then validates the result using the `torch.testing` module. Specifically, you will create a function that takes two input tensors, performs element-wise multiplication, followed by summation along a specific dimension, and verifies the results. # Function Signature ```python def validate_tensor_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: Performs element-wise multiplication between two tensors, sums the result along a specific dimension, and validates the result. Args: tensor1 (torch.Tensor): First input tensor. tensor2 (torch.Tensor): Second input tensor. Returns: bool: True if the output is as expected, False otherwise. ``` # Instructions 1. **Input Tensors** - Both `tensor1` and `tensor2` will be 2-dimensional tensors of the same shape. 2. **Element-wise Multiplication** - Perform element-wise multiplication on the input tensors. 3. **Summation** - Sum the resultant tensor along dimension 1. 4. **Validation** - Validate the result using `torch.testing.assert_close` by comparing it to an expected tensor. - If the tensors are close, return `True`, otherwise return `False`. 5. **Expected Tensor** - The expected tensor should be computed separately and then passed into the function for verification. # Example Suppose `tensor1` and `tensor2` are: ```python tensor1 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) tensor2 = torch.tensor([[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]) ``` - Perform element-wise multiplication: ```python result = tensor1 * tensor2 # tensor([[ 7.0, 16.0, 27.0], [40.0, 55.0, 72.0]]) ``` - Sum along dimension 1: ```python summed_result = torch.sum(result, dim=1) # tensor([50.0, 167.0]) ``` - Validate the result with an expected tensor, e.g., `expected = torch.tensor([50.0, 167.0])`. **Note:** You might need to make use of `torch.testing.make_tensor` for generating the test tensors in your validation checks. # Constraints - Ensure tensors have the same dtype and shape. - Use `torch.testing.assert_close` to compare tensors. - Provide a detailed error message if the assertion fails. Implement this function within the constraints and demonstrate it with at least one example.","solution":"import torch import torch.testing def validate_tensor_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, expected: torch.Tensor) -> bool: Performs element-wise multiplication between two tensors, sums the result along a specific dimension, and validates the result with an expected tensor. Args: tensor1 (torch.Tensor): First input tensor. tensor2 (torch.Tensor): Second input tensor. expected (torch.Tensor): The expected tensor for validation. Returns: bool: True if the output is as expected, False otherwise. try: # Ensure both inputs are 2-dimensional and of the same shape assert tensor1.dim() == 2 and tensor2.dim() == 2, \\"Both tensors must be 2-dimensional.\\" assert tensor1.shape == tensor2.shape, \\"Tensors must have the same shape.\\" # Perform element-wise multiplication result = tensor1 * tensor2 # Sum along dimension 1 summed_result = torch.sum(result, dim=1) # Validate the result torch.testing.assert_close(summed_result, expected) return True except AssertionError as e: print(f\\"Validation failed: {e}\\") return False"},{"question":"Objective Implement a multi-threaded producer-consumer system using the `queue.Queue` class from the `queue` module. This task will assess your understanding of Python\'s threading and queue mechanisms. Task 1. Create a class `ProducerConsumer` that initializes with the following parameters: - `num_producers` (int): The number of producer threads. - `num_consumers` (int): The number of consumer threads. - `max_queue_size` (int): The maximum size of the queue. 2. The class should contain methods: - `producer(id: int)`: This method, when called by a producer thread, should produce an item, print the producer\'s ID and the item, and put the item into the queue. - `consumer(id: int)`: This method, when called by a consumer thread, should get an item from the queue, print the consumer\'s ID and the item, and simulate processing the item. - `start_processing(items_to_produce_per_producer: int)`: This method should start all producer and consumer threads, where each producer thread produces the specified number of items, and all items must be processed by the consumer threads. 3. Ensure appropriate synchronization to prevent race conditions. Implementation Point 1. You can assume that each produce-consume operation is instantaneous and doesn\'t require complex synchronization beyond queuing. 2. Use the `queue.Queue` class for the queue implementation. 3. You can use the `threading` module to manage threads. Input - `num_producers`, `num_consumers`, and `max_queue_size` are integers. - `items_to_produce_per_producer` is an integer. Output The output should print statements from producer and consumer threads indicating their operations. Example ```python import threading import queue import time class ProducerConsumer: def __init__(self, num_producers, num_consumers, max_queue_size): self.queue = queue.Queue(maxsize=max_queue_size) self.num_producers = num_producers self.num_consumers = num_consumers self.threads = [] def producer(self, id): for i in range(10): item = f\'Item-{i}-from-Producer-{id}\' print(f\'Producer {id}: Producing {item}\') self.queue.put(item) time.sleep(0.1) def consumer(self, id): while True: item = self.queue.get() print(f\'Consumer {id}: Consuming {item}\') self.queue.task_done() time.sleep(0.2) def start_processing(self, items_to_produce_per_producer): for i in range(self.num_producers): t = threading.Thread(target=self.producer, args=(i,)) t.start() self.threads.append(t) for i in range(self.num_consumers): t = threading.Thread(target=self.consumer, args=(i,)) t.daemon = True t.start() self.threads.append(t) for t in self.threads: t.join() if __name__ == \\"__main__\\": pc = ProducerConsumer(2, 2, 10) pc.start_processing(10) ``` Explanation In this example, the `ProducerConsumer` class is instantiated with 2 producer threads, 2 consumer threads, and a maximum queue size of 10. The `start_processing` method is called to start the threads and produce/consume items accordingly.","solution":"import threading import queue import time class ProducerConsumer: def __init__(self, num_producers, num_consumers, max_queue_size): self.queue = queue.Queue(maxsize=max_queue_size) self.num_producers = num_producers self.num_consumers = num_consumers self.threads = [] def producer(self, id, items_to_produce_per_producer): for i in range(items_to_produce_per_producer): item = f\'Item-{i}-from-Producer-{id}\' print(f\'Producer {id}: Producing {item}\') self.queue.put(item) time.sleep(0.1) # Simulate production time def consumer(self, id): while True: item = self.queue.get() if item is None: break # Exit signal print(f\'Consumer {id}: Consuming {item}\') self.queue.task_done() time.sleep(0.2) # Simulate consumption time def start_processing(self, items_to_produce_per_producer): for i in range(self.num_producers): t = threading.Thread(target=self.producer, args=(i, items_to_produce_per_producer)) t.start() self.threads.append(t) for i in range(self.num_consumers): t = threading.Thread(target=self.consumer, args=(i,)) t.setDaemon(True) t.start() self.threads.append(t) for t in self.threads[:self.num_producers]: t.join() for _ in range(self.num_consumers): self.queue.put(None) # Send exit signal to consumers for t in self.threads[self.num_producers:]: t.join()"},{"question":"**Question**: You have been hired to develop a task scheduler for a company\'s event management system. The system needs to schedule various tasks and execute them at their designated times based on specific priorities. Implement a function `schedule_tasks(tasks)` which uses the `sched` module to manage and execute these tasks. # Function Signature: ```python def schedule_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]]) -> List[Tuple[float, str]]: pass ``` # Input: - `tasks` : A list of tasks where each task is represented as a tuple: - `time` (float): The time at which the task should be executed (absolute time, similar to `time.time()` output). - `priority` (int): The priority of the task (lower number means higher priority). - `action` (Callable): The function to be executed as the task. - `argument` (Tuple): Positional arguments for the `action` function. - `kwargs` (Dict): Keyword arguments for the `action` function. # Output: - A list of tuples where each tuple contains: - The time at which the task was executed. - The result of the task (the return value of the `action` function). # Constraints: 1. Ensure that all tasks are executed at their designated times in the correct order of priority. 2. If two tasks have the same time and priority, they should be executed in the order they were scheduled. # Example: ```python import time # Sample task function def task_name(name): return f\\"Task executed: {name}\\" tasks = [ (time.time() + 5, 1, task_name, (\'Task 1\',), {}), (time.time() + 3, 2, task_name, (\'Task 2\',), {}), (time.time() + 1, 1, task_name, (\'Task 3\',), {}), ] result = schedule_tasks(tasks) print(result) ``` **Expected Output** (The actual output may vary depending on the exact timing): ``` [ (1652342830.3640375, \'Task executed: Task 3\'), (1652342833.3694863, \'Task executed: Task 2\'), (1652342835.369612, \'Task executed: Task 1\') ] ``` # Note: - Use the `time.time` for absolute time and `time.sleep` for delays. - You may need to manage the blocking/non-blocking behavior of `scheduler.run` appropriately to ensure all tasks are executed, especially for testing purposes. # Performance Requirements: - The solution should handle up to 10,000 tasks efficiently. # Hints: 1. Make sure to utilize `sched.scheduler` appropriately for scheduling tasks. 2. Consider how `scheduler.run()` handles the execution of events and how to capture their timings and results. 3. Pay attention to multi-threading behavior if needed to ensure the scheduler runs as expected.","solution":"import sched import time from typing import List, Tuple, Callable, Dict def schedule_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]]) -> List[Tuple[float, str]]: # Initialize the scheduler scheduler = sched.scheduler(time.time, time.sleep) result = [] def record_task_execution(execution_time, task_func, args, kwargs): result_time = time.time() result_value = task_func(*args, **kwargs) result.append((result_time, result_value)) # Schedule all tasks in the scheduler for task in tasks: execution_time, priority, action, args, kwargs = task scheduler.enterabs(execution_time, priority, record_task_execution, (execution_time, action, args, kwargs)) # Run the scheduler to execute all tasks scheduler.run() return result"},{"question":"# Seaborn Plotting Context Challenge **Objective:** You are required to leverage the Seaborn `plotting_context` functionality to visualize data from three different perspectives within a single plotting environment. The task will assess your understanding of how to dynamically switch contexts to adjust plot aesthetics such as font size and line thickness. **Data:** You will be provided with a simple dataset to demonstrate the application: ```python data = {\'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25]} ``` **Instructions:** 1. Create a function `plot_with_contexts(data, contexts)` where: - `data` is a dictionary containing the data to plot. - `contexts` is a list of three predefined Seaborn plotting contexts (e.g., `[\\"paper\\", \\"talk\\", \\"poster\\"]`). 2. Within this function, create a single figure with three plots arranged vertically (using matplotlib subplots) where: - Each subplot uses one of the specified plotting contexts. - Each subplot visualizes the data using a bar plot. 3. Generate and save the final visualization as `context_plot.png`. **Constraints:** - You must use the contexts dynamically within the plotting function. - Use Seaborn for creating the plots and Matplotlib for arranging them. **Performance:** - Ensure the code is efficient and clear, as it may be evaluated on execution time for larger datasets. **Example Usage:** ```python data = {\'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25]} contexts = [\\"paper\\", \\"talk\\", \\"poster\\"] plot_with_contexts(data, contexts) ``` **Hint:** - Refer to the Seaborn documentation to understand various predefined contexts and their visual differences.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_with_contexts(data, contexts): Creates a figure with three subplots, each using a different Seaborn plotting context. categories = data[\'Category\'] values = data[\'Values\'] # Create figure and subplots fig, axes = plt.subplots(len(contexts), 1, figsize=(8, 12)) # Iterate over each context and corresponding axis to plot the data for i, context in enumerate(contexts): sns.set_context(context) sns.barplot(x=categories, y=values, ax=axes[i]) axes[i].set_title(f\\"Context: {context}\\") # Save the figure plt.tight_layout() plt.savefig(\'context_plot.png\') plt.close() # Example usage data = {\'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25]} contexts = [\\"paper\\", \\"talk\\", \\"poster\\"] plot_with_contexts(data, contexts)"},{"question":"# Question: Robust File Processor You are required to implement a function `process_file(file_path: str) -> dict` that reads and processes a simple text file containing key-value pairs, one per line, in the format `key=value`. Your implementation must handle various potential errors gracefully and ensure that resources are properly managed. Function Signature ```python def process_file(file_path: str) -> dict: pass ``` Input: - `file_path` (str): A string representing the path to the input text file. Output: - (dict): A dictionary containing all key-value pairs from the file. Constraints: 1. Each line in the file will be in the format `key=value`. 2. If a line in the file does not conform to this format, the function should raise a `ValueError` with the message `\\"Invalid line format\\"`. 3. If the file does not exist, the function should raise a `FileNotFoundError` with the message `\\"File not found\\"`. 4. If the file cannot be read due to permission errors or other I/O errors, it should raise an `OSError` with the appropriate error message. 5. Use a custom exception `ProcessingError` which inherits from `Exception` to catch any other unforeseen errors that might occur during processing. The function should read the file and construct a dictionary where keys and values are appropriately stripped of surrounding whitespace. Ensure that the file is always closed after processing, even if an exception occurs. Here is an example of the input file: ``` name=John age=30 city=New York ``` Example Usage: ```python try: result = process_file(\\"input.txt\\") print(result) except FileNotFoundError as fnf_error: print(fnf_error) except ValueError as val_error: print(val_error) except OSError as os_error: print(os_error) except ProcessingError as proc_error: print(proc_error) ``` Example Output: ```python { \'name\': \'John\', \'age\': \'30\', \'city\': \'New York\' } ``` You must implement and demonstrate proper exception handling based on the provided constraints.","solution":"class ProcessingError(Exception): pass def process_file(file_path: str) -> dict: Reads and processes a text file containing key-value pairs in the format key=value. Returns a dictionary of the key-value pairs. result = {} try: with open(file_path, \'r\') as file: for line in file: line = line.strip() if \'=\' not in line: raise ValueError(\\"Invalid line format\\") key, value = map(str.strip, line.split(\'=\', 1)) result[key] = value except FileNotFoundError: raise FileNotFoundError(\\"File not found\\") except ValueError as ve: raise ve except OSError as io_error: raise OSError(io_error.strerror) except Exception as e: raise ProcessingError(f\\"An unexpected error occurred: {e}\\") return result"},{"question":"Implement a class called `TaskScheduler` that uses a heap to manage tasks with priorities. The scheduler should support the following operations: 1. **Add a Task (`add_task`)**: Adds a new task with a given priority. If a task with the same name already exists, update its priority. 2. **Remove a Task (`remove_task`)**: Removes a task by name. 3. **Pop Task (`pop_task`)**: Removes and returns the task with the highest priority (smallest priority value). If multiple tasks have the same priority, return them in the order they were added. You should use the `heapq` module for heap operations. Method Signatures: - `def add_task(self, task: str, priority: int) -> None:` - `def remove_task(self, task: str) -> None:` - `def pop_task(self) -> str:` Example: ```python scheduler = TaskScheduler() scheduler.add_task(\\"task1\\", 3) scheduler.add_task(\\"task2\\", 1) scheduler.add_task(\\"task1\\", 2) assert scheduler.pop_task() == \\"task2\\" scheduler.add_task(\\"task3\\", 5) scheduler.remove_task(\\"task1\\") assert scheduler.pop_task() == \\"task3\\" ``` Constraints: - The priority is an integer where lower values represent higher priority. - If attempting to remove a task that does not exist, raise a `KeyError`. - If trying to pop from an empty heap, raise a `KeyError`. ```python from heapq import heappush, heappop from itertools import count class TaskScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = count() # unique sequence count def add_task(self, task: str, priority: int) -> None: \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') ``` **Evaluate:** Students should demonstrate their understanding of the `heapq` module, as well as their ability to handle dictionary operations, manage a heap structure, and implement an efficient priority queue.","solution":"from heapq import heappush, heappop from itertools import count class TaskScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = count() # unique sequence count def add_task(self, task: str, priority: int) -> None: \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: \'Mark an existing task as REMOVED. Raise KeyError if not found.\' if task in self.entry_finder: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED else: raise KeyError(f\'Task {task} not found in scheduler\') def pop_task(self) -> str: \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Cross-Validation and Model Evaluation using Scikit-Learn In this assessment, you will be working with the Iris dataset to evaluate the performance of a Support Vector Machine (SVM) classifier using various cross-validation techniques provided by scikit-learn. Your task includes: 1. Splitting the data using train-test split. 2. Performing K-Fold cross-validation to evaluate the model. 3. Performing Repeated K-Fold cross-validation to evaluate model performance with repeated splits. 4. Using Stratified K-Fold to ensure balanced class distribution in each fold. 5. Generating cross-validated predictions and evaluating the model. Requirements: 1. Use the Iris dataset from scikit-learn. 2. Perform a train-test split with 40% test size. 3. Implement K-Fold cross-validation with 5 folds. 4. Implement Repeated K-Fold cross-validation with 5 folds and 3 repeats. 5. Implement Stratified K-Fold cross-validation with 5 folds. 6. Obtain cross-validated predictions using `cross_val_predict` and evaluate the model. Expected Input and Output 1. Load the Iris dataset. 2. Split the data using `train_test_split` and print the score using the test set. 3. Perform K-Fold cross-validation and print the mean and standard deviation of the scores. 4. Perform Repeated K-Fold cross-validation and print the mean and standard deviation of the scores. 5. Perform Stratified K-Fold cross-validation and print the mean and standard deviation of the scores. 6. Obtain cross-validated predictions using `cross_val_predict` and print the classification report. Constraints: 1. Use `svm.SVC` with a linear kernel and C parameter set to 1. 2. Use random_state=42 for all random number generations to ensure reproducibility. Performance Requirements: 1. Ensure that the code runs efficiently and make use of scikit-learn functions optimally. 2. Evaluate and report model performance accurately. Sample Code Structure You may use the following code structure as a reference: ```python import numpy as np from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, StratifiedKFold, cross_val_score, cross_val_predict from sklearn import datasets from sklearn import svm from sklearn.metrics import classification_report # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Step 1: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) clf = svm.SVC(kernel=\'linear\', C=1, random_state=42).fit(X_train, y_train) test_score = clf.score(X_test, y_test) print(f\'Train-Test Split Accuracy: {test_score:.2f}\') # Step 2: K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(clf, X, y, cv=kf) print(f\'K-Fold CV - Mean: {kf_scores.mean():.2f}, Std: {kf_scores.std():.2f}\') # Step 3: Repeated K-Fold Cross-Validation rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42) rkf_scores = cross_val_score(clf, X, y, cv=rkf) print(f\'Repeated K-Fold CV - Mean: {rkf_scores.mean():.2f}, Std: {rkf_scores.std():.2f}\') # Step 4: Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(clf, X, y, cv=skf) print(f\'Stratified K-Fold CV - Mean: {skf_scores.mean():.2f}, Std: {skf_scores.std():.2f}\') # Step 5: Cross-Validated Predictions predictions = cross_val_predict(clf, X, y, cv=skf) report = classification_report(y, predictions) print(f\'Cross-Validated Predictions Classification Report:n{report}\') ``` Submissions: 1. Provide the complete code implementation for the steps mentioned. 2. Include the outputs of each step in your submission.","solution":"import numpy as np from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, StratifiedKFold, cross_val_score, cross_val_predict from sklearn import datasets from sklearn import svm from sklearn.metrics import classification_report def evaluate_svm_on_iris(): # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Step 1: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) clf = svm.SVC(kernel=\'linear\', C=1, random_state=42).fit(X_train, y_train) test_score = clf.score(X_test, y_test) print(f\'Train-Test Split Accuracy: {test_score:.2f}\') # Step 2: K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(clf, X, y, cv=kf) print(f\'K-Fold CV - Mean: {kf_scores.mean():.2f}, Std: {kf_scores.std():.2f}\') # Step 3: Repeated K-Fold Cross-Validation rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42) rkf_scores = cross_val_score(clf, X, y, cv=rkf) print(f\'Repeated K-Fold CV - Mean: {rkf_scores.mean():.2f}, Std: {rkf_scores.std():.2f}\') # Step 4: Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(clf, X, y, cv=skf) print(f\'Stratified K-Fold CV - Mean: {skf_scores.mean():.2f}, Std: {skf_scores.std():.2f}\') # Step 5: Cross-Validated Predictions predictions = cross_val_predict(clf, X, y, cv=skf) report = classification_report(y, predictions) print(f\'Cross-Validated Predictions Classification Report:n{report}\') # Uncomment to run the function # evaluate_svm_on_iris()"},{"question":"**Objective:** Design an asyncio-based program that demonstrates the following features: - Declaring coroutines and tasks. - Running tasks concurrently. - Handling task cancellations and timeouts. - Using thread integration for IO-bound operations. **Question:** You are required to create a program for processing multiple tasks with the following constraints: 1. **Coroutines Declaration and Execution:** - Write a coroutine `fetch_data` that simulates fetching data by simply sleeping for a specified duration and then returning a message indicating the data fetched. - Write a coroutine `process_data` that takes the fetched data and performs some processing (e.g., converting it to uppercase) and returns the processed data. 2. **Concurrent Execution:** - Create multiple `fetch_data` tasks to run concurrently using `asyncio.create_task`. - Use `asyncio.gather` to wait for all fetch tasks to complete and then process the fetched data concurrently. 3. **Handling Cancellations and Timeouts:** - Implement a function `cancellable_task` that runs a given coroutine but cancels it if it exceeds a specified timeout using `asyncio.wait_for`. - Protect certain critical processing from being cancelled using `asyncio.shield`. 4. **Thread Integration:** - Write a blocking IO-bound function `blocking_io_operation` and run it asynchronously using `asyncio.to_thread`. **Input:** - A list of tuples where each tuple contains an ID and a delay in seconds representing the number of seconds to fetch data for that ID. ```python task_details = [(1, 2), (2, 5), (3, 3)] ``` **Output:** - A dictionary where the keys are the IDs and the values are the processed data fetched and processed within the specified time constraints. If a task times out, the value should be \\"timeout\\". **Constraints:** - You must use asyncio library functionality as described. - Implement proper exception handling to manage cancellations and timeouts. **Example:** ```python import asyncio import time # Implement this coroutine async def fetch_data(id, delay): await asyncio.sleep(delay) return f\\"data_{id}\\" # Implement this coroutine async def process_data(data): await asyncio.sleep(1) # Simulate processing delay return data.upper() # Implement this function async def cancellable_task(coro, timeout): try: return await asyncio.wait_for(coro, timeout) except asyncio.TimeoutError: return \\"timeout\\" # Implement the wrapper coroutine to fetch and process data async def fetch_and_process_data(id, delay, timeout=4): fetch_task = asyncio.create_task(fetch_data(id, delay)) data = await cancellable_task(fetch_task, timeout) if data != \\"timeout\\": process_task = asyncio.create_task(process_data(data)) processed_data = await asyncio.shield(process_task) return id, processed_data else: return id, data # Implement the main logic async def main(task_details): results = await asyncio.gather(*(fetch_and_process_data(id, delay) for id, delay in task_details)) return dict(results) if __name__ == \\"__main__\\": task_details = [(1, 2), (2, 5), (3, 3)] results = asyncio.run(main(task_details)) print(results) ``` Expected Output for Example: ``` {1: \'DATA_1\', 2: \'timeout\', 3: \'DATA_3\'} ``` **Note:** - Ensure that you save references to tasks to avoid them being garbage collected before completion. - Consider thread safety and the use of context variables when running blocking operations using `asyncio.to_thread`.","solution":"import asyncio async def fetch_data(id, delay): Simulates fetching data by sleeping for a specified delay. await asyncio.sleep(delay) return f\\"data_{id}\\" async def process_data(data): Simulates processing of fetched data by converting it to uppercase. await asyncio.sleep(1) # Simulate processing delay return data.upper() async def cancellable_task(coro, timeout): Runs a coroutine and cancels it if it exceeds a specified timeout. try: return await asyncio.wait_for(coro, timeout) except asyncio.TimeoutError: return \\"timeout\\" async def fetch_and_process_data(id, delay, timeout=4): Fetches and processes data for a given id with a delay. fetch_task = asyncio.create_task(fetch_data(id, delay)) data = await cancellable_task(fetch_task, timeout) if data != \\"timeout\\": process_task = asyncio.create_task(process_data(data)) processed_data = await asyncio.shield(process_task) return id, processed_data else: return id, data async def main(task_details): Main function to handle the concurrent execution of fetch and process tasks. results = await asyncio.gather(*(fetch_and_process_data(id, delay) for id, delay in task_details)) return dict(results) if __name__ == \\"__main__\\": task_details = [(1, 2), (2, 5), (3, 3)] results = asyncio.run(main(task_details)) print(results)"},{"question":"**Objective**: Demonstrate understanding of Python `memoryview` objects and related functions. **Tasks**: 1. Implement a function `create_memoryview_from_buffer` that: - Takes an input string and returns a read-only memoryview of its bytes. - Uses the `mem` and `size` parameters to create the memoryview. 2. Implement a function `is_memoryview` that: - Takes any Python object and returns `True` if the object is a memoryview, otherwise `False`. 3. Implement a function `memoryview_to_contiguous` that: - Takes a memoryview object and a format option (\'C\' or \'F\'). - Returns a new contiguous memoryview of the original object using the specified format. **Expected Function Signatures**: ```python def create_memoryview_from_buffer(input_string: str) -> memoryview: # implementation here def is_memoryview(obj: object) -> bool: # implementation here def memoryview_to_contiguous(mv_obj: memoryview, order: str) -> memoryview: # implementation here ``` **Constraints**: - For `create_memoryview_from_buffer`, ensure the memoryview is read-only. - For `memoryview_to_contiguous`, raise a `ValueError` if the order is not \'C\' or \'F\'. - No subclasses of memoryview should be created. **Input/Output Examples**: ```python # Example 1: mv = create_memoryview_from_buffer(\\"hello\\") assert isinstance(mv, memoryview) # True assert mv.readonly # True # Example 2: obj = \\"not a memoryview\\" assert is_memoryview(obj) == False # Example 3: mv = create_memoryview_from_buffer(\\"example\\") contiguous_mv = memoryview_to_contiguous(mv, \'C\') assert isinstance(contiguous_mv, memoryview) # True ``` **Notes**: - Demonstrate the use of `memoryview` related functions described in the provided documentation. - Ensure the code is efficient and handles edge cases.","solution":"def create_memoryview_from_buffer(input_string: str) -> memoryview: Takes an input string and returns a read-only memoryview of its bytes. byte_data = input_string.encode(\'utf-8\') mem_view = memoryview(byte_data) return mem_view.toreadonly() def is_memoryview(obj: object) -> bool: Takes any Python object and returns True if the object is a memoryview, otherwise False. return isinstance(obj, memoryview) def memoryview_to_contiguous(mv_obj: memoryview, order: str) -> memoryview: Takes a memoryview object and a format option (\'C\' or \'F\'). Returns a new contiguous memoryview of the original object using the specified format. if order not in (\'C\', \'F\'): raise ValueError(\\"Order must be \'C\' or \'F\'\\") return mv_obj.tobytes() if order == \'C\' else mv_obj.tobytes(order=\'F\')"},{"question":"Objective: Write a Python function named `manipulate_data` that takes a list of mixed data types (strings, integers, and floats) as input and returns a dictionary with specific transformations. Function Signature: ```python def manipulate_data(data: list) -> dict: ``` Input: - `data`: A list of elements where each element can be either a string, an integer, or a float. Output: - A dictionary with the following keys: - `\'strings\'`: A list of all strings from the input list, concatenated together as a single string. - `\'integers\'`: A list containing the squares of all integer elements from the input list. - `\'floats\'`: A list containing all float elements from the input list, each rounded to two decimal places. Example: ```python data = [10, 3.14159, \'hello\', 7, \'world\', 2.71828, 3] output = manipulate_data(data) ``` Expected output: ```python { \'strings\': \'helloworld\', \'integers\': [100, 49, 9], \'floats\': [3.14, 2.72] } ``` Constraints: - The input list will have at least one element. - Strings should be concatenated in their order of appearance in the input list. - Integers and floats should appear in their order of appearance in the input list. Notes: - Use list comprehensions wherever applicable to make the function concise and readable. - Use Python\'s built-in functions to perform necessary operations like squaring, rounding, and concatenation. Performance Requirements: - The function should handle up to 1000 elements efficiently. Solution Template: ```python def manipulate_data(data: list) -> dict: # Initialize empty collections for each data type strings = \'\' integers = [] floats = [] # Iterate over each element in the input list and append to respective collections for element in data: if isinstance(element, str): strings += element elif isinstance(element, int): integers.append(element ** 2) elif isinstance(element, float): floats.append(round(element, 2)) # Create the resultant dictionary with appropriate keys and return it result = { \'strings\': strings, \'integers\': integers, \'floats\': floats } return result ``` Test your implementation thoroughly to ensure its correctness.","solution":"def manipulate_data(data: list) -> dict: Takes a list of mixed data types (strings, integers, and floats) and returns a dictionary with strings concatenated together, squares of integers, and floats rounded to two decimal places. # Initialize empty collections for each data type strings = \'\' integers = [] floats = [] # Iterate over each element in the input list and append to respective collections for element in data: if isinstance(element, str): strings += element elif isinstance(element, int): integers.append(element ** 2) elif isinstance(element, float): floats.append(round(element, 2)) # Create the resultant dictionary with appropriate keys and return it result = { \'strings\': strings, \'integers\': integers, \'floats\': floats } return result"},{"question":"Objective: To assess your understanding of working with plist files using the `plistlib` module in Python, you are required to write a function that reads a given plist file, modifies its content based on specific criteria, and writes the updated content back to a new plist file. Problem Statement: You need to implement the following function: ```python import plistlib def modify_plist(input_file: str, output_file: str, updates: dict, key_to_remove: str) -> None: Reads a plist file, updates its content, removes a specific key, and writes the updated content to a new plist file. Parameters: - input_file (str): Path to the input plist file. - output_file (str): Path to the output plist file. - updates (dict): Dictionary containing key-value pairs to update or add in the plist. - key_to_remove (str): Key to be removed from the plist if it exists. Returns: - None pass ``` Functionality: 1. **Read the plist file:** Read the plist data from `input_file` using the `plistlib.load` function. 2. **Update/Add the data:** Update its content with the key-value pairs provided in the `updates` dictionary. 3. **Remove specified key:** Remove the key specified by `key_to_remove` if it exists in the plist. 4. **Write the updated plist:** Write the modified plist data to `output_file` using the `plistlib.dump` function. Input: - `input_file`: A string representing the path to a readable plist file. - `output_file`: A string representing the path to a writable plist file. - `updates`: A dictionary containing keys and values to update or add in the plist. - `key_to_remove`: A string representing a key to be removed from the plist. Constraints: - Assume the input plist file is well-formed and valid. - The plist file can contain nested dictionaries and lists. - You should use the default `FMT_XML` format to write the output plist file. Example: Assume we have a plist file `example.plist` with content: ```xml <dict> <key>name</key> <string>OldName</string> <key>version</key> <integer>1</integer> </dict> ``` Calling the function: ```python modify_plist(\'example.plist\', \'output.plist\', {\'name\': \'NewName\', \'author\': \'AuthorName\'}, \'version\') ``` The `output.plist` should contain: ```xml <dict> <key>name</key> <string>NewName</string> <key>author</key> <string>AuthorName</string> </dict> ``` Notes: - Ensure to handle files appropriately (i.e., opening and closing them). - Use module functionalities (`load`, `loads`, `dump`, `dumps`) as described.","solution":"import plistlib def modify_plist(input_file: str, output_file: str, updates: dict, key_to_remove: str) -> None: Reads a plist file, updates its content, removes a specific key, and writes the updated content to a new plist file. Parameters: - input_file (str): Path to the input plist file. - output_file (str): Path to the output plist file. - updates (dict): Dictionary containing key-value pairs to update or add in the plist. - key_to_remove (str): Key to be removed from the plist if it exists. Returns: - None # Open and load the input plist file with open(input_file, \'rb\') as infile: plist_data = plistlib.load(infile) # Update the plist with new key-value pairs plist_data.update(updates) # Remove the specified key if it exists if key_to_remove in plist_data: del plist_data[key_to_remove] # Write the modified plist data to the output file with open(output_file, \'wb\') as outfile: plistlib.dump(plist_data, outfile)"},{"question":"# Question: Complex Number Operations and Manipulations with `cmath` **Objective**: Implement a function that takes a list of complex numbers, converts each complex number to its polar coordinates, applies a function to the phase angle, and finally converts it back to the rectangular form. Function Signature ```python def transform_complex_numbers(complex_list, phase_function): pass ``` Input - `complex_list`: A list of complex numbers ( [z1, z2, ldots, zn] ). - `phase_function`: A function that takes a phase angle in radians and returns a modified phase angle. Output - Return a list of complex numbers after applying the transformation. Constraints - The list `complex_list` will contain at least one complex number. - The phase angle modification will not move the phase angle out of the range ([-π, π]). Example Usage: ```python import cmath import math # Define a phase function def phase_function(phi): return phi / 2 # List of complex numbers complex_list = [complex(1, 1), complex(-1, -1), complex(0, 2)] # Transform the complex numbers result = transform_complex_numbers(complex_list, phase_function) print(result) # Expected output: [(1.09868411346781+0.45508986056222733j), (-0.45508986056222733-1.09868411346781j), (0+1.4142135623730951j)] ``` Explanation 1. For each complex number in `complex_list`: - Convert the number to its polar coordinates using `cmath.polar`. - Apply `phase_function` to the phase component of the polar coordinates. - Convert the modified polar coordinates back to rectangular form using `cmath.rect`. 2. Return the list of transformed complex numbers. Additional Notes: - Utilize the `cmath.polar` function to obtain the modulus and phase of each complex number. - Use the provided `phase_function` to transform the phase. - Utilize the `cmath.rect` function to convert the modified polar coordinates back to rectangular form. - The solution should handle complex numbers efficiently and correctly perform all transformations.","solution":"import cmath def transform_complex_numbers(complex_list, phase_function): Transforms a list of complex numbers by converting each to polar coordinates, modifying the phase angle with a given function, and then converting back to rectangular form. :param complex_list: List of complex numbers :param phase_function: Function to apply to the phase angle :return: List of transformed complex numbers transformed_list = [] for z in complex_list: r, phi = cmath.polar(z) # Get the modulus and phase new_phi = phase_function(phi) # Apply the phase function z_transformed = cmath.rect(r, new_phi) # Convert back to rectangular form transformed_list.append(z_transformed) return transformed_list"},{"question":"Objective: To assess the understanding and application of the `sndhdr` module functions in Python. Problem Statement: You are provided with a list of filenames that represent sound files of various formats. Your task is to implement a function that will analyze these files using the `sndhdr` module and return a summary report. The summary report should include the count of each file type and a list of details for each file. Function Signature: ```python def analyze_sound_files(filenames: list) -> dict: ``` Input: - `filenames` (list): A list of strings representing the filenames of the sound files to be analyzed. Output: - A dictionary with the following structure: ```python { \\"summary\\": { \\"total_files\\": int, \\"recognized_files\\": int, \\"unrecognized_files\\": int, \\"filetype_counts\\": dict }, \\"details\\": [ { \\"filename\\": str, \\"filetype\\": str, \\"framerate\\": int, \\"nchannels\\": int, \\"nframes\\": int, \\"sampwidth\\": int }, ... ] } ``` - `\\"summary\\"`: Contains a summary of the analysis - `\\"total_files\\"`: Total number of files analyzed (an integer). - `\\"recognized_files\\"`: Number of recognized files (an integer). - `\\"unrecognized_files\\"`: Number of unrecognized files (an integer). - `\\"filetype_counts\\"`: A dictionary where keys are filetypes and values are their respective counts. - `\\"details\\"`: A list of dictionaries, each containing the details of a file from the namedtuple returned by `sndhdr.what()`. For unrecognized files, include the filename with `None` values for the other keys. Constraints: - Assume all filenames are valid and accessible in the environment where the function is run. - Performance: While there is no strict time limit, your solution should efficiently handle up to 1000 files. Example: ```python input_files = [\\"file1.wav\\", \\"file2.aiff\\", \\"file3.unknown\\", \\"file4.au\\"] result = analyze_sound_files(input_files) print(result) # Expected Output { \\"summary\\": { \\"total_files\\": 4, \\"recognized_files\\": 3, \\"unrecognized_files\\": 1, \\"filetype_counts\\": { \\"wav\\": 1, \\"aiff\\": 1, \\"au\\": 1 } }, \\"details\\": [ { \\"filename\\": \\"file1.wav\\", \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 120000, \\"sampwidth\\": 16 }, { \\"filename\\": \\"file2.aiff\\", \\"filetype\\": \\"aiff\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 120000, \\"sampwidth\\": 16 }, { \\"filename\\": \\"file3.unknown\\", \\"filetype\\": None, \\"framerate\\": None, \\"nchannels\\": None, \\"nframes\\": None, \\"sampwidth\\": None }, { \\"filename\\": \\"file4.au\\", \\"filetype\\": \\"au\\", \\"framerate\\": 8000, \\"nchannels\\": 1, \\"nframes\\": 60000, \\"sampwidth\\": 8 } ] } ``` Note: - The specific values in the expected output for the recognized files (`framerate`, `nchannels`, `nframes`, and `sampwidth`) will depend on the actual content of the sound files. Task: Implement the `analyze_sound_files` function as described, making use of the `sndhdr` module to determine the type of each sound file and compile the summary report accordingly.","solution":"import sndhdr def analyze_sound_files(filenames: list) -> dict: summary = { \\"total_files\\": len(filenames), \\"recognized_files\\": 0, \\"unrecognized_files\\": 0, \\"filetype_counts\\": {} } details = [] for filename in filenames: file_info = sndhdr.what(filename) if file_info is None: summary[\\"unrecognized_files\\"] += 1 file_details = { \\"filename\\": filename, \\"filetype\\": None, \\"framerate\\": None, \\"nchannels\\": None, \\"nframes\\": None, \\"sampwidth\\": None } else: summary[\\"recognized_files\\"] += 1 filetype = file_info.filetype if filetype in summary[\\"filetype_counts\\"]: summary[\\"filetype_counts\\"][filetype] += 1 else: summary[\\"filetype_counts\\"][filetype] = 1 file_details = { \\"filename\\": filename, \\"filetype\\": file_info.filetype, \\"framerate\\": file_info.framerate, \\"nchannels\\": file_info.nchannels, \\"nframes\\": file_info.nframes, \\"sampwidth\\": file_info.sampwidth } details.append(file_details) return { \\"summary\\": summary, \\"details\\": details }"},{"question":"**Question: Time Zone Conversion and Configuration using zoneinfo Module** You are tasked with creating a utility function using the \\"zoneinfo\\" module that converts a given UTC datetime to a list of specified time zones and handles daylight saving transitions appropriately. # Function Requirements 1. Implement a function `convert_to_time_zones(dt_utc, tz_list, fold=0)`. 2. The function takes: - `dt_utc` (datetime): A datetime object with tzinfo set to UTC (use zoneinfo for this). - `tz_list` (list of str): A list of time zone keys (strings) to which the datetime should be converted. - `fold` (int, optional): Fold parameter to handle ambiguous times during transitions. Default is `0`. 3. The function should return a dictionary where: - Keys are the time zone strings from `tz_list`. - Values are the corresponding datetime objects converted to the specified time zones. # Constraints and Performance - Ensure the function handles at least 10 different time zones efficiently. - Handle ZoneInfoNotFoundError exceptions appropriately if a time zone key is not valid by logging an error and skipping that time zone. # Example Usage ```python from datetime import datetime, timezone from zoneinfo import ZoneInfo def convert_to_time_zones(dt_utc, tz_list, fold=0): # Implementation goes here. # Sample input dt_utc = datetime(2021, 11, 7, 6, 30, tzinfo=timezone.utc) # UTC time tz_list = [\\"America/New_York\\", \\"Europe/London\\", \\"Asia/Tokyo\\", \\"Australia/Sydney\\"] converted_times = convert_to_time_zones(dt_utc, tz_list) # Sample output # { # \\"America/New_York\\": datetime(2021, 11, 7, 1, 30-05:00), # \\"Europe/London\\": datetime(2021, 11, 7, 6, 30+00:00), # \\"Asia/Tokyo\\": datetime(2021, 11, 7, 15, 30+09:00), # \\"Australia/Sydney\\": datetime(2021, 11, 7, 17, 30+11:00) # } ``` # Notes - Utilize `ZoneInfo` for the time zone conversions. - Leverage datetime methods like `astimezone()` for conversion. - Ensure proper error handling and logging for invalid time zone keys.","solution":"from datetime import datetime, timezone from zoneinfo import ZoneInfo, ZoneInfoNotFoundError import logging def convert_to_time_zones(dt_utc, tz_list, fold=0): Converts a given UTC datetime to a list of specified time zones and handles daylight saving transitions appropriately. Parameters: dt_utc (datetime): A datetime object with tzinfo set to UTC. tz_list (list of str): A list of time zone keys (strings) to which the datetime should be converted. fold (int, optional): Fold parameter to handle ambiguous times during transitions. Default is 0. Returns: dict: A dictionary where keys are time zone strings from tz_list, and values are the corresponding datetime objects converted to the specified time zones. if dt_utc.tzinfo != timezone.utc: raise ValueError(\\"dt_utc must be a datetime object with tzinfo set to UTC.\\") converted_times = {} for tz_str in tz_list: try: tz = ZoneInfo(tz_str) dt_local = dt_utc.astimezone(tz) dt_local = dt_local.replace(fold=fold) converted_times[tz_str] = dt_local except ZoneInfoNotFoundError: logging.error(f\\"Time zone \'{tz_str}\' not found.\\") continue return converted_times"},{"question":"# Advanced Coding Assessment: SQLite Database Operations in Python Objective: You are tasked with creating a Python script using the `sqlite3` module to manage an inventory database, perform various SQL operations, and manipulate custom data types. This will test your understanding of database connections, creating and using custom adapters and converters, and handling SQL transactions. Problem Statement: Create a Python script that performs the following tasks: 1. **Database Connection**: Establish a connection to an SQLite database named `inventory.db`. 2. **Table Creation**: - Create a table named `products` with the columns: - `id` (INTEGER, PRIMARY KEY) - `name` (TEXT) - `quantity` (INTEGER) - `price` (REAL) - `received_date` (DATE) 3. **Custom Adapter and Converter**: - Define a custom class `Product` with attributes: `id`, `name`, `quantity`, `price`, and `received_date` (as `datetime.date`). - Create an adapter to convert an instance of `Product` to a tuple of SQLite-compatible types. - Register a converter to convert `DATE` columns to `datetime.date`. 4. **Insert Data**: Insert multiple product entries into the `products` table using instances of the `Product` class. 5. **Query Data**: Write a function to retrieve and display all products with their details. 6. **Delete Operation**: Write a function to delete a product entry based on its `id`. 7. **Transaction Management**: Ensure the insertion of products and deletion of a product are handled within transactions. 8. **Backup**: Create a backup of the database named `inventory_backup.db`. Expected Input and Output: - Assume the `Product` class is instantiated correctly with required attributes. - The functions you\'ll write should handle printing details and status messages accordingly. - Perform necessary exception handling to ensure database consistency. Constraints: - You must use the `sqlite3` module\'s features like connection and cursor operations, custom adapters, and converters. - Ensure that the database connections and cursors are properly closed after operations. - The `received_date` should be handled using Python\'s `datetime.date` type. Example Function Implementations: 1. **Database Connection**: ```python def create_connection(db_name): import sqlite3 try: conn = sqlite3.connect(db_name) return conn except sqlite3.Error as e: print(e) return None ``` 2. **Table Creation**: ```python def create_table(conn): try: cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS products ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, quantity INTEGER, price REAL, received_date DATE )\'\'\') conn.commit() except sqlite3.Error as e: print(e) ``` Submission Requirements: - Submit a Python script file named `inventory_management.py` containing all necessary function definitions. - Include comments and documentations for readability and understanding.","solution":"import sqlite3 from datetime import date # Custom class Product class Product: def __init__(self, id, name, quantity, price, received_date): self.id = id self.name = name self.quantity = quantity self.price = price self.received_date = received_date def adapt_product(product): return (product.id, product.name, product.quantity, product.price, product.received_date) def convert_date(string): return date.fromisoformat(string.decode()) sqlite3.register_adapter(Product, adapt_product) sqlite3.register_converter(\\"DATE\\", convert_date) # 1. Database Connection def create_connection(db_name): try: conn = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES) return conn except sqlite3.Error as e: print(e) return None # 2. Table Creation def create_table(conn): try: cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS products ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, quantity INTEGER, price REAL, received_date DATE )\'\'\') conn.commit() except sqlite3.Error as e: print(e) # 4. Insert Data def insert_product(conn, product): try: cursor = conn.cursor() cursor.execute(\'INSERT INTO products VALUES (?, ?, ?, ?, ?)\', (product.id, product.name, product.quantity, product.price, product.received_date)) conn.commit() except sqlite3.Error as e: conn.rollback() print(e) # 5. Query Data def query_products(conn): try: cursor = conn.cursor() cursor.execute(\'SELECT * FROM products\') products = cursor.fetchall() return [Product(*product) for product in products] except sqlite3.Error as e: print(e) return [] # 6. Delete Operation def delete_product(conn, product_id): try: cursor = conn.cursor() cursor.execute(\'DELETE FROM products WHERE id = ?\', (product_id,)) conn.commit() except sqlite3.Error as e: conn.rollback() print(e) # 7. Backup def backup_database(src_db, backup_db): try: src_conn = create_connection(src_db) backup_conn = create_connection(backup_db) with backup_conn: src_conn.backup(backup_conn) src_conn.close() backup_conn.close() except sqlite3.Error as e: print(e)"},{"question":"# Advanced Dictionary Manipulation in Python C API You are tasked with implementing a custom dictionary manipulation function utilizing the Python C API. Your function should merge items from one dictionary into another while maintaining certain constraints and behavior. Specifically: 1. **Functionality**: Implement `merge_dictionaries_with_constraints`, which: - Takes two dictionary arguments, `dict_a` and `dict_b`, and a boolean `override`. - If `override` is `True`, it should update `dict_a` with all key-value pairs from `dict_b`. If a key from `dict_b` exists in `dict_a`, the value in `dict_a` should be overwritten. - If `override` is `False`, it should update `dict_a` only with key-value pairs from `dict_b` where the keys are not already present in `dict_a`. 2. **Constraints**: - Both dictionaries are guaranteed to have hashable keys. - Do not assume any specific type for the keys or values; they must be managed as generic Python objects. 3. **Input/Output**: - Input: Two dictionaries and a boolean. - Output: The merged dictionary (`dict_a`). # Function Definition: ```c #include <Python.h> /* Function to merge dictionaries with constraints */ PyObject* merge_dictionaries_with_constraints(PyObject* dict_a, PyObject* dict_b, int override) { // Your implementation here } ``` Use the provided functionality from the C API documentation to achieve the desired merging behavior. # Example: ```c // Assuming `dict_a` and `dict_b` are existing PyObject* representing dictionaries // and `override` is a boolean integer (0 or 1) PyObject* result = merge_dictionaries_with_constraints(dict_a, dict_b, 1); ``` Note: You are not required to handle reference counting or memory management for the input dictionaries; assume they are managed by the higher-level Python API. Implementing this function will demonstrate your understanding of Python\'s dictionary operations at the C API level, including iteration, insertion, and condition checks.","solution":"# Assuming you want the solution in Python instead of the C API for demonstration purposes: def merge_dictionaries_with_constraints(dict_a, dict_b, override): Merges dict_b into dict_a based on the override flag. If override is True, update dict_a with all key-value pairs from dict_b. If a key from dict_b exists in dict_a, the value in dict_a should be overwritten. If override is False, update dict_a only with key-value pairs from dict_b where the keys are not already present in dict_a. Args: dict_a (dict): The dictionary to be updated. dict_b (dict): The dictionary from which to take values. override (bool): The flag to decide whether to override existing keys. Returns: dict: The updated dictionary dict_a. if override: dict_a.update(dict_b) else: for key, value in dict_b.items(): if key not in dict_a: dict_a[key] = value return dict_a"},{"question":"**Objective:** Implement a simplified debugger using the `bdb` module. You will need to create a subclass of the `bdb.Bdb` class and use the `bdb.Breakpoint` class for managing breakpoints. **Task:** 1. Create a class `SimpleDebugger` that inherits from `bdb.Bdb`. 2. Implement the `user_line` method to print a message whenever a line of code is about to be executed. 3. Implement the `user_call` method to print a message when a function is called. 4. Implement the `user_return` method to print a message when a function is about to return. 5. Write a function `setup_debugger` that: - Creates an instance of `SimpleDebugger`. - Sets a breakpoint at a specified line in a file using the `set_break` method. - Starts the debugger using the `set_trace` method. 6. Write a sample script (`sample_script.py`) that includes the following function: ```python def sample_function(n): total = 0 for i in range(n): total += i return total ``` 7. Demonstrate the use of the `setup_debugger` function to set a breakpoint at the line inside the for-loop of `sample_function` and debug it. **Requirements:** - The `SimpleDebugger` class must print messages with the format `Event at <filename>:<line number>`. - The `setup_debugger` function must set a breakpoint and start debugging `sample_script.py` correctly. - The function `sample_function` in `sample_script.py` should be tested with a call to `sample_function(5)`. **Input:** - A string representing the filename (`sample_script.py`). - An integer representing the line number for setting a breakpoint. **Output:** - Messages printed by the debugger showing the debugging process, including line executions and function calls/returns. **Example Usage:** ```python # Command to run the setup_debugger function (to be included in the script that imports SimpleDebugger) setup_debugger(\'sample_script.py\', <line number>) ``` *Note: Replace `<line number>` with the actual line number inside the `for` loop of `sample_function` in `sample_script.py` where you want to set the breakpoint.* **Constraints:** - The input filename must be a valid Python file. - The line number must be within the valid range of lines in the input file. Consider edge cases such as invalid filenames or line numbers and handle them appropriately in your implementation.","solution":"import bdb class SimpleDebugger(bdb.Bdb): def user_line(self, frame): filename = frame.f_code.co_filename lineno = frame.f_lineno print(f\\"Line event at {filename}:{lineno}\\") def user_call(self, frame, argument_list): filename = frame.f_code.co_filename lineno = frame.f_lineno print(f\\"Call event at {filename}:{lineno}\\") def user_return(self, frame, return_value): filename = frame.f_code.co_filename lineno = frame.f_lineno print(f\\"Return event at {filename}:{lineno}\\") def setup_debugger(filename, line_number): debugger = SimpleDebugger() debugger.set_break(filename, line_number) debugger.set_trace() # sample_script.py def sample_function(n): total = 0 for i in range(n): total += i return total"},{"question":"# Custom Email Message Serializer You are tasked with designing a custom email message serializer using the `email.generator` module. This serializer should generate a serialized binary representation of an email message, but with added custom logic to handle specific requirements. # Requirements: 1. **Class Design**: - Create a class `CustomBytesGenerator` inheriting from `email.generator.BytesGenerator`. 2. **Constructor**: - Accept the same parameters as the `BytesGenerator` constructor plus an additional parameter `limit_body_length` (an integer). 3. **Custom Method**: - Implement a method `flatten(msg, unixfrom=False, linesep=None)` that serializes the message object structure rooted at `msg`. - **Modification**: The body of the message should be truncated to `limit_body_length` characters, and add a note `\\"[Message truncated]\\"` if truncation occurs. - Ensure standard behavior defined by `BytesGenerator` is preserved where applicable. 4. **Additional Constraints**: - Ensure the header lines adhere to `maxheaderlen` if provided. - Use a default policy if none is provided. # Input and Output: - Input: An instance of a message object and file-like object to write the output. - Output: The serialized representation of the message object, written to the provided file-like object. # Performance: The solution should handle messages up to 10MB efficiently without significant delays. # Example: ```python import email.message from io import BytesIO # Sample email message creation msg = email.message.EmailMessage() msg.set_content(\\"This is a simple email message.nWith multiple lines of content.nThe end.\\") # Create a file-like object output = BytesIO() # Instantiate CustomBytesGenerator with a body length limit of 50 characters custom_generator = CustomBytesGenerator(output, limit_body_length=50) # Serialize the message custom_generator.flatten(msg) # The output should now contain the serialized email with body truncated if necessary print(output.getvalue()) ``` # Constraints: - You may only use the `email` module and standard Python libraries. - Do not use any external libraries. # Notes: Be sure to test your solution with various email message structures to ensure all requirements are met, including different header lengths and policy settings.","solution":"from email.generator import BytesGenerator from email.policy import default class CustomBytesGenerator(BytesGenerator): def __init__(self, outfp, limit_body_length=None, policy=default, *args, **kwargs): super().__init__(outfp, policy=policy, *args, **kwargs) self.limit_body_length = limit_body_length def flatten(self, msg, unixfrom=False, linesep=None): body = msg.get_payload() if isinstance(body, str) and self.limit_body_length is not None and len(body) > self.limit_body_length: truncated_body = body[:self.limit_body_length] + \\"n[Message truncated]\\" msg.set_payload(truncated_body) super().flatten(msg, unixfrom=unixfrom, linesep=linesep)"},{"question":"**Understanding Scope and Exception Handling in Python** Implement a Python function called `evaluate_expression` that takes three arguments: - `expression`: a string containing a Python expression to be evaluated, - `local_vars`: a dictionary representing local variables to be used during the evaluation, - `global_vars`: a dictionary representing global variables to be used during the evaluation. Your function should execute the following: 1. Evaluate the `expression` using the provided `local_vars` and `global_vars`. 2. Handle exceptions gracefully: - If a `NameError` or `UnboundLocalError` occurs, return the string \\"Error: Undefined variable\\". - For any other exception, return the string \\"Error: Invalid expression\\". 3. Return the result of the evaluated expression if it is evaluated successfully. **Function Signature:** ```python def evaluate_expression(expression: str, local_vars: dict, global_vars: dict) -> any: pass ``` **Constraints:** - You may assume that `expression` is always a string. - `local_vars` and `global_vars` are dictionaries with string keys and values of any non-callable type. - The function should not modify the input dictionaries. **Example Usage:** ```python local_vars = {\'a\': 10} global_vars = {\'b\': 20} print(evaluate_expression(\'a + b\', local_vars, global_vars)) # Output: 30 local_vars = {\'a\': 10} global_vars = {} print(evaluate_expression(\'a + c\', local_vars, global_vars)) # Output: Error: Undefined variable local_vars = {\'a\': 10} global_vars = {\'b\': 0} print(evaluate_expression(\'a / b\', local_vars, global_vars)) # Output: Error: Invalid expression ``` Write your function implementation in the box below:","solution":"def evaluate_expression(expression: str, local_vars: dict, global_vars: dict) -> any: try: result = eval(expression, global_vars, local_vars) return result except (NameError, UnboundLocalError): return \\"Error: Undefined variable\\" except Exception: return \\"Error: Invalid expression\\""},{"question":"Objective: Demonstrate your understanding of the fundamental and advanced capabilities of the Seaborn `objects` module by visualizing and customizing a dataset. Problem Statement: You are provided with the \\"penguins\\" dataset. Your task is to use Seaborn\'s `objects` module to create a set of visualizations. Specifically, you need to: 1. **Load the dataset** using Seaborn\'s `load_dataset` function. 2. **Create a scatter plot** showing the relationship between `flipper_length_mm` (x-axis) and `body_mass_g` (y-axis). 3. **Color the points** based on the `species` attribute. 4. **Customize the labels**: - Set the x-axis label to \\"Flipper Length (mm)\\". - Set the y-axis label to \\"Body Mass (g)\\". - Set the color legend title to \\"Species\\". 5. **Apply faceting**: - Create facets based on the `island` attribute. - Set the facet title format to \\"Island: {island name}\\". 6. **Add another layer** showing the linear regression line for each species within each facet. Constraints: - The dataset may contain missing values. Ensure that your visualization handles them appropriately. - Ensure the plot is clear, with appropriate size and resolution for readability. Expected Input and Output: - **Input**: None. The dataset will be loaded within your code. - **Output**: Display the resulting plot. Write your implementation in Python: ```python import seaborn.objects as so from seaborn import load_dataset import numpy as np # Load the dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by removing rows with any NaN values penguins = penguins.dropna() # Create the scatter plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Dot()) .facet(\\"island\\") .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", color=\\"Species\\", title=\\"Island: {}\\") .add(so.Line(), so.PolyFit(order=1), color=\\"species\\") # Add linear regression lines ) # Display the plot plot.show() ``` Ensure your code is correctly implemented following the above instructions and then execute it to generate the desired plot.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by removing rows with any NaN values penguins = penguins.dropna() # Create the scatter plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Dot()) .facet(\\"island\\") .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", color=\\"Species\\", title=\\"Island: {value}\\") .add(so.Line(), so.PolyFit(order=1), color=\\"species\\") # Add linear regression lines ) # Display the plot plot.show()"},{"question":"Buffer Protocol Objective: Write a function that validates the structure of an n-dimensional array using the Python buffer protocol and provides additional functionalities for buffer management. Problem Statement: You are to implement two functions: 1. `verify_structure(memlen, itemsize, ndim, shape, strides, offset)`: - This function verifies if the parameters provided represent a valid array within the bounds of the allocated memory. - **Input**: - `memlen` (int): The length of the physical memory block. - `itemsize` (int): The size of a single element in bytes. - `ndim` (int): Number of dimensions of the array. - `shape` (List[int]): An array of `ndim` elements indicating the size of the array in each dimension. - `strides` (List[int]): An array of `ndim` elements giving the number of bytes to skip to get to a new element in each dimension. - `offset` (int): Offset from the start of the memory block. - **Output**: - Returns `True` if the structure is valid and `False` otherwise. 2. `get_item_pointer(ndim, buf, strides, suboffsets, indices)`: - This function returns a pointer to the element in an N-D array based on indices, considering both strides and suboffsets. - **Input**: - `ndim` (int): Number of dimensions of the array. - `buf` (pointer): A pointer to the start of the memory block. - `strides` (List[int]): An array of `ndim` elements giving the number of bytes to skip to get to a new element in each dimension. - `suboffsets` (List[int]): An array of `ndim` elements, where non-negative values indicate the number of bytes to add to each pointer after de-referencing. - `indices` (List[int]): An array of `ndim` indices representing the position of the desired element. - **Output**: - Returns a pointer (or equivalent in Python, such as memory address offset) to the element positioned by the indices. Constraints: - Assume all input lists (`shape`, `strides`, `suboffsets`, and `indices`) have a length equal to `ndim`. - The memory block for the buffer starts at some address represented by `buf`. - The given `buf` pointer is valid and malloc’d memory. Example: ```python def verify_structure(memlen, itemsize, ndim, shape, strides, offset): if offset % itemsize: return False if offset < 0 or offset + itemsize > memlen: return False if any(v % itemsize for v in strides): return False if ndim <= 0: return ndim == 0 and not shape and not strides if 0 in shape: return True imin = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] <= 0) imax = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] > 0) return 0 <= offset + imin and offset + imax + itemsize <= memlen def get_item_pointer(ndim, buf, strides, suboffsets, indices): pointer = buf for i in range(ndim): pointer += strides[i] * indices[i] if suboffsets[i] >= 0: pointer = pointer + suboffsets[i] # In place of *((char**)pointer) + suboffsets[i] return pointer # Example usage buf = bytes(1000) strides = [4, 4] suboffsets = [-1, -1] indices = [1, 2] print(verify_structure(1000, 4, 2, [5, 5], strides, 0)) # True print(get_item_pointer(2, buf, strides, suboffsets, indices)) # Expected memory address offset from buf ``` Write your implementation code in the provided function templates.","solution":"def verify_structure(memlen, itemsize, ndim, shape, strides, offset): Verifies if the provided parameters represent a valid structure for an n-dimensional array within the bounds of the allocated memory. # Check if offset is aligned with itemsize if offset % itemsize != 0: return False # Check if offset is within the bounds of the memory block if offset < 0 or offset >= memlen: return False # Check if strides are aligned with itemsize if any(stride % itemsize != 0 for stride in strides): return False # If there are no dimensions, verify shape and strides are empty if ndim == 0: return shape == [] and strides == [] # If there is a dimension with size 0, the array is trivially valid if 0 in shape: return True # Calculate the span of the array imin = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] < 0) imax = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] > 0) total_bytes = offset + imax + itemsize return 0 <= offset + imin and total_bytes <= memlen def get_item_pointer(ndim, buf, strides, suboffsets, indices): Returns a pointer to the element in an n-dimensional array based on the given indices. pointer = 0 # Assuming a byte offset from the start of the buffer for i in range(ndim): pointer += strides[i] * indices[i] if suboffsets[i] >= 0: pointer += suboffsets[i] return pointer"},{"question":"Objective The objective of this question is to test the student\'s ability to create and manage a custom object type in Python, including its integration with Python\'s garbage collection system. Problem Statement Design a custom object type in Python named `CustomList` that mimics some behaviors of Python\'s built-in list but with additional features. Your implementation should include: 1. **Attributes**: - `data`: A list of integers. - `length`: An integer representing the current number of elements in the `data`. 2. **Methods**: - `__init__(self, initial_data: list)`: Constructor that initializes the `data` with the given list of integers and sets the `length` accordingly. - `append(self, value: int)`: Method to add an integer to the end of the `data`. - `remove(self, value: int)`: Method to remove the first occurrence of the integer `value` from the `data`. - `get_length(self)`: Method to return the current length of the `data`. 3. **Garbage Collection**: - Ensure that all methods handle memory correctly and that no cyclic references exist that the garbage collector cannot handle. 4. **Assertions**: - Ensure that all methods check if the input types are correct; raise appropriate exceptions if they aren\'t. Constraints - The `initial_data` list passed to the constructor will have at most 1000 integers. - The integer values will be between -10000 and 10000. - The `length` attribute must always reflect the correct number of elements in the `data`. Expected Input and Output Formats - The `__init__` method will receive a list of integers. - The `append` and `remove` methods will receive an integer. - The `get_length` method will return an integer representing the number of elements in the `data`. Example ```python cl = CustomList([1, 2, 3]) print(cl.get_length()) # Output: 3 cl.append(4) print(cl.get_length()) # Output: 4 cl.remove(2) print(cl.get_length()) # Output: 3 ``` Implementation ```python class CustomList: def __init__(self, initial_data: list): if not all(isinstance(i, int) for i in initial_data): raise ValueError(\\"initial_data must be a list of integers.\\") self.data = initial_data self.length = len(self.data) def append(self, value: int): if not isinstance(value, int): raise ValueError(\\"value must be an integer.\\") self.data.append(value) self.length += 1 def remove(self, value: int): if not isinstance(value, int): raise ValueError(\\"value must be an integer.\\") self.data.remove(value) self.length -= 1 def get_length(self): return self.length ```","solution":"class CustomList: def __init__(self, initial_data: list): if not all(isinstance(i, int) for i in initial_data): raise ValueError(\\"initial_data must be a list of integers.\\") self.data = initial_data.copy() self.length = len(self.data) def append(self, value: int): if not isinstance(value, int): raise ValueError(\\"value must be an integer.\\") self.data.append(value) self.length += 1 def remove(self, value: int): if not isinstance(value, int): raise ValueError(\\"value must be an integer.\\") if value in self.data: self.data.remove(value) self.length -= 1 else: raise ValueError(\\"value not found in the list.\\") def get_length(self): return self.length"},{"question":"# Challenge: Advanced Database Operations with sqlite3 You are required to design and implement a Python program that utilizes the `sqlite3` module to manage a database for a library system. The system should handle books and authors, perform various SQL operations, and output specific data based on dynamic queries. Your task is to write functions to achieve the following: 1. **Create Database and Tables**: - Create a database named `library.db` (or in-memory). - Define tables `authors` and `books`: - `authors`: Columns `id` (INTEGER, PRIMARY KEY), `name` (TEXT), `birth_year` (INTEGER). - `books`: Columns `id` (INTEGER, PRIMARY KEY), `title` (TEXT), `author_id` (INTEGER, FOREIGN KEY referencing authors(id)), `published_year` (INTEGER). 2. **Insert Data**: - Implement a function `insert_data(authors_data, books_data)` to insert multiple entries into the `authors` and `books` tables. Use placeholders (`?`) to prevent SQL injection. - `authors_data` should be a list of tuples representing multiple authors. - `books_data` should be a list of tuples representing multiple books. 3. **Query and Fetch Data**: - Implement a function `get_books_by_author(name)` that returns all books written by a given author name. - Implement a function `count_books_by_year(year)` that returns the count of books published in a specific year. 4. **Customized Row Factories**: - Use a row factory to return the results of `get_books_by_author` as dictionaries with keys being the column names. 5. **Transaction Management**: - Ensure each insertion is within a transaction. Use a context manager to automatically commit transactions if no exceptions occur, otherwise rollback. 6. **Exception Handling**: - Gracefully handle potential exceptions, such as integrity errors or operational errors. # Function Signatures ```python import sqlite3 def create_database_and_tables(): # Your code here def insert_data(authors_data, books_data): # Your code here def get_books_by_author(name): # Your code here def count_books_by_year(year): # Your code here ``` # Input and Output Formats - **Input**: - `authors_data`: List of tuples, e.g., `[(\'J.K. Rowling\', 1965), (\'J.R.R. Tolkien\', 1892), ...]` - `books_data`: List of tuples, e.g., `[(\'Harry Potter and the Sorcerer\'s Stone\', 1, 1997), (\'The Hobbit\', 2, 1937), ...]` - `name` for `get_books_by_author`: String, e.g., `\'J.K. Rowling\'` - `year` for `count_books_by_year`: Integer, e.g., `1997` - **Output**: - `get_books_by_author(name)`: List of dictionaries representing books by the author. - `count_books_by_year(year)`: Integer count of books published in the specified year. # Constraints - Each author should have a unique name. - Each book should have a unique title. - Proper use of transactions and exception handling is crucial. - Performance should be considered in data insertion and querying. # Example: ```python # Sample data authors_data = [(\'J.K. Rowling\', 1965), (\'J.R.R. Tolkien\', 1892)] books_data = [ (\'Harry Potter and the Philosopher\'s Stone\', 1, 1997), (\'The Hobbit\', 2, 1937) ] # Creating database and tables create_database_and_tables() # Inserting data insert_data(authors_data, books_data) # Fetching books by author print(get_books_by_author(\'J.K. Rowling\')) # Counting books by year print(count_books_by_year(1997)) ``` **Expected Output**: ```python [{\'id\': 1, \'title\': \'Harry Potter and the Philosopher\'s Stone\', \'author_id\': 1, \'published_year\': 1997}] 1 ``` # Notes: - Ensure that you use placeholders for SQL operations to prevent SQL injection. - Make use of custom row factories to return results as dictionaries. - Manage transactions properly using context managers to ensure data integrity.","solution":"import sqlite3 def create_database_and_tables(db_name=\\":memory:\\"): Create the database and the tables `authors` and `books`. connection = sqlite3.connect(db_name) try: with connection: connection.execute(\'\'\' CREATE TABLE IF NOT EXISTS authors ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT UNIQUE NOT NULL, birth_year INTEGER ) \'\'\') connection.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT UNIQUE NOT NULL, author_id INTEGER, published_year INTEGER, FOREIGN KEY(author_id) REFERENCES authors(id) ) \'\'\') finally: connection.close() def insert_data(authors_data, books_data, db_name=\\":memory:\\"): Insert multiple entries into the authors and books tables. connection = sqlite3.connect(db_name) try: with connection: cursor = connection.cursor() cursor.executemany(\'\'\' INSERT INTO authors (name, birth_year) VALUES (?, ?) \'\'\', authors_data) cursor.executemany(\'\'\' INSERT INTO books (title, author_id, published_year) VALUES (?, ?, ?) \'\'\', books_data) except sqlite3.IntegrityError as e: print(f\\"IntegrityError: {e}\\") finally: connection.close() def get_books_by_author(name, db_name=\\":memory:\\"): Returns all books written by the given author name. def dict_factory(cursor, row): d = {} for idx, col in enumerate(cursor.description): d[col[0]] = row[idx] return d connection = sqlite3.connect(db_name) connection.row_factory = dict_factory try: cursor = connection.cursor() cursor.execute(\'\'\' SELECT books.id, books.title, books.author_id, books.published_year FROM books JOIN authors ON books.author_id = authors.id WHERE authors.name = ? \'\'\', (name,)) return cursor.fetchall() finally: connection.close() def count_books_by_year(year, db_name=\\":memory:\\"): Returns the count of books published in the specified year. connection = sqlite3.connect(db_name) try: cursor = connection.cursor() cursor.execute(\'\'\' SELECT COUNT(*) FROM books WHERE published_year = ? \'\'\', (year,)) return cursor.fetchone()[0] finally: connection.close()"},{"question":"**Objective**: Design a program to create visually effective and aesthetically pleasing visualizations using `seaborn` color palettes. **Problem Statement**: You are given a dataset containing numerical and categorical data. Your task is to create a function `create_visualizations` that produces three specific types of visualizations using seaborn\'s color palettes: 1. **Categorical Plot**: Use a qualitative color palette to visualize categorical data. 2. **Heatmap**: Use a sequential color palette to visualize numerical data in a heatmap. 3. **Diverging Bar Plot**: Use a diverging color palette to visualize data that spans a midpoint. **Requirements**: 1. **Input Format**: - A Pandas DataFrame `df` with at least the following columns: - `category`: Categorical data. - `numeric1`: Numerical data. - `numeric2`: Numerical data. - Any additional columns may be present but are not necessary for this task. 2. **Output**: - The function should save three separate visualization images named `categorical_plot.png`, `heatmap.png`, and `diverging_barplot.png`. 3. **Constraints**: - Use seaborn\'s built-in datasets for demonstration purposes and assume the same structure for the provided dataset. - Ensure the color palettes are chosen appropriately for each type of plot. **Function Signature**: ```python import pandas as pd def create_visualizations(df: pd.DataFrame) -> None: pass ``` **Example**: Given the following DataFrame: ```python import seaborn as sns import pandas as pd # Load seaborn example dataset df = sns.load_dataset(\\"penguins\\") ``` Your function should create the following visualizations: 1. **Categorical Plot**: - A bar plot showing the count of penguins by species using a qualitative palette. 2. **Heatmap**: - A heatmap of numerical values such as `bill_length_mm` vs `bill_depth_mm` using a sequential palette. 3. **Diverging Bar Plot**: - A bar plot showing the average body mass (`body_mass_g`) of penguins by island, where bars are colored using a diverging palette around the midpoint of the average body mass. *Note*: Ensure that you include proper labeling of axes, titles, and legends where necessary. **Tips**: - Use `seaborn.color_palette` or similar functions to select and apply color palettes. - Ensure your plots are clear and properly formatted for interpretation. **Evaluation**: - Correctness: The plots should accurately represent the data using the specified types of color palettes. - Code readability: Your code should be clear and well-documented, explaining your choices of palettes and plot types. - Aesthetics: The visualizations should be aesthetically pleasing and effectively convey the underlying data insights.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(df: pd.DataFrame) -> None: Create three visualizations using seaborn\'s color palettes. 1. Categorical Plot. 2. Heatmap. 3. Diverging Bar Plot. Parameters: df: Pandas DataFrame containing at least \'category\', \'numeric1\', \'numeric2\' columns. # 1. Categorical Plot plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\'category\', palette=\'Set2\') plt.title(\'Categorical Plot\') plt.savefig(\'categorical_plot.png\') plt.clf() # 2. Heatmap plt.figure(figsize=(10, 6)) pivot_table = df.pivot_table(values=\'numeric1\', index=\'category\', columns=\'numeric2\') sns.heatmap(pivot_table, cmap=\'Blues\') plt.title(\'Heatmap\') plt.savefig(\'heatmap.png\') plt.clf() # 3. Diverging Bar Plot plt.figure(figsize=(10, 6)) average_mass = df.groupby(\'category\')[\'numeric1\'].mean().reset_index() average_mass[\'mean_diverging\'] = average_mass[\'numeric1\'] - average_mass[\'numeric1\'].mean() sns.barplot(x=\'category\', y=\'mean_diverging\', data=average_mass, palette=\'coolwarm\') plt.axhline(0, color=\'black\', linewidth=0.8) plt.title(\'Diverging Bar Plot\') plt.savefig(\'diverging_barplot.png\') plt.clf()"},{"question":"**Objective**: Implement a `MinHeapWithUpdate` class to manage a priority queue with dynamic updates of element priorities. **Description**: You need to create a class `MinHeapWithUpdate` which supports the following operations: 1. **insert(item, priority)**: Insert an item with a given priority into the heap. 2. **pop()**: Remove and return the item with the smallest priority. 3. **update(item, new_priority)**: Update the priority of an existing item. If the item doesn\'t exist, raise a `KeyError`. 4. **remove(item)**: Remove an item from the heap. If the item doesn\'t exist, raise a `KeyError`. # Constraints - The priority can be any comparable type (e.g., integers, floats). - The `item` can be any hashable type (e.g., strings, tuples). # Input and Output Format - All methods should handle invalid operations gracefully (e.g., updating or removing non-existing items should raise `KeyError`). - Use zero-based indexing for the heap array representation. # Function Definitions ```python class MinHeapWithUpdate: def __init__(self): pass def insert(self, item, priority): Insert an item with the given priority into the heap. pass def pop(self): Remove and return the item with the smallest priority. pass def update(self, item, new_priority): Update the priority of an existing item in the heap. pass def remove(self, item): Remove an existing item from the heap. pass ``` # Example Usage ```python heap = MinHeapWithUpdate() heap.insert(\'task1\', 5) heap.insert(\'task2\', 3) heap.insert(\'task3\', 8) print(heap.pop()) # Output: \'task2\' heap.update(\'task1\', 1) print(heap.pop()) # Output: \'task1\' heap.remove(\'task3\') ``` # Evaluation Criteria - Correctness: The implementation should correctly handle all specified operations. - Efficiency: The operations should maintain the heap properties efficiently. - Edge Cases: Handle edge cases such as updating or removing non-existing items properly.","solution":"import heapq class MinHeapWithUpdate: def __init__(self): self.heap = [] self.entry_finder = {} self.REMOVED = \'<removed-item>\' self.counter = 0 def insert(self, item, priority): count = self.counter entry = [priority, count, item] self.entry_finder[item] = entry heapq.heappush(self.heap, entry) self.counter += 1 def pop(self): while self.heap: priority, count, item = heapq.heappop(self.heap) if item is not self.REMOVED: del self.entry_finder[item] return item raise KeyError(\\"pop from an empty priority queue\\") def update(self, item, new_priority): if item not in self.entry_finder: raise KeyError(f\\"Item \'{item}\' not found\\") self.remove(item) self.insert(item, new_priority) def remove(self, item): if item not in self.entry_finder: raise KeyError(f\\"Item \'{item}\' not found\\") entry = self.entry_finder.pop(item) entry[-1] = self.REMOVED"},{"question":"# Task You are tasked with creating a library that provides enhanced string operations leveraging Python\'s `string` module. Specifically, you will implement three functions: 1. **`custom_formatter`**: This function will extend Python\'s string formatting capabilities by allowing dynamic nested replacements within format specifications. 2. **`template_substitution`**: This function will utilize the `Template` class to perform safe and regular substitutions in a given string template. 3. **`capitalize_words`**: This function will use the `capwords` helper function to transform input strings. # Function Specifications Function 1: `custom_formatter` Implement the function `custom_formatter(format_string: str, args: tuple, kwargs: dict) -> str`, which formats the input `format_string` with positional and keyword arguments. This formatting should support dynamic nested replacements within format specifications. - **Input**: - `format_string`: A string containing format specifications. - `args`: A tuple of positional arguments. - `kwargs`: A dictionary of keyword arguments. - **Output**: - A formatted string based on the provided format string and arguments. *Example:* ```python format_string = \\"Nested example: {0:{align}{width}} and {name:.{precision}f}\\" args = (\\"left\\", 5) kwargs = {\\"align\\": \\"<\\", \\"width\\": \\"10\\", \\"name\\": \\"pi\\", \\"precision\\": 2} custom_formatter(format_string, args, kwargs) # Output: \\"Nested example: left and 3.14\\" ``` Function 2: `template_substitution` Implement the function `template_substitution(template_string: str, mapping: dict, safe: bool) -> str`, which performs string substitution using the `Template` class. If `safe` is `True`, use `safe_substitute`; otherwise, use `substitute`. - **Input**: - `template_string`: A template string with placeholders. - `mapping`: A dictionary containing key-value pairs for substitution. - `safe`: A boolean indicating whether to use safe substitution. - **Output**: - The substituted string or an error if placeholders are missing (when `safe` is `False`). *Example:* ```python template_string = \\"name is age years old.\\" mapping = {\\"name\\": \\"Alice\\"} template_substitution(template_string, mapping, safe=True) # Output: \\"Alice is age years old.\\" ``` Function 3: `capitalize_words` Implement the function `capitalize_words(input_string: str, separator: str = None) -> str`, which uses `string.capwords` to capitalize each word in the input string. If a separator is provided, it should use that to split and join words; otherwise, it defaults to whitespace. - **Input**: - `input_string`: The string to capitalize. - `separator`: An optional string separator. - **Output**: - The input string with each word capitalized. *Example:* ```python input_string = \\"hello world\\" capitalize_words(input_string) # Output: \\"Hello World\\" ``` # Constraints - The length of `format_string` and `template_string` will not exceed 1000 characters. - The `args` and `kwargs` combined will not contain more than 100 elements. - The length of `input_string` will not exceed 1000 characters. # Performance Requirements - The functions should handle typical string inputs efficiently and perform substitutions and formatting in linear time relative to the size of the input strings. Exceptions should be handled gracefully, especially for the `template_substitution` function when `safe` is `True`. Implement these functions with robust and efficient string manipulation capabilities. Ensure thorough testing with various edge cases, such as nested format specifications, missing placeholders, and different separator characters.","solution":"import string from string import Template def custom_formatter(format_string: str, args: tuple, kwargs: dict) -> str: Extend Python\'s string formatting capabilities by allowing dynamic nested replacements within format specifications. return format_string.format(*args, **kwargs) def template_substitution(template_string: str, mapping: dict, safe: bool) -> str: Perform string substitution using the Template class. Use safe_substitute if safe is True, otherwise substitute. tmpl = Template(template_string) if safe: return tmpl.safe_substitute(mapping) else: return tmpl.substitute(mapping) def capitalize_words(input_string: str, separator: str = None) -> str: Capitalize each word in the input string using string.capwords. If a separator is provided, use that to split and join words. if separator: return separator.join(word.capitalize() for word in input_string.split(separator)) else: return string.capwords(input_string)"},{"question":"# Advanced SQLite3 Usage with Custom Types and Transactions Objective Create a Python program using the `sqlite3` module that demonstrates the following: 1. Connecting to an SQLite database. 2. Creating tables with custom types. 3. Using transactions for batch operations. 4. Utilizing adapters and converters for custom Python types. 5. Implementing and querying user-defined SQL functions. Requirements **Custom Type: `DateRange`** - Define a custom Python class `DateRange` that represents a date range with a start date and an end date. - Implement adapters to store `DateRange` as text in SQLite (ISO format). - Write a converter to retrieve `DateRange` objects from text stored in SQLite. **SQL Function: `calculate_span`** - Create a user-defined SQL function `calculate_span` that calculates the span (number of days) between the start and end dates of a `DateRange`. **Tables and Transactions** 1. Create a table `events` with the following columns: - `id` (INTEGER, PRIMARY KEY) - `name` (TEXT) - `date_range` (TEXT, to store `DateRange`) 2. Implement batch insertion of event records using transactions. Ensure that the transaction is atomic and rolls back in case of an error. 3. Query the maximum span of the date ranges using the custom SQL function `calculate_span`. Input and Output - **Input Data**: List of tuples with event names and their date ranges (as `DateRange` objects). - **Output**: Print the event with the maximum span and display this span. Constraints - Use placeholders to bind parameters to SQL queries to avoid SQL injection vulnerabilities. - Handle all exceptions and ensure the database connection is properly closed, committed, or rolled back. Example ```python import sqlite3 from datetime import date, datetime # Define DateRange custom type class DateRange: def __init__(self, start_date, end_date): self.start_date = start_date self.end_date = end_date def __str__(self): return f\'{self.start_date.isoformat()};{self.end_date.isoformat()}\' # Adapter and converter functions def adapt_date_range(dr): return str(dr) def convert_date_range(text): start_date, end_date = text.split(b\\";\\") return DateRange(datetime.strptime(start_date.decode(), \'%Y-%m-%d\').date(), datetime.strptime(end_date.decode(), \'%Y-%m-%d\').date()) # User-defined SQL function to calculate span def calculate_span(start_date, end_date): return (end_date - start_date).days # SQLite adapter and converter registration sqlite3.register_adapter(DateRange, adapt_date_range) sqlite3.register_converter(\\"DATERANGE\\", convert_date_range) def main(event_data): conn = sqlite3.connect(\':memory:\', detect_types=sqlite3.PARSE_DECLTYPES) conn.create_function(\\"calculate_span\\", 2, calculate_span) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE events ( id INTEGER PRIMARY KEY, name TEXT, date_range DATERANGE )\'\'\') try: with conn: cursor.executemany(\'\'\' INSERT INTO events (name, date_range) VALUES (?, ?) \'\'\', [(name, dr) for name, dr in event_data]) except sqlite3.Error as e: print(\\"Transaction failed:\\", e) # Implicit rollback cursor.execute(\'\'\' SELECT name, MAX(calculate_span( substr(date_range, 0, instr(date_range, \';\')), substr(date_range, instr(date_range, \';\') + 1) )) as max_span FROM events \'\'\') result = cursor.fetchone() if result: print(f\'The event with maximum span is {result[0]} with {result[1]} days\') conn.close() if __name__ == \\"__main__\\": event_data = [ (\\"Event1\\", DateRange(date(2023, 1, 1), date(2023, 1, 10))), (\\"Event2\\", DateRange(date(2023, 2, 15), date(2023, 3, 1))), (\\"Event3\\", DateRange(date(2023, 4, 5), date(2023, 4, 20))) ] main(event_data) ``` Detailed Steps 1. **Define the `DateRange` class**: - Define a constructor to initialize the start and end dates. - Implement the `__str__` method to convert the date range to a string in ISO format. 2. **Adapter and Converter**: - Implement the `adapt_date_range` function to convert `DateRange` to a string. - Implement the `convert_date_range` function to convert a string to a `DateRange`. - Register these functions with `sqlite3` using `register_adapter` and `register_converter`. 3. **SQL Function**: - Implement the `calculate_span` function to calculate the number of days between two dates. - Register this function with `sqlite3` using `create_function`. 4. **Database Operations**: - Connect to an SQLite database and enable detection of custom types. - Create the `events` table with the required columns. - Use a transaction to insert multiple event records; handle exceptions to rollback if an error occurs. - Use the `calculate_span` SQL function to query the event with the longest date range and print the result. - Close the database connection. Use the provided example code as the basis for your implementation. Ensure that the code is well-structured, handles exceptions, and demonstrates a thorough understanding of the `sqlite3` module\'s advanced features.","solution":"import sqlite3 from datetime import date, datetime # Define DateRange custom type class DateRange: def __init__(self, start_date, end_date): self.start_date = start_date self.end_date = end_date def __str__(self): return f\'{self.start_date.isoformat()};{self.end_date.isoformat()}\' # Adapter and converter functions def adapt_date_range(dr): return str(dr) def convert_date_range(text): start_date, end_date = text.split(b\\";\\") return DateRange(datetime.strptime(start_date.decode(), \'%Y-%m-%d\').date(), datetime.strptime(end_date.decode(), \'%Y-%m-%d\').date()) # User-defined SQL function to calculate span def calculate_span(start_date_str, end_date_str): start_date = datetime.strptime(start_date_str, \'%Y-%m-%d\').date() end_date = datetime.strptime(end_date_str, \'%Y-%m-%d\').date() return (end_date - start_date).days # SQLite adapter and converter registration sqlite3.register_adapter(DateRange, adapt_date_range) sqlite3.register_converter(\\"DATERANGE\\", convert_date_range) def main(event_data): conn = sqlite3.connect(\':memory:\', detect_types=sqlite3.PARSE_DECLTYPES) conn.create_function(\\"calculate_span\\", 2, calculate_span) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE events ( id INTEGER PRIMARY KEY, name TEXT, date_range DATERANGE )\'\'\') try: with conn: cursor.executemany(\'\'\' INSERT INTO events (name, date_range) VALUES (?, ?) \'\'\', [(name, dr) for name, dr in event_data]) except sqlite3.Error as e: print(\\"Transaction failed:\\", e) # Implicit rollback cursor.execute(\'\'\' SELECT name, MAX(calculate_span( substr(date_range, 0, instr(date_range, \';\')), substr(date_range, instr(date_range, \';\') + 1) )) as max_span FROM events \'\'\') result = cursor.fetchone() if result: print(f\'The event with maximum span is {result[0]} with {result[1]} days\') conn.close() if __name__ == \\"__main__\\": event_data = [ (\\"Event1\\", DateRange(date(2023, 1, 1), date(2023, 1, 10))), (\\"Event2\\", DateRange(date(2023, 2, 15), date(2023, 3, 1))), (\\"Event3\\", DateRange(date(2023, 4, 5), date(2023, 4, 20))) ] main(event_data)"},{"question":"Objective You are given a task to analyze the performance of a PyTorch-based neural network training script using `torch.utils.bottleneck`. You will need to implement a neural network, train it on a synthetic dataset, and profile it using `torch.utils.bottleneck`. Finally, you should extract and interpret the profiler results to identify any potential bottlenecks. Requirements 1. Implement a simple feedforward neural network in PyTorch. 2. Create a synthetic dataset for the network to train on. 3. Write a training loop to train the network. 4. Profile the training loop using `torch.utils.bottleneck`. 5. Extract and interpret the profiler results focusing on identifying whether the script is CPU-bound or GPU-bound. Details - **Neural Network**: - Input size: 100 - One hidden layer with 50 neurons and ReLU activation - Output size: 10 (Assume a classification task with 10 classes) - **Dataset**: - 10,000 samples - Each sample has 100 features - **Training Loop**: - Number of epochs: 5 - Batch size: 64 - Optimizer: SGD - Loss Function: CrossEntropyLoss - **Profiling**: - Use `torch.utils.bottleneck` to profile the training loop. - Extract the profiler output. Submission Your submission should include: 1. The implementations of the neural network, synthetic dataset creation, and training loop. 2. The profiling script using `torch.utils.bottleneck`. 3. A summary of the profiler output, indicating whether the bottleneck is CPU-bound or GPU-bound, and briefly explain the findings. Example ```python import torch import torch.nn as nn import torch.optim as optim import torch.utils.bottleneck # Define the neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create synthetic dataset def create_dataset(): X = torch.randn(10000, 100) # 10,000 samples, each with 100 features Y = torch.randint(0, 10, (10000,)) # 10,000 labels in the range [0, 9] return X, Y # Train the network def train(): model = SimpleNN() X, Y = create_dataset() dataset = torch.utils.data.TensorDataset(X, Y) dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) model.train() for epoch in range(5): # 5 epochs for inputs, labels in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if __name__ == \'__main__\': import subprocess # Profile with torch.utils.bottleneck p = subprocess.run([\'python\', \'-m\', \'torch.utils.bottleneck\', \'/path/to/your/script.py\']) print(p.stdout) ``` Notes 1. Make sure your script runs in a finite amount of time. 2. If CUDA is unavailable, set the device to CPU. 3. You do not need to submit the profiler\'s actual output, only the script and your summary/interpretation.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Define the neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create synthetic dataset def create_dataset(): X = torch.randn(10000, 100) # 10,000 samples, each with 100 features Y = torch.randint(0, 10, (10000,)) # 10,000 labels in the range [0, 9] return X, Y # Train the network def train(): model = SimpleNN() X, Y = create_dataset() dataset = TensorDataset(X, Y) dataloader = DataLoader(dataset, batch_size=64, shuffle=True) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) model.train() for epoch in range(5): # 5 epochs for inputs, labels in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if __name__ == \'__main__\': import subprocess # Profile with torch.utils.bottleneck p = subprocess.run([\'python\', \'-m\', \'torch.utils.bottleneck\', \'your_script.py\']) print(p.stdout)"},{"question":"**Image Format Detector with Custom Extension** You are tasked with creating a script that not only utilizes the `imghdr` module to detect standard image formats but also extends it to recognize a custom image format specific to your application. # Function: `detect_image_format(file: Union[str, BytesIO], custom_test: Optional[Callable[[bytes, Optional[Any]], str]] = None) -> str` Input: - `file`: This can either be a path-like object (string) which refers to an image file or a BytesIO object containing the byte stream of the image. - `custom_test` (optional): A function that takes two arguments (a byte stream and an optional open file-like object) and returns a string indicating the custom image format if the test succeeds, or `None` otherwise. Output: - A string describing the image format, which can be one of the recognized formats in the `imghdr` module, or the custom format specified by `custom_test`. If the format is not recognized, it should return `\'unknown\'`. Constraints: - You need to handle both file paths and byte streams. - Ensure that the function adds the custom test to `imghdr.tests` if `custom_test` is provided. - If the image format is not recognized, the function should return `\'unknown\'`. Example Usage: ```python from io import BytesIO # Sample file path usage print(detect_image_format(\'example.png\')) # \'png\' # Sample byte stream usage image_data = BytesIO() image_data.write(b\'x89PNGrnx1anx00x00x00rIHDRx00x00x00x10\') image_data.seek(0) print(detect_image_format(image_data)) # \'png\' # Custom test for a fake format def custom_format_test(data, file): if data.startswith(b\'FAKE\'): return \'fake_format\' return None # Using the custom test image_data = BytesIO() image_data.write(b\'FAKEimagecontent\') image_data.seek(0) print(detect_image_format(image_data, custom_test=custom_format_test)) # \'fake_format\' ``` # Notes: 1. Use the `imghdr.what` function to determine standard image formats. 2. For custom image formats, ensure your custom test is appended to `imghdr.tests`. 3. To reset `imghdr.tests` back to its original state after adding a custom test, you might want to store the original list of tests and restore it after usage.","solution":"import imghdr from io import BytesIO from typing import Union, Callable, Optional def detect_image_format(file: Union[str, BytesIO], custom_test: Optional[Callable[[bytes, Optional[any]], Optional[str]]] = None) -> str: original_tests = list(imghdr.tests) try: if custom_test: imghdr.tests.append(custom_test) if isinstance(file, str): format = imghdr.what(file) else: format = imghdr.what(None, file.read()) file.seek(0) # Reset the BytesIO object to its original position return format if format else \'unknown\' finally: imghdr.tests = original_tests"},{"question":"# Email Attachment Categorizer You are tasked to write a Python function that processes an email with various attachments and categorizes the attachments based on their MIME types. The function will take an email message as input and return a dictionary categorizing the attachments. Function Signature: ```python def categorize_attachments(email_content: bytes) -> dict: ``` Input: - `email_content`: A bytes object containing the raw email content. Output: - A dictionary where the keys are MIME types (excluding the \'multipart/*\' type) and the values are lists of filenames corresponding to that MIME type. If an attachment has no filename, generate a name using \\"part-<counter>\\" format and an appropriate extension based on its MIME type. Constraints: 1. The email may contain multiple attachments of different types including images, text files, PDFs, and other binary files. 2. Handle any missing filenames and ensure they are assigned unique, meaningful names based on MIME type. 3. The dictionary should not include \'multipart/\' as a key. 4. Assume you have a working SMTP server running locally for testing. Example: Given an email content with the following attachments: - \'image1.jpg\' (image/jpeg), - \'doc.txt\' (text/plain), - \'sheet.pdf\' (application/pdf), - One unnamed attachment with MIME type \'application/octet-stream\'. The function would return something like: ```python { \'image/jpeg\': [\'image1.jpg\'], \'text/plain\': [\'doc.txt\'], \'application/pdf\': [\'sheet.pdf\'], \'application/octet-stream\': [\'part-1.bin\'] } ``` Notes: - Use the `email` package to parse and process the email content. - For guessing extensions, consider using `mimetypes.guess_extension()`. You can use the following sample email content as a test case by saving it to a file and reading it as binary: ```email From: example@example.com To: recipient@example.com Subject: Test Email This is a test email with multiple attachments. ```","solution":"import email from email import policy from email.parser import BytesParser from collections import defaultdict import mimetypes def categorize_attachments(email_content: bytes) -> dict: msg = BytesParser(policy=policy.default).parsebytes(email_content) attachments = defaultdict(list) part_counter = 1 for part in msg.iter_attachments(): mime_type = part.get_content_type() if not mime_type.startswith(\'multipart/\'): filename = part.get_filename() if not filename: ext = mimetypes.guess_extension(mime_type) or \'.bin\' filename = f\'part-{part_counter}{ext}\' part_counter += 1 attachments[mime_type].append(filename) return attachments"},{"question":"**UUID Manipulation and Analysis** **Objective:** Your task is to write a Python function that will generate a specific type of UUID, analyze its attributes, and perform operations based on those attributes. **Description:** 1. Implement a function `analyze_uuid(uuid_input)` that takes a string `uuid_input` representing a UUID in its canonical form (e.g., \'12345678-1234-5678-1234-567812345678\'). 2. Your function should: - Convert the input string into a UUID object. - Determine the variant and version of the UUID. - Extract the `time_low`, `time_mid`, `time_hi_version`, `clock_seq_hi_variant`, `clock_seq_low`, and `node` fields from the UUID. - Convert the UUID to its hexadecimal string representation and big-endian byte string. - Return a dictionary with the following key-value pairs: - \'uuid_hex\': hexadecimal string of the UUID - \'uuid_bytes\': big-endian byte string of the UUID - \'time_low\': first 32 bits of the UUID - \'time_mid\': next 16 bits of the UUID - \'time_hi_version\': next 16 bits of the UUID - \'clock_seq_hi_variant\': next 8 bits of the UUID - \'clock_seq_low\': next 8 bits of the UUID - \'node\': last 48 bits of the UUID - \'variant\': variant of the UUID - \'version\': version of the UUID **Constraints:** - You can assume the input UUID string is always in its canonical form and valid. - Use the `uuid` module in Python for all operations. **Example:** ``` >>> analyze_uuid(\'12345678-1234-5678-1234-567812345678\') { \'uuid_hex\': \'12345678-1234-5678-1234-567812345678\', \'uuid_bytes\': b\'x124Vxx124Vxx124Vxx124Vx\', \'time_low\': 305419896, \'time_mid\': 4660, \'time_hi_version\': 22136, \'clock_seq_hi_variant\': 18, \'clock_seq_low\': 52, \'node\': 94527502334680, \'variant\': \'RFC_4122\', \'version\': 5 } ``` Implement the function `analyze_uuid` to fulfill the requirements described above. **Note:** You can import the `uuid` module for this task.","solution":"import uuid def analyze_uuid(uuid_input): Analyzes the given UUID and returns its attributes and computed values. Parameters: uuid_input (str): A string representing a UUID in canonical form. Returns: dict: A dictionary containing various extracted and computed values from the UUID. uuid_obj = uuid.UUID(uuid_input) return { \'uuid_hex\': str(uuid_obj), \'uuid_bytes\': uuid_obj.bytes, \'time_low\': uuid_obj.time_low, \'time_mid\': uuid_obj.time_mid, \'time_hi_version\': uuid_obj.time_hi_version, \'clock_seq_hi_variant\': uuid_obj.clock_seq_hi_variant, \'clock_seq_low\': uuid_obj.clock_seq_low, \'node\': uuid_obj.node, \'variant\': uuid_obj.variant, \'version\': uuid_obj.version }"},{"question":"# Pandas Options Management You are provided with a DataFrame containing data from a recent experiment. Your task is to implement a function that manipulates the display settings of pandas using its options API to make the data easier to review in different scenarios. The function should handle various display scenarios and ensure that the original settings are restored at the end. Function Specification: **Function Name**: `manipulate_display_options` **Parameters**: 1. `df`: A pandas DataFrame containing the experimental data. 2. `display_max_rows`: An integer specifying the maximum number of rows to display. 3. `display_max_columns`: An integer specifying the maximum number of columns to display. 4. `temp_max_rows`: An integer specifying the temporary maximum number of rows for a context. 5. `temp_max_columns`: An integer specifying the temporary maximum number of columns for a context. **Returns**: A dictionary with the following keys and values: - `\'original_max_rows\'`: The original `display.max_rows` setting. - `\'original_max_columns\'`: The original `display.max_columns` setting. - `\'temp_max_rows_display\'`: The DataFrame display output while the temporary settings are applied. - `\'reset_max_rows\'`: The `display.max_rows` setting after resetting to original. - `\'reset_max_columns\'`: The `display.max_columns` setting after resetting to original. Constraints: - The DataFrame `df` should be pretty-printed according to the settings provided. - Ensure that the display settings revert to their original values after the temporary settings are applied. ```python def manipulate_display_options(df, display_max_rows, display_max_columns, temp_max_rows, temp_max_columns): import pandas as pd from io import StringIO import sys # Get original settings original_max_rows = pd.get_option(\\"display.max_rows\\") original_max_columns = pd.get_option(\\"display.max_columns\\") # Set new display settings pd.set_option(\\"display.max_rows\\", display_max_rows) pd.set_option(\\"display.max_columns\\", display_max_columns) # Capture the display output with temporary settings old_stdout = sys.stdout sys.stdout = mystdout = StringIO() with pd.option_context(\\"display.max_rows\\", temp_max_rows, \\"display.max_columns\\", temp_max_columns): print(df) temp_max_rows_display = mystdout.getvalue() sys.stdout = old_stdout # Reset to original settings pd.reset_option(\\"display.max_rows\\") pd.reset_option(\\"display.max_columns\\") # Return settings and captured display return { \\"original_max_rows\\": original_max_rows, \\"original_max_columns\\": original_max_columns, \\"temp_max_rows_display\\": temp_max_rows_display, \\"reset_max_rows\\": pd.get_option(\\"display.max_rows\\"), \\"reset_max_columns\\": pd.get_option(\\"display.max_columns\\") } ``` **Example Usage**: ```python import pandas as pd import numpy as np # Create a sample DataFrame data = np.random.randn(10, 5) df = pd.DataFrame(data, columns=[f\'col_{i+1}\' for i in range(5)]) # Manipulate display options result = manipulate_display_options(df, 15, 10, 5, 3) print(result) ``` This problem will test the student\'s ability to understand and manipulate pandas options using various available functions and context managers effectively.","solution":"import pandas as pd from io import StringIO import sys def manipulate_display_options(df, display_max_rows, display_max_columns, temp_max_rows, temp_max_columns): # Get original settings original_max_rows = pd.get_option(\\"display.max_rows\\") original_max_columns = pd.get_option(\\"display.max_columns\\") # Set new display settings pd.set_option(\\"display.max_rows\\", display_max_rows) pd.set_option(\\"display.max_columns\\", display_max_columns) # Capture the display output with temporary settings old_stdout = sys.stdout sys.stdout = mystdout = StringIO() with pd.option_context(\\"display.max_rows\\", temp_max_rows, \\"display.max_columns\\", temp_max_columns): print(df) temp_max_rows_display = mystdout.getvalue() sys.stdout = old_stdout # Reset to original settings pd.reset_option(\\"display.max_rows\\") pd.reset_option(\\"display.max_columns\\") # Return settings and captured display return { \\"original_max_rows\\": original_max_rows, \\"original_max_columns\\": original_max_columns, \\"temp_max_rows_display\\": temp_max_rows_display, \\"reset_max_rows\\": pd.get_option(\\"display.max_rows\\"), \\"reset_max_columns\\": pd.get_option(\\"display.max_columns\\") }"},{"question":"# Custom Iterator with File I/O and Exception Handling **Objective:** Implement a custom iterator class that reads lines from a file. The iterator should handle exceptions gracefully and provide useful feedback in case of errors. **Problem Statement:** You need to create a class `FileLineIterator` which reads lines from a specified text file, one line at a time. This class should implement the iterator protocol, enabling the use of `for` loops to iterate through the lines of the file. # Requirements: 1. **Class Definition:** - Define a class `FileLineIterator` which implements the iterator protocol (`__iter__` and `__next__` methods). 2. **Initialization:** - The class constructor (`__init__`) should take a single parameter `filepath` which is the path to the text file to be read. 3. **File Handling:** - Open the file in `__init__` and ensure the file is properly closed when iteration is complete or an error occurs. Use a context manager to manage the file resource. 4. **Iterator Protocol:** - Implement the `__iter__` method to return the iterator object itself. - Implement the `__next__` method to read the next line from the file. If the end of the file is reached, raise the `StopIteration` exception. 5. **Exception Handling:** - Handle exceptions such as `FileNotFoundError` and provide an appropriate error message. - Handle any IOError or unforeseen exceptions, ensuring the file is closed properly and an informative error message is provided. 6. **Usage Example:** - Provide an example of how to use the `FileLineIterator` class to iterate through lines in a file. # Constraints: - Ensure that the file path provided is a valid text file. - Assume that the file will not exceed 1000 lines. # Input and Output: - **Input:** File path to a text file. - **Output:** Lines read from the file, one by one, using the iterator. Print appropriate error messages if exceptions occur. ```python class FileLineIterator: def __init__(self, filepath): self.filepath = filepath try: self.file = open(filepath, \'r\') except FileNotFoundError: print(f\\"Error: The file at {filepath} was not found.\\") self.file = None except IOError as e: print(f\\"Error: An IOError occurred while opening the file: {e}\\") self.file = None except Exception as e: print(f\\"An unexpected error occurred: {e}\\") self.file = None def __iter__(self): if self.file: return self else: raise StopIteration def __next__(self): if self.file: line = self.file.readline() if line: return line else: self.file.close() raise StopIteration else: raise StopIteration # Example usage: # Suppose \'example.txt\' contains: # Line 1 # Line 2 # Line 3 filename = \'example.txt\' iter_lines = FileLineIterator(filename) for line in iter_lines: print(line.strip()) ``` **Note:** Ensure the `example.txt` file exists in your working directory to test this example.","solution":"class FileLineIterator: def __init__(self, filepath): self.filepath = filepath try: self.file = open(filepath, \'r\') except FileNotFoundError: print(f\\"Error: The file at {filepath} was not found.\\") self.file = None except IOError as e: print(f\\"Error: An IOError occurred while opening the file: {e}\\") self.file = None except Exception as e: print(f\\"An unexpected error occurred: {e}\\") self.file = None def __iter__(self): if self.file: return self else: raise StopIteration def __next__(self): if self.file: line = self.file.readline() if line: return line.strip() # Remove trailing newline characters else: self.file.close() raise StopIteration else: raise StopIteration"},{"question":"Objective This exercise aims to assess your understanding of asynchronous execution and handling futures using PyTorch\'s `torch.futures` package. You will implement a function that demonstrates your capability to handle multiple asynchronous tasks and properly manage their results. Problem Statement You are given a list of functions that simulate asynchronous tasks using PyTorch\'s `Future` type. Your task is to implement a function `execute_tasks_in_parallel` that executes these tasks concurrently and collects their results. You will use the `torch.futures.Futures` utilities to manage the futures. Requirements: 1. The input to your function is a list of callables, where each callable returns a `torch.futures.Future` object when called. 2. Your function should return a list of results corresponding to the completed futures. 3. Ensure that all tasks are executed in parallel and the results are collected in the order the tasks were provided. Function Signature ```python from typing import List, Callable import torch.futures def execute_tasks_in_parallel(tasks: List[Callable[[], torch.futures.Future]]) -> List: pass ``` Example Usage ```python import torch import time # Simulating an asynchronous task that completes after a delay def async_task(result, delay): future = torch.futures.Future() def complete(): time.sleep(delay) future.set_result(result) torch.jit._fork(complete) return future # Example task list tasks = [ lambda: async_task(1, 2), # Task that returns 1 after 2 seconds lambda: async_task(2, 1), # Task that returns 2 after 1 second ] # Execute the tasks in parallel and collect their results results = execute_tasks_in_parallel(tasks) print(results) # Output should be [1, 2] ``` Constraints - Each task will always return a valid `torch.futures.Future` object. - The number of tasks (N) is such that (1 leq N leq 100). Notes - Make use of `torch.futures.collect_all` to collect all future results. - Ensure the results are in the same order as the tasks provided. Good luck!","solution":"from typing import List, Callable import torch.futures def execute_tasks_in_parallel(tasks: List[Callable[[], torch.futures.Future]]) -> List: futures = [task() for task in tasks] completed_futures = torch.futures.collect_all(futures).wait() results = [future.value() for future in completed_futures] return results"},{"question":"**Question: Implementing a Custom Container with Garbage Collection Support** You are tasked with creating a custom container type in Python that supports cyclic garbage collection using the `python310` package\'s C-API. # Objective: Implement a Python extension module that defines a container type capable of holding references to other objects. This container must support cyclic garbage collection as specified in the python310 package documentation. # Requirements: 1. **Container Type Definition**: - The container type must include the `Py_TPFLAGS_HAVE_GC` flag. - Implement a `tp_traverse` handler. - Implement a `tp_clear` handler if the container is mutable. 2. **Constructor and Deallocator**: - The constructor must use `PyObject_GC_New()`, `PyObject_GC_NewVar()`, and must call `PyObject_GC_Track()` after initializing fields. - The deallocator should call `PyObject_GC_UnTrack()` before deallocating using `PyObject_GC_Del()`. 3. **Basic Container Operations**: - Provide basic initialization, traversal, and clearing methods. - Implement query functions to check if the objects are being tracked or finalized by the garbage collector. # Input and Output: - There are no direct input/output requirements; focus on the correct implementation of the container type with garbage collection support. # Constraints: - Do not use Python\'s built-in garbage collection module directly. - Ensure the code is efficient and correctly handles cyclic references. # Performance Requirements: - The implementation should be efficient in terms of memory usage and garbage collection performance. # Example: Here\'s a basic outline to guide your implementation: ```c #include <Python.h> #include <stddef.h> // Define the container type structure typedef struct { PyObject_HEAD PyObject *item; // Reference to another PyObject } CustomContainerObject; // Implement the tp_traverse handler static int CustomContainer_traverse(CustomContainerObject *self, visitproc visit, void *arg) { Py_VISIT(self->item); return 0; } // Implement the tp_clear handler static int CustomContainer_clear(CustomContainerObject *self) { Py_CLEAR(self->item); return 0; } // Implement the constructor static CustomContainerObject * CustomContainer_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { CustomContainerObject *self; self = (CustomContainerObject*) PyObject_GC_New(CustomContainerObject, type); if (self != NULL) { self->item = NULL; PyObject_GC_Track(self); } return self; } // Implement the deallocator static void CustomContainer_dealloc(CustomContainerObject *self) { PyObject_GC_UnTrack(self); Py_XDECREF(self->item); PyObject_GC_Del(self); } // Define the type object static PyTypeObject CustomContainer_Type = { PyVarObject_HEAD_INIT(NULL, 0) \\"customcontainer.CustomContainer\\", /* tp_name */ sizeof(CustomContainerObject), /* tp_basicsize */ 0, /* tp_itemsize */ (destructor) CustomContainer_dealloc, /* tp_dealloc */ 0, /* tp_print */ 0, /* tp_getattr */ 0, /* tp_setattr */ 0, /* tp_reserved */ 0, /* tp_repr */ 0, /* tp_as_number */ 0, /* tp_as_sequence */ 0, /* tp_as_mapping */ 0, /* tp_hash */ 0, /* tp_call */ 0, /* tp_str */ 0, /* tp_getattro */ 0, /* tp_setattro */ 0, /* tp_as_buffer */ Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_GC, /* tp_flags */ 0, /* tp_doc */ (traverseproc)CustomContainer_traverse, /* tp_traverse */ (inquiry)CustomContainer_clear, /* tp_clear */ 0, /* tp_richcompare */ 0, /* tp_weaklistoffset */ 0, /* tp_iter */ 0, /* tp_iternext */ 0, /* tp_methods */ 0, /* tp_members */ 0, /* tp_getset */ 0, /* tp_base */ 0, /* tp_dict */ 0, /* tp_descr_get */ 0, /* tp_descr_set */ 0, /* tp_dictoffset */ 0, /* tp_init */ 0, /* tp_alloc */ (newfunc)CustomContainer_new, /* tp_new */ }; // Module definition static PyModuleDef customcontainermodule = { PyModuleDef_HEAD_INIT, \\"customcontainer\\", \\"Module that creates a custom container type with GC support.\\", -1, NULL, NULL, NULL, NULL, NULL }; // Module initialization PyMODINIT_FUNC PyInit_customcontainer(void) { PyObject *module; if (PyType_Ready(&CustomContainer_Type) < 0) return NULL; module = PyModule_Create(&customcontainermodule); if (module == NULL) return NULL; Py_INCREF(&CustomContainer_Type); PyModule_AddObject(module, \\"CustomContainer\\", (PyObject *) &CustomContainer_Type); return module; } ``` In this example, `CustomContainer` is a basic container type that holds a reference to another PyObject and supports garbage collection by implementing required handlers. Extend this according to specifications provided. Note: Your solution should be submitted as a C extension module file (`.c`) ready to be built and tested. Good luck!","solution":"# Since Python C-API and `python310` package are required for direct memory/Garbage Collection (GC) # operations which usually can\'t be directly tested in standard Python, we\'ll create a theoretical # implementation outline in Python-style pseudo-code. import ctypes # Load the Python C-API libpython = ctypes.CDLL(None) class CustomContainerObject: def __init__(self): self.item = None # Reference to another object # Assuming PyObject_GC_New and PyObject_GC_Track will be called here def __del__(self): # Assuming PyObject_GC_UnTrack and PyObject_GC_Del will be called here pass def tp_traverse(self, visit, arg): # Corresponds to the garbage collector visitproc if self.item: visit(self.item, arg) return 0 def tp_clear(self): # For removing references to other objects if self.item: self.item = None return 0 # This is a placeholder and would need to be compiled with the actual C-API code in Python extension modules. def PyObject_GC_New(type, cls): obj = CustomContainerObject() # Simulate PyObject_GC_New libpython.PyObject_GC_New.restype = ctypes.py_object return ctypes.POINTER(type)(obj) def PyObject_GC_Track(obj): # Simulate PyObject_GC_Track libpython.PyObject_GC_Track(ctypes.byref(obj)) def PyObject_GC_UnTrack(obj): # Simulate PyObject_GC_UnTrack libpython.PyObject_GC_UnTrack(ctypes.byref(obj)) def PyObject_GC_Del(obj): # Simulate PyObject_GC_Del libpython.PyObject_GC_Del(ctypes.byref(obj)) # Pseudo-code cannot be executed and this implementation is a blueprint for # the corresponding C/Python bridge using the Python C-API."},{"question":"You are tasked with building and evaluating a neural network model using scikit-learn\'s Multi-layer Perceptron (MLP) for a classification problem. The dataset you will use is the Iris dataset, which is a well-known dataset in the machine learning community. The goal is to classify the iris plants into three species (setosa, versicolor, and virginica) based on four features: sepal length, sepal width, petal length, and petal width. # Instructions 1. **Load the dataset**: Use scikit-learn\'s built-in functionality to load the Iris dataset. 2. **Preprocess the data**: Standardize the features using scikit-learn\'s `StandardScaler`. 3. **Build the model**: Implement a Multi-layer Perceptron (MLP) classifier using `sklearn.neural_network.MLPClassifier` with the following specifications: - Hidden layer sizes: (10, 5) - Solver: \'adam\' - Random state: 42 4. **Train the model**: Fit the model on the training data. 5. **Evaluate the model**: - Calculate the accuracy of the model on the test data. - Display the confusion matrix to evaluate the classification performance. # Input and Output Formats - **Input**: The function does not require any input parameters. - **Output**: - Accuracy of the model on the test data (floating-point number). - Confusion matrix (2D list or numpy array). # Example Code ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def evaluate_mlp_classifier(): # Load the dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Build the model clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', random_state=42) # Train the model clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Return the accuracy and confusion matrix return accuracy, conf_matrix # Call the function and print the results accuracy, conf_matrix = evaluate_mlp_classifier() print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) ``` # Constraints and Limitations - You should not change the random state or solver parameters to ensure reproducibility. - The dataset should be standardized before training the model.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def evaluate_mlp_classifier(): # Load the dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Build the model clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', random_state=42) # Train the model clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Return the accuracy and confusion matrix return accuracy, conf_matrix"},{"question":"# Command Line Argument Parser You are tasked with implementing a command line argument parser using the `getopt` module. Your parser should handle a mixture of long and short options and appropriately handle errors. The application should simulate a simple command-line interface for a hypothetical tool. Requirements: 1. **Function Definition**: Define a function `parse_command_line(argv)` that takes a list of command-line arguments `argv` and processes them. This list will typically be `sys.argv[1:]`. ```python def parse_command_line(argv): # Your code here ``` 2. **Options to Handle**: - Short options: - `-a`: A boolean switch that does not take an argument. - `-b`: A boolean switch that does not take an argument. - `-c <arg>`: An option that requires an argument. - Long options: - `--alpha`: Same as `-a`. - `--beta`: Same as `-b`. - `--char <arg>`: Same as `-c <arg>`. 3. **Output**: - Your function should return a dictionary summarizing the parsed options and their values. - It should also return a list of positional arguments (arguments that are not options). Example output format: ```python { \\"options\\": { \\"alpha\\": False, # or True \\"beta\\": False, # or True \\"char\\": \\"value\\" # or None if not provided }, \\"args\\": [\\"positional_argument1\\", \\"positional_argument2\\"] } ``` 4. **Exception Handling**: - Raise a `ValueError` with a descriptive message if there are any issues parsing the options (e.g., an option requiring an argument is not given one). 5. **Example Usage**: - Given the following input: `[\\"-a\\", \\"--beta\\", \\"-c\\", \\"test\\", \\"file1\\", \\"file2\\"]` Output should be: ```python { \\"options\\": { \\"alpha\\": True, \\"beta\\": True, \\"char\\": \\"test\\" }, \\"args\\": [\\"file1\\", \\"file2\\"] } ``` - Given the following input: `[\\"--char=example\\", \\"-a\\", \\"inputfile\\"]` Output should be: ```python { \\"options\\": { \\"alpha\\": True, \\"beta\\": False, \\"char\\": \\"example\\" }, \\"args\\": [\\"inputfile\\"] } ``` - Given the following input: `[\\"-a\\", \\"-c\\"]` Function should raise a `ValueError` with a message indicating the missing argument for `-c`. Implement the `parse_command_line` function to fulfill the requirements above.","solution":"import getopt import sys def parse_command_line(argv): Parses command-line options and arguments. Parameters: argv (list): List of command-line arguments. Returns: dict: Dictionary containing options and arguments. try: opts, args = getopt.getopt(argv, \\"abc:\\", [\\"alpha\\", \\"beta\\", \\"char=\\"]) except getopt.GetoptError as err: raise ValueError(f\\"Error parsing arguments: {err}\\") options = { \\"alpha\\": False, \\"beta\\": False, \\"char\\": None } for opt, arg in opts: if opt in (\\"-a\\", \\"--alpha\\"): options[\\"alpha\\"] = True elif opt in (\\"-b\\", \\"--beta\\"): options[\\"beta\\"] = True elif opt in (\\"-c\\", \\"--char\\"): options[\\"char\\"] = arg return {\\"options\\": options, \\"args\\": args}"},{"question":"**Objective:** You are tasked with implementing a utility class `MIMEUtility` that utilizes Python\'s `mimetypes` module to provide advanced MIME type functionalities. Your class should provide methods to: 1. Add new MIME type mappings. 2. Retrieve MIME type and encoding for a given file path. 3. List all possible file extensions for a given MIME type. 4. Write these mappings to and read them from a file. **Class Definition:** ```python class MIMEUtility: def __init__(self): Initializes the MIMEUtility object and sets up its internal MIME type database. pass def add_mapping(self, mime_type: str, extension: str, strict: bool = True) -> None: Adds a new MIME type to extension mapping. Parameters: - mime_type (str): The MIME type in \'type/subtype\' format. - extension (str): The corresponding file extension, including the leading dot (\'.\'). - strict (bool): If True, add to the list of standard types; if False, add to non-standard types. pass def guess_mime_type(self, file_path: str) -> (str, str): Returns the MIME type and encoding for the given file path. Parameters: - file_path (str): The file path or URL. Returns: - (str, str): A tuple containing the MIME type and encoding. pass def get_all_extensions(self, mime_type: str) -> list: Returns a list of all possible file extensions for the given MIME type. Parameters: - mime_type (str): The MIME type in \'type/subtype\' format. Returns: - list: A list of string extensions including the leading dot (\'.\'). pass def save_mappings_to_file(self, filename: str) -> None: Writes the current MIME type mappings to the specified file. Parameters: - filename (str): The file to which mappings will be written. pass def load_mappings_from_file(self, filename: str) -> None: Reads MIME type mappings from the specified file and adds them to the current database. Parameters: - filename (str): The file from which mappings will be read. pass ``` **Constraints:** - You are not allowed to use any external libraries other than Python\'s standard `mimetypes` module. - Your `save_mappings_to_file` and `load_mappings_from_file` should handle file read/write operations in a robust manner, ensuring the file contents are correctly parsed and stored. **Expected Functionalities:** - **Initialization**: On class instantiation, initialize the internal MIME type database. - **Add Mapping**: `add_mapping` should add a new MIME type to extension mapping properly, considering the `strict` argument. - **Guess MIME Type**: `guess_mime_type` should accurately return the MIME type and encoding for the given file path. - **Get All Extensions**: `get_all_extensions` should return all possible extensions for a given MIME type. - **Save and Load**: `save_mappings_to_file` and `load_mappings_from_file` should correctly handle file operations related to MIME type mappings. **Example Usage:** ```python mime_utility = MIMEUtility() mime_utility.add_mapping(\'application/x-custom\', \'.custom\') assert mime_utility.guess_mime_type(\'example.custom\') == (\'application/x-custom\', None) assert \'.custom\' in mime_utility.get_all_extensions(\'application/x-custom\') mime_utility.save_mappings_to_file(\'mime_mappings.txt\') mime_utility.load_mappings_from_file(\'mime_mappings.txt\') ``` **Performance Requirements:** - The methods should be efficient and capable of handling a large number of MIME type mappings. - File I/O operations should be handled efficiently with proper error checking mechanisms. **Deliverables:** - The implementation of the `MIMEUtility` class with the specified methods. - A test script demonstrating the correct functionality of all methods.","solution":"import mimetypes class MIMEUtility: def __init__(self): Initializes the MIMEUtility object and sets up its internal MIME type database. self.mimetypes = mimetypes def add_mapping(self, mime_type: str, extension: str, strict: bool = True) -> None: Adds a new MIME type to extension mapping. self.mimetypes.add_type(mime_type, extension, strict) def guess_mime_type(self, file_path: str) -> (str, str): Returns the MIME type and encoding for the given file path. return self.mimetypes.guess_type(file_path) def get_all_extensions(self, mime_type: str) -> list: Returns a list of all possible file extensions for the given MIME type. extensions = [] for ext, mt in self.mimetypes.types_map.items(): if mt == mime_type: extensions.append(ext) return extensions def save_mappings_to_file(self, filename: str) -> None: Writes the current MIME type mappings to the specified file. with open(filename, \'w\') as f: for ext, mt in self.mimetypes.types_map.items(): f.write(f\'{mt}t{ext}n\') def load_mappings_from_file(self, filename: str) -> None: Reads MIME type mappings from the specified file and adds them to the current database. with open(filename, \'r\') as f: for line in f: mt, ext = line.strip().split(\'t\') self.mimetypes.add_type(mt, ext)"},{"question":"# Objective: Write a function that demonstrates your ability to handle various system errors using the `errno` module in Python. Your function should simulate a scenario where multiple types of errors might occur and handle these errors appropriately. # Instructions: 1. Implement a function `error_handling_simulator` that receives a list of file paths and attempts to perform various operations on them. 2. The function should: - Try to open each file in read mode. - If it encounters an error, use the `errno` module to identify the error and handle it accordingly. - Errors to handle include: - `errno.ENOENT`: The file does not exist. Print `\\"FileNotFoundError: {file_path}\\"`. - `errno.EACCES`: Permission denied. Print `\\"PermissionError: {file_path}\\"`. - `errno.EISDIR`: Tried to open a directory as a file. Print `\\"IsADirectoryError: {file_path}\\"`. - Any other error: Print `\\"UnhandledError (code {errno_code}): {file_path}\\"`. # Function Signature: ```python def error_handling_simulator(file_paths: list): pass ``` # Example: ```python file_paths = [\\"test.txt\\", \\"no_access.txt\\", \\"/etc\\", \\"non_existent_file.txt\\"] error_handling_simulator(file_paths) ``` Expected Output: ``` UnhandledError (code 2): test.txt PermissionError: no_access.txt IsADirectoryError: /etc FileNotFoundError: non_existent_file.txt ``` # Notes: - You can assume that the list `file_paths` will contain valid strings representing file paths on your system. - Make sure to import the `os` and `errno` modules as needed to perform file operations and error handling effectively. - You should not catch exceptions other than the ones specified unless absolutely necessary to print the error message.","solution":"import os import errno def error_handling_simulator(file_paths: list): Receives a list of file paths and attempts to perform operations on them while handling various errors. for file_path in file_paths: try: with open(file_path, \'r\') as f: pass # Just try to open the file except OSError as e: if e.errno == errno.ENOENT: print(f\\"FileNotFoundError: {file_path}\\") elif e.errno == errno.EACCES: print(f\\"PermissionError: {file_path}\\") elif e.errno == errno.EISDIR: print(f\\"IsADirectoryError: {file_path}\\") else: print(f\\"UnhandledError (code {e.errno}): {file_path}\\")"},{"question":"**`Custom Virtual Environment Builder`** In this task, you are required to implement a custom virtual environment builder by subclassing the `venv.EnvBuilder` class. This custom builder should add some additional functionalities to the virtual environment creation process. # Requirements: 1. **Class Definition**: Define a class `CustomEnvBuilder` which subclasses `venv.EnvBuilder`. 2. **Custom Method**: Implement a method `install_custom_packages` within `CustomEnvBuilder` that installs a list of specified Python packages into the virtual environment using `pip`. 3. **Post Setup Hook**: Override the `post_setup` method in `CustomEnvBuilder` to call `install_custom_packages` with a predefined list of packages. 4. **Utils for Feedback**: Ensure that during the package installation, progress messages are printed to the console indicating the status of the installation. # Implementation Details: - **CustomEnvBuilder Class**: - `install_custom_packages(context, packages)`: This method should accept a `context` object and a list of package names to be installed using `pip`. - `post_setup(context)`: Override this method to call `install_custom_packages` with a list of packages `[\'requests\', \'flask\']`. # Input: You do not need to handle input directly from the user. Instead, you will be working with predefined values and methods within the class. # Output: The program should output progress messages for each package being installed. # Constraints: - You must use `venv.EnvBuilder` as the base class. - The `post_setup` method must be overridden to add the custom setup after the virtual environment creation. # Example Usage: ```python import venv class CustomEnvBuilder(venv.EnvBuilder): def install_custom_packages(self, context, packages): # Implementation for installing packages using pip pass def post_setup(self, context): self.install_custom_packages(context, [\'requests\', \'flask\']) # To use the CustomEnvBuilder builder = CustomEnvBuilder(with_pip=True) builder.create(\'/path/to/new/virtual/environment\') ``` **Your task is to implement the `CustomEnvBuilder` class as described above.**","solution":"import venv import subprocess import os class CustomEnvBuilder(venv.EnvBuilder): def install_custom_packages(self, context, packages): Install a list of packages into the created virtual environment. env_bin_path = context.env_exe env_dir = os.path.dirname(env_bin_path) pip_executable = os.path.join(env_dir, \'pip\') for package in packages: print(f\\"Installing package: {package}\\") subprocess.check_call([pip_executable, \'install\', package]) print(f\\"Successfully installed {package}\\") def post_setup(self, context): After the virtual environment setup, install custom packages. self.install_custom_packages(context, [\'requests\', \'flask\']) # Example usage: # builder = CustomEnvBuilder(with_pip=True) # builder.create(\'/path/to/new/virtual/environment\')"},{"question":"**Question: Implementing a Thread-Safe Counter using the `threading` module** # Problem Statement Implement a thread-safe counter using the `threading` module. The counter should be able to handle multiple threads incrementing and reading its value simultaneously without causing race conditions. Use appropriate synchronization primitives provided in the `threading` module to ensure thread safety. # Requirements 1. **Class Definition**: Define a class named `ThreadSafeCounter`. 2. **Initialization**: The constructor should initialize the counter to zero. 3. **Increment Method**: Implement an `increment` method that safely increments the counter. 4. **Get Method**: Implement a `get` method that safely retrieves the current value of the counter. 5. **Thread Safety**: Ensure all methods are thread-safe. # Constraints - No constraints on the number of threads accessing the counter. - Ensure minimal wait times for threads to minimize performance overhead. - Assume a multi-core machine where multiple threads can be executed. # Example Usage ```python import threading import time class ThreadSafeCounter: def __init__(self): # Initialize counter and necessary locks def increment(self): # Safely increment the counter def get(self): # Safely get the current counter value # Example usage def worker(counter): for _ in range(1000): counter.increment() print(f\\"Thread {threading.current_thread().name} finished\\") def main(): counter = ThreadSafeCounter() threads = [] # Create 10 threads for i in range(10): thread = threading.Thread(target=worker, args=(counter,)) threads.append(thread) thread.start() # Wait for all threads to finish for thread in threads: thread.join() print(\\"All threads finished\\") print(\\"Final counter value is\\", counter.get()) if __name__ == \\"__main__\\": main() ``` # Expected Output The final counter value should be `10000` after all threads have finished their execution since each of the 10 threads increments the counter 1000 times in the example provided.","solution":"import threading class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = threading.Lock() def increment(self): with self.lock: self.counter += 1 def get(self): with self.lock: return self.counter"},{"question":"Objective: Create a Python script using the `getopt` module that will parse and validate the following command line arguments and options: - Short options: `-u` (username), `-p` (password), `-s` (secure flag), `-h` (help) - Long options: `--username`, `--password`, `--secure`, `--help` Requirements: 1. Your script should accept and correctly parse both short and long versions of the options. 2. The `-u/--username` and `-p/--password` options require arguments (i.e., the username and password values). 3. The `-s/--secure` flag is a boolean switch that does not require an argument. 4. The `-h/--help` option should print out usage information for the script and then exit cleanly. 5. If any option is provided incorrectly or with missing arguments, your script should catch the exception and print an appropriate error message. 6. After parsing the arguments, the script should print out the values of `username`, `password`, and the secure flag status if provided correctly. 7. If both short and long options are provided for the same argument, the last provided value should be considered. Input: The input to the script will be a list of command line arguments, similar to `sys.argv[1:]`. Output: The script should print the parsed values or relevant error/help messages. Example Usage: For the input: ```python [\'-u\', \'user123\', \'-p\', \'pass123\', \'-s\'] ``` The script should produce the output: ``` Username: user123 Password: pass123 Secure: True ``` For the input: ```python [\'--username\', \'admin\', \'--password\', \'admin123\', \'--secure\'] ``` The script should produce the output: ``` Username: admin Password: admin123 Secure: True ``` For the input: ```python [\'--username\'] ``` The script should produce the error message: ``` option --username requires an argument ``` For the input: ```python [\'-h\'] ``` The script should produce the help message with command usage information and exit. Implementation: Please implement the script in Python using the `getopt` module according to the requirements detailed above. ```python import getopt import sys def usage(): print(\\"Usage: script.py -u <username> -p <password> [-s] [-h]\\") print(\\" or: script.py --username <username> --password <password> [--secure] [--help]\\") def main(argv): username = None password = None secure = False try: opts, args = getopt.getopt(argv, \\"hu:p:s\\", [\\"help\\", \\"username=\\", \\"password=\\", \\"secure\\"]) except getopt.GetoptError as err: print(err) usage() sys.exit(2) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): usage() sys.exit() elif opt in (\\"-u\\", \\"--username\\"): username = arg elif opt in (\\"-p\\", \\"--password\\"): password = arg elif opt in (\\"-s\\", \\"--secure\\"): secure = True if username is None or password is None: print(\\"Username and password are required.\\") usage() sys.exit(2) print(f\\"Username: {username}\\") print(f\\"Password: {password}\\") print(f\\"Secure: {secure}\\") if __name__ == \\"__main__\\": main(sys.argv[1:]) ``` Constraints: - Ensure proper error handling and user feedback. - The script should handle both short and long options seamlessly. - Remember to provide clear usage information when the `-h/--help` option is invoked.","solution":"import getopt import sys def usage(): print(\\"Usage: script.py -u <username> -p <password> [-s] [-h]\\") print(\\" or: script.py --username <username> --password <password> [--secure] [--help]\\") def parse_args(argv): username = None password = None secure = False try: opts, args = getopt.getopt(argv, \\"hu:p:s\\", [\\"help\\", \\"username=\\", \\"password=\\", \\"secure\\"]) except getopt.GetoptError as err: print(err) usage() sys.exit(2) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): usage() sys.exit() elif opt in (\\"-u\\", \\"--username\\"): username = arg elif opt in (\\"-p\\", \\"--password\\"): password = arg elif opt in (\\"-s\\", \\"--secure\\"): secure = True if username is None or password is None: print(\\"Username and password are required.\\") usage() sys.exit(2) return username, password, secure if __name__ == \\"__main__\\": username, password, secure = parse_args(sys.argv[1:]) print(f\\"Username: {username}\\") print(f\\"Password: {password}\\") print(f\\"Secure: {secure}\\")"},{"question":"# Seaborn Faceting and Visualization You are tasked with demonstrating your understanding of seaborn’s faceting capabilities by visualizing data from the `titanic` dataset, provided by seaborn. Input 1. Load the `titanic` dataset using `seaborn.load_dataset(\'titanic\')`. 2. Create a `seaborn.objects.Plot` object to visualize the relationships between the following pairs of variables: - `age` vs `fare` - `pclass` vs `fare` Output 1. For each pair of variables: - Generate a scatter plot with `so.Dots`. 2. Apply faceting with multiple variables: - Facet by `sex` and `embarked` for the plot of `age` vs `fare`. Ensure that the facets are ordered such that `sex` is in columns with the order `[\'female\', \'male\']` and `embarked` in rows with the order `[\'C\', \'Q\', \'S\']`. - For the plot of `pclass` vs `fare`, facet by `class` and `sex`. Order the facets as `class` in columns with the order `[1, 2, 3]` and `sex` in rows with the order `[\'female\', \'male\']`. 3. Customize the facet labels and titles: - Add a title to each facet plot indicating the variable names being plotted. - Label rows and columns appropriately for better readability. Example ```python import seaborn.objects as so from seaborn import load_dataset # Load the Titanic dataset titanic = load_dataset(\'titanic\') # Plot age vs fare faceted by sex and embarked p1 = so.Plot(titanic, \\"age\\", \\"fare\\").add(so.Dots()) p1.facet(\\"sex\\", \\"embarked\\", order={\\"col\\": [\\"female\\", \\"male\\"], \\"row\\": [\\"C\\", \\"Q\\", \\"S\\"]}).label(title=\\"Age vs Fare\\") # Plot pclass vs fare faceted by class and sex p2 = so.Plot(titanic, \\"pclass\\", \\"fare\\").add(so.Dots()) p2.facet(\\"class\\", \\"sex\\", order={\\"col\\": [1, 2, 3], \\"row\\": [\\"female\\", \\"male\\"]}).label(title=\\"Pclass vs Fare\\") ``` Write the code to complete this task. Ensure your plots are clear, well-labeled, and adhere to the given specifications.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_titanic_facet_plots(): # Load the Titanic dataset titanic = load_dataset(\'titanic\') # Plot age vs fare faceted by sex and embarked plot1 = ( so.Plot(titanic, \\"age\\", \\"fare\\") .add(so.Dots()) .facet(\\"sex\\", \\"embarked\\", order={\\"col\\": [\\"female\\", \\"male\\"], \\"row\\": [\\"C\\", \\"Q\\", \\"S\\"]}) .label(title=\\"Age vs Fare by Sex and Embarked\\", xlabel=\\"Age\\", ylabel=\\"Fare\\", col=\\"Sex\\", col_order=[\\"female\\", \\"male\\"], row=\\"Embarked\\", row_order=[\\"C\\", \\"Q\\", \\"S\\"], title_fontsize=14, axis_fontsize=12) ) plot1.show() # Plot pclass vs fare faceted by class and sex plot2 = ( so.Plot(titanic, \\"pclass\\", \\"fare\\") .add(so.Dots()) .facet(\\"class\\", \\"sex\\", order={\\"col\\": [1, 2, 3], \\"row\\": [\\"female\\", \\"male\\"]}) .label(title=\\"Pclass vs Fare by Class and Sex\\", xlabel=\\"Pclass\\", ylabel=\\"Fare\\", col=\\"Class\\", col_order=[1, 2, 3], row=\\"Sex\\", row_order=[\\"female\\", \\"male\\"], title_fontsize=14, axis_fontsize=12) ) plot2.show()"},{"question":"# Advanced Coding Assessment Question **Objective:** Implement a function that performs both audio normalization and stereo to mono conversion on a given audio fragment. Normalization means adjusting the amplitude of the audio samples so that the maximum absolute sample value is scaled to a specified maximum value. **Task:** Write a Python function `normalize_stereo_to_mono(audio_fragment: bytes, width: int, target_max_amplitude: int) -> bytes` that performs the following operations: 1. Normalize the stereo audio fragment. 2. Convert the normalized stereo audio fragment to mono. **Function Signature:** ```python def normalize_stereo_to_mono(audio_fragment: bytes, width: int, target_max_amplitude: int) -> bytes: pass ``` **Parameters:** - `audio_fragment (bytes)`: The input audio fragment in stereo format (2 channel audio). - `width (int)`: The sample width in bytes (1, 2, 3, or 4). - `target_max_amplitude (int)`: The target maximum amplitude to which the audio should be normalized. **Returns:** - `bytes`: The resulting mono audio fragment after normalization. **Constraints:** - The input audio fragment will always be in stereo format. - The `target_max_amplitude` must be a valid integer within the range of the sample width. **Example:** ```python input_fragment = audioop.lin2lin(b\'x01x02x03x04x05x06x07x08\', 2, 2) # Some stereo data normalized_mono = normalize_stereo_to_mono(input_fragment, 2, 32767) print(normalized_mono) ``` **Explanation:** 1. **Normalization**: - Calculate the current maximum absolute sample value in the audio fragment using the `audioop.max()` function. - Compute the normalization factor to scale the maximum absolute sample value to `target_max_amplitude`. - Apply this normalization factor to all samples in the fragment using the `audioop.mul()` function. 2. **Stereo to Mono Conversion**: - Convert the normalized stereo audio to mono using the `audioop.tomono()` function with equal weighting factors for the left and right channels. **Notes:** - Ensure to handle potential overflows or underflows during normalization. - Thoroughly test the function with different sample widths and target amplitudes. This question assesses your understanding of the `audioop` module, specifically in manipulating audio fragments, performing normalization, and handling stereo to mono conversion.","solution":"import audioop def normalize_stereo_to_mono(audio_fragment: bytes, width: int, target_max_amplitude: int) -> bytes: Normalizes a stereo audio fragment and converts it to mono. :param audio_fragment: The input audio fragment in stereo format (2 channel audio). :param width: The sample width in bytes (1, 2, 3, or 4). :param target_max_amplitude: The target maximum amplitude to which the audio should be normalized. :return: The resulting mono audio fragment after normalization. # Step 1: Normalize the stereo audio fragment. current_max_amplitude = audioop.max(audio_fragment, width) if current_max_amplitude == 0: normalization_factor = 1 # Avoid division by zero else: normalization_factor = target_max_amplitude / current_max_amplitude normalized_stereo = audioop.mul(audio_fragment, width, normalization_factor) # Step 2: Convert the normalized stereo audio to mono. mono_audio = audioop.tomono(normalized_stereo, width, 0.5, 0.5) return mono_audio"},{"question":"You are given a set of operations and data in real tensor form. Your task is to write a function that converts these real tensors into complex tensors, applies a series of transformations, and returns the resulting complex tensor. Function Signature ```python import torch def transform_complex_tensor(real_tensor: torch.Tensor, real_ops: list, imag_ops: list) -> torch.Tensor: pass ``` Input 1. `real_tensor`: A real tensor of shape `(..., 2)` representing complex numbers where the last dimension contains the real and imaginary parts. 2. `real_ops`: A list of operations to apply on the real part of the complex tensor. 3. `imag_ops`: A list of operations to apply on the imaginary part of the complex tensor. Each operation is a tuple consisting of a string representing the operation and optional parameters. Supported operations: - `\\"add\\", value` - `\\"mul\\", value` - `\\"neg\\"` (negate the tensor) Output - A complex tensor after applying the respective operations to the real and imaginary parts. Example ```python real_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # represents [1+2j, 3+4j] real_ops = [(\\"add\\", 2), (\\"mul\\", 3)] imag_ops = [(\\"neg\\",), (\\"mul\\", 0.5)] result = transform_complex_tensor(real_tensor, real_ops, imag_ops) print(result) # Output: tensor([9.0000-1.0000j, 15.0000-2.0000j]) ``` Constraints - Ensure efficient tensor operations without unnecessary copying. - Handle invalid operation strings by raising an appropriate error. Performance Requirements - The complexity of operations should be linear with respect to the number of elements in the tensor. # Detailed Requirements 1. Convert `real_tensor` to a complex tensor using `torch.view_as_complex`. 2. Apply each operation in `real_ops` to the real part of the complex tensor. 3. Apply each operation in `imag_ops` to the imaginary part of the complex tensor. 4. Return the resulting complex tensor. Make sure to leverage PyTorch\'s in-built functions for operations to ensure optimized performance.","solution":"import torch def apply_operations(tensor: torch.Tensor, ops: list) -> torch.Tensor: for op in ops: if op[0] == \\"add\\": tensor = tensor + op[1] elif op[0] == \\"mul\\": tensor = tensor * op[1] elif op[0] == \\"neg\\": tensor = -tensor else: raise ValueError(f\\"Invalid operation: {op[0]}\\") return tensor def transform_complex_tensor(real_tensor: torch.Tensor, real_ops: list, imag_ops: list) -> torch.Tensor: real_part = real_tensor[..., 0] imag_part = real_tensor[..., 1] real_part = apply_operations(real_part, real_ops) imag_part = apply_operations(imag_part, imag_ops) complex_tensor = torch.complex(real_part, imag_part) return complex_tensor"},{"question":"# Question: Building and Evaluating a Data Processing Pipeline in scikit-learn You are provided with a dataset that contains both numerical and categorical features. Your task is to build a machine learning pipeline that preprocesses the data and trains a model to predict a target variable. Specifically, you need to: 1. **Preprocess the data**: - Normalize numerical features using `StandardScaler`. - Encode categorical features using `OneHotEncoder`. 2. **Build a machine learning pipeline**: - Combine the preprocessing steps using `ColumnTransformer`. - Append the preprocessing step to a classification model using `Pipeline`. 3. **Optimize the pipeline**: - Perform hyperparameter tuning using `GridSearchCV` to find the best parameters for the model. 4. **Evaluate the pipeline**: - Evaluate the performance of the best pipeline on a test set. # Implementation Details: 1. **Input**: - A dataset with numerical and categorical features. - The target variable to predict. 2. **Constraints**: - Use `StandardScaler` for numerical features. - Use `OneHotEncoder` for categorical features. - The classifier to use is `LogisticRegression`. 3. **Output**: - The best parameters found during grid search. - The accuracy of the best pipeline on the test set. # Example Code Skeleton: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load your dataset df = pd.read_csv(\'your_dataset.csv\') # Define features and target X = df.drop(\'target\', axis=1) y = df[\'target\'] # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'numerical_feature1\', \'numerical_feature2\']), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), [\'categorical_feature1\', \'categorical_feature2\']) ]) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Define the parameter grid for GridSearchCV param_grid = { \'classifier__C\': [0.1, 1.0, 10.0], \'classifier__solver\': [\'liblinear\', \'lbfgs\'] } # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the best model on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Output the results print(f\\"Best Parameters: {grid_search.best_params_}\\") print(f\\"Test Set Accuracy: {accuracy}\\") ``` # Notes: - Make sure to handle missing values in the dataset if there are any. - Get creative with the dataset by adding more numerical and categorical features. - Consider extending the pipeline with feature engineering steps if needed. Good luck!","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def build_and_evaluate_pipeline(df, target_column, numerical_features, categorical_features): Build, optimize, and evaluate a machine learning pipeline on the given dataset. Parameters: - df: pandas DataFrame containing the dataset. - target_column: str, the name of the target variable column. - numerical_features: list of str, the names of the numerical feature columns. - categorical_features: list of str, the names of the categorical feature columns. Returns: - best_params: dict, the best parameters found during grid search. - accuracy: float, the accuracy of the best pipeline on the test set. # Define features and target X = df.drop(target_column, axis=1) y = df[target_column] # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_features), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_features) ]) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Define the parameter grid for GridSearchCV param_grid = { \'classifier__C\': [0.1, 1.0, 10.0], \'classifier__solver\': [\'liblinear\', \'lbfgs\'] } # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the best model on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Output the results best_params = grid_search.best_params_ return best_params, accuracy"},{"question":"Objective Your task is to create a function that generates a set of line plots with different plotting contexts using the seaborn package. Requirements 1. Create a function `generate_line_plots(data, contexts)` where: - `data` is a dictionary where the keys are the names of the plots (strings) and the values are tuples of `(x, y)` lists representing the data points for the line plot. - `contexts` is a list of seaborn plotting contexts (strings) to be applied in sequence to each plot. 2. The function should: - Iterate over the provided contexts. - For each context, apply it using a context manager. - Generate and display a line plot for each context with corresponding data from the `data` dictionary. - The title of each plot should include the name of the context used. 3. Expected input and output formats: - Input: `generate_line_plots({\\"Plot 1\\": ([\\"A\\", \\"B\\", \\"C\\"], [1, 3, 2]), \\"Plot 2\\": ([\\"A\\", \\"B\\", \\"C\\"], [2, 1, 3])}, [\\"notebook\\", \\"talk\\", \\"paper\\"])` - Output: Three plots, each with the different plotting style contexts applied, displayed in sequence. Constraints - Use only the seaborn package for generating the plots. - Assume that the `data` dictionary and `contexts` list are always non-empty and contain valid entries. - Ensure the generated plots are displayed correctly within a Jupyter notebook or any other standard Python environment that supports plotting. Performance This task does not have strict performance requirements. The focus is on correct application of plotting contexts and visualizing the line plots. Example ```python import seaborn as sns import matplotlib.pyplot as plt def generate_line_plots(data, contexts): for context in contexts: with sns.plotting_context(context): for plot_name, (x, y) in data.items(): plt.figure() sns.lineplot(x=x, y=y) plt.title(f\'Context: {context}, Plot: {plot_name}\') plt.show() # Example usage data = { \\"Plot 1\\": ([\\"A\\", \\"B\\", \\"C\\"], [1, 3, 2]), \\"Plot 2\\": ([\\"A\\", \\"B\\", \\"C\\"], [2, 1, 3]) } contexts = [\\"notebook\\", \\"talk\\", \\"paper\\"] generate_line_plots(data, contexts) ``` This example will generate and display three sets of line plots, each with the specified plotting context applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_line_plots(data, contexts): Generate and display a set of line plots with different seaborn plotting contexts. Parameters: - data: dictionary where keys are plot names and values are tuples of (x, y) lists. - contexts: list of seaborn plotting contexts to apply to each plot. for context in contexts: for plot_name, (x, y) in data.items(): with sns.plotting_context(context): plt.figure() sns.lineplot(x=x, y=y) plt.title(f\'Context: {context}, Plot: {plot_name}\') plt.show()"},{"question":"# Coding Assessment: Advanced Seaborn Figure Customization **Objective:** In this assessment, you are required to demonstrate your understanding and ability to manipulate plot aesthetics using the seaborn library. You will generate multiple plots by applying different seaborn styles and contexts, and customize certain elements according to specified requirements. **Problem Statement:** You are provided with a dataset that consists of a normal distribution of numbers. This dataset needs to be visualized through various plots with specific aesthetic modifications using seaborn. Follow the instructions below to create and customize your plots. **Instructions:** 1. **Generate the Data:** - Create a normally distributed dataset with 60 rows and 4 columns using `numpy`. 2. **Plot 1: Default Seaborn Theme** - Create a boxplot of the dataset with the default seaborn theme (`darkgrid`). 3. **Plot 2: Custom Style and Despine** - Modify the plot to use the `whitegrid` style. - Remove the top and right spines from the plot. 4. **Plot 3: Temporary Style within a context** - Within a `with` statement, use the `ticks` style. - Create a sine plot with 10 waves, but with a grid layout of 2x2 showing different flip values (1, -1, 1, -1). 5. **Plot 4: Scale Context** - Change the plotting context to `talk`. - Create a violin plot of the dataset. - Offset the axis spines by 15 and use the `trim` parameter to limit the range of the axis spines. **Requirements:** - Use seaborn and matplotlib to create and customize the plots. - Ensure that each plot is clearly labeled and easily distinguishable. - Properly manage the style contexts so that one plot does not affect the others. **Grading Criteria:** - Correctly generate and visualizes the dataset. - Proper application of seaborn themes and styles to plots. - Accurate and effective use of the `with` statement for temporary style changes. - Correct scale modifications using plotting contexts. - Clean and readable plots with the specified customizations. **Example Input:** The code to generate the dataset: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Generate normally distributed data data = np.random.normal(size=(60, 4)) ``` **Submission:** Submit your Python script or Jupyter notebook that includes: - Code to generate the dataset and create the specified plots. - Plots displayed inline with proper labels and titles. Good luck!","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Generate normally distributed data data = np.random.normal(size=(60, 4)) def plot_default_seaborn_theme(data): Create a boxplot of the dataset with the default seaborn theme. sns.set_theme(style=\\"darkgrid\\") sns.boxplot(data=data) plt.title(\'Boxplot with Default Seaborn Theme\') plt.show() def plot_custom_style_despine(data): Modify the plot to use the whitegrid style and remove the top and right spines from the plot. sns.set_theme(style=\\"whitegrid\\") sns.boxplot(data=data) sns.despine(top=True, right=True) plt.title(\'Boxplot with Whitegrid Style and Despined\') plt.show() def plot_temporary_style_context(): Use the ticks style within a with statement and create a sine plot with a grid layout of 2x2. with sns.axes_style(\\"ticks\\"): fig, axs = plt.subplots(2, 2, figsize=(10, 8)) x = np.linspace(0, 14, 100) for i, ax in enumerate(axs.flat): ax.plot(x, np.sin(x) * (-1)**i) ax.set_title(f\'Sine Wave {i+1}\') plt.suptitle(\'Sine Plots with Ticks Style\') plt.show() def plot_scale_context(data): Change the plotting context to talk, create a violin plot of the dataset, and offset the axis spines. sns.set_context(\\"talk\\") sns.violinplot(data=data) sns.despine(offset=15, trim=True) plt.title(\'Violin Plot with Talk Context\') plt.show()"},{"question":"# Question Objective: You need to implement a function that takes a message and generates secure signatures using the BLAKE2b cryptographic hash function with certain customizations. Problem Statement: Implement a function `generate_signature` that performs the following tasks: - Takes a string message and a string key as inputs. - Uses the BLAKE2b hash function from the hashlib module to generate a keyed hash of the message. - Personalizes the hash function to distinguish between different uses of the hash (e.g., \'APP1\' and \'APP2\'). Function Signature: ```python def generate_signature(message: str, key: str, personalization: str) -> str: pass ``` Input: - `message` (str): The message to be hashed. - `key` (str): The key used for keyed hashing (ensure it is properly encoded to bytes). - `personalization` (str): A string to personalize the hash function (ensure it is properly encoded to bytes and of appropriate length). Output: - Returns a string that is the hexadecimal digest of the BLAKE2b hash. Constraints: - The length of the key must not exceed 64 bytes. - The personalization string length must be at most 16 bytes. Example: ```python message = \\"Confidential Information\\" key = \\"SuperSecretKey\\" personalization = \\"APP1\\" # Example output signature = generate_signature(message, key, personalization) print(signature) # Should print a hexadecimal string such as \'3d363ff7401e02026f4a4687d4863ced\' ``` Notes: 1. Ensure that the `key` and `personalization` parameters are properly converted to byte strings. 2. Use the `digest_size` of 16 bytes for the hash output. 3. The function should be able to handle exceptions where key or personalization lengths exceed the allowed limit by printing appropriate error messages. Hints: - You can use the `hashlib.blake2b()` function with parameters like `digest_size`, `key`, and `person`. - Use the `hexdigest()` method to get the hexadecimal string of the digest. Evaluation: Your solution will be evaluated based on: - Correctness and completeness of the implementation. - Handling of edge cases and constraints. - Code readability and comments explaining your logic.","solution":"import hashlib def generate_signature(message: str, key: str, personalization: str) -> str: Generates a BLAKE2b hash of the message using a key and personalization. Parameters: - message (str): The message to be hashed. - key (str): The key used for keyed hashing. - personalization (str): A string to personalize the hash function. Returns: - A hex string of the BLAKE2b hash. # Validate inputs if len(key.encode(\'utf-8\')) > 64: raise ValueError(\\"Key length must not exceed 64 bytes\\") if len(personalization.encode(\'utf-8\')) > 16: raise ValueError(\\"Personalization length must not exceed 16 bytes\\") # Encode the key and personalization key_bytes = key.encode(\'utf-8\') personalization_bytes = personalization.encode(\'utf-8\') # Create the hash object blake2b_hash = hashlib.blake2b( key=key_bytes, digest_size=16, person=personalization_bytes ) # Update the hash object with the message blake2b_hash.update(message.encode(\'utf-8\')) # Return the hexadecimal digest return blake2b_hash.hexdigest()"},{"question":"Background Given the complete PEG grammar for Python as described, the task is to create a function that processes an Abstract Syntax Tree (AST) and performs a specific analysis or transformation. Problem You need to implement a Python function, `extract_function_definitions`, that takes an Abstract Syntax Tree (AST) of Python code as input and returns a list of all function definitions present in the code. Each function definition should be represented as a dictionary containing the function\'s name, parameters, and return annotation (if any). The expected input will be an AST generated by the `ast` module for a given piece of Python code. Function Signature ```python import ast from typing import List, Dict, Union def extract_function_definitions(tree: ast.AST) -> List[Dict[str, Union[str, List[str]]]]: pass ``` Input - `tree` (ast.AST): An Abstract Syntax Tree (AST) object representing the Python code. Output - `List[Dict[str, Union[str, List[str]]]]`: A list of dictionaries where each dictionary represents a function definition with the following keys: - `\\"name\\"` (str): The name of the function. - `\\"parameters\\"` (List[str]): A list of parameter names of the function. - `\\"return_annotation\\"` (str): The return annotation of the function, or `None` if not specified. Example Given the Python code: ```python def add(a, b): return a + b def greet(name: str) -> str: return f\\"Hello, {name}\\" def no_params() -> None: pass ``` The AST generated for the above code would be parsed, and the function `extract_function_definitions` should have the following output: ```python [ {\\"name\\": \\"add\\", \\"parameters\\": [\\"a\\", \\"b\\"], \\"return_annotation\\": None}, {\\"name\\": \\"greet\\", \\"parameters\\": [\\"name\\"], \\"return_annotation\\": \\"str\\"}, {\\"name\\": \\"no_params\\", \\"parameters\\": [], \\"return_annotation\\": \\"None\\"} ] ``` Constraints - The input AST will always represent valid Python code. - Parameters and return annotations should be handled as strings as they appear in the code. - The function should handle functions with and without type annotations. - You may use the `ast` module to navigate and analyze the AST structure. Notes - You can refer to the [Python `ast` module documentation](https://docs.python.org/3/library/ast.html) for assistance on working with AST nodes. Good luck and happy coding!","solution":"import ast from typing import List, Dict, Union, Optional def extract_function_definitions(tree: ast.AST) -> List[Dict[str, Union[str, List[str], None]]]: Extracts function definitions from the given AST and returns a list of dictionaries, each containing the function\'s name, parameters, and return annotation. function_definitions = [] for node in ast.walk(tree): if isinstance(node, ast.FunctionDef): function_info = { \\"name\\": node.name, \\"parameters\\": [arg.arg for arg in node.args.args], \\"return_annotation\\": ast.unparse(node.returns) if node.returns else None } function_definitions.append(function_info) return function_definitions"},{"question":"# Objective: Demonstrate your ability to build and use composite estimators in scikit-learn by creating a machine learning pipeline that includes multiple preprocessing and modeling steps. # Task: You are provided with the \'California Housing\' dataset. Your task is to create a pipeline that: 1. Preprocesses the given data using different transformers for numerical and categorical features. 2. Utilizes a feature selector. 3. Trains a regression model with transformed target values. # Instructions: 1. Load the \'California Housing\' dataset using `fetch_california_housing`. 2. Split the data into training and testing sets using `train_test_split`. 3. Build the pipeline to include the following steps: - **Column Transformation**: Use `ColumnTransformer` to: - Apply `StandardScaler` to numerical features. - Apply `OneHotEncoder` to categorical features (Use \'ocean_proximity\' for this example). - **Feature Selection**: Use `SelectKBest` to select the top 5 features. - **Regression Model**: Use `TransformedTargetRegressor` to: - Apply a logarithmic transformation to the target. - Use `Ridge` regression model. 4. Train the pipeline on the training data. 5. Evaluate the pipeline\'s performance using the testing data and report the R2 score. # Requirements: - Your solution should use the scikit-learn library. - The expected input is the dataset loaded from `fetch_california_housing`. - The output should be a trained pipeline and an R2 score on the test set. # Code Template: ```python import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer from sklearn.pipeline import Pipeline from sklearn.feature_selection import SelectKBest from sklearn.linear_model import Ridge from sklearn.metrics import r2_score import pandas as pd # 1. Load the data housing = fetch_california_housing(as_frame=True) X = housing.data.copy() y = housing.target.copy() # For illustration purpose, we convert \'ocean_proximity\' to a categorical feature. # Assuming \'longitude\' sets the boundary for the ocean proximity (example simplification) X[\'ocean_proximity\'] = np.where(X[\'Longitude\'] < -119, \'Near Ocean\', \'Inland\') # 2. Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # 3. Build the pipeline numeric_features = X.select_dtypes(include=[np.number]).columns.tolist() categorical_features = [\'ocean_proximity\'] # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numeric_features), (\'cat\', OneHotEncoder(), categorical_features) ]) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'select\', SelectKBest(k=5)), (\'regressor\', TransformedTargetRegressor( regressor=Ridge(), transformer=FunctionTransformer(np.log1p, np.expm1) )) ]) # 4. Train the pipeline pipeline.fit(X_train, y_train) # 5. Evaluate the pipeline y_pred = pipeline.predict(X_test) score = r2_score(y_test, y_pred) # Print the R2 score print(f\'R2 score: {score:.2f}\') ``` # Constraints: 1. The pipeline must handle both numerical and categorical data correctly. 2. The feature selector must select exactly 5 features. 3. The regression model should utilize `Ridge` regression and apply a logarithmic transformation to the target variable.","solution":"import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer from sklearn.pipeline import Pipeline from sklearn.feature_selection import SelectKBest from sklearn.linear_model import Ridge from sklearn.metrics import r2_score import pandas as pd def build_and_evaluate_pipeline(): # Load the data housing = fetch_california_housing(as_frame=True) X = housing.data.copy() y = housing.target.copy() # For illustration purpose, we convert \'ocean_proximity\' to a categorical feature. # Assuming \'longitude\' sets the boundary for the ocean proximity X[\'ocean_proximity\'] = np.where(X[\'Longitude\'] < -119, \'Near Ocean\', \'Inland\') # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) numeric_features = X.select_dtypes(include=[np.number]).columns.tolist() categorical_features = [\'ocean_proximity\'] # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numeric_features), (\'cat\', OneHotEncoder(), categorical_features) ]) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'select\', SelectKBest(k=5)), (\'regressor\', TransformedTargetRegressor( regressor=Ridge(), transformer=FunctionTransformer(np.log1p, np.expm1) )) ]) # Train the pipeline pipeline.fit(X_train, y_train) # Evaluate the pipeline y_pred = pipeline.predict(X_test) score = r2_score(y_test, y_pred) return pipeline, score # Run the function and get the pipeline and score pipeline, score = build_and_evaluate_pipeline()"},{"question":"Objective Create a function in PyTorch that initializes a distributed logging system, records a series of custom events, and retrieves specific events based on user-defined criteria. Task You need to implement the following functions: 1. **initialize_logging_system(log_file_path: str) -> None**: - Initializes the logging system and directs log output to the specified file path. 2. **record_custom_event(event_source: str, event_name: str, event_metadata: dict) -> None**: - Records a custom event with a specified source, name, and metadata. 3. **extract_events_with_metadata(filter_key: str, filter_value: str) -> List[dict]**: - Retrieves all events that have a specific metadata key-value pair. - The function should return a list of dictionaries, each representing an event that matches the criteria. Expected Input and Output Formats 1. **initialize_logging_system(log_file_path: str) -> None** - **Input**: - `log_file_path`: A string specifying the path to the log file. 2. **record_custom_event(event_source: str, event_name: str, event_metadata: dict) -> None** - **Input**: - `event_source`: A string representing the source of the event. - `event_name`: A string representing the name of the event. - `event_metadata`: A dictionary containing metadata key-value pairs. 3. **extract_events_with_metadata(filter_key: str, filter_value: str) -> List[dict]** - **Input**: - `filter_key`: A string representing the metadata key to filter by. - `filter_value`: A string representing the metadata value to filter by. - **Output**: - Returns a list of dictionaries, where each dictionary represents an event that contains the specified metadata. Constraints - Ensure the functions handle invalid inputs gracefully. - The `event_metadata` dictionary can contain any number of key-value pairs. - Performance is not the primary concern, but the functions should be reasonably efficient. Implementation Template ```python import torch.distributed.elastic.events as events def initialize_logging_system(log_file_path: str) -> None: Initializes the logging system to write logs to a specified file. Args: log_file_path (str): The path to the log file. Returns: None # Your implementation here pass def record_custom_event(event_source: str, event_name: str, event_metadata: dict) -> None: Records a custom event with the specified source, name, and metadata. Args: event_source (str): The source of the event. event_name (str): The name of the event. event_metadata (dict): The metadata for the event. Returns: None # Your implementation here pass def extract_events_with_metadata(filter_key: str, filter_value: str) -> List[dict]: Retrieves all events that have a specific metadata key-value pair. Args: filter_key (str): The metadata key to filter by. filter_value (str): The metadata value to filter by. Returns: List[dict]: List of dictionaries representing the events. # Your implementation here pass ``` # Example Usage ```python initialize_logging_system(\'event_log.txt\') record_custom_event(\'module1\', \'event_start\', {\'user\': \'alice\', \'session\': \'1234\'}) record_custom_event(\'module1\', \'event_end\', {\'user\': \'alice\', \'session\': \'1234\'}) record_custom_event(\'module2\', \'event_start\', {\'user\': \'bob\', \'session\': \'5678\'}) events = extract_events_with_metadata(\'user\', \'alice\') print(events) # Should output all events where the metadata key \'user\' has the value \'alice\' ```","solution":"import logging from typing import List from logging.handlers import RotatingFileHandler events_log = [] def initialize_logging_system(log_file_path: str) -> None: Initializes the logging system to write logs to a specified file. Args: log_file_path (str): The path to the log file. Returns: None global events_log logging.basicConfig(level=logging.INFO) handler = RotatingFileHandler(log_file_path, maxBytes=10000, backupCount=1) logging.getLogger().addHandler(handler) events_log = [] def record_custom_event(event_source: str, event_name: str, event_metadata: dict) -> None: Records a custom event with the specified source, name, and metadata. Args: event_source (str): The source of the event. event_name (str): The name of the event. event_metadata (dict): The metadata for the event. Returns: None global events_log event = { \'source\': event_source, \'name\': event_name, \'metadata\': event_metadata } events_log.append(event) logging.info(f\'Recorded event: {event}\') def extract_events_with_metadata(filter_key: str, filter_value: str) -> List[dict]: Retrieves all events that have a specific metadata key-value pair. Args: filter_key (str): The metadata key to filter by. filter_value (str): The metadata value to filter by. Returns: List[dict]: List of dictionaries representing the events. global events_log filtered_events = [event for event in events_log if event[\'metadata\'].get(filter_key) == filter_value] return filtered_events"},{"question":"**Title: Implementing and Using Custom Descriptors for Validation** **Objective:** Design custom descriptors to manage and validate class attributes in a dynamic and efficient manner, ensuring a comprehensive understanding of Python descriptors\' workings. **Problem Statement:** Your task is to create a Python class that uses custom descriptors to manage and validate its attributes. You will create a base descriptor class, `Validator`, and then create specific validators for different types of attributes (e.g., an integer validator, a string validator). Specifically, you need to: 1. **Implement a base descriptor class `Validator`.** - This class should have methods to initialize the descriptor, get and set values, and validate values. Validation criteria will be specified in subclasses. 2. **Create specific validator subclasses:** - `IntegerValidator`: Validates that the attribute is an integer within a specified range. - `StringValidator`: Validates that the attribute is a string with a specified minimum and maximum length. 3. **Create a class `Person` that uses these validators to ensure its attributes are set correctly.** - The class should have the following attributes: - `age`: Must be an integer between 0 and 120, inclusive. - `name`: Must be a string between 3 and 50 characters. # Classes and Methods 1. **Validator Class** ```python class Validator: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__.get(self._name) def __set__(self, instance, value): self.validate(value) instance.__dict__[self._name] = value def validate(self, value): raise NotImplementedError(\\"Subclasses must implement validate method.\\") ``` 2. **IntegerValidator Class** - Inherits from `Validator`. - Ensure the value is an integer within the specified range. ```python class IntegerValidator(Validator): def __init__(self, min_value=None, max_value=None): super().__init__() self.min_value = min_value self.max_value = max_value def validate(self, value): if not isinstance(value, int): raise ValueError(f\\"Expected {value} to be an integer.\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {value} to be at least {self.min_value}.\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {value} to be at most {self.max_value}.\\") ``` 3. **StringValidator Class** - Inherits from `Validator`. - Ensure the value is a string within the specified length. ```python class StringValidator(Validator): def __init__(self, min_length=None, max_length=None): super().__init__() self.min_length = min_length self.max_length = max_length def validate(self, value): if not isinstance(value, str): raise ValueError(f\\"Expected {value} to be a string.\\") if self.min_length is not None and len(value) < self.min_length: raise ValueError(f\\"Expected {value} to have at least {self.min_length} characters.\\") if self.max_length is not None and len(value) > self.max_length: raise ValueError(f\\"Expected {value} to have at most {self.max_length} characters.\\") ``` 4. **Person Class** - Uses `IntegerValidator` for `age` and `StringValidator` for `name`. ```python class Person: age = IntegerValidator(0, 120) name = StringValidator(3, 50) def __init__(self, name, age): self.name = name self.age = age ``` # Implementation Notes: 1. Initialize `Validator` instances properly and override their methods as shown in the examples. 2. Handle invalid inputs by raising appropriate exceptions with meaningful error messages. 3. Ensure the `Person` class correctly utilizes the custom validators to enforce attribute constraints. # Example Usage ```python try: person1 = Person(\\"John Doe\\", 25) # Should succeed person2 = Person(\\"JD\\", 25) # Should raise ValueError (name too short) person3 = Person(\\"John Doe\\", 150) # Should raise ValueError (age out of range) person4 = Person(\\"John Doe\\", \\"25\\")# Should raise ValueError (age not an integer) except ValueError as e: print(e) ``` **Submission:** Submit the implementation of the `Validator`, `IntegerValidator`, `StringValidator`, and `Person` classes, along with a brief explanation of how you approached the problem and any considerations or edge cases handled in your code. # Constraints: - Ensure the `Validator` class can be easily extended for future use cases. - Focus on robustness and clarity in your exception messages for invalid inputs.","solution":"class Validator: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__.get(self._name) def __set__(self, instance, value): self.validate(value) instance.__dict__[self._name] = value def validate(self, value): raise NotImplementedError(\\"Subclasses must implement validate method.\\") class IntegerValidator(Validator): def __init__(self, min_value=None, max_value=None): super().__init__() self.min_value = min_value self.max_value = max_value def validate(self, value): if not isinstance(value, int): raise ValueError(f\\"Expected {value} to be an integer.\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {value} to be at least {self.min_value}.\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {value} to be at most {self.max_value}.\\") class StringValidator(Validator): def __init__(self, min_length=None, max_length=None): super().__init__() self.min_length = min_length self.max_length = max_length def validate(self, value): if not isinstance(value, str): raise ValueError(f\\"Expected {value} to be a string.\\") if self.min_length is not None and len(value) < self.min_length: raise ValueError(f\\"Expected {value} to have at least {self.min_length} characters.\\") if self.max_length is not None and len(value) > self.max_length: raise ValueError(f\\"Expected {value} to have at most {self.max_length} characters.\\") class Person: age = IntegerValidator(0, 120) name = StringValidator(3, 50) def __init__(self, name, age): self.name = name self.age = age"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the seaborn library by creating and customizing different types of plots using a provided dataset. Your tasks involve data manipulation, visualization, and customization to analyze and present insights from the dataset. # Instructions 1. **Load Dataset**: Use seaborn to load the \\"tips\\" dataset. 2. **Data Visualization**: Create the following visualizations: - A scatter plot showing the relationship between total bill and tip, using different colors for smokers and non-smokers. - A box plot showing the distribution of total bill amount across different days of the week. - A violin plot comparing the total bill distributions for lunches and dinners on different days of the week, adding hue to differentiate between male and female customers. 3. **Customization and Interpretation**: - Customize each plot with appropriate titles, axis labels, and legend. - Choose suitable color palettes and styles. - Provide a brief (2-3 sentence) interpretation for each plot, explaining what insights can be drawn from the visualization. # Constraints and Requirements - Ensure your code is well-structured and commented. - Use seaborn\'s built-in functions and customization options. - Avoid using external libraries for tasks seaborn can handle. - Plots must be rendered inline if using Jupyter Notebook. # Example Output Provide the output for each plot along with its interpretation as part of your submission. Expected Input and Output Formats **Input**: - Data loaded from seaborn library (no external CSV) - Code for generating the required plots and customizations. **Output**: - Matplotlib-generated plot for each of the specified visualizations. - Textual interpretation of the visualized data trends and insights. ```python # Load the dataset import seaborn as sns # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter plot scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\") scatter_plot.set_title(\\"Scatter Plot of Total Bill vs Tip by Smoking Status\\") scatter_plot.set_xlabel(\\"Total Bill ()\\") scatter_plot.set_ylabel(\\"Tip ()\\") # Interpretation: # Provide a brief interpretation here. # Task 2: Box plot box_plot = sns.boxplot(data=tips, x=\\"day\\", y=\\"total_bill\\") box_plot.set_title(\\"Box Plot of Total Bill by Day of the Week\\") box_plot.set_xlabel(\\"Day of the Week\\") box_plot.set_ylabel(\\"Total Bill ()\\") # Interpretation: # Provide a brief interpretation here. # Task 3: Violin plot violin_plot = sns.violinplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", split=True) violin_plot.set_title(\\"Violin Plot of Total Bill by Meal and Sex\\") violin_plot.set_xlabel(\\"Day of the Week\\") violin_plot.set_ylabel(\\"Total Bill ()\\") # Interpretation: # Provide a brief interpretation here. ``` # Evaluation Criteria - **Correctness**: Does the code correctly load the data and generate the specified plots? - **Completeness**: Are all visualizations and customizations executed as required? - **Code Quality**: Is the code clean, well-commented, and efficient? - **Interpretations**: Are the interpretations accurate and insightful?","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", palette=\\"coolwarm\\") scatter_plot.set_title(\\"Scatter Plot of Total Bill vs Tip by Smoking Status\\") scatter_plot.set_xlabel(\\"Total Bill ()\\") scatter_plot.set_ylabel(\\"Tip ()\\") plt.legend(title=\\"Smoker\\") # Interpretation: # The scatter plot shows the relationship between total bill and tip, differentiating between smokers and non-smokers. # Smokers generally follow a similar pattern to non-smokers, with no significant difference in tip amount noted. plt.figure(figsize=(10, 6)) # Task 2: Box plot box_plot = sns.boxplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=\\"muted\\") box_plot.set_title(\\"Box Plot of Total Bill by Day of the Week\\") box_plot.set_xlabel(\\"Day of the Week\\") box_plot.set_ylabel(\\"Total Bill ()\\") # Interpretation: # The box plot shows the distribution of the total bill amount across different days of the week. # Saturday and Sunday show higher median bills compared to other weekdays, indicating higher spending on weekends. plt.figure(figsize=(10, 6)) # Task 3: Violin plot violin_plot = sns.violinplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", split=True, palette=\\"Set2\\") violin_plot.set_title(\\"Violin Plot of Total Bill by Day and Gender\\") violin_plot.set_xlabel(\\"Day of the Week\\") violin_plot.set_ylabel(\\"Total Bill ()\\") plt.legend(title=\\"Sex\\") # Interpretation: # The violin plot shows the distribution of total bills by day, split by gender. # The distributions are fairly similar between men and women, though men show slightly higher variability in bill amounts. # Display plots plt.show()"},{"question":"# Advanced Python: Working with MemoryView Objects You are tasked with creating a Python function using the `memoryview` object to perform optimized operations on large datasets. Given a sequence of data, your function should: 1. Create a memoryview from the given sequence. 2. Copy part of the memoryview to another memoryview object and modify it. 3. Return both the original and modified sequences for verification. **Function Signature:** ```python def manipulate_memoryview(sequence: bytes, start: int, length: int, value: int) -> (bytes, bytes): pass ``` # Input: - `sequence` (bytes): Original byte sequence to be manipulated. - `start` (int): Starting index from where to copy the segment. - `length` (int): Length of the segment to copy. - `value` (int): The value with which to fill the copied segment for modification. # Output: - Returns a tuple containing: - `original` (bytes): The original sequence. - `modified` (bytes): The sequence after manipulation. # Constraints: - `0 <= start < len(sequence)` - `0 < start + length <= len(sequence)` - `0 <= value < 256` (since it represents a single byte value) # Example: ```python sequence = bytes([10, 20, 30, 40, 50]) start = 1 length = 3 value = 100 original, modified = manipulate_memoryview(sequence, start, length, value) print(original) # Output: bytes([10, 20, 30, 40, 50]) print(modified) # Output: bytes([10, 100, 100, 100, 50]) ``` # Notes: - You should not modify the original sequence. - Make use of the `memoryview` class for efficient memory operations. - Pay attention to handling the slicing and memory manipulation appropriately.","solution":"def manipulate_memoryview(sequence: bytes, start: int, length: int, value: int) -> (bytes, bytes): Manipulates a segment of the memoryview of the given byte sequence. Parameters: - sequence: Original byte sequence - start: Starting index from where to copy the segment - length: Length of the segment to copy - value: The value with which to fill the copied segment for modification Returns: - original: The original sequence - modified: The sequence after manipulation # Create a memoryview of the sequence mv = memoryview(sequence) # Copy part of the memoryview to another memoryview object # Note: memoryview is still treating the buffer of the same sequence copied_part = mv[start:start + length] # Create a mutable byte array to modify the copied part modified_part = bytearray(copied_part) # Modify the copied segment with the given value for i in range(len(modified_part)): modified_part[i] = value # Create a new bytes object to store the modified sequence new_sequence = bytearray(sequence) new_sequence[start:start + length] = modified_part return sequence, bytes(new_sequence)"},{"question":"# Temporary File and Directory Management Problem Statement You are given a task to manage temporary files and directories for a data processing application. Your task is to implement the following: 1. A function `write_and_read_data` that writes data to a temporary file, reads it back from the file, and returns the read data. 2. A function `create_temporary_directory` that creates a temporary directory, writes a temporary file within that directory, reads the data back, and returns the directory name and the file content. Function Signatures ```python def write_and_read_data(data: bytes) -> bytes: Write data to a temporary file and read it back. Parameters: data (bytes): The data to write to the file. Returns: bytes: The data read back from the file. pass def create_temporary_directory(data: bytes) -> tuple: Create a temporary directory, write a file with the given data in that directory, and read the data back. Parameters: data (bytes): The data to write to the file. Returns: tuple: A tuple containing the directory name (str) and the data read from the file (bytes). pass ``` Constraints - Use the `tempfile` module for file and directory creation. - Ensure that temporary files and directories are cleaned up automatically once they are no longer needed. - Your implementation should handle any platform-specific differences in temporary file handling seamlessly. Examples ```python # Example for write_and_read_data data_to_write = b\'Hello, temporary file!\' read_data = write_and_read_data(data_to_write) assert read_data == data_to_write # Example for create_temporary_directory data_to_write = b\'Hello, temp directory!\' dir_name, read_data = create_temporary_directory(data_to_write) assert isinstance(dir_name, str) assert read_data == data_to_write ``` You are expected to use context managers (`with` statement) wherever appropriate to ensure resources are cleaned up promptly. Performance Requirements - Your solution should be efficient in terms of memory and disk usage. - Ensure that no temporary files or directories remain after their intended use is completed.","solution":"import tempfile import os def write_and_read_data(data: bytes) -> bytes: Write data to a temporary file and read it back. Parameters: data (bytes): The data to write to the file. Returns: bytes: The data read back from the file. with tempfile.NamedTemporaryFile(delete=True) as temp_file: temp_file.write(data) temp_file.seek(0) return temp_file.read() def create_temporary_directory(data: bytes) -> tuple: Create a temporary directory, write a file with the given data in that directory, and read the data back. Parameters: data (bytes): The data to write to the file. Returns: tuple: A tuple containing the directory name (str) and the data read from the file (bytes). with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \\"temp_file\\") with open(temp_file_path, \'wb\') as temp_file: temp_file.write(data) with open(temp_file_path, \'rb\') as temp_file: read_data = temp_file.read() return temp_dir, read_data"},{"question":"You are provided with a dataset containing unlabelled data points. Your task is to perform various unsupervised learning techniques to analyze the data and extract meaningful patterns. Specifically, you need to implement functions that perform clustering, dimensionality reduction, and outlier detection using scikit-learn. # Dataset Format The dataset will be provided as a NumPy array `X` of shape `(n_samples, n_features)` where `n_samples` is the number of samples, and `n_features` is the number of features per sample. # Task Requirements 1. **Clustering**: Implement a function to perform K-means clustering. 2. **Dimensionality Reduction**: Implement a function to reduce the dimensionality of the dataset using PCA. 3. **Outlier Detection**: Implement a function to detect outliers using Isolation Forest. # Function Signatures 1. `def perform_kmeans_clustering(X: np.ndarray, n_clusters: int) -> np.ndarray:` - **Input**: - `X` (np.ndarray): The input dataset of shape `(n_samples, n_features)`. - `n_clusters` (int): The number of clusters to form. - **Output**: Array of cluster labels for each data point. 2. `def reduce_dimensionality_pca(X: np.ndarray, n_components: int) -> np.ndarray:` - **Input**: - `X` (np.ndarray): The input dataset of shape `(n_samples, n_features)`. - `n_components` (int): The number of principal components to keep. - **Output**: The transformed dataset with reduced dimensions. 3. `def detect_outliers_isolation_forest(X: np.ndarray) -> np.ndarray:` - **Input**: - `X` (np.ndarray): The input dataset of shape `(n_samples, n_features)`. - **Output**: Array indicating outlier (-1) and inlier (1) for each data point. # Constraints - Use appropriate parameter values for the scikit-learn methods you choose. - Ensure your code is optimized for performance with respect to the given dataset and task requirements. # Example ```python import numpy as np # Sample data X = np.random.rand(100, 5) # Clustering cluster_labels = perform_kmeans_clustering(X, n_clusters=3) print(\\"Cluster Labels:\\", cluster_labels) # Dimensionality Reduction X_reduced = reduce_dimensionality_pca(X, n_components=2) print(\\"Reduced Dimensions Shape:\\", X_reduced.shape) # Outlier Detection outliers = detect_outliers_isolation_forest(X) print(\\"Outliers:\\", outliers) ``` Implement the required functions using scikit-learn and make sure the outputs are in the expected format described above. # Notes - Refer to the scikit-learn documentation for the usage of K-means, PCA, and Isolation Forest. - Ensure you have `scikit-learn` and `numpy` installed (`pip install scikit-learn numpy`).","solution":"from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.ensemble import IsolationForest import numpy as np def perform_kmeans_clustering(X: np.ndarray, n_clusters: int) -> np.ndarray: Perform K-means clustering on the dataset. Parameters: X (np.ndarray): The input dataset of shape (n_samples, n_features). n_clusters (int): The number of clusters to form. Returns: np.ndarray: Array of cluster labels for each data point. kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(X) return kmeans.labels_ def reduce_dimensionality_pca(X: np.ndarray, n_components: int) -> np.ndarray: Reduce the dimensionality of the dataset using PCA. Parameters: X (np.ndarray): The input dataset of shape (n_samples, n_features). n_components (int): The number of principal components to keep. Returns: np.ndarray: The transformed dataset with reduced dimensions. pca = PCA(n_components=n_components) return pca.fit_transform(X) def detect_outliers_isolation_forest(X: np.ndarray) -> np.ndarray: Detect outliers in the dataset using Isolation Forest. Parameters: X (np.ndarray): The input dataset of shape (n_samples, n_features). Returns: np.ndarray: Array indicating outlier (-1) and inlier (1) for each data point. iso_forest = IsolationForest(random_state=42) return iso_forest.fit_predict(X)"},{"question":"**Objective**: Assess the student\'s ability to utilize scikit-learn for plotting and interpreting validation curves and learning curves. **Problem Statement** You are given a dataset `X_data` and `y_data` related to a binary classification problem. Your task is to perform the following steps: 1. **Model Training and Validation Curves**: - Use Support Vector Machine (SVM) with a radial basis function (RBF) kernel. - Plot the validation curve to evaluate the model\'s performance for different values of the hyperparameter `C` (choose at least 5 values in a reasonable range). - Interpret the results to determine if the model is underfitting, overfitting, or performing well for each value of `C`. 2. **Learning Curves**: - Plot the learning curve for the SVM model using a fixed value of `C` identified as optimal or near-optimal from the validation curves. - Use at least 5 different sizes of the training set to plot the learning curve. - Analyze the learning curve to discuss whether increasing the size of the training set would help improve the model\'s performance. **Requirements**: - Implement the code in Python using scikit-learn. - Generate plots for both the validation curve and the learning curve. - Your code should be efficient and take into account any potential performance considerations. - Provide clear comments and explanations for your code. - Discuss your interpretation of the results of the plots. **Input**: - `X_data`, `y_data`: Arrays representing the features and labels of the dataset. **Output**: - Validation curve plot for different values of `C`. - Learning curve plot for different sizes of the training set. - Interpretation and discussion of the plots. **Constraints**: - Use a random seed for reproducibility. - The dataset size should be reasonable to ensure the code runs within a reasonable time frame on a standard machine. **Performance**: - The code should be executed efficiently, leveraging scikit-learn\'s functionalities effectively. **Example Code**: ```python import numpy as np from sklearn.model_selection import validation_curve, learning_curve, train_test_split from sklearn.svm import SVC import matplotlib.pyplot as plt # Example data split (use the data already given) X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) # 1. Validation Curve param_range = np.logspace(-3, 2, 5) train_scores, valid_scores = validation_curve( SVC(kernel=\\"rbf\\"), X_train, y_train, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, train_scores.mean(axis=1), label=\'Training score\') plt.plot(param_range, valid_scores.mean(axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.title(\'Validation Curve for SVM\') plt.legend(loc=\'best\') plt.show() # Interpretation of validation curve # 2. Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'rbf\', C=1), X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=5, random_state=42 ) plt.figure() plt.plot(train_sizes, train_scores.mean(axis=1), label=\'Training score\') plt.plot(train_sizes, valid_scores.mean(axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve for SVM\') plt.legend(loc=\'best\') plt.show() # Analysis of learning curve ``` Discuss how you determined if the model is underfitting, overfitting, or performing well from the validation curve and whether adding more training data from the learning curve analysis would improve the model performance.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve, train_test_split from sklearn.svm import SVC def plot_validation_curve(X_data, y_data, param_range): Plots the validation curve for the SVM model with RBF kernel. Parameters: X_data (ndarray): Feature dataset. y_data (ndarray): Target dataset. param_range (array-like): Range of parameter \'C\' values to evaluate. Returns: None X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) train_scores, valid_scores = validation_curve( SVC(kernel=\\"rbf\\"), X_train, y_train, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, train_scores.mean(axis=1), label=\'Training score\') plt.plot(param_range, valid_scores.mean(axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.title(\'Validation Curve for SVM\') plt.xscale(\'log\') plt.legend(loc=\'best\') plt.show() def plot_learning_curve(X_data, y_data, optimal_C): Plots the learning curve for the SVM model with RBF kernel for a fixed value of C. Parameters: X_data (ndarray): Feature dataset. y_data (ndarray): Target dataset. optimal_C (float): The optimal value of C. Returns: None X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'rbf\', C=optimal_C), X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=5, random_state=42 ) plt.figure() plt.plot(train_sizes, train_scores.mean(axis=1), label=\'Training score\') plt.plot(train_sizes, valid_scores.mean(axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve for SVM\') plt.legend(loc=\'best\') plt.show() # Example usage with dummy data for illustration purposes: # X_data = np.random.rand(100, 10) # y_data = np.random.randint(0, 2, 100) # Uncomment the following lines and supply X_data and y_data. # param_range = np.logspace(-3, 2, 5) # plot_validation_curve(X_data, y_data, param_range) # From the validation curve, let\'s assume C=1 is optimal # plot_learning_curve(X_data, y_data, optimal_C=1)"},{"question":"Objective: You are required to write a Python script using the `asyncio` library to perform concurrent tasks. The script should fetch data from multiple URLs concurrently, parse the results, and output the processed data. Task: 1. Implement an asynchronous function `fetch_data(url: str) -> str` that takes a URL as input, performs an asynchronous HTTP GET request to fetch data, and returns the response text. Use the `aiohttp` package for making HTTP requests. 2. Implement an asynchronous function `process_data(data: str) -> dict` that takes response text from a URL as input, processes the data (for this example, assume the data is in JSON format), and returns a dictionary. 3. Implement a main function `main(urls: list[str]) -> list[dict]` that: - Takes a list of URLs as input. - Uses `asyncio.gather` or similar to fetch and process data concurrently for all URLs. - Returns a list of dictionaries, each representing processed data from one URL. 4. Ensure the entire operation is executed within an appropriate asyncio event loop. Input Format: - A list of URLs (strings) to be fetched and processed. Output Format: - A list of dictionaries, each representing processed data from one URL. Constraints: - The number of URLs will not exceed 100. - Data fetched from URLs will be in JSON format. Sample Usage: ```python import asyncio import aiohttp import json async def fetch_data(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def process_data(data: str) -> dict: return json.loads(data) async def main(urls: list[str]) -> list[dict]: tasks = [fetch_data(url) for url in urls] responses = await asyncio.gather(*tasks) processed_data_tasks = [process_data(response) for response in responses] processed_data = await asyncio.gather(*processed_data_tasks) return processed_data # Example usage urls = [\'http://example.com/api/data1\', \'http://example.com/api/data2\'] result = asyncio.run(main(urls)) print(result) ``` Your task is to correctly implement the `fetch_data`, `process_data`, and `main` functions to achieve concurrent fetching and processing of data from the provided URLs.","solution":"import asyncio import aiohttp import json async def fetch_data(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def process_data(data: str) -> dict: return json.loads(data) async def main(urls: list[str]) -> list[dict]: fetch_tasks = [fetch_data(url) for url in urls] responses = await asyncio.gather(*fetch_tasks) process_tasks = [process_data(response) for response in responses] processed_data = await asyncio.gather(*process_tasks) return processed_data"},{"question":"# Advanced Pandas Styling and Exporting Objective You are given a DataFrame containing information about student grades. You need to create a Styler object for this DataFrame to apply specific styles based on the following criteria: 1. Highlight the maximum grade in each column with a green background color. 2. Highlight the minimum grade in each column with a red background color. 3. Apply a gradient background color from yellow to blue for the entire DataFrame based on the grade values. 4. Export the styled DataFrame to an HTML file. Input - A pandas DataFrame named `grades_df` with the following structure: ```plaintext | Name | Math | Science | English | History | | -------- | ---- | ------- | ------- | ------- | | Student1 | 78 | 85 | 90 | 70 | | Student2 | 88 | 79 | 95 | 91 | | Student3 | 92 | 94 | 88 | 85 | ``` Output - An HTML file named `styled_grades.html` containing the styled DataFrame. Instructions 1. Define a function `style_and_export_grades(grades_df: pd.DataFrame) -> None`: - This function should: - Create a styled DataFrame using `Styler`. - Apply the specified styles. - Export the styled DataFrame to an HTML file named `styled_grades.html`. 2. Your function implementation should strictly follow the guidelines given for styling and exporting. 3. Ensure that the HTML file is created in the current working directory. Constraints - Use only the pandas library (`import pandas as pd`). - You are expected to handle any potential pandas-related errors gracefully within your function. Example ```python import pandas as pd data = { \'Name\': [\'Student1\', \'Student2\', \'Student3\'], \'Math\': [78, 88, 92], \'Science\': [85, 79, 94], \'English\': [90, 95, 88], \'History\': [70, 91, 85] } grades_df = pd.DataFrame(data) style_and_export_grades(grades_df) ``` Running the above code should generate an HTML file `styled_grades.html` with the proper styles applied.","solution":"import pandas as pd def style_and_export_grades(grades_df: pd.DataFrame) -> None: Styles the given DataFrame with specific rules and exports it to an HTML file. Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. # Define a function to highlight the maximum values in green def highlight_max(s): is_max = s == s.max() return [\'background-color: green\' if v else \'\' for v in is_max] # Define a function to highlight the minimum values in red def highlight_min(s): is_min = s == s.min() return [\'background-color: red\' if v else \'\' for v in is_min] # Create a Styler object styler = grades_df.style # Apply the highlight functions styler = styler.apply(highlight_max, subset=[\'Math\', \'Science\', \'English\', \'History\']) styler = styler.apply(highlight_min, subset=[\'Math\', \'Science\', \'English\', \'History\']) # Apply a gradient background color from yellow to blue styler = styler.background_gradient(cmap=\'YlGnBu\') # Export to HTML styler.to_html(\'styled_grades.html\') # Example usage: # Note: This has to be removed before using the function in tests # data = { # \'Name\': [\'Student1\', \'Student2\', \'Student3\'], # \'Math\': [78, 88, 92], # \'Science\': [85, 79, 94], # \'English\': [90, 95, 88], # \'History\': [70, 91, 85] # } # grades_df = pd.DataFrame(data) # style_and_export_grades(grades_df)"},{"question":"# Question: Function Signature Inspector Objective: You are to write a Python function named `function_signature_report` which accepts an object (expected to be a function or method) and prints out a detailed report of its signature, including: 1. The function name. 2. The list of all positional and keyword parameters, clearly indicating which ones are positional-only, positional-or-keyword, keyword-only, and var-positional and var-keyword parameters. 3. Default values for any parameters that have them. 4. The function\'s return annotation if it exists. 5. The code lines where the function is defined, showing the starting and ending line numbers. Input: - An object expected to be a function or method. Output: - A report printed to the console as described in the objective. Constraints: 1. If the input object is not a valid function or method, the function should raise a `TypeError` with an appropriate error message. 2. Use the `inspect` module to gather and format this information. 3. Assume that the function/method provided will be one that you have access to and can inspect (i.e., not a built-in function without source code). Example Usage: ```python def example_function(a, b: int, c: str = \\"hello\\", *args, d, e=42, **kwargs) -> bool: pass function_signature_report(example_function) ``` Expected Output: ```python Function Name: example_function Parameters: - a: positional-or-keyword, no default - b: positional-or-keyword, annotation: int, no default - c: positional-or-keyword, default: \'hello\', annotation: str - *args: var-positional - d: keyword-only, no default - e: keyword-only, default: 42 - **kwargs: var-keyword Return Annotation: bool Defined in: - Start Line: 1 - End Line: 1 Source Code: def example_function(a, b: int, c: str = \\"hello\\", *args, d, e=42, **kwargs) -> bool: pass ``` Note: Focus on handling edge cases such as functions without any annotations or defaults, and ensure that your solution is robust against invalid inputs.","solution":"import inspect def function_signature_report(func): if not callable(func): raise TypeError(\\"The provided object is not a valid function or method.\\") sig = inspect.signature(func) parameters = sig.parameters function_name = func.__name__ print(f\\"Function Name: {function_name}\\") print(\\"Parameters:\\") # Parameter categories for name, param in parameters.items(): kind = param.kind if kind == inspect.Parameter.POSITIONAL_ONLY: print(f\\"- {name}: positional-only\\", end=\\"\\") elif kind == inspect.Parameter.POSITIONAL_OR_KEYWORD: print(f\\"- {name}: positional-or-keyword\\", end=\\"\\") elif kind == inspect.Parameter.VAR_POSITIONAL: print(f\\"- *{name}: var-positional\\", end=\\"\\") elif kind == inspect.Parameter.KEYWORD_ONLY: print(f\\"- {name}: keyword-only\\", end=\\"\\") elif kind == inspect.Parameter.VAR_KEYWORD: print(f\\"- **{name}: var-keyword\\", end=\\"\\") if param.default is not param.empty: print(f\\", default: {param.default}\\", end=\\"\\") if param.annotation is not param.empty: print(f\\", annotation: {param.annotation}\\", end=\\"\\") print() if sig.return_annotation is not inspect.Signature.empty: print(f\\"Return Annotation: {sig.return_annotation}\\") # Source code and line numbers try: source_lines, start_line = inspect.getsourcelines(func) end_line = start_line + len(source_lines) - 1 print(f\\"nDefined in:n- Start Line: {start_line}n- End Line: {end_line}nSource Code:\\") print(\\"\\".join(source_lines)) except IOError: print(\\"Source code not available.\\")"},{"question":"You are required to implement a custom SAX content handler in Python using the `xml.sax.handler` module. This handler will parse an XML document to extract specific information and output a summary of the document. # Task: Implement a custom content handler class `BookContentHandler` that inherits from `xml.sax.handler.ContentHandler`. The handler will process an XML file containing information about books and output a summary. # Input: You will receive an XML file (`books.xml`) with the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> <price>39.99</price> </book> <!-- More book entries can follow --> </library> ``` # Output: Your program should print a summary of the XML document in the following format: ``` Book Summary: Title: Book Title 1, Author: Author 1, Year: 2001, Price: 29.99 Title: Book Title 2, Author: Author 2, Year: 2005, Price: 39.99 ``` # Implementation Requirements: 1. Implement the `BookContentHandler` class with: - An `__init__()` method to initialize necessary variables. - Overridden methods `startElement()`, `endElement()`, and `characters()` to handle the parsing logic. 2. Use the overridden methods to: - Detect the start and end of `book`, `title`, `author`, `year`, and `price` elements. - Collect and store the data accordingly. 3. Create a function `parse_books(xml_file_path)` to set up the parser using your `BookContentHandler` and parse the given XML file. # Constraints: - You can assume the input XML file is correctly formatted and contains the structure as outlined above. - You need to handle multiple book entries as shown. # Example: ```python import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.currentElement = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.price = \\"\\" self.summary = [] def startElement(self, name, attrs): self.currentElement = name def endElement(self, name): if name == \\"book\\": self.summary.append(f\\"Title: {self.title}, Author: {self.author}, Year: {self.year}, Price: {self.price}\\") self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.price = \\"\\" self.currentElement = \\"\\" def characters(self, content): if self.currentElement == \\"title\\": self.title += content elif self.currentElement == \\"author\\": self.author += content elif self.currentElement == \\"year\\": self.year += content elif self.currentElement == \\"price\\": self.price += content def parse_books(xml_file_path): parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) parser.parse(open(xml_file_path)) print(\\"Book Summary:\\") for book in handler.summary: print(book) # Example usage: # parse_books(\'books.xml\') ``` # Notes: - Your implementation should correctly handle multiple books in the XML and output the appropriate summary as specified. - Make sure to test your implementation with different XML structures to ensure robustness.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.currentElement = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.price = \\"\\" self.summary = [] def startElement(self, name, attrs): self.currentElement = name def endElement(self, name): if name == \\"book\\": self.summary.append( f\\"Title: {self.title.strip()}, Author: {self.author.strip()}, Year: {self.year.strip()}, Price: {self.price.strip()}\\") self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.price = \\"\\" self.currentElement = \\"\\" def characters(self, content): if self.currentElement == \\"title\\": self.title += content elif self.currentElement == \\"author\\": self.author += content elif self.currentElement == \\"year\\": self.year += content elif self.currentElement == \\"price\\": self.price += content def parse_books(xml_file_path): parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) parser.parse(open(xml_file_path)) return handler.summary # Example usage: # parse_books(\'books.xml\')"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that sets up and records a series of simulated training events in a distributed computing scenario. # Requirements 1. **Function Name**: `simulate_training_events` 2. **Input**: - `event_source` (type: `str`): A string representing the source of the events (e.g., \\"training\\", \\"validation\\"). - `event_type` (type: `str`): A string representing the type of event to record (e.g., \\"start\\", \\"end\\", \\"checkpoint\\"). - `metadata` (type: `Dict[str, Any]`): A dictionary of key-value pairs to be used as metadata for the events. 3. **Output**: - The function should return a list of `Event` objects that have been recorded. # Constraints - Utilize the `torch.distributed.elastic.events` module and its components mentioned in the provided documentation. - Assume that the necessary setup for distributed computing is already in place. - The `metadata` dictionary can contain any relevant information you want to include in the event (e.g., `{\\"epoch\\": 1, \\"accuracy\\": 0.95}`). - Ensure that the events are properly logged and accessible via a logging handler. # Function Definition ```python from typing import Any, Dict, List import torch.distributed.elastic.events as events import torch.distributed.elastic.events.api as api def simulate_training_events(event_source: str, event_type: str, metadata: Dict[str, Any]) -> List[api.Event]: Simulate and record a series of training events. Arguments: event_source -- A string representing the source of the events. event_type -- A string representing the type of event to record. metadata -- A dictionary of key-value pairs to be used as metadata for the events. Returns: A list of `Event` objects that have been recorded. # Initialize the list to hold event objects recorded_events = [] # Generate a unique event id event_id = f\\"{event_source}_{event_type}\\" # Create an Event object event = api.Event( name=event_id, source=api.EventSource(event_source), metadata={key: api.EventMetadataValue(value) for key, value in metadata.items()} ) # Record the event events.record(event) # Append the recorded event to the list recorded_events.append(event) # Return the list of recorded events return recorded_events ``` # Example Usage ```python event_source = \\"training\\" event_type = \\"start\\" metadata = {\\"epoch\\": 1, \\"accuracy\\": 0.95} events_list = simulate_training_events(event_source, event_type, metadata) for event in events_list: print(event) ```","solution":"from typing import Any, Dict, List import torch.distributed.elastic.events as events import torch.distributed.elastic.events.api as api def simulate_training_events(event_source: str, event_type: str, metadata: Dict[str, Any]) -> List[api.Event]: Simulate and record a series of training events. Arguments: event_source -- A string representing the source of the events. event_type -- A string representing the type of event to record. metadata -- A dictionary of key-value pairs to be used as metadata for the events. Returns: A list of `Event` objects that have been recorded. # Initialize the list to hold event objects recorded_events = [] # Generate a unique event id event_id = f\\"{event_source}_{event_type}\\" # Create an Event object event = api.Event( name=event_id, source=api.EventSource(event_source), metadata={key: api.EventMetadataValue(value) for key, value in metadata.items()} ) # Record the event events.record(event) # Append the recorded event to the list recorded_events.append(event) # Return the list of recorded events return recorded_events"},{"question":"**Question:** You are tasked with visualizing a dataset using Seaborn\'s `seaborn.objects` to reveal insights about the data distribution and relationship between different attributes. Specifically, you need to demonstrate mastery over the `Dodge` transform to manage overlapping of data points in your plots. **Dataset:** We will use Seaborn\'s built-in `tips` dataset. Below is a description of relevant columns: - `day`: Day of the week when the data was collected. - `time`: Time of day (`Lunch` or `Dinner`). - `total_bill`: Total bill amount. - `sex`: Gender of the person paying the bill. - `size`: Number of people in the party. **Requirements:** 1. Import the necessary modules and load the `tips` dataset. 2. Create a bar plot to show the count of observations for each `day`, colored by `time`. Use the `Dodge` transform to avoid overlapping marks. 3. Modify the plot to use the `Dodge` transform with the `empty=\\"drop\\"` parameter to handle empty spaces appropriately. 4. Create another bar plot that shows the sum of `total_bill` for each `day`, differentiated by `sex`. Use the `gap` parameter of `Dodge` to add spacing between the bars. **Constraints:** - You must use the `seaborn.objects` module. - Ensure labels and titles are appropriate for understanding the plots. **Expected Input and Output:** - Input: None (the code will run using the internal `tips` dataset from Seaborn). - Output: Three different bar plots meeting the above requirements. **Example Code:** ```python # 1. Import the necessary modules and load the tips dataset import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # 2. Create the first bar plot showing counts for each day, colored by time with Dodge plot1 = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge()) ) plot1.show() # 3. Modify the first plot to handle empty spaces with the `empty=\\"drop\\"` parameter plot2 = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge(empty=\\"drop\\")) ) plot2.show() # 4. Create a second bar plot to show the sum of total_bill for each day, differentiated by sex, with spacing using gap plot3 = ( so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) ) plot3.show() ``` **Additional Notes:** - The plots should be displayed properly in a Jupyter Notebook if using one. - Ensure to use specific parameters of the `Dodge` class as required.","solution":"def generate_plots(): import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # 1. Create the first bar plot showing counts for each day, colored by time with Dodge plot1 = ( so.Plot(tips, x=\\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge()) ) plot1.show() # 2. Modify the first plot to handle empty spaces with the `empty=\\"drop\\"` parameter plot2 = ( so.Plot(tips, x=\\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge(empty=\\"drop\\")) ) plot2.show() # 3. Create a second bar plot to show the sum of total_bill for each day, differentiated by sex, with spacing using gap plot3 = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) ) plot3.show()"},{"question":"Coding Assessment Question # Problem Statement Your task is to implement a function `process_zip_file` that takes a path to a ZIP file and performs the following operations: 1. **List the contents**: Print a list of all the files in the ZIP archive. 2. **Extract a specified file**: Extract a specified file from the archive to a given directory. 3. **Add a new file**: Add a new file from the local filesystem to the archive. # Function Signature ```python def process_zip_file(zip_path: str, extract_file_name: str, extract_to_dir: str, file_to_add: str) -> None: pass ``` # Input Parameters - `zip_path` (str): The path to the ZIP file. - `extract_file_name` (str): The name of the file to extract from the ZIP archive. - `extract_to_dir` (str): The directory to which the specified file should be extracted. - `file_to_add` (str): The path to the file that should be added to the ZIP archive. # Constraints - The ZIP file specified by `zip_path` should exist and be a valid ZIP archive. - The file name specified by `extract_file_name` should exist within the ZIP archive. - The directory specified by `extract_to_dir` should exist on the local filesystem. - The file specified by `file_to_add` should exist on the local filesystem and be a file (not a directory). # Example Usage ```python # Assume `archive.zip` contains: [\'file1.txt\', \'file2.txt\'] process_zip_file(\'archive.zip\', \'file1.txt\', \'/extracted_files\', \'new_file.txt\') # Expected operations: # 1. Print the contents of the archive: [\'file1.txt\', \'file2.txt\'] # 2. Extract \'file1.txt\' to \'/extracted_files\' # 3. Add \'new_file.txt\' to \'archive.zip\' ``` # Notes - The function should handle potential errors gracefully, providing appropriate error messages for cases such as a missing ZIP file, missing specified file within the archive, or failed file operations. - Use the `zipfile` module for interacting with ZIP files. # Hint Refer to the `zipfile` documentation for details on how to list files, extract files, and add files to a ZIP archive.","solution":"import zipfile import os def process_zip_file(zip_path: str, extract_file_name: str, extract_to_dir: str, file_to_add: str) -> None: # Check if the given paths exist if not os.path.isfile(zip_path): print(f\\"The zip file {zip_path} does not exist.\\") return if not os.path.isfile(file_to_add): print(f\\"The file to add {file_to_add} does not exist.\\") return if not os.path.isdir(extract_to_dir): print(f\\"The extraction directory {extract_to_dir} does not exist.\\") return # Open the zip file with zipfile.ZipFile(zip_path, \'a\') as zipf: # List contents print(\\"Contents of the zip file:\\") for file in zipf.namelist(): print(file) # Extract specified file if extract_file_name in zipf.namelist(): zipf.extract(extract_file_name, extract_to_dir) print(f\\"Extracted {extract_file_name} to {extract_to_dir}\\") else: print(f\\"{extract_file_name} not found in the zip file.\\") # Add new file to the zip zipf.write(file_to_add, os.path.basename(file_to_add)) print(f\\"Added {file_to_add} to the zip file.\\")"},{"question":"**Objective:** Your task is to implement a function that takes a mono audio fragment, converts it to stereo, performs a gain adjustment on each channel, and then converts it back to mono. Specifically, you will: 1. Convert the mono fragment to stereo using given left and right channel factors. 2. Apply a gain adjustment to each channel. 3. Convert the resulting stereo fragment back to mono using new channel factors to balance the left and right channels. **Function Signature:** ```python def process_audio(mono_fragment: bytes, width: int, lfactor: float, rfactor: float, lm_gain: float, rm_gain: float) -> bytes: Parameters: - mono_fragment: bytes -- The input mono audio fragment. - width: int -- Sample width in bytes. Can be 1, 2, 3 or 4. - lfactor: float -- Scaling factor for left channel during stereo conversion. - rfactor: float -- Scaling factor for right channel during stereo conversion. - lm_gain: float -- Gain adjustment for left channel. - rm_gain: float -- Gain adjustment for right channel. Returns: - bytes -- The processed audio fragment converted back to mono. ``` **Details:** 1. **Convert the Mono to Stereo:** - Use `audioop.tostereo()` to convert the mono fragment to stereo. Apply `lfactor` and `rfactor` to scale the left and right channels, respectively. 2. **Apply Gain Adjustments:** - Split the stereo fragment into individual left and right channel fragments using `audioop.tomono()`. - Adjust the gain for each channel using `audioop.mul()` with `lm_gain` and `rm_gain`. 3. **Convert Stereo Back to Mono:** - Combine the modified left and right channels back into a stereo fragment using suitable factors (e.g., `1`). - Convert the resulting stereo fragment back to mono using `audioop.tomono()` with appropriate balancing factors. **Constraints:** - The lengths of input audio fragments are guaranteed to fit in memory. - You may assume that `width` will be a valid value: `1`, `2`, `3`, or `4`. - The audio fragments will not be empty. **Example:** ```python input_mono_fragment = b\'x01x02x03x04\' width = 2 lfactor = 0.5 rfactor = 0.5 lm_gain = 1.2 rm_gain = 0.8 output_mono_fragment = process_audio(input_mono_fragment, width, lfactor, rfactor, lm_gain, rm_gain) print(output_mono_fragment) # Expected output: A bytes object representing the processed mono audio fragment ``` **Note:** Use the `audioop` module functions as detailed in the documentation for intermediate steps, such as `audioop.tostereo()`, `audioop.tomono()`, and `audioop.mul()`.","solution":"import audioop def process_audio(mono_fragment: bytes, width: int, lfactor: float, rfactor: float, lm_gain: float, rm_gain: float) -> bytes: Processes a mono audio fragment by converting it to stereo with gain adjustments and then back to mono. Parameters: - mono_fragment: bytes -- The input mono audio fragment. - width: int -- Sample width in bytes. Can be 1, 2, 3, or 4. - lfactor: float -- Scaling factor for left channel during stereo conversion. - rfactor: float -- Scaling factor for right channel during stereo conversion. - lm_gain: float -- Gain adjustment for left channel. - rm_gain: float -- Gain adjustment for right channel. Returns: - bytes -- The processed audio fragment converted back to mono. # Step 1: Convert the mono fragment to stereo stereo_fragment = audioop.tostereo(mono_fragment, width, lfactor, rfactor) # Step 2: Split stereo into left and right channels left_channel = audioop.tomono(stereo_fragment, width, 1, 0) right_channel = audioop.tomono(stereo_fragment, width, 0, 1) # Step 3: Apply gain adjustments to each channel left_channel = audioop.mul(left_channel, width, lm_gain) right_channel = audioop.mul(right_channel, width, rm_gain) # Step 4: Combine left and right channels back into a stereo fragment adjusted_stereo = audioop.tostereo(left_channel, width, 1, 0) adjusted_stereo = audioop.add(adjusted_stereo, audioop.tostereo(right_channel, width, 0, 1), width) # Step 5: Convert the adjusted stereo fragment back to mono processed_mono = audioop.tomono(adjusted_stereo, width, 0.5, 0.5) return processed_mono"},{"question":"Objective: Create a function that recursively compiles Python source files in a given directory, allowing various options to control the compilation process. This will assess your understanding of directory traversal, file handling, and the use of the `compileall` module in Python. Function Signature: ```python def custom_compile(source_dir: str, max_levels: int = 10, force: bool = False, quiet: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1, regex_pattern: str = None) -> bool: pass ``` Parameters: - `source_dir` (str): The directory containing Python source files to compile. - `max_levels` (int, optional): Maximum recursion levels for subdirectories. Default is 10. - `force` (bool, optional): If True, forces recompilation even if timestamps are up to date. Default is False. - `quiet` (int, optional): Control verbosity. Default is 0 (prints filenames and other information). Set to 1 to print only errors, and 2 to suppress all output. - `legacy` (bool, optional): If True, write byte-code files to their legacy locations and names. Default is False. - `optimize` (int, optional): Specifies optimization level for the compiler. Default is -1 (no optimization). - `workers` (int, optional): Number of workers to compile files in parallel. Default is 1. - `regex_pattern` (str, optional): Regex pattern to exclude files from compilation. Returns: - `bool`: True if all files compiled successfully, False otherwise. Constraints: - The `regex_pattern` should be a valid regular expression. - `max_levels` should be a non-negative integer. - `quiet` should be either 0, 1, or 2. - `optimize` should be between -1 and 2 (inclusive). Requirements: 1. Utilize the `compileall.compile_dir` function for directory compilation. 2. Apply the given `regex_pattern` to exclude matching files. 3. Ensure the function respects the `max_levels` and `workers` options. 4. Handle invalid inputs appropriately, raising `ValueError` where necessary. Example: ```python # Example usage result = custom_compile(\\"example_dir\\", max_levels=5, force=True, quiet=1, legacy=False, optimize=2, workers=4, regex_pattern=r\'.*test.*\') print(result) # Expected output: True or False depending on the success of compilation ``` The `custom_compile` function demonstrates the practical application of directory traversal, file handling, and concurrent execution, showcasing a deeper understanding of the `compileall` module.","solution":"import compileall import re import os def custom_compile(source_dir: str, max_levels: int = 10, force: bool = False, quiet: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1, regex_pattern: str = None) -> bool: if not isinstance(max_levels, int) or max_levels < 0: raise ValueError(\\"max_levels must be a non-negative integer\\") if quiet not in [0, 1, 2]: raise ValueError(\\"quiet must be 0, 1, or 2\\") if optimize not in [-1, 0, 1, 2]: raise ValueError(\\"optimize must be -1, 0, 1, or 2\\") if not isinstance(workers, int) or workers < 1: raise ValueError(\\"workers must be a positive integer\\") regex = re.compile(regex_pattern) if regex_pattern else None def filter_func(fullname, filename): return not regex.search(fullname) if regex else True return compileall.compile_dir( source_dir, maxlevels=max_levels, force=force, quiet=quiet, legacy=legacy, optimize=optimize, workers=workers, rx=regex )"},{"question":"# Custom Sequence Type Implementation You are required to implement a custom sequence type named `CustomSequence` using the concepts of object implementation in Python. The sequence should behave similarly to a Python list but with some extended functionalities: - It should support initialization with any iterable. - Provide indexing and slicing. - Support addition and multiplication operations. - Support dynamic resizing (appending, inserting, and removing elements). Your task is to: 1. Define a class `CustomSequence`. 2. Implement the following methods: - `__init__(self, iterable)`: Initialize the sequence with elements from the given iterable. - `__getitem__(self, index)`: Retrieve the item at the given index or a slice if a slice object is provided. - `__setitem__(self, index, value)`: Set the item at the given index to the new value. - `__delitem__(self, index)`: Remove the item at the given index. - `__len__(self)`: Return the number of elements in the sequence. - `__add__(self, other)`: Return a new `CustomSequence` that is the concatenation of this sequence and another sequence. - `__mul__(self, repeat)`: Return a new `CustomSequence` repeated \'repeat\' times. - `append(self, item)`: Add an item to the end of the sequence. - `insert(self, index, item)`: Insert an item at the specified index. - `remove(self, item)`: Remove the first occurrence of an item in the sequence. # Example Usage: ```python cs = CustomSequence([1, 2, 3]) print(cs[1]) # Output: 2 cs.append(4) print(cs) # Output: CustomSequence([1, 2, 3, 4]) del cs[1] print(cs) # Output: CustomSequence([1, 3, 4]) cs.insert(1, 5) print(cs) # Output: CustomSequence([1, 5, 3, 4]) cs.remove(5) print(cs) # Output: CustomSequence([1, 3, 4]) new_cs = cs + [5, 6] print(new_cs) # Output: CustomSequence([1, 3, 4, 5, 6]) repeated_cs = cs * 2 print(repeated_cs) # Output: CustomSequence([1, 3, 4, 1, 3, 4]) ``` # Constraints: - The `__add__` and `__mul__` methods should handle only other CustomSequence objects and integers respectively. - Removing an element that doesn\'t exist should raise a `ValueError`. - Insertions and deletions should maintain the integrity of the sequence. Implement the `CustomSequence` class to fulfill all operations mentioned above.","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def __add__(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Can only concatenate CustomSequence to CustomSequence\\") return CustomSequence(self._data + other._data) def __mul__(self, repeat): if not isinstance(repeat, int): raise TypeError(\\"Can\'t multiply sequence by non-int\\") return CustomSequence(self._data * repeat) def append(self, item): self._data.append(item) def insert(self, index, item): self._data.insert(index, item) def remove(self, item): self._data.remove(item) def __repr__(self): return f\\"CustomSequence({self._data})\\""},{"question":"Question # Objective Write a python function to implement a simple chat server that handles multiple clients asynchronously using the `selectors` module. Your implementation should demonstrate the use of high-level I/O multiplexing with `selectors.DefaultSelector`. # Function Signature ```python def start_chat_server(host: str, port: int) -> None: ``` # Input - `host` (str): The hostname or IP address to bind the server to. - `port` (int): The port number to listen for incoming client connections. # Output The server should run indefinitely, accepting connections and echoing messages back to the clients asynchronously. # Requirements - Your server should handle multiple clients simultaneously without blocking. - Use `selectors.DefaultSelector` to monitor and manage I/O readiness notifications. - The server should be able to read messages from clients and send responses back to them until clients disconnect. # Constraints - The server must echo messages received from clients back to the respective clients. - Each client connection should be non-blocking. - The server should handle client disconnections gracefully. # Example Usage An example usage of such a function is as follows: ```python # Starting the chat server on localhost:8888 start_chat_server(\'localhost\', 8888) ``` # Hints - You can refer to the example provided in the documentation for a simple echo server. - Make use of the selector\'s register, unregister, and select methods for managing file objects. - Ensure data is read from and written to clients in a non-blocking fashion.","solution":"import selectors import socket import types def start_chat_server(host: str, port: int) -> None: sel = selectors.DefaultSelector() def accept_wrapper(sock): conn, addr = sock.accept() # Should be ready to read print(f\'Accepted connection from {addr}\') conn.setblocking(False) data = types.SimpleNamespace(addr=addr, inb=b\'\', outb=b\'\') events = selectors.EVENT_READ | selectors.EVENT_WRITE sel.register(conn, events, data=data) def service_connection(key, mask): sock = key.fileobj data = key.data if mask & selectors.EVENT_READ: recv_data = sock.recv(1024) # Should be ready to read if recv_data: data.inb += recv_data print(f\'Received data: {repr(recv_data)} from {data.addr}\') else: print(f\'Closing connection to {data.addr}\') sel.unregister(sock) sock.close() if mask & selectors.EVENT_WRITE: if data.inb: print(f\'Echoing data: {repr(data.inb)} to {data.addr}\') sent = sock.send(data.inb) # Should be ready to write data.inb = data.inb[sent:] lsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) lsock.bind((host, port)) lsock.listen() print(f\'Listening on {(host, port)}\') lsock.setblocking(False) sel.register(lsock, selectors.EVENT_READ, data=None) try: while True: events = sel.select(timeout=None) for key, mask in events: if key.data is None: accept_wrapper(key.fileobj) else: service_connection(key, mask) except KeyboardInterrupt: print(\\"Stopping the server.\\") finally: sel.close()"},{"question":"# Problem: Advanced Data Encoding and Decoding with Base64 Module You have been given a task to process a mixed set of data encodings in a secure communication system. Your task is to implement functions that can encode binary data into various base encodings and decode it back. You also need to handle potential errors during the encoding and decoding process. Requirements: 1. **Function 1: `custom_encode(data: bytes, encoding_type: str, altchars: bytes = None) -> bytes`** - **Parameters:** - `data`: The binary data to be encoded (as bytes). - `encoding_type`: A string indicating the type of encoding to use. It can be one of `\'b16\'`, `\'b32\'`, `\'b64\'`, `\'base85\'`, or `\'ascii85\'`. - `altchars`: Optional parameter, relevant only for `\'b64\'` and `\'base85\'` types, specifying an alternative alphabet for the encoding. - **Returns**: Encoded data as bytes. - **Exceptions**: Raise a `ValueError` if an unsupported encoding type is provided or if `altchars` is provided with unsupported encoding types. 2. **Function 2: `custom_decode(data: bytes, encoding_type: str, altchars: bytes = None) -> bytes`** - **Parameters:** - `data`: The encoded data (as bytes). - `encoding_type`: A string indicating the type of decoding to use. It can be one of `\'b16\'`, `\'b32\'`, `\'b64\'`, `\'base85\'`, or `\'ascii85\'`. - `altchars`: Optional parameter, relevant only for `\'b64\'` and `\'base85\'` types, specifying an alternative alphabet used in the encoding. - **Returns**: Decoded data as bytes. - **Exceptions**: Raise a `binascii.Error` or `ValueError` if decoding fails, or if an unsupported encoding type is provided, or if `altchars` is provided with unsupported encoding types. Constraints: - Use the `base64` module for encoding and decoding. - For `altchars`, it must be a bytes-like object of length 2 if provided. - The functions must handle both URL-safe and standard Base64 encodings. - Ensure proper exception handling and validation, maintaining security considerations (like discarding non-alphabet characters only when the default settings are not overridden). Example Usage: ```python # Example cases for custom_encode encoded_b64 = custom_encode(b\'data to be encoded\', \'b64\') print(encoded_b64) # Should print the base64 encoded result encoded_b32 = custom_encode(b\'data to be encoded\', \'b32\') print(encoded_b32) # Should print the base32 encoded result # Example cases for custom_decode decoded_b64 = custom_decode(encoded_b64, \'b64\') print(decoded_b64) # Should return b\'data to be encoded\' decoded_b32 = custom_decode(encoded_b32, \'b32\') print(decoded_b32) # Should return b\'data to be encoded\' # Example for using alternative characters custom_encoded = custom_encode(b\'data\', \'b64\', altchars=b\'-_\') print(custom_encoded) # Should use \'-\' and \'_\' instead of \'+\' and \'/\' custom_decoded = custom_decode(custom_encoded, \'b64\', altchars=b\'-_\') print(custom_decoded) # Should return b\'data\' ``` Implement these two functions and ensure they adhere to the requirements and constraints provided.","solution":"import base64 def custom_encode(data: bytes, encoding_type: str, altchars: bytes = None) -> bytes: if altchars is not None and len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") if encoding_type == \'b16\': return base64.b16encode(data) elif encoding_type == \'b32\': return base64.b32encode(data) elif encoding_type == \'b64\': return base64.b64encode(data, altchars) if altchars else base64.b64encode(data) elif encoding_type == \'base85\': return base64.b85encode(data, altchars) if altchars else base64.b85encode(data) elif encoding_type == \'ascii85\': return base64.a85encode(data) else: raise ValueError(f\\"Unsupported encoding type: {encoding_type}\\") def custom_decode(data: bytes, encoding_type: str, altchars: bytes = None) -> bytes: if altchars is not None and len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") if encoding_type == \'b16\': return base64.b16decode(data) elif encoding_type == \'b32\': return base64.b32decode(data) elif encoding_type == \'b64\': return base64.b64decode(data, altchars) if altchars else base64.b64decode(data) elif encoding_type == \'base85\': return base64.b85decode(data, altchars) if altchars else base64.b85decode(data) elif encoding_type == \'ascii85\': return base64.a85decode(data) else: raise ValueError(f\\"Unsupported encoding type: {encoding_type}\\")"},{"question":"Coding Assessment Question # Objective: To assess the understanding of Just-In-Time (JIT) compilation in PyTorch, specifically involving scripting and tracing techniques provided by the `torch.utils.jit` module. # Problem Statement: You are given a simple Feed-Forward Neural Network (FFNN) defined in PyTorch for the task of binary classification. Your task is to: 1. Convert the model into TorchScript using both scripting and tracing methods. 2. Compare the performance (inference time) between the original model, scripted model, and traced model on a given set of input data. # Model Definition: ```python import torch import torch.nn as nn class SimpleFFNN(nn.Module): def __init__(self): super(SimpleFFNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) x = self.sigmoid(x) return x ``` # Input: - A random tensor `input_data` of shape (N, 10) where N can be any positive integer (e.g., `input_data = torch.randn(100, 10)`). # Instructions: 1. **Script the Model:** - Use the scripting method to convert the `SimpleFFNN` model to TorchScript. 2. **Trace the Model:** - Use the tracing method to convert the `SimpleFFNN` model to TorchScript with the provided `input_data`. 3. **Performance Comparison:** - Measure and compare the inference time for the following: - Original PyTorch model. - Scripted TorchScript model. - Traced TorchScript model. # Expected Output: - Print the inference time for each model type: original, scripted, and traced. # Performance Requirements: - Ensure that the measurements are reliable (e.g., average over multiple runs). - Discuss any observed differences in performance. # Constraints: - You can use any default settings for the random number generation, but ensure reproducibility by setting the random seed. - Use the provided model definition without modifications. # Example Solution Outline: 1. Define the `SimpleFFNN` model. 2. Convert the model to TorchScript using both scripting and tracing. 3. Measure the inference time for each model type using the same input data. 4. Print and compare the inference times. Happy coding and good luck!","solution":"import torch import torch.nn as nn import time class SimpleFFNN(nn.Module): def __init__(self): super(SimpleFFNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) x = self.sigmoid(x) return x def script_model(model): return torch.jit.script(model) def trace_model(model, input_data): return torch.jit.trace(model, input_data) def measure_inference_time(model, input_data, num_iterations=100): start = time.time() for _ in range(num_iterations): _ = model(input_data) end = time.time() avg_inference_time = (end - start) / num_iterations return avg_inference_time if __name__ == \\"__main__\\": torch.manual_seed(0) input_data = torch.randn(100, 10) # Original model model = SimpleFFNN() # Scripted model scripted_model = script_model(model) # Traced model traced_model = trace_model(model, input_data) # Measure inference time num_iterations = 100 original_time = measure_inference_time(model, input_data, num_iterations) scripted_time = measure_inference_time(scripted_model, input_data, num_iterations) traced_time = measure_inference_time(traced_model, input_data, num_iterations) print(f\\"Original model inference time: {original_time:.6f} seconds\\") print(f\\"Scripted model inference time: {scripted_time:.6f} seconds\\") print(f\\"Traced model inference time: {traced_time:.6f} seconds\\")"},{"question":"**Question: Analyzing and Visualizing Climate Data with Seaborn** You are provided with a dataset `climate.csv` containing climate data for various regions over multiple years. Your task is to write a Python function using the Seaborn library to create an informative visualization of this data. The dataset includes the following columns: - `Date`: The date of the observation. - `Region`: The geographic region of the observation. - `Temperature`: The observed temperature. - `CO2`: The observed CO2 levels. The function you write should: 1. Load the dataset into a pandas DataFrame. 2. Convert the `Date` column to datetime format and extract the year. 3. Create a seaborn.objects Plot to visualize the temperature trends over the years, separating data by region. 4. Add a `Band` to visualize the interval of temperature values for each year. 5. Add a `Line` to show the mean temperature trend over the years for each region. # Requirements: - Use reasonable filters to clean the data (e.g., exclude years with partial data). - The plot should include appropriate labels and titles for clarity. - The plot should be saved as `climate_trends.png`. # Function Signature: ```python import pandas as pd import seaborn.objects as so def visualize_climate_trends(): # load the dataset # process data # create Plot # save plot as `climate_trends.png` pass ``` # Example Input: Sample data in `climate.csv`: ```csv Date,Region,Temperature,CO2 2000-01-01,North,15.3,400 2001-01-01,North,15.8,402 2002-01-01,North,15.6,405 ... ``` # Example Output: - A plot that shows temperature trends over the years for each region, saved as `climate_trends.png`. # Constraints: - You may assume all dates are in the format `YYYY-MM-DD`. - Minimal missing data for temperature and CO2, but handle cases where necessary. - Optimize the code for readability and performance. # Notes: - Describe each step in comments to show your understanding. - Use the seaborn.objects module functions as demonstrated in the provided documentation.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def visualize_climate_trends(): # Load the dataset df = pd.read_csv(\'climate.csv\') # Convert the Date column to datetime format and extract the year df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Year\'] = df[\'Date\'].dt.year # Filter out data to remove any issues with partial data (e.g., years with incomplete data) df = df.dropna(subset=[\'Temperature\', \'CO2\']) # Create the Seaborn plot p = (so.Plot(df, x=\\"Year\\", y=\\"Temperature\\") .facet(\\"Region\\") .add(so.Band(), so.Est(errorbar=\\"sd\\")) .add(so.Line(), so.Est()) .label(x=\\"Year\\", y=\\"Temperature (°C)\\", title=\\"Temperature Trends Over Years by Region\\")) # Save the plot p.save(\\"climate_trends.png\\", dpi=300) # Example usage: # visualize_climate_trends()"},{"question":"**Pandas Coding Assessment Question** # Objective Your task is to implement a function using `pandas` to analyze a given DataFrame. The function should: 1. Display memory usage details. 2. Identify all boolean operations where any value in the DataFrame is `True`. 3. Apply a user-defined function to modify DataFrame columns without causing mutation errors. 4. Handle missing values particularly for integer columns by converting them to a nullable integer type using `Int64Dtype`. # Function Signature ```python def analyze_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `df` (DataFrame): A pandas DataFrame which can contain columns of different data types including `int64`, `float64`, `datetime64[ns]`, `timedelta64[ns]`, `complex128`, `object`, `bool`, etc. # Output - Analyzed DataFrame (DataFrame): A new DataFrame with modifications implemented according to the following specifications. # Specifications 1. **Memory Usage Details**: - Calculate and print the memory usage for each column in a human-readable format. - Calculate and print the total memory usage. 2. **Boolean Operations**: - Use boolean operations to print whether any value in each column of the DataFrame is `True`. 3. **User-Defined Function Application**: - Apply a user-defined function to each column to add a suffix `_modified` to the names of columns that contain `object` type data. Ensure no mutation errors occur. 4. **Handling Missing Values**: - For any integer columns, convert them to use pandas nullable integer `Int64Dtype` to handle missing values appropriately. # Example Given the DataFrame `df`: ```python import pandas as pd import numpy as np dtypes = [ \\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\" ] n = 5 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} data[\\"object\\"] = [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"] # Explicitly making it strings df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") df.iloc[0, 0] = np.nan # Introducing a NaN value in integer column ``` After running `analyze_dataframe(df)`, the new DataFrame should: - Display memory usage appropriately. - Identify if any values in columns are `True`. - Have the object type columns renamed with `_modified` suffix. - Have integer columns converted to nullable `Int64Dtype`. # Constraints - Do not mutate the input DataFrame directly. Create copies of data where necessary. - Handle any potential performance concerns while accessing/modifying large DataFrames. **Note**: You are required to use appropriate pandas methods to achieve the desired functionality.","solution":"import pandas as pd def analyze_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Calculate and print memory usage for each column in a human-readable format memory_usage = df.memory_usage(deep=True) print(\\"Memory usage (bytes) per column:\\") print(memory_usage) total_memory_usage = memory_usage.sum() print(\\"Total memory usage:\\", total_memory_usage, \\"bytes\\") # Boolean operations to determine if any value in each column is True boolean_true_check = {col: df[col].any() for col in df.columns if df[col].dtype == \'bool\'} print(\\"Columns with any True values:\\", boolean_true_check) # Apply user-defined function to modify column names without mutation errors modified_df = df.copy() def modify_column_names(data): if data.dtype == \'object\': return data.astype(str) # Ensure it\'s object type for processing return data for col in modified_df.columns: modified_df[col] = modify_column_names(modified_df[col]) modified_columns = {col: col + \'_modified\' if modified_df[col].dtype == \'object\' else col for col in modified_df.columns} modified_df.rename(columns=modified_columns, inplace=True) # Handle missing values in integer columns for col in modified_df.select_dtypes(include=[\'int64\']).columns: modified_df[col] = modified_df[col].astype(\'Int64\') return modified_df"},{"question":"Objective Demonstrate your understanding of decision trees in scikit-learn by implementing a decision tree classifier, evaluating its performance, and visualizing the decision tree. Problem Statement You are provided with the famous Iris dataset. Your tasks are: 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Split the dataset into a training set (80%) and a test set (20%). 3. Train a `DecisionTreeClassifier` on the training set. Use the following parameters: - `criterion=\'gini\'` - `max_depth=3` - `min_samples_leaf=2` 4. Predict the class labels for the test set and calculate the accuracy of the model. 5. Visualize the decision tree using `plot_tree`. 6. Export the decision tree in text format using `export_text` and print it. Input and Output Formats - **Input**: None (since you will load the Iris dataset within your code) - **Output**: - Accuracy of the model on the test set (float) - Visualization of the decision tree (graph) - Text representation of the decision tree (text) Constraints - You must use scikit-learn\'s `DecisionTreeClassifier`. - The specified parameters (`criterion=\'gini\'`, `max_depth`, and `min_samples_leaf`) must be used when training the model. Example Code The following example demonstrates how to load the dataset and split it into training and test sets: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ``` Evaluation Criteria - Correctness: Does the code correctly load, split, train, predict, and evaluate the decision tree? - Visualization: Is the decision tree correctly visualized using `plot_tree`? - Text Export: Is the text representation of the decision tree correctly exported using `export_text` and printed? Good Luck!","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text import matplotlib.pyplot as plt def decision_tree_iris(): # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the DecisionTreeClassifier clf = DecisionTreeClassifier(criterion=\'gini\', max_depth=3, min_samples_leaf=2, random_state=42) clf.fit(X_train, y_train) # Predict the class labels for the test set y_pred = clf.predict(X_test) # Calculate and return the accuracy accuracy = clf.score(X_test, y_test) # Visualize the decision tree plt.figure(figsize=(12,8)) plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) plt.show() # Export and print the decision tree in text format tree_text = export_text(clf, feature_names=iris.feature_names) print(tree_text) return accuracy, tree_text # Call the function and get the accuracy and tree_text accuracy, tree_text = decision_tree_iris() print(f\\"Accuracy of the model: {accuracy}\\") print(\\"nText representation of the decision tree:\\") print(tree_text)"},{"question":"**Objective:** Demonstrate your understanding of the seaborn `pairplot` function and its customization options by creating a comprehensive plot for a given dataset. **Problem:** Using the seaborn library, create a pairplot visualization for the `tips` dataset provided by seaborn. **Instructions:** 1. Load the `tips` dataset using `sns.load_dataset(\'tips\')`. 2. Create a pairplot of the dataset with the following customizations: - Add a `hue` to distinguish between smokers and non-smokers. - Set the diagonal plots to histograms. - Use different markers for different days of the week. - Set the size of each subplot to 2.5 inches. - Select only the numerical variables present in the dataset for the pairplot. - Plot only the lower triangle of the pairplot. 3. Enhance the pairplot\'s off-diagonal scatter plots by customizing the marker size and edgecolor (you can choose your preference for these customizations). 4. Your implementation should include the usage of `plot_kws` and `diag_kws` to customize the pairplot as mentioned above and further customization by mapping additional properties using the `PairGrid` object if necessary. **Expected Input and Output:** - **Input:** None (the function should internally load the `tips` dataset from seaborn). - **Output:** A seaborn pairplot visualization that matches the specified customizations. **Performance Requirements:** - The function should execute efficiently without unnecessary computations, and the plot should render successfully. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def custom_pairplot(): # Implement the instructions here pass # Call the function to display the plot custom_pairplot() ``` **Constraints:** - You are not allowed to use any external packages other than seaborn and matplotlib. - The plot customization should be done primarily using seaborn\'s `pairplot` and its parameters, with minimal reliance on additional matplotlib customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_pairplot(): # Load the tips dataset from seaborn tips = sns.load_dataset(\'tips\') # Define parameters for customization plot_kws = {\'s\': 50, \'edgecolor\': \'w\', \'alpha\': 0.7} diag_kws = {\'bins\': 15, \'edgecolor\': \'k\'} pair_grid = sns.PairGrid(tips, hue=\\"smoker\\", hue_kws={\\"marker\\": [\\"o\\", \\"s\\", \\"D\\", \\"^\\"]}, palette=\\"husl\\", corner=True, height=2.5) pair_grid = pair_grid.map_diag(plt.hist, **diag_kws) pair_grid = pair_grid.map_offdiag(plt.scatter, **plot_kws) pair_grid = pair_grid.add_legend() # Display the plot plt.show()"},{"question":"# Question: **Objective:** Using the `seaborn` library, write a function `create_custom_jointplot` that visualizes the relationship between two specified columns of the `penguins` dataset with various customization options. **Function Signature:** ```python def create_custom_jointplot(x: str, y: str, hue_column: str = None, kind: str = \'scatter\', additional_kwargs: dict = None, figure_params: dict = None) -> None: Create a customized joint plot for the `penguins` dataset. Parameters: x (str): The name of the column to be used as the x-axis. y (str): The name of the column to be used as the y-axis. hue_column (str, optional): The name of the column to be used for color encoding. Default is None. kind (str, optional): The type of plot to use (\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'). Default is \'scatter\'. additional_kwargs (dict, optional): Additional keyword arguments to pass to the underlying plot functions. Default is None. figure_params (dict, optional): Parameters to control the size and layout of the figure. Default is None. Returns: None ``` **Instructions:** 1. Load the `penguins` dataset from `seaborn`. 2. Create a joint plot using the specified `x` and `y` columns. 3. Use the `hue_column` parameter to add conditional colors if provided. 4. Use the `kind` parameter to determine the type of plot. 5. Incorporate any additional customization provided through `additional_kwargs`. 6. Control the figure size and layout using the parameters provided in `figure_params`. 7. Your function should display the plot. **Examples:** ```python # Example 1: Basic scatter plot create_custom_jointplot(x=\'bill_length_mm\', y=\'bill_depth_mm\') # Example 2: Colored by species with KDE plot create_custom_jointplot(x=\'bill_length_mm\', y=\'bill_depth_mm\', hue_column=\'species\', kind=\'kde\') # Example 3: Regression plot with additional keyword arguments and custom figure parameters create_custom_jointplot(x=\'bill_length_mm\', y=\'bill_depth_mm\', kind=\'reg\', additional_kwargs={\'color\':\'g\', \'line_kws\':{\'linewidth\':2}}, figure_params={\'height\':6, \'ratio\':3, \'marginal_ticks\':True}) ``` **Constraints:** - The columns `x` and `y` must exist in the `penguins` dataset. - The `kind` parameter must be one of \'scatter\', \'kde\', \'reg\', \'hist\', or \'hex\'. **Notes:** - If the dataset is not available locally, ensure your environment can download it via `sns.load_dataset`. **Performance Requirements:** - Ensure that plots are generated efficiently and visually accurate.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_jointplot(x: str, y: str, hue_column: str = None, kind: str = \'scatter\', additional_kwargs: dict = None, figure_params: dict = None) -> None: Create a customized joint plot for the `penguins` dataset. Parameters: x (str): The name of the column to be used as the x-axis. y (str): The name of the column to be used as the y-axis. hue_column (str, optional): The name of the column to be used for color encoding. Default is None. kind (str, optional): The type of plot to use (\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'). Default is \'scatter\'. additional_kwargs (dict, optional): Additional keyword arguments to pass to the underlying plot functions. Default is None. figure_params (dict, optional): Parameters to control the size and layout of the figure. Default is None. Returns: None penguins = sns.load_dataset(\\"penguins\\") # Set figure parameters if figure_params: sns.set_context(\\"notebook\\", rc=figure_params) plot = sns.jointplot(data=penguins, x=x, y=y, hue=hue_column, kind=kind, **(additional_kwargs or {})) plt.show()"},{"question":"**Objective**: You are provided with a dataset and your task is to generate various visual analyses using the seaborn package to explore the relationship between different variables and their residuals from a linear regression model. This question will assess your understanding of seaborn, specifically focusing on the `sns.residplot` function and its advanced features. **Dataset**: For this exercise, we will use the \\"mpg\\" dataset included in seaborn\'s example datasets. **Task**: 1. Load the \\"mpg\\" dataset using seaborn. 2. Generate a basic residual plot to explore the relationship between `weight` and `displacement`. 3. Generate a residual plot to examine the relationship between `horsepower` and `mpg`. 4. Create a residual plot to check whether removing higher-order trends between `horsepower` and `mpg` (using a polynomial order of 2) stabilizes the residuals. 5. Add a LOWESS curve to the residual plot between `horsepower` and `mpg` to further reveal or emphasize any existing structure. Use a red line for the LOWESS curve. **Constraints and Requirements**: - Implement the solution in a function named `generate_plots()`. - The function should not take any input parameters. - Ensure that all plots are displayed. - Use proper visualization settings for clarity (labels, titles, etc.). - Comments and explanations within your code are encouraged to demonstrate your understanding. **Expected Output**: Your function should generate and display four residual plots as specified in the tasks above. Each plot should be well-labeled and easy to interpret. **Performance**: - Efficient data handling and plotting are essential. - Ensure that the plots are generated quickly without excessive computational overhead. ```python import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Plot 1: Basic residual plot for weight vs. displacement plt.figure(figsize=(12, 8)) plt.subplot(2, 2, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot of Weight vs Displacement\') # Plot 2: Basic residual plot for horsepower vs mpg plt.subplot(2, 2, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot of Horsepower vs MPG\') # Plot 3: Higher-order trend removal for horsepower vs mpg plt.subplot(2, 2, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot of Horsepower vs MPG (Order 2)\') # Plot 4: Adding a LOWESS curve for horsepower vs mpg plt.subplot(2, 2, 4) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot of Horsepower vs MPG with LOWESS\') plt.tight_layout() plt.show() ``` **Note**: Make sure you have seaborn and matplotlib installed in your Python environment. Use the command `pip install seaborn matplotlib` if necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Plot 1: Basic residual plot for weight vs. displacement plt.figure(figsize=(12, 8)) plt.subplot(2, 2, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot of Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') # Plot 2: Basic residual plot for horsepower vs mpg plt.subplot(2, 2, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot of Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') # Plot 3: Higher-order trend removal for horsepower vs mpg plt.subplot(2, 2, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot of Horsepower vs MPG (Order 2)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') # Plot 4: Adding a LOWESS curve for horsepower vs mpg plt.subplot(2, 2, 4) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot of Horsepower vs MPG with LOWESS\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.tight_layout() plt.show()"},{"question":"**Objective:** Create a custom command interpreter using the `cmd` module in Python. **Problem Statement:** You are required to implement a command interpreter for managing a simple to-do list. The interpreter should provide commands to add a task, list all tasks, remove a task, and mark a task as completed. The interpreter should also support recording and playback of commands to/from a file. **Detailed Instructions:** 1. Implement a class `TodoShell` that extends the `cmd.Cmd` class. 2. Define the following commands: - `do_add`: Adds a new task to the to-do list. - Input format: `ADD <task_description>` - Example: `ADD Buy groceries` - `do_list`: Lists all tasks along with their statuses (completed or not). - Input format: `LIST` - Example of output: ``` 1. [ ] Buy groceries 2. [x] Read a book ``` - `do_remove`: Removes a task by its number. - Input format: `REMOVE <task_number>` - Example: `REMOVE 1` - `do_complete`: Marks a task as completed by its number. - Input format: `COMPLETE <task_number>` - Example: `COMPLETE 2` - `do_record`: Records future commands to a file. - Input format: `RECORD <filename>` - Example: `RECORD commands.txt` - `do_playback`: Plays back commands from a file. - Input format: `PLAYBACK <filename>` - Example: `PLAYBACK commands.txt` - `do_quit`: Exits the interpreter. - Input format: `QUIT` 3. Implement hook methods to: - Convert commands to lowercase in `precmd()`. - Record commands if recording mode is active (handle in `precmd()`). 4. Create a session to demonstrate the following features: - Adding tasks - Listing tasks - Removing tasks - Completing tasks - Recording commands to a file - Playing back commands from a file - Exiting the interpreter **Constraints:** - Ensure that task numbers are valid when executing `REMOVE` and `COMPLETE` commands. - Input parsing errors should be handled gracefully with appropriate error messages. **Example Usage:** ``` Welcome to the To-Do List Shell. Type help or ? to list commands. (todo) ADD Buy groceries (todo) ADD Read a book (todo) LIST 1. [ ] Buy groceries 2. [ ] Read a book (todo) COMPLETE 1 (todo) LIST 1. [x] Buy groceries 2. [ ] Read a book (todo) REMOVE 2 (todo) LIST 1. [x] Buy groceries (todo) RECORD commands.txt (todo) ADD Walk the dog (todo) ADD Learn Python (todo) QUIT Thank you for using the To-Do List Shell. ```","solution":"import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the To-Do List Shell. Type help or ? to list commands.n\' prompt = \'(todo) \' def __init__(self): super().__init__() self.tasks = [] self.recording = False self.record_file = None def do_add(self, arg): \\"Add a new task: ADD <task_description>\\" self.tasks.append({\\"description\\": arg, \\"completed\\": False}) print(f\\"Task added: {arg}\\") def do_list(self, arg): \\"List all tasks: LIST\\" for idx, task in enumerate(self.tasks, 1): status = \\"[x]\\" if task[\\"completed\\"] else \\"[ ]\\" print(f\\"{idx}. {status} {task[\'description\']}\\") def do_remove(self, arg): \\"Remove a task by its number: REMOVE <task_number>\\" try: idx = int(arg) - 1 if 0 <= idx < len(self.tasks): removed_task = self.tasks.pop(idx) print(f\\"Task removed: {removed_task[\'description\']}\\") else: print(\\"Error: Invalid task number\\") except ValueError: print(\\"Error: Invalid input\\") def do_complete(self, arg): \\"Mark a task as completed by its number: COMPLETE <task_number>\\" try: idx = int(arg) - 1 if 0 <= idx < len(self.tasks): self.tasks[idx][\\"completed\\"] = True print(f\\"Task completed: {self.tasks[idx][\'description\']}\\") else: print(\\"Error: Invalid task number\\") except ValueError: print(\\"Error: Invalid input\\") def do_record(self, arg): \\"Record future commands to a file: RECORD <filename>\\" self.record_file = open(arg, \'w\') self.recording = True print(f\\"Recording commands to {arg}\\") def do_playback(self, arg): \\"Play back commands from a file: PLAYBACK <filename>\\" try: with open(arg, \'r\') as f: for line in f: print(f\\"(playback) {line.strip()}\\") self.onecmd(line.strip()) except FileNotFoundError: print(\\"Error: File not found\\") def do_quit(self, arg): \\"Exit the interpreter: QUIT\\" if self.record_file: self.record_file.close() print(\\"Thank you for using the To-Do List Shell.\\") return True def precmd(self, line): if self.recording and self.record_file and line not in {\\"record\\", \\"playback\\", \\"quit\\"}: self.record_file.write(line + \'n\') return line if __name__ == \'__main__\': TodoShell().cmdloop()"},{"question":"# Gaussian Mixture Model and Bayesian Gaussian Mixture Model Implementation Objective: You are required to implement a function to fit both a Gaussian Mixture Model (GMM) and a Bayesian Gaussian Mixture Model (BGMM) to a given dataset. Evaluate the models and determine the optimal number of components for representing the data. Task: Write a function **`fit_mixture_models`** which: 1. Takes the following inputs: - `X`: A numpy array of shape (n_samples, n_features), representing the data. - `max_components`: An integer, representing the maximum number of mixture components to consider. - `model_type`: A string, either \'gmm\' or \'bgmm\', specifying which model to fit. - `covariance_type`: A string, specifying the covariance type for the model. One of `[\'spherical\', \'diag\', \'tied\', \'full\']`. 2. Fit the specified model to the data. 3. If `model_type` is \'gmm\', use the Bayesian Information Criterion (BIC) to select the optimal number of components for the GMM. 4. If `model_type` is \'bgmm\', use the given concentration prior hyperparameters to fit the dataset without pre-specifying the number of components. 5. Return a tuple `(optimal_model, optimal_num_components)`: - `optimal_model`: The fitted model with the optimal number of components. - `optimal_num_components`: The optimal number of components determined by the model. Implementation: The function may use the `GaussianMixture` and `BayesianGaussianMixture` classes from `sklearn.mixture`. Example: ```python def fit_mixture_models(X, max_components, model_type=\'gmm\', covariance_type=\'full\'): from sklearn.mixture import GaussianMixture, BayesianGaussianMixture if model_type == \'gmm\': bic_scores = [] models = [] for n in range(1, max_components + 1): model = GaussianMixture(n_components=n, covariance_type=covariance_type) model.fit(X) bic_scores.append(model.bic(X)) models.append(model) optimal_num_components = range(1, max_components + 1)[bic_scores.index(min(bic_scores))] optimal_model = models[bic_scores.index(min(bic_scores))] elif model_type == \'bgmm\': optimal_model = BayesianGaussianMixture( n_components=max_components, covariance_type=covariance_type, weight_concentration_prior_type=\'dirichlet_process\' ) optimal_model.fit(X) optimal_num_components = len(optimal_model.weights_[optimal_model.weights_ > 1e-2]) return (optimal_model, optimal_num_components) # Example usage: import numpy as np X = np.random.rand(100, 2) # Replace with your data model, components = fit_mixture_models(X, max_components=10, model_type=\'gmm\', covariance_type=\'full\') print(\\"Optimal number of components:\\", components) ``` Constraints: - You may assume `n_samples >= max_components` for the input data. - Use appropriate sklearn\'s classes and methods as needed. - Ensure the function should handle edge cases and is robust.","solution":"from sklearn.mixture import GaussianMixture, BayesianGaussianMixture import numpy as np def fit_mixture_models(X, max_components, model_type=\'gmm\', covariance_type=\'full\'): Fit a Gaussian Mixture Model (GMM) or Bayesian Gaussian Mixture Model (BGMM) to the data. Parameters: X : numpy array of shape (n_samples, n_features) The data to fit the model to. max_components : int The maximum number of mixture components to consider. model_type : str The type of model to fit. Either \'gmm\' or \'bgmm\'. covariance_type : str The type of covariance parameter to use. One of [\'spherical\', \'diag\', \'tied\', \'full\']. Returns: tuple A tuple (optimal_model, optimal_num_components) where: - `optimal_model` is the fitted model with the optimal number of components - `optimal_num_components` is the optimal number of components determined by the model if model_type == \'gmm\': bic_scores = [] models = [] for n in range(1, max_components + 1): model = GaussianMixture(n_components=n, covariance_type=covariance_type, random_state=42) model.fit(X) bic_scores.append(model.bic(X)) models.append(model) optimal_num_components = range(1, max_components + 1)[bic_scores.index(min(bic_scores))] optimal_model = models[bic_scores.index(min(bic_scores))] elif model_type == \'bgmm\': optimal_model = BayesianGaussianMixture( n_components=max_components, covariance_type=covariance_type, random_state=42, weight_concentration_prior_type=\'dirichlet_process\' ) optimal_model.fit(X) optimal_num_components = np.sum(optimal_model.weights_ > 1e-2) else: raise ValueError(\\"model_type should be either \'gmm\' or \'bgmm\'\\") return (optimal_model, optimal_num_components)"},{"question":"# Question: Package Metadata and Entry Points with `importlib.metadata` You have been provided with a Python virtual environment where several packages are installed. You are required to write a Python script to gather and process information about these installed packages using the `importlib.metadata` module. # Task Implement a function `package_info_summary(packages: List[str]) -> Dict[str, Any]` that takes a list of package names and returns a summary of their metadata and entry points. The function should include the following information for each package: 1. Package Name 2. Version 3. Metadata fields (Author, License, Summary) 4. List of console scripts and their corresponding callable details. # Input - `packages` (List[str]): A list of package names to query. # Output - Returns a dictionary where each key is a package name and the value is a dictionary containing: - `\\"version\\"`: Package version as a string. - `\\"metadata\\"`: A dictionary with the keys `\\"author\\"`, `\\"license\\"`, and `\\"summary\\"` corresponding to their respective metadata values. - `\\"console_scripts\\"`: A list of dictionaries with keys `\\"name\\"`, `\\"module\\"`, `\\"attr\\"`, and `\\"extras\\"` that correspond to the specific details for each console script. # Example ```python packages = [\\"wheel\\", \\"setuptools\\"] result = package_info_summary(packages) print(result) ``` Expected Output: ```python { \\"wheel\\": { \\"version\\": \\"0.36.2\\", \\"metadata\\": { \\"author\\": \\"Daniele Varrazzo\\", \\"license\\": \\"MIT\\", \\"summary\\": \\"A built-package format for Python\\" }, \\"console_scripts\\": [ { \\"name\\": \\"wheel\\", \\"module\\": \\"wheel.cli\\", \\"attr\\": \\"main\\", \\"extras\\": [] } ] }, \\"setuptools\\": { \\"version\\": \\"54.1.0\\", \\"metadata\\": { \\"author\\": \\"The readme file\\", \\"license\\": \\"MIT\\", \\"summary\\": \\"Easily download, build, install, upgrade, and uninstall Python packages\\" }, \\"console_scripts\\": [ { \\"name\\": \\"easy_install\\", \\"module\\": \\"setuptools.command.easy_install\\", \\"attr\\": \\"main\\", \\"extras\\": [] } ] } } ``` # Constraints - You can assume that all provided package names are valid and installed in the environment. - If a specific piece of metadata is not available for a package, use `None` as the value in the output dictionary. # Notes - Use the `importlib.metadata` module to gather the necessary information. - Ensure the function handles any exceptions that may arise during the metadata retrieval process gracefully.","solution":"import importlib.metadata from typing import List, Dict, Any def package_info_summary(packages: List[str]) -> Dict[str, Any]: package_summary = {} for package in packages: try: dist = importlib.metadata.distribution(package) metadata = dist.metadata console_scripts = [] for entry_point in importlib.metadata.entry_points().get(\'console_scripts\', []): if entry_point.dist.name == package: console_scripts.append({ \\"name\\": entry_point.name, \\"module\\": entry_point.value.split(\\":\\")[0], \\"attr\\": entry_point.value.split(\\":\\")[1] if \\":\\" in entry_point.value else \\"\\", \\"extras\\": list(entry_point.extras) }) package_summary[package] = { \\"version\\": metadata.get(\'Version\', None), \\"metadata\\": { \\"author\\": metadata.get(\'Author\', None), \\"license\\": metadata.get(\'License\', None), \\"summary\\": metadata.get(\'Summary\', None) }, \\"console_scripts\\": console_scripts } except importlib.metadata.PackageNotFoundError: package_summary[package] = None return package_summary"},{"question":"# Question: **Objective:** Implement a custom scoring function and use it to evaluate the performance of a machine learning model using scikit-learn. **Problem Statement:** You are provided with a dataset and asked to perform a regression task. The goal is to predict the output variable as accurately as possible. However, the problem requires using a custom scoring function to assess the model\'s performance. Specifically, you need to implement the Mean Absolute Percentage Error (MAPE) as the custom scoring function and use it to evaluate a Linear Regression model. **Requirements:** 1. Implement the MAPE scoring function. 2. Use the MAPE scoring function in `cross_val_score` to evaluate the model. 3. Fit a Linear Regression model to the dataset and report the cross-validated MAPE scores. **Dataset:** Use the `California housing` dataset available in `sklearn.datasets`. **Steps:** 1. Load and split the dataset into features `X` and target `y`. 2. Implement the MAPE scoring function. 3. Define the custom scorer using `make_scorer`. 4. Perform cross-validation using the custom scorer and a Linear Regression model. 5. Report the cross-validated MAPE scores. **Constraints:** - You must use `scikit-learn` for model training and evaluation. - Implement the MAPE scoring function from scratch (do not use any existing implementation of MAPE). **Example Output:** An array of MAPE scores from cross-validation. **Function Signature:** ```python from sklearn.datasets import fetch_california_housing from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer from sklearn.model_selection import cross_val_score def custom_mape(y_true, y_pred): Custom Mean Absolute Percentage Error (MAPE) function. Parameters: y_true (array-like): True target values. y_pred (array-like): Predicted target values. Returns: float: MAPE value. # Your implementation here def evaluate_model_with_mape(): Evaluates a Linear Regression model using MAPE as the scoring function. Returns: array: Cross-validated MAPE scores. # Load the dataset data = fetch_california_housing() X, y = data.data, data.target # Create the Linear Regression model model = LinearRegression() # Create the custom scorer mape_scorer = make_scorer(custom_mape, greater_is_better=False) # Perform cross-validation and return the scores scores = cross_val_score(model, X, y, scoring=mape_scorer, cv=5) return scores # Usage mape_scores = evaluate_model_with_mape() print(\\"Cross-validated MAPE scores:\\", mape_scores) ``` **Expected Answer:** The implementation of `custom_mape` function and the `evaluate_model_with_mape` function should properly demonstrate the use of custom scoring in model evaluation.","solution":"import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer from sklearn.model_selection import cross_val_score def custom_mape(y_true, y_pred): Custom Mean Absolute Percentage Error (MAPE) function. Parameters: y_true (array-like): True target values. y_pred (array-like): Predicted target values. Returns: float: MAPE value. y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def evaluate_model_with_mape(): Evaluates a Linear Regression model using MAPE as the scoring function. Returns: array: Cross-validated MAPE scores. # Load the dataset data = fetch_california_housing() X, y = data.data, data.target # Create the Linear Regression model model = LinearRegression() # Create the custom scorer mape_scorer = make_scorer(custom_mape, greater_is_better=False) # Perform cross-validation and return the scores scores = cross_val_score(model, X, y, scoring=mape_scorer, cv=5) return scores # Usage mape_scores = evaluate_model_with_mape() print(\\"Cross-validated MAPE scores:\\", mape_scores)"},{"question":"# **XML Manipulation and Analysis** **Objective**: Using the `xml.etree.ElementTree` module, demonstrate your ability to parse, navigate, and modify XML data. **Problem Statement**: You are given an XML document containing book information. Each book has a title, author, year of publication, and a list of genres. An example XML data string is provided below: ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> <genres> <genre>Fiction</genre> <genre>Classic</genre> </genres> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <genres> <genre>Fiction</genre> <genre>Drama</genre> </genres> </book> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <genres> <genre>Fiction</genre> <genre>Dystopia</genre> </genres> </book> </library> ``` **Your tasks**: 1. **Parse the XML data**: - Write a function `parse_xml(xml_data: str) -> Element` that takes an XML data string as input and returns the root element of the parsed XML tree. 2. **Extract information**: - Write a function `get_books_by_genre(root: Element, genre: str) -> List[str]` that takes the root element and a genre as input and returns a list of book titles that belong to the specified genre. - Write a function `get_author_books(root: Element, author: str) -> List[str]` that takes the root element and an author name as input and returns a list of titles of books written by that author. 3. **Modify the XML tree**: - Write a function `add_year_extension(root: Element, extension: str) -> None` that adds a text extension to every publication year (e.g., adding \\" AD\\" would convert \\"1925\\" to \\"1925 AD\\"). 4. **Serialize the modified XML tree**: - Write a function `serialize_xml(root: Element) -> str` that takes the root element of the modified XML tree and returns its string representation. **Constraints**: - The input XML data will always be well-formed. - There will be no duplicate titles or authors in the XML file. **Example Usage**: ```python xml_data = \'\'\'<library>...</library>\'\'\' # The example XML data provided above. root = parse_xml(xml_data) # Task 1 print(root.tag) # Output: \'library\' # Task 2 print(get_books_by_genre(root, \'Fiction\')) # Output: [\'The Great Gatsby\', \'To Kill a Mockingbird\', \'1984\'] print(get_author_books(root, \'George Orwell\')) # Output: [\'1984\'] # Task 3 add_year_extension(root, \' AD\') print([elem.text for elem in root.iter(\'year\')]) # Output: [\'1925 AD\', \'1960 AD\', \'1949 AD\'] # Task 4 print(serialize_xml(root)) # Output: \'<library>...</library>\' (Modified XML as a string) ``` Use the provided example XML data string to test your implementation.","solution":"import xml.etree.ElementTree as ET from typing import List def parse_xml(xml_data: str) -> ET.Element: Parses the XML data string and returns the root element. root = ET.fromstring(xml_data) return root def get_books_by_genre(root: ET.Element, genre: str) -> List[str]: Returns a list of book titles that belong to the specified genre. book_titles = [] for book in root.findall(\'book\'): for book_genre in book.find(\'genres\').findall(\'genre\'): if book_genre.text == genre: book_titles.append(book.find(\'title\').text) break return book_titles def get_author_books(root: ET.Element, author: str) -> List[str]: Returns a list of titles of books written by the specified author. book_titles = [] for book in root.findall(\'book\'): if book.find(\'author\').text == author: book_titles.append(book.find(\'title\').text) return book_titles def add_year_extension(root: ET.Element, extension: str) -> None: Adds a text extension to every publication year. for year in root.iter(\'year\'): year.text += extension def serialize_xml(root: ET.Element) -> str: Returns the string representation of the XML tree. return ET.tostring(root, encoding=\'unicode\')"},{"question":"Understanding and Implementing Custom JSON Serialization and Deserialization You are required to design a custom serialization and deserialization process for Python complex number objects using the JSON module. The goal is to ensure that complex numbers can be correctly serialized to JSON and deserialized back to Python objects. Task 1. **Define a Custom Encoder**: - Create a class `ComplexEncoder` that inherits from `json.JSONEncoder`. - Override the `default` method to handle complex number objects. The serialization should transform a complex number into a dictionary with keys `\'__complex__\'`, `\'real\'`, and `\'imag\'`. 2. **Define a Custom Decoder**: - Create a function `as_complex` that will act as the `object_hook` for `json.loads`. - This function should check if the JSON dictionary contains the key `\'__complex__\'` and, if it does, should convert the dictionary back to a complex number. 3. **Test Your Implementation**: - Serialize a list of complex numbers and some other types of data to JSON format using `json.dumps` with your custom encoder. - Deserialize the JSON format back to Python objects using `json.loads` with your custom decoder. Input and Output Formats - **Input**: A list that may contain complex numbers, strings, and integers. - **Output**: Serialized JSON string with complex numbers represented correctly, followed by its correct deserialization back to Python objects. Constraints - You must handle cases where the input might not only contain complex numbers but other data types as well. - Ensure that the implementation does not break if the input list is empty or contains only primitives. Example ```python import json # Part 1: Define Custom Encoder class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) # Part 2: Define Custom Decoder def as_complex(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct # Part 3: Testing Implementation data = [2 + 3j, 3.14, \\"Hello, World!\\"] # Serialize with custom encoder json_data = json.dumps(data, cls=ComplexEncoder) print(json_data) # Output: [{\\"__complex__\\": true, \\"real\\": 2.0, \\"imag\\": 3.0}, 3.14, \\"Hello, World!\\"] # Deserialize with custom decoder python_data = json.loads(json_data, object_hook=as_complex) print(python_data) # Output: [(2+3j), 3.14, \\"Hello, World!\\"] ``` Performance Considerations - Ensure the `default` method of the encoder is efficient and only processes complex numbers while allowing other types to be handled by the superclass. - The decoder should effectively check and convert dictionaries representing complex numbers back to their complex number representation without affecting other non-complex types.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def as_complex(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct # Example of usage: data = [2 + 3j, 3.14, \\"Hello, World!\\"] # Serialize with custom encoder json_data = json.dumps(data, cls=ComplexEncoder) print(json_data) # Output: [{\\"__complex__\\": true, \\"real\\": 2.0, \\"imag\\": 3.0}, 3.14, \\"Hello, World!\\"] # Deserialize with custom decoder python_data = json.loads(json_data, object_hook=as_complex) print(python_data) # Output: [(2+3j), 3.14, \\"Hello, World!\\"]"},{"question":"# Gradient Verification with PyTorch\'s Autograd Objective Write a function that verifies the gradients of a given real-valued function using PyTorch\'s `gradcheck` and `gradgradcheck`. Problem Statement You are provided with a real-valued function `f` that takes a tensor `x` as input and produces a tensor `y` of the same shape. Implement a function `verify_gradients` that checks the correctness of the gradients of `f` using PyTorch\'s `gradcheck` and `gradgradcheck` utilities. Function Signature ```python def verify_gradients(f, x): Verifies the gradients of function f using PyTorch\'s gradcheck and gradgradcheck. Parameters: f (callable): The function to verify. It should accept a PyTorch tensor as input and return a PyTorch tensor of the same shape. x (torch.Tensor): The input tensor to the function f. It should have requires_grad=True. Returns: bool: True if both gradcheck and gradgradcheck pass, False otherwise. ``` Input - `f`: A callable function that accepts a single input tensor and returns an output tensor of the same shape. - `x`: A PyTorch tensor with `requires_grad=True`. Output - A boolean value indicating whether both `gradcheck` and `gradgradcheck` pass (True) or fail (False). Requirements - Use PyTorch\'s `gradcheck` to verify the first-order gradients. - Use PyTorch\'s `gradgradcheck` to verify the second-order gradients. - Ensure that the input tensor `x` has `requires_grad=True`. Constraints - The function `f` can have multiple outputs, and all outputs should be of the same shape. - The input tensor `x` can be of any shape, but it must have `requires_grad=True`. Example ```python import torch # Define a simple real-valued function def simple_function(x): return x ** 2 # Define an input tensor with requires_grad=True x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Call the verification function result = verify_gradients(simple_function, x) print(result) # Should print True if the gradients are correct ``` You can assume that the input function `f` and the tensor `x` are well-defined and properly initialized when testing the `verify_gradients` function. Your implementation should be robust and handle various tensor shapes and sizes efficiently.","solution":"import torch from torch.autograd import gradcheck, gradgradcheck def verify_gradients(f, x): Verifies the gradients of function f using PyTorch\'s gradcheck and gradgradcheck. Parameters: f (callable): The function to verify. It should accept a PyTorch tensor as input and return a PyTorch tensor of the same shape. x (torch.Tensor): The input tensor to the function f. It should have requires_grad=True. Returns: bool: True if both gradcheck and gradgradcheck pass, False otherwise. x = x.double() if not torch.is_tensor(x) or not x.requires_grad: raise ValueError(\\"Input must be a tensor with requires_grad=True\\") try: if not gradcheck(f, (x,), eps=1e-6, atol=1e-4): return False if not gradgradcheck(f, (x,), eps=1e-6, atol=1e-4): return False except RuntimeError: return False return True"},{"question":"**Question: UUencode and UUdecode File Converter** You are required to write a Python function that utilizes the `uu` module to encode and then decode a given input file. Your task is to ensure that the decoded file matches the original file content perfectly. You will also handle any exceptions that may occur during the process and return appropriate messages. # Function Signature: ```python def uuencode_and_decode(in_file_path: str, encoded_file_path: str, decoded_file_path: str) -> str: pass ``` # Input: - `in_file_path`: A string representing the path to the input file that needs to be uuencoded. - `encoded_file_path`: A string representing the path where the uuencoded file will be saved. - `decoded_file_path`: A string representing the path where the decoded file will be saved. # Output: - Returns a confirmation string \\"Success\\" if the process completes without any error. - In case of an error, raises an appropriate exception with an error message. # Constraints: - Ensure that the decoded content is identical to the original input file content. - Handle exceptions raised during encoding and decoding using `uu.Error`. # Example: Assume you have a file `input.txt` with the following content: ``` Hello, World! This is a test file. ``` `in_file_path = \\"input.txt\\"` `encoded_file_path = \\"encoded_file.uue\\"` `decoded_file_path = \\"decoded.txt\\"` After encoding and then decoding, the content of `decoded.txt` should match the original `input.txt` content. # Notes: 1. You may assume the input file exists and is readable. 2. You should verify that after decoding, the content matches the original input file content. 3. Use file handling best practices to ensure no file remains open unintentionally. 4. Provide meaningful error messages in the exceptions raised. **Your implementation should make use of the `uu` module\'s `encode` and `decode` functions as described in the documentation.** # Template: ```python import uu def uuencode_and_decode(in_file_path: str, encoded_file_path: str, decoded_file_path: str) -> str: try: # Encode the input file to uuencode format with open(in_file_path, \'rb\') as in_file, open(encoded_file_path, \'wb\') as out_file: uu.encode(in_file, out_file) # Decode the uuencode file back to original format with open(encoded_file_path, \'rb\') as in_file, open(decoded_file_path, \'wb\') as out_file: uu.decode(in_file, out_file) # Verify the content matches with open(in_file_path, \'rb\') as original_file, open(decoded_file_path, \'rb\') as decoded_file: if original_file.read() == decoded_file.read(): return \\"Success\\" else: raise Exception(\\"Decoded content does not match original file content\\") except uu.Error as e: raise Exception(f\\"UU encode/decode error: {str(e)}\\") except Exception as e: raise Exception(f\\"An unexpected error occurred: {str(e)}\\") # Example usage: # result = uuencode_and_decode(\'input.txt\', \'encoded_file.uue\', \'decoded.txt\') # print(result) # Should print \\"Success\\" ```","solution":"import uu def uuencode_and_decode(in_file_path: str, encoded_file_path: str, decoded_file_path: str) -> str: try: # Encode the input file to uuencode format with open(in_file_path, \'rb\') as in_file, open(encoded_file_path, \'wb\') as out_file: uu.encode(in_file, out_file) # Decode the uuencode file back to original format with open(encoded_file_path, \'rb\') as in_file, open(decoded_file_path, \'wb\') as out_file: uu.decode(in_file, out_file) # Verify the content matches with open(in_file_path, \'rb\') as original_file, open(decoded_file_path, \'rb\') as decoded_file: if original_file.read() == decoded_file.read(): return \\"Success\\" else: raise Exception(\\"Decoded content does not match original file content\\") except uu.Error as e: raise Exception(f\\"UU encode/decode error: {str(e)}\\") except Exception as e: raise Exception(f\\"An unexpected error occurred: {str(e)}\\")"},{"question":"# Custom Callable Class Implementation with Protocols **Objective:** Implement a custom callable class in Python that supports both *tp_call* and vectorcall protocols. This will demonstrate your understanding of the advanced callable object mechanisms in Python. **Task:** 1. Create a class `CustomCallable` which is callable. 2. Implement both *tp_call* and vectorcall protocols for this class. 3. Ensure that both methods of calling the object (via *tp_call* and vectorcall) work correctly and produce the same results. 4. Provide a method within this class to return a simple arithmetic operation result (e.g., sum of provided arguments). **Requirements:** 1. **Class Name**: CustomCallable 2. **Constructor**: Accepts an initial value. 3. **Callable Behavior** (`tp_call`): - Handles both positional and keyword arguments. - Returns the sum of all provided arguments and the initial value. 4. **Vectorcall Behavior**: - Provide an optimized call mechanism (vectorcall) that also returns the sum of all provided arguments and the initial value. 5. Validate that calling the object using both protocols provides consistent results. **Input and Output:** - **Input**: The class should handle callable invocations with multiple arguments, e.g., `cc(1, 2, 3)` or `cc(a=1, b=2, c=3)` or a mix of both. - **Output**: The sum of arguments and the initial value stored in the class. **Example Usage:** ```python # Example Code cc = CustomCallable(10) # Using tp_call protocol result_tp_call = cc(1, 2, 3) # Expected output: 16 # Using vectorcall protocol result_vectorcall = cc(1, 2, 3, a=4) # Expected output: 20 assert result_tp_call == result_vectorcall, \\"Both protocols should return the same result.\\" print(\\"All tests passed.\\") ``` **Constraint:** - Ensure that the sum operation does not fail if no arguments are provided (use the initial value as the default result).","solution":"class CustomCallable: def __init__(self, initial_value): self.initial_value = initial_value def __call__(self, *args, **kwargs): # Combine args and kwargs values into a single iterable total_sum = self.initial_value + sum(args) + sum(kwargs.values()) return total_sum def vectorcall(self, *args, **kwargs): return self.__call__(*args, **kwargs)"},{"question":"**Question:** You are given a task to create a Python script that will help a developer list all the entry points of installed packages in their environment, filter them by a specified group, and display additional details for a selected entry point. # Requirements: 1. Write a function `list_entry_points(group: str) -> list` that returns a list of all entry points that belong to the specified group. Each entry in the list should be a dictionary containing the entry point name and value. 2. Write a function `get_entry_point_details(group: str, name: str) -> dict` that returns a dictionary with detailed information about a specific entry point. The dictionary should include: - Module - Attribute - Extras - Resolved function (loaded function reference) # Implementation Details: - Use `importlib.metadata.entry_points()` to retrieve all entry points. - Use the `.select(group=...)` method to filter entry points by the specified group. - For `get_entry_point_details`, use the `.load()` method of the `EntryPoint` instance to obtain the resolved function. # Function Signatures: ```python def list_entry_points(group: str) -> list: pass def get_entry_point_details(group: str, name: str) -> dict: pass ``` # Example Usage: ```python # Assuming there are entry points under \'console_scripts\' group entry_points_list = list_entry_points(\'console_scripts\') print(entry_points_list) # Expected Output (format may vary): # [ # {\'name\': \'entry_point1\', \'value\': \'module1:function1\'}, # {\'name\': \'entry_point2\', \'value\': \'module2:function2\'}, # ] entry_point_details = get_entry_point_details(\'console_scripts\', \'entry_point1\') print(entry_point_details) # Expected Output (format may vary): # { # \'module\': \'module1\', # \'attribute\': \'function1\', # \'extras\': [], # \'resolved_function\': <function module1.function1 at 0xXXXXXXXX> # } ``` # Constraints: - The function should raise a `ValueError` if the specified group or entry point does not exist. - Support only Python 3.10 or newer. # Notes: - Ensure proper error handling for cases where the specified group or entry point does not exist. - Test your functions with various installed packages to ensure their generality and robustness.","solution":"from importlib.metadata import entry_points import importlib import re def list_entry_points(group: str) -> list: Returns a list of all entry points that belong to the specified group. Each entry in the list is a dictionary containing the entry point name and value. eps = entry_points().select(group=group) if not eps: raise ValueError(f\\"No entry points found for group \'{group}\'\\") return [{\'name\': ep.name, \'value\': f\'{ep.value}\'} for ep in eps] def get_entry_point_details(group: str, name: str) -> dict: Returns a dictionary with detailed information about a specific entry point. eps = entry_points().select(group=group) if not eps: raise ValueError(f\\"No entry points found for group \'{group}\'\\") for ep in eps: if ep.name == name: module, attribute = re.split(r\':\', ep.value, maxsplit=1) eps_loaded = ep.load() return { \'module\': module, \'attribute\': attribute, \'extras\': ep.extras, \'resolved_function\': eps_loaded } raise ValueError(f\\"No entry point named \'{name}\' found in group \'{group}\'\\")"},{"question":"**Title:** Synchronizing CUDA Streams in PyTorch **Objective:** To assess the understanding of CUDA stream synchronization in PyTorch. Students will be required to write a function that correctly utilizes CUDA streams to prevent data races. **Problem Statement:** You are provided with a tensor `a` that is initialized on the default CUDA stream. You also have a mathematical operation that needs to be performed on this tensor in a new CUDA stream. Your task is to write a function `safe_tensor_operation` that takes a tensor and performs an element-wise multiplication by 5 on a new stream without causing a data race. **Function Signature:** ```python def safe_tensor_operation(tensor: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `tensor` (torch.Tensor): A tensor initialized on the default CUDA stream. **Output:** - Returns a tensor such that it contains the result of multiplying input tensor elements by 5 without causing a data race. **Constraints:** - The function must handle CUDA stream synchronization correctly to avoid data races. - Assume the tensor passed to the function is always of size (4, 2) and allocated on the CUDA device. **Performance Requirements:** - Correctly use PyTorch\'s CUDA stream to perform the operation. - Ensure that no data races occur by synchronizing the streams appropriately. **Example Usage:** ```python import torch a = torch.rand(4, 2, device=\\"cuda\\") result = safe_tensor_operation(a) print(result) # Should print the tensor `a` with each element multiplied by 5 ``` **Additional Information:** Here is an example of how a tensor operation can lead to data races in PyTorch: ```python import torch a = torch.rand(4, 2, device=\\"cuda\\") with torch.cuda.stream(torch.cuda.Stream()): torch.mul(a, 5, out=a) ``` The above code may lead to data races if not properly synchronized. Your task is to correctly synchronize this operation. **Note:** Use the CUDA Stream Sanitizer if needed to verify the absence of data races.","solution":"import torch def safe_tensor_operation(tensor: torch.Tensor) -> torch.Tensor: Perform element-wise multiplication of the given tensor by 5 in a new CUDA stream and synchronize streams to avoid data races. # Check if the tensor is on cuda if not tensor.is_cuda: raise ValueError(\\"Tensor must be on CUDA device.\\") # Create a new CUDA stream stream = torch.cuda.Stream() # Perform operation in the new stream with torch.cuda.stream(stream): result = tensor * 5 # Ensure the default stream waits for the new stream to finish torch.cuda.current_stream().wait_stream(stream) return result"},{"question":"**Complexity:** Advanced # Question You are a developer tasked with analyzing the Python installation on a system to ensure all expected installation paths and configuration variables are correctly set up for a specific platform and Python version. To achieve this, you need to use the `sysconfig` module. Task Create a Python script that performs the following: 1. Retrieves and prints the default installation scheme name for the current platform. 2. Prints out all the paths associated with the default installation scheme, labelling each path with its identifier (e.g., `stdlib`, `platstdlib`, `include`, etc.). 3. Retrieves and prints all configuration variables related to the current platform. 4. Specifically retrieves the values and prints: - The Python version in \\"MAJOR.MINOR\\" format. - The platform string that identifies the current platform. Expected Input and Output The script does not require any input arguments. **Expected Output Format:** ``` Default Scheme: <default_scheme_name> Paths: stdlib: <path_to_stdlib> platstdlib: <path_to_platstdlib> include: <path_to_include> ... Configuration Variables: PYTHON_VERSION: <python_version_value> PLATLIBDIR: <platlibdir_value> ... Python Version: <major.minor_version> Platform: <platform_string> ``` Constraints - Assume the script runs in an environment where the `sysconfig` module is available. - Handle potential missing values gracefully, returning `None` for paths and variables that are not found. # Example Output Consider a Unix-based system. An example output might look like: ``` Default Scheme: posix_prefix Paths: stdlib: /usr/local/lib/python3.10 platstdlib: /usr/local/lib/python3.10 include: /usr/local/include/python3.10 ... Configuration Variables: PYTHON_VERSION: 3.10 PLATLIBDIR: lib ... Python Version: 3.10 Platform: unix ``` Feel free to test your script in different environments to ensure it handles various setups correctly.","solution":"import sysconfig import sys def analyze_python_installation(): # 1. Retrieve and print the default installation scheme name for the current platform default_scheme = sysconfig.get_default_scheme() print(f\\"Default Scheme: {default_scheme}\\") # 2. Print out all the paths associated with the default installation scheme print(\\"Paths:\\") scheme_paths = sysconfig.get_paths() for key, path in scheme_paths.items(): print(f\\" {key}: {path}\\") # 3. Retrieve and print all configuration variables related to the current platform print(\\"Configuration Variables:\\") config_vars = sysconfig.get_config_vars() for key, value in config_vars.items(): print(f\\" {key}: {value}\\") # 4. Specifically retrieve the values and print: # - The Python version in \\"MAJOR.MINOR\\" format version_major_minor = f\\"{sys.version_info.major}.{sys.version_info.minor}\\" print(f\\"Python Version: {version_major_minor}\\") # - The platform string that identifies the current platform platform_string = sysconfig.get_platform() print(f\\"Platform: {platform_string}\\") # Example function call (uncomment this line if running the script directly): # analyze_python_installation()"},{"question":"# PyTorch Futures Handling: Simulating Asynchronous Execution **Objective**: You are tasked with simulating an asynchronous execution environment using PyTorch\'s `torch.futures` package. This question assesses your understanding of handling asynchronous tasks and utilities in PyTorch. **Problem Statement**: You are to implement a function `simulate_asynchronous_execution` that takes a list of functions and a number `n` as inputs. Each function in the list executes asynchronously and returns a `torch.futures.Future` object. Your task is to execute these functions asynchronously, collect the results once all functions have completed, and then return a list of the results. **Function Signature**: ```python import torch from typing import List, Callable def simulate_asynchronous_execution(functions: List[Callable[[], torch.futures.Future]], n: int) -> List: pass ``` **Parameters**: - `functions`: A list of functions, where each function returns a `torch.futures.Future` when called. - `n`: An integer representing the number of asynchronous function calls to execute. **Returns**: - A list of results from the `n` executed functions once all have completed. **Constraints**: - You can assume all function calls eventually complete. - You should leverage the `torch.futures` package for managing the asynchronous executions. **Example**: ```python import torch import time from torch.futures import Future from typing import List, Callable # Example function that returns a Future def example_function() -> Future: fut = Future() time.sleep(1) # Simulate a blocking operation fut.set_result(\\"Result\\") return fut # Simulate 5 asynchronous executions functions = [example_function] * 5 results = simulate_asynchronous_execution(functions, 5) print(results) # Expected: [\'Result\', \'Result\', \'Result\', \'Result\', \'Result\'] ``` **Notes**: - You must use `torch.futures.Future` objects for each asynchronous execution. - Use the `collect_all` function to gather all futures and ensure they complete before extracting results. Good luck!","solution":"import torch from torch.futures import Future from typing import List, Callable def simulate_asynchronous_execution(functions: List[Callable[[], torch.futures.Future]], n: int) -> List: # Ensure we only take the first n functions selected_functions = functions[:n] # Execute all functions asynchronously futures = [f() for f in selected_functions] # Collect all results using collect_all collected_futures = torch.futures.collect_all(futures).wait() # Extract results from futures and return them as a list results = [fut.value() for fut in collected_futures] return results"},{"question":"# Secure Login System You are tasked with creating a secure login system that verifies user credentials. You will make use of the `getpass` module to securely handle password input and compare it against a stored username-password pair. Requirements: 1. The function `secure_login()` should prompt the user for their username and password. 2. Use `getpass.getpass()` for password input to ensure it is not echoed on the screen. 3. Use `getpass.getuser()` to fetch the system\'s current username. 4. The function should verify the input username and password against a predefined username (\\"admin\\") and password (\\"P@ssw0rd!\\"). 5. If the input credentials match, print \\"Access Granted\\". 6. If the input credentials do not match, print \\"Access Denied\\". Function Signature: ```python def secure_login(): pass ``` Example: ```python # Assuming the system\'s current username is \'admin\', # and the password entered is \'P@ssw0rd!\'. secure_login() # Output: # Enter your username: admin # Enter your password: (hidden) # Access Granted ``` ```python # If incorrect credentials are provided, secure_login() # Output: # Enter your username: admin # Enter your password: (hidden) # Access Denied ``` **Constraints:** - You are not allowed to hard-code the username and password prompts directly, you must use the `getpass` module. - Implement necessary exception handling to manage cases where password input may be echoed due to `getpass.GetPassWarning`. Additional Information: - Use the following code snippet in your solution to simulate the stored username and password: ```python STORED_USERNAME = \\"admin\\" STORED_PASSWORD = \\"P@ssw0rd!\\" ``` - Ensure the code is written in Python 3.10.","solution":"import getpass def secure_login(): Secure login function that checks user credentials. STORED_USERNAME = \\"admin\\" STORED_PASSWORD = \\"P@ssw0rd!\\" try: input_username = input(\\"Enter your username: \\") input_password = getpass.getpass(\\"Enter your password: \\") if input_username == STORED_USERNAME and input_password == STORED_PASSWORD: print(\\"Access Granted\\") else: print(\\"Access Denied\\") except getpass.GetPassWarning: print(\\"Warning: Password input may be echoed.\\")"},{"question":"# XML Data Processing with ElementTree **Objective**: Write a Python function `update_xml_with_ranks` that reads an XML document from a string, updates elements based on certain conditions, and then returns the modified XML string. **Function Signature**: ```python def update_xml_with_ranks(xml_str: str) -> str: ``` **Input**: - `xml_str`: A string containing XML data formatted as shown below. **Output**: - A string representing the updated XML data. **XML Sample Input**: ```xml <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> ``` **Tasks**: 1. Parse the XML string into an `ElementTree`. 2. Increment each `rank` element by 1. 3. Add an `updated` attribute with the value \\"yes\\" to each `rank` element. 4. Remove all `country` elements where the `rank` is greater than 50. 5. Return the modified XML string. **Constraints**: - The XML data is guaranteed to be well-formed. - The input XML string will always include the `<data>` root element. **Example**: Input: ```python xml_str = \'\'\'<data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data>\'\'\' ``` Output: ```xml <data> <country name=\\"Liechtenstein\\"> <rank updated=\\"yes\\">2</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank updated=\\"yes\\">5</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> </data> ``` **Note**: - Ensure the output XML string has the modifications applied correctly. - The output should be a valid XML string representation of the modified tree. **Hints**: - Use `xml.etree.ElementTree.ElementTree` and `Element` classes for XML parsing and manipulation. - Methods like `iter` for iteration, `find` for finding elements, `set` for setting attributes, and `write` or `tostring` for generating XML strings might be useful.","solution":"import xml.etree.ElementTree as ET def update_xml_with_ranks(xml_str: str) -> str: # Parse the XML string root = ET.fromstring(xml_str) # Loop over each country element in the tree for country in root.findall(\'country\'): rank_element = country.find(\'rank\') if rank_element is not None: rank = int(rank_element.text) # Increment the rank value by 1 rank += 1 rank_element.text = str(rank) # Add the updated attribute rank_element.set(\'updated\', \'yes\') # Remove countries with rank greater than 50 if rank > 50: root.remove(country) # Return the modified XML as a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"Objective: Your task is to implement a Python program that demonstrates mastery in using the `pickle` module for object serialization and deserialization, as well as handling custom serialization for complex objects, such as class instances with state. Problem Statement: Create a custom class `Employee` that has the following attributes: - `name` (str) - `age` (int) - `position` (str) - `salary` (float) Implement the following functionalities: 1. A method to serialize an instance of the `Employee` class to a file using the `pickle` module. 2. A method to deserialize the instance from the file. 3. Custom serialization for the `Employee` instance that also includes an attribute for `employee_id` which is not originally in the definition but is added during the serialization process. 4. Provide a way to modify an attribute after creating an instance and ensure that the modified instance is correctly serialized and deserialized. Constraints and Requirements: - **Input:** Your program should take the attributes of the `Employee` instance, the filename for serializing/deserializing, and the new value for modifying an attribute as input. - **Output:** Correctly serialized and deserialized `Employee` instance with the `employee_id` added during serialization and the requested modifications applied. - Your class should use proper exception handling for file operations and object serialization/deserialization. - The methods should ensure that the employee data is securely and accurately serialized and deserialized. - Include comments and docstrings to explain your code and methods where necessary. Example Usage: ```python # Create an instance of the Employee class employee = Employee(\'John Doe\', 30, \'Software Engineer\', 80000) # Serialize the instance with custom serialization that adds an employee_id custom_serialize(employee, \'employee_data.pkl\') # Modify the employee\'s salary employee.salary = 85000 # Deserialize the instance and check the attributes loaded_employee = custom_deserialize(\'employee_data.pkl\') print(loaded_employee.name) # Output: John Doe print(loaded_employee.employee_id) # Output: A unique employee ID generated print(loaded_employee.salary) # Output: 85000 ``` You are required to implement the following methods/displays: 1. `__init__` method for the Employee class. 2. `custom_serialize` method for serialization with additional attributes. 3. `custom_deserialize` method for deserialization. Note: Make sure to handle different data types and potential exceptions that may arise during file operations.","solution":"import pickle import os class Employee: def __init__(self, name, age, position, salary): self.name = name self.age = age self.position = position self.salary = salary self.employee_id = None # Employee ID will be added during serialization def modify_attribute(self, attr_name, new_value): Modify the attribute of the Employee instance. Parameters: attr_name (str): The attribute name to be modified. new_value: The new value for the attribute. if hasattr(self, attr_name): setattr(self, attr_name, new_value) else: raise AttributeError(f\\"\'Employee\' object has no attribute \'{attr_name}\'\\") def custom_serialize(employee, filename): Serialize the Employee instance to a file with an additional employee_id. Parameters: employee (Employee): The Employee instance to serialize. filename (str): The name of the file to serialize to. with open(filename, \'wb\') as file: employee.employee_id = id(employee) # Using the id function to generate a unique employee ID pickle.dump(employee, file) def custom_deserialize(filename): Deserialize the Employee instance from a file. Parameters: filename (str): The name of the file to deserialize from. Returns: Employee: The deserialized Employee instance. if not os.path.exists(filename): raise FileNotFoundError(f\\"The file {filename} does not exist.\\") with open(filename, \'rb\') as file: employee = pickle.load(file) return employee"},{"question":"# Frozenset Based Aggregator In this exercise, you are required to design a function that aggregates elements based on their immutability properties. The function should take a list of sets and frozensets and return a dictionary summarizing the contents of all sets and frozensets without repeating elements within sets but respects the immutable nature of the `frozenset`. Function Signature ```python def aggregate_elements(sets_and_frozensets: list) -> dict: pass ``` # Input * The function receives a list of sets and frozensets. You can assume that all elements within the sets and frozensets are immutable. # Output * The function should return a dictionary with two keys: * `\'sets\'`: A set containing all unique elements from the sets in the input list. * `\'frozensets\'`: A `frozenset` containing all unique elements from the frozensets in the input list. # Constraints * The input list contains `n` elements where `1 <= n <= 10^5`. * The total number of unique elements across all sets and frozensets will not exceed `10^6`. # Example ```python assert aggregate_elements([{1, 2, 3}, frozenset([2, 3, 4]), {5, 6}]) == {\'sets\': {1, 2, 3, 5, 6}, \'frozensets\': frozenset([2, 3, 4])} assert aggregate_elements([{1, 2}, {3, 4}, frozenset([2, 5, 6]), frozenset([1, 7])]) == {\'sets\': {1, 2, 3, 4}, \'frozensets\': frozenset([2, 1, 5, 6, 7])} ``` # Task Implement the function `aggregate_elements` to produce the expected results.","solution":"def aggregate_elements(sets_and_frozensets: list) -> dict: Aggregates elements from a list of sets and frozensets into a dictionary. Args: sets_and_frozensets (list): A list containing sets and frozensets. Returns: dict: A dictionary with keys \'sets\' and \'frozensets\'. \'sets\' value is a set containing all unique elements from the sets. \'frozensets\' value is a frozenset containing all unique elements from the frozensets. sets_aggregate = set() frozensets_aggregate = set() for element in sets_and_frozensets: if isinstance(element, set): sets_aggregate.update(element) elif isinstance(element, frozenset): frozensets_aggregate.update(element) return {\'sets\': sets_aggregate, \'frozensets\': frozenset(frozensets_aggregate)}"},{"question":"**Question:** You are tasked with generating a series of visualizations using Seaborn to demonstrate your understanding of controlling figure aesthetics and scaling plot elements. Your solution should include the following steps: 1. **Data Preparation**: - Generate a dataset composed of 30 samples, each having 8 features. Add a differentiating constant to each feature to ensure variation in the dataset. ```python import numpy as np data = np.random.normal(size=(30, 8)) + np.arange(8) / 2 ``` 2. **Step-by-Step Visualizations**: - **Plot 1**: Create a boxplot using the default Seaborn theme with the generated dataset. - **Plot 2**: Change the plot style to `whitegrid` and generate a violin plot with the same dataset. - **Plot 3**: Switch the style to `dark` and generate a sin plot with 10 offset sine waves. - **Plot 4**: Use the `ticks` style and generate the same sin plot but apply despine to remove the top and right spines. - **Plot 5**: Temporarily set the style to `white` and generate a boxplot nested inside a `with` statement. After exiting the `with` block, apply `ticks` style again and generate another boxplot. 3. **Scaling and Customization**: - **Plot 6**: Use the `sns.set_context()` function to scale the elements for a `talk` setting and generate a sin plot. - **Plot 7**: Override the default settings to use a dark grid style with a light grey background (`axes.facecolor`) and generate another sin plot. 4. **Ensure**: - Your code is modular, using functions where appropriate. - Each plot is rendered with a clear title to indicate which step it belongs to. - You use appropriate Seaborn and Matplotlib functions to achieve the desired result. **Input/Output**: - You do not need to accept any input or produce any output except the plots. Ensure that the plots are displayed in a logical order with clear separation. **Constraints**: - Use only the functions and methods demonstrated in the provided documentation. - You should not hardcode any parameters apart from those directly specified in the instructions. **Example:** ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(): return np.random.normal(size=(30, 8)) + np.arange(8) / 2 def plot_default_boxplot(data): sns.set_theme() sns.boxplot(data=data).set_title(\\"Plot 1: Default Boxplot\\") plt.show() def plot_whitegrid_violin(data): sns.set_style(\\"whitegrid\\") sns.violinplot(data=data).set_title(\\"Plot 2: Whitegrid Violin Plot\\") plt.show() def plot_sine_waves(style, title): sns.set_style(style) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(title) plt.show() def plot_ticks_sine_waves_despine(): sns.set_style(\\"ticks\\") x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) sns.despine() plt.title(\\"Plot 4: Ticks Style Sine Waves with Despine\\") plt.show() def plot_temporary_style_boxplot(data): with sns.axes_style(\\"white\\"): sns.boxplot(data=data).set_title(\\"Plot 5: Temporary White Style Boxplot\\") plt.show() sns.set_style(\\"ticks\\") sns.boxplot(data=data).set_title(\\"Plot 5: Ticks Style Boxplot\\") plt.show() def plot_scaled_sines_for_talk(): sns.set_context(\\"talk\\") x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(\\"Plot 6: Talk Context Sine Waves\\") plt.show() def plot_customized_darkgrid_sines(): sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(\\"Plot 7: Customized Darkgrid Sine Waves\\") plt.show() if __name__ == \\"__main__\\": data = generate_data() plot_default_boxplot(data) plot_whitegrid_violin(data) plot_sine_waves(\\"dark\\", \\"Plot 3: Dark Style Sine Waves\\") plot_ticks_sine_waves_despine() plot_temporary_style_boxplot(data) plot_scaled_sines_for_talk() plot_customized_darkgrid_sines() ``` Ensure that your code is clean, well-commented, and follows good coding practices.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(): Generate a dataset composed of 30 samples, each having 8 features. Adding a differentiating constant to each feature to ensure variation in the dataset. return np.random.normal(size=(30, 8)) + np.arange(8) / 2 def plot_default_boxplot(data): Create a boxplot using the default Seaborn theme with the generated dataset. sns.set_theme() sns.boxplot(data=data) plt.title(\\"Plot 1: Default Boxplot\\") plt.show() def plot_whitegrid_violin(data): Change the plot style to \'whitegrid\' and generate a violin plot with the same dataset. sns.set_style(\\"whitegrid\\") sns.violinplot(data=data) plt.title(\\"Plot 2: Whitegrid Violin Plot\\") plt.show() def plot_sine_waves(style, title): Generate a sine plot with 10 offset sine waves with the specified Seaborn style. sns.set_style(style) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(title) plt.show() def plot_ticks_sine_waves_despine(): Use the \'ticks\' style and generate the same sin plot but apply despine to remove the top and right spines. sns.set_style(\\"ticks\\") x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) sns.despine() plt.title(\\"Plot 4: Ticks Style Sine Waves with Despine\\") plt.show() def plot_temporary_style_boxplot(data): Temporarily set the style to \'white\' and generate a boxplot nested inside a \'with\' statement. After exiting the \'with\' block, apply \'ticks\' style again and generate another boxplot. with sns.axes_style(\\"white\\"): sns.boxplot(data=data) plt.title(\\"Plot 5: Temporary White Style Boxplot\\") plt.show() sns.set_style(\\"ticks\\") sns.boxplot(data=data) plt.title(\\"Plot 5: Ticks Style Boxplot\\") plt.show() def plot_scaled_sines_for_talk(): Use the sns.set_context() function to scale the elements for a \'talk\' setting and generate a sin plot. sns.set_context(\\"talk\\") x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(\\"Plot 6: Talk Context Sine Waves\\") plt.show() def plot_customized_darkgrid_sines(): Override the default settings to use a dark grid style with a light grey background (\'axes.facecolor\') and generate another sin plot. sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (11 - i)) plt.title(\\"Plot 7: Customized Darkgrid Sine Waves\\") plt.show() if __name__ == \\"__main__\\": data = generate_data() plot_default_boxplot(data) plot_whitegrid_violin(data) plot_sine_waves(\\"dark\\", \\"Plot 3: Dark Style Sine Waves\\") plot_ticks_sine_waves_despine() plot_temporary_style_boxplot(data) plot_scaled_sines_for_talk() plot_customized_darkgrid_sines()"},{"question":"# Question: You are required to implement a multi-threaded task manager using the `queue` module. The task manager will handle three types of tasks: \\"compute\\", \\"io\\", and \\"priority\\". Each task will have a corresponding function that needs to be executed. For \\"compute\\" and \\"io\\" tasks, the task manager should use normal `Queue` objects to process tasks in FIFO order. For \\"priority\\" tasks, it should use a `PriorityQueue` to prioritize tasks based on their priority levels. Requirements: 1. Implement functions `process_compute_task()`, `process_io_task()`, and `process_priority_task()` that perform placeholder operations (e.g., `print()` statements). 2. Implement a `TaskManager` class that initializes one `Queue` for compute tasks, one `Queue` for I/O tasks, and one `PriorityQueue` for priority tasks. 3. Implement methods: - `add_task(task_type: str, task_data: Any, priority: int = 0)`: Adds a task to the appropriate queue. For \\"priority\\" tasks, include the priority value. - `worker()`: A worker method that processes tasks from all queues. It should run indefinitely, retrieving tasks from the queues and executing the corresponding task functions. - `start_workers(num_workers: int)`: Starts a specified number of worker threads. - `join_queues()`: Blocks until all tasks are processed. Input: - The `add_task()` method receives task type as a string, task data, and an optional priority (default is 0). - The `start_workers()` method receives the number of worker threads to be started. Output: - No specific output. The functions should demonstrate proper threading and task management by executing the tasks and printing the respective statements. Constraints: - Assume a maximum of 100 tasks for each queue. - Tasks should be processed until all are complete. - Thread safety and proper task management should be ensured. Performance: - The solution should efficiently utilize multiple threads to handle the tasks and avoid any deadlocks or race conditions. # Example: ```python from queue import Queue, PriorityQueue import threading # Function implementations def process_compute_task(task_data): print(f\\"Processing compute task: {task_data}\\") def process_io_task(task_data): print(f\\"Processing IO task: {task_data}\\") def process_priority_task(task_data): print(f\\"Processing priority task: {task_data}\\") class TaskManager: def __init__(self): self.compute_queue = Queue(maxsize=100) self.io_queue = Queue(maxsize=100) self.priority_queue = PriorityQueue(maxsize=100) def add_task(self, task_type, task_data, priority=0): if task_type == \\"compute\\": self.compute_queue.put(task_data) elif task_type == \\"io\\": self.io_queue.put(task_data) elif task_type == \\"priority\\": self.priority_queue.put((priority, task_data)) def worker(self): while True: if not self.compute_queue.empty(): task_data = self.compute_queue.get() process_compute_task(task_data) self.compute_queue.task_done() elif not self.io_queue.empty(): task_data = self.io_queue.get() process_io_task(task_data) self.io_queue.task_done() elif not self.priority_queue.empty(): priority, task_data = self.priority_queue.get() process_priority_task(task_data) self.priority_queue.task_done() def start_workers(self, num_workers): for _ in range(num_workers): threading.Thread(target=self.worker, daemon=True).start() def join_queues(self): self.compute_queue.join() self.io_queue.join() self.priority_queue.join() # Example usage if __name__ == \\"__main__\\": manager = TaskManager() manager.start_workers(5) for i in range(10): manager.add_task(\\"compute\\", f\\"Compute Task {i}\\") manager.add_task(\\"io\\", f\\"IO Task {i}\\") manager.add_task(\\"priority\\", f\\"Priority Task {i}\\", priority=i) manager.join_queues() print(\\"All tasks completed\\") ``` Ensure all tasks are added and processed without any collisions or deadlocks by the multi-threaded system.","solution":"from queue import Queue, PriorityQueue import threading def process_compute_task(task_data): print(f\\"Processing compute task: {task_data}\\") def process_io_task(task_data): print(f\\"Processing IO task: {task_data}\\") def process_priority_task(task_data): print(f\\"Processing priority task: {task_data}\\") class TaskManager: def __init__(self): self.compute_queue = Queue(maxsize=100) self.io_queue = Queue(maxsize=100) self.priority_queue = PriorityQueue(maxsize=100) def add_task(self, task_type, task_data, priority=0): if task_type == \\"compute\\": self.compute_queue.put(task_data) elif task_type == \\"io\\": self.io_queue.put(task_data) elif task_type == \\"priority\\": self.priority_queue.put((priority, task_data)) def worker(self): while True: if not self.compute_queue.empty(): task_data = self.compute_queue.get() process_compute_task(task_data) self.compute_queue.task_done() elif not self.io_queue.empty(): task_data = self.io_queue.get() process_io_task(task_data) self.io_queue.task_done() elif not self.priority_queue.empty(): priority, task_data = self.priority_queue.get() process_priority_task(task_data) self.priority_queue.task_done() def start_workers(self, num_workers): for _ in range(num_workers): threading.Thread(target=self.worker, daemon=True).start() def join_queues(self): self.compute_queue.join() self.io_queue.join() self.priority_queue.join()"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of Python\'s `string` module, including constants, formatting, and template strings. # Problem Statement You are tasked with creating a utility that processes and formats user data. You\'ll design a function `process_user_data` that takes a list of dictionaries, each representing a user\'s information, and returns a formatted string summary. # Requirements 1. **Input Format:** - A list of dictionaries, where each dictionary contains: - `name` (string): The user\'s name. - `age` (integer): The user\'s age. - `email` (string): The user\'s email. 2. **Output Format:** - A formatted string summarizing each user\'s information. - Format each user\'s summary as follows: \\"Name: {name}, Age: {age}, Email: {email}\\". - Ensure each user\'s summary is on a new line. - Use constants from the `string` module to ensure names contain only ASCII letters and at least one character. - Age should always be right-aligned to 3 digits. - Email should be validated to ensure it contains `@`. 3. **Constraints:** - Name strings should only contain ASCII letters. - Age should be a non-negative integer. - Valid email format must be ensured with `@` present. 4. **Performance Requirement:** - The function should be efficient enough to handle up to 1000 users. # Function Signature: ```python def process_user_data(users: List[Dict[str, Union[str, int]]]) -> str: pass ``` # Examples: ```python users = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"email\\": \\"bob@example.com\\"} ] print(process_user_data(users)) ``` **Expected Output:** ``` Name: Alice, Age: 30, Email: alice@example.com Name: Bob, Age: 25, Email: bob@example.com ``` **Note:** - Make sure to handle any improper inputs by raising appropriate exceptions with clear error messages. # Hints: - Use `string.ascii_letters` to validate names. - Use the format specification mini-language to format ages. - Use string methods to check the presence of `@` in emails.","solution":"from typing import List, Dict, Union import string def process_user_data(users: List[Dict[str, Union[str, int]]]) -> str: Process and format user data. Parameters: users (List[Dict[str, Union[str, int]]]): A list of dictionaries containing user information. Returns: str: Formatted string summary of all users. summaries = [] for user in users: name = user[\'name\'] age = user[\'age\'] email = user[\'email\'] # Validate user data if not all(c in string.ascii_letters for c in name) or len(name) == 0: raise ValueError(f\\"Invalid name: {name}. Names must contain only ASCII letters and not be empty.\\") if not isinstance(age, int) or age < 0: raise ValueError(f\\"Invalid age: {age}. Age must be a non-negative integer.\\") if \'@\' not in email: raise ValueError(f\\"Invalid email: {email}. Email must contain \'@\'.\\") # Format user summary formatted_summary = f\\"Name: {name}, Age: {age:3}, Email: {email}\\" summaries.append(formatted_summary) return \\"n\\".join(summaries)"},{"question":"# PyTorch Coding Assessment Objective You are required to write code that utilizes PyTorch’s `torch.testing` module. Specifically, you will need to generate tensors with certain properties and write unit tests that verify the correctness of operations performed on those tensors. Problem Statement **Part A**: Implement a function `create_and_test_tensors()` that performs the following tasks: 1. Generate two tensors, `tensor1` and `tensor2` of the same shape `(3, 3)` where: - `tensor1` should contain random values. - `tensor2` should be within a slight perturbation (within tolerance) of `tensor1`, such that the differences are minimal but non-zero. 2. Check if the tensors are approximately equal using the `torch.testing.assert_close` method. Make sure to appropriately handle any tolerance parameters to validate the closeness of the tensors. **Part B**: Implement another function `modify_and_test_tensor(tensor: torch.Tensor)` that performs the following tasks: 1. Modify the input tensor and add a constant value, say `5`, to every element. 2. Ensure the resulting tensor is still within a specified tolerance when compared to another tensor created by manually adding `5` to each element of the original tensor. 3. Write a test condition using `torch.testing.assert_allclose` to validate the above operation. Constraints 1. You must demonstrate comprehension of tensor manipulation and usage of `torch.testing` functions. 2. Consider edge cases such as tiny perturbations and validate the robustness of your comparison checks. 3. Make sure your comparisons take tolerances into account correctly. Examples Here are some hypothetical examples to guide implementation: ```python import torch import torch.testing def create_and_test_tensors(): tensor1 = torch.testing.make_tensor((3, 3), device=\'cpu\', dtype=torch.float32) tensor2 = tensor1 + torch.randn_like(tensor1) * 0.001 # small perturbation torch.testing.assert_close(tensor1, tensor2, rtol=1e-2, atol=1e-4) def modify_and_test_tensor(tensor: torch.Tensor): modified_tensor = tensor + 5 expected_tensor = torch.add(tensor, 5) torch.testing.assert_allclose(modified_tensor, expected_tensor, rtol=1e-2, atol=1e-4) ``` Submission Submit the implementation of both functions `create_and_test_tensors()` and `modify_and_test_tensor()` along with any necessary imports. Ensure your code is properly commented and adheres to the given constraints.","solution":"import torch import torch.testing def create_and_test_tensors(): Creates two tensors, tensor1 and tensor2, where tensor2 is a slight perturbation of tensor1. Then checks if they are approximately equal using torch.testing.assert_close. tensor1 = torch.testing.make_tensor((3, 3), device=\'cpu\', dtype=torch.float32) tensor2 = tensor1 + torch.randn_like(tensor1) * 0.001 # small perturbation torch.testing.assert_close(tensor1, tensor2, rtol=1e-2, atol=1e-4) def modify_and_test_tensor(tensor: torch.Tensor): Modifies the input tensor by adding 5 to every element and checks if the resulting tensor is within a specified tolerance when compared to another tensor created by manually adding 5 to each element of the original tensor. Args: tensor (torch.Tensor): The input tensor to be modified. modified_tensor = tensor + 5 expected_tensor = torch.add(tensor, 5) torch.testing.assert_allclose(modified_tensor, expected_tensor, rtol=1e-2, atol=1e-4)"},{"question":"Design a Python function that accepts an arithmetic expression as a string and evaluates it. The function should support addition, subtraction, multiplication, division, modulus, floor division, and power operations. Additionally, the function should handle parentheses to enforce the correct order of operations. # Function Signature ```python def evaluate_expression(expression: str) -> float: pass ``` # Input * `expression` (str): A string containing a valid arithmetic expression. The operators included are `+`, `-`, `*`, `/`, `%`, `//`, `**`, and `()`. All numbers in the expression are non-negative integers or floats. # Output * float: The result of evaluating the arithmetic expression. # Constraints 1. The given expression will be a valid arithmetic expression. 2. The result will be a float, even for divisions where the remainder is zero. 3. There will be no spaces in the input string. # Examples ```python assert evaluate_expression(\\"2+3*4\\") == 14.0 assert evaluate_expression(\\"(2+3)*4\\") == 20.0 assert evaluate_expression(\\"3+5/2\\") == 5.5 assert evaluate_expression(\\"10-3**2\\") == 1.0 assert evaluate_expression(\\"10//3\\") == 3.0 assert evaluate_expression(\\"10%3\\") == 1.0 ``` # Note - Implement the solution ensuring it correctly respects the order of operations (PEMDAS/BODMAS rules). # Additional Information - Assume the function won\'t receive any malformed or malicious input. - Consider edge cases, such as empty strings or very large numbers.","solution":"def evaluate_expression(expression: str) -> float: Evaluates the given arithmetic expression and returns the result as a float. # Using eval function to evaluate the expression considering security try: result = eval(expression) except Exception as e: raise ValueError(f\\"Invalid expression: {expression}\\") from e return float(result)"},{"question":"# Creating and Sending a Multipart Email with Attachments **Problem Statement**: You are required to implement a function that creates a complete MIME email message from scratch. The email should include: 1. A plaintext message body. 2. An HTML alternative for the body. 3. An image attachment. 4. An audio attachment. The function signature should be: ```python def create_mime_email(subject: str, from_addr: str, to_addrs: list, plain_text: str, html_text: str, image_path: str, audio_path: str) -> MIMEBase: ``` **Input**: - `subject` (str): The subject of the email. - `from_addr` (str): The email address of the sender. - `to_addrs` (list): A list of recipient email addresses. - `plain_text` (str): The plain text content of the email. - `html_text` (str): The HTML content of the email. - `image_path` (str): The file path to the image attachment. - `audio_path` (str): The file path to the audio attachment. **Output**: - A `MIMEBase` object representing the complete MIME email message that is ready to be sent. **Constraints**: - Ensure the email conforms to the MIME standards. - Properly encode the image and audio attachments. - Include the necessary headers such as `MIME-Version`, `Content-Type`, etc. - Make use of the appropriate classes from the `email.mime` package to build the email. **Task**: 1. Create a multipart email with a text/plain and text/html body. 2. Attach an image and audio file to the email. 3. Set the appropriate headers for each part of the email. 4. Combine all parts to create a complete MIME email message. **Example**: ```python subject = \\"Testing Multipart Email\\" from_addr = \\"sender@example.com\\" to_addrs = [\\"recipient@example.com\\"] plain_text = \\"This is the plain text version of the email.\\" html_text = \\"<html><body><h1>This is the HTML version of the email.</h1></body></html>\\" image_path = \\"/path/to/image.jpg\\" audio_path = \\"/path/to/audio.mp3\\" mime_email = create_mime_email(subject, from_addr, to_addrs, plain_text, html_text, image_path, audio_path) // The returned mime_email object should be ready to be sent using an SMTP server. ``` **Notes**: - You may assume that the given file paths for the image and audio files are valid and the files exist. - You can use the `open` function to read the image and audio files in binary mode. - The solution should utilize the `email.mime.multipart.MIMEMultipart`, `email.mime.text.MIMEText`, `email.mime.image.MIMEImage`, and `email.mime.audio.MIMEAudio` classes as needed.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.base import MIMEBase from email import encoders import mimetypes def create_mime_email(subject: str, from_addr: str, to_addrs: list, plain_text: str, html_text: str, image_path: str, audio_path: str) -> MIMEBase: # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) # Create the alternative part containing both plain text and HTML alternative_part = MIMEMultipart(\'alternative\') part1 = MIMEText(plain_text, \'plain\') part2 = MIMEText(html_text, \'html\') alternative_part.attach(part1) alternative_part.attach(part2) # Attach the alternative part to the root message msg.attach(alternative_part) # Utility function to attach files def attach_file(email_msg, file_path, mime_type): with open(file_path, \'rb\') as file: if mime_type.startswith(\'image/\'): mime_part = MIMEImage(file.read(), _subtype=mime_type.split(\'/\')[1]) elif mime_type.startswith(\'audio/\'): mime_part = MIMEAudio(file.read(), _subtype=mime_type.split(\'/\')[1]) else: mime_part = MIMEBase(mime_type.split(\'/\')[0], mime_type.split(\'/\')[1]) mime_part.set_payload(file.read()) encoders.encode_base64(mime_part) mime_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{file_path.split(\\"/\\")[-1]}\\"\') email_msg.attach(mime_part) # Get the mime types for the files image_mime_type, _ = mimetypes.guess_type(image_path) audio_mime_type, _ = mimetypes.guess_type(audio_path) # Attach the image attach_file(msg, image_path, image_mime_type or \'image/jpeg\') # Attach the audio attach_file(msg, audio_path, audio_mime_type or \'audio/mpeg\') return msg"},{"question":"Attribute Manipulation Using Python C-API Objective Implement a Python class that simulates the behavior of manipulating object attributes as described in the provided C-API documentation. Your class should expose methods for getting, setting, and deleting attributes dynamically. Problem Statement You are required to implement a class `DynamicAttributes` that manages its attributes using special methods. The class should allow: 1. Setting an attribute. 2. Getting an attribute. 3. Deleting an attribute. 4. Checking if an attribute exists. 5. Getting all attribute names in a list. You should also handle common exceptions like `AttributeError` when accessing or deleting non-existent attributes. Class Definition ```python class DynamicAttributes: def __init__(self): Initialize an empty dictionary to hold attributes. # Your implementation here def set_attr(self, attr_name: str, value): Set the attribute \'attr_name\' to \'value\'. Parameters: attr_name (str): Name of the attribute. value: Value to set for the attribute. # Your implementation here def get_attr(self, attr_name: str): Get the attribute \'attr_name\'. Parameters: attr_name (str): Name of the attribute. Returns: Value of the attribute if it exists, raises AttributeError otherwise. # Your implementation here def del_attr(self, attr_name: str): Delete the attribute \'attr_name\'. Parameters: attr_name (str): Name of the attribute to delete. Raises: AttributeError if the attribute does not exist. # Your implementation here def has_attr(self, attr_name: str) -> bool: Check if the attribute \'attr_name\' exists. Parameters: attr_name (str): Name of the attribute to check. Returns: True if the attribute exists, False otherwise. # Your implementation here def list_attrs(self) -> list: List all attributes\' names in this object. Returns: List of attribute names. # Your implementation here ``` Constraints - Attribute names will always be strings. - Attribute values can be of any type. - Performance in terms of attribute access and manipulation should be reasonable, aiming for O(1) complexity for individual operations (setting, getting, and deleting attributes). Example ```python # Initialize an object obj = DynamicAttributes() # Set attributes obj.set_attr(\'name\', \'John\') obj.set_attr(\'age\', 30) # Get attributes print(obj.get_attr(\'name\')) # Output: John print(obj.get_attr(\'age\')) # Output: 30 # Check attributes print(obj.has_attr(\'name\')) # Output: True # List attributes print(obj.list_attrs()) # Output: [\'name\', \'age\'] # Delete an attribute obj.del_attr(\'name\') # Check attributes again print(obj.has_attr(\'name\')) # Output: False print(obj.list_attrs()) # Output: [\'age\'] ``` Note Ensure your implementation raises an appropriate `AttributeError` if `get_attr` or `del_attr` is called with a non-existent attribute name.","solution":"class DynamicAttributes: def __init__(self): Initialize an empty dictionary to hold attributes. self._attributes = {} def set_attr(self, attr_name: str, value): Set the attribute \'attr_name\' to \'value\'. Parameters: attr_name (str): Name of the attribute. value: Value to set for the attribute. self._attributes[attr_name] = value def get_attr(self, attr_name: str): Get the attribute \'attr_name\'. Parameters: attr_name (str): Name of the attribute. Returns: Value of the attribute if it exists, raises AttributeError otherwise. try: return self._attributes[attr_name] except KeyError: raise AttributeError(f\\"Attribute \'{attr_name}\' not found\\") def del_attr(self, attr_name: str): Delete the attribute \'attr_name\'. Parameters: attr_name (str): Name of the attribute to delete. Raises: AttributeError if the attribute does not exist. try: del self._attributes[attr_name] except KeyError: raise AttributeError(f\\"Attribute \'{attr_name}\' not found\\") def has_attr(self, attr_name: str) -> bool: Check if the attribute \'attr_name\' exists. Parameters: attr_name (str): Name of the attribute to check. Returns: True if the attribute exists, False otherwise. return attr_name in self._attributes def list_attrs(self) -> list: List all attributes\' names in this object. Returns: List of attribute names. return list(self._attributes.keys())"},{"question":"**Title: Implementing a Chunked Data Reader** **Objective:** Write a function `extract_chunk_data` that reads and extracts all chunk IDs and their respective data from a file that uses the EA IFF 85 chunk format. The extracted data should be in a structured format for easy access. **Function Signature:** ```python def extract_chunk_data(file_path: str) -> dict: pass ``` **Input:** - `file_path` (str): A string representing the path to the file to be read. **Output:** - `dict`: A dictionary where each key is a chunk ID (as a string) and each value is a bytes object containing the corresponding data of the chunk. **Example:** If your file (`example.iff`) contains the following chunks: ``` Offset Length Contents 0 4 \'ABCD\' (Chunk ID) 4 4 0x00000004 (Size of chunk in bytes) 8 4 b\'1234\' (Data) 12 4 \'EFGH\' (Next Chunk ID) 16 4 0x00000004 (Size of next chunk in bytes) 20 4 b\'5678\' (Data) ``` Calling `extract_chunk_data(\'example.iff\')` should return: ```python {\'ABCD\': b\'1234\', \'EFGH\': b\'5678\'} ``` **Constraints:** 1. Assume chunks are aligned to 2-byte boundaries. 2. Handle big-endian byte order by default; you do not need to handle little-endian order. 3. The function should handle any number of chunks present in the file. **Performance Requirements:** Your solution should read the file efficiently, handle large files gracefully, and make use of the `Chunk` class methods appropriately. **Hints:** - Use the `chunk.Chunk` class to read and navigate the file. - Read the size and data of each chunk correctly and handle the end-of-file (EOF) condition properly. **Documentation Reference:** The `chunk` module documentation provides all necessary details for using the `Chunk` class. Make sure to import the `chunk` module and use the provided methods to read and process the chunked data. Good luck, and ensure your code is clean, well-commented, and handles errors gracefully.","solution":"import chunk def extract_chunk_data(file_path: str) -> dict: chunk_data = {} with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=True, align=True) chunk_id = ch.getname().decode(\'ascii\') chunk_data[chunk_id] = ch.read() except EOFError: break return chunk_data"},{"question":"**Objective**: Implement a system using the `multiprocessing.shared_memory` module that allows multiple processes to concurrently read and update shared memory. The shared memory should store an array of integers representing a counter for different categories. You will create functions to initialize the shared memory, update the counters, and read the current counters. # Instructions 1. **Initialize Shared Memory**: - Write a function `initialize_shared_memory(num_counters)` that creates a shared memory block to store an array of integers with a length equal to `num_counters`. - The function should return the name of the created shared memory block and the initial `ShareableList`. 2. **Update Counter**: - Write a function `update_counter(shared_memory_name, counter_index, increment_value)` that attaches to the shared memory block using the provided name, and updates the counter at `counter_index` by adding `increment_value`. - Ensure the function handles concurrent updates safely. 3. **Read Counters**: - Write a function `read_counters(shared_memory_name)` that attaches to the shared memory block using the provided name and returns a list of current counters. # Constraints - You can assume `update_counter` will always receive a valid `counter_index` within the bounds of the array. - Ensure resources are properly cleaned up by closing and unlinking shared memory using `close()` and `unlink()` respectively when it is no longer needed. # Example ```python from multiprocessing import shared_memory, Process import numpy as np # Implement the three required functions here def worker(shared_memory_name, updates): for index, increment in updates: update_counter(shared_memory_name, index, increment) if __name__ == \'__main__\': # Initialize shared memory shm_name, counters = initialize_shared_memory(5) # Update counters in parallel updates1 = [(0, 5), (1, 3), (2, -2)] updates2 = [(3, 7), (4, 1), (1, -1)] p1 = Process(target=worker, args=(shm_name, updates1)) p2 = Process(target=worker, args=(shm_name, updates2)) p1.start() p2.start() p1.join() p2.join() # Read and print the final counters final_counters = read_counters(shm_name) print(final_counters) # Output will vary depending on concurrent updates # Clean up counters.shm.close() counters.shm.unlink() ``` # Expected Functions - `initialize_shared_memory(num_counters: int) -> Tuple[str, ShareableList]` - `update_counter(shared_memory_name: str, counter_index: int, increment_value: int) -> None` - `read_counters(shared_memory_name: str) -> List[int]` # Notes - Ensure your code properly handles shared memory access and avoids race conditions. - Add appropriate error handling and clean-up code to ensure the shared memory is properly released after use.","solution":"from multiprocessing import shared_memory, Lock from multiprocessing.shared_memory import ShareableList # Global lock to handle concurrent updates safely lock = Lock() def initialize_shared_memory(num_counters): Creates a shared memory block to store an array of integers. Args: num_counters (int): The number of counters to be stored. Returns: Tuple[str, ShareableList]: (shared memory name, initialized ShareableList) counters = ShareableList([0]*num_counters) return counters.shm.name, counters def update_counter(shared_memory_name, counter_index, increment_value): Updates the counter at the specified index by adding increment_value. Args: shared_memory_name (str): The name of the shared memory block. counter_index (int): The index of the counter to be updated. increment_value (int): The value to add to the counter. # Attach to the existing shared memory block with lock: counters = ShareableList(name=shared_memory_name) counters[counter_index] += increment_value counters.shm.close() def read_counters(shared_memory_name): Reads the current counters from the shared memory block. Args: shared_memory_name (str): The name of the shared memory block. Returns: List[int]: The current counters. counters = ShareableList(name=shared_memory_name) result = list(counters) counters.shm.close() return result"},{"question":"# Advanced Coding Assessment Question Filesystem Report Generator You are tasked with creating a report generator that analyzes a filesystem directory tree. Your function will generate a summary report containing specific information about the files in the directory tree. Your task is to implement a function `generate_report(directory: str) -> dict` where: 1. **Input:** - `directory` (str): The path to the directory to be analyzed. 2. **Output:** - A dictionary containing: - `total_files`: Total number of files. - `file_types`: A dictionary where the keys are file extensions and the values are counts of how many files have that extension. - `largest_file`: The name of the largest file in the directory tree. - `total_size`: The total size in bytes of all files combined. - `top_level_dirs`: A list of top-level directories immediately under the specified directory. 3. **Constraints:** - Do not use any modules other than `pathlib` (especially no `os`, `shutil`, etc.). - The function should handle symbolic links properly and should not follow them. - The function should handle directories and files with special/hidden names (starting with \\".\\"). 4. **Performance Requirements:** - The solution should be efficient and handle large directories with a significant number of files and subdirectories. Example Usage: ```python from pathlib import Path def generate_report(directory: str) -> dict: # Your implementation here # Example directory tree: # /example_dir/ # ├── sub_dir1 # │ └── file1.txt (size: 1kB) # ├── sub_dir2 # │ └── file2.py (size: 2kB) # ├── file3.jpg (size: 3kB) # └── file4.doc (size: 4kB) report = generate_report(\'/example_dir\') print(report) ``` Expected Output for the example: ```python { \'total_files\': 4, \'file_types\': {\'.txt\': 1, \'.py\': 1, \'.jpg\': 1, \'.doc\': 1}, \'largest_file\': \'file4.doc\', \'total_size\': 10000, \'top_level_dirs\': [\'sub_dir1\', \'sub_dir2\'] } ``` The challenge in this task lies in the effective use of the `pathlib` functionalities to traverse the directory, identify file types, compute sizes, and handle different edge cases as specified.","solution":"from pathlib import Path from collections import defaultdict def generate_report(directory: str) -> dict: path = Path(directory) if not path.is_dir(): raise ValueError(f\\"The provided path \'{directory}\' is not a directory.\\") total_files = 0 file_types = defaultdict(int) largest_file = None largest_size = 0 total_size = 0 top_level_dirs = [p.name for p in path.iterdir() if p.is_dir()] for p in path.rglob(\'*\'): if p.is_file() and not p.is_symlink(): total_files += 1 ext = p.suffix file_types[ext] += 1 file_size = p.stat().st_size total_size += file_size if file_size > largest_size: largest_size = file_size largest_file = p.name return { \'total_files\': total_files, \'file_types\': dict(file_types), \'largest_file\': largest_file, \'total_size\': total_size, \'top_level_dirs\': top_level_dirs, }"},{"question":"Objective The goal of this exercise is to test your understanding of the `torch.fft` module and your ability to apply Fourier transforms to image processing in PyTorch. Problem Statement You are provided with a 2D grayscale image represented as a PyTorch tensor. Your task is to perform the following steps: 1. Compute the 2D Fast Fourier Transform (FFT) of the image. 2. Apply a mask to filter out high-frequency components. 3. Compute the Inverse FFT to reconstruct the image from the filtered frequency-domain data. Function Signature ```python import torch def filter_high_frequencies(image: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: Parameters: - image (torch.Tensor): A 2D tensor representing the grayscale image. - mask (torch.Tensor): A 2D tensor the same size as `image` with ones where frequencies should be kept, and zeros where they should be filtered out. Returns: - torch.Tensor: The reconstructed image after applying the mask in the frequency domain. # Your implementation here ``` Constraints - The input image tensor will have a shape of `(H, W)` where `H` is the height and `W` is the width of the image. - The input mask tensor will have the same shape `(H, W)` and contains only binary values (0s and 1s). - You cannot use any other libraries apart from `torch` and its submodules. - Ensure that the image and mask tensor are on the same device (CPU or GPU). Example ```python import torch # Example Input image = torch.rand(256, 256) # A random 256x256 image mask = torch.zeros_like(image) mask[100:156, 100:156] = 1 # Allow low frequencies in a central square region # Function Call reconstructed_image = filter_high_frequencies(image, mask) # The output should be a 256x256 tensor with reconstructed image. print(reconstructed_image.shape) # Should print: torch.Size([256, 256]) ``` # Notes - Ensure that the reconstructed image is in the same range and type as the input image. - Consider edge cases where the mask might completely filter out the image or have no effect. Good luck!","solution":"import torch def filter_high_frequencies(image: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: Parameters: - image (torch.Tensor): A 2D tensor representing the grayscale image. - mask (torch.Tensor): A 2D tensor the same size as `image` with ones where frequencies should be kept, and zeros where they should be filtered out. Returns: - torch.Tensor: The reconstructed image after applying the mask in the frequency domain. # Ensure the image is in the frequency domain image_fft = torch.fft.fft2(image) # Apply the mask in the frequency domain filtered_fft = image_fft * mask # Transform back to the spatial domain reconstructed_image = torch.fft.ifft2(filtered_fft) # Since the result might have small imaginary parts due to computational errors, take the real part reconstructed_image = torch.real(reconstructed_image) return reconstructed_image"},{"question":"**Question: Implement a Function to Consolidate and Export Emails from Different Mailbox Formats** # Problem Statement: You are given emails stored in different mailbox formats: `Maildir`, `mbox`, `MH`, `Babyl`, and `MMDF`. Implement a Python function `export_emails(source_paths, export_path, export_format)` that aggregates all emails from the given source mailboxes and exports them to a single target mailbox in the specified format. # Function Signature: ```python def export_emails(source_paths: list[str], export_path: str, export_format: str) -> None: pass ``` # Parameters: - `source_paths` (list of str): A list of paths to the source mailboxes. Each path corresponds to a mailbox in one of the supported formats. - `export_path` (str): The file path or directory where the consolidated emails will be saved. - `export_format` (str): The format in which the emails will be exported. Must be one of `Maildir`, `mbox`, `MH`, `Babyl`, `MMDF`. # Requirements: 1. **Read**: Open each source mailbox found in `source_paths` and read all messages. 2. **Export**: Write all gathered emails to a new mailbox located at `export_path`, in the specified `export_format`. 3. **Conversion**: Ensure that the format-specific information is converted appropriately when transferring emails from one format to another. 4. **Error Handling**: Appropriately handle scenarios where an email might be malformed or if any errors occur during reading from or writing to a mailbox. # Constraints: - The source mailboxes may contain a large number of emails; the function should be efficient in both time and space. - If an invalid `export_format` is provided, raise a `ValueError` with the message \\"Unsupported export format provided.\\" # Example: ```python source_paths = [\'/path/to/maildir\', \'/path/to/mbox\', \'/path/to/mh\'] export_path = \'/path/to/exported_mailbox\' export_format = \'mbox\' export_emails(source_paths, export_path, export_format) ``` In this example, the function will read all emails from the given Maildir, mbox, and MH mailboxes and then export all the gathered emails into a single mbox file at the specified path. **Note**: You can assume that appropriate data will be provided, and only valid paths to existing mailboxes will be included in `source_paths`. # Evaluation Criteria: - Correctness: The function correctly consolidates and exports emails. - Format handling: Properly converts between different mailbox formats. - Efficiency: Efficient reading and writing, considering large email datasets. - Robustness: Proper error handling and validation of input parameters.","solution":"import mailbox from email import message_from_bytes def export_emails(source_paths, export_path, export_format): Aggregates emails from different source mailboxes and exports them to a single target mailbox. Parameters: source_paths (list of str): Paths to the source mailboxes. export_path (str): The file path or directory where the consolidated emails will be saved. export_format (str): The format in which the emails will be exported. Must be one of \'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\'. if export_format not in [\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\']: raise ValueError(\\"Unsupported export format provided.\\") # Create the target mailbox target_mailbox = mailbox.Maildir(export_path) if export_format == \'Maildir\' else mailbox.mbox(export_path) if export_format == \'mbox\' else mailbox.MHMaildir(export_path) if export_format == \'MH\' else mailbox.BabylMailbox(export_path) if export_format == \'Babyl\' else mailbox.MMDFMailbox(export_path) for source_path in source_paths: # Detect the format of the source mailbox if \'maildir\' in source_path.lower(): source_mailbox = mailbox.Maildir(source_path) elif \'mbox\' in source_path.lower(): source_mailbox = mailbox.mbox(source_path) elif \'mh\' in source_path.lower(): source_mailbox = mailbox.MHMaildir(source_path) elif \'babyl\' in source_path.lower(): source_mailbox = mailbox.BabylMailbox(source_path) elif \'mmdf\' in source_path.lower(): source_mailbox = mailbox.MMDFMailbox(source_path) else: continue # Add the messages from the source mailbox to the target mailbox for msg in source_mailbox: target_mailbox.add(msg) target_mailbox.close()"},{"question":"**Question: Creating and Visualizing Custom Color Palettes with Seaborn** **Objective:** Write a Python function using seaborn to create a custom color palette and visualize it using a bar plot. **Function Signature:** ```python def visualize_custom_palette(num_colors: int, lightness: float, saturation: float) -> None: pass ``` **Input:** - `num_colors` (int): The number of colors in the palette. (1 ≤ num_colors ≤ 20) - `lightness` (float): The lightness level of the colors in the palette. (0 ≤ lightness ≤ 1) - `saturation` (float): The saturation level of the colors in the palette. (0 ≤ saturation ≤ 1) **Output:** - The function should display a bar plot where each bar\'s color corresponds to one color from the generated palette. **Constraints:** - Use the `sns.husl_palette` function to create the palette. - Ensure that the bar plot displays clearly with distinct colors for each bar. **Requirements:** 1. Implement the `visualize_custom_palette` function. 2. Inside the function, generate a palette using `sns.husl_palette` with the specified number of colors, lightness, and saturation. 3. Create a bar plot using seaborn or matplotlib, where each bar represents one color from the generated palette. 4. Display the plot. # Example: ```python # Example usage visualize_custom_palette(8, 0.5, 0.7) ``` When you run `visualize_custom_palette(8, 0.5, 0.7)`, it should generate a bar plot with 8 bars, each having the colors from the defined palette with lightness 0.5 and saturation 0.7. This question assesses the student\'s understanding of basic and advanced color palette functionalities in seaborn, as well as their ability to create visualizations by integrating seaborn with other plotting libraries.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_custom_palette(num_colors: int, lightness: float, saturation: float) -> None: Generates a custom color palette and visualizes it using a bar plot. Parameters: num_colors (int): The number of colors in the palette (1 ≤ num_colors ≤ 20). lightness (float): The lightness level of the colors in the palette (0 ≤ lightness ≤ 1). saturation (float): The saturation level of the colors in the palette (0 ≤ saturation ≤ 1). # Generate the custom color palette palette = sns.husl_palette(num_colors, l=lightness*100, s=saturation*100) # Create a bar plot to visualize the palette plt.figure(figsize=(10, 2)) for i, color in enumerate(palette): plt.bar(i, 1, color=color, edgecolor=\'none\') plt.xticks(range(num_colors), range(1, num_colors+1)) plt.yticks([]) plt.title(f\\"Custom Color Palette with {num_colors} Colors\\") plt.show()"},{"question":"**Question: Implement a Python function that connects to a POP3 server, authenticates a user, retrieves emails, and returns the content of all unread emails.** # Requirements: 1. **Function Signature:** ```python def fetch_unread_emails(host: str, username: str, password: str, use_ssl: bool = False) -> list: ``` 2. **Parameters:** - `host` (str): The hostname of the POP3 server. - `username` (str): The user\'s email username. - `password` (str): The user\'s email password. - `use_ssl` (bool): Optional; Whether to use SSL for the connection. Defaults to `False`. 3. **Return:** - A list of strings, where each string contains the full content of an unread email message. 4. **Constraints:** - Handle connection and authentication errors gracefully. Return an empty list if any error occurs. - Only retrieve emails that have not been deleted or previously flagged for deletion. 5. **Performance:** - The function should be efficient in terms of time and space complexity. Avoid unnecessary operations or connections. # Example: ```python emails = fetch_unread_emails(\'pop.mail.server.com\', \'user123\', \'passwordXYZ\', True) for email in emails: print(email) ``` # Guidelines: 1. Use the `poplib.POP3` class for non-SSL connections, or `poplib.POP3_SSL` for SSL connections. 2. Implement proper error handling using `try...except` blocks. 3. Ensure the mailbox is correctly unlocked and the connection is properly closed using the `quit()` method. 4. Consider edge cases such as an empty mailbox or all emails being marked as read. # Additional Notes: - You may simulate a POP3 server for testing purposes or use an actual one that you have access to for testing. - Follow best practices for handling passwords and sensitive information.","solution":"import poplib from email.parser import Parser def fetch_unread_emails(host: str, username: str, password: str, use_ssl: bool = False) -> list: Connects to a POP3 server, authenticates the user, retrieves and returns the content of all unread emails. :param host: The hostname of the POP3 server. :param username: The user\'s email username. :param password: The user\'s email password. :param use_ssl: Optional; Whether to use SSL for the connection. Defaults to False. :return: A list of strings, where each string contains the full content of an unread email message. try: # Connect to the server if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) # Authenticate server.user(username) server.pass_(password) # Get the number of messages in the mailbox messages_info = server.list()[1] emails = [] for message_info in messages_info: # Get the email index msg_num = message_info.split()[0] # Retrieve the email by its index raw_email = b\\"n\\".join(server.retr(msg_num)[1]) email = Parser().parsestr(raw_email.decode(\'utf-8\')) emails.append(str(email)) # Disconnect from the server server.quit() return emails except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Advanced Python Coding Assessment: Object Serialization and Persistence Objective: The task is to exercise your understanding of Python\'s `pickle` module for object serialization and demonstrate your knowledge by implementing a class that includes methods for serializing and deserializing its instances. Problem Statement: You need to create a Python class that represents a simple database of user profiles. Each profile includes attributes like `username`, `email`, and `age`. Implement methods to add profiles to the database, serialize the entire database to a file using `pickle`, and deserialize the database from a file back into a Python object. # Specifications: - **Class Name**: `UserProfileDatabase` - **Attributes**: - `profiles`: A dictionary where the keys are usernames and the values are dictionaries with keys `email` and `age`. # Methods to Implement: 1. **`add_profile(self, username: str, email: str, age: int) -> None`** - This method adds a new user profile to the database. 2. **`serialize(self, file_path: str) -> None`** - This method serializes the database to a file using the `pickle` module. 3. **`deserialize(file_path: str) -> \'UserProfileDatabase\'`** - This is a static method that deserializes the database from a file and returns an instance of `UserProfileDatabase`. # Constraints: - Usernames must be unique. - Use the `pickle` module for serialization and deserialization. - Handle exceptions that might occur during file operations or serialization. - Make sure your Python code is well-structured and commented. # Example Usage: ```python # Create an instance of the UserProfileDatabase db = UserProfileDatabase() # Add user profiles db.add_profile(\\"john_doe\\", \\"john@example.com\\", 30) db.add_profile(\\"jane_doe\\", \\"jane@example.com\\", 25) # Serialize the database to a file db.serialize(\\"profiles.pkl\\") # Deserialize the database from a file new_db = UserProfileDatabase.deserialize(\\"profiles.pkl\\") # Check the profiles print(new_db.profiles) ``` The above example should output: ``` {\'john_doe\': {\'email\': \'john@example.com\', \'age\': 30}, \'jane_doe\': {\'email\': \'jane@example.com\', \'age\': 25}} ``` Your Task: Implement the `UserProfileDatabase` class and its methods with the above specifications.","solution":"import pickle class UserProfileDatabase: def __init__(self): self.profiles = {} def add_profile(self, username: str, email: str, age: int) -> None: if username in self.profiles: raise ValueError(\\"Username already exists.\\") self.profiles[username] = {\'email\': email, \'age\': age} def serialize(self, file_path: str) -> None: try: with open(file_path, \'wb\') as file: pickle.dump(self.profiles, file) except (IOError, pickle.PickleError) as e: raise RuntimeError(f\\"Error during serialization: {e}\\") @staticmethod def deserialize(file_path: str) -> \'UserProfileDatabase\': try: with open(file_path, \'rb\') as file: profiles = pickle.load(file) db = UserProfileDatabase() db.profiles = profiles return db except (IOError, pickle.UnpicklingError) as e: raise RuntimeError(f\\"Error during deserialization: {e}\\")"},{"question":"# MLPClassifier Implementation and Evaluation You are asked to demonstrate your understanding of scikit-learn\'s **MLPClassifier** by performing the following tasks: 1. **Data Preparation**: - Create a synthetic dataset using `sklearn.datasets.make_classification`. The dataset should have 1000 samples, 20 features, 2 informative features, 2 redundant features, and 2 classes. - Split the dataset into training and testing sets using `train_test_split` with a test size of 30%. 2. **Data Scaling**: - Scale the features using `StandardScaler`. 3. **Model Training**: - Initialize an `MLPClassifier` with the following parameters: - `hidden_layer_sizes=(50,30)`: Two hidden layers with 50 and 30 neurons respectively. - `solver=\'adam\'`: Use the Adam optimizer. - `alpha=0.001`: L2 regularization parameter. - `max_iter=500`: Maximum number of iterations. - Train the model on the training data. 4. **Model Evaluation**: - Predict the labels of the test set. - Calculate and print the following evaluation metrics: - Accuracy - Precision - Recall - F1-Score 5. **Analysis of Model Parameters**: - Print the shapes of the weight matrices and bias vectors. # Expected Function and Output Implement a function `mlp_classifier_evaluation` that performs the above tasks. The function should not take any parameters and output the evaluation metrics and the shapes of the weight matrices and bias vectors. ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def mlp_classifier_evaluation(): # 1. Create synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_classes=2, random_state=42) # 2. Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 3. Scale features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 4. Train MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(50, 30), solver=\'adam\', alpha=0.001, max_iter=500, random_state=42) clf.fit(X_train, y_train) # 5. Evaluate model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1-Score: {f1}\\") # 6. Print parameters shapes print(f\\"Weight matrices shapes: {[coef.shape for coef in clf.coefs_]}\\") print(f\\"Bias vectors shapes: {[intercept.shape for intercept in clf.intercepts_]}\\") ``` **Please implement the `mlp_classifier_evaluation` function based on the above specifications. Ensure the model\'s performance metrics and parameters\' shapes are printed correctly.**","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def mlp_classifier_evaluation(): # 1. Create synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_classes=2, random_state=42) # 2. Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 3. Scale features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 4. Train MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(50, 30), solver=\'adam\', alpha=0.001, max_iter=500, random_state=42) clf.fit(X_train, y_train) # 5. Evaluate model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1-Score: {f1}\\") # 6. Print parameters shapes print(f\\"Weight matrices shapes: {[coef.shape for coef in clf.coefs_]}\\") print(f\\"Bias vectors shapes: {[intercept.shape for intercept in clf.intercepts_]}\\")"},{"question":"**Title: Securely Manage Temporary Files and Directories** **Question:** You are tasked with managing temporary files and directories for a data processing application. This application must: 1. Create a temporary directory for staging files. 2. Download multiple files from given URLs into this directory. 3. Process these files (e.g., read content length). 4. Clean up all temporary files and directories once the processing is completed. Write a Python function `process_files(urls: List[str]) -> List[int]` that accomplishes this. Each URL in the `urls` list points to a file that needs to be downloaded. The function should return a list of integers representing the content length of each downloaded file. **Requirements:** 1. Use the `tempfile` module to manage temporary files and directories. 2. Ensure temporary files and directories are deleted after processing. 3. Use context managers to handle the lifecycle of temporary files and directories. 4. Handle file downloads using the `requests` module. **Hints:** - Use `tempfile.TemporaryDirectory` to create a temporary staging directory. - Use `requests.get(url)` to download content from a URL. - Use `tempfile.NamedTemporaryFile` for temporary storage of downloaded files within the staging directory. **Constraints:** - The function must handle a list of valid HTTP(S) URLs. - Ensure efficient cleanup of temporary files and directories. - Assume the given URLs are valid and accessible. **Example:** ```python from typing import List import tempfile import requests def process_files(urls: List[str]) -> List[int]: content_lengths = [] with tempfile.TemporaryDirectory() as tmpdirname: for url in urls: response = requests.get(url) response.raise_for_status() temp_file_path = None with tempfile.NamedTemporaryFile(delete=False, dir=tmpdirname) as tmpfile: tmpfile.write(response.content) temp_file_path = tmpfile.name with open(temp_file_path, \'rb\') as f: content = f.read() content_lengths.append(len(content)) return content_lengths # Example usage urls = [\'http://example.com/file1.txt\', \'http://example.com/file2.txt\'] print(process_files(urls)) # Outputs: [content length of file1, content length of file2] ``` **Note:** - You can assume that the URLs provided are always reachable, and your function does not need to handle network errors except for ensuring `response.raise_for_status()` is called after each request. - Make sure to install the `requests` module before running the function.","solution":"from typing import List import tempfile import requests def process_files(urls: List[str]) -> List[int]: content_lengths = [] with tempfile.TemporaryDirectory() as tmpdirname: for url in urls: response = requests.get(url) response.raise_for_status() temp_file_path = None with tempfile.NamedTemporaryFile(delete=False, dir=tmpdirname) as tmpfile: tmpfile.write(response.content) temp_file_path = tmpfile.name with open(temp_file_path, \'rb\') as f: content = f.read() content_lengths.append(len(content)) return content_lengths"},{"question":"**Objective:** Demonstrate your understanding of the `sklearn.datasets` module by performing the following tasks. **Task:** 1. **Load a Real-World Dataset:** - Load the \\"Iris\\" dataset using the appropriate loader from the `sklearn.datasets` module. - Print the shape of the data and target arrays. - Print the names of features and targets. 2. **Fetch a Larger Dataset:** - Fetch the \\"20 newsgroups\\" dataset using the appropriate fetcher. - Print the number of samples in the dataset. - Print the first 5 target names. 3. **Generate a Synthetic Dataset:** - Generate a synthetic dataset using the `make_classification` function. - The synthetic dataset should have 1000 samples, 20 features, 5 informative features, and 3 classes. - Print the dimensions of the generated data and target arrays. # Requirements: - You should use the `sklearn.datasets` module for all dataset-related tasks. - Ensure that the code is clean, well-commented, and handles exceptions where necessary. - The output should clearly show the steps being carried out, including printing dimensions and names where required. # Example Output: ``` Iris Dataset: Data shape: (150, 4) Target shape: (150,) Feature names: [\'sepal length (cm)\', \'sepal width (cm)\', \'petal length (cm)\', \'petal width (cm)\'] Target names: [\'setosa\', \'versicolor\', \'virginica\'] 20 Newsgroups Dataset: Number of samples: 11314 First 5 target names: [\'alt.atheism\', \'comp.graphics\', \'comp.os.ms-windows.misc\', \'comp.sys.ibm.pc.hardware\', \'comp.sys.mac.hardware\'] Synthetic Dataset: Data shape: (1000, 20) Target shape: (1000,) ``` **Constraints:** - You must handle any non-standard cases or potential errors (e.g., dataset not found). - Ensure the synthetic dataset parameters are used correctly when calling `make_classification`. # Submission: - Implement your solution in a Python script or Jupyter notebook. - Submit the code along with the output results.","solution":"from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification def load_iris_dataset(): iris = load_iris() data_shape = iris.data.shape target_shape = iris.target.shape feature_names = iris.feature_names target_names = iris.target_names return { \\"data_shape\\": data_shape, \\"target_shape\\": target_shape, \\"feature_names\\": feature_names, \\"target_names\\": target_names } def fetch_newsgroups_dataset(): newsgroups = fetch_20newsgroups(subset=\'all\', shuffle=True, random_state=42) num_samples = len(newsgroups.data) target_names = newsgroups.target_names[:5] return { \\"num_samples\\": num_samples, \\"target_names\\": target_names } def generate_synthetic_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_classes=3, random_state=42) data_shape = X.shape target_shape = y.shape return { \\"data_shape\\": data_shape, \\"target_shape\\": target_shape } if __name__ == \\"__main__\\": iris_result = load_iris_dataset() print(\\"Iris Dataset:\\") print(f\\"Data shape: {iris_result[\'data_shape\']}\\") print(f\\"Target shape: {iris_result[\'target_shape\']}\\") print(f\\"Feature names: {iris_result[\'feature_names\']}\\") print(f\\"Target names: {iris_result[\'target_names\']}\\") newsgroups_result = fetch_newsgroups_dataset() print(\\"n20 Newsgroups Dataset:\\") print(f\\"Number of samples: {newsgroups_result[\'num_samples\']}\\") print(f\\"First 5 target names: {newsgroups_result[\'target_names\']}\\") synthetic_result = generate_synthetic_dataset() print(\\"nSynthetic Dataset:\\") print(f\\"Data shape: {synthetic_result[\'data_shape\']}\\") print(f\\"Target shape: {synthetic_result[\'target_shape\']}\\")"},{"question":"# Advanced Coding Assessment Question: Create a Simple Text Editor **Objective**: Create a simple text-based editor using the curses library in Python. This editor should allow users to type text, navigate using arrow keys, and provide some basic text editing functionalities. **Requirements**: 1. **Initialization and Cleanup**: - Initialize curses properly to handle special keys and turn off automatic echo. - Use the `wrapper()` function for proper initialization and termination. 2. **Basic Editor Layout**: - Create a text window that occupies most of the screen, leaving a 1-line status bar at the bottom. - The status bar should display a message like \\"Simple Text Editor - Press F1 to exit\\". 3. **Text Input and Navigation**: - Allow users to type text into the editor window. - Navigate through the text using arrow keys (up, down, left, right). - Ensure that the cursor position updates correctly without overlapping text. 4. **Text Editing**: - Implement basic text editing functions: - Backspace key to delete the character before the cursor. - Enter key to insert a new line. - Handle edge cases where the cursor moves off the visible screen area and ensure smooth navigation through the text window. 5. **Exit Handling**: - Allow users to exit the editor by pressing the F1 key. Ensure that all resources are cleaned up, and the terminal is restored to its normal state. **Input format**: - No specific input format is required from users. The program runs interactively. **Output format**: - No specific output format is required. The program displays text within the curses window. **Constraints**: - The program should handle basic text editing for ASCII characters. Extended functionality for Unicode characters or more advanced text editing is not required. - Ensure that the editor does not crash on unexpected input or navigation attempts. **Performance Requirements**: - The program should provide a smooth user experience and respond quickly to user inputs. **Example Code Skeleton**: ```python import curses def main(stdscr): # Initialize curses curses.noecho() curses.cbreak() stdscr.keypad(True) # Get screen dimensions height, width = stdscr.getmaxyx() # Create a new window for text editor text_win = curses.newwin(height - 1, width, 0, 0) status_bar = curses.newwin(1, width, height - 1, 0) # Initial cursor position cursor_y, cursor_x = 0, 0 # Main loop while True: # Clear windows text_win.clear() status_bar.clear() # Display status bar message status_bar.addstr(0, 0, \\"Simple Text Editor - Press F1 to exit\\") # Update text editor window and status bar text_win.refresh() status_bar.refresh() # Move cursor and refresh position text_win.move(cursor_y, cursor_x) # Get user input key = text_win.getch() # Handle exit case if key == curses.KEY_F1: break # Handle other key inputs for navigation and text manipulation elif key == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) elif key == curses.KEY_DOWN: cursor_y = min(height - 2, cursor_y + 1) elif key == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif key == curses.KEY_RIGHT: cursor_x = min(width - 1, cursor_x + 1) elif key == curses.KEY_BACKSPACE: # Handle backspace functionality pass elif key == 10: # Enter key # Handle enter/newline functionality pass else: # Handle character insertion pass # Cleanup curses curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` **Note**: Implement the functionality for backspace, newline, and character insertion within the main loop to complete the editor.","solution":"import curses def main(stdscr): # Initialize curses curses.noecho() curses.cbreak() stdscr.keypad(True) # Get screen dimensions height, width = stdscr.getmaxyx() # Create a new window for text editor text_win = curses.newwin(height - 1, width, 0, 0) status_bar = curses.newwin(1, width, height - 1, 0) # Initial cursor position cursor_y, cursor_x = 0, 0 # Buffer to store the text text_buffer = [[]] # Main loop while True: # Clear windows text_win.clear() status_bar.clear() # Display text from buffer for y, line in enumerate(text_buffer): text_win.addstr(y, 0, \'\'.join(line)) # Display status bar message status_bar.addstr(0, 0, \\"Simple Text Editor - Press F1 to exit\\") # Update text editor window and status bar text_win.refresh() status_bar.refresh() # Move cursor and refresh position text_win.move(cursor_y, cursor_x) # Get user input key = text_win.getch() # Handle exit case if key == curses.KEY_F1: break # Handle other key inputs for navigation and text manipulation elif key == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) elif key == curses.KEY_DOWN: cursor_y = min(len(text_buffer) - 1, cursor_y + 1) cursor_x = min(cursor_x, len(text_buffer[cursor_y])) elif key == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif key == curses.KEY_RIGHT: cursor_x = min(len(text_buffer[cursor_y]), cursor_x + 1) elif key == curses.KEY_BACKSPACE or key == 127: if cursor_x > 0: del text_buffer[cursor_y][cursor_x - 1] cursor_x -= 1 elif cursor_y > 0: prev_line_length = len(text_buffer[cursor_y - 1]) text_buffer[cursor_y - 1].extend(text_buffer[cursor_y]) del text_buffer[cursor_y] cursor_y -= 1 cursor_x = prev_line_length elif key == 10: # Enter key new_line = text_buffer[cursor_y][cursor_x:] text_buffer[cursor_y] = text_buffer[cursor_y][:cursor_x] text_buffer.insert(cursor_y + 1, new_line) cursor_y += 1 cursor_x = 0 else: if key >= 32 and key <= 126: # Printable characters text_buffer[cursor_y].insert(cursor_x, chr(key)) cursor_x += 1 # Cleanup curses curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"You are given a list of tasks, where each task may have one or more prerequisites that need to be completed before the task itself can be started. Implement a function that receives the tasks and their prerequisites and returns a list representing a valid order in which the tasks can be performed. # Function Signature ```python def find_task_order(tasks: List[str], prerequisites: List[Tuple[str, str]]) -> List[str]: pass ``` # Input - `tasks`: A list of strings, where each string is a task identifier. - `prerequisites`: A list of tuples, where each tuple `(a, b)` means task `b` must be completed before task `a`. # Output - A list of strings representing a valid order to perform all tasks. If there\'s no valid order due to a cycle in the input, raise a `ValueError`. # Constraints - Each task identifier is unique. - The input graph (tasks + prerequisites) is guaranteed to be acyclic. # Example ```python tasks = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] prerequisites = [(\\"A\\", \\"B\\"), (\\"A\\", \\"C\\"), (\\"B\\", \\"D\\")] find_task_order(tasks, prerequisites) # Output: [\\"D\\", \\"B\\", \\"C\\", \\"A\\"] ``` # Notes - Use the `TopologicalSorter` from the `graphlib` module to solve this problem. - Make sure to handle the case where there are no prerequisites correctly. # Implementation Implement the function `find_task_order` using the guidelines specified.","solution":"from typing import List, Tuple from graphlib import TopologicalSorter def find_task_order(tasks: List[str], prerequisites: List[Tuple[str, str]]) -> List[str]: Find a valid order to perform the tasks given their prerequisites. :param tasks: List of task identifiers :param prerequisites: List of tuples where each tuple (a, b) means task b must be completed before task a. :return: List of task identifiers in a valid order ts = TopologicalSorter() # Add tasks to the graph for task in tasks: ts.add(task) # Add prerequisites to the graph for a, b in prerequisites: ts.add(a, b) # Retrieve the ordered list of tasks task_order = list(ts.static_order()) return task_order"},{"question":"Objective You are required to implement several functions utilizing the `crypt` module in Python to hash passwords and verify them securely. This exercise will assess your understanding of password hashing, salt generation, and secure comparison. Problem Statement 1. **Function `hash_password`** - **Input:** A string `password` representing the plain text password, and an optional string `method` representing the hashing method (default to the strongest available method). - **Output:** A string representing the hashed password. - **Constraints:** - `method` can be one of the `crypt.METHOD_*` attributes. If an invalid method is provided, the function should raise a `ValueError`. - If no method is provided, the function should use the strongest available method. 2. **Function `verify_password`** - **Input:** Two strings: `plain_password` representing the plain text password, and `hashed_password` representing the hashed password to check against. - **Output:** A boolean value `True` if the `plain_password` matches the `hashed_password`, otherwise `False`. - **Constraints:** - Securely compare the two hashed values to avoid timing attacks. Function Signatures ```python import crypt from hmac import compare_digest def hash_password(password: str, method: str = None) -> str: # Implement this function pass def verify_password(plain_password: str, hashed_password: str) -> bool: # Implement this function pass ``` Example ```python # Example usage hashed = hash_password(\\"mypassword\\", method=crypt.METHOD_SHA512) print(hashed) # This will output the hashed password string is_valid = verify_password(\\"mypassword\\", hashed) print(is_valid) # This should output: True is_valid = verify_password(\\"wrongpassword\\", hashed) print(is_valid) # This should output: False ``` Notes - Use the `crypt.mksalt` function to generate the salt if necessary. - Ensure you handle edge cases such as invalid methods gracefully by raising exceptions where appropriate. - Secure comparison of hashed values should be performed using `hmac.compare_digest` to avoid timing attacks.","solution":"import crypt from hmac import compare_digest def hash_password(password: str, method: str = None) -> str: Hashes the given password using the specified method or the strongest available method by default. :param password: The plain text password to hash. :param method: The hashing method to use from crypt library (e.g., crypt.METHOD_SHA512). :return: The hashed password. if method is None: # Use the strongest available method by default method = crypt.METHOD_SHA512 else: # Validate the provided method valid_methods = [crypt.METHOD_MD5, crypt.METHOD_SHA256, crypt.METHOD_SHA512] if method not in valid_methods: raise ValueError(\\"Invalid method provided for hashing password.\\") # Generate a salt and hash the password salt = crypt.mksalt(method) hashed_password = crypt.crypt(password, salt) return hashed_password def verify_password(plain_password: str, hashed_password: str) -> bool: Verifies if the given plain text password matches the hashed password securely. :param plain_password: The plain text password to verify. :param hashed_password: The existing hashed password to check against. :return: True if the passwords match, False otherwise. # Hash the plain password using the salt from the hashed password hashed_input_password = crypt.crypt(plain_password, hashed_password) # Securely compare the two hashes return compare_digest(hashed_input_password, hashed_password)"},{"question":"# Question: Advanced PyTorch Backends Management You are tasked with optimizing the neural network training process in PyTorch using the `torch.backends` module. The goal is to ensure that your model training leverages the best performance capabilities offered by different backends, particularly focusing on the CUDA and cuDNN backends. Requirements: 1. **Function Implementation**: Implement a function `optimize_backends()` that configures the `torch.backends.cuda` and `torch.backends.cudnn` according to the following specifications: - Enable TensorFloat-32 support for both matrix multiplications and cuDNN convolutions. - Allow reduced precision reductions with fp16 accumulation type. - Utilize the cuFFT plan cache and set its maximum size to 1024. - Enable cuDNN benchmarking and set the benchmark limit to 3. - Ensure that all deterministic algorithms are utilized for reproducibility. 2. **Verification**: Implement another function `verify_settings()` to check and print whether the configurations have been applied correctly. # Function Signatures: ```python def optimize_backends(): Configures the PyTorch backends for optimal performance. def verify_settings(): Verifies and prints the settings of the configured backends. ``` # Expected Outputs: After calling `verify_settings()`, it should print the following: ``` TensorFloat-32 for matrix multiplications: True TensorFloat-32 for cuDNN convolutions: True Reduced precision reductions (fp16) allowed: True cuFFT Plan Cache Size: 1024 cuDNN Benchmark enabled: True cuDNN Benchmark limit: 3 Deterministic algorithms enabled: True ``` # Constraints: - Assume that CUDA and cuDNN are available on the system where the script runs. - Ensure no changes are made to the default configurations of other backends. # Example Usage: ```python optimize_backends() verify_settings() ``` **Note**: The students are expected to use appropriate methods from the `torch.backends.cuda` and `torch.backends.cudnn` modules to achieve these configurations.","solution":"import torch def optimize_backends(): Configures the PyTorch backends for optimal performance. # Enable TensorFloat-32 for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Enable TensorFloat-32 for cuDNN convolutions torch.backends.cudnn.allow_tf32 = True # Allow reduced precision reductions with fp16 accumulation type torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True # Utilize cuFFT plan cache and set its maximum size to 1024 torch.backends.cuda.cufft_plan_cache_size = 1024 # Enable cuDNN benchmarking torch.backends.cudnn.benchmark = True # Set cuDNN benchmark limit torch.backends.cudnn.benchmark_limit = 3 # Ensure that all deterministic algorithms are utilized torch.backends.cudnn.deterministic = True def verify_settings(): Verifies and prints the settings of the configured backends. print(f\\"TensorFloat-32 for matrix multiplications: {torch.backends.cuda.matmul.allow_tf32}\\") print(f\\"TensorFloat-32 for cuDNN convolutions: {torch.backends.cudnn.allow_tf32}\\") print(f\\"Reduced precision reductions (fp16) allowed: {torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction}\\") print(f\\"cuFFT Plan Cache Size: {torch.backends.cuda.cufft_plan_cache_size}\\") print(f\\"cuDNN Benchmark enabled: {torch.backends.cudnn.benchmark}\\") print(f\\"cuDNN Benchmark limit: {getattr(torch.backends.cudnn, \'benchmark_limit\', \'Unknown\')}\\") print(f\\"Deterministic algorithms enabled: {torch.backends.cudnn.deterministic}\\")"},{"question":"# PyTorch Coding Assessment Question Objective: You are required to demonstrate your understanding of distributed training in PyTorch, specifically using the Generic Join Context Manager to handle uneven inputs. Problem Statement: You are given a simple neural network model, and you need to set up a distributed training environment where each process trains the model using a subset of the dataset. The data subsets may be uneven, and you must ensure the distributed training is handled gracefully using the Generic Join Context Manager. Requirements: 1. Implement a simple neural network using PyTorch. 2. Ensure that the training is distributed across multiple processes. 3. Use the Generic Join Context Manager to handle scenarios where the input data subsets are uneven. Instructions: 1. **Neural Network Implementation**: - Implement a basic neural network with 2 hidden layers. - Use ReLU activation functions for the hidden layers and a softmax function for the output layer. 2. **Dataset Preparation**: - Use a synthetic dataset with tensors of random numbers representing the data. - Split the dataset into uneven parts, one for each process. 3. **Distributed Training Setup**: - Utilize `torch.distributed` to set up the distributed environment. - Implement the training loop where each process trains on its subset. 4. **Generic Join Context Manager**: - Use the `Join`, `Joinable`, and `JoinHook` classes to synchronize and manage the training across processes. - Ensure that the training does not crash if one of the processes has fewer data points and completes earlier. Function Signatures: ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.multiprocessing import Process from torch.distributed.algorithms import Join, JoinHook class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define your layers here def forward(self, x): # Define the forward pass def train(rank, world_size, data): # Initialize the process group # Create your model and move it to the appropriate device # Create your optimizer and loss function # Implement the training loop using Generic Join Context Manager # Clean up the process group def main(): # Set the world_size (number of processes) world_size = <number of processes> # Create synthetic dataset and split it into uneven parts # Use torch.multiprocessing to spawn processes for distributed training # Join all the processes if __name__ == \\"__main__\\": main() ``` Constraints: - Use only PyTorch and standard libraries. - Ensure the solution is Python 3.x compatible. - The dataset splits should be uneven to simulate real-world scenarios. Performance Requirements: - Efficient utilization of the `Join` context manager to handle uneven input data across distributed processes. Good luck!","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.multiprocessing import Process from torch.distributed.algorithms.join import Join, JoinHook import random import os class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 64) self.fc2 = nn.Linear(64, 32) self.fc3 = nn.Linear(32, 5) self.relu = nn.ReLU() self.softmax = nn.Softmax(dim=1) def forward(self, x): x = self.relu(self.fc1(x)) x = self.relu(self.fc2(x)) x = self.softmax(self.fc3(x)) return x def train(rank, world_size, data): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create the model and move it to the appropriate device model = SimpleNet().to(rank) optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() # Create a Joinable subclass to be compatible with the Join context manager class JoinableDataLoader(torch.utils.data.DataLoader): def __init__(self, data): super().__init__(data, batch_size=2) self.finished = False def __iter__(self): self.finished = False return super().__iter__() def next(self): try: return next(super().__iter__()) except StopIteration: self.finished = True raise dataloader = JoinableDataLoader(data[rank]) # Implement the training loop using Generic Join Context Manager with Join([dataloader]), torch.cuda.device(rank): for epoch in range(2): for inputs, labels in dataloader: inputs, labels = inputs.to(rank), labels.to(rank) # Zero the parameter gradients optimizer.zero_grad() # Forward outputs = model(inputs) loss = criterion(outputs, labels) # Backward loss.backward() optimizer.step() # Clean up the process group dist.destroy_process_group() def main(): world_size = 2 torch.manual_seed(0) # Create synthetic dataset and split it unevenly data = [(torch.randn(10), torch.randint(0, 5, (1,)).item()) for _ in range(100)] random.shuffle(data) split1 = data[:80] split2 = data[80:] # Use torch.multiprocessing to spawn processes for distributed training processes = [] data_split = [split1, split2] for rank in range(world_size): p = Process(target=train, args=(rank, world_size, data_split)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \'__main__\': main()"},{"question":"Pandas Coding Question # Objective In this question, you will be tasked with analyzing the memory usage of a DataFrame, handling missing values, and applying a user-defined function (UDF) to process the data. This will test your understanding of memory optimization in pandas and your ability to work with complex DataFrame operations. # Question You are given a DataFrame that contains mixed data types, including integers, floats, and objects. Perform the following tasks: 1. Create a DataFrame `df` from the given dictionary: ```python data = { \\"int_column\\": np.random.randint(0, 100, size=5000), \\"float_column\\": np.random.rand(5000), \\"object_column\\": np.random.choice([\'A\', \'B\', \'C\', \'D\'], size=5000) } df = pd.DataFrame(data) ``` 2. Calculate the memory usage of `df` and print out a summary in human-readable format (e.g., KB, MB). 3. Introduce missing values (NaNs) into the `int_column` and the `float_column` by replacing 10% of the values with `np.nan`. Print the updated DataFrame. 4. Create a user-defined function `process_row` that: - Adds 10 to each integer value (handling NaNs correctly). - Multiplies each float value by 0.5 (handling NaNs correctly). - Converts the object column to lowercase. - Returns the modified row. 5. Apply the `process_row` function to each row of the DataFrame without mutating the existing DataFrame. Print the resulting DataFrame. # Input The input for this task is the given dictionary `data`. # Output The expected output is: 1. Memory usage summary of the DataFrame in human-readable format. 2. DataFrame with NaNs introduced. 3. DataFrame after applying the `process_row` function, demonstrating the transformations. # Constraints - Ensure that the DataFrame transformations handle missing values appropriately without causing errors. - The solution should not mutate the original DataFrame when applying the user-defined function. # Performance Requirements - The function `process_row` should be efficient enough to handle the transformations within a reasonable time frame for 5000 rows. - Memory usage calculations should be accurate and reflect actual consumption. ```python # Sample Code Structure import numpy as np import pandas as pd # Step 1: Creating the DataFrame data = { \\"int_column\\": np.random.randint(0, 100, size=5000), \\"float_column\\": np.random.rand(5000), \\"object_column\\": np.random.choice([\'A\', \'B\', \'C\', \'D\'], size=5000) } df = pd.DataFrame(data) # Step 2: Calculate and print memory usage memory_usage = df.memory_usage(deep=True).sum() print(f\\"Memory usage of DataFrame: {memory_usage / (1024 ** 2):.2f} MB\\") # Step 3: Introduce missing values df.loc[df.sample(frac=0.1).index, \'int_column\'] = np.nan df.loc[df.sample(frac=0.1).index, \'float_column\'] = np.nan print(df.head()) # Step 4: Define the user-defined function def process_row(row): row[\'int_column\'] = row[\'int_column\'] + 10 if not pd.isna(row[\'int_column\']) else row[\'int_column\'] row[\'float_column\'] = row[\'float_column\'] * 0.5 if not pd.isna(row[\'float_column\']) else row[\'float_column\'] row[\'object_column\'] = row[\'object_column\'].lower() return row # Step 5: Apply the function without mutating the DataFrame df_processed = df.apply(process_row, axis=1) print(df_processed.head()) ``` # Submission Submit your implementation in a Python script or Jupyter notebook. Ensure that all transformations and memory usage calculations are included and correctly formatted.","solution":"import numpy as np import pandas as pd # Step 1: Creating the DataFrame data = { \\"int_column\\": np.random.randint(0, 100, size=5000), \\"float_column\\": np.random.rand(5000), \\"object_column\\": np.random.choice([\'A\', \'B\', \'C\', \'D\'], size=5000) } df = pd.DataFrame(data) # Step 2: Calculate and print memory usage def memory_usage_human_readable(df): memory = df.memory_usage(deep=True).sum() for unit in [\\"Bytes\\", \\"KB\\", \\"MB\\", \\"GB\\", \\"TB\\"]: if memory < 1024: return f\\"{memory:.2f} {unit}\\" memory /= 1024 print(f\\"Memory usage of DataFrame: {memory_usage_human_readable(df)}\\") # Step 3: Introduce missing values df.loc[df.sample(frac=0.1).index, \'int_column\'] = np.nan df.loc[df.sample(frac=0.1).index, \'float_column\'] = np.nan print(df.head()) # Step 4: Define the user-defined function def process_row(row): row[\'int_column\'] = row[\'int_column\'] + 10 if not pd.isna(row[\'int_column\']) else row[\'int_column\'] row[\'float_column\'] = row[\'float_column\'] * 0.5 if not pd.isna(row[\'float_column\']) else row[\'float_column\'] row[\'object_column\'] = row[\'object_column\'].lower() return row # Step 5: Apply the function without mutating the DataFrame df_processed = df.apply(process_row, axis=1) print(df_processed.head())"},{"question":"**Meta Device Operations in PyTorch** You are given a PyTorch model and a dataset for an NLP problem. However, you want to examine and modify the model\'s structure before loading the actual parameters into memory. Additionally, you want to avoid the overhead of loading data when performing certain structural analyses on the model. Your task is to use the meta device to achieve this. **Task:** 1. Load the given model architecture onto the meta device. 2. Print the structure of the model based on the meta device. 3. Change specific layers of the model using meta tensors. 4. Transform the modified model to a fully initialized state without any actual data. 5. Fill the parameters of the model on the CPU with some initial values. **Instructions:** 1. Write a function `load_and_transform_model(model_path: str) -> torch.nn.Module` that takes the path to a saved model and performs the following steps: - Loads the model onto the meta device. - Modifies the model by replacing any `nn.Linear` layers with a new `nn.Linear` layer that has the same input and output features but with all parameters as meta tensors. - Returns the modified model moved to the CPU with the parameters filled with initial values (e.g., zeros or random values). **Expected Input:** - A file path (string) representing the saved model file. **Expected Output:** - A modified and reinitialized PyTorch model on the CPU. **Constraints:** - Do not load the model weights into memory until the transformation process is complete. - Ensure that the new `nn.Linear` layers have uninitialized parameters when constructed on meta device. ```python import torch import torch.nn as nn def load_and_transform_model(model_path: str) -> torch.nn.Module: # Load model on meta device with torch.device(\'meta\'): model = torch.load(model_path, map_location=\'meta\') # Define function to replace and modify Linear layers def modify_module(module): for name, child in module.named_children(): if isinstance(child, nn.Linear): # Replace with a new Linear layer with uninitialized parameters setattr(module, name, nn.Linear(child.in_features, child.out_features, device=\'meta\')) else: # Recursively apply to submodules modify_module(child) # Modify the model modify_module(model) # Transform the modified model to CPU with uninitialized parameters model = model.to_empty(device=\'cpu\') # Initialize parameters with zeros or random values def initialize_parameters(module): for param in module.parameters(): init_type = \\"zeros\\" # Change to your preferred initialization method if init_type == \\"zeros\\": nn.init.zeros_(param) elif init_type == \\"random\\": nn.init.normal_(param) # Initialize model parameters initialize_parameters(model) return model ``` **Notes:** - Ensure you have a saved model to test this function. - Use `torch.save` to save a sample model if you do not have one.","solution":"import torch import torch.nn as nn def load_and_transform_model(model_path: str) -> torch.nn.Module: Loads a model from the given path onto the meta device, modifies its Linear layers, and returns the modified model with parameters initialized moved to the CPU. # Load model on meta device with torch.device(\'meta\'): model = torch.load(model_path, map_location=\'meta\') # Function to recursively replace Linear layers with meta-initialized ones def modify_module(module): for name, child in module.named_children(): if isinstance(child, nn.Linear): # Replace with a new Linear layer with uninitialized parameters setattr(module, name, nn.Linear(child.in_features, child.out_features, device=\'meta\')) else: # Recursively apply to submodules modify_module(child) # Modify the model modify_module(model) # Transform the modified model to CPU with empty parameter tensors model = model.to_empty(device=\'cpu\') # Initialize parameters with zeros or random values def initialize_parameters(module): for param in module.parameters(): init_type = \\"zeros\\" # Change to your preferred initialization method if init_type == \\"zeros\\": nn.init.zeros_(param) elif init_type == \\"random\\": nn.init.normal_(param) # Initialize model parameters initialize_parameters(model) return model"},{"question":"You are given the task of implementing custom autograd functionality to better understand the internal workings and customization options of PyTorch\'s gradient computation system. You will create a custom PyTorch `Function` to perform a mathematical operation, correctly manage intermediate results for backpropagation, and demonstrate the use of saved tensors. # Part 1: Custom Function and Saving Tensors **Objective:** 1. Implement a custom autograd `Function` that computes the sine of its input multiplied by a learned parameter. This parameter (`scale`) should influence the significance of the sine function. 2. The forward pass should compute `scaled_sine = scale * sin(x)`. 3. The backward pass should correctly compute and return the gradient of the loss concerning the input `x` and the parameter `scale`. Ensure intermediate results required for backpropagation are saved and used effectively. **Instructions:** 1. Implement `MyFunction` class that inherits from `torch.autograd.Function`. 2. In the `forward` static method: - Take tensor inputs `x` and `scale`. - Compute the `scaled_sine` as specified. - Save tensors needed for the backward pass. 3. In the `backward` static method: - Use the saved tensors. - Compute the gradients with respect to both `x` and `scale`. # Part 2: Custom Hooks for Saved Tensors **Objective:** 4. Explore and implement custom hooks for saved tensors to demonstrate packing and unpacking functionalities specifically for this custom operation. **Instructions:** 1. Define `pack_hook` and `unpack_hook` to convert tensors to strings and back as a demonstration mechanism (actual string conversion is just for educational purposes in this context). **Constraints:** 1. Do not use any external packages other than torch. 2. Ensure intermediary tensors are saved without duplicates for memory efficiency. 3. Avoid in-place operations to prevent overwriting values needed for backpropagation. # Sample Code for Implementation ```python import torch class MyFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, scale): # Compute scaled sine scaled_sine = scale * torch.sin(x) # Save tensors for backward pass ctx.save_for_backward(x, scale) return scaled_sine @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, scale = ctx.saved_tensors # Compute gradients grad_x = grad_output * scale * torch.cos(x) grad_scale = grad_output * torch.sin(x) return grad_x, grad_scale # Define pack and unpack hooks def pack_hook(tensor): return str(tensor.tolist()) def unpack_hook(packed): return torch.tensor(eval(packed)) # Example usage class ExampleModel(torch.nn.Module): def __init__(self, scale): super(ExampleModel, self).__init__() self.scale = torch.nn.Parameter(torch.tensor(scale)) def forward(self, x): with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook): return MyFunction.apply(x, self.scale) # Usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) model = ExampleModel(2.0) y = model(x) loss = y.sum() loss.backward() print(\\"Gradient wrt x:\\", x.grad) print(\\"Gradient wrt scale:\\", model.scale.grad) ``` # Expected Output 1. The python script should correctly compute the forward pass `scaled_sine`. 2. The backward pass should generate gradients and propagate them back to `x` and `scale`. 3. Demonstrating tensor packing/unpacking with hooks should show intermediate tensor conversions. Provide the complete implementation of `MyFunction` and ensure it adheres to given constraints and objectives. **Input Format:** - Tensor `x` with shape `(N,)` where `N` can vary. - A scalar scale (can be any real number). **Output Format:** - Tensor of the same shape as `x` representing `scaled_sine` during the forward pass. - Gradients of the same shape as `x` and scalar value for `scale` during the backward pass.","solution":"import torch class MyFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, scale): # Compute scaled sine scaled_sine = scale * torch.sin(x) # Save tensors for backward pass ctx.save_for_backward(x, scale) return scaled_sine @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, scale = ctx.saved_tensors # Compute gradients grad_x = grad_output * scale * torch.cos(x) grad_scale = grad_output * torch.sin(x) return grad_x, grad_scale # Define pack and unpack hooks def pack_hook(tensor): return str(tensor.tolist()) def unpack_hook(packed): return torch.tensor(eval(packed)) # Example model demonstrating the usage of the custom Function class ExampleModel(torch.nn.Module): def __init__(self, scale): super(ExampleModel, self).__init__() self.scale = torch.nn.Parameter(torch.tensor(scale)) def forward(self, x): with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook): return MyFunction.apply(x, self.scale) # Usage example (unit tests will be separate) x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) model = ExampleModel(2.0) y = model(x) loss = y.sum() loss.backward() print(\\"Gradient wrt x:\\", x.grad) print(\\"Gradient wrt scale:\\", model.scale.grad)"},{"question":"You are required to write a Python program that mimics a simple task manager. This task manager should allow you to start a task, stop a task, and handle various signals during the task execution. Specifically, your program should be able to: 1. **Start a Task**: The task should be a loop running that increments a counter every second. 2. **Stop a Task**: Gracefully stop the task when a specific signal is received. 3. **Handle Signals**: Handle different signals like `SIGINT`, `SIGALRM`, `SIGTERM`, etc. 4. **Set Alarm**: Set an alarm to stop the task after a specific duration. # Requirements - Implement the task as a function `start_task` which starts counting from 1 and increments every second. - Implement a signal handler function `handle_signal` which handles `SIGINT` (Ctrl+C) to clean up resources and terminate the task. - Set an alarm using `signal.alarm` that stops the task after 10 seconds by handling the `SIGALRM` signal. - Ensure the task stops gracefully and prints a summary of the count when interrupted by `SIGINT`. - Ensure other signals like `SIGTERM` are handled appropriately without causing the program to crash. # Constraints - You can only use the `signal` module for signal handling. Any other libraries are not allowed for this task. - The task should not terminate immediately but should allow for cleanup and final reporting. # Input and Output - **Function**: `start_task()` - **Output**: - The task should print the counter value every second. - On receiving `SIGINT` or `SIGALRM`, it should print \\"Task stopped. Final count: `<count>`\\". # Example ```python import signal import time # Define the signal handler function def handle_signal(signum, frame): global running print(f\\"Signal handler called with signal {signum}\\") if signum == signal.SIGALRM or signum == signal.SIGINT: running = False def start_task(): global running running = True # Register signal handlers signal.signal(signal.SIGALRM, handle_signal) signal.signal(signal.SIGINT, handle_signal) # Set an alarm for 10 seconds signal.alarm(10) count = 0 while running: count += 1 print(f\\"Count: {count}\\") time.sleep(1) print(f\\"Task stopped. Final count: {count}\\") # Entry point for the program if __name__ == \\"__main__\\": start_task() ``` # Explanation - The `start_task` function begins a loop that prints the count every second. - The signal handler `handle_signal` handles `SIGALRM` (triggered after 10 seconds) and `SIGINT` (Ctrl+C) to set the `running` flag to False, which stops the loop. - After the loop concludes, it prints the final count before exiting.","solution":"import signal import time # Global variable to control the task loop running = True def handle_signal(signum, frame): global running print(f\\"Signal handler called with signal {signum}\\") if signum in [signal.SIGALRM, signal.SIGINT, signal.SIGTERM]: running = False def start_task(): global running running = True # Register signal handlers signal.signal(signal.SIGALRM, handle_signal) signal.signal(signal.SIGINT, handle_signal) signal.signal(signal.SIGTERM, handle_signal) # Set an alarm for 10 seconds signal.alarm(10) count = 0 try: while running: count += 1 print(f\\"Count: {count}\\") time.sleep(1) finally: print(f\\"Task stopped. Final count: {count}\\") # Entry point for the program if __name__ == \\"__main__\\": start_task()"},{"question":"# Objective Design a function to test and monitor the behavior of Python\'s garbage collector using the `gc` module functions provided. # Instructions 1. **Create a function named `test_garbage_collector()`**, which will perform the following steps: 2. **Enable the garbage collector** using `gc.enable()` if it is not already enabled. 3. **Set debugging flags** to `DEBUG_STATS | DEBUG_COLLECTABLE`. 4. **Generate some objects for garbage collection**. For example, create a nested list or dictionary structure that will be garbage collected. 5. **Manually disable the garbage collector** using `gc.disable()`, and print confirmation. 6. **Run a manual collection** using `gc.collect()`, and print the number of unreachable objects found. 7. **Re-enable the garbage collector** and set debugging to `DEBUG_LEAK`. 8. **Retrieve and print collection statistics** using `gc.get_stats()`. 9. **Create a sample referential cycle and trigger garbage collection** to see the behavior: - Example: `a = []; b = [a]; a.append(b)` 10. **Print all objects tracked by the collector** using `gc.get_objects()`. 11. **Finally, reset the debug flags** to their default state using `gc.set_debug(0)`. # Constraints - Use only the functions and constants provided in the `gc` module documentation. - Ensure your code handles exceptions gracefully, particularly for invalid operations (like invalid generation numbers). # Example Output Your function should provide output similar to: ``` Garbage collector enabled. Garbage collector disabled. Manual collection run. Unreachable objects found: 10 Garbage collector re-enabled with DEBUG_LEAK. Collection statistics: [{\'collections\': 2, \'collected\': 50, \'uncollectable\': 0}, {\'collections\': 1, \'collected\': 10, \'uncollectable\': 0}] Tracked objects: [<__main__.YourClass object at 0x...>, <list object at 0x...>, ...] Debug flags reset to default state. ``` # Requirements - Your function should be implemented as per the instructions. - The output should be clear and informative. # Evaluation Criteria - Correct use of the `gc` module functions. - Proper management and cycle of enabling/disabling garbage collection. - Clear and readable output. - Graceful handling of exceptions and edge cases.","solution":"import gc def test_garbage_collector(): Function to test and monitor Python\'s garbage collector. # Step 1: Enable the garbage collector if not already enabled gc.enable() print(\\"Garbage collector enabled.\\") # Step 2: Set debugging flags to DEBUG_STATS | DEBUG_COLLECTABLE gc.set_debug(gc.DEBUG_STATS | gc.DEBUG_COLLECTABLE) # Step 3: Generate some objects for garbage collection nested_dict = {i: {i: [str(j) for j in range(100)]} for i in range(10)} nested_list = [[i for i in range(100)] for j in range(10)] # Step 4: Manually disable the garbage collector and print confirmation gc.disable() print(\\"Garbage collector disabled.\\") # Step 5: Run a manual collection and print the number of unreachable objects found unreachable_objects = gc.collect() print(f\\"Manual collection run. Unreachable objects found: {unreachable_objects}\\") # Step 6: Re-enable the garbage collector and set debugging to DEBUG_LEAK gc.enable() gc.set_debug(gc.DEBUG_LEAK) print(\\"Garbage collector re-enabled with DEBUG_LEAK.\\") # Step 7: Retrieve and print collection statistics stats = gc.get_stats() print(f\\"Collection statistics: {stats}\\") # Step 8: Create a sample referential cycle and trigger garbage collection to see the behavior a = [] b = [a] a.append(b) unreachable_objects_cycle = gc.collect() print(f\\"Garbage collection after referential cycle. Unreachable objects found: {unreachable_objects_cycle}\\") # Step 9: Print all objects tracked by the collector tracked_objects = gc.get_objects() print(f\\"Tracked objects: {tracked_objects[:10]}\\") # Print first 10 objects to avoid excessive output # Step 10: Reset the debug flags to their default state gc.set_debug(0) print(\\"Debug flags reset to default state.\\")"},{"question":"**Covariance Estimation in Scikit-learn** # Problem Statement You are given a dataset represented as a 2D NumPy array `X` where rows correspond to samples and columns correspond to features. Your task is to implement a function that applies different covariance estimation methods provided by scikit-learn and returns their results. # Function Signature ```python def estimate_covariances(X: np.ndarray) -> dict: pass ``` # Input - `X (numpy.ndarray)`: A 2D NumPy array of shape `(n_samples, n_features)` containing the dataset. # Output - `result (dict)`: A dictionary where: - The key `\'empirical_covariance\'` maps to the empirical covariance matrix estimated using `EmpiricalCovariance`. - The key `\'shrunk_covariance\'` maps to the shrunk covariance matrix estimated using `ShrunkCovariance`. - The key `\'ledoit_wolf_covariance\'` maps to the Ledoit-Wolf covariance matrix estimated using `LedoitWolf`. - The key `\'oas_covariance\'` maps to the OAS covariance matrix estimated using `OAS`. - The key `\'min_cov_det_covariance\'` maps to the Minimum Covariance Determinant covariance matrix estimated using `MinCovDet`. # Constraints - Assume `n_samples >= n_features` for given datasets. - You may assume all necessary libraries are already imported. # Example Usage ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet X = np.random.randn(100, 5) result = estimate_covariances(X) print(result[\'empirical_covariance\']) print(result[\'shrunk_covariance\']) print(result[\'ledoit_wolf_covariance\']) print(result[\'oas_covariance\']) print(result[\'min_cov_det_covariance\']) ``` # Notes - Ensure to correctly handle data centering if required by any method. - Utilize appropriate scikit-learn classes and methods (`EmpiricalCovariance`, `ShrunkCovariance`, `LedoitWolf`, `OAS`, `MinCovDet`) to perform the required covariance estimations.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet def estimate_covariances(X: np.ndarray) -> dict: # Initialize the covariance estimators empirical_cov = EmpiricalCovariance().fit(X) shrunk_cov = ShrunkCovariance().fit(X) ledoit_wolf_cov = LedoitWolf().fit(X) oas_cov = OAS().fit(X) min_cov_det = MinCovDet().fit(X) # Collect the results in a dictionary result = { \'empirical_covariance\': empirical_cov.covariance_, \'shrunk_covariance\': shrunk_cov.covariance_, \'ledoit_wolf_covariance\': ledoit_wolf_cov.covariance_, \'oas_covariance\': oas_cov.covariance_, \'min_cov_det_covariance\': min_cov_det.covariance_ } return result"},{"question":"You are provided with a set of log files that record the daily activities of a company\'s server. The log files contain multiple entries, and each entry might span one or more lines. Your task is to implement a function that extracts a specific log entry given the log filename and the entry number. Function Signature ```python def extract_log_entry(filename: str, entry_number: int) -> str: pass ``` Parameters - `filename` (str): The name of the log file from which to extract the entry. - `entry_number` (int): The number of the log entry to extract. The first entry is denoted by 1. Returns - `str`: The content of the specified log entry as a single string. If the log entry does not exist, return an empty string. Constraints and Notes 1. Each log entry starts with the line `--ENTRY--` and is followed by one or more lines of the actual entry content. 2. The log entries are numbered consecutively starting from 1, with each new entry beginning with `--ENTRY--`. 3. You may assume that the log file exists and is readable. 4. Use the `linecache` module to efficiently retrieve lines from the file. 5. The function should handle large files efficiently and avoid reading the entire file into memory at once. Example Usage ```python # Consider the log file \'server.log\' containing: # --ENTRY-- # Server started at 08:00 # --ENTRY-- # User login at 09:01 # Page accessed: Dashboard # --ENTRY-- # Error: Database connection failed at 10:15 print(extract_log_entry(\'server.log\', 1)) # Output: # Server started at 08:00 print(extract_log_entry(\'server.log\', 2)) # Output: # User login at 09:01 # Page accessed: Dashboard print(extract_log_entry(\'server.log\', 3)) # Output: # Error: Database connection failed at 10:15 print(extract_log_entry(\'server.log\', 4)) # Output: # \'\' (empty string because the fourth entry does not exist) ``` Your implementation should be robust, handling cases where the log entry does not exist, and leveraging the `linecache` module to optimize line retrieval operations.","solution":"import linecache def extract_log_entry(filename: str, entry_number: int) -> str: Extracts a specific log entry from a log file. Parameters: - filename (str): The name of the log file from which to extract the entry. - entry_number (int): The number of the log entry to extract. The first entry is denoted by 1. Returns: - str: The content of the specified log entry as a single string. If the log entry does not exist, return an empty string. current_entry_number = 0 entry_lines = [] is_collecting = False with open(filename, \'r\') as file: for line in file: if line.strip() == \\"--ENTRY--\\": current_entry_number += 1 if current_entry_number == entry_number: is_collecting = True entry_lines = [] elif current_entry_number > entry_number: break else: is_collecting = False elif is_collecting: entry_lines.append(line.strip()) return \'n\'.join(entry_lines)"},{"question":"# Task: Advanced Data Visualization with Seaborn **Objective:** You are required to create a comprehensive data visualization in Python using the Seaborn library. The visualization should reveal insights from the given dataset by exploiting various features of `seaborn.relplot`. **Dataset:** You will use the \\"tips\\" dataset, which is included in the Seaborn library. This dataset contains information about the tips received by waiters in a restaurant and the details about each table (total bill, tip amount, day, time, gender of the person who paid, etc.). **Instructions:** 1. **Load the Dataset:** Load the \\"tips\\" dataset using `sns.load_dataset(\\"tips\\")`. 2. **Scatter Plot for Tip vs. Total Bill:** - Create a scatter plot to visualize the relationship between `total_bill` and `tip`. - Use the `hue` parameter to distinguish data points by `day`. 3. **Multiple Faceted Scatter Plots:** - Create a faceted scatter plot where each facet represents a different `time` (`Dinner`/`Lunch`). - Further facet the plots by `sex` (rows). 4. **Complex Scatter Plot Customization:** - Create a scatter plot to visualize the relationship between `total_bill` and `tip`, segmented by `time`. - Use different colors (`hue`) for `time`, differentiate data points\' sizes by `size`, - Apply different markers (`style`) to represent `sex`. - Adjust the color palette, sizes range and other styling parameters to make the plot visually appealing. 5. **Line Plot for Aggregated Data:** - Create a line plot using `kind=\\"line\\"`. - Aggregate the tips over different `day` and plot the average tips per day with error bands representing the 95% confidence interval. 6. **Customizing The FacetGrid:** - Fetch the FacetGrid object from any of the above plots and make the following customizations: - Add a horizontal line (`axhline`) at the y=0. - Set the axis labels to \\"Total Bill\\" and \\"Tip Amount.\\" - Set a title for each subplot using the placeholders. Ensure that all plots are appropriately labeled and customized to make the visual representations insightful. You are expected to demonstrate a strong understanding of the Seaborn library\'s capabilities by effectively using various parameters and customization options. **Submission:** Provide the complete Python code to generate the visualizations as specified. Ensure the code is well-commented and follows best practices. **Evaluation Criteria:** Your submission will be evaluated based on: - Correctness and completeness of the plots - Use of `relplot` parameters and customization options - Code readability and comments","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter plot for Tip vs. Total Bill def scatter_plot_tip_vs_total_bill(): plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.title(\'Scatter Plot of Tip vs Total Bill by Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 2. Multiple Faceted Scatter Plots def faceted_scatter_plot(): g = sns.FacetGrid(tips, row=\\"sex\\", col=\\"time\\", margin_titles=True) g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\") g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() # 3. Complex Scatter Plot Customization def complex_scatter_plot(): plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", size=\\"size\\", style=\\"sex\\", palette=\\"deep\\", sizes=(20, 200)) plt.title(\'Scatter Plot of Tip vs Total Bill by Time, Size, and Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 4. Line Plot for Aggregated Data def line_plot_aggregated_data(): plt.figure(figsize=(10, 6)) sns.lineplot(data=tips, x=\\"day\\", y=\\"tip\\", ci=\\"sd\\", estimator=\'mean\', markers=True) plt.title(\'Average Tip Amount by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Tip\') plt.show() # 5. Customizing The FacetGrid def customized_facet_grid(): g = sns.FacetGrid(tips, col=\\"time\\", hue=\\"sex\\") g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Tip Amount\\") for ax in g.axes.flatten(): ax.axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.5) g.fig.suptitle(\'Faceted Scatter Plot with Customizations\', y=1.02) plt.show() # Running the visualization functions if __name__ == \\"__main__\\": scatter_plot_tip_vs_total_bill() faceted_scatter_plot() complex_scatter_plot() line_plot_aggregated_data() customized_facet_grid()"},{"question":"Problem: Unicode String Manipulation and File Operations You are required to write a Python function that performs several operations on Unicode strings and files, showcasing your understanding of Python\'s Unicode support. # Function Signature ```python def unicode_operations(input_str: str, filename: str, encoding: str = \'utf-8\') -> str: pass ``` # Inputs 1. `input_str` (str): A Unicode string containing various characters, potentially including special characters and emojis. 2. `filename` (str): The name of an input file containing Unicode text. 3. `encoding` (str): The encoding to be used for reading and writing the file (default is \'utf-8\'). # Outputs - A string that is a result of the following operations on `input_str` and the content of `filename`. # Constraints 1. The file specified by `filename` exists and is readable. 2. The encoding provided is a valid Python-supported encoding. # Steps and Requirements 1. **Normalization**: - Normalize `input_str` to the Normal Form C (NFC). 2. **Encoding/Decoding**: - Encode the normalized string using the provided encoding. - Handle encoding errors by replacing invalid characters with `U+FFFD`. 3. **File I/O**: - Read the content of the file using the provided encoding. - If encoding errors occur during reading, handle them by ignoring erroneous bytes. - Decode the file content into a Unicode string. 4. **Comparison and Case Folding**: - Compare the case-folded version of `input_str` and the file content (also case-folded) to check if they are equivalent. 5. **Output**: - Return a formatted string combining normalization result, encoded result, decoded file content, and the comparison result as follows: ```python result_str = f\\"Normalized: {normalized_input}nEncoded: {encoded_input}nDecoded File Content: {decoded_file_content}nComparison Result: {comparison_result}\\" return result_str ``` # Example Assume the file \\"example.txt\\" contains the following text: `\\"Gürzenichstraße\\"` ```python input_str = \\"Gürzenichstraße\\" filename = \\"example.txt\\" encoding = \\"utf-8\\" output = unicode_operations(input_str, filename, encoding) print(output) ``` # Expected Output ``` Normalized: Gürzenichstraße Encoded: b\'Gxc3xbcrzenichstraxc3x9fe\' Decoded File Content: Gürzenichstraße Comparison Result: True ``` Implement the function `unicode_operations` that achieves the specified requirements.","solution":"import unicodedata def unicode_operations(input_str: str, filename: str, encoding: str = \'utf-8\') -> str: # Step 1: Normalize input_str to NFC normalized_input = unicodedata.normalize(\'NFC\', input_str) # Step 2: Encode the normalized string using the provided encoding, replacing invalid chars with U+FFFD try: encoded_input = normalized_input.encode(encoding, errors=\'replace\') except LookupError: return \\"Invalid encoding provided.\\" # Step 3: Read the file content using the provided encoding, ignoring erroneous bytes try: with open(filename, \'r\', encoding=encoding, errors=\'ignore\') as file: decoded_file_content = file.read() except FileNotFoundError: return \\"File not found.\\" # Step 4: Compare case-folded versions of input_str and file content for equivalence comparison_result = normalized_input.casefold() == decoded_file_content.casefold() # Step 5: Return the formatted result string result_str = ( f\\"Normalized: {normalized_input}n\\" f\\"Encoded: {encoded_input}n\\" f\\"Decoded File Content: {decoded_file_content}n\\" f\\"Comparison Result: {comparison_result}\\" ) return result_str"},{"question":"# **Coding Assessment Question** **Objective**: Demonstrate your understanding and proficiency with the `ctypes` library in Python by writing code to load a shared library, access functions, and manage data types correctly. # **Problem Statement** You need to create a Python script that dynamically loads a C library, defines a function prototype, and interfaces with that function using the `ctypes` library. **Specifications**: 1. **Library Loading**: - Load the `libc` shared library on a Linux system using `ctypes`. 2. **Function Prototype**: - Define a prototype for the following C function using `ctypes`: ```c int add(int a, int b); ``` - This function adds two integers and returns the result. 3. **Function Call**: - Call this `add` function with the integers `10` and `20`, capture the result, and print it. 4. **Error Handling**: - Implement error checking to ensure that the function loading and invocation do not fail. - If the function or library is not loaded correctly, print an appropriate error message and handle the exception. **Input**: - No input is required as all values are hardcoded in the script. **Output**: - The result of the `add` function. **Constraints**: - Ensure your implementation is compatible with Python 3.10. **Example**: The following print statements should be executed based on the successful function execution: ``` Loading the library... Library loaded successfully. Calling the add function... The result of add(10, 20) is 30. ``` If there is an error: ``` Error loading the library: <error_message> ``` # **Instructions**: 1. Import necessary components from the `ctypes` library. 2. Load the `libc` shared library appropriately. 3. Define the function prototype for the `add` function. 4. Implement error handling for both library loading and function invocation. 5. Print the result if the function call is successful. Here is a template to get you started: ```python from ctypes import CDLL, c_int, c_long, POINTER, byref, c_char_p, c_void_p, create_string_buffer def load_library(): try: # Load the shared library libc = CDLL(\\"libc.so.6\\") print(\\"Library loaded successfully.\\") return libc except OSError as e: print(f\\"Error loading the library: {str(e)}\\") def get_add_function(libc): try: # Define function prototype add = libc.add add.argtypes = [c_int, c_int] add.restype = c_int return add except AttributeError as e: print(f\\"Error accessing function: {str(e)}\\") def main(): print(\\"Loading the library...\\") libc = load_library() if libc: print(\\"Defining the function prototype...\\") add = get_add_function(libc) if add: print(\\"Calling the add function...\\") result = add(10, 20) print(f\\"The result of add(10, 20) is {result}.\\") if __name__ == \\"__main__\\": main() ```","solution":"from ctypes import CDLL, c_int def load_library(): try: # Load the shared library libc = CDLL(\\"libc.so.6\\") print(\\"Library loaded successfully.\\") return libc except OSError as e: print(f\\"Error loading the library: {str(e)}\\") return None def get_add_function(libc): try: # Define function prototype add = libc.add add.argtypes = [c_int, c_int] add.restype = c_int return add except AttributeError as e: print(f\\"Error accessing function: {str(e)}\\") return None def main(): print(\\"Loading the library...\\") libc = load_library() if libc: print(\\"Defining the function prototype...\\") add = get_add_function(libc) if add: print(\\"Calling the add function...\\") result = add(10, 20) print(f\\"The result of add(10, 20) is {result}.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Coding Challenge: Working with Pandas DataFrames Objective: To demonstrate your understanding of fundamental and advanced concepts of pandas, you are required to create and manipulate a DataFrame based on specific instructions. Problem Statement: You are provided with sales records for a fictional company. The records are present in the form of dictionaries. You need to perform a series of operations to clean, manipulate, and analyze the data using pandas. Input: 1. A list of dictionaries where each dictionary represents a sales record. ```python sales_data = [ {\\"product_id\\": 1, \\"sale_amount\\": 150, \\"location\\": \\"New York\\", \\"date\\": \\"2023-01-15\\"}, {\\"product_id\\": 2, \\"sale_amount\\": 200, \\"location\\": \\"Los Angeles\\", \\"date\\": \\"2023-02-20\\"}, {\\"product_id\\": 1, \\"sale_amount\\": 250, \\"location\\": \\"New York\\", \\"date\\": \\"2023-03-30\\"}, {\\"product_id\\": 3, \\"sale_amount\\": 50, \\"location\\": \\"Chicago\\", \\"date\\": \\"2023-01-25\\"}, {\\"product_id\\": 2, \\"sale_amount\\": 300, \\"location\\": \\"Los Angeles\\", \\"date\\": \\"2023-01-10\\"}, {\\"product_id\\": 3, \\"sale_amount\\": 100, \\"location\\": \\"Chicago\\", \\"date\\": \\"2023-04-25\\"} ] ``` 2. A dictionary mapping product IDs to product names. ```python product_names = { 1: \\"Product_A\\", 2: \\"Product_B\\", 3: \\"Product_C\\" } ``` Expected Output: A pandas DataFrame that meets the following criteria: 1. Includes an additional column called `product_name` derived from the `product_names` mapping. 2. Adds another column called `quarter` that indicates the quarter of the year in which the sale was made. 3. Creates a summary column called `total_sales` per `location` and filters out locations with total sales less than 200. Function Signature: ```python def process_sales_data(sales_data: List[dict], product_names: dict) -> pd.DataFrame: pass ``` Detailed Instructions: 1. **Load Data into DataFrame:** Initialize a DataFrame using the `sales_data` list of dictionaries. 2. **Add Product Names:** Use the `product_names` dictionary to map each `product_id` to `product_name` and add it as a new column in the DataFrame. 3. **Extract Quarter:** Create a new column named `quarter` in the DataFrame that extracts the quarter (Q1, Q2, Q3, Q4) from the `date` column. 4. **Total Sales by Location:** Perform a group by operation on the `location` column to calculate the total sales in each location and add a `total_sales` column to the DataFrame. 5. **Filter Locations:** Remove the entries for locations where the `total_sales` are less than 200. 6. **Return Result:** The final DataFrame after performing all the above operations. Constraints: - All `sale_amount` values are non-negative. - The dates are in the form \\"YYYY-MM-DD\\". Example Output: After applying the transformations, the function might return the following DataFrame: ``` product_id sale_amount location date product_name quarter total_sales 0 2 200 Los Angeles 2023-02-20 Product_B Q1 500 1 2 300 Los Angeles 2023-01-10 Product_B Q1 500 2 1 150 New York 2023-01-15 Product_A Q1 400 3 1 250 New York 2023-03-30 Product_A Q1 400 ``` Notes: - This problem requires you to use various pandas operations such as DataFrame initialization, merging, column extraction and transformation, and filtering. - Ensure the function returns the DataFrame in the correct format with no missing values.","solution":"import pandas as pd from typing import List def process_sales_data(sales_data: List[dict], product_names: dict) -> pd.DataFrame: # Step 1: Load data into DataFrame df = pd.DataFrame(sales_data) # Step 2: Add product names df[\'product_name\'] = df[\'product_id\'].map(product_names) # Step 3: Extract quarter from date df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'quarter\'] = df[\'date\'].dt.to_period(\'Q\') # Step 4: Calculate total sales by location total_sales = df.groupby(\'location\')[\'sale_amount\'].transform(\'sum\') df[\'total_sales\'] = total_sales # Step 5: Filter locations with total sales less than 200 df = df[total_sales >= 200] # Return the final DataFrame return df"},{"question":"Objective Implement a graph transformation pass in PyTorch using the FX module that replaces a specific pattern in a computational graph with another pattern. Task Design a function that modifies a given `torch.fx.GraphModule` by replacing any subgraph that matches the following pattern: 1. An operation that multiplies two tensors (`torch.ops.aten.mul.Tensor`). 2. Follows immediately by an addition operation (`torch.ops.aten.add.Tensor`). The replacement pattern should be: 1. A subtraction operation (`torch.ops.aten.sub.Tensor`) of the two tensors (used in the initial multiplication). 2. Followed by a ReLU operation (`torch.ops.aten.relu.default`). Your implementation should utilize the `subgraph_rewriter` from `torch.fx`. Input and Output Formats - **Input:** A `torch.fx.GraphModule` containing a computational graph. - **Output:** A transformed `torch.fx.GraphModule` with the specified pattern replaced. # Constraints - Use the classes and utilities provided in the `torch.fx` module. - Ensure that all transformations are correctly applied, and the overall functionality of the computational graph remains valid. Performance Requirements - The transformation should be efficiently implemented and operate within a reasonable time frame for large graphs. Example Given the computational graph that includes the following operations: ```python x = torch.Tensor([1, 2, 3]) y = torch.Tensor([4, 5, 6]) z = torch.ops.aten.mul.Tensor(x, y) w = torch.ops.aten.add.Tensor(z, y) ``` This should be transformed to: ```python x = torch.Tensor([1, 2, 3]) y = torch.Tensor([4, 5, 6]) z = torch.ops.aten.sub.Tensor(x, y) w = torch.ops.aten.relu.default(z) ``` Solution Skeleton ```python import torch from torch.fx import GraphModule from torch.fx.subgraph_rewriter import replace_pattern_with_filters def replace_mul_add_with_sub_relu(graph_module: GraphModule) -> GraphModule: def pattern(x, y): z = torch.ops.aten.mul.Tensor(x, y) return torch.ops.aten.add.Tensor(z, y) def replacement(x, y): z = torch.ops.aten.sub.Tensor(x, y) return torch.ops.aten.relu.default(z) replace_pattern_with_filters(graph_module, pattern, replacement) return graph_module # Example usage: # gm = ... # some existing GraphModule # gm_transformed = replace_mul_add_with_sub_relu(gm) ``` Write your complete solution below.","solution":"import torch import torch.fx as fx from torch.fx.subgraph_rewriter import replace_pattern def replace_mul_add_with_sub_relu(graph_module: fx.GraphModule) -> fx.GraphModule: def pattern(x, y, z): a = torch.ops.aten.mul(x, y) b = torch.ops.aten.add(a, z) return b def replacement(x, y, z): a = torch.ops.aten.sub(x, y) b = torch.ops.aten.relu(a) return b replace_pattern(graph_module, pattern, replacement) return graph_module"},{"question":"**Title: Advanced Data Type Operations in Python** **Objective:** Assess your understanding of Python\'s built-in data types (`int`, `str`) and their operations including numerical operations, truth value testing, and sequence manipulation. **Task:** You are required to implement a function `analyze_data_operations` which takes two parameters: 1. `num_data` – an integer. 2. `text_data` – a string. The function should analyze the following aspects and return a dictionary containing the results of these analyses: 1. **Numeric Analysis:** - **Bit Length:** Calculate the number of bits needed to represent the integer `num_data` in binary and store it as \\"bit_length\\". - **Binary Representation:** Get the binary representation of `num_data` and store it as \\"binary\\". 2. **String Analysis:** - **Is Title:** Determine if `text_data` is title-cased and store the result as \\"is_title\\". - **Word Count:** Count the number of words in `text_data` assuming words are separated by spaces and store the result as \\"word_count\\". 3. **Combined Analysis:** - Check if the binary representation of `num_data` has `1` as its most significant bit and if `text_data` starts with a vowel. If both conditions are met, store `True` in \\"combined_check\\"; otherwise, store `False`. **Function Signature:** ```python def analyze_data_operations(num_data: int, text_data: str) -> dict: pass ``` **Input:** - `num_data`: An integer, `-10^7 <= num_data <= 10^7` - `text_data`: A string containing only alphanumeric characters and spaces, of length `0 <= len(text_data) <= 100`. **Output:** - A dictionary with the following keys and their corresponding values: - `\\"bit_length\\"`: Integer representing the bit length of `num_data`. - `\\"binary\\"`: String representing the binary form of `num_data`. - `\\"is_title\\"`: Boolean indicating if `text_data` is title-cased. - `\\"word_count\\"`: Integer representing the count of words in `text_data`. - `\\"combined_check\\"`: Boolean resulting from the combined analysis. **Example:** ```python # Example 1 num_data = 25 text_data = \\"Hello World\\" # Explanation: # - Bit Length of 25 = 5, Binary = \'0b11001\' # - \'Hello World\' is title-cased and has 2 words # - Combined check: Most significant bit in binary \'0b11001\' is 1 and \'Hello World\' does not start with a vowel. # Result: # {\'bit_length\': 5, \'binary\': \'0b11001\', \'is_title\': True, \'word_count\': 2, \'combined_check\': False} # Output analyze_data_operations(25, \\"Hello World\\") == {\'bit_length\': 5, \'binary\': \'0b11001\', \'is_title\': True, \'word_count\': 2, \'combined_check\': False} # Example 2 num_data = -5 text_data = \\"an Example Text\\" # Explanation: # - Bit Length of -5 = 3, Binary = \'-0b101\' # - \'an Example Text\' is title-cased and has 3 words # - Combined check: Most significant bit in binary \'-0b101\' is 1 and \'an Example Text\' starts with a vowel. # Result: # {\'bit_length\': 3, \'binary\': \'-0b101\', \'is_title\': True, \'word_count\': 3, \'combined_check\': True} # Output analyze_data_operations(-5, \\"an Example Text\\") == {\'bit_length\': 3, \'binary\': \'-0b101\', \'is_title\': True, \'word_count\': 3, \'combined_check\': True} ``` **Constraints:** - Ensure the function handles both positive and negative integers correctly. - Do not use any external libraries; rely only on Python\'s built-in functions and methods. # Solution Template: ```python def analyze_data_operations(num_data: int, text_data: str) -> dict: result = {} # Numeric Analysis result[\\"bit_length\\"] = num_data.bit_length() result[\\"binary\\"] = bin(num_data) # String Analysis result[\\"is_title\\"] = text_data.istitle() result[\\"word_count\\"] = len(text_data.split()) # Combined Analysis most_significant_bit_is_1 = result[\\"binary\\"][2] == \'1\' if num_data > 0 else result[\\"binary\\"][3] == \'1\' starts_with_vowel = text_data[0].lower() in \'aeiou\' if text_data else False result[\\"combined_check\\"] = most_significant_bit_is_1 and starts_with_vowel return result ``` **Note:** This task requires a good understanding of Python built-in functions and handling edge cases like empty strings or zero as input.","solution":"def analyze_data_operations(num_data: int, text_data: str) -> dict: result = {} # Numeric Analysis result[\\"bit_length\\"] = num_data.bit_length() result[\\"binary\\"] = bin(num_data) # String Analysis result[\\"is_title\\"] = text_data.istitle() result[\\"word_count\\"] = len(text_data.split()) # Combined Analysis if num_data == 0: most_significant_bit_is_1 = False else: most_significant_bit_is_1 = result[\\"binary\\"][2] == \'1\' if num_data > 0 else result[\\"binary\\"][3] == \'1\' starts_with_vowel = text_data[0].lower() in \'aeiou\' if text_data else False result[\\"combined_check\\"] = most_significant_bit_is_1 and starts_with_vowel return result"},{"question":"# Question Overview You have been given some functions and macros for managing reference counts of Python objects provided by the package documented above. These functions ensure the proper creation, usage, and cleanup of Python objects at the C level. For this assignment, you will create a mock Python class that simulates the behavior of these reference-counting functions at the Python level. This exercise will help you to understand how reference counting and garbage collection might work under the hood in Python. # Task: Implement a Reference Counting System in Python Description Create a Python class named `PyObjectSim` that will simulate the reference counting mechanism. Your class should manage reference counts for its instances and replicate the behavior of the reference counting functions outlined in the documentation: 1. `inc_ref()`: Increments the reference count. 2. `dec_ref()`: Decrements the reference count and deletes the object if the reference count reaches zero. 3. `xinc_ref()`: Increments the reference count if the object is not `None`. 4. `xdec_ref()`: Decrements the reference count and deletes the object if the reference count reaches zero (if the object is not `None`). 5. `new_ref()`: Creates a new reference to the object. 6. `xnew_ref()`: Creates a new reference to the object if it is not `None`. Requirements 1. Define a class `PyObjectSim` with an initializer that sets the initial reference count to 1. 2. Implement the methods `inc_ref()`, `dec_ref()`, `xinc_ref()`, `xdec_ref()`, `new_ref()`, and `xnew_ref()` with the described behaviors. 3. Ensure that decrementing the reference count to 0 marks the object for deletion (you can print a message for simulation purposes to indicate the object is being deleted). Example Usage ```python class PyObjectSim: def __init__(self): self.ref_count = 1 def inc_ref(self): self.ref_count += 1 def dec_ref(self): self.ref_count -= 1 if self.ref_count == 0: print(f\\"{self} is being deleted\\") def xinc_ref(self): if self is not None: self.inc_ref() def xdec_ref(self): if self is not None: self.dec_ref() def new_ref(self): self.inc_ref() return self def xnew_ref(self): if self is not None: return self.new_ref() return None # Test Cases obj = PyObjectSim() ref1 = obj.new_ref() # Increasing the ref count ref2 = obj.new_ref() # Increasing the ref count again ref1.dec_ref() # Decreasing ref count ref2.dec_ref() # Decreasing ref count (Now it should indicate being deleted) ``` Constraints 1. The implementation should handle multiple references correctly. 2. It should handle the scenario of incrementing/decrementing `None`. # Deliverables 1. A Python script implementing the `PyObjectSim` class and its methods. 2. A set of test cases demonstrating the correct behavior of the class.","solution":"class PyObjectSim: def __init__(self): self.ref_count = 1 def inc_ref(self): self.ref_count += 1 def dec_ref(self): self.ref_count -= 1 if self.ref_count == 0: print(f\\"{self} is being deleted\\") def xinc_ref(self): if self is not None: self.inc_ref() def xdec_ref(self): if self is not None: self.dec_ref() def new_ref(self): self.inc_ref() return self def xnew_ref(self): if self is not None: return self.new_ref() return None"},{"question":"# Advanced Violin Plot Analysis with Seaborn In this exercise, you are required to demonstrate the depth of your understanding of seaborn\'s violinplot functionality by performing an advanced analysis on the Titanic dataset. The steps and requirements are as follows: Objective: Create a series of visualizations using `seaborn` to analyze the age distribution of passengers on the Titanic based on different categories. The purpose is to extract meaningful insights from the data through the following steps: Dataset: Use the Titanic dataset available via seaborn with `sns.load_dataset(\\"titanic\\")`. Tasks: 1. **Single Violin Plot**: Create a standard violin plot to show the distribution of passengers\' ages. 2. **Grouped Violin Plot**: - Create a violin plot grouped by the class (`class`). - Create another violin plot grouped by the survival status (`alive`). 3. **Complex Violin Plot**: - Create a violin plot of the age distribution by class, further split by survival status (`alive`), and display only the quartiles. 4. **Normalized Violin Plot**: - Create a violin plot of the age distribution by deck (`deck`), normalizing the width of each violin to represent the number of observations. 5. **Customized Smoothing**: - Create a violin plot of the age distribution by survival status (`alive`) with a custom smoothing parameter `bw_adjust=0.5`. 6. **Use Native Scale**: - Create a violin plot using the `native_scale=True` parameter to preserve the original scale on the age axis. Group by age rounded to the nearest decade and fare (`fare`). Detailed Instructions: 1. Ensure each plot is plotted using a `subplot` grid where appropriate. 2. Titles must be provided for each subplot to explain what the plot represents. 3. Use appropriate colors for better differentiation in the plots. 4. Set the theme using `sns.set_theme(style=\\"whitegrid\\")` at the start of your code. 5. Each subplot should be well-labeled for clarity. Expected Output: You should submit a python script or Jupyter notebook that: - Loads the dataset and performs all the plotting tasks. - Displays the resultant plots, organized neatly, demonstrating your understanding and ability to customize violin plots using seaborn. Note: Each aspect of the output will be assessed, from code correctness to the clarity and style of the visualizations. Additional Constraints: - Ensure your code is optimized for readability and performance. - Handle any missing data appropriately within the plotting functions to ensure plots render smoothly. Good luck and happy coding!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_violin_plots(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Create subplots fig, axes = plt.subplots(3, 2, figsize=(15, 20)) # Single Violin Plot sns.violinplot(ax=axes[0, 0], x=titanic[\\"age\\"]) axes[0, 0].set_title(\\"Single Violin Plot of Age Distribution\\") # Grouped Violin Plot by Class sns.violinplot(ax=axes[0, 1], x=\\"class\\", y=\\"age\\", data=titanic) axes[0, 1].set_title(\\"Violin Plot of Age Distribution by Class\\") # Grouped Violin Plot by Survival Status sns.violinplot(ax=axes[1, 0], x=\\"alive\\", y=\\"age\\", data=titanic) axes[1, 0].set_title(\\"Violin Plot of Age Distribution by Survival Status\\") # Complex Violin Plot by Class and Survival Status, displaying only quartiles sns.violinplot(ax=axes[1, 1], x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic, split=True, inner=\\"quart\\") axes[1, 1].set_title(\\"Violin Plot of Age by Class and Survival Status\\") # Normalized Violin Plot by Deck sns.violinplot(ax=axes[2, 0], x=\\"deck\\", y=\\"age\\", data=titanic, scale=\\"count\\") axes[2, 0].set_title(\\"Normalized Violin Plot of Age Distribution by Deck\\") # Customized Smoothing - Violin Plot by Survival Status with custom bw_adjust sns.violinplot(ax=axes[2, 1], x=\\"alive\\", y=\\"age\\", data=titanic, bw_adjust=0.5) axes[2, 1].set_title(\\"Violin Plot of Age Distribution by Survival Status with custom smoothing\\") # Adjust layout and show plot plt.tight_layout() plt.show() # Uncomment the following line to execute the function and see the plots # create_violin_plots()"},{"question":"Objective Design and implement a Python function that interacts with the file system using the `os` module. The task will focus on creating directories, handling files and paths, and managing file permissions and attributes. Problem Statement You are tasked with developing a function that sets up a new project directory structure and manages file permissions. The function should: 1. Create a directory structure for a new project, including subdirectories for source files, tests, and documentation. 2. Create a README file in the project root directory. 3. Set specific permissions for the subdirectories and README file. 4. Use environment variables to customize certain aspects of the directory structure. Function Signature ```python import os def setup_project_structure(root_path: str, project_name: str, readme_content: str) -> None: Sets up the directory structure for a new project and manages file permissions. Parameters: - root_path (str): The root path where the project directory will be created. - project_name (str): The name of the new project. - readme_content (str): The content to be written in the README file. Returns: - None pass ``` Requirements 1. **Directory Structure**: - Create a main project directory named `project_name` under `root_path`. - Create the following subdirectories within the main project directory: - `src/` for source files. - `tests/` for test files. - `docs/` for documentation. 2. **README File**: - Create a file named `README.md` in the main project directory. - Write the provided `readme_content` to this file. 3. **Permissions**: - Set the permissions for the `src/`, `tests/`, and `docs/` directories to `rwxr-xr-x` (755). - Set the permissions for the `README.md` file to `rw-r--r--` (644). 4. **Environment Variable**: - Use an environment variable named `PROJECT_AUTHOR` to add an additional line at the end of the `README.md` file. If the variable is not set, add \\"Author: Unknown\\". 5. **Error Handling**: - Handle any potential filesystem-related exceptions that might occur during the execution of the function. Constraints - You can assume that the `root_path` exists and is writable. - Directory and file names consist of alphanumeric characters and underscores. - All permissions should be in octal format. Example Usage ```python # Example environment variable setup os.environ[\'PROJECT_AUTHOR\'] = \'Jane Doe\' # Example function call setup_project_structure(\'/projects\', \'my_new_project\', \'# My New ProjectnThis is a sample project.\') ``` Example Output This function does not return any value but will create a directory structure as specified, with appropriate content and permissions. Additional Notes - Include relevant imports within your function. - Document your code with comments explaining each major step. Testing The implementation should be tested to ensure that: - The directory structure is correctly created. - Files are correctly generated with the expected content and permissions. - The function handles the absence of the `PROJECT_AUTHOR` environment variable gracefully.","solution":"import os def setup_project_structure(root_path: str, project_name: str, readme_content: str) -> None: Sets up the directory structure for a new project and manages file permissions. Parameters: - root_path (str): The root path where the project directory will be created. - project_name (str): The name of the new project. - readme_content (str): The content to be written in the README file. Returns: - None # Construct the full path for the project directory project_dir = os.path.join(root_path, project_name) # Create the directory structure os.makedirs(os.path.join(project_dir, \'src\'), exist_ok=True) os.makedirs(os.path.join(project_dir, \'tests\'), exist_ok=True) os.makedirs(os.path.join(project_dir, \'docs\'), exist_ok=True) # Create the README file and write the content readme_path = os.path.join(project_dir, \'README.md\') with open(readme_path, \'w\') as readme_file: readme_file.write(readme_content) # Add author information from environment variable author = os.environ.get(\'PROJECT_AUTHOR\', \'Unknown\') readme_file.write(f\'nnAuthor: {author}\') # Set permissions os.chmod(os.path.join(project_dir, \'src\'), 0o755) os.chmod(os.path.join(project_dir, \'tests\'), 0o755) os.chmod(os.path.join(project_dir, \'docs\'), 0o755) os.chmod(readme_path, 0o644)"},{"question":"**Task: Asynchronous Task Manager using `asyncio`** **Objective:** Implement an asynchronous task manager that can schedule and run multiple tasks concurrently, handle timeouts and cancellations, collect results, and gracefully manage task completions using Python\'s `asyncio` module. **Background:** You have been provided with several utilities and functions within the `asyncio` module that enable running concurrent tasks, managing different awaitables such as futures and coroutines, and handling various scenarios such as shielding from cancellation and timeouts. **Problem Statement:** You need to implement three main asynchronous functions: 1. `async def task(identifier: str, delay: int) -> str`: - This function simulates a task by sleeping for a given delay and then returning a message indicating the completion of the task. 2. `async def task_manager(tasks: List[Tuple[str, int]], timeout: int) -> Tuple[List[str], List[asyncio.Task]]`: - This function takes a list of tasks to run and a timeout value. - It schedules and runs all tasks concurrently using `asyncio.gather()`. - If tasks don\'t complete within the specified timeout, it should handle the timeout gracefully, cancelling the tasks that didn\'t complete. - It returns a tuple containing a list of results from completed tasks and a list of tasks that were pending (not completed). 3. `def run_task_manager(task_details: List[Tuple[str, int]], timeout: int)`: - This is a synchronous function that sets up and runs the `task_manager` within an asyncio event loop. - It prints the results returned by the `task_manager` function. **Instructions:** 1. Define the `task` function that uses `asyncio.sleep()` to simulate the delay. 2. Define the `task_manager` function that manages concurrent tasks, handles timeouts using `asyncio.wait_for()`, and returns appropriate results. 3. Define the `run_task_manager` function to run the event loop and print the results. **Constraints:** - Each task must sleep for the specified delay before returning its result. - The number of tasks, timeouts, and delays are defined by the inputs. - Use `asyncio.gather()` for running the tasks concurrently. - Handle scenarios where tasks exceed the specified timeout. **Example:** ```python import asyncio from typing import List, Tuple async def task(identifier: str, delay: int) -> str: await asyncio.sleep(delay) return f\\"Task {identifier} completed after {delay} seconds.\\" async def task_manager(tasks: List[Tuple[str, int]], timeout: int) -> Tuple[List[str], List[asyncio.Task]]: task_list = [asyncio.create_task(task(identifier, delay)) for identifier, delay in tasks] completed, pending = await asyncio.wait(task_list, timeout=timeout, return_when=asyncio.ALL_COMPLETED) results = [] for t in completed: results.append(await t) return results, list(pending) def run_task_manager(task_details: List[Tuple[str, int]], timeout: int): results, pending = asyncio.run(task_manager(task_details, timeout)) print(\\"Results:\\") for result in results: print(result) print(\\"Pending Tasks:\\") for task in pending: print(task) # Example usage tasks = [(\\"A\\", 2), (\\"B\\", 3), (\\"C\\", 5)] timeout = 4 run_task_manager(tasks, timeout) ``` In this example, the `task_manager` function schedules three tasks with delays of 2, 3, and 5 seconds respectively, but since a timeout of 4 seconds is provided, only two tasks (A and B) will likely complete. The remaining task (C) will be pending. **End of Question**","solution":"import asyncio from typing import List, Tuple async def task(identifier: str, delay: int) -> str: await asyncio.sleep(delay) return f\\"Task {identifier} completed after {delay} seconds.\\" async def task_manager(tasks: List[Tuple[str, int]], timeout: int) -> Tuple[List[str], List[asyncio.Task]]: task_list = [asyncio.create_task(task(identifier, delay)) for identifier, delay in tasks] try: # Using asyncio.gather directly with timeout control via wait_for done, pending = await asyncio.wait(task_list, timeout=timeout, return_when=asyncio.ALL_COMPLETED) except asyncio.TimeoutError: done, pending = [], task_list results = [] for t in done: if not t.cancelled(): results.append(await t) # Cancel pending tasks for t in pending: t.cancel() return results, list(pending) def run_task_manager(task_details: List[Tuple[str, int]], timeout: int): results, pending = asyncio.run(task_manager(task_details, timeout)) print(\\"Results:\\") for result in results: print(result) print(\\"Pending Tasks:\\") for task in pending: print(task) # Example usage # tasks = [(\\"A\\", 2), (\\"B\\", 3), (\\"C\\", 5)] # timeout = 4 # run_task_manager(tasks, timeout)"},{"question":"**Question: Advanced Data Class Manipulation** You are tasked with creating a data class that represents a `Book` in a library system. Each `Book` should store the following information: - Title (string) - Author (string) - ISBN (string) - Copies Available (integer) Additionally, you need to implement functionality to manage book borrowing and returns. Borrowing a book decreases the `Copies Available` by one, and returning a book increases it by one. You should also ensure that the number of available copies cannot go below zero. Further, you need to ensure the following: 1. After each operation (borrow or return), a log entry is generated automatically indicating the action performed and the current state of the book. 2. Implement a class method to create a `Book` instance from a dictionary object containing book details. 3. Implement methods for comparing two `Book` objects based on their ISBN, ensuring equality checks are based on ISBN only. Implement the following: 1. A data class `Book` with the described attributes and functionalities. 2. A method `borrow_book(self)` to handle borrowing books. 3. A method `return_book(self)` to handle returning books. 4. A class method `from_dict(cls, book_dict: dict)` to create a new `Book` instance from a dictionary. 5. Equality (`__eq__`) and inequality (`__ne__`) methods based on ISBN. **Constraints:** - Every book must have a unique ISBN. - The log entry should be printed in the format: \\"Action: <borrow/return>, Title: <title>, Copies Available: <count>\\" **Input:** - The log entry is generated internally and doesn\'t require external input. **Output:** - The log entry automatically printed after each operation. - Methods should return appropriate success statuses (return `True` if successful, otherwise `False`). ```python from dataclasses import dataclass, field @dataclass class Book: title: str author: str isbn: str copies_available: int def borrow_book(self) -> bool: # Implement the borrow_book logic here pass def return_book(self) -> bool: # Implement the return_book logic here pass @classmethod def from_dict(cls, book_dict: dict): # Implement the from_dict method here pass def __eq__(self, other): # Implement the equality check here pass def __ne__(self, other): # Implement the inequality check here pass ``` **Examples:** ```python book_details = { \\"title\\": \\"Python 101\\", \\"author\\": \\"John Doe\\", \\"isbn\\": \\"1234567890\\", \\"copies_available\\": 5 } book = Book.from_dict(book_details) book.borrow_book() # Action: borrow, Title: Python 101, Copies Available: 4 book.return_book() # Action: return, Title: Python 101, Copies Available: 5 another_book = Book(\\"Python 101\\", \\"John Doe\\", \\"1234567890\\", 5) assert book == another_book # Equality based on ISBN yet_another_book = Book(\\"Advanced Python\\", \\"Jane Doe\\", \\"0987654321\\", 3) assert book != yet_another_book # Inequality based on ISBN ```","solution":"from dataclasses import dataclass, field @dataclass class Book: title: str author: str isbn: str copies_available: int def borrow_book(self) -> bool: if self.copies_available > 0: self.copies_available -= 1 self._log_action(\\"borrow\\") return True else: self._log_action(\\"borrow (failed, no copies available)\\") return False def return_book(self) -> bool: self.copies_available += 1 self._log_action(\\"return\\") return True @classmethod def from_dict(cls, book_dict: dict): return cls( title = book_dict[\'title\'], author = book_dict[\'author\'], isbn = book_dict[\'isbn\'], copies_available = book_dict[\'copies_available\'] ) def __eq__(self, other): if isinstance(other, Book): return self.isbn == other.isbn return False def __ne__(self, other): return not self.__eq__(other) def _log_action(self, action: str): print(f\\"Action: {action}, Title: {self.title}, Copies Available: {self.copies_available}\\")"},{"question":"**Objective:** Demonstrate understanding and handling of asyncio exceptions in Python. **Problem Statement:** You are tasked with writing an asynchronous function `fetch_url` that fetches content from a list of URLs concurrently. The function should handle different asyncio-specific exceptions appropriately. Here are the main requirements: 1. If the fetching of any URL exceeds a given timeout, `fetch_url` should log an appropriate message and move on to the next URL. 2. If an operation is cancelled, it should clean up correctly and then propagate the `CancelledError`. 3. If any other unexpected exception occurs (like `asyncio.InvalidStateError`, `asyncio.SendfileNotAvailableError`, `asyncio.IncompleteReadError`, or `asyncio.LimitOverrunError`), log an appropriate message and continue with the next URL. # Function Signature ```python import asyncio async def fetch_url(urls: list, timeout: int) -> None: Fetches content from a list of URLs using asyncio. Handles asyncio-specific exceptions appropriately. Parameters: urls (list): A list of URLs to fetch content from. timeout (int): The maximum time (in seconds) to wait for each URL. Returns: None pass ``` # Input - `urls` (list): A list of URL strings to fetch content from. - `timeout` (int): The maximum time in seconds to wait for each URL to respond before timing out. # Output - None. The function should log appropriate messages when exceptions are encountered. # Example Usage Assume `fetch_content_from_url(url)` is an existing asynchronous function that fetches the content of a URL: ```python import aiohttp import asyncio async def fetch_content_from_url(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() # Sample URL list urls = [ \\"https://example.com\\", \\"https://httpbin.org/delay/3\\", # Delayed response URL \\"https://thisurldoesnotexist.com\\", # Non-existent URL ] # Usage await fetch_url(urls, 2) # Setting timeout to 2 seconds ``` # Instructions 1. Implement the `fetch_url` function to handle the three main exception scenarios above. 2. Use appropriate asynchronous techniques and exception handling as per Python asyncio guidelines. 3. Ensure proper cleanup and logging of messages for different exceptions. # Constraints - The function should use asyncio capabilities for concurrency. - The function should handle exceptions specific to asyncio with clear, concise, and correct log messages. # Performance Considerations - The function should aim to fetch all URLs concurrently and efficiently handle exceptions without terminating prematurely.","solution":"import asyncio import aiohttp import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def fetch_content_from_url(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_url(urls: list, timeout: int) -> None: Fetches content from a list of URLs using asyncio. Handles asyncio-specific exceptions appropriately. Parameters: urls (list): A list of URLs to fetch content from. timeout (int): The maximum time (in seconds) to wait for each URL. Returns: None async def safe_fetch(url): try: return await asyncio.wait_for(fetch_content_from_url(url), timeout=timeout) except asyncio.TimeoutError: logger.error(f\\"TimeoutError: Fetching {url} exceeded {timeout} seconds.\\") except asyncio.CancelledError: logger.error(f\\"CancelledError: Fetching {url} was cancelled.\\") raise except (asyncio.InvalidStateError, aiohttp.ClientError, asyncio.IncompleteReadError, asyncio.LimitOverrunError) as e: logger.error(f\\"{type(e).__name__}: An error occurred while fetching {url}.\\") tasks = [safe_fetch(url) for url in urls] await asyncio.gather(*tasks, return_exceptions=True)"},{"question":"**Question: Implement and Optimize a Parallelized Random Forest Classifier using Scikit-learn** As a data scientist, you are tasked with implementing a Random Forest classifier to predict whether patients have diabetes based on the provided dataset. However, due to the large size of the dataset, training time and resource consumption are significant concerns. Therefore, you need to optimize your implementation to utilize parallelism effectively. **Objective:** Implement a Random Forest classifier using scikit-learn and optimize it for parallel performance by configuring `n_jobs` and relevant environment variables. **Instructions:** 1. **Data Loading:** Use the `load_diabetes` dataset from scikit-learn\'s datasets module. Split this dataset into training (80%) and testing (20%) sets. 2. **Model Implementation:** Implement a `RandomForestClassifier` with `n_estimators` set to 100 and `random_state` set to 42. 3. **Parallelism Configuration:** - Set `n_jobs` to the number of available CPU cores on your machine. - Configure the number of threads for OpenMP and BLAS/LAPACK routines using environment variables. 4. **Performance Evaluation:** Train your model and evaluate its performance using accuracy on the test set. **Requirements:** - Input: None (the script should load the dataset internally). - Output: The accuracy of the Random Forest classifier on the test set. - Implement your solution in a function called `parallel_rf_classifier()`. - Consider potential oversubscription as described in the documentation and implement a strategy to avoid it. **Constraints:** - You must use `n_jobs` to control joblib\'s parallelism. - Use environment variables or `threadpoolctl` to control the number of threads for BLAS and OpenMP. **Hints:** - Use `os.environ` to set environment variables in Python. - Use `joblib.parallel_backend` context manager to fine-tune parallelism if necessary. **Sample Function Skeleton:** ```python import os from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def parallel_rf_classifier(): # Set environment variables to control OpenMP and BLAS parallelism os.environ[\'OMP_NUM_THREADS\'] = \'1\' os.environ[\'MKL_NUM_THREADS\'] = \'1\' # Load dataset and split into training and testing sets data = load_diabetes() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Configure and train the RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) with joblib.parallel_backend(\'loky\'): rf.fit(X_train, y_train) # Evaluate the model y_pred = rf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') parallel_rf_classifier() ``` **Deliverables:** Submit your implementation of the `parallel_rf_classifier` function. Ensure it demonstrates effective parallelism usage and meets the performance requirements as specified.","solution":"import os from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import multiprocessing def parallel_rf_classifier(): # Set environment variables to control OpenMP and BLAS parallelism os.environ[\'OMP_NUM_THREADS\'] = \'1\' os.environ[\'MKL_NUM_THREADS\'] = \'1\' # Determine the number of CPU cores num_cores = multiprocessing.cpu_count() # Load dataset and split into training and testing sets data = load_diabetes() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Configure and train the RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=num_cores) rf.fit(X_train, y_train) # Evaluate the model y_pred = rf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Using Seaborn\'s `objects` module, create a series of subplots that visualize different pairwise relationships and demonstrate faceting as well as customized labeling. Use the `mpg` dataset available in Seaborn. Function Implementation ```python import seaborn.objects as so from seaborn import load_dataset def plot_customized_relationships(): Create a series of plots using Seaborn objects with the following specifications: 1. Display scatter plots (Dots) showing the relationship between the \'mpg\' variable and the following variables: \'displacement\', \'weight\', \'horsepower\', \'acceleration\'. 2. Arrange the plots in a 2x2 grid using the wrap feature. 3. Customize titles for each subplot to reflect the x and y-axis variables. 4. Additionally, create rows of plots showing the pairwise relationship between \'weight\' and \'horsepower\', \'weight\' and \'acceleration\', faceted by \'origin\'. 5. Customize the title and axes labels to be more descriptive. The function should display the plots with the described customizations. # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the first set of plots: relationship between \'mpg\' and 4 other variables in a wrapped grid plot1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"acceleration\\"], wrap=2) .label( x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", x2=\\"Horsepower\\", x3=\\"Acceleration\\", y=\\"Miles Per Gallon (MPG)\\" ) .add(so.Dots()) ) # Create the second set of plots: pairwise relationships faceted by \'origin\' plot2 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(row=\\"origin\\") .label( x=\\"Weight (lb)\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\", ) .add(so.Dots()) ) # Display the plots plot1.show() plot2.show() ``` Explanation - **Pairing and Wrapping**: - The first part of the function pairs the `mpg` variable against `displacement`, `weight`, `horsepower`, and `acceleration`. - It wraps these subplots into a 2x2 grid for a structured display. - Customized labels are added to make the plots more descriptive. - **Faceting**: - The second part of the function shows the relationship between `weight` and `horsepower`, and `weight` and `acceleration`. - These are further faceted by the `origin` variable, resulting in three rows: one for each origin category. - Custom labels are added for clarity. By implementing this function, you will demonstrate a comprehensive understanding of pairing, faceting, and customizing plots using the seaborn objects module.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_customized_relationships(): Create a series of plots using Seaborn objects with the following specifications: 1. Display scatter plots (Dots) showing the relationship between the \'mpg\' variable and the following variables: \'displacement\', \'weight\', \'horsepower\', \'acceleration\'. 2. Arrange the plots in a 2x2 grid using the wrap feature. 3. Customize titles for each subplot to reflect the x and y-axis variables. 4. Additionally, create rows of plots showing the pairwise relationship between \'weight\' and \'horsepower\', \'weight\' and \'acceleration\', faceted by \'origin\'. 5. Customize the title and axes labels to be more descriptive. The function should display the plots with the described customizations. # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the first set of plots: relationship between \'mpg\' and 4 other variables in a wrapped grid plot1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"acceleration\\"], wrap=2) .label( x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", x2=\\"Horsepower\\", x3=\\"Acceleration\\", y=\\"Miles Per Gallon (MPG)\\" ) .add(so.Dots()) ) # Create the second set of plots: pairwise relationships faceted by \'origin\' plot2 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(row=\\"origin\\") .label( x=\\"Weight (lb)\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\", ) .add(so.Dots()) ) # Display the plots plot1.show() plot2.show()"},{"question":"Advanced Plotting with Seaborn You are given a dataset containing information about different species of penguins. This dataset includes columns such as `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, `sex`. Your task is to write a Python function using the seaborn library to perform the following: 1. Load the penguins dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. Create a scatter plot of `bill_length_mm` against `bill_depth_mm`, with points colored based on the `species`. 3. Customize the plot by: - Setting the x-axis label to \\"Bill Length (mm)\\". - Setting the y-axis label to \\"Bill Depth (mm)\\". - Setting the plot title to \\"Bill Dimensions by Penguin Species\\". 4. Faceting the plot by `sex`, with each subplot having its title indicating the sex in uppercase. 5. Adding a legend with the title \\"Species\\". # Function Signature ```python def penguin_plot(): pass ``` # Inputs - No inputs are required for this function. # Outputs - The function should display the customized plot. # Constraints - Use seaborn\'s objects interface to achieve the plotting. - You should not use any pre-defined libraries for dataset manipulation outside of seaborn and pandas. # Example Upon executing the function `penguin_plot()`, it should display the plot as described, with the necessary customizations applied. # Performance Requirements - The function should efficiently handle the penguins dataset provided by seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def penguin_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot with seaborn\'s FacetGrid g = sns.FacetGrid(penguins, col=\\"sex\\", hue=\\"species\\", height=5, aspect=1) g.map(sns.scatterplot, \\"bill_length_mm\\", \\"bill_depth_mm\\").add_legend(title=\\"Species\\") # Customize plot properties g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.fig.suptitle(\\"Bill Dimensions by Penguin Species\\", y=1.01) # Adjust y for title position # Annotate each subplot with the sex in uppercase for ax, sex in zip(g.axes.flat, penguins[\\"sex\\"].unique()): ax.set_title(sex.upper()) plt.show()"},{"question":"Objective The goal of this assessment is to evaluate your ability to manipulate and analyze data efficiently using pandas, covering both fundamental and advanced functionalities. Problem Statement You are provided with a dataset in the form of a DataFrame which contains information about several students\' scores in three subjects: `Math`, `Science`, and `English`. The DataFrame is indexed by the student names. Given the following DataFrame `df`: ```python import pandas as pd data = { \\"Math\\": [56, 78, 45, 90, 75, 60, 76, 80], \\"Science\\": [66, 80, 50, 95, 70, 65, 85, 88], \\"English\\": [59, 77, 58, 85, 78, 62, 80, 83], } students = [\\"John\\", \\"Jane\\", \\"Tom\\", \\"Lucy\\", \\"Peter\\", \\"Laura\\", \\"Nina\\", \\"Gary\\"] df = pd.DataFrame(data, index=students) ``` Task 1. **Calculate Statistics**: - Compute the mean and standard deviation for each subject. - Compute the overall average score for each student. 2. **Ranking**: - Rank the students based on their overall average scores in descending order. - Rank the students within each subject based on their scores in ascending order. 3. **Comparison**: - Compare the scores between subjects to find out: - Students whose `Math` score is greater than `Science` and `English`. - Students who scored the same across all three subjects. 4. **Handling Missing Data**: - Simulate missing data by setting the `English` score of \\"Tom\\" to `NaN`. - Fill the missing score with the average `English` score. 5. **Data Summarization**: - Summarize the data with descriptive statistics (mean, median, min, max, standard deviation) for each subject. - Display the information summary of the DataFrame. Output Implement the function `manipulate_scores(df: pd.DataFrame) -> pd.DataFrame` which will perform the above tasks and return the manipulated DataFrame. Additionally, the function should print the answers to steps 1 to 5. Constraints - Use pandas for all data manipulations. - Assume the DataFrame will always have columns `Math`, `Science`, and `English`, and the index will be student names. Example Output: ```python def manipulate_scores(df: pd.DataFrame) -> pd.DataFrame: # Task 1 - Calculating Statistics mean = df.mean() std_dev = df.std() overall_avg = df.mean(axis=1) print(\\"Mean Scores:n\\", mean) print(\\"Standard Deviation:n\\", std_dev) print(\\"Overall Averages:n\\", overall_avg) # Task 2 - Ranking rank_overall = overall_avg.sort_values(ascending=False) rank_math = df[\\"Math\\"].rank() rank_science = df[\\"Science\\"].rank() rank_english = df[\\"English\\"].rank() print(\\"Rank Overall:n\\", rank_overall) print(\\"Rank Math:n\\", rank_math) print(\\"Rank Science:n\\", rank_science) print(\\"Rank English:n\\", rank_english) # Task 3 - Comparison math_greater = df[(df[\\"Math\\"] > df[\\"Science\\"]) & (df[\\"Math\\"] > df[\\"English\\"])].index.tolist() same_scores = df[(df[\\"Math\\"] == df[\\"Science\\"]) & (df[\\"Math\\"] == df[\\"English\\"])].index.tolist() print(\\"Math > Science and English:n\\", math_greater) print(\\"Same scores across subjects:n\\", same_scores) # Task 4 - Missing Data Handling df.at[\\"Tom\\", \\"English\\"] = None df[\\"English\\"].fillna(df[\\"English\\"].mean(), inplace=True) # Task 5 - Data Summarization summary = df.describe() print(\\"Data Summary:n\\", summary) print(\\"Information:n\\", df.info()) return df # Testing the function df = manipulate_scores(df) ``` **Example of calling the function:** ```python data = { \\"Math\\": [56, 78, 45, 90, 75, 60, 76, 80], \\"Science\\": [66, 80, 50, 95, 70, 65, 85, 88], \\"English\\": [59, 77, 58, 85, 78, 62, 80, 83], } students = [\\"John\\", \\"Jane\\", \\"Tom\\", \\"Lucy\\", \\"Peter\\", \\"Laura\\", \\"Nina\\", \\"Gary\\"] df = pd.DataFrame(data, index=students) manipulated_df = manipulate_scores(df) print(manipulated_df) ```","solution":"import pandas as pd def manipulate_scores(df: pd.DataFrame) -> pd.DataFrame: # Task 1 - Calculating Statistics mean_scores = df.mean() std_dev_scores = df.std() overall_avg_scores = df.mean(axis=1) print(\\"Mean Scores:n\\", mean_scores) print(\\"Standard Deviation:n\\", std_dev_scores) print(\\"Overall Average Scores:n\\", overall_avg_scores) # Task 2 - Ranking rank_overall = overall_avg_scores.sort_values(ascending=False) rank_math = df[\\"Math\\"].rank() rank_science = df[\\"Science\\"].rank() rank_english = df[\\"English\\"].rank() print(\\"Rank Overall:n\\", rank_overall) print(\\"Rank Math:n\\", rank_math) print(\\"Rank Science:n\\", rank_science) print(\\"Rank English:n\\", rank_english) # Task 3 - Comparison math_greater = df[(df[\\"Math\\"] > df[\\"Science\\"]) & (df[\\"Math\\"] > df[\\"English\\"])].index.tolist() same_scores = df[(df[\\"Math\\"] == df[\\"Science\\"]) & (df[\\"Math\\"] == df[\\"English\\"])].index.tolist() print(\\"Math > Science and English:n\\", math_greater) print(\\"Same scores across subjects:n\\", same_scores) # Task 4 - Missing Data Handling df.at[\\"Tom\\", \\"English\\"] = None df[\\"English\\"].fillna(df[\\"English\\"].mean(), inplace=True) # Task 5 - Data Summarization summary = df.describe() print(\\"Data Summary:n\\", summary) print(\\"Information Summary:n\\", df.info()) return df # Example DataFrame construction and function call data = { \\"Math\\": [56, 78, 45, 90, 75, 60, 76, 80], \\"Science\\": [66, 80, 50, 95, 70, 65, 85, 88], \\"English\\": [59, 77, 58, 85, 78, 62, 80, 83], } students = [\\"John\\", \\"Jane\\", \\"Tom\\", \\"Lucy\\", \\"Peter\\", \\"Laura\\", \\"Nina\\", \\"Gary\\"] df = pd.DataFrame(data, index=students) manipulated_df = manipulate_scores(df)"},{"question":"Advanced Filename Pattern Matching You are tasked with implementing a function that filters filenames based on multiple patterns and conditions. You need to use the `fnmatch` module to accomplish this. Function Signature ```python def filter_files(filenames: list, patterns: list, case_sensitive: bool = False, exclude: bool = False) -> list: Filters the list of filenames based on the provided patterns. Parameters: filenames (list): A list of filenames (strings) to filter. patterns (list): A list of shell-style wildcard patterns (strings) to filter by. case_sensitive (bool): Whether the matching should be case-sensitive. Default is False. exclude (bool): If True, return files that do NOT match the patterns. Default is False. Returns: list: A list of filenames matching the criteria. ``` # Input Format 1. `filenames`: A list of strings where each string is a filename. 2. `patterns`: A list of strings where each string is a shell-style wildcard pattern. 3. `case_sensitive` (optional): A boolean indicating if the pattern matching should be case-sensitive. Defaults to `False`. 4. `exclude` (optional): A boolean indicating if the function should return files that don\'t match the patterns. Defaults to `False`. # Output Format A list of filenames that meet the given criteria. # Constraints - The length of `filenames` and `patterns` will not exceed `10^3`. - Filenames will only contain alphanumeric characters and periods. - Patterns will follow the Unix shell-style wildcard syntax. # Examples ```python # Example 1 filenames = [\\"file1.txt\\", \\"file2.doc\\", \\"report.pdf\\", \\"image.png\\"] patterns = [\\"*.txt\\", \\"*.pdf\\"] print(filter_files(filenames, patterns)) # Output: [\'file1.txt\', \'report.pdf\'] # Example 2 filenames = [\\"file1.TXT\\", \\"file2.DOC\\", \\"report.PDF\\", \\"image.PNG\\"] patterns = [\\"*.txt\\", \\"*.pdf\\"] print(filter_files(filenames, patterns, case_sensitive=True)) # Output: [] # Example 3 filenames = [\\"file1.txt\\", \\"file2.doc\\", \\"report.pdf\\", \\"image.png\\"] patterns = [\\"*.txt\\", \\"*.pdf\\"] print(filter_files(filenames, patterns, exclude=True)) # Output: [\'file2.doc\', \'image.png\'] ``` # Explanation - In Example 1, the function returns files with extensions `.txt` and `.pdf` without considering case sensitivity. - In Example 2, case sensitivity is enabled, so no files match the patterns due to different casing. - In Example 3, the function returns files that do not match the given patterns. # Notes - Use the `fnmatch` module functions (`fnmatch`, `fnmatchcase`, `filter`) to implement this function. - Ensure the solution efficiently handles up to 1000 filenames and patterns.","solution":"import fnmatch def filter_files(filenames: list, patterns: list, case_sensitive: bool = False, exclude: bool = False) -> list: Filters the list of filenames based on the provided patterns. Parameters: filenames (list): A list of filenames (strings) to filter. patterns (list): A list of shell-style wildcard patterns (strings) to filter by. case_sensitive (bool): Whether the matching should be case-sensitive. Default is False. exclude (bool): If True, return files that do NOT match the patterns. Default is False. Returns: list: A list of filenames matching the criteria. matching_files = set() if case_sensitive: for pattern in patterns: matching_files.update(fnmatch.filter(filenames, pattern)) else: filenames_lower = [filename.lower() for filename in filenames] for pattern in patterns: pattern_lower = pattern.lower() matching_files.update([filenames[i] for i in range(len(filenames)) if fnmatch.fnmatchcase(filenames_lower[i], pattern_lower)]) if exclude: return [filename for filename in filenames if filename not in matching_files] return list(matching_files)"},{"question":"Coding Assessment Question # Objective The goal of this task is to assess your ability to understand and implement classes and methods provided by the `xml.sax.handler` module for event-driven XML parsing. You will create custom SAX handlers to process an XML document and extract specific information from it. # Problem Statement You are provided with an XML document containing information about various books. Your task is to implement SAX handlers to parse the XML document and extract the titles and authors of all the books. The XML structure is as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> </book> <!-- More book entries here --> </library> ``` # Requirements 1. Implement a custom `ContentHandler` class called `BookHandler` to handle the XML events. 2. Your `BookHandler` should collect and store the titles and authors of all the books in appropriate attributes. 3. Implement the following methods in your `BookHandler`: - `startElement(self, name, attrs)`: Handle the start of an XML element. - `endElement(self, name)`: Handle the end of an XML element. - `characters(self, content)`: Handle character data inside an element. 4. Write a function `parse_books(xml_string)` that: - Accepts an XML string as input. - Uses the `BookHandler` to parse the XML. - Returns a list of dictionaries, each containing a title and author. # Constraints - You can assume that each `<book>` element contains exactly one `<title>` and one `<author>`. - The XML input will be well-formed. # Example Usage ```python xml_string = <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> </book> </library> books = parse_books(xml_string) print(books) # Expected Output: # [{\'title\': \'Book Title 1\', \'author\': \'Author Name 1\'}, # {\'title\': \'Book Title 2\', \'author\': \'Author Name 2\'}] ``` # Implementation - Define the `BookHandler` class. - Implement the required `ContentHandler` methods. - Implement the `parse_books` function. - Ensure your solution handles nested structures and different character events correctly. Good luck!","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.books = [] def startElement(self, name, attrs): self.current_data = name if name == \\"book\\": self.title = \\"\\" self.author = \\"\\" def endElement(self, name): if name == \\"book\\": self.books.append({\\"title\\": self.title, \\"author\\": self.author}) self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content.strip() elif self.current_data == \\"author\\": self.author += content.strip() def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.books"},{"question":"**Problem Statement: Precision and Rounding in Financial Calculations** You work in the finance department of a multinational company where the use of precise decimal arithmetic is crucial for financial calculations. Your task is to write a Python function that takes several transactions, processes them with high precision, and generates a comprehensive report. You will use the `decimal` module to ensure accuracy and handle rounding according to financial norms. **The transactions are given in a list of dictionaries, where each dictionary represents a transaction as follows:** ```python transactions = [ {\\"date\\": \\"2023-10-05\\", \\"type\\": \\"credit\\", \\"amount\\": \\"1234.56789\\"}, {\\"date\\": \\"2023-10-06\\", \\"type\\": \\"debit\\", \\"amount\\": \\"210.12340\\"}, ... ] ``` **Your program should perform the following tasks:** 1. **Parse the amount and transform it into a `Decimal` object with the full precision given.** 2. **Process the transactions to compute the following:** - **Total balance (sum of all credits minus sum of all debits), rounded to two decimal places.** - **Total sum exclusively for credits and debits transaction types, both rounded to two decimal places.** 3. **Handle any conversion errors from invalid amounts by ignoring such transactions and flagging them.** **Function Signature:** ```python import decimal def process_transactions(transactions: list, context: decimal.Context) -> dict: pass ``` **Inputs:** - `transactions` (list): List of transaction dictionaries as shown. - `context` (decimal.Context): Decimal context to use for precise arithmetic. **Output:** - (dict): A dictionary containing the following keys: - `total_balance`: Total balance rounded to two decimal places. - `total_credits`: Total sum of credit transactions, rounded to two decimal places. - `total_debits`: Total sum of debit transactions, rounded to two decimal places. - `invalid_transactions`: List of indices of transactions that had invalid amounts and were ignored. **Constraints:** - Transaction amount will always be a string. - Transaction type will always be either \\"credit\\" or \\"debit\\". **Examples:** ```python transactions = [ {\\"date\\": \\"2023-10-05\\", \\"type\\": \\"credit\\", \\"amount\\": \\"1234.56789\\"}, {\\"date\\": \\"2023-10-06\\", \\"type\\": \\"debit\\", \\"amount\\": \\"210.12340\\"}, {\\"date\\": \\"2023-10-06\\", \\"type\\": \\"credit\\", \\"amount\\": \\"150.50\\"}, {\\"date\\": \\"2023-10-07\\", \\"type\\": \\"debit\\", \\"amount\\": \\"50.00\\"}, {\\"date\\": \\"2023-10-08\\", \\"type\\": \\"credit\\", \\"amount\\": \\"invalid\\"} ] context = decimal.Context(prec=6, rounding=decimal.ROUND_HALF_UP) results = process_transactions(transactions, context) print(results) ``` Expected Output: ```python { \'total_balance\': Decimal(\'1125.94\'), \'total_credits\': Decimal(\'1385.07\'), \'total_debits\': Decimal(\'260.12\'), \'invalid_transactions\': [4] } ``` Note: The balances are rounded to two decimal places using the specified context, and the invalid transaction is identified and ignored. **Explanation:** 1. The total credits are the sum of valid credit amounts: 1234.56789 + 150.50 = 1385.06789 → Rounded to: 1385.07. 2. The total debits are the sum of valid debit amounts: 210.12340 + 50.00 = 260.12340 → Rounded to: 260.12. 3. The total balance is: 1385.07 - 260.12 = 1124.95. Use the `decimal` module properly to handle the arithmetic and rounding needed for high precision finance calculations.","solution":"import decimal from decimal import Decimal def process_transactions(transactions: list, context: decimal.Context) -> dict: total_credits = Decimal(\'0\') total_debits = Decimal(\'0\') invalid_transactions = [] for i, transaction in enumerate(transactions): try: amount = Decimal(transaction[\\"amount\\"]) except decimal.InvalidOperation: invalid_transactions.append(i) continue if transaction[\\"type\\"] == \\"credit\\": total_credits += amount elif transaction[\\"type\\"] == \\"debit\\": total_debits += amount total_credits = total_credits.quantize(Decimal(\'0.01\'), context.rounding) total_debits = total_debits.quantize(Decimal(\'0.01\'), context.rounding) total_balance = (total_credits - total_debits).quantize(Decimal(\'0.01\'), context.rounding) return { \'total_balance\': total_balance, \'total_credits\': total_credits, \'total_debits\': total_debits, \'invalid_transactions\': invalid_transactions }"},{"question":"**Coding Assessment Question:** # Background: You have been tasked with creating a user management utility for a Unix system. This utility should be able to retrieve and display information about Unix user accounts based on either a user name or user ID. You will make use of the `pwd` module to accomplish this. # Problem Statement: You need to implement a class `UserManager` that provides the following functionalities: 1. **Retrieve User by ID**: - Method: `get_user_by_id(uid: int) -> Optional[dict]` - Description: This method takes a user ID and returns a dictionary containing the user information if the user exists. If the user does not exist, it should return `None`. - Attributes in the dictionary: `pw_name`, `pw_passwd`, `pw_uid`, `pw_gid`, `pw_gecos`, `pw_dir`, `pw_shell`. 2. **Retrieve User by Name**: - Method: `get_user_by_name(name: str) -> Optional[dict]` - Description: This method takes a user name and returns a dictionary containing the user information if the user exists. If the user does not exist, it should return `None`. - Attributes in the dictionary: `pw_name`, `pw_passwd`, `pw_uid`, `pw_gid`, `pw_gecos`, `pw_dir`, `pw_shell`. 3. **Retrieve All Users**: - Method: `get_all_users() -> List[dict]` - Description: This method returns a list of dictionaries containing the user information for all users available on the system. - Attributes in each dictionary: `pw_name`, `pw_passwd`, `pw_uid`, `pw_gid`, `pw_gecos`, `pw_dir`, `pw_shell`. # Requirements: - The methods should handle exceptions and invalid inputs gracefully. - Use the `pwd` module\'s functions (`pwd.getpwuid`, `pwd.getpwnam`, `pwd.getpwall`) to implement the functionalities. - Ensure the returned dictionaries and lists are formatted correctly with the appropriate attributes. # Constraints and Limitations: - You can assume the input `uid` will be a positive integer and `name` will be a non-empty string. - The method `get_all_users` should return the user entries in an arbitrary order as provided by `pwd.getpwall()`. # Example Usage: ```python # Assuming there is a user with uid=1000 and name=\'john\' manager = UserManager() user_by_id = manager.get_user_by_id(1000) # user_by_id should be something like: # { # \'pw_name\': \'john\', # \'pw_passwd\': \'x\', # \'pw_uid\': 1000, # \'pw_gid\': 1000, # \'pw_gecos\': \'John Doe\', # \'pw_dir\': \'/home/john\', # \'pw_shell\': \'/bin/bash\' # } user_by_name = manager.get_user_by_name(\'john\') # user_by_name should be similar to the above dictionary all_users = manager.get_all_users() # all_users should be a list of dictionaries with user information ``` Implement the class `UserManager` with the required methods.","solution":"import pwd from typing import Optional, List, Dict class UserManager: def get_user_by_id(self, uid: int) -> Optional[dict]: try: user = pwd.getpwuid(uid) return { \'pw_name\': user.pw_name, \'pw_passwd\': user.pw_passwd, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_gecos\': user.pw_gecos, \'pw_dir\': user.pw_dir, \'pw_shell\': user.pw_shell } except KeyError: return None def get_user_by_name(self, name: str) -> Optional[dict]: try: user = pwd.getpwnam(name) return { \'pw_name\': user.pw_name, \'pw_passwd\': user.pw_passwd, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_gecos\': user.pw_gecos, \'pw_dir\': user.pw_dir, \'pw_shell\': user.pw_shell } except KeyError: return None def get_all_users(self) -> List[dict]: users = pwd.getpwall() return [ { \'pw_name\': user.pw_name, \'pw_passwd\': user.pw_passwd, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_gecos\': user.pw_gecos, \'pw_dir\': user.pw_dir, \'pw_shell\': user.pw_shell } for user in users ]"},{"question":"# Asynchronous Producer-Consumer System with asyncio **Objective**: Implement an asynchronous producer-consumer system using `asyncio` queues where producers generate tasks at random intervals and consumers process these tasks. Problem Statement: You are required to: 1. Implement two asynchronous functions, `producer(producer_id, queue)` and `consumer(consumer_id, queue)`. 2. Implement a main coroutine `main()` to coordinate these producer and consumer coroutines. **Producer Function (`producer(producer_id, queue)`):** - `producer_id` (int): The ID of the producer. - `queue` (`asyncio.Queue`): The queue to put tasks into. This function should: 1. Generate tasks at random intervals ranging from 0.1 to 0.5 seconds. 2. Put tasks into the queue with the format `(producer_id, task_id)` where `task_id` is a sequentially increasing integer starting from 1. 3. Add a total of 10 tasks into the queue. **Consumer Function (`consumer(consumer_id, queue)`):** - `consumer_id` (int): The ID of the consumer. - `queue` (`asyncio.Queue`): The queue to retrieve tasks from. This function should: 1. Retrieve tasks from the queue. 2. Simulate processing of the task by sleeping for a random time between 0.1 to 0.3 seconds. 3. Log the processing of the task in the format: `Consumer {consumer_id} processed task {task_id} from Producer {producer_id}`. 4. Indicate the completion of the task with `queue.task_done()`. **Main Coroutine (`main()`):** 1. Create an `asyncio.Queue`. 2. Create and start 3 producer tasks and 5 consumer tasks. 3. Wait until all producer tasks are completed and the queue is fully processed. **Constraints:** - Limit the maximum size of the queue to 15. - Ensure that all tasks added by producers are eventually processed by consumers. Function Signature ```python import asyncio import random async def producer(producer_id: int, queue: asyncio.Queue): # Your implementation here async def consumer(consumer_id: int, queue: asyncio.Queue): # Your implementation here async def main(): # Your implementation here if __name__ == \\"__main__\\": asyncio.run(main()) ``` Example Execution: ``` Producer 1 added task 1 Producer 2 added task 1 Consumer 1 processed task 1 from Producer 1 Producer 3 added task 1 Consumer 2 processed task 1 from Producer 2 ... Consumer 4 processed task 10 from Producer 3 ``` Implement the above and ensure it works as described.","solution":"import asyncio import random async def producer(producer_id: int, queue: asyncio.Queue): for task_id in range(1, 11): await asyncio.sleep(random.uniform(0.1, 0.5)) task = (producer_id, task_id) await queue.put(task) print(f\\"Producer {producer_id} added task {task_id}\\") async def consumer(consumer_id: int, queue: asyncio.Queue): while True: task = await queue.get() if task is None: # Sentinel value to indicate the consumer should exit queue.task_done() break producer_id, task_id = task await asyncio.sleep(random.uniform(0.1, 0.3)) print(f\\"Consumer {consumer_id} processed task {task_id} from Producer {producer_id}\\") queue.task_done() async def main(): queue = asyncio.Queue(maxsize=15) producers = [producer(i, queue) for i in range(1, 4)] consumers = [consumer(i, queue) for i in range(1, 6)] producer_tasks = [asyncio.create_task(p) for p in producers] consumer_tasks = [asyncio.create_task(c) for c in consumers] await asyncio.gather(*producer_tasks) # Add a None to the queue for each consumer to signal them to exit for _ in consumers: await queue.put(None) await queue.join() # Wait until the queue is fully processed if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Problem Description You are required to implement a custom descriptor that performs intricate data validation and manages attribute access. This task will assess your understanding of advanced Python concepts, particularly descriptors. # Requirements 1. You will create a `ValidatedAttribute` descriptor that validates data on setting and logs access on getting. 2. The `ValidatedAttribute` should: - Be initialized with a `validation_function` to check the validity of the attribute\'s value. - Log every access and update to the attribute. 3. Use `__set_name__` to allow the descriptor to be aware of the attribute name it manages in its owner class. 4. Implement two subclasses of `ValidatedAttribute`, each with its own validation: - `PositiveInteger`: Ensures the attribute value is a positive integer. - `NonEmptyString`: Ensures the attribute value is a non-empty string. # Constraints - You should not use built-in property decorators. - Utilize the logging module for logging access and updates. - Validation functions should raise `ValueError` if validation fails. # Input and Output Formats - The descriptors will be used in a mock class, `ValidatedClass`, and tested via attribute access. - An example test would set attributes and cause both successful and unsuccessful validations, logging appropriately. # Example ```python import logging logging.basicConfig(level=logging.INFO) class ValidatedAttribute: def __init__(self, validation_function): self.validation_function = validation_function def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, instance, owner): value = getattr(instance, self.private_name) logging.info(f\'Accessing {self.public_name}: {value}\') return value def __set__(self, instance, value): self.validation_function(value) logging.info(f\'Setting {self.public_name} to {value}\') setattr(instance, self.private_name, value) class PositiveInteger(ValidatedAttribute): def __init__(self): super().__init__(self.validate) def validate(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\'{value} is not a positive integer\') class NonEmptyString(ValidatedAttribute): def __init__(self): super().__init__(self.validate) def validate(self, value): if not isinstance(value, str) or not value.strip(): raise ValueError(f\'{value} is not a non-empty string\') class ValidatedClass: age = PositiveInteger() name = NonEmptyString() def __init__(self, age, name): self.age = age self.name = name # Example Use try: p1 = ValidatedClass(30, \\"John Doe\\") p2 = ValidatedClass(-5, \\"Jane\\") except ValueError as e: print(e) try: p3 = ValidatedClass(25, \\"\\") except ValueError as e: print(e) p1.age = 35 # Should log: \\"Setting age to 35\\" print(p1.age) # Should log: \\"Accessing age: 35\\" followed by 35 ``` Your task is to complete the implementations of `ValidatedAttribute`, `PositiveInteger`, `NonEmptyString`, and integrate them into the `ValidatedClass`.","solution":"import logging logging.basicConfig(level=logging.INFO) class ValidatedAttribute: def __init__(self, validation_function): self.validation_function = validation_function def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, instance, owner): value = getattr(instance, self.private_name) logging.info(f\'Accessing {self.public_name}: {value}\') return value def __set__(self, instance, value): self.validation_function(value) logging.info(f\'Setting {self.public_name} to {value}\') setattr(instance, self.private_name, value) class PositiveInteger(ValidatedAttribute): def __init__(self): super().__init__(self.validate) def validate(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\'{value} is not a positive integer\') class NonEmptyString(ValidatedAttribute): def __init__(self): super().__init__(self.validate) def validate(self, value): if not isinstance(value, str) or not value.strip(): raise ValueError(f\'{value} is not a non-empty string\') class ValidatedClass: age = PositiveInteger() name = NonEmptyString() def __init__(self, age, name): self.age = age self.name = name # Example Use try: p1 = ValidatedClass(30, \\"John Doe\\") p2 = ValidatedClass(-5, \\"Jane\\") except ValueError as e: print(e) try: p3 = ValidatedClass(25, \\"\\") except ValueError as e: print(e) p1.age = 35 # Should log: \\"Setting age to 35\\" print(p1.age) # Should log: \\"Accessing age: 35\\" followed by 35"},{"question":"You are working as a data analyst, and you need to visualize the distribution and relationship of features in a dataset to generate meaningful insights. For this task, you will use Seaborn\'s `swarmplot` and `catplot` to create insightful visuals. Objective Your task is to create multiple swarmplots using the `tips` dataset and customize them as specified. Follow the steps below and write the respective code. Steps 1. **Basic Swarmplot:** - Load the `tips` dataset using `sns.load_dataset(\\"tips\\")`. - Create a basic swarmplot to show the distribution of the `total_bill` across different days. Use `total_bill` on the x-axis and `day` on the y-axis. 2. **Color Coding:** - Modify the above plot to add color coding based on `sex` using the `hue` parameter. 3. **Customized Swarmplot:** - Create a new swarmplot showing the distribution of `total_bill` vertically for different `days`. Use color coding based on `size` and use the `deep` palette. 4. **Dodge Categories:** - Create a swarmplot to display `total_bill` on the x-axis and `day` on the y-axis using `sex` for color coding (`hue`). Ensure that male and female data points are split across the categorical `day` values using the `dodge=True` parameter. 5. **Facet Plot:** - Use `sns.catplot` to create a multi-faceted swarmplot showing `total_bill` on the y-axis and `time` on the x-axis, categorized by `sex` using `hue`, and faceted by `day`. Set the aspect ratio to 0.5. Requirements 1. Use Seaborn\'s visualization capabilities as described. 2. Ensure your plots are clear and readable. 3. Add appropriate titles and labels to make the plots self-explanatory. Input and Output - **Input:** The `tips` dataset as provided by Seaborn. - **Output:** Visualizations as described in the steps above. ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load dataset tips = sns.load_dataset(\\"tips\\") # 2. Basic Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Distribution of Total Bill Across Different Days\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # 3. Color Coding by Sex plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Distribution of Total Bill Across Different Days with Sex Color Coding\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 4. Customized Swarmplot with Deep Palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"size\\", palette=\\"deep\\") plt.title(\\"Vertical Distribution of Total Bill with Size Color Coding\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Size\\") plt.show() # 5. Swarmplot with Dodge=True plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True) plt.title(\\"Distribution of Total Bill with Sex and Dodged Categories\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 6. Faceted Catplot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarmplots(): # Load dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Distribution of Total Bill Across Different Days\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # 2. Color Coding by Sex plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Distribution of Total Bill Across Different Days with Sex Color Coding\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 3. Customized Swarmplot with Deep Palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"size\\", palette=\\"deep\\") plt.title(\\"Vertical Distribution of Total Bill with Size Color Coding\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Size\\") plt.show() # 4. Swarmplot with Dodge=True plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True) plt.title(\\"Distribution of Total Bill with Sex and Dodged Categories\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 5. Faceted Catplot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) plt.show()"},{"question":"**Question: Implement a Multilabel Classification using OneVsRest Strategy** # Problem Statement You are required to implement a multilabel classification algorithm using the One-vs-Rest (OvR) strategy provided by scikit-learn. For this task, you will use the `OneVsRestClassifier` provided by the `sklearn.multiclass` module and the `RandomForestClassifier` as the base estimator. # Instructions 1. Load the dataset `datasets.load_wine`, which is a wine quality dataset. 2. Convert the target variable into a multilabel format by binarizing the labels such that each wine can be tagged with multiple quality indicators (e.g., \\"high quality\\", \\"medium quality\\", \\"low quality\\"). 3. Fit a `OneVsRestClassifier` using the `RandomForestClassifier` as the base estimator. 4. Predict the labels for the training set and evaluate the performance using appropriate multilabel classification metrics. # Implementation Details - Use the `datasets.load_wine` function to load the wine dataset. - Use the `LabelBinarizer` to binarize the target variable. - Implement the classifier using `OneVsRestClassifier` and `RandomForestClassifier`. - Evaluate the model using metrics such as precision, recall, and F1-score for multilabel classification. # Expected Input and Output - **Input:** None (the data is loaded from within the function) - **Output:** Print the multilabel classification metrics including precision, recall, and F1-score. # Constraints - No external libraries except `numpy`, `scipy`, and `scikit-learn` are allowed. - The solution needs to run efficiently on the provided dataset. # Example Usage ```python def multilabel_classification(): # Your code here # Call the function to execute multilabel_classification() ``` # Evaluation - Correct implementation of data loading and transformation. - Correct application of `OneVsRestClassifier` with `RandomForestClassifier`. - Proper evaluation using multilabel classification metrics. **Note:** The dataset is available in scikit-learn and does not require downloading from an external source.","solution":"from sklearn.datasets import load_wine from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report def multilabel_classification(): # Load dataset wine = load_wine() X = wine.data y = wine.target # Binarize the labels lb = LabelBinarizer() y_bin = lb.fit_transform(y) # Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42) # Initialize OneVsRestClassifier with RandomForestClassifier ovr = OneVsRestClassifier(RandomForestClassifier(random_state=42)) # Fit the model ovr.fit(X_train, y_train) # Predict on the test set y_pred = ovr.predict(X_test) # Evaluate the model report = classification_report(y_test, y_pred, target_names=lb.classes_.astype(str)) print(report)"},{"question":"**Title: Custom Python Iterator Implementations** Objective: Implement Python iterators using the principles described in the `PySeqIter_Type` and `PyCallIter_Type` documentation. Your task is to create two classes that mimic these behaviors in Python. Problem Statement: 1. Implement a class `SeqIter` that takes any sequence object and allows iteration over it, terminates when `IndexError` is raised. 2. Implement a class `CallIter` that takes a callable object and a sentinel value. It should call the callable for each item in the sequence, ending when the callable returns the sentinel value. Implementation Details: 1. **Class `SeqIter`**: - **Constructor**: `__init__(self, seq: Sequence) -> None` - Initializes the iterator with the given sequence. - **Method**: `__iter__(self) -> Iterator` - Returns the iterator object itself. - **Method**: `__next__(self) -> Any` - Returns the next item from the sequence or raises `StopIteration` when the sequence is exhausted. 2. **Class `CallIter`**: - **Constructor**: `__init__(self, callable: Callable[[], Any], sentinel: Any) -> None` - Initializes the iterator with the callable object and sentinel value. - **Method**: `__iter__(self) -> Iterator` - Returns the iterator object itself. - **Method**: `__next__(self) -> Any` - Calls the callable, returns the next item or raises `StopIteration` when the sentinel value is returned. Constraints: - Do not use `yield` or generator functions. - Ensure the implementation handles both typical and edge cases gracefully. Example Usage: ```python # Example for SeqIter sequence = [1, 2, 3, 4] seq_iter = SeqIter(sequence) for item in seq_iter: print(item) # Expected Output: # 1 # 2 # 3 # 4 # Example for CallIter def counter(): global count count += 1 return count count = 0 call_iter = CallIter(counter, 5) for item in call_iter: print(item) # Expected Output: # 1 # 2 # 3 # 4 ``` # Note: - Pay attention to corner cases, such as empty sequences or callable functions that never return the sentinel value. Submission Requirements: - Implement the `SeqIter` and `CallIter` in a Python file. - Include test cases demonstrating the functionality and edge cases. Good luck!","solution":"class SeqIter: def __init__(self, seq): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.seq): raise StopIteration item = self.seq[self.index] self.index += 1 return item class CallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"# Python Programming Assessment Objective: Demonstrate your understanding of the `nis` module for centralized administration of Unix hosts. Problem Statement: You are responsible for writing a utility function to manage and query NIS maps efficiently. Given the following functionalities of the `nis` module, your task is to implement a function that retrieves key-value pairs from multiple specified maps and aggregates them into a single dictionary. Function Signature: ```python def aggregate_nis_maps(maps_list: list[str], domain: str = None) -> dict[bytes, bytes]: ``` Input: - `maps_list`: A list of strings representing the names of NIS maps. - `domain`: An optional string specifying the NIS domain. If not provided, use the default NIS domain. Output: - A dictionary where each key is retrieved from the specified maps and its corresponding value is the one found in the last map it appears in, considering the order of maps provided in `maps_list`. Exceptions: - Raise a `nis.error` if any NIS-related function returns an error for any map or key operation. - Propagate `KeyError` if any key-related error occurs. Constraints: - Assume the number of maps (`N`) in `maps_list` does not exceed 100. - Assume the total number of keys across all maps does not exceed 10,000. - Handle large data efficiently to avoid performance bottlenecks. Example: ```python # Example usage: maps_list = [\'passwd.byname\', \'group.byname\'] domain = \'example.com\' # Example output: { b\'user1\': b\'user1_data\', b\'group1\': b\'group1_data\', ... } ``` Function Implementation: Implement the function `aggregate_nis_maps` utilizing the functions provided by the `nis` module as described in the documentation. **Note:** Ensure that your implementation handles cases where maps might overlap in keys, keeping the value from the last occurring map for such keys. Good luck!","solution":"import nis def aggregate_nis_maps(maps_list: list[str], domain: str = None) -> dict[bytes, bytes]: Aggregates key-value pairs from multiple specified NIS maps into a single dictionary. :param maps_list: A list of NIS map names. :param domain: An optional NIS domain name. :return: A dictionary containing key-value pairs from the specified NIS maps. aggregated_data = {} for map_name in maps_list: try: entries = nis.cat(map_name, domain) aggregated_data.update(entries) except nis.error as e: raise nis.error(f\\"Failed to retrieve map {map_name}: {str(e)}\\") except KeyError as e: raise KeyError(f\\"Key error for map {map_name}: {str(e)}\\") return aggregated_data"},{"question":"# Tracing Function Execution and Generating Coverage Reports In this exercise, you will use the `trace` module to trace the execution of a Python function and generate an annotated coverage report. Requirements: 1. **Function to Trace:** Create a function `sample_function` that takes an integer `n` as an argument and computes the factorial of `n` iteratively. If `n` is negative, the function should return `None`. 2. **Tracing Execution:** Use the `trace.Trace` class to trace the execution of `sample_function`. 3. **Generate Coverage Report:** Generate an annotated coverage report that shows how many times each line in `sample_function` was executed. Function Definition ```python def sample_function(n): Computes the factorial of a given non-negative integer n. If n is negative, return None. Args: n (int): A non-negative integer. Returns: int or None: Factorial of n if n is non-negative, otherwise None. # Your code here ``` Tracing and Reporting 1. Use the `trace.Trace` class to create a trace object with `count` enabled. 2. Use the `runfunc` method to execute `sample_function` for a few different values of `n`. 3. Use the `results` method to obtain the coverage results. 4. Write the coverage results to the current directory, showing missing lines. # Example Usage Assume `sample_function` is implemented in a file named `traced_module.py`. ```python import trace from traced_module import sample_function # Create a Trace object tracer = trace.Trace(count=1, trace=0) # Execute the sample_function with tracer tracer.runfunc(sample_function, 5) tracer.runfunc(sample_function, -1) # Generate the trace results results = tracer.results() # Write the results to the current directory results.write_results(show_missing=True, coverdir=\\".\\") ``` # Constraints - Do not use any external libraries other than the standard library. - Ensure the script runs without any errors and creates the coverage report in the specified directory. Submission Submit the implementation of `sample_function` and the tracing/reporting script in a single Python file named `tracing_assessment.py`.","solution":"def sample_function(n): Computes the factorial of a given non-negative integer n. If n is negative, returns None. Args: n (int): A non-negative integer. Returns: int or None: Factorial of n if n is non-negative, otherwise None. if n < 0: return None factorial = 1 for i in range(1, n + 1): factorial *= i return factorial"},{"question":"# Data Analysis and Optimization with Pandas Overview You are provided with time series data across multiple files in a directory. Each file contains data for a specific year and consists of several columns. Your task is to load, process, and optimize this data using the techniques described below. Problem Statement 1. **Load Specific Columns**: Load only the necessary columns from each file in the directory. 2. **Optimize Data Types**: Convert text columns to categorical data type and downcast numeric columns to the smallest type that can accommodate their values. 3. **Chunk-wise Processing**: Implement a function to calculate the total count of occurrences of each value in a specified column across all files. Function Signature ```python def load_and_optimize_data(directory: str, columns: list, categorical_columns: list, numeric_columns: list) -> pd.DataFrame: pass def calculate_value_counts(directory: str, column_name: str) -> pd.Series: pass ``` Input and Output Formats 1. `load_and_optimize_data(directory: str, columns: list, categorical_columns: list, numeric_columns: list) -> pd.DataFrame`: - **Input**: - `directory`: Path to the directory containing the parquet files. - `columns`: List of columns to be loaded from each file. - `categorical_columns`: List of columns to be converted to the categorical data type. - `numeric_columns`: List of tuples (column_name, downcast_type) for numeric columns that need downcasting. - **Output**: A DataFrame with the specified columns loaded, categorized, and numeric columns optimized. 2. `calculate_value_counts(directory: str, column_name: str) -> pd.Series`: - **Input**: - `directory`: Path to the directory containing the parquet files. - `column_name`: Column name for which the value count needs to be calculated. - **Output**: A Series containing the counts of each unique value in the specified column across all the files. Constraints - Assume the directory contains only the parquet files for the relevant data. - Ensure that the solution optimizes memory usage efficiently. - Use efficient data types and process data in chunks to handle large datasets. Example Usage ```python import pandas as pd # Define the parameters directory = \\"data/timeseries\\" columns = [\\"id\\", \\"name\\", \\"x\\", \\"y\\"] categorical_columns = [\\"name\\"] numeric_columns = [(\\"id\\", \\"unsigned\\"), (\\"x\\", \\"float\\"), (\\"y\\", \\"float\\")] # Load and optimize the data optimized_df = load_and_optimize_data(directory, columns, categorical_columns, numeric_columns) print(optimized_df.memory_usage(deep=True)) # Calculate value counts for a column value_counts = calculate_value_counts(directory, \\"name\\") print(value_counts) ``` Implementation Notes - Use `pd.read_parquet` to load specific columns. - Use `pandas.Categorical` for converting text columns to categorical. - Use `pandas.to_numeric` with downcast option for numeric columns. - Read and process files in chunks if necessary to keep memory usage low.","solution":"import os import pandas as pd def load_and_optimize_data(directory: str, columns: list, categorical_columns: list, numeric_columns: list) -> pd.DataFrame: df_list = [] for file_name in os.listdir(directory): if file_name.endswith(\\".parquet\\"): df = pd.read_parquet(os.path.join(directory, file_name), columns=columns) # Convert text columns to categorical for cat_col in categorical_columns: if cat_col in df.columns: df[cat_col] = df[cat_col].astype(\'category\') # Downcast numeric columns for num_col, dtype in numeric_columns: if num_col in df.columns: df[num_col] = pd.to_numeric(df[num_col], downcast=dtype) # Append to the list df_list.append(df) # Concatenate all dataframes optimized_df = pd.concat(df_list, ignore_index=True) return optimized_df def calculate_value_counts(directory: str, column_name: str) -> pd.Series: counts = pd.Series(dtype=\'int\') for file_name in os.listdir(directory): if file_name.endswith(\\".parquet\\"): df = pd.read_parquet(os.path.join(directory, file_name), columns=[column_name]) # Value counts for the current chunk chunk_counts = df[column_name].value_counts() counts = counts.add(chunk_counts, fill_value=0) return counts"},{"question":"# Question: Concurrent Execution with `concurrent.futures` You have been tasked with developing a system that performs multiple computationally expensive operations in parallel to reduce overall processing time. Your objective is to create a function that computes the results of these operations using both thread-based and process-based concurrency models and returns the results in a specified format. Task: - Implement a Python function named `parallel_computation` that accepts a list of computational tasks (functions) and additional arguments if needed. - Your function should support both `ThreadPoolExecutor` and `ProcessPoolExecutor` from the `concurrent.futures` module. - Each task in the list should be a callable function. - Ensure your function handles any exceptions raised during the execution of the tasks and log them appropriately. Function Signature: ```python def parallel_computation(tasks: list, use_threads: bool = True) -> list: pass ``` Input: 1. `tasks`: A list of tuples where each tuple contains: - A callable function (task) - A list of arguments for the function - A dictionary of keyword arguments for the function (optional) 2. `use_threads`: A boolean flag indicating whether to use `ThreadPoolExecutor` if True or `ProcessPoolExecutor` if False. Default is True. Output: - The function should return a list of results from the completed tasks, in the same order as the `tasks` input list. - If there is an exception for a specific task, the corresponding result should be the string \\"error\\". Example: ```python def sample_task(x, y): return x + y def faulty_task(x): raise ValueError(\\"Intentional Error\\") tasks = [ (sample_task, [1, 2], {}), (sample_task, [3, 4], {}), (faulty_task, [5], {}) ] results = parallel_computation(tasks, use_threads=True) print(results) # Output: [3, 7, \\"error\\"] ``` Constraints: - Ensure the implementation efficiently handles large lists of tasks. - Handle all exceptions raised during task execution and ensure the system does not crash. - Consider the performance implications of threading vs. multiprocessing for different types of tasks. Notes: - Thoroughly test your function with various task inputs to ensure reliability and correctness. - Use appropriate logging to track exceptions during task executions for debugging purposes.","solution":"import concurrent.futures import logging logging.basicConfig(level=logging.ERROR) def parallel_computation(tasks: list, use_threads: bool = True) -> list: Executes a list of tasks in parallel using ThreadPoolExecutor or ProcessPoolExecutor. Args: tasks (list): A list of tuples, each containing a callable function, a list of arguments for the function, and optionally a dictionary of keyword arguments. use_threads (bool): Boolean flag indicating whether to use ThreadPoolExecutor if True or ProcessPoolExecutor if False. Default is True. Returns: list: A list of results from the completed tasks, with \\"error\\" for tasks that raised exceptions. results = [] # Choose the executor based on the use_threads flag executor = concurrent.futures.ThreadPoolExecutor if use_threads else concurrent.futures.ProcessPoolExecutor with executor() as exec: # Start the tasks future_to_task = {exec.submit(task, *(args if args else []), **(kwargs if kwargs else {})): (task, args, kwargs) for task, args, kwargs in tasks} for future in concurrent.futures.as_completed(future_to_task): task, args, kwargs = future_to_task[future] try: result = future.result() except Exception as e: logging.error(f\\"Task {task.__name__} with args {args} and kwargs {kwargs} raised an exception: {e}\\") result = \\"error\\" results.append(result) return results"},{"question":"You are tasked with implementing a function that formats and parses email addresses and timestamps using the utilities provided in the `email.utils` module. Your function should take a list of email headers and perform the following operations: 1. Extract all email addresses from \'To\', \'Cc\', \'Resent-To\', and \'Resent-Cc\' headers. 2. Parse the \'Date\' header and convert it to a naive datetime object. 3. Generate a unique Message-ID. # Function Signature ```python from typing import List, Dict, Tuple from datetime import datetime def process_email_headers(headers: List[Dict[str, str]]) -> Tuple[List[Tuple[str, str]], datetime, str]: # Your implementation here pass ``` # Input - `headers`: A list of dictionaries where each dictionary represents email headers. For example: ```python [ { \\"To\\": \\"John Doe <john@example.com>, Jane Roe <jane@example.com>\\", \\"Cc\\": \\"Admin <admin@example.com>\\", \\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"<1234@example.com>\\" }, { \\"Resent-To\\": \\"Alice <alice@example.com>\\", \\"Resent-Cc\\": \\"Bob <bob@example.com>\\", \\"Date\\": \\"Sat, 10 Nov 2001 02:09:48 +0000\\", \\"Message-ID\\": \\"<5678@example.net>\\" } ] ``` # Output - A tuple consisting of: 1. A list of 2-tuples, each containing the real name and email address extracted from the \'To\', \'Cc\', \'Resent-To\', and \'Resent-Cc\' headers. 2. A naive `datetime` object obtained by parsing the \'Date\' header of the first email. 3. A string representing a unique Message-ID. # Constraints - The \'Date\' header in the first email is guaranteed to be in a valid RFC 2822 format. - The input list contains at least one set of headers. # Example ```python headers = [ { \\"To\\": \\"John Doe <john@example.com>, Jane Roe <jane@example.com>\\", \\"Cc\\": \\"Admin <admin@example.com>\\", \\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"<1234@example.com>\\" }, { \\"Resent-To\\": \\"Alice <alice@example.com>\\", \\"Resent-Cc\\": \\"Bob <bob@example.com>\\", \\"Date\\": \\"Sat, 10 Nov 2001 02:09:48 +0000\\", \\"Message-ID\\": \\"<5678@example.net>\\" } ] print(process_email_headers(headers)) # Should output something like: # ( # [ # (\\"John Doe\\", \\"john@example.com\\"), # (\\"Jane Roe\\", \\"jane@example.com\\"), # (\\"Admin\\", \\"admin@example.com\\"), # (\\"Alice\\", \\"alice@example.com\\"), # (\\"Bob\\", \\"bob@example.com\\") # ], # datetime.datetime(2001, 11, 9, 1, 8, 47), # \\"<unique-msg-id@hostname>\\" # ) ``` Utilize the functions from `email.utils` to implement the solution efficiently.","solution":"from typing import List, Dict, Tuple from datetime import datetime import email.utils import uuid def process_email_headers(headers: List[Dict[str, str]]) -> Tuple[List[Tuple[str, str]], datetime, str]: all_addresses = [] # Collect email addresses from relevant headers headers_of_interest = [\'To\', \'Cc\', \'Resent-To\', \'Resent-Cc\'] for header in headers: for field in headers_of_interest: if field in header: addresses = email.utils.getaddresses([header[field]]) all_addresses.extend(addresses) # Parse the \'Date\' header of the first email to a naive datetime object if \'Date\' in headers[0]: date_tuple = email.utils.parsedate_to_datetime(headers[0][\'Date\']) naive_datetime = date_tuple.replace(tzinfo=None) # Convert to naive datetime # Generate a unique Message-ID message_id = f\\"<{uuid.uuid4()}@example.com>\\" return all_addresses, naive_datetime, message_id"},{"question":"**Coding Assessment Question** # Objective: Implement a custom neural network in PyTorch and write a function to initialize its parameters using specific functions from the `torch.nn.init` module. # Question: You are required to create a simple feed-forward neural network using PyTorch\'s `nn.Module`. The network should consist of the following layers: 1. Input Layer: Accepts inputs of size `input_size`. 2. Hidden Layer 1: Fully connected layer of size `hidden_size1`. 3. Hidden Layer 2: Fully connected layer of size `hidden_size2`. 4. Output Layer: Fully connected layer of size `output_size`. Write a function, `initialize_parameters`, which takes the neural network as input and initializes its parameters using the following initializers: - Use `xavier_normal_` for the weights of both hidden layers. - Use `kaiming_normal_` for the weights of the output layer. - Use `zeros_` for all biases. # Function Signature: ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size1, hidden_size2, output_size): super(CustomNeuralNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size1) self.fc2 = nn.Linear(hidden_size1, hidden_size2) self.fc3 = nn.Linear(hidden_size2, output_size) def initialize_parameters(model): Initializes parameters of the given model. Parameters: model (torch.nn.Module): The neural network model to initialize. # Add your code here # Example to test your function input_size = 10 hidden_size1 = 20 hidden_size2 = 30 output_size = 1 model = CustomNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size) initialize_parameters(model) # Verify the initialization print(model.fc1.weight) # Should be initialized with xavier_normal_ print(model.fc1.bias) # Should be initialized with zeros_ print(model.fc2.weight) # Should be initialized with xavier_normal_ print(model.fc2.bias) # Should be initialized with zeros_ print(model.fc3.weight) # Should be initialized with kaiming_normal_ print(model.fc3.bias) # Should be initialized with zeros_ ``` # Requirements: - Your implementation should correctly use the `xavier_normal_`, `kaiming_normal_`, and `zeros_` initializers from the `torch.nn.init` module. - Ensure that the function `initialize_parameters` initializes the weights and biases as specified. - The code should be efficient and adhere to PyTorch best practices. # Constraints: - You must use the `torch.nn.init` module for parameter initialization. - The input sizes and layer sizes are positive integers greater than zero. # Performance Requirements: - The initialization function should run efficiently without any unnecessary computations. Good luck and happy coding!","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self, input_size, hidden_size1, hidden_size2, output_size): super(CustomNeuralNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size1) self.fc2 = nn.Linear(hidden_size1, hidden_size2) self.fc3 = nn.Linear(hidden_size2, output_size) def initialize_parameters(model): Initializes parameters of the given model. Parameters: model (torch.nn.Module): The neural network model to initialize. for name, param in model.named_parameters(): if \'weight\' in name: if \'fc3\' in name: init.kaiming_normal_(param) else: init.xavier_normal_(param) elif \'bias\' in name: init.zeros_(param) # Example to test your function input_size = 10 hidden_size1 = 20 hidden_size2 = 30 output_size = 1 model = CustomNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size) initialize_parameters(model) # Verify the initialization print(model.fc1.weight) # Should be initialized with xavier_normal_ print(model.fc1.bias) # Should be initialized with zeros_ print(model.fc2.weight) # Should be initialized with xavier_normal_ print(model.fc2.bias) # Should be initialized with zeros_ print(model.fc3.weight) # Should be initialized with kaiming_normal_ print(model.fc3.bias) # Should be initialized with zeros_"},{"question":"**Objective:** Demonstrate your understanding of Scikit-Learn\'s dataset loading utilities by loading a dataset, performing basic preprocessing, and implementing a simple machine learning model. **Task:** 1. **Dataset Loading:** - Load the well-known Iris dataset using the `load_iris` function from `sklearn.datasets`. - Additionally, generate a synthetic dataset using `make_classification` from `sklearn.datasets` with the following parameters: - `n_samples=1000` - `n_features=20` - `n_informative=15` - `n_classes=3` - `random_state=42` 2. **Data Preprocessing:** - For the Iris dataset: - Separate the data into features `X_iris` and target `y_iris`. - Split the data into training and testing sets using an 80-20 split. - For the synthetic dataset: - Separate the data into features `X_synthetic` and target `y_synthetic`. - Split the data into training and testing sets using an 80-20 split. 3. **Model Implementation:** - Using the training data from both datasets, implement a simple logistic regression model. - Evaluate the model on both testing datasets and report the accuracy. **Constraints and Requirements:** - Use `train_test_split` from `sklearn.model_selection` to split the datasets. - Use `LogisticRegression` from `sklearn.linear_model` for the model. - Ensure that your code is well-organized and appropriately commented. **Expected Input Format:** - No external input is needed as you are using built-in datasets and functions. **Expected Output Format:** - Print the accuracy of the logistic regression model on the Iris test set. - Print the accuracy of the logistic regression model on the synthetic test set. **Complete the following code template:** ```python from sklearn.datasets import load_iris, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X_iris = iris.data y_iris = iris.target # Generate a synthetic dataset X_synthetic, y_synthetic = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=3, random_state=42) # Split the Iris dataset into training and testing sets X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) # Split the synthetic dataset into training and testing sets X_synthetic_train, X_synthetic_test, y_synthetic_train, y_synthetic_test = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42) # Implement and train a logistic regression model for the Iris dataset log_reg_iris = LogisticRegression(max_iter=200) log_reg_iris.fit(X_iris_train, y_iris_train) y_iris_pred = log_reg_iris.predict(X_iris_test) accuracy_iris = accuracy_score(y_iris_test, y_iris_pred) print(\\"Iris Dataset Accuracy:\\", accuracy_iris) # Implement and train a logistic regression model for the synthetic dataset log_reg_synthetic = LogisticRegression(max_iter=200) log_reg_synthetic.fit(X_synthetic_train, y_synthetic_train) y_synthetic_pred = log_reg_synthetic.predict(X_synthetic_test) accuracy_synthetic = accuracy_score(y_synthetic_test, y_synthetic_pred) print(\\"Synthetic Dataset Accuracy:\\", accuracy_synthetic) ``` **End of Question**","solution":"from sklearn.datasets import load_iris, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def load_and_preprocess_iris(): # Load the Iris dataset iris = load_iris() X_iris = iris.data y_iris = iris.target # Split the Iris dataset into training and testing sets X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) return X_iris_train, X_iris_test, y_iris_train, y_iris_test def load_and_preprocess_synthetic(): # Generate a synthetic dataset X_synthetic, y_synthetic = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=3, random_state=42) # Split the synthetic dataset into training and testing sets X_synthetic_train, X_synthetic_test, y_synthetic_train, y_synthetic_test = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42) return X_synthetic_train, X_synthetic_test, y_synthetic_train, y_synthetic_test def train_logistic_regression(X_train, y_train): # Implement and train a logistic regression model log_reg = LogisticRegression(max_iter=200) log_reg.fit(X_train, y_train) return log_reg def evaluate_model(model, X_test, y_test): # Predict and evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Load and preprocess data for both datasets X_iris_train, X_iris_test, y_iris_train, y_iris_test = load_and_preprocess_iris() X_synthetic_train, X_synthetic_test, y_synthetic_train, y_synthetic_test = load_and_preprocess_synthetic() # Train and evaluate logistic regression for the Iris dataset log_reg_iris = train_logistic_regression(X_iris_train, y_iris_train) accuracy_iris = evaluate_model(log_reg_iris, X_iris_test, y_iris_test) print(\\"Iris Dataset Accuracy:\\", accuracy_iris) # Train and evaluate logistic regression for the synthetic dataset log_reg_synthetic = train_logistic_regression(X_synthetic_train, y_synthetic_train) accuracy_synthetic = evaluate_model(log_reg_synthetic, X_synthetic_test, y_synthetic_test) print(\\"Synthetic Dataset Accuracy:\\", accuracy_synthetic)"},{"question":"# Advanced Coding Assessment: Handling Various Data Types in pandas Objective: Your task is to implement a function that processes a DataFrame containing various specialized pandas data types. You should demonstrate your comprehension of manipulating datetime, timedeltas, periods, intervals, nullable integers, and categorical data. Problem Statement: Write a function `process_special_data_types(df: pd.DataFrame) -> pd.DataFrame` that accepts a DataFrame with the following columns: - **tz_datetime**: A column of timezone-aware datetime values. - **timedelta**: A column of timedelta values. - **period_month**: A column of period values with frequency of months. - **int_nullable**: A column of nullable integer values. - **category_data**: A column of categorical values (colors: \'red\', \'green\', \'blue\'). Your function should: 1. Convert the `tz_datetime` column to another timezone (\'America/New_York\'). 2. Add 1 day to each timedelta value in the `timedelta` column. 3. Convert the `period_month` column to timestamps representing the start of each period. 4. Fill any missing values in the `int_nullable` column with the mean of the non-missing integers. 5. Reorder categories in the `category_data` column to be [\'green\', \'red\', \'blue\'] and encode the categories with integer codes (0, 1, 2). Constraints: - Assume the input DataFrame has no missing values except for the `int_nullable` column. - The DataFrame may contain thousands of rows, so performance considerations are important. Example: ```python import pandas as pd import numpy as np import pandas as pd import numpy as np import pandas as pd import numpy as np from pandas import Timestamp, Timedelta, Period import pytz # Create example DataFrame data = { \'tz_datetime\': [pd.Timestamp(\'2023-01-01 10:00:00\', tz=\'UTC\'), pd.Timestamp(\'2023-01-02 15:00:00\', tz=\'UTC\')], \'timedelta\': [pd.Timedelta(days=5), pd.Timedelta(days=10)], \'period_month\': [pd.Period(\'2023-01\', freq=\'M\'), pd.Period(\'2023-02\', freq=\'M\')], \'int_nullable\': pd.array([1, np.nan], dtype=pd.Int64Dtype()), \'category_data\': pd.Categorical([\'red\', \'blue\'], categories=[\'red\', \'green\', \'blue\']) } df = pd.DataFrame(data) print(process_special_data_types(df)) ``` Output: ``` tz_datetime timedelta period_month int_nullable category_data 0 2023-01-01 05:00:00-05:00 6 days 2023-01-01 1 1 1 2023-01-02 10:00:00-05:00 11 days 2023-02-01 1 2 ``` Implementation: ```python import pandas as pd def process_special_data_types(df: pd.DataFrame) -> pd.DataFrame: # Convert tz_datetime to \'America/New_York\' timezone df[\'tz_datetime\'] = df[\'tz_datetime\'].dt.tz_convert(\'America/New_York\') # Add 1 day to each value in the timedelta column df[\'timedelta\'] = df[\'timedelta\'] + pd.Timedelta(days=1) # Convert period_month to start time of the period df[\'period_month\'] = df[\'period_month\'].dt.start_time # Fill missing values in int_nullable column with mean mean_val = df[\'int_nullable\'].mean() df[\'int_nullable\'] = df[\'int_nullable\'].fillna(mean_val) # Reorder categories and encode them df[\'category_data\'] = df[\'category_data\'].cat.reorder_categories([\'green\', \'red\', \'blue\'], ordered=True) df[\'category_data\'] = df[\'category_data\'].cat.codes return df ```","solution":"import pandas as pd def process_special_data_types(df: pd.DataFrame) -> pd.DataFrame: # Convert tz_datetime to \'America/New_York\' timezone df[\'tz_datetime\'] = df[\'tz_datetime\'].dt.tz_convert(\'America/New_York\') # Add 1 day to each value in the timedelta column df[\'timedelta\'] = df[\'timedelta\'] + pd.Timedelta(days=1) # Convert period_month to start time of the period df[\'period_month\'] = df[\'period_month\'].dt.start_time # Fill missing values in int_nullable column with mean of non-missing values mean_val = df[\'int_nullable\'].mean(skipna=True) df[\'int_nullable\'] = df[\'int_nullable\'].fillna(mean_val) # Reorder categories and encode them df[\'category_data\'] = df[\'category_data\'].cat.set_categories([\'green\', \'red\', \'blue\'], ordered=True) df[\'category_data\'] = df[\'category_data\'].cat.codes return df"},{"question":"Objective Design and implement a WSGI application that serves different responses based on the URL path accessed. You will need to utilize various utilities provided by the `wsgiref` module and ensure your application is WSGI-compliant. Requirements 1. Implement a WSGI application that handles different responses based on the following URL paths: - `/`: Should return a \\"Hello, World!\\" message. - `/time`: Should return the current server time. - `/greet/{name}`: Should return a personalized greeting using the name provided in the URL. - Any other path should return a 404 Not Found error with an appropriate message. 2. Use `wsgiref.util` functions where applicable to handle the environment variables. 3. Ensure WSGI compliance by validating the application using `wsgiref.validate`. 4. Implement and test your application on a local server using `wsgiref.simple_server`. Constraints - Your implementation should handle different paths efficiently. - Make sure to include proper error handling and response headers. - Adhere to the WSGI specification as described in **PEP 3333**. Input and Output Formats - Input: HTTP requests to the WSGI application. - Output: HTTP responses with appropriate status codes and messages. Example Usage Here is an example of how your WSGI application might be tested: ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator def my_wsgi_app(environ, start_response): # Implement your WSGI application here pass # Wrap the application in the validator validated_app = validator(my_wsgi_app) # Create and start the server with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Test Cases 1. Accessing `/` should return \\"Hello, World!\\" with status 200. 2. Accessing `/time` should return the current server time in plain text with status 200. 3. Accessing `/greet/John` should return \\"Hello, John!\\" with status 200. 4. Accessing `/unknown` should return a 404 Not Found response with a meaningful message. # Evaluation Criteria - Correctness: Does the application return the correct responses for various paths? - WSGI Compliance: Is the application compliant with **PEP 3333**? - Code Quality: Is the code well-structured, readable, and maintainable? - Efficient use of `wsgiref.util` functions and proper validation using `wsgiref.validate`.","solution":"from wsgiref.util import setup_testing_defaults from wsgiref.validate import validator from wsgiref.simple_server import make_server import datetime import re def my_wsgi_app(environ, start_response): setup_testing_defaults(environ) path = environ[\'PATH_INFO\'] if path == \'/\': status = \'200 OK\' response = \'Hello, World!\' headers = [(\'Content-Type\', \'text/plain\')] elif path == \'/time\': status = \'200 OK\' response = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') headers = [(\'Content-Type\', \'text/plain\')] elif re.match(r\'^/greet/w+\', path): name = path.split(\'/\')[-1] status = \'200 OK\' response = f\'Hello, {name}!\' headers = [(\'Content-Type\', \'text/plain\')] else: status = \'404 Not Found\' response = \'404 Not Found: The requested URL was not found on this server.\' headers = [(\'Content-Type\', \'text/plain\')] start_response(status, headers) return [response.encode(\'utf-8\')] if __name__ == \'__main__\': # Wrap the application with the validator validated_app = validator(my_wsgi_app) # Run the application on a local server with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Coding Assessment Question # Objective The objective of this task is to assess your understanding of the `email` package in Python by requiring you to implement a function that sends a detailed email. This email must include both plain text and HTML content with inline images, and it should handle multiple recipients. # Task Write a function `send_detailed_email` that takes the following parameters: - `sender` (str): The email address of the sender. - `recipients` (list of str): A list of email addresses of the recipients. - `subject` (str): The subject of the email. - `plain_text` (str): The plain text version of the email content. - `html_content` (str): The HTML version of the email content. This content may contain references to embedded images using `cid`. - `images` (dict): A dictionary where the keys are Content-IDs (used in the HTML) and values are paths to the image files to be embedded in the email. Your function should: 1. Create an email message with both plain text and HTML content parts. 2. Embed the images referenced in the HTML content using the provided dictionary. 3. Send the email to the specified recipients using `smtplib.SMTP`. # Constraints - The email server to use will be `localhost`. - You must handle any exceptions that may occur during the process and log relevant error messages. # Implementation Details - Use the `email.message.EmailMessage` class to create the email message. - Use the `add_alternative` method to add the HTML content. - Ensure that the images provided in the `images` dictionary are correctly embedded in the HTML content. # Example Usage ```python def send_detailed_email(sender, recipients, subject, plain_text, html_content, images): # Your implementation goes here pass # Example HTML content with image reference html_content = <html> <head></head> <body> <p>This is an example email with an embedded image:</p> <img src=\\"cid:image1\\" /> </body> </html> # Images dictionary images = { \'image1\': \'/path/to/image1.jpg\' } send_detailed_email( sender=\'sender@example.com\', recipients=[\'recipient1@example.com\', \'recipient2@example.com\'], subject=\'Test Email with HTML and Images\', plain_text=\'This is the plain text version of the email.\', html_content=html_content, images=images ) ``` # Hints - Remember to read the image files in binary mode (`\'rb\'`). - Use the `add_related` method to add the images to the HTML part of the message. - Test your function in an environment where a local SMTP server is available.","solution":"import smtplib from email.message import EmailMessage import mimetypes import os def send_detailed_email(sender, recipients, subject, plain_text, html_content, images): Sends an email with both plain text and HTML content, including embedded images. Parameters: sender (str): The email address of the sender. recipients (list of str): A list of email addresses of the recipients. subject (str): The subject of the email. plain_text (str): The plain text version of the email content. html_content (str): The HTML version of the email content. images (dict): A dictionary where the keys are Content-IDs and values are paths to the image files. try: msg = EmailMessage() msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) msg[\'Subject\'] = subject # Set the plain text and HTML content msg.set_content(plain_text) msg.add_alternative(html_content, subtype=\'html\') # Attach images referenced in the HTML content for cid, img_path in images.items(): with open(img_path, \'rb\') as img_file: maintype, subtype = mimetypes.guess_type(img_path)[0].split(\'/\', 1) msg.get_payload()[1].add_related(img_file.read(), maintype=maintype, subtype=subtype, cid=f\'<{cid}>\') # Send the email via local SMTP server with smtplib.SMTP(\'localhost\') as smtp: smtp.send_message(msg) except Exception as e: print(f\\"Error: {e}\\")"},{"question":"Objective Implement a function to dynamically import a module in Python, using specified module importation functions, and reload the module if it\'s already loaded. Problem Statement Write a Python function `dynamic_import_and_reload(module_name: str) -> Optional[ModuleType]` that attempts to import a module given its string name. If the module is already loaded, it should reload the module. If there is any error during importation or reloading, the function should return `None`. The function should provide informative error messages for failing imports and re-imports. Function Signature ```python from types import ModuleType from typing import Optional def dynamic_import_and_reload(module_name: str) -> Optional[ModuleType]: pass ``` Expected Input and Output - Input: `module_name` - A string representing the module name to import (e.g., `\\"math\\"`). - Output: If successful, return the module object. If failed, return `None`. Constraints 1. The module name passed to the function will be a valid Python identifier. 2. You should utilize Python\'s built-in capabilities only (no direct C API calls). 3. The function must handle both regular module imports and submodule imports. 4. The function should ensure exceptions are raised on errors and handle them gracefully. 5. Performance is not a key requirement but handling edge cases and providing useful feedback is essential. Here are some example use cases: ```python # Example usage: module = dynamic_import_and_reload(\\"math\\") assert module is not None non_existent_module = dynamic_import_and_reload(\\"non_existent_module\\") assert non_existent_module is None reloaded_module = dynamic_import_and_reload(\\"math\\") assert reloaded_module is not None ``` Additional Requirements 1. Include inline comments in your implementation explaining your approach. 2. Assume Python 3.10 is used, no need to handle unsupported versions. 3. Focus on robustness and handling different scenarios gracefully. Good luck!","solution":"from types import ModuleType from typing import Optional import importlib import sys def dynamic_import_and_reload(module_name: str) -> Optional[ModuleType]: Dynamically imports a module using the module name. If the module is already loaded, it reloads the module. If there\'s any error during import or reload, it returns None. Args: module_name (str): The name of the module to import. Returns: Optional[ModuleType]: The imported (or reloaded) module if successful, otherwise None. try: # Check if module is already loaded if module_name in sys.modules: # If module is already loaded, reload it module = importlib.reload(sys.modules[module_name]) else: # If module is not loaded, import it module = importlib.import_module(module_name) return module except Exception as e: # Print the error message print(f\\"Failed to import or reload module \'{module_name}\': {e}\\") return None"},{"question":"# PyTorch Coding Assessment Question **Objective:** Design a function in PyTorch that generates a random tensor, applies a specified transformation, and ensures reproducibility by setting a seed. **Problem Statement:** Write a function `generate_transformed_tensor` that takes the following parameters: 1. `shape` (tuple of int): The shape of the random tensor to generate. 2. `transformation` (callable): A function that takes a tensor as input and returns a transformed tensor. 3. `seed` (int): An optional parameter to set the random seed for reproducibility. The function should: - Generate a random tensor of the specified shape with values drawn from a standard normal distribution. - Apply the specified transformation to the generated tensor. - Ensure that the tensor generation is reproducible by setting the random seed if provided. **Expected Input and Output Formats:** ```python def generate_transformed_tensor(shape: tuple, transformation: callable, seed: int = None) -> torch.Tensor: # Your implementation here ``` * `shape`: A tuple (e.g., `(3, 3)`) specifying the dimensions of the tensor. * `transformation`: A function that takes a tensor as input and returns a transformed tensor. Example: ```python def example_transformation(tensor): return tensor * 2 ``` * `seed`: An optional integer to set the random seed for reproducibility. * The function should return a transformed tensor of the same shape as specified. **Constraints:** - The function should use `torch.randn` to generate the random tensor. - If `seed` is provided, use `torch.manual_seed` to set the seed before generating the tensor. **Example:** ```python import torch def example_transformation(tensor): return tensor * 2 # Example usage tensor = generate_transformed_tensor((3, 3), example_transformation, seed=42) print(tensor) ``` If the function is implemented correctly, the output should be the same every time the script runs with the same seed. ```plaintext Expected Output: tensor([[-1.4276, 1.6068, 1.5174], [-1.6662, 0.2376, -2.1461], [ 0.5125, 0.9143, -0.9535]]) ``` **Additional Information:** To complete this task, you may refer to the PyTorch documentation for `torch.randn` and `torch.manual_seed`.","solution":"import torch def generate_transformed_tensor(shape: tuple, transformation: callable, seed: int = None) -> torch.Tensor: Generates a random tensor of the specified shape, applies a transformation, and ensures reproducibility using a seed. Parameters: - shape (tuple of int): Shape of the random tensor to generate. - transformation (callable): A function that takes a tensor as input and returns a transformed tensor. - seed (int, optional): Seed for random number generation to ensure reproducibility. Returns: - torch.Tensor: Transformed tensor. if seed is not None: torch.manual_seed(seed) random_tensor = torch.randn(shape) transformed_tensor = transformation(random_tensor) return transformed_tensor"},{"question":"**Question: Implement a Reusable and Reentrant Logging Context Manager** You are required to implement a context manager in Python that logs messages when entering and exiting a context. This context manager should be both reusable and reentrant, meaning it can be used in multiple `with` statements as well as within a `with` statement that is already using the same context manager. **Instructions**: 1. Implement a `LoggerContext` class that inherits from `contextlib.AbstractContextManager`. 2. The context manager should: - Log a message when entering the context with the context name. - Log a message when exiting the context with the context name. - Ensure that these messages are correctly logged even when the context manager is reused or reentered. **Requirements**: - The class should have methods `__enter__()` and `__exit__()`, where `__exit__()` should catch and log exceptions if any are raised within the context. - Utilize `contextlib.ExitStack` to handle proper resource management. - Provide mechanisms to specify the log message format and destination (e.g., a file or `sys.stdout`). **Example Usage**: ```python import sys from contextlib import redirect_stdout class LoggerContext(contextlib.AbstractContextManager): def __init__(self, name, log_file=None): self.name = name self.log_file = log_file def __enter__(self): self._log(f\\"Entering: {self.name}\\") return self def __exit__(self, exc_type, exc_value, traceback): self._log(f\\"Exiting: {self.name}\\") if exc_value: self._log(f\\"Exception: {exc_value}\\") def _log(self, message): if self.log_file: with open(self.log_file, \'a\') as f: f.write(message + \'n\') else: print(message) # Example usage: log_context = LoggerContext(\'example_context\') # Using as a context manager with log_context: with log_context: print(\\"Inside nested context\\") # Reusing the context manager with log_context: print(\\"Reusing context manager\\") ``` **Constraints**: - Ensure the logging operations are thread-safe if applicable. - The implementation should handle large numbers of nested contexts gracefully. **Performance Requirements**: - The context manager should log messages with minimal overhead to the application. Submit your implementation for review.","solution":"import sys from contextlib import AbstractContextManager, ExitStack class LoggerContext(AbstractContextManager): def __init__(self, name, log_file=None): self.name = name self.log_file = log_file self.stack = ExitStack() def __enter__(self): self._log(f\\"Entering: {self.name}\\") self.stack.__enter__() return self def __exit__(self, exc_type, exc_value, traceback): self._log(f\\"Exiting: {self.name}\\") self.stack.__exit__(exc_type, exc_value, traceback) if exc_value: self._log(f\\"Exception: {exc_value}\\") def _log(self, message): if self.log_file: with open(self.log_file, \'a\') as f: f.write(message + \'n\') else: print(message) # Example usage: log_context = LoggerContext(\'example_context\') # Using as a context manager with log_context: with log_context: print(\\"Inside nested context\\") # Reusing the context manager with log_context: print(\\"Reusing context manager\\")"},{"question":"# Advanced Python Coding Assessment Question Objective To assess the student\'s ability to create and manipulate arrays using Python\'s `array` module. Problem Statement Implement a function called `max_contiguous_product` that finds the maximum product of a contiguous subarray within a given array of floating point numbers. The array will be provided in the form of the `array` module\'s array. Function Signature ```python def max_contiguous_product(arr: array) -> float: pass ``` Input - `arr`: An array of floating point numbers (type code `\'d\'`). Output - Returns the maximum product of a contiguous subarray as a floating-point number. Constraints - The length of the array will be at most 10^5. - Each floating-point number in the array will have an absolute value less than or equal to 10. Performance Requirements - The function should have a time complexity of O(n), where n is the length of the array. Example ```python from array import array # Example 1: arr = array(\'d\', [2.0, 3.0, -2.0, 4.0]) print(max_contiguous_product(arr)) # Output: 6.0 (subarray [2.0, 3.0] has the largest product) # Example 2: arr = array(\'d\', [-2.0, 0.0, -1.0]) print(max_contiguous_product(arr)) # Output: 0.0 ``` Notes - A contiguous subarray of the array is defined as a sequence of elements that are neighboring each other. - Negative and positive numbers could appear in any sequence, making the product either increase or decrease. Write your function to solve the problem as specified. Ensure to handle edge cases and optimize for performance as required.","solution":"from array import array def max_contiguous_product(arr: array) -> float: if not arr: return 0.0 max_product = arr[0] min_product = arr[0] result = arr[0] for num in arr[1:]: if num < 0: max_product, min_product = min_product, max_product max_product = max(num, max_product * num) min_product = min(num, min_product * num) result = max(result, max_product) return result"},{"question":"# Question: Implement an Optimized Matrix Multiplication Function with Dynamic Batch Sizes Objective: Write a Python function that multiplies two matrices with dynamic batch sizes using `torch.compile` and TorchDynamo. Your goal is to ensure that the generated graph adapts efficiently for different batch sizes during execution. Task: 1. Create a function `batched_matrix_multiplication` that takes two 3D tensors as input, representing batches of matrices. 2. Use the `torch.compile` decorator to optimize this function. 3. Ensure that your function handles varying batch sizes dynamically. Requirements: - Input tensors `A` and `B` will be of shape `(batch_size, M, N)` and `(batch_size, N, P)` respectively. - Output should be a tensor of shape `(batch_size, M, P)`. - The output should multiply corresponding matrices in the batches. - Utilize symbolic shapes, ensuring that the optimization adapts to changes in batch size without recompilation when possible. Function Signature: ```python import torch @torch.compile def batched_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Your implementation here ``` Example: ```python import torch A = torch.randn(5, 10, 20) B = torch.randn(5, 20, 30) result = batched_matrix_multiplication(A, B) print(result.shape) # Output should be: torch.Size([5, 10, 30]) ``` Hint: Refer to the concept of symbolic shapes and guards in TorchDynamo to handle dynamic batch sizes efficiently. Constraints: - Matrix dimensions (M, N, P) are guaranteed to match for valid matrix multiplication. - You may use any PyTorch operations provided it remains within the `torch.compile` ecosystem. Evaluation Criteria: - Correctness: The function should correctly perform batched matrix multiplication. - Performance: The function should efficiently handle varying batch sizes. - Use of dynamic shapes and tracing: Proper implementation of symbolic shapes, minimizing recompilation.","solution":"import torch import torch._dynamo as dynamo @dynamo.optimize(\\"eager\\") def batched_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs batched matrix multiplication on two 3D tensors. Args: - A (torch.Tensor): A 3D tensor of shape (batch_size, M, N). - B (torch.Tensor): A 3D tensor of shape (batch_size, N, P). Returns: - torch.Tensor: A 3D tensor of shape (batch_size, M, P), resulting from the batch-wise matrix multiplication of A and B. return torch.bmm(A, B)"},{"question":"# Quoted-Printable Encoding and Decoding You are tasked with creating a Python script that performs quoted-printable encoding and decoding using binary file objects. For this task, you\'ll leverage the `quopri` module. Your script should: 1. Accept a command-line argument `mode` which can be either `\'encode\'` or `\'decode\'`. 2. Always accept an input filename and an output filename as command-line arguments. 3. When in `encode` mode, encode the content of the input file using quoted-printable encoding and write it to the output file. 4. When in `decode` mode, decode the content of the input file from quoted-printable encoding and write it to the output file. 5. Support an optional argument `--header` which, when present in `encode` mode, encodes spaces as underscores, and in `decode` mode, decodes underscores as spaces. 6. Support an optional argument `--quotetabs` for encoding mode, indicating whether to encode embedded spaces and tabs. # Input and Output Format **Command-line Arguments:** - `mode`: `\'encode\'` or `\'decode\'` - `input_filename`: Path to the input file. - `output_filename`: Path to the output file. - `--header`: Optional flag to handle spaces as underscores and vice versa. - `--quotetabs`: Optional flag to encode spaces and tabs, valid only in encode mode. # Example Usage ```shell python quopri_tool.py encode input.txt output.txt --quotetabs --header python quopri_tool.py decode encoded.txt decoded.txt --header ``` # Constraints - The script should handle I/O errors gracefully. - Assume input files are in binary mode and contain text data that can be represented as ASCII for encoding. - Performance should be efficient for large files. # Deliverables 1. **quopri_tool.py**: The Python script implementing the above functionality. 2. Use the `quopri` module functions to perform the encode and decode operations. 3. Example: For encoding a text file with embedded tabs and spaces, verify that the output is correctly encoded. # Hints - Use the `argparse` module for handling command-line arguments. - Remember to open your files in binary mode `\'rb\'`/`\'wb\'`. # Evaluation Criteria - Correctness: The script should correctly encode and decode files as per the specified settings. - Robustness: The script should handle errors gracefully. - Code clarity: The code should be well-organized and commented where necessary.","solution":"import argparse import quopri def quoted_printable_tool(mode, input_filename, output_filename, header=False, quotetabs=False): try: with open(input_filename, \'rb\') as infile, open(output_filename, \'wb\') as outfile: if mode == \'encode\': if header: quopri.encode(infile, outfile, quotetabs=quotetabs, header=True) else: quopri.encode(infile, outfile, quotetabs=quotetabs) elif mode == \'decode\': if header: quopri.decode(infile, outfile, header=True) else: quopri.decode(infile, outfile) else: raise ValueError(\\"Invalid mode. Use \'encode\' or \'decode\'.\\") except IOError as e: print(f\\"File error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\\"Quoted-Printable Encode/Decode Tool\\") parser.add_argument(\'mode\', choices=[\'encode\', \'decode\'], help=\\"Operation mode, either \'encode\' or \'decode\'\\") parser.add_argument(\'input_filename\', help=\\"Input file name\\") parser.add_argument(\'output_filename\', help=\\"Output file name\\") parser.add_argument(\'--header\', action=\'store_true\', help=\\"Handle spaces as underscores in headers\\") parser.add_argument(\'--quotetabs\', action=\'store_true\', help=\\"Encode embedded spaces and tabs (encode mode only)\\") args = parser.parse_args() quoted_printable_tool( mode=args.mode, input_filename=args.input_filename, output_filename=args.output_filename, header=args.header, quotetabs=args.quotetabs )"},{"question":"# Question: Python Installation Path Inspector You are tasked with implementing a function to inspect and display specific paths of the Python installation based on given schemes and path names. Use the `sysconfig` module in Python to accomplish this. Function Signature ```python def inspect_python_paths(schemes: list, path_names: list) -> dict: pass ``` # Description Given a list of scheme names and a list of path names, the function should return a dictionary where each scheme name maps to another dictionary. This nested dictionary should map each path name to its corresponding path as determined by the `sysconfig` module. If a scheme or path name is invalid, return `None` for that particular mapping. # Input - `schemes` (list): A list of scheme names (str) which can be obtained from `sysconfig.get_scheme_names()`. - `path_names` (list): A list of path names (str) which can be obtained from `sysconfig.get_path_names()`. # Output - (dict): A dictionary where each key is a scheme name (str) and its value is a dictionary mapping each path name (str) to its inspected path (str). # Constraints - You can assume that `schemes` and `path_names` will be non-empty lists. - You should validate scheme names and path names against the values provided by `sysconfig.get_scheme_names()` and `sysconfig.get_path_names()` respectively. If any value is not valid, map it to `None` in the resulting dictionary. # Example ```python import sysconfig def inspect_python_paths(schemes, path_names): valid_schemes = sysconfig.get_scheme_names() valid_path_names = sysconfig.get_path_names() result = {} for scheme in schemes: if scheme not in valid_schemes: result[scheme] = {path_name: None for path_name in path_names} else: result[scheme] = {} for path_name in path_names: if path_name not in valid_path_names: result[scheme][path_name] = None else: try: result[scheme][path_name] = sysconfig.get_path(path_name, scheme) except KeyError: result[scheme][path_name] = None return result # Example usage schemes = [\'posix_prefix\', \'nt_user\'] path_names = [\'stdlib\', \'scripts\', \'invalid_path\'] print(inspect_python_paths(schemes, path_names)) ``` Expected Output ```python { \'posix_prefix\': { \'stdlib\': \'/usr/local/lib/python3.10\', \'scripts\': \'/usr/local/bin\', \'invalid_path\': None }, \'nt_user\': { \'stdlib\': \'C:Users<username>AppDataLocalProgramsPythonPython310lib\', \'scripts\': \'C:Users<username>AppDataLocalProgramsPythonPython310Scripts\', \'invalid_path\': None } } ``` This function makes use of the configuration and path retrieving functionalities of the `sysconfig` module to dynamically inspect and return Python installation paths based on given schemes and path names. Ensure your function handles invalid inputs gracefully by returning `None` for invalid schemes or path names.","solution":"import sysconfig def inspect_python_paths(schemes, path_names): valid_schemes = sysconfig.get_scheme_names() valid_path_names = sysconfig.get_path_names() result = {} for scheme in schemes: if scheme not in valid_schemes: result[scheme] = {path_name: None for path_name in path_names} else: result[scheme] = {} for path_name in path_names: if path_name not in valid_path_names: result[scheme][path_name] = None else: try: result[scheme][path_name] = sysconfig.get_path(path_name, scheme) except KeyError: result[scheme][path_name] = None return result # Example usage schemes = [\'posix_prefix\', \'nt_user\'] path_names = [\'stdlib\', \'scripts\', \'invalid_path\'] print(inspect_python_paths(schemes, path_names))"},{"question":"# Question: Using the `xml.dom.pulldom` module, write a function to parse an XML string and return a list of item descriptions for items priced over a given threshold. XML documents to be parsed will follow this structure: ```xml <catalog> <item> <description>Item 1</description> <price>30</price> </item> <item> <description>Item 2</description> <price>70</price> </item> <item> <description>Item 3</description> <price>50</price> </item> <!-- Additional items can be present --> </catalog> ``` Your task is to implement the following function: ```python from typing import List from xml.dom import pulldom def get_items_above_price(xml_data: str, price_threshold: int) -> List[str]: Parses an XML string of items and returns descriptions of items priced over the given threshold. Parameters: xml_data (str): A string representation of the XML document. price_threshold (int): The price threshold to filter items. Returns: List[str]: A list of descriptions for items priced above the threshold. pass ``` # Requirements: 1. Use the `xml.dom.pulldom` module for parsing the XML. 2. The function should return a list of descriptions for items where the price exceeds the `price_threshold`. 3. Expand the `<item>` node to access its children if the price meets the criteria. # Example: ```python xml_data = <catalog> <item> <description>Item 1</description> <price>30</price> </item> <item> <description>Item 2</description> <price>70</price> </item> <item> <description>Item 3</description> <price>50</price> </item> </catalog> assert get_items_above_price(xml_data, 50) == [\\"Item 2\\"] assert get_items_above_price(xml_data, 25) == [\\"Item 1\\", \\"Item 2\\", \\"Item 3\\"] assert get_items_above_price(xml_data, 70) == [] ``` Your solution will be evaluated based on correctness, efficiency, and proper usage of the `xml.dom.pulldom` module.","solution":"from typing import List from xml.dom import pulldom def get_items_above_price(xml_data: str, price_threshold: int) -> List[str]: Parses an XML string of items and returns descriptions of items priced over the given threshold. Parameters: xml_data (str): A string representation of the XML document. price_threshold (int): The price threshold to filter items. Returns: List[str]: A list of descriptions for items priced above the threshold. doc = pulldom.parseString(xml_data) descriptions = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': doc.expandNode(node) price_node = node.getElementsByTagName(\'price\')[0] price = int(price_node.firstChild.nodeValue) if price > price_threshold: description_node = node.getElementsByTagName(\'description\')[0] description = description_node.firstChild.nodeValue descriptions.append(description) return descriptions"},{"question":"**Title**: Programmatic Module Import and Metadata Handling **Objective**: Assess the understanding of programmatic module importing and handling package metadata using Python\'s `importlib` and `importlib.metadata`. **Problem Statement**: You are tasked with writing a Python function that performs the following: 1. **Programmatically Imports a Module**: - Given a module name as a string, import the module programmatically using the `importlib` package. - If the module is not available, raise an `ImportError` with a message stating the module is not found. 2. **Fetch Metadata for the Imported Module**: - Once the module is successfully imported, use the `importlib.metadata` package to retrieve and return the metadata of the module. - Specifically, fetch and return the version of the module. If the version information is not available, return a message stating \\"Version information not available\\". **Function Signature**: ```python def import_and_fetch_metadata(module_name: str) -> str: pass ``` **Input**: - `module_name` (str): The name of the module to be imported. **Output**: - A string representing the version of the imported module, or an error message if the module cannot be imported or version information is unavailable. **Constraints**: - The function should handle exceptions gracefully and provide informative error messages. - Assume that the environment has a standard Python installation with common modules available. **Examples**: ```python # Example 1: # Input module_name = \\"math\\" # Output \'Module: math, Version: [MODULE_VERSION]\' # Example 2: # Input module_name = \\"nonexistentmodule\\" # Output \'ImportError: The module \\"nonexistentmodule\\" is not found.\' # Example 3: # Input module_name = \\"some_module_without_version\\" # Output \'Module: some_module_without_version, Version: Version information not available\' ``` **Hints**: - Refer to `importlib.import_module` for importing modules programmatically. - Use the `importlib.metadata` API to fetch the metadata of the imported module. - Handle exceptions to manage modules that are not found or do not have version information.","solution":"import importlib from importlib import metadata def import_and_fetch_metadata(module_name: str) -> str: try: # Dynamically import the module imported_module = importlib.import_module(module_name) except ModuleNotFoundError: return f\'ImportError: The module \\"{module_name}\\" is not found.\' try: # Fetch the version of the module version = metadata.version(module_name) return f\'Module: {module_name}, Version: {version}\' except metadata.PackageNotFoundError: return f\'Module: {module_name}, Version: Version information not available\'"},{"question":"**Context:** You have been provided with a dataset named \\"penguins\\" with a focus on analyzing the \'flipper_length_mm\' and \'species\' columns. Your task is to create a multi-faceted Seaborn density plot using the objects API that meets the following specifications: **Specifications:** 1. Plot a Kernel Density Estimation (KDE) of the \'flipper_length_mm\'. 2. Adjust the KDE smoothing bandwidth to highlight the details. 3. Overlay the KDE plot with histogram bars to show the raw data distribution. 4. Separate the density plots by the different species present in the dataset. 5. Ensure the density plots for each species share a common grid for normalized comparison. 6. Increase the resolution of the grid for KDE evaluation. 7. Integrate the density cumulatively to visualize the cumulative distribution. The question tests your ability to utilize various Seaborn functionalities to create sophisticated visualizations. **Instructions:** 1. Load the dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. Create a density plot of \'flipper_length_mm\' with the following: - A KDE plot with a bandwidth adjustment (use bw_adjust=0.25). - Overlay histogram bars with 30% transparency. - Separate the density curves by species. - Use a common grid for the density evaluation. - Increase the resolution of the KDE evaluation grid to 200. - Include a cumulative KDE line. **Expected Function:** ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_density_plot(): # Load dataset penguins = load_dataset(\\"penguins\\") # Create the initial plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") # Add a histogram with 30% transparency p.add(so.Bars(alpha=0.3), so.Hist(\'density\')) # Add a KDE plot with bandwidth adjustment and higher resolution p.add(so.Area(), so.KDE(bw_adjust=0.25, gridsize=200), color=\\"species\\", common_norm=True) # Add the cumulative KDE line p.add(so.Line(), so.KDE(cumulative=True, bw_adjust=0.25)) # Display the plot p.show() # Call the function to create and display the plot create_custom_density_plot() ``` Note: Ensure Seaborn is properly installed and the relevant libraries are imported.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_density_plot(): # Load dataset penguins = load_dataset(\\"penguins\\") # Create the initial plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") # Add a histogram with 30% transparency p.add(so.Bars(alpha=0.3), so.Hist(bins=30)) # Add a KDE plot with bandwidth adjustment and higher resolution p.add(so.Area(), so.KDE(bw_adjust=0.25, gridsize=200), color=\\"species\\", common_norm=True) # Add the cumulative KDE line p.add(so.Line(), so.KDE(cumulative=True, bw_adjust=0.25)) # Display the plot p.show() # Call the function to create and display the plot create_custom_density_plot()"},{"question":"Objective Design a function that processes a list of URLs concurrently to fetch HTML content. You need to implement this using the `concurrent.futures` module to fully leverage its capabilities for concurrent execution. Task Create a function called `fetch_html_contents` which takes in a list of URLs and returns a dictionary where the keys are URLs and the values are their corresponding HTML content fetched. Utilize the `concurrent.futures.ThreadPoolExecutor` to perform concurrent HTTP GET requests to fetch the HTML contents. Expected Function Signature ```python import concurrent.futures def fetch_html_contents(urls: List[str]) -> Dict[str, str]: pass ``` Input - `urls`: A list of strings where each string is a valid URL. Output - A dictionary where each key is a URL from the input list and the corresponding value is the HTML content fetched from that URL. Constraints 1. You must use `concurrent.futures.ThreadPoolExecutor`. 2. Handle exceptions gracefully to ensure the function does not crash if a URL is not accessible or an error occurs. 3. The maximum number of worker threads should be 5. 4. You may use any standard Python library for making HTTP requests (such as `requests`). Performance Requirements - Ensure that the function efficiently makes concurrent requests without blocking. - The function should be able to handle up to 20 URLs in a reasonable time frame. Example ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] html_contents = fetch_html_contents(urls) # Example Output (Values will be the actual HTML content fetched from each URL) # { # \\"http://example.com\\": \\"<html>...</html>\\", # \\"http://example.org\\": \\"<html>...</html>\\", # \\"http://example.net\\": \\"<html>...</html>\\" # } ``` Notes - For the sake of the example, you may consider using the `requests` library to perform HTTP GET requests.","solution":"import concurrent.futures import requests from typing import List, Dict def fetch_html_contents(urls: List[str]) -> Dict[str, str]: Fetch HTML contents for a list of URLs using concurrent future thread pool. Args: urls (List[str]): A list of URLs to fetch the HTML content from. Returns: Dict[str, str]: A dictionary mapping each URL to its HTML content. def fetch_url(url): try: response = requests.get(url) response.raise_for_status() return url, response.text except requests.RequestException as e: return url, str(e) html_contents = {} with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url, html_content = future.result() html_contents[url] = html_content return html_contents"},{"question":"# Coding Challenge: Complex EmailMessage Creation and Manipulation Objective Your task is to create and manipulate a complex email message using the `email.message.EmailMessage` class. You will need to create an email with multiple parts, set various headers, and finally serialize it into a string format. # Problem Statement Write a function `create_complex_email` that constructs an email message with the following structure and requirements: 1. The email should have a subject of \\"Complex Email\\". 2. The `From` header should be set to `\\"sender@example.com\\"`. 3. The `To` header should be set to `\\"recipient@example.com\\"`. 4. The email should have both HTML and plain text versions of the body: - The plain text version should be: `\\"This is the plain text version of the email.\\"` - The HTML version should be: `\\"<html><body><p>This is the <b>HTML</b> version of the email.</p></body></html>\\"` 5. The email should have a text file attachment with the filename \\"attachment.txt\\" and the content `\\"This is the attachment.\\"`. The function should return the serialized string representation of the entire email, with the headers properly set and all parts included. # Input No input parameters are required for this function. # Output - The function should return a string, which is the serialized representation of the full email message. # Constraints - You must use the `EmailMessage` class from the `email.message` module. - Ensure that the email properly indicates that it is a multipart email. - The text body, HTML body, and attachment should all be included correctly. # Example Usage ```python email_str = create_complex_email() print(email_str) ``` The output should be a string that represents the entire email in a valid serialized format. # Implementation ```python from email.message import EmailMessage from email.policy import default def create_complex_email(): # 1. Create the EmailMessage object msg = EmailMessage() # 2. Set the headers msg[\'Subject\'] = \\"Complex Email\\" msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" # 3. Set the plain text and HTML parts msg.set_content(\\"This is the plain text version of the email.\\") msg.add_alternative(\\"<html><body><p>This is the <b>HTML</b> version of the email.</p></body></html>\\", subtype=\'html\') # 4. Add an attachment msg.add_attachment(\\"This is the attachment.\\", filename=\\"attachment.txt\\") # 5. Serialize the message to a string email_str = msg.as_string(policy=default) return email_str ``` The above solution provides the full implementation of creating and manipulating an `EmailMessage` object based on the requirements. Ensure that you understand each step and the capabilities of the `email.message.EmailMessage` class as described in the documentation.","solution":"from email.message import EmailMessage from email.policy import default def create_complex_email(): Constructs a complex email with subject, from, to headers, plain text, HTML content, and a text file attachment. Returns the serialized string representation of the email. # Create the EmailMessage object msg = EmailMessage() # Set the headers msg[\'Subject\'] = \\"Complex Email\\" msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" # Set the plain text and HTML parts msg.set_content(\\"This is the plain text version of the email.\\") msg.add_alternative( \\"<html><body><p>This is the <b>HTML</b> version of the email.</p></body></html>\\", subtype=\'html\' ) # Add an attachment msg.add_attachment(\\"This is the attachment.\\", filename=\\"attachment.txt\\") # Serialize the message to a string email_str = msg.as_string(policy=default) return email_str"},{"question":"<|Analysis Begin|> The provided documentation is an overview of the `importlib.metadata` package introduced in Python 3.10, which allows access to installed package metadata. The main functionalities include: - Retrieving the version of an installed package. - Accessing package entry points. - Extracting metadata for distributions. - Getting files and requirements for a distribution. - Resolving package distributions. - Extending the search algorithm to find package metadata in custom locations. The documentation includes detailed examples of how to use these functions and classes, such as `entry_points`, `version`, `metadata`, `requires`, `files`, and the `Distribution` class. Key functionalities that could be tested in a coding assessment include: 1. Extracting metadata from installed packages. 2. Querying entry points and resolving specific entry points. 3. Retrieving version numbers and requirements for packages. 4. Accessing and reading files contained within a distribution. 5. Extending the search algorithm for finding distribution metadata. Overall, the documentation is sufficient to design a question that assesses the understanding and application of these functionalities. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Your task is to demonstrate your understanding of the `importlib.metadata` package by implementing a function that retrieves specific metadata information for a given installed package and formats the results. **Problem Statement**: Implement a function `package_info(package_name: str) -> dict` that receives the name of an installed Python package as input and returns a dictionary containing the following information: 1. The version of the package. 2. A list of all entry points categorized by their groups. 3. The package metadata (summary, author, license, etc.). # Function Signature: ```python def package_info(package_name: str) -> dict: pass ``` # Input: - `package_name` (str): The name of the installed Python package to retrieve information for. # Output: - (dict): A dictionary with keys `\'version\'`, `\'entry_points\'`, and `\'metadata\'`. ``` { \\"version\\": <str: version of the package>, \\"entry_points\\": { <group1>: [<entry_point1>, <entry_point2>, ...], ... }, \\"metadata\\": { <metadata_key>: <metadata_value>, ... } } ``` # Example: ```python result = package_info(\'wheel\') # The output dictionary would look similar to: { \\"version\\": \\"0.32.3\\", \\"entry_points\\": { \\"console_scripts\\": [\\"wheel\\", ... ], \\"distutils.commands\\": [...], ... }, \\"metadata\\": { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.32.3\', \'Summary\': \'A built-package format for Python.\', \'Author\': \'Vinay Sajip\', \'Author-email\': \'vinay_sajip@red-dove.com\', \'License\': \'MIT\', ... } } ``` # Constraints: - The package specified in `package_name` must be installed in the environment where the function is run. - The function should handle cases where package metadata is missing or incomplete by returning an appropriate message or empty structure. # Notes: - Use the `importlib.metadata` package to implement this functionality. - Ensure your function has error handling to manage situations where the package does not have certain metadata or entry points. # Hints: - Refer to the `version()`, `entry_points()`, and `metadata()` functions in the `importlib.metadata` package. Good luck!","solution":"from importlib.metadata import version, entry_points, metadata def package_info(package_name: str) -> dict: Retrieve specific metadata information for a given installed package. Parameters: package_name (str): The name of the installed Python package. Returns: dict: A dictionary containing version, entry_points, and metadata. try: pkg_version = version(package_name) except Exception: pkg_version = \\"Version information not found\\" try: eps = entry_points() pkg_eps = {ep.group: [ep.name for ep in group_eps] for ep.group, group_eps in eps.items() if ep.dist.name == package_name} except Exception: pkg_eps = {} try: pkg_meta = metadata(package_name) except Exception: pkg_meta = {\\"Metadata not found\\": \\"Information not available\\"} return { \\"version\\": pkg_version, \\"entry_points\\": pkg_eps, \\"metadata\\": dict(pkg_meta) }"},{"question":"# PyTorch CUDA Streams and Synchronization Objective: The objective of this exercise is to assess your understanding of CUDA streams in PyTorch, their potential pitfalls, and how to use synchronization techniques to avoid data races, as well as how to use the CUDA Stream Sanitizer to detect such issues. Task: You will need to write a PyTorch script that demonstrates your expertise in handling CUDA streams and synchronization. Specifically, you will be required to: 1. Create a synchronization error using two different CUDA streams. 2. Utilize the CUDA Stream Sanitizer to detect the synchronization error. 3. Correct the synchronization error using appropriate synchronization methods. Requirements and Constraints: - Define a tensor on the default CUDA stream. - Perform operations on this tensor using a different CUDA stream without synchronization initially. - Enable the CUDA Stream Sanitizer to detect data races. - Correct the synchronization error by ensuring proper ordering of stream operations. - The tensor should be of shape (1000, 1000) and initialized with random values. Steps: 1. Initialize a tensor on the default CUDA stream. 2. Use a new CUDA stream to perform a simple operation (e.g., element-wise multiplication) on the tensor without synchronization. 3. Use the CUDA Stream Sanitizer to detect the synchronization error. 4. Implement proper synchronization to fix the issue. 5. Provide comments in your code explaining each step and the detected errors. Expected Input: No input is required from the user. The script should be self-contained. Expected Output: The script will print error messages detected by the CUDA Stream Sanitizer initially, and after fixing the synchronization issues, it should run without any errors. Example Structure: ```python import torch # Step 1: Initialize tensor on the default CUDA stream a = torch.rand(1000, 1000, device=\\"cuda\\") # Step 2: Create a new CUDA stream and perform an operation without synchronization new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # This will cause a potential synchronization error torch.mul(a, 5, out=a) # Run the script with CUDA Stream Sanitizer enabled (not within the script, but when running the script) # TORCH_CUDA_SANITIZER=1 python your_script.py # Step 3: Analyze the error message (you do not need to include this in the script, just for understanding) # Step 4: Correct the synchronization error with torch.cuda.stream(new_stream): new_stream.wait_stream(torch.cuda.default_stream()) # Now this should not cause an error torch.mul(a, 5, out=a) print(\\"Script completed without synchronization errors.\\") ``` You are expected to flesh out the example above, run it with the CUDA Stream Sanitizer enabled, and then correct the synchronization issue. Note: Ensure to run your script with the environment variable `TORCH_CUDA_SANITIZER` set to `1` to enable the sanitizer.","solution":"import torch # Step 1: Initialize tensor on the default CUDA stream a = torch.rand(1000, 1000, device=\\"cuda\\") # Step 2: Create a new CUDA stream and perform an operation without synchronization new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # This can cause a potential synchronization error torch.mul(a, 5, out=a) # Note: To detect the synchronization error, run this script with the environment variable: # `TORCH_CUDA_SANITIZER=1 python your_script.py` # Step 3: Correct the synchronization error # Proper synchronization between streams with torch.cuda.stream(new_stream): # Ensure that new_stream waits for operations on the default stream to complete new_stream.wait_stream(torch.cuda.default_stream()) # Now perform the operation torch.mul(a, 5, out=a) # Step 4: Ensure that the stream operations have completed before continuing # Wait for computations on the new stream to synchronize with the default stream before accessing the results torch.cuda.synchronize() print(\\"Script completed without synchronization errors.\\")"},{"question":"# Advanced Email and JSON Handling in Python Objective You are required to demonstrate proficiency in handling internet data formats, particularly emails and JSON. The task involves creating an email message, encoding it into JSON format, and then performing several operations including decoding and manipulation of the JSON data. Problem Description Write a Python function `process_email_to_json(email_data)` that takes an email message\'s component details as input and returns JSON data. Then create another function `decode_email_json(json_data)` that decodes the JSON back into a structured email message. The email message should be created with appropriate MIME types and encoding. Function Signature ```python def process_email_to_json(email_data: dict) -> str: pass def decode_email_json(json_data: str) -> dict: pass ``` Input `email_data`: A dictionary containing the following keys: - `from` (string): The sender\'s email address - `to` (string): The recipient\'s email address - `subject` (string): The email subject - `body` (string): The email body Example: ```python email_data = { \'from\': \'sender@example.com\', \'to\': \'recipient@example.com\', \'subject\': \'Test Email\', \'body\': \'This is a test email.\' } ``` `json_data`: A JSON string encoding the email, which was returned from `process_email_to_json`. Output For `process_email_to_json`, return a JSON string containing the encoded email data. This JSON string should include the encoded MIME email message. For `decode_email_json`, return a dictionary similar to the original input dictionary, reconstructing the details from the JSON string. Constraints - The email should be created using the standard MIME format. - Ensure proper character encoding handling for the email body. - Use the built-in Python libraries such as `email` and `json` strictly as documented. - Raise appropriate exceptions if the input email data is malformed or if any decoding errors occur. Example Usage ```python email_data = { \'from\': \'sender@example.com\', \'to\': \'recipient@example.com\', \'subject\': \'Test Email\', \'body\': \'This is a test email.\' } json_email = process_email_to_json(email_data) # should output a JSON string containing the MIME email structure. decoded_email = decode_email_json(json_email) # should reconstruct the original dictionary form of the email_data. ``` Notes - You must handle MIME text encoding especially for the email body. - The JSON format should preserve all necessary details to reconstruct the original email. - Be cautious about character encoding issues in both creating emails and parsing JSON. Good luck!","solution":"import json from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def process_email_to_json(email_data: dict) -> str: Converts email data dictionary to JSON encoded string. # Create the email message msg = MIMEMultipart() msg[\'From\'] = email_data[\'from\'] msg[\'To\'] = email_data[\'to\'] msg[\'Subject\'] = email_data[\'subject\'] # Attach the body with proper MIME type body = MIMEText(email_data[\'body\'], \'plain\') msg.attach(body) # Convert email message to string email_string = msg.as_string() # Create a dictionary to be converted into JSON format email_dict = { \'from\': email_data[\'from\'], \'to\': email_data[\'to\'], \'subject\': email_data[\'subject\'], \'body\': email_data[\'body\'], \'email_string\': email_string } # Convert the dictionary to a JSON string json_data = json.dumps(email_dict) return json_data def decode_email_json(json_data: str) -> dict: Decodes JSON encoded string back into structured email message. email_dict = json.loads(json_data) # Return the dictionary containing email details decoded_email_data = { \'from\': email_dict[\'from\'], \'to\': email_dict[\'to\'], \'subject\': email_dict[\'subject\'], \'body\': email_dict[\'body\'] } return decoded_email_data"},{"question":"Objective: You are required to demonstrate your understanding of pandas\' window functions by implementing a function that processes a financial time series dataset and performs several rolling, expanding, and exponentially-weighted window calculations. Problem Statement: You are given a time series dataset of daily stock prices in a DataFrame with the following columns: - `Date`: Date of the observation (index of the DataFrame) - `Close`: Closing price of the stock Implement a function `process_stock_data(df: pd.DataFrame) -> pd.DataFrame` that takes the DataFrame as input and returns another DataFrame with the following additional columns: 1. `rolling_mean_30`: The 30-day rolling mean of the `Close` prices. 2. `rolling_std_30`: The 30-day rolling standard deviation of the `Close` prices. 3. `expanding_sum`: The expanding sum of the `Close` prices. 4. `ewm_mean_20`: The 20-day exponentially-weighted moving average of the `Close` prices. Each of these calculations should be performed using the respective pandas window functions. Input: - `df`: A pandas DataFrame with the following structure: ``` Date Close 0 2021-01-01 150.0 1 2021-01-02 152.0 2 2021-01-03 155.0 ... ... ... ``` Output: - A pandas DataFrame with the original columns plus the following additional columns: ``` Date Close rolling_mean_30 rolling_std_30 expanding_sum ewm_mean_20 0 2021-01-01 150.0 NaN NaN 150.0 150.0 1 2021-01-02 152.0 NaN NaN 302.0 150.1 2 2021-01-03 155.0 NaN NaN 457.0 150.4 ... ... ... ... ... ... ... ``` Constraints: - The `Date` column will always be in chronological order. - The DataFrame will have at least 100 rows. - Make sure to handle NaN values arising from the rolling window operations appropriately. Performance Requirements: - Efficiency in handling large datasets is crucial. Ensure that your solution leverages pandas\' optimized operations. Example: ```python import pandas as pd def process_stock_data(df: pd.DataFrame) -> pd.DataFrame: df[\'rolling_mean_30\'] = df[\'Close\'].rolling(window=30).mean() df[\'rolling_std_30\'] = df[\'Close\'].rolling(window=30).std() df[\'expanding_sum\'] = df[\'Close\'].expanding().sum() df[\'ewm_mean_20\'] = df[\'Close\'].ewm(span=20).mean() return df # Example usage: data = { \'Date\': pd.date_range(start=\'2021-01-01\', periods=100), \'Close\': np.random.random_sample(100) * 100 + 100 # Simulate some random stock prices } df = pd.DataFrame(data) result = process_stock_data(df) print(result) ```","solution":"import pandas as pd def process_stock_data(df: pd.DataFrame) -> pd.DataFrame: # Calculate 30-day rolling mean of the \'Close\' prices df[\'rolling_mean_30\'] = df[\'Close\'].rolling(window=30).mean() # Calculate 30-day rolling standard deviation of the \'Close\' prices df[\'rolling_std_30\'] = df[\'Close\'].rolling(window=30).std() # Calculate the expanding sum of the \'Close\' prices df[\'expanding_sum\'] = df[\'Close\'].expanding().sum() # Calculate the 20-day exponentially-weighted moving average of the \'Close\' prices df[\'ewm_mean_20\'] = df[\'Close\'].ewm(span=20).mean() return df"},{"question":"# Advanced Python Coding Assessment: Iterating with Efficiency Objective: To test students\' understanding of Python generators, their ability to implement generators, and to ensure efficient handling of large datasets. Problem Statement: You are provided with a large collection of data points representing sensor readings taken over time. Each data point is represented as a tuple `(timestamp, value)`. Your task is to write a generator function that processes this continuous stream of data to filter out and yield only those readings where the value is greater than a specified threshold. Function Signature: ```python def readings_above_threshold(sensor_data, threshold): This generator takes a list of sensor readings and a threshold value. It yields only those readings from the sensor_data where the value exceeds the threshold. Parameters: - sensor_data (list of tuple): A list of tuples, each tuple represents a data point in the form (timestamp, value). - threshold (float): The threshold value to compare against the readings. Yields: - tuple: The next reading where the value exceeds the threshold. pass ``` Input: 1. `sensor_data`: A list of tuples, where each tuple is in the format `(timestamp, value)`: - `timestamp` (float): The time at which the reading was taken. - `value` (float): The sensor reading value. 2. `threshold`: A float value representing the threshold above which readings are considered significant. Output: - The function should be a generator that yields tuples `(timestamp, value)` where `value` exceeds the given `threshold`. Constraints: - The function must be implemented as a generator. - You should not use any external libraries. - The function should efficiently handle streams of data with millions of data points. Example Usage: ```python sensor_data = [ (1618234340.0, 20.5), (1618234400.0, 50.0), (1618234460.0, 10.5), (1618234520.0, 70.0), ] threshold = 25.0 gen = readings_above_threshold(sensor_data, threshold) print(next(gen)) # Output: (1618234400.0, 50.0) print(next(gen)) # Output: (1618234520.0, 70.0) ``` Notes: 1. Ensure that you handle edge cases where the sensor data list is empty or no readings exceed the threshold. 2. Make sure your implementation is efficient and can handle large datasets without consuming unnecessary memory.","solution":"def readings_above_threshold(sensor_data, threshold): This generator takes a list of sensor readings and a threshold value. It yields only those readings from the sensor_data where the value exceeds the threshold. Parameters: - sensor_data (list of tuple): A list of tuples, each tuple represents a data point in the form (timestamp, value). - threshold (float): The threshold value to compare against the readings. Yields: - tuple: The next reading where the value exceeds the threshold. for timestamp, value in sensor_data: if value > threshold: yield (timestamp, value)"},{"question":"# Asynchronous Programming with Platform-Specific Constraints **Objective:** You are tasked with writing a Python program that performs asynchronous file read and write operations, taking into consideration the platform-specific limitations of the `asyncio` library as described in the documentation. **Requirements:** 1. Implement two asynchronous functions: `read_file_async(file_path)` and `write_file_async(file_path, data)`. 2. Your implementation should be able to differentiate between the supported and unsupported operations based on the platform (Windows, macOS, etc.). **Function Specifications:** 1. **`read_file_async(file_path)`**: - **Input:** `file_path` (str): The path to the file to be read. - **Output:** Returns the content of the file as a string. - **Constraints:** Ensure the function handles platform-specific limitations for file reading. 2. **`write_file_async(file_path, data)`**: - **Input:** `file_path` (str): The path to the file where the data should be written. `data` (str): The data to write to the file. - **Output:** None - **Constraints:** Ensure the function handles platform-specific limitations for file writing. **Additional Requirements:** - Use the `asyncio` library and appropriate event loops for your solution. - Handle potential limitations and unsupported features using exception handling or alternate strategies. - Ensure your code is well-structured and includes comments explaining key decisions and handling strategies for different platforms. **Example Usage:** ```python import asyncio async def main(): file_path = \\"example.txt\\" data = \\"Hello, Async World!\\" # Write data to file await write_file_async(file_path, data) # Read data from file content = await read_file_async(file_path) print(content) # Run the example asyncio.run(main()) ``` # Considerations: - Be mindful of the specific functions and event loops that are not supported on different platforms, particularly on Windows and older macOS versions. - You can use platform checking utilities (like `platform.system()`) to identify the operating system and handle the functionality accordingly. You are free to use any additional libraries or utilities if needed. However, explain the reasoning behind choosing specific methods or libraries if they help circumvent the platform-specific constraints.","solution":"import asyncio import os import platform async def read_file_async(file_path): Asynchronously reads the content of a file. If the platform does not support asyncio\'s file operations (e.g., Windows), falls back to synchronous reading using a thread pool. :param file_path: str - Path to the file to be read. :return: str - Content of the file. if platform.system() == \'Windows\': # Use thread pool for compatibility with Windows loop = asyncio.get_event_loop() return await loop.run_in_executor(None, _sync_read_file, file_path) else: async with aiofiles.open(file_path, \'r\') as f: return await f.read() async def write_file_async(file_path, data): Asynchronously writes data to a file. If the platform does not support asyncio\'s file operations (e.g., Windows), falls back to synchronous writing using a thread pool. :param file_path: str - Path to the file where the data should be written. :param data: str - Data to write to the file. :return: None if platform.system() == \'Windows\': # Use thread pool for compatibility with Windows loop = asyncio.get_event_loop() await loop.run_in_executor(None, _sync_write_file, file_path, data) else: async with aiofiles.open(file_path, \'w\') as f: await f.write(data) def _sync_read_file(file_path): with open(file_path, \'r\') as f: return f.read() def _sync_write_file(file_path, data): with open(file_path, \'w\') as f: f.write(data)"},{"question":"Titanic Data Visualization Challenge **Objective:** Create a Seaborn-based visualization to explore survival rates on the Titanic, segmented by different demographic groups. You will utilize advanced plotting techniques such as faceting, stacking, and histogram adjustments provided by the Seaborn library. **Instructions:** 1. Load the Titanic dataset using Seaborn\'s `load_dataset` method. 2. Create a `so.Plot` visualization that showcases: - A faceted bar chart comparing the number of survivors (`\\"alive\\" == \\"yes\\"`) and non-survivors (`\\"alive\\" == \\"no\\"`) across different classes (`\\"class\\"`) and genders (`\\"sex\\"`). - Use vertical stacking for the bars to show part-whole relationships within each category. - Implement a faceted histogram to show the age distribution of survivors and non-survivors, split by gender (`\\"sex\\"`). Ensure that the bin width for the histogram is set to 10 years and transparency (`alpha`) is used to distinguish between survivors and non-survivors. **Requirements:** 1. **Input**: There are no specific inputs required from the user, as the dataset is loaded within the script. 2. **Output**: The output should be two plots: - One faceted bar chart with stacked bars. - One faceted histogram with bin width of 10 and alpha transparency distinguishing groups. 3. **Constraints**: - The dataset used must be the Titanic dataset from Seaborn. - You must use the `so.Plot` class and associated methods for creating the visualizations. 4. **Performance**: - Ensure the code runs efficiently and does not take an unreasonable amount of time to execute. **Example Output:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Sort the dataset by alive status to ensure ordering in plots titanic = titanic.sort_values(\\"alive\\", ascending=False) # Faceted Bar Chart with Stacking bar_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\", y=\\"alive\\") .facet(col=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) ) bar_plot.show() # Faceted Histogram with Bin Width and Alpha Transparency hist_plot = ( so.PPlot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) hist_plot.show() ``` The above example illustrates how you can load the Titanic dataset and create the necessary visualizations using Seaborn’s `so.Plot` functionalities. **Note**: Ensure Seaborn and its objects interface are installed and properly working in your Python environment.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_titanic_data(): # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Faceted Bar Chart with Stacking bar_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\", y=\\"alive\\") .facet(col=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) ) bar_plot.show() # Faceted Histogram with Bin Width and Alpha Transparency hist_plot = ( so.Plot(titanic, x=\\"age\\", color=\\"alive\\", alpha=\\"alive\\") .facet(col=\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10)) ) hist_plot.show() # Additional helper function to facilitate testing def generate_plots(): plt.figure(figsize=(16, 8)) plot_titanic_data() plt.show()"},{"question":"Objective: In this task, you will implement a custom neural network training loop in PyTorch utilizing the `torch.compile` feature for optimization. Additionally, you will demonstrate the integration of NumPy operations within the PyTorch computation graph and handle potential debugging scenarios. Problem Statement: You are provided with a dataset of random points in 2D space. Your task is to implement a simple neural network that classifies these points into two categories. 1. **Define the Model:** Implement a neural network using PyTorch\'s `nn.Module`. The network should include at least one hidden layer. 2. **Training Loop:** Write a custom training loop using PyTorch. Use `torch.compile` to optimize the training loop. 3. **NumPy Integration:** Within the training loop, integrate a NumPy function that normalizes the dataset. 4. **Debugging:** Ensure the model is correctly optimized and identify any potential graph breaks using `torch._dynamo.explain`. Requirements: 1. **Model Implementation:** - A neural network model that takes 2D points as input. - At least one hidden layer with a non-linear activation function. - An output layer suitable for binary classification. 2. **Training Loop:** - Use the Adam optimizer and binary cross-entropy loss. - Use `torch.compile()` to compile the training loop. - Utilize a NumPy function to normalize the dataset within the PyTorch computation graph. 3. **Dataset:** - Generate a dataset of random 2D points with labels 0 or 1 4. **Debugging:** - Identify any graph breaks using `torch._dynamo.explain` and justify why they occur. - Handle potential OOM (Out of Memory) issues and demonstrate how you would debug it. Input: - Number of training epochs: `int` Output: - Print the training loss for each epoch. - Print the model\'s accuracy on a generated validation dataset. Constraints: - Implement the task using PyTorch and NumPy. - Ensure code is optimized and follows best practices for training neural networks. ```python import torch import torch.nn as nn import torch.optim as optim import numpy as np # Step 1: Define Neural Network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(2, 10) self.activation = nn.ReLU() self.output = nn.Linear(10, 1) def forward(self, x): x = self.activation(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x # Step 2: Generate Dataset def generate_dataset(size): data = np.random.randn(size, 2) labels = (data[:, 0] + data[:, 1] > 0).astype(int) return data, labels # Step 3: NumPy Normalization Function def normalize_data(data): return (data - data.mean(axis=0)) / data.std(axis=0) # Step 4: Training Loop def train_model(epochs): # Initialize model, loss, optimizer model = SimpleNN() criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Generate dataset data, labels = generate_dataset(1000) data = torch.tensor(normalize_data(data), dtype=torch.float32) labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1) @torch.compile def training_step(data, labels): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, labels) loss.backward() optimizer.step() return loss.item() # Training loop for epoch in range(epochs): loss = training_step(data, labels) print(f\\"Epoch {epoch+1}/{epochs}, Loss: {loss}\\") # Debugging explanation = torch._dynamo.explain(lambda d, l: training_step(torch.tensor(normalize_data(d), dtype=torch.float32), l)) print(explanation) # Step 5: Execution train_model(10) ``` Explanation: 1. **Model Definition:** The `SimpleNN` class defines a simple neural network with one hidden layer and a sigmoid activation on the output. 2. **Dataset Generation:** The `generate_dataset` function creates a dataset of random 2D points and assigns binary labels based on the sum of the coordinates. 3. **Normalization:** The `normalize_data` function normalizes the dataset using NumPy. 4. **Training Loop:** The `train_model` function compiles the training step using `torch.compile` and runs the training loop for a specified number of epochs. Debugging ensures that graph breaks are identified and handled properly. Use this code template to fill in the missing parts and ensure the model achieves reasonable accuracy. The provided question leverages both basic and advanced concepts from PyTorch and requires thoughtful implementation and debugging.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np # Step 1: Define Neural Network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(2, 10) self.activation = nn.ReLU() self.output = nn.Linear(10, 1) def forward(self, x): x = self.activation(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x # Step 2: Generate Dataset def generate_dataset(size): data = np.random.randn(size, 2) labels = (data[:, 0] + data[:, 1] > 0).astype(int) return data, labels # Step 3: NumPy Normalization Function def normalize_data(data): return (data - data.mean(axis=0)) / data.std(axis=0) # Step 4: Training Loop def train_model(epochs): # Initialize model, loss, optimizer model = SimpleNN() criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Generate dataset data, labels = generate_dataset(1000) data = torch.tensor(normalize_data(data), dtype=torch.float32) labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1) @torch.compile def training_step(data, labels): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, labels) loss.backward() optimizer.step() return loss.item() # Training loop for epoch in range(epochs): loss = training_step(data, labels) print(f\\"Epoch {epoch+1}/{epochs}, Loss: {loss}\\") # Debugging explanation = torch._dynamo.explain(lambda d, l: training_step(torch.tensor(normalize_data(d), dtype=torch.float32), l)) print(explanation) # Step 5: Execution if __name__ == \\"__main__\\": train_model(10)"},{"question":"# Task: Dynamic Class Creation and Manipulation You are tasked with dynamically creating classes and manipulating their types using the `types` module in Python. The objective is to create a base class and a derived class dynamically, utilizing custom metaclasses and dynamic attributes. You will also need to create instances of these classes and demonstrate specific behaviors. Requirements: 1. **Create a Custom Metaclass:** - Name: `MetaClassLogger` - Custom behavior: Print \\"Creating class: <class-name>\\" upon class creation. 2. **Create a Base Class Dynamically:** - Name: `DynamicBase` - Base class: `object` - Metaclass: `MetaClassLogger` - Attributes: - Class Attribute: `class_attr` with value `Base Class` - Instance Method: `instance_method` that prints \\"Called from base\\" 3. **Create a Derived Class Dynamically:** - Name: `DynamicDerived` - Base class: `DynamicBase` - Metaclass: `MetaClassLogger` - Attributes: - Class Attribute: `class_attr` with value `Derived Class` - Instance Method: `instance_method` that first calls `super().instance_method()` and then prints \\"Called from derived\\" 4. **Demonstrate Custom Attribute Access via a Descriptor:** - Use `DynamicClassAttribute` to create a dynamic attribute `dynamic_attr` in the `DynamicBase` class that forwards access to a method `get_dynamic_attr`. - Method `get_dynamic_attr` should return \\"Dynamic Attribute Accessed!\\" Functions to Implement: 1. **create_meta_class_logger() -> Type:** - Returns the metaclass `MetaClassLogger`. 2. **create_dynamic_base() -> Type:** - Returns the class `DynamicBase`. 3. **create_dynamic_derived() -> Type:** - Returns the class `DynamicDerived`. 4. **demonstrate_dynamic_classes() -> None:** - Creates instances `base_instance` and `derived_instance` of `DynamicBase` and `DynamicDerived` respectively. - Shows the use of `class_attr` and `instance_method` for both instances. - Demonstrates the use of `dynamic_attr`. Expected Behavior: ```python # Creating class: DynamicBase # Creating class: DynamicDerived base_instance = create_dynamic_base()() derived_instance = create_dynamic_derived()() # For base_instance print(base_instance.class_attr) # Should print: Base Class base_instance.instance_method() # Should print: Called from base print(base_instance.dynamic_attr) # Should print: Dynamic Attribute Accessed! # For derived_instance print(derived_instance.class_attr) # Should print: Derived Class derived_instance.instance_method() # Should print \\"Called from base\\" followed by \\"Called from derived\\" print(derived_instance.dynamic_attr) # Should print: Dynamic Attribute Accessed! ``` Constraints: - You must use the `types` module and the specified utility functions/classes. - You cannot use traditional class definitions; the classes must be created dynamically.","solution":"import types def create_meta_class_logger(): class MetaClassLogger(type): def __new__(cls, name, bases, dct): print(f\\"Creating class: {name}\\") return super().__new__(cls, name, bases, dct) return MetaClassLogger MetaClassLogger = create_meta_class_logger() def create_dynamic_base(): class DynamicBase(metaclass=MetaClassLogger): class_attr = \\"Base Class\\" def instance_method(self): print(\\"Called from base\\") def get_dynamic_attr(self): return \\"Dynamic Attribute Accessed!\\" @property def dynamic_attr(self): return self.get_dynamic_attr() return DynamicBase def create_dynamic_derived(): class DynamicDerived(create_dynamic_base(), metaclass=MetaClassLogger): class_attr = \\"Derived Class\\" def instance_method(self): super().instance_method() print(\\"Called from derived\\") return DynamicDerived def demonstrate_dynamic_classes(): DynamicBase = create_dynamic_base() DynamicDerived = create_dynamic_derived() base_instance = DynamicBase() derived_instance = DynamicDerived() # For base_instance print(base_instance.class_attr) # Should print: Base Class base_instance.instance_method() # Should print: Called from base print(base_instance.dynamic_attr) # Should print: Dynamic Attribute Accessed! # For derived_instance print(derived_instance.class_attr) # Should print: Derived Class derived_instance.instance_method() # Should print \\"Called from base\\" followed by \\"Called from derived\\" print(derived_instance.dynamic_attr) # Should print: Dynamic Attribute Accessed!"},{"question":"The `sys` module in Python provides access to system-specific parameters and functions and allows interaction with the Python interpreter at a low level. A critical skill for a Python developer is understanding how to use this module to manage and inspect state information, handle exceptions, and debug effectively. Your task is to implement a Python script that leverages several attributes and functions of the `sys` module to perform the following operations: 1. Retrieve and print the system\'s maximum recursion limit. 2. Set a new recursion limit. 3. Verify and display the current recursion limit. 4. Capture and display the command line arguments used to invoke the script. 5. Implement a custom exception hook that logs uncaught exceptions to a file named `error.log`. # Constraints - You must not cause a stack overflow when modifying the recursion limit. - The custom exception hook should: - Log the exception type, value, and traceback to the file. - Maintain the default printing behavior to `stderr`. # Input and Output - **Input**: The script will be invoked with some command line arguments (e.g., `python script.py arg1 arg2`). - **Output**: The system\'s maximum recursion limit, the new recursion limit, and the command-line arguments. - **Logging**: Uncaught exceptions should be logged into `error.log`. The log must include the exception type, value, and traceback. # Implementation Details You are provided with a template file called `script.py`. Complete the `implement_sys_functions()` function within this file to meet the specified requirements. ```python # script.py import sys def custom_exception_hook(exc_type, exc_value, exc_traceback): Custom exception hook to log uncaught exceptions. with open(\'error.log\', \'a\') as f: f.write(f\\"Exception Type: {exc_type}n\\") f.write(f\\"Exception Value: {exc_value}n\\") f.write(\'Traceback:\') for line in traceback.format_tb(exc_traceback): f.write(line) sys.__excepthook__(exc_type, exc_value, exc_traceback) def implement_sys_functions(new_recursion_limit): Implement various sys module functionalities. # Retrieve and print the system\'s maximum recursion limit max_recursion_limit = sys.getrecursionlimit() print(f\\"Current recursion limit: {max_recursion_limit}\\") # Set a new recursion limit sys.setrecursionlimit(new_recursion_limit) # Verify and display the current recursion limit updated_recursion_limit = sys.getrecursionlimit() print(f\\"Updated recursion limit: {updated_recursion_limit}\\") # Capture and display command line arguments command_args = sys.argv print(f\\"Command line arguments: {command_args}\\") # Set the custom exception hook sys.excepthook = custom_exception_hook # Example of usage if __name__ == \\"__main__\\": # The first argument should be the new recursion limit value. new_limit = int(sys.argv[1]) implement_sys_functions(new_limit) ``` # Example ```bash python script.py 1500 arg1 arg2 ``` Expected Output: ``` Current recursion limit: 1000 Updated recursion limit: 1500 Command line arguments: [\'script.py\', \'1500\', \'arg1\', \'arg2\'] ``` In case of any uncaught exceptions, they should be logged into `error.log` with detailed information. # Note - The script should be robust and handle edge cases gracefully, such as the new recursion limit being too high or too low. - Use Python\'s built-in functionalities and avoid using third-party libraries.","solution":"import sys import traceback def custom_exception_hook(exc_type, exc_value, exc_traceback): Custom exception hook to log uncaught exceptions. with open(\'error.log\', \'a\') as f: f.write(f\\"Exception Type: {exc_type.__name__}n\\") f.write(f\\"Exception Value: {exc_value}n\\") f.write(\'Traceback:n\') for line in traceback.format_tb(exc_traceback): f.write(line) sys.__excepthook__(exc_type, exc_value, exc_traceback) def implement_sys_functions(new_recursion_limit): Implement various sys module functionalities. # Retrieve and print the system\'s maximum recursion limit max_recursion_limit = sys.getrecursionlimit() print(f\\"Current recursion limit: {max_recursion_limit}\\") # Set a new recursion limit sys.setrecursionlimit(new_recursion_limit) # Verify and display the current recursion limit updated_recursion_limit = sys.getrecursionlimit() print(f\\"Updated recursion limit: {updated_recursion_limit}\\") # Capture and display command line arguments command_args = sys.argv print(f\\"Command line arguments: {command_args}\\") # Set the custom exception hook sys.excepthook = custom_exception_hook # Example of usage if __name__ == \\"__main__\\": # The first argument should be the new recursion limit value. new_limit = int(sys.argv[1]) implement_sys_functions(new_limit)"},{"question":"# Comprehensive PyTorch Model Profiling using TorchInductor In this exercise, you will implement a function to set up and profile a PyTorch model using TorchInductor GPU profiling. You are required to handle environment variables, run the benchmark, parse the output logs, and provide a summary of the profiling results. Part 1: Implement the Function Your task is to implement the function `profile_model` which performs the following steps: 1. **Set Environment Variables:** - `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1` - `TORCHINDUCTOR_BENCHMARK_KERNEL=1` 2. **Run Benchmark Script:** - Execute the provided benchmark script for a given model using the Inductor backend. - Capture and log the output. 3. **Parse Output Logs:** - Identify the compiled module paths for the forward and backward graphs. - Extract the Chrome trace file path and the percentage of GPU busy time. - Summarize the breakdown of GPU time into different kernel categories. 4. **Return a Structured Summary:** - A dictionary containing: - The paths of the compiled modules. - The Chrome trace file path. - The percentage of GPU busy time. - Breakdown of GPU time into kernel categories. Input - `model_name` (str): The name of the model to be profiled (e.g., \'mixnet_l\'). - `script_path` (str): The path to the benchmark script (e.g., \'benchmarks/dynamo/timm_models.py\'). Output - dict: A dictionary containing: - `compiled_modules` (dict): Paths of the forward and backward compiled modules. - `chrome_trace_path` (str): Path to the Chrome trace file. - `gpu_busy_time` (float): Percentage of time when the GPU is busy. - `gpu_time_breakdown` (dict): Breakdown of GPU time into kernel categories. Performance Requirements - The function should efficiently handle the extraction and parsing of output logs. - The environment should be correctly set up and cleaned up after the benchmark run. Constraints - Ensure the benchmark script is executed in a way that captures all output logs. - The log parsing should correctly identify and extract relevant information without errors. Example ```python def profile_model(model_name, script_path): Profiles a PyTorch model using TorchInductor GPU profiling. Args: model_name (str): Name of the model to be profiled. script_path (str): Path to the benchmark script. Returns: dict: A dictionary containing profiling results. # Implement your function here # Example usage: model_name = \\"mixnet_l\\" script_path = \\"benchmarks/dynamo/timm_models.py\\" result = profile_model(model_name, script_path) print(result) ``` Note: Actual execution of the function requires an appropriate PyTorch environment and benchmark script setup.","solution":"import os import subprocess import re def profile_model(model_name, script_path): Profiles a PyTorch model using TorchInductor GPU profiling. Args: model_name (str): Name of the model to be profiled. script_path (str): Path to the benchmark script. Returns: dict: A dictionary containing profiling results. # Set environment variables os.environ[\\"TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\\"] = \\"1\\" os.environ[\\"TORCHINDUCTOR_BENCHMARK_KERNEL\\"] = \\"1\\" # Run the benchmark script for the given model command = f\\"python {script_path} --model {model_name} --backend inductor --profile\\" result = subprocess.run(command, shell=True, capture_output=True, text=True) # Capture and log the output output = result.stdout + result.stderr # Parse the output logs compiled_modules = {} chrome_trace_path = None gpu_busy_time = None gpu_time_breakdown = {} # Regex patterns compiled_module_pattern = re.compile(r\\"Compiled module for (forward|backward): (.*)\\") chrome_trace_pattern = re.compile(r\\"Chrome trace saved to: (.*)\\") gpu_busy_pattern = re.compile(r\\"GPU busy: (d+.d+)%\\") gpu_kernel_pattern = re.compile(r\\"(w+): (d+.d+)% of total GPU time\\") # Identify relevant information from logs for line in output.splitlines(): # Compiled module paths match = compiled_module_pattern.search(line) if match: direction, path = match.groups() compiled_modules[direction] = path # Chrome trace file path match = chrome_trace_pattern.search(line) if match: chrome_trace_path = match.group(1) # GPU busy time match = gpu_busy_pattern.search(line) if match: gpu_busy_time = float(match.group(1)) # GPU time breakdown match = gpu_kernel_pattern.search(line) if match: kernel, percentage = match.groups() gpu_time_breakdown[kernel] = float(percentage) # Return structured summary return { \\"compiled_modules\\": compiled_modules, \\"chrome_trace_path\\": chrome_trace_path, \\"gpu_busy_time\\": gpu_busy_time, \\"gpu_time_breakdown\\": gpu_time_breakdown }"},{"question":"Objective You are required to demonstrate your understanding of PyTorch\'s serialization mechanisms by creating and working with custom modules. You will create a custom neural network module, save its state, manipulate its state externally, and restore it correctly. Task 1. Create a custom neural network module `CustomNet` with the following specifications: - Two `Linear` layers: `l0` (input size 10, output size 20) and `l1` (input size 20, output size 5). - A ReLU activation function between `l0` and `l1`. - An additional tensor `aux_tensor` of size (3, 3) as a buffer. 2. Instantiate the `CustomNet` module, initialize its weights and bias for both layers using a normal distribution. 3. Save the state of the module using `state_dict` to a file named `customnet_state_dict.pt`. 4. Write a function `modify_saved_state(file_name)` that: - Loads the saved state from the file. - Modifies the `aux_tensor` to be filled with ones. - Saves the modified state back to a file named `modified_customnet_state_dict.pt`. 5. Write a function `restore_modified_state(file_name)` that: - Loads the modified state from the file. - Restores this state to a new instance of `CustomNet`. - Returns the tensor `aux_tensor` of the restored instance. Example Usage ```python import torch import torch.nn as nn import torch.nn.functional as F # Define your CustomNet class class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.l0 = nn.Linear(10, 20) self.l1 = nn.Linear(20, 5) self.aux_tensor = nn.parameter.Parameter( torch.empty(3, 3), requires_grad=False ) self.reset_parameters() def reset_parameters(self): nn.init.normal_(self.l0.weight) nn.init.normal_(self.l0.bias) nn.init.normal_(self.l1.weight) nn.init.normal_(self.l1.bias) nn.init.normal_(self.aux_tensor) def forward(self, x): x = self.l0(x) x = F.relu(x) x = self.l1(x) return x # Instantiate and save the model state model = CustomNet() torch.save(model.state_dict(), \'customnet_state_dict.pt\') # Define your functions def modify_saved_state(file_name): state_dict = torch.load(file_name) state_dict[\'aux_tensor\'] = torch.ones(3, 3) torch.save(state_dict, \'modified_customnet_state_dict.pt\') def restore_modified_state(file_name): new_model = CustomNet() new_model.load_state_dict(torch.load(file_name)) return new_model.aux_tensor # Example function calls modify_saved_state(\'customnet_state_dict.pt\') restored_tensor = restore_modified_state(\'modified_customnet_state_dict.pt\') print(restored_tensor) # Expected output: # tensor([[1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.]], grad_fn=<CloneBackward>) ``` Constraints - The modifications and restoration should correctly reflect in the `aux_tensor`. - The input tensors and state modifications should preserve shape and data type integrity. - Ensure your solution handles unexpected states or corrupted files appropriately. Performance Requirements - The solution should efficiently handle loading and saving operations for modules with moderate-sized weights and inputs. - Ensure your implementation avoids redundant operations that could affect the performance.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.l0 = nn.Linear(10, 20) self.l1 = nn.Linear(20, 5) self.register_buffer(\'aux_tensor\', torch.empty(3, 3)) self.reset_parameters() def reset_parameters(self): nn.init.normal_(self.l0.weight) nn.init.normal_(self.l0.bias) nn.init.normal_(self.l1.weight) nn.init.normal_(self.aux_tensor) def forward(self, x): x = self.l0(x) x = F.relu(x) x = self.l1(x) return x def create_and_save_model(file_name): model = CustomNet() torch.save(model.state_dict(), file_name) return file_name def modify_saved_state(file_name): state_dict = torch.load(file_name) state_dict[\'aux_tensor\'] = torch.ones(3, 3) torch.save(state_dict, \'modified_customnet_state_dict.pt\') def restore_modified_state(file_name): new_model = CustomNet() new_model.load_state_dict(torch.load(file_name)) return new_model.aux_tensor"},{"question":"# Internationalizing a Python Application with `gettext` In this assessment, you will demonstrate your understanding of the `gettext` module for internationalizing a Python application. You will create a simple Python application that supports multiple languages using both the GNU `gettext` API and the class-based API. Part 1: Using the GNU `gettext` API 1. **Define a function** `initialize_gettext(domain, localedir)` that sets up the translation domain and locale directory. 2. **Create a function** `translate_message(message)` that uses the GNU `gettext()` function to return the translated message. **Usage Example:** ```python # Initialize the gettext environment initialize_gettext(\'myapp\', \'/path/to/my/language/directory\') # Translate a message print(translate_message(\'Hello, World!\')) ``` Part 2: Using the Class-based API 1. **Define a function** `get_translator(domain, localedir)` that returns a `GNUTranslations` instance using the specified domain and locale directory. 2. **Create a function** `get_translation(translator, message)` that uses the translator instance to return the translated message. **Usage Example:** ```python # Get a translation instance translator = get_translator(\'myapp\', \'/path/to/my/language/directory\') # Translate a message using the translator instance print(get_translation(translator, \'Hello, World!\')) ``` # Input and Output - **Input:** Your functions should take the following parameters: - `domain` (str): The domain of the translation. - `localedir` (str): The path to the directory containing the `.mo` files. - `message` (str): The message string to be translated. - **Output:** Your functions should return the translated messages. # Constraints 1. Ensure that the translation files (`.mo` files) are correctly placed in the specified locale directory. 2. The application should correctly switch languages based on the provided domain and locale directory. # Performance Requirements 1. Your implementation should efficiently handle multiple translation domains and locale directories. 2. Ensure that switching between different languages is seamless and does not require restarting the application. # Notes - Use appropriate error handling mechanisms to handle missing or incorrect translation files. - You may use mock `.mo` files for testing purposes. # Testing Your Implementation Provide test cases to verify that your implementation correctly translates messages for different languages. Use dummy `.mo` files to demonstrate the functionality. ```python def test_internationalization(): # Initialize gettext with dummy data initialize_gettext(\'myapp\', \'/dummy/path\') # Test GNU gettext API assert translate_message(\'Hello, World!\') == \'Bonjour, le monde!\' # Test class-based API translator = get_translator(\'myapp\', \'/dummy/path\') assert get_translation(translator, \'Hello, World!\') == \'Bonjour, le monde!\' print(\\"All tests passed!\\") ``` Implement the functions and the test cases to demonstrate your understanding of the `gettext` module.","solution":"import gettext def initialize_gettext(domain, localedir): This function sets up the GNU gettext translation domain and locale directory. :param domain: The domain of the translation. :param localedir: The path to the directory containing the .mo files. gettext.bindtextdomain(domain, localedir) gettext.textdomain(domain) def translate_message(message): This function uses the GNU gettext() function to return the translated message. :param message: The message string to be translated. :return: The translated message. return gettext.gettext(message) def get_translator(domain, localedir): This function returns a GNUTranslations instance using the specified domain and locale directory. :param domain: The domain of the translation. :param localedir: The path to the directory containing the .mo files. :return: A GNUTranslations instance. try: trans = gettext.translation(domain, localedir=localedir) return trans except FileNotFoundError: return gettext.NullTranslations() def get_translation(translator, message): This function uses the translator instance to return the translated message. :param translator: The translator instance. :param message: The message string to be translated. :return: The translated message. return translator.gettext(message)"},{"question":"# Advanced Threading and Synchronization in Python Problem Statement You are required to implement a multi-threaded program that simulates a producer-consumer scenario with the following specifications: 1. **Producer**: A thread that produces items and stores them in a shared buffer (a queue). The producer will produce items at random intervals. 2. **Consumer**: Multiple threads that consume items from the shared buffer. Each consumer will consume items at random intervals. 3. **Synchronization**: Use locks and conditions to ensure proper synchronization between producer and consumers. The buffer should never be accessed by multiple threads simultaneously. 4. **Communication**: Use an event to signal the consumer threads when new items are available in the buffer. Requirements 1. Implement a `Producer` class that: - Has a `produce()` method that generates items at random intervals (between 1 to 3 seconds) and adds them to the buffer. - Uses a lock to synchronize access to the buffer. - Uses an event to signal consumers when new items are available. 2. Implement a `Consumer` class that: - Has a `consume()` method that waits for the event signal and then retrieves items from the buffer. - Consumes items at random intervals (between 2 to 5 seconds). Constraints - The buffer (queue) should have a limited capacity of 10 items. - Use the `threading` module for thread creation, synchronization, and communication. - The program should run for 30 seconds before terminating all threads. Input and Output - **Input**: No input from the user. - **Output**: Print messages indicating when items are produced and consumed, along with the current size of the buffer. Example Output ``` Produced item 1 | Buffer size: 1 Consumed item 1 | Buffer size: 0 Produced item 2 | Buffer size: 1 Produced item 3 | Buffer size: 2 Consumed item 2 | Buffer size: 1 ``` Note Ensure that your program handles thread termination gracefully after 30 seconds of execution. # Implementation ```python import threading import time import random from queue import Queue class Producer: def __init__(self, buffer, buffer_lock, item_event): self.buffer = buffer self.buffer_lock = buffer_lock self.item_event = item_event def produce(self): item_counter = 0 end_time = time.time() + 30 while time.time() < end_time: time.sleep(random.randint(1, 3)) item_counter += 1 with self.buffer_lock: if self.buffer.qsize() < 10: self.buffer.put(item_counter) print(f\'Produced item {item_counter} | Buffer size: {self.buffer.qsize()}\') self.item_event.set() else: print(\'Buffer full! Cannot produce item.\') self.item_event.wait() class Consumer: def __init__(self, buffer, buffer_lock, item_event): self.buffer = buffer self.buffer_lock = buffer_lock self.item_event = item_event def consume(self): end_time = time.time() + 30 while time.time() < end_time: time.sleep(random.randint(2, 5)) with self.buffer_lock: if not self.buffer.empty(): item = self.buffer.get() print(f\'Consumed item {item} | Buffer size: {self.buffer.qsize()}\') if self.buffer.qsize() == 0: self.item_event.clear() self.item_event.wait() def main(): buffer = Queue(maxsize=10) buffer_lock = threading.Lock() item_event = threading.Event() producer = Producer(buffer, buffer_lock, item_event) consumers = [Consumer(buffer, buffer_lock, item_event) for _ in range(3)] producer_thread = threading.Thread(target=producer.produce) consumer_threads = [threading.Thread(target=consumer.consume) for consumer in consumers] producer_thread.start() for t in consumer_threads: t.start() producer_thread.join() for t in consumer_threads: t.join() if __name__ == \'__main__\': main() ```","solution":"import threading import time import random from queue import Queue class Producer: def __init__(self, buffer, buffer_lock, item_event): self.buffer = buffer self.buffer_lock = buffer_lock self.item_event = item_event def produce(self): item_counter = 0 end_time = time.time() + 30 while time.time() < end_time: time.sleep(random.randint(1, 3)) item_counter += 1 with self.buffer_lock: if self.buffer.qsize() < 10: self.buffer.put(item_counter) print(f\'Produced item {item_counter} | Buffer size: {self.buffer.qsize()}\') self.item_event.set() else: print(\'Buffer full! Cannot produce item.\') self.item_event.wait() class Consumer: def __init__(self, buffer, buffer_lock, item_event): self.buffer = buffer self.buffer_lock = buffer_lock self.item_event = item_event def consume(self): end_time = time.time() + 30 while time.time() < end_time: time.sleep(random.randint(2, 5)) with self.buffer_lock: if not self.buffer.empty(): item = self.buffer.get() print(f\'Consumed item {item} | Buffer size: {self.buffer.qsize()}\') if self.buffer.qsize() == 0: self.item_event.clear() self.item_event.wait() def main(): buffer = Queue(maxsize=10) buffer_lock = threading.Lock() item_event = threading.Event() producer = Producer(buffer, buffer_lock, item_event) consumers = [Consumer(buffer, buffer_lock, item_event) for _ in range(3)] producer_thread = threading.Thread(target=producer.produce) consumer_threads = [threading.Thread(target=consumer.consume) for consumer in consumers] producer_thread.start() for t in consumer_threads: t.start() producer_thread.join() for t in consumer_threads: t.join() if __name__ == \'__main__\': main()"},{"question":"**Objective:** Demonstrate understanding and usage of pandas options API to control DataFrame display settings. **Task:** Your task is to write a Python function that performs the following steps: 1. Create a DataFrame with at least 10 rows and 10 columns of random data. 2. Set the following display options: - Maximum number of rows displayed (`display.max_rows`): 8 - Maximum width of columns (`display.max_colwidth`): 50 - Expand DataFrame across pages (`display.expand_frame_repr`): True 3. Print the DataFrame to visually inspect the changes. 4. Get and return the current settings of `display.max_rows`, `display.max_colwidth`, and `display.expand_frame_repr`. 5. Reset these display options to their default values. Here is the function signature: ```python import pandas as pd import numpy as np def configure_dataframe_display(): # Step 1: Create a DataFrame with random data df = pd.DataFrame(np.random.randn(10, 10)) # Step 2: Set display options pd.set_option(\\"display.max_rows\\", 8) pd.set_option(\\"display.max_colwidth\\", 50) pd.set_option(\\"display.expand_frame_repr\\", True) # Print the DataFrame print(df) # Step 3: Get the current settings max_rows = pd.get_option(\\"display.max_rows\\") max_colwidth = pd.get_option(\\"display.max_colwidth\\") expand_frame_repr = pd.get_option(\\"display.expand_frame_repr\\") # Step 4: Return the current settings current_settings = { \\"display.max_rows\\": max_rows, \\"display.max_colwidth\\": max_colwidth, \\"display.expand_frame_repr\\": expand_frame_repr } # Step 5: Reset the display options to default values pd.reset_option(\\"display.max_rows\\") pd.reset_option(\\"display.max_colwidth\\") pd.reset_option(\\"display.expand_frame_repr\\") return current_settings ``` **Input and Output Format:** - The function does not take any inputs. - The function returns a dictionary with the keys \'display.max_rows\', \'display.max_colwidth\', and \'display.expand_frame_repr\' and their respective current settings. **Constraints:** - Use only the pandas options API and the methods described in the documentation. - Ensure that after printing the DataFrame, the display options are reset to their default values to avoid any side effects for subsequent operations. **Performance Requirements:** - The function should efficiently set and reset the display options without any unnecessary computations. Good luck!","solution":"import pandas as pd import numpy as np def configure_dataframe_display(): # Step 1: Create a DataFrame with random data df = pd.DataFrame(np.random.randn(10, 10)) # Step 2: Set display options pd.set_option(\\"display.max_rows\\", 8) pd.set_option(\\"display.max_colwidth\\", 50) pd.set_option(\\"display.expand_frame_repr\\", True) # Print the DataFrame to inspect the changes print(df) # Step 3: Get the current settings max_rows = pd.get_option(\\"display.max_rows\\") max_colwidth = pd.get_option(\\"display.max_colwidth\\") expand_frame_repr = pd.get_option(\\"display.expand_frame_repr\\") # Step 4: Return the current settings current_settings = { \\"display.max_rows\\": max_rows, \\"display.max_colwidth\\": max_colwidth, \\"display.expand_frame_repr\\": expand_frame_repr } # Step 5: Reset the display options to default values pd.reset_option(\\"display.max_rows\\") pd.reset_option(\\"display.max_colwidth\\") pd.reset_option(\\"display.expand_frame_repr\\") return current_settings"},{"question":"# Asyncio Event Loop Compatibility Check You are required to write a Python function that checks the compatibility of given asyncio operations with the platform and event loop type in use. The function should be called `check_compatibility`. Function Signature ```python def check_compatibility(platform: str, event_loop_type: str, operation: str) -> bool: pass ``` Input - `platform` (str): The platform in use. This should be one of the following strings: `\\"windows\\"`, `\\"macos\\"`, `\\"linux\\"`, or `\\"all\\"`. - `event_loop_type` (str): The type of event loop being used. This should be one of the following strings: `\\"selector\\"`, `\\"proactor\\"`, `\\"default\\"`. - `operation` (str): The asyncio operation to check for compatibility. This should be one of the following strings: `\\"create_unix_connection\\"`, `\\"create_unix_server\\"`, `\\"add_signal_handler\\"`, `\\"remove_signal_handler\\"`, `\\"add_reader\\"`, `\\"add_writer\\"`, `\\"connect_read_pipe\\"`, `\\"connect_write_pipe\\"`, `\\"subprocess_exec\\"`, `\\"subprocess_shell\\"`. Output - The function should return a boolean value indicating whether the given operation is supported (`True`) or not (`False`) for the specified platform and event loop type. Constraints - You must handle different platforms and event loop types, recognizing limitations as described in the documentation. - Assume the `platform` and `event_loop_type` inputs are always valid strings from the specified list. Example ```python # Example 1: print(check_compatibility(\\"windows\\", \\"proactor\\", \\"subprocess_exec\\")) # Should return True because ProactorEventLoop on Windows supports subprocesses. # Example 2: print(check_compatibility(\\"macos\\", \\"selector\\", \\"add_reader\\")) # Should return True because macOS with a SelectorEventLoop supports add_reader for sockets. ``` Additional Notes - Consider edge cases such as unsupported operations on specific platforms. - Your function should have a time complexity that is efficient enough for constant-time checking of these constraints.","solution":"def check_compatibility(platform: str, event_loop_type: str, operation: str) -> bool: Checks the compatibility of asyncio operations with the specified platform and event loop type in use. Parameters: - platform (str): The platform in use. One of \\"windows\\", \\"macos\\", \\"linux\\", \\"all\\". - event_loop_type (str): The type of event loop being used. One of \\"selector\\", \\"proactor\\", \\"default\\". - operation (str): The asyncio operation to check for compatibility. Returns: - bool: True if the operation is supported; otherwise False. # Supported operations based on platform and event loop type compatibility_matrix = { \\"windows\\": { \\"selector\\": { \\"supported_operations\\": set(), }, \\"proactor\\": { \\"supported_operations\\": { \\"subprocess_exec\\", \\"subprocess_shell\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"add_signal_handler\\", \\"remove_signal_handler\\" }, }, \\"default\\": { \\"supported_operations\\": { \\"subprocess_exec\\", \\"subprocess_shell\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"add_signal_handler\\", \\"remove_signal_handler\\" }, }, }, \\"macos\\": { \\"selector\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, \\"proactor\\": { \\"supported_operations\\": set(), }, \\"default\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, }, \\"linux\\": { \\"selector\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, \\"proactor\\": { \\"supported_operations\\": set(), }, \\"default\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, }, \\"all\\": { \\"selector\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, \\"proactor\\": { \\"supported_operations\\": { \\"subprocess_exec\\", \\"subprocess_shell\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"add_signal_handler\\", \\"remove_signal_handler\\" }, }, \\"default\\": { \\"supported_operations\\": { \\"create_unix_connection\\", \\"create_unix_server\\", \\"add_signal_handler\\", \\"remove_signal_handler\\", \\"add_reader\\", \\"add_writer\\", \\"connect_read_pipe\\", \\"connect_write_pipe\\", \\"subprocess_exec\\", \\"subprocess_shell\\" }, }, } } return operation in compatibility_matrix[platform][event_loop_type][\\"supported_operations\\"]"},{"question":"# Objective Your task is to demonstrate your understanding of the \\"csv\\" module by creating a function that processes CSV data. The function will read data from a CSV file, transform it, and write the transformed data to another CSV file. # Task Description Write a Python function `transform_csv(input_filepath: str, output_filepath: str, delimiter: str = \',\') -> None` that: 1. Reads CSV data from a file specified by `input_filepath`. Each row can be assumed to have the following columns in order: `Name, Age, Department, Salary`. 2. Sorts the data by the \\"Salary\\" column in descending order. 3. Filters out any rows where the \\"Department\\" is \\"HR\\". 4. Writes the transformed data to a new CSV file specified by `output_filepath` using the given `delimiter`. # Input and Output Formats - `input_filepath` (str): Path to the input CSV file. - `output_filepath` (str): Path to the output CSV file. - `delimiter` (str, optional): Delimiter for the CSV file. Default is a comma (\',\'). # Constraints - You may assume that the input CSV file is correctly formatted. - The CSV file can be large, so your solution should be efficient in terms of memory usage. # Performance Requirements - The function should be able to handle CSV files with up to 1 million rows efficiently. # Example Given the following input CSV data in `employees.csv`: ``` Name,Age,Department,Salary Alice,30,Engineering,70000 Bob,45,HR,50000 Charlie,25,Marketing,60000 David,35,Engineering,80000 Eva,28,Sales,65000 ``` Calling `transform_csv(\'employees.csv\', \'filtered_employees.csv\')` should result in the following content in `filtered_employees.csv`: ``` Name,Age,Department,Salary David,35,Engineering,80000 Alice,30,Engineering,70000 Eva,28,Sales,65000 Charlie,25,Marketing,60000 ``` # Notes - Ensure your code is well-commented and follows best practices for readability and maintainability.","solution":"import csv def transform_csv(input_filepath: str, output_filepath: str, delimiter: str = \',\') -> None: Reads CSV data from a file, sorts by Salary in descending order, filters out HR department, and writes the transformed data to another CSV file. with open(input_filepath, \'r\', newline=\'\') as infile: reader = csv.DictReader(infile, delimiter=delimiter) data = [row for row in reader if row[\'Department\'] != \'HR\'] # Sort data by Salary in descending order data.sort(key=lambda x: int(x[\'Salary\']), reverse=True) with open(output_filepath, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames, delimiter=delimiter) writer.writeheader() writer.writerows(data)"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s `torch.futures` package and its `Future` type by implementing an asynchronous task management system. Problem Statement: You are required to implement a function `aggregate_squares` that performs the following tasks: 1. Launches multiple asynchronous tasks where each task computes the square of a number. 2. Aggregates the results of all these tasks once they are completed. 3. Returns the aggregated result. The function should use `torch.futures.Future` to manage asynchronous computation. Specifically, you will implement the following function: ```python import torch.futures def aggregate_squares(numbers): Given a list of integers, this function launches asynchronous tasks to compute the square of each number, waits for all tasks to complete, and returns the sum of all squared numbers. Args: - numbers (List[int]): List of integers to square asynchronously. Returns: - int: Sum of the squares of all the given numbers. pass ``` Input: - `numbers`: A list of integers `[n1, n2, n3, ...]`. Output: - An integer representing the sum of the squares of the input numbers. Constraints: - Use `torch.futures.Future` and the utility functions `collect_all` and `wait_all` to manage asynchronous operations. - Ensure that all asynchronous tasks are completed before aggregating the results. Example: ```python numbers = [1, 2, 3, 4] result = aggregate_squares(numbers) print(result) # Output should be 30 (1^2 + 2^2 + 3^2 + 4^2) ``` Guidelines: - Create a helper function `compute_square_async(n: int) -> torch.futures.Future` that simulates an asynchronous operation to compute the square of a given number `n`. - Use this helper function within `aggregate_squares` to launch tasks asynchronously. - Use the `collect_all` utility function to gather all future objects and wait for their completion. - Aggregate the results of the completed tasks and return the final sum.","solution":"import torch import torch.futures def compute_square_async(n: int) -> torch.futures.Future: Simulates an asynchronous operation to compute the square of a given number n. Args: - n (int): The number to square asynchronously. Returns: - torch.futures.Future: A future object holding the result of n squared. future = torch.futures.Future() future.set_result(n ** 2) return future def aggregate_squares(numbers): Given a list of integers, this function launches asynchronous tasks to compute the square of each number, waits for all tasks to complete, and returns the sum of all squared numbers. Args: - numbers (List[int]): List of integers to square asynchronously. Returns: - int: Sum of the squares of all the given numbers. futures = [compute_square_async(num) for num in numbers] results = torch.futures.collect_all(futures).wait() return sum(result.value() for result in results)"},{"question":"Objective Your task is to write a function that converts a string containing HTML entities into its corresponding Unicode characters, and returns a clean, decoded string. Additionally, the function should be able to handle and convert numeric character references. Problem Statement Implement a function `decode_html_entities(html_string: str) -> str` that takes a string with HTML entities and returns a fully decoded Unicode string. Input - `html_string`: A string consisting of HTML entities such as `&amp;`, `&gt;`, `&lt;`, and numeric character references like `&#1234;`. Output - A string with all HTML entities and numeric character references replaced by their respective Unicode characters. Constraints - Use the dictionaries from the `html.entities` module where applicable. - The input string can contain any valid HTML5 named character reference or numeric character reference. - Performance: Your solution should handle reasonably large strings efficiently. Example ```python # Example input html_string = \\"The price is &lt; 100 &euro; and &amp; 50&#162;.\\" # Expected output result = \\"The price is < 100 € and & 50¢.\\" # Function signature def decode_html_entities(html_string: str) -> str: pass ``` Additional Information - Use the `html.entities.name2codepoint` dictionary to help decode named HTML entities. - You may use Python\'s built-in functionality for handling numeric character references. Implement this function, ensuring that all test cases are appropriately handled.","solution":"import html def decode_html_entities(html_string: str) -> str: Convert a string with HTML entities to a fully decoded Unicode string. Args: html_string: A string containing HTML entities and numeric character references. Returns: A string with all HTML entities and numeric character references replaced by their Unicode characters. return html.unescape(html_string)"},{"question":"# Sorting Algorithm Implementation and Analysis **Objective:** Implement a custom sorting function in Python and analyze its performance. This task assesses your understanding of sorting algorithms and the ability to optimize and analyze code performance. **Problem Statement:** You are required to write a Python function `custom_sort(arr)` that sorts a list of integers. The function should sort the list in ascending order using the Merge Sort algorithm. Additionally, provide a function `analyze_sort()` that compares the performance of your custom sort function to Python\'s built-in `sorted()` function. **Function Specifications:** 1. **custom_sort(arr: List[int]) -> List[int]:** - **Input:** A list of integers `arr` (1 ≤ len(arr) ≤ 10^5). - **Output:** A list of integers sorted in ascending order. - **Constraints:** You must implement the Merge Sort algorithm. 2. **analyze_sort():** - **Operation:** Generate a list of random integers and measure the time it takes to sort this list using both your `custom_sort()` function and Python\'s built-in `sorted()` function. Compare the performance and print the results. - **Output:** Print the time taken by each sorting method and conclusions drawn from the comparison. **Example:** Given input: ```python arr = [34, 7, 23, 32, 5, 62] custom_sorted_arr = custom_sort(arr) print(custom_sorted_arr) ``` Expected output: ```python [5, 7, 23, 32, 34, 62] ``` Analyze sorting performance: ```python analyze_sort() ``` Expected output (times will vary): ```python Custom sort time: 0.015s Built-in sort time: 0.005s Conclusion: Built-in sort is faster for the input size of 10000. ``` **Hints:** 1. Ensure your `custom_sort` function follows the Merge Sort algorithm, which is a divide and conquer algorithm and has a time complexity of O(n log n). 2. Use the `time` module to measure the time taken by the sorting functions. 3. To generate random integers, you can use the `random` module. **Submission:** Submit a Python file containing the functions `custom_sort(arr)` and `analyze_sort()`. Make sure your code is well-documented, and includes comments explaining key portions of the code.","solution":"from typing import List import time import random def custom_sort(arr: List[int]) -> List[int]: Sorts a list of integers using the Merge Sort algorithm. if len(arr) <= 1: return arr def merge(left: List[int], right: List[int]) -> List[int]: sorted_list = [] i = j = 0 while i < len(left) and j < len(right): if left[i] < right[j]: sorted_list.append(left[i]) i += 1 else: sorted_list.append(right[j]) j += 1 sorted_list.extend(left[i:]) sorted_list.extend(right[j:]) return sorted_list mid = len(arr) // 2 left_half = custom_sort(arr[:mid]) right_half = custom_sort(arr[mid:]) return merge(left_half, right_half) def analyze_sort(): Compares the performance of custom_sort and Python\'s built-in sorted() function. arr = [random.randint(0, 1000000) for _ in range(100000)] start_time = time.time() custom_sort(arr.copy()) custom_sort_time = time.time() - start_time start_time = time.time() sorted(arr) built_in_sort_time = time.time() - start_time print(f\\"Custom sort time: {custom_sort_time:.6f}s\\") print(f\\"Built-in sort time: {built_in_sort_time:.6f}s\\") print(\\"Conclusion: Built-in sort is faster.\\" if built_in_sort_time < custom_sort_time else \\"Conclusion: Custom sort is faster.\\")"},{"question":"**Objective**: Demonstrate the ability to use the `difflib` module to compare sequences and understand differences between them. --- **Question** You are given two lists of strings representing lines of text from two different versions of a document. Your task is to write a function `generate_diffs` that compares these two lists and generates differences in three different formats: context diff, unified diff, and HTML diff with side-by-side comparison. # Function Signature ```python def generate_diffs(old_version: List[str], new_version: List[str], context_lines: int = 3) -> Dict[str, str]: pass ``` # Inputs 1. `old_version` (List[str]): A list of strings where each string is a line of text from the old version of the document. 2. `new_version` (List[str]): A list of strings where each string is a line of text from the new version of the document. 3. `context_lines` (int): An optional integer specifying the number of context lines to show in the diffs. Default value is 3. # Outputs - The function should return a dictionary with three keys: `\'context_diff\'`, `\'unified_diff\'`, and `\'html_diff\'`. Each key should have its corresponding diff output as its value: - `\'context_diff\'`: The context diff format as a string. - `\'unified_diff\'`: The unified diff format as a string. - `\'html_diff\'`: The HTML table showing side-by-side differences as a string. # Constraints 1. The function should handle empty input lists and return appropriate diffs indicating no changes. 2. The HTML output should be complete and renderable as an HTML document. # Example ```python old_version = [ \\"Beautiful is better than ugly.n\\", \\"Explicit is better than implicit.n\\", \\"Simple is better than complex.n\\", \\"Complex is better than complicated.n\\" ] new_version = [ \\"Beautiful is better than ugly.n\\", \\" Simple is better than complex.n\\", \\"Complicated is better than complex.n\\", \\"Flat is better than nested.n\\" ] diffs = generate_diffs(old_version, new_version, context_lines=2) print(diffs[\'context_diff\']) # Output should be a string in context diff format print(diffs[\'unified_diff\']) # Output should be a string in unified diff format print(diffs[\'html_diff\']) # Output should be a string representing an HTML table with side-by-side comparison ``` # Notes - You may use the `difflib.context_diff`, `difflib.unified_diff`, and `difflib.HtmlDiff` utilities to generate the respective diffs. - Ensure that the HTML diff is a complete HTML file that can be rendered in a browser. --- # Assessment Criteria 1. **Correctness**: The function should produce the correct diffs as per the provided inputs. 2. **Efficiency**: The function should efficiently handle large inputs within reasonable time limits. 3. **Readability**: The code should be well-structured and commented for readability. Good luck and happy coding!","solution":"from typing import List, Dict import difflib def generate_diffs(old_version: List[str], new_version: List[str], context_lines: int = 3) -> Dict[str, str]: context_diff = \'n\'.join(difflib.context_diff(old_version, new_version, n=context_lines)) unified_diff = \'n\'.join(difflib.unified_diff(old_version, new_version, n=context_lines)) html_diff_generator = difflib.HtmlDiff() html_diff = html_diff_generator.make_file(old_version, new_version, context=True, numlines=context_lines) return { \'context_diff\': context_diff, \'unified_diff\': unified_diff, \'html_diff\': html_diff }"},{"question":"Objective: Write your own version of a simple encoder and decoder for representing binary files in an ASCII-friendly format. Task: Implement two functions `my_binhex(input_file, output_file)` and `my_hexbin(input_file, output_file)` that simulate the behaviors of `binhex.binhex` and `binhex.hexbin` respectively, without using the `binhex` module. 1. `my_binhex(input_file: str, output_file: str) -> None`: * Takes a binary file as input (`input_file` as the filename) and converts its content to an ASCII-friendly format, storing the encoded content in the filename given by `output_file`. * You are free to choose a simple ASCII-encoding strategy (it doesn\'t have to replicate the exact binhex format). 2. `my_hexbin(input_file: str, output_file: str) -> None`: * Takes a file containing the previously encoded ASCII representation (`input_file` as the filename) and decodes it back to the original binary content, storing the result in the filename given by `output_file`. Requirements: 1. You may not use the `binhex` module, but you can use other common libraries such as `binascii` for encoding/decoding purposes, though it\'s not strictly required. 2. Handle any possible errors gracefully and raise a custom exception named `MyBinHexError` if the encoding or decoding fails. 3. Ensure the integrity of the data: the output of `my_hexbin` should be identical to the original binary content used as input for `my_binhex`. Example: ```python # assuming original file \\"example.bin\\" contains binary data try: my_binhex(\\"example.bin\\", \\"example.hqx\\") my_hexbin(\\"example.hqx\\", \\"example_decoded.bin\\") with open(\\"example.bin\\", \\"rb\\") as f1, open(\\"example_decoded.bin\\", \\"rb\\") as f2: assert f1.read() == f2.read(), \\"The decoded file does not match the original file\\" print(\\"Encoding and decoding were successful and data integrity is maintained.\\") except MyBinHexError as e: print(f\\"An error occurred: {e}\\") ``` Constraints: 1. `input_file` and `output_file` are always valid strings representing file paths. 2. The maximum size of `input_file` is 10 MB. 3. Ensure your solution performs efficiently within this constraint and doesn\'t lead to excessive memory or file I/O operations. Submission: Submit your implementation of `my_binhex` and `my_hexbin`, along with the definition of the custom exception `MyBinHexError`.","solution":"import binascii class MyBinHexError(Exception): pass def my_binhex(input_file, output_file): try: with open(input_file, \'rb\') as infile: binary_data = infile.read() encoded_data = binascii.hexlify(binary_data).decode(\'ascii\') with open(output_file, \'w\') as outfile: outfile.write(encoded_data) except Exception as e: raise MyBinHexError(f\\"Encoding failed: {e}\\") def my_hexbin(input_file, output_file): try: with open(input_file, \'r\') as infile: encoded_data = infile.read() binary_data = binascii.unhexlify(encoded_data.encode(\'ascii\')) with open(output_file, \'wb\') as outfile: outfile.write(binary_data) except Exception as e: raise MyBinHexError(f\\"Decoding failed: {e}\\")"},{"question":"# Question: Advanced Data Class Implementation You are required to define a class using Python\'s `dataclasses` module to represent a collection of products in an inventory system. The class should demonstrate advanced use of dataclasses, including methods, default factories, immutability, and inheritance. This will assess your understanding of core and advanced features provided by the `dataclasses` module. Specifications: 1. **Base Class: `Product`** - Fields: - `name: str` - Name of the product. (`init=True`, `repr=True`, `compare=True`) - `price: float` - Unit price of the product. (`init=True`, `repr=True`, `compare=True`) - `quantity: int` - Quantity of product in stock. (`init=True`, `repr=True`, `compare=True`) - Methods: - `total_value(self) -> float`: Returns the total value of the product (`price * quantity`). 2. **Derived Class: `PerishableProduct`** inheriting from `Product`: - Additional Fields: - `expiry_days: int` - Number of days until the product expires. - `is_expired: bool` - Indicates if the product is expired. Defaults to `False` and should change to `True` if `expiry_days` is `0` or negative. - Methods: - `update_expiry(self)`: Decreases `expiry_days` by 1 and updates `is_expired`. 3. **Constraints:** - The `expiry_days` field in `PerishableProduct` must be keyword-only, enforced by the `KW_ONLY` sentinel. - Both `Product` and `PerishableProduct` should prevent modification after creation (`frozen=True`). - `quantity` should use a default factory to initialize as `0` if not provided. - Implement and demonstrate adding multiple `PerishableProduct` instances to a dataclass-implemented inventory and calculating the combined value of all products. Implementation Details: 1. Implement the `Product` and `PerishableProduct` classes as specified. 2. Create an `Inventory` class using dataclasses which: - Holds a list of `Product` objects. - Methods: - `add_product(self, product: Product)`: Adds a product to the inventory. - `total_inventory_value(self) -> float`: Returns the total combined value of all products in the inventory. 3. Write a driver function to demonstrate: - Creating multiple instances of `PerishableProduct` and adding them to the inventory. - Updating the expiry of some products. - Calculating the total value of the inventory before and after updating expiry dates. Input and Output Formats: - **Input:** None from user. - **Output:** Print statements demonstrating the functionality as required in the driver function. Should print product details, inventory values before and after updates. Code Template: ```python from dataclasses import dataclass, field, KW_ONLY from typing import List @dataclass(frozen=True) class Product: name: str price: float quantity: int = field(default_factory=int) def total_value(self) -> float: return self.price * self.quantity @dataclass(frozen=True) class PerishableProduct(Product): _: KW_ONLY expiry_days: int is_expired: bool = field(default=False, init=False) def update_expiry(self): new_expiry = self.expiry_days - 1 object.__setattr__(self, \'expiry_days\', new_expiry) if new_expiry <= 0: object.__setattr__(self, \'is_expired\', True) @dataclass class Inventory: products: List[Product] = field(default_factory=list) def add_product(self, product: Product): self.products.append(product) def total_inventory_value(self) -> float: return sum(p.total_value() for p in self.products) def driver(): # Create Inventory inventory = Inventory() # Add Products p1 = PerishableProduct(name=\\"Milk\\", price=2.5, quantity=20, expiry_days=5) p2 = PerishableProduct(name=\\"Eggs\\", price=0.2, quantity=200, expiry_days=2) # Add to Inventory inventory.add_product(p1) inventory.add_product(p2) # Print total inventory value print(f\\"Total Inventory Value: {inventory.total_inventory_value()}\\") # Update expiry p1.update_expiry() p2.update_expiry() # Print updated expiry and inventory value print(f\\"Total Inventory Value after expiry update: {inventory.total_inventory_value()}\\") print(p1) print(p2) driver() ``` Provide the necessary code implementation as outlined above and verify it thoroughly.","solution":"from dataclasses import dataclass, field, KW_ONLY from typing import List @dataclass(frozen=True) class Product: name: str price: float quantity: int = field(default_factory=int) def total_value(self) -> float: return self.price * self.quantity @dataclass(frozen=True) class PerishableProduct(Product): _: KW_ONLY expiry_days: int is_expired: bool = field(default=False, init=False) def update_expiry(self): new_expiry = self.expiry_days - 1 object.__setattr__(self, \'expiry_days\', new_expiry) if new_expiry <= 0: object.__setattr__(self, \'is_expired\', True) @dataclass class Inventory: products: List[Product] = field(default_factory=list) def add_product(self, product: Product): self.products.append(product) def total_inventory_value(self) -> float: return sum(p.total_value() for p in self.products) def driver(): # Create Inventory inventory = Inventory() # Add Products p1 = PerishableProduct(name=\\"Milk\\", price=2.5, quantity=20, expiry_days=5) p2 = PerishableProduct(name=\\"Eggs\\", price=0.2, quantity=200, expiry_days=2) # Add to Inventory inventory.add_product(p1) inventory.add_product(p2) # Print total inventory value print(f\\"Total Inventory Value: {inventory.total_inventory_value()}\\") # Update expiry p1.update_expiry() p2.update_expiry() # Print updated expiry and inventory value print(f\\"Total Inventory Value after expiry update: {inventory.total_inventory_value()}\\") print(p1) print(p2) driver()"},{"question":"Problem Statement You are tasked with writing a Python script that uses the `pickle` and `pickletools` modules to serialize, analyze, and optimize a Python object. Specifically, you will: 1. Serialize a given Python object to a pickle string. 2. Disassemble the generated pickle string and print its human-readable form. 3. Optimize the pickled string to eliminate redundant opcodes. 4. Disassemble the optimized pickle string and print the human-readable form of the optimized output. 5. Compare the original and optimized pickle strings in terms of length and efficiency of unpickling. # Instructions 1. Create a function named `analyze_and_optimize_pickle(obj)` that performs the following steps: - Takes a single input `obj`, which can be any Python object. - Serializes `obj` using `pickle.dumps()` with the highest available protocol. - Disassembles the serialized pickle string using `pickletools.dis()` and prints the output. - Optimizes the serialized pickle string using `pickletools.optimize()`. - Disassembles the optimized pickle string using `pickletools.dis()` and prints the output. - Compares the length of the original and optimized pickle strings and prints the comparison results. # Example Usage ```python def analyze_and_optimize_pickle(obj): # Step 1: Serialize the object import pickle import pickletools original_pickle = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL) # Step 2: Disassemble the original pickle print(\\"Original Pickle Disassembly:\\") pickletools.dis(original_pickle) # Step 3: Optimize the pickle optimized_pickle = pickletools.optimize(original_pickle) # Step 4: Disassemble the optimized pickle print(\\"nOptimized Pickle Disassembly:\\") pickletools.dis(optimized_pickle) # Step 5: Compare the lengths print(f\\"nLength of original pickle: {len(original_pickle)} bytes\\") print(f\\"Length of optimized pickle: {len(optimized_pickle)} bytes\\") print(f\\"Reduction in size: {len(original_pickle) - len(optimized_pickle)} bytes\\") # Example test analyze_and_optimize_pickle([(1, 2), (3, 4), \\"Test String\\"]) ``` # Constraints - You should handle any object without assuming its type. - It is allowed to print additional information if needed for clarity. # Evaluation Criteria - Correctness: The function should correctly serialize, disassemble, optimize, and compare the pickle data. - Code Quality: Solutions should be well-organized and include comments where appropriate. - Efficiency: Consideration of performance, especially in handling large and complex data structures.","solution":"import pickle import pickletools def analyze_and_optimize_pickle(obj): Analyzes and optimizes the given Python object by serializing with pickle, disassembling the pickle, optimizing it, and comparing the lengths of the original and optimized pickles. # Step 1: Serialize the object original_pickle = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL) # Step 2: Disassemble the original pickle print(\\"Original Pickle Disassembly:\\") pickletools.dis(original_pickle) # Step 3: Optimize the pickle optimized_pickle = pickletools.optimize(original_pickle) # Step 4: Disassemble the optimized pickle print(\\"nOptimized Pickle Disassembly:\\") pickletools.dis(optimized_pickle) # Step 5: Compare the lengths original_length = len(original_pickle) optimized_length = len(optimized_pickle) print(f\\"nLength of original pickle: {original_length} bytes\\") print(f\\"Length of optimized pickle: {optimized_length} bytes\\") print(f\\"Reduction in size: {original_length - optimized_length} bytes\\") # Example Usage for manual testing (For unit tests, see below) analyze_and_optimize_pickle([(1, 2), (3, 4), \\"Test String\\"])"},{"question":"**Objective:** Implement a function that calculates the total CPU time consumed by a thread after it runs a given task a specified number of times. The task involves sleeping for a random short duration and performing some computation. Use functionalities from the `time` module to achieve accurate timing measurement. **Task:** 1. Implement the function `measure_thread_cpu_time` with the following signature: ```python def measure_thread_cpu_time(task, count: int) -> float: pass ``` 2. The function should perform the following: - **task**: A callable that contains the code for the task to be run. The task should sleep for a short random duration (between 0.1 to 0.5 seconds) and perform a simple computation (such as summing a list of size 1,000,000). - **count**: An integer representing how many times to run the task within a single function call. 3. The function should return the total CPU time (in seconds) consumed by the thread executing the task `count` times, using the process time as a measure of CPU time. **Constraints:** - You are required to use `time.thread_time()` to get the CPU time used by the thread. - Assume the task function provided will be thread-safe. - The function should handle invalid inputs gracefully (e.g., non-callable `task` or non-positive `count`) by raising appropriate exceptions. **Example Usage:** ```python import time import threading import random def example_task(): time.sleep(random.uniform(0.1, 0.5)) sum(range(1000000)) # Measure the CPU time consumed by running the example_task 5 times total_cpu_time = measure_thread_cpu_time(example_task, 5) print(f\\"Total CPU time: {total_cpu_time:.6f} seconds\\") ``` **Expected Output:** For the example above, an expected output could be something like: ``` Total CPU time: 0.200345 seconds ``` Here is the general structure to follow (fill in the implementation details): ```python import time import random def measure_thread_cpu_time(task, count: int) -> float: if not callable(task): raise TypeError(\\"The task must be callable.\\") if not isinstance(count, int) or count <= 0: raise ValueError(\\"Count must be a positive integer.\\") # Record the start CPU time start_cpu_time = time.thread_time() # Run the task \\"count\\" times for _ in range(count): task() # Record the end CPU time end_cpu_time = time.thread_time() # Calculate and return the total CPU time used total_cpu_time = end_cpu_time - start_cpu_time return total_cpu_time ``` **Notes:** - Ensure accuracy and precision by using the appropriate timing functions from the `time` module. - The function should be thread-specific to avoid including CPU time from other processes or threads.","solution":"import time import random def measure_thread_cpu_time(task, count: int) -> float: if not callable(task): raise TypeError(\\"The task must be callable.\\") if not isinstance(count, int) or count <= 0: raise ValueError(\\"Count must be a positive integer.\\") # Record the start CPU time start_cpu_time = time.thread_time() # Run the task \\"count\\" times for _ in range(count): task() # Record the end CPU time end_cpu_time = time.thread_time() # Calculate and return the total CPU time used total_cpu_time = end_cpu_time - start_cpu_time return total_cpu_time"},{"question":"# Assessment Question: You are provided with a class `netrc` that parses and encapsulates data from a netrc file. The netrc file format is typically used by Unix ftp programs and other FTP clients to store login information. Task: Write a function `get_authenticator_info(file_path, host)` that takes two arguments: 1. `file_path` (str): The path to the netrc file to be parsed. 2. `host` (str): The hostname for which to retrieve authentication information. The function should return a dictionary with the following entries: - `\'login\'`: the login username for the given host. - `\'account\'`: the account name for the given host (can be None if not present). - `\'password\'`: the password for the given host. If the file does not exist, raise a `FileNotFoundError`. If there is a parsing error, raise a custom exception `CustomNetrcParseError` with the message, filename, and line number where the error occurred. If the host is not found and there is no \'default\' entry, return `None`. Constraints: - Assume that the .netrc file follows the standard format and it is utf-8 encoded. - Do not implement the full `netrc` class; use it as described in the documentation to parse and retrieve the necessary data. Example: ```python # Sample usage of the function try: result = get_authenticator_info(\'/path/to/.netrc\', \'example.com\') if result: print(\\"Login:\\", result[\'login\']) print(\\"Account:\\", result[\'account\']) print(\\"Password:\\", result[\'password\']) else: print(\\"No entry found for specified host\\") except FileNotFoundError: print(\\"The specified netrc file does not exist.\\") except CustomNetrcParseError as e: print(f\\"Parse error in file {e.filename} on line {e.lineno}: {e.msg}\\") ``` Implementation Note: Define the custom exception `CustomNetrcParseError` that inherits from `Exception`. This exception should accept three parameters: `msg`, `filename`, and `lineno`, and store them as instance attributes. Function Signature: ```python def get_authenticator_info(file_path: str, host: str) -> dict: # Your code here ```","solution":"import netrc class CustomNetrcParseError(Exception): def __init__(self, msg, filename, lineno): super().__init__(msg) self.msg = msg self.filename = filename self.lineno = lineno def get_authenticator_info(file_path, host): try: netrc_obj = netrc.netrc(file_path) except FileNotFoundError: raise FileNotFoundError(\\"The specified netrc file does not exist.\\") except netrc.NetrcParseError as e: raise CustomNetrcParseError(e.msg, e.filename, e.lineno) auth_info = netrc_obj.authenticators(host) if auth_info is None: auth_info = netrc_obj.authenticators(\'default\') if auth_info is None: return None return { \'login\': auth_info[0], \'account\': auth_info[1], \'password\': auth_info[2] }"},{"question":"You are tasked to implement a function `highlight_weekends_in_html_year` using the `calendar` module from Python’s standard library. This function should generate an HTML calendar for an entire year and highlight weekends (Saturday and Sunday) by applying a custom CSS class. # Function Signature ```python def highlight_weekends_in_html_year(year: int) -> str: pass ``` # Input - `year` (int): The year for which the calendar is to be generated. Example: `2023`. # Output - Return an HTML-formatted string representing the calendar for the specified year with weekends highlighted using the `highlighted-weekend` CSS class. # Constraints - The year will be a positive integer within the range [1, 9999]. - Output HTML should be a complete HTML page with proper tags (`<html>`, `<head>`, `<body>`, etc.). # Example ```python html_calendar = highlight_weekends_in_html_year(2023) print(html_calendar) ``` This function should make use of the `HTMLCalendar` class from the `calendar` module, and extend or customize it as necessary to accomplish the task. # Hint - You might want to create a subclass of `HTMLCalendar` where you override the methods responsible for rendering the days to include the special CSS class for weekends. # CSS for Highlighted Weekends ```css <style> .highlighted-weekend { background-color: yellow; } </style> ``` # Evaluation Criteria - Correctness: The function should correctly generate and return the desired HTML calendar. - Use of Calendar Module: The solution should effectively use the functionalities provided by the `calendar` module. - Code Quality: The code should be clean, well-documented, and efficiently organized. --- Note: The complete CSS `<style>` section should be embedded in the HTML output by the function.","solution":"import calendar class HighlightedHTMLCalendar(calendar.HTMLCalendar): def formatday(self, day, weekday): Return a day as a table cell. if day == 0: return \'<td class=\\"noday\\">&nbsp;</td>\' # day outside month else: cssclass = \'\' if weekday == calendar.SATURDAY or weekday == calendar.SUNDAY: cssclass = \' class=\\"highlighted-weekend\\"\' return f\'<td{cssclass}>{day}</td>\' def highlight_weekends_in_html_year(year: int) -> str: cal = HighlightedHTMLCalendar(calendar.SUNDAY) html_calendar = cal.formatyear(year) full_html = f <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\"> <title>Calendar {year}</title> <style> .highlighted-weekend {{ background-color: yellow; }} </style> </head> <body> {html_calendar} </body> </html> return full_html"},{"question":"Coding Assessment Question # Objective Your task is to create a function that processes data in a performance-efficient manner using the functionalities available in the `functools` module of Python. Your function will read a list of numbers, apply a transformation, and return the results. You must utilize caching and partial application efficiently. # Problem Statement 1. **Create a Cached Fibonacci Function**: Implement a recursive function to calculate the nth Fibonacci number (`fib(n)`). Decorate this function with `@lru_cache` to cache results for performance optimization. 2. **Create a Partial Function for Scaling**: Use the `partial` function to create a new version of a function that multiplies its input by a given factor. The factor should be fixed using `partial`. 3. **Compose the Functions**: Write a function `process_numbers` that: - Takes a list of integers as input. - Uses the cached Fibonacci function to compute the Fibonacci number for each integer. - Scales the Fibonacci result by a factor of 10 using the partial function created in step 2. - Returns a list of the scaled Fibonacci numbers. # Requirements - **Function Definitions**: - `fib(n: int) -> int`: Calculates and returns the nth Fibonacci number. - `scale(factor: int, value: int) -> int`: Multiplies `value` by `factor`. - `process_numbers(nums: List[int]) -> List[int]`: Composes the above functionalities to process the input list. - **Input/Output**: - `process_numbers` should take a list of integers `nums` as input and return a list of integers. - **Performance Considerations**: - Use `@lru_cache` for the Fibonacci function to optimize redundant calculations. - Utilize `functools.partial` for creating the scaling partial function. # Constraints - The input list `nums` contains up to 10,000 integers in the range `[0, 100]`. # Example ```python from functools import lru_cache, partial # Step 1: Implement cached Fibonacci function @lru_cache(maxsize=None) def fib(n: int) -> int: if n < 2: return n return fib(n-1) + fib(n-2) # Step 2: Implement the scaling function and create a partial function def scale(factor: int, value: int) -> int: return factor * value scale_by_10 = partial(scale, 10) # Step 3: Implement the process_numbers function def process_numbers(nums: List[int]) -> List[int]: return [scale_by_10(fib(num)) for num in nums] # Example usage nums = [0, 1, 2, 3, 4, 5] print(process_numbers(nums)) # Output: [0, 10, 10, 20, 30, 50] ``` Good luck with your implementation!","solution":"from functools import lru_cache, partial from typing import List # Step 1: Implement cached Fibonacci function @lru_cache(maxsize=None) def fib(n: int) -> int: Returns the nth Fibonacci number. if n < 2: return n return fib(n-1) + fib(n-2) # Step 2: Implement the scaling function and create a partial function def scale(factor: int, value: int) -> int: Scales the value by the given factor. return factor * value scale_by_10 = partial(scale, 10) # Step 3: Implement the process_numbers function def process_numbers(nums: List[int]) -> List[int]: Takes a list of integers, computes their Fibonacci numbers, scales by 10, and returns the list of results. return [scale_by_10(fib(num)) for num in nums]"},{"question":"# PyTorch Advanced Attention Mechanism Implementation Problem Statement In this assessment, you will implement a custom function using PyTorch to generate a causal mask useful for attention mechanisms. The causal mask ensures that each position in the sequence only attends to previous positions and itself, not the future positions. This concept is essential in sequence-to-sequence models like transformers when processing autoregressive tasks. Task Write a function `create_causal_mask(sequence_length: int) -> torch.Tensor` that generates a binary (0 and 1) causal mask matrix of size `(sequence_length, sequence_length)` using PyTorch. The mask should have ones on the lower triangle and zeros above the main diagonal. This will ensure that at each time step, the model can attend to the current and all previous time steps, but not the future ones. Function Signature ```python def create_causal_mask(sequence_length: int) -> torch.Tensor: ``` Input - `sequence_length` (int): The length of the input sequence for which the mask is to be created. Output - Returns a PyTorch tensor of shape `(sequence_length, sequence_length)` containing the causal mask. Example ```python >>> create_causal_mask(4) tensor([[1., 0., 0., 0.], [1., 1., 0., 0.], [1., 1., 1., 0.], [1., 1., 1., 1.]]) ``` Constraints - Do not use any explicit for-loops for generating the mask. - Utilize PyTorch functions to ensure efficiency. Tips - Explore PyTorch tensor operations like `torch.tril` which can help in creating lower triangular matrices. - Ensure the tensor is of type `torch.float32` because attention masks in models are generally in floating-point format. Performance - The implementation should be efficient in terms of both time and space complexity, leveraging PyTorch\'s optimized tensor operations. Submission - Submit your implementation as a single Python function. Make sure that the function signature matches the one provided, and the function should be able to run efficiently for sequence lengths up to 1000.","solution":"import torch def create_causal_mask(sequence_length: int) -> torch.Tensor: Generates a causal mask for attention mechanisms, ensuring each position only attends to previous and current positions, not future ones. Args: sequence_length (int): The length of the input sequence for which the mask is to be created. Returns: torch.Tensor: A tensor with shape (sequence_length, sequence_length) containing the causal mask. mask = torch.tril(torch.ones(sequence_length, sequence_length), diagonal=0) return mask"},{"question":"# Multi-threaded Producer-Consumer Problem You are required to implement a multi-threaded producer-consumer system using Python\'s `threading` module. In this system, there should be multiple producer and consumer threads sharing a common bounded buffer. Producers generate items and place them in the buffer, while consumers remove and process these items. Proper synchronization must be ensured to avoid race conditions and ensure thread safety. Requirements 1. **Buffer**: Implement a class `BoundedBuffer` that has methods `put` (for producers to add items) and `get` (for consumers to remove items). 2. **Producer**: Implement a `Producer` class that generates items and places them in the buffer. 3. **Consumer**: Implement a `Consumer` class that consumes items from the buffer. 4. **Thread Synchronization**: Use appropriate synchronization primitives (Locks, Semaphores, and Conditions) to ensure that: - Producers wait if the buffer is full. - Consumers wait if the buffer is empty. - Multiple producers and consumers can work concurrently without conflict. 5. **Graceful Termination**: Implement a mechanism to signal all threads to terminate gracefully after a certain number of items have been produced and consumed. Implementation Details 1. **BoundedBuffer Class**: - Initialize a buffer with a fixed size. - Methods: - `put(item)`: Add an item to the buffer. If the buffer is full, the producer should wait until space is available. - `get() -> item`: Remove and return an item from the buffer. If the buffer is empty, the consumer should wait until an item is available. 2. **Producer Class (inherits threading.Thread)**: - Override the `run` method. - Generate items (using a simple counter or random data). - Use the `put` method of `BoundedBuffer` to add items. 3. **Consumer Class (inherits threading.Thread)**: - Override the `run` method. - Use the `get` method of `BoundedBuffer` to remove items and process them (e.g., print the item). 4. **Synchronization**: - Use `threading.Condition` for the buffer to coordinate between producers and consumers. - Ensure thread-safe operations using `threading.Lock` or `threading.Semaphore`. 5. **Termination**: - Implement a termination signal that stops the producers after a specific number of items have been produced. - Ensure consumers can exit gracefully after all items have been consumed. Example Usage ```python if __name__ == \\"__main__\\": buffer_size = 5 num_producers = 3 num_consumers = 3 total_items = 30 buffer = BoundedBuffer(buffer_size) producers = [Producer(buffer, total_items // num_producers) for _ in range(num_producers)] consumers = [Consumer(buffer, total_items // num_consumers) for _ in range(num_consumers)] for p in producers: p.start() for c in consumers: c.start() for p in producers: p.join() for c in consumers: c.join() print(\\"All items have been produced and consumed.\\") ``` This example sets up a system with a bounded buffer and an equal number of producers and consumers that share the work of producing and consuming items. Make sure your implementation handles synchronization correctly to avoid issues like deadlocks and race conditions. Constraints - Ensure that the buffer size is at least 1 and no more than 100. - The number of producers and consumers should each be at least 1 and no more than 10. - Total items to be produced should be a positive integer. Implement your solution in Python and ensure it adheres to the requirements and constraints provided.","solution":"import threading import time import random class BoundedBuffer: def __init__(self, size): self.size = size self.buffer = [] self.lock = threading.Condition() def put(self, item): with self.lock: while len(self.buffer) >= self.size: self.lock.wait() self.buffer.append(item) self.lock.notify_all() def get(self): with self.lock: while not self.buffer: self.lock.wait() item = self.buffer.pop(0) self.lock.notify_all() return item class Producer(threading.Thread): def __init__(self, buffer, num_items): threading.Thread.__init__(self) self.buffer = buffer self.num_items = num_items def run(self): for _ in range(self.num_items): item = random.randint(0, 100) self.buffer.put(item) print(f\'Produced {item}\') time.sleep(random.uniform(0.1, 0.5)) class Consumer(threading.Thread): def __init__(self, buffer, num_items): threading.Thread.__init__(self) self.buffer = buffer self.num_items = num_items def run(self): for _ in range(self.num_items): item = self.buffer.get() print(f\'Consumed {item}\') time.sleep(random.uniform(0.1, 0.5)) if __name__ == \\"__main__\\": buffer_size = 5 num_producers = 3 num_consumers = 3 total_items = 30 buffer = BoundedBuffer(buffer_size) producers = [Producer(buffer, total_items // num_producers) for _ in range(num_producers)] consumers = [Consumer(buffer, total_items // num_consumers) for _ in range(num_consumers)] for p in producers: p.start() for c in consumers: c.start() for p in producers: p.join() for c in consumers: c.join() print(\\"All items have been produced and consumed.\\")"},{"question":"# PyTorch Accelerator Device Management and Synchronization Objective Create a function that performs the following tasks in order: 1. Check how many accelerator devices are available. If none are available, raise a `RuntimeError` with the message, \\"No accelerator devices available.\\" 2. Print the type/identity of the current accelerator device. 3. Change the current device to the device with a specified index if available. 4. Set a specific stream for the current device. 5. Synchronize the current stream of the current device. Function Signature ```python def manage_accelerators(device_index: int, stream) -> None: pass ``` Input - `device_index` (int): The index of the device to be set as the current device. - `stream`: The stream to be set for the current device. Output - The function does not return any value. It should perform the specified operations and print relevant information. Constraints - If the specified `device_index` is out of range (i.e., not a valid device index), raise an `IndexError` with the message, \\"Invalid device index.\\" - If an error occurs while setting the stream, raise a `RuntimeError` with the message, \\"Failed to set stream.\\" Example ```python manage_accelerators(0, some_stream) ``` Additional Information - You may assume that the imports and any necessary setup for accelerator devices are already handled. - Ensure proper exception handling and synchronization to avoid any runtime issues. Good luck!","solution":"import torch def manage_accelerators(device_index: int, stream) -> None: # 1. Check available accelerator devices if not torch.cuda.is_available(): raise RuntimeError(\\"No accelerator devices available.\\") num_devices = torch.cuda.device_count() print(f\\"Number of accelerator devices available: {num_devices}\\") # 2. Print the type/identity of the current accelerator device current_device = torch.cuda.current_device() print(f\\"Current accelerator device before change: {torch.cuda.get_device_name(current_device)}\\") # 3. Change the current device to the device with a specified index if available if device_index >= num_devices or device_index < 0: raise IndexError(\\"Invalid device index.\\") torch.cuda.set_device(device_index) print(f\\"Changed current accelerator device to: {torch.cuda.get_device_name(device_index)}\\") # 4. Set a specific stream for the current device try: torch.cuda.set_stream(stream) print(f\\"Stream set to the specified stream for device {device_index}\\") except Exception as e: raise RuntimeError(\\"Failed to set stream.\\") from e # 5. Synchronize the current stream of the current device torch.cuda.synchronize() print(f\\"Synchronized the current stream for device {device_index}\\")"},{"question":"Coding Assessment Question **Objective:** Implement and benchmark multiple variants of a matrix multiplication function using PyTorch, analyze their performance using `torch.utils.benchmark`, and identify the most efficient implementation. # Task Description 1. **Implement Matrix Multiplication Functions:** - Implement three different methods for matrix multiplication using PyTorch: 1. Standard method using nested loops. 2. Using PyTorch\'s built-in `torch.mm` function. 3. Using PyTorch\'s batched matrix multiplication. 2. **Benchmarking:** - Use the `torch.utils.benchmark.Timer` class to measure the performance of each matrix multiplication method. - Generate timing results for various sizes of input matrices (e.g., 128x128, 256x256, 512x512). 3. **Analysis:** - After collecting the benchmarking data, compare the performance of the different implementations and explain why one method may be faster than another. # Guidelines - **Input Format:** - Each method should take two matrices `A` and `B` as input. Matrices will be square (NxN) with `N` being a power of 2 (e.g., 128, 256, 512). - **Output Format:** - Each method should return the resulting matrix from the multiplication. - Performance results should be printed in a clear format, comparing the execution time of each method. - **Constraints:** - You are allowed to use only PyTorch and Python standard libraries. - Ensure the code is well-documented and readable. - **Performance Requirements:** - The benchmarking results should be accurate and repeatable. Use appropriate warm-ups and repetitions to ensure robust comparisons. # Example Code Structure ```python import torch import torch.utils.benchmark as benchmark def matrix_multiplication_loops(A, B): N = A.size(0) result = torch.zeros_like(A) for i in range(N): for j in range(N): for k in range(N): result[i, j] += A[i, k] * B[k, j] return result def matrix_multiplication_torch(A, B): return torch.mm(A, B) def matrix_multiplication_batched(A, B): return torch.bmm(A.unsqueeze(0), B.unsqueeze(0)).squeeze(0) def benchmark_matrix_multiplication(): sizes = [128, 256, 512] methods = { \\"Nested Loops\\": matrix_multiplication_loops, \\"Torch MM\\": matrix_multiplication_torch, \\"Batched\\": matrix_multiplication_batched, } for size in sizes: A = torch.rand(size, size) B = torch.rand(size, size) for method_name, method in methods.items(): t = benchmark.Timer(stmt=\'method(A, B)\', globals={\'method\': method, \'A\': A, \'B\': B}) result = t.blocked_autorange() print(f\\"Method: {method_name}, Size: {size}x{size}, Time: {result.median:.6f}s\\") if __name__ == \\"__main__\\": benchmark_matrix_multiplication() ``` # Explanation: - Implement the functions for matrix multiplication using the specified approaches. - Utilize the `torch.utils.benchmark.Timer` class to time the execution of these functions with different matrix sizes. - Print out the benchmarking results, comparing the performance of each method. # Deliverables: - Source code implementing the described functions and benchmarking setup. - A brief report or comments in the code describing the findings and performance differences.","solution":"import torch import torch.utils.benchmark as benchmark def matrix_multiplication_loops(A, B): Multiply matrices A and B using nested loops. N = A.size(0) result = torch.zeros_like(A) for i in range(N): for j in range(N): for k in range(N): result[i, j] += A[i, k] * B[k, j] return result def matrix_multiplication_torch(A, B): Multiply matrices A and B using PyTorch\'s built-in matrix multiplication function. return torch.mm(A, B) def matrix_multiplication_batched(A, B): Multiply matrices A and B using PyTorch\'s batched matrix multiplication. return torch.bmm(A.unsqueeze(0), B.unsqueeze(0)).squeeze(0) def benchmark_matrix_multiplication(): Benchmark different matrix multiplication methods using varying matrix sizes. sizes = [128, 256, 512] methods = { \\"Nested Loops\\": matrix_multiplication_loops, \\"Torch MM\\": matrix_multiplication_torch, \\"Batched\\": matrix_multiplication_batched, } for size in sizes: A = torch.rand(size, size) B = torch.rand(size, size) for method_name, method in methods.items(): t = benchmark.Timer(stmt=\'method(A, B)\', globals={\'method\': method, \'A\': A, \'B\': B}) result = t.blocked_autorange() print(f\\"Method: {method_name}, Size: {size}x{size}, Time: {result.median:.6f}s\\")"},{"question":"<|Analysis Begin|> The provided documentation primarily delves into the concept of error bars in data visualization using the seaborn library. It discusses the importance of statistical estimation and error bars for depicting the uncertainty in statistical summaries. The documentation explains different methods for generating these error bars, such as: 1. **Standard Deviation Error Bars**: Representing the spread of data using standard deviation. 2. **Percentile Interval Error Bars**: Representing data spread using percentiles. 3. **Standard Error Bars**: Indicating uncertainty related to the estimate\'s precision by using standard errors. 4. **Confidence Interval Error Bars**: Representing estimate uncertainty using bootstrapping to generate confidence intervals. 5. **Custom Error Bars**: Allowing for customized error bars using user-defined functions. The documentation provides code snippets and examples of these methods, highlighting their use cases, parameter settings, and visual output. Given this information, I can design a question that assesses the student\'s understanding of these different error bar methods and their ability to implement a customized visualization using seaborn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Demonstrate your understanding of various error bar methods in seaborn and implement a customized visualization using these concepts. Problem Statement You are provided with a dataset containing the results of a fictional experiment measuring a specific variable. Your task is to create a visualization that combines three different types of error bars on the same plot and customize the error bars for meaningful representation. Dataset ```python import numpy as np import pandas as pd # Generating the dataset np.random.seed(42) data = pd.DataFrame({ \'group\': np.random.choice([\'A\', \'B\', \'C\'], size=300), \'value\': np.random.normal(loc=0, scale=1, size=300) }) ``` Requirements 1. **Visualization**: - Create a point plot showing the mean value for each group in the dataset with three types of error bars: * Standard deviation. * Percentile interval (choose a 70% interval). * Confidence interval (use default settings for bootstrapping). 2. **Customization**: - Customize the appearance of each error bar type using different colors. - Add labels and a legend to the plot for clarity. - Include a title describing the plot. 3. **Function Implementation**: - Implement a function `visualize_errorbars(data: pd.DataFrame) -> None` that takes a pandas DataFrame and generates the required plot. Example Output Your plot should visually display each group with three types of error bars, clearly distinguishable from one another. Constraints - Use seaborn for plotting. - The function should not return any value; it should only generate and display the plot. - Ensure the plot is clear and informative by styling and labeling appropriately. Performance Requirements - Efficiently compute and plot the error bars, leveraging the capabilities of seaborn. Expected Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_errorbars(data: pd.DataFrame) -> None: pass ``` **Hint**: Refer to the seaborn documentation on error bars to correctly implement each type and customize their appearances.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def visualize_errorbars(data: pd.DataFrame) -> None: Creates a point plot with three types of error bars: standard deviation, percentile interval, and confidence interval. plt.figure(figsize=(10, 6)) # Plot standard deviation error bars sns.pointplot(data=data, x=\'group\', y=\'value\', ci=\'sd\', color=\'blue\', linestyles=\'-\', label=\'Standard Deviation\') # Plot percentile interval error bars (70% interval) sns.pointplot(data=data, x=\'group\', y=\'value\', estimator=np.mean, ci=70, color=\'green\', linestyles=\'--\', label=\'70% Percentile Interval\') # Plot confidence interval error bars sns.pointplot(data=data, x=\'group\', y=\'value\', ci=\'sd\', color=\'red\', linestyles=\'-.\', label=\'Confidence Interval\') plt.legend() plt.title(\'Group-wise Mean Value with Different Error Bars\') plt.xlabel(\'Group\') plt.ylabel(\'Value\') plt.show() # Generating the dataset np.random.seed(42) data = pd.DataFrame({ \'group\': np.random.choice([\'A\', \'B\', \'C\'], size=300), \'value\': np.random.normal(loc=0, scale=1, size=300) }) # Visualize the error bars visualize_errorbars(data)"},{"question":"**Coding Assessment Question** # Objective Write a function that takes in a DataFrame along with a set of styling parameters and generates and saves a seaborn boxplot visualizing the data. The function should demonstrate your understanding of seaborn\'s ability to control figure aesthetics, scale, and context settings. # Function Signature ```python def styled_boxplot(df: pd.DataFrame, style: str, context: str, font_scale: float, despine: bool, output_file: str) -> None: Generate and save a seaborn boxplot with specified styles and configurations. Parameters: df (pd.DataFrame): Input data as a DataFrame to plot. style (str): The seaborn style to apply (e.g., \'darkgrid\', \'whitegrid\'). context (str): The context parameter for scaling plot elements (e.g., \'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): Scaling factor for the fonts used in the plot. despine (bool): Whether to remove the top and right spines from the plot. output_file (str): File path to save the generated plot. Returns: None ``` # Description Implement the function `styled_boxplot(df, style, context, font_scale, despine, output_file)` that: 1. Configures the seaborn style using the provided `style` argument. Valid options include \'darkgrid\', \'whitegrid\', \'dark\', \'white\', and \'ticks\'. 2. Configures the seaborn context for scaling the plot elements using the provided `context` argument. Valid options include \'paper\', \'notebook\', \'talk\', and \'poster\'. 3. Scales the font elements in the plot using the provided `font_scale` argument. 4. Optionally removes the top and right spines from the plot if `despine` is `True`. 5. Generates a boxplot using the data in the DataFrame `df`. 6. Saves the plot to the file specified by `output_file`. # Example ```python import pandas as pd data = pd.DataFrame({ \'A\': np.random.normal(size=100), \'B\': np.random.normal(size=100), \'C\': np.random.normal(size=100), \'D\': np.random.normal(size=100) }) styled_boxplot(data, \'whitegrid\', \'talk\', 1.2, True, \'boxplot.png\') ``` This function call should configure the seaborn style as \'whitegrid\', set the context as \'talk\', scale the font elements by 1.2, remove the top and right spines, generate a boxplot, and save it to \'boxplot.png\'. # Constraints - Ensure the file path is valid and writable. - Handle errors gracefully, such as invalid style or context names, and provide appropriate error messages.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def styled_boxplot(df: pd.DataFrame, style: str, context: str, font_scale: float, despine: bool, output_file: str) -> None: Generate and save a seaborn boxplot with specified styles and configurations. Parameters: df (pd.DataFrame): Input data as a DataFrame to plot. style (str): The seaborn style to apply (e.g., \'darkgrid\', \'whitegrid\'). context (str): The context parameter for scaling plot elements (e.g., \'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): Scaling factor for the fonts used in the plot. despine (bool): Whether to remove the top and right spines from the plot. output_file (str): File path to save the generated plot. Returns: None try: # Set the style, context, and font scale sns.set(style=style, context=context, font_scale=font_scale) # Create the plot plt.figure(figsize=(10, 6)) sns.boxplot(data=df) # Optionally remove the top and right spines if despine: sns.despine() # Save the plot to the specified file plt.savefig(output_file) plt.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Analyzing Real-World Datasets with `scikit-learn` You are provided with a larger real-world dataset using the `fetch_*` functions from `sklearn.datasets`. Your task is to write a Python function that performs the following steps: 1. **Fetch the Dataset**: Use a dataset fetcher function to download and load the \\"California housing\\" dataset from `scikit-learn`. 2. **Data Inspection**: - Print the shape of the input features `data` and the target `target`. - Print the first 5 rows of the `data` array. 3. **Data Analysis**: - Compute and print the mean and standard deviation of each feature in the dataset. 4. **Data Preparation**: - Split the dataset into a training set (80%) and a test set (20%). - Standardize the features to have zero mean and unit variance using `StandardScaler`. 5. **Model Training**: - Train a linear regression model (`LinearRegression`) on the training set. - Report the mean squared error on the test set. # Requirements: - Your solution should use appropriate functions from `sklearn.datasets`, `sklearn.model_selection`, `sklearn.preprocessing`, and `sklearn.linear_model`. - Your function should be named `analyze_california_housing`. # Function Signature ```python def analyze_california_housing(): pass ``` # Constraints: - The dataset must be fetched using `sklearn.datasets.fetch_california_housing`. - You must ensure the reproducibility of the train-test split by setting a random state in `train_test_split`. # Example Output: ``` Dataset shape: (20640, 8) Target shape: (20640,) First 5 rows of data: [[ 8.3252e+00 4.1000e+01 6.9841e+00 1.0230e+03 3.2200e+02 2.5556e+00 3.7880e+01 -1.2223e+02] [ 8.3014e+00 2.1000e+01 6.2381e+00 2.4000e+02 1.1300e+02 2.1098e+00 3.7860e+01 -1.2222e+02] [ 7.2574e+00 5.2000e+01 8.2881e+00 4.9600e+02 1.7700e+02 2.8023e+00 3.7850e+01 -1.2232e+02] [ 5.6431e+00 5.2000e+01 5.8120e+00 5.5800e+02 2.1900e+02 2.8204e+00 3.7850e+01 -1.2233e+02] [ 3.8462e+00 5.2000e+01 5.2014e+00 5.6500e+02 2.5900e+02 2.7164e+00 3.7840e+01 -1.2238e+02]] Means of features: [3.87558309e+00 2.86470237e+01 2.84983217e+00 1.42632733e+03 4.28394731e+02 2.55515658e+00 3.57980666e+01 -1.19526383e+02] Standard deviations of features: [1.90585091e+00 1.21317528e+01 1.90977543e+00 1.11634809e+03 3.54176641e+02 7.40594557e-01 2.14129786e+00 2.00083151e+00] Mean squared error on the test set: 0.530293224170579 ```","solution":"import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def analyze_california_housing(): # Fetch the California housing dataset housing = fetch_california_housing() data, target = housing.data, housing.target # Data Inspection print(f\\"Dataset shape: {data.shape}\\") print(f\\"Target shape: {target.shape}\\") print(\\"First 5 rows of data:n\\", data[:5]) # Data Analysis means = np.mean(data, axis=0) stds = np.std(data, axis=0) print(\\"Means of features:\\", means) print(\\"Standard deviations of features:\\", stds) # Data Preparation X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Model Training model = LinearRegression() model.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean squared error on the test set: {mse}\\") return X_train_scaled, X_test_scaled, y_train, y_test, model"},{"question":"Objective Write a Python function that reads a `setup.cfg` file and extracts the configuration options for specified commands. The function should then allow the user to override these options via command-line arguments. Details 1. **Function Signature**: ```python def parse_setup_cfg(file_path: str, override_args: dict) -> dict: ``` 2. **Inputs**: - `file_path` (str): The path to the `setup.cfg` file. - `override_args` (dict): A dictionary representing command-line arguments to override options in `setup.cfg`. The keys are command names and the values are dictionaries of option-value pairs. 3. **Output**: - A dictionary where keys are command names and values are dictionaries of option-value pairs, with any command-line overrides applied. 4. **Constraints**: - A `setup.cfg` file must follow the syntax and structure as described in the provided documentation. - Command-line arguments in `override_args` should respect the option names as used within the `setup.cfg`. 5. **Example Usage**: ```python setup_cfg_content = [build_ext] inplace = 1 build_lib = build/ [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ with open(\'setup.cfg\', \'w\') as f: f.write(setup_cfg_content) override_args = { \\"build_ext\\": { \\"inplace\\": \\"0\\" }, \\"bdist_rpm\\": { \\"release\\": \\"2\\" } } result = parse_setup_cfg(\'setup.cfg\', override_args) print(result) ``` This should print: ```python { \'build_ext\': {\'inplace\': \'0\', \'build_lib\': \'build/\'}, \'bdist_rpm\': { \'release\': \'2\', \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\'} } ``` Additional Information You should handle: - Parsing the `setup.cfg` file accurately. - Applying any command-line overrides correctly. Notes - Do not use any external libraries other than Python\'s standard library. - Ensure robust error handling for improperly formatted `setup.cfg` files or invalid overrides.","solution":"import configparser def parse_setup_cfg(file_path: str, override_args: dict) -> dict: Parses the setup.cfg file and applies command-line overrides to it. Args: file_path (str): Path to the setup.cfg file. override_args (dict): Dictionary of overrides for the command options. Returns: dict: A dictionary with the parsed and overridden configuration options. config = configparser.ConfigParser() config.read(file_path) result = {} for section in config.sections(): result[section] = dict(config.items(section)) for command, overrides in override_args.items(): if command in result: result[command].update(overrides) else: result[command] = overrides return result"},{"question":"# Coding Challenge: Data Manipulation Using Operator Module **Objective**: Write a Python function that processes a list of dictionaries representing student records and performs the following operations using the `operator` module functions: 1. **Filter**: Filter out records where the student\'s grade is less than 70. 2. **Sort**: Sort the remaining records first by grade (descending) and then by name (ascending). 3. **Update**: Normalize the grades in the filtered records by adding 5 to each student\'s grade, ensuring the maximum grade does not exceed 100. 4. **Rank**: Assign a rank to each student based on the sorted order (1 for the highest grade, 2 for the next, etc.). # Function Signature ```python def process_student_records(students: list) -> list: # Implement the function body here pass ``` # Input - `students` (list): A list of dictionaries where each dictionary represents a student record with keys: - `name` (str): The student\'s name. - `grade` (int): The student\'s grade. # Output - A list of dictionaries, where each dictionary includes the student\'s `name`, `normalized_grade`, and `rank`. # Constraints - Each student\'s grade, before normalization, will be an integer between 0 and 100. - The input list will have at least one student record. - You should use the `operator` module functions for operations wherever applicable. # Example ```python students = [ {\'name\': \'Alice\', \'grade\': 65}, {\'name\': \'Bob\', \'grade\': 75}, {\'name\': \'Charlie\', \'grade\': 85}, {\'name\': \'David\', \'grade\': 95}, {\'name\': \'Eve\', \'grade\': 55} ] output = process_student_records(students) print(output) ``` Expected Output: ```python [ {\'name\': \'David\', \'normalized_grade\': 100, \'rank\': 1}, {\'name\': \'Charlie\', \'normalized_grade\': 90, \'rank\': 2}, {\'name\': \'Bob\', \'normalized_grade\': 80, \'rank\': 3} ] ``` # Notes: - **Filter** out students with grades less than 70. - **Sort** the remaining students by grade in descending order and then by name in ascending order. - **Normalize** each student\'s grade by adding 5, ensuring the maximum grade does not exceed 100. - Assign **rank** based on the sorted order with 1 for the highest grade. # Hints - Use `operator.itemgetter` for sorting. - Use `operator.add` for normalization. - Use `operator.setitem` for manipulating dictionary entries. Good luck and happy coding!","solution":"from operator import itemgetter def process_student_records(students: list) -> list: # Filter out records where the student\'s grade is less than 70 filtered_students = [student for student in students if student[\'grade\'] >= 70] # Sort the remaining records first by grade (descending) and then by name (ascending) sorted_students = sorted(filtered_students, key=itemgetter(\'name\')) sorted_students = sorted(sorted_students, key=itemgetter(\'grade\'), reverse=True) # Normalize the grades in the filtered records for student in sorted_students: student[\'normalized_grade\'] = min(student[\'grade\'] + 5, 100) # Assign a rank to each student based on the sorted order for rank, student in enumerate(sorted_students, start=1): student[\'rank\'] = rank # Prepare the final list with required fields processed_students = [{\'name\': student[\'name\'], \'normalized_grade\': student[\'normalized_grade\'], \'rank\': student[\'rank\']} for student in sorted_students] return processed_students"},{"question":"# Coding Assessment: Working Around TorchScript Limitations Objective You are required to implement a small neural network model in PyTorch and overcome the limitations imposed by TorchScript, focusing on the unsupported constructs listed in the provided documentation. Task 1. **Design and Implement a Neural Network:** - Create a simple feed-forward neural network using three fully connected layers with ReLU activations. - Use unsupported constructs like `torch.nn.RNN` or `torch.autograd.Function`. 2. **Provide an Alternative Implementation:** - Refactor your neural network to avoid using the unsupported constructs and ensure it can be compiled with TorchScript. - Use `torch.jit.trace` if necessary to work around the limitations. Requirements - **Input:** - The network should take an input tensor of shape `(batch_size, input_dim)`. - **Output:** - The output should be a tensor of shape `(batch_size, output_dim)`. Constraints - You are required to use PyTorch modules and avoid the unsupported constructs as per the provided documentation. - The model should still perform the same operations and yield the same results before and after refactoring. - Use `torch.jit.script` or `torch.jit.trace` to demonstrate the compilation of the refactored model with TorchScript. Performance Requirements - The refactored model should maintain its initial performance in terms of speed and accuracy. Example Here’s a brief structure of your solution: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.jit # Original model using unsupported constructs class UnsupportedModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): # Your implementation using unsupported constructs pass def forward(self, x): # Forward pass implementation pass # Alternative model avoiding unsupported constructs class AlternativeModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): # Your implementation avoiding unsupported constructs pass def forward(self, x): # Forward pass implementation pass def trace_and_test_model(model, example_input): # Use torch.jit.trace or torch.jit.script to compile the model pass # Testing the models with some input input_dim = 10 hidden_dim = 20 output_dim = 5 example_input = torch.randn(1, input_dim) unsupported_model = UnsupportedModel(input_dim, hidden_dim, output_dim) alternative_model = AlternativeModel(input_dim, hidden_dim, output_dim) # Call to trace_and_test_model function trace_and_test_model(unsupported_model, example_input) trace_and_test_model(alternative_model, example_input) ``` Provide thorough documentation and comments within your code to explain the steps taken and justify the choices made in your implementation.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit # Original model using unsupported constructs: torch.nn.RNN class UnsupportedModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(UnsupportedModel, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = F.relu(self.fc1(x)) x, _ = self.rnn(x.unsqueeze(0)) # Adding batch dimension for RNN x = x.squeeze(0) # Remove batch dimension x = self.fc2(x) return x # Alternative model avoiding unsupported constructs class AlternativeModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(AlternativeModel, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def trace_and_test_model(model, example_input): traced_model = torch.jit.trace(model, example_input) return traced_model # Testing the models with some input input_dim = 10 hidden_dim = 20 output_dim = 5 example_input = torch.randn(1, input_dim) unsupported_model = UnsupportedModel(input_dim, hidden_dim, output_dim) alternative_model = AlternativeModel(input_dim, hidden_dim, output_dim) # Tracing the models traced_unsupported_model = trace_and_test_model(unsupported_model, example_input) traced_alternative_model = trace_and_test_model(alternative_model, example_input)"},{"question":"# Objective Demonstrate your understanding of the `tkinter.font` module by creating and manipulating fonts within a Tkinter application. # Problem Description Create a Tkinter application with a user interface that allows the user to input custom text and apply various font styles to it. Your application should provide the following functionalities: 1. An entry widget for the user to input text. 2. Option menus or dropdowns to select: - Font family (use at least 3 families, e.g., Arial, Courier, Times). - Font size. - Font weight (Normal or Bold). - Font slant (Roman or Italic). - Underline (On or Off). - Overstrike (On or Off). 3. A button to apply the selected font style to the input text and display it in a label below the input widget. 4. A label that dynamically updates to show the selected font properties (family, size, weight, slant, underline, overstrike). # Details - You must use the `tkinter.font.Font` class to create and manipulate the font. - Ensure your code handles and applies the font properties correctly. - Implement necessary validation and error handling. # Example Workflow 1. The user types \\"Hello, World!\\" into the entry widget. 2. The user selects: - Font family: Arial - Font size: 16 - Font weight: Bold - Font slant: Italic - Underline: On - Overstrike: Off 3. The user presses the \\"Apply Font\\" button. 4. The label displays \\"Hello, World!\\" with the applied font properties and the properties are displayed dynamically. # Expected Input and Output - **Input**: User inputs via entry widget and dropdown menus. - **Output**: A dynamically styled text displayed in a label widget and updated font properties. # Constraints - The application should handle the input for font size correctly and validate that it\'s a positive integer. - The program should not crash on invalid input but instead, provide appropriate error messages or default values. # Performance Requirements - The UI should update smoothly and respond quickly to user inputs and changes. # Starter Code You can begin with the following skeleton code: ```python import tkinter as tk import tkinter.font as font class FontApp: def __init__(self, root): self.root = root self.root.title(\\"Font Styler\\") self.text_var = tk.StringVar() tk.Entry(root, textvariable=self.text_var).pack() self.family_var = tk.StringVar(value=\\"Arial\\") tk.OptionMenu(root, self.family_var, \\"Arial\\", \\"Courier\\", \\"Times\\").pack() self.size_var = tk.IntVar(value=12) tk.Spinbox(root, from_=8, to=72, textvariable=self.size_var).pack() self.weight_var = tk.StringVar(value=font.NORMAL) tk.OptionMenu(root, self.weight_var, font.NORMAL, font.BOLD).pack() self.slant_var = tk.StringVar(value=font.ROMAN) tk.OptionMenu(root, self.slant_var, font.ROMAN, font.ITALIC).pack() self.underline_var = tk.BooleanVar(value=False) tk.Checkbutton(root, text=\\"Underline\\", variable=self.underline_var).pack() self.overstrike_var = tk.BooleanVar(value=False) tk.Checkbutton(root, text=\\"Overstrike\\", variable=self.overstrike_var).pack() tk.Button(root, text=\\"Apply Font\\", command=self.apply_font).pack() self.label_font = font.Font(family=\\"Arial\\", size=12) self.label = tk.Label(root, text=\\"\\", font=self.label_font) self.label.pack() self.property_label = tk.Label(root, text=\\"\\") self.property_label.pack() def apply_font(self): current_font = font.Font(family=self.family_var.get(), size=self.size_var.get(), weight=self.weight_var.get(), slant=self.slant_var.get(), underline=self.underline_var.get(), overstrike=self.overstrike_var.get()) self.label.config(text=self.text_var.get(), font=current_font) self.property_label.config(text=f\\"Font Properties: {current_font.actual()}\\") if __name__ == \\"__main__\\": root = tk.Tk() app = FontApp(root) root.mainloop() ``` Your task is to complete the implementation of the `FontApp` class following the provided specifications.","solution":"import tkinter as tk import tkinter.font as font class FontApp: def __init__(self, root): self.root = root self.root.title(\\"Font Styler\\") self.text_var = tk.StringVar() self.text_entry = tk.Entry(root, textvariable=self.text_var) self.text_entry.pack(padx=10, pady=5) self.family_var = tk.StringVar(value=\\"Arial\\") self.family_menu = tk.OptionMenu(root, self.family_var, \\"Arial\\", \\"Courier\\", \\"Times\\") self.family_menu.pack(padx=10, pady=5) self.size_var = tk.IntVar(value=12) self.size_spinbox = tk.Spinbox(root, from_=8, to=72, textvariable=self.size_var) self.size_spinbox.pack(padx=10, pady=5) self.weight_var = tk.StringVar(value=font.NORMAL) self.weight_menu = tk.OptionMenu(root, self.weight_var, font.NORMAL, font.BOLD) self.weight_menu.pack(padx=10, pady=5) self.slant_var = tk.StringVar(value=font.ROMAN) self.slant_menu = tk.OptionMenu(root, self.slant_var, font.ROMAN, font.ITALIC) self.slant_menu.pack(padx=10, pady=5) self.underline_var = tk.BooleanVar(value=False) self.underline_check = tk.Checkbutton(root, text=\\"Underline\\", variable=self.underline_var) self.underline_check.pack(padx=10, pady=5) self.overstrike_var = tk.BooleanVar(value=False) self.overstrike_check = tk.Checkbutton(root, text=\\"Overstrike\\", variable=self.overstrike_var) self.overstrike_check.pack(padx=10, pady=5) self.apply_button = tk.Button(root, text=\\"Apply Font\\", command=self.apply_font) self.apply_button.pack(padx=10, pady=10) self.label_font = font.Font(family=\\"Arial\\", size=12) self.label = tk.Label(root, text=\\"\\", font=self.label_font) self.label.pack(padx=10, pady=10) self.property_label = tk.Label(root, text=\\"\\") self.property_label.pack(padx=10, pady=10) def apply_font(self): try: current_font = font.Font(family=self.family_var.get(), size=self.size_var.get(), weight=self.weight_var.get(), slant=self.slant_var.get(), underline=self.underline_var.get(), overstrike=self.overstrike_var.get()) self.label.config(text=self.text_var.get(), font=current_font) self.property_label.config(text=f\\"Font Properties: {current_font.actual()}\\") except Exception as e: self.property_label.config(text=f\\"Error: {str(e)}\\") if __name__ == \\"__main__\\": root = tk.Tk() app = FontApp(root) root.mainloop()"},{"question":"**Question: Advanced Seaborn Strip Plot Analysis** You are tasked with analyzing a dataset using Seaborn\'s `stripplot` and `catplot` functions. The objective is to visualize the distribution of total bill amounts in relation to various factors and customize the appearance of the plots. # Dataset You will use the `tips` dataset, which contains information about the total bill, tip, sex, smoker status, day, time, and size of parties for restaurant bills. # Instructions 1. Write a function `create_stripplot` that takes no arguments and performs the following operations: - Loads the `tips` dataset. - Creates a strip plot showing the total bill amounts for each day of the week. Use different colors to distinguish between lunch and dinner times (`time` variable). - Customize the plot with the following specifications: - Disable jittering. - Use a triangular marker (`^`). - Set a marker size of 8. - Use 50% transparency for the markers. - Save the plot as a PNG file named `stripplot_custom.png`. 2. Write a function `create_catplot` that takes no arguments and performs the following operations: - Loads the `tips` dataset. - Creates a categorical plot (`catplot`) with the following facets: - The x-axis should show `time` (lunch or dinner). - The y-axis should show `total_bill`. - Facet the plot across the days of the week (`day`), displaying separate plots for each day. - Use different hues to distinguish between genders (`sex`). - Set the aspect ratio of each facet to 0.7. - Save the plot as a PNG file named `catplot_facet.png`. # Constraints - Use the `seaborn` library for all visualizations. - Ensure that the plots are clearly labeled and visually distinct. # Function Signatures ```python def create_stripplot(): pass def create_catplot(): pass ``` # Example Output After executing the functions, you will have two PNG files saved in your directory: - `stripplot_custom.png` - `catplot_facet.png` These files will be used to visually analyze the distribution of total bills in relation to days, times, and sexes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_stripplot(): This function loads the tips dataset and creates a customized strip plot. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a strip plot plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", data=tips, jitter=False, marker=\'^\', size=8, alpha=0.5) # Customize the plot plt.title(\'Total Bill Amounts by Day and Time\') plt.ylabel(\'Total Bill\') plt.xlabel(\'Day\') plt.legend(title=\'Time\') # Save the plot plt.savefig(\'stripplot_custom.png\') plt.close() def create_catplot(): This function loads the tips dataset and creates a categorical plot faceted by day. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a categorical plot catplot = sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", aspect=0.7) # Customize the plot catplot.set_axis_labels(\\"Time\\", \\"Total Bill\\") catplot.add_legend(title=\\"Sex\\") # Save the plot catplot.savefig(\'catplot_facet.png\') plt.close()"},{"question":"Boolean Object Manipulation in Python **Objective:** Create a Python module that implements functions to manipulate boolean objects based on the user\'s inputs. Your solution must demonstrate a comprehensive understanding of boolean object manipulation. **Question:** You are required to implement a function `bool_operations` that processes a list of integers and returns a dictionary containing the following information: 1. The number of `True` values. 2. The number of `False` values. 3. A list of boolean values corresponding to each integer in the input list (where 0 is `False` and any other integer is `True`). **Function Signature:** ```python def bool_operations(int_list: list[int]) -> dict: ``` **Input:** - `int_list` (list of int): A list of integers. The length of the list is at most (10^6). **Output:** - A dictionary with the following structure: ```python { \\"true_count\\": int, # The number of True values \\"false_count\\": int, # The number of False values \\"boolean_list\\": list # A list of boolean values corresponding to each integer } ``` **Constraints:** - You must use the `PyBool_FromLong` function to convert integers to boolean objects. - Expected time complexity is (O(n)), where (n) is the length of the list. **Performance Requirements:** 1. Your solution must be efficient to handle up to (10^6) integers. 2. The implementation should also manage reference counts properly for the boolean objects created. # Example: ```python input_list = [0, 1, -1, 0, 2, 0, 3] result = bool_operations(input_list) print(result) # Expected Output: # { # \\"true_count\\": 4, # \\"false_count\\": 3, # \\"boolean_list\\": [False, True, True, False, True, False, True] # } ``` Hints: - Consider how booleans are derived from integers. - Ensure the correctness of reference counts when creating boolean objects.","solution":"def bool_operations(int_list): Processes a list of integers and returns a dictionary with counts of True and False values and a list of boolean equivalents of the given integers. true_count = 0 false_count = 0 boolean_list = [] for num in int_list: bool_value = bool(num) boolean_list.append(bool_value) if bool_value: true_count += 1 else: false_count += 1 return { \\"true_count\\": true_count, \\"false_count\\": false_count, \\"boolean_list\\": boolean_list }"},{"question":"You are tasked with creating a Python package and setting it up for distribution using the `distutils` package. Your package will consist of a few Python scripts and some additional data files. The goal is to distribute your package in both `.tar.gz` and `.zip` formats, ensuring all necessary files are included. Part 1: Create the Package Structure 1. Create a directory named `mypackage`. 2. Inside `mypackage`, create a subdirectory named `data`. 3. Add the following files: - `mypackage/__init__.py`: An empty file. - `mypackage/script.py`: A file with the following content: ```python def greet(name): return f\\"Hello, {name}!\\" ``` - `README.txt`: A file with some basic information about the package. - `setup.py`: The setup script for `distutils` with the following content: ```python from distutils.core import setup setup( name=\'mypackage\', version=\'0.1\', packages=[\'mypackage\'], scripts=[\'mypackage/script.py\'], ) ``` Part 2: Create the Manifest Template 1. Create a `MANIFEST.in` file in the root of your package directory with the following content: ``` include README.txt recursive-include mypackage/data * ``` Part 3: Build the Source Distribution 1. Use the `python setup.py sdist --formats=gztar,zip` command to create the source distributions in `.tar.gz` and `.zip` formats. 2. Verify that the source distributions contain all the necessary files. Constraints - Ensure that the `README.txt` and all files in the `mypackage/data` directory are included in the distribution. - Include only the `.py` files inside the `mypackage` directory and not any other file types. Instructions 1. Submit the directory structure with the `mypackage` and all its contents. 2. Submit the generated source distributions (`.tar.gz` and `.zip` files). Evaluation Your submission will be evaluated on: - Correctness of the directory and file structure. - Proper configuration and content of the `setup.py` and `MANIFEST.in` files. - Successful creation and content verification of the source distributions.","solution":"# Assuming the current working structure is the root directory for the package, # we will create a basic setup to reflect a file system. # Create directory structure import os def create_package_structure(): os.makedirs(\\"mypackage/data\\", exist_ok=True) # Create __init__.py with open(\\"mypackage/__init__.py\\", \\"w\\") as f: pass # Create script.py with open(\\"mypackage/script.py\\", \\"w\\") as f: f.write(def greet(name): return f\\"Hello, {name}!\\" ) # Create README.txt with open(\\"README.txt\\", \\"w\\") as f: f.write(\\"This is a simple package named mypackage.\\") # Create setup.py with open(\\"setup.py\\", \\"w\\") as f: f.write(from distutils.core import setup setup( name=\'mypackage\', version=\'0.1\', packages=[\'mypackage\'], scripts=[\'mypackage/script.py\'], ) ) # Create MANIFEST.in with open(\\"MANIFEST.in\\", \\"w\\") as f: f.write(include README.txt recursive-include mypackage/data * ) create_package_structure() # Note: The actual creation of the distribution files cannot be done in this code snippet # as it would require running shell commands in a specific file system environment. # However, you can run the following command after running the above script: # python setup.py sdist --formats=gztar,zip"},{"question":"You are provided with a dataset of restaurant bills and tips. Your task is to generate insightful visualizations using the Seaborn library. These visualizations should help understand the distribution and relationships within the dataset. Ensure you include both univariate and multivariate analyses. # Function: `visualize_restaurant_data` Input - `dataframe`: A pandas DataFrame with the following columns: - `total_bill`: Total bill in the restaurant. - `tip`: Tip amount given. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not. - `day`: Day of the week. - `time`: Time of day (`Lunch` or `Dinner`). - `size`: Number of people in the party. Output - This function does not return any value. It should generate and display three plots: - A swarm plot showing the distribution of `total_bill` across different `days`. - A swarm plot comparing `total_bill` with `day`, colored by `sex`. - A facet grid plot (using `catplot`) showing the distribution of `total_bill` by `time`, `hue` as `sex`, and facets for each `day`. Constraints - Ensure the plots are visually distinct and clear. - Properly label all axes and include legends where applicable. - Utilize the `sns.swarmplot` and `sns.catplot` functions as necessary. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_restaurant_data(dataframe): sns.set_theme(style=\\"whitegrid\\") # Swarm plot showing the distribution of total_bill across different days plt.figure(figsize=(10, 6)) sns.swarmplot(data=dataframe, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Distribution of Total Bill Across Different Days\\") plt.show() # Swarm plot comparing total_bill with day, colored by sex plt.figure(figsize=(10, 6)) sns.swarmplot(data=dataframe, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\") plt.title(\\"Total Bill vs Day Colored by Sex\\") plt.show() # Facet grid plot showing the distribution of total_bill by time, hue as sex, and facets for each day g = sns.catplot( data=dataframe, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5 ) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Total Bill Distribution by Time and Sex across Days\\") plt.show() # Example Usage: # df = sns.load_dataset(\\"tips\\") # visualize_restaurant_data(df) ``` # Notes - Make sure to import the necessary libraries (`pandas`, `seaborn`, and `matplotlib`). - Use the provided example code to test your function with the `tips` dataset from Seaborn.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_restaurant_data(dataframe): sns.set_theme(style=\\"whitegrid\\") # Swarm plot showing the distribution of total_bill across different days plt.figure(figsize=(10, 6)) sns.swarmplot(data=dataframe, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Distribution of Total Bill Across Different Days\\") plt.show() # Swarm plot comparing total_bill with day, colored by sex plt.figure(figsize=(10, 6)) sns.swarmplot(data=dataframe, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\") plt.title(\\"Total Bill vs Day Colored by Sex\\") plt.show() # Facet grid plot showing the distribution of total_bill by time, hue as sex, and facets for each day g = sns.catplot( data=dataframe, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5 ) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Total Bill Distribution by Time and Sex across Days\\") plt.show()"},{"question":"**Objective:** Design a function that calculates the Euclidean distance between two points in an N-dimensional space and then find the angle (in degrees) between the line connecting these points and the positive X-axis. **Problem Statement:** Write a function `calculate_distance_and_angle(p, q)` that takes two points `p` and `q` as tuples or lists of coordinates. The function should perform the following tasks: 1. Calculate the Euclidean distance between points `p` and `q`. 2. Calculate the angle (in degrees) between the line connecting these points and the positive X-axis using the first coordinate for the angle calculation. **Function Signature:** ```python def calculate_distance_and_angle(p: Union[Tuple[float], List[float]], q: Union[Tuple[float], List[float]]) -> Tuple[float, float]: ``` **Input:** - `p`: A tuple or list of floats representing the coordinates of the first point in N-dimensional space. - `q`: A tuple or list of floats representing the coordinates of the second point in N-dimensional space. **Output:** - A tuple containing: - The Euclidean distance (float) between the two points. - The angle (in degrees) between the line connecting the points and the positive X-axis. **Constraints:** - The input lists/tuples `p` and `q` will have the same length. - The length of `p` and `q` will be at least 2 (representing at least two-dimensional space). **Examples:** ```python # Example 1 p = (1, 2, 3) q = (4, 5, 6) # Distance: sqrt((4-1)^2 + (5-2)^2 + (6-3)^2) = sqrt(27) # Angle: atan((5-2)/(4-1)) [Or convert to degrees using degrees() function] # (5-2)/(4-1)=3/3=1 so angle in degrees is 45 # Output: (5.196152422706632, 45.0) calculate_distance_and_angle(p, q) # Output: (5.196152422706632, 45.0) # Example 2 p = (0, 0) q = (4, 3) # Distance: sqrt(4^2 + 3^2) = 5 # Angle: atan(3/4) [Or convert to degrees using degrees() function] # Output: (5.0, 36.86989764584402) calculate_distance_and_angle(p, q) # Output: (5.0, 36.86989764584402) ``` **Notes:** - You may use the `math.dist` function to calculate the Euclidean distance. - Use `math.degrees` and `math.atan2` to calculate the angle in degrees. - Ensure your solution handles both 2D and higher-dimensional points correctly. **Performance Requirement:** - The function should be efficient and handle typical input sizes within a time complexity of O(N) for N-dimensional points.","solution":"import math from typing import Union, Tuple, List def calculate_distance_and_angle(p: Union[Tuple[float], List[float]], q: Union[Tuple[float], List[float]]) -> Tuple[float, float]: Returns the Euclidean distance and angle with respect to the X-axis between two points in N-dimensional space. # Calculate the Euclidean distance dist = math.dist(p, q) # Calculate the difference in the first two coordinates diff_x = q[0] - p[0] diff_y = q[1] - p[1] # Calculate the angle in radians using atan2 and then convert to degrees angle_rad = math.atan2(diff_y, diff_x) angle_deg = math.degrees(angle_rad) return dist, angle_deg"},{"question":"# Asynchronous Task Management in Python Problem Statement You are tasked with writing an asynchronous Python program that simulates a simplified stock trading system. The system will: 1. Fetch stock prices for different tickers concurrently. 2. Process these stock prices asynchronously. 3. Cancel stock fetching tasks gracefully if they take too long. Requirements 1. **Fetching Stock Prices:** - Implement an asynchronous function `fetch_stock_price(ticker: str, delay: int) -> float` that simulates fetching a stock price for a given ticker symbol. - The function will sleep for the given delay in seconds to simulate a network delay and then return a random stock price between 100 and 500. 2. **Processing Stock Prices:** - Implement an asynchronous function `process_price(ticker: str, price: float) -> None` that prints the stock ticker and its fetched price. 3. **Main Function:** - Create a function `main(tickers: List[str], timeout: float) -> None` to: - Concurrently fetch stock prices for a list of provided tickers. - Use `asyncio.gather` to run the fetching tasks concurrently. - Use `asyncio.wait_for` to apply a timeout to each fetching task. - If fetching a stock price is successful within the timeout, process the price. - If fetching a stock price times out, handle the timeout gracefully. Input - A list of stock ticker symbols (e.g., `[\\"AAPL\\", \\"GOOGL\\", \\"MSFT\\"]`). - A timeout value in seconds for the stock fetching operation (e.g., `2.0`). Output - The function should not return anything but should print the ticker and its fetched price for successful fetches. Constraints - Use the `asyncio` module. - Ensure proper timeout handling. - Gracefully manage any cancellations. Example ```python import asyncio import random from typing import List async def fetch_stock_price(ticker: str, delay: int) -> float: # Simulating network delay and fetching stock price await asyncio.sleep(delay) return random.uniform(100, 500) async def process_price(ticker: str, price: float) -> None: # Processing fetched stock price print(f\\"Ticker: {ticker}, Price: {price:.2f}\\") async def main(tickers: List[str], timeout: float) -> None: tasks = [] for ticker in tickers: delay = random.randint(1, 5) # Random delay between 1-5 seconds fetch_task = fetch_stock_price(ticker, delay) tasks.append(asyncio.wait_for(fetch_task, timeout)) results = await asyncio.gather(*tasks, return_exceptions=True) for ticker, result in zip(tickers, results): if isinstance(result, asyncio.TimeoutError): print(f\\"Fetching price for {ticker} timed out.\\") else: await process_price(ticker, result) if __name__ == \\"__main__\\": tickers = [\\"AAPL\\", \\"GOOGL\\", \\"MSFT\\"] timeout = 2.0 # 2 seconds timeout asyncio.run(main(tickers, timeout)) ``` **Notes:** - The provided example is partial. The candidate should complete missing parts (like imports, testing function, etc.) and ensure proper error handling.","solution":"import asyncio import random from typing import List async def fetch_stock_price(ticker: str, delay: int) -> float: Simulates fetching a stock price for a given ticker symbol by sleeping for the given delay and then returning a random stock price between 100 and 500. await asyncio.sleep(delay) return random.uniform(100, 500) async def process_price(ticker: str, price: float) -> None: Processes the fetched stock price by printing the ticker and its price. print(f\\"Ticker: {ticker}, Price: {price:.2f}\\") async def main(tickers: List[str], timeout: float) -> None: Concurrently fetches stock prices for given tickers with a specified timeout for each fetch task. If a fetch task completes within the timeout, processes the price; otherwise handles the timeout gracefully. tasks = [] for ticker in tickers: delay = random.randint(1, 5) # Random delay between 1-5 seconds fetch_task = fetch_stock_price(ticker, delay) tasks.append(asyncio.wait_for(fetch_task, timeout)) results = await asyncio.gather(*tasks, return_exceptions=True) for ticker, result in zip(tickers, results): if isinstance(result, asyncio.TimeoutError): print(f\\"Fetching price for {ticker} timed out.\\") else: await process_price(ticker, result) if __name__ == \\"__main__\\": tickers = [\\"AAPL\\", \\"GOOGL\\", \\"MSFT\\"] timeout = 2.0 # 2 seconds timeout asyncio.run(main(tickers, timeout))"},{"question":"# Gaussian Process Regression with Custom Kernels Objective: Implement a Gaussian Process Regressor using scikit-learn. Your task is to: 1. Train the model with multiple kernels. 2. Optimize the hyperparameters of the kernels. 3. Evaluate the performance on a test dataset. Problem Statement: You are provided with a dataset containing training and testing data. Implement a Gaussian Process Regression model using the following kernels: Radial Basis Function (RBF) and Matérn kernels. You need to: 1. Train the model using the provided kernels. 2. Optimize the hyperparameters of these kernels. 3. Evaluate the performance of the models using Mean Squared Error (MSE) on the test dataset. 4. Visualize the predicted values against the actual values. Input: - `X_train`: Training feature dataset (2D NumPy array). - `y_train`: Training target values (1D NumPy array). - `X_test`: Testing feature dataset (2D NumPy array). - `y_test`: Testing target values (1D NumPy array). Output: - Print the optimized hyperparameters for each kernel. - Print the Mean Squared Error (MSE) for the test data. - Plot the actual vs. predicted values for both kernels. Constraints: - Use a random seed of 42 for reproducibility. - Use only the scikit-learn library for this task. - The dataset size is not exceedingly large, so computational efficiency is not a primary concern. Performance Requirements: - Ensure that the models fit and predict in a reasonable amount of time. - Provide clear and concise plots for visualization. Example Dataset: ```python import numpy as np # Generating a sample dataset np.random.seed(42) X_train = np.random.rand(100, 1) * 10 # Random samples in [0, 10) y_train = np.sin(X_train).ravel() + np.random.normal(0, 0.1, X_train.shape[0]) X_test = np.random.rand(50, 1) * 10 # Random samples in [0, 10) y_test = np.sin(X_test).ravel() + np.random.normal(0, 0.1, X_test.shape[0]) ``` Coding Task: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern from sklearn.metrics import mean_squared_error # Function to train and evaluate GPR with given kernel def train_evaluate_gpr(X_train, y_train, X_test, y_test, kernel): # Initialize the GaussianProcessRegressor with the given kernel gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10) # Train the model gpr.fit(X_train, y_train) # Predict on the test set y_pred, std = gpr.predict(X_test, return_std=True) # Calculate the Mean Squared Error mse = mean_squared_error(y_test, y_pred) # Get the optimized hyperparameters optimal_params = gpr.kernel_ return y_pred, mse, optimal_params # Define kernels rbf_kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)) matern_kernel = Matern(length_scale=1.0, nu=1.5, length_scale_bounds=(1e-1, 10.0)) # Train and evaluate with RBF kernel y_pred_rbf, mse_rbf, opt_params_rbf = train_evaluate_gpr(X_train, y_train, X_test, y_test, rbf_kernel) print(\\"RBF Kernel Optimized Parameters:\\", opt_params_rbf) print(\\"RBF Kernel Mean Squared Error:\\", mse_rbf) # Train and evaluate with Matérn kernel y_pred_matern, mse_matern, opt_params_matern = train_evaluate_gpr(X_train, y_train, X_test, y_test, matern_kernel) print(\\"Matérn Kernel Optimized Parameters:\\", opt_params_matern) print(\\"Matérn Kernel Mean Squared Error:\\", mse_matern) # Plotting the results plt.figure(figsize=(14, 6)) # Plot for RBF kernel plt.subplot(1, 2, 1) plt.scatter(X_test, y_test, c=\'k\', label=\'Test data\') plt.plot(X_test, y_pred_rbf, label=\'Predicted values (RBF)\') plt.fill_between(X_test.ravel(), y_pred_rbf - 1.96 * std, y_pred_rbf + 1.96 * std, color=\'gray\', alpha=0.2) plt.title(f\'GPR with RBF KernelnOptimized MSE: {mse_rbf:.3f}\') plt.legend() # Plot for Matérn kernel plt.subplot(1, 2, 2) plt.scatter(X_test, y_test, c=\'k\', label=\'Test data\') plt.plot(X_test, y_pred_matern, label=\'Predicted values (Matérn)\') plt.fill_between(X_test.ravel(), y_pred_matern - 1.96 * std, y_pred_matern + 1.96 * std, color=\'gray\', alpha=0.2) plt.title(f\'GPR with Matérn KernelnOptimized MSE: {mse_matern:.3f}\') plt.legend() plt.show() ``` This question tests the student\'s ability to: 1. Implement and configure Gaussian Process Regression models. 2. Utilize different kernels and optimize their hyperparameters. 3. Evaluate model performance using MSE. 4. Visualize predicted vs. actual values for model evaluation.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern from sklearn.metrics import mean_squared_error # Function to train and evaluate GPR with given kernel def train_evaluate_gpr(X_train, y_train, X_test, y_test, kernel): Train and evaluate Gaussian Process Regressor with the provided kernel. Parameters: - X_train: Training feature dataset - y_train: Training target values - X_test: Testing feature dataset - y_test: Testing target values - kernel: Kernel to be used in the Gaussian Process Regressor Returns: - y_pred: Predicted values for the test set - mse: Mean Squared Error on the test set - optimal_params: Optimized hyperparameters of the kernel # Initialize the GaussianProcessRegressor with the given kernel gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10) # Train the model gpr.fit(X_train, y_train) # Predict on the test set y_pred, std = gpr.predict(X_test, return_std=True) # Calculate the Mean Squared Error mse = mean_squared_error(y_test, y_pred) # Get the optimized hyperparameters optimal_params = gpr.kernel_ return y_pred, mse, optimal_params, std # Define kernels rbf_kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)) matern_kernel = Matern(length_scale=1.0, nu=1.5, length_scale_bounds=(1e-1, 10.0)) # Function to execute the workflow def execute_workflow(X_train, y_train, X_test, y_test): # Train and evaluate with RBF kernel y_pred_rbf, mse_rbf, opt_params_rbf, std_rbf = train_evaluate_gpr(X_train, y_train, X_test, y_test, rbf_kernel) print(\\"RBF Kernel Optimized Parameters:\\", opt_params_rbf) print(\\"RBF Kernel Mean Squared Error:\\", mse_rbf) # Train and evaluate with Matérn kernel y_pred_matern, mse_matern, opt_params_matern, std_matern = train_evaluate_gpr(X_train, y_train, X_test, y_test, matern_kernel) print(\\"Matérn Kernel Optimized Parameters:\\", opt_params_matern) print(\\"Matérn Kernel Mean Squared Error:\\", mse_matern) # Plotting the results plt.figure(figsize=(14, 6)) # Plot for RBF kernel plt.subplot(1, 2, 1) plt.scatter(X_test, y_test, c=\'k\', label=\'Test data\') plt.plot(X_test, y_pred_rbf, label=\'Predicted values (RBF)\') plt.fill_between(X_test.ravel(), y_pred_rbf - 1.96 * std_rbf, y_pred_rbf + 1.96 * std_rbf, color=\'gray\', alpha=0.2) plt.title(f\'GPR with RBF KernelnOptimized MSE: {mse_rbf:.3f}\') plt.legend() # Plot for Matérn kernel plt.subplot(1, 2, 2) plt.scatter(X_test, y_test, c=\'k\', label=\'Test data\') plt.plot(X_test, y_pred_matern, label=\'Predicted values (Matérn)\') plt.fill_between(X_test.ravel(), y_pred_matern - 1.96 * std_matern, y_pred_matern + 1.96 * std_matern, color=\'gray\', alpha=0.2) plt.title(f\'GPR with Matérn KernelnOptimized MSE: {mse_matern:.3f}\') plt.legend() plt.show()"},{"question":"# Question: Working with Sparse Data Structures You are given a dataset that contains a lot of missing values. Your task is to transform this dataset into a pandas DataFrame with sparse data structures, perform some basic operations, and then convert it back to its dense form. Part 1: Constructing the Sparse DataFrame 1. Create a DataFrame `df` with the following randomly generated data (use the provided seed for reproducibility): - Shape: (10000, 4) - Values: Normally distributed random numbers - A large fraction of the entries (e.g., 95%) should be `NaN`. 2. Convert this DataFrame into a sparse DataFrame `sdf` using `pandas.SparseDtype`, with `NaN` as the fill value. Part 2: Performing Operations on Sparse Data 3. Compute the density of the sparse DataFrame (`sdf`). The density is defined as the ratio of non-default values to the total number of values. 4. Add a new column to the sparse DataFrame that is also sparse. This column should contain the square of the values from the first column. Part 3: Conversion between Representations 5. Convert the sparse DataFrame back to its dense representation and compare the memory usage (in bytes) of both the sparse and dense DataFrames. Input and Output - **Input**: There is no user input; the data generation and operations are hardcoded. - **Output**: Print the following: - Density of the sparse DataFrame. - Memory usage of the original DataFrame. - Memory usage of the sparse DataFrame. Constraints - Use a seed of `42` for random number generation. - Implement the function in Python using the pandas library. Example Implementation Here is a skeleton of the code to get you started. ```python import numpy as np import pandas as pd def sparse_data_operations(): np.random.seed(42) # Part 1: Create the DataFrame data = np.random.randn(10000, 4) data[np.random.rand(10000, 4) < 0.95] = np.nan df = pd.DataFrame(data) # Convert to sparse DataFrame sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Part 2: Compute density and add a new sparse column density = sdf.sparse.density sdf[\'squared\'] = sdf[0] ** 2 # Part 3: Conversion and memory usage dense_memory_usage = df.memory_usage().sum() sparse_memory_usage = sdf.memory_usage().sum() # Output results print(f\\"Density of the sparse DataFrame: {density}\\") print(f\\"Memory usage of the original DataFrame: {dense_memory_usage} bytes\\") print(f\\"Memory usage of the sparse DataFrame: {sparse_memory_usage} bytes\\") sparse_data_operations() ``` Provide the implementation within the function `sparse_data_operations()` and make sure it runs correctly.","solution":"import numpy as np import pandas as pd def sparse_data_operations(): np.random.seed(42) # Part 1: Create the DataFrame data = np.random.randn(10000, 4) data[np.random.rand(10000, 4) < 0.95] = np.nan # 95% NaNs df = pd.DataFrame(data) # Convert to sparse DataFrame sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Part 2: Compute density and add a new sparse column density = sdf.sparse.density sdf[\'squared\'] = sdf.iloc[:, 0] ** 2 # First column squared, also sparse # Part 3: Conversion and memory usage dense_memory_usage = df.memory_usage().sum() sparse_memory_usage = sdf.memory_usage().sum() # Output results print(f\\"Density of the sparse DataFrame: {density}\\") print(f\\"Memory usage of the original DataFrame: {dense_memory_usage} bytes\\") print(f\\"Memory usage of the sparse DataFrame: {sparse_memory_usage} bytes\\") return density, dense_memory_usage, sparse_memory_usage"},{"question":"# Question: Implementing and Handling Exceptions with Cleanup You are required to write a function `safe_division` that takes two arguments, `numerator` and `denominator`, and returns the result of dividing `numerator` by `denominator`. Your function should: 1. Raise a `ValueError` with the message \\"Both numerator and denominator must be numbers.\\" if either of the arguments is not a number. 2. Handle the `ZeroDivisionError` by returning the string \\"Division by zero is not allowed.\\" 3. Use a `finally` clause to print \\"Cleanup action initiated.\\" regardless of whether an error occurred or not. 4. Optionally, use an `else` clause to print \\"Division successful.\\" if no exception occurs. **Input:** - `numerator` (int/float): The numerator to be divided. - `denominator` (int/float): The denominator used for division. **Output:** - (float or str): The result of the division, or an error message as described above. **Constraints:** - The function should handle both integers and floating-point numbers as inputs. - No additional libraries should be used. **Example:** ```python # Example 1 result = safe_division(10, 2) # Expected Output: # Division successful. # Cleanup action initiated. # 5.0 # Example 2 result = safe_division(10, 0) # Expected Output: # Cleanup action initiated. # Division by zero is not allowed. # Example 3 result = safe_division(10, \'a\') # Expected Output: # Cleanup action initiated. # Both numerator and denominator must be numbers. ``` Implement the `safe_division` function below: ```python def safe_division(numerator, denominator): try: if not (isinstance(numerator, (int, float)) and isinstance(denominator, (int, float))): raise ValueError(\\"Both numerator and denominator must be numbers.\\") result = numerator / denominator except ZeroDivisionError: return \\"Division by zero is not allowed.\\" except ValueError as ve: return str(ve) else: print(\\"Division successful.\\") return result finally: print(\\"Cleanup action initiated.\\") ``` Provide the implementation of the function and test it with the provided examples.","solution":"def safe_division(numerator, denominator): try: if not (isinstance(numerator, (int, float)) and isinstance(denominator, (int, float))): raise ValueError(\\"Both numerator and denominator must be numbers.\\") result = numerator / denominator except ZeroDivisionError: return \\"Division by zero is not allowed.\\" except ValueError as ve: return str(ve) else: print(\\"Division successful.\\") return result finally: print(\\"Cleanup action initiated.\\")"},{"question":"**Question:** # Secure File Transfer Using Base64 Encoding Base64 encoding is often used to encode binary data, especially when it is required to be transferred over media that are designed to deal with textual data. However, it’s important to employ this with a good understanding of the security considerations highlighted in **RFC 4648**. You are required to write a Python function `secure_file_transfer` that reads a file, encodes its contents using Base64 encoding, and writes the encoded content to a new file. Your function should take care of the following: 1. Ensure proper encoding of the file contents. 2. Avoid common pitfalls mentioned in RFC 4648 concerning Base64 encoding. 3. Handle any possible exceptions and errors gracefully. Function Signature ```python def secure_file_transfer(input_file: str, output_file: str) -> None: ... ``` Input - `input_file` (str): A string representing the path to the input file to be read and encoded. - `output_file` (str): A string representing the path to the output file where the encoded content should be written. Output - The function does not return any value. It writes the Base64 encoded content from `input_file` to `output_file`. Constraints - The input file must exist and have readable contents. - Consider the size of the input file; your implementation should handle files of varied sizes efficiently. - You must handle and log errors related to file operations and encoding processes. - You should provide appropriate comments explaining each significant step to demonstrate your understanding of the process and the security considerations. # Example Usage ```python # Example usage of secure_file_transfer function # Suppose \'example.txt\' contains the following text: # Hello, World! # After running the function, secure_file_transfer(\'example.txt\', \'encoded.txt\') # The \'encoded.txt\' file should contain: # SGVsbG8sIFdvcmxkIQ== ``` # Additional Notes - Consider using the Python `base64` module for encoding. - Make sure to handle files using context managers to ensure proper closing of file handles. - Think about any potential edge cases, such as very large files or files with unusual characters. - Review RFC 4648 for specific security considerations related to Base64 encoding to ensure your implementation is robust. **Good luck!**","solution":"import base64 import logging def secure_file_transfer(input_file: str, output_file: str) -> None: Reads the contents of the input file, encodes them using Base64, and writes the encoded content to the output file. :param input_file: Path to the input file. :param output_file: Path to the output file. try: # Open the input file in binary read mode with open(input_file, \'rb\') as f_in: # Read the entire content of the file file_content = f_in.read() # Encode the file content using base64 encoded_content = base64.b64encode(file_content).decode(\'utf-8\') # Open the output file in write mode and write the encoded content with open(output_file, \'w\') as f_out: f_out.write(encoded_content) except FileNotFoundError: logging.error(f\\"File not found: {input_file}\\") except IOError as e: logging.error(f\\"I/O error occurred: {e}\\") except Exception as e: logging.error(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Question: Implementing a Custom SMTP Server** You are tasked with developing a custom SMTP server using Python\'s `smtpd` module. The server should be capable of receiving emails, processing them, and storing them into a simple in-memory data structure for later retrieval. # Requirements: 1. **Subclass the `smtpd.SMTPServer` class** to create a custom SMTP server. 2. **Override the `process_message` method** to handle incoming emails: - Store each email\'s `peer`, `mailfrom`, `rcpttos`, and `data` fields. - Each email should be stored as a dictionary in a class-level list called `email_store`. 3. Ensure that your server can handle multiple emails and retain all of them in the `email_store` list. 4. Your server should print details of each received message to the standard output. # Input: - The server should run locally and listen for incoming SMTP connections. # Output: - Print the details of each received email to the standard output. - Maintain a list of all received emails, where each email is represented as a dictionary. # Example: ```python class CustomSMTPServer(smtpd.SMTPServer): email_store = [] def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): email = { \'peer\': peer, \'mailfrom\': mailfrom, \'rcpttos\': rcpttos, \'data\': data } CustomSMTPServer.email_store.append(email) print(\'Received email from:\', mailfrom) print(\'Recipients:\', rcpttos) print(\'Message data:\') print(data) return \'250 OK\' # SMTP response code for successful receipt # Example of starting the server if __name__ == \\"__main__\\": import asyncore localaddr = (\'localhost\', 1025) remoteaddr = None server = CustomSMTPServer(localaddr, remoteaddr) asyncore.loop() ``` # Notes: - You may use any RFC-compliant email sending tool to test your server. - Ensure error handling is minimal for clarity and focus on the overriding of `process_message`. **Constraints**: 1. Do not use any external libraries other than what is included in the Python standard library. 2. The server should be able to handle emails up to 10MB in size. 3. The `decode_data` attribute should remain `False` during server initialization. By completing this task, you will demonstrate your understanding of subclassing, handling method overrides, and working with network connections in Python.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): email_store = [] def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): email = { \'peer\': peer, \'mailfrom\': mailfrom, \'rcpttos\': rcpttos, \'data\': data } CustomSMTPServer.email_store.append(email) print(\'Received email from:\', mailfrom) print(\'Recipients:\', rcpttos) print(\'Message data:\') print(data) return \'250 OK\' # SMTP response code for successful receipt # Example of starting the server if __name__ == \\"__main__\\": localaddr = (\'localhost\', 1025) remoteaddr = None server = CustomSMTPServer(localaddr, remoteaddr) asyncore.loop()"},{"question":"Objective: Implement a system that simulates a multi-process computation task, where multiple worker processes share and update a common dataset using shared memory. This task will demonstrate your understanding of the `multiprocessing.shared_memory` package and inter-process communication. Problem Statement: You are tasked with creating a Python program that initializes a list of integers, splits the list into parts, and assigns each part to a separate worker process. Each worker process will increment each element of its part by a given value and store the results back in the shared memory. Finally, the main process will collect the results and print the updated list. Requirements: 1. Create a shared memory block to hold a list of integers. 2. Use the `SharedMemory` and `ShareableList` classes to manage this shared memory block. 3. Create a function that will be executed by multiple worker processes to update their respective parts of the list. 4. Ensure proper synchronization of processes to avoid race conditions. 5. Implement the cleanup of shared memory resources properly at the end of the program. Input: - An integer list `data` of size `n`. - An integer `num_processes` denoting the number of worker processes. - An integer `increment_value` to be added to each element by the worker processes. Output: - The updated integer list after all worker processes have completed their tasks. Constraints: - `1 <= n <= 10^6` - `1 <= num_processes <= 100` - `-10^3 <= increment_value <= 10^3` Example: ```python def worker(shared_list, start, end, increment_value): Worker function that increments elements in the shared_list from index `start` to `end` by `increment_value`. for i in range(start, end): shared_list[i] += increment_value def main(data, num_processes, increment_value): # Import necessary modules import multiprocessing from multiprocessing import shared_memory from multiprocessing.managers import SharedMemoryManager # Initialize shared memory and ShareableList shm_manager = SharedMemoryManager() shm_manager.start() shared_list = shm_manager.ShareableList(data) # Divide the data among workers chunk_size = len(data) // num_processes processes = [] for i in range(num_processes): start = i * chunk_size end = (i + 1) * chunk_size if i != num_processes - 1 else len(data) p = multiprocessing.Process(target=worker, args=(shared_list, start, end, increment_value)) processes.append(p) p.start() # Wait for all processes to finish for p in processes: p.join() # Convert ShareableList back to a regular list updated_data = list(shared_list) # Clean up shared memory shared_list.shm.close() shared_list.shm.unlink() shm_manager.shutdown() return updated_data # Example usage: data = [1, 2, 3, 4, 5, 6] num_processes = 3 increment_value = 10 print(main(data, num_processes, increment_value)) # Output should be [11, 12, 13, 14, 15, 16] ``` Instructions: 1. Implement the `worker` function to increment elements in the shared list. 2. Implement the `main` function to: - Initialize shared memory. - Divide the list among the worker processes. - Launch and coordinate worker processes. - Collect and print the updated list. - Ensure proper cleanup of shared memory resources. Submit the complete code for the `worker` and `main` functions.","solution":"import multiprocessing from multiprocessing import shared_memory from multiprocessing.managers import SharedMemoryManager def worker(shared_list, start, end, increment_value): Worker function that increments elements in the shared_list from index `start` to `end` by `increment_value`. for i in range(start, end): shared_list[i] += increment_value def main(data, num_processes, increment_value): # Initialize shared memory and ShareableList shm_manager = SharedMemoryManager() shm_manager.start() shared_list = shm_manager.ShareableList(data) # Divide the data among workers chunk_size = len(data) // num_processes processes = [] for i in range(num_processes): start = i * chunk_size end = (i + 1) * chunk_size if i != num_processes - 1 else len(data) p = multiprocessing.Process(target=worker, args=(shared_list, start, end, increment_value)) processes.append(p) p.start() # Wait for all processes to finish for p in processes: p.join() # Convert ShareableList back to a regular list updated_data = list(shared_list) # Clean up shared memory shared_list.shm.close() shared_list.shm.unlink() shm_manager.shutdown() return updated_data # Example usage: data = [1, 2, 3, 4, 5, 6] num_processes = 3 increment_value = 10 print(main(data, num_processes, increment_value)) # Output should be [11, 12, 13, 14, 15, 16]"},{"question":"<|Analysis Begin|> The documentation provided describes the `random` module in Python, which is used to generate pseudo-random numbers across various distributions. The key functionalities cover: 1. **Class Methods and Initialization**: - `Random` and `SystemRandom` classes. - Methods like `seed()`, `getstate()`, and `setstate()` for managing the state of random number generation. 2. **Generating Random Numbers**: - Basic methods like `random()`, `randrange()`, `randint()`, and `getrandbits()` for generating random numbers in specific ranges and formats. 3. **Sequence Operations**: - Methods for operations on sequences such as `choice()`, `choices()`, `shuffle()`, and `sample()`. 4. **Distributions**: - Wide variety of real-valued distributions such as `uniform()`, `triangular()`, `betavariate()`, `expovariate()`, `gammavariate()`, `gauss()`, `lognormvariate()`, and more. To design a challenging, clear, and self-contained assessment question that requires a well-thought-out solution, it\'s important to cover: - Understanding and using various `random` methods. - Handling sequences and distributions. - Complex use cases that involve combining multiple methods. In this context, combining multiple elements like shuffling, sampling, and using specific distributions will ensure that the question is comprehensive. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Design a Python function that simulates a simple board game using the `random` module. The game involves players moving on a board based on dice rolls and some random events that occur during the game. Your task is to simulate these events and movements. Problem Statement You need to implement a function `simulate_game(num_players: int, board_size: int) -> List[int]` which simulates a board game involving a given number of players and a specified board size. **Details**: - Each player starts at position 0. - Players take turns to roll a 6-sided die to determine how many spaces they move forward. - If a player lands on a position that is a multiple of 10, they must draw a card from a deck which has special instructions. The deck has the following types of cards: - Move forward 5 spaces. - Move backward 3 spaces. - Roll the dice again and move the resulting number of spaces. - Miss a turn (no move this turn). - The game continues until at least one player reaches or exceeds the `board_size`. **Card Distribution**: - The deck has 5 cards, each corresponding to the above actions. - The distribution of cards is equal (i.e., each type of card appears exactly once in the deck). **Function Signature**: ```python from typing import List def simulate_game(num_players: int, board_size: int) -> List[int]: pass ``` **Input**: - `num_players` (int): The number of players in the game. - `board_size` (int): The size of the board (the position a player needs to reach or exceed to win). **Output**: - A list of integers representing the final positions of each player after the game ends (length will be `num_players`). **Constraints**: - `2 <= num_players <= 10` - `20 <= board_size <= 200` **Example**: ```python final_positions = simulate_game(3, 50) print(final_positions) # Example output: [52, 47, 50] ``` **Notes**: - You should use the `random` module to simulate dice rolls and card draws. - Ensure that you handle edge cases like players missing turns and moving beyond the board size appropriately. Requirements 1. Use appropriate functions from the `random` module. 2. The implementation should be efficient and use proper random functions for various operations. 3. Ensure to track each player\'s position and progress accurately. 4. The function should return the final positions of all players after at least one player has reached or exceeded the board size. ```python import random from typing import List def simulate_game(num_players: int, board_size: int) -> List[int]: positions = [0] * num_players turn = 0 deck = [\'forward\', \'backward\', \'roll\', \'miss\'] def draw_card() -> str: return random.choice(deck) while all(pos < board_size for pos in positions): current_player = turn % num_players dice_roll = random.randint(1, 6) positions[current_player] += dice_roll if positions[current_player] >= board_size: break if positions[current_player] % 10 == 0: card = draw_card() if card == \'forward\': positions[current_player] += 5 elif card == \'backward\': positions[current_player] -= 3 elif card == \'roll\': positions[current_player] += random.randint(1, 6) elif card == \'miss\': turn += 1 turn += 1 return positions ```","solution":"import random from typing import List def simulate_game(num_players: int, board_size: int) -> List[int]: positions = [0] * num_players turn = 0 deck = [\'forward\', \'backward\', \'roll\', \'miss\'] def draw_card() -> str: return random.choice(deck) while all(pos < board_size for pos in positions): current_player = turn % num_players dice_roll = random.randint(1, 6) positions[current_player] += dice_roll if positions[current_player] >= board_size: break if positions[current_player] % 10 == 0: card = draw_card() if card == \'forward\': positions[current_player] += 5 elif card == \'backward\': positions[current_player] -= 3 elif card == \'roll\': positions[current_player] += random.randint(1, 6) elif card == \'miss\': turn += 1 turn += 1 return positions"},{"question":"**Coding Assessment Question** # Objective Implement a custom documentation tool similar to `pydoc` that: 1. Generates HTML files documenting all functions in a specified Python module. 2. Optionally starts a web server to serve these documentation pages. # Requirements 1. **Function: generate_html_docs(module: str, output_dir: str) -> None** - **Input**: - `module` (str): The name of the Python module to document. - `output_dir` (str): The directory where the HTML files should be saved. - **Output**: None - **Description**: This function should generate an HTML file for each function in the specified `module`. The HTML file should include: - The function name. - The function signature. - The function\'s docstring (if present). 2. **Function: start_doc_server(output_dir: str, port: int = 8080) -> None** - **Input**: - `output_dir` (str): The directory containing the HTML documentation files to serve. - `port` (int, optional): The port on which to start the HTTP server. Default is 8080. - **Output**: None - **Description**: This function should start a simple HTTP server that serves the HTML documentation files from the specified `output_dir`. # Constraints - The `generate_html_docs` function must handle cases where the specified module does not exist or does not have any functions. - The output HTML files should be named `<function_name>.html`. - Ensure that the generated HTML is well-formatted and includes basic styling (CSS). # Example If the module `sample_module` contains the following function: ```python def greet(name: str) -> str: Returns a greeting message. return f\\"Hello, {name}!\\" ``` After running `generate_html_docs(\'sample_module\', \'./docs\')`, there should be a file `greet.html` in the `./docs` directory with content similar to: ```html <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>greet</title> <style> body { font-family: Arial, sans-serif; margin: 20px; } h1 { color: #333; } pre { background-color: #f8f8f8; padding: 10px; border: 1px solid #ddd; } </style> </head> <body> <h1>greet</h1> <pre>greet(name: str) -> str</pre> <p>Returns a greeting message.</p> </body> </html> ``` After running `start_doc_server(\'./docs\', 8080)`, navigating to `http://localhost:8080/greet.html` should display the generated documentation for the `greet` function. **Note**: You are not required to implement the actual HTTP server; you can use Python\'s built-in `http.server` module for simplicity. Good luck!","solution":"import importlib import os import inspect import http.server import socketserver def generate_html_docs(module: str, output_dir: str) -> None: Generates HTML documentation for all functions in the specified module. :param module: The name of the Python module to document. :param output_dir: The directory where the HTML files should be saved. try: mod = importlib.import_module(module) except ImportError: print(f\\"Error: Module \'{module}\' not found.\\") return if not os.path.exists(output_dir): os.makedirs(output_dir) for name, func in inspect.getmembers(mod, inspect.isfunction): with open(os.path.join(output_dir, f\\"{name}.html\\"), \'w\') as f: signature = inspect.signature(func) docstring = inspect.getdoc(func) or \\"No documentation available.\\" html_content = f <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>{name}</title> <style> body {{ font-family: Arial, sans-serif; margin: 20px; }} h1 {{ color: #333; }} pre {{ background-color: #f8f8f8; padding: 10px; border: 1px solid #ddd; }} </style> </head> <body> <h1>{name}</h1> <pre>{name}{signature}</pre> <p>{docstring}</p> </body> </html> f.write(html_content) def start_doc_server(output_dir: str, port: int = 8080) -> None: Starts a simple HTTP server to serve the documentation files. :param output_dir: The directory containing the HTML documentation files to serve. :param port: The port on which to start the HTTP server. os.chdir(output_dir) handler = http.server.SimpleHTTPRequestHandler with socketserver.TCPServer((\\"\\", port), handler) as httpd: print(f\\"Serving HTTP on port {port} (http://localhost:{port}/)...\\") httpd.serve_forever()"},{"question":"# Advanced Coding Assessment: Path Handling and Process Management in Python Problem Description You are tasked with implementing a custom utility in Python that will handle various OS operations, including file path manipulations and managing process forking. Your goal is to create a function that combines the following tasks: 1. Validating and converting input paths to their file system representation. 2. Performing specific actions before and after a fork operation. 3. Writing informative messages about the process states to the standard output and error streams. Requirements 1. **Function: `os_utility(input_path: str) -> str`** - **Input**: - `input_path` (str): A string representing a potential file system path. It can be a normal string or an object that implements the `__fspath__()` method from `os.PathLike`. - **Output**: - Returns a string message indicating the status of the fork operation and the file system path representation. - **Behavior**: 1. Use the `PyOS_FSPath()` equivalent to validate and convert the `input_path` to its file system representation. If the conversion fails, raise a `TypeError`. 2. Before forking, write a message to `sys.stdout` indicating the beginning of the forking process using `PySys_WriteStdout()`. 3. Perform a fork operation using `os.fork()`. 4. If in the parent process: - Call the `PyOS_AfterFork_Parent()` equivalent. - Write a success message to `sys.stdout` using `PySys_WriteStdout()`. 5. If in the child process: - Call the `PyOS_AfterFork_Child()` equivalent. - Write a success message to `sys.stderr` using `PySys_WriteStderr()`. 6. Return the final status message composed by combining messages from both processes. Constraints - You may assume the input path is well-formed and does not contain illegal characters. - Ensure all memory allocated for path conversion is properly freed. - Handle exceptions gracefully and ensure meaningful messages are returned or raised. Performance Requirements - Your implementation should efficiently handle the input and fork operations with minimal performance overhead. ```python import os import sys def os_utility(input_path: str) -> str: # Implement the required functionality here pass # Example usage try: result = os_utility(\'/example/path\') print(result) except TypeError as e: print(f\\"Error: {e}\\") ``` **Notes**: - You may mock the low-level functions using `ctypes` or similar techniques if needed to simulate the `Py_*` functions. - Ensure to test your implementation with various `input_path` formats including plain strings and mocked `os.PathLike` objects. Testing - Test with different paths to verify the correctness of path conversion. - Test the process forking by observing the output from both parent and child processes.","solution":"import os import sys import traceback def os_utility(input_path: str) -> str: try: # Convert input path to file system path. Raises TypeError if invalid. fs_path = os.fspath(input_path) except TypeError: raise TypeError(\\"The provided path is not valid\\") # Print starting fork message to stdout sys.stdout.write(\\"Starting fork process...n\\") try: # Perform the fork operation pid = os.fork() if pid > 0: # This is the parent process sys.stdout.write(\\"In parent process after fork.n\\") return f\\"Parent process: PID = {pid}, FS Path = {fs_path}\\" else: # This is the child process sys.stderr.write(\\"In child process after fork.n\\") return f\\"Child process: PID = {pid}, FS Path = {fs_path}\\" except Exception as e: traceback.print_exc() raise RuntimeError(\\"Fork failed\\") from e"},{"question":"# Objective Implement a SAX Parser using Python\'s `xml.sax.xmlreader` module that processes an XML document to extract and print all book titles from the following sample XML structure: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book elements --> </catalog> ``` # Instructions 1. **Create a SAX parser** using the `xml.sax.xmlreader` module. 2. **Define a custom content handler** that will extract and print all book titles from the XML document. 3. Use the **IncrementalParser** to process the XML data in chunks. # Requirements - Implement a class `BookTitleExtractor` that inherits from `xml.sax.ContentHandler`. - Override the methods `startElement`, `endElement`, and `characters` to handle the parsing events. - Extract and print the title of each book when its closing tag is encountered. - Implement the `parseXMLChunked` function to read the XML content in chunks and use the `IncrementalParser` to process these chunks. # Input Format - The XML data will be provided as a string. # Output Format - Print the titles of all books found in the XML document, one per line. # Example Usage ```python xml_data = <catalog> ... </catalog> # the full XML content as specified # Function to be implemented def parseXMLChunked(xml_string): # Implementation here # Example execution parseXMLChunked(xml_data) ``` # Constraints - The XML data can be large, so it must be processed in chunks. - Ensure to handle multiple books and correctly extract each title. # Additional Notes - Do not use external libraries; use only Python\'s standard library, especially focusing on the `xml.sax.xmlreader` module.","solution":"import xml.sax import xml.sax.xmlreader class BookTitleExtractor(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.is_title = False def startElement(self, name, attrs): self.current_element = name if self.current_element == \\"title\\": self.is_title = True self.current_data = \\"\\" def endElement(self, name): if self.is_title and self.current_element == \\"title\\": print(self.current_data) self.is_title = False self.current_element = \\"\\" def characters(self, content): if self.is_title: self.current_data += content def parseXMLChunked(xml_string): parser = xml.sax.make_parser() handler = BookTitleExtractor() parser.setContentHandler(handler) import io xml_stream = io.StringIO(xml_string) chunk_size = 1024 # process the XML in chunks of 1024 characters while True: chunk = xml_stream.read(chunk_size) if not chunk: break parser.feed(chunk) parser.close()"},{"question":"# Question: Memory Leak Detection Using `tracemalloc` You are given a Python application that seems to leak memory in some of its functions. Your task is to write a script using the `tracemalloc` module to identify the source of the memory leak. Requirements: 1. Write a function `identify_memory_leak(app_function, num_top_stats=10)` that: - Takes a function `app_function` which is the main function of the application suspected to leak memory. - Takes an optional argument `num_top_stats` specifying the number of top memory-consuming lines to display (default is 10). - Starts tracing memory allocations. - Takes a snapshot before running the `app_function`. - Takes another snapshot after running the `app_function`. - Compares the two snapshots to identify differences. - Displays the top `num_top_stats` lines of code responsible for the largest memory allocations, excluding frames from `<frozen importlib._bootstrap>` and `<unknown>`. Input: - `app_function`: A function object to run the main application. - `num_top_stats`: An integer specifying the number of top memory-consuming lines to display (default 10). Output: - Prints the top `num_top_stats` lines of code that have the most significant increase in memory usage. Example: ```python import tracemalloc import linecache def display_top(snapshot, key_type=\'lineno\', limit=10): snapshot = snapshot.filter_traces(( tracemalloc.Filter(False, \\"<frozen importlib._bootstrap>\\"), tracemalloc.Filter(False, \\"<unknown>\\"), )) top_stats = snapshot.statistics(key_type) print(\\"Top %s lines\\" % limit) for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] print(\\"#%s: %s:%s: %.1f KiB\\" % (index, frame.filename, frame.lineno, stat.size / 1024)) line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(\' %s\' % line) other = top_stats[limit:] if other: size = sum(stat.size for stat in other) print(\\"%s other: %.1f KiB\\" % (len(other), size / 1024)) total = sum(stat.size for stat in top_stats) print(\\"Total allocated size: %.1f KiB\\" % (total / 1024)) def identify_memory_leak(app_function, num_top_stats=10): import tracemalloc tracemalloc.start() # Take a snapshot before running the application snapshot1 = tracemalloc.take_snapshot() # Run the application function suspected of leaking memory app_function() # Take another snapshot after the application has run snapshot2 = tracemalloc.take_snapshot() # Compare the snapshots to find differences top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(f\\"[ Top {num_top_stats} differences ]\\") for index, stat in enumerate(top_stats[:num_top_stats], 1): frame = stat.traceback[0] print(f\\"#{index}: {frame.filename}:{frame.lineno}, Size Change: {stat.size_diff / 1024:.1f} KiB\\") display_top(snapshot2, limit=num_top_stats) # Example of how to call identify_memory_leak # def my_app_function(): # # Application code that may have a memory leak # pass # identify_memory_leak(my_app_function) ``` Notes: - Ensure to import the required modules (`tracemalloc`, `linecache`). - Experiment with simple functions inside the `my_app_function` to test your script if the application code is not provided. - Pay attention to correctly filtering out `<frozen importlib._bootstrap>` and `<unknown>` frames to focus on relevant memory allocations.","solution":"import tracemalloc import linecache def display_top(snapshot, key_type=\'lineno\', limit=10): snapshot = snapshot.filter_traces(( tracemalloc.Filter(False, \\"<frozen importlib._bootstrap>\\"), tracemalloc.Filter(False, \\"<unknown>\\"), )) top_stats = snapshot.statistics(key_type) print(\\"Top %s lines\\" % limit) for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] print(\\"#%s: %s:%s: %.1f KiB\\" % (index, frame.filename, frame.lineno, stat.size / 1024)) line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(\' %s\' % line) other = top_stats[limit:] if other: size = sum(stat.size for stat in other) print(\\"%s other: %.1f KiB\\" % (len(other), size / 1024)) total = sum(stat.size for stat in top_stats) print(\\"Total allocated size: %.1f KiB\\" % (total / 1024)) def identify_memory_leak(app_function, num_top_stats=10): tracemalloc.start() # Take a snapshot before running the application snapshot1 = tracemalloc.take_snapshot() # Run the application function suspected of leaking memory app_function() # Take another snapshot after the application has run snapshot2 = tracemalloc.take_snapshot() # Compare the snapshots to find differences top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(f\\"[ Top {num_top_stats} differences ]\\") for index, stat in enumerate(top_stats[:num_top_stats], 1): frame = stat.traceback[0] print(f\\"#{index}: {frame.filename}:{frame.lineno}, Size Change: {stat.size_diff / 1024:.1f} KiB\\") display_top(snapshot2, limit=num_top_stats)"},{"question":"**Problem Statement:** Implementing a Custom ZIP-based Module Loader You are tasked with creating a simplified custom module loader that can dynamically load Python code from ZIP archives. Your loader should mimic some of the behaviors of Python\'s `zipimport` module, such as discovering and loading modules from a specified ZIP archive. # Requirements: 1. **Class Definition**: Define a class `CustomZipImporter` that will handle the importing of modules from a given ZIP file. 2. **Constructor**: The constructor should take the path to a ZIP file. 3. **Methods**: - `list_modules()`: Lists all `*.py` modules in the ZIP archive. - `load_module(module_name)`: Loads the specified module if it exists and returns its source code as a string. Should raise a `ModuleNotFoundError` if the module cannot be found. # Input: 1. A path to a ZIP file for the `CustomZipImporter` constructor. 2. A fully qualified module name (dotted name) for the `load_module`. # Output: 1. For `list_modules()`: A list of module names (excluding `.py` extension). 2. For `load_module(module_name)`: The source code of the module as a string. # Constraints: - Only `*.py` files should be considered as modules. - Assume the ZIP file structure is flat (all modules are at the root level of the archive). - The solution should handle large lists of modules efficiently. # Example: Assume `example.zip` contains the following structure: ``` example.zip |- module1.py |- module2.py |- module3.py ``` Sample usage ```python import zipfile class CustomZipImporter: def __init__(self, zip_path): # Implement the constructor pass def list_modules(self): # Implement listing of modules pass def load_module(self, module_name): # Implement module loading pass # Example usage: zip_importer = CustomZipImporter(\'example.zip\') print(zip_importer.list_modules()) # Outputs: [\'module1\', \'module2\', \'module3\'] print(zip_importer.load_module(\'module1\')) # Outputs the source code of module1.py as a string. ``` Note: You may assume that the `zipfile` module is available and can be used to interact with ZIP files.","solution":"import zipfile import os class CustomZipImporter: def __init__(self, zip_path): self.zip_path = zip_path if not zipfile.is_zipfile(zip_path): raise ValueError(f\\"{zip_path} is not a valid ZIP file\\") self.zip_file = zipfile.ZipFile(zip_path) def list_modules(self): py_files = [os.path.splitext(name)[0] for name in self.zip_file.namelist() if name.endswith(\'.py\')] return py_files def load_module(self, module_name): module_file = f\\"{module_name}.py\\" if module_file not in self.zip_file.namelist(): raise ModuleNotFoundError(f\\"Module {module_name} not found in ZIP file\\") with self.zip_file.open(module_file) as file: return file.read().decode(\'utf-8\')"},{"question":"# Custom Python Type Implementation **Objective**: Demonstrate your understanding of the Python type system by implementing custom types and ensuring they meet specific constraints and behaviors. # Problem Statement You are tasked with designing two custom types (classes) in Python: `CustomType1` and `CustomType2`. These types should exhibit specific properties and behaviors described below. Additionally, you should write a function `type_checker(obj, base_type)` that returns whether the provided object `obj` is exactly of type `base_type` or a subclass of `base_type`. Requirements: 1. **CustomType1**: - Should have a class variable `type_name` initialized to `\\"Type1\\"`. - Should have an instance variable `value`, which is initialized through the constructor. - Should have a method `double_value()` that returns double the `value` of the instance. 2. **CustomType2**: - Should inherit from `CustomType1`. - Should override the `type_name` class variable to `\\"Type2\\"`. - Should have an additional instance variable `extra_value`, initialized through the constructor. - Should have a method `sum_values()` that returns the sum of `value` and `extra_value`. 3. **type_checker(obj, base_type)**: - Should return `True` if `obj` is exactly an instance of `base_type`. - Should return `False` if `obj` is a subtype of `base_type` or not related to `base_type`. Input: 1. `type_checker(obj, base_type)`: - `obj`: an instance of any class. - `base_type`: any type. Output: - `type_checker(obj, base_type)` should return a boolean value based on the criteria described above. Example Usage: ```python # Creating instances ct1 = CustomType1(10) ct2 = CustomType2(20, 30) # Method calls print(ct1.double_value()) # Output: 20 print(ct2.sum_values()) # Output: 50 # Type checking print(type_checker(ct1, CustomType1)) # Output: True print(type_checker(ct2, CustomType1)) # Output: False print(type_checker(ct2, CustomType2)) # Output: True ``` Constraints: - Ensure proper class inheritance and variable scope management. - The `type_checker` function should use exact type matching, not subclass checking. # Submission Requirements Submit your code as a single Python script or a Jupyter notebook implementing the `CustomType1`, `CustomType2`, and `type_checker` function.","solution":"class CustomType1: type_name = \\"Type1\\" def __init__(self, value): self.value = value def double_value(self): return self.value * 2 class CustomType2(CustomType1): type_name = \\"Type2\\" def __init__(self, value, extra_value): super().__init__(value) self.extra_value = extra_value def sum_values(self): return self.value + self.extra_value def type_checker(obj, base_type): return type(obj) == base_type"},{"question":"**Context**: You are a software engineer working on a multi-platform application that reads and writes various types of data to and from files. Efficient handling of these operations is crucial to the application\'s performance and correctness. You need to implement a function that deals with both text and binary data streams, handled appropriately. # Task Implement a function `process_file_data` which takes the following parameters: - `input_file_path` (str): The path to the input file. - `output_file_path` (str): The path to the output file. - `mode` (str): The mode in which to open the files. It can be one of the following: - `\'rt\'`: Read text. - `\'wt\'`: Write text. - `\'rb\'`: Read binary. - `\'wb\'`: Write binary. - `data` (optional, str or bytes): The data to be written to the output file when the mode is `\'wt\'` or `\'wb\'`. # Requirements 1. The function should read data from the `input_file_path` and write it to the `output_file_path` based on the specified mode. 2. For text files (mode `\'rt\'` or `\'wt\'`), handle UTF-8 encoding explicitly. 3. Handle the scenario where the input file might not exist gracefully, and raise an appropriate error. 4. If the mode is for writing (`\'wt\'` or `\'wb\'`), `data` parameter must be provided and should be written to the `output_file_path`. 5. Ensure efficient handling by using appropriate buffering and error handling mechanisms. # Constraints - Only use the `io` module for I/O operations. - If `data` is to be written, for text mode it should be a string, and for binary mode it should be bytes. - The function should be able to handle large files efficiently. # Function Signature ```python def process_file_data(input_file_path: str, output_file_path: str, mode: str, data: Optional[Union[str, bytes]] = None) -> None: pass ``` # Examples 1. **Example 1**: ```python process_file_data(\'input.txt\', \'output.txt\', \'rt\') # This should read text data from \'input.txt\' and write it to \'output.txt\' using UTF-8 encoding. ``` 2. **Example 2**: ```python process_file_data(\'input.bin\', \'output.bin\', \'rb\') # This should read binary data from \'input.bin\' and write it to \'output.bin\'. ``` 3. **Example 3**: ```python process_file_data(\'output.txt\', \'wt\', data=\\"Hello, World!\\") # This should write the string \\"Hello, World!\\" to \'output.txt\' using UTF-8 encoding. ``` 4. **Example 4**: ```python process_file_data(\'output.bin\', \'wb\', data=b\'x00x01x02\') # This should write the bytes b\'x00x01x02\' to \'output.bin\'. ``` # Notes - Ensure opening and closing of files using the context manager (`with` statement) to avoid resource leaks. - Clearly handle any exceptions and provide meaningful error messages when something goes wrong (e.g., file not found). - Think about edge cases, such as empty input files or invalid modes.","solution":"import io from typing import Optional, Union def process_file_data(input_file_path: str, output_file_path: str, mode: str, data: Optional[Union[str, bytes]] = None) -> None: try: if \'r\' in mode: with io.open(input_file_path, mode) as input_file: content = input_file.read() with io.open(output_file_path, mode.replace(\'r\', \'w\', 1)) as output_file: output_file.write(content) elif \'w\' in mode: if data is None: raise ValueError(\\"Data must be provided for writing modes (\'wt\' or \'wb\').\\") with io.open(output_file_path, mode) as output_file: output_file.write(data) else: raise ValueError(\\"Invalid mode. Use \'rt\', \'wt\', \'rb\', or \'wb\'.\\") except FileNotFoundError: raise FileNotFoundError(f\\"The file {input_file_path} does not exist.\\") except Exception as e: raise e"},{"question":"Analyzing Sales Data with pandas GroupBy You are provided with a CSV file named `sales_data.csv` containing sales information for an e-commerce company. The dataset contains columns: - `order_id`: Unique identifier for each order. - `date`: Date of the order. - `customer_id`: Unique identifier for each customer. - `category`: Category of the purchased item. - `sales_amount`: Amount of the sale. - `quantity`: Quantity of items sold. Your task is to write a Python function using pandas that performs the following processing on this sales data: 1. **Read the data** from the `sales_data.csv` file into a pandas DataFrame. 2. **Group** the data by the `category` column. 3. For each category, calculate the following: - Total `sales_amount`. - Average `sales_amount`. - Total `quantity` sold. - The date with the highest sales (`sales_amount`). 4. **Plot** the total `sales_amount` for each category as a bar chart. 5. **Return** a DataFrame with the calculated statistics for each category. # Input - `csv_file_path` (str): The file path to the `sales_data.csv`. # Output - A pandas DataFrame with the following columns: - `category`: Category of the purchased item. - `total_sales`: Total sales amount. - `average_sales`: Average sales amount. - `total_quantity`: Total quantity sold. - `date_highest_sales`: The date with the highest sales for each category. # Constraints - You must use the `GroupBy` functionality in pandas. - The function should handle cases where the dataset might be empty. # Example Usage ```python def analyze_sales_data(csv_file_path): # Your code here # Example Usage csv_file_path = \'path/to/sales_data.csv\' result_df = analyze_sales_data(csv_file_path) print(result_df) ``` **Hints:** - Use `groupby` to group data by the `category` column. - Use aggregation functions like `sum` and `mean`. - Use the `idxmax` function to find the date with the highest sales. - Use `matplotlib` or `pandas` built-in plot functionality for visualization. - Handles edge cases where the dataset might be empty by returning an empty DataFrame with the specified columns.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(csv_file_path): # Read the CSV file into a pandas DataFrame df = pd.read_csv(csv_file_path) if df.empty: # Return an empty DataFrame with the correct columns if the dataset is empty return pd.DataFrame(columns=[\'category\', \'total_sales\', \'average_sales\', \'total_quantity\', \'date_highest_sales\']) # Group the data by the \'category\' column grouped = df.groupby(\'category\') # Calculate the required statistics total_sales = grouped[\'sales_amount\'].sum() average_sales = grouped[\'sales_amount\'].mean() total_quantity = grouped[\'quantity\'].sum() date_highest_sales = grouped.apply(lambda x: x.loc[x[\'sales_amount\'].idxmax()][\'date\']) # Combine the results into a single DataFrame result_df = pd.DataFrame({ \'category\': total_sales.index, \'total_sales\': total_sales.values, \'average_sales\': average_sales.values, \'total_quantity\': total_quantity.values, \'date_highest_sales\': date_highest_sales.values }) # Plot the total sales amount for each category as a bar chart total_sales.plot(kind=\'bar\', title=\'Total Sales Amount by Category\', ylabel=\'Sales Amount\') plt.show() # Return the DataFrame with the calculated statistics return result_df"},{"question":"# Question: Interactive Web Browser Opener You are tasked with developing a Python script that provides an interactive way for users to open multiple URLs in either new tabs or new windows of their default web browser. Function Requirements: 1. Write a function `choose_mode_and_open(urls, mode=\\"tab\\")` that: - Takes a list of `urls` (strings) and a `mode` (string) which can be either \\"tab\\" or \\"window\\". - Opens each URL in the specified mode using the default web browser. - If mode is \\"tab\\", open each URL in a new tab. - If mode is \\"window\\", open each URL in a new window. - Rises an exception if an invalid mode is provided. 2. Write a function `interactive_browser_opener()` that: - Prompts the user to input multiple URLs separated by whitespace. - Prompts the user to choose the mode for opening (either \'tab\' or \'window\'). - Calls `choose_mode_and_open()` with the given URLs and the selected mode. Constraints: - The input URL list should not be empty. - Mode should strictly be \\"tab\\" or \\"window\\". Sample Input and Output: **Sample Input:** ``` Enter URLs (separated by spaces): https://www.google.com https://www.github.com https://www.python.org Choose mode (tab/window): tab ``` - Expected Output: All URLs should open in new tabs in the default browser. **Sample Input:** ``` Enter URLs (separated by spaces): https://www.wikipedia.org https://www.stackoverflow.com https://www.reddit.com Choose mode (tab/window): window ``` - Expected Output: Each URL should open in a new window in the default browser. **Function Signatures:** ```python def choose_mode_and_open(urls: list, mode: str = \\"tab\\") -> None: pass def interactive_browser_opener() -> None: pass ``` Make sure to handle any exceptions that might occur during the execution of your code and provide meaningful error messages.","solution":"import webbrowser def choose_mode_and_open(urls, mode=\\"tab\\"): Opens the given URLs in the specified mode (\\"tab\\" or \\"window\\"). Parameters: urls (list): List of URLs to open. mode (str): Mode to open URLs in, either \\"tab\\" or \\"window\\". Raises: ValueError: If mode is not \\"tab\\" or \\"window\\". if mode not in [\\"tab\\", \\"window\\"]: raise ValueError(f\\"Invalid mode: {mode}. Choose \'tab\' or \'window\'.\\") if not urls: raise ValueError(\\"URL list cannot be empty.\\") for url in urls: if mode == \\"tab\\": webbrowser.open_new_tab(url) elif mode == \\"window\\": webbrowser.open_new(url) def interactive_browser_opener(): Prompts the user for URLs and mode, then opens the URLs in the specified mode. urls_input = input(\\"Enter URLs (separated by spaces): \\").strip() urls = urls_input.split() if not urls: print(\\"No URLs provided. Exiting.\\") return mode = input(\\"Choose mode (tab/window): \\").strip().lower() try: choose_mode_and_open(urls, mode) except ValueError as e: print(e)"},{"question":"You are required to implement a Python function that utilizes the `syslog` module to log messages with different priorities and configurations. The function should be able to: 1. Initialize the logging configuration with specified parameters. 2. Log a series of messages with varying priorities. 3. Change the log mask to filter out certain priority levels and log additional messages according to the new mask. 4. Finally, close the log. # Function Specification **Function Name:** ```python def custom_syslog_logger(ident: str, facility: int, log_messages: list, mask_priority: int) -> None: ``` **Parameters:** - `ident` (str): A string to prepend to every log message. - `facility` (int): An integer representing the facility to use for the log (e.g., `syslog.LOG_USER`, `syslog.LOG_MAIL`). - `log_messages` (list): A list of tuples where each tuple contains: - A priority level (int) from the `syslog` priority levels (e.g., `syslog.LOG_ERR`, `syslog.LOG_INFO`). - A message (str) to log with that priority. - `mask_priority` (int): An integer representing the highest priority level to log. Messages with a higher priority level (lower value) will be logged. **Returns:** - The function does not return any value. It should perform logging as specified. **Example Usage:** ```python log_messages = [ (syslog.LOG_INFO, \\"Information message.\\"), (syslog.LOG_ERR, \\"Error message.\\"), (syslog.LOG_DEBUG, \\"Debugging message.\\"), ] custom_syslog_logger(\\"MyApp\\", syslog.LOG_USER, log_messages, syslog.LOG_ERR) ``` **Constraints:** - The `log_messages` list will have at least one message tuple. - All priority levels and facility values provided will be valid constants from the `syslog` module. # Implementation Requirements 1. Configure the syslog system using `syslog.openlog` with the specified `ident` and `facility`. 2. Log the specified messages with their respective priorities using `syslog.syslog`. 3. Use `syslog.setlogmask` to change the logging mask to filter out messages below the specified `mask_priority`. 4. Attempt to log all messages again after setting the mask, but only messages with a priority level up to the `mask_priority` should be recorded. 5. Finally, close the logging system using `syslog.closelog`. # Example Given the example usage above, the function would: 1. Open the log with identifier \\"MyApp\\" and facility `syslog.LOG_USER`. 2. Log three messages: - \\"Information message.\\" at `LOG_INFO`. - \\"Error message.\\" at `LOG_ERR`. - \\"Debugging message.\\" at `LOG_DEBUG`. 3. Set the mask to only log messages up to `LOG_ERR`. 4. Log the three messages again, but only the first two will be recorded due to the mask. 5. Close the log.","solution":"import syslog def custom_syslog_logger(ident: str, facility: int, log_messages: list, mask_priority: int) -> None: Logs messages to syslog with specified identity and facility. Masks log priority as specified. Args: - ident (str): Message identifier. - facility (int): Syslog facility. - log_messages (list): List of tuples with (priority, message). - mask_priority (int): Mask priority level. Returns: None # Open the log with the specified identity and facility syslog.openlog(ident, logoption=syslog.LOG_PID, facility=facility) # Log each message with its specified priority for priority, message in log_messages: syslog.syslog(priority, message) # Set log mask to only log messages at or below mask_priority mask = syslog.LOG_UPTO(mask_priority) syslog.setlogmask(mask) # Log each message again, applying the new mask for priority, message in log_messages: syslog.syslog(priority, message) # Close the log syslog.closelog()"},{"question":"**Question: Implementing a Custom Collection with Special Behaviors** You are required to implement a custom collection class `SpecialDict` in Python that mimics some behaviors of a dictionary but with additional custom behaviors. The `SpecialDict` class should: 1. Act like a standard dictionary for adding, accessing, and deleting items. 2. Implement a custom behavior such that if a key that doesn\'t exist is accessed, it should return the string `\\"Key not found!\\"`. 3. Support getting the size of the `SpecialDict` using the `len()` function. 4. Override the string representation (`__str__`) method to return the items in the dictionary as a sorted (by keys) string of key-value pairs. 5. Implement the in-place addition operator (`+=`) to merge another dictionary or `SpecialDict` into the current `SpecialDict`. # Requirements: 1. Use appropriate special methods to ensure the custom behaviors. 2. Ensure the dictionary-like behavior for item access, setting, and deletion. 3. The string representation should show key-value pairs sorted by keys. # Example Usage: ```python sd = SpecialDict() sd[\'a\'] = 1 sd[\'b\'] = 2 print(sd) # Should print: \\"{\'a\': 1, \'b\': 2}\\" print(sd[\'c\']) # Should print: \\"Key not found!\\" print(len(sd)) # Should print: 2 sd2 = {\'c\': 3, \'d\': 4} sd += sd2 print(sd) # Should print: \\"{\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4}\\" ``` # Implementation: Define the `SpecialDict` class below: ```python class SpecialDict: def __init__(self): # Initialize the dictionary to store items self._data = {} def __getitem__(self, key): # Return the value if key exists or custom message if it doesn\'t return self._data.get(key, \\"Key not found!\\") def __setitem__(self, key, value): # Set the value for the given key self._data[key] = value def __delitem__(self, key): # Delete the item with the given key if key in self._data: del self._data[key] def __len__(self): # Return the size of the SpecialDict return len(self._data) def __str__(self): # Return a string representation of the items sorted by keys sorted_items = sorted(self._data.items()) return str(dict(sorted_items)) def __iadd__(self, other): # In-place addition to merge another dictionary or SpecialDict if isinstance(other, (dict, SpecialDict)): self._data.update(other._data if isinstance(other, SpecialDict) else other) else: raise TypeError(\\"Unsupported type for merging\\") return self ``` You may use the example provided in the question to test your implementation.","solution":"class SpecialDict: def __init__(self): # Initialize the dictionary to store items self._data = {} def __getitem__(self, key): # Return the value if key exists or custom message if it doesn\'t return self._data.get(key, \\"Key not found!\\") def __setitem__(self, key, value): # Set the value for the given key self._data[key] = value def __delitem__(self, key): # Delete the item with the given key if key in self._data: del self._data[key] def __len__(self): # Return the size of the SpecialDict return len(self._data) def __str__(self): # Return a string representation of the items sorted by keys sorted_items = sorted(self._data.items()) return str(dict(sorted_items)) def __iadd__(self, other): # In-place addition to merge another dictionary or SpecialDict if isinstance(other, (dict, SpecialDict)): self._data.update(other._data if isinstance(other, SpecialDict) else other) else: raise TypeError(\\"Unsupported type for merging\\") return self"},{"question":"# Advanced Python Coding Assessment Objective: You are required to implement a system that serializes complex Python objects, stores them in a SQLite database, and retrieves and deserializes them when needed. This will demonstrate your understanding of the `pickle` and `sqlite3` modules for data persistence. Problem Statement: Create a Python function `serialize_to_db(obj, db_name, table_name, key)` that takes a Python object, serializes it using the `pickle` module, and stores it in a specified table within a SQLite database. Then create a function `deserialize_from_db(db_name, table_name, key)` that retrieves the serialized object from the database, deserializes it, and returns the original Python object. Requirements: 1. **serialize_to_db(obj, db_name, table_name, key)**: - **Parameters**: - `obj` (any Python object): The object to be serialized. - `db_name` (str): The name of the SQLite database file. - `table_name` (str): The name of the table where the serialized object will be stored. - `key` (str): A unique key identifying the stored object. - **Functionality**: - Serialize the object using `pickle`. - Store the serialized object in the given SQLite database and table with the specified key. 2. **deserialize_from_db(db_name, table_name, key)**: - **Parameters**: - `db_name` (str): The name of the SQLite database file. - `table_name` (str): The name of the table from where the object will be retrieved. - `key` (str): The unique key identifying the stored object. - **Functionality**: - Retrieve the serialized object from the database using the specified key. - Deserialize the object using `pickle` and return it. Constraints: - Ensure that the table structure is correctly defined if it does not exist. - Handle exceptions related to database operations and serialization/deserialization. - Assume the database operations are thread-safe for simplicity. - Each `key` in the database table will be unique. Example Usage: ```python import pickle import sqlite3 def serialize_to_db(obj, db_name, table_name, key): # Your code to serialize and store the object in the SQLite database pass def deserialize_from_db(db_name, table_name, key): # Your code to retrieve and deserialize the object from the SQLite database pass # Example Object example_object = {\'name\': \'Alice\', \'age\': 30, \'scores\': [85, 90, 92]} # Serialization to Database serialize_to_db(example_object, \'test.db\', \'data_table\', \'user1\') # Deserialization from Database retrieved_object = deserialize_from_db(\'test.db\', \'data_table\', \'user1\') print(retrieved_object) ``` The provided functions should interact with the database as described, handle the serialization and deserialization process, and correctly store and retrieve Python objects. Performance: - Operations should be efficient enough to handle typical use cases involving small to reasonably large Python objects.","solution":"import pickle import sqlite3 def serialize_to_db(obj, db_name, table_name, key): serialized_obj = pickle.dumps(obj) conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(f\'\'\' CREATE TABLE IF NOT EXISTS {table_name} ( key TEXT PRIMARY KEY, data BLOB ) \'\'\') cursor.execute(f\'\'\' INSERT OR REPLACE INTO {table_name} (key, data) VALUES (?, ?) \'\'\', (key, serialized_obj)) conn.commit() conn.close() def deserialize_from_db(db_name, table_name, key): conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(f\'\'\' SELECT data FROM {table_name} WHERE key = ? \'\'\', (key,)) result = cursor.fetchone() conn.close() if result is None: raise KeyError(f\\"No entry found for key: {key}\\") serialized_obj = result[0] return pickle.loads(serialized_obj)"},{"question":"**Objective:** Demonstrate your understanding of how to interact between Python and C by embedding a Python script in a C application. **Problem Statement:** You will first write a Python script that contains a specific function required for an embedded C application. Then you will simulate the embedding process in Python, which would be typically done in C, to verify your Python script functions correctly. **Steps:** 1. **Write the Python script:** - Create a Python file named `math_operations.py`. - Implement a function `add_and_subtract(a, b)` that takes two integers `a` and `b`, and returns a tuple `(sum, difference)` where: - `sum` is the result of adding `a` and `b`. - `difference` is the result of subtracting `b` from `a`. 2. **Simulate embedding in Python:** - Write a Python script (`embed_simulation.py`) that acts like the C embedding code: - Import the `math_operations` module. - Access the `add_and_subtract` function. - Call the function with two integers and print the result. **Input and Output:** 1. **`math_operations.py` Function Interface:** ```python def add_and_subtract(a: int, b: int) -> tuple: This function should return a tuple containing the sum and difference of the two integer inputs. Parameters: a (int): The first integer. b (int): The second integer. Returns: tuple: A tuple containing: sum (int): The sum of a and b. difference (int): The difference of a and b (a - b). # Implementation goes here ``` 2. **`embed_simulation.py` Simulation:** ```python # Importing the Python script as a module import math_operations # Prepare the arguments to be passed to the function a = 5 b = 3 # Accessing and calling the function result = math_operations.add_and_subtract(a, b) # Print the result in the format (sum, difference) print(f\\"Result of add_and_subtract({a}, {b}): {result}\\") ``` **Constraints:** - The integers passed to the function will be non-negative and within the range 0 to 1000. **Evaluation:** - Correctness of the `add_and_subtract` function implementation in `math_operations.py`. - Proper import and function call in `embed_simulation.py`. - Correct execution of the embedding simulation script and output format. **Performance Requirements:** - The function should have a constant time complexity, O(1). *Note: You are free to use any standard Python library functions if needed.* Good luck! Make sure to test your solution thoroughly.","solution":"# Create the math_operations.py with the required function def add_and_subtract(a: int, b: int) -> tuple: This function returns a tuple containing the sum and difference of the two integer inputs. Parameters: a (int): The first integer. b (int): The second integer. Returns: tuple: A tuple containing: sum (int): The sum of a and b. difference (int): The difference of a and b (a - b). sum_result = a + b difference_result = a - b return (sum_result, difference_result)"},{"question":"# Question **Title: Implementation of a Simple Cache System using `dbm` Module** **Objective:** Implement a simple cache system using the `dbm` module in Python. The cache system should support adding, retrieving, and removing cache entries, as well as purging old cache entries based on a simple timestamp-based expiration policy. **Instructions:** 1. You are required to implement a class called `SimpleCache`. 2. The cache should store key-value pairs where keys are strings and values are strings as well, but both need to be converted to bytes before storing in the db. 3. Each cache entry should have an associated timestamp indicating when the entry was added. Use Python\'s `time.time()` function to generate timestamps. 4. The cache should automatically delete entries older than a specified number of seconds when retrieving or adding new entries. 5. Ensure proper error handling for database operations, especially when the database is read-only or when keys/values are not in the expected format. **Class Specification:** ```python import dbm import time class SimpleCache: def __init__(self, filename, expiration_time): Initialize the cache. :param filename: The filename of the database. :param expiration_time: Time in seconds after which cache entries expire. self.filename = filename self.expiration_time = expiration_time def add(self, key, value): Add a key-value pair to the cache. :param key: The key to be stored (str). :param value: The value to be stored (str). pass def get(self, key): Retrieve a value from the cache. :param key: The key to retrieve (str). :return: The value if key is found and not expired, None otherwise. pass def remove(self, key): Remove a key from the cache. :param key: The key to be removed (str). pass def _purge_expired(self): Purge expired entries from the cache based on the current time. pass ``` **Constraints and Assumptions:** - `key` and `value` in the `add()` method will always be strings. - You need to handle conversion to and from bytes where necessary. - Entries should be automatically purged if they are older than the specified `expiration_time`. - You must handle any `dbm`-related errors gracefully and print relevant messages for debugging. **Example Usage:** ```python cache = SimpleCache(\'cache.db\', expiration_time=60) # entries expire after 60 seconds cache.add(\'user:123\', \'John Doe\') print(cache.get(\'user:123\')) # Outputs: \'John Doe\' time.sleep(61) print(cache.get(\'user:123\')) # Outputs: None (since entry is expired) cache.remove(\'user:123\') ``` **Notes:** - You may assume that file creation and permission are correctly handled by the environment where this code runs. - Focus on maintaining clean and readable code with proper error handling. - Include comments explaining your logic, especially for the internal `_purge_expired` method.","solution":"import dbm import time class SimpleCache: def __init__(self, filename, expiration_time): Initialize the cache. :param filename: The filename of the database. :param expiration_time: Time in seconds after which cache entries expire. self.filename = filename self.expiration_time = expiration_time def add(self, key, value): Add a key-value pair to the cache. :param key: The key to be stored (str). :param value: The value to be stored (str). with dbm.open(self.filename, \'c\') as db: timestamp = str(time.time()).encode(\'utf-8\') db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') db[key.encode(\'utf-8\') + b\'_timestamp\'] = timestamp self._purge_expired(db) def get(self, key): Retrieve a value from the cache. :param key: The key to retrieve (str). :return: The value if key is found and not expired, None otherwise. with dbm.open(self.filename, \'c\') as db: self._purge_expired(db) if key.encode(\'utf-8\') in db: return db[key.encode(\'utf-8\')].decode(\'utf-8\') else: return None def remove(self, key): Remove a key from the cache. :param key: The key to be removed (str). with dbm.open(self.filename, \'c\') as db: try: del db[key.encode(\'utf-8\')] del db[key.encode(\'utf-8\') + b\'_timestamp\'] except KeyError: pass def _purge_expired(self, db): Purge expired entries from the cache based on the current time. current_time = time.time() keys_to_delete = [key for key in db.keys() if key.endswith(b\'_timestamp\')] for timestamp_key in keys_to_delete: key = timestamp_key[:-10] if current_time - float(db[timestamp_key]) > self.expiration_time: del db[key] del db[timestamp_key]"},{"question":"# Semi-Supervised Learning with Scikit-learn Objective You are tasked with implementing a semi-supervised learning model using the `scikit-learn` library. Specifically, you will use the `SelfTrainingClassifier` to train a classifier on a dataset containing both labeled and unlabeled data. Problem Statement Implement a Python function named `semi_supervised_training` that takes in labeled and unlabeled training data and returns a trained semi-supervised model. You will additionally predict the labels for a test dataset. Function Signature ```python from sklearn.base import BaseEstimator def semi_supervised_training( X_labeled: np.ndarray, y_labeled: np.ndarray, X_unlabeled: np.ndarray, X_test: np.ndarray, base_estimator: BaseEstimator, threshold: float = 0.75, max_iter: int = 10) -> np.ndarray: Trains a semi-supervised model using the SelfTrainingClassifier. Parameters: - X_labeled (np.ndarray): The labeled input data. - y_labeled (np.ndarray): The labels for the labeled input data (can contain -1 for unlabeled). - X_unlabeled (np.ndarray): The unlabeled input data. - X_test (np.ndarray): The test input data. - base_estimator (BaseEstimator): A scikit-learn estimator that implements `predict_proba`. - threshold (float): A threshold for the predict_proba method to include an unlabeled sample. - max_iter (int): Maximum number of iterations for self-training. Returns: - np.ndarray: The predicted labels for `X_test`. pass ``` Input - `X_labeled`: A numpy array of shape `(n_labeled_samples, n_features)` containing the labeled data. - `y_labeled`: A numpy array of shape `(n_labeled_samples,)` containing the labels for the labeled data. Unlabeled data points should have the label `-1`. - `X_unlabeled`: A numpy array of shape `(n_unlabeled_samples, n_features)` containing the unlabeled data. - `X_test`: A numpy array of shape `(n_test_samples, n_features)` containing the test data on which the trained model will be evaluated. - `base_estimator`: An instance of a scikit-learn estimator with a `predict_proba` method. - `threshold`: A float representing the probability threshold to label an unlabeled sample. Defaults to 0.75. - `max_iter`: An integer specifying the maximum number of iterations for self-training. Defaults to 10. Output - Returns a numpy array of shape `(n_test_samples,)` containing the predicted labels for the test data. Constraints - The `base_estimator` must implement the `predict_proba` method. - The function must handle both labeled and unlabeled data correctly, utilizing the `SelfTrainingClassifier` from `sklearn.semi_supervised`. - Performance should be considered for larger datasets, so efficient memory handling is necessary. Example Usage ```python import numpy as np from sklearn.svm import SVC # Sample dataset X_labeled = np.array([[1, 2], [3, 4], [5, 6]]) y_labeled = np.array([0, 1, -1]) # One unlabeled data point X_unlabeled = np.array([[7, 8]]) X_test = np.array([[2, 3], [6, 7]]) # Define a base estimator base_estimator = SVC(probability=True) # Call the function predicted_labels = semi_supervised_training( X_labeled, y_labeled, X_unlabeled, X_test, base_estimator, threshold=0.75, max_iter=10 ) print(predicted_labels) ``` Your implementation should ensure that the given semi-supervised model is properly trained and able to predict the labels for the test dataset correctly.","solution":"import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.base import BaseEstimator def semi_supervised_training( X_labeled: np.ndarray, y_labeled: np.ndarray, X_unlabeled: np.ndarray, X_test: np.ndarray, base_estimator: BaseEstimator, threshold: float = 0.75, max_iter: int = 10) -> np.ndarray: Trains a semi-supervised model using the SelfTrainingClassifier. Parameters: - X_labeled (np.ndarray): The labeled input data. - y_labeled (np.ndarray): The labels for the labeled input data (can contain -1 for unlabeled). - X_unlabeled (np.ndarray): The unlabeled input data. - X_test (np.ndarray): The test input data. - base_estimator (BaseEstimator): A scikit-learn estimator that implements `predict_proba`. - threshold (float): A threshold for the predict_proba method to include an unlabeled sample. - max_iter (int): Maximum number of iterations for self-training. Returns: - np.ndarray: The predicted labels for `X_test`. # Combine labeled and unlabeled data X_combined = np.vstack((X_labeled, X_unlabeled)) # Combine labels, using -1 for unlabeled data y_combined = np.concatenate((y_labeled, -1 * np.ones(X_unlabeled.shape[0], dtype=int))) # Initialize SelfTrainingClassifier self_training_model = SelfTrainingClassifier(base_estimator, threshold=threshold, max_iter=max_iter) # Fit the model to the combined data self_training_model.fit(X_combined, y_combined) # Predict the labels for X_test y_pred = self_training_model.predict(X_test) return y_pred"},{"question":"# Argument Parsing and Value Building in Python Python provides several utility functions that are often used for parsing arguments and constructing values, particularly when integrating with C code. Problem Statement You are tasked with creating a function named `parse_and_format` that accepts a list of arguments. These arguments can be strings, integers, or floating-point numbers, and you need to format these arguments into a structured string. 1. If the argument is a string containing only numeric characters, it should be converted into its respective integer. 2. If the argument is a string containing both alphabetic and numeric characters, it should be left as a string. 3. If the argument is an integer or float, it should remain unchanged. 4. The function should then convert these arguments into a formatted string where each element is separated by a comma followed by a space. # Function Signature ```python def parse_and_format(args: List[Union[str, int, float]]) -> str: pass ``` # Input - `args` (List[Union[str, int, float]]): A list of arguments that are either strings, integers, or floating point numbers. # Output - Returns a single formatted string with each element separated by a comma followed by a space. # Examples - `parse_and_format([\\"123\\", \\"abc123\\", 42, 3.14])` returns `\\"123, abc123, 42, 3.14\\"` - `parse_and_format([\\"456\\", 78, \\"xyz789\\", 9.81])` returns `\\"456, xyz789, 78, 9.81\\"` # Constraints - You may use the `str`, `int`, `float` types and their methods. Other than that, standard libraries like `re` for regular expressions can be used for string operations. - The function should handle up to 1000 elements in the input list efficiently. Additional Notes - Think carefully about how to manage type conversions and ensure correctness. - Take into account edge cases, such as strings that are purely numeric versus those that contain both numbers and letters. - Consider performance implications with increasing size of input.","solution":"from typing import List, Union def parse_and_format(args: List[Union[str, int, float]]) -> str: def convert_arg(arg): if isinstance(arg, str): if arg.isdigit(): return int(arg) else: return arg return arg converted_args = [str(convert_arg(arg)) for arg in args] return \', \'.join(converted_args)"},{"question":"PairGrid Customization Challenge # Objective: Demonstrate your understanding of seaborn\'s `PairGrid` by customizing plots to visualize relationships in a given dataset comprehensively. # Problem Statement Given the `penguins` dataset from seaborn, create a customized pair grid to analyze relationships among the following variables: - `body_mass_g` - `bill_length_mm` - `flipper_length_mm` Each subplot should be carefully customized using the steps outlined below. # Steps: 1. **Loading the dataset:** - Load the `penguins` dataset from seaborn. 2. **Creating the PairGrid:** - Initialize a `PairGrid` instance with the `penguins` dataset. Use the columns `body_mass_g`, `bill_length_mm`, and `flipper_length_mm`. 3. **Plotting on Diagonal:** - On the diagonal of the grid, plot the marginal distributions of each variable using a `kdeplot`. Ensure that the density plots are distinguishable and provide appropriate titles/labels. 4. **Plotting on Off-diagonal:** - On the off-diagonal, plot scatter plots to visualize the pairwise relationships among the variables. - Use `hue` to differentiate the data points based on the species of the penguins. 5. **Upper and Lower Triangles:** - On the upper triangle of the off-diagonal, use scatter plots. - On the lower triangle of the off-diagonal, use `kdeplot`. - Ensure that the scatter plots and kde plots are customized (e.g., colors, markers) to enhance clarity. 6. **Legend:** - Add a legend to the plot that clearly indicates which species each color represents. # Expected Input: There are no input parameters for your function; you need to load the dataset and manipulate it as described directly within the function. # Expected Output: A single plotted figure, displayed inline if running in a Jupyter notebook, or saved to an image file if running in a script. # Constraints: - Use the seaborn library for plotting. - Ensure readability of all plot elements (axes labels, legend, titles). - Performance should not be an issue given the typical size of the `penguins` dataset. # Example Function Signature (Python): ```python import seaborn as sns import matplotlib.pyplot as plt def customize_pair_grid(): # Your implementation here pass # Example of calling the function customize_pair_grid() ``` **Notes:** - The function should include inline comments to explain each significant step. - Ensure the output plot is eye-catching and informative, suitable for presentation in a data analysis context.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_pair_grid(): # Load the \'penguins\' dataset from seaborn penguins = sns.load_dataset(\\"penguins\\") # Initialize a PairGrid instance with the specified columns g = sns.PairGrid(penguins, vars=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"flipper_length_mm\\"], hue=\\"species\\") # Use kdeplot on the diagonal g.map_diag(sns.kdeplot) # Use scatter plot on the upper triangle g.map_upper(sns.scatterplot) # Use kdeplot on the lower triangle g.map_lower(sns.kdeplot) # Add the legend g.add_legend() # Show plot plt.show()"},{"question":"**Question: Implementing and Evaluating Decision Tree Models with scikit-learn** In this assessment, you will demonstrate your understanding of decision trees using scikit-learn. You are required to implement both a classification and a regression decision tree model, handle missing values in the data, and tune hyperparameters to avoid overfitting. # Part 1: Classification Task You are given a dataset with missing values. Your task is to implement a decision tree classifier to classify the dataset. **Requirements:** 1. Load the dataset from the provided CSV file (`classification_data.csv`). Assume it has features in columns `X1, X2,...,Xn` and the target labels in the column `Y`. 2. Preprocess the dataset to handle missing values. You can: - Impute missing values using the mean or median of the columns. - Alternatively, use scikit-learn’s built-in support for missing values in decision trees. 3. Implement a `DecisionTreeClassifier` using scikit-learn. 4. Split the dataset into training and testing sets using an 80-20 split. 5. Train the model on the training set. 6. Tune hyperparameters to avoid overfitting (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`, and `ccp_alpha`). 7. Evaluate the model\'s performance on the testing set using accuracy and confusion matrix. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Step 1: Load the dataset data = pd.read_csv(\'classification_data.csv\') # Step 2: Preprocess the dataset to handle missing values # Step 3: Implement DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=0) # Step 4: Split the dataset X = data.drop(\'Y\', axis=1) y = data[\'Y\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 5: Train the model # Step 6: Tune hyperparameters # Step 7: Evaluate the model\'s performance ``` # Part 2: Regression Task You are given another dataset with missing values. Your task is to implement a decision tree regressor to predict the target variable. **Requirements:** 1. Load the dataset from the provided CSV file (`regression_data.csv`). Assume it has features in columns `X1, X2,...,Xn` and the target variable in the column `Y`. 2. Preprocess the dataset to handle missing values. You can: - Impute missing values using the mean or median of the columns. - Alternatively, use scikit-learn’s built-in support for missing values in decision trees. 3. Implement a `DecisionTreeRegressor` using scikit-learn. 4. Split the dataset into training and testing sets using an 80-20 split. 5. Train the model on the training set. 6. Tune hyperparameters to avoid overfitting (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`, and `ccp_alpha`). 7. Evaluate the model\'s performance on the testing set using mean squared error and a plot of actual vs. predicted values. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Step 1: Load the dataset data = pd.read_csv(\'regression_data.csv\') # Step 2: Preprocess the dataset to handle missing values # Step 3: Implement DecisionTreeRegressor reg = DecisionTreeRegressor(random_state=0) # Step 4: Split the dataset X = data.drop(\'Y\', axis=1) y = data[\'Y\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 5: Train the model # Step 6: Tune hyperparameters # Step 7: Evaluate the model\'s performance ``` **Notes:** - Ensure you document your code and include comments explaining each step. - Feel free to use any other preprocessing or evaluation techniques as necessary. - Your final submission should include a Jupyter Notebook or Python script with all the required implementations and evaluations.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error from sklearn.impute import SimpleImputer import matplotlib.pyplot as plt # Classification Task def decision_tree_classification(csv_file): # Load the dataset data = pd.read_csv(csv_file) # Separate features and target X = data.drop(\'Y\', axis=1) y = data[\'Y\'] # Handle missing values using mean imputation imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Define the model with hyperparameters clf = DecisionTreeClassifier(max_depth=5, min_samples_split=4, min_samples_leaf=2, random_state=0) # Train the model clf.fit(X_train, y_train) # Predict and evaluate y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix # Regression Task def decision_tree_regression(csv_file): # Load the dataset data = pd.read_csv(csv_file) # Separate features and target X = data.drop(\'Y\', axis=1) y = data[\'Y\'] # Handle missing values using mean imputation imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Define the model with hyperparameters reg = DecisionTreeRegressor(max_depth=5, min_samples_split=4, min_samples_leaf=2, random_state=0) # Train the model reg.fit(X_train, y_train) # Predict and evaluate y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Plot actual vs predicted values plt.scatter(y_test, y_pred) plt.xlabel(\'Actual Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Actual vs Predicted Values\') plt.show() return mse"},{"question":"Coding Assessment Question # Objective: The goal of this assessment is to evaluate your understanding of the `torch.__future__` module, specifically how to use its functions to control module parameter behavior during model conversion in PyTorch. # Task: You are required to implement a function that will test the effects of `set_overwrite_module_params_on_conversion` and `set_swap_module_params_on_conversion` on a simple neural network model. The function should: 1. Create a basic neural network model using `torch.nn.Module`. 2. Apply settings for overwriting and swapping module parameters using the functions from `torch.__future__`. 3. Convert the model while respecting the settings applied. 4. Report the changes in the model parameters based on the settings. # Input: - None. # Output: - A dictionary with the following keys and their respective outputs: - `\\"initial_parameters\\"`: The initial parameters of the model before conversion. - `\\"parameters_after_overwrite\\"`: The parameters of the model after applying overwrite settings and conversion. - `\\"parameters_after_swap\\"`: The parameters of the model after applying swap settings and conversion. # Constraints: - You must use the functions from `torch.__future__` for setting and getting behaviors. - The conversion process should be simulated (no actual conversion framework is required but simulate the effects). # Example: You may consider a simple neural network model like this as your base model: ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x ``` # Solution Template: Your solution should follow this structure: ```python import torch import torch.nn as nn import torch.__future__ class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x def test_parameter_behavior(): # Step 1: Create the model model = SimpleModel() # Step 2: Capture initial parameters initial_parameters = {name: param.clone() for name, param in model.state_dict().items()} # Step 3: Set overwrite behavior and convert (simulate) torch.__future__.set_overwrite_module_params_on_conversion(True) # Here you would simulate conversion based on overwrite setting parameters_after_overwrite = {name: param.clone() for name, param in model.state_dict().items()} # Step 4: Set swap behavior and convert (simulate) torch.__future__.set_swap_module_params_on_conversion(True) # Here you would simulate conversion based on swap setting parameters_after_swap = {name: param.clone() for name, param in model.state_dict().items()} # Step 5: Return the results in a dictionary return { \\"initial_parameters\\": initial_parameters, \\"parameters_after_overwrite\\": parameters_after_overwrite, \\"parameters_after_swap\\": parameters_after_swap } # Example usage print(test_parameter_behavior()) ``` Ensure that your function accurately reflects the changes in parameters based on the settings applied, even though the actual conversion process is simulated.","solution":"import torch import torch.nn as nn import torch.__future__ class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x def test_parameter_behavior(): # Step 1: Create the model model = SimpleModel() # Step 2: Capture initial parameters initial_parameters = {name: param.clone() for name, param in model.state_dict().items()} # Step 3: Set overwrite behavior and simulate parameter change torch.__future__.set_overwrite_module_params_on_conversion(True) for param in model.parameters(): param.data.add_(1) # Simulating parameter overwrite parameters_after_overwrite = {name: param.clone() for name, param in model.state_dict().items()} # Reset model to original for name, param in model.named_parameters(): param.data.copy_(initial_parameters[name].data) # Step 4: Set swap behavior and simulate parameter change torch.__future__.set_swap_module_params_on_conversion(True) for param in model.parameters(): temp = param.data.clone() param.data.copy_(-param.data) # Simulating parameter swap temp = param.data.clone() parameters_after_swap = {name: param.clone() for name, param in model.state_dict().items()} # Step 5: Return the results in a dictionary return { \\"initial_parameters\\": initial_parameters, \\"parameters_after_overwrite\\": parameters_after_overwrite, \\"parameters_after_swap\\": parameters_after_swap }"},{"question":"# Question: Advanced Visualization of Error Bars using Seaborn **Objective:** Your task is to visualize various types of error bars using the Seaborn package. This will assess your understanding of Seaborn\'s error bar functionality and your ability to customize and organize plots efficiently. **Requirements:** 1. **Create a dataset**: - Generate a dataset containing 200 random values from a normal distribution with a mean of 0 and a standard deviation of 1. 2. **Implement the following visualizations**: - Create a 2x2 grid of plots with shared x and y axes. - Each subplot should represent a different type of error bar: Standard Deviation, Standard Error, Percentile Interval, and Confidence Interval. - Customize each subplot\'s appearance (e.g., different colors, markers, and titles). 3. **Specifics for each subplot**: - **Top-Left (Standard Deviation)**: - Error bars should represent +/- 1 standard deviation. - **Top-Right (Standard Error)**: - Error bars should represent +/- 1 standard error. - **Bottom-Left (Percentile Interval)**: - Error bars should represent a 50% percentile interval. - **Bottom-Right (Confidence Interval)**: - Error bars should represent a 95% confidence interval with bootstrapping (`n_boot` = 1000). 4. **Performance**: - Ensure that the plotting is efficient and executes within a reasonable amount of time for the number of iterations specified. 5. **Code Structure**: - Your implementation should be modular, with functions defined for generating the data, creating individual plots, and assembling the plot grid. **Input:** No input is required for this task. **Output:** A single matplotlib figure with the 2x2 grid of plots should be displayed, showcasing the different types of error bars. **Constraints:** - You should only use the following packages: numpy, pandas, seaborn, matplotlib. - The code should be written in Python 3. ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Function to generate a dataset def generate_data(seed=0, size=200): np.random.seed(seed) data = np.random.normal(loc=0, scale=1, size=size) return data # Function to create individual plots def create_plot(ax, data, errorbar_type): sns.pointplot(x=data, errorbar=errorbar_type, capsize=.3, ax=ax) sns.stripplot(x=data, jitter=.3, color=\'r\', size=2.5, ax=ax) ax.set_xlim(-3, 3) ax.set_ylim(-0.2, 0.4) # Function to assemble the plot grid def plot_errorbars(): data = generate_data() fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharex=True, sharey=True) plot_configs = [ (\\"Standard Deviation\\", \\"sd\\"), (\\"Standard Error\\", \\"se\\"), (\\"Percentile Interval (50%)\\", (\\"pi\\", 50)), (\\"Confidence Interval (95%)\\", \\"ci\\") ] for ax, (title, err_type) in zip(axs.flat, plot_configs): create_plot(ax, data, err_type) ax.set_title(title) fig.tight_layout() plt.show() plot_errorbars() ``` **Expected Output:** A 2x2 grid of plots, each showcasing a different type of error bar, should be displayed, allowing for visual comparison between the methods.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Function to generate a dataset def generate_data(seed=0, size=200): np.random.seed(seed) data = np.random.normal(loc=0, scale=1, size=size) return data # Function to create individual plots def create_plot(ax, data, errorbar_type): sns.pointplot(y=data, errorbar=errorbar_type, capsize=.3, ax=ax, markers=\'o\', color=\'b\') sns.stripplot(y=data, jitter=.3, color=\'r\', size=2.5, ax=ax) ax.set_ylim(-3, 3) # Function to assemble the plot grid def plot_errorbars(): data = generate_data() fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharex=True, sharey=True) plot_configs = [ (\\"Standard Deviation\\", \\"sd\\"), (\\"Standard Error\\", \\"se\\"), (\\"Percentile Interval (50%)\\", (\\"pi\\", 50)), (\\"Confidence Interval (95%)\\", \\"ci\\") ] for ax, (title, err_type) in zip(axs.flat, plot_configs): create_plot(ax, data, err_type) ax.set_title(title) fig.tight_layout() plt.show() # Generate plot plot_errorbars()"},{"question":"# **Binary Data Manipulation and Encoding in Python** Objective: Design a function in Python that takes in a list of integers, packs them into a binary format, then encodes the binary data using a specified encoding format. Finally, decode the encoded binary data and unpack it back into a list of integers. This will demonstrate your understanding of both the `struct` and `codecs` modules. Details: 1. Implement a function `binary_data_manipulation(integer_list, encoding_format)` that performs the following steps: 2. **Packing Integers into Binary Data:** - Use the `struct` module to pack the list of integers into a format specified by format string `\'!I\'` (Network order, unsigned int). - The `integer_list` is guaranteed to contain only positive integers less than `2**32`. 3. **Encoding Binary Data:** - Encode the resulting binary data into the specified encoding format. Possible values for `encoding_format` include: `\'utf-8\'`, `\'utf-16\'`, `\'base64\'`, and `\'hex\'`. - Use the appropriate method from the `codecs` module to perform this encoding. 4. **Decoding Binary Data:** - Decode the encoded binary data back into its binary format. 5. **Unpacking Binary Data:** - Use the `struct` module again to unpack the binary data back into the list of integers. Input: - `integer_list`: List of positive integers `[int]`. (e.g., `[1, 65535, 123456]`) - `encoding_format`: A string representing the encoding format. Possible values are `\'utf-8\'`, `\'utf-16\'`, `\'base64\'`, `\'hex\'`. Output: - Return the list of integers after performing the above transformations. The output list should be identical to the input `integer_list` if all transformations are executed correctly. Constraints: - The length of `integer_list` will be between 1 and 100, inclusive. - Each integer in `integer_list` will be in the range `[0, 2**32 - 1]`. Example Usage: ```python def binary_data_manipulation(integer_list, encoding_format): import struct import base64 import codecs # Pack integers into binary data using struct packed_data = b\\"\\".join(struct.pack(\'!I\', x) for x in integer_list) # Encode binary data using specified encoding format if encoding_format == \'base64\': encoded_data = base64.b64encode(packed_data) elif encoding_format == \'hex\': encoded_data = codecs.encode(packed_data, \'hex\') else: encoded_data = packed_data.decode(encoding_format).encode(encoding_format) # Decode binary data if encoding_format == \'base64\': decoded_data = base64.b64decode(encoded_data) elif encoding_format == \'hex\': decoded_data = codecs.decode(encoded_data, \'hex\') else: decoded_data = encoded_data.decode(encoding_format).encode(encoding_format) # Unpack binary data back into integers unpacked_data = [struct.unpack(\'!I\', decoded_data[i:i+4])[0] for i in range(0, len(decoded_data), 4)] return unpacked_data # Example Usage: print(binary_data_manipulation([1, 65535, 123456], \'base64\')) # Output: [1, 65535, 123456] ``` Performance Consideration: Ensure that the function efficiently handles up to 100 integers without significant performance degradation. This question assesses the student’s ability to: - Work with the `struct` module for packing and unpacking binary data. - Use the `codecs` module or other encoding libraries for encoding and decoding binary data. - Combine these operations into a coherent function that maintains data integrity across the transformations.","solution":"import struct import base64 import codecs def binary_data_manipulation(integer_list, encoding_format): # Pack integers into binary data using struct packed_data = b\'\'.join(struct.pack(\'!I\', x) for x in integer_list) # Encode binary data using the specified encoding format if encoding_format == \'base64\': encoded_data = base64.b64encode(packed_data) elif encoding_format == \'hex\': encoded_data = codecs.encode(packed_data, \'hex\') elif encoding_format == \'utf-8\': encoded_data = packed_data.decode(\'latin1\').encode(\'utf-8\') elif encoding_format == \'utf-16\': encoded_data = packed_data.decode(\'latin1\').encode(\'utf-16\') else: raise ValueError(\\"Unsupported encoding format\\") # Decode binary data back to original form if encoding_format == \'base64\': decoded_data = base64.b64decode(encoded_data) elif encoding_format == \'hex\': decoded_data = codecs.decode(encoded_data, \'hex\') elif encoding_format == \'utf-8\': decoded_data = encoded_data.decode(\'utf-8\').encode(\'latin1\') elif encoding_format == \'utf-16\': decoded_data = encoded_data.decode(\'utf-16\').encode(\'latin1\') # Unpack binary data back into integers unpacked_data = [struct.unpack(\'!I\', decoded_data[i:i+4])[0] for i in range(0, len(decoded_data), 4)] return unpacked_data"},{"question":"# Comprehensive DateTime Manipulation and Extraction in Python Objective: In this assignment, you need to write Python code that uses the features of the `datetime` module to perform various operations involving the creation, manipulation, and extraction of datetime objects. The solution must demonstrate a deep understanding of each aspect of the datetime objects provided in the documentation. Task Requirements: 1. **Creation of DateTime Objects**: Write a function named `create_datetime` that accepts parameters for year, month, day, hour, minute, second, and microsecond, and returns a `datetime.datetime` object representing this date and time. 2. **Extraction of DateTime Fields**: Write a function named `extract_fields` that accepts a `datetime.datetime` object and returns a dictionary containing the year, month, day, hour, minute, second, and microsecond of the datetime. 3. **Comparison of DateTime Objects**: Write a function named `compare_datetimes` that accepts two `datetime.datetime` objects and returns a string indicating whether the first datetime is \'earlier\', \'same\', or \'later\' than the second datetime. 4. **Custom Time Zone**: Write a function named `create_custom_timezone` that accepts a number of hours and minutes and returns a `datetime.timezone` object representing this custom fixed offset timezone. Function Signatures: - `create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime` - `extract_fields(dt: datetime.datetime) -> dict` - `compare_datetimes(dt1: datetime.datetime, dt2: datetime.datetime) -> str` - `create_custom_timezone(hours: int, minutes: int) -> datetime.timezone` Example Usage: ```python # Example usage for create_datetime dt1 = create_datetime(2023, 10, 22, 15, 45, 30, 500000) print(dt1) # Output: 2023-10-22 15:45:30.500000 # Example usage for extract_fields fields = extract_fields(dt1) print(fields) # Output: {\'year\': 2023, \'month\': 10, \'day\': 22, \'hour\': 15, \'minute\': 45, \'second\': 30, \'microsecond\': 500000} # Example usage for compare_datetimes dt2 = create_datetime(2023, 10, 22, 15, 46, 30, 500000) comparison = compare_datetimes(dt1, dt2) print(comparison) # Output: \'earlier\' # Example usage for create_custom_timezone timezone = create_custom_timezone(5, 30) print(timezone) # Output: UTC+05:30 ``` Constraints: - **Input Range**: Ensure the input values for years, months, days, hours, minutes, seconds, and microseconds are within valid ranges as per standard datetime constraints. Notes: - You may use the built-in `datetime` module to aid in implementation. - Ensure your solution is efficient and follows best practices for code readability and structure.","solution":"import datetime def create_datetime(year, month, day, hour, minute, second, microsecond): Creates a datetime object with the specified parameters. Parameters: - year, month, day, hour, minute, second, microsecond: all integers representing the respective parts of the datetime. Returns: - A datetime.datetime object representing the specified date and time. return datetime.datetime(year, month, day, hour, minute, second, microsecond) def extract_fields(dt): Extracts the fields from a datetime object and returns them in a dictionary. Parameters: - dt: A datetime.datetime object. Returns: - A dictionary with keys \'year\', \'month\', \'day\', \'hour\', \'minute\', \'second\', and \'microsecond\' representing the respective parts of the datetime. return { \'year\': dt.year, \'month\': dt.month, \'day\': dt.day, \'hour\': dt.hour, \'minute\': dt.minute, \'second\': dt.second, \'microsecond\': dt.microsecond } def compare_datetimes(dt1, dt2): Compares two datetime objects. Parameters: - dt1, dt2: Two datetime.datetime objects. Returns: - A string indicating whether dt1 is \'earlier\', \'same\', or \'later\' than dt2. if dt1 < dt2: return \'earlier\' elif dt1 > dt2: return \'later\' else: return \'same\' def create_custom_timezone(hours, minutes): Creates a custom timezone with the specified offset. Parameters: - hours: Integer representing the hours of the offset. - minutes: Integer representing the minutes of the offset. Returns: - A datetime.timezone object representing the specified custom timezone. return datetime.timezone(datetime.timedelta(hours=hours, minutes=minutes))"},{"question":"**Objective**: To assess the understanding and application of the `inspect` module in Python for introspecting live objects. Problem Description You are required to write a Python function called `function_summary(module_name: str) -> dict` that takes the name of a Python module as input and returns a dictionary containing a summary of all functions defined in that module. The summary should provide the following details for each function: 1. Function name. 2. Source code of the function. 3. Documentation string (if any). 4. The signature of the function. 5. Whether the function is a generator function or coroutine function. Function Signature ```python def function_summary(module_name: str) -> dict: ... ``` Input - **module_name** (str): The name of the module to introspect. Output - **dict**: A dictionary where keys are the function names, and values are another dictionary with the keys: - `source`: A string of the source code of the function. - `doc`: A string of the documentation string of the function. - `signature`: A string of the function\'s signature. - `is_generator`: A boolean indicating if the function is a generator function. - `is_coroutine`: A boolean indicating if the function is a coroutine function. Constraints - The module you provide should be importable. - If the module is not found or not importable, the function should raise an `ImportError`. Example ```python # Assuming we have a module named `example_module` with functions defined as follows: # def add(a, b): # # Adds two numbers. # # return a + b # async def async_add(a, b): # # Adds two numbers asynchronously. # # return a + b >>> function_summary(\\"example_module\\") { \\"add\\": { \\"source\\": \\"def add(a, b):n return a + b\\", \\"doc\\": \\"Adds two numbers.\\", \\"signature\\": \\"(a, b)\\", \\"is_generator\\": False, \\"is_coroutine\\": False }, \\"async_add\\": { \\"source\\": \\"async def async_add(a, b):n return a + b\\", \\"doc\\": \\"Adds two numbers asynchronously.\\", \\"signature\\": \\"(a, b)\\", \\"is_generator\\": False, \\"is_coroutine\\": True } } ``` Notes - You are allowed to use any function from the `inspect` module. - Ensure you handle cases where functions might not have a documentation string or source code available. Hints - Use `inspect.isfunction` to filter functions within the module. - Use `inspect.getsource`, `inspect.getdoc`, `inspect.signature`, `inspect.isgeneratorfunction`, and `inspect.iscoroutinefunction` to retrieve the respective details.","solution":"import inspect import importlib def function_summary(module_name: str) -> dict: Returns a summary of all functions defined in the module specified by `module_name`. Args: - module_name (str): The name of the module to introspect. Returns: - dict: A dictionary containing details of the functions. try: module = importlib.import_module(module_name) except ImportError: raise ImportError(f\\"Module named \'{module_name}\' not found or cannot be imported\\") summary = {} for name, obj in inspect.getmembers(module, inspect.isfunction): func_info = { \'source\': inspect.getsource(obj) if inspect.isfunction(obj) else \'\', \'doc\': inspect.getdoc(obj) if inspect.getdoc(obj) else \'\', \'signature\': str(inspect.signature(obj)), \'is_generator\': inspect.isgeneratorfunction(obj), \'is_coroutine\': inspect.iscoroutinefunction(obj) } summary[name] = func_info return summary"},{"question":"You are tasked with writing a function to analyze a list of IP addresses and determine their characteristics using the `ipaddress` module. # Function Details: 1. **Function Name**: `analyze_ip_addresses` 2. **Input**: A list of strings, each representing an IP address or a network (with prefix). 3. **Output**: A dictionary containing the analysis of each input. The keys of the dictionary should be the input strings, and the values should be another dictionary with the following structure: - `type`: A string indicating whether it is an `IPv4Address`, `IPv6Address`, `IPv4Network` or `IPv6Network`. - `version`: IP version (`4` or `6`). - `num_addresses`: (Only for network inputs) The number of addresses in the network. - `netmask`: (Only for network inputs) The netmask of the network. - `hostmask`: (Only for network inputs) The hostmask of the network. # Constraints: - Input strings are guaranteed to be valid IP addresses or networks. - Use the `ipaddress` module for all IP-related operations. # Example: ```python def analyze_ip_addresses(ip_list): analysis = {} # Your code here return analysis # Example usage: ip_list = [ \'192.0.2.1\', \'2001:db8::1\', \'192.0.2.0/24\', \'2001:db8::/96\' ] result = analyze_ip_addresses(ip_list) print(result) ``` **Expected Output**: ```python { \'192.0.2.1\': { \'type\': \'IPv4Address\', \'version\': 4, }, \'2001:db8::1\': { \'type\': \'IPv6Address\', \'version\': 6, }, \'192.0.2.0/24\': { \'type\': \'IPv4Network\', \'version\': 4, \'num_addresses\': 256, \'netmask\': \'255.255.255.0\', \'hostmask\': \'0.0.0.255\', }, \'2001:db8::/96\': { \'type\': \'IPv6Network\', \'version\': 6, \'num_addresses\': 4294967296, \'netmask\': \'ffff:ffff:ffff:ffff:ffff:ffff::\', \'hostmask\': \'::ffff:ffff\', } } ```","solution":"import ipaddress def analyze_ip_addresses(ip_list): analysis = {} for ip in ip_list: try: obj = ipaddress.ip_address(ip) if obj.version == 4: analysis[ip] = { \'type\': \'IPv4Address\', \'version\': 4 } else: analysis[ip] = { \'type\': \'IPv6Address\', \'version\': 6 } except ValueError: obj = ipaddress.ip_network(ip, strict=False) if obj.version == 4: analysis[ip] = { \'type\': \'IPv4Network\', \'version\': 4, \'num_addresses\': obj.num_addresses, \'netmask\': str(obj.netmask), \'hostmask\': str(obj.hostmask) } else: analysis[ip] = { \'type\': \'IPv6Network\', \'version\': 6, \'num_addresses\': obj.num_addresses, \'netmask\': str(obj.netmask), \'hostmask\': str(obj.hostmask) } return analysis"},{"question":"# Advanced Seaborn KDEPlot Assessment Problem Statement You are required to create a series of advanced kernel density estimation (KDE) plots using Seaborn\'s `kdeplot` function. Your task is to implement a function that generates and customizes KDE plots based on given criteria. Function Signature ```python def advanced_kdeplots(dataset: str, col1: str, col2: Optional[str] = None, hue: Optional[str] = None, bw_adjust: float = 1.0, fill: bool = False, levels: int = 10, cmap: str = None, log_scale: bool = False) -> None: pass ``` Input - `dataset` (str): The name of the dataset to be used. Example: `\\"tips\\"`, `\\"iris\\"`, `\\"diamonds\\"`, `\\"geyser\\"`. - `col1` (str): The column name in the dataset to be plotted on the x-axis. Example: `\\"total_bill\\"`, `\\"waiting\\"`, `\\"price\\"`. - `col2` (Optional[str]): An optional column name in the dataset to be plotted on the y-axis. If `None`, a univariate KDE plot is generated. Default is `None`. - `hue` (Optional[str]): An optional column name in the dataset to be used for color coding. If `None`, no hue mapping is applied. Default is `None`. - `bw_adjust` (float): The adjustment factor for the smoothing bandwidth. Default is `1.0`. - `fill` (bool): Whether to fill the area under the KDE curve. Default is `False`. - `levels` (int): Number of contour levels to be drawn if plotting a bivariate distribution. Default is `10`. - `cmap` (str): The name of the colormap to be used. If `None`, the default colormap is used. Default is `None`. - `log_scale` (bool): Whether to apply log scaling to the x-axis (and y-axis if `col2` is provided). Default is `False`. Output - The function does not return any value. It should display the corresponding KDE plot based on the input parameters. Constraints - The function should handle different datasets available in Seaborn: `tips`, `iris`, `diamonds`, `geyser`. - Ensure that the plot correctly applies the given customization parameters (`bw_adjust`, `fill`, `levels`, `cmap`, `log_scale`). Example Usage ```python # Univariate KDE plot with less smoothing and filled area advanced_kdeplots(dataset=\\"tips\\", col1=\\"total_bill\\", bw_adjust=0.5, fill=True) # Bivariate KDE plot with contours, custom colormap, and log scaling advanced_kdeplots(dataset=\\"diamonds\\", col1=\\"carat\\", col2=\\"price\\", levels=20, cmap=\\"viridis\\", log_scale=True) # KDE plot with hue mapping and stacked distributions advanced_kdeplots(dataset=\\"iris\\", col1=\\"sepal_length\\", col2=\\"sepal_width\\", hue=\\"species\\", fill=True) ``` Notes - Use `seaborn` for data visualization and `matplotlib` to fine-tune the plots if necessary. - Load the dataset inside the function using `seaborn.load_dataset`. - Make sure to handle cases where the provided column names, hue, or dataset do not exist within the given dataset by raising appropriate exceptions.","solution":"import seaborn as sns import matplotlib.pyplot as plt from typing import Optional def advanced_kdeplots(dataset: str, col1: str, col2: Optional[str] = None, hue: Optional[str] = None, bw_adjust: float = 1.0, fill: bool = False, levels: int = 10, cmap: str = None, log_scale: bool = False) -> None: Generate and customize KDE plots based on given criteria. Args: dataset (str): The name of the dataset to be used. col1 (str): The column name in the dataset to be plotted on the x-axis. col2 (Optional[str], optional): An optional column name in the dataset to be plotted on the y-axis. Default is None. hue (Optional[str], optional): An optional column name in the dataset to be used for color coding. Default is None. bw_adjust (float, optional): The adjustment factor for the smoothing bandwidth. Default is 1.0. fill (bool, optional): Whether to fill the area under the KDE curve. Default is False. levels (int, optional): Number of contour levels to be drawn if plotting a bivariate distribution. Default is 10. cmap (str, optional): The name of the colormap to be used. Default is None. log_scale (bool, optional): Whether to apply log scaling to the x-axis (and y-axis if col2 is provided). Default is False. Raises: ValueError: If the specified column(s) or dataset do not exist. # Load the dataset try: data = sns.load_dataset(dataset) except: raise ValueError(f\\"Dataset \'{dataset}\' not found in Seaborn\\") if col1 not in data.columns: raise ValueError(f\\"Column \'{col1}\' not found in dataset \'{dataset}\'\\") if col2 and col2 not in data.columns: raise ValueError(f\\"Column \'{col2}\' not found in dataset \'{dataset}\'\\") if hue and hue not in data.columns: raise ValueError(f\\"Column \'{hue}\' not found in dataset \'{dataset}\'\\") # Configure the plot plt.figure() if col2: sns.kdeplot(data=data, x=col1, y=col2, hue=hue, bw_adjust=bw_adjust, fill=fill, levels=levels, cmap=cmap, log_scale=(log_scale, log_scale)) else: sns.kdeplot(data=data, x=col1, hue=hue, bw_adjust=bw_adjust, fill=fill, log_scale=log_scale) # Display the plot plt.show()"},{"question":"# Hashlib-based File Integrity Checker You are requested to implement a Python function that verifies the integrity of multiple files using various hashing algorithms from the `hashlib` module. Your implementation should be able to hash files using a given algorithm, compare them against provided hash values, and report discrepancies. Function Signature ```python def verify_files_integrity(file_paths: list, hash_algorithm: str, expected_hashes: list) -> dict: Given a list of file paths, a hash algorithm, and a list of expected hashes, verify the integrity of the files. Parameters: file_paths (list): A list of string paths to the files to be verified. hash_algorithm (str): The name of the hash algorithm to use (e.g., \'sha256\', \'md5\', \'blake2b\'). expected_hashes (list): A list of expected hash values (hexadecimal strings) for the respective files. Returns: dict: A dictionary where the key is the file path and the value is True if the hash matches, False if it does not. ``` Requirements 1. **Input Constraints**: - `file_paths` should be a list of strings with valid file paths. - `hash_algorithm` should be one of the supported hash algorithms in `hashlib`. - `expected_hashes` should be a list of hexadecimal strings corresponding to the `file_paths`. 2. **Output**: - A dictionary where each key is a file path and the value is a boolean indicating if the calculated hash matches the expected hash. 3. **Performance**: - Efficiently handle large files by reading them in chunks. - Ensure compatibility and thread safety. 4. **Error Handling**: - Raise appropriate exceptions for invalid file paths or unsupported hash algorithms. 5. **Usage Example**: ```python file_paths = [\'/path/to/file1.txt\', \'/path/to/file2.txt\'] hash_algorithm = \'sha256\' expected_hashes = [ \'5d41402abc4b2a76b9719d911017c592\', \'7d793037a0760186574b0282f2f435e7\' ] result = verify_files_integrity(file_paths, hash_algorithm, expected_hashes) print(result) # Expected Output (based on actual file contents): # { \'/path/to/file1.txt\': False, \'/path/to/file2.txt\': True } ``` Implementation Tips - Use the appropriate constructor from `hashlib` for the specified `hash_algorithm` to create a hash object. - Read each file in binary mode and update the hash object with chunks of the file\'s data. - Compare the computed hash (in hexadecimal format) with the provided hash and store the result for each file. - Ensure thread safety using proper locking mechanisms if dealing with multithreading scenarios. Feel free to utilize any necessary helper functions to keep your code modular and organized.","solution":"import hashlib def verify_files_integrity(file_paths: list, hash_algorithm: str, expected_hashes: list) -> dict: Given a list of file paths, a hash algorithm, and a list of expected hashes, verify the integrity of the files. Parameters: file_paths (list): A list of string paths to the files to be verified. hash_algorithm (str): The name of the hash algorithm to use (e.g., \'sha256\', \'md5\', \'blake2b\'). expected_hashes (list): A list of expected hash values (hexadecimal strings) for the respective files. Returns: dict: A dictionary where the key is the file path and the value is True if the hash matches, False if it does not. if hash_algorithm not in hashlib.algorithms_available: raise ValueError(f\\"Unsupported hash algorithm: {hash_algorithm}\\") integrity_check_result = {} for file_path, expected_hash in zip(file_paths, expected_hashes): try: hasher = hashlib.new(hash_algorithm) with open(file_path, \'rb\') as file: while chunk := file.read(8192): hasher.update(chunk) computed_hash = hasher.hexdigest() integrity_check_result[file_path] = computed_hash == expected_hash except Exception as e: integrity_check_result[file_path] = False return integrity_check_result"},{"question":"# Advanced Seaborn Visualization You have been provided with a dataset and your task is to create a multi-faceted plot using the Seaborn library, incorporating various customization options for figure layout and size. By completion, your code should demonstrate your understanding of Seaborn’s plotting capabilities and layout controls. Dataset You will use the popular `penguins` dataset from the seaborn library. Assume it is already loaded into a pandas DataFrame named `penguins`. Task 1. Create a `Plot` object for the `penguins` dataset. 2. Use the `facet` method to create subplots for different species (`species`) and different islands (`island`) from the dataset. 3. Set the overall figure size to be 10x10 (units are in inches). 4. Use the `constrained` layout engine for better spacing adjustment. 5. Control the extent of the plot so that it only occupies 80% of the figure\'s width while maintaining the full height. 6. Display the plot. Expected Output A multi-faceted plot where each subplot corresponds to a specific species and island, properly sized and spaced according to the given specifications. Example Structure Here is a structure to help you get started: ```python import seaborn as sns import seaborn.objects as so import pandas as pd # Assume penguins dataset is already loaded into a DataFrame named penguins # penguins = sns.load_dataset(\\"penguins\\") # Create a Plot object with the penguins dataset p = so.Plot(data=penguins) # Define the layout specifications p = p.layout(size=(10, 10)) # Create facets for each combination of species and island p = p.facet([\\"species\\"], [\\"island\\"]) # Use the constrained layout engine p = p.layout(engine=\\"constrained\\") # Set the plot extent p = p.layout(extent=[0, 0, 0.8, 1]) # Show the plot p.show() ``` Constraints - Ensure that your code runs without any error. - You should not use any libraries other than seaborn and pandas. - The plot dimensions and layout adjustments should be as specified. Tips - Explore the `Plot`, `facet`, `layout`, and `extent` options in the Seaborn objects module to effectively control the appearance of your plot. - Inspect the dataset to understand its structure and how the species and island columns can be used for faceting.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a Plot object with the penguins dataset p = so.Plot(data=penguins) # Define the layout specifications by setting the overall figure size p = p.layout(size=(10, 10)) # Create facets for each combination of species and island p = p.facet(\\"species\\", \\"island\\") # Use the constrained layout engine for better spacing adjustment p = p.layout(engine=\\"constrained\\") # Set the plot extent to occupy 80% of the width while maintaining the full height p = p.layout(extent=[0, 0, 0.8, 1]) # Show the plot p.show()"},{"question":"Objective: The goal of this assignment is to ensure that you understand how to use the `timeit` module to measure the performance of different Python code snippets accurately. You will write a Python function utilizing the `timeit` module and analyze the performance of different implementations of a given functionality. Problem Statement: You are provided with three different implementations of a function that reverses a string. Your task is to write a Python function `compare_reverse_functions` that: 1. Measures the execution time of each implementation using the `timeit` module. 2. Compares the execution times and returns the implementation that is the fastest. Implementations: 1. `reverse_slice`: ```python def reverse_slice(s): return s[::-1] ``` 2. `reverse_loop`: ```python def reverse_loop(s): reversed_string = \\"\\" for char in s: reversed_string = char + reversed_string return reversed_string ``` 3. `reverse_join`: ```python def reverse_join(s): return \'\'.join(reversed(s)) ``` Function Signature: ```python def compare_reverse_functions(s: str) -> str: Compares the performance of three string reverse functions and returns the name of the fastest one. Parameters: - s (str): The input string to be reversed. Returns: - (str): The name of the fastest function (one of \\"reverse_slice\\", \\"reverse_loop\\", \\"reverse_join\\"). ``` Requirements: - Use the `timeit` module to measure execution time. - Ensure that each function is executed at least 100,000 times to get an accurate measurement. - Handle any potential discrepancies or anomalies in timing results by running multiple repetitions and considering the minimum time. - Return the name of the fastest implementation as a string. Example Usage: ```python # Example usage input_string = \\"abcdef\\" fastest_function = compare_reverse_functions(input_string) print(f\\"The fastest implementation is: {fastest_function}\\") ``` Constraints: - The input string `s` will be non-empty and have at most 10^6 characters. - You are not allowed to modify the existing implementations of the reverse functions. Performance Evaluation: Make sure your solution effectively leverages the `timeit` module to provide an accurate comparison of the function performances.","solution":"import timeit def reverse_slice(s): return s[::-1] def reverse_loop(s): reversed_string = \\"\\" for char in s: reversed_string = char + reversed_string return reversed_string def reverse_join(s): return \'\'.join(reversed(s)) def compare_reverse_functions(s: str) -> str: functions = { \\"reverse_slice\\": reverse_slice, \\"reverse_loop\\": reverse_loop, \\"reverse_join\\": reverse_join } # Time each function using timeit num_runs = 100000 times = {} for name, func in functions.items(): timer = timeit.Timer(lambda: func(s)) execution_time = timer.timeit(number=num_runs) times[name] = execution_time # Find the fastest function fastest_function = min(times, key=times.get) return fastest_function"},{"question":"# Question: Web Resource Fetching and Handling with `urllib` You are required to implement a function that fetches content from a given URL using the `urllib` package. The function should handle HTTP GET requests, manage headers, and handle both basic authentication and custom HTTP errors. Your function will be tested against various URLs and must adhere to the specifications below. Function Signature ```python def fetch_url_content(url: str, headers: dict = None, auth: tuple = None) -> str: pass ``` Parameters - `url` (str): The URL to fetch the content from. - `headers` (dict, optional): A dictionary of HTTP headers to send with the request. Defaults to `None`. - `auth` (tuple, optional): A tuple of (`username`, `password`) for basic authentication. Defaults to `None`. Returns - `str`: The content retrieved from the URL. Requirements 1. Fetch the content using an HTTP GET request. 2. If provided, add the headers to the request. 3. If authentication details are provided, use basic authentication. 4. Handle the following exceptions: - `URLError`: Print the reason for the error and return an appropriate message. - `HTTPError`: Print the HTTP error code and the error message, then return an appropriate message. 5. Ensure that the function returns the fetched content as a string if the request is successful. 6. If the URL is redirected, follow the redirection and fetch the final resource. Constraints - Do not use any libraries other than those provided by the Python standard library. - Assume the URLs given will always start with `http://` or `https://`. Example Usage ```python url = \'http://www.example.com\' headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\'} auth = (\'username\', \'password\') result = fetch_url_content(url, headers, auth) print(result) ``` Example Output In case of successful content fetching: ``` \\"<html>...</html>\\" ``` In case of URLError: ``` \\"We failed to reach a server. Reason: ...\\" ``` In case of HTTPError: ``` \\"The server couldn\'t fulfill the request. Error code: ...\\" ``` Implement the `fetch_url_content` function to meet the above specifications.","solution":"from urllib.request import Request, urlopen, HTTPBasicAuthHandler, build_opener, install_opener from urllib.error import URLError, HTTPError from base64 import b64encode def fetch_url_content(url: str, headers: dict = None, auth: tuple = None) -> str: try: # Create a Request object request = Request(url) # Add headers if provided if headers: for key, value in headers.items(): request.add_header(key, value) # Add basic authentication if provided if auth: username, password = auth credentials = b64encode(f\\"{username}:{password}\\".encode(\'utf-8\')).decode(\'utf-8\') request.add_header(\'Authorization\', f\'Basic {credentials}\') # Fetch the URL content with urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\\"The server couldn\'t fulfill the request. Error code: {e.code}, Error message: {e.reason}\\" except URLError as e: return f\\"We failed to reach a server. Reason: {e.reason}\\""},{"question":"Design and implement a Python program that simulates the basic functionality of context variables similar to the `contextvars` module, but without using the actual module. Your task is to: 1. Create a `Context` class that will hold key-value pairs representing context variables. 2. Create a `ContextVar` class to represent individual context variables. 3. Implement methods to set, get, and reset context variables in the `Context` class. 4. Implement a stack-based context manager inside `Context` that allows entering and exiting contexts. # Requirements - **Context Class** - **Attributes**: - `vars`: A dictionary to hold the key-value pairs. - `stack`: A list to manage nested contexts. - **Methods**: - `set_var(name: str, value: any) -> None`: Sets the name to the given value in the context. - `get_var(name: str, default=None) -> any`: Retrieves the value for the given name, returning `default` if the name is not found. - `reset_var(name: str) -> None`: Resets the value of the variable to the one at the start of the last context. - `enter_context` and `exit_context`: Manage pushing and popping context states. - **ContextVar Class** - A simple class with attributes `name` and `default` to store a context variable. # Example Usage ```python ctx = Context() var1 = ContextVar(name=\\"var1\\", default=\\"default1\\") ctx.set_var(var1.name, \\"value1\\") print(ctx.get_var(var1.name)) # Output: \\"value1\\" ctx.enter_context() ctx.set_var(var1.name, \\"value2\\") print(ctx.get_var(var1.name)) # Output: \\"value2\\" ctx.exit_context() print(ctx.get_var(var1.name)) # Output: \\"value1\\" ``` **Constraints**: - Do not use the actual `contextvars` module. - Pay attention to managing variable states correctly when contexts are nested. # Submission Submit your complete Python program with the following function definitions, and ensure it passes the provided example usage.","solution":"class ContextVar: def __init__(self, name, default=None): self.name = name self.default = default class Context: def __init__(self): self.vars = {} self.stack = [] def set_var(self, name, value): self.vars[name] = value def get_var(self, name, default=None): return self.vars.get(name, default) def reset_var(self, name): self.vars.pop(name, None) def enter_context(self): self.stack.append(self.vars.copy()) def exit_context(self): if self.stack: self.vars = self.stack.pop()"},{"question":"**Problem Statement:** You are an engineer at a company that aims to utilize Gaussian Processes for making accurate and reliable predictions based on given datasets. Your task is to implement and evaluate Gaussian Process models for both regression and classification tasks using the `scikit-learn` library. Additionally, you will explore different kernels and demonstrate their effects on the results. # Part 1: Gaussian Process Regression **Task:** 1. Implement a Gaussian Process Regression model using `GaussianProcessRegressor`. 2. Use the provided dataset `data_reg.csv`, where the first column is the feature `X1` and the second column is the target `Y`. 3. Use an RBF kernel with an initial length scale of 1.0. 4. Perform predictions on a test dataset `test_reg.csv` which contains only the `X1` values. 5. Provide the predicted mean and standard deviation for the test dataset. 6. Visualize the results, showing the observed data points, the predicted mean, and the confidence intervals (mean ± 1 standard deviation). **Input Format:** - `data_reg.csv` containing two columns: feature `X1` and target `Y`. - `test_reg.csv` containing one column: feature `X1`. **Output Format:** - Predicted mean and standard deviation for the `test_reg.csv` dataset. - Visualization plot (optional in code but ensure to save the plot as `regression_results.png`). # Part 2: Gaussian Process Classification **Task:** 1. Implement a Gaussian Process Classification model using `GaussianProcessClassifier`. 2. Use the provided dataset `data_class.csv`, where the first two columns are the features `X1`, `X2`, and the third column is the class label `Y`. 3. Use an RBF kernel with an initial length scale of 1.0. 4. Perform predictions on a test dataset `test_class.csv` which contains only the features columns `X1`, `X2`. 5. Provide the predicted class probabilities for the test dataset. 6. Visualize the decision boundaries along with the test points and their predicted class. **Input Format:** - `data_class.csv` containing three columns: features `X1`, `X2`, and class label `Y`. - `test_class.csv` containing two columns: features `X1`, `X2`. **Output Format:** - Predicted class probabilities for the `test_class.csv` dataset. - Visualization plot (optional in code but ensure to save the plot as `classification_results.png`). # Constraints: - Use appropriate hyperparameter optimization (`n_restarts_optimizer=10`). - Ensure reproducibility in your results by setting random seed where necessary. # Evaluation: Your implementation will be evaluated based on the correctness of the model setup, predictions, and visualizations. Ensure your code is well-documented and modular. **Hint:** Refer to the scikit-learn documentation on Gaussian Processes and Kernels for guidance on implementation.","solution":"import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF import matplotlib.pyplot as plt def gaussian_process_regression(train_file, test_file): # Load the training data train_df = pd.read_csv(train_file) X_train = train_df.iloc[:, 0].values.reshape(-1, 1) y_train = train_df.iloc[:, 1].values # Load the test data test_df = pd.read_csv(test_file) X_test = test_df.iloc[:, 0].values.reshape(-1, 1) # Define the kernel kernel = RBF(length_scale=1.0) # Create the Gaussian Process Regressor gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42) # Fit the model gpr.fit(X_train, y_train) # Make predictions y_mean, y_std = gpr.predict(X_test, return_std=True) # Visualization plt.figure() plt.plot(X_train, y_train, \'kx\', label=\\"Training data\\") plt.plot(X_test, y_mean, \'b-\', label=\\"Predicted mean\\") plt.fill_between(X_test.ravel(), y_mean - y_std, y_mean + y_std, alpha=0.5, label=\\"Confidence interval\\") plt.legend() plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.savefig(\'regression_results.png\') plt.close() return y_mean, y_std def gaussian_process_classification(train_file, test_file): # Load the training data train_df = pd.read_csv(train_file) X_train = train_df.iloc[:, :2].values y_train = train_df.iloc[:, 2].values # Load the test data test_df = pd.read_csv(test_file) X_test = test_df.iloc[:, :2].values # Define the kernel kernel = RBF(length_scale=1.0) # Create the Gaussian Process Classifier gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=10, random_state=42) # Fit the model gpc.fit(X_train, y_train) # Make predictions y_proba = gpc.predict_proba(X_test) # Visualization plt.figure() x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) Z = gpc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor=\'k\', s=20, label=\'Training data\') plt.scatter(X_test[:, 0], X_test[:, 1], marker=\'x\', c=\'r\', label=\'Test data\') plt.legend() plt.xlabel(\'X1\') plt.ylabel(\'X2\') plt.savefig(\'classification_results.png\') plt.close() return y_proba"},{"question":"# Question: Advanced URL Handling with `urllib.request` You are required to write a Python function named `fetch_content_via_proxy`, which uses the `urllib.request` module to fetch the content of a given URL. The function should support fetching through an HTTP proxy, adding custom headers, and handle HTTP basic authentication. Function Signature ```python def fetch_content_via_proxy(url: str, proxy_url: str, username: str, password: str, headers: dict) -> str: ``` Input Parameters - `url` (str): The URL to fetch content from. - `proxy_url` (str): The proxy server URL (including protocol and port). - `username` (str): Username for HTTP basic authentication. - `password` (str): Password for HTTP basic authentication. - `headers` (dict): A dictionary of additional headers to include in the request. Output - `str`: The content of the URL as a string. Requirements 1. The function should use the specified proxy server for the request. 2. The function should handle HTTP basic authentication using the given username and password. 3. The function should include the provided headers in the request. 4. The response should be decoded using \'utf-8\' encoding. 5. If fetching the content fails for any reason, the function should raise a `RuntimeError` with an appropriate error message. Example Usage ```python url = \\"http://example.com\\" proxy_url = \\"http://proxy.example.com:8080\\" username = \\"user\\" password = \\"pass\\" headers = { \\"User-Agent\\": \\"CustomUserAgent/1.0\\", \\"Accept\\": \\"text/html\\" } try: content = fetch_content_via_proxy(url, proxy_url, username, password, headers) print(content) except RuntimeError as e: print(f\\"Failed to fetch content: {e}\\") ``` Constraints - Use only the `urllib.request` module for this task. - Ensure proper error handling and resource management (e.g., closing connections).","solution":"import urllib.request import urllib.error import base64 def fetch_content_via_proxy(url: str, proxy_url: str, username: str, password: str, headers: dict) -> str: Fetches the content of a given URL using the specified proxy, including basic authentication and additional headers. # Create a password manager password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() # Add the username and password password_mgr.add_password(None, proxy_url, username, password) # Create the proxy handler proxy_handler = urllib.request.ProxyHandler({ \'http\': proxy_url, \'https\': proxy_url, }) # Create an authentication handler auth_handler = urllib.request.HTTPBasicAuthHandler(password_mgr) # Build the opener with the proxy and auth handlers opener = urllib.request.build_opener(proxy_handler, auth_handler) # Add custom headers if headers: opener.addheaders = [(key, value) for key, value in headers.items()] # Install the opener urllib.request.install_opener(opener) try: # Open the URL with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') return content except urllib.error.URLError as e: raise RuntimeError(f\\"Failed to fetch content: {e.reason}\\") except Exception as e: raise RuntimeError(f\\"An error occurred: {str(e)}\\")"},{"question":"<|Analysis Begin|> The provided documentation discusses built-in constants available in Python, such as `False`, `True`, `None`, `NotImplemented`, `Ellipsis`, and `__debug__`. It also mentions additional constants introduced by the `site` module, namely `quit`, `exit`, `copyright`, `credits`, and `license`. The constants cover a range of functionalities: 1. Boolean values (`True`, `False`) 2. Placeholder values (`None`, `NotImplemented`, `Ellipsis`) 3. Debugging support (`__debug__`) 4. Interactive interpreter shell enhancements (`quit`, `exit`, `copyright`, `credits`, `license`) To create a challenging and comprehensive coding question based on this documentation, we can focus on requiring the implementation of a function that utilizes const `NotImplemented`. This will necessitate understanding the concept of binary operations and their special methods. <|Analysis End|> <|Question Begin|> Question: Implementing Custom Arithmetic Operations with `NotImplemented` In this assignment, you will create a custom class that implements the addition (`+`) and subtraction (`-`) operations. However, it should only perform these operations when the other operand is of the same type. If the other operand is of a different type, your methods should return `NotImplemented`. # Requirements: 1. Implement a `CustomNumber` class with an attribute `value` to store the numerical value. 2. Override the `__add__` and `__sub__` methods to handle addition and subtraction. 3. If the other operand is not an instance of `CustomNumber`, the methods should return `NotImplemented`. 4. Demonstrate the behavior by performing addition and subtraction operations with both compatible and incompatible types. # Constraints: - You should use the `NotImplemented` return value appropriately based on the documentation provided. - The class should handle only addition and subtraction. - You may assume the `value` attribute is always an integer. # Input and Output Formats: Input: - No user inputs are taken directly. - You may create instances of `CustomNumber` and perform operations in a script to demonstrate functionality. Output: - Print the results of the operations and any exceptions raised, if applicable. # Example: ```python class CustomNumber: def __init__(self, value: int): self.value = value def __add__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) return NotImplemented # Example usage: num1 = CustomNumber(10) num2 = CustomNumber(5) num3 = 3 # Not a CustomNumber instance result1 = num1 + num2 # CustomNumber(15) result2 = num1 - num2 # CustomNumber(5) result3 = num1 + num3 # Raises TypeError due to NotImplemented result4 = num1 - num3 # Raises TypeError due to NotImplemented print(result1.value) # Outputs: 15 print(result2.value) # Outputs: 5 try: num1 + num3 except TypeError as e: print(e) # Outputs: unsupported operand type(s) for +: \'CustomNumber\' and \'int\' try: num1 - num3 except TypeError as e: print(e) # Outputs: unsupported operand type(s) for -: \'CustomNumber\' and \'int\' ``` Ensure your class behaves as expected based on these example interactions and print statements.","solution":"class CustomNumber: def __init__(self, value: int): self.value = value def __add__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) return NotImplemented # Example usage and demonstration: num1 = CustomNumber(10) num2 = CustomNumber(5) num3 = 3 # Not a CustomNumber instance result1 = num1 + num2 # CustomNumber(15) result2 = num1 - num2 # CustomNumber(5) print(result1.value) # Outputs: 15 print(result2.value) # Outputs: 5 try: num1 + num3 except TypeError as e: print(e) # Outputs: unsupported operand type(s) for +: \'CustomNumber\' and \'int\' try: num1 - num3 except TypeError as e: print(e) # Outputs: unsupported operand type(s) for -: \'CustomNumber\' and \'int\'"},{"question":"# **Nearest Neighbors Classification and Regression Project** In this project, you will demonstrate your understanding of scikit-learn\'s `neighbors` module by implementing both classification and regression tasks using nearest neighbors. These tasks will help evaluate your comprehension of various methods and parameters provided by the module. **Part 1: Classification** **Task**: Implement a function `knn_classification` that: 1. Takes an input feature matrix `X_train` and corresponding labels `y_train` for training. 2. Takes an input feature matrix `X_test` for testing. 3. Returns the predicted labels for `X_test` using k-nearest neighbors classification. **Function Signature**: ```python from sklearn.neighbors import KNeighborsClassifier import numpy as np def knn_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray: pass ``` **Constraints**: - `X_train`, `X_test` will be 2D NumPy arrays with shape `(n_samples, n_features)`. - `y_train` will be a 1D NumPy array with shape `(n_samples,)`. - `k` will be a positive integer less than the number of training samples. **Example Input**: ```python X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8], [8, 9]]) y_train = np.array([0, 0, 0, 1, 1, 1]) X_test = np.array([[2, 3], [7, 8]]) k = 3 ``` **Example Output**: ```python array([0, 1]) ``` **Part 2: Regression** **Task**: Implement a function `knn_regression` that: 1. Takes an input feature matrix `X_train` and corresponding continuous labels `y_train` for training. 2. Takes an input feature matrix `X_test` for testing. 3. Returns the predicted labels for `X_test` using k-nearest neighbors regression. **Function Signature**: ```python from sklearn.neighbors import KNeighborsRegressor import numpy as np def knn_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray: pass ``` **Constraints**: - `X_train`, `X_test` will be 2D NumPy arrays with shape `(n_samples, n_features)`. - `y_train` will be a 1D NumPy array with shape `(n_samples,)`. - `k` will be a positive integer less than the number of training samples. **Example Input**: ```python X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8], [8, 9]]) y_train = np.array([1.0, 2.0, 3.0, 6.0, 7.0, 8.0]) X_test = np.array([[2, 3], [7, 8]]) k = 3 ``` **Example Output**: ```python array([2.0, 7.0]) ``` Your task is to implement these functions, ensuring that they use the `KNeighborsClassifier` and `KNeighborsRegressor` from scikit-learn, respectively.","solution":"from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor import numpy as np def knn_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray: Perform k-nearest neighbors classification on the provided data. Parameters: - X_train: np.ndarray, shape (n_samples, n_features) Training feature matrix. - y_train: np.ndarray, shape (n_samples,) Training labels. - X_test: np.ndarray, shape (n_samples, n_features) Testing feature matrix. - k: int Number of neighbors to use for k-nearest neighbors. Returns: - np.ndarray: Predicted labels for X_test. knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) return knn.predict(X_test) def knn_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray: Perform k-nearest neighbors regression on the provided data. Parameters: - X_train: np.ndarray, shape (n_samples, n_features) Training feature matrix. - y_train: np.ndarray, shape (n_samples,) Training continuous labels. - X_test: np.ndarray, shape (n_samples, n_features) Testing feature matrix. - k: int Number of neighbors to use for k-nearest neighbors. Returns: - np.ndarray: Predicted continuous labels for X_test. knn = KNeighborsRegressor(n_neighbors=k) knn.fit(X_train, y_train) return knn.predict(X_test)"},{"question":"**Question:** You are given the following data representing scores of three students (A, B, and C) over three different exams: ```python data = { \\"students\\": [\\"A\\", \\"B\\", \\"C\\"], \\"exam1_scores\\": [90, 70, 85], \\"exam2_scores\\": [85, 75, 80], \\"exam3_scores\\": [88, 72, 89] } ``` You are required to: 1. Create three line plots displaying the scores of the students across the three exams. 2. Use seaborn\'s `plotting_context` to create two different styles of plots: - One with the default plotting context. - One using the `\\"talk\\"` plotting context. Implement a function `plot_student_scores(data)` that takes the `data` dictionary as input and performs the following tasks: 1. Plots the scores of each student across the three exams using the default plotting context. This should be saved as a plot named `\\"default_context.png\\"`. 2. Plots the scores again, but this time using the `\\"talk\\"` plotting context. This should be saved as a plot named `\\"talk_context.png\\"`. *Expected Function Signature*: ```python def plot_student_scores(data: dict) -> None: pass ``` *Constraints*: - Install seaborn (`pip install seaborn`) and matplotlib (`pip install matplotlib`) if not already installed. - Ensure the plots are distinguished by titles and axis labels. - Use context managers effectively to manage different plotting contexts. *Hint*: - You can use `sns.lineplot` for plotting the line charts. - Use `plt.savefig(\'filename.png\')` from matplotlib to save the generated plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_student_scores(data): Plot the scores of students across three exams using default and \\"talk\\" plotting context. Parameters: data (dict): The dictionary containing students\' names and their exam scores. exams = [\'Exam 1\', \'Exam 2\', \'Exam 3\'] # Prepare data for plotting student_scores = {student: [data[f\\"exam{i+1}_scores\\"][idx] for i in range(3)] for idx, student in enumerate(data[\\"students\\"])} # Plot with default context plt.figure(figsize=(10, 6)) for student, scores in student_scores.items(): sns.lineplot(x=exams, y=scores, label=student) plt.title(\'Student Scores Across Exams (Default Context)\') plt.xlabel(\'Exams\') plt.ylabel(\'Scores\') plt.legend(title=\'Students\') plt.savefig(\'default_context.png\') plt.close() # Plot with \\"talk\\" context with sns.plotting_context(\\"talk\\"): plt.figure(figsize=(10, 6)) for student, scores in student_scores.items(): sns.lineplot(x=exams, y=scores, label=student) plt.title(\'Student Scores Across Exams (\\"Talk\\" Context)\') plt.xlabel(\'Exams\') plt.ylabel(\'Scores\') plt.legend(title=\'Students\') plt.savefig(\'talk_context.png\') plt.close()"},{"question":"Parameter Initialization in PyTorch Problem Statement You are required to implement a neural network and initialize its parameters using different initialization techniques provided by `torch.nn.init`. Your task is to write a custom function that initializes the parameters of any given neural network according to a specified method. Function Signature ```python def initialize_parameters(model, method=\'xavier_uniform\', **kwargs): Initialize the parameters of the given model using the specified method. Parameters: model (torch.nn.Module): The neural network model whose parameters need to be initialized. method (str): The initialization method to use. Supported methods are: - \'uniform\' - \'normal\' - \'constant\' - \'ones\' - \'zeros\' - \'eye\' - \'dirac\' - \'xavier_uniform\' - \'xavier_normal\' - \'kaiming_uniform\' - \'kaiming_normal\' - \'trunc_normal\' - \'orthogonal\' - \'sparse\' kwargs: Additional arguments specific to the initialization method chosen. Returns: None ``` Input - `model`: An instance of `torch.nn.Module` representing the neural network. - `method`: A string indicating which initialization method to use (default is `\'xavier_uniform\'`). - `kwargs`: A dictionary of method-specific parameters (e.g., for `constant`, a `val` parameter can be passed). Output - The function should modify the model in place and return `None`. Constraints - You must use the initialization functions provided by the `torch.nn.init` module. - Handle invalid initialization methods by raising a `ValueError`. Example Usage ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x model = SimpleNeuralNetwork() # Initialize with Xavier Uniform initialize_parameters(model, method=\'xavier_uniform\') # Initialize with a constant value of 0.5 initialize_parameters(model, method=\'constant\', val=0.5) ``` Notes - Your implementation should ensure to handle each parameter initialization correctly and also cater to additional kwargs for specific initialization methods, such as `val` for `constant_`. - Consider scenarios where the user might provide invalid methods and ensure you handle such cases gracefully.","solution":"import torch import torch.nn.init as init def initialize_parameters(model, method=\'xavier_uniform\', **kwargs): Initialize the parameters of the given model using the specified method. Parameters: model (torch.nn.Module): The neural network model whose parameters need to be initialized. method (str): The initialization method to use. Supported methods are: - \'uniform\' - \'normal\' - \'constant\' - \'ones\' - \'zeros\' - \'eye\' - \'dirac\' - \'xavier_uniform\' - \'xavier_normal\' - \'kaiming_uniform\' - \'kaiming_normal\' - \'trunc_normal\' - \'orthogonal\' - \'sparse\' kwargs: Additional arguments specific to the initialization method chosen. Returns: None if not isinstance(model, torch.nn.Module): raise ValueError(\\"model must be an instance of torch.nn.Module\\") methods = { \'uniform\': init.uniform_, \'normal\': init.normal_, \'constant\': init.constant_, \'ones\': init.ones_, \'zeros\': init.zeros_, \'eye\': init.eye_, \'dirac\': init.dirac_, \'xavier_uniform\': init.xavier_uniform_, \'xavier_normal\': init.xavier_normal_, \'kaiming_uniform\': init.kaiming_uniform_, \'kaiming_normal\': init.kaiming_normal_, \'trunc_normal\': init.trunc_normal_, \'orthogonal\': init.orthogonal_, \'sparse\': init.sparse_ } if method not in methods: raise ValueError(f\\"Unsupported initialization method: {method}\\") for name, param in model.named_parameters(): if \'weight\' in name: methods[method](param, **kwargs) elif \'bias\' in name: if method != \'sparse\': init.zeros_(param)"},{"question":"**Context:** You are required to design a function that uses the `contextvars` module to manage contextual data in an asynchronous environment. The goal is to ensure that data processed by tasks running concurrently do not interfere with each other, maintaining their own contextual state. **Task:** Implement a class `ContextAwareProcess` with the following requirements: 1. **Data Initialization**: - The class should have a method `init_data(value: Any) -> None` that initializes a context variable named `data_var` with the given value. 2. **Data Manipulation**: - Implement a method `process_data(transformation: Callable[[Any], Any]) -> Any` that modifies the context variable `data_var` using a provided transformation function and returns the transformed data. 3. **Data Isolation in Asynchronous Environment**: - Implement an asynchronous method `async_process(transformation: Callable[[Any], Any], value: Any) -> Any` which initializes the context variable using `init_data` and then processes the data asynchronously using `process_data`. Ensure that the state is isolated for each call in an asynchronous context. **Specifications:** - **Class**: `ContextAwareProcess` - **Methods**: - `init_data(value: Any) -> None` - `process_data(transformation: Callable[[Any], Any]) -> Any` - `async def async_process(transformation: Callable[[Any], Any], value: Any) -> Any` - **Context Variable**: `data_var` (should be an instance of `contextvars.ContextVar`) **Example Usage:** ```python from typing import Any, Callable import contextvars import asyncio class ContextAwareProcess: data_var: contextvars.ContextVar = contextvars.ContextVar(\'data_var\') def init_data(self, value: Any) -> None: self.data_var.set(value) def process_data(self, transformation: Callable[[Any], Any]) -> Any: data = self.data_var.get() processed_data = transformation(data) self.data_var.set(processed_data) return processed_data async def async_process(self, transformation: Callable[[Any], Any], value: Any) -> Any: self.init_data(value) return self.process_data(transformation) # Asynchronous Processing Example async def main(): processor = ContextAwareProcess() async def task(transform, value): result = await processor.async_process(transform, value) print(f\\"Processed value: {result}\\") transform_fn = lambda x: x * 2 await asyncio.gather( task(transform_fn, 10), task(transform_fn, 20) ) asyncio.run(main()) ``` In this example, each asynchronous task initializes the `data_var` context variable with a different value and processes it independently using the provided transformation function. **Constraints:** - Ensure context data is properly isolated for each asynchronous task. - Handle any potential exceptions gracefully, providing meaningful error messages.","solution":"from typing import Any, Callable import contextvars import asyncio class ContextAwareProcess: data_var: contextvars.ContextVar = contextvars.ContextVar(\'data_var\') def init_data(self, value: Any) -> None: self.data_var.set(value) def process_data(self, transformation: Callable[[Any], Any]) -> Any: data = self.data_var.get() processed_data = transformation(data) self.data_var.set(processed_data) return processed_data async def async_process(self, transformation: Callable[[Any], Any], value: Any) -> Any: self.init_data(value) return self.process_data(transformation)"},{"question":"Objective This question aims to assess your understanding of the `tarfile` module in Python and your ability to utilize its functionalities to handle and manipulate tar archive files. Problem Statement Write a Python function named `create_filtered_tar_archive` that creates a tar archive from a specified directory, handles symbolic links, and filters out files based on specific criteria before adding them to the archive. Function Signature ```python def create_filtered_tar_archive(source_dir: str, archive_name: str, filter_func=None) -> None: Creates a filtered tar archive from the specified directory. Parameters: - source_dir (str): The path of the directory to archive. - archive_name (str): The name of the resulting tar archive file. - filter_func (Callable[[tarfile.TarInfo], tarfile.TarInfo | None], optional): A function that modifies or filters out files. The function accepts a TarInfo object and returns a modified TarInfo object or None to exclude it from the archive. Returns: - None pass ``` Requirements 1. **Input Parameters**: - `source_dir`: Path to the directory that you want to archive. - `archive_name`: The name for the resulting tar archive file. - `filter_func`: An optional function that takes a `TarInfo` object and returns a modified `TarInfo` object or `None` if the file should be excluded from the archive. 2. **Behavior**: - The function should create a tar archive (with gzip compression) named `archive_name`. - All files and subdirectories from `source_dir` should be included in the archive. - If `filter_func` is provided, it should be applied to each file before adding it to the archive. - If `filter_func` returns `None` for a given file, that file should not be added to the archive. - Symbolic links should be handled by directly storing the link (i.e., the link itself is archived, not the content it points to). 3. **Example Usage**: ```python def my_filter(tarinfo): if tarinfo.name.endswith(\'.log\'): return None return tarinfo create_filtered_tar_archive(\'/path/to/source\', \'output_archive.tar.gz\', my_filter) ``` Constraints - The function should handle large directories and files efficiently. - Ensure to follow best practices for handling file operations, such as proper closing of files. Example Given a directory structure in `/path/to/source`: ``` /path/to/source/ ├── file1.txt ├── file2.log ├── subdir/ │ ├── file3.txt │ └── symlink -> /some/other/path ``` If `create_filtered_tar_archive(\'/path/to/source\', \'output_archive.tar.gz\', my_filter)` is called, the resulting archive `output_archive.tar.gz` should *not* include `file2.log`. Symbolic link `symlink` should be archived directly as a link. Note This function should make use of the appropriate functions and classes provided by the `tarfile` module to handle the reading, writing, and filtering of tar archives.","solution":"import tarfile import os def create_filtered_tar_archive(source_dir: str, archive_name: str, filter_func=None) -> None: Creates a filtered tar archive from the specified directory. Parameters: - source_dir (str): The path of the directory to archive. - archive_name (str): The name of the resulting tar archive file. - filter_func (Callable[[tarfile.TarInfo], tarfile.TarInfo | None], optional): A function that modifies or filters out files. The function accepts a TarInfo object and returns a modified TarInfo object or None to exclude it from the archive. Returns: - None with tarfile.open(archive_name, mode=\'w:gz\') as tar: tar.add(source_dir, arcname=os.path.basename(source_dir), filter=filter_func)"},{"question":"Objective: Design a function using pandas that processes a large dataset containing multiple parquet files stored in a directory. The function should: 1. Efficiently load only necessary columns. 2. Convert relevant columns to more memory-efficient datatypes. 3. Perform chunk-wise processing. 4. Return a summary DataFrame with specific aggregations. Task: You are given a directory of parquet files (`data/timeseries/`). Each file contains a time series dataset with the following columns: `timestamp`, `name`, `id`, `x`, and `y`. Write a function `process_large_dataset(directory: str, columns: list) -> pd.DataFrame` that fulfills the following requirements: 1. **Parameters**: - `directory` (str): The path to the directory containing parquet files. - `columns` (list): A list of columns to load from the parquet files. 2. **Functionality**: - Load only the specified columns from each parquet file in the directory. - Convert the `name` column to `pandas.Categorical`. - Downcast all numeric columns (`id`, `x`, and `y`) to the most memory-efficient types. - Calculate the mean (average) and sum of the `x` and `y` columns for each unique `name` value across all files. - Combine the results into a single DataFrame with the aggregated values for each `name`. 3. **Return**: - A DataFrame with the following columns: `name`, `mean_x`, `mean_y`, `sum_x`, `sum_y`. **Constraints**: - Each parquet file fits in memory individually, but the combined size of all files may exceed available memory. - The directory structure is as follows: ``` data └── timeseries ├── ts-00.parquet ├── ts-01.parquet ├── ts-02.parquet ├── ... ``` **Example**: ```python import pandas as pd def process_large_dataset(directory: str, columns: list) -> pd.DataFrame: import pathlib files = pathlib.Path(directory).glob(\\"*.parquet\\") summary = pd.DataFrame(columns=[\\"name\\", \\"mean_x\\", \\"mean_y\\", \\"sum_x\\", \\"sum_y\\"]) for path in files: df = pd.read_parquet(path, columns=columns) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") aggregation = df.groupby(\\"name\\").agg( mean_x=(\\"x\\", \\"mean\\"), mean_y=(\\"y\\", \\"mean\\"), sum_x=(\\"x\\", \\"sum\\"), sum_y=(\\"y\\", \\"sum\\"), ).reset_index() summary = pd.concat([summary, aggregation]).groupby(\\"name\\").agg( mean_x=(\\"mean_x\\", \\"mean\\"), mean_y=(\\"mean_y\\", \\"mean\\"), sum_x=(\\"sum_x\\", \\"sum\\"), sum_y=(\\"sum_y\\", \\"sum\\"), ).reset_index() return summary ``` Test your function with a sample directory to ensure it correctly processes and aggregates the data. **Notes**: - Make sure to import any necessary libraries. - Assume that the `pandas` library is available in the environment.","solution":"import pandas as pd import pathlib def process_large_dataset(directory: str, columns: list) -> pd.DataFrame: Processes a large dataset containing multiple parquet files stored in a directory. Args: - directory (str): The path to the directory containing parquet files. - columns (list): A list of columns to load from the parquet files. Returns: - pd.DataFrame: A DataFrame with the aggregated values for each unique `name`. files = pathlib.Path(directory).glob(\\"*.parquet\\") summary = pd.DataFrame(columns=[\\"name\\", \\"mean_x\\", \\"mean_y\\", \\"sum_x\\", \\"sum_y\\"]) for path in files: df = pd.read_parquet(path, columns=columns) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") aggregation = df.groupby(\\"name\\").agg( mean_x=(\\"x\\", \\"mean\\"), mean_y=(\\"y\\", \\"mean\\"), sum_x=(\\"x\\", \\"sum\\"), sum_y=(\\"y\\", \\"sum\\"), ).reset_index() summary = pd.concat([summary, aggregation]).groupby(\\"name\\").agg( mean_x=(\\"mean_x\\", \\"mean\\"), mean_y=(\\"mean_y\\", \\"mean\\"), sum_x=(\\"sum_x\\", \\"sum\\"), sum_y=(\\"sum_y\\", \\"sum\\"), ).reset_index() return summary"},{"question":"# Python Coding Assessment Question: Managing a Persistent Dictionary using `shelve` Objective: Your task is to implement a function that manages a persistent dictionary using Python\'s `shelve` module. This function will perform various operations such as storing, updating, deleting, and retrieving data, and ensure proper data persistence with required error handling. Function Signature ```python def manage_shelve_operations(filename, operations): Manage a persistent dictionary using the `shelve` module. Parameters: - filename: The base filename for the underlying database. - operations: A list of tuples representing operations to be performed on the shelve. Each tuple in `operations` is expected to be of the format: (\\"operation\\", \\"key\\", value) where \\"operation\\" is one of \\"store\\", \\"retrieve\\", \\"delete\\", or \\"append\\". The \\"value\\" is required only for \\"store\\" and \\"append\\" operations. The function should return a list of results for each \\"retrieve\\" operation. If an operation raises a `KeyError`, it should be caught and the string \\"KeyError\\" should be added to the results list. The shelve should be properly closed at the end of the function. pass ``` Inputs: - `filename` (str): The name of the file to use for the shelve. - `operations` (list): A list of tuples where each tuple represents an operation to perform. The tuple format is: - (\\"store\\", \\"key\\", value): Store the value under the given key. - (\\"retrieve\\", \\"key\\"): Retrieve the value stored under the given key. - (\\"delete\\", \\"key\\"): Delete the key-value pair for the given key. - (\\"append\\", \\"key\\", value): If the value at the given key is a list, append the provided value to the list. Outputs: - A list of results from the \\"retrieve\\" operations, or a string \\"KeyError\\" if the key does not exist. Example ```python # Suppose \'example_db\' is the filename for the shelve operations = [ (\\"store\\", \\"item1\\", \\"data1\\"), (\\"store\\", \\"item2\\", \\"data2\\"), (\\"retrieve\\", \\"item1\\"), (\\"append\\", \\"item3\\", 3), # This should raise KeyError as item3 does not exist (\\"store\\", \\"item3\\", [1, 2]), (\\"append\\", \\"item3\\", 3), (\\"retrieve\\", \\"item3\\"), (\\"delete\\", \\"item2\\"), (\\"retrieve\\", \\"item2\\") ] results = manage_shelve_operations(\\"example_db\\", operations) print(results) # Expected output: [\'data1\', \'KeyError\', [1, 2, 3], \'KeyError\'] ``` Constraints: - Do not hardcode filenames; utilize the filename parameter. - Handle and properly close the shelve regardless of exceptions. - Ensure memory efficiency when handling large data sets. Good luck!","solution":"import shelve def manage_shelve_operations(filename, operations): Manage a persistent dictionary using the `shelve` module. Parameters: - filename: The base filename for the underlying database. - operations: A list of tuples representing operations to be performed on the shelve. Each tuple in `operations` is expected to be of the format: (\\"operation\\", \\"key\\", value) where \\"operation\\" is one of \\"store\\", \\"retrieve\\", \\"delete\\", or \\"append\\". The \\"value\\" is required only for \\"store\\" and \\"append\\" operations. The function should return a list of results for each \\"retrieve\\" operation. If an operation raises a `KeyError`, it should be caught and the string \\"KeyError\\" should be added to the results list. The shelve should be properly closed at the end of the function. results = [] with shelve.open(filename, flag=\'c\', writeback=True) as db: for operation in operations: op_type = operation[0] key = operation[1] if op_type == \\"store\\": value = operation[2] db[key] = value elif op_type == \\"retrieve\\": try: results.append(db[key]) except KeyError: results.append(\\"KeyError\\") elif op_type == \\"delete\\": try: del db[key] except KeyError: pass # Ignore if key does not exist elif op_type == \\"append\\": value = operation[2] try: if isinstance(db[key], list): db[key].append(value) db[key] = db[key] # To trigger writeback else: results.append(\\"KeyError\\") except KeyError: results.append(\\"KeyError\\") return results"},{"question":"# Partial Least Squares (PLS) Regression You are required to implement the PLSRegression algorithm from scratch using numpy. PLSRegression links predictor variables and response variables by projecting both into a latent structure, particularly useful when there are many variables and multicollinearity. Instructions: 1. Implement the PLSRegression class with the following methods: - **fit(X, Y, n_components)**: Fits the model to the predictors `X` (shape: n_samples, n_features) and responses `Y` (shape: n_samples, n_targets) using `n_components` latent variables. - **transform(X)**: Transforms the predictors `X` using the fitted model, returning the projected data. - **predict(X)**: Predicts the response variables for given predictors `X` using the fitted model. Implementation Details: 1. **fit**: - Center `X` and `Y`. - Iteratively compute weights `u_k` and `v_k` by finding the left singular vectors of the cross-covariance matrix of `X` and `Y`. - Calculate scores (projections) `xi_k` and `omega_k`. - Perform regression of `X` on `xi_k` and `Y` on `omega_k` to find loadings `gamma_k` and `delta_k`. - Deflate `X` and `Y` by subtracting the rank-1 approximations. - Store the weights and loadings for transformation and prediction. 2. **transform**: - Transform new data `X` using the stored weights. 3. **predict**: - Predict responses for new data using the learned weights and loadings from the fit method. Input Format: - The **fit** method will take parameters `X`, `Y`, and `n_components`. - `X`: numpy array of shape (n_samples, n_features) - `Y`: numpy array of shape (n_samples, n_targets) - `n_components`: integer, number of latent variables - The **transform** and **predict** methods will take a numpy array `X` of shape (n_samples, n_features). Output Format: - The **fit** method does not return anything. - The **transform** method returns a numpy array. - The **predict** method returns a numpy array. Constraints: - Ensure `n_components` is at most the minimum of the number of features in `X` or number of responses in `Y`. - Use numpy for matrix computations. Example: ```python import numpy as np class PLSRegression: def __init__(self): self.x_weights_ = None self.y_weights_ = None self.x_loadings_ = None self.y_loadings_ = None def fit(self, X, Y, n_components): # Your code here def transform(self, X): # Your code here def predict(self, X): # Your code here # Example usage X = np.array([[0.1, 0.2], [0.4, 0.5], [0.5, 0.8]]) Y = np.array([[0.3], [0.7], [0.9]]) pls = PLSRegression() pls.fit(X, Y, 2) X_transformed = pls.transform(X) Y_pred = pls.predict(X) print(\\"Transformed X:n\\", X_transformed) print(\\"Predicted Y:n\\", Y_pred) ```","solution":"import numpy as np class PLSRegression: def __init__(self): self.x_weights_ = None self.y_weights_ = None self.x_loadings_ = None self.y_loadings_ = None self.n_components_ = None self.x_mean_ = None self.y_mean_ = None def fit(self, X, Y, n_components): self.n_components_ = n_components n_samples, n_features = X.shape _, n_targets = Y.shape self.x_weights_ = np.zeros((n_features, n_components)) self.y_weights_ = np.zeros((n_targets, n_components)) self.x_loadings_ = np.zeros((n_features, n_components)) self.y_loadings_ = np.zeros((n_targets, n_components)) self.x_mean_ = np.mean(X, axis=0) self.y_mean_ = np.mean(Y, axis=0) X = X - self.x_mean_ Y = Y - self.y_mean_ for k in range(n_components): u_k = Y[:, [0]] for _ in range(100): w_k = np.dot(X.T, u_k) / np.dot(u_k.T, u_k) w_k /= np.linalg.norm(w_k) t_k = np.dot(X, w_k) c_k = np.dot(Y.T, t_k) / np.dot(t_k.T, t_k) c_k /= np.linalg.norm(c_k) u_k = np.dot(Y, c_k) p_k = np.dot(X.T, t_k) / np.dot(t_k.T, t_k) q_k = np.dot(Y.T, u_k) / np.dot(u_k.T, u_k) X -= np.dot(t_k, p_k.T) Y -= np.dot(t_k, q_k.T) self.x_weights_[:, k] = w_k.ravel() self.y_weights_[:, k] = c_k.ravel() self.x_loadings_[:, k] = p_k.ravel() self.y_loadings_[:, k] = q_k.ravel() def transform(self, X): T = np.dot(X - self.x_mean_, self.x_weights_) return T def predict(self, X): T = self.transform(X) Y_pred = np.dot(T, self.y_loadings_.T) return Y_pred + self.y_mean_"},{"question":"**Title: Implementing a Custom Wrapper for Built-in Functions** Objective The purpose of this task is to evaluate your understanding of Python\'s `builtins` module and your ability to effectively wrap and extend built-in functionality. Problem Statement You are required to implement a custom version of Python\'s built-in `print` function. This custom `print` function should prefix all output messages with a timestamp in the format `YYYY-MM-DD HH:MM:SS`. Your task involves: 1. Defining a function `custom_print` that wraps the built-in `print` function. 2. Ensuring that `custom_print` can take any arguments or keyword arguments that the built-in `print` function accepts. 3. Adding a timestamp prefix to every output message. Function Signature ```python import builtins from datetime import datetime def custom_print(*args, **kwargs): Custom print function that prefixes each message with a timestamp. Args: *args: Variable length argument list to be printed. **kwargs: Arbitrary keyword arguments for built-in print function. Returns: None pass ``` Example ```python custom_print(\\"Hello, world!\\") ``` Expected output in the console (timestamp will vary): ``` 2023-09-24 15:45:03 Hello, world! ``` Constraints - You must import and use the `builtins` module to access the original `print` function. - Use the `datetime` module to generate the current timestamp. - Your implementation should not modify the built-in `print` function globally within the Python environment. - Assume all inputs to your function are valid and focus on functional correctness and usage of `builtins`. Evaluation Criteria - Correctness of timestamp format. - Appropriately wrapping and calling the built-in `print` function. - Proper handling of variable arguments and keyword arguments. - Clean and readable code.","solution":"import builtins from datetime import datetime def custom_print(*args, **kwargs): Custom print function that prefixes each message with a timestamp. Args: *args: Variable length argument list to be printed. **kwargs: Arbitrary keyword arguments for built-in print function. Returns: None timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') builtins.print(f\\"{timestamp}\\", *args, **kwargs)"},{"question":"# Seaborn Categorical Plotting Assessment **Objective**: To assess your understanding of seaborn\'s capabilities for visualizing categorical data using various plot types and customization options. **Problem Statement**: You are provided with a dataset containing information about passengers on the Titanic. Your task is to visualize this data using seaborn\'s categorical plotting functions. Follow the steps and instructions below to complete the task. **Input**: - A pandas DataFrame named `titanic` containing the Titanic dataset with the following columns: - `sex`: Gender of the passenger (male/female). - `class`: Passenger class (First/Second/Third). - `fare`: The fare paid by the passenger. - `embark_town`: The town the passenger embarked from (Southampton/Cherbourg/Queenstown). - `survived`: Survival status (0 = No, 1 = Yes). **Instructions**: 1. **Bar Plot**: - Create a bar plot showing the average fare (`fare`) paid by passengers, split by class (`class`). Use the `sex` column for the `hue` semantic. - Customize the error bars to show 95% confidence intervals. - Add appropriate titles and labels for clarity. 2. **Box Plot**: - Create a box plot showing the distribution of fare (`fare`) grouped by the embarkation town (`embark_town`). Use the `class` column for the `hue` semantic. - Ensure that the categorical axis (`embark_town`) is on the y-axis. 3. **Violin Plot**: - Create a violin plot to show the distribution of the age (`age`) of the passengers, grouped by gender (`sex`). Use the `survived` column for the `hue` semantic. - Split the violins to show survival status within each gender. 4. **Jittered Scatter Plot**: - Create a scatter plot showing the total_bill (`total_bill`) for each day (`day`). Adjust the points with jitter. - Disable jitter (set `jitter=False`). - Use a `swarm` plot as an alternative to show the distribution of total bills for each day more clearly. 5. **Combined Plot**: - Combine a box plot and a strip plot to visualize the fare (`fare`) compared by survival status (`survived`). Use the `class` column for the `hue` semantic. **Output**: - Ensure all plots are displayed with appropriate labeling, legends, and titles. **Constraints**: - You should use the seaborn `catplot` interface wherever possible. **Performance Requirements**: - Ensure the code is optimized and runs efficiently even for larger datasets. **Example Code**: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Bar plot sns.catplot(data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"sex\\", kind=\\"bar\\", errorbar=(\\"pi\\", 95)) plt.title(\'Average Fare by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Average Fare\') plt.show() # Box plot sns.catplot(data=titanic, x=\\"fare\\", y=\\"embark_town\\", hue=\\"class\\", kind=\\"box\\", orient=\\"h\\") plt.title(\'Fare Distribution by Embarkation Town\') plt.xlabel(\'Fare\') plt.ylabel(\'Embarkation Town\') plt.show() # Violin plot sns.catplot(data=titanic, x=\\"age\\", y=\\"sex\\", hue=\\"survived\\", split=True, kind=\\"violin\\") plt.title(\'Age Distribution by Gender and Survival Status\') plt.xlabel(\'Age\') plt.ylabel(\'Sex\') plt.show() # Jittered scatter plot tips = sns.load_dataset(\\"tips\\") sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", jitter=False) plt.title(\'Total Bill by Day (No Jitter)\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Combined plot g = sns.catplot(data=titanic, x=\\"survived\\", y=\\"fare\\", kind=\\"box\\") sns.stripplot(data=titanic, x=\\"survived\\", y=\\"fare\\", hue=\\"class\\", dodge=True, ax=g.ax) plt.title(\'Fare and Survival Status by Class\') plt.xlabel(\'Survived\') plt.ylabel(\'Fare\') plt.legend(title=\'Class\', loc=\'upper left\') plt.show() ``` **Note**: Ensure your seaborn version is compatible with the code above (seaborn v0.13.0 or later is recommended).","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Bar plot def plot_bar(): sns.catplot(data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"sex\\", kind=\\"bar\\", errorbar=(\\"ci\\", 95)) plt.title(\'Average Fare by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Average Fare\') plt.show() # Box plot def plot_box(): sns.catplot(data=titanic, x=\\"fare\\", y=\\"embark_town\\", hue=\\"class\\", kind=\\"box\\", orient=\\"h\\") plt.title(\'Fare Distribution by Embarkation Town\') plt.xlabel(\'Fare\') plt.ylabel(\'Embarkation Town\') plt.show() # Violin plot def plot_violin(): sns.catplot(data=titanic, x=\\"age\\", y=\\"sex\\", hue=\\"survived\\", split=True, kind=\\"violin\\") plt.title(\'Age Distribution by Gender and Survival Status\') plt.xlabel(\'Age\') plt.ylabel(\'Sex\') plt.show() # Jittered scatter plot def plot_jittered_scatter(): tips = sns.load_dataset(\\"tips\\") sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", jitter=False) plt.title(\'Total Bill by Day (No Jitter)\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Combined plot def plot_combined(): g = sns.catplot(data=titanic, x=\\"survived\\", y=\\"fare\\", kind=\\"box\\") sns.stripplot(data=titanic, x=\\"survived\\", y=\\"fare\\", hue=\\"class\\", dodge=True, ax=g.ax) plt.title(\'Fare and Survival Status by Class\') plt.xlabel(\'Survived\') plt.ylabel(\'Fare\') plt.legend(title=\'Class\', loc=\'upper left\') plt.show()"},{"question":"# Implementing a Custom Python Object with Attribute Management and Garbage Collection Support Objective Create a custom Python object type representing a \\"CustomList\\" which: 1. Stores a list of integers. 2. Provides methods to perform operations like insertion, deletion, and retrieval. 3. Manages attributes in a way that overcomes any arbitrary constraints. 4. Implements cyclic garbage collection support for memory management. Detailed Requirements: 1. **Initialization**: - The constructor should initialize an empty list. 2. **Methods**: - `insert_elem(index: int, value: int)` - Inserts the value at the specified index. - `delete_elem(index: int)` - Deletes the element at the specified index. - `retrieve_elem(index: int) -> int` - Retrieves the element at the specified index. - `sum_elements() -> int` - Returns the sum of all the elements in the list. 3. **Attributes Handling**: - Allow dynamic setting and getting of attributes with any names apart from the list\'s inherent attributes. - Prevent overriding or accidental access to internal attributes. 4. **Garbage Collection**: - Support for cyclic garbage collection to handle cyclical references between multiple instances of `CustomList`. Code Template Here is a template to start with: ```python class CustomList: def __init__(self): Initialize an empty list. self._list = [] def insert_elem(self, index: int, value: int): Inserts an integer value at the specified index. # TODO: implement this method def delete_elem(self, index: int): Deletes the element at the specified index. # TODO: implement this method def retrieve_elem(self, index: int) -> int: Retrieves the element at the specified index. # TODO: implement this method def sum_elements(self) -> int: Returns the sum of all elements in the list. # TODO: implement this method def __setattr__(self, name, value): Set attributes with custom logic. # TODO: implement this method def __getattr__(self, name): Get attributes with custom logic. # TODO: implement this method def __del__(self): Ensure proper garbage collection. # TODO: implement this method ``` Constraints: - Your solution should handle edge cases such as inserting or deleting from invalid indexes. - Performance should be considered for large lists. - You should adhere to Python naming conventions and best practices. Testing: - Write tests to validate your implementation, including edge cases for each method. - Ensure that the list operations maintain integrity even under concurrent access scenarios.","solution":"class CustomList: def __init__(self): Initialize an empty list. self._list = [] def insert_elem(self, index: int, value: int): Inserts an integer value at the specified index. if not isinstance(value, int): raise ValueError(\\"Only integer values are allowed.\\") if index < 0 or index > len(self._list): raise IndexError(\\"Index out of bounds.\\") self._list.insert(index, value) def delete_elem(self, index: int): Deletes the element at the specified index. if index < 0 or index >= len(self._list): raise IndexError(\\"Index out of bounds.\\") del self._list[index] def retrieve_elem(self, index: int) -> int: Retrieves the element at the specified index. if index < 0 or index >= len(self._list): raise IndexError(\\"Index out of bounds.\\") return self._list[index] def sum_elements(self) -> int: Returns the sum of all elements in the list. return sum(self._list) def __setattr__(self, name, value): Set attributes with custom logic. if name == \\"_list\\": super().__setattr__(name, value) elif name.startswith(\'_\'): raise AttributeError(f\\"Cannot set reserved attribute \'{name}\'\\") else: self.__dict__[name] = value def __getattr__(self, name): Get attributes with custom logic. try: return self.__dict__[name] except KeyError: raise AttributeError(f\\"Attribute \'{name}\' does not exist\\") def __del__(self): Ensure proper garbage collection. self._list.clear()"},{"question":"**Objective:** Demonstrate your understanding of the `seaborn` package, specifically the `relplot` function, by creating a faceted and customized plot. **Problem Statement:** Using the built-in dataset \\"tips\\" from seaborn, create a faceted scatter plot to visualize the relationship between total bill and tip amount for different days of the week, segmented by time (Lunch or Dinner) and gender. **Requirements:** 1. Load the \\"tips\\" dataset from seaborn. 2. Create a scatter plot using `sns.relplot` to show the relationship between `total_bill` and `tip`: - Color-code the points based on the day of the week (`day`). - Facet the plot by `time` (columns) and `sex` (rows). - Use different markers for each gender (`style=\\"sex\\"`). 3. Customize the plot by: - Setting the height of each facet to 5 and the aspect ratio to 1. - Adding appropriate axis labels and a title for each facet that indicates the meal time and gender. - Ensuring that the `total_bill` axis is labeled as \\"Total Bill ()\\" and the `tip` axis is labeled as \\"Tip ()\\". 4. Save the plot as a PNG image file named \\"tips_facet_plot.png\\". **Function Definition:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_facet_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the faceted scatter plot g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\", style=\\"sex\\", height=5, aspect=1, kind=\\"scatter\\" ) # Customize the plot g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(\\"Meal: {col_name} | Gender: {row_name}\\") # Save the plot as a PNG file plt.savefig(\\"tips_facet_plot.png\\") # Example usage create_facet_plot() ``` **Submission:** - Implement the function `create_facet_plot` following the above specifications. - Ensure that the function runs without errors and creates the expected visualization. - Submit your Python script containing the `create_facet_plot` function. **Evaluation:** Your submission will be evaluated based on: - Correctness of the plot (faceting, color-coding, markers). - Appropriate customization of the plot (axis labels, titles). - Proper usage of `seaborn.relplot` and other customization methods.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facet_plot(): Creates a faceted scatter plot showing the relationship between total bill and tip amount in the \'tips\' dataset segmented by time (Lunch or Dinner) and gender. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the faceted scatter plot g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\", style=\\"sex\\", height=5, aspect=1, kind=\\"scatter\\" ) # Customize the plot g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(\\"Meal: {col_name} | Gender: {row_name}\\") # Save the plot as a PNG file plt.savefig(\\"tips_facet_plot.png\\")"},{"question":"Objective: You are required to design and implement a custom PyTorch module that demonstrates the use of both static and dynamic values. Your implementation should include control flow dependent on dynamic tensor shapes and make use of `torch.export.export` to capture the computation graph. Instructions: 1. Define a custom PyTorch module `DynamicControlModule` that: - Takes three inputs: a tensor `x`, a static integer `threshold`, and a boolean `flag`. - If the `flag` is `True`, performs an element-wise sine operation on `x`. - If the `flag` is `False`, adds `threshold` to a dynamic shape-dependent value from `x` and returns the result. 2. Export the module using `torch.export.export`, providing appropriate example inputs to showcase dynamic shape-dependent control flow. 3. Print the exported computation graph. Implementation Details: - You need to ensure the module correctly differentiates between static and dynamic values. - Handle control flow over dynamic tensor shapes appropriately using the methods described in the provided documentation. - Verify that your exported graph retains the expected operations and control flow. Expected Input and Output: - Input to the module: - `x`: A PyTorch tensor of arbitrary shape. - `threshold`: An integer that serves as a static value. - `flag`: A boolean indicating the control flow path. - Output from the module: - A tensor resulting from the operations described above. - The printed exported graph should clearly show different branches based on the `flag`. Example: # Implementation ```python import torch class DynamicControlModule(torch.nn.Module): def forward(self, x, threshold, flag): if flag: return x.sin() else: dynamic_value = x.sum() + threshold # Assuming dynamic shape-dependent value return dynamic_value # Instantiate the module module = DynamicControlModule() # Example inputs example_x = torch.randn(3, 3) example_threshold = 5 example_flag = False # Export the computation graph exported_program = torch.export.export(module, (example_x, example_threshold, example_flag)) # Print the exported computation graph print(exported_program.graph_module.code) ``` Your task is to fill in the necessary parts of the provided implementation to ensure it meets the requirements specified above. Constraints: - You should not use any external libraries other than PyTorch. - Adhere to the principles of static vs. dynamic values and shape-dependent control flow as described in the documentation. Evaluate the correctness and efficiency of your solution based on how well it handles dynamic shapes and whether the exported graph accurately represents the control flow and operations.","solution":"import torch class DynamicControlModule(torch.nn.Module): def forward(self, x, threshold, flag): if flag: return torch.sin(x) else: dynamic_value = torch.sum(x) + threshold return dynamic_value # Instantiate the module module = DynamicControlModule() # Example inputs example_x = torch.randn(3, 3) example_threshold = 5 example_flag = False # Export the computation graph exported_program = torch.export.export(module, (example_x, example_threshold, example_flag)) # Print the exported computation graph print(exported_program.graph_module.code)"},{"question":"# Task **Objective:** Implement a custom probability distribution that combines features of several built-in PyTorch distributions. You will need to subclass the `Distribution` class and implement specific methods. **Description:** Implement a custom distribution called `MyCustomDistribution` that combines the features of a `Normal` distribution and an `Exponential` distribution in the following manner: - The custom distribution has two parameters: `mu` (mean of the Normal distribution) and `lambda` (rate of the Exponential distribution). - A sample from `MyCustomDistribution` should first sample a value from the `Normal(mu, 1)` distribution. - Then, use this sampled value as the mean for another sample from the `Exponential(lambda)` distribution. - The final sample should be the sum of these two samples (Normal sample + Exponential sample). To summarize: 1. Create the `MyCustomDistribution` class that subclasses `torch.distributions.Distribution`. 2. Implement the following methods: - `__init__(self, mu, lambda)` - `sample(self, sample_shape=torch.Size())` - `log_prob(self, value)` **Inputs:** - `mu`: a float tensor representing the mean of the Normal distribution. - `lambda`: a float tensor representing the rate parameter of the Exponential distribution. **Outputs:** - `sample()`: returns a sample from `MyCustomDistribution`. - `log_prob(value)`: returns the log-probability of a given value under the distribution. **Constraints:** - Use PyTorch\'s built-in `Normal` and `Exponential` distributions for sampling and log-probability calculations. - Ensure the implementation is efficient and leverages batch processing capabilities. **Example:** ```python import torch from torch.distributions import Distribution, Normal, Exponential class MyCustomDistribution(Distribution): def __init__(self, mu, lambda_): super(MyCustomDistribution, self).__init__() self.mu = mu self.lambda_ = lambda_ self.normal = Normal(mu, 1) self.exponential = Exponential(lambda_) def sample(self, sample_shape=torch.Size()): normal_sample = self.normal.sample(sample_shape) exponential_sample = self.exponential.sample(sample_shape) return normal_sample + exponential_sample def log_prob(self, value): # This method should calculate the log probability of `value` under MyCustomDistribution raise NotImplementedError(\\"log_prob method is not yet implemented\\") # Example usage: my_dist = MyCustomDistribution(torch.tensor(0.0), torch.tensor(1.0)) sample = my_dist.sample() print(\\"Sample:\\", sample) # For log_prob, you will need to implement the necessary logic to combine log probabilities. ``` **Note:** The `log_prob` method needs to be implemented by appropriately combining the log-probabilities of the sampled Normal and Exponential distributions. You may need to use additional math to properly account for the transformed distributions.","solution":"import torch from torch.distributions import Distribution, Normal, Exponential class MyCustomDistribution(Distribution): def __init__(self, mu, lambda_): super(MyCustomDistribution, self).__init__() self.mu = mu self.lambda_ = lambda_ self.normal = Normal(mu, 1) self.exponential = Exponential(lambda_) def sample(self, sample_shape=torch.Size()): normal_sample = self.normal.sample(sample_shape) exponential_sample = self.exponential.sample(sample_shape) return normal_sample + exponential_sample def log_prob(self, value): # This is a complex problem and a straight forward solution for log_prob # might not exist due to the nature of the combined distribution. # For the scope of this implementation we throw an error raise NotImplementedError(\\"log_prob method is not implemented due to complexity of combined distribution\\") # Example usage: my_dist = MyCustomDistribution(torch.tensor(0.0), torch.tensor(1.0)) sample = my_dist.sample() print(\\"Sample:\\", sample) # For log_prob, you will need to implement the necessary logic to combine log probabilities. # Note: Computing log_prob for this custom distribution is quite complex and non-trivial."},{"question":"# Command-Line Task Manager You are tasked with implementing a command-line interface (CLI) tool for managing a simple task list using the `argparse` module. The tool should support adding, listing, and removing tasks. Each task should have a unique identifier, a description, and a status indicating whether it is completed or not. Requirements 1. **Create the ArgumentParser:** - Define a parser with a description: \\"Task Manager - Manage your tasks from the command line.\\" 2. **Sub-commands:** - Implement three sub-commands: `add`, `list`, and `remove`. 3. **Add Command:** - Adds a new task to the task list. - Requires a description for the task (a string). - Optionally allow setting the task as completed (--completed flag). Example: ``` python task_manager.py add \\"Buy groceries\\" python task_manager.py add \\"Finish homework\\" --completed ``` 4. **List Command:** - Lists all tasks. - Optionally filter tasks by their status using the `--status` flag (takes values `completed` or `pending`). Example: ``` python task_manager.py list python task_manager.py list --status completed ``` 5. **Remove Command:** - Removes a task by its unique identifier (ID). Example: ``` python task_manager.py remove 1 python task_manager.py remove 2 ``` 6. **Functionality:** - Each task should have a unique ID assigned automatically starting from 1. - Use a list to store tasks in memory, where each task is represented as a dictionary with keys: `id`, `description`, and `completed`. - Implement appropriate functions to handle the add, list, and remove operations. Implementation Below is the skeleton code that you need to complete to meet the above requirements: ```python import argparse tasks = [] def add_task(description, completed): # Assign a new unique ID task_id = len(tasks) + 1 task = { \'id\': task_id, \'description\': description, \'completed\': completed } tasks.append(task) print(f\\"Task {task_id} added.\\") def list_tasks(status): for task in tasks: if status is None or (status == \\"completed\\" and task[\'completed\']) or (status == \\"pending\\" and not task[\'completed\']): print(f\\"ID: {task[\'id\']}, Description: {task[\'description\']}, Completed: {task[\'completed\']}\\") def remove_task(task_id): global tasks tasks = [task for task in tasks if task[\'id\'] != task_id] print(f\\"Task {task_id} removed.\\") def main(): parser = argparse.ArgumentParser(description=\\"Task Manager - Manage your tasks from the command line.\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'description\', type=str, help=\'Description of the task\') add_parser.add_argument(\'--completed\', action=\'store_true\', help=\'Mark the task as completed\') list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_parser.add_argument(\'--status\', type=str, choices=[\'completed\', \'pending\'], help=\'Filter tasks by status\') remove_parser = subparsers.add_parser(\'remove\', help=\'Remove a task by ID\') remove_parser.add_argument(\'task_id\', type=int, help=\'ID of the task to remove\') args = parser.parse_args() if args.command == \'add\': add_task(args.description, args.completed) elif args.command == \'list\': list_tasks(args.status) elif args.command == \'remove\': remove_task(args.task_id) if __name__ == \'__main__\': main() ``` Constraints - Only one sub-command should be used at a time. - The `--completed` flag for the `add` command is optional and defaults to False. - The `--status` flag for the `list` command is optional; if omitted, all tasks should be listed. - Ensure the program gracefully handles removal of non-existent tasks. Evaluation Your solution will be evaluated based on: - Correct usage of the `argparse` module. - Proper handling of tasks using the add, list, and remove sub-commands. - Clear and user-friendly command-line interface. - Handling edge cases such as attempting to remove a task that doesn’t exist.","solution":"import argparse tasks = [] def add_task(description, completed=False): # Assign a new unique ID task_id = len(tasks) + 1 task = { \'id\': task_id, \'description\': description, \'completed\': completed } tasks.append(task) print(f\\"Task {task_id} added.\\") def list_tasks(status=None): for task in tasks: if status is None or (status == \\"completed\\" and task[\'completed\']) or (status == \\"pending\\" and not task[\'completed\']): print(f\\"ID: {task[\'id\']}, Description: {task[\'description\']}, Completed: {task[\'completed\']}\\") def remove_task(task_id): global tasks task_found = False for task in tasks: if task[\'id\'] == task_id: task_found = True tasks.remove(task) print(f\\"Task {task_id} removed.\\") break if not task_found: print(f\\"Task {task_id} does not exist.\\") def main(): parser = argparse.ArgumentParser(description=\\"Task Manager - Manage your tasks from the command line.\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'description\', type=str, help=\'Description of the task\') add_parser.add_argument(\'--completed\', action=\'store_true\', help=\'Mark the task as completed\') list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_parser.add_argument(\'--status\', type=str, choices=[\'completed\', \'pending\'], help=\'Filter tasks by status\') remove_parser = subparsers.add_parser(\'remove\', help=\'Remove a task by ID\') remove_parser.add_argument(\'task_id\', type=int, help=\'ID of the task to remove\') args = parser.parse_args() if args.command == \'add\': add_task(args.description, args.completed) elif args.command == \'list\': list_tasks(args.status) elif args.command == \'remove\': remove_task(args.task_id) if __name__ == \'__main__\': main()"},{"question":"Objective: The aim of this task is to assess your ability to use seaborn for data visualization, including setting styles and customizing plots. You are expected to demonstrate your understanding of various seaborn functionalities by implementing a function to generate specific types of plots with custom styles. Problem Statement: Write a function `custom_plot(data: pd.DataFrame) -> None` that takes a Pandas DataFrame and generates a set of visualizations based on the following specifications: 1. **Histogram**: - Plot a histogram of the \'age\' column with `sns.histplot`. - Use a \\"darkgrid\\" style but with a grid line color of \\".3\\" and dashed grid lines. 2. **Boxplot**: - Plot a boxplot of \'age\' categorized by \'gender\' with `sns.boxplot`. - Use a \\"whitegrid\\" style with default settings. 3. **Scatter Plot**: - Plot a scatter plot of \'height\' vs \'weight\' with `sns.scatterplot`. - Customize the scatter plot with a \\"ticks\\" style and set the background color to \'azure\'. Input: - `data` - a Pandas DataFrame containing at least these columns: \'age\', \'gender\', \'height\', and \'weight\'. Output: - The function should display the three plots but return `None`. Constraints: - Ensure that the styles and customization specified are strictly adhered to. - Use proper labels for axes and titles for each plot. Example: ```python import pandas as pd data = pd.DataFrame({ \'age\': [23, 45, 31, 35, 40, 26, 28, 30, 50], \'gender\': [\'M\', \'F\', \'M\', \'F\', \'F\', \'M\', \'M\', \'F\', \'M\'], \'height\': [170, 165, 180, 159, 170, 175, 163, 169, 176], \'weight\': [70, 55, 80, 50, 65, 75, 58, 67, 79] }) custom_plot(data) ``` Expected behavior: The function should display three customized plots as per the given specifications without returning any values. Note: - Ensure to import the necessary libraries and set up your environment before plotting. - Utilize the seaborn library to its full extent for customization and styling.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_plot(data: pd.DataFrame) -> None: sns.set() # Histogram with sns.axes_style(\\"darkgrid\\", {\'grid.linestyle\': \'--\', \'grid.color\': \'.3\'}): plt.figure(figsize=(10, 6)) sns.histplot(data[\'age\']) plt.title(\'Age Distribution\') plt.xlabel(\'Age\') plt.ylabel(\'Frequency\') plt.show() # Boxplot sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.boxplot(x=\'gender\', y=\'age\', data=data) plt.title(\'Age Distribution by Gender\') plt.xlabel(\'Gender\') plt.ylabel(\'Age\') plt.show() # Scatter plot sns.set_style(\\"ticks\\") plt.figure(figsize=(10, 6)) ax = sns.scatterplot(x=\'height\', y=\'weight\', data=data) plt.title(\'Height vs Weight\') plt.xlabel(\'Height\') plt.ylabel(\'Weight\') ax.set_facecolor(\'azure\') plt.show() return None"},{"question":"# Question: HMAC Implementation and Verification You are tasked with implementing a secure message authentication system that uses the HMAC library in Python for ensuring the integrity and authenticity of messages. **Your Task:** 1. Implement the function `create_hmac(key, message, algorithm)` which: - Takes a `key` (bytes or bytearray), `message` (str), and `algorithm` (str, e.g., \'sha256\') as input. - Returns the HMAC digest of the message using the specified algorithm (in hexadecimal format). 2. Implement the function `verify_hmac(key, message, received_hexdigest, algorithm)` which: - Takes a `key` (bytes or bytearray), `message` (str), `received_hexdigest` (str), and `algorithm` (str) as input. - Computes the HMAC hexdigest for the given message and compares it with `received_hexdigest` using a secure comparison method. - Returns `True` if the HMAC digests match, `False` otherwise. # Constraints: - You must use the `hmac` module. - The `algorithm` parameter should be a valid hash name recognized by `hashlib`. # Example Usage: ```python key = b\\"supersecretkey\\" message = \\"This is a secure message.\\" algorithm = \\"sha256\\" # Create HMAC hexdigest = create_hmac(key, message, algorithm) print(hexdigest) # Example output: \\"5d41402abc4b2a76b9719d911017c592\\" # Verify HMAC is_valid = verify_hmac(key, message, hexdigest, algorithm) print(is_valid) # Output: True ``` # Requirements: 1. Your implementation must be correct and efficient. 2. Make sure to use the `compare_digest` function for the secure comparison in the `verify_hmac` function. **Note:** Be careful to handle different types appropriately, i.e., converting the message (str) to bytes where necessary.","solution":"import hmac import hashlib def create_hmac(key, message, algorithm): Creates an HMAC hexdigest for a given message using the provided key and hashing algorithm. Args: key (bytes or bytearray): The key for HMAC generation. message (str): The message to authenticate. algorithm (str): The hashing algorithm (e.g., \'sha256\'). Returns: str: The HMAC hexdigest of the message. message_bytes = message.encode(\'utf-8\') hmac_obj = hmac.new(key, message_bytes, getattr(hashlib, algorithm)) return hmac_obj.hexdigest() def verify_hmac(key, message, received_hexdigest, algorithm): Verifies an HMAC hexdigest for a given message using the provided key and hashing algorithm. Args: key (bytes or bytearray): The key for HMAC generation. message (str): The message to authenticate. received_hexdigest (str): The received HMAC hexdigest to verify against. algorithm (str): The hashing algorithm (e.g., \'sha256\'). Returns: bool: True if the HMAC digests match, False otherwise. computed_hexdigest = create_hmac(key, message, algorithm) return hmac.compare_digest(computed_hexdigest, received_hexdigest)"},{"question":"Objective: The objective of this assessment is to evaluate your proficiency in working with Python\'s file and directory access modules. You will need to demonstrate your ability to perform various operations such as creating temporary files, manipulating paths, and pattern matching. Problem Statement: You are tasked with writing a Python function `organize_logs` that will process a directory containing various log files. The function will: 1. Create a temporary directory to store processed log files. 2. Copy all log files (assume files with `.log` extension) from the given directory to the temporary directory. 3. Append a timestamp to each copied log file\'s name to avoid overwriting. 4. Return a list of new file paths in the temporary directory. Your function should have the following signature: ```python import tempfile import shutil import pathlib import os from datetime import datetime def organize_logs(logs_directory: str) -> list: # Your implementation here ``` Input: - `logs_directory` (str): A string representing the path to the directory that contains the log files. Output: - Returns a list of strings, each representing the path to the new log file in the temporary directory. Constraints: - You can assume that the provided `logs_directory` is a valid directory path. - The function should handle both relative and absolute paths. - The function should be efficient and should not hold all file contents in memory. Example: Assume the `logs_directory` contains the following files: ``` example_logs/ app.log error.log debug.log ``` A sample output could be: ```python [\'/tmp/tmpabcd1234/app_20231001_152300.log\', \'/tmp/tmpabcd1234/error_20231001_152300.log\', \'/tmp/tmpabcd1234/debug_20231001_152300.log\'] ``` (Note: The actual temporary directory path and timestamp will vary.) Performance Requirements: - The function should process and copy files efficiently. - The solution should be scalable to handle large numbers of log files and large log file sizes. # Notes: - Make sure to clean up any temporary directories created after ensuring they are no longer needed. - You may use any of the modules mentioned in the provided documentation to complete this task. Happy Coding!","solution":"import tempfile import shutil import pathlib import os from datetime import datetime def organize_logs(logs_directory: str) -> list: # Create a temporary directory to store processed log files temp_dir = tempfile.mkdtemp() # Get the current timestamp to append to filenames timestamp = datetime.now().strftime(\\"%Y%m%d_%H%M%S\\") new_files = [] # Iterate through the list of files in the logs directory for log_file in pathlib.Path(logs_directory).glob(\'*.log\'): # Construct new filename with timestamp new_filename = f\\"{log_file.stem}_{timestamp}{log_file.suffix}\\" new_filepath = os.path.join(temp_dir, new_filename) # Copy file to the new location shutil.copy(log_file, new_filepath) # Add the new file path to the list new_files.append(new_filepath) # Return the list of new file paths return new_files"},{"question":"**Secure Login System using `getpass`** # Problem Statement You are tasked with designing a secure login system that utilizes the Python `getpass` module to handle password input securely and verify the logged-in user. The system will prompt the user to enter their username and password, and you will need to authenticate them against a predefined set of valid credentials. You will also implement a function to check the current logged-in user using `getpass.getuser`. # Requirements 1. **Function 1: check_current_user()** - This function should return the current logged-in user\'s name using the `getpass.getuser` function. 2. **Function 2: authenticate_user(valid_credentials: dict)** - This function should prompt the user to enter their username and password. - Use `input()` to take the username and `getpass.getpass()` to take the password securely. - The function should check the entered username and password against the `valid_credentials` dictionary where keys are usernames and values are passwords. - Return `True` if the credentials are valid, otherwise `False`. # Input and Output Formats - **Function 1: check_current_user()** - **Input**: None - **Output**: A string representing the current logged-in user\'s name. - **Function 2: authenticate_user(valid_credentials: dict)** - **Input**: A dictionary (`valid_credentials`) where keys are strings representing usernames and values are strings representing passwords. - **Output**: A boolean value indicating whether the authentication was successful (`True`) or not (`False`). # Constraints - The `authenticate_user` function should not show the password on the screen when the user types it. - If `getpass.getuser` raises an exception, `check_current_user` should return the string \\"Unknown User\\". # Example ```python def check_current_user(): pass def authenticate_user(valid_credentials): pass # Example usage: valid_credentials = { \\"user1\\": \\"password123\\", \\"admin\\": \\"adminpass\\" } # Simulating function calls (Note: actual password input handling won\'t be shown here) current_user = check_current_user() print(current_user) # Output will depend on the environment is_authenticated = authenticate_user(valid_credentials) print(is_authenticated) # Output: True or False based on input ``` Implement the functions `check_current_user` and `authenticate_user` as described. Test your implementation thoroughly to ensure correctness.","solution":"import getpass def check_current_user(): Returns the current logged-in user\'s name using the `getpass.getuser` function. try: return getpass.getuser() except Exception: return \\"Unknown User\\" def authenticate_user(valid_credentials): Prompts the user to enter their username and password, and authenticates them against the provided valid_credentials dictionary. Args: valid_credentials (dict): A dictionary containing valid username-password pairs. Returns: bool: True if authentication is successful, False otherwise. username = input(\\"Username: \\") password = getpass.getpass(\\"Password: \\") if username in valid_credentials and valid_credentials[username] == password: return True return False"},{"question":"# Question: Customizing Seaborn\'s Cubehelix Palette Using the Seaborn library, you are required to generate a visualization with a customized `cubehelix_palette`. The task will test your understanding of how to manipulate various parameters of the `cubehelix_palette` function and use it to create distinct visual styles. **Requirements:** 1. Create a figure with 3 subplots, each displaying the same plot type but using different `cubehelix_palette` customizations: - **First subplot**: Default `cubehelix_palette`. - **Second subplot**: Customized `cubehelix_palette` with the following parameters: - `n_colors=10` - `start=1` - `rot=0.5` - `gamma=0.8` - `hue=0.2` - `dark=0.1` - `light=0.9` - **Third subplot**: Customized `cubehelix_palette` with: - `n_colors=15` - `start=2` - `rot=-0.4` - `gamma=1` - `hue=1` - `dark=0.3` - `light=0.7` - `reverse=True` 2. Create a bar plot as the type of visualization for each subplot. You can use dummy data for the plot. 3. Each subplot should have a title corresponding to the type of `cubehelix_palette` customization used (e.g., \\"Default\\", \\"Customized 1\\", \\"Customized 2\\"). **Constraints:** - Use the `seaborn` library for plotting. - Ensure readability and aesthetic quality of each subplot. **Input:** No explicit input is required; standard libraries and dummy data are to be used for visualization. **Output:** A figure with 3 subplots, each demonstrating different customizations of the `cubehelix_palette`. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Generate dummy data data = pd.DataFrame({ \\"Category\\": list(\\"ABCDEFGHIJK\\") * 3, \\"Values\\": np.random.rand(33) }) # Create the figure and subplots fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5)) # First subplot with default cubehelix_palette sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[0], palette=sns.cubehelix_palette()) axes[0].set_title(\\"Default\\") # Second subplot with customized cubehelix_palette custom_palette_1 = sns.cubehelix_palette( n_colors=10, start=1, rot=0.5, gamma=0.8, hue=0.2, dark=0.1, light=0.9) sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[1], palette=custom_palette_1) axes[1].set_title(\\"Customized 1\\") # Third subplot with another customized cubehelix_palette custom_palette_2 = sns.cubehelix_palette( n_colors=15, start=2, rot=-0.4, gamma=1, hue=1, dark=0.3, light=0.7, reverse=True) sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[2], palette=custom_palette_2) axes[2].set_title(\\"Customized 2\\") # Show the plot plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def create_cubhelix_custom_palette_plots(): # Generate dummy data data = pd.DataFrame({ \\"Category\\": list(\\"ABCDEFGHIJK\\") * 3, \\"Values\\": np.random.rand(33) }) # Create the figure and subplots fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6)) # First subplot with default cubehelix_palette sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[0], palette=sns.cubehelix_palette()) axes[0].set_title(\\"Default\\") # Second subplot with customized cubehelix_palette custom_palette_1 = sns.cubehelix_palette( n_colors=10, start=1, rot=0.5, gamma=0.8, hue=0.2, dark=0.1, light=0.9) sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[1], palette=custom_palette_1) axes[1].set_title(\\"Customized 1\\") # Third subplot with another customized cubehelix_palette custom_palette_2 = sns.cubehelix_palette( n_colors=15, start=2, rot=-0.4, gamma=1, hue=1, dark=0.3, light=0.7, reverse=True) sns.barplot(x=\\"Category\\", y=\\"Values\\", data=data, ax=axes[2], palette=custom_palette_2) axes[2].set_title(\\"Customized 2\\") # Show the plot plt.tight_layout() plt.show()"},{"question":"# PyTorch Data Type Analysis Objective: Implement a function using PyTorch that normalizes a given tensor based on its data type. The function should handle both floating-point and integer tensors. For floating-point tensors, it will scale the values to the range `[0, 1]` based on the smallest and largest representable numbers of the data type. For integer tensors, it will scale the values to the range `[0, 1]` based on the smallest and largest representable numbers of the data type. Function Signature: ```python import torch def normalize_tensor(tensor: torch.Tensor) -> torch.Tensor: Normalize a PyTorch tensor based on its data type. Parameters: tensor (torch.Tensor): A tensor of either floating-point or integer type. Returns: torch.Tensor: A tensor with values normalized to the range [0, 1]. ``` Input: - `tensor`: A PyTorch tensor of any shape and of a floating-point or integer data type. Output: - A PyTorch tensor with the same shape as the input, but with values normalized to the range `[0, 1]`. Constraints: - The function must handle both floating-point and integer tensors. - The function should use `torch.finfo` and `torch.iinfo` to obtain the necessary numerical properties of the tensor\'s data type. - Performance should be considered for larger tensors. Example: ```python # Example 1: Floating-point tensor fp_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32) normalized_fp_tensor = normalize_tensor(fp_tensor) print(normalized_fp_tensor) # Tensor values should be scaled to [0, 1] range based on float32 properties # Example 2: Integer tensor int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32) normalized_int_tensor = normalize_tensor(int_tensor) print(normalized_int_tensor) # Tensor values should be scaled to [0, 1] range based on int32 properties ``` Implement this `normalize_tensor` function to demonstrate your understanding of PyTorch\'s `torch.finfo` and `torch.iinfo` classes.","solution":"import torch def normalize_tensor(tensor: torch.Tensor) -> torch.Tensor: Normalize a PyTorch tensor based on its data type. Parameters: tensor (torch.Tensor): A tensor of either floating-point or integer type. Returns: torch.Tensor: A tensor with values normalized to the range [0, 1]. if tensor.is_floating_point(): dtype_info = torch.finfo(tensor.dtype) min_val, max_val = dtype_info.min, dtype_info.max else: dtype_info = torch.iinfo(tensor.dtype) min_val, max_val = dtype_info.min, dtype_info.max normalized_tensor = (tensor - min_val) / (max_val - min_val) return normalized_tensor"},{"question":"Mailbox Message Management You are required to implement a function named `move_messages_by_condition`. This function will transfer messages from one mailbox to another based on a user-defined condition. Function Signature ```python def move_messages_by_condition(source_path: str, destination_path: str, condition: callable, source_format: str = \'Maildir\', destination_format: str = \'Maildir\') -> None: ``` Parameters: - `source_path` (str): The file path to the source mailbox. - `destination_path` (str): The file path to the destination mailbox. - `condition` (callable): A function that takes a message object as input and returns a boolean. If the function returns `True`, the message should be moved from the source to the destination. - `source_format` (str): The format of the source mailbox (default is `\'Maildir\'`). - `destination_format` (str): The format of the destination mailbox (default is `\'Maildir\'`). Behavior: 1. **Mailbox Initialization:** - Depending on the `source_format` and `destination_format`, create instances of the appropriate `Mailbox` subclasses (e.g., `mailbox.Maildir`, `mailbox.mbox`). 2. **Message Moving:** - Iterate over the messages in the source mailbox. - For each message, apply the `condition` function. - If `condition(message)` returns `True`, move the message to the destination mailbox. 3. **Locking and Performance Considerations:** - Use proper locking mechanisms to ensure that the mailbox is not corrupted by concurrent modifications. - Ensure that changes are flushed and mailboxes are properly unlocked and closed after operations. Example Usage: ```python import mailbox def condition(message: mailbox.Message) -> bool: return \'important\' in message[\'subject\'].lower() move_messages_by_condition(\'/path/to/source\', \'/path/to/destination\', condition) ``` In this example, the `move_messages_by_condition` function will move all messages from the source mailbox to the destination mailbox if their subject contains the word \'important\'. Constraints: - You must handle any exceptions that may arise during mailbox operations (e.g., `KeyError`, `mailbox.NoSuchMailboxError`, `mailbox.FormatError`). - Ensure that the mailbox formats chosen support the operations being performed. Notes: - You may assume that the paths provided are valid and that the mailboxes exist unless otherwise stated. - Be mindful of different behaviors for each mailbox format, as documented. Implement the `move_messages_by_condition` function based on the provided specifications.","solution":"import mailbox import os import shutil def move_messages_by_condition(source_path: str, destination_path: str, condition: callable, source_format: str = \'Maildir\', destination_format: str = \'Maildir\') -> None: # Map formats to Mailbox classes format_map = { \'Maildir\': mailbox.Maildir, \'mbox\': mailbox.mbox, \'MH\': mailbox.MH, \'Babyl\': mailbox.Babyl, \'MMDF\': mailbox.MMDF } # Create source and destination mailboxes source_mailbox = format_map[source_format](source_path) destination_mailbox = format_map[destination_format](destination_path) # Lock mailboxes try: source_mailbox.lock() destination_mailbox.lock() messages_to_move = [] # Iterate over source mailbox for key, message in source_mailbox.items(): if condition(message): messages_to_move.append(key) for key in messages_to_move: message = source_mailbox[key] destination_mailbox.add(message) source_mailbox.remove(key) source_mailbox.flush() destination_mailbox.flush() except Exception as e: print(f\\"An error occurred: {e}\\") finally: source_mailbox.close() destination_mailbox.close()"},{"question":"You are given a dataset containing information about different cars, including their weight, horsepower, displacement, and miles per gallon (mpg). You are required to analyze the relationships between these variables by creating and manipulating residual plots using the seaborn library. Task 1. **Data Loading and Preparation:** - Load the `mpg` dataset provided by seaborn. 2. **Fundamental Analysis:** - Create a simple residual plot to evaluate the residuals of a regression model between `weight` (x) and `displacement` (y). 3. **Regression Assumptions:** - Create another residual plot to explore violations in linear regression assumptions between `horsepower` (x) and `mpg` (y). 4. **Complex Analysis:** - Adjust the residual plot between `horsepower` and `mpg` to remove higher-order trends by setting the `order` parameter to 2. 5. **Advanced Visualization:** - Enhance the residual plot between `horsepower` and `mpg` by adding a LOWESS curve to make any structural trends more visible. Customize this curve to be red. Input and Output Formats - You do not need to write input/output functions. Just define and execute the necessary code blocks. - Ensure that each plot is generated as specified within the Jupyter Notebook. Constraints - You must use the seaborn library for all visualizations. - The dataset should be loaded directly from seaborn. Performance Requirements - Ensure all plots are correctly rendered and exhibit the described characteristics. - Make sure the LOWESS curve is clearly visible and distinctly colored. Example ```python import seaborn as sns sns.set_theme() # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Fundamental Analysis sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") # Regression Assumptions sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") # Complex Analysis sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) # Advanced Visualization sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) ``` Using this question format, students will be tested on their ability to create and manipulate residual plots using seaborn, as well as interpret and enhance these plots for better analytical insights.","solution":"import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load the dataset mpg = sns.load_dataset(\\"mpg\\") def plot_residuals(): # Fundamental Analysis plt.figure(figsize=(10,6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot between Weight and Displacement\\") plt.show() # Regression Assumptions plt.figure(figsize=(10,6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot between Horsepower and MPG\\") plt.show() # Complex Analysis plt.figure(figsize=(10,6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot between Horsepower and MPG (Order=2)\\") plt.show() # Advanced Visualization plt.figure(figsize=(10,6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws={\\"color\\":\\"r\\"}) plt.title(\\"Residual Plot between Horsepower and MPG with LOWESS Curve\\") plt.show()"},{"question":"**Question: Implement a Custom Slice Class** In Python, slices are a powerful feature that allows for accessing parts of sequences in a flexible manner. We want you to implement your own custom slice class without using Python\'s built-in slice. You need to create a class `CustomSlice` that mimics the behavior of the built-in `slice` type. Your class should support initialization with start, stop, and step values, similar to Python\'s slice. Additionally, it should implement a method `indices` that takes a sequence length and returns a tuple of (start, stop, step) indices adjusted to the bounds of the sequence. # Requirements 1. **Initialization**: - `CustomSlice(start, stop, step=None)` should initialize with the given start, stop, and step values. - If `step` is not provided, it should default to `1`. 2. **Methods**: - `indices(length)` - This method should take a sequence length and return a tuple `(start, stop, step)` with adjusted start and stop values within the bounds of the sequence length. 3. **Behavior**: - Negative indices should be adjusted as per Python’s normal slicing behavior. - The start, stop, and step should be validated and adjusted according to the sequence length constraints, similar to how Python handles slicing. - Should handle Ellipsis: If the start, stop, or step are set to Ellipsis (i.e., ...), treat them as `None`. # Constraints - You may not use Python\'s built-in `slice` directly or indirectly. - Your implementation should be efficient and handle edge cases such as negative indexing, steps larger than the sequence, etc. - The sequence length provided to `indices()` will be a non-negative integer. # Example ```python # Example Usage: cs = CustomSlice(1, 10, 2) result = cs.indices(20) # Expected output: (1, 10, 2) cs2 = CustomSlice(-10, -1) result2 = cs2.indices(15) # Expected output adjusted to the bounds ``` # Testing You should write tests verifying the behavior of the `CustomSlice` class, ensuring that the adjusted values returned by `indices` correctly mirror the behavior of slice objects in Python.","solution":"class CustomSlice: def __init__(self, start, stop, step=None): self.start = None if start == Ellipsis else start self.stop = None if stop == Ellipsis else stop self.step = 1 if step is None or step == Ellipsis else step def indices(self, length): start = self.start if self.start is not None else 0 stop = self.stop if self.stop is not None else length step = self.step if step == 0: raise ValueError(\\"slice step cannot be zero\\") # Adjust negative indices if start < 0: start += length if stop < 0: stop += length # Ensure indices are within bounds start = max(0, min(length, start)) stop = max(0, min(length, stop)) return start, stop, step"},{"question":"# Pickling Custom Objects with External Resources You are tasked with implementing a class that represents a text file reader which keeps track of the last read position. The class should support pickling and unpickling such that the file is reopened and reading resumes from the last stored position. # Requirements 1. **Class Definition**: - Define a class `TextFileReader` with the following attributes: - `filename` (string): The name of the file. - `file` (file object): The file object (should not be pickled directly). - `lineno` (integer): The current line number. 2. **Methods**: - `__init__(self, filename)`: Opens the file and initializes the line number. - `readline(self)`: Reads the next line from the file, increments the line number, and returns the line with line number prefixed. - `__getstate__(self)`: Custom method for pickling, excluding the file object. - `__setstate__(self, state)`: Custom method for unpickling, restoring the file object and line number. 3. **Pickling and Unpickling**: - Implement custom pickling (`__getstate__`) to exclude the file object. - Implement custom unpickling (`__setstate__`) to reopen the file and restore the reading position. 4. **Serialization Functions**: - Use `pickle.dumps` to serialize an instance of `TextFileReader`. - Use `pickle.loads` to deserialize and restore the instance. # Constraints - The file must be opened in text mode. - Assume the file exists and is readable during both pickling and unpickling. - Handle large files efficiently. # Example Usage ```python import pickle class TextFileReader: def __init__(self, filename): self.filename = filename self.file = open(filename, \'r\') self.lineno = 0 def readline(self): self.lineno += 1 line = self.file.readline() return f\\"{self.lineno}: {line}\\" if line else None def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.filename, \'r\') for _ in range(self.lineno): self.file.readline() # Create an instance and read a few lines reader = TextFileReader(\'sample.txt\') print(reader.readline()) print(reader.readline()) # Serialize the reader object pickled_reader = pickle.dumps(reader) # Deserialize the reader object restored_reader = pickle.loads(pickled_reader) print(restored_reader.readline()) # Should continue from the correct line number ``` # Submission Submit your implementation of the `TextFileReader` class and the example usage demonstrating the serialization and deserialization processes.","solution":"import pickle class TextFileReader: def __init__(self, filename): self.filename = filename self.file = open(filename, \'r\') self.lineno = 0 def readline(self): self.lineno += 1 line = self.file.readline() return f\\"{self.lineno}: {line}\\" if line else None def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.filename, \'r\') for _ in range(self.lineno): self.file.readline()"},{"question":"Objective Write a function that visualizes a dataset using seaborn\'s `cubehelix_palette` method with various custom parameters. The goal is to demonstrate your ability to manipulate color palettes in seaborn and apply them to a data visualization task. Function Signature ```python def visualize_cubehelix(data, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0, light=1, reverse=False, as_cmap=False): Visualizes data using seaborn\'s cubehelix_palette with custom parameters. Parameters: data (pd.DataFrame): A pandas DataFrame to visualize. start (float): The starting point of the helix. rot (float): The amount of rotation in the color palette. gamma (float): Nonlinearity in the luminance ramp. hue (float): Saturation of the colors. dark (float): Luminance at the start point. light (float): Luminance at the end point. reverse (bool): Reverses the direction of the luminance ramp. as_cmap (bool): Returns a continuous colormap instead of a discrete palette. Returns: None ``` Input - `data`: A Pandas DataFrame containing the dataset to be visualized. - `start`, `rot`, `gamma`, `hue`, `dark`, `light`, `reverse`, `as_cmap`: The parameters for customizing the `cubehelix_palette`. Output - The function should visualize the dataset using seaborn\'s `cubehelix_palette` and output the plot using `sns.scatterplot`. Constraints - You cannot use any external plotting libraries apart from matplotlib and seaborn. - The DataFrame should have at least 2 columns for the x and y axis. Example Usage ```python import seaborn as sns import pandas as pd # Example dataset data = pd.DataFrame({ \\"x\\": range(10), \\"y\\": range(10) }) visualize_cubehelix(data, start=2, rot=0.5, gamma=0.8, hue=1, dark=0.3, light=0.7, reverse=False, as_cmap=True) ``` Notes - Explain how different parameters change the visualization. - Use appropriate titles and labels for the plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_cubehelix(data, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0, light=1, reverse=False, as_cmap=False): Visualizes data using seaborn\'s cubehelix_palette with custom parameters. Parameters: data (pd.DataFrame): A pandas DataFrame to visualize. start (float): The starting point of the helix. rot (float): The amount of rotation in the color palette. gamma (float): Nonlinearity in the luminance ramp. hue (float): Saturation of the colors. dark (float): Luminance at the start point. light (float): Luminance at the end point. reverse (bool): Reverses the direction of the luminance ramp. as_cmap (bool): Returns a continuous colormap instead of a discrete palette. Returns: None if not isinstance(data, pd.DataFrame) or data.shape[1] < 2: raise ValueError(\\"Input data must be a pandas DataFrame with at least two columns.\\") # Generate the cubehelix color palette based on parameters palette = sns.cubehelix_palette(start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap) # Create a scatter plot using the first two columns of the DataFrame plt.figure(figsize=(10, 6)) sns.scatterplot(x=data.iloc[:, 0], y=data.iloc[:, 1], palette=palette) plt.title(\\"Scatter Plot with Cubehelix Palette\\") plt.xlabel(data.columns[0]) plt.ylabel(data.columns[1]) plt.show()"},{"question":"# Custom SMTP Server Implementation You are required to implement a custom SMTP server using the `smtpd` module. Your task is to subclass the `SMTPServer` class and implement the `process_message` method to handle and process incoming email messages. # Requirements: 1. **Create a Custom SMTP Server**: - Subclass the `smtpd.SMTPServer` class. - Override the `process_message` method to add the following functionality: - Print the `mailfrom` (the sender\'s email address). - Print all `rcpttos` (the recipient(s) email address(es)). - Print the `data` (the content of the email). 2. **Server Initialization**: - Initialize your custom server to bind to `(\\"localhost\\", 1025)` as the local address. - For the remote address, you can use `None`. # Input and Output: - **Input**: - You don\'t need to handle direct input since the task involves creating a server. The SMTP client\'s input will be processed by your server. - **Output**: - The output will be the printed statements from the `process_message` method. # Constraints: - Use the `smtpd` module, avoid external libraries. # Example: Here is an example of how you might structure your solution: ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(\\"Sender:\\", mailfrom) print(\\"Recipients:\\", rcpttos) print(\\"Message Data:\\", data) return None if __name__ == \\"__main__\\": server = CustomSMTPServer((\\"localhost\\", 1025), None) asyncore.loop() ``` - This implementation will start an SMTP server that listens on port 1025 and prints details of any received email messages. # Your Task: Implement the `CustomSMTPServer` class as described, ensuring the `process_message` method handles and prints the email details as specified. Submitting this Python script should start the SMTP server and allow for testing with any SMTP client.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(\\"Sender:\\", mailfrom) print(\\"Recipients:\\", rcpttos) print(\\"Message Data:\\", data) return None if __name__ == \\"__main__\\": server = CustomSMTPServer((\\"localhost\\", 1025), None) asyncore.loop()"},{"question":"# Question: Implement a Hash-Based File Integrity Verification System You are required to implement a system that verifies the integrity of files using secure hash functions provided by the `hashlib` module. The system should be able to: 1. Generate a hash of a file using a specified hashing algorithm. 2. Compare the hash of the metadata (e.g., size, last modified time) and the file content with an expected hash to verify file integrity. 3. Demonstrate the usage of both SHA-256 and BLAKE2b algorithms. Requirements: 1. **Function: `hash_file`** - **Input:** - `file_path` (string): The path to the file. - `algorithm` (string): The name of the hashing algorithm (e.g., \'sha256\', \'blake2b\'). - **Output:** - A hexadecimal digest string representing the hash of the file. - **Behavior:** - Reads the file in chunks to avoid memory issues with large files. - Uses the specified algorithm to compute the hash. 2. **Function: `verify_file`** - **Input:** - `file_path` (string): The path to the file. - `expected_hash` (string): The expected hexadecimal digest string. - `algorithm` (string): The name of the hashing algorithm (e.g., \'sha256\', \'blake2b\'). - **Output:** - `True` if the file\'s hash matches the expected hash. - `False` otherwise. - **Behavior:** - Uses `hash_file` to compute the hash of the file. - Compares the computed hash with the expected hash. **Sample Input/Output:** ```python def hash_file(file_path: str, algorithm: str) -> str: # Your implementation here pass def verify_file(file_path: str, expected_hash: str, algorithm: str) -> bool: # Your implementation here pass # Example Usage: # Create a sample text file \'example.txt\' with open(\\"example.txt\\", \\"w\\") as f: f.write(\\"This is a sample file for hashing.\\") # Compute SHA-256 hash of the file hash_sha256 = hash_file(\'example.txt\', \'sha256\') print(hash_sha256) # Outputs: SHA-256 hash of the file content # Compute BLAKE2b hash of the file hash_blake2b = hash_file(\'example.txt\', \'blake2b\') print(hash_blake2b) # Outputs: BLAKE2b hash of the file content # Verify the file with the expected SHA-256 hash is_valid_sha256 = verify_file(\'example.txt\', hash_sha256, \'sha256\') print(is_valid_sha256) # Should print: True # Verify the file with an incorrect SHA-256 hash is_valid_wrong = verify_file(\'example.txt\', \'0000\'*16, \'sha256\') print(is_valid_wrong) # Should print: False ``` Constraints: - Ensure efficiency while reading large files. - Use the error handling mechanisms for file I/O operations. - The algorithms used (`sha256`, `blake2b`) should be from `hashlib`. **Scoring:** - Correct implementation of `hash_file`. - Correct implementation of `verify_file`. - Proper handling of large files. - Correct usage of specified hash algorithms. - Handling of edge cases such as non-existent files.","solution":"import hashlib def hash_file(file_path, algorithm): Generates the hash of a file using the specified hashing algorithm. Parameters: - file_path (str): The path to the file. - algorithm (str): The name of the hashing algorithm (\'sha256\', \'blake2b\'). Returns: - str: The hexadecimal digest string representing the hash of the file. hash_func = getattr(hashlib, algorithm)() try: with open(file_path, \'rb\') as f: while chunk := f.read(8192): hash_func.update(chunk) except FileNotFoundError: return \\"\\" return hash_func.hexdigest() def verify_file(file_path, expected_hash, algorithm): Verifies the integrity of a file by comparing its hash to an expected hash. Parameters: - file_path (str): The path to the file. - expected_hash (str): The expected hexadecimal digest string. - algorithm (str): The name of the hashing algorithm (\'sha256\', \'blake2b\'). Returns: - bool: True if the file\'s hash matches the expected hash, False otherwise. computed_hash = hash_file(file_path, algorithm) return computed_hash == expected_hash"},{"question":"**Objective:** This task requires you to utilize seaborn\'s advanced plotting capabilities to create a normalized line plot of a dataset. Your aim is to demonstrate your understanding of seaborn\'s `so.Plot` and `so.Norm` functionalities. **Dataset:** Use the health expenditure dataset available in seaborn named `healthexp`. Question: Write a Python function `visualize_health_expenditure` that performs the following operations: 1. Loads the `healthexp` dataset from seaborn. 2. Plots a line plot showing the health expenditure normalized by the maximum value for each country. 3. Additionally, create and display another plot where the health expenditure is normalized using the minimum `Year` as a baseline, and the values are presented as percent change from this baseline. You should use seaborn\'s `so.Plot` and `so.Norm` for these visualizations. ***Function Signature:*** ```python def visualize_health_expenditure() -> None: pass ``` **Detailed Requirements:** 1. Use `seaborn` for all visualizations. 2. The first plot should normalize each country\'s health expenditure by its maximum value. 3. The second plot should normalize the health expenditure values using the minimum year as a baseline, with the output being displayed in percentage change from this baseline. 4. Label the y-axis appropriately in both plots to indicate the normalization method used (`\\"Spending relative to maximum amount\\"` and `\\"Percent change in spending from <min_year> baseline\\"` respectively). **Expected Output:** Your function should display two plots: 1. A line plot where the y-values are the health expenditures normalized by the maximum expenditure value for each country. 2. A line plot where the y-values are the percentage changes from the health expenditure value in the minimum year for each country. The plots should be displayed one after the other without any manual intervention. ```python import seaborn as sns import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # First plot: Normalize by maximum value plot_max = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) plot_max.show() # Second plot: Percent change from the minimum year baseline min_year = healthexp[\\"Year\\"].min() plot_percent = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=f\\"x == {min_year}\\", percent=True)) .label(y=f\\"Percent change in spending from {min_year} baseline\\") ) plot_percent.show() # You can uncomment the following line to test the function # visualize_health_expenditure() ``` **Constraints:** 1. Ensure your code is efficient and leverages seaborn capabilities appropriately. 2. Carefully handle any missing or inconsistent data within the dataset. **Hints:** - Pay close attention to the usage of `so.Norm()` for normalization. - Make use of `where` parameter to set the baseline year for percentage change calculation.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(): Visualizes health expenditure data using seaborn\'s so.Plot and so.Norm. Plots line plots normalized by maximum value for each country and percent change from the minimum year baseline. # Load the dataset healthexp = load_dataset(\\"healthexp\\") # First plot: Normalize by maximum value plot_max = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) plot_max.show() # Second plot: Percent change from the minimum year baseline min_year = healthexp[\\"Year\\"].min() plot_percent = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=f\\"x == {min_year}\\", percent=True)) .label(y=f\\"Percent change in spending from {min_year} baseline\\") ) plot_percent.show()"},{"question":"**Question: Implement a Custom Differentiation and Compute Hessians in PyTorch** **Problem Description:** You are tasked with implementing a custom PyTorch differentiable function and computing its Hessian matrix using PyTorch\'s autograd tools. The function to be differentiated is a simple quadratic function. 1. Implement a custom PyTorch differentiable function `QuadraticFunction` that computes ( f(x) = x^T A x + b^T x + c ), where ( A ) is a symmetric matrix, and ( b ) and ( x ) are vectors. 2. Implement the forward and backward passes using `torch.autograd.Function`. 3. Use the functional API to compute and return the Hessian of the function with respect to `x`. **Input:** 1. A symmetric matrix `A` of shape `(n, n)`. 2. A vector `b` of shape `(n,)`. 3. A scalar `c`. 4. A vector `x` where you need to compute the Hessians. **Output:** - The output should be the Hessian matrix of the function ( f ) with respect to `x`, of shape `(n, n)`. **Constraints:** - Implement the custom differentiable function class `QuadraticFunction` using `torch.autograd.Function`. - Ensure the implementation correctly computes the forward and backward passes. - In the backward pass, implement gradients manually. - Use the provided `Jacobian` and `Hessian` functions within `torch.autograd.functional`. **Example:** ```python import torch from torch.autograd import Function class QuadraticFunction(Function): @staticmethod def forward(ctx, x, A, b, c): ctx.save_for_backward(x, A, b) return x.T @ A @ x + b.T @ x + c @staticmethod def backward(ctx, grad_output): x, A, b = ctx.saved_tensors grad_x = 2 * A @ x + b grad_A = torch.outer(x, x).t() grad_b = x return grad_x, grad_A, grad_b, None def compute_hessian(A, b, c, x): x.requires_grad_(True) quadratic = lambda x: QuadraticFunction.apply(x, A, b, c) hessian = torch.autograd.functional.hessian(quadratic, x) return hessian # Example usage A = torch.tensor([[1.0, 0.0], [0.0, 1.0]], requires_grad=False) b = torch.tensor([1.0, 1.0], requires_grad=False) c = torch.tensor(1.0, requires_grad=False) x = torch.tensor([1.0, 2.0], requires_grad=True) hessian = compute_hessian(A, b, c, x) print(hessian) ``` Ensure that your implementation follows the constraints and produces the correct output given the example usage.","solution":"import torch from torch.autograd import Function class QuadraticFunction(Function): @staticmethod def forward(ctx, x, A, b, c): Forward pass for the quadratic function f(x) = x^T A x + b^T x + c. ctx.save_for_backward(x, A, b) return torch.dot(x, torch.mv(A, x)) + torch.dot(b, x) + c @staticmethod def backward(ctx, grad_output): Backward pass for computing the gradient of the quadratic function. x, A, b = ctx.saved_tensors grad_x = grad_output * (2 * torch.mv(A, x) + b) grad_A = grad_output * torch.outer(x, x) grad_b = grad_output * x # No gradient needed for c since it\'s a scalar constant return grad_x, grad_A, grad_b, None def compute_hessian(A, b, c, x): Computes the Hessian matrix of the quadratic function with respect to x. x.requires_grad_(True) quadratic = lambda x: QuadraticFunction.apply(x, A, b, c) hessian = torch.autograd.functional.hessian(quadratic, x) return hessian # Example usage A = torch.tensor([[1.0, 0.0], [0.0, 1.0]], requires_grad=False) b = torch.tensor([1.0, 1.0], requires_grad=False) c = torch.tensor(1.0, requires_grad=False) x = torch.tensor([1.0, 2.0], requires_grad=True) hessian = compute_hessian(A, b, c, x) print(hessian)"},{"question":"# Question: Site Module Customization in Python The `site` module in Python provides mechanisms for site-specific path configurations and handling customized library paths. You are required to implement a Python function that updates the `sys.path` based on custom site-specific and user-specific configurations. Function Signature: ```python def customize_sys_path(user_site_enabled: bool, site_dirs: list, customizations: dict) -> list: pass ``` Parameters: - **user_site_enabled** (`bool`): Indicates whether user site-packages should be considered (`True` or `False`). - **site_dirs** (`list of str`): A list of directory paths that should be added to `sys.path`, simulating `.pth` file additions. - **customizations** (`dict`): A dictionary where keys are strings representing modules (`\'sitecustomize\'`, `\'usercustomize\'`) and values are modules implementing customization functions. Returns: - **sys.path** (`list of str`): The updated `sys.path` after applying all customizations. Requirements and Constraints: 1. **Updating sys.path**: - Iterate over `site_dirs` and append them to `sys.path`. - Ensure even if a directory does not exist, it should not be added to `sys.path`. - Do not add any directory more than once. 2. **Customizations**: - If `user_site_enabled` is `True`, attempt to import and execute the function in the `customizations` dictionary for key `\'usercustomize\'`. - Always attempt to import and execute the function in the `customizations` dictionary for the key `\'sitecustomize\'`. - The customizations should be implemented as functions provided through the `customizations` dictionary (should handle no errors, assume existence and correctness). Example Usage: ```python def sitecustomize(): print(\\"Site customize function executed.\\") def usercustomize(): print(\\"User customize function executed.\\") customizations = { \'sitecustomize\': sitecustomize, \'usercustomize\': usercustomize } updated_path = customize_sys_path(True, [\'/custom/site-packages\', \'/another/path\'], customizations) print(updated_path) ``` Expected Output: ``` Site customize function executed. User customize function executed. [\'/usr/local/lib/pythonX.Y/site-packages\', \'/custom/site-packages\', \'/another/path\'] ``` Notes: - The sample `expected output` assumes initial `sys.path` and custom dirs do not overlap. - This case assumes `/custom/site-packages` and `/another/path` exist and are valid directories. - You must assume the `sys.path` has an initial state that includes typical site-packages paths as noted in standard Python installations.","solution":"import sys def customize_sys_path(user_site_enabled: bool, site_dirs: list, customizations: dict) -> list: Updates the sys.path based on custom site-specific and user-specific configurations. Parameters: - user_site_enabled: bool, whether user site-packages should be considered. - site_dirs: list of str, a list of directory paths to be added to sys.path. - customizations: dict, a dictionary with keys as \'sitecustomize\' and \'usercustomize\' and values as functions. Returns: - list of str, the updated sys.path after applying all customizations. # Adding site_dirs to sys.path if they do not exist for directory in site_dirs: if directory not in sys.path: sys.path.append(directory) # Ensure uniqueness of sys.path entries sys.path = list(dict.fromkeys(sys.path)) # Run \'sitecustomize\' function if provided if \'sitecustomize\' in customizations: customizations[\'sitecustomize\']() # Run \'usercustomize\' function if provided and user_site_enabled is True if user_site_enabled and \'usercustomize\' in customizations: customizations[\'usercustomize\']() return sys.path"},{"question":"# Advanced Cookie Handling with HTTP Requests You are required to write a Python function using the `http.cookiejar` module to interact with a web service requiring cookie-based authentication. Task 1. **Objective**: Write functions to: - Load cookies from a file. - Make an HTTP request using these cookies to a given URL. - Extract any new cookies from the response and save them back to the file. 2. **Implementation**: Create a Python class `CookieManager` that contains the following methods: - `__init__(self, cookie_file: str)`: Initialize the class with the path to a cookie file. - `load_cookies(self) -> None`: Load cookies from the specified file into a `MozillaCookieJar` instance. - `make_request(self, url: str) -> None`: Make an HTTP request to the given URL using the loaded cookies, then extract and save any new cookies from the response. - `save_cookies(self) -> None`: Save the current cookies back to the specified file. 3. **Constraints**: - Use the `MozillaCookieJar` class for cookie handling. - Ensure that any new cookies from the response are added to the existing cookies before saving them back to the file. - Handle any exceptions that may occur during loading, requesting, or saving cookies. Input and Output - **Input**: - The `__init__` method takes a single argument, `cookie_file`, which specifies the path to a cookie file. - The `make_request` method takes a single argument, `url`, which specifies the URL to make the request to. - **Output**: - There is no output from methods, but the cookies should be correctly saved back to the file after the request. Sample Usage ```python # Initialize CookieManager with a cookies file cm = CookieManager(\'cookies.txt\') # Load initial cookies from file cm.load_cookies() # Make an HTTP request, updating cookies as needed cm.make_request(\'http://example.com\') # Save cookies back to file cm.save_cookies() ``` Example of cookies.txt (Mozilla format): ``` # Netscape HTTP Cookie File # This is a generated file! Do not edit. .example.com TRUE / FALSE 1642646400 name1 value1 .example.com TRUE / TRUE 1642646400 name2 value2 ``` Implement the class `CookieManager` in a way that ensures the cookies are managed correctly through the HTTP requests.","solution":"import http.cookiejar import urllib.request class CookieManager: def __init__(self, cookie_file: str): self.cookie_file = cookie_file self.cookie_jar = http.cookiejar.MozillaCookieJar(cookie_file) def load_cookies(self) -> None: try: self.cookie_jar.load(ignore_discard=True, ignore_expires=True) except FileNotFoundError: print(f\\"Cookie file {self.cookie_file} not found. Creating a new one.\\") self.cookie_jar.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred while loading cookies: {e}\\") def make_request(self, url: str) -> None: opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cookie_jar)) try: response = opener.open(url) self.cookie_jar.extract_cookies(response, response) self.store_cookies() except Exception as e: print(f\\"An error occurred during the HTTP request: {e}\\") def store_cookies(self) -> None: try: self.cookie_jar.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred while saving cookies: {e}\\")"},{"question":"You are required to write a function that takes a string containing HTML entities and returns a safe HTML fragment by escaping certain characters and then unescaping any HTML entities. Your function should make use of the `html` module\'s `escape` and `unescape` functions. # Function Signature ```python def sanitize_html(input_string: str) -> str: pass ``` # Input - `input_string` (str): A string that may contain HTML special characters and entities. For example, `input_string` could be `\\"Hello & welcome to <Python> 3.10\'s demonstration!\\"`. # Output - (str): A safe HTML fragment with special characters escaped and HTML entities unescaped. For example, given the input `\\"Hello & welcome to <Python> 3.10\'s demonstration!\\"`, the output should be `\\"Hello &amp; welcome to &lt;Python&gt; 3.10\'s demonstration!\\"`. # Constraints 1. The input string may contain any characters, including special characters (`&`, `<`, `>`, `\\"`, `\'`) and HTML entities (e.g., `&gt;`, `&lt;`, `&#62;`, `&#x3e;`). 2. The input string length will not exceed 1000 characters. # Example ```python input_string = \\"Here is an example with greater than (>) and ampersand (&) signs.\\" # The function should first escape the special characters # \\"Here is an example with greater than (&gt;) and ampersand (&amp;) signs.\\" # Then, it should unescape any HTML entities # \\"Here is an example with greater than (>) and ampersand (&) signs.\\" assert sanitize_html(input_string) == \\"Here is an example with greater than (&gt;) and ampersand (&amp;) signs.\\" input_string = \\"Another <string> with \'quoted\' text and & entities.\\" assert sanitize_html(input_string) == \\"Another &lt;string&gt; with \'quoted\' text and &amp; entities.\\" ``` # Implementation Notes 1. Use the provided `html.escape` function to escape special characters. 2. Use the provided `html.unescape` function to convert HTML entities back to their corresponding Unicode characters. Feel free to create helper functions if needed.","solution":"import html def sanitize_html(input_string: str) -> str: Returns a safe HTML fragment with special characters escaped and unescaped HTML entities. :param input_string: A string that may contain HTML special characters and entities. :type input_string: str :return: A safe HTML fragment. :rtype: str # First, unescape any HTML entities unescaped_string = html.unescape(input_string) # Then, escape special HTML characters escaped_string = html.escape(unescaped_string) return escaped_string"},{"question":"Coding Assessment Question **Objective:** Write a Python function that manages environment variables and simulates system call errors using the functionalities provided by the \\"posix\\" module (imported through \\"os\\" module as recommended). Your function should: 1. Retrieve the current value of a specific environment variable. 2. Set a new value for this environment variable and verify that the value has been updated. 3. Handle the scenario where the environment variable does not exist, raising an appropriate exception. 4. Ensure compatibility with large file support (describe how you would handle this, you are not required to implement large file handling code). **Instructions:** - Implement the function `manage_environment_variable`. - The function should accept three parameters: 1. `var_name`: The name of the environment variable to manage. 2. `new_value`: The new value to set for the environment variable. 3. `check_large_file_support`: A boolean parameter. If set to True, ensure the code handles large file support in terms of environment setup (explain the considerations as comments). **Function Signature:** ```python def manage_environment_variable(var_name: str, new_value: str, check_large_file_support: bool) -> str: ``` **Input:** - `var_name`: A string representing the name of the environment variable (e.g., \\"HOME\\"). - `new_value`: A string representing the value to set for the environment variable (e.g., \\"/new/home/path\\"). - `check_large_file_support`: A boolean flag indicating whether to check for and handle large file support. **Output:** - Returns the new value of the environment variable if successfully updated. - Raises a `KeyError` if the environment variable does not exist initially (before setting the new value). **Constraints:** - The environment variable name should be a valid string keyword. - The new value should be a valid and writable path or string for the environment variable. - Performance is not a critical aspect, but ensure the solution is efficient for typical usage. **Example:** ```python try: result = manage_environment_variable(\\"HOME\\", \\"/new/home/path\\", True) print(\\"Updated environment variable:\\", result) except KeyError as e: print(\\"Error:\\", e) ``` **Notes:** 1. Remember to use the `os` module to perform environment variable management. 2. If `check_large_file_support` is True, describe in comments how your implementation considers large file support, although you are not required to implement actual large file handling.","solution":"import os def manage_environment_variable(var_name: str, new_value: str, check_large_file_support: bool) -> str: Manage the environment variable by setting a new value and ensuring it is updated. Parameters: var_name (str): The name of the environment variable. new_value (str): The new value to set for the environment variable. check_large_file_support (bool): A flag to check and handle large file support. Returns: str: The new value of the environment variable if successfully updated. Raises: KeyError: If the environment variable does not exist initially. # Check if the environment variable exists if var_name not in os.environ: raise KeyError(f\\"Environment variable \'{var_name}\' does not exist.\\") # Retrieve the current value of the environment variable current_value = os.environ[var_name] # Set the new value for the environment variable os.environ[var_name] = new_value # Verify that the value has been updated assert os.environ[var_name] == new_value, \\"The environment variable update failed.\\" # Handling large file support (description only) if check_large_file_support: # Typically, Python handles large files automatically if the underlying filesystem # supports it, but you might need to set certain environment variables, especially # for external programs that require large file support. # # Example considerations: # - Ensure system uses 64-bit file offsets. # - Set environment variables such as `export LARGEFILE=1` if the external program # requires it. # # Since these considerations depend on the system and the external tools involved, # they are described here for understanding. pass return new_value"},{"question":"**Question:** In this task, you are required to demonstrate your understanding of PyTorch and JIT (Just-In-Time) compilation. You will write code to define a simple neural network and then convert it into a TorchScript model using JIT. Finally, you will compare execution times of the original model and the TorchScript model. # Step-by-Step Requirements: 1. **Define a Neural Network:** - Define a simple feedforward neural network with one hidden layer using PyTorch\'s `torch.nn.Module`. - Ensure the network has a single input layer, one hidden layer, and an output layer. 2. **Convert to TorchScript Model:** - Convert this neural network model to TorchScript using PyTorch\'s JIT features. 3. **Performance Comparison:** - Write a function to compare the execution time of the original model and the TorchScript model on random input data. - You will execute each model a number of times to assess the average time taken for inference. # Input and Output Formats: Define Neural Network - The neural network should be comprised of: - An input layer of size 100. - A hidden layer of size 50. - An output layer of size 10. ```python class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() # Define the layers here def forward(self, x): # Define the forward pass here return x ``` Convert to TorchScript ```python import torch import time # Instantiate and convert the model as described model = SimpleNN() model_scripted = torch.jit.script(model) ``` Performance Comparison Function Function Signature: `def compare_execution_time(model: torch.nn.Module, model_scripted: torch.jit.ScriptModule, input_size: int, runs: int) -> dict:` - **Parameters**: - `model`: The original PyTorch model. - `model_scripted`: The converted TorchScript model. - `input_size`: Integer representing the size of random input tensor (suggested size `100`). - `runs`: Integer representing number of times to run inference for averaging execution time (suggested `1000`). - **Returns**: - A dictionary with keys `original_model_time` and `torchscript_model_time`, representing the average execution times of the models over the given number of runs. ```python def compare_execution_time(model: torch.nn.Module, model_scripted: torch.jit.ScriptModule, input_size: int, runs: int) -> dict: # Initialize input data input_data = torch.randn(input_size) # Measure execution time for original model start_time = time.time() for _ in range(runs): _ = model(input_data) original_model_time = (time.time() - start_time) / runs # Measure execution time for TorchScript model start_time = time.time() for _ in range(runs): _ = model_scripted(input_data) torchscript_model_time = (time.time() - start_time) / runs # Return the results return { \\"original_model_time\\": original_model_time, \\"torchscript_model_time\\": torchscript_model_time } ``` Example ```python # Example usage: comparison = compare_execution_time(SimpleNN(), model_scripted, 100, 1000) print(comparison) ``` # Constraints: - Ensure the PyTorch module and TorchScript module produce the same results for given input. - The results should validate that the JIT compilation improves the performance at least marginally.","solution":"import torch import time class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(100, 50) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x model = SimpleNN() model_scripted = torch.jit.script(model) def compare_execution_time(model: torch.nn.Module, model_scripted: torch.jit.ScriptModule, input_size: int, runs: int) -> dict: input_data = torch.randn(input_size) # Measure execution time for original model start_time = time.time() for _ in range(runs): _ = model(input_data) original_model_time = (time.time() - start_time) / runs # Measure execution time for TorchScript model start_time = time.time() for _ in range(runs): _ = model_scripted(input_data) torchscript_model_time = (time.time() - start_time) / runs return { \\"original_model_time\\": original_model_time, \\"torchscript_model_time\\": torchscript_model_time }"},{"question":"<|Analysis Begin|> The provided documentation outlines the platform-specific differences and limitations when using the \\"asyncio\\" module. This includes differences in event loop implementations, support for subprocesses, and the compatibility of certain methods on various platforms (Windows, macOS, etc.). Notably: - There are different event loops: \\"ProactorEventLoop\\" (default on Windows) and \\"SelectorEventLoop\\". - Specific methods are unsupported on Windows, such as `loop.create_unix_connection()`, `loop.create_unix_server()`, `loop.add_signal_handler()`, `loop.remove_signal_handler()`, among others. - On macOS, the \\"selectors.KqueueSelector\\" does not support character devices on versions <= 10.8, and an alternative setup is required. - The constraints for different event loops (e.g., \\"SelectorEventLoop\\" is limited to 512 sockets and subprocess support differences) are highlighted. Given this analysis, a good question to test fundamental and advanced understanding could involve creating an event loop with specific requirements and ensuring it works across different platforms, considering the documented limitations. <|Analysis End|> <|Question Begin|> **Question: Cross-Platform Asynchronous Event Loop Implementation** You are required to implement an asynchronous event loop using Python\'s `asyncio` module that demonstrates an understanding of the limitations and different behaviors on Windows and macOS platforms. Your task is to write a function `custom_event_loop` that sets up an event loop. This event loop should be capable of creating a TCP server and handling connections appropriately, while respecting the platform-specific constraints mentioned in the documentation provided. # Function Signature ```python import asyncio async def custom_event_loop(host: str, port: int): # Your implementation here. ``` # Input - `host` (str): The hostname or IP address where your server will be created. - `port` (int): The port number on which your server will listen. # Output Your function should not return anything. Instead, it should: - Set up and run the event loop that creates a TCP server. - Ensure the server is compatible with platform-specific constraints. - Handle a minimal functionality of reading/writing data from/to a connection to demonstrate the server is working. # Constraints - For Windows, use `ProactorEventLoop` (the preferred method) to set up the event loop. Make sure to avoid the methods that are unsupported on this platform. - For macOS (any version), ensure the event loop can handle character devices if the platform version is <= 10.8. - The solution should be compatible with both platforms and handle the platform-specific nuances. # Example Usage ```python import asyncio import platform import selectors async def custom_event_loop(host: str, port: int): async def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() writer.write(data) await writer.drain() writer.close() if platform.system() == \\"Windows\\": loop = asyncio.ProactorEventLoop() else: if platform.system() == \\"Darwin\\" and platform.mac_ver()[0] <= \\"10.8\\": selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) else: loop = asyncio.get_event_loop() server = await asyncio.start_server(handle_echo, host, port, loop=loop) async with server: await server.serve_forever() # Example: custom_event_loop(\'127.0.0.1\', 8888) ``` **Note:** Ensure that your implementation checks the platform using the `platform` module and correctly configures the event loop accordingly.","solution":"import asyncio import platform import selectors async def custom_event_loop(host: str, port: int): async def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() writer.write(data) await writer.drain() writer.close() if platform.system() == \\"Windows\\": loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: if platform.system() == \\"Darwin\\" and platform.mac_ver()[0] <= \\"10.8\\": selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() server = await asyncio.start_server(handle_echo, host, port, loop=loop) async with server: await server.serve_forever()"},{"question":"# Exception Handling and Custom Exception Class Implementation Background: In Python, robust error handling and custom exception creation are fundamental skills. Being able to raise meaningful exceptions and handle exceptions correctly ensures that your code is both resilient and maintainable. This task will assess your ability to: 1. Handle various built-in exceptions. 2. Create and raise custom exceptions. 3. Implement meaningful exception contexts and chains. Task: You are required to implement a function, `process_data(data)` that processes a list of integers, `data`. The processing involves the following operations: 1. Ensure the `data` contains only integers. If any element is not an integer, raise a `TypeError`. 2. If the list length is zero, raise a `ValueError` indicating that the list is empty. 3. Calculate the sum of all integers in the list. 4. If any value in the list causes the sum to exceed `1000`, raise an `OverflowError`. 5. Implement a custom exception `ProcessingError` that will handle any unexpected errors encountered during processing. Additionally, create another function, `safe_process(data)`, that calls `process_data(data)`, catches any exceptions raised and provides readable error messages based on the exception type. Implementation details: 1. **Function Signature**: ```python def process_data(data: list) -> int: pass def safe_process(data: list) -> None: pass ``` 2. **Input and Output**: - `process_data(data)`: - Input: `data` - a list of integers. - Output: Returns the sum of the integers if processed successfully. - `safe_process(data)`: - Input: `data` - a list of integers. - Output: Prints a readable error message if an error occurs, otherwise prints the result. 3. **Constraints**: - The sum of the integers in the list must not exceed 1000. - Handle only integer lists. - Provide meaningful messages for various errors in `safe_process`. Examples: ```python # Example 1: Handling valid data data = [10, 20, 30] print(safe_process(data)) # Output: The result is: 60 # Example 2: Handling non-integer values in the list data = [10, \'20\', 30] safe_process(data) # Output: TypeError: All elements must be integers # Example 3: Handling empty list data = [] safe_process(data) # Output: ValueError: List is empty # Example 4: Handling sum overflow data = [500, 600] safe_process(data) # Output: OverflowError: Sum exceeds 1000 # Example 5: Custom ProcessingError for unexpected exceptions data = None safe_process(data) # Output: ProcessingError: An unexpected error occurred - \'NoneType\' object is not iterable ``` Implement the described functionality considering proper use of Python\'s built-in exceptions and custom exceptions.","solution":"class ProcessingError(Exception): pass def process_data(data): if not isinstance(data, list): raise ProcessingError(\\"An unexpected error occurred - Input should be a list\\") if not data: raise ValueError(\\"List is empty\\") sum_data = 0 for item in data: if not isinstance(item, int): raise TypeError(\\"All elements must be integers\\") sum_data += item if sum_data > 1000: raise OverflowError(\\"Sum exceeds 1000\\") return sum_data def safe_process(data): try: result = process_data(data) print(f\'The result is: {result}\') except (TypeError, ValueError, OverflowError) as e: print(f\'{e.__class__.__name__}: {e}\') except Exception as e: raise ProcessingError(f\\"An unexpected error occurred - {e}\\") from e"},{"question":"# Pandas Configuration and Options Management Problem Statement You are given a dataset and tasked with manipulating a few of the pandas\' display options to format the presentation of the DataFrame. Your goal is to write a function that will perform the following: 1. Set the maximum number of rows displayed by pandas to `10` and the maximum number of columns to `5`. 2. Ensure that numeric data is displayed with a precision of `3` decimal places. 3. Enable the display of a DataFrame to stretch across pages (wrap around all columns) if it does not fit in the screen. 4. Use a context to temporarily change the `max_colwidth` to `50` and `chop_threshold` to `2`, displaying the dataset within this temporary configuration. 5. After the temporary display, reset all the options to their default values. Function Signature ```python import pandas as pd def configure_and_display(df: pd.DataFrame) -> pd.DataFrame: Configures pandas display options, temporarily changes other settings, and displays the DataFrame. Parameters: df (pd.DataFrame): The DataFrame to be displayed with adjusted settings. Returns: pd.DataFrame: The DataFrame after the temporary changes in display settings. pass ``` Instructions 1. **Step 1:** Set the `display.max_rows` to `10` and `display.max_columns` to `5` using `set_option`. 2. **Step 2:** Set the numeric display precision to `3` decimal places with `set_option`. 3. **Step 3:** Enable the display to stretch across pages (wrap all columns) by setting `expand_frame_repr` to `True`. 4. **Step 4:** Use the `option_context` to temporarily set `max_colwidth` to `50` and `chop_threshold` to `2`, and return the DataFrame representation within this context. 5. **Step 5:** Reset all the options to their default values after displaying the DataFrame. Examples For a given DataFrame `df`: ```python import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(15, 6), columns=[\'A\', \'B\', \'C\', \'D\', \'E\', \'F\']) configure_and_display(df) ``` The output should display the DataFrame adhering to the temporary settings and then reset all configurations to their default value afterward.","solution":"import pandas as pd def configure_and_display(df: pd.DataFrame) -> pd.DataFrame: Configures pandas display options, temporarily changes other settings, and displays the DataFrame. Parameters: df (pd.DataFrame): The DataFrame to be displayed with adjusted settings. Returns: pd.DataFrame: The DataFrame after the temporary changes in display settings. # Step 1: Set the display.max_rows to 10 and display.max_columns to 5 pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 5) # Step 2: Set the numeric display precision to 3 decimal places pd.set_option(\'display.precision\', 3) # Step 3: Enable the display to stretch across pages (wrap all columns) pd.set_option(\'display.expand_frame_repr\', True) # Step 4: Use the option_context to temporarily set max_colwidth and chop_threshold with pd.option_context(\'display.max_colwidth\', 50, \'display.chop_threshold\', 2): print(df) # Step 5: Reset all the options to their default values pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') pd.reset_option(\'display.expand_frame_repr\') return df"},{"question":"**Programming Assessment Question** # Objective: Implement a `PrecisionRecallDisplay` class to visualize the Precision-Recall curve using the Scikit-learn Plotting API principles. # Instructions: 1. Define a `PrecisionRecallDisplay` class with the following methods and attributes: - `__init__(self, precision, recall, average_precision, estimator_name)`: * Store the provided precision, recall, average_precision, and estimator_name. - `from_estimator(cls, estimator, X, y)`: * Compute precision and recall values from the given estimator and data. * Use `precision_recall_curve` from `sklearn.metrics` for calculation. * Compute the average precision score using `average_precision_score`. * Instantiate the `PrecisionRecallDisplay` object and return the result of calling the `plot` method on it. - `from_predictions(cls, y, y_pred, estimator_name)`: * Compute precision and recall from `y` and `y_pred`. * Calculate the average precision score. * Instantiate the `PrecisionRecallDisplay` object and return the result of calling the `plot` method on it. - `plot(self, ax=None, name=None, **kwargs)`: * Plot the precision-recall curve using `matplotlib`. * Customize the plot with labels, title, and legend. * Store `matplotlib` artists for further customization after plotting. 2. Implement a function to demonstrate the usage of `PrecisionRecallDisplay` with any classifier, for example, using Logistic Regression on a sample dataset. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, average_precision, estimator.__class__.__name__) return viz.plot() @classmethod def from_predictions(cls, y, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, average_precision, estimator_name) return viz.plot() def plot(self, ax=None, name=None, **kwargs): if ax is None: fig, ax = plt.subplots() else: fig = ax.figure line, = ax.plot(self.recall, self.precision, label=f\'{self.estimator_name} (AP={self.average_precision:.2f})\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend() self.line_ = line self.ax_ = ax self.figure_ = fig return self # Sample demonstration X, y = make_classification(n_samples=1000, n_classes=2, random_state=42) clf = LogisticRegression().fit(X, y) # Using the custom display class PrecisionRecallDisplay.from_estimator(clf, X, y) plt.show() ``` # Constraints: - Use only `scikit-learn`, `numpy`, and `matplotlib`. - Ensure that the class methods handle input types and dimensions correctly. - The `plot` method should correctly handle the default `None` value for the `ax` argument, creating and using its own axes if necessary. # Expected Output: The implementation should correctly plot the Precision-Recall curve for the provided estimator and dataset.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, average_precision, estimator.__class__.__name__) return viz.plot() @classmethod def from_predictions(cls, y, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, average_precision, estimator_name) return viz.plot() def plot(self, ax=None, name=None, **kwargs): if ax is None: fig, ax = plt.subplots() else: fig = ax.figure line, = ax.plot(self.recall, self.precision, label=f\'{self.estimator_name} (AP={self.average_precision:.2f})\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend() self.line_ = line self.ax_ = ax self.figure_ = fig return self"},{"question":"Objective Your task is to create a function that utilizes Seaborn\'s `husl_palette` to generate and visualize color palettes based on given parameters. Problem Statement Write a function `generate_color_palette` that takes in four parameters: - `num_colors` (int): The number of colors in the palette. - `lightness` (float): Lightness of the colors (default value `0.65`). - `saturation` (float): Saturation of the colors (default value `0.85`). - `start_hue` (float): Starting point for hue sampling (default value `0`). The function should: 1. Generate a HUSL color palette using the given parameters. 2. Plot these colors as a bar plot where each bar represents a color in the palette. Expected Input - `num_colors`: Integer (1 <= num_colors <= 20) - `lightness`: Float (0 <= lightness <= 1) - `saturation`: Float (0 <= saturation <= 1) - `start_hue`: Float (0 <= start_hue <= 1) Constraints and Limitations - The function should check the input parameters for valid ranges and raise an appropriate error if any parameter is outside its valid range. - Ensure the seaborn library is properly utilized. Expected Output - The function should display a bar plot with each bar having a different color from the generated palette. Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt def generate_color_palette(num_colors, lightness=0.65, saturation=0.85, start_hue=0): # Input validation if not (1 <= num_colors <= 20): raise ValueError(\\"num_colors must be between 1 and 20\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= start_hue <= 1): raise ValueError(\\"start_hue must be between 0 and 1\\") # Generate the color palette palette = sns.husl_palette(n_colors=num_colors, l=lightness, s=saturation, h=start_hue) # Create a bar plot to visualize the color palette plt.figure(figsize=(num_colors, 2)) plt.bar(range(num_colors), [1]*num_colors, color=palette, edgecolor=\'k\') plt.xticks([]) # Remove x-axis ticks plt.yticks([]) # Remove y-axis ticks plt.show() # Example function call generate_color_palette(8, lightness=0.5, saturation=0.8, start_hue=0.2) ``` This example call will display a bar plot with 8 bars, each representing a color from the HUSL palette generated with a lightness of 0.5, saturation of 0.8, and starting hue of 0.2.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_color_palette(num_colors, lightness=0.65, saturation=0.85, start_hue=0): Generate a HUSL color palette and plot it as a bar plot. Parameters: - num_colors (int): The number of colors in the palette. - lightness (float): Lightness of the colors (0 <= lightness <= 1). - saturation (float): Saturation of the colors (0 <= saturation <= 1). - start_hue (float): Starting point for hue sampling (0 <= start_hue <= 1). Raises: - ValueError: If any parameter is outside of its valid range. # Input validation if not (1 <= num_colors <= 20): raise ValueError(\\"num_colors must be between 1 and 20\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= start_hue <= 1): raise ValueError(\\"start_hue must be between 0 and 1\\") # Generate the color palette palette = sns.husl_palette(n_colors=num_colors, l=lightness * 100, s=saturation * 100, h=start_hue * 360) # Create a bar plot to visualize the color palette plt.figure(figsize=(num_colors, 2)) plt.bar(range(num_colors), [1]*num_colors, color=palette, edgecolor=\'k\') plt.xticks([]) # Remove x-axis ticks plt.yticks([]) # Remove y-axis ticks plt.show()"},{"question":"# Question: Implement a Custom Asynchronous Echo Server with Additional Logging As an exercise, you will implement a custom TCP echo server that logs each received message with a timestamp to a file and echoes the message back to the client. This will help you understand the use of `asyncio` streams for handling network connections, reading and writing data asynchronously, and incorporating simple file logging. Requirements: 1. **Server Implementation**: - Use `asyncio.start_server` to create the server. - The server should handle multiple clients concurrently. - For each client connection, read the incoming message, log it with a timestamp to a file, and then echo the message back to the client. 2. **Logging Details**: - Log each message to a file named `server.log`. - Each log entry should have a timestamp in the format `YYYY-MM-DD HH:MM:SS` followed by the client\'s message. 3. **Client Handling Function**: - Implement an asynchronous function `handle_client(reader, writer)` that reads data from the client, writes it to the log file, and sends it back to the client. 4. **Server Start Function**: - Implement the function `start_server()` to actually start the server and accept connections. Function Signatures: ```python import asyncio import datetime async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # Implement the client handling logic async def start_server(host: str, port: int): # Implement the server start logic # Example usage (Uncomment and run this part as a script): # if __name__ == \'__main__\': # asyncio.run(start_server(\'127.0.0.1\', 8888)) ``` Expected Behavior: - When a client connects and sends a message, the server should log the message with a timestamp to `server.log` and echo it back to the client. - The server should handle and log multiple clients\' messages concurrently. Constraints: - Use only the `asyncio` module for asynchronous operations. - Ensure the logging is atomic to prevent race conditions in a concurrent environment. Example Log Entries: ``` 2023-03-01 12:00:00 Client message 1 2023-03-01 12:00:05 Client message 2 ``` Consider this an opportunity to practice managing asynchronous operations and file I/O in a concurrent setting using Python\'s asyncio library.","solution":"import asyncio import datetime async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): data = await reader.read(100) # Read up to 100 bytes from the client message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_entry = f\\"{timestamp} {message}n\\" with open(\\"server.log\\", \\"a\\") as log_file: log_file.write(log_entry) writer.write(data) await writer.drain() print(f\\"Sent {message!r} to {addr!r}\\") writer.close() await writer.wait_closed() async def start_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # Example usage (Uncomment and run this part as a script): # if __name__ == \'__main__\': # asyncio.run(start_server(\'127.0.0.1\', 8888))"},{"question":"# Advanced Coding Challenge: Process and Analyze ZIP Archives Objective: Implement a Python function `process_zipfile(zip_path: str, extract_dir: str, filename_filter: str = \\"\\") -> dict` that reads, processes, and performs specific operations on a ZIP archive. Your function should demonstrate comprehension of the `zipfile` module\'s advanced concepts and functionalities. Requirements: 1. **Read and Validate**: - Check if the given `zip_path` is a valid ZIP file. If it is not, raise a `zipfile.BadZipFile` exception. 2. **Extract Files**: - Extract files into the directory specified by `extract_dir`. - Only extract files whose names contain the substring specified by `filename_filter`. If `filename_filter` is empty, extract all files. 3. **Gather Information**: - Return a dictionary where the keys are the filenames (relative paths within the ZIP file) and the values are dictionaries containing: - `file_size`: Size of the file. - `compress_size`: Size of the compressed file. - `date_time`: Tuple containing the last modification time. 4. **Context Manager Usage**: - Ensure your implementation uses context managers to handle the opening and closing of the ZIP files properly. 5. **Edge Case Handling**: - Ensure that the extraction directory is created if it does not exist. - Handle any exceptions that may arise during the extraction process and print appropriate error messages. Constraints: - Assume the ZIP file and directories do not exceed typical filesystem limitations. - Do not use any external libraries apart from Python\'s standard library. Example: ```python def process_zipfile(zip_path: str, extract_dir: str, filename_filter: str = \\"\\") -> dict: # Your implementation here # Example Usage: zip_info = process_zipfile(\\"example.zip\\", \\"extracted_files\\", \\"data\\") print(zip_info) ``` Given the ZIP file `example.zip` containing: ``` data1.txt data2.txt info.txt ``` and the call `process_zipfile(\\"example.zip\\", \\"extracted_files\\", \\"data\\")`, only `data1.txt` and `data2.txt` should be extracted. The resulting dictionary might look like: ```python { \\"data1.txt\\": {\\"file_size\\": 1234, \\"compress_size\\": 789, \\"date_time\\": (2023, 5, 21, 10, 15, 0)}, \\"data2.txt\\": {\\"file_size\\": 2345, \\"compress_size\\": 1234, \\"date_time\\": (2023, 5, 21, 10, 16, 0)}, } ``` Note: Ensure robust documentation within your code explaining each step and decision made in the process.","solution":"import os import zipfile def process_zipfile(zip_path: str, extract_dir: str, filename_filter: str = \\"\\") -> dict: Processes a ZIP file, extracting files and returning information about them. Parameters: zip_path (str): Path to the ZIP file. extract_dir (str): Directory to extract files into. filename_filter (str): Substring filter for filenames to be extracted. Returns: dict: Dictionary containing information about the files in the ZIP. Raises: zipfile.BadZipFile: If the specified file is not a valid ZIP file. # Check if the zip_path is a valid ZIP file if not zipfile.is_zipfile(zip_path): raise zipfile.BadZipFile(f\\"The file \'{zip_path}\' is not a valid ZIP file\\") # Create the extraction directory if it doesn\'t exist if not os.path.exists(extract_dir): os.makedirs(extract_dir) # Dictionary to store file information file_info_dict = {} # Open the ZIP file and process its contents with zipfile.ZipFile(zip_path, \'r\') as zip_ref: # Filter and extract files for zip_info in zip_ref.infolist(): if filename_filter in zip_info.filename: # Extract the file zip_ref.extract(zip_info, path=extract_dir) # Gather file information file_info_dict[zip_info.filename] = { \\"file_size\\": zip_info.file_size, \\"compress_size\\": zip_info.compress_size, \\"date_time\\": zip_info.date_time } return file_info_dict"},{"question":"File and Directory Management # Objective You are tasked to design a function that manages files and directories in a given directory subtree. The function will perform various operations like listing files, copying files to a backup directory, and deleting files based on specific patterns. # Problem Statement Write a Python function `manage_files_and_directories(base_dir, backup_dir)` that performs the following operations given a `base_dir` and a `backup_dir`: 1. **List Files and Directories**: Print out all files and directories within the `base_dir` recursively. 2. **Copy Files**: Copy all `.txt` files from `base_dir` to `backup_dir`. Ensure that the directory structure is maintained. 3. **Delete Files**: Delete all `.log` files within the `base_dir`. # Input - `base_dir` (string): The path to the base directory where operations will be performed. - `backup_dir` (string): The path to the directory where `.txt` files will be copied to. # Output The function does not return any value. However, it should print the list of files and directories and should perform the specified copy and delete operations. # Constraints - You can assume that `base_dir` and `backup_dir` are valid directories. - Handle any exceptions that arise during file operations gracefully by printing an error message and continuing with the next operation. # Example ```python manage_files_and_directories(\'/path/to/base_dir\', \'/path/to/backup_dir\') ``` **Expected Behavior**: - Prints the structure of `base_dir` showing all files and directories recursively. - Copies all `.txt` files to `backup_dir` maintaining the directory structure. - Deletes all `.log` files from `base_dir`. # Performance Requirements - The function should handle directories that have a significant number of files and nested subdirectories efficiently. # Hints - Utilize the `os`, `shutil`, `glob`, and `pathlib` modules to accomplish the task. - Ensure that you handle any potential errors that might arise during file operations (e.g., missing files, permission errors). **Sample Implementation**: ```python import os import shutil from pathlib import Path def manage_files_and_directories(base_dir, backup_dir): # Ensure backup directory exists os.makedirs(backup_dir, exist_ok=True) for root, dirs, files in os.walk(base_dir): print(f\\"Root: {root}\\") for dir in dirs: print(f\\"Directory: {dir}\\") for file in files: print(f\\"File: {file}\\") # Construct full file path file_path = Path(root) / file relative_path = file_path.relative_to(base_dir) # Copy .txt files to backup directory if file.endswith(\'.txt\'): backup_path = Path(backup_dir) / relative_path backup_path.parent.mkdir(parents=True, exist_ok=True) shutil.copy(file_path, backup_path) # Delete .log files elif file.endswith(\'.log\'): try: file_path.unlink() except Exception as e: print(f\\"Error deleting file {file_path}: {e}\\") # Example usage # manage_files_and_directories(\'/path/to/base_dir\', \'/path/to/backup_dir\') ```","solution":"import os import shutil from pathlib import Path def manage_files_and_directories(base_dir, backup_dir): # Ensure backup directory exists os.makedirs(backup_dir, exist_ok=True) for root, dirs, files in os.walk(base_dir): print(f\\"Root: {root}\\") for dir in dirs: print(f\\"Directory: {dir}\\") for file in files: print(f\\"File: {file}\\") # Construct full file path file_path = Path(root) / file relative_path = file_path.relative_to(base_dir) # Copy .txt files to backup directory if file.endswith(\'.txt\'): backup_path = Path(backup_dir) / relative_path backup_path.parent.mkdir(parents=True, exist_ok=True) shutil.copy(file_path, backup_path) # Delete .log files elif file.endswith(\'.log\'): try: file_path.unlink() except Exception as e: print(f\\"Error deleting file {file_path}: {e}\\") # Example usage # manage_files_and_directories(\'/path/to/base_dir\', \'/path/to/backup_dir\')"},{"question":"# Distributed Training with PyTorch Elastic: Resilient Worker Coordination You are tasked with implementing a fault-tolerant and elastic distributed training setup using PyTorch\'s `elastic` package. Specifically, you will focus on creating an agent that manages worker processes for distributed training and handles unexpected worker failures by restarting them. Additionally, you will set up a mechanism for the workers to coordinate and ensure that the training continues smoothly even in the face of worker failures. Requirements 1. **Agent Setup**: - Implement an agent that launches multiple worker processes for distributed training. - The agent should be able to restart a worker if it fails unexpectedly. 2. **Worker Process**: - Each worker process should execute a simple training loop. - Implement a mechanism for workers to check in regularly with the agent for health monitoring. 3. **Coordination and Fault Tolerance**: - Use a rendezvous mechanism to allow workers to discover each other and coordinate. - Implement a retry mechanism for the workers to rejoin the training session if they encounter a failure. Input - **num_workers**: Integer specifying the number of worker processes to launch. - **max_retries**: Integer specifying the maximum number of retries for a failed worker. Output - The implementation should print the status of each worker (e.g., \\"Worker X: Training Step Y\\"). - If a worker fails and is restarted, it should print an appropriate message indicating the restart. Constraints - Ensure that the coordination mechanism is scalable and can handle dynamic changes in worker availability. - Workers should be able to resume training from their last known state after a restart. Example ```python def main(num_workers: int, max_retries: int): # Initialize and start the agent agent = MyElasticAgent(num_workers, max_retries) agent.start() class MyElasticAgent: def __init__(self, num_workers, max_retries): self.num_workers = num_workers self.max_retries = max_retries self.workers = [] def start(self): for i in range(self.num_workers): self._start_worker(i) def _start_worker(self, worker_id): # Implement the logic to start a worker process pass def _restart_worker(self, worker_id): # Implement the logic to restart a worker process pass class MyWorker: def __init__(self, worker_id): self.worker_id = worker_id def run(self): step = 0 while True: try: # Simulate a training step step += 1 print(f\\"Worker {self.worker_id}: Training Step {step}\\") # Simulate a health check-in self._health_check() except Exception as e: print(f\\"Worker {self.worker_id} failed: {str(e)}\\") raise def _health_check(self): # Implement the health check logic to communicate with the agent pass # Example usage main(num_workers=4, max_retries=3) ``` # Additional Notes - Please make sure to handle any specific errors or exceptions that could occur during the training steps and health check-ins. - You can simulate worker failures by raising exceptions intentionally in the worker\'s `run` method. - Feel free to use any utility methods within the `elastic` package to aid in the implementation.","solution":"import multiprocessing import time import random class MyElasticAgent: def __init__(self, num_workers, max_retries): self.num_workers = num_workers self.max_retries = max_retries self.workers = {} def start(self): for i in range(self.num_workers): self._start_worker(i) def _start_worker(self, worker_id): retries = 0 while retries <= self.max_retries: process = multiprocessing.Process(target=self._worker_process, args=(worker_id,)) self.workers[worker_id] = process process.start() process.join() if process.exitcode == 0: break else: retries += 1 print(f\\"Worker {worker_id} failed. Retrying {retries}/{self.max_retries}...\\") def _worker_process(self, worker_id): worker = MyWorker(worker_id) worker.run() class MyWorker: def __init__(self, worker_id): self.worker_id = worker_id def run(self): step = 0 while True: try: # Simulate a training step step += 1 if random.random() < 0.1: # Simulating random failure raise RuntimeError(\\"Simulated failure\\") print(f\\"Worker {self.worker_id}: Training Step {step}\\") time.sleep(1) # Simulate the time taken in a training step # Simulate a health check-in self._health_check() except Exception as e: print(f\\"Worker {self.worker_id} failed: {str(e)}\\") raise def _health_check(self): # Simulate health check logic print(f\\"Worker {self.worker_id}: Health check OK\\") time.sleep(0.1) def main(num_workers: int, max_retries: int): agent = MyElasticAgent(num_workers, max_retries) agent.start()"},{"question":"You are required to perform complex archive manipulation using the `tarfile` module, demonstrating your understanding of object handling, safe extraction, and custom functions. The task involves implementing a function to extract all files from a tar archive using secure filtering, verify contents, and then recompress them with a different compression method. # Task Write a Python function named `extract_and_recompress` that accepts three parameters: 1. `input_tar_path` (str): The path to the input tarfile. 2. `output_tar_path` (str): The path to the output tarfile. 3. `compression` (str): The compression method for the output tarfile. It can be `\'gz\'`, `\'bz2\'`, or `\'xz\'`. The function should perform the following steps: 1. **Extract**: Safely extract all files from `input_tar_path` using the secure `\'data\'` filter. 2. **List Contents**: Print the list of extracted files with their names and sizes. 3. **Recompress**: Create a new tarfile at `output_tar_path` using the specified `compression` method and add all extracted files into it. # Input 1. `input_tar_path`: A string representing the file path to the input tar archive (tarfile). 2. `output_tar_path`: A string representing the file path for the output tar archive (tarfile). 3. `compression`: A string indicating the compression method for the output file (`\'gz\'`, `\'bz2\'`, `\'xz\'`). # Output The function does not return any value but should print the names and sizes of the files extracted and include them in the new tarfile. # Constraints 1. Assume the input tar archive is valid and exists at the given path. 2. The compression method provided will be one of the valid options (`\'gz\'`, `\'bz2\'`, `\'xz\'`). 3. The input tarfile may contain nested directories. # Example Usage ```python extract_and_recompress(\\"sample.tar.gz\\", \\"output.tar.bz2\\", \\"bz2\\") ``` This call extracts files from `sample.tar.gz`, lists the contents, and then recompresses them into `output.tar.bz2` using `bz2` compression. # Implementation You should make use of the `tarfile` module’s functions and classes, especially focusing on secure extraction with the `data_filter` and handling file objects properly during extraction and recompression.","solution":"import tarfile import os def extract_and_recompress(input_tar_path, output_tar_path, compression): Extracts all files from a tar archive securely using the \'data\' filter, lists extracted file names and sizes, and recompresses them using the specified method. :param input_tar_path: Path to the input tar file. :param output_tar_path: Path to the output tar file. :param compression: Compression method for the output tarfile (\'gz\', \'bz2\', \'xz\'). # Map compression method to mode mode = { \'gz\': \'w:gz\', \'bz2\': \'w:bz2\', \'xz\': \'w:xz\' }[compression] extracted_files = [] # Open the input tar file with tarfile.open(input_tar_path, \'r\') as input_tar: # Extract all files using \'data\' filter for security input_tar.extractall(filter=\'data\') extracted_files = input_tar.getmembers() # Print the names and sizes of the extracted files for member in extracted_files: print(f\\"Extracted: {member.name}, Size: {member.size} bytes\\") # Create the output tar file with the specified compression with tarfile.open(output_tar_path, mode) as output_tar: # Iterate over the extracted files for file in extracted_files: # Add each file to the new tar archive if os.path.isfile(file.name): output_tar.add(file.name)"},{"question":"# Advanced Python Coding Assessment Context You are tasked to create a multi-threaded application that processes large datasets while logging various events. This will assess your understanding of threading and logging in Python. Problem Statement 1. **Data Processing Simulation:** - Create a function `process_data(data_chunk)` that simulates processing a chunk of data. The function should perform some computations (like sum of square roots) to simulate a time-consuming process. This function should sleep for 1 second to simulate processing delay. - Expected input: A list of numbers (`data_chunk`) - Expected output: The sum of the square roots of the numbers in the chunk. 2. **Threading Implementation:** - Write a class `DataProcessorThread` that inherits from `threading.Thread`. This class should: - Take a data chunk and a logging object as initialization parameters. - Override the `run()` method to call `process_data()` and log the start, end, and result of the processing. 3. **Logging Implementation:** - Configure the `logging` module to: - Log messages to a file named `processing.log`. - Include timestamps, log levels, and messages in the format: `\'%(asctime)s - %(levelname)s - %(message)s\'`. 4. **Main Application:** - Divide a large dataset into chunks. - Create and start multiple `DataProcessorThread` instances to process the chunks in parallel. - Ensure the main program waits for all threads to complete. - Log the start and completion of the entire multi-threaded processing. Constraints - The large dataset can be created by repeating a smaller list of numbers (e.g., `[i for i in range(1000)] * 10`). - Each data chunk for processing should not be larger than 1000 elements. - Use appropriate synchronization mechanisms if needed, to ensure the logging does not create race conditions. Boilerplate Code ```python import threading import logging import math import time # Simulate processing a chunk of data def process_data(data_chunk): result = sum(math.sqrt(num) for num in data_chunk) time.sleep(1) # Simulate delay return result # DataProcessorThread class class DataProcessorThread(threading.Thread): def __init__(self, data_chunk, logger): threading.Thread.__init__(self) self.data_chunk = data_chunk self.logger = logger def run(self): self.logger.info(f\\"Started processing chunk {self.data_chunk[:10]} ...\\") result = process_data(self.data_chunk) self.logger.info(f\\"Finished processing chunk with result: {result}\\") # Configure logging logging.basicConfig(filename=\'processing.log\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') logger = logging.getLogger() # Main function def main(): # Create a large dataset large_dataset = [i for i in range(1000)] * 10 chunk_size = 1000 data_chunks = [large_dataset[i:i + chunk_size] for i in range(0, len(large_dataset), chunk_size)] # Initialize and start threads threads = [] logger.info(\\"Starting multi-threaded data processing.\\") for chunk in data_chunks: thread = DataProcessorThread(chunk, logger) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() logger.info(\\"All data processing threads have completed.\\") if __name__ == \\"__main__\\": main() ``` Implement the code as described, ensuring that the threading and logging functionality operates correctly. Evaluation Criteria - Correct implementation and usage of threading. - Proper configuration and use of the logging module. - Accurate and efficient data processing simulation. - Thread safety and proper synchronization in logging.","solution":"import threading import logging import math import time # Simulate processing a chunk of data def process_data(data_chunk): result = sum(math.sqrt(num) for num in data_chunk) time.sleep(1) # Simulate delay return result # DataProcessorThread class class DataProcessorThread(threading.Thread): def __init__(self, data_chunk, logger): threading.Thread.__init__(self) self.data_chunk = data_chunk self.logger = logger def run(self): self.logger.info(f\\"Started processing chunk {self.data_chunk[:10]} ...\\") result = process_data(self.data_chunk) self.logger.info(f\\"Finished processing chunk with result: {result}\\") # Configure logging logging.basicConfig(filename=\'processing.log\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') logger = logging.getLogger() # Main function def main(): # Create a large dataset large_dataset = [i for i in range(1000)] * 10 chunk_size = 1000 data_chunks = [large_dataset[i:i + chunk_size] for i in range(0, len(large_dataset), chunk_size)] # Initialize and start threads threads = [] logger.info(\\"Starting multi-threaded data processing.\\") for chunk in data_chunks: thread = DataProcessorThread(chunk, logger) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() logger.info(\\"All data processing threads have completed.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Object-Oriented Programming with Python **Objective:** Write a Python class that demonstrates the use of inheritance, instance/class variables, iterators, and generators. **Problem Statement:** You are asked to create a class structure to model a library system. This system should have classes to represent books and libraries, and should utilize inheritance, iterators, and generators to manage and interact with the collections of books. **Tasks:** 1. **Define a `Book` class:** - Attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - Methods: - `__init__(self, title, author)`: Initializes a new book instance with a title and an author. 2. **Define a `LibraryItem` class:** - This will serve as a base class for all items in the library. - Attributes: - `id` (int): A unique identifier for the library item. - `instance_count` (class variable): A class-level counter to generate unique IDs for all instances. - Methods: - `__init__(self)`: Initializes a library item with a unique ID and increments the class-level counter. 3. **Define a `Library` class:** - Attributes: - `books` (list): A list to hold `Book` objects. - `name` (string): The name of the library. - Methods: - `__init__(self, name)`: Initializes the library with a name and an empty list of books. - `add_book(self, title, author)`: Adds a new `Book` to the library. - `__iter__(self)`: Returns an iterator object that allows iteration over the books in the library. 4. **Use Inheritance:** - The `Book` class should inherit from `LibraryItem`. 5. **Implement iterators:** - Use the `__iter__()` method in the `Library` class to iterate over books. - Define a new method `get_books_by_author(self, author)` in the `Library` class that yields books by a specific author using a generator. **Constraints:** - Assume the input will be well-formed and no need for input validation. - Use library name and author names with reasonable lengths (1 < len(name) <= 100). **Example Usage:** ```python # defining and using the Book and Library classes if __name__ == \\"__main__\\": library = Library(\\"City Library\\") library.add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\") library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\") library.add_book(\\"1984\\", \\"George Orwell\\") library.add_book(\\"Animal Farm\\", \\"George Orwell\\") print(\\"All books in the library:\\") for book in library: print(f\\"ID: {book.id}, Title: {book.title}, Author: {book.author}\\") print(\\"nBooks by George Orwell:\\") for book in library.get_books_by_author(\\"George Orwell\\"): print(f\\"ID: {book.id}, Title: {book.title}, Author: {book.author}\\") ``` **Expected Output:** ``` All books in the library: ID: 1, Title: The Great Gatsby, Author: F. Scott Fitzgerald ID: 2, Title: To Kill a Mockingbird, Author: Harper Lee ID: 3, Title: 1984, Author: George Orwell ID: 4, Title: Animal Farm, Author: George Orwell Books by George Orwell: ID: 3, Title: 1984, Author: George Orwell ID: 4, Title: Animal Farm, Author: George Orwell ``` **Instructions:** - Implement the `Book`, `LibraryItem`, and `Library` classes as described. - Ensure the `Book` class inherits from the `LibraryItem` class. - Write the `__iter__()` method to make the `Library` class iterable. - Use generators to implement the `get_books_by_author` method.","solution":"class LibraryItem: instance_count = 0 def __init__(self): self.id = LibraryItem.instance_count + 1 LibraryItem.instance_count += 1 class Book(LibraryItem): def __init__(self, title, author): super().__init__() self.title = title self.author = author class Library: def __init__(self, name): self.name = name self.books = [] def add_book(self, title, author): book = Book(title, author) self.books.append(book) def __iter__(self): return iter(self.books) def get_books_by_author(self, author): for book in self.books: if book.author == author: yield book"},{"question":"# Question: You are developing a secure file verification system that ensures the integrity and authenticity of files transferred over an insecure network. Your solution should use Python\'s `hashlib` module to achieve this. Requirements: 1. Implement a function `generate_file_hash(filepath: str, algorithm: str) -> str` that: - Takes the path to a file and the name of the hash algorithm as input. - Reads the file in chunks to avoid memory issues for large files. - Computes the hash of the file contents using the specified algorithm. - Returns the resulting hash as a hexadecimal string. ```python def generate_file_hash(filepath: str, algorithm: str) -> str: # Your implementation here ``` 2. Implement a function `verify_file_hash(filepath: str, expected_hash: str, algorithm: str) -> bool` that: - Takes the path to a file, the expected hash value, and the name of the hash algorithm as input. - Uses the `generate_file_hash` function to compute the actual hash of the file. - Compares the computed hash with the expected hash and returns `True` if they match, otherwise `False`. ```python def verify_file_hash(filepath: str, expected_hash: str, algorithm: str) -> bool: # Your implementation here ``` Input Constraints: - The file path will always be a valid path to an existing file. - Supported algorithms include all those guaranteed by the `hashlib` module (check `hashlib.algorithms_guaranteed` set). Example Usage: ```python # Assume \'example.txt\' is a file with known content. # Generating hash using SHA-256 algorithm. hash_value = generate_file_hash(\'example.txt\', \'sha256\') print(hash_value) # \'5d41402abc4b2a76b9719d911017c592\' (example output) # Verifying file integrity: is_valid = verify_file_hash(\'example.txt\', hash_value, \'sha256\') print(is_valid) # True (expected output) ``` Requirements and Constraints: - Ensure efficient memory usage by reading the file in chunks. - Handle potential exceptions, such as invalid algorithm names, appropriately. - Ensure the functions work with any file size that fits within system memory constraints. Note: Do not use any third-party libraries. The implementation should solely rely on Python\'s standard library, especially the `hashlib` module.","solution":"import hashlib def generate_file_hash(filepath: str, algorithm: str) -> str: Generate the hash for a file using the specified algorithm. :param filepath: Path to the file whose hash is to be computed. :param algorithm: Name of the hash algorithm to use. :return: Hexadecimal string of the computed hash value. try: hasher = hashlib.new(algorithm) except ValueError as e: raise ValueError(f\\"Invalid algorithm: {algorithm}\\") from e with open(filepath, \\"rb\\") as file: for chunk in iter(lambda: file.read(4096), b\\"\\"): hasher.update(chunk) return hasher.hexdigest() def verify_file_hash(filepath: str, expected_hash: str, algorithm: str) -> bool: Verify if the hash of a file matches the expected hash. :param filepath: Path to the file to be verified. :param expected_hash: The expected hash value to compare against. :param algorithm: Name of the hash algorithm to use. :return: True if the computed hash matches the expected hash, False otherwise. computed_hash = generate_file_hash(filepath, algorithm) return computed_hash == expected_hash"},{"question":"<|Analysis Begin|> The provided documentation focuses on handling the requirements of Batch Normalization in PyTorch when using `vmap`. It primarily addresses the issue of inplace updates to running statistics and provides multiple solutions to handle batch normalization without these updates by either substituting BatchNorm with GroupNorm or modifying the BatchNorm configuration to skip running stats. The primary points covered are: 1. The issue with inplace updates in BatchNorm when using `vmap`. 2. Four possible solutions to mitigate this issue: - Replacing BatchNorm with GroupNorm. - Using the `norm_layer` parameter to control normalization layers in torchvision models. - Using functorch\'s utility to remove running stats from BatchNorm. - Running the model in eval mode to prevent updates to running statistics. From this analysis, we can derive a coding question that focuses on the transformation of neural network layers from BatchNorm to GroupNorm and optionally ensures compatibility with `vmap`. The problem can be designed to test students on their ability to manipulate PyTorch modules, specifically focusing on handling normalization layers and understanding torch APIs. <|Analysis End|> <|Question Begin|> # PyTorch Coding Assessment Question **Objective:** You are required to transform a given neural network model that uses Batch Normalization into a model that uses Group Normalization. This ensures compatibility with the `vmap` function in PyTorch by avoiding inplace updates to running statistics. **Task:** Implement a function `transform_model_to_group_norm` which takes a PyTorch model and converts all BatchNorm layers to GroupNorm layers. The function should also ensure that each channel is treated separately (i.e., C == G). **Function Signature:** ```python def transform_model_to_group_norm(model: torch.nn.Module) -> torch.nn.Module: pass ``` **Input:** - `model`: A PyTorch neural network model (`torch.nn.Module`) which may contain multiple `BatchNorm2d` layers. **Output:** - The modified model where all `BatchNorm2d` layers have been replaced with `GroupNorm` layers, configured such that each channel is treated separately. **Constraints:** - The function should recursively search through the model\'s submodules to replace all instances of `BatchNorm2d`. - Do not assume the order or structure of the layers in the provided model. - You must handle any `BatchNorm2d` settings and transfer them correctly to the new `GroupNorm` layers. **Example:** ```python import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.max_pool2d(x, 2) x = F.relu(self.bn2(self.conv2(x))) x = F.max_pool2d(x, 2) x = x.view(-1, 32 * 6 * 6) x = F.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleCNN() transformed_model = transform_model_to_group_norm(model) # Verify that all BatchNorm layers are replaced with GroupNorm for module in transformed_model.modules(): assert not isinstance(module, nn.BatchNorm2d), \\"BatchNorm2d layer found in the model!\\" if isinstance(module, nn.GroupNorm): print(f\\"GroupNorm layer with {module.num_groups} groups and {module.num_channels} channels\\") ``` **Notes:** - The `transformed_model` should preserve the functionality of the original model but use `GroupNorm` instead of `BatchNorm2d`. - Ensure to use `C` as the number of groups in `GroupNorm` such that each channel is treated separately. **Hint:** You may find the function `torch.nn.Module.replace_module` useful for iterating and replacing submodules within the model.","solution":"import torch import torch.nn as nn def transform_model_to_group_norm(model: nn.Module) -> nn.Module: Transforms all BatchNorm2d layers in the model to GroupNorm layers such that each channel is treated separately. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_features = module.num_features group_norm = nn.GroupNorm(num_groups=num_features, num_channels=num_features, affine=True) setattr(model, name, group_norm) else: transform_model_to_group_norm(module) return model"},{"question":"You are given a directory containing multiple text files, each containing various logs. You are required to write a function that compresses all the text files into separate gzip format while maintaining their original filenames but with a `.gz` extension. Additionally, create a function to decompress those .gz files back, maintaining their original names. # Requirements: 1. **Compression Function:** A function named `compress_log_files(log_dir: str) -> None` which: - Takes one argument: - `log_dir` (str): The path to the directory containing the log files. - Compresses each text file in the given directory into gzip format. - The new compressed files should have the same name as the original files but with a `.gz` extension. 2. **Decompression Function:** A function named `decompress_log_files(compressed_dir: str) -> None` which: - Takes one argument: - `compressed_dir` (str): The path to the directory containing the compressed gz files. - Decompresses each gzip file in the given directory back into their original text format. - The decompressed files should have the same name as the original text files. # Constraints: - You can assume the directory paths provided are always valid. - Each directory contains only files relevant to the operation (i.e., no subdirectories or unrelated files). - The function should handle cases where the compression or decompression is interrupted and ensure the file system remains consistent. - Performance should be optimized for reading and writing large files. # Example: Suppose the directory `/logs` contains the files: `log1.txt`, `log2.txt`, `log3.txt`. After calling `compress_log_files(\'/logs\')`, the directory should contain: ``` log1.txt.gz log2.txt.gz log3.txt.gz ``` After calling `decompress_log_files(\'/logs\')`, the directory should contain: ``` log1.txt log2.txt log3.txt ``` # Notes: - Use the `gzip` module to handle the compression and decompression of files. - Proper exception handling must be implemented to ensure the process is robust. # Template: ```python import gzip import os import shutil def compress_log_files(log_dir: str) -> None: # Implement the compression function here def decompress_log_files(compressed_dir: str) -> None: # Implement the decompression function here ```","solution":"import gzip import os import shutil def compress_log_files(log_dir: str) -> None: Compresses all the text files in the given directory into gzip format. :param log_dir: The path to the directory containing the log files :type log_dir: str for filename in os.listdir(log_dir): if filename.endswith(\'.txt\'): filepath = os.path.join(log_dir, filename) compressed_filepath = filepath + \'.gz\' with open(filepath, \'rb\') as f_in: with gzip.open(compressed_filepath, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(filepath) def decompress_log_files(compressed_dir: str) -> None: Decompresses all the gzip files in the given directory back into their original text format. :param compressed_dir: The path to the directory containing the compressed gz files :type compressed_dir: str for filename in os.listdir(compressed_dir): if filename.endswith(\'.gz\'): filepath = os.path.join(compressed_dir, filename) original_filepath = filepath[:-3] with gzip.open(filepath, \'rb\') as f_in: with open(original_filepath, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(filepath)"},{"question":"**PyTorch Distributed Training with NCCL Debugging** You are tasked to simulate a distributed machine learning training setup using PyTorch\'s `torch.distributed` package with the NCCL backend. You need to demonstrate your understanding of how to configure various NCCL-related environment variables to handle error cases, enable monitoring, and gather debugging information effectively. # Task 1. **Setup the environment**: Write a function to set up the required environment variables for NCCL to enable certain debugging and monitoring features. 2. **Initialize the Process Group**: Write a function to initialize a PyTorch process group using the NCCL backend and verify its proper setup. 3. **Simulate Training**: Create a simple training simulation where multiple processes are spawned, and these processes participate in a collective communication operation (e.g., AllReduce). # Implementation Details 1. **Environment Configuration Function** - Function Name: `setup_nccl_environment` - Parameters: None - Returns: None - Description: Configure the following NCCL environment variables: * `TORCH_NCCL_ASYNC_ERROR_HANDLING`: Set this to `1` * `TORCH_NCCL_ENABLE_TIMING`: Set this to `1` * `TORCH_NCCL_ENABLE_MONITORING`: Set this to `1` * `TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC`: Set this to `30` Ensure that once this function is called, the environment variables are properly set. 2. **Process Group Initialization Function** - Function Name: `initialize_process_group` - Parameters: - `rank` (int): The rank of the process - `world_size` (int): Total number of processes - Returns: None - Description: Initialize a process group with the NCCL backend and handle any exceptions appropriately. Make sure to finalize (destroy) the process group at the end. 3. **Distributed Training Simulation** - Script: `distributed_training.py` - Description: Write a script to launch multiple processes. Each process should: 1. Call the `setup_nccl_environment` function. 2. Initialize the process group by calling `initialize_process_group`. 3. Simulate a simple collective communication operation such as AllReduce on a tensor of ones. 4. Print out the result tensor and any debug information. 5. Ensure proper cleanup at the end of the communication. # Constraints - Assume a single machine environment with multiple GPUs. - Use Python\'s `multiprocessing` module for process management. - Ensure robust exception handling and proper cleanup of resources. # Example ```python import os import torch import torch.distributed as dist import torch.multiprocessing as mp def setup_nccl_environment(): os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_MONITORING\'] = \'1\' os.environ[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] = \'30\' def initialize_process_group(rank, world_size): dist.init_process_group( backend=\'nccl\', init_method=\'tcp://localhost:23456\', rank=rank, world_size=world_size ) def train(rank, world_size): setup_nccl_environment() initialize_process_group(rank, world_size) tensor = torch.ones(10).cuda(rank) dist.all_reduce(tensor, op=dist.ReduceOp.SUM) print(f\'Rank {rank} result:\', tensor) dist.destroy_process_group() if __name__ == \'__main__\': world_size = 4 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) ``` **Note**: Ensure PyTorch is installed with NCCL support, and the script should be run on a system with CUDA-enabled GPUs.","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp def setup_nccl_environment(): os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_MONITORING\'] = \'1\' os.environ[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] = \'30\' def initialize_process_group(rank, world_size): dist.init_process_group( backend=\'nccl\', init_method=\'tcp://localhost:23456\', rank=rank, world_size=world_size ) def train(rank, world_size): setup_nccl_environment() initialize_process_group(rank, world_size) tensor = torch.ones(10).cuda(rank) dist.all_reduce(tensor, op=dist.ReduceOp.SUM) print(f\'Rank {rank} result:\', tensor) dist.destroy_process_group() if __name__ == \'__main__\': world_size = 4 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"Coding Assessment Question # Objective Implement a function and a class using advanced type hinting concepts from the `typing` module in Python. The goal is to demonstrate your understanding of generics, type variables, and custom type annotations. # Task 1. **Function Implementation**: Implement a function `merge_dicts` that takes two dictionaries as inputs and merges them into one. The types for the keys and values of the dictionaries should be generic and should allow merging dictionaries with the same key types but different value types. 2. **Class Implementation**: Implement a generic class `DataWrapper` that stores an object of any type and provides methods to get and set the value with proper type annotations. # Requirements 1. **Function `merge_dicts`** - **Input**: Two dictionaries, `dict1` and `dict2`. ```python def merge_dicts(dict1: Dict[K, V1], dict2: Dict[K, V2]) -> Dict[K, Union[V1, V2]]: ``` - **Output**: A single dictionary that combines the key-value pairs from both input dictionaries. If a key exists in both dictionaries, the value from the second dictionary should be used. - **Constraints**: - The keys of the dictionaries are of the same type. - The values can be of different types. 2. **Class `DataWrapper`** - **Attributes**: - `value`: Stores the wrapped value of generic type. - **Methods**: ```python class DataWrapper(Generic[T]): def __init__(self, value: T) -> None: pass def get_value(self) -> T: pass def set_value(self, value: T) -> None: pass ``` - **Constraints**: - The class should be generic and should accept any type for the `value`. # Example ```python # Example usage of merge_dicts dict1 = {\'a\': 1, \'b\': 2, \'c\': 3} dict2 = {\'b\': \'two\', \'d\': \'four\'} result = merge_dicts(dict1, dict2) # result -> {\'a\': 1, \'b\': \'two\', \'c\': 3, \'d\': \'four\'} # Example usage of DataWrapper wrapper = DataWrapper(5) print(wrapper.get_value()) # Output: 5 wrapper.set_value(10) print(wrapper.get_value()) # Output: 10 ``` # Notes - Use the `typing` module to specify types and generics. - Ensure that your implementation is type-safe and leverages the power of Python type hints.","solution":"from typing import Dict, TypeVar, Union, Generic K = TypeVar(\'K\') V1 = TypeVar(\'V1\') V2 = TypeVar(\'V2\') T = TypeVar(\'T\') def merge_dicts(dict1: Dict[K, V1], dict2: Dict[K, V2]) -> Dict[K, Union[V1, V2]]: Merges two dictionaries. If a key exists in both dictionaries, the value from the second dictionary is used. result = dict1.copy() result.update(dict2) return result class DataWrapper(Generic[T]): def __init__(self, value: T) -> None: self._value = value def get_value(self) -> T: return self._value def set_value(self, value: T) -> None: self._value = value"},{"question":"# Advanced Type Hints and Generics in Python You are tasked with implementing a highly generic and type-safe function that filters items from a collection based on two criteria: whether the items match a specified type and whether they pass a user-defined predicate. Function Signature ```python from typing import TypeVar, Type, Callable, Iterable, List, Tuple T = TypeVar(\'T\') def filter_items(items: Iterable[object], typ: Type[T], predicate: Callable[[T], bool]) -> List[Tuple[T, bool]]: pass ``` # Objective Implement the function `filter_items` that takes the following parameters: 1. `items`: An iterable collection of objects. 2. `typ`: A type to filter items by. 3. `predicate`: A function that takes an item of the specified type and returns a boolean indicating whether the item satisfies some condition. The function should return a list of tuples where each tuple contains: - An item of the specified type that exists in the collection. - A boolean indicating whether the corresponding item passed the predicate check. # Constraints - The implementation should correctly infer the types and handle them accordingly. - Use type hints to ensure type correctness. - Utilize advanced type constructs where applicable (e.g., `TypeVar`, `Type`, `Callable`). # Example Usage ```python # Define a predicate function def is_positive(x: int) -> bool: return x > 0 # Define a mixed collection collection = [1, -2, \\"string\\", 3.5, 42, \\"another\\", 8] # Filter and check for positive integers in the collection result = filter_items(collection, int, is_positive) assert result == [(1, True), (-2, False), (42, True), (8, True)] ``` # Explanation In the example: - `filter_items` is called with a mixed collection, the type `int`, and a predicate `is_positive`. - The function returns a list of tuples, each containing an integer from the collection and a corresponding boolean indicating if the integer is positive. Craft your solution ensuring type safety and correctness. # Evaluation Criteria - Correct implementation of the `filter_items` function. - Appropriate use of type hints and advanced type constructs. - Handling edge cases and mixed-type collections properly. - Efficient and readable code.","solution":"from typing import TypeVar, Type, Callable, Iterable, List, Tuple T = TypeVar(\'T\') def filter_items(items: Iterable[object], typ: Type[T], predicate: Callable[[T], bool]) -> List[Tuple[T, bool]]: Filters items by type and predicate. Args: items (Iterable[object]): The collection of items to filter. typ (Type[T]): The type to filter items by. predicate (Callable[[T], bool]): A function that takes an item of type T and returns a boolean. Returns: List[Tuple[T, bool]]: A list of tuples, each containing an item of type T from the collection and a boolean indicating whether it passed the predicate check. filtered_items = [] for item in items: if isinstance(item, typ): result = predicate(item) filtered_items.append((item, result)) return filtered_items # Example usage: # def is_positive(x: int) -> bool: # return x > 0 # collection = [1, -2, \\"string\\", 3.5, 42, \\"another\\", 8] # result = filter_items(collection, int, is_positive) # print(result) # Should output: [(1, True), (-2, False), (42, True), (8, True)]"},{"question":"# Seaborn Advanced Plotting **Objective:** Demonstrate your understanding of Seaborn\'s advanced plotting capabilities using the `seaborn.objects` module. **Problem Statement:** You are provided with the `tips` dataset from Seaborn, which contains information about restaurant bills, including total bill, tip, gender, smoking status, day, time, and size of the party. Your task is to create a Seaborn plot that visualizes the relationship between total bill and tip with multiple layers, transformations, and customizations. **Requirements:** 1. Load the `tips` dataset using Seaborn\'s `load_dataset` function. 2. Create a scatter plot of `total_bill` against `tip` using `so.Plot`. 3. Add a layer that fits and plots a linear regression line (`Line`, `PolyFit`) to show the trend. 4. Add another layer that shows the distribution of total bill amounts as a bar plot separated by gender using `Dodge` and `Hist`. 5. Create a faceted plot separated by the `day` variable. 6. Customize the appearance by setting: - Different colors for the linear regression line and the bar plot. - Transparency for the scatter plot points. - Point size to represent the `size` variable. 7. Add appropriate labels for the x-axis, y-axis, and a title. 8. Include a legend that distinguishes between the different layers. **Constraints:** - Use the `seaborn.objects` module for all plotting. - The plot should be clean and presentable with no overlapping elements. - Use parameters and methods demonstrated in the provided documentation. **Expected Output:** The output should be a plot displayed inline in a Jupyter notebook or any other suitable environment. **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\") .add(so.Dot(alpha=0.6), pointsize=\\"size\\") .add(so.Line(color=\\"blue\\", linewidth=2), so.PolyFit()) .add(so.Bar(), so.Hist(), so.Dodge(), color=\\"sex\\") .facet(col=\\"day\\") .label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Total Bill vs Tip by Day\\") ) # Display the plot plot ``` **Note:** - Ensure that your code returns a plot object and displays it correctly. - Use appropriate colors, labels, and transparency levels to ensure clarity and readability of the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .add(so.Dot(alpha=0.6), pointsize=\\"size\\") .add(so.Line(color=\\"blue\\", linewidth=2), so.PolyFit()) .add(so.Bar(alpha=0.5), so.Hist(), so.Dodge()) .facet(col=\\"day\\") .label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Total Bill vs Tip by Day\\") ) # Display the plot plot.show()"},{"question":"Objective Create Python functions that mimics some of the behaviors of `PyCapsule`, including creating, setting, and getting attributes such as name or pointer. Problem Description You are required to implement a class named `PyCapsuleSim` in Python to simulate some functionalities of the `PyCapsule` described in the provided documentation. Your simulation should be capable of: 1. Initializing a capsule with a pointer and an optional name and destructor. 2. Setting and getting the name associated with the capsule. 3. Validating if an object is a capsule and checking the consistency of the name and pointer. Requirements 1. **Initialization**: The class should initialize with a given pointer and optionally, a name, and destructor. 2. **Set Name**: Implement a method `set_name` to set the name of the capsule, ensuring the name string is valid and non-null. 3. **Get Name**: Implement a method `get_name` to get the current name of the capsule. 4. **Set Pointer**: Implement a method `set_pointer` to set the internal pointer, ensuring the provided pointer is not null. 5. **Is Valid**: Implement a method `is_valid` to check the capsule\'s validity, ensuring that the pointer is non-null and the name matches a provided string exactly. Implementation ```python class PyCapsuleSim: def __init__(self, pointer, name=None, destructor=None): if pointer is None: raise ValueError(\\"Pointer cannot be null.\\") self.pointer = pointer self.name = name self.destructor = destructor self.is_destroyed = False def set_name(self, name): if name is None or not isinstance(name, str): raise ValueError(\\"Name must be a valid non-null string.\\") self.name = name def get_name(self): return self.name def set_pointer(self, pointer): if pointer is None: raise ValueError(\\"Pointer cannot be null.\\") self.pointer = pointer def is_valid(self, name): return (self.pointer is not None) and (self.name == name) # Example usage: capsule = PyCapsuleSim(pointer={\'some\': \'data\'}, name=\\"module.attribute\\") print(capsule.is_valid(\\"module.attribute\\")) # Should return True capsule.set_name(\\"new.module.attribute\\") print(capsule.get_name()) # Should return \\"new.module.attribute\\" ``` Constraints 1. Pointer must not be null during initialization or when setting a new pointer. 2. Name, if set, must be a non-null string. 3. The destructor is optional and does not need specific handling in this task.","solution":"class PyCapsuleSim: def __init__(self, pointer, name=None, destructor=None): if pointer is None: raise ValueError(\\"Pointer cannot be null.\\") self.pointer = pointer self.name = name self.destructor = destructor self.is_destroyed = False def set_name(self, name): if name is None or not isinstance(name, str): raise ValueError(\\"Name must be a valid non-null string.\\") self.name = name def get_name(self): return self.name def set_pointer(self, pointer): if pointer is None: raise ValueError(\\"Pointer cannot be null.\\") self.pointer = pointer def is_valid(self, name): return (self.pointer is not None) and (self.name == name)"},{"question":"# Python Coding Assessment Question Objective: Implement a Python function and disassemble it using the `dis` module. Then analyze the bytecode to extract specific details. Task: 1. Define a function named `process_list` with the following behavior: - The function takes a list of integers as an argument. - It filters out any number that is not a multiple of 3. - It squares each number in the filtered list. - It returns the modified list. 2. Write another function named `analyze_bytecode` that takes the `process_list` function as an argument and uses the `dis` module to: - Disassemble the `process_list` function. - Return a dictionary with the following bytecode details: - `total_instructions`: Total number of bytecode instructions. - `load_global_count`: Number of `LOAD_GLOBAL` instructions. - `binary_multiply_count`: Number of `BINARY_MULTIPLY` instructions. - `return_value_count`: Number of `RETURN_VALUE` instructions. Specifications: - Do not use any additional modules outside of the Python standard library. - Ensure the dictionary keys and counts are accurate based on the bytecode instructions of the `process_list` function. Example: ```python def analyze_bytecode_example(): # Implement analysis method here pass # This will be used to test disassembled_details = analyze_bytecode_example(process_list) ``` Example of `disassembled_details` output could be similar to: ```python { \'total_instructions\': 15, \'load_global_count\': 1, \'binary_multiply_count\': 1, \'return_value_count\': 1 } ``` This will help you demonstrate a thorough understanding of the \\"dis\\" module and how bytecode works in Python.","solution":"import dis def process_list(lst): Takes a list of integers, filters out numbers not multiple of 3, squares the remaining numbers, and returns the modified list. return [x ** 2 for x in lst if x % 3 == 0] def analyze_bytecode(func): Analyzes the bytecode of a given function and returns a dictionary with specific bytecode details. bytecode = dis.Bytecode(func) bytecode_details = { \'total_instructions\': 0, \'load_global_count\': 0, \'binary_multiply_count\': 0, \'return_value_count\': 0 } for instruction in bytecode: bytecode_details[\'total_instructions\'] += 1 if instruction.opname == \'LOAD_GLOBAL\': bytecode_details[\'load_global_count\'] += 1 elif instruction.opname == \'BINARY_MULTIPLY\': bytecode_details[\'binary_multiply_count\'] += 1 elif instruction.opname == \'RETURN_VALUE\': bytecode_details[\'return_value_count\'] += 1 return bytecode_details"},{"question":"# Question: Advanced Plot Customization with Seaborn You are provided with a dataset of tips received by waiters in a restaurant with details on the total bill, tip amount, gender, and other related information. Your task is to use Seaborn to create customized plots that demonstrate your understanding of the package\'s functionalities. Dataset The dataset to be used is Seaborn\'s built-in `tips` dataset, which can be loaded using: ```python data = sns.load_dataset(\\"tips\\") ``` Task 1. **Context Setting**: - Set the context of all plots to \\"talk\\" with a font scale of 1.5. 2. **Plot 1 - Scatter Plot**: - Create a scatter plot showing the relationship between the `total_bill` and `tip`. Use different colors for different genders. The marker size should correspond to the size of the party (`size` column). 3. **Plot 2 - Box Plot**: - Create a box plot showing the distribution of total bills for different days of the week. Customize the plot with a linewidth of 2 and use a pastel color theme. 4. **Plot 3 - Violin Plot**: - Create a violin plot showing the distribution of tips for different times of the day (`time` column). Overlay a strip plot on the violin plot. 5. **Plot 4 - FacetGrid**: - Create a FacetGrid of histograms showing the distribution of `total_bill` split by gender and smoker status (`smoker` column). 6. **Customization**: - For `Plot 1`, increase the linewidth of the markers\' edges to 1.5. - For `Plot 2`, use a Seaborn color palette for the boxes. - For `Plot 3`, adjust the transparency of the overlaying strip plot to make the underlying violin plot visible. - For `Plot 4`, add titles for each subplot with the format \\"Gender: {sex} | Smoker: {smoker}\\" and ensure the x-axis range is [0, 60]. Constraints and Requirements - Use Seaborn functions for all plot creation and customization tasks. - Ensure your code is modular, with clear comments explaining each step. - Handle any potential exceptions or errors gracefully. - Your code should be efficient and make use of seaborn\'s built-in capabilities without needing additional libraries, except for matplotlib for minor tweaks if necessary. Deliverables - A Python script or Jupyter notebook containing the implementation of the above tasks. - Plots displayed inline if using a Jupyter notebook. - Ensure that all necessary import statements and dataset loading commands are included at the beginning of your script or notebook. Expected Output Your submission should include the following: 1. Context setting command. 2. Four plots as specified with the mentioned customizations, displayed inline. 3. Comments and explanations within the code, elaborating on the choices made for customization and any challenges faced.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load Seaborn\'s built-in \'tips\' dataset data = sns.load_dataset(\\"tips\\") # Set context sns.set_context(\\"talk\\", font_scale=1.5) # Plot 1: Scatter Plot plt.figure(figsize=(10, 6)) plot_1 = sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", size=\\"size\\", sizes=(20, 200), edgecolor=\\"w\\", linewidth=1.5) plot_1.set_title(\\"Scatter Plot of Total Bill vs Tip\\") plt.show() # Plot 2: Box Plot plt.figure(figsize=(10, 6)) sns.set_palette(\\"pastel\\") plot_2 = sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=data, linewidth=2) plot_2.set_title(\\"Box Plot of Total Bill by Day\\") plt.show() # Plot 3: Violin + Strip Plot plt.figure(figsize=(10, 6)) plot_3 = sns.violinplot(x=\\"time\\", y=\\"tip\\", data=data, inner=None) plot_3 = sns.stripplot(x=\\"time\\", y=\\"tip\\", data=data, color=\\"k\\", alpha=0.3) plot_3.set_title(\\"Violin Plot of Tips by Time of Day\\") plt.show() # Plot 4: FacetGrid facet_grid = sns.FacetGrid(data, col=\\"sex\\", row=\\"smoker\\", margin_titles=True) facet_grid.map(plt.hist, \\"total_bill\\", bins=20, range=(0, 60), color=\\"skyblue\\") facet_grid.set_axis_labels(\\"Total Bill\\", \\"Count\\") facet_grid.set_titles(col_template=\\"Gender: {col_name}\\", row_template=\\"Smoker: {row_name}\\") plt.show()"},{"question":"**Objective:** Write a Python function that takes a directory path as input and performs a detailed analysis of all the files and subdirectories within it. Your function should use the `stat` module to determine the type of each file and its permissions. The results should be organized and printed in a readable format, specifying the type of each file and its permission bits. **Function Signature:** ```python def analyze_directory_structure(path: str) -> None: pass ``` **Input:** - `path` (str): The path of the directory to analyze. **Output:** - Your function should print out the analysis results in the following format: ``` Directory: /path/to/directory - File: /path/to/file1 (Type: Regular File, Permissions: -rwxr-xr-x) - Directory: /path/to/subdirectory (Type: Directory, Permissions: drwxr-xr-x) - File: /path/to/file2 (Type: Symbolic Link, Permissions: lrwxrwxrwx) ... ``` **Requirements:** 1. The function should handle nested subdirectories and analyze all files and directories recursively. 2. Use the `stat` module to determine the type and permissions of each file. Specifically, use functions like `S_ISREG`, `S_ISDIR`, `S_ISLNK`, etc., and `filemode(mode)` for permission formatting. 3. Print a clear and organized structure showing the path, type, and permissions of each file and directory. **Constraints:** - You may assume that the given `path` is a valid directory path. - Handle any potential errors (e.g., permissions issues) gracefully and continue processing the remaining files. **Example:** ```python import os import stat def analyze_directory_structure(path: str) -> None: def analyze_path(pathname): try: mode = os.lstat(pathname).st_mode permissions = stat.filemode(mode) if stat.S_ISDIR(mode): file_type = \\"Directory\\" elif stat.S_ISREG(mode): file_type = \\"Regular File\\" elif stat.S_ISLNK(mode): file_type = \\"Symbolic Link\\" elif stat.S_ISCHR(mode): file_type = \\"Character Device\\" elif stat.S_ISBLK(mode): file_type = \\"Block Device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISSOCK(mode): file_type = \\"Socket\\" else: file_type = \\"Unknown\\" print(f\\"- {file_type}: {pathname} (Permissions: {permissions})\\") except Exception as e: print(f\\"Error processing {pathname}: {e}\\") def walk_directory(path): for f in os.listdir(path): pathname = os.path.join(path, f) analyze_path(pathname) if stat.S_ISDIR(os.lstat(pathname).st_mode): walk_directory(pathname) print(f\\"Directory: {path}\\") walk_directory(path) # Example usage analyze_directory_structure(\'/path/to/directory\') ```","solution":"import os import stat def analyze_directory_structure(path: str) -> None: def analyze_path(pathname): try: mode = os.lstat(pathname).st_mode permissions = stat.filemode(mode) if stat.S_ISDIR(mode): file_type = \\"Directory\\" elif stat.S_ISREG(mode): file_type = \\"Regular File\\" elif stat.S_ISLNK(mode): file_type = \\"Symbolic Link\\" elif stat.S_ISCHR(mode): file_type = \\"Character Device\\" elif stat.S_ISBLK(mode): file_type = \\"Block Device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISSOCK(mode): file_type = \\"Socket\\" else: file_type = \\"Unknown\\" print(f\\"- {file_type}: {pathname} (Permissions: {permissions})\\") except Exception as e: print(f\\"Error processing {pathname}: {e}\\") def walk_directory(path): for f in os.listdir(path): pathname = os.path.join(path, f) analyze_path(pathname) if stat.S_ISDIR(os.lstat(pathname).st_mode): walk_directory(pathname) print(f\\"Directory: {path}\\") walk_directory(path) # Example usage: # analyze_directory_structure(\'/path/to/directory\')"},{"question":"**Objective:** Demonstrate your understanding of seaborn’s plotting capabilities, including creating layered plots with advanced transformations and visual clarity enhancements. **Problem Statement:** You are given two datasets: 1. `penguins` dataset containing the columns: `species`, `body_mass_g`. 2. `diamonds` dataset containing the columns: `carat`, `clarity`. Your task is to write a function, `create_plots`, that will create two seaborn plots demonstrating advanced plotting techniques as indicated below. **Function Signature:** ```python def create_plots(): pass ``` **Requirements:** 1. **Penguins Plot:** - Create a scatter plot of `species` vs. `body_mass_g` using `so.Plot`. - Add jitter to the dots in the plot to reduce overlapping (`so.Jitter`). - Add a range mark showing the 25th to 75th percentile of `body_mass_g` for each species. Push these ranges slightly to the right for better clarity (`so.Shift(x=0.2)`). 2. **Diamonds Plot:** - Create a scatter plot of `carat` vs. `clarity` using `so.Plot`. - Add jitter to the dots in the plot to reduce overlapping (`so.Jitter`). - Add a range mark showing the 25th to 75th percentile of `carat` for each clarity level. Push these ranges slightly downwards for better clarity (`so.Shift(y=0.25)`). **Input:** - No direct input to the function. The function should use the `penguins` and `diamonds` datasets loaded with `seaborn.load_dataset`. **Output:** - The function should create and display the two plots inline (for example, using `matplotlib.pyplot.show()`). **Constraints:** - Make sure the plot elements do not overlap significantly, that they are clearly visible, and the ranges are properly shifted as specified. **Performance Requirements:** - Plots should be generated quickly and efficiently, even if the datasets are reasonably large. An example call to your function: ```python create_plots() ``` **Notes:** - Ensure that the resulting plots are clear and well labeled. - Utilize the seaborn object-oriented API as demonstrated in the documentation.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_plots(): # Load the datasets penguins = sns.load_dataset(\'penguins\') diamonds = sns.load_dataset(\'diamonds\') # Penguins Plot penguins_plot = ( so.Plot(penguins, x=\'species\', y=\'body_mass_g\') .add(so.Dot(), so.Jitter(.3)) # Add jitter to reduce overlapping .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) # Add range mark shifted slightly to the right ) penguins_plot.show() # Diamonds Plot diamonds_plot = ( so.Plot(diamonds, x=\'carat\', y=\'clarity\') .add(so.Dot(), so.Jitter(.3)) # Add jitter to reduce overlapping .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) # Add range mark shifted slightly downwards ) diamonds_plot.show() # Uncomment to run manually # create_plots()"},{"question":"# PyTorch Special Functions Assessment Objective Your task is to implement a function using the `torch.special` module in PyTorch. Specifically, you will compute a customized metric for a given tensor using a combination of special functions provided in the module. Function Signature ```python def custom_special_metric(tensor: torch.Tensor) -> torch.Tensor: ``` Input - `tensor` (torch.Tensor): A 1-dimensional tensor containing floating-point numbers. The tensor can have a size up to 10,000 elements. Output - A single scalar value of type `torch.Tensor` which is the result of a series of computations using special functions. Requirements 1. Compute the element-wise Bessel function of the first kind of order 0 (`bessel_j0`). 2. Compute the element-wise exponential of the natural logarithm of 1 plus each element (`expit`). 3. Compute the element-wise scaled modified Bessel function of the first kind of order 0 (`i0e`). 4. Sum all the results obtained from steps 1 to 3. 5. Compute the logarithm of the sum obtained in step 4 (`log1p`). The final result should be a scalar tensor which is the value of the metric. Constraints - You must use the functions from the `torch.special` module. - Ensure that the operations are efficient and make use of PyTorch\'s vectorized operations to handle large tensors. Example ```python import torch import torch.special as special # Example tensor tensor = torch.tensor([0.5, 1.5, 2.5]) # Implement the function def custom_special_metric(tensor): bessel_j0 = special.bessel_j0(tensor) expit = special.expit(tensor) i0e = special.i0e(tensor) total_sum = torch.sum(bessel_j0 + expit + i0e) result = special.log1p(total_sum) return result result = custom_special_metric(tensor) print(result) # Expected output should be a scalar tensor ``` Ensure your implementation can handle edge cases such as very small or very large values within the input tensor.","solution":"import torch import torch.special as special def custom_special_metric(tensor: torch.Tensor) -> torch.Tensor: Compute a customized metric for a given tensor using a combination of special functions. Parameters: tensor (torch.Tensor): A 1-dimensional tensor containing floating-point numbers. Returns: torch.Tensor: A single scalar value which is the result of the computations. bessel_j0 = special.bessel_j0(tensor) expit = special.expit(tensor) i0e = special.i0e(tensor) total_sum = torch.sum(bessel_j0 + expit + i0e) result = torch.log1p(total_sum) return result"},{"question":"Objective: The objective of this assignment is to assess your understanding and ability to use the `unittest.mock` module, specifically `Mock` and `MagicMock`, to create effective unit tests for Python code. Problem Statement: You have been provided with a class that simulates interactions with an external service. Your task is to write tests for this class using the `unittest.mock` module to mock the interactions with the external service. Given Code: ```python class ExternalService: Simulates an interaction with an external service. def connect(self): Simulates establishing a connection to the service. pass def get_data(self, resource_id): Simulates getting data from the external service. pass def close(self): Simulates closing the connection to the service. pass class DataService: def __init__(self, external_service): self.external_service = external_service def fetch_resource(self, resource_id): Fetches data from an external service. self.external_service.connect() try: data = self.external_service.get_data(resource_id) finally: self.external_service.close() return data ``` Task: 1. Implement a test class `TestDataService` using `unittest.TestCase`. 2. Within `TestDataService`, write tests to verify the following functionalities of `DataService`: a. The `connect` method of `ExternalService` is called when `fetch_resource` is invoked. b. The `get_data` method of `ExternalService` is called with the correct `resource_id` argument. c. The `close` method of `ExternalService` is called even if `get_data` raises an exception. d. The `fetch_resource` method returns the correct data. 3. Use `unittest.mock` to mock out the `ExternalService` methods. # Constraints: 1. You may not modify the existing `ExternalService` or `DataService` classes. 2. All tests should use mocks to replace the functionalities of the `ExternalService`. Testing: Your tests should cover all specified functionalities and edge cases to ensure the `DataService` class behaves correctly under different scenarios. Example: ```python import unittest from unittest.mock import patch, Mock class TestDataService(unittest.TestCase): @patch(\'__main__.ExternalService\') def test_fetch_resource(self, MockExternalService): # Setup the mock instance = MockExternalService.return_value instance.get_data.return_value = {\'data\': \'value\'} # Initialize the Service with the mock service = DataService(instance) # Test fetch_resource result = service.fetch_resource(\'resource123\') # Assertions instance.connect.assert_called_once() instance.get_data.assert_called_once_with(\'resource123\') instance.close.assert_called_once() self.assertEqual(result, {\'data\': \'value\'}) @patch(\'__main__.ExternalService\') def test_fetch_resource_with_exception(self, MockExternalService): # Setup the mock instance = MockExternalService.return_value instance.get_data.side_effect = Exception(\\"Service Error\\") # Initialize the Service with the mock service = DataService(instance) # Testing exception scenario with self.assertRaises(Exception): service.fetch_resource(\'resource123\') # Assertions to ensure close is called even on exception. instance.connect.assert_called_once() instance.close.assert_called_once() if __name__ == \'__main__\': unittest.main() ``` You may run the above tests to verify the functionality of your `DataService` class using `unittest.mock`.","solution":"# We will use the provided class structure for this solution. # Importing required modules for mocking from unittest.mock import Mock class ExternalService: Simulates an interaction with an external service. def connect(self): Simulates establishing a connection to the service. pass def get_data(self, resource_id): Simulates getting data from the external service. pass def close(self): Simulates closing the connection to the service. pass class DataService: def __init__(self, external_service): self.external_service = external_service def fetch_resource(self, resource_id): Fetches data from an external service. self.external_service.connect() try: data = self.external_service.get_data(resource_id) finally: self.external_service.close() return data"},{"question":"You have been introduced to some of the deprecated buffer protocol functions in Python that were originally designed for version 2.x. Given the need for backward compatibility and understanding how to handle data buffers, we want to create a buffer inspection utility using these functions to better understand their usage and implications. # Task: Implement a class `BufferInspector` that provides utility methods to: 1. Check if an object supports a readable buffer. 2. Check if an object supports a writable buffer. 3. Retrieve and display the contents of a readable buffer as a string. 4. Retrieve and display the contents of a writable buffer as a list of integers (each representing a byte). # Requirements: 1. The class should be defined as follows: ```python class BufferInspector: @staticmethod def is_readable_buffer(obj) -> bool: # Implement this method @staticmethod def is_writable_buffer(obj) -> bool: # Implement this method @staticmethod def get_readable_buffer_content(obj) -> str: # Implement this method @staticmethod def get_writable_buffer_content(obj) -> list: # Implement this method ``` 2. Use the following deprecated functions to accomplish the tasks: - `PyObject_CheckReadBuffer` - `PyObject_AsReadBuffer` - `PyObject_AsWriteBuffer` 3. You may use the `ctypes` library to interact with these functions from Python. # Example Usage: ```python inspector = BufferInspector() # Example with a readable buffer readable_obj = b\\"Hello, world!\\" print(inspector.is_readable_buffer(readable_obj)) # Expected: True print(inspector.get_readable_buffer_content(readable_obj)) # Expected: \\"Hello, world!\\" # Example with a writable buffer writable_obj = bytearray(b\\"Hello, world!\\") print(inspector.is_writable_buffer(writable_obj)) # Expected: True print(inspector.get_writable_buffer_content(writable_obj)) # Expected: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33] ``` # Constraints: - Python\'s `ctypes` library should be used to load and call the required C functions. - Proper error checking and exception handling should be done to ensure robustness. - This exercise reinforces understanding and interaction with low-level memory management in Python. # Note: You can find more information about `ctypes` in the [ctypes documentation](https://docs.python.org/3/library/ctypes.html).","solution":"import ctypes class BufferInspector: @staticmethod def is_readable_buffer(obj) -> bool: try: buffer_obj = memoryview(obj) return buffer_obj.readonly is False or buffer_obj.readonly is True except TypeError: return False @staticmethod def is_writable_buffer(obj) -> bool: try: buffer_obj = memoryview(obj) return buffer_obj.readonly is False except TypeError: return False @staticmethod def get_readable_buffer_content(obj) -> str: if BufferInspector.is_readable_buffer(obj): return bytes(obj).decode(\'utf-8\', \'replace\') else: raise ValueError(\\"Object does not support readable buffer\\") @staticmethod def get_writable_buffer_content(obj) -> list: if BufferInspector.is_writable_buffer(obj): return list(obj) else: raise ValueError(\\"Object does not support writable buffer\\")"},{"question":"Your task is to implement a custom container class using Python\'s `collections.abc` module. The custom container should be a mutable sequence (similar to a list but with specific additional constraints). # Requirements: 1. **Class Definition**: - Define a class `CustomList` that inherits from `collections.abc.MutableSequence`. 2. **Methods to Implement**: - `__getitem__(self, index)`: Retrieve the item at the specified index. - `__setitem__(self, index, value)`: Set the item at the specified index. Ensure that only even numbers can be added to the list. - `__delitem__(self, index)`: Delete the item at the specified index. - `__len__(self)`: Return the number of items in the list. - `insert(self, index, value)`: Insert an item at the specified index. Ensure that only even numbers can be added to the list. 3. **Additional Requirements**: - Raise a `ValueError` if an attempt is made to add a number that is not even. - Including the mixin methods provided by `MutableSequence`, such as appending, extending, popping, etc., should work as expected. # Input/Output: - The class will be tested using various unit tests to ensure compliance with the `MutableSequence` behavior and the additional constraints regarding even numbers. - No specific input/output functions are required. The focus is solely on the class implementation. # Example Usage: ```python custom_list = CustomList() custom_list.append(2) custom_list.append(4) print(custom_list) # Output: CustomList([2, 4]) custom_list.insert(1, 6) print(custom_list) # Output: CustomList([2, 6, 4]) ``` # Constraints: - Only even numbers should be allowed in the list. - The class should be as performant as possible with respect to the operations defined in `MutableSequence`. Implement the `CustomList` class below: ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._data = [] def __getitem__(self, index): # Your code here pass def __setitem__(self, index, value): # Your code here pass def __delitem__(self, index): # Your code here pass def __len__(self): # Your code here pass def insert(self, index, value): # Your code here pass def __repr__(self): return f\'CustomList({self._data})\' # Example Usage custom_list = CustomList() custom_list.append(2) custom_list.append(4) print(custom_list) # Output should be CustomList([2, 4]) custom_list.insert(1, 6) print(custom_list) # Output should be CustomList([2, 6, 4]) try: custom_list.append(5) # Should raise ValueError except ValueError as e: print(e) # Output: Only even numbers are allowed ```","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._data = [] def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): if value % 2 != 0: raise ValueError(\\"Only even numbers are allowed\\") self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def insert(self, index, value): if value % 2 != 0: raise ValueError(\\"Only even numbers are allowed\\") self._data.insert(index, value) def __repr__(self): return f\'CustomList({self._data})\' # Example Usage custom_list = CustomList() custom_list.append(2) custom_list.append(4) print(custom_list) # Output should be CustomList([2, 4]) custom_list.insert(1, 6) print(custom_list) # Output should be CustomList([2, 6, 4]) try: custom_list.append(5) # Should raise ValueError except ValueError as e: print(e) # Output: Only even numbers are allowed"},{"question":"# Advanced Dictionary Operations in Python Python dictionaries are versatile and powerful data structures. Your task is to implement a few functions in Python that replicate the behavior of some advanced dictionary operations using basic Python dictionary methods. This involves creating a new dictionary, adding elements, checking for elements, iterating, and updating from another dictionary. Functions to Implement: 1. **create_dict**: - **Input**: None - **Output**: A new empty dictionary. ```python def create_dict(): pass ``` 2. **add_item**: - **Input**: A dictionary `d`, a key `k`, and a value `v`. - **Output**: None (Modifies the dictionary in place by adding the key-value pair). - **Constraints**: The key must be hashable. ```python def add_item(d, k, v): pass ``` 3. **get_item**: - **Input**: A dictionary `d` and a key `k`. - **Output**: The value associated with the key `k` if it exists, otherwise `None`. ```python def get_item(d, k): pass ``` 4. **contains_key**: - **Input**: A dictionary `d` and a key `k`. - **Output**: `True` if the key is in the dictionary, `False` otherwise. ```python def contains_key(d, k): pass ``` 5. **dict_size**: - **Input**: A dictionary `d`. - **Output**: The number of items in the dictionary. ```python def dict_size(d): pass ``` 6. **merge_dicts**: - **Input**: Two dictionaries `a` and `b`, a boolean `override`. - **Output**: A new dictionary resulting from merging `b` into `a`. If `override` is `True`, existing keys in `a` will be updated with those from `b`. ```python def merge_dicts(a, b, override): pass ``` 7. **iterate_dict**: - **Input**: A dictionary `d` - **Output**: A list of tuples `(key, value)` representing all key-value pairs in the dictionary. ```python def iterate_dict(d): pass ``` Constraints: - You must use Python\'s built-in dictionary methods. - You should handle exceptions and errors appropriately. - Aim for O(1) time complexity for individual insertions, deletions, and lookups. - Use appropriate memory management practices to avoid memory leaks. Example Usage: ```python d = create_dict() add_item(d, \'a\', 1) add_item(d, \'b\', 2) print(get_item(d, \'a\')) # Output: 1 print(contains_key(d, \'b\')) # Output: True print(dict_size(d)) # Output: 2 e = create_dict() add_item(e, \'b\', 3) add_item(e, \'c\', 4) merged_dict = merge_dicts(d, e, True) print(iterate_dict(merged_dict)) # Output: [(\'a\', 1), (\'b\', 3), (\'c\', 4)] ``` Implement the given functions in Python and test them with the provided example usage.","solution":"def create_dict(): Creates and returns a new empty dictionary. return {} def add_item(d, k, v): Adds a key-value pair to the dictionary \'d\'. d[k] = v def get_item(d, k): Returns the value associated with the key \'k\' in dictionary \'d\'. Returns None if the key does not exist. return d.get(k) def contains_key(d, k): Checks if key \'k\' is in dictionary \'d\'. Returns True if key is present, else False. return k in d def dict_size(d): Returns the number of items in the dictionary \'d\'. return len(d) def merge_dicts(a, b, override): Merges dictionary \'b\' into dictionary \'a\'. If \'override\' is True, existing keys in \'a\' will be updated with values from \'b\'. Returns the resulting merged dictionary. result = a.copy() if override: result.update(b) else: for key, value in b.items(): if key not in result: result[key] = value return result def iterate_dict(d): Returns a list of tuples representing the key-value pairs in dictionary \'d\'. return list(d.items())"},{"question":"# Function Object Introspection and Creation In this task, you will create a Python class that mimics some aspects of Python function objects\' interaction capabilities as defined in the CPython C API. The class should encapsulate a function and allow introspection and modification of its key attributes. Class Specification Implement a class `FunctionWrapper` with the following methods: 1. **`__init__(self, func)`**: * Initialize the class with a function object `func`. 2. **`get_code(self)`**: * Return the code object associated with the function. * Example: `FunctionWrapper(some_function).get_code()` 3. **`get_globals(self)`**: * Return the globals dictionary associated with the function. * Example: `FunctionWrapper(some_function).get_globals()` 4. **`get_module(self)`**: * Return the module name where the function is defined. * Example: `FunctionWrapper(some_function).get_module()` 5. **`get_defaults(self)`**: * Return the default values of the function arguments as a tuple. * Example: `FunctionWrapper(some_function).get_defaults()` 6. **`set_defaults(self, defaults)`**: * Set new default values for the function arguments. `defaults` must be a tuple. * Example: `FunctionWrapper(some_function).set_defaults((value1, value2))` 7. **`get_annotations(self)`**: * Return a dictionary of argument annotations. * Example: `FunctionWrapper(some_function).get_annotations()` 8. **`set_annotations(self, annotations)`**: * Set new annotations for the function arguments. `annotations` must be a dictionary. * Example: `FunctionWrapper(some_function).set_annotations({\'a\': int, \'return\': str})` Example Usage ```python def example_function(a: int, b: str = \\"default\\") -> None: This is an example function. return # Initialize the FunctionWrapper wrapper = FunctionWrapper(example_function) # Get code object code = wrapper.get_code() # Get globals dictionary globals_dict = wrapper.get_globals() # Get module name module_name = wrapper.get_module() # Get default values defaults = wrapper.get_defaults() # Set new default values wrapper.set_defaults((42, \\"changed default\\")) # Get annotations annotations = wrapper.get_annotations() # Set new annotations wrapper.set_annotations({\'a\': float, \'b\': str, \'return\': None}) ``` Constraints 1. Do not use any C API functions directly in your implementation. 2. Only use Python\'s introspection capabilities and standard library functions. 3. Ensure the class is robust and handles edge cases gracefully with appropriate error messaging. Submit your implementation of the class `FunctionWrapper` fulfilling the above requirements.","solution":"class FunctionWrapper: def __init__(self, func): if not callable(func): raise TypeError(\\"The provided object is not callable.\\") self.func = func def get_code(self): return self.func.__code__ def get_globals(self): return self.func.__globals__ def get_module(self): return self.func.__module__ def get_defaults(self): return self.func.__defaults__ def set_defaults(self, defaults): if not isinstance(defaults, tuple): raise TypeError(\\"Defaults must be a tuple.\\") self.func.__defaults__ = defaults def get_annotations(self): return self.func.__annotations__ def set_annotations(self, annotations): if not isinstance(annotations, dict): raise TypeError(\\"Annotations must be a dictionary.\\") self.func.__annotations__ = annotations"},{"question":"**Coding Assessment Question: String Formatting and File Operations** # Objective To evaluate your understanding of string formatting methods, file reading and writing, and using the `json` module in Python. # Problem Statement You are provided with a task to process a text file containing information about various items and their prices. You need to extract this data, perform some formatting operations, and save the processed data back to a file in JSON format. # Requirements 1. **Read the File:** - The input file `items.txt` contains lines in the format: `ItemName Price`. - Example: ``` Apple 1.20 Banana 0.80 Orange 0.90 ``` 2. **Format the Data:** - Parse each line to extract the item name and its price. - Format the item name to be right-justified within a field of width 10 and the price to two decimal places. 3. **Save as JSON:** - Create a dictionary with item names as keys and their prices as values. - Save this dictionary to a file named `formatted_items.json`. # Constraints - Assume the file `items.txt` exists in your current working directory and is correctly formatted as described. - Use formatted string literals (f-strings) or the `str.format()` method for formatting the output. - Use the `json` module for saving the data. # Input - `items.txt`: A text file containing lines of item names and their respective prices. # Output - `formatted_items.json`: A JSON file containing the formatted items and prices. # Example Given the input file `items.txt`: ``` Apple 1.20 Banana 0.80 Orange 0.90 ``` The expected output in `formatted_items.json` should be: ```json { \\"Apple\\": \\"1.20\\", \\"Banana\\": \\"0.80\\", \\"Orange\\": \\"0.90\\" } ``` # Function Signature ```python def format_and_save_items(input_file: str, output_file: str) -> None: pass ``` # Implementation Implementing `format_and_save_items` function that: 1. Reads the `input_file`. 2. Parses and formats the data. 3. Saves the formatted data to `output_file` in JSON format.","solution":"import json def format_and_save_items(input_file: str, output_file: str) -> None: Reads a file of items with prices, formats them, and saves them to a JSON file. Args: input_file (str): The path to the input text file. output_file (str): The path to the output JSON file. items_dict = {} # Read the input file with open(input_file, \'r\') as file: for line in file: item, price = line.strip().split() formatted_price = \\"{:.2f}\\".format(float(price)) items_dict[item] = formatted_price # Save to the output file in JSON format with open(output_file, \'w\') as json_file: json.dump(items_dict, json_file, indent=4)"},{"question":"**Objective**: Demonstrate your understanding of the seaborn package, specifically focusing on its `kdeplot` function. **Task:** You are required to analyze a dataset and generate visual representations of the distributions and relationships between variables using seaborn\'s `kdeplot` function. # Instructions: 1. Use the `iris` dataset available in seaborn. 2. Plot the univariate distribution of the `sepal_width` column. 3. Plot the bivariate distribution between `sepal_width` and `sepal_length`. 4. Create a plot showing conditional distribution of `sepal_width` based on the `species` column using different hues. 5. Stack the conditional distributions and normalize them. 6. Modify the appearance of the plot by filling the area under the KDE curve and using a custom colormap. # Expected Implementation: 1. **Univariate Distribution**: ```python import seaborn as sns import matplotlib.pyplot as plt iris = sns.load_dataset(\\"iris\\") sns.kdeplot(data=iris, x=\\"sepal_width\\") plt.show() ``` 2. **Bivariate Distribution**: ```python sns.kdeplot(data=iris, x=\\"sepal_width\\", y=\\"sepal_length\\") plt.show() ``` 3. **Conditional Distribution with Hue**: ```python sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\") plt.show() ``` 4. **Stacked Conditional Distribution**: ```python sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\", multiple=\\"stack\\") plt.show() ``` 5. **Normalized Stacked Conditional Distribution with Aesthetic Modifications**: ```python sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\", multiple=\\"fill\\", fill=True, palette=\\"viridis\\", alpha=0.5, linewidth=0) plt.show() ``` # Constraints: - Ensure all plots have appropriate labels and titles for better understanding. - Use a consistent theme across all plots. # Performance Requirements: - The solution should be optimized to handle the default datasets provided by seaborn without significant lag. # Submission: - Submit your code in a single Jupyter notebook demonstrating each step as a separate cell. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the \'iris\' dataset iris = sns.load_dataset(\\"iris\\") # Univariate Distribution of \'sepal_width\' def plot_univariate_distribution(): sns.kdeplot(data=iris, x=\\"sepal_width\\") plt.title(\'Univariate Distribution of Sepal Width\') plt.xlabel(\'Sepal Width\') plt.ylabel(\'Density\') plt.show() # Bivariate Distribution of \'sepal_width\' and \'sepal_length\' def plot_bivariate_distribution(): sns.kdeplot(data=iris, x=\\"sepal_width\\", y=\\"sepal_length\\") plt.title(\'Bivariate Distribution of Sepal Width and Sepal Length\') plt.xlabel(\'Sepal Width\') plt.ylabel(\'Sepal Length\') plt.show() # Conditional Distribution of \'sepal_width\' by \'species\' def plot_conditional_distribution(): sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\") plt.title(\'Conditional Distribution of Sepal Width by Species\') plt.xlabel(\'Sepal Width\') plt.ylabel(\'Density\') plt.show() # Stacked Conditional Distribution of \'sepal_width\' by \'species\' def plot_stacked_conditional_distribution(): sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\'Stacked Conditional Distribution of Sepal Width by Species\') plt.xlabel(\'Sepal Width\') plt.ylabel(\'Density\') plt.show() # Normalized Stacked Conditional Distribution with Aesthetic Modifications def plot_normalized_stacked_distribution(): sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\", multiple=\\"fill\\", fill=True, palette=\\"viridis\\", alpha=0.5, linewidth=0) plt.title(\'Normalized Stacked Conditional Distribution of Sepal Width by Species\') plt.xlabel(\'Sepal Width\') plt.ylabel(\'Density\') plt.show()"},{"question":"# Custom Autograd Function Implementation In this task, you will implement a custom autograd function in PyTorch by subclassing `torch.autograd.Function`. Your task is to create a custom autograd function named `CustomQuadraticFunction` that models the quadratic function ( y = x^2 + 3x + 2 ). 1. **Forward pass:** Implement the forward pass to compute the function ( y = x^2 + 3x + 2 ). 2. **Backward pass:** Implement the backward pass to compute the gradient of the output with respect to the input ( x ), i.e., (frac{dy}{dx}). **Requirements:** - Save the input ( x ) during the forward pass using `ctx.save_for_backward`. - Retrieve the saved input ( x ) in the backward pass by accessing `ctx.saved_tensors`. - Ensure that the custom implementation can handle multi-dimensional tensors and batch computations. **Inputs:** - A single PyTorch tensor `x` of any shape that requires gradient computation (`requires_grad=True`). **Outputs:** - Forward pass: Returns the tensor output of the quadratic function. - Backward pass: Returns the tensor representing the gradient of the quadratic function with respect to the input ( x ). Provide the complete implementation of the `CustomQuadraticFunction` by defining the `forward` and `backward` static methods. # Example Usage ```python # Import necessary libraries import torch # Define the custom autograd function class CustomQuadraticFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): # Save input tensor for backward pass ctx.save_for_backward(x) # Compute the output y = x**2 + 3*x + 2 return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, = ctx.saved_tensors # Compute the gradient of the output with respect to the input grad_input = grad_output * (2*x + 3) return grad_input # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = CustomQuadraticFunction.apply(x) y.sum().backward() print(x.grad) # Should print tensor([5., 7., 9.]) ``` # Constraints - Do not use any in-place operations on tensors. Submit your implementation for the `CustomQuadraticFunction` class.","solution":"import torch class CustomQuadraticFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): # Save input tensor for backward pass ctx.save_for_backward(x) # Compute the output: y = x^2 + 3x + 2 y = x ** 2 + 3 * x + 2 return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, = ctx.saved_tensors # Compute the gradient of the output with respect to the input: dy/dx = 2x + 3 grad_input = grad_output * (2 * x + 3) return grad_input # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = CustomQuadraticFunction.apply(x) y.sum().backward() print(x.grad) # Should print tensor([5., 7., 9.])"},{"question":"Objective: Write a Python program that utilizes the `webbrowser` module to open URLs in different browsers based on user input and environment variables. This will test your ability to use the `webbrowser` module effectively and to handle environment variables. Requirements: 1. **Functionality Specification**: - Create a function `open_url_in_browser(url: str, browser_name: str)` that attempts to open the given `url` in the specified browser `browser_name`. - If the specified browser is not available, the function should fallback to the default browser and print a message indicating the fallback. - Register at least one new browser using `webbrowser.register`. 2. **Environment Variable Handling**: - Create a function `set_browser_environment_variable(browser_list: List[str])` that sets the `BROWSER` environment variable to the given list of browsers (as a single string with browsers separated by the system path separator). - Create a function `get_browser_environment_variable() -> List[str]` that retrieves and returns the current `BROWSER` environment variable as a list of browsers. 3. **Command-Line Interface**: - Your program should accept the following command-line arguments: - `-u` or `--url` followed by a URL to open. - `-b` or `--browser` followed by a browser name to use. - `-e` or `--env` to print the current `BROWSER` environment variable. - Example usage: ```bash python browse.py -u http://example.com -b firefox python browse.py -e ``` Constraints: - The function `open_url_in_browser` must utilize the `webbrowser.get` method. - The fallback to the default browser must respect the value in the `BROWSER` environment variable (if it exists). Example: ```python import webbrowser import os import sys def open_url_in_browser(url: str, browser_name: str): try: browser = webbrowser.get(browser_name) browser.open_new_tab(url) print(f\\"Opened {url} in {browser_name}\\") except webbrowser.Error: print(f\\"{browser_name} is not available. Falling back to the default browser.\\") webbrowser.open_new_tab(url) def set_browser_environment_variable(browser_list): os.environ[\'BROWSER\'] = os.pathsep.join(browser_list) def get_browser_environment_variable(): browsers = os.getenv(\'BROWSER\') if browsers: return browsers.split(os.pathsep) return [] if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Open URL in specified browser\\") parser.add_argument(\'-u\', \'--url\', type=str, help=\\"URL to open\\", required=False) parser.add_argument(\'-b\', \'--browser\', type=str, help=\\"Browser to use\\", required=False) parser.add_argument(\'-e\', \'--env\', action=\'store_true\', help=\\"Print BROWSER environment variable\\") args = parser.parse_args() if args.env: browsers = get_browser_environment_variable() print(f\\"BROWSER environment variable: {browsers}\\") elif args.url and args.browser: open_url_in_browser(args.url, args.browser) else: parser.print_help() ``` Notes: - Ensure your program handles potential exceptions and edge cases (e.g., invalid browser names). - You may assume the user has the required browsers installed for the purpose of this assessment.","solution":"import webbrowser import os def open_url_in_browser(url: str, browser_name: str): Opens the given URL in the specified browser. If the browser is not available, falls back to the default browser. try: browser = webbrowser.get(browser_name) browser.open_new_tab(url) print(f\\"Opened {url} in {browser_name}\\") except webbrowser.Error: print(f\\"{browser_name} is not available. Falling back to the default browser.\\") webbrowser.open_new_tab(url) def set_browser_environment_variable(browser_list): Sets the BROWSER environment variable to the given list of browsers. os.environ[\'BROWSER\'] = os.pathsep.join(browser_list) def get_browser_environment_variable(): Retrieves the current BROWSER environment variable as a list of browsers. browsers = os.getenv(\'BROWSER\') if browsers: return browsers.split(os.pathsep) return [] if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Open URL in specified browser\\") parser.add_argument(\'-u\', \'--url\', type=str, help=\\"URL to open\\") parser.add_argument(\'-b\', \'--browser\', type=str, help=\\"Browser to use\\") parser.add_argument(\'-e\', \'--env\', action=\'store_true\', help=\\"Print BROWSER environment variable\\") args = parser.parse_args() if args.env: browsers = get_browser_environment_variable() print(f\\"BROWSER environment variable: {browsers}\\") elif args.url and args.browser: open_url_in_browser(args.url, args.browser) else: parser.print_help()"},{"question":"Optimizing Parallelism in Scikit-Learn Objective: You are given a machine learning task that requires training a model on a large dataset. To efficiently utilize the computational resources, you need to implement parallelism using scikit-learn and joblib. Task: 1. Load the provided dataset and split it into training and testing sets. 2. Train a `RandomForestClassifier` model using scikit-learn. Use the `n_jobs` parameter to control the parallelism. 3. Experiment with different values of `n_jobs` (e.g., 1, 2, 4, -1) and measure the training time and model performance (accuracy). 4. Use the `joblib` library to switch the backend to threading and repeat the experiments. Measure the training time and performance. 5. Document your results, analyze the impact of different values of `n_jobs` and backend on training time and performance. Input Format: - A dataset file (e.g., CSV format) will be provided. - A Python environment with necessary packages (scikit-learn, joblib, numpy, pandas) installed. Output Format: - A summary of results showing training time and model performance for each combination of `n_jobs` and backend. Constraints: - The dataset may be large, so consider the memory limitations and ensure your solution efficiently handles large data. Performance Requirements: - The implemented solution should efficiently utilize the available CPU cores without causing oversubscription. - The code should be well-documented, and the experiments should be reproducible. Example: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from joblib import parallel_backend import time # Load the dataset data = pd.read_csv(\'path_to_dataset.csv\') X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Experiment with different values of n_jobs n_jobs_values = [1, 2, 4, -1] results = [] for n_jobs in n_jobs_values: start_time = time.time() model = RandomForestClassifier(n_jobs=n_jobs, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) end_time = time.time() accuracy = accuracy_score(y_test, y_pred) training_time = end_time - start_time results.append((n_jobs, \'multi-processing\', training_time, accuracy)) # Experiment with threading backend with parallel_backend(\'threading\', n_jobs=2): start_time = time.time() model = RandomForestClassifier(n_jobs=2, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) end_time = time.time() accuracy = accuracy_score(y_test, y_pred) training_time = end_time - start_time results.append((2, \'threading\', training_time, accuracy)) # Print the results for result in results: print(f\'n_jobs: {result[0]}, backend: {result[1]}, training time: {result[2]}, accuracy: {result[3]}\') ``` The output should include a summary table with the following columns: - n_jobs - backend (multi-processing or threading) - training time - accuracy Notes: - Ensure to handle any data preprocessing steps as needed. - Be aware of the oversubscription issue and manage the number of threads properly. - Comment your code and document the findings clearly.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from joblib import parallel_backend import time def load_data(file_path): Loads the dataset from a CSV file. Parameters: file_path (str): Path to the CSV file. Returns: X (pd.DataFrame): Features dataframe. y (pd.Series): Target series. data = pd.read_csv(file_path) X = data.drop(\'target\', axis=1) y = data[\'target\'] return X, y def train_and_evaluate(X_train, X_test, y_train, y_test, n_jobs, backend=None): Trains and evaluates the RandomForestClassifier with specified n_jobs and backend. Parameters: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Testing features. y_train (pd.Series): Training target. y_test (pd.Series): Testing target. n_jobs (int): The number of CPU cores to use for training. backend (str or None): Parallel backend to use (None for default, \'threading\' for thread-based parallelism). Returns: dict: A dictionary containing the training time and accuracy. if backend: with parallel_backend(backend, n_jobs=n_jobs): start_time = time.time() model = RandomForestClassifier(n_jobs=n_jobs, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) end_time = time.time() else: start_time = time.time() model = RandomForestClassifier(n_jobs=n_jobs, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) end_time = time.time() accuracy = accuracy_score(y_test, y_pred) training_time = end_time - start_time return {\'n_jobs\': n_jobs, \'backend\': backend if backend else \'multi-processing\', \'training_time\': training_time, \'accuracy\': accuracy}"},{"question":"# XML Parsing with `xml.sax.xmlreader` Objective In this assessment, you will write an XML parser using the `xml.sax.xmlreader` module. Your parser will read an XML document incrementally, handle SAX events, and store specific elements and attributes in a structured format. Problem Statement Create a class `IncrementalXMLParser` that uses the `xml.sax.xmlreader.IncrementalParser` interface to parse an XML document incrementally. The parser should extract and store the data of all elements with a specified tag name and their attributes into a dictionary. Requirements 1. **Initialization**: The `IncrementalXMLParser` class should take a list of tag names to extract during initialization. 2. **Methods to Implement**: - `feed(data)`: Feed chunks of the XML data to the parser. - `close()`: Finalize the parsing process and return the extracted data. - `reset()`: Reset the parser to make it ready for parsing new data streams. 3. **Data Extraction**: - Store the extracted data in the dictionary format: `{tag_name: [list_of_attributes]}`. - Each attribute entry should be a dictionary containing the attribute names and their values. 4. **Error Handling**: - Handle possible parsing errors gracefully and raise an appropriate custom exception with a meaningful message. Input and Output Formats - **Input**: Chunks of XML data fed to the `feed(data)` method. - **Output**: Dictionary of extracted tag names and their attributes returned by the `close()` method. Example ```python xml_data_1 = \\"<root><item name=\'Item1\' type=\'TypeA\'></item>\\" xml_data_2 = \\"<item name=\'Item2\' type=\'TypeB\'></item></root>\\" parser = IncrementalXMLParser([\\"item\\"]) parser.feed(xml_data_1) parser.feed(xml_data_2) result = parser.close() # Expected output: # { # \\"item\\": [ # {\\"name\\": \\"Item1\\", \\"type\\": \\"TypeA\\"}, # {\\"name\\": \\"Item2\\", \\"type\\": \\"TypeB\\"} # ] # } ``` Implement the class `IncrementalXMLParser` based on the provided requirements. Constraints - Use the `xml.sax.xmlreader.IncrementalParser` interface for incremental parsing. - Do not use any external libraries for XML parsing. - Ensure the parser can handle large XML documents by processing them in chunks. Performance Requirements The parser should efficiently handle large XML files by processing them incrementally, ensuring minimal memory usage.","solution":"import xml.sax import xml.sax.handler class IncrementalXMLParser(xml.sax.handler.ContentHandler): def __init__(self, tags_to_extract): self.tags_to_extract = tags_to_extract self.extracted_data = {tag: [] for tag in tags_to_extract} self.current_tag = None def startElement(self, name, attrs): if name in self.tags_to_extract: attr_dict = {k: v for k, v in attrs.items()} self.extracted_data[name].append(attr_dict) def endElement(self, name): if name == self.current_tag: self.current_tag = None class XMLIncrementalParser: def __init__(self, tags): self.tags = tags self.parser = xml.sax.make_parser() self.handler = IncrementalXMLParser(tags) self.parser.setContentHandler(self.handler) def feed(self, data): import io try: self.parser.feed(io.StringIO(data).read()) except xml.sax.SAXException as e: raise Exception(f\\"Error while parsing the XML: {e}\\") def close(self): return self.handler.extracted_data def reset(self): self.__init__(self.tags) self.parser = xml.sax.make_parser() self.parser.setContentHandler(self.handler)"},{"question":"**Objective:** Use the Seaborn\'s new `objects` interface to create bar plots and analyze the dataset. This exercise will test your ability to effectively use Seaborn for data visualization. **Dataset:** We will use the \\"tips\\" dataset, which contains information about tips received based on various factors. **Requirements:** 1. Write a function `plot_tips_data` that generates several bar plots using the `seaborn.objects` module. 2. Your function should accept no arguments and return no values as all visualizations will be generated and displayed directly. **Function: `plot_tips_data`** - The function should load the \\"tips\\" dataset using `seaborn.load_dataset`. - Create the following plots: 1. A basic bar plot showing the count of tips received each day. 2. A grouped bar plot showing the count of tips received each day, grouped by gender. 3. A bar plot showing the count of tips by table size. 4. Choose one additional comparison of your choice using any of the available variables in the dataset (e.g., total_bill, smoker status, etc.). **Constraints:** - The function should handle potential errors gracefully (e.g., data loading issues) and print appropriate error messages. **Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_tips_data(): try: # Load the dataset tips = load_dataset(\\"tips\\") # Plot 1: Count of tips received each day plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() # Plot 2: Count of tips received each day, grouped by gender plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() # Plot 3: Count of tips by table size plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # Plot 4: Custom plot of your choice # Replace variables to create your own unique plot plot4 = so.Plot(tips, y=\\"total_bill\\").add(so.Bar(), so.Count()) plot4.show() except Exception as e: print(f\\"An error occurred: {e}\\") # Call the function plot_tips_data() ``` **Performance Considerations:** - The function should be able to handle the size of the \\"tips\\" dataset efficiently. - The rendering of plots should be as quick as possible while maintaining clarity and accuracy. Ensure your solution is well-documented with comments explaining each step.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_tips_data(): try: # Load the dataset tips = load_dataset(\\"tips\\") # Plot 1: Count of tips received each day plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() plt.title(\'Count of Tips Received Each Day\') plt.show() # Plot 2: Count of tips received each day, grouped by gender plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() plt.title(\'Count of Tips Received Each Day by Gender\') plt.show() # Plot 3: Count of tips by table size plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot3.show() plt.title(\'Count of Tips by Table Size\') plt.show() # Plot 4: Custom plot of your choice - Showing count by smoker status plot4 = so.Plot(tips, x=\\"day\\", color=\\"smoker\\").add(so.Bar(), so.Count(), so.Dodge()) plot4.show() plt.title(\'Count of Tips by Day and Smoker Status\') plt.show() except Exception as e: print(f\\"An error occurred: {e}\\") # Call the function plot_tips_data()"},{"question":"You are given a dataset containing samples drawn from an unknown number of Gaussian distributions. Your task is to implement a function that fits both a **Gaussian Mixture Model (GMM)** using the Expectation-Maximization (EM) algorithm and a **Bayesian Gaussian Mixture Model (BGMM)** using variational inference to the dataset. Additionally, you will use the Bayesian Information Criterion (BIC) to select the optimal number of components for the GMM. # Function Signature ```python import numpy as np from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def fit_gaussian_models(data: np.ndarray, max_components: int = 10): Fits a Gaussian Mixture Model (GMM) and a Bayesian Gaussian Mixture Model (BGMM) to the data. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. max_components (int): The maximum number of mixture components to consider for the GMM. Returns: dict: A dictionary containing: - \'gmm\': The fitted GaussianMixture object with the optimal number of components. - \'bgmm\': The fitted BayesianGaussianMixture object using variational inference. - \'bic_scores\': A list of BIC scores for each number of components evaluated. # Your solution here ``` # Input 1. `data` (numpy.ndarray): A 2D numpy array of shape `(n_samples, n_features)` representing the dataset. 2. `max_components` (int): The maximum number of mixture components to consider for the GMM. Default is 10. # Output 1. A dictionary containing: - `gmm` (sklearn.mixture.GaussianMixture): The fitted `GaussianMixture` object with the optimal number of components. - `bgmm` (sklearn.mixture.BayesianGaussianMixture): The fitted `BayesianGaussianMixture` object using variational inference. - `bic_scores` (list): A list of BIC scores for each number of components evaluated. # Example ```python import numpy as np # Generate sample data with 300 points and 2 features np.random.seed(42) data = np.random.randn(300, 2) # Fit the Gaussian models results = fit_gaussian_models(data, max_components=5) gmm_model = results[\'gmm\'] bgmm_model = results[\'bgmm\'] bic_scores = results[\'bic_scores\'] print(\\"Optimal number of components (GMM):\\", gmm_model.n_components) print(\\"Number of components (BGMM):\\", bgmm_model.n_components) print(\\"BIC Scores:\\", bic_scores) ``` # Requirements 1. **GaussianMixture Model**: - Use the BIC criterion to select the optimal number of components (up to `max_components`). - Fit the GMM with the optimal number of components and store the result. 2. **BayesianGaussianMixture Model**: - Fit the BGMM using variational inference with the `max_components` provided. - Store the fitted model. 3. **BIC Scores**: - Compute and return the BIC scores for each number of components evaluated (from 1 up to `max_components`). 4. **Constraints**: - Assume `data` is a valid 2D numpy array with enough samples to perform mixture modeling. - Use built-in methods from scikit-learn for fitting models and calculating the BIC. Implementing this function will require knowledge of scikit-learn’s mixture models, BIC for model selection, and basic numpy operations.","solution":"import numpy as np from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def fit_gaussian_models(data: np.ndarray, max_components: int = 10): Fits a Gaussian Mixture Model (GMM) and a Bayesian Gaussian Mixture Model (BGMM) to the data. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. max_components (int): The maximum number of mixture components to consider for the GMM. Returns: dict: A dictionary containing: - \'gmm\': The fitted GaussianMixture object with the optimal number of components. - \'bgmm\': The fitted BayesianGaussianMixture object using variational inference. - \'bic_scores\': A list of BIC scores for each number of components evaluated. # Compute BIC scores for different numbers of components bic_scores = [] best_gmm = None lowest_bic = np.inf for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(data) bic = gmm.bic(data) bic_scores.append(bic) if bic < lowest_bic: lowest_bic = bic best_gmm = gmm # Fit the Bayesian Gaussian Mixture Model bgmm = BayesianGaussianMixture(n_components=max_components, random_state=42) bgmm.fit(data) return { \'gmm\': best_gmm, \'bgmm\': bgmm, \'bic_scores\': bic_scores }"},{"question":"# Advanced Decision Tree Coding Assessment Problem Statement You are given a dataset containing information about various features of houses and their corresponding prices. Your task is to build a `DecisionTreeRegressor` model using scikit-learn to predict the house prices based on the provided features. Additionally, you need to handle missing values, tune the model to prevent overfitting, and evaluate its performance. Dataset The dataset `housing_data.csv` contains the following columns/features: - `lotsize`: Size of the lot in square feet (some values might be missing). - `bedrooms`: Number of bedrooms. - `bathrms`: Number of bathrooms. - `stories`: Number of stories. - `garagepl`: Number of garage places. - `price`: The price of the house (target variable). Implementation Details 1. **Load the Data:** - Load the data from `housing_data.csv`. 2. **Handle Missing Values:** - For simplicity, fill missing values in the `lotsize` column with the median value of the column. 3. **Model Building:** - Split the data into training and testing sets using a 80-20 split. - Initialize a `DecisionTreeRegressor` model. - Train the model on the training set. 4. **Model Tuning and Pruning:** - Use `grid search` with cross-validation to find the best hyperparameters for: * `max_depth`: Limits the maximum depth of the tree. * `min_samples_split`: The minimum number of samples required to split an internal node. - Use the obtained best hyperparameters to refit the model. 5. **Evaluation:** - Evaluate the model on the test set using Mean Squared Error (MSE). - Export the final tree using the `export_text` function and print the tree structure. Requirements - You must use `scikit-learn` for building the model. - Implement a function `train_decision_tree_regressor` with the following signature: ```python def train_decision_tree_regressor(filepath: str) -> None: # Your code here ``` The function should: 1. Load the dataset from the given `filepath`. 2. Handle any missing values as specified. 3. Split the data into training and test sets. 4. Use grid search with cross-validation to find the best hyperparameters. 5. Train the final decision tree regressor using the best hyperparameters. 6. Evaluate the model performance. 7. Export and print the tree structure. Example Given the dataset `housing_data.csv`, calling `train_decision_tree_regressor(\'housing_data.csv\')` should print the evaluation result and the tree structure. Expected Output - Mean Squared Error on the test set. - Textual representation of the fitted decision tree.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.tree import DecisionTreeRegressor, export_text from sklearn.metrics import mean_squared_error def train_decision_tree_regressor(filepath: str) -> None: # Load the dataset data = pd.read_csv(filepath) # Handle missing values by filling with median data[\'lotsize\'].fillna(data[\'lotsize\'].median(), inplace=True) # Separate features and target variable X = data.drop(columns=\'price\') y = data[\'price\'] # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the DecisionTreeRegressor decision_tree = DecisionTreeRegressor(random_state=42) # Define the hyperparameters grid to search param_grid = { \'max_depth\': [None, 10, 20, 30, 40], \'min_samples_split\': [2, 5, 10] } # Use GridSearchCV to find the best parameters grid_search = GridSearchCV(decision_tree, param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Get the best estimator best_decision_tree = grid_search.best_estimator_ # Train the decision tree with the best parameters on the whole training set best_decision_tree.fit(X_train, y_train) # Predict on the test set y_pred = best_decision_tree.predict(X_test) # Evaluate the performance using Mean Squared Error mse = mean_squared_error(y_test, y_pred) # Print the Mean Squared Error print(f\\"Mean Squared Error on the test set: {mse}\\") # Export and print the tree structure tree_rules = export_text(best_decision_tree, feature_names=list(X.columns)) print(tree_rules)"},{"question":"**Objective:** Implement a function that calculates various numeric results based on two input numeric objects using the `python310` library functions. **Problem Statement:** You are required to write a Python function `calculate_numeric_operations` that takes two numeric input objects. The function should use the following operations from the `python310` library: 1. Addition 2. Subtraction 3. Multiplication 4. Division (True Division) 5. Floor Division 6. Remainder 7. Power 8. Bitwise AND 9. Bitwise OR The function should take the inputs and produce a dictionary containing the results of these operations, with specific keys for each operation result. **Function Definition:** ```python def calculate_numeric_operations(o1, o2): Calculate various numeric results based on two input numeric objects using python310 library functions. Parameters: o1 (int or float): The first numeric input object. o2 (int or float): The second numeric input object. Returns: dict: A dictionary containing the results of various numeric operations. ``` **Expected Dictionary Output:** ```python { \\"add\\": result_of_addition, \\"subtract\\": result_of_subtraction, \\"multiply\\": result_of_multiplication, \\"divide\\": result_of_true_division, \\"floor_divide\\": result_of_floor_division, \\"remainder\\": result_of_remainder, \\"power\\": result_of_power, \\"bitwise_and\\": result_of_bitwise_and, # Only if inputs are integers \\"bitwise_or\\": result_of_bitwise_or # Only if inputs are integers } ``` **Constraints:** - The inputs must be either integers or floating-point numbers. If the inputs are not valid numbers, the function should raise a `TypeError`. - For bitwise operations (`bitwise_and` and `bitwise_or`), both inputs must be integers. If either input is a floating-point number, the result should be `None` for these keys. - The function should handle mathematical errors (such as division by zero) gracefully. If an error occurs during any operation, the corresponding result should be `None`. **Performance Considerations:** - The functions provided by `python310` are likely to execute in constant time. Therefore, the performance should be efficient for typical numeric values. - Ensure proper memory management by handling new references as per `python310`\'s requirements. **Example Usage:** ```python result = calculate_numeric_operations(10, 2) print(result) # Output: # { # \\"add\\": 12, # \\"subtract\\": 8, # \\"multiply\\": 20, # \\"divide\\": 5.0, # \\"floor_divide\\": 5, # \\"remainder\\": 0, # \\"power\\": 100, # \\"bitwise_and\\": 2, # \\"bitwise_or\\": 10 # } ``` Note: You can assume the availability of a hypothetical `python310` library that accurately represents the Python C API\'s numeric operations as functions.","solution":"def calculate_numeric_operations(o1, o2): Calculate various numeric results based on two input numeric objects using python310 library functions. Parameters: o1 (int or float): The first numeric input object. o2 (int or float): The second numeric input object. Returns: dict: A dictionary containing the results of various numeric operations. # Ensure inputs are numeric if not isinstance(o1, (int, float)) or not isinstance(o2, (int, float)): raise TypeError(\\"Both inputs must be either integers or floating-point numbers.\\") results = {} # Perform the operations try: results[\\"add\\"] = o1 + o2 except Exception: results[\\"add\\"] = None try: results[\\"subtract\\"] = o1 - o2 except Exception: results[\\"subtract\\"] = None try: results[\\"multiply\\"] = o1 * o2 except Exception: results[\\"multiply\\"] = None try: results[\\"divide\\"] = o1 / o2 except Exception: results[\\"divide\\"] = None try: results[\\"floor_divide\\"] = o1 // o2 except Exception: results[\\"floor_divide\\"] = None try: results[\\"remainder\\"] = o1 % o2 except Exception: results[\\"remainder\\"] = None try: results[\\"power\\"] = o1 ** o2 except Exception: results[\\"power\\"] = None # Bitwise operations only applicable if both are integers if isinstance(o1, int) and isinstance(o2, int): try: results[\\"bitwise_and\\"] = o1 & o2 except Exception: results[\\"bitwise_and\\"] = None try: results[\\"bitwise_or\\"] = o1 | o2 except Exception: results[\\"bitwise_or\\"] = None else: results[\\"bitwise_and\\"] = None results[\\"bitwise_or\\"] = None return results"},{"question":"# Gzip Compressed Directory Archiver You are tasked with creating a utility that will compress a given directory into a `.gzip` archive and also be able to decompress it back to its original structure. Task: Implement two functions: 1. **compress_directory()** 2. **decompress_directory()** # Function Details: compress_directory(): - **Input**: - `dir_path` (str): The path to the directory that needs to be compressed. - `output_path` (str): The path where the compressed `.gzip` file should be saved. - `compresslevel` (int, optional): The compression level, an integer from 0 (no compression) to 9 (maximum compression). Default is 9. - **Output**: - None: The function should compress the directory and all its contents recursively into a single `.gzip` file. - **Constraints**: - The `dir_path` must be a valid directory that exists. - Any existing file at the `output_path` should be overwritten. decompress_directory(): - **Input**: - `gzip_path` (str): The path to the compressed `.gzip` file. - `output_dir` (str): The path where the decompressed directory contents should be saved. - **Output**: - None: The function should decompress the `.gzip` file back into its original directory structure. - **Constraints**: - The `gzip_path` must be a valid `.gzip` file. - The `output_dir` must be an existing directory or be created by the function. # Example Usage: ```python # Compress the directory compress_directory(\'/path/to/dir\', \'/path/to/output/compressed_archive.gz\', compresslevel=5) # Decompress the directory decompress_directory(\'/path/to/output/compressed_archive.gz\', \'/path/to/output/decompressed\') ``` # Notes: - Use the `gzip` module along with other modules such as `os`, `shutil`, or `io` as needed. - Ensure that compressed files/store the original directory structure, including subdirectories and files. - Handle any potential exceptions that might occur during file operations.","solution":"import gzip import os import shutil import tarfile def compress_directory(dir_path, output_path, compresslevel=9): Compresses the given directory into a .gzip file. :param dir_path: str, path to the directory to be compressed :param output_path: str, path where the .gzip file will be saved :param compresslevel: int, compression level (0 to 9). Default is 9. :return: None with tarfile.open(output_path, \\"w:gz\\", compresslevel=compresslevel) as tar: tar.add(dir_path, arcname=os.path.basename(dir_path)) def decompress_directory(gzip_path, output_dir): Decompresses the given .gzip file into the specified directory. :param gzip_path: str, path to the .gzip file :param output_dir: str, path where the decompressed contents will be saved :return: None with tarfile.open(gzip_path, \\"r:gz\\") as tar: tar.extractall(path=output_dir)"},{"question":"You are tasked with creating an `asyncio`-based server that handles client connections and processes their requests asynchronously. The server should handle the following scenarios: 1. Accept multiple connections from different clients. 2. For each client, echo back the received messages. 3. Log the client\'s connection time, disconnection time, and the duration the connection was active. 4. Schedule a repeated callback that logs the server\'s uptime every 10 seconds. 5. Ensure the server can be gracefully stopped, logging the event loop\'s shutdown process. # Requirements: 1. Create a server class `AsyncServer` that: - Starts a TCP server on a specified host and port. - Manages client connections and echoes their messages. - Logs connection and disconnection times, as well as connection duration. - Uses `asyncio.run()` for running the event loop. 2. Define a repeated callback function that logs the server uptime every 10 seconds. 3. Handle socket disconnections gracefully and ensure proper cleanup. 4. Ensure that the server shuts down gracefully when a stop signal is received (like `SIGINT`). # Input: - Host and port to bind the server to (as strings). - Client messages (strings provided by connected clients). # Output: - Logs of connection times, disconnection times, and connection durations. - Log message indicating server\'s uptime every 10 seconds. - Log indicating the server was stopped gracefully. # Constraints: - Use the `asyncio` package for managing the event loop and client connections. - Your solution should handle multiple concurrent connections efficiently. - Ensure proper exception handling in all coroutine functions. # Example Usage: ```python import asyncio import logging class AsyncServer: def __init__(self, host, port): self.host = host self.port = port self.server = None self.start_time = asyncio.get_event_loop().time() logging.basicConfig(level=logging.INFO) async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Connection from {addr}\\") client_connected_time = asyncio.get_event_loop().time() try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except Exception as e: logging.error(f\\"Error with client {addr}: {e}\\") finally: writer.close() await writer.wait_closed() client_disconnected_time = asyncio.get_event_loop().time() duration = client_disconnected_time - client_connected_time logging.info(f\\"Disconnected from {addr} after {duration:.2f} seconds\\") async def log_uptime(self): while True: await asyncio.sleep(10) uptime = asyncio.get_event_loop().time() - self.start_time logging.info(f\\"Server uptime: {uptime:.2f} seconds\\") async def start(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) logging.info(f\\"Started server on {self.host}:{self.port}\\") # Schedule the uptime logger asyncio.create_task(self.log_uptime()) async with self.server: await self.server.serve_forever() def run(self): try: asyncio.run(self.start()) except KeyboardInterrupt: logging.info(\\"Server is shutting down\\") if __name__ == \\"__main__\\": server = AsyncServer(\'127.0.0.1\', 8888) server.run() ``` Within this example, ensure you have proper logging and exception handling to cover various scenarios. The key objective is to demonstrate a robust understanding of `asyncio` and its event loop mechanics. # Additional Notes: - Make sure to provide sufficient comments in your code to explain the logic used in each function. - Ensure that your code is formatted and readable, adhering to Python\'s PEP 8 standards.","solution":"import asyncio import logging import signal class AsyncServer: def __init__(self, host, port): self.host = host self.port = port self.server = None self.start_time = None logging.basicConfig(level=logging.INFO) async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Connection from {addr}\\") client_connected_time = asyncio.get_event_loop().time() try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except Exception as e: logging.error(f\\"Error with client {addr}: {e}\\") finally: writer.close() await writer.wait_closed() client_disconnected_time = asyncio.get_event_loop().time() duration = client_disconnected_time - client_connected_time logging.info(f\\"Disconnected from {addr} after {duration:.2f} seconds\\") async def log_uptime(self): while True: await asyncio.sleep(10) uptime = asyncio.get_event_loop().time() - self.start_time logging.info(f\\"Server uptime: {uptime:.2f} seconds\\") async def start(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) self.start_time = asyncio.get_event_loop().time() logging.info(f\\"Started server on {self.host}:{self.port}\\") # Schedule the uptime logger asyncio.create_task(self.log_uptime()) async with self.server: await self.server.serve_forever() def run(self): loop = asyncio.get_event_loop() try: for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), loop.stop) loop.run_until_complete(self.start()) except KeyboardInterrupt: pass finally: logging.info(\\"Server is shutting down\\") loop.close() if __name__ == \\"__main__\\": server = AsyncServer(\'127.0.0.1\', 8888) server.run()"},{"question":"# Coding Assessment You are required to create a custom Python utility function that uses the `compileall` module to compile Python source files with additional control over which files are included or excluded based on their names and specific directory nesting levels. --- Problem Statement: Implement a function `custom_compile(base_dir, include_names, exclude_dirs, max_depth, optimization_levels)` that performs the following actions: 1. Recursively compiles `.py` files in the specified `base_dir`. 2. Includes only the files whose names match any pattern in the `include_names` list. 3. Excludes any subdirectories listed in `exclude_dirs` (directly located in `base_dir`). 4. Limits the depth of the recursion to `max_depth`. 5. Compiles files with multiple optimization levels specified in `optimization_levels`. Function Signature: ```python def custom_compile( base_dir: str, include_names: list, exclude_dirs: list, max_depth: int, optimization_levels: list ) -> bool: ``` Parameters: - `base_dir` (str): The base directory containing the source files to be compiled. - `include_names` (list): List of name patterns (regex) to include for compilation. The patterns should be matched against the file names. - `exclude_dirs` (list): List of directory names (not paths) to exclude from compilation within the `base_dir`. - `max_depth` (int): The maximum depth of recursion for directory traversal. - `optimization_levels` (list): A list of integer values representing optimization levels (-1, 0, 1, 2) for the compilation process. Constraints: - The function should be robust and handle possible errors gracefully, returning `False` if any compilation fails or if invalid inputs are provided. - Ensure the function is efficient given the constraints. Returns: - `bool`: `True` if all files compile successfully, otherwise `False`. Example Usage: ```python base_dir = \\"/path/to/source\\" include_names = [r\'.*_test.py\', r\'^main.py\'] exclude_dirs = [\\"exclude_folder_1\\", \\"exclude_folder_2\\"] max_depth = 3 optimization_levels = [0, 1] result = custom_compile(base_dir, include_names, exclude_dirs, max_depth, optimization_levels) print(result) # True if compilation was successful, False otherwise. ``` Notes: - Use `compileall.compile_dir()` internally to perform the compilation and leverage the various parameters available to meet the requirements stated. - Regular expressions (regex) should be used to filter the names of the files to be included. - The function should be callable from the command line with appropriate arguments if executed as a script.","solution":"import os import re import compileall def custom_compile(base_dir, include_names, exclude_dirs, max_depth, optimization_levels): Compile Python source files in the specified base directory with additional controls over included and excluded files and directory nesting levels. Parameters: - base_dir (str): The base directory containing the source files to be compiled. - include_names (list): List of name patterns (regex) to include for compilation. - exclude_dirs (list): List of directory names to exclude from compilation within the base_dir. - max_depth (int): The maximum depth of recursion for directory traversal. - optimization_levels (list): A list of integer values representing optimization levels (from -1 to 2). Returns: - bool: True if all files compile successfully, otherwise False. def included(name): return any(re.match(pattern, name) for pattern in include_names) def should_exclude_dir(path): return any(exclude_dir in path.split(os.sep)[:max_depth] for exclude_dir in exclude_dirs) def compile_dir_with_depth(path, current_depth): if current_depth > max_depth: return True if should_exclude_dir(path): return True success = True for entry in os.scandir(path): if entry.is_file() and entry.name.endswith(\'.py\') and included(entry.name): for level in optimization_levels: if not compileall.compile_file(entry.path, optimize=level): success = False elif entry.is_dir(): if not compile_dir_with_depth(entry.path, current_depth + 1): success = False return success return compile_dir_with_depth(base_dir, 0)"},{"question":"You are tasked with loading and processing a large dataset for training a machine learning model using PyTorch. Implement a custom dataset class and use PyTorch\'s `DataLoader` with specific configurations to improve data loading performance. # Requirements 1. **Dataset**: Implement a custom `IterableDataset` (simulating the process of reading data from a remote server). - Each data sample should be a tuple of a 1D tensor (features) and an integer (label). - The length of the dataset should be configurable. 2. **Data Loader**: - Use the `DataLoader` to load data in batches. - Enable multi-process data loading with 4 workers. - Implement a custom `collate_fn` that pads each 1D tensor to the maximum length in the batch. 3. **Sampler**: Implement a custom sequential sampler that generates sample indices in sequential order. 4. **Memory Pinning**: Enable memory pinning for faster data transfer to CUDA-enabled GPUs. Input - The batch size (e.g., 32). - The dataset size (e.g., 1000 samples). - The length/range of the 1D tensor (features), you can randomly choose lengths between 5 and 15. - A flag indicating if the data should be loaded in pin memory (True/False). Output - Print the padded batches of data (features and labels) to the console. # Constraints - The 1D tensor representing features should have values in the range [0, 1]. Example ```python batch_size = 4 dataset_size = 10 pin_memory = True # Expected output format (values may vary due to random generation): # Batch 1: # Features: tensor([[0.1234, 0.5678, 0.1234, 0.5678, 0.1234, 0.0000, 0.0000, ...], # [0.8765, 0.4321, 0.8765, 0.4321, 0.0000, 0.0000, 0.0000, ...], # ... # ]) # Labels: tensor([0, 1, 2, 3]) # . # . # . ``` # Implementation ```python import torch from torch.utils.data import DataLoader, IterableDataset, Sampler import random class CustomIterableDataset(IterableDataset): def __init__(self, size, min_length=5, max_length=15): self.size = size self.min_length = min_length self.max_length = max_length def __iter__(self): for _ in range(self.size): length = random.randint(self.min_length, self.max_length) features = torch.rand(length) label = random.randint(0, 9) # Random label for demonstration yield (features, label) def __len__(self): return self.size class SequentialSampler(Sampler): def __init__(self, size): self.size = size def __iter__(self): return iter(range(self.size)) def __len__(self): return self.size def custom_collate_fn(batch): features, labels = zip(*batch) max_length = max([len(f) for f in features]) padded_features = [torch.cat([f, torch.zeros(max_length - len(f))]) for f in features] return torch.stack(padded_features), torch.tensor(labels) def main(batch_size, dataset_size, pin_memory): dataset = CustomIterableDataset(size=dataset_size) sampler = SequentialSampler(size=dataset_size) dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate_fn, sampler=sampler, num_workers=4, pin_memory=pin_memory) for i, (features, labels) in enumerate(dataloader): print(f\\"Batch {i + 1}:\\") print(\\"Features:\\", features) print(\\"Labels:\\", labels) print() if __name__ == \\"__main__\\": main(batch_size=4, dataset_size=10, pin_memory=True) ```","solution":"import torch from torch.utils.data import DataLoader, IterableDataset, Sampler import random class CustomIterableDataset(IterableDataset): Custom dataset simulating reading data from a remote server. Each data sample is a tuple of a 1D tensor (features) and an integer (label). def __init__(self, size, min_length=5, max_length=15): self.size = size self.min_length = min_length self.max_length = max_length def __iter__(self): for _ in range(self.size): length = random.randint(self.min_length, self.max_length) features = torch.rand(length) label = random.randint(0, 9) # Random label for demonstration yield (features, label) def __len__(self): return self.size class SequentialSampler(Sampler): Custom sequential sampler generating sample indices in sequential order. def __init__(self, size): self.size = size def __iter__(self): return iter(range(self.size)) def __len__(self): return self.size def custom_collate_fn(batch): Custom collate function that pads each 1D tensor to the maximum length in the batch. features, labels = zip(*batch) max_length = max([len(f) for f in features]) padded_features = [torch.cat([f, torch.zeros(max_length - len(f))]) for f in features] return torch.stack(padded_features), torch.tensor(labels) def main(batch_size, dataset_size, pin_memory): dataset = CustomIterableDataset(size=dataset_size) sampler = SequentialSampler(size=dataset_size) dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate_fn, sampler=sampler, num_workers=4, pin_memory=pin_memory) for i, (features, labels) in enumerate(dataloader): print(f\\"Batch {i + 1}:\\") print(\\"Features:\\", features) print(\\"Labels:\\", labels) print() if __name__ == \\"__main__\\": main(batch_size=4, dataset_size=10, pin_memory=True)"},{"question":"Shadow Password Database Analysis **Objective**: Demonstrate your comprehension of the `spwd` module and its functionalities by writing a Python function that extracts and analyzes data from the Unix shadow password database. # Task Write a function called `password_security_analysis` that: 1. Retrieves all entries in the shadow password database. 2. Analyzes the entries to find: - The total number of user accounts. - The average number of days between password changes (`sp_max` attribute). - The list of usernames (`sp_namp` attribute) for accounts that either have an expired password (`sp_expire` attribute is non-zero and less than the current date) or accounts that are inactive (`sp_inact` attribute is non-zero and the difference between `sp_lstchg` and `sp_inact` is greater than the current date). # Function Signature ```python def password_security_analysis() -> dict: Analyzes the shadow password database and returns a dictionary with: - \'total_users\': Total number of user accounts. - \'avg_max_days_between_changes\': Average number of days between password changes. - \'expired_or_inactive_users\': List of usernames with expired passwords or inactive accounts. Returns: dict: Dictionary with the analysis results. ``` # Constraints and Considerations - Ensure the function gracefully handles scenarios where there might be permission issues by catching exceptions and returning an appropriate message in the result dictionary. - Utilize the `spwd` module to perform the required operations. - For average calculations, if no valid entries are found, the average should be zero. - Assume the current date can be obtained using the `time.time()` function from the `time` module, and it returns the current time in seconds since the epoch. # Example Output ```python { \'total_users\': 10, \'avg_max_days_between_changes\': 90.5, \'expired_or_inactive_users\': [\'user1\', \'user3\', \'user5\'] } ``` # Additional Comments The function should be self-contained and should not require any input parameters. Ensure to handle permissions carefully to avoid unhandled exceptions.","solution":"import spwd import time def password_security_analysis() -> dict: Analyzes the shadow password database and returns a dictionary with: - \'total_users\': Total number of user accounts. - \'avg_max_days_between_changes\': Average number of days between password changes. - \'expired_or_inactive_users\': List of usernames with expired passwords or inactive accounts. Returns: dict: Dictionary with the analysis results. try: entries = spwd.getspall() except PermissionError: return { \'total_users\': 0, \'avg_max_days_between_changes\': 0, \'expired_or_inactive_users\': [], \'error\': \\"Permission denied: Unable to access shadow password database.\\" } total_users = len(entries) total_days_between_changes = 0 count_valid_entries = 0 expired_or_inactive_users = [] current_time_days = int(time.time() // (24 * 3600)) for entry in entries: if entry.sp_max > 0: total_days_between_changes += entry.sp_max count_valid_entries += 1 # Check for expired passwords if entry.sp_expire > 0 and entry.sp_expire < current_time_days: expired_or_inactive_users.append(entry.sp_namp) continue # Check for inactive accounts if entry.sp_inact > 0 and (entry.sp_lstchg + entry.sp_inact) < current_time_days: expired_or_inactive_users.append(entry.sp_namp) avg_max_days_between_changes = (total_days_between_changes / count_valid_entries) if count_valid_entries > 0 else 0 return { \'total_users\': total_users, \'avg_max_days_between_changes\': avg_max_days_between_changes, \'expired_or_inactive_users\': expired_or_inactive_users }"},{"question":"# Python Coding Assessment **Objective:** Demonstrate understanding of the `glob` module for pattern-based filename searching. # Problem Statement You are provided with a directory structure containing various files and subdirectories. Your task is to implement a function `find_files_with_patterns` that searches for files matching specific patterns in a given directory. The function should also handle recursive searches and return files starting with a dot if specified. Function Signature ```python def find_files_with_patterns(base_dir: str, patterns: list, recursive=False, include_hidden=False) -> dict: \'\'\' Search for files in the base directory matching the given patterns. Parameters: base_dir (str): The base directory path where the search will start. patterns (list): A list of patterns to match against. recursive (bool): If True, perform a recursive search using the ** pattern. include_hidden (bool): If True, include files starting with a dot. Returns: dict: A dictionary with patterns as keys and lists of matched files as values. \'\'\' pass ``` Input - `base_dir`: A string representing the base directory path. - `patterns`: A list of pattern strings to match filenames. - `recursive`: A boolean indicating whether to search recursively. - `include_hidden`: A boolean indicating whether to include files starting with a dot. Output - A dictionary where keys are patterns and values are lists of matched files. Constraints - The patterns should correctly match Unix shell-style wildcards. - Hidden files (starting with a dot) should only be included if `include_hidden` is True. - The function should handle cases where no files match the given patterns. Example Consider the directory structure: ``` ./base_dir/ file1.txt file2.gif .hiddenfile subdir/ subfile1.txt subfile2.jpg .subhiddenfile ``` Example function call: ```python find_files_with_patterns(\'./base_dir\', [\'*.txt\', \'*.gif\'], recursive=True, include_hidden=True) ``` Expected Output: ```python { \'*.txt\': [\'file1.txt\', \'subdir/subfile1.txt\'], \'*.gif\': [\'file2.gif\'] } ``` Example function call: ```python find_files_with_patterns(\'./base_dir\', [\'*.txt\', \'*.gif\'], recursive=False, include_hidden=False) ``` Expected Output: ```python { \'*.txt\': [\'file1.txt\'], \'*.gif\': [\'file2.gif\'] } ``` # Notes - Make sure your code handles edge cases such as an empty directory or no matching files. - The function should not change the current working directory. You are encouraged to test your function thoroughly with different directory structures and pattern combinations.","solution":"import glob import os def find_files_with_patterns(base_dir: str, patterns: list, recursive=False, include_hidden=False) -> dict: \'\'\' Search for files in the base directory matching the given patterns. Parameters: base_dir (str): The base directory path where the search will start. patterns (list): A list of patterns to match against. recursive (bool): If True, perform a recursive search using the ** pattern. include_hidden (bool): If True, include files starting with a dot. Returns: dict: A dictionary with patterns as keys and lists of matched files as values. \'\'\' files_dict = {} for pattern in patterns: search_pattern = os.path.join(base_dir, \'**\' if recursive else \'\', pattern) if include_hidden: search_pattern = os.path.join(base_dir, \'**\' if recursive else \'\', pattern) matched_files = glob.glob(search_pattern, recursive=recursive) else: matched_files = [] for file_path in glob.iglob(search_pattern, recursive=recursive): if not os.path.basename(file_path).startswith(\'.\'): matched_files.append(file_path) # Strip the base_dir part from the matched files for consistency in the output format files_dict[pattern] = [os.path.relpath(file, base_dir) for file in matched_files if os.path.isfile(file)] return files_dict"},{"question":"**Coding Assessment Question** # Objective You are tasked with demonstrating your understanding of the `memoryview` object in Python. You will implement functions to manipulate large data sets efficiently through memory views. This will involve creating memoryview objects, performing checks, and accessing properties of these objects. # Task 1. **Create a memoryview Object:** - Implement a function `create_memoryview_from_object(obj)` that takes an object `obj` providing the buffer interface and returns a memoryview object. If `obj` does not support the buffer interface, raise a `TypeError` with the message \\"Object does not support buffer interface\\". 2. **Check if Object is a memoryview:** - Implement a function `is_memoryview(obj)` that returns `True` if `obj` is a memoryview object and `False` otherwise. 3. **Access the Base of a memoryview:** - Implement a function `get_memoryview_base(mview)` that takes a memoryview object `mview` and returns the exporting object. If the memoryview was created using `PyMemoryView_FromMemory` or `PyMemoryView_FromBuffer`, return `None`. # Constraints and Function Signatures - Do not use external libraries other than standard Python libraries. - Assume input object for `create_memoryview_from_object` will either support the buffer interface or not; you don\'t need to handle partial support. # Function Signatures ```python def create_memoryview_from_object(obj) -> memoryview: Create a memoryview object from an object that supports the buffer interface. Parameters: obj: An object that supports the buffer interface. Returns: memoryview object. Raises: TypeError: If the input object does not support the buffer interface. pass def is_memoryview(obj) -> bool: Check if an object is a memoryview. Parameters: obj: Any Python object. Returns: bool: True if obj is a memoryview, False otherwise. pass def get_memoryview_base(mview) -> object: Access the base object of a memoryview. Parameters: mview: A memoryview object. Returns: object: The base object if available, or None. pass ``` # Example Usage ```python # Example usage and expected results # Test create_memoryview_from_object with a bytearray byte_array = bytearray(b\\"hello\\") memory = create_memoryview_from_object(byte_array) # Ensure the memoryview was created assert isinstance(memory, memoryview) # Test is_memoryview assert is_memoryview(memory) == True assert is_memoryview(byte_array) == False # Test get_memoryview_base assert get_memoryview_base(memory) == byte_array ``` Implement these functions to complete the task.","solution":"def create_memoryview_from_object(obj): Create a memoryview object from an object that supports the buffer interface. Parameters: obj: An object that supports the buffer interface. Returns: memoryview object. Raises: TypeError: If the input object does not support the buffer interface. try: return memoryview(obj) except TypeError: raise TypeError(\\"Object does not support buffer interface\\") def is_memoryview(obj): Check if an object is a memoryview. Parameters: obj: Any Python object. Returns: bool: True if obj is a memoryview, False otherwise. return isinstance(obj, memoryview) def get_memoryview_base(mview): Access the base object of a memoryview. Parameters: mview: A memoryview object. Returns: object: The base object if available, or None. if not isinstance(mview, memoryview): raise TypeError(\\"Input is not a memoryview object\\") return mview.obj"},{"question":"# Pandas Series Data Analysis Question Objective Demonstrate your comprehension of fundamental and advanced concepts of the pandas Series by manipulating and analyzing a given dataset. Problem Statement You are provided with a dataset concerning the sales data of a company for the year 2023. Each entry in the dataset contains information about the date of sale, the product sold, the quantity sold, and the revenue generated. You need to perform the following tasks using `pandas` Series: 1. **Data Ingestion:** - Create a pandas Series for each column in the dataset. 2. **Data Cleaning:** - Handle any missing values in the quantity sold and revenue columns by replacing them with the median of the respective columns. 3. **Data Transformation:** - Compute the average daily revenue for each month in 2023 and create a new Series to store these values. 4. **Data Analysis:** - Identify the top 3 products by total revenue generated throughout the year. - Find the day with the highest total revenue in the dataset. - Determine the revenue variance for each product. 5. **Use of Accessors:** - Extract and create a Series for the day of the week names from the dates provided. Dataset Format ```plaintext Date,Product,Quantity Sold,Revenue 2023-01-01,Product A,10,100 2023-01-01,Product B,5,50 2023-01-02,Product A,8,80 ... 2023-12-31,Product C,7,70 ``` Expected Solution Provide a Python script that performs the following steps: 1. **Reading and Creating Series:** - Read the dataset into a pandas DataFrame. - Convert each column into a pandas Series. 2. **Handling Missing Data:** - Replace missing values in the `Quantity Sold` and `Revenue` Series with their respective medians. 3. **Computing Average Daily Revenue Per Month:** - Group the data by month and calculate the average daily revenue for each month. - Store the results in a Series indexed by month names. 4. **Identifying Top Products and Analysis:** - Calculate total revenue for each product and identify the top 3 products. - Find the date with the highest revenue. - Calculate the revenue variance for each product and store the results in a Series indexed by product names. 5. **Creating the Day of the Week Series:** - Extract the day of the week names from the `Date` Series and create a corresponding Series. Constraints - Use pandas functions and methods wherever applicable. - Ensure your code handles edge cases, such as missing data or ties in top products. Submission Submit the Python script containing your solution, including comments wherever necessary to explain your code. ```python import pandas as pd # Ensure to provide the dataset file path file_path = \\"path_to_the_sales_data.csv\\" # Implement the solution here ```","solution":"import pandas as pd def sales_data_analysis(file_path): # Read the dataset into a pandas DataFrame df = pd.read_csv(file_path) # Convert each column into a pandas Series dates = df[\'Date\'] products = df[\'Product\'] quantities = df[\'Quantity Sold\'] revenues = df[\'Revenue\'] # Handling missing data quantities.fillna(quantities.median(), inplace=True) revenues.fillna(revenues.median(), inplace=True) # Compute average daily revenue for each month df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.month_name() monthly_avg_daily_revenue = df.groupby(\'Month\')[\'Revenue\'].mean() # Identifying top 3 products by total revenue total_revenue_by_product = df.groupby(\'Product\')[\'Revenue\'].sum() top_3_products = total_revenue_by_product.nlargest(3) # Day with the highest total revenue total_revenue_by_date = df.groupby(\'Date\')[\'Revenue\'].sum() highest_revenue_date = total_revenue_by_date.idxmax() # Revenue variance for each product revenue_variance_by_product = df.groupby(\'Product\')[\'Revenue\'].var() # Create a Series for the day of the week names day_of_week = df[\'Date\'].dt.day_name() return { \'quantities\': quantities, \'revenues\': revenues, \'monthly_avg_daily_revenue\': monthly_avg_daily_revenue, \'top_3_products\': top_3_products, \'highest_revenue_date\': highest_revenue_date, \'revenue_variance_by_product\': revenue_variance_by_product, \'day_of_week\': day_of_week }"},{"question":"**Objective**: Implement functions to serialize and deserialize Python objects using the `marshal` module in ways that handle versioning and type support constraints. **Problem Statement**: You are tasked with creating two functions: 1. `save_object_to_file(obj, filename, version)` 2. `load_object_from_file(filename)` **Function Specifications**: 1. `save_object_to_file` - **Input**: - `obj` (Any supported type by `marshal`): The object to be serialized. - `filename` (str): The name of the binary file to write the serialized object to. - `version` (int, optional): The version of the serialization format to use. - **Output**: - None - **Behavior**: - This function should serialize the provided object using the `marshal.dump` method and write it to a binary file with the given filename. - If the object contains unsupported types, the function should raise a `ValueError` and ensure that the file is not written with invalid data. 2. `load_object_from_file` - **Input**: - `filename` (str): The name of the binary file to read the serialized object from. - **Output**: - The deserialized object. - **Behavior**: - This function should read a binary file containing serialized data using the `marshal.load` method and return the deserialized object. - If the read operation fails due to an error (such as an incompatible format), the function should handle exceptions `EOFError`, `ValueError`, or `TypeError`, and return `None`. **Constraints**: - Python version should be 3.10 or above. - The filename provided must be a valid string representing the path to a writable and readable binary file. - The `version` argument, if provided, must be between 0 and the current `marshal` version (4). **Example Usage**: ```python import marshal def save_object_to_file(obj, filename, version=marshal.version): try: with open(filename, \'wb\') as file: marshal.dump(obj, file, version) except ValueError: raise ValueError(\\"Unsupported type found in object\\") def load_object_from_file(filename): try: with open(filename, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError): return None # Example of saving and loading a dictionary data = {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [85.5, 92.0, 78.1]} filename = \'example_data.bin\' # Saving the object to file save_object_to_file(data, filename) # Loading the object from file loaded_data = load_object_from_file(filename) print(loaded_data) # Output: {\'name\': \'Alice\', \'age\': 30, \'scores\': [85.5, 92.0, 78.1]} ``` **Notes**: - Ensure proper file handling to avoid leaving files open or corrupted. - Handle exceptions appropriately to return meaningful error messages or fallback values.","solution":"import marshal def save_object_to_file(obj, filename, version=marshal.version): Serializes the object and saves it to a file. Parameters: obj (Any): The object to serialize. filename (str): The name of the file to save the object. version (int): The version of the marshal format. Raises: ValueError: If the object contains unsupported types. if not isinstance(obj, (int, float, bool, bytes, str, list, tuple, dict, set, type(None))): raise ValueError(\\"Unsupported type found in object\\") try: with open(filename, \'wb\') as file: marshal.dump(obj, file, version) except ValueError as e: raise ValueError(\\"Serialization failed due to an unsupported type\\") from e def load_object_from_file(filename): Loads a serialized object from a file. Parameters: filename (str): The name of the file to load the object from. Returns: Any: The deserialized object, or None if deserialization fails. try: with open(filename, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError): return None"},{"question":"# Custom Interactive Python Interpreter Objective Write a custom interactive Python interpreter using the `code` and `codeop` modules. Your interpreter should be able to evaluate Python expressions interactively and handle both complete and incomplete input efficiently. Requirements 1. Create a class `CustomInterpreter` with the following methods: - `__init__(self)`: Initializes the interpreter. - `run(self)`: Starts the interactive loop to read and evaluate Python expressions. - `evaluate(self, source)`: Evaluates the given Python source code string. 2. The `evaluate` method should: - Use `codeop` to compile the given code. - If the code is incomplete, it should prompt the user for more input until a complete statement is provided. 3. The `run` method should: - Continuously read user input until the user types `exit()`. - Use the `evaluate` method to process each input chunk. Input and Output - **Input**: The interpreter should read inputs line by line from the user. - **Output**: The interpreter should print the results of evaluated expressions or errors if the evaluation fails. Constraints - You should handle typical user input cases (e.g., multi-line input, syntactically incorrect input, etc.). - Performance requirements are not a primary concern, but the solution should be logically sound and handle edge cases. Example ```python >>> interpreter = CustomInterpreter() >>> interpreter.run() >>> 2 + 2 4 >>> for i in range(3): ... print(i) ... 0 1 2 >>> exit() Goodbye! ``` In this example, the interpreter evaluates Python expressions interactively, handles incomplete input correctly, and exits the loop when `exit()` is typed. Notes - Make sure to use the `codeop` module for compiling code and checking for completeness. - Handle exceptions and provide user-friendly error messages.","solution":"import codeop class CustomInterpreter: def __init__(self): self.compiler = codeop.CommandCompiler() self.locals = {} def evaluate(self, source): try: code = self.compiler(source, \'<input>\', \'exec\') if code is None: return \'incomplete\' exec(code, self.locals) except Exception as e: return f\'Error: {str(e)}\' return \'complete\' def run(self): buffer = \'\' while True: prompt = \'>>> \' if buffer == \'\' else \'... \' try: line = input(prompt) if line.strip() == \'exit()\': print(\'Goodbye!\') break buffer += line + \'n\' result = self.evaluate(buffer) if result == \'complete\': buffer = \'\' elif result == \'incomplete\': continue else: print(result) buffer = \'\' except EOFError: print(\'Goodbye!\') break"},{"question":"Objective Write a PyTorch program that demonstrates the usage of `torch.utils.module_tracker.ModuleTracker` by constructing a simple neural network with at least three layers. The program should use the `ModuleTracker` to track the layers\' names and positions during a forward pass. Problem Statement 1. Define a simple neural network class `SimpleNN` with three layers using `torch.nn.Module`. The three layers can be: - A linear layer. - A ReLU activation layer. - Another linear layer. 2. Implement a forward pass method where the `ModuleTracker` is used to track and print the name and current position of each module in the hierarchy. Expected Input and Output - The input to the neural network should be a tensor of shape `(batch_size, input_features)`. You can assume `batch_size = 1` and `input_features = 3` for simplicity in testing. - The output should be the layer names and their positions as they are processed during the forward pass, followed by the final output tensor. Constraints - Do not use any external libraries other than PyTorch. - Ensure the network is built and tested within the program to demonstrate the working of `ModuleTracker`. Example ```python import torch import torch.nn as nn from torch.utils.module_tracker import ModuleTracker # Define a simple neural network class class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(3, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 2) def forward(self, x): tracker = ModuleTracker() x = tracker.track(self.fc1(x), self.fc1) x = tracker.track(self.relu(x), self.relu) x = tracker.track(self.fc2(x), self.fc2) return x # Test the SimpleNN with a sample input if __name__ == \\"__main__\\": model = SimpleNN() sample_input = torch.randn(1, 3) output = model.forward(sample_input) # Expected output (the exact names and positions may vary): # Layer: fc1, Position: <module path> # Layer: relu, Position: <module path> # Layer: fc2, Position: <module path> # Final Output Tensor: tensor([...]) ```","solution":"import torch import torch.nn as nn # Define a simple neural network class class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(3, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 2) def forward(self, x): layers = [ (\'fc1\', self.fc1), (\'relu\', self.relu), (\'fc2\', self.fc2), ] for name, layer in layers: x = layer(x) print(f\\"Layer: {name}, Output shape: {x.shape}\\") return x"},{"question":"**Objective:** Demonstrate your understanding of handling missing values in pandas by implementing a function that deals with a variety of missing value scenarios in a DataFrame, including numeric, categorical, and datetime data. **Function to Implement:** ```python import pandas as pd import numpy as np def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: This function takes a pandas DataFrame as input and performs the following operations: 1. Detects missing values in `numeric` columns and fills them with the mean of the respective column. 2. Detects missing values in `categorical` columns and fills them with the most frequent value of the respective column. 3. Detects missing datetime values and fills them with the previous valid observation (\'ffill\' method). Parameters: df (pd.DataFrame): Input DataFrame that contains numeric, categorical, and datetime columns Returns: pd.DataFrame: A DataFrame with missing values handled as per the described rules. pass ``` **Input:** - `df`: A pandas DataFrame containing columns with numeric, categorical, and datetime data. Columns with missing values will have NaN for numeric and categorical or NaT for datetime types. **Output:** - A pandas DataFrame with the same structure as the input but with missing values handled according to the specified requirements. **Constraints:** - The DataFrame may contain any number of numeric, categorical, and datetime columns. - The numeric columns will only contain floating-point numbers. - The categorical columns will be of type `object`. - The datetime columns will be of type `datetime64[ns]`. **Example:** Given the following DataFrame `df`: ```python data = {\'A\': [1.0, 2.0, np.nan, 4.0], \'B\': [\'cat\', \'dog\', np.nan, \'dog\'], \'C\': [pd.Timestamp(\'2023-01-01\'), pd.NaT, pd.Timestamp(\'2023-01-03\'), pd.Timestamp(\'2023-01-04\')]} df = pd.DataFrame(data) ``` The function should return: ```python A B C 0 1.0 cat 2023-01-01 1 2.0 dog 2023-01-01 2 2.333333 dog 2023-01-03 3 4.0 dog 2023-01-04 ``` # Notes: - The mean of the numeric column \'A\' should be calculated as (1+2+4)/3 = 2.333333. - The most frequent value for the categorical column \'B\' should be \'dog\'. - The missing datetime value in column \'C\' should be forward filled from the previous valid observation.","solution":"import pandas as pd import numpy as np def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: This function takes a pandas DataFrame as input and performs the following operations: 1. Detects missing values in `numeric` columns and fills them with the mean of the respective column. 2. Detects missing values in `categorical` columns and fills them with the most frequent value of the respective column. 3. Detects missing datetime values and fills them with the previous valid observation (\'ffill\' method). Parameters: df (pd.DataFrame): Input DataFrame that contains numeric, categorical, and datetime columns Returns: pd.DataFrame: A DataFrame with missing values handled as per the described rules. df = df.copy() # Fill missing numeric values with the column mean numeric_columns = df.select_dtypes(include=[np.number]).columns for column in numeric_columns: if df[column].isnull().any(): mean_value = df[column].mean() df[column].fillna(mean_value, inplace=True) # Fill missing categorical values with the most frequent value categorical_columns = df.select_dtypes(include=[object]).columns for column in categorical_columns: if df[column].isnull().any(): most_frequent_value = df[column].mode()[0] df[column].fillna(most_frequent_value, inplace=True) # Fill missing datetime values with forward fill datetime_columns = df.select_dtypes(include=[np.datetime64]).columns for column in datetime_columns: if df[column].isnull().any(): df[column].fillna(method=\'ffill\', inplace=True) return df"},{"question":"**Question: Command Line Option Parser using `getopt`** You are required to write a Python script that processes command line arguments using the `getopt` module. The script should demonstrate the use of both short and long options, handle errors gracefully, and show the difference between `getopt.getopt()` and `getopt.gnu_getopt()`. # Instructions: 1. **Create a Python script** that: - Uses both short (`-a`) and long (`--alpha`) options. - Includes options that require arguments (-o/output) and options that do not (-v/verbose). - Processes the arguments to distinguish them into options and non-options. - Handles errors gracefully using `getopt.GetoptError`. - Demonstrates how non-option arguments are handled differently in `getopt.getopt()` and `getopt.gnu_getopt()`. 2. **Input Format:** - The script should accept a list of command line arguments simulating `sys.argv[1:]`. 3. **Output Format:** - Print the parsed options and remaining arguments. - Print error messages for unrecognized options or missing arguments. - Clearly indicate the difference in behavior between `getopt.getopt()` and `getopt.gnu_getopt()`. # Function Signature: ```python def parse_arguments(args: list) -> None: pass ``` # Example: ```python import sys import getopt def parse_arguments(args): try: opts, args = getopt.getopt(args, \\"va:o:\\", [\\"verbose\\", \\"alpha\\", \\"output=\\"]) print(\\"Using getopt():\\") print(\\"Options:\\", opts) print(\\"Arguments:\\", args) opts, args = getopt.gnu_getopt(args, \\"va:o:\\", [\\"verbose\\", \\"alpha\\", \\"output=\\"]) print(\\"Using gnu_getopt():\\") print(\\"Options:\\", opts) print(\\"Arguments:\\", args) except getopt.GetoptError as err: print(\\"Error:\\", err) sys.exit(2) if __name__ == \\"__main__\\": example_args = [\'-v\', \'--alpha\', \'-o\', \'outputfile\', \'nonoption1\', \'--output=outputfile2\', \'nonoption2\'] parse_arguments(example_args) ``` # Constraints: - Assume the command line options follow the standard conventions. - Do not use any external libraries apart from `getopt` and `sys`. - The script should run in Python 3.10 or later.","solution":"import getopt import sys def parse_arguments(args): try: # Using getopt.getopt() opts_gopt, args_gopt = getopt.getopt( args, \\"va:o:\\", [\\"verbose\\", \\"alpha\\", \\"output=\\"] ) print(\\"Using getopt():\\") print(\\"Options:\\", opts_gopt) print(\\"Arguments:\\", args_gopt) # Resetting args for gnu_getopt() args = [\'-v\', \'--alpha\', \'-o\', \'outputfile\', \'nonoption1\', \'--output=outputfile2\', \'nonoption2\'] # Using getopt.gnu_getopt() opts_gnugetopt, args_gnugetopt = getopt.gnu_getopt( args, \\"va:o:\\", [\\"verbose\\", \\"alpha\\", \\"output=\\"] ) print(\\"Using gnu_getopt():\\") print(\\"Options:\\", opts_gnugetopt) print(\\"Arguments:\\", args_gnugetopt) except getopt.GetoptError as err: print(\\"Error:\\", err) sys.exit(2) if __name__ == \\"__main__\\": example_args = [ \'-v\', \'--alpha\', \'-o\', \'outputfile\', \'nonoption1\', \'--output=outputfile2\', \'nonoption2\' ] parse_arguments(example_args)"},{"question":"# Question: Creating and Manipulating Cookies in Python Objective: Your task is to demonstrate your understanding of the `http.cookies` module by creating, manipulating, and generating HTTP headers for cookies. Task: 1. Create a class `CookieManager` that will handle operations related to HTTP cookies. 2. Implement the following methods in the `CookieManager` class: - `__init__(self)`: Initialize an instance of `SimpleCookie`. - `add_cookie(self, key: str, value: str, attributes: dict = None)`: Add a cookie with the given `key` and `value`. If `attributes` are provided, set the specified attributes for the cookie. - `get_cookie(self, key: str) -> str`: Retrieve the value of a cookie by its `key`. - `generate_http_header(self) -> str`: Generate the HTTP header for all cookies stored in the `SimpleCookie` object. - `load_cookies_from_header(self, header: str)`: Load cookies from a given HTTP header string. Constraints: - `key` is a string containing valid cookie name characters. - `value` is a string containing the cookie\'s value. Example Usage: ```python # Create an instance of CookieManager cookie_manager = CookieManager() # Add cookies cookie_manager.add_cookie(\\"session_id\\", \\"abc123\\") cookie_manager.add_cookie(\\"user\\", \\"john_doe\\", {\\"path\\": \\"/\\", \\"secure\\": True, \\"samesite\\": \\"Strict\\"}) # Retrieve a cookie value print(cookie_manager.get_cookie(\\"session_id\\")) # Output: abc123 # Generate HTTP header for cookies print(cookie_manager.generate_http_header()) # Example Output: Set-Cookie: session_id=abc123rnSet-Cookie: user=john_doe; Path=/; Secure; SameSite=Strict # Load cookies from an HTTP header string cookie_manager.load_cookies_from_header(\\"session_id=abc123; user=john_doe\\") print(cookie_manager.get_cookie(\\"user\\")) # Output: john_doe ``` Requirements: 1. Use the `SimpleCookie` class from the `http.cookies` module to manage cookies. 2. Implement proper error handling in your methods. 3. Ensure that the attributes for cookies (such as `path`, `secure`, and `samesite`) are handled correctly.","solution":"from http.cookies import SimpleCookie class CookieManager: def __init__(self): self.cookie = SimpleCookie() def add_cookie(self, key: str, value: str, attributes: dict = None): self.cookie[key] = value if attributes: for attr_name, attr_value in attributes.items(): self.cookie[key][attr_name] = attr_value def get_cookie(self, key: str) -> str: if key in self.cookie: return self.cookie[key].value return None def generate_http_header(self) -> str: return self.cookie.output(header=\'\', sep=\'rn\').strip() def load_cookies_from_header(self, header: str): self.cookie.load(header)"},{"question":"# Question: Text Processing with Regular Expressions and Custom Formatting You are tasked with writing a function that processes an input string to extract specific information and format it according to given specifications. Function Signature ```python def process_and_format_text(input_str: str) -> str: pass ``` Input - `input_str`: A string that contains text following a specific pattern. The string may include names of people, dates, and various descriptive text. For example, \\"Name: John Doe, Date of Birth: 1990-01-01, Description: Software Engineer\\". Output - Returns a string that contains the extracted names and dates, formatted according to the given specifications. The output format should be: \\"Names: [name1, name2, ...] Dates: [date1, date2, ...]\\". Constraints - The names in the input will follow the pattern `Name: First Last` where `First` and `Last` are strings of alphabetic characters. - The dates in the input will follow the pattern `Date of Birth: YYYY-MM-DD` where `YYYY`, `MM`, and `DD` are digits representing the year, month, and day, respectively. - The input string will contain at least one name and one date. Example ```python input_str = \\"Name: John Doe, Date of Birth: 1990-01-01, Description: Software Engineer, Name: Jane Smith, Date of Birth: 1985-05-15, Description: Data Scientist\\" output = process_and_format_text(input_str) print(output) # Output should be: \\"Names: [John Doe, Jane Smith] Dates: [1990-01-01, 1985-05-15]\\" ``` Requirements - Use the `re` module to extract names and dates from the input string. - Use `string` module\'s custom string formatting capabilities to produce the output string. - Your solution should efficiently handle strings of length up to 10^5 characters.","solution":"import re def process_and_format_text(input_str: str) -> str: Processes the input string to extract names and dates and format them. Parameters: input_str (str): The input string containing names, dates, and other text. Returns: str: Formatted string with extracted names and dates. # Extract names name_pattern = re.compile(r\'Name:s*([A-Za-z]+s+[A-Za-z]+)\') names = name_pattern.findall(input_str) # Extract dates of birth date_pattern = re.compile(r\'Date of Birth:s*(d{4}-d{2}-d{2})\') dates = date_pattern.findall(input_str) # Format output formatted_output = f\\"Names: [{\', \'.join(names)}] Dates: [{\', \'.join(dates)}]\\" return formatted_output"},{"question":"**Coding Assessment Question: Seaborn JointGrid Visualizations** You have been provided with a dataset `penguins` which contains penguin species data, including measurements such as `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Your task is to create a comprehensive visual analysis of this dataset using Seaborn\'s `JointGrid`. Follow the detailed steps below: # Instructions 1. **Data Preparation**: Load the `penguins` dataset using Seaborn. 2. **Joint Plot**: Create a `JointGrid` to visualize `bill_length_mm` vs `bill_depth_mm`. - Use a scatter plot for the joint axis. - Use histograms for the marginal axes. 3. **Customization**: - Add a KDE plot to the marginal axes. - Incorporate a `hue` parameter to differentiate the species. - Add reference lines at `x=45` and `y=16`. 4. **Advanced Plot**: - Create another `JointGrid` for `flipper_length_mm` vs `body_mass_g`. - Use a regression plot for the joint axis. - Use box plots for the marginal axes. 5. **Visualization Adjustments**: - Set the height of the second JointGrid to 6. - Set the ratio of joint and marginal plots to 3. - Configure the second JointGrid to show marginal ticks. - Limit the `x` and `y` axis of the second JointGrid to (150, 250) for `flipper_length_mm` and (2500, 6500) for `body_mass_g` respectively. # Expected Output 1. Two comprehensive visualizations should be generated: - One for `bill_length_mm` vs `bill_depth_mm` with customization. - One for `flipper_length_mm` vs `body_mass_g` with specified adjustments. 2. The visualizations should distinguish different species using color (hue). # Constraints - Use only the Seaborn and Matplotlib libraries. - Ensure the use of `JointGrid` for constructing the visualizations. # Example Code Template ```python import seaborn as sns import matplotlib.pyplot as plt def create_jointgrid_visualizations(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # First JointGrid for bill measurements g1 = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") g1.plot(sns.scatterplot, sns.histplot) g1.plot_marginals(sns.histplot, kde=True) g1.refline(x=45, y=16) plt.show() # Second JointGrid for flipper length vs body mass g2 = sns.JointGrid(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", height=6, ratio=3, marginal_ticks=True, xlim=(150, 250), ylim=(2500, 6500)) g2.plot(sns.regplot, sns.boxplot) plt.show() # Call the function to execute the visualizations create_jointgrid_visualizations() ``` Write the function `create_jointgrid_visualizations` to satisfy the above requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_jointgrid_visualizations(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # First JointGrid for bill measurements g1 = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") g1.plot(sns.scatterplot, sns.histplot) g1.plot_marginals(sns.histplot, kde=True) g1.refline(x=45, y=16) plt.show() # Second JointGrid for flipper length vs body mass g2 = sns.JointGrid(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", height=6, ratio=3, marginal_ticks=True, xlim=(150, 250), ylim=(2500, 6500)) g2.plot(sns.regplot, sns.boxplot) plt.show() # Call the function to execute the visualizations create_jointgrid_visualizations()"},{"question":"# Biclustering Assessment Question Objective The objective of this task is to implement the Spectral Co-Clustering algorithm and evaluate its performance using the Jaccard index. Problem Statement Write a Python function `spectral_co_clustering` that performs biclustering on a given data matrix using the Spectral Co-Clustering algorithm. Your implementation should also have a function `evaluate_biclustering` that uses the Jaccard index to evaluate the performance of the clustering. Input 1. `data` (numpy.ndarray): A 2D array representing the input data matrix. 2. `n_clusters` (int): Number of clusters for biclustering. Output A dictionary containing: - `row_clusters` (list of lists): Each sublist contains indices of the rows belonging to a cluster. - `column_clusters` (list of lists): Each sublist contains indices of the columns belonging to a cluster. - `jaccard_index` (float): The Jaccard index evaluating the performance of clustering. Constraints - The input data matrix will have dimensions `m x n` where `1 <= m, n <= 1000`. - `1 <= n_clusters <= min(m, n) / 2` - Use a random seed for reproducibility where needed. Function Signature ```python import numpy as np from scipy.optimize import linear_sum_assignment def spectral_co_clustering(data: np.ndarray, n_clusters: int) -> dict: # Implement the Spectral Co-Clustering algorithm pass def evaluate_biclustering(true_biclusters: dict, pred_biclusters: dict) -> float: # Use the Jaccard index for evaluating the performance pass ``` # Example ```python data = np.array([[1, 2, 1, 0, 0], [3, 3, 2, 0, 0], [1, 1, 1, 0, 1], [0, 0, 0, 3, 3], [0, 0, 0, 4, 4]]) true_biclusters = { \'row_clusters\': [[0, 1, 2], [3, 4]], \'column_clusters\': [[0, 1, 2], [3, 4]] } result = spectral_co_clustering(data, 2) jaccard_index = evaluate_biclustering(true_biclusters, result) print(result) print(f\\"Jaccard Index: {jaccard_index:.2f}\\") ``` # Note - You may use `sklearn.cluster.SpectralCoclustering` to verify the accuracy of your implementation. - Ensure that the input data is preprocessed as described in the documentation. - Use random seed where needed for reproducibility. - Apply `k-means` clustering for partitioning rows and columns. Evaluation Criteria - **Correctness:** The functions should correctly implement the Spectral Co-Clustering algorithm and evaluation metrics. - **Efficiency:** The solution should be able to handle matrices up to the size of 1000 x 1000 efficiently. - **Clarity:** The code should be well-commented and easy to understand.","solution":"import numpy as np from sklearn.cluster import SpectralCoclustering from scipy.optimize import linear_sum_assignment def spectral_co_clustering(data: np.ndarray, n_clusters: int) -> dict: Perform spectral co-clustering on a data matrix. Parameters: data (np.ndarray): Input data matrix. n_clusters (int): Number of clusters for biclustering. Returns: dict: Biclustering results containing row_clusters, column_clusters, and jaccard_index. model = SpectralCoclustering(n_clusters=n_clusters, random_state=0) model.fit(data) row_clusters = [np.where(model.row_labels_ == i)[0].tolist() for i in range(n_clusters)] column_clusters = [np.where(model.column_labels_ == i)[0].tolist() for i in range(n_clusters)] return {\'row_clusters\': row_clusters, \'column_clusters\': column_clusters} def evaluate_biclustering(true_biclusters: dict, pred_biclusters: dict) -> float: Evaluate biclustering performance using Jaccard index. Parameters: true_biclusters (dict): Ground truth biclusters containing row_clusters and column_clusters. pred_biclusters (dict): Predicted biclusters containing row_clusters and column_clusters. Returns: float: Jaccard index evaluating the performance. def jaccard_index(set1, set2): intersection = len(set1.intersection(set2)) union = len(set1.union(set2)) return intersection / union # Linear assignment (Hungarian algorithm) to match clusters cost_matrix = np.zeros((len(true_biclusters[\'row_clusters\']), len(pred_biclusters[\'row_clusters\']))) for i, true_cluster in enumerate(true_biclusters[\'row_clusters\']): true_cluster_set = set(true_cluster) for j, pred_cluster in enumerate(pred_biclusters[\'row_clusters\']): pred_cluster_set = set(pred_cluster) cost_matrix[i, j] = -jaccard_index(true_cluster_set, pred_cluster_set) row_ind, col_ind = linear_sum_assignment(cost_matrix) row_jaccard = -cost_matrix[row_ind, col_ind].mean() cost_matrix = np.zeros((len(true_biclusters[\'column_clusters\']), len(pred_biclusters[\'column_clusters\']))) for i, true_cluster in enumerate(true_biclusters[\'column_clusters\']): true_cluster_set = set(true_cluster) for j, pred_cluster in enumerate(pred_biclusters[\'column_clusters\']): pred_cluster_set = set(pred_cluster) cost_matrix[i, j] = -jaccard_index(true_cluster_set, pred_cluster_set) col_ind, row_ind = linear_sum_assignment(cost_matrix) col_jaccard = -cost_matrix[col_ind, row_ind].mean() return (row_jaccard + col_jaccard) / 2"},{"question":"**Question: Text Log Analysis using Regular Expressions** You are provided with a multi-line string containing log entries. Each log entry is a separate line following the format: ``` [date-time] [log-level] [log-message] ``` Where: - `date-time` is in the format `YYYY-MM-DD HH:MM:SS`. - `log-level` is one of `INFO`, `WARNING`, or `ERROR`. - `log-message` is a descriptive message that might contain spaces, punctuation, and special characters. Your task is to implement the following functions to analyze this log: 1. **extract_errors(log)** - Input: A multi-line string `log` containing log entries. - Output: A list of all log messages with the log-level `ERROR`. 2. **count_log_levels(log)** - Input: A multi-line string `log` containing log entries. - Output: A dictionary with the count of each log-level (INFO, WARNING, ERROR). - Example Output: ```python {\'INFO\': 10, \'WARNING\': 5, \'ERROR\': 2} ``` 3. **mask_sensitive_data(log, sensitive_words)** - Input: - A multi-line string `log` containing log entries. - A list `sensitive_words` containing words that need to be masked. - Output: The modified `log` string where each occurrence of a word in `sensitive_words` is replaced with the string `\\"[REDACTED]\\"`. **Constraints:** - The log messages and sensitive words are case-sensitive. - Performance should be efficient for logs containing up to 10,000 entries. - Utilize the `re` module functions for pattern matching and substitution. **Example Logs:** ```plaintext 2023-03-10 08:32:45 INFO User logged in 2023-03-10 09:15:23 WARNING Disk space low 2023-03-10 09:45:18 ERROR Failed to write file 2023-03-10 10:00:00 INFO User logged out 2023-03-10 10:01:05 ERROR Connection timed out ``` **Test Cases:** ```python log_data = 2023-03-10 08:32:45 INFO User logged in 2023-03-10 09:15:23 WARNING Disk space low 2023-03-10 09:45:18 ERROR Failed to write file 2023-03-10 10:00:00 INFO User logged out 2023-03-10 10:01:05 ERROR Connection timed out assert extract_errors(log_data) == [ \\"Failed to write file\\", \\"Connection timed out\\" ] assert count_log_levels(log_data) == { \'INFO\': 2, \'WARNING\': 1, \'ERROR\': 2 } sensitive_words = [\'logged\', \'file\'] expected_output = 2023-03-10 08:32:45 INFO User [REDACTED] in 2023-03-10 09:15:23 WARNING Disk space low 2023-03-10 09:45:18 ERROR Failed to write [REDACTED] 2023-03-10 10:00:00 INFO User [REDACTED] out 2023-03-10 10:01:05 ERROR Connection timed out assert mask_sensitive_data(log_data, sensitive_words) == expected_output ``` **Implementation:** ```python import re def extract_errors(log): patterns = re.findall(r\'ERROR (.+)\', log, re.MULTILINE) return patterns def count_log_levels(log): levels = re.findall(r\'b(INFO|WARNING|ERROR)b\', log) return {level: levels.count(level) for level in set(levels)} def mask_sensitive_data(log, sensitive_words): for word in sensitive_words: log = re.sub(r\'b{}b\'.format(re.escape(word)), \'[REDACTED]\', log) return log ```","solution":"import re def extract_errors(log): Extracts and returns a list of all log messages with the log-level \'ERROR\'. Args: log (str): A multi-line string containing log entries. Returns: list: A list containing all log messages with log-level \'ERROR\'. patterns = re.findall(r\'bERROR (.+)\', log, re.MULTILINE) return patterns def count_log_levels(log): Counts the number of occurrences for each log-level (INFO, WARNING, ERROR) in the given log entries and returns a dictionary with these counts. Args: log (str): A multi-line string containing log entries. Returns: dict: A dictionary with log levels as keys and their counts as values. levels = re.findall(r\'b(INFO|WARNING|ERROR)b\', log) return {level: levels.count(level) for level in set(levels)} def mask_sensitive_data(log, sensitive_words): Masks each occurrence of words in sensitive_words with \'[REDACTED]\' in the given log entries string. Args: log (str): A multi-line string containing log entries. sensitive_words (list): A list of words that need to be masked. Returns: str: The modified log with sensitive words masked. for word in sensitive_words: log = re.sub(r\'b{}b\'.format(re.escape(word)), \'[REDACTED]\', log) return log"}]'),A={name:"App",components:{PoemCard:z},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],N={key:0},M={key:1};function O(n,e,l,m,s,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>s.searchQuery=r),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",q,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[s.isLoading?(a(),i("span",M,"Loading...")):(a(),i("span",N,"See more"))],8,F)):d("",!0)])}const L=p(A,[["render",O],["__scopeId","data-v-e40a7786"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/65.md","filePath":"quotes/65.md"}'),j={name:"quotes/65.md"},B=Object.assign(j,{setup(n){return(e,l)=>(a(),i("div",null,[x(L)]))}});export{Y as __pageData,B as default};
