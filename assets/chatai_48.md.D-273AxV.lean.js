import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},S={class:"review-content"};function E(n,e,l,m,s,o){return a(),i("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",E],["__scopeId","data-v-5ebcb59a"]]),I=JSON.parse('[{"question":"<|Analysis Begin|> From the provided documentation, we understand that python310 offers a variety of object types and specific functions catered to these types. These include: 1. **Fundamental Objects** such as \\"None\\". 2. **Numeric Objects** including Integer, Boolean, Floating Point, and Complex Number Objects. 3. **Sequence Objects** such as Bytes, Byte Arrays, Unicode Strings, Tuples, and Lists. 4. **Container Objects** which include Dictionary and Set Objects. 5. **Function Objects** and their various forms like Instance Methods, Methods, and Code Objects. 6. **Other Miscellaneous Objects** including File Objects, Module Objects, Iterator Objects, and various others. Given the wide range of objects and the detailed operations that can be performed on them, there is a lot of material to work with to design a challenging coding question. Key concepts that can be tested include: - Understanding and manipulating different object types. - Using specific functions correctly based on object types. - Implementing advanced coding solutions that require integration and interplay of various object types. The focus of the question will be on a function that utilizes different types of objects and their methods. <|Analysis End|> <|Question Begin|> You are working on a project that requires handling multiple types of Python objects and performing various operations on them. You are tasked to implement a function that takes a combination of numeric and container objects, manipulates them, and produces a certain output. # Task Write a Python function `process_objects` that performs the following operations: 1. Accepts three arguments: - `num_obj`: This can be an integer, float, or complex number. - `seq_obj`: This can be a list or a tuple containing numeric values. - `dict_obj`: This should be a dictionary where the keys are strings and the values are numeric. 2. The function should: - Check if `num_obj` is of type `int`, `float`, or `complex`. If not, raise a `TypeError`. - Check if `seq_obj` is a list or tuple. If it contains any non-numeric values, raise a `ValueError`. - Check if `dict_obj` is a dictionary where all values are numeric. If any value is not numeric, raise a `ValueError`. 3. Perform the following operations: - Calculate the sum of all numeric values in `seq_obj`. - Calculate the product of `num_obj` and the previously calculated sum. - Create a new dictionary where each value from `dict_obj` is squared. - Return a tuple containing: - The product calculated. - The new dictionary with squared values. # Input 1. `num_obj` - An integer, float, or complex number. 2. `seq_obj` - A list or tuple of numeric values. 3. `dict_obj` - A dictionary with string keys and numeric values. # Output A tuple (`prod_result`, `squared_dict`) where: - `prod_result` is the product of `num_obj` and the sum of all values in `seq_obj`. - `squared_dict` is a dictionary with each value from `dict_obj` squared. # Example ```python num_obj = 2 seq_obj = [1, 2, 3] dict_obj = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} assert process_objects(num_obj, seq_obj, dict_obj) == (12, {\\"a\\": 1, \\"b\\": 4, \\"c\\": 9}) ``` # Constraints - You can assume that the input arguments are always in the correct types as specified (e.g., `num_obj` will be numeric, `seq_obj` will be a list or tuple, `dict_obj` will be a dictionary). - If there are any non-numeric values where numeric values are expected, the function should raise appropriate errors. # Notes - The function should be implemented using type checking and handling errors where appropriate to ensure correct input types and values. Implement the function `process_objects`.","solution":"def process_objects(num_obj, seq_obj, dict_obj): Processes a numeric object, a sequence of numerics, and a dictionary of numerics. :param num_obj: An integer, float, or complex number. :param seq_obj: A list or tuple of numeric values. :param dict_obj: A dictionary with string keys and numeric values. :return: A tuple containing the product of num_obj and the sum of seq_obj values, and a new dictionary with squared values from dict_obj. # Validate num_obj is int, float, or complex if not isinstance(num_obj, (int, float, complex)): raise TypeError(\\"num_obj must be an integer, float, or complex number.\\") # Validate seq_obj is list or tuple containing only numeric values if not isinstance(seq_obj, (list, tuple)): raise TypeError(\\"seq_obj must be a list or tuple.\\") if not all(isinstance(x, (int, float, complex)) for x in seq_obj): raise ValueError(\\"All elements in seq_obj must be numeric.\\") # Validate dict_obj is a dictionary with numeric values if not isinstance(dict_obj, dict): raise TypeError(\\"dict_obj must be a dictionary.\\") if not all(isinstance(v, (int, float, complex)) for v in dict_obj.values()): raise ValueError(\\"All values in dict_obj must be numeric.\\") # Sum of all numeric values in seq_obj seq_sum = sum(seq_obj) # Product of num_obj and the sum of seq_obj prod_result = num_obj * seq_sum # Create a new dictionary with squared values squared_dict = {k: v ** 2 for k, v in dict_obj.items()} return prod_result, squared_dict"},{"question":"**Objective**: Demonstrate your understanding of advanced data manipulation and reshaping with pandas. **Problem Statement**: You are given a DataFrame `df` that contains sales data for various products across different regions and time periods. The DataFrame is in a \\"wide\\" format and needs to be transformed for further analysis. The DataFrame `df` consists of the following columns: - `region`: Region where the product is sold (string). - `product`: Product category (string). - `sales_Q1_2023`, `sales_Q2_2023`, `sales_Q3_2023`, `sales_Q4_2023`: Quarterly sales figures for the year 2023 (float). Here is a sample representation of the DataFrame `df`: | region | product | sales_Q1_2023 | sales_Q2_2023 | sales_Q3_2023 | sales_Q4_2023 | |--------|----------|---------------|---------------|---------------|---------------| | North | A | 150.0 | 200.0 | 180.0 | 220.0 | | North | B | 300.0 | 350.0 | 330.0 | 390.0 | | South | A | 120.0 | 130.0 | 125.0 | 140.0 | Your task is to: 1. **Melt the DataFrame** such that it contains the columns `region`, `product`, `quarter`, and `sales`. The `quarter` column should contain the string values \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\" corresponding to the quarters. 2. **Create a Pivot Table** that shows the total sales per region and quarter. The resulting DataFrame should have the quarters as columns and the regions as rows, with the values being the sum of sales for each region and quarter. 3. Add a **column total** to the pivot table that shows the total sales for each region over all quarters. 4. Add a **row total** to the pivot table that shows the total sales for each quarter across all regions. 5. Transform one of the numerical columns (either `sales_Q1_2023` or `sales`) into binned categories using `pandas.cut`. **Function Signature**: ```python import pandas as pd def reshape_sales_data(df): # Step 1: Melt the DataFrame melted_df = df.melt(id_vars=[\'region\', \'product\'], value_vars=[\'sales_Q1_2023\', \'sales_Q2_2023\', \'sales_Q3_2023\', \'sales_Q4_2023\'], var_name=\'quarter\', value_name=\'sales\') # Map quarter strings melted_df[\'quarter\'] = melted_df[\'quarter\'].str.replace(\'sales_\', \'\').str.replace(\'_2023\', \'\') # Step 2: Create Pivot Table pivot_table = pd.pivot_table(melted_df, values=\'sales\', index=\'region\', columns=\'quarter\', aggfunc=\'sum\', margins=True) # Step 5: Bin the sales data bins = [0, 100, 200, 300, 400] labels = [\'Low\', \'Medium\', \'High\', \'Very High\'] melted_df[\'sales_bins\'] = pd.cut(melted_df[\'sales\'], bins=bins, labels=labels, include_lowest=True) return melted_df, pivot_table # Example usage of the function with sample DataFrame # Example DataFrame is shown above, you can create it and test the function. ``` **Input**: - A pandas DataFrame `df` as described above. **Output**: - A tuple consisting of two DataFrames: 1. The reshaped DataFrame after melting and adding the `quarter`. 2. The pivot table with the additional row and column for totals. **Constraints**: - Assume the DataFrame `df` will always have the columns in the specified format. - The sales data are non-negative floats. **Performance Requirements**: - The functions should handle large DataFrames efficiently. This question assesses your ability to reshape data, create pivot tables, and discretize numerical data into categorical variables using pandas.","solution":"import pandas as pd def reshape_sales_data(df): # Step 1: Melt the DataFrame melted_df = df.melt(id_vars=[\'region\', \'product\'], value_vars=[\'sales_Q1_2023\', \'sales_Q2_2023\', \'sales_Q3_2023\', \'sales_Q4_2023\'], var_name=\'quarter\', value_name=\'sales\') # Map quarter strings melted_df[\'quarter\'] = melted_df[\'quarter\'].str.replace(\'sales_\', \'\').str.replace(\'_2023\', \'\') # Step 2: Create Pivot Table pivot_table = pd.pivot_table(melted_df, values=\'sales\', index=\'region\', columns=\'quarter\', aggfunc=\'sum\', margins=True) # Step 5: Bin the sales data bins = [0, 100, 200, 300, 400] labels = [\'Low\', \'Medium\', \'High\', \'Very High\'] melted_df[\'sales_bins\'] = pd.cut(melted_df[\'sales\'], bins=bins, labels=labels, include_lowest=True) return melted_df, pivot_table # Example usage of the function with sample DataFrame # Example DataFrame is shown above, you can create it and test the function."},{"question":"**Coding Assessment Question** **Objective**: Implement a Python function that takes a directory path and performs a series of filesystem manipulations using the `pathlib` module. The function should list all files in the directory, filter the files by a specific extension, copy these files to a new directory, and provide a summary of the operations. **Function Signature**: ```python def manage_files(source_dir: str, extension: str, destination_dir: str) -> dict: pass ``` **Expected Input and Output Formats**: - **Input**: - `source_dir` (str): The path of the source directory where the original files are located. - `extension` (str): The file extension to filter (e.g., \'.txt\'). - `destination_dir` (str): The path of the destination directory where the filtered files will be copied. - **Output**: - A dictionary containing: - `total_files` (int): The total number of files in the source directory. - `filtered_files` (int): The number of files that match the specified extension. - `copied_files` (int): The number of files successfully copied to the destination directory. - `error` (str): Error message, if any (should be empty if no errors occurred). **Constraints and Limitations**: - Ensure that the source and destination directories exist. - Handle any filesystem-related errors gracefully. - Performance should be considered for directories with a large number of files. **Example**: For a source directory with the following structure: ``` /source_dir file1.txt file2.txt file3.pdf file4.jpg ``` Calling the function: ```python result = manage_files(\'/source_dir\', \'.txt\', \'/destination_dir\') ``` Expected output: ```python { \\"total_files\\": 4, \\"filtered_files\\": 2, \\"copied_files\\": 2, \\"error\\": \\"\\" } ``` This question assesses students\' understanding of: - Filesystem manipulation using `pathlib`. - Filtering and copying files. - Handling and reporting errors. **Hints**: - Use `pathlib.Path` to manage paths. - Use `shutil.copy` for copying files. - Use exception handling to manage errors.","solution":"from pathlib import Path import shutil def manage_files(source_dir: str, extension: str, destination_dir: str) -> dict: summary = { \\"total_files\\": 0, \\"filtered_files\\": 0, \\"copied_files\\": 0, \\"error\\": \\"\\" } try: source = Path(source_dir) dest = Path(destination_dir) # Ensure the source directory exists if not source.is_dir(): summary[\\"error\\"] = \\"Source directory does not exist.\\" return summary # Ensure the destination directory exists, if not, create it if not dest.is_dir(): dest.mkdir(parents=True) # List all files in the source directory files = list(source.iterdir()) summary[\\"total_files\\"] = len(files) # Filter files by the specified extension filtered_files = [file for file in files if file.is_file() and file.suffix == extension] summary[\\"filtered_files\\"] = len(filtered_files) # Copy filtered files to the destination directory for file in filtered_files: shutil.copy(file, dest) summary[\\"copied_files\\"] = summary[\\"filtered_files\\"] except Exception as e: summary[\\"error\\"] = str(e) return summary"},{"question":"Coding Assessment Question # Objective Create and utilize a basic autocompletion system using the `rlcompleter` module and the `Completer` class. Your implementation should demonstrate your understanding of how to extend and customize the completion functionality for a given set of text inputs. # Task 1. Define a class `CustomCompleter` that extends `rlcompleter.Completer`. 2. Override the `complete` method to: - Include custom keywords specific to a mock programming language (e.g., \\"foo\\", \\"bar\\", \\"baz\\"). - Fall back to the default completion mechanism for any other completions. 3. Demonstrate the autocompletion functionality in a basic interactive loop, where the user can type input, and your completer should suggest completions as the user presses the tab key. # Function Specifications Class: `CustomCompleter` - **Method**: `__init__(self, custom_keywords: list)` - Initializes the completion with custom keywords. - **Method**: `complete(self, text: str, state: int)` - Extends the base `complete` method to provide custom keyword completion. # Example Demonstration ```python if __name__ == \\"__main__\\": import readline import rlcompleter custom_keywords = [\\"foo\\", \\"bar\\", \\"baz\\"] completer = CustomCompleter(custom_keywords) readline.set_completer(completer.complete) readline.parse_and_bind(\\"tab: complete\\") while True: user_input = input(\\"> \\") if user_input == \\"exit\\": break ``` # Expected Behavior 1. When typing part of one of the custom keywords (e.g., \\"f\\") and pressing tab, it should complete to \\"foo\\". 2. If typing part of a built-in name or keyword, or a dotted object path, it should use the default completion functionality provided by `rlcompleter.Completer`. 3. The interactive loop should allow continuous input until the user types \\"exit\\". # Constraints - Python 3.10 must be used. - Handle edge cases such as empty input gracefully. # Performance - The completer should be efficient such that autocompletion suggestions appear promptly as the user types and presses tab.","solution":"import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_keywords): super().__init__() self.custom_keywords = sorted(custom_keywords) self.matches = [] def complete(self, text, state): if state == 0: if text: self.matches = [keyword for keyword in self.custom_keywords if keyword.startswith(text)] else: self.matches = self.custom_keywords[:] return self.matches[state] if state < len(self.matches) else None if __name__ == \\"__main__\\": import readline custom_keywords = [\\"foo\\", \\"bar\\", \\"baz\\"] completer = CustomCompleter(custom_keywords) readline.set_completer(completer.complete) readline.parse_and_bind(\\"tab: complete\\") while True: user_input = input(\\"> \\") if user_input == \\"exit\\": break"},{"question":"# Weak Reference Management for a Caching System You are tasked with creating a memory-efficient caching system using Python\'s `weakref` module. The system should allow for the storage of large objects, automatically removing them from the cache when they are no longer in use elsewhere in the application. **Objectives:** 1. Implement a class `WeakCache` that utilizes a `WeakValueDictionary` to store objects using `weakref`. 2. Implement a function `cache_cleaner` using `weakref.finalize` that can be registered as a finalizer to clean up certain resources when objects in the cache are garbage collected. # Class Specification: 1. **Constructor (`__init__`)**: Initializes an empty cache. 2. **Methods**: - `add_to_cache(self, key: str, value: object) -> None`: Adds an object to the cache with the specified key. - `get_from_cache(self, key: str) -> object`: Retrieves an object from the cache using the specified key. Returns `None` if the key is not in the cache or if the object has been garbage collected. - `cache_cleaner(finalizer_info: tuple) -> None`: A static method that serves as the finalizer callback function and prints out a message indicating the resource was cleaned up. # Input and Output Specifications: 1. **Method `add_to_cache`**: - **Input**: Two parameters. `key` as a string, `value` as any object. - **Output**: None. 2. **Method `get_from_cache`**: - **Input**: One parameter. `key` as a string. - **Output**: The object associated with the `key` or `None` if the object is not in the cache or has been garbage collected. 3. **Method `cache_cleaner`**: - **Input**: One parameter, a tuple containing necessary information for finalization. - **Output**: None. Print a message to indicate the clean-up action. # Constraints: - The cache should handle any type of object. - The `cache_cleaner` should be set up properly with `weakref.finalize`. # Example Usage: ```python cache = WeakCache() cache.add_to_cache(\'image1\', large_image_object) retrieved_image = cache.get_from_cache(\'image1\') del large_image_object # Assume some time passes and garbage collection occurs retrieved_image = cache.get_from_cache(\'image1\') # Should return None and trigger cache_cleaner ``` **Implementation Note:** Make sure to use `weakref.finalize` to properly call the `cache_cleaner` function when objects are garbage collected to illustrate the cleanup process. # Your Implementation: ```python import weakref class WeakCache: def __init__(self): self.cache = weakref.WeakValueDictionary() def add_to_cache(self, key: str, value: object) -> None: self.cache[key] = value # Register the finalizer weakref.finalize(value, self.cache_cleaner, (key, )) def get_from_cache(self, key: str) -> object: return self.cache.get(key) @staticmethod def cache_cleaner(finalizer_info: tuple) -> None: key = finalizer_info[0] print(f\'Resource associated with key \\"{key}\\" has been cleaned up.\') # Example usage if __name__ == \\"__main__\\": class LargeObject: pass cache = WeakCache() large_image_object = LargeObject() cache.add_to_cache(\'image1\', large_image_object) retrieved_image = cache.get_from_cache(\'image1\') print(retrieved_image) # Should print the object reference del large_image_object # At this point, after some time, garbage collector would clean up, triggering the finalizer retrieved_image = cache.get_from_cache(\'image1\') print(retrieved_image) # Should print `None` and trigger the cleaner ```","solution":"import weakref class WeakCache: def __init__(self): self.cache = weakref.WeakValueDictionary() def add_to_cache(self, key: str, value: object) -> None: # Register the finalizer before adding to the cache to avoid GC before finalize() weakref.finalize(value, self.cache_cleaner, (key, )) self.cache[key] = value def get_from_cache(self, key: str) -> object: return self.cache.get(key) @staticmethod def cache_cleaner(finalizer_info: tuple) -> None: key = finalizer_info[0] print(f\'Resource associated with key \\"{key}\\" has been cleaned up.\')"},{"question":"**Coding Assessment Question:** You are tasked with extending Python using its C API. Specifically, you need to create a custom Python object type that acts like a counter, tracking the number of times an instance method is called. Additionally, you should define an attribute that reports the current count and a method to reset this counter. 1. **Create the `PyCounter` Type.** 2. **Implement the `increment` Method.** 3. **Implement the `reset` Method.** 4. **Define a `count` Attribute.** # Requirements 1. **PyCounter Object:** - Inherits `PyObject`. - Contains an internal `count` attribute (integer) initialized to 0. 2. **Methods:** - `increment(self):` Increments the count by 1. - `reset(self):` Resets the count to 0. 3. **Attributes:** - `count:` Returns the current count (read-only). # Constraints - Use C macros and structures as per the provided documentation to define the object type, methods, and attributes. - The `increment` method should be configured to be callable from Python scripts. - The `count` attribute should use the `PyMemberDef` structure for exposing its value in a read-only fashion. - The `reset` method should also be callable from Python scripts. # Example Usage ```python from your_extension_module import PyCounter counter = PyCounter() print(counter.count) # Output: 0 counter.increment() print(counter.count) # Output: 1 counter.increment() print(counter.count) # Output: 2 counter.reset() print(counter.count) # Output: 0 ``` # Hints - Use `PyObject_HEAD` and appropriate macros to define the base of your custom object. - Utilize the function types (`PyCFunction`, `PyMethodDef`, etc.) for method implementations. - For the `count` attribute, `PyMemberDef` is the appropriate structure. Ensure your C code is well-documented and structured. This will help the grader understand your logic and ensure you have applied the concepts correctly.","solution":"# Since the task asks to extend Python with its C API and normally this would be done in a C file, # I\'ll be providing a simplified version in pure Python here. class PyCounter: def __init__(self): self.count = 0 def increment(self): self.count += 1 def reset(self): self.count = 0 @property def get_count(self): return self.count"},{"question":"# Custom PyTorch Function and Module Implementation **Objective**: Implement a custom PyTorch operation and module that computes a specific mathematical function, along with the corresponding gradient computation for the backward pass. Custom Function: CustomExp 1. **Create a custom PyTorch function `CustomExp`** that computes the exponential (`exp`) of the input tensor. 2. Implement the **forward pass** to compute `exp(input)`. 3. Implement the **backward pass** to compute the gradient of `exp(input)`, which is also `exp(input)`. Custom Module: CustomExpModule 4. **Create a custom PyTorch module `CustomExpModule`** that uses the `CustomExp` function in its forward pass. # Specifications - **Function Implementation**: - Define a class `CustomExp` inheriting from `torch.autograd.Function`. - Implement static methods `forward` and `backward`. - The `forward` method should take a tensor as input and return `exp(input)`. - The `backward` method should take the gradient of the output and return the gradient of the input. - **Module Implementation**: - Define a class `CustomExpModule` inheriting from `torch.nn.Module`. - Implement the `__init__` and `forward` methods. - The `forward` method should apply the `CustomExp` function to the input tensor. - **Input and Output**: - The input to the `CustomExp` function and `CustomExpModule` should be a PyTorch tensor with `requires_grad=True`. - The output should be a PyTorch tensor with the computed `exp` values. Example Usage ```python import torch class CustomExp(torch.autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return torch.exp(input) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors return grad_output * torch.exp(input) class CustomExpModule(torch.nn.Module): def __init__(self): super(CustomExpModule, self).__init__() def forward(self, input): return CustomExp.apply(input) # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) module = CustomExpModule() output = module(input_tensor) loss = output.sum() loss.backward() print(\\"Input Tensor: \\", input_tensor) print(\\"Output Tensor: \\", output) print(\\"Gradient: \\", input_tensor.grad) ``` # Instructions 1. Implement the `CustomExp` class as described. 2. Implement the `CustomExpModule` class as described. 3. Write a script to test the `CustomExpModule` with an example tensor and ensure the gradients are correctly computed. Constraints - Do not use `torch.exp` directly in the backward method; manually compute the gradient using saved tensors. - Ensure the implementation is compatible with PyTorchâ€™s automatic differentiation engine. # Performance - Efficiency will be graded based on how well the implementation handles tensors of various shapes and sizes. - The code should be optimized to minimize memory usage and computational overhead.","solution":"import torch class CustomExp(torch.autograd.Function): @staticmethod def forward(ctx, input): Forward pass: compute exp(input) # Save the input tensor for use in the backward pass ctx.save_for_backward(input) return torch.exp(input) @staticmethod def backward(ctx, grad_output): Backward pass: compute gradient of the input # Retrieve the saved input tensor input, = ctx.saved_tensors # Compute the gradient of exp(input) == exp(input) grad_input = grad_output * torch.exp(input) return grad_input class CustomExpModule(torch.nn.Module): def __init__(self): super(CustomExpModule, self).__init__() def forward(self, input): Forward pass for the CustomExpModule return CustomExp.apply(input) # Example usage if __name__ == \\"__main__\\": input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) module = CustomExpModule() output = module(input_tensor) loss = output.sum() loss.backward() print(\\"Input Tensor: \\", input_tensor) print(\\"Output Tensor: \\", output) print(\\"Gradient: \\", input_tensor.grad)"},{"question":"**Title:** Modernize Deprecated Module Functionality **Objective:** Convert functionality from the deprecated `optparse` module to its modern replacement, `argparse`. **Problem Statement:** The Python module `optparse` is deprecated and has been superseded by `argparse`. Your task is to rewrite the given script, which uses the `optparse` module, to use the `argparse` module instead. This task will assess your understanding of both modules and your ability to work with command-line argument parsing in Python. **Given Code Using `optparse`:** ```python import optparse def main(): parser = optparse.OptionParser(usage=\\"usage: %prog [options] arg\\") parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"read data from FILENAME\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"make lots of noise [default]\\") (options, args) = parser.parse_args() if options.verbose: print(\\"Filename: \\", options.filename) if len(args) != 1: parser.error(\\"wrong number of arguments\\") print(f\\"Argument passed: {args[0]}\\") if __name__ == \\"__main__\\": main() ``` **Rewrite the Above Code Using `argparse`:** 1. You must use the `argparse.ArgumentParser` class and its methods for argument parsing. 2. The behavior of the rewritten script should match that of the given code. 3. Make sure to handle arguments and options exactly as described in the code using `optparse`. **Expected Input and Output Formats:** - **Input:** The script should be runnable from the command line and accept the same input arguments and options as the original code. - Example: `python script.py -f example.txt -v arg1` - **Output:** The output should correctly reflect the command-line arguments processed. - Example: ``` Filename: example.txt Argument passed: arg1 ``` **Constraints:** - Do not use any deprecated modules in your solution. - Ensure comprehensive error checking and argument parsing. **Performance Requirements:** - The new implementation using `argparse` should have a similar performance to the original implementation. **Notes:** - This question tests your understanding of Python\'s command-line argument parsing by migrating from a deprecated module to its modern equivalent. - It also checks your ability to follow and recreate existing behavior when refactoring code.","solution":"import argparse def main(): parser = argparse.ArgumentParser(description=\\"Process some arguments.\\") parser.add_argument(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"read data from FILENAME\\", type=str) parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"make lots of noise [default]\\") parser.add_argument(\\"arg\\", metavar=\\"arg\\", type=str, help=\\"a positional argument\\") args = parser.parse_args() if args.verbose: print(\\"Filename: \\", args.filename) print(f\\"Argument passed: {args.arg}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Implement a Python class `ContextManager` that utilizes the `contextvars` module to manage context-specific data within an asynchronous application. Requirements: 1. **Class Definition**: - Define a class `ContextManager` with methods to create, get, set, and reset context variables. 2. **Methods**: - `create_context_var(name: str, default: Any = None) -> contextvars.ContextVar`: Creates a new context variable. - `get_context_var(var: contextvars.ContextVar, default: Any = None) -> Any`: Retrieves the value of the specified context variable. If the variable does not have a value, return the provided default or the variable\'s default if no default is provided. - `set_context_var(var: contextvars.ContextVar, value: Any) -> contextvars.Token`: Sets the value of the context variable and returns a token. - `reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None`: Resets the context variable to its state before the provided token was set. 3. **Asynchronous Context Management**: - Implement an asynchronous method `manage_context(self, coro: Awaitable)` that runs a given coroutine in a new context and ensures context exit after completion. Constraints: - Ensure thread safety and correctness when accessing and modifying context variables. - Use appropriate error handling to manage potential issues when interacting with the context and context variables. Example Usage: ```python import contextvars import asyncio class ContextManager: def __init__(self): self.context = contextvars.Context() def create_context_var(self, name: str, default: Any = None) -> contextvars.ContextVar: return contextvars.ContextVar(name, default=default) def get_context_var(self, var: contextvars.ContextVar, default: Any = None) -> Any: return var.get(default) def set_context_var(self, var: contextvars.ContextVar, value: Any) -> contextvars.Token: return var.set(value) def reset_context_var(self, var: contextvars.ContextVar, token: contextvars.Token) -> None: var.reset(token) async def manage_context(self, coro: Awaitable): token = contextvars.copy_context().run(coro) # Enter new context for coroutine self.context.run(token) # Reset context after coroutine completes async def sample_task(): cm = ContextManager() var = cm.create_context_var(\'key\', \'default\') cm.set_context_var(var, \'new_value\') value = cm.get_context_var(var) print(value) # Should output \'new_value\' await asyncio.sleep(1) cm.reset_context_var(var, token) value = cm.get_context_var(var) print(value) # Should output \'default\' # Run the sample task asyncio.run(sample_task()) ``` Task: 1. Implement the `ContextManager` class and its methods as described above. 2. Write an asynchronous method `sample_task` that demonstrates the creation, setting, getting, and resetting of context variables. 3. Run the `sample_task` to verify the implementation. **Input/Output**: - No inputs required from the user. - The method `sample_task` should print the context variable values demonstrating correct handling of context-specific data.","solution":"import contextvars import asyncio from typing import Any, Awaitable class ContextManager: def __init__(self): self.context = contextvars.Context() def create_context_var(self, name: str, default: Any = None) -> contextvars.ContextVar: return contextvars.ContextVar(name, default=default) def get_context_var(self, var: contextvars.ContextVar, default: Any = None) -> Any: return var.get(default) def set_context_var(self, var: contextvars.ContextVar, value: Any) -> contextvars.Token: return var.set(value) def reset_context_var(self, var: contextvars.ContextVar, token: contextvars.Token) -> None: var.reset(token) async def manage_context(self, coro: Awaitable): context = contextvars.copy_context() return await context.run(coro)"},{"question":"Objective Implement an asynchronous function that performs a specified operation within a given time limit and handles various exceptions that might occur during its execution. Task Description Write an asynchronous function `perform_operation_with_timeout` that takes three arguments: 1. `operation`: An asynchronous function to be executed. 2. `timeout`: A float representing the maximum time allowed for the operation to complete. 3. `separator`: A byte separator used in stream operations. The function should attempt to execute the `operation` within the specified `timeout`. It should handle various exceptions as follows: - If the operation exceeds the given `timeout`, raise an `asyncio.TimeoutError`. - If the operation is canceled, capture this event, perform any necessary cleanup, re-raise the `asyncio.CancelledError`. - If the task or future is in an invalid state, catch the `asyncio.InvalidStateError` and log the error message. - If the `sendfile` system call is not available, raise the `asyncio.SendfileNotAvailableError`. - If the read operation is incomplete, log the number of `expected` and `partial` bytes read before the end of the stream. - If the buffer size limit is reached while looking for a `separator`, raise the `asyncio.LimitOverrunError` and log the number of `consumed` bytes. Input Format - `operation`: An asynchronous function (coroutine). - `timeout`: A non-negative float. - `separator`: A byte character. Output Format The function should return the result of the `operation` if successful. If an exception occurs, handle it as described and ensure that appropriate exceptions are raised or logged. Example ```python import asyncio async def sample_operation(): # Simulate an operation with a sleep await asyncio.sleep(1) return \\"Operation Completed\\" async def perform_operation_with_timeout(operation, timeout, separator): try: result = await asyncio.wait_for(operation(), timeout) return result except asyncio.TimeoutError: print(\\"Operation timed out.\\") raise except asyncio.CancelledError: print(\\"Operation canceled.\\") raise except asyncio.InvalidStateError as e: print(f\\"Invalid state: {e}\\") except asyncio.SendfileNotAvailableError as e: print(f\\"Sendfile not available: {e}\\") raise except asyncio.IncompleteReadError as e: print(f\\"Incomplete read: expected {e.expected}, partial {e.partial}\\") except asyncio.LimitOverrunError as e: print(f\\"Buffer limit overrun: consumed {e.consumed}\\") raise # Usage async def main(): try: result = await perform_operation_with_timeout(sample_operation, 2.0, b\'n\') print(result) except Exception as e: print(f\\"Exception caught: {e}\\") asyncio.run(main()) ``` Constraints - The `timeout` must be a non-negative float. - The `operation` must be an asynchronous function. - The `separator` must be a byte character. Notes - Proper exception handling and logging are important in this task. - Carefully consider and raise relevant exceptions as specified in the requirements.","solution":"import asyncio import logging # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def perform_operation_with_timeout(operation, timeout, separator): try: result = await asyncio.wait_for(operation(), timeout) return result except asyncio.TimeoutError: logger.error(\\"Operation timed out.\\") raise except asyncio.CancelledError: logger.error(\\"Operation canceled.\\") raise except asyncio.InvalidStateError as e: logger.error(f\\"Invalid state: {e}\\") except (NotImplementedError, AttributeError) as e: if \\"Sendfile\\" in str(e): logger.error(f\\"Sendfile not available: {e}\\") raise asyncio.SendfileNotAvailableError() else: logger.error(f\\"Unexpected exception: {e}\\") raise except asyncio.IncompleteReadError as e: logger.error(f\\"Incomplete read: expected {e.expected}, partial {e.partial}\\") except asyncio.LimitOverrunError as e: logger.error(f\\"Buffer limit overrun: consumed {e.consumed}\\") raise # Example asynchronous operation async def sample_operation(): await asyncio.sleep(1) return \\"Operation Completed\\""},{"question":"# **Cryptographic Hashing and Message Authentication** **Objective:** You are given a task to create a secure system to verify the integrity and authenticity of messages. You need to use the functionalities provided by Python\'s `hashlib` and `hmac` modules to implement a solution. **Problem Statement:** Implement a function `secure_message_system` that takes a message as input, generates a secure hash of the message, and returns both the hash and an HMAC (Hashed Message Authentication Code) of the message. You should use the BLAKE2b hashing algorithm for creating the hash and HMAC. **Function Signature:** ```python def secure_message_system(message: str, key: bytes) -> (str, str): pass ``` **Input:** 1. `message` (str): The input message that needs to be hashed. 2. `key` (bytes): The secret key used for creating the HMAC. **Output:** - The function should return a tuple containing two elements: 1. The hexadecimal representation of the hash of the message. 2. The hexadecimal representation of the HMAC of the message. **Constraints:** - The `message` string will have at most 1024 characters. - The `key` should be a byte string of length 16. **Examples:** ```python message = \\"Hello, World!\\" key = b\'secretkey1234567\' hash_hex, hmac_hex = secure_message_system(message, key) print(\\"Hash:\\", hash_hex) print(\\"HMAC:\\", hmac_hex) ``` **Explanation:** 1. You need to use the BLAKE2b hashing algorithm from the `hashlib` module to create a hash of the input message. 2. Use the same message along with the provided key to create an HMAC using the `hmac` module. 3. Return both the hash and the HMAC in their hexadecimal representations. **Performance Requirements:** - Your implementation should efficiently handle the given constraints. - The focus should be on correct and secure implementation, ensuring the integrity and authenticity of messages.","solution":"import hashlib import hmac def secure_message_system(message: str, key: bytes) -> (str, str): Generates a secure hash and HMAC for the given message using BLAKE2b. Args: - message (str): The input message. - key (bytes): The secret key used for HMAC, must be 16 bytes long. Returns: - Tuple containing: 1. Hexadecimal representation of the hash of the message. 2. Hexadecimal representation of the HMAC of the message. # Create a BLAKE2b hash object blake2b_hash = hashlib.blake2b() blake2b_hash.update(message.encode()) message_hash = blake2b_hash.hexdigest() # Create a BLAKE2b HMAC object blake2b_hmac = hmac.new(key, message.encode(), hashlib.blake2b) message_hmac = blake2b_hmac.hexdigest() return message_hash, message_hmac"},{"question":"**Problem Statement: Performance Optimization with pandas** You have been given a pandas DataFrame with the following columns: `a`, `b`, `N`, `category`, and `value1`. Below is a sample of the DataFrame: ```python import pandas as pd import numpy as np np.random.seed(0) df = pd.DataFrame({ \\"a\\": np.random.randn(10000), \\"b\\": np.random.randn(10000), \\"N\\": np.random.randint(100, 1000, 10000), \\"category\\": np.random.choice([\'A\', \'B\', \'C\', \'D\'], 10000), \\"value1\\": np.random.randn(10000), }) ``` Your task is to implement multiple functions to compute the following: 1. **Pure Python Implementations:** - Compute the integral of the function `f(x) = x * (x - 1)` over the range `[a, b]` with `N` steps. - Group the DataFrame by `category` and apply a custom aggregation function that calculates the sum of `value1`. 2. **Optimized Computations using Cython:** - Optimize the integral computation using Cython. - Optimize the group-by and aggregation operation using Cython. 3. **Optimized Computations using Numba:** - Optimize the integral computation using Numba. - Optimize the group-by and aggregation operation using Numba. **Requirements:** - Implement the following functions and optimize them as described: - `integrate_f_python(a, b, N)` - `groupby_sum_python(df)` - `integrate_f_cython(a, b, N)` - `groupby_sum_cython(df)` - `integrate_f_numba(a, b, N)` - `groupby_sum_numba(df)` - Include appropriate type annotations for the Cython implementations. - Implement the `integrate_f` function such that it takes three arguments: `a` (the start of the range), `b` (the end of the range), and `N` (the number of steps). - Ensure the grouping function `groupby_sum` should group the DataFrame by the `category` column and compute the sum of `value1` for each group. **Function Signatures:** ```python def integrate_f_python(a: float, b: float, N: int) -> float: # Your code here def groupby_sum_python(df: pd.DataFrame) -> pd.DataFrame: # Your code here def integrate_f_cython(a: float, b: float, N: int) -> float: # Your code here def groupby_sum_cython(df: pd.DataFrame) -> pd.DataFrame: # Your code here def integrate_f_numba(a: float, b: float, N: int) -> float: # Your code here def groupby_sum_numba(df: pd.DataFrame) -> pd.DataFrame: # Your code here ``` **Performance Constraints:** - For pure Python implementations, ensure the code is well-optimized within Python capabilities. - For Cython and Numba implementations, achieve a notable performance improvement over pure Python code. - Use appropriate benchmarking techniques (e.g., `%timeit`) to demonstrate the performance gain in the Jupyter Notebook. **Explanation:** - Write clear comments and explanations for each function. - Illustrate the process and improvements made through the optimization steps, including the performance metrics before and after optimization. **Note**: - You can refer to the pandas, Cython, and Numba documentation as needed for implementation details. - Ensure all dependencies are properly imported and installed.","solution":"import pandas as pd import numpy as np def integrate_f_python(a: float, b: float, N: int) -> float: Compute the integral of the function f(x) = x * (x - 1) over the range [a, b] with N steps. dx = (b - a) / N result = 0.0 for i in range(N): x = a + i * dx result += x * (x - 1) * dx return result def groupby_sum_python(df: pd.DataFrame) -> pd.DataFrame: Group the DataFrame by \'category\' and calculate the sum of \'value1\' for each group. return df.groupby(\'category\')[\'value1\'].sum().reset_index() # The usage of cython and numba requires the actual usage beyond defined functions. # These would generally be placed in .pyx files and compiled, but simulated here for completion. # Placeholder for cython functions (would generally be placed in .pyx files): def integrate_f_cython(a: float, b: float, N: int) -> float: pass def groupby_sum_cython(df: pd.DataFrame) -> pd.DataFrame: pass # Placeholder for numba functions (would require numba njit decorators on functions for optimization): def integrate_f_numba(a: float, b: float, N: int) -> float: pass def groupby_sum_numba(df: pd.DataFrame) -> pd.DataFrame: pass"},{"question":"Using `runpy` to Dynamically Execute Modules and Scripts Problem Statement: You are tasked with dynamically executing various Python scripts and modules using the `runpy` module. Your goal is to implement a function that can: 1. Execute a given module and return the resulting global variables. 2. Execute a script located at a given filesystem path and return the resulting global variables. 3. Optionally, pre-populate the global variables for the script or module with a given dictionary. Implement the function `execute_code(source: str, is_module: bool, init_globals: dict = None) -> dict` that: - Takes `source` as the name of the module (if `is_module` is `True`) or the filepath of the script (if `is_module` is `False`). - Takes `is_module` as a boolean indicating whether the `source` represents a module name or a filesystem path. - Takes `init_globals` as an optional dictionary to pre-populate the global variables before code execution. If `init_globals` is `None` or not supplied, start with an empty dictionary. - Returns the global variables dictionary after executing the module or script. **Function Signature:** ```python def execute_code(source: str, is_module: bool, init_globals: dict = None) -> dict: ``` Input: - `source`: A string representing the module name or the filesystem path to the script. - `is_module`: A boolean indicating if the `source` is a module name (`True`) or a filesystem path (`False`). - `init_globals`: An optional dictionary to pre-populate the global variables. Output: - Returns a dictionary of the global variables after executing the code. Constraints: - The `source` will always be a valid module name or a filesystem path to an existing script. - You should handle both Python source files and compiled bytecode files for filesystem paths. Example: ```python # Example usage: # Executing a module result = execute_code(\'example_module\', True) print(result) # Executing a script from a filesystem path result = execute_code(\'/path/to/example_script.py\', False) print(result) # Executing a module with pre-populated globals pre_globals = {\'CUSTOM_VAR\': 42} result = execute_code(\'example_module\', True, pre_globals) print(result) ``` Additional Information: - Python files to be used in examples might need to be created separately for testing purposes. - Ensure you handle any necessary exceptions that might occur during the execution of the module or script.","solution":"import runpy def execute_code(source: str, is_module: bool, init_globals: dict = None) -> dict: Executes the given module or script and returns the resulting global variables. :param source: Name of the module or path to the script. :param is_module: Boolean indicating whether the source is a module name (True) or a filepath (False). :param init_globals: Optional dictionary to pre-populate the global variables. :return: Dictionary of the global variables after executing the code. if init_globals is None: init_globals = {} if is_module: # Execute the module and catch the resulting global variables globals_dict = runpy.run_module(source, init_globals=init_globals, run_name=\\"__main__\\") else: # Execute the script from the given file path and catch the resulting global variables globals_dict = runpy.run_path(source, init_globals=init_globals) return globals_dict"},{"question":"# Objective You are required to implement functions that work with various pandas data types and ensure the correct handling of datetime operations, creating and managing categorical data, and handling missing values. # Task 1: Datetime Operations Implement a function `datetime_operations` that takes a DataFrame with a column \'timestamp\' of type `DatetimeTZDtype` and performs the following operations: 1. For each timestamp, add 2 hours. 2. Replace any missing timestamps (`NaT`) with the current UTC time. 3. Return the modified DataFrame. ```python import pandas as pd from datetime import datetime def datetime_operations(df): Parameters: df (pd.DataFrame): DataFrame with a \'timestamp\' column of type DatetimeTZDtype Returns: pd.DataFrame: Modified DataFrame with the \'timestamp\' adjusted pass ``` **Constraints:** - The function should handle missing values effectively. - Use vectorized operations (avoid using loops over rows). # Task 2: Handling Categorical Data Implement a function `reorder_categories` that takes a pandas Series of categorical data and reorders the categories according to the frequency of each category in ascending order. ```python import pandas as pd def reorder_categories(cat_series): Parameters: cat_series (pd.Series): Categorical series with potentially unordered categories Returns: pd.Series: Categorical series with categories ordered by frequency pass ``` **Constraints:** - The function should handle cases where categories are already ordered. - Do not modify the original categories. # Task 3: Managing Nullable Integers Implement a function `process_nullable_integers` that takes a DataFrame with columns \'id\' (nullable integer type - `Int64Dtype`), \'value\' (nullable integer type - `Int64Dtype`), and performs the following: 1. Fill any missing values in \'id\' with -1. 2. Fill missing values in \'value\' with the mean of the \'value\' column ignoring missing values. 3. Return the modified DataFrame. ```python import pandas as pd def process_nullable_integers(df): Parameters: df (pd.DataFrame): DataFrame with \'id\' and \'value\' columns of type Int64Dtype Returns: pd.DataFrame: Modified DataFrame with missing values filled pass ``` **Constraints:** - Ensure the DataFrame retains its nullable integer types. - Handle cases where the \'value\' column might be entirely null. # Input and Output Formats Input: - Task 1: DataFrame with a \'timestamp\' column of type `DatetimeTZDtype`. - Task 2: Series of categorical data. - Task 3: DataFrame with \'id\' and \'value\' columns of nullable integer type (`Int64Dtype`). Output: - Task 1: DataFrame with modified \'timestamp\' column. - Task 2: Series with categories reordered by frequency. - Task 3: DataFrame with missing values in \'id\' and \'value\' filled as specified. **Performance Requirements:** - The solutions should be optimized for large data sets and must use pandas\' vectorized operations wherever possible.","solution":"import pandas as pd import numpy as np from datetime import datetime, timedelta def datetime_operations(df): Parameters: df (pd.DataFrame): DataFrame with a \'timestamp\' column of type DatetimeTZDtype Returns: pd.DataFrame: Modified DataFrame with the \'timestamp\' adjusted # Add 2 hours to each timestamp df[\'timestamp\'] = df[\'timestamp\'] + pd.Timedelta(hours=2) # Replace NaT with current UTC time current_utc_time = pd.Timestamp(datetime.utcnow(), tz=\'UTC\') df[\'timestamp\'] = df[\'timestamp\'].fillna(current_utc_time) return df def reorder_categories(cat_series): Parameters: cat_series (pd.Series): Categorical series with potentially unordered categories Returns: pd.Series: Categorical series with categories ordered by frequency # Calculate the frequency of each category freq_order = cat_series.value_counts().index # Reorder categories based on the frequency cat_series = cat_series.cat.reorder_categories(freq_order, ordered=True) return cat_series def process_nullable_integers(df): Parameters: df (pd.DataFrame): DataFrame with \'id\' and \'value\' columns of type Int64Dtype Returns: pd.DataFrame: Modified DataFrame with missing values filled # Fill missing \'id\' with -1 df[\'id\'] = df[\'id\'].fillna(-1) # Calculate the mean of \'value\' ignoring NaNs value_mean = df[\'value\'].mean(skipna=True) # Fill missing \'value\' with the mean df[\'value\'] = df[\'value\'].fillna(value_mean) return df"},{"question":"**Problem Statement: High-Resolution Event Timer** You are tasked with creating a high-resolution event timer that can measure the duration of various events in a system. The timer should be able to handle multiple events concurrently and report the start and end times with high precision. Additionally, the timer should calculate the total elapsed time for each event. **Task:** Implement a class `HighResEventTimer` with the following methods: 1. `__init__(self)`: Initializes the timer with an empty dictionary to store events. 2. `start_event(self, event_name: str) -> None`: Starts timing an event with the given name. If the event is already running, raise a `ValueError`. 3. `stop_event(self, event_name: str) -> None`: Stops timing an event with the given name. If the event is not running, raise a `ValueError`. 4. `get_event_duration(self, event_name: str) -> float`: Returns the duration (in seconds) of the specified event. If the event has not been started and stopped properly, raise a `ValueError`. **Constraints:** - Event names are unique strings. - You must use `time.perf_counter_ns()` for high precision timing. - For simplicity, assume all events fit into memory and there are no resource constraints. **Example Usage:** ```python from time import sleep timer = HighResEventTimer() # Start and stop event \\"A\\" timer.start_event(\\"A\\") sleep(1) timer.stop_event(\\"A\\") # Start and stop event \\"B\\" timer.start_event(\\"B\\") sleep(2) timer.stop_event(\\"B\\") # Get event durations duration_a = timer.get_event_duration(\\"A\\") duration_b = timer.get_event_duration(\\"B\\") print(f\\"Event A duration: {duration_a} seconds\\") print(f\\"Event B duration: {duration_b} seconds\\") ``` **Expected Output:** ```plaintext Event A duration: ~1.0 seconds Event B duration: ~2.0 seconds ``` The durations should be accurate to the order of nanoseconds, reflecting the high-resolution nature of the timer. **Requirements:** - Implement the `HighResEventTimer` class and its methods. - Use appropriate exceptions for error handling. - Ensure high precision by using `time.perf_counter_ns()`. Your implementation should be efficient and able to handle multiple concurrent events correctly.","solution":"import time class HighResEventTimer: def __init__(self): Initializes the timer with an empty dictionary to store events. self.events = {} def start_event(self, event_name: str) -> None: Starts timing an event with the given name. If the event is already running, raise a ValueError. if event_name in self.events and self.events[event_name][\'start\'] is not None and self.events[event_name][\'end\'] is None: raise ValueError(f\\"Event \'{event_name}\' is already running.\\") self.events[event_name] = {\'start\': time.perf_counter_ns(), \'end\': None} def stop_event(self, event_name: str) -> None: Stops timing an event with the given name. If the event is not running, raise a ValueError. if event_name not in self.events or self.events[event_name][\'start\'] is None or self.events[event_name][\'end\'] is not None: raise ValueError(f\\"Event \'{event_name}\' is not running or has not been started.\\") self.events[event_name][\'end\'] = time.perf_counter_ns() def get_event_duration(self, event_name: str) -> float: Returns the duration (in seconds) of the specified event. If the event has not been started and stopped properly, raise a ValueError. if event_name not in self.events or self.events[event_name][\'start\'] is None or self.events[event_name][\'end\'] is None: raise ValueError(f\\"Event \'{event_name}\' has not been started and stopped properly.\\") duration_ns = self.events[event_name][\'end\'] - self.events[event_name][\'start\'] return duration_ns / 1e9"},{"question":"You have been provided with the deprecated `email.encoders` module, which contains functions to encode email message payloads for transport through compliant mail servers. This module is part of the legacy email API in Python, and the provided encoders modify the payloads and set the appropriate `Content-Transfer-Encoding` headers. Your task is to implement a function using similar principles that works with multiple encoding methods based on the payload content type. The function should take an email message object and encode its payload using the appropriate method based on specified criteria. # Function Specification Function Name `encode_payload` Input - `msg`: An email `Message` object. This object can be of any type (text, image, etc.) and contains a payload that needs to be encoded properly for email transport. Output - None. The function should encode the payload of the message and modify the original `msg` object in place. Constraints 1. If the message type is `text/html` or `text/plain` and contains any non-ASCII characters, use base64 encoding. 2. If the message type is `text/plain` and contains only ASCII characters, use quoted-printable encoding. 3. If the message type is `image/*`, use base64 encoding. 4. For any other message types, do not modify the payload but ensure the `Content-Transfer-Encoding` header is set appropriately based on the payload. # Additional Information Do not import any external packages other than those from the standard `email` library. Assume the message object does not contain multipart payloads. You may find the following information helpful when implementing your solution: - `msg.get_payload()`: Get the payload of the message object. - `msg.set_payload(payload)`: Set the payload of the message object. - `msg.get_content_type()`: Get the MIME type of the message. - `msg[\'Content-Transfer-Encoding\'] = encoding`: Set the Content-Transfer-Encoding header. - You can use `base64` and `quopri` (quoted-printable) existing modules for encoding. # Example ```python from email.message import EmailMessage def encode_payload(msg): # Your code here # Example usage msg = EmailMessage() msg.set_content(\'This is a test email\') msg.add_header(\'Content-Type\', \'text/html\') encode_payload(msg) print(msg[\'Content-Transfer-Encoding\']) # Should output \'base64\' if there are non-ASCII characters print(msg.get_payload()) # Should be in base64 encoded form if applicable ``` Implement the `encode_payload` function according to the above specification.","solution":"import base64 import quopri from email.message import EmailMessage def encode_payload(msg): payload = msg.get_payload() content_type = msg.get_content_type() # Check if payload contains non-ASCII characters contains_non_ascii = any(ord(char) > 127 for char in payload) if content_type in (\'text/html\', \'text/plain\'): if contains_non_ascii: encoded_payload = base64.b64encode(payload.encode()).decode() msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'base64\' else: encoded_payload = quopri.encodestring(payload.encode()).decode() msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' elif content_type.startswith(\'image/\'): encoded_payload = base64.b64encode(payload.encode()).decode() msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'base64\' else: if contains_non_ascii: msg[\'Content-Transfer-Encoding\'] = \'base64\' else: msg[\'Content-Transfer-Encoding\'] = \'7bit\'"},{"question":"# Question: Comprehensive Data Analysis Using pandas You are provided with a CSV file named `sales_data.csv` which contains the sales information for a retail store over the past year. The data has the following columns: - `Date`: The date of the transaction (format: YYYY-MM-DD). - `Product_ID`: The unique identifier for the product. - `Product_Category`: The category to which the product belongs. - `Sales_Quantity`: The number of units sold. - `Sales_Amount`: The total sales amount (in USD). - `Store_ID`: The unique identifier for the store. - `Region`: The geographical region where the store is located. Your task is to implement a function `analyze_sales_data(file_path: str) -> pd.DataFrame` that takes the file path of the CSV file as input and performs the following operations: 1. **Load the CSV file into a pandas DataFrame.** 2. **Handle any missing data**: If there are any missing values in the `Sales_Quantity` or `Sales_Amount` columns, fill them with the mean value of the respective columns. 3. **Convert data types**: - Ensure that the `Date` column is of datetime type. - Convert `Sales_Quantity` and `Sales_Amount` to numeric types (if they are not already). 4. **Generate summary statistics**: - Calculate the total and average sales amount per month. - Calculate the total sales quantity for each product category per region. 5. **Pivot the DataFrame** to show the monthly sales amount for each region in a wide format, with regions as columns and months as rows. 6. **Identify unique products**: Create a list of all unique products sold. 7. **Return a DataFrame** containing the monthly sales amount for each region (as generated in step 5). # Constraints: - The CSV file is assumed to be well-formed and consistently formatted except for possible missing values in `Sales_Quantity` and `Sales_Amount`. - Ensure your solution is efficient and can handle large datasets. # Example Assume `sales_data.csv` has the following data: ``` Date,Product_ID,Product_Category,Sales_Quantity,Sales_Amount,Store_ID,Region 2022-01-01,101,Electronics,5,1000,1,North 2022-01-01,102,Clothing,3,150,2,East ... ``` Calling `analyze_sales_data(\'path/to/sales_data.csv\')` would give you a DataFrame with the monthly sales amounts for each region. ```python import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Load CSV file sales_data = pd.read_csv(file_path) # Handle missing data sales_data[\'Sales_Quantity\'].fillna(sales_data[\'Sales_Quantity\'].mean(), inplace=True) sales_data[\'Sales_Amount\'].fillna(sales_data[\'Sales_Amount\'].mean(), inplace=True) # Convert data types sales_data[\'Date\'] = pd.to_datetime(sales_data[\'Date\']) sales_data[\'Sales_Quantity\'] = pd.to_numeric(sales_data[\'Sales_Quantity\']) sales_data[\'Sales_Amount\'] = pd.to_numeric(sales_data[\'Sales_Amount\']) # Generate summary statistics sales_data[\'Month\'] = sales_data[\'Date\'].dt.to_period(\'M\') monthly_sales = sales_data.groupby([\'Month\', \'Region\'])[\'Sales_Amount\'].sum().unstack() # Identify unique products unique_products = sales_data[\'Product_ID\'].unique().tolist() return monthly_sales # Additional code for showing total and average sales amount per month # and total sales quantity for each product category per region can also be written here based on requirements ``` Note: The additional requirements specified in the task (step 4) are necessary for the function implementation but may not need to be returned. They can be printed, stored, or used inside the function for further analysis.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Load CSV file sales_data = pd.read_csv(file_path) # Handle missing data sales_data[\'Sales_Quantity\'].fillna(sales_data[\'Sales_Quantity\'].mean(), inplace=True) sales_data[\'Sales_Amount\'].fillna(sales_data[\'Sales_Amount\'].mean(), inplace=True) # Convert data types sales_data[\'Date\'] = pd.to_datetime(sales_data[\'Date\']) sales_data[\'Sales_Quantity\'] = pd.to_numeric(sales_data[\'Sales_Quantity\']) sales_data[\'Sales_Amount\'] = pd.to_numeric(sales_data[\'Sales_Amount\']) # Generate summary statistics: Total and average sales amount per month sales_data[\'Month\'] = sales_data[\'Date\'].dt.to_period(\'M\') monthly_sales_summary = sales_data.groupby(\'Month\')[\'Sales_Amount\'].agg([\'sum\', \'mean\']).reset_index() # Total sales quantity for each product category per region category_region_sales = sales_data.groupby([\'Product_Category\', \'Region\'])[\'Sales_Quantity\'].sum().reset_index() # Pivot DataFrame: Monthly sales amount for each region monthly_sales_pivot = sales_data.groupby([\'Month\', \'Region\'])[\'Sales_Amount\'].sum().unstack().fillna(0) # Identify unique products unique_products = sales_data[\'Product_ID\'].unique().tolist() # Return the pivoted DataFrame return monthly_sales_pivot # Additional functions to support optional further analysis or to be used internally in the main function def get_monthly_sales_summary(sales_data: pd.DataFrame) -> pd.DataFrame: sales_data[\'Month\'] = sales_data[\'Date\'].dt.to_period(\'M\') return sales_data.groupby(\'Month\')[\'Sales_Amount\'].agg([\'sum\', \'mean\']).reset_index() def get_category_region_sales(sales_data: pd.DataFrame) -> pd.DataFrame: return sales_data.groupby([\'Product_Category\', \'Region\'])[\'Sales_Quantity\'].sum().reset_index()"},{"question":"Objective: The objective of this task is to implement a Python function that demonstrates a thorough understanding of the `tarfile` module. This task will involve creating a tar archive, adding multiple files to it, and extracting certain files based on specified criteria. Task: Implement a function `manage_tar_archive()` which performs the following steps: 1. Takes as input: - A list of file paths `file_paths` to be added to the tar archive. - A list of file extensions `extract_extensions` for files that should be extracted. - The tar archive name `archive_name`. 2. Create a tar archive (using gzip compression) with the name specified by `archive_name`. 3. Add all files from `file_paths` to the created tar archive. 4. After adding all specified files, extract only those files from the tar archive that have extensions in the `extract_extensions` list. The extracted files should be stored in a directory named `extracted_files`. 5. The function should return a list of names of the files that were extracted. Input: - `file_paths` (List[str]): A list of paths to files to be added to the tar archive. - `extract_extensions` (List[str]): A list of file extensions specifying which files to extract. - `archive_name` (str): The name of the tar archive to be created. Output: - List[str]: A list of names of files that were extracted. Example: Suppose you have the following input: ```python file_paths = [\\"example1.txt\\", \\"example2.py\\", \\"example3.md\\"] extract_extensions = [\\".py\\", \\".md\\"] archive_name = \\"my_archive.tar.gz\\" ``` After running `manage_tar_archive(file_paths, extract_extensions, archive_name)`, the function should create a tar archive named `my_archive.tar.gz`, add the files `example1.txt`, `example2.py`, and `example3.md` to the archive, and then extract `example2.py` and `example3.md` to the `extracted_files` directory. The function should return: ```python [\\"example2.py\\", \\"example3.md\\"] ``` Constraints: - All file paths are valid strings and point to existing, readable files. - The `extract_extensions` list contains valid file extensions (starts with a period). - The `archive_name` is a valid tar file name ending in `.tar.gz`. Note: To avoid issues with any existing files/directories, ensure the `extracted_files` directory is either created new or any existing one is used without conflicts. Additionally, handle possible exceptions such as read/write errors neatly within your function. ```python import tarfile import os def manage_tar_archive(file_paths, extract_extensions, archive_name): # Implement your solution here pass # Example usage file_paths = [\\"example1.txt\\", \\"example2.py\\", \\"example3.md\\"] extract_extensions = [\\".py\\", \\".md\\"] archive_name = \\"my_archive.tar.gz\\" print(manage_tar_archive(file_paths, extract_extensions, archive_name)) ```","solution":"import tarfile import os def manage_tar_archive(file_paths, extract_extensions, archive_name): # Create tar archive with tarfile.open(archive_name, \\"w:gz\\") as tar: for file_path in file_paths: tar.add(file_path, arcname=os.path.basename(file_path)) # Create a directory for extracted files extract_dir = \\"extracted_files\\" if not os.path.exists(extract_dir): os.makedirs(extract_dir) # Extract specified files extracted_files = [] with tarfile.open(archive_name, \\"r:gz\\") as tar: for member in tar.getmembers(): if any(member.name.endswith(ext) for ext in extract_extensions): tar.extract(member, path=extract_dir) extracted_files.append(member.name) return extracted_files"},{"question":"**Environment Manager with Cross-Platform Compatibility** You are tasked with creating a class `EnvironmentManager` that abstracts environment variable operations. This class should ensure cross-platform compatibility using the \\"os\\" module and should provide methods to fetch, set, and delete environment variables. The operations should mimic the behavior of the `posix` module functionalities but be wrapped in a structured and user-friendly way. # Requirements: 1. **Class**: `EnvironmentManager` 2. **Methods**: - `get_variable(name: str) -> str`: Fetches the value of the environment variable `name`. Returns `None` if the variable does not exist. - `set_variable(name: str, value: str) -> None`: Sets the environment variable `name` to the specified `value`. - `delete_variable(name: str) -> None`: Deletes the environment variable `name`. If the variable does not exist, it should do nothing. - `list_variables() -> dict`: Returns a dictionary of all environment variables with their values. # Expected Input and Output: - `get_variable(\'HOME\')` -> Might return the path to the home directory (e.g., `/home/user`). - `set_variable(\'MY_VAR\', \'123\')` -> Sets the environment variable `MY_VAR` to `123`. - `delete_variable(\'MY_VAR\')` -> Deletes the environment variable `MY_VAR`. - `list_variables()` -> Returns a dictionary of all current environment variables. # Constraints: 1. The class should use the \\"os\\" module extensively for cross-platform compatibility. 2. Handle both cases where environment variables might be bytes or strings, ensuring the outputs are consistent. 3. Ensure that operations on the environment variables are performant and do not involve unnecessary overhead. # Code Template: ```python import os class EnvironmentManager: def get_variable(self, name: str) -> str: # TODO: implement this method pass def set_variable(self, name: str, value: str) -> None: # TODO: implement this method pass def delete_variable(self, name: str) -> None: # TODO: implement this method pass def list_variables(self) -> dict: # TODO: implement this method pass # Example usage: # env_manager = EnvironmentManager() # env_manager.set_variable(\'MY_VAR\', \'123\') # print(env_manager.get_variable(\'MY_VAR\')) # Should output \'123\' # env_manager.delete_variable(\'MY_VAR\') # print(env_manager.get_variable(\'MY_VAR\')) # Should output None # print(env_manager.list_variables()) # Should list all environment variables ``` **Note:** Ensure all operations are tested thoroughly for both Unix and Windows environments.","solution":"import os class EnvironmentManager: def get_variable(self, name: str) -> str: Fetches the value of the environment variable `name`. Returns `None` if the variable does not exist. return os.environ.get(name) def set_variable(self, name: str, value: str) -> None: Sets the environment variable `name` to the specified `value`. os.environ[name] = value def delete_variable(self, name: str) -> None: Deletes the environment variable `name`. If the variable does not exist, it should do nothing. os.environ.pop(name, None) def list_variables(self) -> dict: Returns a dictionary of all environment variables with their values. return dict(os.environ)"},{"question":"The task involves working with the `scikit-learn` library to implement and apply various distance metrics and kernel functions using the `pairwise_distances` and `pairwise_kernels` functions. You will also need to convert distance metrics into kernel similarity measures. Function Implementation 1. **Function: `calculate_pairwise_distances`** - **Input**: - `X` (numpy array of shape (n_samples_X, n_features)) - `Y` (numpy array of shape (n_samples_Y, n_features)) - `metric` (string specifying the distance metric; options: \'euclidean\', \'manhattan\', \'cosine\') - **Output**: - A numpy array of shape (n_samples_X, n_samples_Y) containing the pairwise distances between the row vectors in `X` and `Y`. 2. **Function: `calculate_pairwise_kernels`** - **Input**: - `X` (numpy array of shape (n_samples_X, n_features)) - `Y` (numpy array of shape (n_samples_Y, n_features)) - `kernel` (string specifying the kernel type; options: \'linear\', \'polynomial\', \'rbf\', \'sigmoid\', \'cosine\') - `**kwargs` (additional arguments specific to the kernel type, such as `degree` for polynomial, `gamma` for rbf, sigmoid, and polynomial, `coef0` for polynomial and sigmoid) - **Output**: - A numpy array of shape (n_samples_X, n_samples_Y) containing the pairwise kernel similarities between the row vectors in `X` and `Y`. 3. **Function: `convert_distance_to_kernel`** - **Input**: - `distances` (numpy array of pairwise distances of shape (n_samples, n_samples)) - `conversion_type` (string specifying the conversion method; options: \'exponential\', \'inverse\') - `**kwargs` (additional arguments specific to the conversion method, such as `gamma` for exponential) - **Output**: - A numpy array of shape (n_samples, n_samples) containing the converted kernel similarity values. Constraints - You must use the `sklearn.metrics.pairwise` submodule to compute distances and kernels. - The inputs `X` and `Y` will be non-empty numpy arrays. - For `calculate_pairwise_kernels`, if the `kernel` is \'polynomial\' or \'sigmoid\', ensure the implementation handles the required arguments (`degree`, `gamma`, `coef0`) correctly. - For `convert_distance_to_kernel`, the conversions should be consistent with the described methods in the documentation. Performance - The implemented functions should handle inputs with dimensions up to (1000, 1000) efficiently. - Avoid using explicit loops for element-wise computation. Utilize numpy and scikit-learn\'s built-in functions for performance. # Example Usage ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) # Calculate Pairwise Distances distances = calculate_pairwise_distances(X, Y, metric=\'manhattan\') print(distances) # Calculate Pairwise Kernels kernels = calculate_pairwise_kernels(X, Y, kernel=\'linear\') print(kernels) # Convert Distance to Kernel with exponential method kernel_from_dist = convert_distance_to_kernel(distances, conversion_type=\'exponential\', gamma=0.5) print(kernel_from_dist) ```","solution":"import numpy as np from sklearn.metrics import pairwise_distances, pairwise_kernels def calculate_pairwise_distances(X, Y, metric=\'euclidean\'): Calculate pairwise distances between rows of X and Y using the specified metric. return pairwise_distances(X, Y, metric=metric) def calculate_pairwise_kernels(X, Y, kernel=\'linear\', **kwargs): Calculate pairwise kernel similarities between rows of X and Y using the specified kernel function. return pairwise_kernels(X, Y, metric=kernel, **kwargs) def convert_distance_to_kernel(distances, conversion_type=\'exponential\', **kwargs): Convert pairwise distances to kernel similarities using the specified conversion type. if conversion_type == \'exponential\': gamma = kwargs.get(\'gamma\', 1.0) return np.exp(-gamma * distances) elif conversion_type == \'inverse\': with np.errstate(divide=\'ignore\'): # handle division by zero return 1.0 / (1.0 + distances) else: raise ValueError(\\"Unknown conversion_type. Supported types: \'exponential\', \'inverse\'\\")"},{"question":"You are required to write a Python function that processes a list of URLs and retrieves their status codes in parallel using the `concurrent.futures` module. # Function Signature ```python def fetch_status_codes(urls: List[str]) -> Dict[str, int]: ``` # Input - `urls`: A list of URLs (strings) to fetch the status codes from. (1 <= len(urls) <= 1000) # Output - Returns a dictionary where each key is a URL and its corresponding value is the HTTP status code. # Requirements 1. Use `concurrent.futures.ThreadPoolExecutor` to manage the parallel execution of network requests. 2. Handle network exceptions gracefully. If a request fails, store the value `-1` for that URL in the output dictionary. 3. Ensure that the solution is efficient and handles up to 1000 URLs. # Constraints - Assume all URLs are valid and reachable. - No authentication or additional HTTP headers are required. - Use `requests` library to perform the HTTP GET requests. If `requests` is not installed, it can be installed using `pip install requests`. # Example ```python urls = [ \\"http://example.com\\", \\"http://thisurldoesnotexist.com\\", \\"https://httpstat.us/200\\", \\"https://httpstat.us/404\\" ] print(fetch_status_codes(urls)) ``` Expected Output: ```python { \\"http://example.com\\": 200, \\"http://thisurldoesnotexist.com\\": -1, \\"https://httpstat.us/200\\": 200, \\"https://httpstat.us/404\\": 404 } ``` # Notes 1. The function must use `concurrent.futures.ThreadPoolExecutor` for fetching the status codes in parallel. 2. Network errors should not cause the entire function to fail; they should be handled and reported as `-1`. # Hints - Utilize `concurrent.futures.as_completed` to handle results as they complete. - Use the `requests` library for making HTTP requests.","solution":"from typing import List, Dict import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_status_codes(urls: List[str]) -> Dict[str, int]: def get_status_code(url): try: response = requests.get(url) return url, response.status_code except requests.RequestException: return url, -1 result = {} with ThreadPoolExecutor() as executor: future_to_url = {executor.submit(get_status_code, url): url for url in urls} for future in as_completed(future_to_url): url, status_code = future.result() result[url] = status_code return result"},{"question":"**Question: Efficient Data Processing with Itertools and Functools** You are given a dataset of numbers, and your task is to perform a series of operations using `itertools` and `functools` to transform and analyze the data efficiently. Specifically, you need to implement a function `process_data` that: 1. Takes a list of integers as input. 2. Filters out any integers that are not prime. 3. Creates all possible combinations of the remaining integers. 4. Calculates the sum of each combination. 5. Takes the sums of the combinations and finds the maximum sum using `functools.reduce`. You should also write helper functions `is_prime` to check for prime numbers and `sum_combinations` to create and sum all possible combinations. **Function Signature:** ```python def process_data(data: List[int]) -> int: # Your code here def is_prime(n: int) -> bool: # Your code here def sum_combinations(data: List[int]) -> List[int]: # Your code here ``` **Expected Input and Output:** - Input: A list of integers `data`. - Output: An integer representing the highest sum of any combination. **Constraints:** - The input list can be empty or contain up to 1000 integers. - Each integer in the input list will be in the range `[1, 10^6]`. **Example:** ```python data = [2, 3, 4, 5, 6] print(process_data(data)) # Output should be the largest possible sum of any combination of prime numbers from the list. ``` **Performance Requirements:** - The solution should efficiently handle the upper limits of constraints. - Use of the `itertools` module for combinations and `functools.reduce` for the final sum is mandatory. Note: Assume that the `itertools` and `functools` modules are available and imported.","solution":"from itertools import combinations from functools import reduce from typing import List def is_prime(n: int) -> bool: Returns True if the given number n is a prime number, otherwise False. if n < 2: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def sum_combinations(data: List[int]) -> List[int]: Given a list of integers, returns a list of sums of all possible combinations. result = [] for i in range(1, len(data) + 1): for combo in combinations(data, i): result.append(sum(combo)) return result def process_data(data: List[int]) -> int: Processes the data by filtering out non-prime numbers, creating all possible combinations of the remaining integers, summing each combination, and finding the maximum sum. primes = [x for x in data if is_prime(x)] if not primes: return 0 sums = sum_combinations(primes) return reduce(max, sums)"},{"question":"# Question: Analyzing Titanic Dataset with Seaborn You have been provided with the Titanic dataset. Your task is to create several different visualizations to extract and present insights regarding the passengers\' demographics and survival rates. The Titanic dataset can be loaded directly using seaborn\'s `load_dataset` function. **Specifications:** 1. Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 2. Create a violin plot to visualize the distribution of passengers\' ages across different classes. Use the `hue` parameter to differentiate between genders. 3. Modify the plot to: - Adjust the bandwidth of the kernel density estimate using `bw_adjust=0.6`. - Trim the tails of the violins to the data range by setting `cut=0`. 4. Create a bar plot to show the survival rate of passengers grouped by the class and split by gender, using subplots for each gender. 5. Customize the bar plot by: - Setting the figure size to height=5 and aspect=1. - Modifying the axis labels to \\"Passenger Class\\" for the x-axis and \\"Survival Rate\\" for the y-axis. - Setting the y-axis limits from 0 to 1. - Adding titles to subplots showing \\"Male\\" and \\"Female\\". - Removing the left spine of the plots. **Input:** - The Titanic dataset loaded using seaborn\'s `load_dataset`. **Output:** - Two visualizations: 1. A violin plot as described. 2. A customized bar plot as described. **Constraints:** - Use seaborn for all visualizations. - Ensure the plots include proper labels and titles as specified. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Step 2: Create a violin plot sns.catplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.6, cut=0 ) # Show the plot plt.show() # Step 3: Create a bar plot to show survival rates grouped by class and gender, and customize it g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=1, ) g.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") g.set(ylim=(0, 1)) g.set_titles(\\"{col_name}\\") g.despine(left=True) # Show the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plots(): # Step 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Step 2: Create a violin plot sns.catplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.6, cut=0 ) # Show the plot plt.show() # Step 3: Create a bar plot to show survival rates grouped by class and gender, and customize it g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=1, ) g.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") g.set(ylim=(0, 1)) g.set_titles(\\"{col_name}\\") g.despine(left=True) # Show the plot plt.show()"},{"question":"**Problem Statement:** You are tasked with implementing a function that compresses and then decompresses a given text string using the zlib library, ensuring that the round-trip transformation (compression followed by decompression) returns the original text accurately. Additionally, you will need to implement an error-handling mechanism to catch potential compression and decompression errors. **Function Signature:** ```python def compress_and_decompress(text: str, compression_level: int = -1) -> str: Compresses and decompresses a given text string using the zlib library. Args: - text (str): The text string to be compressed and decompressed. - compression_level (int): The level of compression to apply (default is -1). Returns: - str: The decompressed text, which should match the original input text. Raises: - ValueError: If any compression or decompression error occurs. pass ``` **Detailed Requirements:** 1. **Input and Output**: - The input `text` is a string that needs to be compressed and then decompressed. - The optional `compression_level` is an integer from -1 to 9: - `-1` represents the default compression level. - `0` represents no compression. - `1` (Z_BEST_SPEED) is the fastest but with the least compression. - `9` (Z_BEST_COMPRESSION) is the slowest but with the maximum compression. - The function should return the decompressed text if the process is successful. 2. **Constraints**: - Ensure that the decompressed text matches the original text. - Raise a `ValueError` with an appropriate message if any compression or decompression error occurs. 3. **Error Handling**: - Use the `zlib.error` exception to handle and raise errors as `ValueError` if the compression or decompression fails. **Example Usage:** ```python try: original_text = \\"Hello, world!\\" result = compress_and_decompress(original_text, 5) assert result == original_text print(\\"Compression and decompression were successful.\\") except ValueError as e: print(f\\"An error occurred: {e}\\") ``` **Notes**: - You may use the `zlib.compress()` and `zlib.decompress()` functions. - This task will test the understanding of zlib library functions, error handling, and data integrity during compression and decompression processes.","solution":"import zlib def compress_and_decompress(text: str, compression_level: int = -1) -> str: Compresses and decompresses a given text string using the zlib library. Args: - text (str): The text string to be compressed and decompressed. - compression_level (int): The level of compression to apply (default is -1). Returns: - str: The decompressed text, which should match the original input text. Raises: - ValueError: If any compression or decompression error occurs. try: compressed_data = zlib.compress(text.encode(\'utf-8\'), compression_level) decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') return decompressed_data except zlib.error as e: raise ValueError(f\\"Compression or decompression error occurred: {e}\\")"},{"question":"Python OS Module Assessment **Objective:** Implement a function that simulates a basic shell, which can execute commands with environment variables and redirect output to specific files. The function should handle errors appropriately and return meaningful error messages. **Description:** You are tasked with creating a function, `run_shell_command(command: str, env: dict, output_file: str) -> str`, which: 1. Executes a shell command given by the `command` string. 2. Uses the environment variables provided in the `env` dictionary. 3. Redirects the standard output and error to a file specified by `output_file`. 4. Returns the standard output and standard error messages as a single string. **Input:** - `command` (str): A valid shell command to execute. - `env` (dict): A dictionary of environment variables to set during the command execution. - `output_file` (str): A file path where the standard output and error should be redirected. **Output:** - Returns a single string containing both the standard output and error messages. **Constraints:** - The function should print meaningful error messages if any operations like opening the file or executing the command fail. - The function should create the output file if it does not exist, and it should append to the file if it already exists. **Example Usage:** ```python result = run_shell_command(\\"echo Hello World\\", {\\"GREETING\\": \\"Hello\\"}, \\"output.txt\\") print(result) # Output might include content from the output.txt file and additional error/status messages. result = run_shell_command(\\"ls -l\\", {}, \\"output.txt\\") print(result) # Output might include the directory listing and additional error/status messages. ``` **Advanced Challenge:** - Ensure that commands are executed in a new process using `os.fork` and `os.exec`. - Handle the environment setup using `os.environ`. - Redirect the outputs using file descriptors and `os.dup2`. Function Signature: ```python def run_shell_command(command: str, env: dict, output_file: str) -> str: pass ``` Requirements: 1. Use file-related functions from the `os` module to handle output redirection. 2. Use process-related functions from the `os` module to execute the command. 3. Handle environment variables appropriately during the command execution. 4. Implement proper error handling to manage different exceptions that might occur during file operations and process management. Hints: - You might find `os.environ.update`, `os.open`, `os.dup2`, `os.execvp`, `os.fork`, and `os.waitpid` useful. - Ensure to close any file descriptors you open to avoid resource leaks. - Use try-except blocks to handle possible exceptions and return user-friendly error messages.","solution":"import os import subprocess def run_shell_command(command: str, env: dict, output_file: str) -> str: try: with open(output_file, \'a\') as file: process = subprocess.Popen( command, shell=True, env={**os.environ, **env}, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) stdout, stderr = process.communicate() file.write(stdout) file.write(stderr) return stdout + stderr except Exception as e: return str(e)"},{"question":"Reshaping and Summarizing Data Objective: The goal of this task is to demonstrate your understanding of pandas\' reshaping methods, specifically `pivot`, `melt`, and `crosstab`. You will be provided with a DataFrame and asked to manipulate it to achieve a specific format and to create summary tables. Question: You are given a dataset containing details of students\' grades across multiple subjects over several semesters. The dataset is in a \\"long\\" format with one row per subject per student. Your task is to reshape this DataFrame into a more meaningful format and create summary tables. Here is your dataset: ```python import pandas as pd data = { \'student_id\': [\'S1\', \'S1\', \'S1\', \'S2\', \'S2\', \'S2\', \'S3\', \'S3\', \'S3\'], \'semester\': [\'2020-01\', \'2020-01\', \'2020-01\', \'2020-02\', \'2020-02\', \'2020-02\', \'2020-03\', \'2020-03\', \'2020-03\'], \'subject\': [\'Math\', \'Science\', \'Art\', \'Math\', \'Science\', \'Art\', \'Math\', \'Science\', \'Art\'], \'grade\': [90, 85, 92, 88, 78, 80, 84, 89, 91], \'credits\': [3, 4, 2, 3, 4, 2, 3, 4, 2] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Reshape the DataFrame to a Wide Format:** Create a pivot table where each row represents a student and each column represents a combination of subjects and semesters. The values inside the table should be the students\' grades. **Output format:** ```plaintext Math_2020-01 Science_2020-01 Art_2020-01 Math_2020-02 ... Art_2020-03 student_id S1 90 85 92 NaN NaN S2 NaN NaN NaN 88.0 80.0 S3 NaN NaN NaN NaN 84.0 89 91 ``` 2. **Reverse the Reshaping:** Convert the previously reshaped DataFrame back to the original long format using the `melt` function. **Output format:** It should be similar to the provided `data`. 3. **Create a Summary Table:** Use the `crosstab` function to create a summary table showing the total credits taken by each student across all semesters. **Output format:** ```plaintext Total Credits student_id S1 9 S2 9 S3 9 ``` **Constraints:** - You cannot use loops to manually transform the data. - Use appropriate pandas functions to perform the transformations efficiently. Implementation: Write your implementation in Python, following the task descriptions above. ```python def reshape_and_summarize(df): # Task 1: Reshape to wide format wide_df = df.pivot_table(index=\'student_id\', columns=[\'subject\', \'semester\'], values=\'grade\') wide_df.columns = [f\'{sub}_{sem}\' for sub, sem in wide_df.columns] # Task 2: Reverse the reshaping long_df = pd.melt(wide_df.reset_index(), id_vars=\'student_id\', var_name=[\'subject\', \'semester\'], value_name=\'grade\') long_df[[\'subject\', \'semester\']] = long_df[\'subject_semester\'].str.split(\'_\', expand=True) long_df = long_df.drop(columns=[\'subject_semester\']) # Task 3: Create summary table summary_table = pd.crosstab(df[\'student_id\'], df[\'credits\'], aggfunc=\'sum\', normalize=False) return wide_df, long_df, summary_table # Sample usage df = pd.DataFrame(data) reshape_and_summarize(df) ``` **Expected Output:** # Reshaped to Wide Format: ``` subject Math_2020-01 Science_2020-01 Art_2020-01 Math_2020-02 ... Art_2020-03 student_id S1 90.0 85 92 NaN NaN S2 NaN NaN NaN 88.0 80 S3 NaN NaN NaN NaN NaN ................ ``` # Reversed to Long Format: ``` student_id subject semester grade 0 S1 Math_2020-01 90.0 1 S1 Science_2020-01 85.0 2 S1 complete... ``` # Summary Table: ``` credits 2 3 4 student_id S1 1 1 1 S2 1 1 1 S3 1 1 1 ```","solution":"import pandas as pd def reshape_and_summarize(df): # Task 1: Reshape to wide format wide_df = df.pivot_table(index=\'student_id\', columns=[\'subject\', \'semester\'], values=\'grade\') wide_df.columns = [f\'{sub}_{sem}\' for sub, sem in wide_df.columns] # Task 2: Reverse the reshaping wide_df_reset = wide_df.reset_index() long_df = pd.melt(wide_df_reset, id_vars=\'student_id\', var_name=\'subject_semester\', value_name=\'grade\') long_df[[\'subject\', \'semester\']] = long_df[\'subject_semester\'].str.split(\'_\', expand=True) long_df = long_df.drop(columns=[\'subject_semester\']).dropna().reset_index(drop=True) # Task 3: Create summary table summary_table = df.groupby(\'student_id\')[\'credits\'].sum().reset_index() summary_table.columns = [\'student_id\', \'Total Credits\'] summary_table.set_index(\'student_id\', inplace=True) return wide_df, long_df, summary_table # Sample usage data = { \'student_id\': [\'S1\', \'S1\', \'S1\', \'S2\', \'S2\', \'S2\', \'S3\', \'S3\', \'S3\'], \'semester\': [\'2020-01\', \'2020-01\', \'2020-01\', \'2020-02\', \'2020-02\', \'2020-02\', \'2020-03\', \'2020-03\', \'2020-03\'], \'subject\': [\'Math\', \'Science\', \'Art\', \'Math\', \'Science\', \'Art\', \'Math\', \'Science\', \'Art\'], \'grade\': [90, 85, 92, 88, 78, 80, 84, 89, 91], \'credits\': [3, 4, 2, 3, 4, 2, 3, 4, 2] } df = pd.DataFrame(data) wide_df, long_df, summary_table = reshape_and_summarize(df)"},{"question":"# Task You are required to implement a Python function that ensures the security of user passwords in a Unix system by verifying specific password maintenance policies using the `spwd` module. # Objective Write a function `verify_password_policy(username: str) -> dict` that: 1. **Takes** a single argument `username`, a string representing the login name of the user whose password policy compliance needs to be verified. 2. **Returns** a dictionary containing: - `\\"status\\"`: A boolean indicating if the policy check passed. - `\\"reasons\\"`: A list of reasons if the policy check failed. # Policy Conditions Your function should verify the following conditions: 1. The password must have been changed within the last 90 days. 2. The minimum number of days allowed between password changes must be at least 7 days. 3. The maximum number of days allowed between password changes must not exceed 365 days. 4. The number of days before the password expires to warn the user should be at least 7 days. # Input - `username` (str): A string representing the login name of the user. # Output A dictionary with two keys: - `\\"status\\"` (bool): `True` if all policy conditions are met, otherwise `False`. - `\\"reasons\\"` (list): A list of strings explaining which conditions were not met if any. # Constraints - Your function should raise a `PermissionError` if it does not have the necessary privileges to access the shadow password database. # Example ```python # Given the following shadow password tuple for testuser: # (\'testuser\', \'encryptedpwd\', 18590, 0, 99999, 7, -1, -1, -1) result = verify_password_policy(\'testuser\') # Output: # { # \\"status\\": False, # \\"reasons\\": [ # \\"Password has not been changed within the last 90 days\\", # \\"Maximum number of days between password changes exceeded\\" # ] # } ``` # Notes - Assume the `spwd` module is available and you have the necessary privileges to access the shadow password database. - Use the `spwd.getspnam(name)` function to retrieve the user\'s shadow password database entry. - If the user does not exist or cannot be found, handle the `KeyError` exception gracefully and return an appropriate response.","solution":"import spwd import time def verify_password_policy(username: str) -> dict: Verify the password policy compliance for a given Unix user. Parameters: username (str): The login name of the user. Returns: dict: {\'status\': bool, \'reasons\': list} try: spwd_entry = spwd.getspnam(username) except KeyError: return {\\"status\\": False, \\"reasons\\": [\\"User not found\\"]} except PermissionError as e: return {\\"status\\": False, \\"reasons\\": [str(e)]} except Exception as e: return {\\"status\\": False, \\"reasons\\": [str(e)]} current_time = time.time() / (24 * 3600) # Current time in days since epoch last_change = spwd_entry.sp_lstchg min_days = spwd_entry.sp_min max_days = spwd_entry.sp_max warn_days = spwd_entry.sp_warn reasons = [] # Condition 1: The password must have been changed within the last 90 days. if current_time - last_change > 90: reasons.append(\\"Password has not been changed within the last 90 days\\") # Condition 2: The minimum number of days allowed between password changes must be at least 7 days. if min_days < 7: reasons.append(\\"Minimum number of days between password changes is less than 7 days\\") # Condition 3: The maximum number of days allowed between password changes must not exceed 365 days. if max_days > 365: reasons.append(\\"Maximum number of days between password changes exceeds 365 days\\") # Condition 4: The number of days before the password expires to warn the user should be at least 7 days. if warn_days < 7: reasons.append(\\"Number of days before warning the user is less than 7 days\\") return {\\"status\\": len(reasons) == 0, \\"reasons\\": reasons}"},{"question":"# PyTorch Distributions Coding Assessment **Objective:** Demonstrate your understanding of PyTorch\'s `torch.distributions` module by performing a series of tasks involving various probability distributions. # Question You are tasked with implementing a utility function in PyTorch that performs the following steps: 1. **Sample Generation:** - Generate 1000 samples from a multivariate normal distribution `MultivariateNormal` with a predefined mean vector and covariance matrix. - Generate 1000 samples from a categorical distribution `Categorical` with predefined probabilities. 2. **Probability Calculation:** - Calculate the probability of a given sample under the multivariate normal distribution. - Calculate the probability of a given sample under the categorical distribution. 3. **KL Divergence Calculation:** - Compute the KL divergence between two normal distributions with given parameters (means and covariance matrices). 4. **Output Information:** - Return a summary dictionary containing the generated samples and calculated probabilities. # Function Signature: ```python def pytorch_distributions_util(): Performs various operations using PyTorch Distributions. Returns: dict: A dictionary containing: \'mv_samples\': Tensor of shape (1000, 2) with samples from Multivariate Normal Distribution. \'cat_samples\': Tensor of shape (1000,) with samples from Categorical Distribution. \'mv_sample_probability\': Float, probability of a given sample under the Multivariate Normal Distribution. \'cat_sample_probability\': Float, probability of a given sample under the Categorical Distribution. \'kl_divergence\': Float, KL divergence between two normal distributions. pass ``` Details: 1. **Step 1: Sample Generation** - Use the `MultivariateNormal` class from `torch.distributions` to generate 1000 samples with mean vector `[0.0, 0.0]` and covariance matrix `[[1.0, 0.5], [0.5, 1.0]]`. - Use the `Categorical` class from `torch.distributions` to generate 1000 samples with probabilities `[0.1, 0.2, 0.3, 0.4]`. 2. **Step 2: Probability Calculation** - Use the multivariate normal distribution to calculate the probability of the sample `[0.5, 0.5]`. - Use the categorical distribution to calculate the probability of the sample `2`. 3. **Step 3: KL Divergence Calculation** - Define two multivariate normal distributions with mean vectors `[0.0, 0.0]` and `[1.0, 1.0]`, and covariance matrices `[[1.0, 0.5], [0.5, 1.0]]` for both. - Calculate the KL divergence between these two distributions using `kl_divergence` function from `torch.distributions.kl`. You must return a dictionary with the following keys: - `\'mv_samples\'`: Tensor of shape (1000, 2) with samples from the Multivariate Normal Distribution. - `\'cat_samples\'`: Tensor of shape (1000,) with samples from the Categorical Distribution. - `\'mv_sample_probability\'`: Float, probability of the sample `[0.5, 0.5]` under the Multivariate Normal Distribution. - `\'cat_sample_probability\'`: Float, probability of the sample `2` under the Categorical Distribution. - `\'kl_divergence\'`: Float, KL divergence between the two normal distributions. **Constraints:** - Use only the functions and classes from the `torch.distributions` module to complete your task. **Example Input/Output:** The function does not take any inputs. Below is an example of the expected output format: ```json { \'mv_samples\': <1000x2 tensor>, \'cat_samples\': <1000 tensor>, \'mv_sample_probability\': <float>, \'cat_sample_probability\': <float>, \'kl_divergence\': <float> } ``` You are required to implement the `pytorch_distributions_util` function that adheres to the above specifications.","solution":"import torch from torch.distributions import MultivariateNormal, Categorical, kl_divergence def pytorch_distributions_util(): # Step 1: Sample Generation mean = torch.tensor([0.0, 0.0]) cov_matrix = torch.tensor([[1.0, 0.5], [0.5, 1.0]]) mv_dist = MultivariateNormal(mean, cov_matrix) mv_samples = mv_dist.sample((1000,)) probabilities = torch.tensor([0.1, 0.2, 0.3, 0.4]) cat_dist = Categorical(probabilities) cat_samples = cat_dist.sample((1000,)) # Step 2: Probability Calculation sample_mv = torch.tensor([0.5, 0.5]) mv_sample_probability = torch.exp(mv_dist.log_prob(sample_mv)) sample_cat = torch.tensor(2) cat_sample_probability = torch.exp(cat_dist.log_prob(sample_cat)) # Step 3: KL Divergence Calculation mean2 = torch.tensor([1.0, 1.0]) mv_dist2 = MultivariateNormal(mean2, cov_matrix) kl_div = kl_divergence(mv_dist, mv_dist2).item() return { \'mv_samples\': mv_samples, \'cat_samples\': cat_samples, \'mv_sample_probability\': mv_sample_probability.item(), \'cat_sample_probability\': cat_sample_probability.item(), \'kl_divergence\': kl_div }"},{"question":"Objective The purpose of this assessment is to evaluate your understanding of PyTorch\'s \\"meta\\" device and your ability to work with meta tensors for model initialization, transformation, and analysis. Question You are given a neural network architecture that needs to be analyzed and transformed before being loaded into memory. Your task is to implement a function that performs the following operations: 1. Load a given model architecture (defined using `torch.nn.Module`) onto the \\"meta\\" device. 2. Print the model architecture showing the metadata without loading the actual data. 3. Convert the meta-tensor-based model into an equivalent uninitialized model on the CPU. 4. Re-initialize the model parameters using a normal distribution with mean 0 and standard deviation 1. 5. Return the fully initialized model on the CPU. You are provided with a sample model architecture for testing purposes. Function Signature ```python import torch import torch.nn as nn def process_model_on_meta(model: nn.Module) -> nn.Module: Processes a given PyTorch model architecture using meta tensors. Args: model (nn.Module): The PyTorch model architecture to be processed. Returns: nn.Module: The fully initialized model on the CPU. ``` Constraints - The model should be transformed using the \\"meta\\" device to perform metadata-only operations. - The re-initialization should be done using the `torch.nn.init.normal_` method with mean 0 and standard deviation 1. Example - Sample Model Architecture ```python class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = nn.ReLU()(x) x = self.fc2(x) return x # Usage of the function with sample model sample_model = SampleModel() processed_model = process_model_on_meta(sample_model) print(processed_model) ``` Expected Output The function should print the metadata of the model architecture and return the fully re-initialized model on the CPU. ```plaintext SampleModel( (fc1): Linear(in_features=10, out_features=20, bias=True) (fc2): Linear(in_features=20, out_features=10, bias=True) ) SampleModel( (fc1): Linear(in_features=10, out_features=20, bias=True) (fc2): Linear(in_features=20, out_features=10, bias=True) ) ```","solution":"import torch import torch.nn as nn def process_model_on_meta(model: nn.Module) -> nn.Module: Processes a given PyTorch model architecture using meta tensors. Args: model (nn.Module): The PyTorch model architecture to be processed. Returns: nn.Module: The fully initialized model on the CPU. # Load the model onto the \\"meta\\" device meta_model = model.to(\'meta\') # Print the meta model to show the metadata (note: actual weights are not instantiated) print(meta_model) # Move back to the CPU, but the parameters will be uninitialized uninitialized_model = type(model)() # Initialize the model parameters using normal distribution with mean 0 and std 1 for name, param in uninitialized_model.named_parameters(): if \'weight\' in name or \'bias\' in name: torch.nn.init.normal_(param, mean=0, std=1) return uninitialized_model # Sample Model Definition for testing purposes class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = nn.ReLU()(x) x = self.fc2(x) return x # Usage of the function with the sample model if __name__ == \\"__main__\\": sample_model = SampleModel() processed_model = process_model_on_meta(sample_model) print(processed_model)"},{"question":"Objective Your task is to create a Python function that introspects a given function and returns detailed information about the underlying code object, which includes the number of free variables, the names of the variables, and the line number mapping of the code. Problem Statement Write a Python function `introspect_function(func: Callable) -> Dict[str, Any]` that takes a single parameter `func`, which is a callable (function) and returns a dictionary containing the following information about the underlying code object: 1. `num_free_vars`: The number of free variables in the code object. 2. `var_names`: A list of variable names used in the code object. 3. `line_numbers`: A dictionary mapping bytecode offsets to line numbers. Input - `func`: A callable (function). Output - A dictionary with the following keys: - `num_free_vars` (int): Number of free variables. - `var_names` (List[str]): List of variable names used in the code object. - `line_numbers` (Dict[int, int]): Mapping from bytecode offsets (int) to line numbers (int). Constraints - The function passed as an argument will be a well-defined Python function. - The code should work for functions defined within the same Python script. - Performance should be efficient for typical function sizes seen in introductory data structures and algorithms courses. Example ```python def sample_function(x): y = x + 1 return y result = introspect_function(sample_function) # Expected output (values may vary based on implementation details) { \'num_free_vars\': 0, \'var_names\': [\'x\', \'y\'], \'line_numbers\': {0: 1, 2: 2, 4: 3} } ``` Notes - You may use Python\'s `inspect` module to obtain the underlying code object. - Bytecode offsets and line numbers can be accessed through the code object\'s attributes.","solution":"import inspect from typing import Callable, Any, Dict, List def introspect_function(func: Callable) -> Dict[str, Any]: Introspects a given function and returns detailed information about the underlying code object. Parameters: - func (Callable): The function to introspect. Returns: - Dict[str, Any]: A dictionary with information about the code object. code_obj = func.__code__ # Number of free variables num_free_vars = code_obj.co_freevars # Variable names var_names = list(code_obj.co_varnames) # Line number mapping line_numbers = {offset: lineno for offset, lineno in enumerate(code_obj.co_lnotab)} return { \'num_free_vars\': len(num_free_vars), \'var_names\': var_names, \'line_numbers\': line_numbers }"},{"question":"**File Organizer Script with Command-Line Interface** **Objective**: Write a Python script that organizes files in a specified directory based on their file extensions. The script should be able to handle different types of file compressions and produce a summary report of the actions performed. # Instructions: 1. **Command-Line Arguments**: - The script should accept two command-line arguments: 1. `--src-dir`: Path to the source directory containing the files to organize. 2. `--dst-dir`: Path to the destination directory where organized files will be moved. Example: ```shell python file_organizer.py --src-dir=\'/path/to/source\' --dst-dir=\'/path/to/destination\' ``` 2. **File Organizing**: - For each file in the source directory, create a subdirectory in the destination directory named after the file\'s extension (e.g., a file `example.txt` should go to `dst-dir/txt/`). - Move the file to the corresponding subdirectory. - If a file is compressed (`.zip`, `.gz`, `.bz2`), decompress it before moving. Assume that compressed files contain a single file. 3. **Decompression**: - Handle `.zip` files using the `zipfile` module. - Handle `.gz` and `.bz2` files using the `gzip` and `bz2` modules, respectively. 4. **Report**: - After organizing the files, the script should print a summary report specifying: - Total files processed. - Number of files moved to each subdirectory. - Number of compressed files decompressed. # Function Implementation: Implement the following function in your script: ```python import os import shutil import zipfile import gzip import bz2 import argparse import sys def organize_files(src_dir, dst_dir): # Implement your code here pass if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Organize files by extension.\') parser.add_argument(\'--src-dir\', type=str, required=True, help=\'Source directory containing files to organize\') parser.add_argument(\'--dst-dir\', type=str, required=True, help=\'Destination directory for organized files\') args = parser.parse_args() organize_files(args.src_dir, args.dst_dir) ``` # Constraints: - Assume that the `src_dir` and `dst_dir` are provided and valid. - Handle possible exceptions (e.g., permission issues, missing files) gracefully and report them in the summary. # Example Output: ``` Total files processed: 10 Files moved to txt/: 3 Files moved to py/: 4 Files moved to csv/: 2 Compressed files decompressed: 1 ``` # Notes: - Ensure the script uses a clean and Pythonic approach. - Your solution will be evaluated based on correctness, efficiency, and code readability.","solution":"import os import shutil import zipfile import gzip import bz2 import argparse def organize_files(src_dir, dst_dir): files_processed = 0 decompressed_files = 0 file_count_by_extension = {} if not os.path.exists(dst_dir): os.makedirs(dst_dir) for filename in os.listdir(src_dir): source_path = os.path.join(src_dir, filename) if os.path.isfile(source_path): _, ext = os.path.splitext(filename) ext = ext.lstrip(\'.\').lower() if ext in [\'zip\', \'gz\', \'bz2\']: decompressed_filename = decompress_file(source_path, ext) if decompressed_filename: source_path = os.path.join(src_dir, decompressed_filename) _, ext = os.path.splitext(decompressed_filename) ext = ext.lstrip(\'.\').lower() decompressed_files += 1 destination_dir = os.path.join(dst_dir, ext) if not os.path.exists(destination_dir): os.makedirs(destination_dir) shutil.move(source_path, destination_dir) files_processed += 1 if ext not in file_count_by_extension: file_count_by_extension[ext] = 0 file_count_by_extension[ext] += 1 print_report(files_processed, file_count_by_extension, decompressed_files) def decompress_file(file_path, ext): try: if ext == \'zip\': with zipfile.ZipFile(file_path, \'r\') as zip_ref: zip_ref.extractall(os.path.dirname(file_path)) return zip_ref.namelist()[0] elif ext == \'gz\': with gzip.open(file_path, \'rb\') as f_in, open(file_path[:-3], \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) return os.path.basename(file_path[:-3]) elif ext == \'bz2\': with bz2.BZ2File(file_path, \'rb\') as f_in, open(file_path[:-4], \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) return os.path.basename(file_path[:-4]) except Exception as e: print(f\\"Error decompressing file {file_path}: {e}\\") return None def print_report(files_processed, file_count_by_extension, decompressed_files): print(f\\"Total files processed: {files_processed}\\") for ext, count in file_count_by_extension.items(): print(f\\"Files moved to {ext}/: {count}\\") print(f\\"Compressed files decompressed: {decompressed_files}\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Organize files by extension.\') parser.add_argument(\'--src-dir\', type=str, required=True, help=\'Source directory containing files to organize\') parser.add_argument(\'--dst-dir\', type=str, required=True, help=\'Destination directory for organized files\') args = parser.parse_args() organize_files(args.src_dir, args.dst_dir)"},{"question":"**Objective**: Demonstrate your understanding of the `stat` module for interpreting file system metadata by implementing a custom file system scanner utility. **Question**: You are required to implement a function called `scan_filesystem` that takes a root directory path as input and returns a dictionary summarizing the types and permissions of all files and directories within that path. **Function Signature**: ```python def scan_filesystem(root_path: str) -> dict: pass ``` **Input**: - `root_path` (str): A string representing the root directory path to scan. **Output**: - A dictionary where: * Keys are the paths of the scanned entities (files, directories, etc.) relative to `root_path`. * Values are tuples containing: 1. A string representing the type of the entity (e.g., \'directory\', \'file\', \'symlink\', etc.). 2. A string representing the permissions of the entity in the form `-rwxrwxrwx`. **Constraints**: - You must use functions from the `stat` module to determine the type and permissions of each entity. - You should recursively scan all subdirectories of the given root path. - Consider the permissions and types recognized by the `stat` module as mentioned in the provided documentation. **Example**: ```python import os def scan_filesystem(root_path): result = {} for dirpath, dirnames, filenames in os.walk(root_path): for name in dirnames + filenames: path = os.path.join(dirpath, name) try: st_mode = os.lstat(path).st_mode if stat.S_ISDIR(st_mode): f_type = \'directory\' elif stat.S_ISREG(st_mode): f_type = \'file\' elif stat.S_ISLNK(st_mode): f_type = \'symlink\' elif stat.S_ISCHR(st_mode): f_type = \'character special device\' elif stat.S_ISBLK(st_mode): f_type = \'block special device\' elif stat.S_ISFIFO(st_mode): f_type = \'FIFO\' elif stat.S_ISSOCK(st_mode): f_type = \'socket\' else: f_type = \'unknown\' permissions = stat.filemode(st_mode) relative_path = os.path.relpath(path, root_path) result[relative_path] = (f_type, permissions) except FileNotFoundError: continue return result # Example usage: # scan_result = scan_filesystem(\\"/path/to/root\\") # print(scan_result) ``` This question challenges the student to: - Use file metadata effectively. - Apply `stat` module functions to interpret file types and permissions. - Implement a recursive directory scan. - Handle potential errors like `FileNotFoundError`. **Performance Requirements**: - The function should efficiently handle large directories with many nested subdirectories.","solution":"import os import stat def scan_filesystem(root_path: str) -> dict: result = {} for dirpath, dirnames, filenames in os.walk(root_path): for name in dirnames + filenames: path = os.path.join(dirpath, name) try: st_mode = os.lstat(path).st_mode if stat.S_ISDIR(st_mode): f_type = \'directory\' elif stat.S_ISREG(st_mode): f_type = \'file\' elif stat.S_ISLNK(st_mode): f_type = \'symlink\' elif stat.S_ISCHR(st_mode): f_type = \'character special device\' elif stat.S_ISBLK(st_mode): f_type = \'block special device\' elif stat.S_ISFIFO(st_mode): f_type = \'FIFO\' elif stat.S_ISSOCK(st_mode): f_type = \'socket\' else: f_type = \'unknown\' permissions = stat.filemode(st_mode) relative_path = os.path.relpath(path, root_path) result[relative_path] = (f_type, permissions) except FileNotFoundError: continue return result"},{"question":"**Objective:** Create a visual representation of a dataset demonstrating the use of various seaborn properties. Problem Statement You are provided with a dataset that includes information about various products, their categories, sales figures, and ratings. Your task is to visualize the dataset using `seaborn.objects` module, showcasing different properties such as color, alpha, and pointsize. **Dataset Schema:** - `Product_ID` (int): Unique identifier for each product. - `Category` (str): Category to which the product belongs. - `Sales` (float): Sales figure for the product in dollars. - `Rating` (float): Customer rating for the product on a scale of 1 to 5. **Requirements:** 1. Plot a scatter plot where: - `x` coordinate represents `Sales`. - `y` coordinate represents `Rating`. - Each point\'s `color` represents the `Category`. 2. The points should have varying sizes based on the `Sales` figures. Higher sales should correspond to larger point sizes. 3. Apply a transparency/alpha level to the points based on the `Rating`. Higher ratings should correspond to higher opacity. 4. Customize the plot to make the axes, background, and other aesthetics visually pleasing. **Input:** A Pandas DataFrame named `df` with the schema described above. **Output:** A seaborn plot visualizing the dataset with the specified properties. **Constraints:** - The dataset will have at least 50 rows. - All numeric values will be positive. - Assume appropriate libraries are imported. **Performance Requirements:** - The visualization should be generated efficiently without excessive computational overhead. **Code Skeleton:** ```python import numpy as np import pandas as pd import seaborn.objects as so from seaborn import axes_style, color_palette # Sample data (You will be provided with a dataset, here is just an example) data = { \'Product_ID\': range(1, 101), \'Category\': np.random.choice([\'Electronics\', \'Furniture\', \'Clothing\'], 100), \'Sales\': np.random.uniform(100, 10000, 100), \'Rating\': np.random.uniform(1, 5, 100) } # Create DataFrame df = pd.DataFrame(data) # Define the plot plot = ( so.Plot(df, x=\'Sales\', y=\'Rating\', color=\'Category\') .add(so.Dot(), size=\'Sales\', alpha=\'Rating\') .scale(size=so.Continuous().legend(), alpha=so.Continuous().legend()) .theme({ **axes_style(\\"whitegrid\\"), \\"axes.spines.left\\": False, \\"axes.spines.top\\": False, \\"xtick.labelsize\\": 12, \\"axes.xmargin\\": .015, \\"ytick.labelsize\\": 12, }) ) # Show the plot plot.show() ``` Your task is to complete the code skeleton by implementing the full code to load the dataset and generate the required plot. You can use seaborn\'s flexibility to enhance the plot further as you see fit. **Note:** You can assume that seaborn, pandas, and other required libraries are already installed in the execution environment.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample data (You will be provided with a dataset, here is just an example) np.random.seed(42) data = { \'Product_ID\': range(1, 101), \'Category\': np.random.choice([\'Electronics\', \'Furniture\', \'Clothing\'], 100), \'Sales\': np.random.uniform(100, 10000, 100), \'Rating\': np.random.uniform(1, 5, 100) } # Create DataFrame df = pd.DataFrame(data) # Define the plot using seaborn plt.figure(figsize=(10, 6)) # Create scatter plot with different properties scatter_plot = sns.scatterplot( data=df, x=\'Sales\', y=\'Rating\', hue=\'Category\', size=\'Sales\', sizes=(20, 200), alpha=0.6, palette=\'muted\' ) # Customize plot scatter_plot.set_title(\\"Product Sales vs Rating by Category\\", fontsize=16) scatter_plot.set_xlabel(\\"Sales ()\\", fontsize=14) scatter_plot.set_ylabel(\\"Rating\\", fontsize=14) scatter_plot.legend(title=\'Category\', loc=\'upper right\', bbox_to_anchor=(1.15, 1)) Displaying the plot plt.grid(True) plt.show()"},{"question":"# Cryptographic Application: Secure Message Digest System Objective: Design a Python function that generates a secure digest of a given message, authenticates it using a secret key, and then verifies the integrity of the message using the generated digest. This will test your understanding of hashlib, hmac, and secrets modules for cryptographic operations. Function Signature: ```python def secure_digest_system(message: str, key: str) -> bool: pass ``` Description: 1. **Input**: - `message`: A string representing the message to be hashed. - `key`: A string representing the secret key used for keyed hashing (HMAC). 2. **Output**: - A boolean value `True` if the message verification is successful, otherwise `False`. 3. **Steps**: - **Random Salt Generation**: Use the `secrets` module to generate a secure random salt of 16 bytes. - **Hash Message**: Hash the original message concatenated with the salt using the SHA-256 algorithm from the `hashlib` module. - **Create HMAC**: Generate an HMAC using the hashed message, the provided key, and the SHA-256 algorithm from the `hmac` module. - **Verify Integrity**: Write a verification step that re-computes the HMAC for the given message and compares it with the original HMAC to ensure message integrity. Constraints: - Use only the `hashlib`, `hmac`, and `secrets` modules from the Python Standard Library. Example Usage: ```python message = \\"This is a secret message\\" key = \\"supersecretkey\\" # Call the function result = secure_digest_system(message, key) # Output would be True if the message has been verified correctly. print(result) # Expected: True or False based on message integrity ``` Notes: - Ensure that the function is secure against common cryptographic attacks. - Performance considerations: The function should complete in a reasonable time frame for typical short message lengths (e.g., under 1 second for a message of 100 characters).","solution":"import hashlib import hmac import secrets def secure_digest_system(message: str, key: str) -> bool: Generate a secure digest of a given message using a provided key and verify its integrity. Args: message (str): The message to hash. key (str): The secret key for HMAC. Returns: bool: True if the message verification is successful, False otherwise. # Step 1: Generate a random salt salt = secrets.token_bytes(16) # Step 2: Hash the original message with the salt hasher = hashlib.sha256() hasher.update(message.encode(\'utf-8\') + salt) hashed_message = hasher.digest() # Step 3: Generate HMAC of the hashed message using the key hmac_obj = hmac.new(key.encode(\'utf-8\'), hashed_message, hashlib.sha256) digest = hmac_obj.digest() # Step 4: Verification step # Recompute the HMAC to ensure message integrity recomputed_hmac = hmac.new(key.encode(\'utf-8\'), hashed_message, hashlib.sha256).digest() return hmac.compare_digest(digest, recomputed_hmac)"},{"question":"**Problem Statement:** You are required to implement a Telnet client that connects to a given host, logs in using provided credentials, and executes a set of commands. The client will read the output of each command and return the results. Specifically, you need to handle the login prompt, password prompt, and distinguish between successful and unsuccessful login attempts. If the login is successful, the client should execute the specified commands and return their outputs combined in a single string. If the login fails, it should return an appropriate error message. **Requirements:** 1. **Function Signature:** ```python def telnet_client(host: str, username: str, password: str, commands: List[str], timeout: int = 30) -> str: ``` - `host`: The hostname or IP address of the Telnet server. - `username`: The username for login. - `password`: The password for login. - `commands`: A list of commands to be executed after login. - `timeout`: The maximum waiting time for connection and read operations (default is 30 seconds). 2. **Functionality:** - Establish a Telnet connection to the `host` with the specified `timeout`. - Login using the `username` and `password`. - Execute each command in `commands` list sequentially. - Capture the output of each command and combine them into a single string to be returned. - If the login fails (e.g., incorrect credentials or timeout), return the string `\\"Login failed\\"`. 3. **Constraints:** - You must use the `telnetlib` module for Telnet operations. - Handle potential `EOFError` and other exceptions gracefully, returning appropriate messages (e.g., `\\"Connection closed unexpectedly\\"`). **Example:** ```python host = \\"localhost\\" username = \\"test_user\\" password = \\"test_pass\\" commands = [\\"echo Hello\\", \\"ls\\"] timeout = 10 result = telnet_client(host, username, password, commands, timeout) print(result) ``` **Expected Output:** The function should output the combined results of the commands if login is successful or an error message otherwise.","solution":"import telnetlib from typing import List def telnet_client(host: str, username: str, password: str, commands: List[str], timeout: int = 30) -> str: try: tn = telnetlib.Telnet(host, timeout=timeout) tn.read_until(b\'login: \') tn.write(username.encode(\'ascii\') + b\'n\') tn.read_until(b\'Password: \') tn.write(password.encode(\'ascii\') + b\'n\') login_result = tn.read_until(b\' \', timeout=timeout) if b\'incorrect\' in login_result.lower(): return \\"Login failed\\" result = \\"\\" for command in commands: tn.write(command.encode(\'ascii\') + b\'n\') result += tn.read_until(b\' \', timeout=timeout).decode(\'ascii\') tn.close() return result.strip() except EOFError: return \\"Connection closed unexpectedly\\" except Exception as e: return str(e)"},{"question":"Objective You are required to demonstrate your understanding of the `seaborn` library, specifically the `diverging_palette` function, by generating and customizing diverging color palettes. This will test your ability to set up and customize different components of color palettes crucial for representing data effectively. Task Write a function `create_custom_palettes` that generates three specific diverging color palettes using the `seaborn.diverging_palette` function and saves them as images. Specifications: 1. **Input**: - There is no input for this function. 2. **Output**: - The function should save three diverging color palettes as images: 1. `palette1.png`: A diverging palette from blue to red (standard). 2. `palette2.png`: A diverging palette from magenta to green with 50% saturation and a dark center. 3. `palette3.png`: A diverging palette from blue to red around a light center, with increased separation and as a continuous colormap. Requirements: - Use `seaborn.diverging_palette` for generating palettes. - Use `matplotlib.pyplot` to create and save the color palette images. - Ensure all images are saved in the current working directory. Constraints: - Implement all customization options specified for `palette2` and `palette3` accurately. - The images should be visually distinguishable with the specified attributes. Example Usage: ```python # The function should be callable without any arguments create_custom_palettes() # Upon execution, three images should be saved: # - \'palette1.png\' # - \'palette2.png\' # - \'palette3.png\' ``` # Implementation Notes: - Ensure to import necessary libraries (`seaborn` and `matplotlib.pyplot`) within the function. - You can utilize any method to visualize and save the palettes, such as displaying the palette with `sns.palplot` and saving the figure.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palettes(): Creates three specific diverging color palettes and saves them as images. # Create a diverging palette from blue to red (standard) palette1 = sns.diverging_palette(240, 10, n=10) sns.palplot(palette1) plt.savefig(\'palette1.png\') plt.clf() # Create a diverging palette from magenta to green with 50% saturation and a dark center palette2 = sns.diverging_palette(275, 150, s=50, l=40, n=10) sns.palplot(palette2) plt.savefig(\'palette2.png\') plt.clf() # Create a diverging palette from blue to red around a light center, with increased separation and as a continuous colormap palette3 = sns.diverging_palette(240, 10, sep=50, as_cmap=True) fig, ax = plt.subplots(figsize=(8, 1)) fig.subplots_adjust(bottom=0.5) cb = plt.colorbar(plt.cm.ScalarMappable(cmap=palette3), cax=ax, orientation=\'horizontal\') plt.savefig(\'palette3.png\') plt.clf()"},{"question":"# Python Coding Assessment Question **Objective**: Implement a custom completion function to simulate the basic behavior of Python\'s `rlcompleter` module, specifically the `Completer.complete` method. Task Create a class `CustomCompleter` with a method `complete` that mimics the basic completion feature of the `rlcompleter` module based on the description below: 1. **Class**: `CustomCompleter` 2. **Method**: `complete(self, text: str, state: int) -> Union[str, None]` - **Input**: - `text`: A string containing the text to be completed. - `state`: An integer specifying which completion to return, starting from `0`. - **Output**: - Returns the `state`-th completion for the given `text`. - If no more completions are found, return `None`. - **Constraints and Functionality**: - If `text` does not contain a period (`\'.\'`), complete from current global variables, built-in functions, and Python keywords. - If `text` contains a period, consider it as a dotted name and try to complete using readily evaluable parts. - Handle exceptions gracefully during evaluation and introspection. - **Performance**: - Assume the text length to complete will not exceed 100 characters. - The state will be a small non-negative integer. Example: ```python import keyword import builtins class CustomCompleter: def __init__(self, namespace): self.namespace = namespace def complete(self, text: str, state: int) -> Union[str, None]: # Implementation here pass # Example usage: namespace = {\'my_var\': 10, \'my_func\': lambda x: x + 1} completer = CustomCompleter(namespace) assert completer.complete(\'my_v\', 0) == \'my_var\' assert completer.complete(\'my_v\', 1) == None import math namespace[\'math\'] = math assert completer.complete(\'math.si\', 0) == \'math.sin\' assert completer.complete(\'math.si\', 1) == \'math.sinh\' or None # Depending on the state provided ``` You may use the built-in `dir()`, `getattr()`, and `keyword` modules to facilitate the completion logic in your implementation.","solution":"import keyword import builtins class CustomCompleter: def __init__(self, namespace): self.namespace = namespace self.text = None self.matches = [] def complete(self, text: str, state: int): if state == 0: self.text = text self.matches = self.global_matches(text) if \'.\' not in text else self.attr_matches(text) try: return self.matches[state] except IndexError: return None def global_matches(self, text): matches = [] n = len(text) for word in keyword.kwlist: if word.startswith(text): matches.append(word) for word in dir(builtins): if word.startswith(text): matches.append(word) for word in self.namespace: if word.startswith(text): matches.append(word) return matches def attr_matches(self, text): matches = [] try: object, attr = text.rsplit(\'.\', 1) object = eval(object, self.namespace) for word in dir(object): if word.startswith(attr): matches.append(f\\"{text.rsplit(\'.\', 1)[0]}.{word}\\") except Exception: pass return matches"},{"question":"Question: Custom Yearly Calendar Formatter # Objective Implement a function that generates a custom formatted string representing a calendar for a given year and width of months per row. Additionally, highlight all Mondays that fall on the 1st of the month as special days. # Function Signature ```python def custom_yearly_calendar(year: int, width: int = 3) -> str: pass ``` # Input - `year` (int): The year for which the calendar should be generated. - `width` (int, optional): Number of months per row in the calendar (default is 3). # Output - Returns a string representing the yearly calendar formatted in a special way: - Each month is represented as a matrix of weeks, where each week contains tuples of the form `(day, is_special)`. - `day` is an integer representing the day of the month, with `0` for days outside the current month. - `is_special` is a boolean indicating whether the day is a Monday falling on the 1st of the month. # Constraints - The year should be between 1 and 9999, inclusive. - The width should be a positive integer. # Example ```python print(custom_yearly_calendar(2023, 3)) ``` Expected output format (simplified example): ``` January [(0, False), (0, False), (0, False), (0, False), (1, False), (2, False), (3, False)] [(4, False), (5, False), (6, False), (7, False), (8, False), (9, False), (10, False)] ... February [(1, True), (2, False), (3, False), (4, False), (5, False), (6, False), (7, False)] [(8, False), (9, False), (10, False), (11, False), (12, False), (13, False), (14, False)] ... ... ``` # Notes - Use the `Calendar` class from the `calendar` module to generate the calendar structure. - Determine if a day is special by using the `weekday` method of the `calendar` module. - Make sure that the calendar is formatted correctly with the given width of months per row. - Handle edge cases such as leap years accordingly. # Implementation Details The implementation should: 1. Instantiate a `calendar.Calendar` object. 2. Iterate through each month of the year using methods provided by the `calendar` module. 3. Collect and format the data into the specified format. 4. Highlight Mondays that are the 1st of the month as special days. 5. Ensure the output string is well-formatted and easy to read.","solution":"import calendar def custom_yearly_calendar(year: int, width: int = 3) -> str: cal = calendar.Calendar(firstweekday=0) months = [calendar.month_name[i] for i in range(1, 13)] result = \\"\\" for m in range(1, 13, width): for row in range(7): # Maximum of 6 rows and 1 row for month names for w in range(width): if m + w <= 12: month_days = cal.monthdayscalendar(year, m + w) if row == 0: result += months[m + w - 1].center(20) else: try: week = month_days[row - 1] formatted_week = [ (day, day == 1 and calendar.weekday(year, m + w, day) == 0) if day != 0 else (0, False) for day in week ] result += str(formatted_week).ljust(20) except IndexError: result += \\" \\" * 20 result += \\"n\\" result += \\"n\\" return result.strip()"},{"question":"Overview You are tasked with developing a Python script that performs various file operations, data manipulations, and validations using different standard library modules. This will test your understanding of file handling, working with command line arguments, data processing, and quality control using Python\'s standard library. Requirements 1. **File Operations** - The script should accept a directory path and a filename pattern from the command line using the `argparse` module. - Search for files in the specified directory that match the provided filename pattern using the `glob` module. - Create a copy of each found file into a new directory named `backup` using the `shutil` module. 2. **Data Processing** - Process text files found during the search: - For each file, read its content. - Find all lines matching a specific regular expression pattern given via another command-line argument using the `re` module. - Compile the results into a dictionary where the keys are the filenames and values are lists of matched lines. 3. **Statistics Calculation** - For each list of matched lines per file, calculate the following statistics using the `statistics` module: - The mean and median lengths of the lines. - The count of lines. 4. **Output Result** - Print out the statistics for each file in a readable format. - Also, save these statistics in a JSON file named `summary.json` in the current directory. 5. **Quality Control** - Write and include at least three `unittest` test cases to validate the correctness of the major functions you implemented. Constraints - Use Python 3.10 and appropriate standard library modules. - Ensure the script handles errors gracefully, such as directory not found or invalid file patterns. - The script should not use external packages beyond the standard library. Example Command Line Usage ```shell python data_processor.py --directory=\\"./logs\\" --pattern=\\"*.log\\" --regex=\\"ERROR.*\\" ``` Example Output ```plaintext Processing file: error_log1.log Mean length of matched lines: 45.6 Median length of matched lines: 43 Total matched lines: 10 Processing file: error_log2.log Mean length of matched lines: 50.1 Median length of matched lines: 48 Total matched lines: 8 Summary has been saved to summary.json ``` Tips - Structure your code with functions to handle different tasks: file search, data processing, statistics calculation, and result output. - Ensure your `unittest` cases cover scenarios like no files found, no matches for the regex, and correct statistics calculation.","solution":"import argparse import os import shutil import glob import re import json from statistics import mean, median from typing import Dict, List def copy_files(src_dir: str, pattern: str, dest_dir: str = \'backup\') -> List[str]: os.makedirs(dest_dir, exist_ok=True) files_to_copy = glob.glob(os.path.join(src_dir, pattern)) for file in files_to_copy: shutil.copy(file, dest_dir) return files_to_copy def process_files(files: List[str], regex_pattern: str) -> Dict[str, List[str]]: matched_lines = {} pattern = re.compile(regex_pattern) for file in files: with open(file, \'r\') as f: lines = f.readlines() matches = [line.strip() for line in lines if pattern.search(line)] matched_lines[os.path.basename(file)] = matches return matched_lines def calculate_statistics(data: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]: statistics_results = {} for file, lines in data.items(): lengths = [len(line) for line in lines] if lengths: stats = { \\"mean\\": mean(lengths), \\"median\\": median(lengths), \\"count\\": len(lengths) } else: stats = { \\"mean\\": 0, \\"median\\": 0, \\"count\\": 0 } statistics_results[file] = stats return statistics_results def save_statistics_to_json(statistics: Dict[str, Dict[str, float]], output_file: str = \'summary.json\'): with open(output_file, \'w\') as f: json.dump(statistics, f, indent=4) def main(): parser = argparse.ArgumentParser(description=\\"Process some logs.\\") parser.add_argument(\'--directory\', required=True, help=\\"Directory to search the files in.\\") parser.add_argument(\'--pattern\', required=True, help=\\"Filename pattern to search for.\\") parser.add_argument(\'--regex\', required=True, help=\\"Regular expression to match lines.\\") args = parser.parse_args() try: files_found = copy_files(args.directory, args.pattern) if not files_found: print(\\"No files found with the given pattern.\\") return matched_lines = process_files(files_found, args.regex) statistics_results = calculate_statistics(matched_lines) for file, stats in statistics_results.items(): print(f\\"Processing file: {file}\\") print(f\\"Mean length of matched lines: {stats[\'mean\']}\\") print(f\\"Median length of matched lines: {stats[\'median\']}\\") print(f\\"Total matched lines: {stats[\'count\']}n\\") save_statistics_to_json(statistics_results) print(\\"Summary has been saved to summary.json\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: MIME Types Mapping Suppose you are working on a web server that serves files based on their MIME types. You are required to implement a function to initialize MIME type mappings, add custom mappings, and retrieve extensions for a given MIME type. Implement the following functionalities using the `mimetypes` module: # Function Definitions 1. **initialize_mime_types(custom_files: list = None) -> None** This function initializes the MIME types. If `custom_files` is provided, it should augment the default type map with the provided files. - **Parameters**: - `custom_files` (list, optional): A list of file paths to MIME type maps. Defaults to None. - **Returns**: - `None` 2. **add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None** This function adds a custom MIME type to the MIME type mappings. - **Parameters**: - `mime_type` (str): The MIME type to be added. - `extension` (str): The associated file extension (e.g., \'.ext\'). - `strict` (bool, optional): Whether to add to the list of official types. Defaults to True. - **Returns**: - `None` 3. **get_extensions_for_mime_type(mime_type: str, strict: bool = True) -> list** This function returns a list of all possible extensions associated with a given MIME type. - **Parameters**: - `mime_type` (str): The MIME type for which extensions are to be fetched. - `strict` (bool, optional): Whether to restrict results to only the official types. Defaults to True. - **Returns**: - `list`: A list of possible extensions for the given MIME type. # Constraints - You may assume that all files provided in `custom_files` exist and are readable. - The custom MIME types and extensions added via `add_custom_mime_type` should be available for extension retrieval. # Example Usage ```python # Initialize with default MIME types initialize_mime_types() # Add a custom MIME type add_custom_mime_type(\'application/x-example\', \'.ex\') # Retrieve extensions for a given MIME type extensions = get_extensions_for_mime_type(\'application/x-example\') print(extensions) # Expected Output: [\'.ex\'] # Initialize with custom MIME type files custom_files = [\'path/to/custom_mime.types\'] initialize_mime_types(custom_files) # Continue using the API as normal ``` # Notes - Ensure that the internal state is handled correctly with each function called. - Use the mimetypes module functions wherever applicable. - The functions should be efficient and handle typical usage scenarios for web server MIME type management.","solution":"import mimetypes def initialize_mime_types(custom_files=None): Initializes the MIME types. If custom_files is provided, it augments the default type map with the provided files. Parameters: custom_files (list, optional): A list of file paths to MIME type maps. Defaults to None. Returns: None mimetypes.init() if custom_files: for file in custom_files: mimetypes.read_mime_types(file) def add_custom_mime_type(mime_type, extension, strict=True): Adds a custom MIME type to the MIME type mappings. Parameters: mime_type (str): The MIME type to be added. extension (str): The associated file extension (e.g., \'.ext\'). strict (bool, optional): Whether to add to the list of official types. Defaults to True. Returns: None mimetypes.add_type(mime_type, extension, strict) def get_extensions_for_mime_type(mime_type, strict=True): Returns a list of all possible extensions associated with a given MIME type. Parameters: mime_type (str): The MIME type for which extensions are to be fetched. strict (bool, optional): Whether to restrict results to only the official types. Defaults to True. Returns: list: A list of possible extensions for the given MIME type. return mimetypes.guess_all_extensions(mime_type, strict)"},{"question":"# Problem Description You are tasked with simulating a simple producer-consumer pattern using `asyncio` primitives. There are two types of tasks: 1. **Producer Tasks**: These tasks produce items and store them in a shared buffer. 2. **Consumer Tasks**: These tasks consume items from the shared buffer. You must ensure that the following conditions are met: - Producers must wait if the buffer is full before adding more items. - Consumers must wait if the buffer is empty before consuming items. - Synchronization should be achieved using `asyncio.Lock` and `asyncio.Event`. # Requirements 1. Implement the `Buffer` class which should allow for item production and consumption with the following methods: - `add_item(self, item: Any) -> None`: Adds an item to the buffer. Waits if the buffer is full. - `get_item(self) -> Any`: Removes and returns an item from the buffer. Waits if the buffer is empty. 2. Each method should be a coroutine that respects the conditions mentioned above using proper synchronization primitives. 3. Demonstrate the `Buffer` class with an example setup of multiple producers and consumers using asyncio tasks. Print messages to track the production and consumption of items. # Class Definition ```python import asyncio from typing import Any class Buffer: def __init__(self, max_size: int): Initialize the buffer with the given max size. Args: - max_size (int): The maximum number of items the buffer can hold. self.max_size = max_size self.buffer = [] self.lock = asyncio.Lock() self.not_full = asyncio.Event() self.not_empty = asyncio.Event() self.not_full.set() # Initially, buffer is not full async def add_item(self, item: Any) -> None: Add an item to the buffer. If the buffer is full, wait until there is space. Args: - item (Any): The item to be added to the buffer. async with self.lock: while len(self.buffer) >= self.max_size: self.not_full.clear() await self.not_full.wait() self.buffer.append(item) self.not_empty.set() async def get_item(self) -> Any: Remove and return an item from the buffer. If the buffer is empty, wait until there is an item. Returns: - Any: The item removed from the buffer. async with self.lock: while not self.buffer: self.not_empty.clear() await self.not_empty.wait() item = self.buffer.pop(0) self.not_full.set() return item async def producer(buffer: Buffer, id: int): for i in range(5): item = f\\"item{id}.{i}\\" await buffer.add_item(item) print(f\\"Producer {id} added {item}\\") await asyncio.sleep(0.1) # Simulate production time async def consumer(buffer: Buffer, id: int): for i in range(5): item = await buffer.get_item() print(f\\"Consumer {id} got {item}\\") await asyncio.sleep(0.2) # Simulate consumption time async def main(): buffer = Buffer(max_size=5) producers = [asyncio.create_task(producer(buffer, i)) for i in range(2)] consumers = [asyncio.create_task(consumer(buffer, i)) for i in range(2)] await asyncio.gather(*producers) await asyncio.gather(*consumers) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Constraints - The buffer size will be a positive integer. - There will be a fixed number of producers and consumers. - Each producer produces exactly 5 items. - Each consumer consumes exactly 5 items. # Output - The output should clearly show the order of items being produced and consumed, demonstrating the proper synchronization.","solution":"import asyncio from typing import Any, List class Buffer: def __init__(self, max_size: int): Initialize the buffer with the given max size. Args: - max_size (int): The maximum number of items the buffer can hold. self.max_size = max_size self.buffer: List[Any] = [] self.lock = asyncio.Lock() self.not_full = asyncio.Event() self.not_empty = asyncio.Event() self.not_full.set() # Initially, buffer is not full async def add_item(self, item: Any) -> None: Add an item to the buffer. If the buffer is full, wait until there is space. Args: - item (Any): The item to be added to the buffer. async with self.lock: while len(self.buffer) >= self.max_size: self.not_full.clear() await self.not_full.wait() self.buffer.append(item) self.not_empty.set() async def get_item(self) -> Any: Remove and return an item from the buffer. If the buffer is empty, wait until there is an item. Returns: - Any: The item removed from the buffer. async with self.lock: while not self.buffer: self.not_empty.clear() await self.not_empty.wait() item = self.buffer.pop(0) self.not_full.set() return item async def producer(buffer: Buffer, id: int): for i in range(5): item = f\\"item{id}.{i}\\" await buffer.add_item(item) print(f\\"Producer {id} added {item}\\") await asyncio.sleep(0.1) # Simulate production time async def consumer(buffer: Buffer, id: int): for i in range(5): item = await buffer.get_item() print(f\\"Consumer {id} got {item}\\") await asyncio.sleep(0.2) # Simulate consumption time async def main(): buffer = Buffer(max_size=5) producers = [asyncio.create_task(producer(buffer, i)) for i in range(2)] consumers = [asyncio.create_task(consumer(buffer, i)) for i in range(2)] await asyncio.gather(*producers) await asyncio.gather(*consumers) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Title: Advanced Python Statements and Control Flow Objective: Assess the ability to use various Python simple statements effectively, including assignment types, variable scoping with `global` and `nonlocal`, and control flow statements. Students should demonstrate their understanding by implementing a function that orchestrates these elements correctly. Problem Description: You are required to implement a function `manipulate_data(data: list, operations: list) -> dict` that performs various operations on the input list based on the provided operations list. Each operation is detailed in the operations list and can include different types of assignments, scoping adjustments using `global` and `nonlocal`, and control flow directives. Expected Input and Output: - **Input:** - `data`: A list of integers. - `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is the operation type (string) and subsequent elements are the required arguments for the operation. - **Output:** - A dictionary with keys: `\\"modified_data\\"` and `\\"results\\"`. - `\\"modified_data\\"`: The altered `data` list after all operations. - `\\"results\\"`: A list containing results of each operation (if applicable). Constraints: - Operations can include: - `\\"increment(index, value)\\"`: Increment the value at `data[index]` by `value`. - `\\"decrement(index, value)\\"`: Decrement the value at `data[index]` by `value`. - `\\"assign_global(variable_name, value)\\"`: Assign the value to a global variable with `variable_name`. - `\\"assign_nonlocal(variable_name, value)\\"`: Assign the value to a nonlocal variable within a nested function scope. - `\\"assert(expression, error_message)\\"`: Assert the given expression with an optional error message. - `\\"return(expression)\\"`: Return the evaluated expression. - The operations will be valid within the context (no run-time errors expected on valid input). Example: ```python def manipulate_data(data, operations): results = [] # Define possible global and nonlocal variables global_var = 0 def outer_scope(): nonlocal_var = 0 def inner_scope(ops): nonlocal nonlocal_var for op in ops: if op[0] == \\"increment\\": data[op[1]] += op[2] elif op[0] == \\"decrement\\": data[op[1]] -= op[2] elif op[0] == \\"assign_global\\": global global_var global_var = op[2] elif op[0] == \\"assign_nonlocal\\": nonlocal_var = op[2] elif op[0] == \\"assert\\": assert eval(op[1]), op[2] if len(op) > 2 else \\"Assertion Failed\\" elif op[0] == \\"return\\": results.append(eval(op[1])) inner_scope(operations) outer_scope() return {\\"modified_data\\": data, \\"results\\": results} # Example usage: data = [1, 2, 3] operations = [ (\\"increment\\", 0, 5), (\\"decrement\\", 1, 1), (\\"assign_global\\", \\"global_var\\", 10), (\\"assert\\", \\"data[0] == 6\\", \\"Increment failed\\"), (\\"return\\", \\"data[1] - 3\\"), ] result = manipulate_data(data, operations) print(result) # Output should be: {\\"modified_data\\": [6, 1, 3], \\"results\\": [-2]} ``` Notes: - Pay careful attention to variable scoping rules and the behavior of mutable objects. - Ensure proper implementation of control flow within nested functions.","solution":"def manipulate_data(data, operations): results = [] # Define possible global and nonlocal variables global_var = 0 def outer_scope(): nonlocal_var = 0 def inner_scope(ops): nonlocal nonlocal_var for op in ops: if op[0] == \\"increment\\": data[op[1]] += op[2] elif op[0] == \\"decrement\\": data[op[1]] -= op[2] elif op[0] == \\"assign_global\\": global global_var global_var = op[2] elif op[0] == \\"assign_nonlocal\\": nonlocal_var = op[2] elif op[0] == \\"assert\\": assert eval(op[1]), op[2] if len(op) > 2 else \\"Assertion Failed\\" elif op[0] == \\"return\\": results.append(eval(op[1])) inner_scope(operations) outer_scope() return {\\"modified_data\\": data, \\"results\\": results}"},{"question":"# Comprehensive Python310 Coding Assessment Problem Statement: You are required to use the `concurrent.futures` module to implement a function that processes a list of numbers to determine whether they are prime. The function should handle the task in parallel using both `ThreadPoolExecutor` and `ProcessPoolExecutor`. Function Definitions: 1. `check_prime_threadpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]` 2. `check_prime_processpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]` Requirements: - Both functions should accept a list of numbers (`numbers`) and the maximum number of worker threads/processes (`max_workers`). - Each function should return a list of tuples, where each tuple contains a number and a boolean indicating whether it is prime. - Use `ThreadPoolExecutor` for `check_prime_threadpool` and `ProcessPoolExecutor` for `check_prime_processpool`. - Handle potential exceptions that may occur during the execution of tasks. - Ensure proper shutdown of executors by using the `with` statement. Example: ```python from typing import List, Tuple def check_prime_threadpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]: # Implementation here... pass def check_prime_processpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]: # Implementation here... pass # Example of numbers to check for primes numbers = [29, 15, 18, 47, 77, 93, 97] # Example usage print(check_prime_threadpool(numbers, max_workers=5)) print(check_prime_processpool(numbers, max_workers=5)) ``` Constraints: - Maximum value in `numbers` should not exceed (10^6). - The `max_workers` should be a positive integer. - Consider potential performance benefits between threading and multiprocessing for different sizes of the input list. Evaluation Criteria: - Correct implementation of threading and multiprocessing using `concurrent.futures`. - Accurate detection of prime numbers. - Proper handling and reporting of exceptions. - Clean and efficient code following Python best practices. Good luck!","solution":"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from typing import List, Tuple def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def check_prime_threadpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]: results = [] with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(is_prime, number): number for number in numbers} for future in future_to_number.keys(): try: result = future.result() results.append((future_to_number[future], result)) except Exception as e: results.append((future_to_number[future], False)) return results def check_prime_processpool(numbers: List[int], max_workers: int) -> List[Tuple[int, bool]]: results = [] with ProcessPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(is_prime, number): number for number in numbers} for future in future_to_number.keys(): try: result = future.result() results.append((future_to_number[future], result)) except Exception as e: results.append((future_to_number[future], False)) return results"},{"question":"# Custom Object Manager Using Reference Counting You are asked to write a custom object manager in Python, leveraging the reference counting functions described in the documentation provided. Task: 1. **Implement a class `CustomObjectManager`**. This class should: - **Initialize with a list of Python objects.** - **Manage the reference counts of these objects using the provided reference counting functions.** 2. Your class should provide the following methods: - `add_object(obj)`: Add a new object to the manager and handle its reference count appropriately. - `remove_object(obj)`: Remove an object from the manager, carefully handling its reference count. - `get_objects()`: Return a list of current managed objects. 3. **Constraints**: - You may assume that the objects are of any type that supports reference counting. - The manager should properly handle null objects where appropriate. Example Usage: ```python # Example of how the CustomObjectManager class would be used: manager = CustomObjectManager([]) # Adding objects manager.add_object(obj1) manager.add_object(obj2) # Listing objects print(manager.get_objects()) # Removing objects manager.remove_object(obj1) print(manager.get_objects()) ``` Requirements: - Your implementation should ensure that reference counting is correctly managed to avoid memory leaks or premature deallocation of objects. - Handle exceptions properly where necessary (e.g., trying to remove an object that does not exist). Evaluation: Your solution will be evaluated based on: - Correctness: Proper handling of reference counting. - Efficiency: No unnecessary reference increments or decrements. - Code quality: Clear, concise, and well-documented code. Implement the class `CustomObjectManager` below. ```python # Implement your class here. ```","solution":"import sys class CustomObjectManager: def __init__(self, objects): self._objects = [] # Managing a list of objects for obj in objects: self.add_object(obj) def add_object(self, obj): if obj not in self._objects: self._objects.append(obj) sys._debug_info = f\\"Added object: {obj}, Reference count: {sys.getrefcount(obj)}\\" def remove_object(self, obj): if obj in self._objects: self._objects.remove(obj) sys._debug_info = f\\"Removed object: {obj}, Reference count before decrementation: {sys.getrefcount(obj)}\\" def get_objects(self): return self._objects"},{"question":"**Coding Assessment Question** **Problem Statement:** You are tasked with creating a utility script for managing backups of project directories. The script should be able to create a backup of a given directory as a compressed archive (in tar.gz format) and also restore from such a backup. You need to implement two functions for this purpose: 1. `create_backup(source_dir: str, backup_name: str) -> str` 2. `restore_backup(backup_path: str, restore_dir: str) -> None` # Function Details: **Function 1:** `create_backup(source_dir: str, backup_name: str) -> str` - **Input:** - `source_dir` (str): The path to the directory that needs to be backed up. - `backup_name` (str): The base name for the backup archive (should not include an extension). - **Output:** - Returns the full path to the created archive file. - **Behavior:** - The function should create a compressed tar archive of the `source_dir` and name it `backup_name.tar.gz`. - Be sure to handle cases where the source directory does not exist. - Use the `shutil.make_archive` function to create the archive. **Function 2:** `restore_backup(backup_path: str, restore_dir: str) -> None` - **Input:** - `backup_path` (str): The path to the backup archive that needs to be restored. - `restore_dir` (str): The directory where the contents of the backup will be restored. - **Output:** - None - **Behavior:** - The function should extract the contents of the given `backup_path` archive into the `restore_dir`. - Be sure to handle cases where the backup archive does not exist, or the restore directory cannot be written to. - Use the `shutil.unpack_archive` function to extract the archive. # Constraints: - You may assume that you have necessary permissions for the file and directory operations. - The paths provided will be valid and within acceptable limits for your operating system. - Do not perform any actual file I/O outside of the described operations. # Example Usage: ```python # Assume the existence of a directory \'project_dir\' for this example. # Create a backup backup_path = create_backup(\'project_dir\', \'project_backup\') print(f\\"Backup created at: {backup_path}\\") # Restore the backup to a different directory restore_backup(backup_path, \'restored_project_dir\') print(\\"Backup successfully restored.\\") ``` # Additional Notes: - Provide proper exception handling to ensure that errors (e.g., non-existent directories, inaccessible files) are managed gracefully. - Include appropriate docstrings for the functions you implement. - Ensure that the functionalities of both functions are tested in your script.","solution":"import os import shutil def create_backup(source_dir: str, backup_name: str) -> str: Creates a compressed tar archive of the source directory. Args: - source_dir (str): The path of the directory to be backed up. - backup_name (str): The base name for the backup archive (without extension). Returns: - str: The full path to the created archive file. # Ensure the source directory exists if not os.path.isdir(source_dir): raise FileNotFoundError(f\\"The source directory \'{source_dir}\' does not exist.\\") # Create the archive archive_path = shutil.make_archive(backup_name, \'gztar\', source_dir) return archive_path def restore_backup(backup_path: str, restore_dir: str) -> None: Restores the contents of the backup archive to the specified directory. Args: - backup_path (str): The path to the backup archive file. - restore_dir (str): The directory where the archive contents will be restored. Returns: - None # Ensure the backup file exists if not os.path.isfile(backup_path): raise FileNotFoundError(f\\"The backup file \'{backup_path}\' does not exist.\\") # Ensure the restore directory exists or can be created if not os.path.exists(restore_dir): os.makedirs(restore_dir) # Extract the archive shutil.unpack_archive(backup_path, restore_dir)"},{"question":"# Python Development Mode and Resource Management with Asyncio Background In Python, resource management and proper handling of asynchronous tasks are crucial to build robust applications. Python Development Mode helps to detect issues like resource leaks through warnings and provides additional debugging features. Your task is to write a Python program that reads a list of URLs from a file, fetches data from those URLs asynchronously, and saves the content to individual files. You should ensure that all resources (e.g., file handles, network connections) are explicitly managed. The program should be compatible with Python Development Mode, and it should not emit any warnings when run with this mode enabled. Requirements 1. **Input File Format**: The input file (`urls.txt`) contains one URL per line. 2. **Output**: For each URL, create a file named after the URL\'s host with the received content. 3. **Async Fetching**: Use the `aiohttp` library to fetch data from URLs asynchronously. 4. **Explicit Resource Management**: Ensure that all file handles are properly closed using context managers. 5. **Run with Development Mode**: Your solution should be run with `python3 -X dev script.py urls.txt` and should not produce any warnings. Function Signature ```python import asyncio import aiohttp import sys async def fetch_and_save_content(url: str): Fetch the content from the URL and save it to a file named after the host. Arguments: url -- The URL to fetch data from. Output: A file named after the URL\'s host containing the fetched content. pass async def main(input_file: str): Main function to read URLs from a file, fetch content asynchronously, and save it. Arguments: input_file -- The path to the input file with URLs. pass if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <input_file>\\") sys.exit(1) input_file = sys.argv[1] asyncio.run(main(input_file)) ``` Constraints and Assumptions 1. Assume the input URLs are valid and accessible. 2. The program should handle typical network exceptions gracefully. 3. URLs will not contain characters that are illegal in file names. 4. Each URL in the input file is unique. Implementation Notes 1. **Asyncio and aiohttp**: Utilize the `aiohttp` library for asynchronous HTTP requests. 2. **Context Managers**: Use context managers to ensure files are properly closed. 3. **Testing**: Ensure the solution produces no warnings when executed in Python Development Mode. Good luck!","solution":"import asyncio import aiohttp import sys from urllib.parse import urlparse async def fetch_and_save_content(session, url): Fetch the content from the URL and save it to a file named after the URL\'s host. Arguments: session -- The aiohttp client session. url -- The URL to fetch data from. try: async with session.get(url) as response: response.raise_for_status() content = await response.text() host = urlparse(url).hostname if host: filename = f\\"{host}.txt\\" with open(filename, \\"w\\") as f: f.write(content) print(f\\"Saved content from {url} to {filename}\\") except Exception as e: print(f\\"Failed to fetch {url}: {e}\\") async def main(input_file): Main function to read URLs from a file, fetch content asynchronously, and save it. Arguments: input_file -- The path to the input file with URLs. async with aiohttp.ClientSession() as session: with open(input_file) as f: urls = f.read().splitlines() tasks = [fetch_and_save_content(session, url) for url in urls] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <input_file>\\") sys.exit(1) input_file = sys.argv[1] asyncio.run(main(input_file))"},{"question":"# Question: Implement a Comprehensive NNTP Client for Automated Article Retrieval and Posting Your task is to implement a Python class `NNTPClient` that utilizes the `nntplib` module to connect to an NNTP server, retrieve articles from specified newsgroups, decode header information, and post new articles. This assessment will test your understanding of network connections, protocol handling, and exception management using the `nntplib` module. Requirements: 1. **Initialization**: - The `NNTPClient` class should initialize with parameters for the NNTP server hostname, optional port number, optional username, optional password, and optional SSL context. - The class should handle both regular and secure connections (NNTP and NNTP_SSL). - Implement a method `connect` to establish the connection to the NNTP server. - Implement a method `disconnect` to properly close the connection. 2. **Fetching Articles**: - Implement a method `fetch_subjects` that retrieves the subjects of the latest articles from a specified newsgroup. It should: - Accept the newsgroup name and the number of latest articles to retrieve. - Use the `group` and `over` methods from `nntplib` to fetch the required articles. - Return a list of tuples containing the article number and the decoded subject. 3. **Posting Articles**: - Implement a method `post_article` that posts a new article to the specified newsgroup. It should: - Accept the newsgroup name and the article contents, which include both headers and body. - Ensure proper formatting of the article before posting using the `post` method from `nntplib`. - Handle any exceptions related to posting and inform the user of the outcome. 4. **Exception Handling and Robustness**: - The class methods should handle potential exceptions raised by `nntplib` (e.g., `NNTPReplyError`, `NNTPTemporaryError`, `NNTPPermanentError`, etc.) and provide meaningful error messages to the user. - Ensure that the connection is always properly closed even if an error occurs. Input and Output Formats: - **Input**: Parameters for the initialization, methods for fetching and posting articles. - **Output**: Lists of article subjects, successful or error messages for posting articles. Constraints: - The NNTP server you connect to must support the specified newsgroups and allow the operations you are performing. - Ensure proper exception handling to make the client robust and user-friendly. # Example Usage: ```python # Initialize client for secure connection client = NNTPClient(host=\'news.example.com\', port=563, user=\'yourusername\', password=\'yourpassword\', ssl_context=None) # Connect to the server client.connect() # Fetch the subjects of the last 10 articles from the \'comp.lang.python\' newsgroup articles = client.fetch_subjects(\'comp.lang.python\', 10) for article in articles: print(f\\"Article {article[0]}: {article[1]}\\") # Post an article to the \'comp.lang.python\' newsgroup article_content = From: user@example.com Subject: Test Posting Newsgroups: comp.lang.python This is a test post. result = client.post_article(\'comp.lang.python\', article_content) print(result) # Disconnect from the server client.disconnect() ``` Implementation Notes: - Use appropriate `nntplib.NNTP` and `nntplib.NNTP_SSL` methods for handling connections and commands. - Decode headers using the `nntplib.decode_header` function to properly handle non-ASCII characters in subjects. - Ensure proper resource management using context managers (`with` statement) where applicable to handle connections gracefully.","solution":"import nntplib from ssl import create_default_context import logging class NNTPClient: def __init__(self, host, port=None, user=None, password=None, ssl_context=None): self.host = host self.port = port self.user = user self.password = password self.ssl_context = ssl_context if ssl_context else create_default_context() self.connection = None def connect(self): try: if self.port == 563: # Default port for NNTP over SSL self.connection = nntplib.NNTP_SSL(self.host, self.port, user=self.user, password=self.password, ssl_context=self.ssl_context) else: self.connection = nntplib.NNTP(self.host, self.port, user=self.user, password=self.password) logging.info(\\"Connected to NNTP server.\\") except Exception as e: logging.error(f\\"Failed to connect: {e}\\") raise def disconnect(self): if self.connection: self.connection.quit() logging.info(\\"Disconnected from NNTP server.\\") def fetch_subjects(self, newsgroup, num_articles=10): try: resp, count, first, last, name = self.connection.group(newsgroup) start = max(0, int(last) - num_articles + 1) resp, overviews = self.connection.over((start, last)) subjects = [(art_num, nntplib.decode_header(ov[\'subject\'])) for art_num, ov in overviews] return subjects except Exception as e: logging.error(f\\"Failed to fetch subjects: {e}\\") raise def post_article(self, newsgroup, article): try: self.connection.post(article) return \\"Article posted successfully.\\" except Exception as e: logging.error(f\\"Failed to post article: {e}\\") return f\\"Failed to post article: {e}\\""},{"question":"# Custom Array Extension Using Pandas **Objective**: Implement a custom extension array in pandas and demonstrate its usage. # Scenario You are required to implement a custom extension array for pandas that can store and operate on data that represents complex numbers. Complex numbers have a real part and an imaginary part. The goal is to create an array type that enables pandas to handle complex numbers natively in DataFrame and Series objects. # Instructions 1. Define a class `ComplexDtype` which extends `pandas.api.extensions.ExtensionDtype`. This will describe the new data type. 2. Implement a class `ComplexArray` which extends `pandas.api.extensions.ExtensionArray`. This will handle the operations for the complex number array. 3. Implement the following methods in `ComplexArray`: - `__init__`: To initialize the array with a list of complex tuples. - `dtype`: Return the instance of `ComplexDtype`. - `__len__`: Return the length of the array. - `__getitem__`: Fetch complex number at a given index. - `take`: Select elements from the array by index. - `copy`: Create a copy of the complex array. - `astype`: Convert complex array to another dtype. - `isna`: Check for missing values in the array. - `__setitem__`: Set a complex number at a given index. - `values_for_factorize`: Return an array to be used for factorization. - `unique`: Find the unique complex numbers in the array. - Any other methods necessary for efficient array operations. # Example Usage ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray # Define ComplexDtype class ComplexDtype(ExtensionDtype): name = \'complex\' kind = \'O\' type = complex na_value = complex(float(\'nan\'), float(\'nan\')) @classmethod def construct_array_type(cls): return ComplexArray # Define ComplexArray class ComplexArray(ExtensionArray): def __init__(self, data): self.data = data @property def dtype(self): return ComplexDtype() def __len__(self): return len(self.data) def __getitem__(self, idx): if isinstance(idx, int): return self.data[idx] else: return ComplexArray([self.data[i] for i in idx]) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = [self.data[i] if i != -1 else fill_value for i in indices] else: result = [self.data[i] for i in indices] return ComplexArray(result) def copy(self): return ComplexArray(self.data[:]) def astype(self, dtype, copy=True): if dtype == self.dtype: return self.copy() if copy else self else: return pd.array(self.data, dtype=dtype, copy=copy) def isna(self): return [x != x for x in self.data] def __setitem__(self, idx, value): if isinstance(idx, int): self.data[idx] = value else: for i in idx: self.data[i] = value def values_for_factorize(self): return self.data, self.dtype.na_value def unique(self): uniques = list(dict.fromkeys(self.data)) return ComplexArray(uniques) # Create DataFrame with ComplexArray data = ComplexArray([ complex(1, 2), complex(3, 4), complex(1, 2), complex(2, 3) ]) df = pd.DataFrame({ \'complex_data\': data }) print(df) ``` # Constraints: 1. Ensure that the solution is efficient for large datasets. 2. Handle edge cases such as empty arrays and invalid complex number values. **Hints**: - Carefully consider how to handle missing data. - Utilize available methods in `pandas.api.extensions.ExtensionArray` to simplify implementation. - Review pandas documentation on `ExtensionDtype` and `ExtensionArray` for additional context.","solution":"import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray import numpy as np class ComplexDtype(ExtensionDtype): name = \'complex\' kind = \'O\' type = np.complex_ na_value = np.nan + 1j * np.nan @classmethod def construct_array_type(cls): return ComplexArray class ComplexArray(ExtensionArray): def __init__(self, data): self.data = np.asarray(data, dtype=np.complex_) @property def dtype(self): return ComplexDtype() def __len__(self): return len(self.data) def __getitem__(self, idx): if isinstance(idx, int): return self.data[idx] else: return ComplexArray(self.data[idx]) def take(self, indices, allow_fill=False, fill_value=None): result = np.take(self.data, indices, mode=\'wrap\' if not allow_fill else \'clip\') if allow_fill: fill_mask = indices == -1 result[fill_mask] = fill_value if fill_value is not None else self.dtype.na_value return ComplexArray(result) def copy(self): return ComplexArray(self.data.copy()) def astype(self, dtype, copy=True): if dtype == self.dtype: return self.copy() if copy else self elif np.issubdtype(dtype, np.complexfloating): return ComplexArray(self.data.astype(dtype, copy=copy)) else: return np.array(self.data, dtype=dtype, copy=copy) def isna(self): return np.isnan(self.data.real) & np.isnan(self.data.imag) def __setitem__(self, idx, value): if isinstance(idx, int): self.data[idx] = value else: self.data[idx] = value def values_for_factorize(self): return self.data, self.dtype.na_value def unique(self): uniques = np.unique(self.data) return ComplexArray(uniques) # Example Usage data = ComplexArray([complex(1, 2), complex(3, 4), complex(1, 2), complex(2, 3)]) df = pd.DataFrame({ \'complex_data\': data }) print(df)"},{"question":"# Question: Titanic Survival Analysis with Seaborn Boxplots Objective: Write a function that creates multiple customized seaborn boxplots to analyze the Titanic dataset. The function should demonstrate various seaborn capabilities, including grouping, nested grouping, and advanced customizations. Function Signature: ```python def titanic_boxplot_analysis(): pass ``` Task Description: Implement the function `titanic_boxplot_analysis()` that: 1. Loads the Titanic dataset using `seaborn.load_dataset(\\"titanic\\")`. 2. Creates and displays the following boxplots: a. **Boxplot 1**: A single horizontal boxplot of the `age` variable. b. **Boxplot 2**: A vertical boxplot showing the distribution of age grouped by the `class` variable. c. **Boxplot 3**: A vertical boxplot with nested grouping by the `class` and `alive` variables (displayed side by side for each class). d. **Boxplot 4**: A vertical boxplot for the `age` variable grouped by the `deck` variable, covering the full range of the data with the whiskers. e. **Boxplot 5**: A vertical boxplot for the `age` variable grouped by the `class` variable with the following customizations: - Notched boxes. - No caps on the whiskers. - Custom marker for outliers (use \\"x\\"). - Custom color and transparency for the box (RGBA (0.3, 0.5, 0.7, 0.5)). - Custom color and line width for the median line (red color and linewidth of 2). 3. The function should display each boxplot sequentially using `matplotlib.pyplot.show()` after creating each plot. Constraints: - None of the plots should be displayed in an overlapping manner. Ensure each plot is shown before creating the next one. Expected Output: The function should create and display five customized boxplots in sequence, as described above. Example Usage: ```python titanic_boxplot_analysis() ``` When called, this function should load the Titanic dataset and sequentially display the five described boxplots with the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_boxplot_analysis(): # Load Titanic dataset titanic = sns.load_dataset(\'titanic\') # Boxplot 1: Single horizontal boxplot of the age variable plt.figure(figsize=(10, 6)) sns.boxplot(x=\'age\', data=titanic) plt.title(\'Horizontal Boxplot of Age\') plt.show() # Boxplot 2: Vertical boxplot showing the distribution of age grouped by the class variable plt.figure(figsize=(10, 6)) sns.boxplot(x=\'class\', y=\'age\', data=titanic) plt.title(\'Vertical Boxplot of Age Grouped by Class\') plt.show() # Boxplot 3: Vertical boxplot with nested grouping by the class and alive variables plt.figure(figsize=(10, 6)) sns.boxplot(x=\'class\', y=\'age\', hue=\'alive\', data=titanic) plt.title(\'Vertical Boxplot of Age Grouped by Class and Alive\') plt.show() # Boxplot 4: Vertical boxplot for the age variable grouped by the deck variable, with full range whiskers plt.figure(figsize=(10, 6)) sns.boxplot(x=\'deck\', y=\'age\', data=titanic, whis=[0, 100]) plt.title(\'Vertical Boxplot of Age Grouped by Deck with Full Range Whiskers\') plt.show() # Boxplot 5: Vertical boxplot for the age variable grouped by the class variable with customizations plt.figure(figsize=(10, 6)) sns.boxplot(x=\'class\', y=\'age\', data=titanic, notch=True, showcaps=False, flierprops={\'marker\':\'x\', \'color\':\'green\', \'alpha\':0.5}, boxprops={\'facecolor\': (0.3, 0.5, 0.7, 0.5)}, medianprops={\'color\':\'red\', \'linewidth\':2}) plt.title(\'Customized Vertical Boxplot of Age Grouped by Class\') plt.show()"},{"question":"Coding Assessment Question **Objective:** To assess the understanding of environment variable manipulation and subprocess execution in Python using the `os` module. This question will help determine the student\'s ability to interact with the operating system\'s environment and execute system commands effectively. **Problem Statement:** Write a function `modify_and_execute(command: str, new_vars: dict) -> int` that performs the following tasks: 1. Modify the current environment variables by updating them with the key-value pairs provided in the `new_vars` dictionary. 2. Execute the given command using the modified environment variables. 3. Return the exit status of the command executed. **Requirements:** - Use the `os` module to access and manipulate the environment variables. - Use the `subprocess` module to execute the system command. - Ensure that the environment modifications do not persist beyond the execution of the command. **Function Signature:** ```python import os import subprocess def modify_and_execute(command: str, new_vars: dict) -> int: pass ``` **Inputs:** - `command (str)`: A shell command to be executed (e.g., `\\"ls -la\\"`). - `new_vars (dict)`: A dictionary containing the environment variables to be updated (e.g., `{\\"TEST_VAR\\": \\"123\\"}`). **Output:** - Return an integer representing the exit status of the executed command. **Constraints:** - The command string should be a valid shell command. - The dictionary keys and values should be valid strings representing environment variable names and values. **Example:** ```python # Example Usage: exit_status = modify_and_execute(\\"echo TEST_VAR\\", {\\"TEST_VAR\\": \\"123\\"}) print(exit_status) # Should print \'0\' indicating successful execution. ``` **Notes:** - The `os.environ` should be temporarily updated for the execution of the command. - Handle any potential exceptions that may arise during the execution process.","solution":"import os import subprocess def modify_and_execute(command: str, new_vars: dict) -> int: Modify the current environment variables by updating them with the key-value pairs provided in the new_vars dictionary. Execute the given command using the modified environment variables. Return the exit status of the executed command. Args: command (str): A shell command to be executed (e.g., \\"ls -la\\"). new_vars (dict): A dictionary containing the environment variables to be updated. Returns: int: Exit status of the executed command. # Backup the current environment old_environ = dict(os.environ) try: # Update the environment with new_vars os.environ.update(new_vars) # Execute the command result = subprocess.run(command, shell=True, env=os.environ) return result.returncode finally: # Restore the original environment os.environ.clear() os.environ.update(old_environ)"},{"question":"# Linear Algebra in PyTorch: Advanced Matrix Operations Problem Statement In this problem, you are required to develop a function `analyze_matrix` that performs several linear algebra operations on a given matrix using the PyTorch library. The function will take a single matrix as input and will output a dictionary containing various computed properties and decompositions. Function Signature ```python import torch import torch.linalg as linalg def analyze_matrix(matrix: torch.Tensor) -> dict: pass ``` Input - `matrix` (torch.Tensor): A square matrix of shape `(n, n)` where `n` is an integer greater than 1. Output - A dictionary containing the following keys and their associated values: - `\'determinant\'`: The determinant of the matrix. - `\'rank\'`: The matrix rank. - `\'inverse\'`: The inverse of the matrix. - `\'eigenvalues\'`: The eigenvalues of the matrix. - `\'svd\'`: The tuple (U, S, V) obtained from the Singular Value Decomposition (SVD) of the matrix. - `\'trace\'`: The trace of the matrix (sum of all diagonal elements). Constraints - The input matrix will always be invertible. - Use built-in functions from the `torch.linalg` module wherever applicable. - Maintain precision and handle potential numerical issues by using appropriate data types provided by PyTorch. Example ```python # Example Input matrix = torch.tensor([[4.0, 2.0], [3.0, 1.0]]) # Function Call result = analyze_matrix(matrix) # Expected Output print(result) # { # \'determinant\': -2.0, # \'rank\': 2, # \'inverse\': tensor([[-0.5, 1.0], # [ 1.5, -2.0]]), # \'eigenvalues\': tensor([ 5.3723, -0.3723]), # \'svd\': ( # tensor([[-0.8507, -0.5257], [-0.5257, 0.8507]]), # tensor([5.4649, 0.3659]), # tensor([[-0.8507, -0.5257], [-0.5257, 0.8507]]) # ), # \'trace\': 5.0 # } ``` Note - Ensure the `torch` library is imported and available in the environment where this function is implemented. - Avoid using any manual implementations for matrix operations; leverage `torch.linalg` functions as much as possible for optimal performance and precision.","solution":"import torch import torch.linalg as linalg def analyze_matrix(matrix: torch.Tensor) -> dict: Perform various linear algebra operations on a given matrix. Parameters: - matrix (torch.Tensor): A square matrix of shape (n, n) Returns: - dict: A dictionary containing the determinant, rank, inverse, eigenvalues, SVD components, and trace of the matrix. result = { \'determinant\': linalg.det(matrix).item(), \'rank\': linalg.matrix_rank(matrix).item(), \'inverse\': linalg.inv(matrix), \'eigenvalues\': linalg.eigvals(matrix).real, # Using the real part of eigenvalues \'svd\': linalg.svd(matrix), \'trace\': torch.trace(matrix).item() } return result"},{"question":"# PyTorch Coding Assessment Objective: To assess your understanding of the `torch.Size` class in PyTorch by requiring you to write a function that demonstrates the manipulation and utilization of tensor sizes. Problem Statement: You are provided with two tensors of potentially different shapes. Write a function `tensor_size_operations` that takes these two tensors as inputs and returns a tuple containing: 1. The sizes of both tensors. 2. The sum of the values in their first dimensions. 3. A boolean indicating if the tensors have the same shape. 4. A list of integers representing the smaller dimensions between the two tensors (by element-wise comparison). Function Signature: ```python import torch from typing import Tuple, List def tensor_size_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Tuple[torch.Size, torch.Size, int, bool, List[int]]: # Your implementation here pass ``` Input: - `tensor1` (torch.Tensor): The first input tensor. - `tensor2` (torch.Tensor): The second input tensor. Output: - A tuple containing: - The size of the first tensor (torch.Size). - The size of the second tensor (torch.Size). - The sum of the values in the first dimension of both tensors (int). - A boolean indicating if the two tensors have the same shape (bool). - A list of integers representing the smaller dimensions between the two tensors (by element-wise comparison). Example: ```python # Example Inputs tensor1 = torch.ones(2, 3, 4) tensor2 = torch.ones(2, 5, 4) # Function Call result = tensor_size_operations(tensor1, tensor2) # Expected Output # Sizes: torch.Size([2, 3, 4]), torch.Size([2, 5, 4]) # Sum of first dimensions: 2 + 2 = 4 # Same shape: False # Smaller dimensions: [2, 3, 4] assert result == (torch.Size([2, 3, 4]), torch.Size([2, 5, 4]), 4, False, [2, 3, 4]) ``` Constraints: - Only consider tensors with the same number of dimensions. - The tensors will have at least one dimension. - The size of corresponding dimensions can be different. Notes: - The function should make use of `torch.Size` for size manipulation. - The function should be efficient in terms of computational complexity.","solution":"import torch from typing import Tuple, List def tensor_size_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Tuple[torch.Size, torch.Size, int, bool, List[int]]: size1 = tensor1.size() size2 = tensor2.size() sum_first_dims = tensor1.size(0) + tensor2.size(0) same_shape = size1 == size2 smaller_dims = [min(dim1, dim2) for dim1, dim2 in zip(size1, size2)] return (size1, size2, sum_first_dims, same_shape, smaller_dims)"},{"question":"# Question: XML Data Parsing with Custom ContentHandler You are tasked with parsing XML data using the `xml.sax` package in Python. Your goal is to extract specific information from the XML structure and handle potential parsing errors appropriately. Task: Write a Python function `parse_and_extract(xml_data: str) -> List[Dict[str, Any]]` that parses the provided XML `xml_data` string and extracts information about all \\"book\\" elements. Each \\"book\\" element contains \\"title\\", \\"author\\", and \\"year\\" child elements. The function should return a list of dictionaries, where each dictionary represents a book with keys \\"title\\", \\"author\\", and \\"year\\", and corresponding values extracted from the XML data. Input: - `xml_data` (str): A string representing the XML content to be parsed. Output: - List of dictionaries, where each dictionary contains the keys \\"title\\", \\"author\\", and \\"year\\" with their respective values extracted from the XML. Constraints and Requirements: - Use `xml.sax` package for parsing. - Implement a custom `ContentHandler` class to handle events and extract data. - Handle any parsing errors using appropriate exception handling (e.g., `SAXParseException`). - Assume the XML structure is well-formed but may contain several book elements. Example: ```python xml_data = <library> <book> <title>Python 101</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2021</year> </book> </library> expected_output = [ {\'title\': \'Python 101\', \'author\': \'John Doe\', \'year\': \'2020\'}, {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': \'2021\'} ] assert parse_and_extract(xml_data) == expected_output ``` Considerations: - Ensure your `ContentHandler` correctly identifies and processes \\"book\\" elements and their child elements. - Make sure to handle nested structures and potential errors gracefully. **Hints:** - You may need to implement methods such as `startElement`, `characters`, and `endElement` in your `ContentHandler`. - Use `xml.sax.parseString` for parsing the XML string.","solution":"import xml.sax from typing import List, Dict, Any class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.current_book = {} self.books = [] def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.current_book = {\\"title\\": \\"\\", \\"author\\": \\"\\", \\"year\\": \\"\\"} def endElement(self, tag): if tag == \\"book\\": self.books.append(self.current_book) def characters(self, content): if self.current_data == \\"title\\": self.current_book[\\"title\\"] += content elif self.current_data == \\"author\\": self.current_book[\\"author\\"] += content elif self.current_data == \\"year\\": self.current_book[\\"year\\"] += content def endDocument(self): for book in self.books: book[\'title\'] = book[\'title\'].strip() book[\'author\'] = book[\'author\'].strip() book[\'year\'] = book[\'year\'].strip() def parse_and_extract(xml_data: str) -> List[Dict[str, Any]]: handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_data, handler) except xml.sax.SAXParseException as e: raise ValueError(f\\"Error parsing XML: {e}\\") return handler.books"},{"question":"**Objective:** Demonstrate your understanding of `torch.nn.init` initialization functions by implementing a custom neural network class and initializing its weights using different strategies. # Problem Statement You are required to implement a custom neural network class using PyTorch. Your task is to define the network architecture and properly initialize the weights of the network parameters using various initialization functions from the `torch.nn.init` module. Specifically, you need to apply different initialization methods to different layers of the network. # Network Architecture: 1. Input Layer with 784 units (corresponding to a 28x28 image) 2. Hidden Layer 1 with 256 units and ReLU activation 3. Hidden Layer 2 with 128 units and ReLU activation 4. Output Layer with 10 units (for classification into 10 classes) # Initialization Requirements: 1. Initialize the weights of the first hidden layer (`Hidden Layer 1`) using `xavier_uniform_`. 2. Initialize the weights of the second hidden layer (`Hidden Layer 2`) using `kaiming_normal_`. 3. Initialize the weights of the output layer using `constant_` with value 0.1. 4. Initialize biases of all the layers to zero using `zeros_`. # Input and Output Format: - No input required for the function; just define the class as specified. - The class should be fully self-contained. - Print the weight and bias tensors of each layer after initialization to demonstrate successful application of the initialization functions. # Constraints: - Use only the functions and features from the `torch.nn.init` module to initialize the network parameters. - Ensure that your implementation runs efficiently. # Performance Requirements: - The initialization process should not significantly slow down the model creation. It should avoid unnecessary computations while ensuring correct tensor initialization. # Example: ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 10) self.initialize_weights() def initialize_weights(self): init.xavier_uniform_(self.fc1.weight) init.zeros_(self.fc1.bias) init.kaiming_normal_(self.fc2.weight, nonlinearity=\\"relu\\") init.zeros_(self.fc2.bias) init.constant_(self.fc3.weight, 0.1) init.zeros_(self.fc3.bias) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Create an instance of the network network = CustomNet() # Print initialized weights and biases print(\\"Initialized weights and biases:\\") print(\\"Layer 1 weights:\\", network.fc1.weight) print(\\"Layer 1 biases:\\", network.fc1.bias) print(\\"Layer 2 weights:\\", network.fc2.weight) print(\\"Layer 2 biases:\\", network.fc2.bias) print(\\"Output layer weights:\\", network.fc3.weight) print(\\"Output layer biases:\\", network.fc3.bias) ``` Ensure you follow the structure and requirements specified. Good luck!","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 10) self.initialize_weights() def initialize_weights(self): init.xavier_uniform_(self.fc1.weight) init.zeros_(self.fc1.bias) init.kaiming_normal_(self.fc2.weight, nonlinearity=\\"relu\\") init.zeros_(self.fc2.bias) init.constant_(self.fc3.weight, 0.1) init.zeros_(self.fc3.bias) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Create an instance of the network network = CustomNet() # Print initialized weights and biases print(\\"Initialized weights and biases:\\") print(\\"Layer 1 weights:\\", network.fc1.weight) print(\\"Layer 1 biases:\\", network.fc1.bias) print(\\"Layer 2 weights:\\", network.fc2.weight) print(\\"Layer 2 biases:\\", network.fc2.bias) print(\\"Output layer weights:\\", network.fc3.weight) print(\\"Output layer biases:\\", network.fc3.bias)"},{"question":"Objective: Demonstrate your understanding of Python 3.10 built-in functions and classes by implementing a real-world inspired problem that leverages these functionalities effectively. Problem Statement: You are required to implement a function `generate_statistics` that will perform the following tasks: 1. **Count Non-empty Lines**: Count the number of non-empty lines in a given multiline string. 2. **Check All Numbers are Positive**: Check if all numbers in a given list are positive. 3. **Find Maximum and Minimum**: Find the maximum and minimum values in a given list. 4. **Round Values**: Round these maximum and minimum values to 2 decimal places. 5. **Convert Decimal to Binary**: Convert these values to their binary string equivalents. 6. **Enumerate with Custom Start**: - An additional list of elements is provided. Enumerate this list starting from a custom starting index. 7. **Async Iterator** (Advanced): - If an async iterable is provided, retrieve the first element using async methods. Function Definition: ```python from typing import List, Optional, AsyncIterable async def generate_statistics( multiline_string: str, numbers: List[int], elements: List[str], start_index: int = 0, async_iterable: Optional[AsyncIterable] = None ) -> dict: ``` Input: - `multiline_string` (str): A multiline string where lines are separated by `n`. - `numbers` (List[int]): A list of integers. - `elements` (List[str]): A list of string elements. - `start_index` (int, optional): A custom starting index for enumeration. Default is 0. - `async_iterable` (AsyncIterable, optional): An optional asynchronous iterable. Output: - A dictionary containing: - `non_empty_lines_count` (int): The count of non-empty lines in the provided multiline string. - `all_positive` (bool): `True` if all numbers in the list are positive; otherwise, `False`. - `max_value` (str): The maximum value in binary format. - `min_value` (str): The minimum value in binary format. - `enumerated_elements` (List[tuple]): A list of tuples containing the enumerated `elements` starting from `start_index`. - `async_first_element` (any, optional): The first element from the async iterable (if provided), otherwise `None`. Example: ```python import asyncio async def test_generate_statistics(): multiline_string = \\"HellonnWorldnPythonnn\\" numbers = [3, 8, 2, 7] elements = [\\"apple\\", \\"banana\\", \\"cherry\\"] async def async_gen(): for i in range(10): yield i result = await generate_statistics( multiline_string, numbers, elements, start_index=1, async_iterable=async_gen() ) assert result == { \\"non_empty_lines_count\\": 3, \\"all_positive\\": True, \\"max_value\\": \'0b1000\', \\"min_value\\": \'0b10\', \\"enumerated_elements\\": [(1, \'apple\'), (2, \'banana\'), (3, \'cherry\')], \\"async_first_element\\": 0 } print(\\"All test cases passed.\\") asyncio.run(test_generate_statistics()) ``` Constraints: - The function must handle potential errors gracefully, such as empty lists or invalid inputs, by returning appropriate fallback values. - The function should be efficient and avoid unnecessary computations or memory usage. Note: The implementation must strictly use Python 3.10 features where applicable, especially focusing on the usage of built-in functions documented.","solution":"from typing import List, Optional, AsyncIterable import asyncio async def generate_statistics( multiline_string: str, numbers: List[int], elements: List[str], start_index: int = 0, async_iterable: Optional[AsyncIterable] = None ) -> dict: # Count non-empty lines non_empty_lines_count = sum(1 for line in multiline_string.split(\'n\') if line.strip()) # Check if all numbers are positive all_positive = all(n > 0 for n in numbers) # Find maximum and minimum values max_value = max(numbers, default=None) min_value = min(numbers, default=None) # Round to 2 decimal places and convert to binary (if they exist) max_value_bin = f\\"{max_value:#b}\\" if max_value is not None else None min_value_bin = f\\"{min_value:#b}\\" if min_value is not None else None # Enumerate elements starting from custom start index enumerated_elements = list(enumerate(elements, start=start_index)) # Retrieve first element from async iterable (if provided) async_first_element = None if async_iterable is not None: async for item in async_iterable: async_first_element = item break return { \\"non_empty_lines_count\\": non_empty_lines_count, \\"all_positive\\": all_positive, \\"max_value\\": max_value_bin, \\"min_value\\": min_value_bin, \\"enumerated_elements\\": enumerated_elements, \\"async_first_element\\": async_first_element }"},{"question":"# Coding Assessment: Creating Built Distributions with `distutils` Objective: Write a Python script that uses `distutils` to create both source and built distributions of a given Python package. The script should allow customization to specify various distribution formats. Background: You are provided with the following simple setup script for a Python package named `example_pkg`: ```python # setup.py from setuptools import setup, find_packages setup( name=\\"example_pkg\\", version=\\"0.1\\", author=\\"Your Name\\", author_email=\\"your.email@example.com\\", description=\\"A simple example package\\", long_description=\\"This is a long description of a simple example package.\\", url=\\"https://example.com/example_pkg\\", packages=find_packages(), classifiers=[ \\"Programming Language :: Python :: 3\\", \\"License :: OSI Approved :: MIT License\\", \\"Operating System :: OS Independent\\", ], python_requires=\'>=3.6\', ) ``` Task: Write a function `create_distributions` that automates the creation of multiple types of built distributions using `distutils`. The function should: 1. Accept the list of desired distribution formats (e.g., `[\'gztar\', \'zip\', \'rpm\']`). 2. Run the appropriate `distutils` commands to create the distributions in these formats. 3. Ensure that the resulting distributions are saved in a directory named `dist` within the current working directory. Input: - A list of strings representing the desired distribution formats. Output: - The function should not return any value. Instead, it should create the specified distributions in the `dist` directory. Constraints: - The setup script (`setup.py`) is provided and exists in the current working directory. - You should handle various formats supported by `distutils`. Example Usage: ```python formats = [\'gztar\', \'zip\', \'rpm\'] create_distributions(formats) ``` Implementation Notes: - Use the `subprocess` module to run shell commands from within your Python script. - Ensure your solution is platform-independent where possible. - Use appropriate error handling to catch and report failures during the distribution creation process. # Sample Solution Outline: ```python import subprocess def create_distributions(formats): Creates specified built distributions for the package. for fmt in formats: try: subprocess.check_call([\'python\', \'setup.py\', \'bdist\', f\'--formats={fmt}\']) print(f\\"Distribution {fmt} created successfully.\\") except subprocess.CalledProcessError as e: print(f\\"Failed to create distribution {fmt}: {e}\\") # Example usage: formats = [\'gztar\', \'zip\', \'rpm\'] create_distributions(formats) ``` Ensure your script handles the creation process correctly and reports any issues encountered during the build process.","solution":"import subprocess def create_distributions(formats): Creates specified built distributions for the package. for fmt in formats: try: subprocess.check_call([\'python\', \'setup.py\', \'sdist\']) subprocess.check_call([\'python\', \'setup.py\', \'bdist\', f\'--formats={fmt}\']) print(f\\"Distribution {fmt} created successfully in \'dist\' directory.\\") except subprocess.CalledProcessError as e: print(f\\"Failed to create distribution {fmt}: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: # formats = [\'gztar\', \'zip\', \'rpm\'] # create_distributions(formats)"},{"question":"# Advanced Coding Challenge: Implementing and Utilizing Generators and Iterators in Python **Objective:** To demonstrate your understanding of Python\'s functional programming features, particularly iterators, generators, and the `itertools` and `functools` modules, by implementing a custom data processing pipeline. **Problem Statement:** You are required to create a Python module that processes a stream of numerical data. This module should include a class `DataProcessor` with the following functionalities: 1. **Initialization:** - The class should be initialized with an iterable of numerical data. 2. **Methods:** - `filter_data(predicate)`: This method should take a predicate function and filter the data according to it using an iterator. - `transform_data(transformation)`: This method should take a transformation function and apply it to each element of the data using a generator. - `aggregate_data(aggregator)`: This method should take an aggregation function and return the aggregated result using `functools.reduce`. 3. **Utilities:** - `compute_moving_average(window_size)`: This method should compute the moving average of the data stream with a given window size using `itertools.islice` and other functional tools. 4. **Iterator Implementation:** - `__iter__`: The class itself should be iterable, iterating over the processed data. **Example Usage:** ```python from functools import partial # Predicate to filter even numbers def is_even(x): return x % 2 == 0 # Transformation function to square numbers def square(x): return x * x # Aggregation function to sum numbers def add(a, b): return a + b # Sample data data = range(1, 11) # Create DataProcessor instance processor = DataProcessor(data) # Apply filtering, transformation, and aggregation filtered_data = processor.filter_data(is_even) transformed_data = processor.transform_data(square) sum_of_squares = processor.aggregate_data(add) print(list(filtered_data)) # Output: [2, 4, 6, 8, 10] print(list(transformed_data)) # Output: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] print(sum_of_squares) # Output: 100 # Compute moving average with a window size of 3 moving_avg = processor.compute_moving_average(3) print(list(moving_avg)) # Output: [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] ``` **Constraints:** - Your implementation should not modify the original data. - You should use generator expressions where applicable. - Assume that the input data will be large, so efficiency is important. **Performance Requirements:** - The filtering and transformation operations should be lazy, leveraging Python\'s generator capabilities. - The solution should effectively handle large datasets without significant memory overhead. **Deliverables:** 1. A Python class `DataProcessor` meeting the specified requirements. 2. Example usages demonstrating the capabilities of your class as shown in the problem statement. **Evaluation Criteria:** - Correctness of the implementation. - Efficient use of Python\'s functional programming constructs. - Code readability and documentation. Good luck!","solution":"from functools import reduce import itertools class DataProcessor: def __init__(self, data): self.data = data def filter_data(self, predicate): return filter(predicate, self.data) def transform_data(self, transformation): return (transformation(x) for x in self.data) def aggregate_data(self, aggregator): return reduce(aggregator, self.data) def compute_moving_average(self, window_size): it = iter(self.data) window = list(itertools.islice(it, window_size)) if len(window) == window_size: yield sum(window) / window_size for elem in it: window.pop(0) window.append(elem) yield sum(window) / window_size def __iter__(self): return iter(self.data) # Example usage # Predicate to filter even numbers def is_even(x): return x % 2 == 0 # Transformation function to square numbers def square(x): return x * x # Aggregation function to sum numbers def add(a, b): return a + b # Sample data data = range(1, 11) # Create DataProcessor instance processor = DataProcessor(data) # Apply filtering, transformation, and aggregation filtered_data = processor.filter_data(is_even) transformed_data = processor.transform_data(square) sum_of_squares = processor.aggregate_data(add) print(list(filtered_data)) # Output: [2, 4, 6, 8, 10] print(list(transformed_data)) # Output: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] print(sum_of_squares) # Output: 55 # Compute moving average with a window size of 3 moving_avg = processor.compute_moving_average(3) print(list(moving_avg)) # Output: [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"},{"question":"<|Analysis Begin|> The provided documentation is for the Python \\"random\\" module, which is used to generate pseudo-random numbers. The module provides a variety of functions to work with integers, sequences, real-valued distributions, and custom generators. Some notable features include generating random integers, floating-point numbers, bytes, shuffling sequences, and sampling from populations. It also supports various statistical distributions like uniform, normal (Gaussian), beta, exponential, gamma, and others. Additionally, there is support for seeding the random number generator and ensuring reproducibility of results. Key points to note: - Importance of seeding and state management for reproducibility. - Functions available for different types of random number generation. - Support for various statistical distributions. - Custom random generators can be created by subclassing. - Use of the SystemRandom class for cryptographic purposes. Given this information, I can create a question that tests the understanding of using the \\"random\\" module to perform specific tasks, focusing on generating random samples and managing state for reproducibility. <|Analysis End|> <|Question Begin|> **Coding Assessment Question** You are tasked with creating a simulation for a simple lottery system, where tickets can be randomly selected from a pool of participants. The lottery system should be able to: 1. Generate a list of random ticket numbers. 2. Randomly select a specified number of winners. 3. Ensure that the lottery drawing can be reproduced using a seed value. **Requirements:** 1. Implement a function `generate_tickets` that takes two parameters: - `num_tickets` (int): The total number of tickets to generate. - `seed` (Optional[int]): An optional seed value for reproducibility. This function should return a list of unique ticket numbers, each represented as a string of six digits (e.g., \\"123456\\"). Ensure that the random state is reset to the original state at the beginning of the function if a seed value is provided. 2. Implement a function `draw_winners` that takes three parameters: - `tickets` (List[str]): The list of ticket numbers. - `num_winners` (int): The number of winners to select. - `seed` (Optional[int]): An optional seed value for reproducibility. This function should return a list of randomly selected winning ticket numbers. Ensure that the random state is reset to the original state at the beginning of the function if a seed value is provided. 3. Ensure that the functions raise appropriate errors if invalid parameters are provided (e.g., negative numbers, more winners than available tickets, etc.). **Example Usage:** ```python # Example 1: Generating tickets with a seed and drawing winners tickets = generate_tickets(1000, seed=42) winners = draw_winners(tickets, 5, seed=42) print(winners) # Output should be reproducible with the seed value # Example 2: Handling invalid parameters try: tickets = generate_tickets(-5) except ValueError as e: print(e) # Expected to raise a ValueError for negative number of tickets try: winners = draw_winners(tickets, 1005) except ValueError as e: print(e) # Expected to raise a ValueError for more winners than tickets ``` **Constraints:** - `num_tickets` and `num_winners` must be non-negative integers. - The ticket numbers must be unique and in the format of a six-digit string. - The function should handle seeds properly to ensure reproducibility. Good luck!","solution":"import random def generate_tickets(num_tickets, seed=None): Generates a list of unique ticket numbers, each represented as a string of six digits. Parameters: - num_tickets (int): The total number of tickets to generate. - seed (Optional[int]): An optional seed value for reproducibility. Returns: List of unique ticket numbers. if num_tickets < 0: raise ValueError(\\"Number of tickets must be a non-negative integer.\\") if seed is not None: random.seed(seed) tickets = set() while len(tickets) < num_tickets: ticket = f\\"{random.randint(0, 999999):06}\\" tickets.add(ticket) return list(tickets) def draw_winners(tickets, num_winners, seed=None): Randomly selects a specified number of winners from the list of ticket numbers. Parameters: - tickets (List[str]): The list of ticket numbers. - num_winners (int): The number of winners to select. - seed (Optional[int]): An optional seed value for reproducibility. Returns: List of randomly selected winning ticket numbers. if num_winners < 0: raise ValueError(\\"Number of winners must be a non-negative integer.\\") if num_winners > len(tickets): raise ValueError(\\"Number of winners cannot exceed the number of tickets.\\") if seed is not None: random.seed(seed) return random.sample(tickets, num_winners)"},{"question":"**Title:** Custom Multi-Faceted Plot using seaborn.objects Interface **Problem Statement:** You are provided with a dataset and your task is to visualize it using the `seaborn.objects` interface to create a multi-faceted plot. The goal is to effectively utilize different properties and customization options provided by `seaborn.objects` to create a comprehensive visualization. **Dataset:** ``` data = { \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Value1\': [20, 34, 30, 35, 27, 40, 50, 44, 22, 39, 30, 45], \'Value2\': [25, 32, 34, 20, 29, 41, 49, 43, 21, 38, 35, 48] } ``` **Requirements:** 1. **Customize Coordinates:** - Use both `x` and `y` coordinates for plotting. - Implement spans using `xmin`, `xmax`, `ymin`, `ymax`. 2. **Apply Different Colors:** - Use at least three distinct color properties (`color`, `fillcolor`, `edgecolor`) and customize them using a palette or hex codes. 3. **Use and Customize Markers:** - Use various marker styles (circle, triangle, square, etc.). - Customize the size and edgewidth of the markers. 4. **Work with Text:** - Add a text annotation for each data point. - Customize the text properties including `fontsize`, `halign`, and `valign`. 5. **Combine Multiple Plots:** - Create a multi-faceted plot (multiple subplots) to show different visualizations (e.g., bar plot, dot plot). - Consider using properties like `fill`, `marker`, etc., to differentiate elements in the subplots. **Expected Input and Output:** - **Input:** The given `data` dictionary. - **Output:** A multi-faceted plot that meets the requirements outlined above. **Additional Notes:** 1. Utilize the methods and properties from `seaborn.objects` as needed. 2. Ensure that the visualizations are well-labeled and the different aspects (colors, markers, etc.) are clearly distinguishable. 3. Provide inline comments to explain the choices and customizations made in the code. **Function Signature:** ```python from seaborn import objects as so def create_custom_plot(data): # Your implementation here pass # Example of how to call your function: data = { \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Value1\': [20, 34, 30, 35, 27, 40, 50, 44, 22, 39, 30, 45], \'Value2\': [25, 32, 34, 20, 29, 41, 49, 43, 21, 38, 35, 48] } create_custom_plot(data) ``` **Performance Requirements:** - The solution should be efficient enough to handle the data visualization smoothly. - Visualization should be understandable and easily interpretable.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_plot(data): # Convert the dictionary to a DataFrame df = pd.DataFrame(data) # Create a FacetGrid g = sns.FacetGrid(df, col=\\"Category\\", height=5, aspect=1) # Customize the bar plot in the facet grid g.map_dataframe( sns.barplot, x=\\"Value1\\", y=\\"Value2\\", color=\\"#2ca02c\\", edgecolor=\\".2\\", errcolor=\\"gray\\" ) # Adding text annotations for each point for ax in g.axes.flat: for i, point in df.iterrows(): ax.text(point[\'Value1\'], point[\'Value2\'], f\\"({point[\'Value1\']}, {point[\'Value2\']})\\", fontsize=9, ha=\'right\', va=\'bottom\') # Additional plot customization g.set_titles(col_template=\\"{col_name} category\\") g.set_axis_labels(\\"Value 1\\", \\"Value 2\\") g.add_legend() # Show plot plt.show()"},{"question":"# Tokenize Python Source Code and Count Keywords In this exercise, you are required to create a Python function that uses the `tokenize` module to analyze Python source code. Your task is to write a function that reads Python source code and counts the occurrences of each Python keyword in the code. The function should return a dictionary where the keys are the keywords and the values are their counts. Function Signature: ```python def count_keywords(source_code: str) -> dict: pass ``` Input: - `source_code`: A string containing the entire Python source code. Output: - Returns a dictionary with keywords as keys and the number of occurrences as values. Constraints: - You must use the `tokenize` module to process the source code. - The function should be capable of handling valid Python code only. - Ignore any tokens that are not keywords. Example: ```python source_code = \'\'\' def add(a, b): return a + b if __name__ == \\"__main__\\": print(add(1, 2)) \'\'\' output = count_keywords(source_code) # Should return {\'def\': 1, \'return\': 1, \'if\': 1} ``` Notes: - Python keywords include: `False`, `None`, `True`, `and`, `as`, `assert`, `async`, `await`, `break`, `class`, `continue`, `def`, `del`, `elif`, `else`, `except`, `finally`, `for`, `from`, `global`, `if`, `import`, `in`, `is`, `lambda`, `nonlocal`, `not`, `or`, `pass`, `raise`, `return`, `try`, `while`, `with`, `yield` - You can use the `keyword` module in Python to get the list of keywords. Your implementation should ensure the proper handling of the `tokenize` module as described, and focus on efficiently counting the keywords in the provided source code.","solution":"import tokenize import io from keyword import kwlist def count_keywords(source_code: str) -> dict: Counts the number of occurrences of each Python keyword in the provided source code. :param source_code: A string containing the entire Python source code. :return: A dictionary with keywords as keys and their occurrence counts as values. keyword_counts = {key: 0 for key in kwlist} tokens = tokenize.generate_tokens(io.StringIO(source_code).readline) for token_type, token_string, _, _, _ in tokens: if token_type == tokenize.NAME and token_string in kwlist: keyword_counts[token_string] += 1 # Remove keywords with zero counts keyword_counts = {key: count for key, count in keyword_counts.items() if count > 0} return keyword_counts"},{"question":"You are given two datasets: `penguins` and `flights`. Use these datasets to demonstrate your understanding of Seaborn Objects (so) interface by creating specific visualizations. # Task: 1. Load the `penguins` dataset. 2. Load the `flights` dataset and filter it for the year 1960. 3. Implement the following functions: Function 1: `penguins_species_histogram` - **Input**: None - **Output**: A Seaborn bar plot displaying the count of each penguin species. - **Constraints**: - Use the `penguins` dataset. - The x-axis should represent the different species. - The y-axis should represent the count of penguins. Function 2: `flights_passenger_plot` - **Input**: None - **Output**: A Seaborn bar plot showing the number of passengers for each month in the year 1960. - **Constraints**: - Use the `flights` dataset filtered for the year 1960. - The x-axis should represent the months. - The y-axis should represent the number of passengers. - Rotate the bar orientation to use months on the y-axis (vertical bars). Function 3: `enhanced_penguins_plot` - **Input**: None - **Output**: A Seaborn bar plot that: - Displays the distribution of penguin body mass across different species and sexes. - Uses different colors for each sex. - Adds error bars representing the standard deviation. # Implementation: ```python import seaborn.objects as so from seaborn import load_dataset def penguins_species_histogram(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create and show the bar plot p = so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) return p def flights_passenger_plot(): # Load the flights dataset and filter for the year 1960 flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Create and show the bar plot with orientation adjustment p = so.Plot(flights[\\"passengers\\"], flights[\\"month\\"]).add(so.Bar()) return p def enhanced_penguins_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create and show the enhanced bar plot p = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) return p ``` Note: You can display the plots by calling `plot.show()` on each returned object in an IPython environment.","solution":"import seaborn.objects as so from seaborn import load_dataset def penguins_species_histogram(): Returns a Seaborn bar plot displaying the count of each penguin species. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create and return the bar plot p = so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) return p def flights_passenger_plot(): Returns a Seaborn bar plot showing the number of passengers for each month in the year 1960. # Load the flights dataset and filter for the year 1960 flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Create and return the bar plot with orientation adjustment p = so.Plot(flights, y=\\"month\\", x=\\"passengers\\").add(so.Bar()) return p def enhanced_penguins_plot(): Returns a Seaborn bar plot showing the distribution of penguin body mass across different species and sexes, with different colors for each sex and error bars representing the standard deviation. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create and return the enhanced bar plot p = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) return p"},{"question":"# Unicode String Processing with unicodedata Module You are tasked with creating a function that processes a given Unicode string and returns a detailed analysis of its characters. The analysis should include the following information for each character in the string: 1. The character itself. 2. The official Unicode name. 3. The general category. 4. The bidirectional class. 5. The canonical combining class. 6. The decimal value (if applicable). 7. The digit value (if applicable). 8. The numeric value (if applicable). 9. The mirrored property. 10. The decomposition mapping. 11. The normalized form D (NFD) of the string. Your function should comply with the following requirements: Function Signature ```python def unicode_string_analysis(input_string: str) -> dict: ``` Input - `input_string` (str): A Unicode string to be analyzed. Output - (dict): A dictionary where each key is a character from the input string and each value is another dictionary containing the properties listed above. The dictionary should also include an additional key `\\"normalized_form_D\\"` for the normalized form D (NFD) of the input string, whose value is the normalized string. Example ```python input_string = \'A9Î±\' result = unicode_string_analysis(input_string) print(result) ``` Expected output: ```python { \'A\': { \'character\': \'A\', \'name\': \'LATIN CAPITAL LETTER A\', \'category\': \'Lu\', \'bidirectional\': \'L\', \'combining\': 0, \'decimal\': None, \'digit\': None, \'numeric\': None, \'mirrored\': 0, \'decomposition\': \'\', }, \'9\': { \'character\': \'9\', \'name\': \'DIGIT NINE\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'combining\': 0, \'decimal\': 9, \'digit\': 9, \'numeric\': 9.0, \'mirrored\': 0, \'decomposition\': \'\', }, \'Î±\': { \'character\': \'Î±\', \'name\': \'GREEK SMALL LETTER ALPHA\', \'category\': \'Ll\', \'bidirectional\': \'L\', \'combining\': 0, \'decimal\': None, \'digit\': None, \'numeric\': None, \'mirrored\': 0, \'decomposition\': \'\', }, \'normalized_form_D\': \'A9Î±\' } ``` Constraints - Your solution should handle any valid Unicode string. - You may assume the input string will not be longer than 100,000 characters. Notes - You can use the `unicodedata` module\'s functions to gather the required properties for each character. - For properties where no applicable value exists, the corresponding value in the result dictionary should be `None`. - The functions you may need from the `unicodedata` module are: `name()`, `category()`, `bidirectional()`, `combining()`, `decimal()`, `digit()`, `numeric()`, `mirrored()`, `decomposition()`, and `normalize()`. Good luck!","solution":"import unicodedata def unicode_string_analysis(input_string: str) -> dict: Processes a given Unicode string and returns a detailed analysis of its characters. analysis = {} # Analyze each character in the string for char in input_string: char_info = { \'character\': char, \'name\': unicodedata.name(char, None), \'category\': unicodedata.category(char), \'bidirectional\': unicodedata.bidirectional(char), \'combining\': unicodedata.combining(char), \'decimal\': unicodedata.decimal(char, None), \'digit\': unicodedata.digit(char, None), \'numeric\': unicodedata.numeric(char, None), \'mirrored\': unicodedata.mirrored(char), \'decomposition\': unicodedata.decomposition(char) } analysis[char] = char_info # Compute the normalized form D (NFD) of the string analysis[\'normalized_form_D\'] = unicodedata.normalize(\'NFD\', input_string) return analysis"},{"question":"# NNTP Client Implementation Challenge You are required to write a Python function that connects to a specified NNTP server, retrieves the list of newsgroups created within the past week, and extracts the headers of the most recent articles from those newsgroups. Your function should handle any errors that might occur during the connection or data retrieval process. Function Signature ```python def fetch_recent_articles(server: str) -> dict: pass ``` Input - `server`: A string representing the NNTP server address. Output - A dictionary where the keys are newsgroup names and the values are lists of headers of the most recent articles from those newsgroups. Constraints - Only newsgroups created within the past 7 days should be considered. - Retrieve up to 5 recent articles\' headers from each newsgroup. Notes - Use the `nntplib.NNTP` class to create the connection and interact with the server. - Implement appropriate exception handling to manage potential connection or data retrieval errors. - Use the `decode_header()` utility function to decode headers. - Ensure the connection is properly closed after data retrieval. Example Here is an example input and expected output format: ```python server = \'news.gmane.io\' # Example output structure { \'gmane.comp.python.committers\': [ \'Re: Commit privileges for Åukasz Langa\', \'Re: 3.2 alpha 2 freeze\', ... ], \'gmane.network.tor.devel\': [ \'Updated ssh key\', \'Re: Updated ssh key\', ... ] } ``` Hints - Use the `nntplib.NNTP.newgroups()` method to fetch newsgroups created within the specified timeframe. - Use the `nntplib.NNTP.group()` method to select a newsgroup. - Use the `nntplib.NNTP.over()` method to retrieve article headers from the newsgroups. - Properly decode the headers using `nntplib.decode_header()`. Happy coding!","solution":"import nntplib import datetime from time import mktime def fetch_recent_articles(server: str) -> dict: result = {} try: with nntplib.NNTP(server) as nntp: current_time = datetime.datetime.now() one_week_ago = current_time - datetime.timedelta(days=7) one_week_ago_tuple = one_week_ago.timetuple() newgroup_resp, newgroups = nntp.newgroups(one_week_ago_tuple) for newgroup in newgroups: newsgroup_name = newgroup.group response, count, first, last, name = nntp.group(newsgroup_name) if count > 0: headers = [] overviews = nntp.over((max(1, int(last)-4), int(last))) for id, article in overviews[1]: subject = article.get(\\"subject\\", \\"\\") headers.append(subject) result[newsgroup_name] = headers except nntplib.NNTPError as e: print(f\\"NNTP error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") return result"},{"question":"Task Description You are required to implement a Python script that performs the following tasks: 1. **Create and manage files and directories:** - Create a directory named `test_dir` in the current working directory. - Inside `test_dir`, create a file named `sample.txt`. - Write a predefined string \\"Hello, World!\\" to `sample.txt`. 2. **Modify the environment and execute a subprocess:** - Set an environment variable `GREETING` to the value \\"Hello, Python!\\". - Execute a subprocess that reads from `sample.txt` and prints its content to standard output, prefixed by the value of the `GREETING` environment variable. 3. **Process management:** - Create a child process using fork. - In the child process, change the current working directory to `test_dir`, and then execute a program (`ls -l`) to list the contents of `test_dir`. - In the parent process, wait for the child process to complete and report its exit status. Constraints - Ensure that all file and directory operations handle errors gracefully. - The subprocess should read the environment variable and file content correctly. - Use only functions from the `os` module to achieve the task. - Do not use high-level functions from other libraries like `subprocess` or `shutil`. Input and Output - The script does not take any input. - The script outputs the result of the subprocess execution and the exit status of the child process. Example Output The expected output might look like: ```plaintext Hello, Python! Hello, World! total 4 -rw-r--r-- 1 user group 13 Oct 20 12:00 sample.txt Child process exited with status: 0 ``` Implementation Implement the function `manage_files_and_processes()` that performs the above tasks. ```python import os def manage_files_and_processes(): # Implement the task as described above ```","solution":"import os def manage_files_and_processes(): try: # Step 1: Create and manage files and directories os.makedirs(\\"test_dir\\", exist_ok=True) with open(\\"test_dir/sample.txt\\", \\"w\\") as f: f.write(\\"Hello, World!\\") # Step 2: Modify the environment and execute a subprocess os.environ[\'GREETING\'] = \\"Hello, Python!\\" def read_and_print_file(): with open(\\"test_dir/sample.txt\\", \\"r\\") as f: content = f.read().strip() print(f\\"{os.environ[\'GREETING\']} {content}\\") # Using fork to create a subprocess to print the file content pid = os.fork() if pid == 0: # In child process read_and_print_file() os._exit(0) else: # In parent process, wait for child to complete os.waitpid(pid, 0) # Step 3: Process management pid = os.fork() if pid == 0: # In child process os.chdir(\\"test_dir\\") os.execlp(\\"ls\\", \\"ls\\", \\"-l\\") else: # In parent process, wait for child to complete and fetch status pid, status = os.waitpid(pid, 0) print(f\\"Child process exited with status: {os.WEXITSTATUS(status)}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of the `torch.fft` package by processing an input signal using FFT and performing various operations to analyze and manipulate the frequency spectrum. # Problem Statement Given a one-dimensional signal represented as a PyTorch tensor, your task is to perform the following steps using the `torch.fft` package: 1. Compute the Fourier Transform of the signal. 2. Shift the zero-frequency component to the center of the spectrum. 3. Create a function to filter out high-frequency components by setting them to zero (you will be provided with a cutoff frequency). 4. Shift the spectrum back. 5. Compute the inverse Fourier Transform to retrieve the filtered signal. 6. Compute the frequency-domain sample frequencies and provide both the filtered signal and the sample frequencies. # Function Signature ```python def filter_high_frequencies(signal: torch.Tensor, cutoff: float) -> (torch.Tensor, torch.Tensor): Filters out high-frequency components from the input signal. Parameters: signal (torch.Tensor): The input one-dimensional signal. cutoff (float): The cutoff frequency. Frequencies higher than this will be zeroed out. Returns: (torch.Tensor, torch.Tensor): A tuple containing the filtered signal in time domain and the frequency domain sample frequencies. pass ``` # Input - `signal`: A one-dimensional PyTorch tensor representing the signal. - `cutoff`: A float value denoting the cutoff frequency. # Output - A tuple containing: - The filtered signal as a one-dimensional PyTorch tensor. - The frequency domain sample frequencies as a one-dimensional PyTorch tensor. # Constraints - The input `signal` tensor should have a length of at least 2. - The `cutoff` frequency should be a positive float value less than half of the signal sampling rate. - Ensure that the operations are efficiently performed using the PyTorch FFT functions. # Example ```python import torch signal = torch.tensor([0.0, 1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0]) cutoff = 1.0 filtered_signal, freq_sample = filter_high_frequencies(signal, cutoff) print(filtered_signal) print(freq_sample) ``` # Notes - Use `torch.fft.fft`, `torch.fft.ifft`, `torch.fft.fftshift`, `torch.fft.ifftshift`, and other relevant functions from the `torch.fft` module. - You may refer to the PyTorch documentation to understand how to use these functions effectively.","solution":"import torch import torch.fft def filter_high_frequencies(signal: torch.Tensor, cutoff: float) -> (torch.Tensor, torch.Tensor): Filters out high-frequency components from the input signal. Parameters: signal (torch.Tensor): The input one-dimensional signal. cutoff (float): The cutoff frequency. Frequencies higher than this will be zeroed out. Returns: (torch.Tensor, torch.Tensor): A tuple containing the filtered signal in time domain and the frequency domain sample frequencies. # Compute the Fourier Transform of the signal fft_signal = torch.fft.fft(signal) # Shift the zero-frequency component to the center of the spectrum fft_shifted = torch.fft.fftshift(fft_signal) # Compute the sample frequencies n = signal.numel() sample_freq = torch.fft.fftfreq(n) # Filter the high-frequency components cutoff_filter = torch.abs(sample_freq) <= cutoff fft_shifted_filtered = fft_shifted * cutoff_filter # Shift the spectrum back fft_filtered = torch.fft.ifftshift(fft_shifted_filtered) # Compute the inverse Fourier Transform to retrieve the filtered signal filtered_signal = torch.fft.ifft(fft_filtered).real return filtered_signal, sample_freq"},{"question":"Buffer Management in Python In Python, handling buffers efficiently is critical in scenarios involving large data manipulations or interfacing with lower-level system components. You are required to demonstrate your understanding of buffer management by implementing a Python function that reads data from a buffer, processes it, and writes the processed data back to a buffer. Task: Write a Python function `process_buffer(buffer: memoryview) -> memoryview` that: 1. Receives a `memoryview` object as input. This `memoryview` represents a contiguous block of memory. Assume the input buffer contains character data. 2. Processes the data in the buffer by converting all lowercase alphabetic characters to uppercase alphabetic characters. 3. Returns a new `memoryview` object representing the processed data, ensuring it is writable. Function Signature: ```python def process_buffer(buffer: memoryview) -> memoryview: pass ``` Input: - `buffer`: A `memoryview` object containing character data. The buffer is read-only initially. Output: - Returns a `memoryview` object that is writable and contains the processed data. Constraints: - You may not use deprecated buffer functions. Instead, utilize the modern `memoryview` and `bytearray` features in Python. - Ensure that the original `buffer` remains unchanged. Example: ```python # Create a byte array data = bytearray(b\'Hello, World!\') # Create a memoryview buffer = memoryview(data) # Process the buffer new_buffer = process_buffer(buffer) # Print the original and new buffers print(data) # Output should be: b\'Hello, World!\' print(new_buffer.tobytes()) # Output should be: b\'HELLO, WORLD!\' ``` Note: - You are not allowed to use the deprecated functions such as `PyObject_AsCharBuffer`, `PyObject_AsReadBuffer`, `PyObject_AsWriteBuffer`, etc. Leverage the capabilities provided by `memoryview` and `bytearray` in Python 3.10. - Efficient memory handling is required. Make sure your solution does not create unnecessary copies of large data. Good luck!","solution":"def process_buffer(buffer: memoryview) -> memoryview: This function receives a memoryview object containing character data, converts all lowercase alphabetic characters to uppercase alphabetic characters, and returns a new writable memoryview object with the processed data. # Convert the memoryview to bytes, process it, and then to a bytearray processed_data = bytearray(buffer.tobytes()) for i in range(len(processed_data)): if 97 <= processed_data[i] <= 122: # ASCII values for \'a\' to \'z\' processed_data[i] -= 32 # Convert to uppercase by subtracting 32 from ASCII value # Return a writable memoryview of the processed data return memoryview(processed_data)"},{"question":"You are provided with a dataset containing information about different species of penguins. The dataset includes variables such as `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, and `sex`. Your task is to create a series of visualizations to analyze the distribution and relationships of these variables using the seaborn library. Part 1: Univariate Distribution 1. Load the penguins dataset using seaborn\'s `load_dataset` function. 2. Create a histogram to visualize the distribution of `flipper_length_mm`. Set the number of bins to 20. 3. Overlay a KDE plot on the same histogram to provide a smoother view of the distribution. 4. Separate the histogram by `species`, using different colors for each species. Part 2: Bivariate Distribution 1. Create a bivariate KDE plot to visualize the joint distribution of `bill_length_mm` and `bill_depth_mm`. 2. Create another bivariate plot to visualize the relationship between `flipper_length_mm` and `body_mass_g` using a hexbin plot. Part 3: Conditional Distribution 1. Create a grid of KDE plots to show the distribution of `flipper_length_mm` separated by both `species` and `sex`. Use different colors for each species. Constraints - You must use seaborn for all visualizations. - Ensure that all plots include appropriate labels and titles for clarity. Input Format - No specific input format is required, the dataset can be loaded directly using the seaborn library. Output Format - The output should be a set of plots, each correctly labeled and titled. Example: Hereâ€™s a simple example to get you started with the seaborn plotting functions: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Part 1: Univariate Distribution # Histogram and KDE plot for flipper length sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\'Distribution of Flipper Length\') plt.show() # Separate histogram by species sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True, hue=\\"species\\") plt.title(\'Distribution of Flipper Length by Species\') plt.show() # Part 2: Bivariate Distribution # KDE plot for bill length and bill depth sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", fill=True) plt.title(\'Bivariate KDE of Bill Length and Bill Depth\') plt.show() # Hexbin plot for flipper length and body mass sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", kind=\\"hex\\", color=\\"orange\\") plt.title(\'Hexbin Plot of Flipper Length and Body Mass\') plt.show() # Part 3: Conditional Distribution # KDE plots separated by species and sex g = sns.FacetGrid(penguins, row=\\"sex\\", col=\\"species\\", margin_titles=True) g.map(sns.kdeplot, \\"flipper_length_mm\\") plt.suptitle(\'KDE of Flipper Length by Species and Sex\', y=1.02) plt.show() ``` This example covers the basics required for the task. Be sure to expand on this template to meet all the specifications provided in the question.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_penguins_dataset(): Load the penguins dataset using seaborn\'s load_dataset function. return sns.load_dataset(\\"penguins\\") def plot_univariate_distribution(penguins): Create histograms to visualize the distribution of flipper_length_mm. # Histogram and KDE plot for flipper length sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\'Distribution of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') plt.show() # Separate histogram by species sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True, hue=\\"species\\") plt.title(\'Distribution of Flipper Length by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') plt.show() def plot_bivariate_distribution(penguins): Create bivariate visualizations for specific variable pairs. # KDE plot for bill length and bill depth sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", fill=True) plt.title(\'Bivariate KDE of Bill Length and Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.show() # Hexbin plot for flipper length and body mass sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", kind=\\"hex\\", color=\\"orange\\") plt.suptitle(\'Hexbin Plot of Flipper Length and Body Mass\', y=1.02) plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show() def plot_conditional_distribution(penguins): Create a grid of KDE plots for flipper_length_mm separated by species and sex. g = sns.FacetGrid(penguins, row=\\"sex\\", col=\\"species\\", margin_titles=True) g.map(sns.kdeplot, \\"flipper_length_mm\\") plt.suptitle(\'KDE of Flipper Length by Species and Sex\', y=1.02) plt.show() # Main function to execute all plots def main(): # Load dataset penguins = load_penguins_dataset() # Plot univariate distribution plot_univariate_distribution(penguins) # Plot bivariate distribution plot_bivariate_distribution(penguins) # Plot conditional distribution plot_conditional_distribution(penguins) if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: Implementing and Comparing LDA with Different Covariance Estimators Objective Demonstrate your understanding of Linear Discriminant Analysis (LDA) and the use of different covariance estimators by implementing an LDA classifier with and without shrinkage, and comparing their performance on a dataset. Problem Statement You are given a dataset with multiple features and classes. Your task is to: 1. Implement an LDA classifier using scikit-learn\'s `LinearDiscriminantAnalysis`. 2. Implement an LDA classifier with Ledoit Wolf shrinkage for covariance matrix estimation. 3. Implement an LDA classifier with the Oracle Approximating Shrinkage (OAS) estimator for covariance matrix estimation. 4. Compare the performance of these classifiers using accuracy as the metric on the given dataset. Input - A training dataset (`X_train`, `y_train`) and a testing dataset (`X_test`, `y_test`) in the form of NumPy arrays. - `X_train`, `X_test` are 2D arrays where rows represent samples and columns represent features. - `y_train`, `y_test` are 1D arrays where each element is the class label for the corresponding sample. Output - A dictionary with the accuracy scores of each classifier: ```python { \'lda_no_shrinkage\': <accuracy of LDA without shrinkage>, \'lda_ledoit_wolf\': <accuracy of LDA with Ledoit Wolf shrinkage>, \'lda_oas\': <accuracy of LDA with OAS shrinkage> } ``` Constraints - Use scikit-learn for implementing the classifiers. - You can assume that the dataset is preprocessed and ready for modeling. Instructions 1. **Import necessary libraries**: ```python from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.covariance import LedoitWolf, OAS ``` 2. **Implement LDA without shrinkage**: ```python lda_no_shrinkage = LinearDiscriminantAnalysis(solver=\'svd\') ``` 3. **Implement LDA with Ledoit Wolf shrinkage**: ```python lda_ledoit_wolf = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') ``` 4. **Implement LDA with OAS estimator**: ```python oas = OAS() lda_oas = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=oas) ``` 5. **Train each classifier on the training data**: ```python lda_no_shrinkage.fit(X_train, y_train) lda_ledoit_wolf.fit(X_train, y_train) lda_oas.fit(X_train, y_train) ``` 6. **Evaluate each classifier on the test data and report their accuracy**: ```python accuracy_no_shrinkage = lda_no_shrinkage.score(X_test, y_test) accuracy_ledoit_wolf = lda_ledoit_wolf.score(X_test, y_test) accuracy_oas = lda_oas.score(X_test, y_test) ``` 7. **Return the accuracy scores in the specified format**: ```python results = { \'lda_no_shrinkage\': accuracy_no_shrinkage, \'lda_ledoit_wolf\': accuracy_ledoit_wolf, \'lda_oas\': accuracy_oas } return results ``` Example Usage ```python # Assuming X_train, y_train, X_test, y_test are defined results = lda_comparison(X_train, y_train, X_test, y_test) print(results) ``` You need to implement the function `lda_comparison` that takes the training and testing data as inputs and returns the dictionary with the accuracy scores as shown above.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.covariance import LedoitWolf, OAS def lda_comparison(X_train, y_train, X_test, y_test): results = {} # LDA without shrinkage lda_no_shrinkage = LinearDiscriminantAnalysis(solver=\'svd\') lda_no_shrinkage.fit(X_train, y_train) accuracy_no_shrinkage = lda_no_shrinkage.score(X_test, y_test) results[\'lda_no_shrinkage\'] = accuracy_no_shrinkage # LDA with Ledoit Wolf shrinkage lda_ledoit_wolf = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') lda_ledoit_wolf.fit(X_train, y_train) accuracy_ledoit_wolf = lda_ledoit_wolf.score(X_test, y_test) results[\'lda_ledoit_wolf\'] = accuracy_ledoit_wolf # LDA with OAS estimator oas = OAS() lda_oas = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=oas) lda_oas.fit(X_train, y_train) accuracy_oas = lda_oas.score(X_test, y_test) results[\'lda_oas\'] = accuracy_oas return results"},{"question":"# Question: You are provided with a dataset containing information on car models, including their fuel efficiency in the city and highway, engine size, and their make and body style. Your task is to create a visualization that analyzes these properties. **Dataset Schema:** - `make`: The manufacturer of the car (categorical). - `model`: The model of the car (categorical). - `year`: The year the car was manufactured (integer). - `engine_size`: Size of the engine in liters (float). - `city_mpg`: Fuel efficiency in the city, measured in miles per gallon (float). - `highway_mpg`: Fuel efficiency on the highway, measured in miles per gallon (float). - `body_style`: The style of the car body, e.g., sedan, SUV, etc. (categorical). **Your Task:** Using Seaborn, create a scatter plot that visualizes the relationship between `engine_size` and `city_mpg`. Enhance this scatter plot by: 1. Mapping `highway_mpg` to the `hue` parameter. 2. Mapping `body_style` to the `style` parameter. 3. Ensuring that all unique body styles are represented in the legend. 4. Using a sequential palette for `hue` mapping to highlight fuel efficiencies on the highway. 5. Setting the size of the markers proportional to `engine_size` within a range of sizes from 50 to 300. 6. Utilizing `relplot` to create facet plots split by the `make` of the car, with a maximum of two `make`s displayed per row. **Constraints and Requirements:** - The input dataset will be in a CSV format. - The output should be a single visualization with correctly labeled axes, legends, and titles. **Input:** - A CSV file named `cars.csv` with the aforementioned schema. **Output:** - A scatter plot visualization displayed within a Jupyter notebook cell. **Example Code to Start With:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset cars = pd.read_csv(\'cars.csv\') # Create the scatter plot sns.set_theme(style=\\"whitegrid\\") # Write your code here # ... # Ensure to create facets by \'make\' and follow the question constraints plt.show() ``` # Performance Considerations: Ensure that your code runs efficiently even for datasets with up to 1000 rows. Avoid unnecessary data duplications or computations.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_car_scatter_plot(csv_filename): # Load the dataset cars = pd.read_csv(csv_filename) # Set Seaborn theme sns.set_theme(style=\\"whitegrid\\") # Create scatter plot with required parameters scatterplot = sns.relplot( data=cars, x=\\"engine_size\\", y=\\"city_mpg\\", hue=\\"highway_mpg\\", size=\\"engine_size\\", sizes=(50, 300), style=\\"body_style\\", palette=\\"viridis\\", alpha=0.7, height=8, aspect=1, col=\\"make\\", col_wrap=2 ) # Enhance plot with labels and titles scatterplot.set_axis_labels(\\"Engine Size (Liters)\\", \\"City MPG\\") scatterplot.fig.suptitle(\\"Scatter Plot of Engine Size vs City MPG\\", y=1.05) # Show plot plt.show()"},{"question":"**Title:** Implementing an Asynchronous File Downloader **Description:** You are tasked with creating a Python program that downloads files asynchronously using the `asyncio` library. This will test your ability to manage concurrent tasks and handle IO-bound operations efficiently. **Task:** Implement an asynchronous function `download_files` that takes a list of file URLs and a destination directory as input, downloads all the files concurrently, and saves them to the specified directory. **Function Signature:** ```python import asyncio async def download_files(urls: list, dest_dir: str) -> None: # Implementation here ``` **Expected Input:** - `urls`: A list of strings, where each string is a URL pointing to a file to be downloaded. - `dest_dir`: A string representing the destination directory where the files will be saved. **Expected Output:** - The function does not return any value but downloads all specified files to the `dest_dir`. **Constraints:** 1. Each URL is assumed to be valid and accessible. 2. The destination directory exists and is writable. 3. You may use any HTTP client library compatible with `asyncio` (e.g., `aiohttp`). **Performance Requirements:** - The function should handle up to 100 URLs efficiently. - Ensure that the solution does not block the main thread and allows other tasks to proceed concurrently. **Example:** ```python import asyncio import os from aiohttp import ClientSession async def download_files(urls: list, dest_dir: str) -> None: async def fetch_url(session, url): async with session.get(url) as response: filename = os.path.join(dest_dir, url.split(\'/\')[-1]) with open(filename, \'wb\') as f: f.write(await response.read()) async with ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] await asyncio.gather(*tasks) # Example usage: urls = [ \\"https://example.com/file1.txt\\", \\"https://example.com/file2.jpg\\", # -- more URLs -- ] dest_dir = \\"/path/to/downloads\\" # Run the download asyncio.run(download_files(urls, dest_dir)) ``` In this example, the `download_files` function uses the `aiohttp` library to fetch each URL asynchronously and save the file to the specified directory. The `await asyncio.gather(*tasks)` ensures all downloads are run concurrently.","solution":"import asyncio import os from aiohttp import ClientSession async def download_files(urls: list, dest_dir: str) -> None: async def fetch_url(session, url): async with session.get(url) as response: filename = os.path.join(dest_dir, url.split(\'/\')[-1]) content = await response.read() with open(filename, \'wb\') as f: f.write(content) async with ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] await asyncio.gather(*tasks)"},{"question":"**Problem Statement: Color Conversion Utility** Your task is to create a utility class that leverages the `colorsys` module to convert colors between different color spaces. This class should provide methods to perform conversions both individually and collectively. # Class Definition Create a class `ColorConverter` with the following methods: 1. `__init__(self, r, g, b)`: Initializes the object with RGB values. RGB values are assumed to be in the range [0, 1]. 2. `to_yiq(self)`: Converts the stored RGB values to YIQ and returns them as a tuple `(y, i, q)`. 3. `from_yiq(self, y, i, q)`: Updates the stored RGB values by converting the provided YIQ values to RGB. 4. `to_hls(self)`: Converts the stored RGB values to HLS and returns them as a tuple `(h, l, s)`. 5. `from_hls(self, h, l, s)`: Updates the stored RGB values by converting the provided HLS values to RGB. 6. `to_hsv(self)`: Converts the stored RGB values to HSV and returns them as a tuple `(h, s, v)`. 7. `from_hsv(self, h, s, v)`: Updates the stored RGB values by converting the provided HSV values to RGB. # Constraints - All input RGB values will be floating point numbers in the range [0, 1]. - For the YIQ color space, Y will be in the range [0, 1], but I and Q can be negative. - For the HLS and HSV color spaces, all values will be in the range [0, 1]. # Examples ```python # Initialize with RGB values converter = ColorConverter(0.2, 0.4, 0.4) # Convert to YIQ yiq = converter.to_yiq() print(yiq) # Example Output: (0.3479999999999999, -0.09200000000000008, -0.007999999999999952) # Update RGB values from YIQ converter.from_yiq(yiq[0], yiq[1], yiq[2]) print(converter.to_hls()) # Example Output: (0.5, 0.30000000000000004, 0.3333333333333333) # Convert to HLS hls = converter.to_hls() print(hls) # Example Output: (0.5, 0.30000000000000004, 0.3333333333333333) # Update RGB values from HLS converter.from_hls(hls[0], hls[1], hls[2]) print(converter.to_hsv()) # Example Output: (0.5, 0.5, 0.4) ``` Write the complete implementation for the `ColorConverter` class.","solution":"import colorsys class ColorConverter: def __init__(self, r, g, b): self.r = r self.g = g self.b = b def to_yiq(self): return colorsys.rgb_to_yiq(self.r, self.g, self.b) def from_yiq(self, y, i, q): self.r, self.g, self.b = colorsys.yiq_to_rgb(y, i, q) def to_hls(self): return colorsys.rgb_to_hls(self.r, self.g, self.b) def from_hls(self, h, l, s): self.r, self.g, self.b = colorsys.hls_to_rgb(h, l, s) def to_hsv(self): return colorsys.rgb_to_hsv(self.r, self.g, self.b) def from_hsv(self, h, s, v): self.r, self.g, self.b = colorsys.hsv_to_rgb(h, s, v)"},{"question":"Objective Implement a function that calculates the geometric properties of a given list of 2D points including: 1. The centroid of the points. 2. The pair of points that are the farthest apart, and the distance between them. Function Signature ```python def geometric_properties(points: List[Tuple[float, float]]) -> Tuple[Tuple[float, float], Tuple[Tuple[float, float], Tuple[float, float], float]]: pass ``` Input - `points`: A list of tuples, where each tuple represents the coordinates of a point in a 2D plane. For example: `[(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)]` Output - The function should return a tuple where: 1. The first element is a tuple representing the centroid of the points. 2. The second element is a tuple containing: - The pair of points that are the farthest apart. - The distance between these two points. Constraints - The list of points will contain at least two points. - The coordinates of each point will be finite floating-point numbers. Example ```python assert geometric_properties([(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)]) == ((3.0, 4.0), ((1.0, 2.0), (5.0, 6.0), 5.656854249492381)) assert geometric_properties([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]) == ((0.5, 0.5), ((0.0, 0.0), (1.0, 1.0), 1.4142135623730951)) ``` Notes - Use the functions provided by the `math` module such as `math.hypot()` and `math.dist()` where appropriate. - The centroid of a set of points ((x_1, y_1), (x_2, y_2), dots, (x_n, y_n)) is given by ((frac{x_1+x_2+dots+x_n}{n}, frac{y_1+y_2+dots+y_n}{n})).","solution":"from typing import List, Tuple import math def geometric_properties(points: List[Tuple[float, float]]) -> Tuple[Tuple[float, float], Tuple[Tuple[float, float], Tuple[float, float], float]]: # Calculate the centroid n = len(points) sum_x = sum(p[0] for p in points) sum_y = sum(p[1] for p in points) centroid = (sum_x / n, sum_y / n) # Calculate the pair of points with the maximum distance max_distance = 0 point_pair = (points[0], points[1]) for i in range(n): for j in range(i + 1, n): p1, p2 = points[i], points[j] distance = math.dist(p1, p2) if distance > max_distance: max_distance = distance point_pair = (p1, p2) return centroid, (point_pair[0], point_pair[1], max_distance)"},{"question":"# **Coding Assessment Question: Advanced DataClass Management** **Objective:** You are required to implement a class using Python\'s dataclasses that mimics a library management system. The goal is to assess your understanding of creating classes, handling class and instance variables, managing immutability, and controlling post-initialization behavior. **Task:** Create a `Book` class using Python\'s `dataclasses` module with the following specifications: 1. **Attributes:** - `title` (str): The title of the book. - `author` (str): The author of the book. - `year` (int): The publication year of the book. - `in_stock` (bool): Indicates whether the book is currently available in the library. Default is `True`. - `init_date` (datetime): The date and time when the book was added to the system. 2. **Requirements:** - The `title`, `author`, and `year` should be immutable after the creation (i.e., frozen). - The `init_date` should automatically be set to the current date and time at the instance initialization (`post-init processing`). - Create a class variable `total_books` that keeps track of the total number of books created. - Define a method `checkout()` that sets `in_stock` to `False`. - Define a method `return_book()` that sets `in_stock` to `True`. 3. **Input and Output Specifications:** - There will be no direct input or output operations. - You should provide implementations as described and adequate test cases demonstrating the correct functionality of your class methods and attributes. **Constraints:** - Use appropriate imports and decorators from the `dataclasses` module. - Ensure that the class and methods work efficiently even with a large number of book instances. - Properly handle any edge cases, such as trying to checkout a book that is not in stock. ```python # Your implementation here from dataclasses import dataclass, field from datetime import datetime @dataclass(frozen=True) class Book: # Define the attributes # Implement __post_init__ to set init_date # Implement checkout method # Implement return_book method # Example of using your Book class: # book1 = Book(title=\\"The Great Gatsby\\", author=\\"F. Scott Fitzgerald\\", year=1925) # book1.checkout() # book1.return_book() ``` **Note:** Your implementation should be followed by several test cases demonstrating the behavior of the `Book` class, including: - Creation of book instances - Checking out and returning books - Ensuring immutability of `title`, `author`, and `year` - Correct handling of `total_books` count","solution":"from dataclasses import dataclass, field from datetime import datetime @dataclass(frozen=True) class Book: title: str author: str year: int in_stock: bool = field(default=True, init=False) init_date: datetime = field(default_factory=datetime.now, init=False) _total_books: int = field(default=0, init=False, repr=False) def __post_init__(self): object.__setattr__(self, \'in_stock\', True) # Increment total_books count type(self)._total_books += 1 @classmethod def total_books(cls): return cls._total_books def checkout(self): if not self.in_stock: raise ValueError(\\"Cannot checkout a book that is not in stock.\\") object.__setattr__(self, \'in_stock\', False) def return_book(self): if self.in_stock: raise ValueError(\\"Cannot return a book that is already in stock.\\") object.__setattr__(self, \'in_stock\', True)"},{"question":"Objective: To assess the studentâ€™s ability to interact with the filesystem, manipulate data, and handle command line inputs using Python\'s standard library modules. Task: Write a Python script that performs the following tasks: 1. **Command Line Interaction**: - The script should accept command line arguments for a directory path (`dir_path`) and a file extension (`file_ext`). - It should also have an optional argument indicating the maximum number of files to process (`max_files`). 2. **File Manipulation**: - The script should scan the specified directory for all files with the given extension. - If `max_files` is specified, only process up to that many files. 3. **String Processing**: - For each file found, the script should read its content and identify all unique words (a word is defined as a continuous sequence of alphabetic characters). - Compute the frequency of each unique word across all files processed. 4. **Output**: - Display the top 10 most frequent words along with their counts. - Display the total number of unique words found. Input: The script should accept the following command line arguments: - `dir_path` (str): The directory to search for files. - `file_ext` (str): The file extension to filter by (e.g., `.txt`). - `max_files` (int, optional): The maximum number of files to process. Output: - Top 10 most frequent words across all processed files. - Total number of unique words found. Constraints: - You can assume that the provided directory path is valid. - The files to be processed are small enough to fit into the memory. Example Usage: ```bash python word_frequency.py /path/to/directory .txt --max_files=5 ``` Example Output: ``` Top 10 most frequent words: 1. the - 120 2. of - 115 3. and - 99 ... 10. Python - 35 Total unique words: 1500 ``` # Guidelines: - Make use of the `os` and `glob` modules for file handling. - Utilize the `argparse` module for handling command line arguments. - Use the `collections.Counter` class for counting word frequencies. - The script should be well-structured and include error handling where appropriate.","solution":"import os import glob import argparse from collections import Counter import re def get_all_files(dir_path, file_ext, max_files=None): Get a list of files from the specified directory with the given file extension. If max_files is specified, only return up to that many files. search_pattern = os.path.join(dir_path, f\\"*{file_ext}\\") all_files = glob.glob(search_pattern) if max_files: return all_files[:max_files] return all_files def read_file(file_path): Read the content of a file and return it as a string. with open(file_path, \'r\', encoding=\'utf-8\') as file: return file.read() def extract_words(text): Extract words from a text using regex. A word is defined as a continuous sequence of alphabet characters. return re.findall(r\'b[a-zA-Z]+b\', text.lower()) def main(dir_path, file_ext, max_files=None): Main function to process the files and compute the word frequencies. files = get_all_files(dir_path, file_ext, max_files) word_counter = Counter() for file in files: content = read_file(file) words = extract_words(content) word_counter.update(words) most_common_words = word_counter.most_common(10) unique_words_count = len(word_counter) print(\\"Top 10 most frequent words:\\") for i, (word, count) in enumerate(most_common_words, 1): print(f\\"{i}. {word} - {count}\\") print(f\\"nTotal unique words: {unique_words_count}\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Word frequency counter\\") parser.add_argument(\'dir_path\', type=str, help=\\"Directory path to search for files\\") parser.add_argument(\'file_ext\', type=str, help=\\"File extension to filter by (e.g., \'.txt\')\\") parser.add_argument(\'--max_files\', type=int, help=\\"Maximum number of files to process\\", default=None) args = parser.parse_args() main(args.dir_path, args.file_ext, args.max_files)"},{"question":"# Python Coding Assessment Objective: Demonstrate your understanding of the `tokenize` module by writing a script that transforms all integer literals in a Python source file into hexadecimal format. Problem Statement: Write a function `transform_integers_to_hex(file_path: str) -> str` that takes the path of a Python source file as input. The function should read the source file, tokenize it, transform all integer literals to their hexadecimal representations, and then return the modified source code as a string. Function Signature: ```python def transform_integers_to_hex(file_path: str) -> str: pass ``` Input: - `file_path` (str): The path to the Python source file to be transformed. Output: - Returns a string representing the modified source code with all integer literals transformed to hexadecimal format. Example: Assume the content of the Python script at `example.py` is: ```python def compute(): a = 10 b = 255 c = -42 print(a + b + c) ``` After calling `transform_integers_to_hex(\'example.py\')`, the output should be: ```python def compute(): a = 0xa b = 0xff c = -0x2a print(a + b + c) ``` Constraints: 1. Your solution should handle valid Python syntax only. 2. Ensure that the variable names or other non-integer literals are not altered. 3. The exact format for hexadecimal integers should be used, i.e., `0x` prefix for positive integers, `-0x` for negative integers without changing the sign. Hints: - Use `tokenize.tokenize()` to generate tokens from the file. - Use the `tokenize.untokenize()` function to convert tokens back to the source code. - The `token` module may be helpful in identifying integer literals. Additional Notes: Submit your implementation along with appropriate test cases to demonstrate the correctness of your solution.","solution":"import tokenize from io import BytesIO def transform_integers_to_hex(file_path: str) -> str: with open(file_path, \'rb\') as f: tokens = tokenize.tokenize(f.readline) modified_tokens = [] for token in tokens: if token.type == tokenize.NUMBER: try: value = int(token.string) if value < 0: hex_value = f\'-0x{-value:x}\' else: hex_value = f\'0x{value:x}\' modified_token = tokenize.TokenInfo(token.type, hex_value, token.start, token.end, token.line) except ValueError: # In case the number is not an integer, keep it unchanged modified_token = token else: modified_token = token modified_tokens.append(modified_token) # Use untokenize to convert back to source code modified_source = tokenize.untokenize(modified_tokens).decode(\'utf-8\') return modified_source"},{"question":"# PyTorch MPS Backend Assessment Objective In this task, you are required to demonstrate your understanding of the PyTorch MPS backend. You will write a function that: 1. Checks whether the MPS backend is available. 2. Creates a tensor on the MPS device (if available). 3. Moves a given model to the MPS device (if available). 4. Performs a forward pass of the model using this tensor. 5. Returns the model\'s output or an appropriate message if MPS is not available. Function Signature ```python def run_with_mps(model: torch.nn.Module) -> Union[torch.Tensor, str]: pass ``` Inputs - `model`: A PyTorch neural network model (`torch.nn.Module`). Outputs - If MPS is available, return the output tensor after performing a forward pass on the model using a tensor of ones with shape (5,). - If MPS is not available, return a message string detailing why MPS is not available (use information from the documentation). Constraints - Use the `torch.device(\\"mps\\")` identifier to specify the MPS device. - Ensure correct handling for cases where MPS is not built or the current MacOS version or hardware does not support MPS. Example ```python # Sample Model class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(5, 2) def forward(self, x): return self.linear(x) model = SimpleModel() # Running the function output = run_with_mps(model) # The output will depend on the availability of MPS in the environment if isinstance(output, torch.Tensor): print(f\\"Model output: {output}\\") else: print(f\\"Message: {output}\\") ``` This task will assess your understanding of the PyTorch MPS backend and your ability to effectively utilize it for tensor operations and model computations.","solution":"import torch from typing import Union def run_with_mps(model: torch.nn.Module) -> Union[torch.Tensor, str]: Checks if the MPS backend is available and performs a forward pass with the model on MPS device using a tensor of ones with shape (5,). If MPS is not available, returns a message indicating why MPS is not available. if not torch.backends.mps.is_available(): return \\"MPS is not available because PyTorch was not built with MPS support.\\" if not torch.backends.mps.is_built(): return \\"MPS is not available because your hardware does not support MPS.\\" device = torch.device(\\"mps\\") model.to(device) input_tensor = torch.ones(5).to(device) with torch.no_grad(): output = model(input_tensor) return output"},{"question":"# Custom Dataset and DataLoader with Multi-process Loading in PyTorch Objective: Implement a custom dataset and a custom collate function, then load the dataset using PyTorch\'s `DataLoader` with multiple worker processes. Your task is to demonstrate proficiency with PyTorchâ€™s data loading utilities, including dataset creation, collate function customization, and multi-process data loading. Instructions: 1. **Custom Dataset**: - Create a custom dataset `SquareDataset` that generates squares of integers. - The dataset should take two parameters: `start` and `end`, representing the range of integers (inclusive) for which squares will be generated. - Implement `__getitem__` and `__len__` methods. 2. **Custom Collate Function**: - Create a custom collate function `sum_collate_fn` that sums the squares for each batch. 3. **DataLoader**: - Use PyTorchâ€™s `DataLoader` to load this `SquareDataset` with the following parameters: - `batch_size`: 4 - `num_workers`: 2 - `collate_fn`: `sum_collate_fn` - Ensure that data loading is performed with multiprocessing. 4. **Output**: - Print the first 5 batches of summed squares to verify the functionality. Expected Function Signatures: ```python class SquareDataset: def __init__(self, start: int, end: int): pass def __getitem__(self, idx: int) -> int: pass def __len__(self) -> int: pass def sum_collate_fn(batch: list) -> int: pass if __name__ == \'__main__\': dataset = SquareDataset(start=1, end=20) dataloader = DataLoader(dataset, batch_size=4, num_workers=2, collate_fn=sum_collate_fn) for i, batch_sum in enumerate(dataloader): if i >= 5: break print(f\\"Batch {i+1} Sum: {batch_sum}\\") ``` Constraints: - The range of integers provided to the dataset will always be positive. - Ensure that your implementation handles the bounds correctly. - Test with a variety of ranges to ensure robustness. Performance Considerations: - Implement efficient data loading and minimize computation overhead within the dataset and collate function. - Ensure that your solution scales with an increasing number of data points and batch sizes. Good luck!","solution":"import torch from torch.utils.data import Dataset, DataLoader class SquareDataset(Dataset): def __init__(self, start: int, end: int): self.data = list(range(start, end+1)) def __getitem__(self, idx: int) -> int: return self.data[idx] ** 2 def __len__(self) -> int: return len(self.data) def sum_collate_fn(batch: list) -> int: return sum(batch) if __name__ == \'__main__\': dataset = SquareDataset(start=1, end=20) dataloader = DataLoader(dataset, batch_size=4, num_workers=2, collate_fn=sum_collate_fn) for i, batch_sum in enumerate(dataloader): if i >= 5: break print(f\\"Batch {i+1} Sum: {batch_sum}\\")"},{"question":"**Pandas Coding Assessment: Advanced Time-Series Data Analysis** **Objective**: To assess your understanding and skills in utilizing pandas window functions for time-series data analysis. **Problem Statement**: You are given a time-series dataset containing daily weather measurements: temperature, humidity, and atmospheric pressure. Your task is to implement functions to compute and analyze rolling, expanding, and exponentially weighted statistics for this dataset. **Dataset Sample**: ``` Date, Temperature, Humidity, Pressure 2021-01-01, 15, 85, 1012 2021-01-02, 16, 80, 1013 2021-01-03, 14, 78, 1014 2021-01-04, 17, 82, 1011 ... ``` The `Date` column is the index. **Requirements**: 1. **Rolling Statistics Calculation**: Implement a function `calculate_rolling_statistics` to compute the rolling mean and standard deviation for the temperature over a given window size. ```python def calculate_rolling_statistics(df: pd.DataFrame, window_size: int) -> pd.DataFrame: Computes the rolling mean and standard deviation of the \'Temperature\' column. Args: df (pd.DataFrame): The input dataframe with date as index. window_size (int): The window size for rolling calculations. Returns: pd.DataFrame: DataFrame with original columns plus \'Rolling_Mean_Temperature\' and \'Rolling_Std_Temperature\' columns. # Your Code Here ``` 2. **Expanding Statistics Calculation**: Implement a function `calculate_expanding_statistics` to compute the expanding mean and variance for the humidity. ```python def calculate_expanding_statistics(df: pd.DataFrame) -> pd.DataFrame: Computes the expanding mean and variance of the \'Humidity\' column. Args: df (pd.DataFrame): The input dataframe with date as index. Returns: pd.DataFrame: DataFrame with original columns plus \'Expanding_Mean_Humidity\' and \'Expanding_Var_Humidity\' columns. # Your Code Here ``` 3. **Exponentially Weighted Statistics Calculation**: Implement a function `calculate_ewm_statistics` to compute the exponentially weighted mean and standard deviation for the atmospheric pressure with a specified span. ```python def calculate_ewm_statistics(df: pd.DataFrame, span: int) -> pd.DataFrame: Computes the exponentially weighted mean and standard deviation of the \'Pressure\' column. Args: df (pd.DataFrame): The input dataframe with date as index. span (int): The span for EWM calculations. Returns: pd.DataFrame: DataFrame with original columns plus \'EWM_Mean_Pressure\' and \'EWM_Std_Pressure\' columns. # Your Code Here ``` **Constraints**: - The dataset will contain at least 30 days of continuous daily records. - You are expected to handle missing values appropriately by using forward fill. **Performance Requirements**: - The solution should efficiently handle datasets with up to 10,000 records. **Additional Notes**: - Remember to provide appropriate docstrings for each function. - Ensure your functions return dataframes with the new calculated columns appended to the original dataframe. You can write and test your code in a Jupyter notebook and submit it once you have verified its correctness. **Good Luck!**","solution":"import pandas as pd def calculate_rolling_statistics(df: pd.DataFrame, window_size: int) -> pd.DataFrame: Computes the rolling mean and standard deviation of the \'Temperature\' column. Args: df (pd.DataFrame): The input dataframe with date as index. window_size (int): The window size for rolling calculations. Returns: pd.DataFrame: DataFrame with original columns plus \'Rolling_Mean_Temperature\' and \'Rolling_Std_Temperature\' columns. df[\'Rolling_Mean_Temperature\'] = df[\'Temperature\'].rolling(window=window_size).mean() df[\'Rolling_Std_Temperature\'] = df[\'Temperature\'].rolling(window=window_size).std() return df def calculate_expanding_statistics(df: pd.DataFrame) -> pd.DataFrame: Computes the expanding mean and variance of the \'Humidity\' column. Args: df (pd.DataFrame): The input dataframe with date as index. Returns: pd.DataFrame: DataFrame with original columns plus \'Expanding_Mean_Humidity\' and \'Expanding_Var_Humidity\' columns. df[\'Expanding_Mean_Humidity\'] = df[\'Humidity\'].expanding().mean() df[\'Expanding_Var_Humidity\'] = df[\'Humidity\'].expanding().var() return df def calculate_ewm_statistics(df: pd.DataFrame, span: int) -> pd.DataFrame: Computes the exponentially weighted mean and standard deviation of the \'Pressure\' column. Args: df (pd.DataFrame): The input dataframe with date as index. span (int): The span for EWM calculations. Returns: pd.DataFrame: DataFrame with original columns plus \'EWM_Mean_Pressure\' and \'EWM_Std_Pressure\' columns. df[\'EWM_Mean_Pressure\'] = df[\'Pressure\'].ewm(span=span).mean() df[\'EWM_Std_Pressure\'] = df[\'Pressure\'].ewm(span=span).std() return df"},{"question":"Objective Your task is to demonstrate your understanding of PyTorch\'s JIT compilation capabilities by implementing and optimizing a simple neural network model using the `torch.utils.jit` module. Description 1. Implement a simple feedforward neural network in PyTorch. 2. Convert this neural network to a scripted module using JIT compilation. 3. Evaluate and compare the performance of the original and the scripted models. Requirements 1. **Neural Network Implementation**: - Implement a neural network with one hidden layer. - Use activation functions and other necessary components. 2. **JIT Compilation**: - Convert your neural network into a scripted module using `torch.jit.script`. 3. **Performance Evaluation**: - Create input data to test your model (you may use random data for simplicity). - Measure and compare the execution time of both the original and the scripted models on this data. Input and Output Formats - Implement the following function: ```python def compare_jit_performance(): # Define your neural network # Script the model using `torch.jit.script` # Create random input data # Measure the performance of the original model # Measure the performance of the scripted model # Return the comparison results as a dictionary ``` - **Expected Output**: - A dictionary containing the execution times of both the original and the scripted models. ```python { \\"original_time\\": float, # execution time of the original model \\"scripted_time\\": float # execution time of the scripted model } ``` Constraints - Ensure your code runs efficiently and handle any potential errors during model scripting. - The input data for model evaluation should be a 2D tensor of size `[1000, input_features]` where `input_features` is the number of input features for your neural network.","solution":"import torch import torch.nn as nn import torch.jit import time class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def compare_jit_performance(input_features=10, hidden_size=50, output_size=1, num_samples=1000): # Define the neural network model model = SimpleNN(input_features, hidden_size, output_size) # Convert the model to a scripted module using torch.jit.script scripted_model = torch.jit.script(model) # Create random input data input_data = torch.randn(num_samples, input_features) # Measure the performance of the original model start_time = time.time() _ = model(input_data) original_time = time.time() - start_time # Measure the performance of the scripted model start_time = time.time() _ = scripted_model(input_data) scripted_time = time.time() - start_time # Return the comparison results as a dictionary return { \\"original_time\\": original_time, \\"scripted_time\\": scripted_time }"},{"question":"Dataclass Management System Objective Demonstrate your understanding of Python dataclasses by creating and manipulating a system of interconnected dataclasses. This task will test your ability to correctly define, initialize, and manage dataclasses using features such as default values, post-initialization logic, inheritance, and utility functions. Task You are tasked with creating a small library management system consisting of the following dataclasses: 1. **Author**: Represents an author with their name and email address. 2. **Book**: Represents a book with its title, author, publication year, and genres. 3. **Library**: Manages a collection of books, enabling adding and removing books, and providing methods to get books by various criteria. Requirements 1. **Author** - Fields: - `name: str` - `email: str` - This class has no custom behavior. 2. **Book** - Fields: - `title: str` - `author: Author` - `year: int` - `genres: list[str]` (default value should be an empty list) - Methods: - `__post_init__(self)`: Ensure the year is not in the future. If it is, set it to the current year. 3. **Library** - Fields: - `books: list[Book]` (default value should be an empty list) - Methods: - `add_book(self, book: Book)`: Adds a book to the library. - `remove_book(self, title: str)`: Removes a book with the given title from the library. Raise a `ValueError` if the book is not found. - `get_books_by_author(self, author_name: str) -> list[Book]`: Returns a list of books by the specified author. - `get_books_by_genre(self, genre: str) -> list[Book]`: Returns a list of books with the specified genre. Constraints 1. The classes should be decorated using the `@dataclass` decorator. 2. Ensure the `__post_init__` method in the `Book` class properly validates the year field. 3. Utilize `field(default_factory=list)` for attributes that require a mutable default value. Example Usage ```python from typing import List # Define the Author dataclass @dataclass class Author: name: str email: str # Define the Book dataclass with appropriate post initialization validation @dataclass class Book: title: str author: Author year: int genres: List[str] = field(default_factory=list) def __post_init__(self): import datetime current_year = datetime.datetime.now().year if self.year > current_year: self.year = current_year # Define the Library dataclass with methods for book management @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def remove_book(self, title: str): for book in self.books: if book.title == title: self.books.remove(book) return raise ValueError(f\\"Book with title \'{title}\' not found\\") def get_books_by_author(self, author_name: str) -> List[Book]: return [book for book in self.books if book.author.name == author_name] def get_books_by_genre(self, genre: str) -> List[Book]: return [book for book in self.books if genre in book.genres] # Example usage author = Author(name=\\"J.K. Rowling\\", email=\\"jk.rowling@example.com\\") book = Book(title=\\"Harry Potter\\", author=author, year=1997, genres=[\\"Fiction\\", \\"Fantasy\\"]) library = Library() library.add_book(book) print(library.get_books_by_author(\\"J.K. Rowling\\")) print(library.get_books_by_genre(\\"Fantasy\\")) ``` # Submission Provide the complete implementation of the `Author`, `Book`, and `Library` classes as described above, including the example usage that demonstrates the functionality of the library management system.","solution":"from typing import List from dataclasses import dataclass, field import datetime @dataclass class Author: name: str email: str @dataclass class Book: title: str author: Author year: int genres: List[str] = field(default_factory=list) def __post_init__(self): current_year = datetime.datetime.now().year if self.year > current_year: self.year = current_year @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def remove_book(self, title: str): for book in self.books: if book.title == title: self.books.remove(book) return raise ValueError(f\\"Book with title \'{title}\' not found\\") def get_books_by_author(self, author_name: str) -> List[Book]: return [book for book in self.books if book.author.name == author_name] def get_books_by_genre(self, genre: str) -> List[Book]: return [book for book in self.books if genre in book.genres]"},{"question":"<|Analysis Begin|> The provided documentation focuses on event loops and process watchers within the asyncio library. This includes: 1. Policies for event loops: - Custom and built-in event loop policies. - Functions for getting and setting the current event loop policy. - Classes and methods involved in defining and manipulating these policies. 2. Process watchers: - Details of the AbstractChildWatcher and its various implementations. - Functions for customizing the child process watcher implementation. Key concepts include working with event loop policies, customizing behaviors through subclassing and implementing abstract base classes, and how process watchers are managed within the asyncio environment. A challenging question aligned with these concepts would involve implementing a custom event loop policy or process watcher, demonstrating understanding of abstract base classes and the intricacies of asyncio event loop management. <|Analysis End|> <|Question Begin|> # Question: Implement a Custom Event Loop Policy In this exercise, you will demonstrate your understanding of asyncio\'s event loop policies by implementing a custom event loop policy. Your custom policy will override certain behaviors of the default policy and introduce logging functionality each time the event loop is retrieved or set. Objectives 1. Implement a class `LoggingEventLoopPolicy` that inherits from `asyncio.DefaultEventLoopPolicy`. 2. Override the methods `get_event_loop()` and `set_event_loop(loop)` in your custom policy to add logging (print statements) when these methods are called. 3. Ensure your custom policy retrieves and sets the event loop correctly while maintaining the default behavior except for the additional logging. Requirements - Define the class `LoggingEventLoopPolicy`. - Override the following methods: - `get_event_loop(self):` Should print \\"Getting the event loop...\\" before calling the superclass method and returning the event loop. - `set_event_loop(self, loop):` Should print \\"Setting the event loop...\\" before calling the superclass method to set the event loop to `loop`. - Set your custom policy as the current event loop policy using `asyncio.set_event_loop_policy()`. - Write a function `test_custom_policy()` to test your custom policy: - This function should set up the `LoggingEventLoopPolicy`, retrieve the event loop, and then set a new event loop to demonstrate the logging behavior. Example ```python import asyncio # Define the custom event loop policy with logging class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Getting the event loop...\\") return super().get_event_loop() def set_event_loop(self, loop): print(\\"Setting the event loop...\\") super().set_event_loop(loop) # Function to test the custom policy def test_custom_policy(): policy = LoggingEventLoopPolicy() asyncio.set_event_loop_policy(policy) # Get and set event loops to demonstrate custom logging loop = asyncio.get_event_loop_policy().get_event_loop() new_loop = asyncio.new_event_loop() asyncio.get_event_loop_policy().set_event_loop(new_loop) # Execute the test function test_custom_policy() ``` Expected Output ``` Getting the event loop... Setting the event loop... ``` This exercise will test your ability to work with class inheritance, understand asynchronous programming concepts, and apply customization to built-in libraries in Python.","solution":"import asyncio class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Getting the event loop...\\") return super().get_event_loop() def set_event_loop(self, loop): print(\\"Setting the event loop...\\") super().set_event_loop(loop) def test_custom_policy(): policy = LoggingEventLoopPolicy() asyncio.set_event_loop_policy(policy) # Get and set event loops to demonstrate custom logging loop = asyncio.get_event_loop_policy().get_event_loop() new_loop = asyncio.new_event_loop() asyncio.get_event_loop_policy().set_event_loop(new_loop)"},{"question":"# Comprehensive System Information Report You are tasked to create a Python utility function that generates a comprehensive system information report by consolidating various pieces of data provided by different functions in the `platform` module. The utility should extract specific information about the current system and return them in a structured format. Function Signature ```python def generate_system_report(): # returns a dictionary containing system information ``` Input The `generate_system_report` function does not take any parameters. Output The function should return a dictionary containing the following keys and their respective system information: - `architecture`: with sub-keys `bits` and `linkage`. - `machine` - `network_name` - `platform` - `processor` - `python_version_info`: with sub-keys `version`, `build`, `compiler`, `branch`, `implementation`, and `revision`. - `os_release` - `system` - `system_version` - `uname`: with keys `system`, `node`, `release`, `version`, `machine`, `processor`. Example Output ```python { \\"architecture\\": {\\"bits\\": \\"64bit\\", \\"linkage\\": \\"ELF\\"}, \\"machine\\": \\"x86_64\\", \\"network_name\\": \\"my-computer\\", \\"platform\\": \\"Linux-5.4.0-42-generic-x86_64-with-glibc2.29\\", \\"processor\\": \\"x86_64\\", \\"python_version_info\\": { \\"version\\": \\"3.10.0\\", \\"build\\": (\\"default\\", \\"Sep 28 2023\\"), \\"compiler\\": \\"GCC 7.5.0\\", \\"branch\\": \\"main\\", \\"implementation\\": \\"CPython\\", \\"revision\\": \\"1c3be2d7e799\\" }, \\"os_release\\": \\"5.4.0-42-generic\\", \\"system\\": \\"Linux\\", \\"system_version\\": \\"#46~18.04.1-Ubuntu SMP Tue Jul 7 20:54:33 UTC 2020\\", \\"uname\\": { \\"system\\": \\"Linux\\", \\"node\\": \\"my-computer\\", \\"release\\": \\"5.4.0-42-generic\\", \\"version\\": \\"#46~18.04.1-Ubuntu SMP Tue Jul 7 20:54:33 UTC 2020\\", \\"machine\\": \\"x86_64\\", \\"processor\\": \\"x86_64\\" } } ``` Constraints - The information fields should be gathered using the provided `platform` functions. - Handle exceptions gracefully where information cannot be retrieved. Notes - Each key should be populated by the corresponding function from the `platform` module based on the provided documentation. - Use default values as specified in the functions\' descriptions where information is not available or applicable. - Insert comments in your code to explain the purpose of each step. Performance The function should be efficient and should not unduly lag in retrieving the system information.","solution":"import platform import sys def generate_system_report(): Gathers various pieces of system information and returns them in a structured format. report = {} # Retrieve architecture information architecture = platform.architecture() report[\\"architecture\\"] = {\\"bits\\": architecture[0], \\"linkage\\": architecture[1]} # Retrieve machine type report[\\"machine\\"] = platform.machine() # Retrieve network hostname report[\\"network_name\\"] = platform.node() # Retrieve the platform information report[\\"platform\\"] = platform.platform() # Retrieve processor information report[\\"processor\\"] = platform.processor() # Retrieve Python version related information python_version_info = { \\"version\\": platform.python_version(), \\"build\\": platform.python_build(), \\"compiler\\": platform.python_compiler(), \\"branch\\": \\"\\", # `branch` and `revision` might not always be available \\"implementation\\": platform.python_implementation(), \\"revision\\": \\"\\" } report[\\"python_version_info\\"] = python_version_info # Retrieve OS release information report[\\"os_release\\"] = platform.release() # Retrieve system type (OS) report[\\"system\\"] = platform.system() # Retrieve additional system version information report[\\"system_version\\"] = platform.version() # Retrieve uname system information uname_info = platform.uname() uname_dict = { \\"system\\": uname_info.system, \\"node\\": uname_info.node, \\"release\\": uname_info.release, \\"version\\": uname_info.version, \\"machine\\": uname_info.machine, \\"processor\\": uname_info.processor } report[\\"uname\\"] = uname_dict return report"},{"question":"Objective: Implement a TorchScript class that allows you to: 1. Initialize a 2D tensor of arbitrary dimensions. 2. Perform various tensor operations (addition, multiplication, and transpose). 3. Validate type constraints and statically ensure type correctness. Question: You need to implement a TorchScript class `MatrixOps` in PyTorch, following the constraints and type checks as described in the documentation. Requirements: 1. **Initialization**: - The class should be initialized with a 2D tensor. - Perform validation to ensure the input is a tensor. 2. **Methods**: - `add_matrix`: Adds another matrix (tensor) of the same dimensions and returns the result. - `multiply_matrix`: Multiplies the matrix element-wise with another matrix (tensor) of the same dimensions and returns the result. - `transpose_matrix`: Transposes the matrix and returns the result. 3. **Type Constraints**: - Use appropriate type annotations for class methods and attributes. - Ensure that operations adhere to the type constraints imposed by TorchScript. Specifications: - Input tensor should be of type `torch.Tensor` with 2D shape. - Methods should raise appropriate errors when input dimensions do not match the required constraints. - The class and its methods should be decorated with `@torch.jit.script` to enforce TorchScript constraints. Example: ```python import torch @torch.jit.script class MatrixOps: def __init__(self, matrix: torch.Tensor): self.matrix = matrix def add_matrix(self, other: torch.Tensor) -> torch.Tensor: assert self.matrix.size() == other.size(), \\"Matrices must have the same dimensions\\" return self.matrix + other def multiply_matrix(self, other: torch.Tensor) -> torch.Tensor: assert self.matrix.size() == other.size(), \\"Matrices must have the same dimensions\\" return self.matrix * other def transpose_matrix(self) -> torch.Tensor: return self.matrix.t() # Example Usage: a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) b = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) matrix_ops = MatrixOps(a) print(matrix_ops.add_matrix(b)) print(matrix_ops.multiply_matrix(b)) print(matrix_ops.transpose_matrix()) ``` Constraints: - You should ensure that the tensor input is 2D. - The methods `add_matrix` and `multiply_matrix` should only accept tensors of matching dimensions. - Handle any exceptions that arise due to type mismatches or incorrect tensor dimensions. Submission: Submit your implementation of the `MatrixOps` class ensuring all type constraints and TorchScript compatibility as specified above.","solution":"import torch from typing import Tuple @torch.jit.script class MatrixOps: def __init__(self, matrix: torch.Tensor): assert matrix.dim() == 2, \\"Input must be a 2D tensor.\\" self.matrix = matrix def add_matrix(self, other: torch.Tensor) -> torch.Tensor: assert other.dim() == 2, \\"Input must be a 2D tensor.\\" assert self.matrix.size() == other.size(), \\"Matrices must have the same dimensions.\\" return self.matrix + other def multiply_matrix(self, other: torch.Tensor) -> torch.Tensor: assert other.dim() == 2, \\"Input must be a 2D tensor.\\" assert self.matrix.size() == other.size(), \\"Matrices must have the same dimensions.\\" return self.matrix * other def transpose_matrix(self) -> torch.Tensor: return self.matrix.t()"},{"question":"**Objective:** The objective of this task is to assess your ability to visualize data using seaborn\'s advanced plotting functionalities, particularly involving customization and handling overlapping data points. **Problem Description:** You are provided with the `diamonds` dataset which contains information about diamonds, including their `price`, `cut`, and `clarity`. Write a function `customize_diamond_plot()` that generates and customizes a seaborn bar plot to meet the following requirements: **Function Specifications:** - **Input:** - `data` (DataFrame): DataFrame containing the `diamonds` dataset. - **Output:** - Return the seaborn plot object. **Requirements:** 1. Create a histogram plot of the `price` feature using a logarithmic scale for the x-axis. 2. Add bars representing the histogram using `so.Bars()`. 3. Color the bars based on the `cut` feature to show variation. 4. Use a `Stack` transform to resolve any overlapping bars. 5. Customize the bars: - Set the `edgewidth` to be `0` to give the bars a full width appearance. - Use `alpha` to map the `clarity` feature for transparency. 6. Return the final plot object. **Constraints:** - Assume that the dataset is correctly loaded and passed as a DataFrame. - Ensure the code is efficient and well-commented. **Example Usage:** ```python import seaborn as sns import seaborn.objects as so # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Define the function def customize_diamond_plot(data): p = so.Plot(data, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\", color=\\"cut\\") return p # Generate the plot diamond_plot = customize_diamond_plot(diamonds) ``` When executed, `customize_diamond_plot(diamonds)` should return a seaborn plot object that meets the specified requirements.","solution":"import seaborn as sns import seaborn.objects as so def customize_diamond_plot(data): Generates and customizes a seaborn bar plot for the diamonds dataset. Parameters: - data (DataFrame): DataFrame containing the diamonds dataset. Returns: - The seaborn plot object. p = so.Plot(data, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\", color=\\"cut\\") return p"},{"question":"In this coding assessment, you will demonstrate your understanding of Seaborn\'s `cubehelix_palette` function by creating specific color palettes based on provided specifications. # Task 1. **Default Discrete Palette with 5 Colors** - Generate a discrete palette using `cubehelix_palette` function with 5 colors. 2. **Continuous Colormap with Custom Hue and Saturation** - Create a continuous colormap with the following specifications: - Hue: 2 - Saturation: 1.5 3. **Reverse Luminance Palette with Custom Rotation** - Create a color palette with a reverse direction of luminance ramp and 0.5 rotation in the helix. 4. **Custom Luminance and Gamma Correction Palette** - Generate a palette with: - Dark luminance set to 0.3 - Light luminance set to 0.8 - Gamma correction set to 0.7 5. **Combined Custom Palette** - Create a color palette incorporating multiple parameters: - Start: 3 - Rotation: -0.4 - Hue: 1 - Dark luminance: 0.2 - Light luminance: 0.9 - Number of colors: 7 # Requirements For each task: - Implement the function that generates the specified palette with its requirements. - Display the palette using Seaborn\'s `palplot` function. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Default Discrete Palette with 5 Colors palette_1 = sns.cubehelix_palette(5) sns.palplot(palette_1) plt.show() # Implement tasks 2-5 by following the example given for task 1. # ... ``` # Note - Make sure to import necessary libraries like `seaborn` and `matplotlib.pyplot` at the beginning of your script. - Use descriptive variable names for each palette to maintain clarity in your code. Our evaluation will focus on the correctness and clarity of your implementation. Ensure that each palette generated matches its specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_palettes(): # Task 1: Default Discrete Palette with 5 Colors palette_1 = sns.cubehelix_palette(5) sns.palplot(palette_1) plt.show() # Task 2: Continuous Colormap with Custom Hue and Saturation palette_2 = sns.cubehelix_palette(as_cmap=True, rot=2, gamma=1.5) sns.palplot(palette_2.colors) plt.show() # Task 3: Reverse Luminance Palette with Custom Rotation palette_3 = sns.cubehelix_palette(reverse=True, rot=0.5) sns.palplot(palette_3) plt.show() # Task 4: Custom Luminance and Gamma Correction Palette palette_4 = sns.cubehelix_palette(start=0, rot=0, dark=0.3, light=0.8, gamma=0.7) sns.palplot(palette_4) plt.show() # Task 5: Combined Custom Palette palette_5 = sns.cubehelix_palette(n_colors=7, start=3, rot=-0.4, hue=1, dark=0.2, light=0.9) sns.palplot(palette_5) plt.show()"},{"question":"Python Coding Assessment Question # Objective Demonstrate your understanding of Python\'s `errno` module and exception handling. # Problem Statement Write a function `error_to_exception` that takes an integer error code as input and returns the corresponding exception class if it exists. If the error code does not map to any exception class, the function should return `None`. # Function Signature ```python def error_to_exception(error_code: int) -> type: pass ``` # Input - `error_code`: An integer representing a system error code. # Output - The exception class corresponding to the provided error code, or `None` if there\'s no corresponding class. # Constraints - You can assume all inputs are valid integers. - You need to handle the mapping based on the standard `errno` module in Python. # Examples ```python assert error_to_exception(1) == PermissionError assert error_to_exception(2) == FileNotFoundError assert error_to_exception(9999) == None assert error_to_exception(32) == BrokenPipeError ``` # Additional Requirements: - Utilize the `errno` module for getting the mappings. - Your implementation should be efficient and handle the mappings correctly. # Hints - Refer to `errno.errorcode` to find the string name of the error code. - Use the built-in exception classes if they exist, otherwise return `None`.","solution":"import errno def error_to_exception(error_code: int) -> type: Given an error code, return the corresponding exception class if it exists, or None if it doesn\'t. # mapping of errno error codes to their corresponding exception classes error_map = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.EIO: IOError, errno.ENXIO: OSError, errno.E2BIG: OSError, errno.ENOEXEC: OSError, errno.EBADF: OSError, errno.ECHILD: ChildProcessError, errno.EAGAIN: BlockingIOError, errno.ENOMEM: MemoryError, errno.EACCES: PermissionError, errno.EFAULT: OSError, errno.ENOTBLK: OSError, errno.EBUSY: OSError, errno.EEXIST: FileExistsError, errno.EXDEV: OSError, errno.ENODEV: OSError, errno.ENOTDIR: NotADirectoryError, errno.EISDIR: IsADirectoryError, errno.EINVAL: ValueError, errno.ENFILE: OSError, errno.EMFILE: OSError, errno.ENOTTY: OSError, errno.ETXTBSY: OSError, errno.EFBIG: OSError, errno.ENOSPC: OSError, errno.ESPIPE: OSError, errno.EROFS: OSError, errno.EMLINK: OSError, errno.EPIPE: BrokenPipeError, errno.EDOM: ValueError, errno.ERANGE: OverflowError, # More mappings can be added here if necessary } return error_map.get(error_code, None)"},{"question":"Objective Create a Python function that mimics the behavior of Python\'s built-in `open()` function but ensures that any file operations (reading and writing) are performed using low-level file descriptors behind the scenes. This will demonstrate an understanding of handling file operations at a lower level while still providing a familiar interface. Task Implement the following function: ```python def custom_open(file_path: str, mode: str, buffering: int = -1, encoding: str = None, errors: str = None, newline: str = None, closefd: bool = True): Open a file and return a file object. This function mimics Python\'s built-in open() function, but uses low-level file descriptors for reading and writing. Parameters: - file_path (str): The path to the file to open. - mode (str): The mode in which to open the file. - buffering (int, optional): The buffer settings. Default is -1 which means use default buffering. - encoding (str, optional): The encoding to use. Default is None. - errors (str, optional): The error handling scheme. Default is None. - newline (str, optional): Controls how newlines are handled. Default is None. - closefd (bool, optional): If True (default), the file descriptor will be closed when the file is closed. Returns: - file object: A file object that can be used to read or write as per the provided mode. pass ``` Expected Input and Output: - The `file_path` should be a valid string representing an existing file path. - The `mode` should be a valid string representing the file mode (\'r\', \'w\', \'a\', etc.). - The function should return a file object that supports methods like `read()`, `write()`, `readline()`, and `close()`. - The function should raise appropriate exceptions in cases of errors such as file not found or invalid mode. Constraints: - You must use the `os.open()` function to open the file and obtain a file descriptor. - You should use the `os.read()`, `os.write()`, and other relevant os functions to perform file reading and writing. - Ensure that the file is properly closed after operations are completed to prevent resource leakage. - Maintain compatibility with both text and binary modes. Notes: - Mimic the behavior of `io.open()` as closely as possible including handling different modes, buffering, and encoding. - Focus on ensuring that the lower-level file operations are correctly translated to higher-level file methods. Example Usage: ```python # Writing to a file with custom_open(\\"example.txt\\", \\"w\\") as f: f.write(\\"Hello World\\") # Reading from a file with custom_open(\\"example.txt\\", \\"r\\") as f: content = f.read() print(content) # Output: Hello World ``` Evaluation Criteria: - Correct implementation of file operations using lower-level file descriptors. - Handling different modes (read, write, append) correctly. - Proper exception handling and resource management. - Code readability and adherence to Python conventions.","solution":"import os class CustomFile: def __init__(self, file_path, mode, fd, encoding=None): self.file_path = file_path self.mode = mode self.fd = fd self.encoding = encoding self.closed = False def read(self, size=-1): if \'r\' not in self.mode: raise IOError(\\"File not open for reading\\") if size == -1: buffer = bytearray() while chunk := os.read(self.fd, 4096): buffer.extend(chunk) return buffer.decode(self.encoding) if self.encoding else bytes(buffer) else: buffer = os.read(self.fd, size) return buffer.decode(self.encoding) if self.encoding else buffer def write(self, data): if \'w\' not in self.mode and \'a\' not in self.mode: raise IOError(\\"File not open for writing\\") if isinstance(data, str) and self.encoding: data = data.encode(self.encoding) return os.write(self.fd, data) def close(self): if not self.closed: os.close(self.fd) self.closed = True def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.close() def custom_open(file_path: str, mode: str, buffering: int = -1, encoding: str = None, errors: str = None, newline: str = None, closefd: bool = True): os_mode = 0 if \'r\' in mode: os_mode |= os.O_RDONLY if \'w\' in mode: os_mode |= os.O_WRONLY | os.O_CREAT | os.O_TRUNC if \'a\' in mode: os_mode |= os.O_WRONLY | os.O_CREAT | os.O_APPEND if \'+\' in mode: os_mode = os.O_RDWR fd = os.open(file_path, os_mode) return CustomFile(file_path, mode, fd, encoding)"},{"question":"# Python Coding Assessment Question Objective: You are required to demonstrate your understanding of the `copyreg` module and its interaction with the `pickle` module. Specifically, you will implement a custom reduction function for a complex class and register it with `copyreg` to correctly handle the serialization and deserialization processes. Problem Statement: Implement a class `Person`, which includes attributes for a person\'s `name`, `age`, and a complex attribute `address` which itself is an instance of another class `Address`. Then, write the appropriate reduction functions and register them using `copyreg` so that instances of `Person` can be serialized and deserialized using the `pickle` module. Below are the detailed class definitions and requirements: 1. **Class `Address`:** - Attributes: - `street`: a string - `city`: a string - `zipcode`: an integer 2. **Class `Person`:** - Attributes: - `name`: a string - `age`: an integer - `address`: an instance of the `Address` class 3. **Reduction Functions:** - Create reduction functions for both `Person` and `Address` classes. - Ensure that the `address` attribute of `Person` is correctly handled during serialization and deserialization. 4. **Tests:** - Create an instance of `Person` with nested `Address` information. - Use the `pickle` module to serialize and deserialize the `Person` instance. - Verify that the deserialized object maintains the same state as the original. Constraints: - The `Address` and `Person` constructors should not be modified. - Ensure your solution is effective and reasonably efficient. Example: ```python import copyreg import pickle # Class Definitions class Address: def __init__(self, street, city, zipcode): self.street = street self.city = city self.zipcode = zipcode class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address # Reduction Functions def pickle_address(address): return Address, (address.street, address.city, address.zipcode) def pickle_person(person): return Person, (person.name, person.age, person.address) # Register Classes with copyreg copyreg.pickle(Address, pickle_address) copyreg.pickle(Person, pickle_person) # Test Cases if __name__ == \\"__main__\\": addr = Address(\\"123 Main St\\", \\"Anytown\\", 12345) person = Person(\\"John Doe\\", 30, addr) # Serialize serialized_person = pickle.dumps(person) # Deserialize deserialized_person = pickle.loads(serialized_person) assert deserialized_person.name == person.name assert deserialized_person.age == person.age assert deserialized_person.address.street == person.address.street assert deserialized_person.address.city == person.address.city assert deserialized_person.address.zipcode == person.address.zipcode print(\\"All tests passed!\\") ``` Your solution should successfully serialize and deserialize `Person` instances while preserving their state.","solution":"import copyreg import pickle class Address: def __init__(self, street, city, zipcode): self.street = street self.city = city self.zipcode = zipcode class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address def pickle_address(address): return Address, (address.street, address.city, address.zipcode) def pickle_person(person): return Person, (person.name, person.age, person.address) copyreg.pickle(Address, pickle_address) copyreg.pickle(Person, pickle_person)"},{"question":"**Question:** Suppose you are given a Python script that performs various computational tasks. Your goal is to analyze the performance of a specific function in this script. Write a Python program that: 1. Uses the `cProfile` module to profile the execution of the given function. 2. Saves the profiling results to a file. 3. Uses the `pstats` module to load the profiling results and print a detailed analysis. The Python function to be profiled is named `compute_statistics` and is defined as follows: ```python def compute_statistics(data): Computes and prints various statistics (mean, median, variance) of the given dataset. import statistics print(\\"Mean:\\", statistics.mean(data)) print(\\"Median:\\", statistics.median(data)) print(\\"Variance:\\", statistics.variance(data)) # Example usage data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] compute_statistics(data) ``` # Instructions: 1. Profile the `compute_statistics` function using `cProfile` and save the results to a file named `profile_results.prof`. 2. Load the profiling results using the `pstats` module. 3. Print the following analysis from the profiling results: - The total number of function calls. - The total time taken by the `compute_statistics` function. - The cumulative time spent in the `compute_statistics` function. - List the top 5 functions sorted by the time spent within each function (including sub-functions). # Constraints: - Assume that the `compute_statistics` function is called with a sufficiently large dataset (at least 10,000 elements) to make profiling meaningful. - Your analysis output should be clear and well-formatted for readability. # Sample Input: ```python data = [i for i in range(1, 10001)] ``` # Expected Output: The output will vary depending on the system\'s performance and implementation of the `compute_statistics` function. However, it should follow a similar format to: ``` Total function calls: [Total Number of Calls] Total time: [Total Time Taken] Cumulative time in compute_statistics: [Cumulative Time] Top 5 functions by time spent: 1. [Function Name] [Time] 2. [Function Name] [Time] 3. [Function Name] [Time] 4. [Function Name] [Time] 5. [Function Name] [Time] ``` Write your solution to the problem in Python.","solution":"import cProfile import pstats def compute_statistics(data): Computes and prints various statistics (mean, median, variance) of the given dataset. import statistics print(\\"Mean:\\", statistics.mean(data)) print(\\"Median:\\", statistics.median(data)) print(\\"Variance:\\", statistics.variance(data)) def profile_function(): # Profile the \'compute_statistics\' function data = [i for i in range(1, 10001)] cProfile.runctx(\'compute_statistics(data)\', globals(), locals(), \'profile_results.prof\') def analyze_profiling(): # Load profiling results p = pstats.Stats(\'profile_results.prof\') # Print total number of function calls print(f\\"Total function calls: {p.total_calls}\\") # Print total time print(f\\"Total time: {p.total_tt:.6f}\\") # Cumulative time spent in the `compute_statistics` function p.sort_stats(\'cumulative\').print_stats(1) # List top 5 functions sorted by time spent within each function (including sub-functions) print(\\"Top 5 functions by time spent:\\") p.sort_stats(\'time\').print_stats(5) # Example usage profile_function() analyze_profiling()"},{"question":"# AsyncIO Tasks and Synchronization **Objective:** Implement a function that demonstrates the ability to perform multiple asynchronous tasks concurrently and synchronize their results. **Task:** Write an asynchronous function `fetch_and_process` that simulates fetching data from three different sources concurrently and then processes the fetched data. Each fetching operation will have a different delay to simulate network latency. # Function Signature: ```python import asyncio async def fetch_and_process() -> str: pass ``` # Requirements: 1. **Fetching Data:** - Create three asynchronous functions `fetch_data_source_1`, `fetch_data_source_2`, and `fetch_data_source_3` that simulate fetching data with `asyncio.sleep`. Each function returns a string indicating the data fetched (e.g., \'Data from source 1\', \'Data from source 2\', and \'Data from source 3\'). - `fetch_data_source_1`: Takes 2 seconds to fetch data. - `fetch_data_source_2`: Takes 3 seconds to fetch data. - `fetch_data_source_3`: Takes 1 second to fetch data. 2. **Processing Simulated Data:** - Once data is fetched from all three sources, concatenate the results into a single string separated by commas and return this string. 3. **Concurrency and Synchronization:** - Use `asyncio.gather` or similar mechanisms to ensure that the three fetching operations run concurrently. # Constraints: - You must use `asyncio` for concurrency. - The function should return only after all fetching and processing are complete. # Example Output: ```python >>> asyncio.run(fetch_and_process()) \'Data from source 1, Data from source 2, Data from source 3\' ``` Use proper async/await syntax and handle any potential exceptions that may arise during the execution. # Notes: - Ensure your code is clear and well-documented. - Test your function to validate that fetching operations indeed run concurrently.","solution":"import asyncio async def fetch_data_source_1(): await asyncio.sleep(2) return \'Data from source 1\' async def fetch_data_source_2(): await asyncio.sleep(3) return \'Data from source 2\' async def fetch_data_source_3(): await asyncio.sleep(1) return \'Data from source 3\' async def fetch_and_process(): results = await asyncio.gather( fetch_data_source_1(), fetch_data_source_2(), fetch_data_source_3() ) return \', \'.join(results)"},{"question":"# Advanced Python Assessment **Objective**: Transition from using the deprecated **imp** module to the modern **importlib** module and demonstrate advanced module manipulation techniques. **Problem Statement**: Re-implement the function `import_module_legacy` using the modern `importlib` library. The legacy function `import_module_legacy` performs module imports using the deprecated `imp` module. Your task is to rewrite this function using `importlib`, which is the recommended library for module importing. ```python import sys import imp def import_module_legacy(name): Legacy function to import a module using the imp module. Args: name (str): The name of the module to import. Returns: module: The imported module. try: return sys.modules[name] except KeyError: pass fp, pathname, description = imp.find_module(name) try: return imp.load_module(name, fp, pathname, description) finally: if fp: fp.close() ``` **Requirements**: 1. Implement `import_module_modern` that replicates the functionality of `import_module_legacy` using `importlib`. 2. Ensure your new function handles similar exceptions and clean-up tasks. 3. The function signature should be identical to `import_module_legacy`. **Expected Function Signature**: ```python import importlib def import_module_modern(name: str): Modern implementation to import a module using importlib. Args: name (str): The name of the module to import. Returns: module: The imported module. pass ``` **Constraints**: - You must use `importlib.util.find_spec()` and other relevant `importlib` utilities to achieve the desired behavior. - Ensure thread safety during module import operations, similar to the deprecated `imp` locking mechanisms. - Maintain compatibility with Python 3.6+. **Example Usage**: ```python # Suppose there\'s a module named \'jsonp\' available in the path. import_module_modern(\'jsonp\') # The above function call should import and return the \'jsonp\' module. ``` **Note**: You are not allowed to use the `imp` module in your new implementation. Rely solely on functionalities provided by `importlib`.","solution":"import importlib import sys def import_module_modern(name: str): Modern implementation to import a module using importlib. Args: name (str): The name of the module to import. Returns: module: The imported module. try: return sys.modules[name] except KeyError: pass spec = importlib.util.find_spec(name) if spec is None: raise ImportError(f\\"No module named \'{name}\'\\") module = importlib.util.module_from_spec(spec) loader = spec.loader if loader is None: raise ImportError(f\\"Cannot load module \'{name}\'\\") loader.exec_module(module) sys.modules[name] = module return module"},{"question":"# Advanced Coding Assessment: Comprehensive Archiving and Compression **Objective:** Create a Python script that can intelligently compress and archive a collection of files while providing a choice of algorithms. The script should also have features to decompress and extract files from a given archive. **Details:** 1. **Function Name:** `compress_files` 2. **Functionality:** - Accepts a list of file paths to be archived and compressed. - Allows selection of compression format (`zip`, `tar.gz`, `tar.bz2`, `tar.lzma`). - Performs the compression and creates the archive file. - Throws appropriate errors if files do not exist or unsupported algorithms are specified. 3. **Input Parameters:** - `file_paths` (List of strings): List containing paths to files. - `archive_name` (String): The desired name of the resulting archive file (without extension). - `compression_format` (String): The desired compression format (`zip`, `tar.gz`, `tar.bz2`, `tar.lzma`). 4. **Return**: - On success: The path to the created archive file. - On failure: Raise an appropriate exception with a descriptive message. 5. **Constraints:** - All files in `file_paths` must exist. - The compression format must be one of the specified formats. - Handle large files efficiently to avoid excessive memory usage. 6. **Performance Requirements**: - The solution should handle compression and decompression of multiple large files efficiently. Additionally, implement a function to decompress and extract files: 1. **Function Name:** `decompress_archive` 2. **Functionality:** - Accepts the path to an archive file and extracts its contents. 3. **Input Parameters:** - `archive_path` (String): The path to the archive file. - `extract_to` (String): The directory where the files should be extracted. 4. **Return**: - On success: List of extracted file paths. - On failure: Raise an appropriate exception with a descriptive message. **Example Usage:** ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] archive_name = \'my_archive\' compression_format = \'tar.gz\' # Compressing files archive_path = compress_files(file_paths, archive_name, compression_format) # Decompressing files extracted_files = decompress_archive(archive_path, \'./extracted\') ``` **Notes:** - Ensure your code is well-documented and follows PEP 8 standards. - Include necessary import statements for the specified modules. - Handle potential errors gracefully and provide meaningful error messages.","solution":"import os import tarfile import zipfile def compress_files(file_paths, archive_name, compression_format): Compresses the given files into an archive using the specified compression format. :param file_paths: List of strings, paths to the files to compress. :param archive_name: String, name of the resulting archive file (without extension). :param compression_format: String, desired compression format (\'zip\', \'tar.gz\', \'tar.bz2\', \'tar.lzma\'). :return: String, path to the created archive file. :raises: ValueError if files do not exist or unsupported compression format is specified. if not all(os.path.exists(file) for file in file_paths): raise ValueError(\\"One or more files do not exist.\\") if compression_format == \'zip\': archive_path = f\\"{archive_name}.zip\\" with zipfile.ZipFile(archive_path, \'w\', zipfile.ZIP_DEFLATED) as archive: for file in file_paths: archive.write(file, os.path.basename(file)) elif compression_format in [\'tar.gz\', \'tar.bz2\', \'tar.lzma\']: mode = \'w:gz\' if compression_format == \'tar.gz\' else \'w:bz2\' if compression_format == \'tar.bz2\' else \'w:xz\' extension = \'.tar.gz\' if compression_format == \'tar.gz\' else \'.tar.bz2\' if compression_format == \'tar.bz2\' else \'.tar.lzma\' archive_path = f\\"{archive_name}{extension}\\" with tarfile.open(archive_path, mode) as archive: for file in file_paths: archive.add(file, arcname=os.path.basename(file)) else: raise ValueError(\\"Unsupported compression format specified.\\") return archive_path def decompress_archive(archive_path, extract_to): Decompresses and extracts the files from the given archive file. :param archive_path: String, path to the archive file. :param extract_to: String, directory where the files should be extracted. :return: List of strings, paths to the extracted files. :raises: ValueError if the archive file does not exist or is unsupported. if not os.path.exists(archive_path): raise ValueError(\\"The archive file does not exist.\\") extracted_files = [] if zipfile.is_zipfile(archive_path): with zipfile.ZipFile(archive_path, \'r\') as archive: archive.extractall(extract_to) extracted_files = archive.namelist() elif tarfile.is_tarfile(archive_path): with tarfile.open(archive_path, \'r:*\') as archive: archive.extractall(extract_to) extracted_files = archive.getnames() else: raise ValueError(\\"Unsupported archive format.\\") return [os.path.join(extract_to, file) for file in extracted_files]"},{"question":"You are required to implement a Python class and a function that demonstrate your understanding of naming, binding, and exceptions as described in the provided documentation. # Task 1. **Class Definitions and Functions**: - Define a class `DynamicResolver` that includes: - An initialization method `__init__` that sets up an attribute `self.name`. - A method `bind_name` that takes a parameter `name_value` and binds this parameter to `self.name`. - A method `resolve_name` that attempts to: - Print `self.name`. - Raise and catch an exception if `self.name` is `None`, printing an appropriate error message. - Use a global variable if `self.name` is not defined within the method. 2. **Dynamic Code Execution**: - Write a separate function `execute_dynamic_code` that: - Accepts a string of code as an argument. - Executes the code using `exec()`. - Demonstrates interaction with both local and global namespaces. # Requirements - **Input and Output**: - The `bind_name` method should accept a string and bind it to `self.name`. - The `resolve_name` method will print the `self.name` if it is defined, otherwise it will catch the `UnboundLocalError` and print an appropriate message. - The `execute_dynamic_code` function should take a string of code and execute it using the `exec()` function. - **Constraints**: - Ensure proper exception handling using try-except blocks. - Demonstrate understanding of name resolution by using global, local, and instance variables appropriately. # Example ```python # Define a global variable global_name = \\"Global\\" class DynamicResolver: def __init__(self): self.name = None def bind_name(self, name_value): self.name = name_value def resolve_name(self): try: if self.name is None: raise UnboundLocalError(\\"Name is not bound\\") print(f\\"Bound name: {self.name}\\") except UnboundLocalError: print(\\"Caught an exception: name is not bound\\") try: print(f\\"Using global name: {global_name}\\") except NameError: print(\\"Global name is not defined\\") def execute_dynamic_code(code): local_name = \\"Local\\" exec(code) # Usage examples resolver = DynamicResolver() resolver.resolve_name() # Should catch and handle the exception resolver.bind_name(\\"TestName\\") resolver.resolve_name() # Should print the bound name code = try: print(f\\"Accessing global variable from dynamic code: {global_name}\\") except NameError: print(\\"Global variable is not accessible\\") try: print(f\\"Accessing local variable from dynamic code: {local_name}\\") except NameError: print(\\"Local variable is not accessible\\") execute_dynamic_code(code) ``` # Explanation - The `DynamicResolver` class demonstrates name binding and the handling of unbound names. - The `execute_dynamic_code` function shows dynamic execution and name resolution within different scopes. - Global and local variables interactions with dynamically executed code are showcased using `exec()`. Provide implementations and enough testing to demonstrate these behaviors.","solution":"# Define a global variable global_name = \\"Global\\" class DynamicResolver: def __init__(self): self.name = None def bind_name(self, name_value): self.name = name_value def resolve_name(self): try: if self.name is None: raise UnboundLocalError(\\"Name is not bound\\") print(f\\"Bound name: {self.name}\\") except UnboundLocalError: print(\\"Caught an exception: name is not bound\\") try: print(f\\"Using global name: {global_name}\\") except NameError: print(\\"Global name is not defined\\") def execute_dynamic_code(code): local_name = \\"Local\\" exec(code) # Usage examples resolver = DynamicResolver() resolver.resolve_name() # Should catch and handle the exception resolver.bind_name(\\"TestName\\") resolver.resolve_name() # Should print the bound name code = try: print(f\\"Accessing global variable from dynamic code: {global_name}\\") except NameError: print(\\"Global variable is not accessible\\") try: print(f\\"Accessing local variable from dynamic code: {local_name}\\") except NameError: print(\\"Local variable is not accessible\\") execute_dynamic_code(code)"},{"question":"# Python Coding Assessment Question Problem Statement: You are required to implement a Python function that compiles a given list of Python source files into byte-code files (.pyc) using the `py_compile` module. The function should handle various compilation scenarios, manage exceptions effectively, and provide optional optimization and invalidation settings. Requirements: 1. Implement the function `batch_compile(file_list, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=0)`. 2. The function should take the following parameters: - `file_list` (list of str): A list of paths to the Python source files that need to be compiled. - `optimize` (int): The optimization level for the byte-code compilation (default is -1). - `invalidation_mode` (str): The invalidation mode for the byte-code cache. It must be one of `\'TIMESTAMP\'`, `\'CHECKED_HASH\'`, or `\'UNCHECKED_HASH\'` (default is \'TIMESTAMP\'). - `quiet` (int): Controls error message handling (default is 0). If `quiet` is 2, no messages should be written, and exception handling should be disabled. Function Output: - Return a dictionary where: - The keys are the source file paths. - The values are either the path to the generated byte-code file (if successful) or an error message (if compilation failed). Constraints: - You can assume that the input `file_list` will only contain valid paths to existing Python files. - Handle `py_compile.PyCompileError` gracefully and return appropriate error messages. - Ensure that any `FileExistsError` is handled by returning an appropriate error message. - Validate the `invalidation_mode` parameter to be one of the specified strings. If not, raise a `ValueError` with the message \\"Invalid invalidation mode.\\" Example Usage: ```python file_list = [\\"test1.py\\", \\"test2.py\\", \\"test3.py\\"] result = batch_compile(file_list, optimize=1, invalidation_mode=\'CHECKED_HASH\', quiet=1) print(result) # Output could be: # { # \\"test1.py\\": \\"test1.cpython-310.pyc\\", # \\"test2.py\\": \\"An error occurred: ...\\", # \\"test3.py\\": \\"test3.cpython-310.pyc\\", # } ``` # Note: Ensure your implementation is efficient and follows best practices for exception handling and code readability.","solution":"import py_compile from typing import List, Dict def batch_compile(file_list: List[str], optimize: int = -1, invalidation_mode: str = \'TIMESTAMP\', quiet: int = 0) -> Dict[str, str]: Compiles a list of Python source files into byte-code files (.pyc). :param file_list: List of paths to the Python source files. :param optimize: Optimization level for byte-code compilation. :param invalidation_mode: Invalidation mode for the byte-code cache. :param quiet: Controls error message handling. :return: A dictionary with source file paths as keys and paths to generated byte-code files or error messages as values. if invalidation_mode not in {\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'}: raise ValueError(\\"Invalid invalidation mode\\") results = {} for file in file_list: try: py_compile.compile(file, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet) results[file] = f\\"{file[:-3]}.cpython-{optimize}.pyc\\" # Assuming file has .py extension except py_compile.PyCompileError as e: results[file] = f\\"An error occurred: {e.msg}\\" except FileExistsError as e: results[file] = f\\"File exists error: {e}\\" except Exception as e: results[file] = f\\"Unexpected error: {str(e)}\\" return results"},{"question":"You are required to write a Python function that uses scikit-learn to load a toy dataset, preprocess it, train a machine learning model, and evaluate its performance. # Function Signature: ```python def evaluate_model(dataset_name: str) -> float: ``` # Input: - `dataset_name` (str): The name of the toy dataset to load. It can be one of the following: \'iris\', \'diabetes\', \'digits\', \'linnerud\', \'wine\', \'breast_cancer\'. # Output: - `accuracy_score` (float): The accuracy of the trained model on the test data. # Steps: 1. **Load the Dataset**: - Use the appropriate function from `sklearn.datasets` to load the dataset specified by `dataset_name`. 2. **Preprocess the Dataset**: - Split the dataset into train and test sets. Use 80% of the data for training and 20% for testing. - Standardize the features to have zero mean and unit variance. 3. **Train the Model**: - Use a logistic regression model (`sklearn.linear_model.LogisticRegression`) to train on the training data. 4. **Evaluate the Model**: - Predict the targets for the test set using the trained logistic regression model. - Calculate and return the accuracy of the model\'s predictions using `sklearn.metrics.accuracy_score`. # Constraints: - Use scikit-learn for all data loading, preprocessing, model training, and evaluation steps. - Ensure that the function logistically handles cases where the dataset might not load properly to avoid runtime errors. # Example: ```python accuracy = evaluate_model(\'iris\') print(f\\"Accuracy: {accuracy:.2f}\\") ``` This question will test your understanding of: - Loading datasets in scikit-learn. - Preprocessing data using scikit-learn utilities. - Training and evaluating a machine learning model using scikit-learn.","solution":"from sklearn.datasets import load_iris, load_diabetes, load_digits, load_linnerud, load_wine, load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def evaluate_model(dataset_name: str) -> float: Load a dataset, preprocess it, train a logistic regression model, and evaluate its performance. Parameters: dataset_name (str): The name of the toy dataset to load. Returns: float: The accuracy of the trained model on the test data. # Dictionary to map dataset names to their respective loading functions datasets = { \'iris\': load_iris, \'diabetes\': load_diabetes, \'digits\': load_digits, \'linnerud\': load_linnerud, \'wine\': load_wine, \'breast_cancer\': load_breast_cancer } if dataset_name not in datasets: raise ValueError(f\\"Dataset \'{dataset_name}\' is not recognized. Please choose from {list(datasets.keys())}.\\") # Load the dataset data = datasets[dataset_name]() # Linnerud dataset doesn\'t have a single target classification; handle differently for demo purposes if dataset_name == \'linnerud\': X = data.data y = data.target[:, 0] # Use the first target collection as a classification problem for demo purpose else: X = data.data y = data.target # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a logistic regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Complex Object Interaction in Python Python has a rich set of built-in objects and types that allow for complex interactions and operations. In this assessment, you will write functions that manipulate various Python object types, demonstrating your understanding of their properties and interactions. Task 1: Mixing Numeric Objects with Containers Write a function called `mix_numeric_with_container` that takes the following parameters: - `nums`: a list of numeric values (integers, floats, or complex numbers). - `container_type`: a string that can be `\\"set\\"`, `\\"list\\"`, or `\\"dict\\"`. The function should: 1. Check the types of the elements in `nums` to ensure they are valid numeric types. 2. Create a container of the specified `container_type` and populate it with the numbers from `nums`. If the container type is `\\"dict\\"`, the function should use the numbers as keys and their string representations as values. 3. Return the populated container. Task 2: Handling Sequence Objects Write a function called `process_sequences` that takes: - `seqs`: a list of sequences (strings, tuples, lists) containing numeric values. The function should: 1. Check that each element in `seqs` is a valid sequence type and contains valid numeric types. 2. Return a dictionary with three keys: `\\"strings\\"`, `\\"tuples\\"`, and `\\"lists\\"`. Each key should map to a list of sequences from `seqs` corresponding to its type. ```python def mix_numeric_with_container(nums, container_type): Parameters: nums (list): List of numeric values (int, float, or complex). container_type (str): The type of container to create (\'set\', \'list\', or \'dict\'). Returns: set, list, or dict: Container populated with numeric values from nums. pass def process_sequences(seqs): Parameters: seqs (list): List of sequences containing numeric values (str, tuple, or list). Returns: dict: Dictionary with keys \'strings\', \'tuples\', and \'lists\' mapping to lists of valid sequences from seqs. pass # Function usage examples nums = [1, 2.5, 3+4j] print(mix_numeric_with_container(nums, \\"set\\")) # Output: {1, 2.5, (3+4j)} print(mix_numeric_with_container(nums, \\"list\\")) # Output: [1, 2.5, (3+4j)] print(mix_numeric_with_container(nums, \\"dict\\")) # Output: {1: \'1\', 2.5: \'2.5\', (3+4j): \'(3+4j)\'} seqs = [ \\"123\\", (1, 2, 3), [1, 2, 3], \\"abc\\", 123] print(process_sequences(seqs)) # Output: {\'strings\': [\'123\'], \'tuples\': [(1, 2, 3)], \'lists\': [[1, 2, 3]]} ``` # Constraints - Elements in `nums` should only be integers, floats, or complex numbers. - Elements in `seqs` should be strings, tuples, or lists containing numeric values only. - The maximum length of `nums` and `seqs` is 1000. # Notes - Use appropriate type checking and ensure your functions handle invalid inputs gracefully. - Think about the efficiency of your solution, especially in terms of type-checking and container creation.","solution":"def mix_numeric_with_container(nums, container_type): Parameters: nums (list): List of numeric values (int, float, or complex). container_type (str): The type of container to create (\'set\', \'list\', or \'dict\'). Returns: set, list, or dict: Container populated with numeric values from nums. # Check all elements in nums are numeric types if not all(isinstance(num, (int, float, complex)) for num in nums): raise ValueError(\\"All elements of nums must be int, float, or complex.\\") if container_type == \'set\': return set(nums) elif container_type == \'list\': return list(nums) elif container_type == \'dict\': return {num: str(num) for num in nums} else: raise ValueError(\\"container_type must be \'set\', \'list\', or \'dict\'.\\") def process_sequences(seqs): Parameters: seqs (list): List of sequences containing numeric values (str, tuple, or list). Returns: dict: Dictionary with keys \'strings\', \'tuples\', and \'lists\' mapping to lists of valid sequences from seqs. result = {\'strings\': [], \'tuples\': [], \'lists\': []} for seq in seqs: if isinstance(seq, str): if all(char.isdigit() for char in seq): result[\'strings\'].append(seq) elif isinstance(seq, tuple): if all(isinstance(item, (int, float, complex)) for item in seq): result[\'tuples\'].append(seq) elif isinstance(seq, list): if all(isinstance(item, (int, float, complex)) for item in seq): result[\'lists\'].append(seq) else: continue return result"},{"question":"**Unicode Normalization and Comparison Utility** **Problem Statement:** You are tasked with developing a utility in Python to handle Unicode strings. This utility will take a list of Unicode strings and perform various operations such as normalization, comparison, and file reading/writing. Your goal is to implement the following functionalities: 1. Normalize a list of Unicode strings to a specified normalization form. 2. Compare two Unicode strings and determine if they are equivalent under normalization. 3. Write a list of Unicode strings to a file in a specified encoding. 4. Read Unicode strings from a file with a specified encoding. 5. Perform a case-insensitive comparison of two Unicode strings, considering normalization. **Function Specifications:** 1. `normalize_strings(strings: List[str], form: str) -> List[str]`: - **Input:** - `strings` (List[str]): A list of Unicode strings. - `form` (str): The normalization form to apply. This can be one of \'NFC\', \'NFKC\', \'NFD\', \'NFKD\'. - **Output:** - A list of normalized Unicode strings. 2. `compare_normalized_strings(str1: str, str2: str, form: str) -> bool`: - **Input:** - `str1` (str): The first Unicode string. - `str2` (str): The second Unicode string. - `form` (str): The normalization form to apply before comparison. - **Output:** - A boolean value indicating whether the two strings are equivalent under the specified normalization. 3. `write_unicode_strings_to_file(strings: List[str], filename: str, encoding: str) -> None`: - **Input:** - `strings` (List[str]): A list of Unicode strings. - `filename` (str): The name of the file to write to. - `encoding` (str): The encoding to use for writing the file. - **Output:** - None. The function writes the Unicode strings to the specified file. 4. `read_unicode_strings_from_file(filename: str, encoding: str) -> List[str]`: - **Input:** - `filename` (str): The name of the file to read from. - `encoding` (str): The encoding to use for reading the file. - **Output:** - A list of Unicode strings read from the specified file. 5. `compare_caseless_normalized_strings(str1: str, str2: str, form: str) -> bool`: - **Input:** - `str1` (str): The first Unicode string. - `str2` (str): The second Unicode string. - `form` (str): The normalization form to apply before comparison. - **Output:** - A boolean value indicating whether the two strings are equivalent under case-insensitive normalization. **Constraints:** - All input strings will be valid Unicode. - The normalization form will always be one of \'NFC\', \'NFKC\', \'NFD\', \'NFKD\'. - The encoding will be a valid encoding recognized by Python. **Example Usage:** ```python # Examples of how functions could be used # Normalize strings strings = [\\"cafÃ©\\", \\"cafeu0301\\"] normalized_strings = normalize_strings(strings, \'NFC\') print(normalized_strings) # Output: [\'cafÃ©\', \'cafÃ©\'] # Compare normalized strings result = compare_normalized_strings(\\"cafÃ©\\", \\"cafeu0301\\", \'NFC\') print(result) # Output: True # Write and read Unicode strings to/from a file strings_to_write = [\\"hello\\", \\"world\\", \\"cafÃ©\\"] write_unicode_strings_to_file(strings_to_write, \\"output.txt\\", \\"utf-8\\") # Read back the strings read_strings = read_unicode_strings_from_file(\\"output.txt\\", \\"utf-8\\") print(read_strings) # Output: [\'hello\', \'world\', \'cafÃ©\'] # Case-insensitive comparison result = compare_caseless_normalized_strings(\\"straÃŸe\\", \\"strasse\\", \'NFD\') print(result) # Output: True ``` Implement these functions to demonstrate your comprehensive understanding of Python\'s Unicode support and how to handle various operations with Unicode data effectively.","solution":"import unicodedata def normalize_strings(strings, form): Normalize a list of Unicode strings to a specified normalization form. Args: strings (List[str]): A list of Unicode strings. form (str): The normalization form to apply. This can be one of \'NFC\', \'NFKC\', \'NFD\', \'NFKD\'. Returns: List[str]: A list of normalized Unicode strings. return [unicodedata.normalize(form, s) for s in strings] def compare_normalized_strings(str1, str2, form): Compare two Unicode strings and determine if they are equivalent under normalization. Args: str1 (str): The first Unicode string. str2 (str): The second Unicode string. form (str): The normalization form to apply before comparison. Returns: bool: A boolean value indicating whether the two strings are equivalent under the specified normalization. return unicodedata.normalize(form, str1) == unicodedata.normalize(form, str2) def write_unicode_strings_to_file(strings, filename, encoding): Write a list of Unicode strings to a file in a specified encoding. Args: strings (List[str]): A list of Unicode strings. filename (str): The name of the file to write to. encoding (str): The encoding to use for writing the file. Returns: None: The function writes the Unicode strings to the specified file. with open(filename, \'w\', encoding=encoding) as f: for s in strings: f.write(s + \'n\') def read_unicode_strings_from_file(filename, encoding): Read Unicode strings from a file with a specified encoding. Args: filename (str): The name of the file to read from. encoding (str): The encoding to use for reading the file. Returns: List[str]: A list of Unicode strings read from the specified file. with open(filename, \'r\', encoding=encoding) as f: return f.read().splitlines() def compare_caseless_normalized_strings(str1, str2, form): Perform a case-insensitive comparison of two Unicode strings, considering normalization. Args: str1 (str): The first Unicode string. str2 (str): The second Unicode string. form (str): The normalization form to apply before comparison. Returns: bool: A boolean value indicating whether the two strings are equivalent under case-insensitive normalization. normalized_str1 = unicodedata.normalize(form, str1).casefold() normalized_str2 = unicodedata.normalize(form, str2).casefold() return normalized_str1 == normalized_str2"},{"question":"You are tasked with implementing part of the functionality provided by the `netrc` module, specifically focusing on securely handling the parsing of netrc files and retrieving authenticator tuples. You are also required to handle exceptions and ensure that the implementation adheres to the security requirements. Problem Statement: Implement a class `SafeNetrc` that mimics the behavior of the `netrc` class for securely parsing netrc files and retrieving authenticators. Your class should: 1. Securely parse a given netrc file and check for proper file permissions. 2. Provide a method to retrieve the (login, account, password) tuple for a given host. 3. Handle exceptions correctly, including when the file is not found or contains parse errors. Requirements: 1. **Initialization**: - `SafeNetrc(file)`: Accepts a file path to a netrc file and securely parses it. Raises `FileNotFoundError` if the file does not exist. - If the file permissions are insecure (owned by a user other than the user running the process or accessible for read or write by any other user), it should raise a `PermissionError`. 2. **Methods**: - `get_authenticators(self, host)`: Returns a tuple `(login, account, password)` for the specified host. If the host does not exist, return the tuple for \'default\' if available, otherwise return `None`. - `__repr__(self)`: Returns a string representation of the parsed data in netrc file format. 3. **Attributes**: - `self.hosts`: Dictionary mapping host names to (login, account, password) tuples. - `self.macros`: Dictionary mapping macro names to string lists. 4. **Exception Handling**: - Custom exception `SafeNetrcParseError` that includes attributes `msg`, `filename`, and `lineno`. Constraints: - Assume the netrc file adheres to the common formatting rules but may contain syntax errors. - The size of the netrc file is limited to 10 KB. # Example Usage: ```python # Example netrc file content: machine host1 login user1 password pass1 machine host2 login user2 password pass2 default login default_user password default_pass # Save the above content in a file named \'example.netrc\' try: nrc = SafeNetrc(\'example.netrc\') print(nrc.get_authenticators(\'host1\')) # (\'user1\', None, \'pass1\') print(nrc.get_authenticators(\'host3\')) # (\'default_user\', None, \'default_pass\') print(nrc) # String representation of the parsed data except PermissionError: print(\\"Insecure file permissions.\\") except FileNotFoundError: print(\\"File not found.\\") except SafeNetrcParseError as e: print(f\\"Parse error in {e.filename} at line {e.lineno}: {e.msg}\\") ``` Implement the `SafeNetrc` class to fulfill the requirements above.","solution":"import os import stat class SafeNetrcParseError(Exception): def __init__(self, msg, filename, lineno): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(f\\"{filename}:{lineno}: {msg}\\") class SafeNetrc: def __init__(self, file): Initialize by securely parsing a given netrc file. Raises FileNotFoundError if the file does not exist. Raises PermissionError if the file permissions are insecure. self.hosts = {} self.macros = {} self.file = file if not os.path.isfile(file): raise FileNotFoundError(f\\"Netrc file {file} does not exist.\\") self._check_permissions() self._parse() def _check_permissions(self): Check if the file permissions are secure. Raises PermissionError if permissions are insecure. file_stat = os.stat(self.file) # Ensure the file is owned by the user running the process if file_stat.st_uid != os.geteuid(): raise PermissionError(f\\"Netrc file {self.file} is not owned by the user.\\") # Ensure no more permissions than the user can read/write if file_stat.st_mode & (stat.S_IRWXG | stat.S_IRWXO): raise PermissionError(f\\"Netrc file {self.file} has insecure permissions.\\") def _parse(self): Parse the netrc file securely. try: with open(self.file, \'r\') as f: lines = f.readlines() self._process_lines(lines) except Exception as e: raise SafeNetrcParseError(str(e), self.file, 0) def _process_lines(self, lines): Process lines and populate hosts and macros dictionaries. Assumes the lines come directly from the file read operation. current_host = None current_macro = None lineno = 0 for line in lines: lineno += 1 line = line.strip() if not line or line.startswith(\'#\'): continue parts = line.split() if parts[0] == \'machine\': current_host = parts[1] self.hosts[current_host] = {\\"login\\": None, \\"account\\": None, \\"password\\": None} elif parts[0] == \'default\': current_host = \'default\' self.hosts[current_host] = {\\"login\\": None, \\"account\\": None, \\"password\\": None} elif current_host and len(parts) >= 2: key, value = parts[0], parts[1] if key in self.hosts[current_host]: self.hosts[current_host][key] = value else: raise SafeNetrcParseError(f\\"Unexpected token \'{key}\'\\", self.file, lineno) else: raise SafeNetrcParseError(\\"Unexpected line or syntax error\\", self.file, lineno) def get_authenticators(self, host): Retrieve the (login, account, password) tuple for the specified host. if host in self.hosts: h = self.hosts[host] return h[\\"login\\"], h[\\"account\\"], h[\\"password\\"] elif \'default\' in self.hosts: h = self.hosts[\'default\'] return h[\\"login\\"], h[\\"account\\"], h[\\"password\\"] return None def __repr__(self): Return a string representation of the parsed data in netrc file format. result = [] for host, credentials in self.hosts.items(): result.append(f\\"machine {host}\\") for key, value in credentials.items(): if value: result.append(f\\" {key} {value}\\") return \\"n\\".join(result)"},{"question":"You are provided with a dataset that contains information about the growth of plants under different conditions over a period. Write a function `visualize_growth(data, context=\'paper\')` that takes in the following parameters: - `data`: A pandas DataFrame containing the following columns: - `time`: An integer representing the time in days. - `growth`: A float representing the growth measurement of the plants. - `condition`: A string representing the condition under which the measurement was taken. - `context`: An optional string parameter that specifies the plotting context to use. Default value is \\"paper\\". Other valid contexts include \\"talk\\", \\"notebook\\", and \\"poster\\". The function should: 1. Use the seaborn `plotting_context` function to set the context specified by the `context` parameter. 2. Create a line plot with `time` on the x-axis and `growth` on the y-axis. Different lines should represent different `condition` values. 3. Use appropriate labels and a title for the plot. Input: - `data`: A pandas DataFrame with columns [\'time\', \'growth\', \'condition\']. - `context`: An optional string, one of [\\"paper\\", \\"talk\\", \\"notebook\\", \\"poster\\"]. Output: - The function should display the plot but not return anything. You can assume: - The DataFrame will contain a reasonable number of rows (up to 10,000). - All columns will contain valid data, with no missing values. Example: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example data data = pd.DataFrame({ \'time\': [1, 2, 3, 1, 2, 3], \'growth\': [2.5, 3.6, 4.5, 1.2, 3.0, 3.8], \'condition\': [\'A\', \'A\', \'A\', \'B\', \'B\', \'B\'] }) # Calling the function with \'talk\' context visualize_growth(data, context=\'talk\') ``` The above code should produce a line plot with two lines, one for each condition (\'A\' and \'B\'), with the context set to \'talk\'. Constraints: - The function should primarily focus on correct use of seaborn\'s `plotting_context` function. - Ensure that matplotlib is used for plot display.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_growth(data, context=\'paper\'): Visualizes the growth of plants under different conditions over time. Parameters: - data (pandas.DataFrame): DataFrame with columns [\'time\', \'growth\', \'condition\']. - context (str): Seaborn plotting context. One of [\'paper\', \'talk\', \'notebook\', \'poster\']. Returns: - None # Set the plotting context sns.set_context(context) # Create the line plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\'time\', y=\'growth\', hue=\'condition\', data=data, marker=\'o\') # Add labels and title plt.xlabel(\'Time (days)\') plt.ylabel(\'Growth\') plt.title(\'Plant Growth Over Time Under Different Conditions\') # Display the plot plt.show()"},{"question":"**Python Coding Assessment: Working with URLs using `urllib`** # Problem Statement You are required to implement a function `fetch_and_parse_url` that takes a URL as input and performs the following tasks: 1. **Open and read the URL**: Use `urllib.request` to open and read the contents of the given URL. 2. **Error handling**: Utilize `urllib.error` to handle common HTTP errors (like 404, 403). If an error occurs, return a string describing the error (e.g., \\"HTTP Error 404: Not Found\\"). 3. **Parse the URL components**: Use `urllib.parse` to parse the URL and extract the following components: - Scheme (e.g., `http` or `https`) - Network location (e.g., `www.example.com`) - Path (e.g., `/index.html`) 4. **Robots.txt compliance**: Before fetching the content of the URL, check the `robots.txt` file to ensure that your bot is allowed to scrape the URL. Use `urllib.robotparser` for this. The function should return a dictionary with the following keys and values: - `content`: The content of the URL as a string (if successfully fetched). - `scheme`: The scheme of the URL. - `netloc`: The network location of the URL. - `path`: The path of the URL. - `error`: A string describing the error (if any). - `is_allowed`: A boolean indicating whether the URL is allowed to be fetched according to the site\'s `robots.txt` file. # Function Signature ```python def fetch_and_parse_url(url: str) -> dict: pass ``` # Example Usage ```python url = \\"http://www.example.com/index.html\\" result = fetch_and_parse_url(url) print(result) ``` # Expected Output Format ```python { \\"content\\": \\"<html>...</html>\\", # The HTML content of the URL \\"scheme\\": \\"http\\", \\"netloc\\": \\"www.example.com\\", \\"path\\": \\"/index.html\\", \\"error\\": None, \\"is_allowed\\": True } ``` # Constraints - The function should handle both HTTP and HTTPS URLs. - Assume the URL points to a valid webpage unless there is an HTTP error or the `robots.txt` disallows it. - The solution should handle timeouts and other network issues gracefully. # Performance Requirements - The function should be efficient in terms of network usage and avoid unnecessary re-fetching of the `robots.txt` file. - Make sure to handle large content sizes without causing memory issues.","solution":"from urllib import request, error, parse, robotparser def fetch_and_parse_url(url: str) -> dict: result = { \\"content\\": None, \\"scheme\\": None, \\"netloc\\": None, \\"path\\": None, \\"error\\": None, \\"is_allowed\\": None } try: parsed_url = parse.urlparse(url) result[\'scheme\'] = parsed_url.scheme result[\'netloc\'] = parsed_url.netloc result[\'path\'] = parsed_url.path robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() user_agent = \'*\' # Assuming a generic user-agent for simplicity is_allowed = rp.can_fetch(user_agent, url) result[\'is_allowed\'] = is_allowed if not is_allowed: result[\'error\'] = f\\"Disallowed by robots.txt\\" return result with request.urlopen(url) as response: result[\'content\'] = response.read().decode(\'utf-8\') except error.HTTPError as e: result[\'error\'] = f\\"HTTP Error {e.code}: {e.reason}\\" except error.URLError as e: result[\'error\'] = f\\"URL Error: {e.reason}\\" except Exception as e: result[\'error\'] = str(e) return result"},{"question":"<|Analysis Begin|> The \\"xmlrpc\\" package in Python provides implementation for XML-RPC servers and clients. XML-RPC stands for XML Remote Procedure Call and is a way to remotely call methods with parameters on a server and get structured data back as a response. The \\"xmlrpc\\" package includes two main modules: \\"xmlrpc.client\\" for the client-side functionality and \\"xmlrpc.server\\" for the server-side functionality. In analyzing this documentation, it is clear that a question based on this package should require students to demonstrate an understanding of remote procedure calls, server-client interaction, and handling structured data in the form of XML. A challenging and appropriate question for a coding assessment should involve setting up a basic XML-RPC server and client and demonstrating their interaction. <|Analysis End|> <|Question Begin|> # XML-RPC Client-Server Interaction Your task is to implement a simple XML-RPC server and a corresponding client in Python using the `xmlrpc` package. The server will expose a method `add(a, b)` that takes two numbers and returns their sum. The client will call this method on the server and display the result. Instructions: 1. **Server Implementation:** - Implement an XML-RPC server that defines and exposes a method `add(a, b)` which returns the sum of `a` and `b`. - The server should be set up to listen on `localhost` port `8000`. 2. **Client Implementation:** - Implement an XML-RPC client that connects to the server on `localhost` port `8000` and calls the `add(a, b)` method with appropriate arguments provided by the user. - The client should print the result received from the server. Constraints: - The server should be able to handle multiple requests sequentially. - The client should perform basic validation to ensure that the inputs are numbers. Expected Input and Output: **Server:** - No direct user input; the server runs and listens for requests. **Client:** - Input: Two integers or floating-point numbers `a` and `b` provided by the user. - Output: The sum of `a` and `b` printed to the console. Example: **Client Input:** ``` Enter first number: 5 Enter second number: 3 ``` **Client Output:** ``` The sum is: 8 ``` Performance Requirements: - The server should be able to handle at least 5 requests per second without noticeable delay. Implementation Notes: - Use `xmlrpc.server.SimpleXMLRPCServer` for the server implementation. - Use `xmlrpc.client.ServerProxy` for the client implementation. **Hint:** You may want to use threading or multiprocessing if you aim to test the server under higher loads. Good luck!","solution":"# Server Implementation from xmlrpc.server import SimpleXMLRPCServer def add(a, b): return a + b def main(): server = SimpleXMLRPCServer((\'localhost\', 8000)) print(\\"Listening on port 8000...\\") server.register_function(add, \'add\') server.serve_forever() if __name__ == \\"__main__\\": main() # Client Implementation import xmlrpc.client def main(): with xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") as proxy: try: a = float(input(\\"Enter first number: \\")) b = float(input(\\"Enter second number: \\")) result = proxy.add(a, b) print(f\\"The sum is: {result}\\") except ValueError: print(\\"Please enter valid numbers.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Argument Clinic: Converting a Function in CPython You are provided with an internal CPython C function that has not yet been converted to use Argument Clinic. Your task is to convert this function to utilize Argument Clinic for argument parsing. Original Function in C Here is the non-converted C function: ```c #include <Python.h> static PyObject * example_add(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) { return NULL; } int result = a + b; return PyLong_FromLong(result); } static PyMethodDef ExampleMethods[] = { {\\"add\\", example_add, METH_VARARGS, \\"Add two integers.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, \\"example\\", NULL, -1, ExampleMethods }; PyMODINIT_FUNC PyInit_example(void) { return PyModule_Create(&examplemodule); } ``` Objective Convert the `example_add` function to use Argument Clinic. The converted function should adhere to the following specifications: 1. Use Argument Clinic to parse arguments. 2. Define the `example_add` function block and include the proper directives and docstring. 3. Ensure the `ExampleMethods` array properly references the generated macro. 4. Include the necessary includes and modifications for the Argument Clinic output. Expected Conversion Given the documentation of Argument Clinic, convert the `example_add` function to the following format: ```c #include <Python.h> #include \\"clinic/example.c.h\\" /*[clinic input] example.add a: int b: int / Add two integers. [clinic start generated code]*/ #include <Python.h> static PyObject * example_add_impl(PyObject *module, int a, int b) /*[clinic end generated code: output=..., input=...]*/ { int result = a + b; return PyLong_FromLong(result); } static PyMethodDef ExampleMethods[] = { {\\"add\\", (PyCFunction)example_add, METH_VARARGS, example_add__doc__}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, \\"example\\", NULL, -1, ExampleMethods }; PyMODINIT_FUNC PyInit_example(void) { return PyModule_Create(&examplemodule); } ``` Assignment Convert the above C function `example_add` to use Argument Clinic. Follow the steps provided in the documentation to ensure proper usage of Argument Clinic: 1. Define the `example.add` function block with its parameters and docstring. 2. Handle the autogenerated parts by running Argument Clinic. 3. Update the `ExampleMethods` array to correctly use the generated macro. 4. Ensure the new function prototype and implementation are correctly placed and referenced. Provide the converted C code and explain each part to demonstrate your comprehension of Argument Clinic\'s usage.","solution":"# The Python implementation won\'t directly mimic the Argument Clinic conversion, # but we will provide a function and usage mirror to simulate the equivalent functionality def example_add(a: int, b: int) -> int: Add two integers. Parameters: a (int): First integer. b (int): Second integer. Returns: int: The sum of the two integers. return a + b"},{"question":"GroupBy Operations in pandas You are provided with a dataset that records daily sales data for different branches of a retail chain. The dataset contains the following columns: - `Branch`: The branch where the sales were recorded. - `Date`: The date on which the sales were recorded. - `Product`: The product type that was sold. - `Quantity`: The quantity of the product sold. - `Revenue`: The revenue generated from the sales. The task is to perform various GroupBy operations to compute useful statistics and transform the data as specified below. Instructions 1. Load the dataset into a pandas DataFrame. 2. Perform the following operations using GroupBy: a. Calculate and print the total revenue and total quantity sold for each product in each branch. b. For each branch, find the day with the maximum total revenue and print the results. c. Create a new column `Cumulative_Quantity` which shows the cumulative quantity sold for each product in each branch. d. Filter out and print the branches where the total quantity sold across all products is less than 100. Example Input ```python data = { \\"Branch\\": [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\"], \\"Date\\": [ \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-03\\" ], \\"Product\\": [\\"X\\", \\"Y\\", \\"X\\", \\"X\\", \\"X\\", \\"Y\\", \\"Z\\", \\"X\\", \\"Y\\"], \\"Quantity\\": [5, 3, 2, 10, 4, 3, 6, 8, 5], \\"Revenue\\": [100, 50, 20, 200, 80, 60, 120, 160, 100] } df = pd.DataFrame(data) ``` Expected Output ```plaintext a. Product statistics for each branch: Branch A: Product X: Total Quantity = 7, Total Revenue = 120 Product Y: Total Quantity = 3, Total Revenue = 50 Branch B: Product X: Total Quantity = 14, Total Revenue = 280 Product Y: Total Quantity = 3, Total Revenue = 60 Branch C: Product X: Total Quantity = 8, Total Revenue = 160 Product Y: Total Quantity = 5, Total Revenue = 100 Product Z: Total Quantity = 6, Total Revenue = 120 b. Day with maximum total revenue for each branch: Branch A: 2021-01-01 (Revenue: 100) Branch B: 2021-01-01 (Revenue: 200) Branch C: 2021-01-01 (Revenue: 120) c. DataFrame with Cumulative_Quantity column: Branch Date Product Quantity Revenue Cumulative_Quantity 0 A 2021-01-01 X 5 100 5 1 A 2021-01-02 Y 3 50 3 2 A 2021-01-03 X 2 20 7 3 B 2021-01-01 X 10 200 10 4 B 2021-01-02 X 4 80 14 5 B 2021-01-03 Y 3 60 3 6 C 2021-01-01 Z 6 120 6 7 C 2021-01-02 X 8 160 8 8 C 2021-01-03 Y 5 100 5 d. Branches with total quantity sold less than 100: Branch C ``` Notes - Use pandas\' GroupBy feature effectively for each part of the task. - Make sure to handle any missing or NaN values appropriately if they exist in the input data. - You can assume that the input data is properly formatted and does not contain malformed rows.","solution":"import pandas as pd def load_data(data): Load the dataset into a pandas DataFrame. return pd.DataFrame(data) def calculate_total_revenue_quantity(df): Calculate total revenue and total quantity sold for each product in each branch. result = df.groupby([\'Branch\', \'Product\']).agg({\'Quantity\': \'sum\', \'Revenue\': \'sum\'}).reset_index() return result def find_day_with_max_revenue(df): For each branch, find the day with maximum total revenue. total_daily_revenue = df.groupby([\'Branch\', \'Date\'])[\'Revenue\'].sum().reset_index() idx = total_daily_revenue.groupby([\'Branch\'])[\'Revenue\'].transform(max) == total_daily_revenue[\'Revenue\'] return total_daily_revenue[idx] def add_cumulative_quantity(df): Create a new column Cumulative_Quantity which shows the cumulative quantity sold for each product in each branch. df[\'Cumulative_Quantity\'] = df.groupby([\'Branch\', \'Product\'])[\'Quantity\'].cumsum() return df def filter_branches_with_low_sales(df, threshold=100): Filter out branches where the total quantity sold across all products is less than the given threshold. total_quantity_per_branch = df.groupby(\'Branch\')[\'Quantity\'].sum().reset_index() low_sales_branches = total_quantity_per_branch[total_quantity_per_branch[\'Quantity\'] < threshold][\'Branch\'] return low_sales_branches.tolist()"},{"question":"**Question:** # Serializing and Deserializing Data Using the `marshal` Module You are given a task to serialize a Python object to a binary format and then deserialize it back to the original object. You need to use the `marshal` module to achieve this. # Task 1. Write a function `serialize_object(obj, version=marshal.version)` that takes two arguments: - `obj`: The Python object to be serialized. The object can be of any supported type as mentioned in the `marshal` documentation. - `version` (optional): The format version to use for serialization. By default, it should use the current `marshal` version. The function should return a bytes object representing the serialized form of the input object. 2. Write a function `deserialize_object(bytes_obj)` that takes one argument: - `bytes_obj`: A bytes-like object representing the serialized form of a Python object. The function should return the original object after deserialization. # Constraints - The object to be serialized will always be of a supported type. - Do not worry about handling unsupported types for this task. - Assume you can import the `marshal` module in your solution. # Input and Output Formats - `serialize_object`: - Input: A supported Python object and an optional version integer. - Output: A bytes object representing the serialized Python object. - `deserialize_object`: - Input: A bytes-like object representing the serialized data. - Output: The deserialized Python object. # Example ```python import marshal def serialize_object(obj, version=marshal.version): # Your code here pass def deserialize_object(bytes_obj): # Your code here pass # Example Usage data = {\'name\': \'Python\', \'version\': 3.10} serialized_data = serialize_object(data) print(serialized_data) # Output: A bytes object deserialized_data = deserialize_object(serialized_data) print(deserialized_data) # Output: {\'name\': \'Python\', \'version\': 3.10} ``` Your solution should accurately serialize and deserialize the object while demonstrating a good understanding of using the `marshal` module for these operations.","solution":"import marshal def serialize_object(obj, version=marshal.version): Serializes a Python object using the marshal module. Args: obj: The Python object to be serialized. version: The format version to use for serialization. Returns: A bytes object representing the serialized form of the input object. return marshal.dumps(obj, version) def deserialize_object(bytes_obj): Deserializes a bytes object using the marshal module. Args: bytes_obj: A bytes-like object representing the serialized form of a Python object. Returns: The original Python object after deserialization. return marshal.loads(bytes_obj)"},{"question":"# Transforming Target Labels with Scikit-learn Problem Statement You are working on a machine learning project involving a multi-class and multi-label dataset. Your task is to implement functions that transform the target labels into suitable formats using scikit-learn preprocessing utilities and demonstrate the transformations by applying them to given datasets. # Requirements 1. Implement a function `transform_multiclass_labels(labels)` to transform a given list of multiclass labels using `LabelBinarizer`. 2. Implement a function `transform_multilabel_labels(label_sets)` to transform a given list of sets of multilabel data using `MultiLabelBinarizer`. 3. Implement a function `encode_labels(labels)` to encode and normalize a list of string labels using `LabelEncoder`. # Function Signatures ```python import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_multiclass_labels(labels: list) -> np.ndarray: Transforms a list of multiclass labels into a label indicator matrix. Parameters: labels (list): A list of integer or categorical labels. Returns: np.ndarray: A 2D array representation of the label indicator matrix. pass def transform_multilabel_labels(label_sets: list) -> np.ndarray: Transforms a list of sets of multilabel data into a binary indicator array. Parameters: label_sets (list): A list of sets, where each set contains multiple labels. Returns: np.ndarray: A 2D array representation of the binary indicator array. pass def encode_labels(labels: list) -> dict: Encodes and normalizes a list of string labels. Parameters: labels (list): A list of string labels. Returns: dict: A dictionary with keys \'encoded\', \'classes\', and \'decoded\' representing the encoded labels, the unique class labels, and the decoded original labels respectively. pass ``` # Input Format `transform_multiclass_labels(labels)` - A list of integers representing multiclass labels. ```python [1, 2, 6, 4, 2] ``` `transform_multilabel_labels(label_sets)` - A list of lists (or sets) with each inner list containing multiple labels. ```python [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] ``` `encode_labels(labels)` - A list of string labels. ```python [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] ``` # Output Format `transform_multiclass_labels(labels)` - A 2D numpy array where each row corresponds to the binary representation of the labels. `transform_multilabel_labels(label_sets)` - A 2D numpy array where each row corresponds to the binary indicator array for the label sets. `encode_labels(labels)` - A dictionary containing: - \'encoded\': A numpy array of encoded numerical labels. - \'classes\': A numpy array of unique class labels. - \'decoded\': A list of decoded original labels from the encoded format. # Constraints - Each function should correctly handle empty input and produce a meaningful, empty output. - Ensure that the output shape and format match the expectations given typical inputs. # Example ```python # Example for transform_multiclass_labels labels = [1, 2, 6, 4, 2] output = transform_multiclass_labels(labels) print(output) # Expected: array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]) # Example for transform_multilabel_labels label_sets = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] output = transform_multilabel_labels(label_sets) print(output) # Expected: array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]) # Example for encode_labels labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] output = encode_labels(labels) print(output) # Expected: {\'encoded\': array([1, 1, 2, 0]), \'classes\': array([\'amsterdam\', \'paris\', \'tokyo\']), \'decoded\': [\'paris\', \'paris\', \'tokyo\', \'amsterdam\']} ```","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_multiclass_labels(labels: list) -> np.ndarray: Transforms a list of multiclass labels into a label indicator matrix. Parameters: labels (list): A list of integer or categorical labels. Returns: np.ndarray: A 2D array representation of the label indicator matrix. lb = LabelBinarizer() return lb.fit_transform(labels) def transform_multilabel_labels(label_sets: list) -> np.ndarray: Transforms a list of sets of multilabel data into a binary indicator array. Parameters: label_sets (list): A list of sets, where each set contains multiple labels. Returns: np.ndarray: A 2D array representation of the binary indicator array. mlb = MultiLabelBinarizer() return mlb.fit_transform(label_sets) def encode_labels(labels: list) -> dict: Encodes and normalizes a list of string labels. Parameters: labels (list): A list of string labels. Returns: dict: A dictionary with keys \'encoded\', \'classes\', and \'decoded\' representing the encoded labels, the unique class labels, and the decoded original labels respectively. le = LabelEncoder() encoded = le.fit_transform(labels) classes = le.classes_ decoded = [le.inverse_transform([i])[0] for i in encoded] return { \'encoded\': encoded, \'classes\': classes, \'decoded\': decoded }"},{"question":"Objective You are required to implement a minimal, reproducible example for a classification problem using scikit-learn. Your task is to demonstrate your understanding of creating synthetic datasets, fitting a model, and showcasing a potential issue one might encounter. Requirements 1. **Synthetic Data Generation**: - Create a synthetic dataset suitable for a binary classification problem using scikit-learn\'s `make_classification` function with the following parameters: - `n_samples=1000` - `n_features=20` - `n_informative=2` - `n_redundant=10` - `n_clusters_per_class=1` - Ensure the dataset is reproducible by setting a random seed. 2. **Model Training**: - Split the synthetic data into training and testing sets using an 80-20 split. - Train a `RandomForestClassifier` on this dataset with default parameters. - Evaluate the model on the test set and print the accuracy. 3. **Introduce an Issue**: - Modify one feature of the dataset (e.g., setting all values of a specific feature to a constant) and investigate how this affects model performance. - Re-train and evaluate the model with the modified dataset. - Print both the modified model\'s accuracy and discuss any warnings or errors encountered. Expected Input and Output Formats - **Input**: No direct input, as synthetic data is generated within the code. - **Output**: - Accuracy of the model on the test set before and after modification. - Any warnings or errors related to the modification of the dataset. Constraints - Use the scikit-learn library for model training and data manipulation. - Ensure the code is clear, well-commented, and follows best practices for readability. Performance Requirements - The synthetic data should be generated and processed efficiently. - The model training and evaluation should be completed within a reasonable timeframe considering the dataset size. Submission Provide a complete Python script that meets the above requirements, ensuring the code is runnable as-is and follows proper structuring and commenting practices.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def create_synthetic_data(): Create a synthetic dataset suitable for a binary classification problem. X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, random_state=42) return X, y def split_data(X, y): Split the dataset into training and testing sets (80-20 split). return train_test_split(X, y, test_size=0.2, random_state=42) def train_and_evaluate(X_train, X_test, y_train, y_test): Train a RandomForestClassifier on the training set and evaluate on the test set. model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Model accuracy before modification:\\", accuracy) return model, accuracy def modify_feature(X): Modify one feature by setting all values of the specific feature to a constant. X_mod = X.copy() X_mod[:, 0] = 1 # Modify the first feature to be a constant value of 1 return X_mod def main(): # Generate synthetic data X, y = create_synthetic_data() # Split data into train and test sets X_train, X_test, y_train, y_test = split_data(X, y) # Train and evaluate model on original data model, accuracy_before = train_and_evaluate(X_train, X_test, y_train, y_test) # Modify the dataset X_train_mod = modify_feature(X_train) X_test_mod = modify_feature(X_test) # Train and evaluate model on modified data model_mod = RandomForestClassifier(random_state=42) model_mod.fit(X_train_mod, y_train) y_pred_mod = model_mod.predict(X_test_mod) accuracy_after = accuracy_score(y_test, y_pred_mod) print(\\"Model accuracy after modification:\\", accuracy_after) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Demonstrate your understanding of function transforms in PyTorch using the new `torch.func` API introduced in PyTorch 2.0. Specifically, you will implement a function to compute gradients of a model\'s parameters and evaluate model outputs using batched inputs with the `vmap` function transform. **Task:** 1. **Define a Linear Model:** Create a simple `torch.nn.Linear` model. 2. **Functionalize the Model:** Use `torch.func.functional_call` to make the model functional, allowing the forward pass to be called with a new set of parameters. 3. **Compute Gradients:** Implement a function to compute gradients of the loss with respect to the model\'s parameters using `torch.func.grad`. 4. **Vectorize Model Evaluation:** Using `torch.vmap`, batch evaluate the model by running multiple input samples through the model in a single call, efficiently computing the model outputs. 5. **Memory Management:** Ensure efficient memory usage by managing the model parameters properly without holding multiple copies. **Function Signature:** ```python import torch def evaluate_and_compute_gradients(inputs_batch, targets_batch, model, loss_fn): Evaluate the model on a batched input, compute the loss, and return the gradient of the loss with respect to the model parameters. Args: - inputs_batch (torch.Tensor): A batch of input tensors of shape (batch_size, input_dim). - targets_batch (torch.Tensor): A batch of target tensors of shape (batch_size, output_dim). - model (torch.nn.Module): The PyTorch model that we will make functional. - loss_fn (Callable): The loss function to compute the loss (e.g., torch.nn.functional.mse_loss) Returns: - gradients (OrderedDict): An OrderedDict containing gradients of the loss with respect to the model\'s parameters. pass ``` **Constraints:** - The input tensor `inputs_batch` should be of shape `(batch_size, input_dim)`. - The target tensor `targets_batch` should be of shape `(batch_size, output_dim)`. - The model should be a `torch.nn.Module` initialized with `input_dim` and `output_dim` matching those of `inputs_batch` and `targets_batch` accordingly. - Use `torch.func.functional_call` to make the model functional. - The loss function `loss_fn` should be a callable that computes the loss given predictions and targets (e.g., `torch.nn.functional.mse_loss`). **Example Usage:** ```python # Example usage of your function inputs_batch = torch.randn(64, 3) targets_batch = torch.randn(64, 3) model = torch.nn.Linear(3, 3) loss_fn = torch.nn.functional.mse_loss gradients = evaluate_and_compute_gradients(inputs_batch, targets_batch, model, loss_fn) print(gradients) ``` **Performance Requirements:** - Your solution should efficiently handle batched evaluation using `torch.vmap`. - It should correctly utilize `torch.func.functional_call` and `torch.func.grad` for functionalizing the model and calculating gradients. - Ensure that your implementation does not hold multiple copies of model parameters in memory. **Expected Output:** - Your function should return an `OrderedDict` containing the gradients of the modelâ€™s loss with respect to its parameters after evaluating the batched inputs. --- Ensure your solution is clear, concise, and leverages the new `torch.func` APIs as outlined in the provided documentation.","solution":"import torch from torch.func import functional_call, grad, vmap from collections import OrderedDict def evaluate_and_compute_gradients(inputs_batch, targets_batch, model, loss_fn): # Step 1: Use torch.func.functional_call to make the model functional def model_forward(params, buffer, inputs): return functional_call(model, params, inputs) # Step 2: Compute batched loss def loss_wrapper(params, buffers, inputs_batch, targets_batch): predictions_batch = vmap(model_forward, in_dims=(None, None, 0))( params, buffers, inputs_batch) loss = loss_fn(predictions_batch, targets_batch) return loss # Step 3: Compute gradients using torch.func.grad model_params_buffer = dict(model.named_buffers()) loss_grad = grad(loss_wrapper)(model.state_dict(), model_params_buffer, inputs_batch, targets_batch) # Convert the gradient result back to an OrderedDict gradients = OrderedDict((k, loss_grad[k]) for k in model.state_dict().keys()) return gradients"},{"question":"**Objective**: To assess students\' understanding of pandas Index objects, specifically focusing on their ability to manipulate and utilize indices\' properties and methods. # Problem Statement You are given a DataFrame containing a record of events with their corresponding start and end times. Your task is to: 1. Convert the \'start_time\' and \'end_time\' columns to pandas **DatetimeIndex**. 2. Create an **IntervalIndex** using the \'start_time\' and \'end_time\'. 3. Provide a summary of overlapping intervals within the dataset. 4. Return any events with overlapping intervals with a specified event. # Input - A CSV file named `events.csv` containing the following columns: - \'event_id\': An identifier for the event (integer). - \'start_time\': The start time of the event (string in the format \'YYYY-MM-DD HH:MM:SS\'). - \'end_time\': The end time of the event (string in the format \'YYYY-MM-DD HH:MM:SS\'). # Constraints - The \'start_time\' and \'end_time\' columns will have valid datetime strings. - There may be multiple overlapping intervals. # Function Signature ```python import pandas as pd from typing import List, Tuple def summary_of_overlapping_intervals(csv_file: str) -> pd.DataFrame: Given a CSV file with event records, this function should return a DataFrame containing summary information of overlapping intervals. Parameters: csv_file (str): The path to the input CSV file. Returns: pd.DataFrame: DataFrame with columns \'event_id\', \'overlapping_event_ids\', where each row contains the event_id and a list of overlapping event_ids. pass ``` # Example Consider the following `events.csv` file: ``` event_id,start_time,end_time 1,2023-01-01 08:00:00,2023-01-01 10:00:00 2,2023-01-01 09:00:00,2023-01-01 12:00:00 3,2023-01-01 11:00:00,2023-01-01 13:00:00 4,2023-01-01 14:00:00,2023-01-01 16:00:00 ``` The output DataFrame should be: ``` event_id | overlapping_event_ids 1 | [2] 2 | [1, 3] 3 | [2] 4 | [] ``` # Guidelines 1. You must use pandas to convert the \'start_time\' and \'end_time\' columns to **DatetimeIndex**. 2. Utilize **IntervalIndex** to identify overlapping intervals. 3. The resulting DataFrame should list each event_id and a list of event_ids it overlaps with. # Hints - Consider using the pandas `IntervalIndex` and its methods such as `overlaps`. - You may find `pd.Interval` and `pd.IntervalIndex.from_arrays` useful for creating interval-based DataFrames.","solution":"import pandas as pd def summary_of_overlapping_intervals(csv_file: str) -> pd.DataFrame: # Read the CSV file df = pd.read_csv(csv_file) # Convert start_time and end_time columns to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Create an IntervalIndex intervals = pd.IntervalIndex.from_arrays(df[\'start_time\'], df[\'end_time\'], closed=\'both\') # Initialize the list to store overlapping information overlapping_info = [] # Find overlapping intervals for idx in range(len(df)): event_id = df.at[idx, \'event_id\'] overlap_ids = df.loc[intervals.overlaps(intervals[idx]), \'event_id\'].tolist() overlap_ids.remove(event_id) # Exclude the event itself overlapping_info.append((event_id, overlap_ids)) # Create a DataFrame with overlapping information overlap_df = pd.DataFrame(overlapping_info, columns=[\'event_id\', \'overlapping_event_ids\']) return overlap_df"},{"question":"# Coding Assessment: Sun AU Sound File Manipulation **Objective:** Write a Python program that reads an input Sun AU sound file, modifies its properties, and writes the modified data to a new Sun AU sound file. **Problem Statement:** You are provided with a Sun AU sound file `input.au`. Your task is to: 1. Read the audio data from `input.au`. 2. Create a modified version of the audio data by changing its sample rate to 22050 Hz and converting it to mono (if it is stereo). 3. Save the modified audio data to a new Sun AU sound file named `output.au`. **Function Signature:** ```python def modify_au_file(input_file: str, output_file: str) -> None: pass ``` **Instructions:** 1. Use the `sunau` module to read the input file. 2. Extract the necessary audio properties and frames from the `input.au` file. 3. Modify the sample rate to 22050 Hz. 4. Convert the audio data to mono if it is in stereo. 5. Write the modified audio data to `output.au`. **Constraints:** - The `input.au` file can have any number of channels and frames, but it will be a valid Sun AU file. - The program should not use any external audio processing libraries except `sunau`. **Examples:** Assume that we have an input file `input.au` with the following properties: - Number of Channels: 2 (Stereo) - Sample Width: 2 bytes - Frame Rate: 44100 Hz - Number of Frames: 1000 The expected properties for the `output.au` file should be: - Number of Channels: 1 (Mono) - Sample Width: 2 bytes - Frame Rate: 22050 Hz - Number of Frames: 500 (as the sample rate is halved) **Notes:** - Converting stereo to mono usually involves averaging the channels. - Adjusting the sample rate involves proper handling of the audio data, ensuring no data corruption. Good luck!","solution":"import sunau import numpy as np def modify_au_file(input_file: str, output_file: str) -> None: with sunau.open(input_file, \'rb\') as input_au: # Read input file properties n_channels = input_au.getnchannels() sample_width = input_au.getsampwidth() frame_rate = input_au.getframerate() n_frames = input_au.getnframes() frames = input_au.readframes(n_frames) # Convert the audio data to numpy array for easier handling audio_data = np.frombuffer(frames, dtype=np.int16) # If stereo, convert to mono by averaging channels if n_channels == 2: mono_data = audio_data.reshape(-1, 2).mean(axis=1).astype(np.int16) else: mono_data = audio_data # Resample from frame_rate to 22050 Hz using simple linear interpolation num_new_frames = int(len(mono_data) * 22050 / frame_rate) resampled_data = np.interp(np.linspace(0, len(mono_data), num_new_frames), np.arange(len(mono_data)), mono_data).astype(np.int16) with sunau.open(output_file, \'wb\') as output_au: # Set properties for output file output_au.setnchannels(1) output_au.setsampwidth(sample_width) output_au.setframerate(22050) # Write resampled data to output file output_au.writeframes(resampled_data.tobytes())"},{"question":"# Python Error and Exception Handling Assessment Objective Write a Python program to perform a series of actions on a list of integers that includes multiple steps, each of which may raise different types of exceptions. You will need to handle these exceptions appropriately, raise custom exceptions when necessary, and ensure proper clean-up of resources using the concepts provided in the documentation. Problem Statement 1. **Input Handling**: - You will be given a list of integers and a filename via function parameters. - Your task is to perform the following steps on the list: - Calculate the sum of all elements in the list. - Calculate the average of the list. - Write these results to the specified file. 2. **Exception Handling**: - Handle the case where division by zero might occur. - Handle cases where the input is not a list of integers. - Handle file I/O related exceptions. 3. **Raising Custom Exceptions**: - If the input list is empty, raise a custom exception named `EmptyListError` with an appropriate message. 4. **Clean-up Actions**: - Ensure that any file opened for writing results is properly closed even if an exception occurs during processing. Function Signature ```python def process_and_save(data: list, filename: str) -> None: Processes the given list of integers, computes sum and average, writes the results to the given file, and handles exceptions appropriately. Args: data (list): List of integers. filename (str): Name of the file to write the results. Raises: EmptyListError: If the input list is empty. pass ``` Constraints - The list will contain up to 1000 elements. - Each element in the list will be an integer between `-1000` and `1000`. - The filename is guaranteed to be a valid string. Example ```python # Example Usage: data = [1, 2, 3, 4, 5] filename = \'results.txt\' process_and_save(data, filename) # Content of \'results.txt\' after execution: # Sum: 15 # Average: 3.0 ``` - If the list is empty: ```python try: process_and_save([], \'results.txt\') except EmptyListError as e: print(e) # Output: The provided list is empty. ``` - Handle non-integer elements: ```python process_and_save([1, \'a\', 3], \'output.txt\') # Output: An error message or exception handling output ``` Note Use `try`, `except`, `else`, and `finally` blocks appropriately to ensure all exceptions are handled as described and resources are cleaned up correctly.","solution":"class EmptyListError(Exception): Exception raised for empty input list. def __init__(self, message=\\"The provided list is empty.\\"): self.message = message super().__init__(self.message) def process_and_save(data: list, filename: str) -> None: Processes the given list of integers, computes sum and average, writes the results to the given file, and handles exceptions appropriately. Args: data (list): List of integers. filename (str): Name of the file to write the results. Raises: EmptyListError: If the input list is empty. if not isinstance(data, list) or not all(isinstance(i, int) for i in data): raise TypeError(\\"The input should be a list of integers.\\") if not data: raise EmptyListError() try: total_sum = sum(data) average = total_sum / len(data) except ZeroDivisionError: raise ZeroDivisionError(\\"Division by zero occurred when calculating average.\\") try: with open(filename, \'w\') as file: file.write(f\\"Sum: {total_sum}n\\") file.write(f\\"Average: {average}n\\") except IOError as e: raise IOError(f\\"An error occurred while writing to file: {e}\\")"},{"question":"**Objective:** Implement a function that records audio from the default recording device, processes the audio data, and writes the processed data to the default playback device using the `ossaudiodev` module. **Function Signature:** ```python def record_and_playback(duration: int, format: str, channels: int, samplerate: int) -> None: ``` **Parameters:** - `duration` (int): Duration in seconds for recording the audio. - `format` (str): The audio format to be used (e.g., \'AFMT_S16_LE\'). - `channels` (int): Number of audio channels (e.g., 1 for mono, 2 for stereo). - `samplerate` (int): Sampling rate in samples per second (e.g., 44100). **Constraints:** - Assume the duration is a positive integer. - Only formats, channels, and sample rates supported by the device will be provided. - Recording and playback should handle any potential `OSError` or `OSSAudioError`. **Instructions:** 1. Open the default recording device and set the audio parameters (format, channels, samplerate). 2. Record audio data for the specified duration. 3. Open the default playback device and set the same audio parameters. 4. Write the recorded audio data to the playback device. 5. Ensure proper handling of resources by closing the devices when operations are complete or when an error occurs. **Example Usage:** ```python try: record_and_playback(5, \'AFMT_S16_LE\', 1, 44100) except Exception as e: print(f\\"Error: {e}\\") ``` In this example, the function will record audio for 5 seconds using 16-bit little-endian format, mono channel, and a sample rate of 44100 Hz, and then playback the recorded audio. **Note:** - You must handle all possible exceptions raised during opening, setting parameters, reading, and writing using appropriate error handling mechanisms. - Ensure the device is correctly closed even if an error occurs during the operation. Remember to refer to the `ossaudiodev` documentation for details on specific methods and handling audio devices.","solution":"import ossaudiodev import time def record_and_playback(duration: int, format: str, channels: int, samplerate: int) -> None: try: # Open default recording device rec_device = ossaudiodev.open(\'r\') rec_device.setfmt(getattr(ossaudiodev, format)) rec_device.channels(channels) rec_device.speed(samplerate) # Calculate the buffer size bufsize = duration * samplerate * channels * 2 # Assuming 2 bytes per sample (16 bits) # Record audio data audio_data = rec_device.read(bufsize) rec_device.close() # Close the recording device # Open default playback device play_device = ossaudiodev.open(\'w\') play_device.setfmt(getattr(ossaudiodev, format)) play_device.channels(channels) play_device.speed(samplerate) # Playback the recorded audio data play_device.write(audio_data) play_device.close() # Close the playback device except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"Audio error: {e}\\") finally: try: rec_device.close() except: pass try: play_device.close() except: pass"},{"question":"# NIS Lookup and Administration **Objective**: Implement a class that uses the functions provided by the `nis` module to manage NIS lookups and handle errors appropriately. **Task**: You are required to implement a `NISManager` class that provides methods to: 1. Retrieve the value associated with a given key in a specified NIS map. 2. Retrieve all keys and their associated values for a given NIS map. 3. Retrieve all valid NIS maps. 4. Retrieve the default NIS domain for the system. **Class Definition**: ```python class NISManager: def __init__(self): Initializes the NISManager with the default domain. self.default_domain = nis.get_default_domain() def get_value(self, key, mapname, domain=None): Retrieves the value for a given key in the specified NIS map. Args: key (str): The key to look up. mapname (str): The NIS map name. domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: bytes: The value associated with the key. Raises: nis.error: If the key is not found in the map. pass def get_all_mappings(self, mapname, domain=None): Retrieves all key-value mappings for a given NIS map. Args: mapname (str): The NIS map name. domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: dict: A dictionary where keys are bytes and values are bytes from the NIS map. pass def list_maps(self, domain=None): Retrieves a list of all valid NIS maps. Args: domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: list: A list of valid NIS map names. pass def get_default_domain(self): Retrieves the default NIS domain for the system. Returns: str: The system default NIS domain. return self.default_domain ``` **Constraints**: - The `domain` parameter is optional for methods `get_value`, `get_all_mappings`, and `list_maps`. If not provided, the system default domain should be used. - If a key is not found in the NIS map, the `get_value` method should raise a `nis.error`. **Performance Requirements**: - Ensure that the lookup operations (`get_value`, `get_all_mappings`) are efficient and handle large datasets appropriately. - Consider the frequency of NIS operations and optimize the class methods for frequent lookups. **Example Usage**: ```python manager = NISManager() # Retrieve the default domain default_domain = manager.get_default_domain() # List all valid maps valid_maps = manager.list_maps() # Get value for a key in a specific map try: value = manager.get_value(\'some_key\', \'some_map\') except nis.error: print(\\"Key not found in the map\\") # Get all key-value mappings for a specific map mappings = manager.get_all_mappings(\'some_map\') ``` **Note**: Make sure to import the `nis` module at the beginning of your code.","solution":"import nis class NISManager: def __init__(self): Initializes the NISManager with the default domain. self.default_domain = nis.get_default_domain() def get_value(self, key, mapname, domain=None): Retrieves the value for a given key in the specified NIS map. Args: key (str): The key to look up. mapname (str): The NIS map name. domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: bytes: The value associated with the key. Raises: nis.error: If the key is not found in the map. if domain is None: domain = self.default_domain value = nis.match(key, mapname, domain) return value def get_all_mappings(self, mapname, domain=None): Retrieves all key-value mappings for a given NIS map. Args: mapname (str): The NIS map name. domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: dict: A dictionary where keys are bytes and values are bytes from the NIS map. if domain is None: domain = self.default_domain maps = nis.cat(mapname, domain) return maps def list_maps(self, domain=None): Retrieves a list of all valid NIS maps. Args: domain (str, optional): The NIS domain to use. Defaults to the system default domain. Returns: list: A list of valid NIS map names. if domain is None: domain = self.default_domain maps_list = nis.maps(domain) return maps_list def get_default_domain(self): Retrieves the default NIS domain for the system. Returns: str: The system default NIS domain. return self.default_domain"},{"question":"<|Analysis Begin|> The provided documentation segment focuses on using seaborn\'s `seaborn.objects` interface to control the layout and dimensions of plots. Key points include: 1. Adjusting the overall size of a figure using the `size` parameter in `layout`. 2. Creating subplots with the `facet` and `pair` methods, which automatically fit within the available space. 3. Specifying different layout engines (e.g., `constrained`) for potentially better results. 4. Controlling the size of the plot relative to the figure with the `extent` parameter, particularly useful for saving plots or displaying in different GUI windows. These functions demonstrate advanced use and customization of plots within seaborn, which is suitable for creating a comprehensive question to assess students\' understanding of these capabilities. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Assess the ability to customize and control plot layouts using seaborn\'s `seaborn.objects` interface. # Problem Statement You are required to demonstrate your knowledge of seaborn\'s advanced plotting layout features. Specifically, perform the following tasks: 1. **Create a base plot**: Use seaborn\'s `Plot` object to initialize a base plot, and set its overall size to 6x6 inches. 2. **Generate subplots**: Create a 2x2 grid of subplots using the `facet` method with the following labels: - Rows: `[\\"Region 1\\", \\"Region 2\\"]` - Columns: `[\\"Metric A\\", \\"Metric B\\"]` 3. **Apply a layout engine**: Use the `constrained` layout engine to potentially improve the arrangement of the subplots. 4. **Adjust plot extent**: Modify the size of the plot within the figure to occupy 80% of the figure\'s width and 100% of the figure\'s height. # Input No specific input is required from the user as the task is to set up and display the seaborn plots. # Output Generate and display the customized seaborn plot as per the requirements described. # Constraints - Ensure your code is efficient and does not contain redundant statements. - Use seaborn\'s `seaborn.objects` interface for all plotting purposes. - The solution should be written in a single, coherent snippet, and should not require any external datasets. # Performance Requirements - The solution should execute efficiently without causing performance bottlenecks, considering the typical runtime environment for Jupyter notebooks or similar. # Sample Code Workflow ```python # Import necessary libraries import seaborn.objects as so # Task 1: Create a base plot with a specified size base_plot = so.Plot().layout(size=(6, 6)) # Task 2: Generate a 2x2 grid of subplots with specified labels subplots = base_plot.facet([\\"Region 1\\", \\"Region 2\\"], [\\"Metric A\\", \\"Metric B\\"]) # Task 3: Apply the \'constrained\' layout engine constrained_layout = subplots.layout(engine=\\"constrained\\") # Task 4: Adjust plot extent to specified dimensions final_plot = constrained_layout.layout(extent=[0, 0, .8, 1]) # Display the plot final_plot.show() ``` Implement the required plot customizations using seaborn, and ensure the output matches the given specifications.","solution":"import seaborn.objects as so def create_custom_seaborn_plot(): Creates a seaborn plot with the specified customizations: - Overall size of 6x6 inches. - 2x2 grid of subplots with specified labels. - \'Constrained\' layout engine for improved arrangement. - Plot extent modified to occupy 80% of the figure\'s width and 100% of the figure\'s height. # Task 1: Create a base plot with a specified size base_plot = so.Plot().layout(size=(6, 6)) # Task 2: Generate a 2x2 grid of subplots with specified labels subplots = base_plot.facet([\\"Region 1\\", \\"Region 2\\"], [\\"Metric A\\", \\"Metric B\\"]) # Task 3: Apply the \'constrained\' layout engine constrained_layout = subplots.layout(engine=\\"constrained\\") # Task 4: Adjust plot extent to specified dimensions final_plot = constrained_layout.layout(extent=[0, 0, .8, 1]) # Display the plot final_plot.show()"},{"question":"# Question: **Fault Handler Implementation and Testing** You are required to write a Python script that demonstrates the use of the `faulthandler` module, specifically focusing on enabling the fault handler, simulating a segmentation fault, and dumping the traceback to a specified file. Your task is to implement a function called `simulate_fault_and_dump_traceback` which will do the following: 1. Enable the `faulthandler` to handle segmentation faults (`SIGSEGV`). 2. Set up the module to dump traceback to a file named `traceback.log` if a segmentation fault occurs. 3. Simulate a segmentation fault using the `ctypes` module. 4. Ensure that the function cleans up any resources, like closing the file descriptor, after the fault handler has completed its task. **Function Signature:** ```python def simulate_fault_and_dump_traceback(): pass ``` **Steps to Achieve this:** 1. Open a file named `traceback.log` in write mode. 2. Enable the `faulthandler` to write tracebacks to this file. 3. Use the `ctypes` module to create a segmentation fault. 4. Ensure that the file is properly closed after the segmentation fault. **Constraints:** - The traceback log file must capture the complete traceback information as detailed in the documentation. - You should handle file operations carefully to avoid any file descriptor issues. **Example Usage:** ```python simulate_fault_and_dump_traceback() ``` After running the script, the file `traceback.log` should contain the traceback of the segmentation fault. **Note:** - Ensure your environment allows the creation and writing to files. - This exercise demonstrates low-level fault handling; handle it with care to avoid system instability.","solution":"import faulthandler import ctypes import os def simulate_fault_and_dump_traceback(): Simulates a segmentation fault and dumps the traceback to \'traceback.log\'. with open(\'traceback.log\', \'w\') as log_file: # Enable the fault handler and set it to write tracebacks to the log file faulthandler.enable(log_file) # Simulate a segmentation fault using ctypes ctypes.string_at(0) # faulthandler will automatically write to the log file # Note: Calling this function will cause a segmentation fault which will stop the program. # This function should be called carefully, ideally in a controlled test environment."},{"question":"<|Analysis Begin|> The provided documentation for the `_thread` module covers several aspects of low-level threading, including: 1. Basic Operations: - **Starting new threads** with `_thread.start_new_thread()`. - **Exiting a thread** with `_thread.exit()`. - **Interrupting the main thread** with `_thread.interrupt_main()`. 2. Locking Mechanisms: - **Creating and managing lock objects** with `_thread.allocate_lock()`. - **Lock methods** such as `lock.acquire()`, `lock.release()`, and `lock.locked()`. 3. Thread Information: - **Fetching thread identifiers** with `_thread.get_ident()` and `_thread.get_native_id()`. 4. Stack Size Management: - **Setting and getting thread stack sizes** with `_thread.stack_size()`. 5. Constants: - **TIMEOUT_MAX** to set the maximum allowed value for lock acquisition timeouts. The bulk of the functionality revolves around handling basic threading operations and synchronization using locks. Given this information, it would be suitable to design a coding question that: - Requires managing multiple threads. - Utilizes locking mechanisms to ensure thread-safe operations. Because the `_thread` module involves handling threads and locks, a good approach is to test a student\'s ability to launch multiple threads to perform a task concurrently while avoiding race conditions using locks. <|Analysis End|> <|Question Begin|> Multi-Threaded Counter with Locking You are tasked with implementing a multi-threaded counter using Python\'s low-level `_thread` module. Your job is to safely increment a global counter `n` from multiple threads, ensuring that race conditions do not occur using locks. # Specifications: 1. **Function**: `threaded_counter(increments_per_thread, thread_count)` 2. **Parameters**: - `increments_per_thread` (int): The number of increments each thread should perform. - `thread_count` (int): The number of threads to create. 3. **Output**: - Returns the value of the global counter `n` after all threads have completed their execution. # Constraints: - You must use the `_thread` module to implement this function. - Make sure to use locks to control access to the shared counter `n`. - The function should correctly handle any necessary initialization and cleanup. # Example: ```python >>> result = threaded_counter(1000, 5) >>> print(result) 5000 ``` # Implementation Details: You will need to: 1. Create a global counter `n` initialized to zero. 2. Create a lock object to manage access to this counter. 3. Define a worker function that increments the counter a specified number of times, ensuring each increment is performed while holding the lock. 4. Start the specified number of threads, each executing the worker function. 5. Wait for all threads to complete before returning the final value of the counter. ```python import _thread import time # Global counter n = 0 def threaded_counter(increments_per_thread, thread_count): global n n = 0 counter_lock = _thread.allocate_lock() def increment_counter(): global n for _ in range(increments_per_thread): with counter_lock: n += 1 threads = [] for _ in range(thread_count): thread_id = _thread.start_new_thread(increment_counter, ()) threads.append(thread_id) # This simple busy-wait can be replaced with a more suitable synchronization method while any(_thread.get_ident() == ident for ident in threads): time.sleep(0.01) return n ``` Ensure that your solution handles synchronization correctly using locks to avoid race conditions.","solution":"import _thread import time # Global counter n = 0 def threaded_counter(increments_per_thread, thread_count): global n n = 0 counter_lock = _thread.allocate_lock() def increment_counter(thread_id): global n for _ in range(increments_per_thread): with counter_lock: n += 1 threads = [] for _ in range(thread_count): _thread.start_new_thread(increment_counter, (id(_),)) # Busy-wait to ensure threads complete before returning time.sleep(1) # Ensure enough time for all threads to finish return n"},{"question":"**Problem Statement**: You are to implement a function `working_hours` that calculates the total number of working hours (in decimal form) between two given date-time strings. The function should consider standard working hours as 9 AM to 5 PM from Monday to Friday, excluding any hours outside this range and any weekend hours. # Function Signature ```python def working_hours(start: str, end: str) -> float: pass ``` # Input: - **start** (`str`): The start datetime in ISO 8601 format (`YYYY-MM-DDTHH:MM:SS`). - **end** (`str`): The end datetime in ISO 8601 format (`YYYY-MM-DDTHH:MM:SS`). # Output: - **float**: Total working hours (in decimal form) between the given start and end datetimes. # Constraints: - The `start` datetime will always be before the `end` datetime. - The dates will be given in the same timezone and can be naive datetime strings. # Example: ```python print(working_hours(\\"2023-10-02T08:00:00\\", \\"2023-10-02T18:00:00\\")) # Output: 8.0 print(working_hours(\\"2023-10-01T08:00:00\\", \\"2023-10-03T18:00:00\\")) # Output: 16.0 print(working_hours(\\"2023-10-02T08:00:00\\", \\"2023-10-02T09:00:00\\")) # Output: 0.0 ``` # Detailed Requirements: 1. **Ignore non-working hours**: Any time before 9 AM and after 5 PM on weekdays should be ignored. 2. **Ignore weekends**: Only consider Monday through Friday as working days. 3. **Exact overlap consideration**: If `start` or `end` falls exactly at 9 AM or 5 PM, include those times appropriately in calculations. 4. **Fractional hours**: The result can involve fractions (decimal) as it should reflect the precise amount of working hours between the given times. # Approach 1. Parse the input strings to `datetime` objects. 2. Iterate from the `start` datetime to the `end` datetime, day by day, while accumulating the working hours. 3. For each day, calculate the effective working hours within the 9 AM to 5 PM window. 4. Return the total working hours as a float. # Implementation Notes: - Utilize the `datetime` class for parsing and manipulating date-time objects. - Ensure your function handles edge cases such as start and end times falling outside working hours or spanning weekends. # Testing: - The implementation will be tested across different scenarios, including: - Single-day intervals. - Multi-day intervals. - Intervals spanning weekends.","solution":"from datetime import datetime, timedelta def working_hours(start: str, end: str) -> float: start_dt = datetime.fromisoformat(start) end_dt = datetime.fromisoformat(end) # Define working hour start and end work_start_hour = 9 work_end_hour = 17 # Current calculation variables total_working_hours = 0.0 # Iterate over the range of dates current_dt = start_dt while current_dt < end_dt: # Check if the current day is on a weekend, continue if it is if current_dt.weekday() >= 5: # 5=Saturday, 6=Sunday current_dt += timedelta(days=1) current_dt = current_dt.replace(hour=work_start_hour, minute=0, second=0, microsecond=0) continue # Define the workday start and end times work_start_time = current_dt.replace(hour=work_start_hour, minute=0, second=0, microsecond=0) work_end_time = current_dt.replace(hour=work_end_hour, minute=0, second=0, microsecond=0) # Calculate overlap within a single day actual_start = max(start_dt, work_start_time) actual_end = min(end_dt, work_end_time) if actual_start < actual_end: total_working_hours += (actual_end - actual_start).total_seconds() / 3600.0 # Move to the next day current_dt += timedelta(days=1) current_dt = current_dt.replace(hour=work_start_hour, minute=0, second=0, microsecond=0) return total_working_hours"},{"question":"Objective As a data analyst, you are tasked to explore the relationship between multiple variables in a dataset using faceted plots. Your goal is to write a function that takes a dataset and creates a customized faceted plot using seaborn. Task Write a function `custom_faceted_plot` that takes in the following parameters: - `df`: the dataset to be visualized (a pandas DataFrame). - `x_var`: the variable name for the x-axis (string). - `y_var`: the variable name for the y-axis (string). - `facet_var_1`: the first faceting variable (string). - `facet_var_2`: the second faceting variable (string or None). - `wrap`: an integer indicating if and how to wrap the facets, 0 if no wrapping is needed (default is 0). - `share_x`: a boolean to indicate if the x-axis should be shared across facets (default is True). - `share_y`: a boolean to indicate if the y-axis should be shared across facets (default is True). - `facet_order`: a dictionary to specify the order of levels for faceting variables (default is None). - `title_template`: a string template for facet titles (default is \\"Facet: {}\\"). The function should: 1. Load the seaborn library and prepare the dataset. 2. Create a faceted plot using seaborn\'s `Plot` class and `facet` method with the provided parameters. 3. Apply the custom `order` for facet levels if given. 4. Implement the `wrap` functionality if a value greater than 0 is provided. 5. Use `share` to determine if the axis scales should be shared. 6. Customize the facet titles with the provided `title_template`. 7. Display the plot. Constraints - The dataset will always be a pandas DataFrame. - The variables passed (x_var, y_var, facet_var_1, facet_var_2) will always exist in the DataFrame. - If facet_var_2 is None, only one faceting variable will be used. - The values of `wrap` will always be from 0 to the number of levels in the faceting variable. Example ```python import seaborn.objects as so from seaborn import load_dataset def custom_faceted_plot(df, x_var, y_var, facet_var_1, facet_var_2=None, wrap=0, share_x=True, share_y=True, facet_order=None, title_template=\\"Facet: {}\\"): p = so.Plot(df, x=x_var, y=y_var).add(so.Dots()) if facet_var_2: p = p.facet(facet_var_1, facet_var_2, order=facet_order) else: p = p.facet(facet_var_1, wrap=wrap, order=facet_order).share(x=share_x, y=share_y) p = p.label(title=title_template.format) p.show() # Load example dataset penguins = load_dataset(\\"penguins\\") # Example use of the function custom_faceted_plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"species\\", \\"sex\\", wrap=0, share_x=False, share_y=True, facet_order={\\"species\\": [\\"Gentoo\\", \\"Adelie\\"], \\"sex\\": [\\"Female\\", \\"Male\\"]}, title_template=\\"Species: {}\\") ``` In the example, the `custom_faceted_plot` function creates a faceted plot of the `penguins` dataset, with `bill_length_mm` on the x-axis, `bill_depth_mm` on the y-axis, faceting by `species` and `sex`, with a custom order and titles.","solution":"import seaborn.objects as so import pandas as pd def custom_faceted_plot(df: pd.DataFrame, x_var: str, y_var: str, facet_var_1: str, facet_var_2: str = None, wrap: int = 0, share_x: bool = True, share_y: bool = True, facet_order: dict = None, title_template: str = \\"Facet: {}\\"): Create a customized faceted plot using seaborn. Args: - df: pandas DataFrame, the dataset to be visualized. - x_var: str, the variable name for the x-axis. - y_var: str, the variable name for the y-axis. - facet_var_1: str, the first faceting variable. - facet_var_2: str or None, the second faceting variable. - wrap: int, indicating if and how to wrap the facets, 0 if no wrapping is needed. - share_x: bool, if the x-axis should be shared across facets. - share_y: bool, if the y-axis should be shared across facets. - facet_order: dict or None, to specify the order of levels for faceting variables. - title_template: str, template for facet titles. # Prepare the basic plot with seaborn p = so.Plot(df, x=x_var, y=y_var).add(so.Dots()) # Apply faceting according to provided parameters if facet_var_2: p = p.facet(facet_var_1, facet_var_2, order=facet_order).share(x=share_x, y=share_y) else: p = p.facet(facet_var_1, wrap=wrap if wrap > 0 else None, order=facet_order).share(x=share_x, y=share_y) # Customize titles for facets p = p.label(title=title_template.format) # Display the plot p.show()"},{"question":"# Advanced Exception Handling in Python Objective In this assessment, you will demonstrate your understanding of Python\'s exception handling and related concepts provided in the Python C API documentation. Your task is to design a Python-based solution that mimics some of the behavior described in the documentation using standard Python. Task Write a Python class `CustomExceptionManager` that manages exceptions in a controlled environment. This class should include the following functionalities: 1. **Raise Exceptions**: - A method `raise_custom_exception(exc_type, message)` which raises a specified exception with a given message. - A method `raise_and_print_exception(exc_type, message)` that raises an exception and immediately prints it using a custom traceback formatting. 2. **Clear Exceptions**: - A method `clear_exception()` that will clear any currently raised exception. 3. **Fetch and Inspect Exceptions**: - A method `fetch_current_exception()` that returns a tuple of `(type, value, traceback)` of the currently raised exception. - A method `check_if_exception_occurred()` that returns `True` if an exception has occurred, else `False`. 4. **Issue Warnings**: - A method `issue_custom_warning(warning_type, message)` to issue a warning of a specific type with a message. Implementation Constraints and Requirements: - `CustomExceptionManager` must handle exception and warning types dynamically. - It should use standard exceptions and warnings available in Python. - Your methods should appropriately handle and clear exceptions where necessary. - The exception and warning handling should be demonstrable through a test scenario. Expected Input and Output: - Input: Calls to the methods of `CustomExceptionManager` class with proper parameters. - Output: Raised exceptions, printed custom tracebacks, warnings issued, and boolean checks for exceptions. Example Usage: ```python from custom_exception_manager import CustomExceptionManager # Initialize the manager manager = CustomExceptionManager() # Raising and printing a custom exception try: manager.raise_and_print_exception(ValueError, \\"This is a custom ValueError\\") except ValueError: pass # Checking if an exception occurred after an error is raised has_exception_occurred = manager.check_if_exception_occurred() print(has_exception_occurred) # Expected: True # Fetching details of the current exception exc_info = manager.fetch_current_exception() print(exc_info) # Expected Output: (<class \'ValueError\'>, ValueError(\'This is a custom ValueError\'), traceback object) # Clearing the current exception manager.clear_exception() # Issuing a custom warning manager.issue_custom_warning(UserWarning, \\"This is a custom UserWarning message\\") ``` In your implementation, ensure you demonstrate a test scenario that shows all functionalities working as expected.","solution":"import sys import traceback import warnings class CustomExceptionManager: def __init__(self): self._caught_exception = None def raise_custom_exception(self, exc_type, message): raise exc_type(message) def raise_and_print_exception(self, exc_type, message): try: self.raise_custom_exception(exc_type, message) except exc_type as e: self._caught_exception = sys.exc_info() formatted_traceback = \'\'.join(traceback.format_exception(*self._caught_exception)) print(formatted_traceback) raise e def clear_exception(self): self._caught_exception = None def fetch_current_exception(self): return self._caught_exception def check_if_exception_occurred(self): return self._caught_exception is not None def issue_custom_warning(self, warning_type, message): warnings.warn(message, warning_type)"},{"question":"**Title:** Handling Specific URLLib Errors in Python **Objective:** To assess the knowledge of handling various exceptions provided by the `urllib.error` module when making HTTP requests. **Problem Statement:** You are required to write a Python function named `fetch_url_data` that attempts to download data from a given URL. Your function should handle specific exceptions raised by the `urllib.error` module and respond accordingly. The function should: 1. **Attempt to open the URL** using `urllib.request.urlopen`. 2. **Handle the following exceptions**: - `URLError`: Print \\"URL Error occurred: [reason]\\". - `HTTPError`: - Print \\"HTTP Error: [HTTP status code] - [reason]\\". - Print \\"HTTP Headers: [headers]\\". - `ContentTooShortError`: Print \\"Content Too Short Error: [message]\\" and \\"Downloaded Content: [content]\\". 3. **If no exceptions occur**, return the content of the URL in string format. **Function Signature:** ```python import urllib.request import urllib.error def fetch_url_data(url: str) -> str: # Your code here ``` **Input:** - `url`: A string representing the URL from which to fetch data. **Output:** - A string containing the content of the URL if the request is successful. - Print appropriate error messages for handled exceptions as described above. **Constraints:** - The function should handle network-related issues appropriately. - Assume that the URL provided is a valid string. **Example Usage:** ```python url = \\"http://example.com\\" result = fetch_url_data(url) print(result) ``` When executed, if any of the specific exceptions occur, the appropriate error message should be printed. If the URL content is successfully retrieved, it should be returned as a string. **Note:** The focus of this question is on exception handling using the `urllib.error` module\'s classes.","solution":"import urllib.request import urllib.error def fetch_url_data(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: print(f\\"URL Error occurred: {e.reason}\\") except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") print(f\\"HTTP Headers: {e.headers}\\") except urllib.error.ContentTooShortError as e: print(f\\"Content Too Short Error: {e}\\") print(f\\"Downloaded Content: {e.content.decode(\'utf-8\')}\\") return \\"\\""},{"question":"You are tasked with developing a machine learning pipeline using scikit-learn to classify handwritten digits from the popular `digits` dataset. This assessment will test your ability to load datasets, preprocess data, build and evaluate a model. # Task Details: 1. **Dataset Loading**: - Load the digits dataset using the appropriate function in `sklearn.datasets`. - Split the dataset into 75% training and 25% test subsets. 2. **Data Preprocessing**: - Apply a StandardScaler to standardize the features (mean = 0 and variance = 1). 3. **Model Building**: - Implement a Support Vector Machine (SVM) classifier using `sklearn.svm.SVC`. - Train the model on the training data. 4. **Model Evaluation**: - Evaluate the model using accuracy score and confusion matrix on the test data. # Implementation Details: - You should use `sklearn.datasets` for loading the dataset. - Use `sklearn.model_selection.train_test_split` for splitting the dataset. - Use `sklearn.preprocessing.StandardScaler` for data preprocessing. - Use `sklearn.svm.SVC` for the SVM classifier. - Use `sklearn.metrics` for model evaluation metrics (accuracy score and confusion matrix). # Example: ```python from sklearn import datasets, model_selection, preprocessing, svm, metrics # Step 1: Load the dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Step 2: Split the dataset into training and test sets X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42) # Step 3: Preprocess the data scaler = preprocessing.StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Build and train the model model = svm.SVC() model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) conf_matrix = metrics.confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Confusion Matrix:n{conf_matrix}\\") ``` # Input and Output: - Your solution should return the accuracy score and the confusion matrix. # Constraints and Points to Note: - Your code should be efficient and well-optimized. - Document and comment your code appropriately.","solution":"from sklearn import datasets, model_selection, preprocessing, svm, metrics def classify_digits(): # Step 1: Load the dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Step 2: Split the dataset into training and test sets X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42) # Step 3: Preprocess the data scaler = preprocessing.StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Build and train the model model = svm.SVC() model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) conf_matrix = metrics.confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"# Question: Implement a Library Management System You need to implement a class `Library` that models a simple library management system. The library should be able to maintain a collection of books and allow users to perform operations such as adding books, removing books, and searching for books based on various criteria. Class Definition 1. **Class: `Library`** - Attributes: - `books`: A list of dictionaries, where each dictionary represents a book with the following keys: - `title`: (string) The title of the book. - `author`: (string) The name of the author. - `year`: (integer) The year the book was published. - `isbn`: (string) The unique ISBN of the book. Methods 1. **`add_book(title: str, author: str, year: int, isbn: str) -> None`** - Adds a book to the collection. - Ensures that the book\'s `isbn` is unique. 2. **`remove_book(isbn: str) -> bool`** - Removes a book from the collection based on its `isbn`. - Returns `True` if the book was successfully removed, otherwise `False`. 3. **`find_books_by_title(title: str) -> list`** - Returns a list of books (dictionaries) that contain the given `title` substring (case-insensitive). 4. **`find_books_by_author(author: str) -> list`** - Returns a list of books (dictionaries) written by the given author (case-insensitive). 5. **`find_books_by_year(year: int) -> list`** - Returns a list of books (dictionaries) published in the given year. 6. **`list_books() -> list`** - Returns a list of all books in the library. Example ```python library = Library() # Add books library.add_book(\\"The Hitchhiker\'s Guide to the Galaxy\\", \\"Douglas Adams\\", 1979, \\"978-0345391803\\") library.add_book(\\"1984\\", \\"George Orwell\\", 1949, \\"978-0451524935\\") library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, \\"978-0446310789\\") # List all books print(library.list_books()) # Find books by title print(library.find_books_by_title(\\"1984\\")) # Find books by author print(library.find_books_by_author(\\"George Orwell\\")) # Remove a book print(library.remove_book(\\"978-0451524935\\")) # List all books again to see the updated collection print(library.list_books()) ``` Constraints - The `isbn` will always be a unique identifier for each book. - Use appropriate exception handling to manage errors such as trying to remove a book that doesn\'t exist. - Implement appropriate input validation (e.g., checking if year is a positive integer). Your solution should demonstrate understanding of Python classes, data structures (lists, dictionaries), and control flow (if statements, loops).","solution":"class Library: def __init__(self): self.books = [] def add_book(self, title: str, author: str, year: int, isbn: str) -> None: for book in self.books: if book[\'isbn\'] == isbn: raise ValueError(\\"A book with this ISBN already exists.\\") new_book = { \'title\': title, \'author\': author, \'year\': year, \'isbn\': isbn } self.books.append(new_book) def remove_book(self, isbn: str) -> bool: for book in self.books: if book[\'isbn\'] == isbn: self.books.remove(book) return True return False def find_books_by_title(self, title: str) -> list: return [book for book in self.books if title.lower() in book[\'title\'].lower()] def find_books_by_author(self, author: str) -> list: return [book for book in self.books if author.lower() in book[\'author\'].lower()] def find_books_by_year(self, year: int) -> list: return [book for book in self.books if book[\'year\'] == year] def list_books(self) -> list: return self.books"},{"question":"You are given a binary classification task with a dataset containing feature vectors and corresponding binary labels. Your objective is to train a classifier, evaluate its probability calibration, and improve it if necessary. Follow the steps below to complete this task: Step-by-Step Instructions: 1. **Data Preparation**: - Load the dataset `data.csv` which contains two columns: `features` and `label`. - Split the data into training and test sets. Use 80% of the data for training and 20% for testing. 2. **Model Training**: - Train a `RandomForestClassifier` on the training data. - Use the `predict_proba` method of the trained classifier to predict probabilities for the test set. 3. **Evaluate Calibration**: - Plot a calibration curve for the trained `RandomForestClassifier` using the test set. - Calculate the Brier score loss for the predicted probabilities on the test set. 4. **Improve Calibration**: - Use the `CalibratedClassifierCV` class to calibrate the `RandomForestClassifier` using the training data. - Apply both `sigmoid` and `isotonic` calibration methods. - For each method, plot the calibration curve and calculate the Brier score loss on the test set. 5. **Interpret Results**: - Compare the calibration curve and Brier score loss before and after calibration. - Identify which calibration method (if any) improved the model\'s probability calibration. Constraints: - You must implement the plotting of the calibration curve yourself, without using `CalibrationDisplay.from_estimator`. - The Brier score loss should be calculated using `sklearn.metrics.brier_score_loss`. Function Signature: ```python def evaluate_model_calibration(data_path: str) -> None: Evaluate and improve the probability calibration of a RandomForestClassifier. Parameters: data_path (str): Path to the dataset file \'data.csv\'. Returns: None ``` # Example Usage: ```python # Assuming the dataset \'data.csv\' is in the current directory. evaluate_model_calibration(\'data.csv\') ``` Expected Output: - Calibration plots (before and after applying each method). - Brier score loss values (before and after applying each method). # Notes: - The `data.csv` file can be assumed to be in the format: ``` features,label [1, 2, 3, ..., n],0 [2, 3, 4, ..., n],1 ... ```","solution":"import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import brier_score_loss from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt def plot_calibration_curve(y_true, probas_pred, label, ax): Plot a calibration curve using the true labels and predicted probabilities. from sklearn.calibration import calibration_curve fraction_of_positives, mean_predicted_value = calibration_curve(y_true, probas_pred, n_bins=10) ax.plot(mean_predicted_value, fraction_of_positives, \\"s-\\", label=label) ax.plot([0, 1], [0, 1], \\"k--\\") def evaluate_model_calibration(data_path: str) -> None: # Load the dataset data = pd.read_csv(data_path) X = data[\'features\'].apply(eval).tolist() # converting string representation of list to list y = data[\'label\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the RandomForestClassifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Predict probabilities for the test set probas = clf.predict_proba(X_test)[:, 1] # Calculate initial Brier score loss initial_brier_score = brier_score_loss(y_test, probas) # Plot initial calibration curve fig, ax = plt.subplots(1, 1, figsize=(10, 8)) plot_calibration_curve(y_test, probas, \'Uncalibrated\', ax) ax.set_xlabel(\'Mean predicted value\') ax.set_ylabel(\'Fraction of positives\') ax.set_title(\'Initial Calibration Curve\') ax.legend() plt.show() # Calibrate using sigmoid method calibrated_clf_sigmoid = CalibratedClassifierCV(clf, method=\'sigmoid\', cv=\'prefit\') calibrated_clf_sigmoid.fit(X_train, y_train) calibrated_probas_sigmoid = calibrated_clf_sigmoid.predict_proba(X_test)[:, 1] # Calculate Brier score loss for sigmoid calibration sigmoid_brier_score = brier_score_loss(y_test, calibrated_probas_sigmoid) # Plot calibration curve for sigmoid method fig, ax = plt.subplots(1, 1, figsize=(10, 8)) plot_calibration_curve(y_test, calibrated_probas_sigmoid, \'Sigmoid Calibration\', ax) ax.set_xlabel(\'Mean predicted value\') ax.set_ylabel(\'Fraction of positives\') ax.set_title(\'Sigmoid Calibration Curve\') ax.legend() plt.show() # Calibrate using isotonic method calibrated_clf_isotonic = CalibratedClassifierCV(clf, method=\'isotonic\', cv=\'prefit\') calibrated_clf_isotonic.fit(X_train, y_train) calibrated_probas_isotonic = calibrated_clf_isotonic.predict_proba(X_test)[:, 1] # Calculate Brier score loss for isotonic calibration isotonic_brier_score = brier_score_loss(y_test, calibrated_probas_isotonic) # Plot calibration curve for isotonic method fig, ax = plt.subplots(1, 1, figsize=(10, 8)) plot_calibration_curve(y_test, calibrated_probas_isotonic, \'Isotonic Calibration\', ax) ax.set_xlabel(\'Mean predicted value\') ax.set_ylabel(\'Fraction of positives\') ax.set_title(\'Isotonic Calibration Curve\') ax.legend() plt.show() # Print Brier score losses print(f\'Initial Brier score loss: {initial_brier_score}\') print(f\'Sigmoid Brier score loss: {sigmoid_brier_score}\') print(f\'Isotonic Brier score loss: {isotonic_brier_score}\')"},{"question":"# XML-RPC Communication: Client-Server Implementation **Objective:** Your task is to implement an XML-RPC server and a corresponding client to demonstrate your understanding of remote procedure calls using Python\'s `xmlrpc` package. The communication should involve at least one function call from the client to the server, with the server performing a specified operation and returning the result to the client. # Server Requirements: 1. Implement an XML-RPC server that exposes a function named `add_numbers`. 2. The `add_numbers` function should take two integers as parameters and return their sum. # Client Requirements: 1. Implement an XML-RPC client that connects to the server. 2. The client should call the `add_numbers` function on the server with user-provided integers and print the result. # Input and Output Formats: - **Server:** No direct input-output from files. The server should log all received requests and responses for debugging purposes. - **Client:** The client should read two integers from standard input and call the server\'s function, then print the result to standard output. # Constraints: - Ensure the server can handle multiple requests sequentially (synchronous processing). - The solution must handle network or remote exceptions gracefully. # Performance Requirements: - The server should respond to requests within a reasonably low latency (a few milliseconds for most operations). # Example Interaction: 1. **Server-side Code:** ```python from xmlrpc.server import SimpleXMLRPCServer def add_numbers(x, y): return x + y server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Server started at localhost:8000\\") server.register_function(add_numbers, \\"add_numbers\\") server.serve_forever() ``` 2. **Client-side Code:** ```python from xmlrpc.client import ServerProxy def main(): proxy = ServerProxy(\\"http://localhost:8000/\\") x = int(input(\\"Enter first number: \\")) y = int(input(\\"Enter second number: \\")) result = proxy.add_numbers(x, y) print(f\\"Result: {result}\\") if __name__ == \\"__main__\\": main() ``` # Deliverables: 1. The complete server-side implementation. 2. The complete client-side implementation. 3. Instructions on how to run both server and client to demonstrate the interaction. Your submitted code should be well-documented, with comments explaining each part of the implementation.","solution":"# Server-side implementation from xmlrpc.server import SimpleXMLRPCServer def add_numbers(x, y): Returns the sum of x and y. return x + y def start_server(): server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Server started at localhost:8000\\") server.register_function(add_numbers, \\"add_numbers\\") server.serve_forever() if __name__ == \\"__main__\\": start_server() # Client-side implementation from xmlrpc.client import ServerProxy def main(): proxy = ServerProxy(\\"http://localhost:8000/\\") x = int(input(\\"Enter first number: \\")) y = int(input(\\"Enter second number: \\")) result = proxy.add_numbers(x, y) print(f\\"Result: {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Color Palette Manipulation **Objective:** Your task is to demonstrate proficiency in creating and manipulating color palettes using the `sns.mpl_palette()` function from the Seaborn library. **Description:** 1. Create a continuous colormap palette from the predefined \\"viridis\\" colormap with 10 colors. 2. Create a qualitative colormap palette from the \\"Set2\\" colormap with 8 colors. 3. Use both palettes in separate visualizations of the `iris` dataset to display their effect. **Instructions:** 1. Write a function `create_palettes()` that returns two palettes: - A continuous colormap with 10 colors using \\"viridis\\". - A qualitative colormap with 8 colors using \\"Set2\\". 2. Write another function `plot_with_palettes()` that takes in two palettes and visualizes a scatter plot of the `iris` dataset with the following requirements: - Use the first palette (continuous colormap) for the scatter plot of sepal length vs sepal width, where the color represents the species. - Use the second palette (qualitative colormap) for the scatter plot of petal length vs petal width, where the color represents the species. 3. The function should display the two scatter plots side by side for comparison. **Function Signatures:** ```python def create_palettes(): Create and return two color palettes: - A continuous colormap with 10 colors using \'viridis\'. - A qualitative colormap with 8 colors using \'Set2\'. Returns: tuple: (continuous_palette, qualitative_palette) pass def plot_with_palettes(continuous_palette, qualitative_palette): Plot scatter plots of the iris dataset using the provided color palettes. Parameters: continuous_palette (list): A list of colors from a continuous colormap. qualitative_palette (list): A list of colors from a qualitative colormap. Returns: None pass ``` **Dataset:** Use the `iris` dataset provided by Seaborn: ```python import seaborn as sns iris = sns.load_dataset(\\"iris\\") ``` **Expected Output:** - Two palettes should be created and returned by `create_palettes()`. - Two scatter plots should be generated by `plot_with_palettes()` showing the difference between continuous and qualitative colormaps. **Constraints:** - Ensure the color palettes contain the correct number of colors as specified. - You must use `sns.mpl_palette()` to create the palettes. - Visualizations should be clear and should use appropriate labels for axes and legends. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_palettes(): Create and return two color palettes: - A continuous colormap with 10 colors using \'viridis\'. - A qualitative colormap with 8 colors using \'Set2\'. Returns: tuple: (continuous_palette, qualitative_palette) continuous_palette = sns.color_palette(\'viridis\', 10) qualitative_palette = sns.color_palette(\'Set2\', 8) return continuous_palette, qualitative_palette def plot_with_palettes(continuous_palette, qualitative_palette): Plot scatter plots of the iris dataset using the provided color palettes. Parameters: continuous_palette (list): A list of colors from a continuous colormap. qualitative_palette (list): A list of colors from a qualitative colormap. Returns: None iris = sns.load_dataset(\'iris\') # Create scatter plots fig, ax = plt.subplots(1, 2, figsize=(14, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=continuous_palette, ax=ax[0]) ax[0].set_title(\'Sepal Length vs Sepal Width (Continuous Palette)\') sns.scatterplot(data=iris, x=\'petal_length\', y=\'petal_width\', hue=\'species\', palette=qualitative_palette, ax=ax[1]) ax[1].set_title(\'Petal Length vs Petal Width (Qualitative Palette)\') plt.tight_layout() plt.show()"},{"question":"Using the seaborn package, write a function `create_titanic_plot()` that meets the following requirements: 1. Loads the \'titanic\' dataset. 2. Creates a count plot showing the number of passengers in each class, grouped by survival status. Use different colors to differentiate between survivors and non-survivors. 3. Normalizes the counts to show percentages instead of raw counts. 4. Applies the \'darkgrid\' theme to the plot. 5. Adds a custom title to the plot, \\"Titanic Passenger Class Distribution by Survival Status\\". 6. Saves the final plot as a PNG file named `titanic_class_distribution.png`. # Constraints - The function should not take any input parameters. - The function should save the plot file in the current working directory. # Input/Output Format - **Input:** None - **Output:** A file named `titanic_class_distribution.png` saved in the current working directory. # Example After executing the function `create_titanic_plot()`, you should be able to find a file `titanic_class_distribution.png` in your working directory, that visually represents the specified requirements. Write your function implementation below: ```python import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plot(): # Load the titanic dataset provided by seaborn titanic = sns.load_dataset(\\"titanic\\") # Set the seaborn theme to \'darkgrid\' sns.set_theme(style=\\"darkgrid\\") # Create a count plot normalized to show percentages plt.figure(figsize=(10, 6)) ax = sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") # Add the custom title to the plot ax.set_title(\\"Titanic Passenger Class Distribution by Survival Status\\") # Save the plot as a PNG file named \'titanic_class_distribution.png\' plt.savefig(\\"titanic_class_distribution.png\\") # Close the plot to free up memory plt.close() # Example function call (Uncomment to test) # create_titanic_plot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plot(): # Load the titanic dataset provided by seaborn titanic = sns.load_dataset(\\"titanic\\") # Set the seaborn theme to \'darkgrid\' sns.set_theme(style=\\"darkgrid\\") # Create a count plot normalized to show percentages plt.figure(figsize=(10, 6)) ax = sns.histplot(data=titanic, x=\'class\', hue=\'survived\', multiple=\'fill\', shrink=.8, stat=\'percent\') # Add the custom title to the plot ax.set_title(\\"Titanic Passenger Class Distribution by Survival Status\\") # Save the plot as a PNG file named \'titanic_class_distribution.png\' plt.savefig(\\"titanic_class_distribution.png\\") # Close the plot to free up memory plt.close()"},{"question":"# SQLite Database Query and Custom Type Handling with Pythonâ€™s `sqlite3` Module Objective: Create a Python script using the `sqlite3` module to: 1. Create and manage an SQLite database. 2. Implement custom types and converters. 3. Handle transactions efficiently. Task Details: 1. **Database Creation and Insertion:** - Create a new SQLite database named `library.db`. - Create a table named `books` with the following schema: ```sql CREATE TABLE books ( id INTEGER PRIMARY KEY, title TEXT NOT NULL, author TEXT NOT NULL, published_date TEXT NOT NULL -- Store dates as text in the format \'YYYY-MM-DD\' ); ``` - Insert the following data into the `books` table: ```plaintext id | title | author | published_date ----------------------------------------------------------- 1 | To Kill a Mockingbird | Harper Lee | 1960-07-11 2 | 1984 | George Orwell | 1949-06-08 3 | Moby Dick | Herman Melville | 1851-10-18 ``` 2. **Custom Type Handling:** - Define a Python class `Book` to represent a book with attributes `id`, `title`, `author`, and `published_date`. Store the `published_date` as a `datetime.date` object. - Write an adapter function to convert `Book` objects into a format that can be stored in the SQLite database. - Write a converter function to convert the text representation of the `published_date` back into a `datetime.date` object while fetching from the database. 3. **Transaction Handling and Querying:** - Use transaction control to ensure that the data insertion is atomic. - Write a function `fetch_books_by_year(year: int)` that fetches and returns all books published in a given year, utilizing the custom `Book` class. Requirements: - The `published_date` should be stored and retrieved as `datetime.date` objects using custom adapters and converters. - Ensure proper use of transactions to handle database operations. - Include appropriate exception handling for database errors. - The script should be self-contained and executable as is. Expected Output Format: - The `fetch_books_by_year(1949)` should return and print: ```plaintext [Book(id=2, title=\'1984\', author=\'George Orwell\', published_date=datetime.date(1949, 6, 8))] ``` Implementation Constraints: - Adhere to PEP 249 for database operations. - Use the `sqlite3` module\'s adapters and converters for custom type handling. - Ensure the script commits transactions only upon successful data operations. **Starter Code:** ```python import sqlite3 import datetime # Define the Book class class Book: def __init__(self, id, title, author, published_date): self.id = id self.title = title self.author = author self.published_date = published_date def __repr__(self): return f\\"Book(id={self.id}, title=\'{self.title}\', author=\'{self.author}\', published_date={self.published_date})\\" # Adapter and Converter functions def adapt_date(date): return date.isoformat() def convert_date(date_str): return datetime.date.fromisoformat(date_str.decode()) # Register the adapter and converter with sqlite3 sqlite3.register_adapter(datetime.date, adapt_date) sqlite3.register_converter(\\"DATE\\", convert_date) # Database connection and operations def create_database(): con = sqlite3.connect(\\"library.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.executescript(\'\'\' DROP TABLE IF EXISTS books; CREATE TABLE books ( id INTEGER PRIMARY KEY, title TEXT NOT NULL, author TEXT NOT NULL, published_date DATE NOT NULL ); \'\'\') books = [ (1, \'To Kill a Mockingbird\', \'Harper Lee\', datetime.date(1960, 7, 11)), (2, \'1984\', \'George Orwell\', datetime.date(1949, 6, 8)), (3, \'Moby Dick\', \'Herman Melville\', datetime.date(1851, 10, 18)) ] cur.executemany(\\"INSERT INTO books VALUES (?, ?, ?, ?)\\", books) con.commit() con.close() def fetch_books_by_year(year): con = sqlite3.connect(\\"library.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\\"SELECT * FROM books WHERE strftime(\'%Y\', published_date) = ?\\", (year,)) rows = cur.fetchall() books = [Book(row[0], row[1], row[2], row[3]) for row in rows] con.close() return books # Create the database and fetch books create_database() print(fetch_books_by_year(1949)) ``` Write your logic and complete the above script to meet the task requirements.","solution":"import sqlite3 import datetime # Define the Book class class Book: def __init__(self, id, title, author, published_date): self.id = id self.title = title self.author = author self.published_date = published_date def __repr__(self): return f\\"Book(id={self.id}, title=\'{self.title}\', author=\'{self.author}\', published_date={self.published_date})\\" # Adapter and Converter functions def adapt_date(date): return date.isoformat() def convert_date(date_str): return datetime.date.fromisoformat(date_str.decode()) # Register the adapter and converter with sqlite3 sqlite3.register_adapter(datetime.date, adapt_date) sqlite3.register_converter(\\"DATE\\", convert_date) # Database connection and operations def create_database(): con = sqlite3.connect(\\"library.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.executescript(\'\'\' DROP TABLE IF EXISTS books; CREATE TABLE books ( id INTEGER PRIMARY KEY, title TEXT NOT NULL, author TEXT NOT NULL, published_date DATE NOT NULL ); \'\'\') books = [ (1, \'To Kill a Mockingbird\', \'Harper Lee\', datetime.date(1960, 7, 11)), (2, \'1984\', \'George Orwell\', datetime.date(1949, 6, 8)), (3, \'Moby Dick\', \'Herman Melville\', datetime.date(1851, 10, 18)) ] cur.executemany(\\"INSERT INTO books VALUES (?, ?, ?, ?)\\", books) con.commit() con.close() def fetch_books_by_year(year): con = sqlite3.connect(\\"library.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\\"SELECT * FROM books WHERE strftime(\'%Y\', published_date) = ?\\", (str(year),)) rows = cur.fetchall() books = [Book(row[0], row[1], row[2], row[3]) for row in rows] con.close() return books # Create the database create_database() # This print is for manual verification, can be removed in production code print(fetch_books_by_year(1949))"},{"question":"You are required to create a PyTorch function that performs matrix multiplication on two input tensors. After defining this function, use the Just-In-Time (JIT) utilities provided by `torch.utils.jit` to optimize its performance using TorchScript. Function Signature ```python import torch from torch import Tensor from typing import Tuple def matrix_multiply_jit(a: Tensor, b: Tensor) -> Tensor: pass ``` # Requirements 1. **Function Implementation**: - Define a function `matrix_multiply_jit` that takes in two input tensors `a` and `b`. - Perform matrix multiplication on these two tensors. - Return the resulting tensor. 2. **JIT Compilation**: - Use the `torch.jit.script` decorator or functionality to transform the function into TorchScript for performance optimization. Expected Input and Output - **Input**: Two 2D tensors `a` and `b`, where the number of columns in `a` matches the number of rows in `b`. - **Output**: A 2D tensor that is the result of matrix multiplication. Constraints - Input tensors `a` and `b` will have compatible shapes for matrix multiplication. - The function should handle large input sizes efficiently by leveraging JIT optimization. # Example ```python import torch from torch import Tensor # Example tensors a = torch.tensor([[1, 2], [3, 4]]) b = torch.tensor([[5, 6], [7, 8]]) # Function definition and JIT compilation (to be done by the student) result = matrix_multiply_jit(a, b) print(result) # Expected output: tensor([[19, 22], # [43, 50]]) ``` # Performance Requirements - Ensure the function performs efficiently with large tensor inputs by utilizing JIT compilation. # Notes - You are free to use additional PyTorch functions and modules as needed. - Ensure your code is clean, efficient, and well-documented.","solution":"import torch from torch import Tensor def matrix_multiply(a: Tensor, b: Tensor) -> Tensor: Perform matrix multiplication on two input tensors a and b. return torch.matmul(a, b) # Script and JIT compile the function matrix_multiply_jit = torch.jit.script(matrix_multiply)"},{"question":"Objective: Write a Python function that processes a nested list of integers and performs specific transformations using comprehensions and generator expressions. The input will be a list of lists of integers, and the function should return a list of results based on the steps provided. Problem Description: 1. Given a list of lists of integers, flatten the list using a generator expression. 2. Filter out all negative integers and even numbers from the flattened list. Use a list comprehension to achieve this. 3. Generate a dictionary where keys are integers from the filtered list and values are their respective squares, using a dictionary comprehension. 4. The function should finally return a dictionary where only those keys are included whose values are greater than a given threshold. Function Signature: ```python def process_nested_list(input_list: list, threshold: int) -> dict: pass ``` Input: - `input_list`: A list of lists of integers (e.g., [[1, -2, 3], [-4, 5, 6], [7, -8, 9]]) - `threshold`: An integer value for filtering dictionary items. Output: - The function should return a dictionary where keys are integers from the filtered list and values are their squares, only if the squared value is greater than the threshold provided. Constraints: 1. The input list can contain up to 10^3 sublists, and each sublist can contain up to 10^3 integers. 2. The threshold will be a non-negative integer. Example: ```python input_list = [[1, -2, 3], [-4, 5, 6], [7, -8, 9]] threshold = 20 process_nested_list(input_list, threshold) # Output: {5: 25, 7: 49, 9: 81} ``` Notes: - Be sure to use comprehensions and generator expressions in your solution as specified. - Handle any potential exceptions gracefully. - Ensure the performance of your function is optimal, especially for the upper constraint limits.","solution":"def process_nested_list(input_list, threshold): Process a nested list of integers through specific transformations and return a dictionary. :param input_list: List of lists of integers :param threshold: Non-negative integer threshold for filtering squared values :return: Dictionary with integers as keys and their squares as values, filtered by threshold # Flattening the list flattened = (num for sublist in input_list for num in sublist) # Filtering out negative integers and even numbers filtered = [x for x in flattened if x > 0 and x % 2 != 0] # Generating dictionary with integers and their squares squared_dict = {x: x**2 for x in filtered} # Filtering the dictionary by threshold value result = {k: v for k, v in squared_dict.items() if v > threshold} return result"},{"question":"# Challenge: Implementing and Visualizing Kernel Density Estimation **Problem Statement:** Write a Python function using scikit-learn\'s `KernelDensity` to estimate the density of a given 1D dataset. The function should take as input the data, kernel type, and bandwidth. It should return the density estimates and also plot them for visualization. Function Signature ```python def estimate_and_plot_kde(data: np.ndarray, kernel: str, bandwidth: float) -> np.ndarray: Parameters: data (np.ndarray): A 1D array of data points. kernel (str): The type of kernel to use for density estimation. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. bandwidth (float): The bandwidth parameter for the KDE. Returns: np.ndarray: An array of log density estimates for the input data. Plots: A plot showing the KDE for the given data using the specified kernel and bandwidth. pass ``` Example Usage ```python import numpy as np # Example data data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # Compute and plot KDE using a Gaussian kernel with bandwidth 0.5 density_estimates = estimate_and_plot_kde(data, kernel=\'gaussian\', bandwidth=0.5) print(density_estimates) ``` Requirements 1. **Data Input**: The input `data` is a 1D NumPy array of numeric values. 2. **Kernel Selection**: The `kernel` parameter should accept the following strings: \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. 3. **Bandwidth Specification**: The `bandwidth` parameter is a float that controls the smoothness of the density estimate. 4. **Density Estimation**: Implement the KDE using scikit-learn\'s `KernelDensity` and fit it to the input data. 5. **Density Scoring**: Calculate the log density estimates for the input data points. 6. **Visualization**: Plot the density estimates using a line plot with the data points along the x-axis. Constraints and Considerations - The function should handle invalid kernel types by raising a `ValueError` with an appropriate message. - Ensure that the bandwidth parameter is a positive float; otherwise, raise a `ValueError`. - Quality of visualization and proper labeling of the plot will be considered. **Notes:** - You might find `matplotlib` useful for plotting in this task. - For large datasets, consider the performance implications of the chosen bandwidth. Evaluation Criteria - Correct implementation of KDE using different kernels. - Accurate calculation of log density estimates. - Clear and informative visualization of the KDE results. - Proper handling of edge cases and invalid inputs.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def estimate_and_plot_kde(data: np.ndarray, kernel: str, bandwidth: float) -> np.ndarray: Parameters: data (np.ndarray): A 1D array of data points. kernel (str): The type of kernel to use for density estimation. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', and \'cosine\'. bandwidth (float): The bandwidth parameter for the KDE. Returns: np.ndarray: An array of log density estimates for the input data. Plots: A plot showing the KDE for the given data using the specified kernel and bandwidth. if bandwidth <= 0: raise ValueError(\\"Bandwidth must be a positive float.\\") valid_kernels = [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] if kernel not in valid_kernels: raise ValueError(f\\"Invalid kernel type. Valid options are {valid_kernels}.\\") kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data[:, np.newaxis]) x_d = np.linspace(min(data) - 1, max(data) + 1, 1000) log_density = kde.score_samples(x_d[:, np.newaxis]) plt.figure(figsize=(8, 6)) plt.plot(x_d, np.exp(log_density), label=f\'KDE with {kernel} kernel\') plt.fill_between(x_d, np.exp(log_density), alpha=0.5) plt.scatter(data, np.zeros_like(data) - 0.01, marker=\'+\', color=\'red\', label=\'Data points\') plt.title(f\'Kernel Density Estimation with {kernel} kernel\') plt.xlabel(\'Data\') plt.ylabel(\'Density\') plt.legend() plt.show() return kde.score_samples(data[:, np.newaxis])"},{"question":"**Coding Assessment Question** **Objective**: Assess the ability to work with Python\'s \\"sysconfig\\" module to retrieve and manipulate configuration information. **Problem Statement**: You are tasked with creating a function that prints out detailed configuration information for the current platform and verifies certain configurations. Implement the following function: ```python import sysconfig def print_and_verify_sysconfig(required_paths, required_vars): Prints detailed configuration information including current platform, Python version, and installation paths. Also verifies if the specified paths and configuration variables exist and meet certain criteria. Args: required_paths (list): A list of path names to be verified. required_vars (list): A list of configuration variable names to be verified. Returns: dict: A dictionary indicating the presence and correctness of each required path and variable. The keys are the names from required_paths and required_vars. The values are booleans indicating if the path or variable exists and meets the criteria. pass ``` **Function Requirements**: 1. **Print Configuration Details**: - Print the current platform using `sysconfig.get_platform()`. - Print the Python version using `sysconfig.get_python_version()`. - Print all installation paths using `sysconfig.get_paths()`. - Print all configuration variables using `sysconfig.get_config_vars()`. 2. **Verify Configuration**: - For each path in `required_paths`, verify that it exists in the list returned by `sysconfig.get_path_names()` and its associated value is a valid, non-empty string from `sysconfig.get_path()`. - For each variable in `required_vars`, verify that it exists and its associated value is not `None`. 3. **Return Verification Results**: - Return a dictionary where the keys are the names from `required_paths` and `required_vars`, and the values are booleans indicating whether each path or variable exists and meets the criteria mentioned above. **Input and Output Formats**: - **Input**: - `required_paths`: A list of strings representing the names of paths to verify (e.g., `[\'stdlib\', \'scripts\']`). - `required_vars`: A list of strings representing the names of configuration variables to verify (e.g., `[\'AR\', \'CXX\']`). - **Output**: - A dictionary with the required path and variable names as keys and booleans as values indicating their existence and correctness. **Example**: ```python required_paths = [\'stdlib\', \'scripts\'] required_vars = [\'AR\', \'CXX\'] result = print_and_verify_sysconfig(required_paths, required_vars) # Example print output: # Platform: \\"win-amd64\\" # Python version: \\"3.10\\" # Paths: {...} # Variables: {...} # Example return value: # { # \'stdlib\': True, # \'scripts\': True, # \'AR\': True, # \'CXX\': True # } ``` **Constraints**: - The lists `required_paths` and `required_vars` will have a maximum length of 10 each. - Ensure that all path and variable names in the input lists conform to the naming conventions described in the documentation. **Performance**: - The function should efficiently handle retrieving and validating the configurations without significant delays.","solution":"import sysconfig def print_and_verify_sysconfig(required_paths, required_vars): Prints detailed configuration information including current platform, Python version, and installation paths. Also verifies if the specified paths and configuration variables exist and meet certain criteria. Args: required_paths (list): A list of path names to be verified. required_vars (list): A list of configuration variable names to be verified. Returns: dict: A dictionary indicating the presence and correctness of each required path and variable. The keys are the names from required_paths and required_vars. The values are booleans indicating if the path or variable exists and meets the criteria. # Print configuration details print(f\\"Platform: {sysconfig.get_platform()}\\") print(f\\"Python version: {sysconfig.get_python_version()}\\") print(f\\"Paths: {sysconfig.get_paths()}\\") print(f\\"Variables: {sysconfig.get_config_vars()}\\") results = {} # Verify required paths available_paths = sysconfig.get_paths() for path in required_paths: results[path] = path in available_paths and bool(available_paths[path]) # Verify required variables config_vars = sysconfig.get_config_vars() for var in required_vars: results[var] = var in config_vars and config_vars[var] is not None return results"},{"question":"**Question: Handling Temporary Files and Directories Using `tempfile` Module** **Objective:** Ensure students understand the creation and management of temporary files and directories using the `tempfile` module, highlighting the use of context managers and clean-up operations. **Problem Statement:** Write a Python script that performs the following tasks using the `tempfile` module: 1. Creates a temporary directory. 2. Within this temporary directory, creates a specified number of temporary files. 3. Writes unique content to each temporary file. 4. Reads back the content from each file to verify it was written correctly. 5. Ensures all temporary files and the temporary directory are deleted after their usage (i.e., upon exiting the context). **Requirements:** - Use `tempfile.TemporaryDirectory` for creating the temporary directory. - Use `tempfile.NamedTemporaryFile` for creating the temporary files. - Each temporary file should have a suffix of `.tmp`. - The script should take an argument `n`, specifying the number of temporary files to create. - The content written to each file should be `\\"Content of file <i>\\"`, where `<i>` is the index of the file (starting from 1). **Constraints:** - The maximum value of `n` is 1000. - Ensure that all files and directories are cleaned up properly without leaving any residual temporary files or directories. **Function Signature:** ```python def handle_temp_files(n: int) -> None: pass ``` **Example Execution:** ```python def test_handle_temp_files(): handle_temp_files(5) test_handle_temp_files() ``` In this test example, the function `handle_temp_files` should create a temporary directory containing 5 temporary files. Each file should contain the appropriate content, and the directory along with all its contents should be automatically cleaned up after the script completes.","solution":"import tempfile import os def handle_temp_files(n: int) -> None: Creates a temporary directory and within it creates `n` temporary files. Writes unique content to each file and reads it back to verify. if n <= 0 or n > 1000: raise ValueError(\\"The value of n must be between 1 and 1000.\\") with tempfile.TemporaryDirectory() as temp_dir: for i in range(1, n+1): file_path = os.path.join(temp_dir, f\'tempfile_{i}.tmp\') with open(file_path, \'w+\') as temp_file: content = f\\"Content of file {i}\\" temp_file.write(content) temp_file.seek(0) read_content = temp_file.read() if read_content != content: raise IOError(f\\"File content mismatch for file {i}\\")"},{"question":"# Advanced Coding Assessment: Implementing Stochastic Gradient Descent (SGD) on Custom Data **Objective**: This assessment is designed to evaluate your understanding of the `SGDClassifier` and `SGDRegressor` implementations in scikit-learn. You will need to preprocess the data, configure the models, and evaluate their performance. # Problem Statement You are given a dataset with both classification and regression tasks. Your objective is to: 1. Load and preprocess the data, ensuring the features are scaled. 2. Implement and train an `SGDClassifier` for the classification task. 3. Implement and train an `SGDRegressor` for the regression task. 4. Evaluate the performance of both models using appropriate metrics. # Dataset The dataset consists of `n_samples = 5000` and `n_features = 20`. The first 1000 samples are for the classification task with binary labels (0 or 1), and the remaining 4000 samples are for the regression task with continuous target values. # Instructions 1. **Data Preprocessing**: - Split the data into training and testing sets (80% train, 20% test) separately for classification and regression. - Scale the features using `StandardScaler`. 2. **Classification Task**: - Implement an `SGDClassifier` using the hinge loss and L2 penalty. - Use grid search to find the best hyperparameters for `alpha` and `max_iter`. Consider the following values for the grid search: - `alpha`: [0.0001, 0.001, 0.01] - `max_iter`: [1000, 2000, 3000] - Evaluate the model using accuracy and F1-score on the test set. 3. **Regression Task**: - Implement an `SGDRegressor` using the squared_error loss and Elastic Net penalty. - Set `l1_ratio=0.15`. - Use early stopping with `validation_fraction=0.1` and `n_iter_no_change=5`. - Evaluate the model using Mean Squared Error (MSE) and R^2 score on the test set. 4. **Performance Reporting**: - Report the best hyperparameters and performance metrics for the classification model. - Report the performance metrics for the regression model. # Input Format - `data.npy`: A NumPy array of shape `(5000, 21)`. The first 20 columns are the features, the 21st column is the target. # Output Format - Print the best hyperparameters for the `SGDClassifier`. - Print accuracy and F1-score for the `SGDClassifier`. - Print MSE and R^2 score for the `SGDRegressor`. # Constraints - Use `sklearn` library for model implementation. - Ensure reproducibility by setting a random seed. - The code should handle potential exceptions during model training and evaluation. # Example ```python import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score # Load data data = np.load(\'data.npy\') X = data[:, :-1] y = data[:, -1] # Split data into classification and regression X_class, y_class = X[:1000], y[:1000] X_reg, y_reg = X[1000:], y[1000:] # Further split into training and test sets X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.2, random_state=42) X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_class_train = scaler.fit_transform(X_class_train) X_class_test = scaler.transform(X_class_test) X_reg_train = scaler.fit_transform(X_reg_train) X_reg_test = scaler.transform(X_reg_test) # Classification Task param_grid = {\'alpha\': [0.0001, 0.001, 0.01], \'max_iter\': [1000, 2000, 3000]} clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', random_state=42) grid_search = GridSearchCV(clf, param_grid, cv=5) grid_search.fit(X_class_train, y_class_train) best_clf = grid_search.best_estimator_ # Evaluate Classification y_class_pred = best_clf.predict(X_class_test) accuracy = accuracy_score(y_class_test, y_class_pred) f1 = f1_score(y_class_test, y_class_pred) # Regression Task reg = SGDRegressor(loss=\'squared_error\', penalty=\'elasticnet\', l1_ratio=0.15, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42) reg.fit(X_reg_train, y_reg_train) # Evaluate Regression y_reg_pred = reg.predict(X_reg_test) mse = mean_squared_error(y_reg_test, y_reg_pred) r2 = r2_score(y_reg_test, y_reg_pred) # Print Results print(\\"Best Classifier Hyperparameters:\\", grid_search.best_params_) print(\\"Classification Accuracy:\\", accuracy) print(\\"Classification F1-Score:\\", f1) print(\\"Regression MSE:\\", mse) print(\\"Regression R^2 Score:\\", r2) ```","solution":"import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score def stochastic_gradient_descent(data): X = data[:, :-1] y = data[:, -1] # Split data into classification and regression problems X_class, y_class = X[:1000], y[:1000] X_reg, y_reg = X[1000:], y[1000:] # Splitting and scaling data for classification X_class_train, X_class_test, y_class_train, y_class_test = train_test_split( X_class, y_class, test_size=0.2, random_state=42) scaler_class = StandardScaler() X_class_train = scaler_class.fit_transform(X_class_train) X_class_test = scaler_class.transform(X_class_test) # Classification with SGDClassifier param_grid = {\'alpha\': [0.0001, 0.001, 0.01], \'max_iter\': [1000, 2000, 3000]} sgd_clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', random_state=42) grid_search = GridSearchCV(sgd_clf, param_grid, cv=5) grid_search.fit(X_class_train, y_class_train) best_clf = grid_search.best_estimator_ y_class_pred = best_clf.predict(X_class_test) accuracy = accuracy_score(y_class_test, y_class_pred) f1 = f1_score(y_class_test, y_class_pred) # Splitting and scaling data for regression X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split( X_reg, y_reg, test_size=0.2, random_state=42) scaler_reg = StandardScaler() X_reg_train = scaler_reg.fit_transform(X_reg_train) X_reg_test = scaler_reg.transform(X_reg_test) # Regression with SGDRegressor sgd_reg = SGDRegressor(loss=\'squared_error\', penalty=\'elasticnet\', l1_ratio=0.15, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42) sgd_reg.fit(X_reg_train, y_reg_train) y_reg_pred = sgd_reg.predict(X_reg_test) mse = mean_squared_error(y_reg_test, y_reg_pred) r2 = r2_score(y_reg_test, y_reg_pred) return { \\"best_class_params\\": grid_search.best_params_, \\"classification_accuracy\\": accuracy, \\"classification_f1\\": f1, \\"regression_mse\\": mse, \\"regression_r2\\": r2 }"},{"question":"Objective Create a Python module that reads a text file with mixed text and binary data, processes it to separate the text from the binary data, and saves the results into two distinct files (one with text data and one with binary data). Your solution should be compatible with both Python 2.7 and Python 3.10, showcasing your ability to write code that works seamlessly across both versions. Requirements 1. **Input**: - A filename (string) of a file containing mixed text and binary data. 2. **Output**: - Two new files: - \\"text_data.txt\\": A file containing all the text data extracted from the input file. - \\"binary_data.bin\\": A file containing all the binary data extracted from the input file. - Return a tuple with the names of the two new files created: `(\\"text_data.txt\\", \\"binary_data.bin\\")`. 3. **Constraints**: - You must handle Python 2.7 and Python 3.10 compatibility within your code. - Use feature detection instead of version detection as much as possible. 4. **Performance**: - The code should handle large files efficiently, ensuring it doesn\'t load the entire file contents into memory if not necessary. Function Signature ```python def process_file(filename): Reads a file containing mixed text and binary data, separates the text and binary data, and writes them to two separate files compatible with Python 2.7 and Python 3.10. Args: filename (str): The name of the input file to process. Returns: tuple: A tuple containing the names of the text and binary data files. pass ``` Sample Usage ```python # Assuming the file \'mixed_data.dat\' contains a mix of text and binary data. result = process_file(\'mixed_data.dat\') print(result) # Output: (\'text_data.txt\', \'binary_data.bin\') ``` Important Notes - Ensure you use the `io` module for file operations to support compatibility between Python 2.7 and Python 3.10. - Make sure binary data is correctly identified and separated from text data. - Include the necessary import statements and \\"__future__\\" imports at the beginning of your module to ensure compatibility. - You can assume the binary data and text data are not interleaved in the same lines. Good luck, and make sure your solution demonstrates a deep understanding of both Python 2 and Python 3 differences as outlined in the documentation.","solution":"from __future__ import print_function, unicode_literals import io def process_file(filename): Reads a file containing mixed text and binary data, separates the text and binary data, and writes them to two separate files compatible with Python 2.7 and Python 3.10. Args: filename (str): The name of the input file to process. Returns: tuple: A tuple containing the names of the text and binary data files. text_filename = \\"text_data.txt\\" binary_filename = \\"binary_data.bin\\" is_binary = lambda byte: byte > 127 with io.open(filename, \'rb\') as mixed_file: with io.open(text_filename, \'w\', encoding=\'utf-8\') as text_file, io.open(binary_filename, \'wb\') as binary_file: while True: chunk = mixed_file.read(1024) if not chunk: break text_data = [] binary_data = [] for byte in chunk: if isinstance(byte, int): if is_binary(byte): binary_data.append(byte) else: text_data.append(chr(byte)) elif isinstance(byte, str): # Python 2 case if is_binary(ord(byte)): binary_data.append(byte) else: text_data.append(byte) if text_data: text_file.write(\'\'.join(text_data)) if binary_data: binary_file.write(bytearray(binary_data)) return (text_filename, binary_filename)"},{"question":"Objective Write a Python script that demonstrates your understanding of the asyncio package by creating a simple TCP echo server. The server should be able to handle multiple clients concurrently, echoing back any messages received. You must also implement a client to test the server. # Requirements 1. **Server Implementation** - The server should listen on a specified port (e.g., 8888). - The server must handle multiple clients concurrently using asyncio. - For each client connection, the server should receive data and send it back (echo) to the client. - The server should handle the connection lifecycle correctly, managing connection setup and teardown. 2. **Client Implementation** - The client should connect to the server and send a series of messages. - The client should print out the messages received back from the server. 3. **Validation** - Test the server and client to ensure that the server echoes the messages correctly. - Handle any potential errors gracefully, ensuring that the server remains operational even when clients disconnect unexpectedly. # Input and Output - The server does not require any input to start but will process messages from clients. - The client will send predefined messages to the server and print the responses. # Constraints - Use the low-level asyncio API and methods such as `loop.create_server()`, `loop.run_until_complete()`, `loop.create_connection()`, and other relevant methods described in the documentation. - Ensure proper exception handling and logging for debugging purposes. - The implementation should be performant and capable of managing multiple client connections effectively. # Starter Code ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\\"Connection established.\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) def connection_lost(self, exc): print(\\"Connection closed.\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Server entry point asyncio.run(main()) # Client implementation (for testing purposes) async def tcp_echo_client(message): reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() # Test the client asyncio.run(tcp_echo_client(\'Hello, World!\')) ``` Instructions 1. Complete the `EchoServerProtocol` class to handle the client connection lifecycle. 2. Modify the client implementation to send multiple messages and handle responses. 3. Ensure that the server is capable of handling multiple clients concurrently. 4. Test the server and client, providing evidence that they work as expected.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\\"Connection established.\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) def connection_lost(self, exc): print(\\"Connection closed.\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Server entry point if __name__ == \'__main__\': asyncio.run(main()) # Client implementation (for testing purposes) async def tcp_echo_client(message): reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed()"},{"question":"# Custom Calendar Event Planner **Objective**: Implement a function to generate a custom calendar display with highlighted events for a given month. **Description**: You need to write a function `generate_event_calendar(year, month, events, firstweekday=0)` that generates an HTML calendar for a specific month and highlights specified events. The function should use the `calendar.HTMLCalendar` class to generate the calendar and customize it to highlight the days with events. **Function Signature**: ```python def generate_event_calendar(year: int, month: int, events: dict, firstweekday: int = 0) -> str: pass ``` **Parameters**: - `year` (int): The year for the desired calendar. - `month` (int): The month for the desired calendar (1-12). - `events` (dict): A dictionary where keys are integers representing days of the month, and values are strings describing the event on that day. - `firstweekday` (int): The first day of the week (0 for Monday, 6 for Sunday). Defaults to 0. **Returns**: - `str`: A string containing the month\'s calendar in HTML format with events highlighted. **Requirements**: 1. The HTML calendar should include the specified year and month. 2. Days with events should be highlighted with a CSS class called \\"event\\". 3. Each event day should have a tooltip displaying the event description when hovered over. 4. The calendar should start with the specified weekday, as set by the `firstweekday` parameter. **Example Usage**: ```python events = { 1: \\"New Year\'s Day\\", 14: \\"Friend\'s Birthday\\", 25: \\"Conference\\" } print(generate_event_calendar(2023, 1, events, firstweekday=0)) ``` **Example Output** (Embedded within an HTML document): ```html <table border=\\"0\\" cellpadding=\\"0\\" cellspacing=\\"0\\" class=\\"month\\"> <tr><th colspan=\\"7\\" class=\\"month\\">January 2023</th></tr> <tr><th class=\\"mon\\">Mon</th><th class=\\"tue\\">Tue</th><th class=\\"wed\\">Wed</th><th class=\\"thu\\">Thu</th><th class=\\"fri\\">Fri</th><th class=\\"sat\\">Sat</th><th class=\\"sun\\">Sun</th></tr> <tr><td><span class=\\"event\\" title=\\"New Year\'s Day\\">1</span></td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td></tr> <tr><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td><span class=\\"event\\" title=\\"Friend\'s Birthday\\">14</span></td></tr> <tr><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td></tr> <tr><td>22</td><td>23</td><td>24</td><td><span class=\\"event\\" title=\\"Conference\\">25</span></td><td>26</td><td>27</td><td>28</td></tr> <tr><td>29</td><td>30</td><td>31</td><td class=\\"noday\\"></td><td class=\\"noday\\"></td><td class=\\"noday\\"></td><td class=\\"noday\\"></td></tr> </table> ``` **Note**: - You may need to use custom CSS to properly highlight event days and attach tooltips. Assume a class `.event` is defined in your CSS for this purpose. - Focus on the functionality and correctness of the `generate_event_calendar` function. The actual styling can be adjusted as needed.","solution":"import calendar class EventHTMLCalendar(calendar.HTMLCalendar): def __init__(self, events, firstweekday=0): super().__init__(firstweekday) self.events = events def formatday(self, day, weekday): if day == 0: return \'<td class=\\"noday\\"></td>\' # day outside month else: cssclass = self.cssclasses[weekday] if day in self.events: event = self.events[day] return f\'<td class=\\"{cssclass}\\"><span class=\\"event\\" title=\\"{event}\\">{day}</span></td>\' return f\'<td class=\\"{cssclass}\\">{day}</td>\' def formatmonth(self, year, month, withyear=True): v = [] a = v.append a(\'<table border=\\"0\\" cellpadding=\\"0\\" cellspacing=\\"0\\" class=\\"month\\">\') a(\'n\') a(self.formatmonthname(year, month, withyear=withyear)) a(\'n\') a(self.formatweekheader()) a(\'n\') for week in self.monthdays2calendar(year, month): a(self.formatweek(week)) a(\'n\') a(\'</table>\') a(\'n\') return \'\'.join(v) def generate_event_calendar(year, month, events, firstweekday=0): cal = EventHTMLCalendar(events, firstweekday) return cal.formatmonth(year, month)"},{"question":"**Objective**: Use the `shutil` module to perform a series of file operations that involve copying, moving, and archiving files and directories. Question: You are given a directory structure as follows: ``` project/ â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ file1.txt â”‚ â”œâ”€â”€ file2.txt â”‚ â””â”€â”€ old/ â”‚ â””â”€â”€ to_be_archived.txt â”œâ”€â”€ bin/ â””â”€â”€ archive/ ``` 1. **Task 1**: Implement a function `copy_files` that takes two arguments: `source_dir` and `dest_dir`. The function should: - Copy all the files from `source_dir` to `dest_dir`. - Preserve the file metadata (timestamps, permissions, etc.). - Ensure that if destination files already exist, they are replaced. 2. **Task 2**: Implement a function `archive_old_files` that takes two arguments: `source_dir` and `dest_dir`. The function should: - Create a tar.gz archive containing all files in the `source_dir` and save it to `dest_dir`. - The archive name should be `archived_files.tar.gz`. - Ensure that the original files are preserved. 3. **Task 3**: Implement a function `move_files_to_bin` that takes no arguments. The function should: - Move all files from the `src/` directory to the `bin/` directory within the `project/` structure. - Ensure that the move is atomic and handles cross-filesystem moves correctly. Constraints: - You should handle any potential exceptions that might occur due to file operations (e.g., file not found, permission errors). - Assume that the current working directory is the `project/` directory, and the relative paths are based on this. Example: ```python import shutil import os def copy_files(source_dir, dest_dir): # Your implementation here pass def archive_old_files(source_dir, dest_dir): # Your implementation here pass def move_files_to_bin(): # Your implementation here pass # Example usage: copy_files(\'src\', \'archive\') archive_old_files(\'src/old\', \'archive\') move_files_to_bin() ``` **Explanation**: - The `copy_files` function should use `shutil.copy2` to copy files from `src` to `archive`, preserving metadata. - The `archive_old_files` function should use `shutil.make_archive` to create a tar.gz archive of `src/old` in the `archive` directory. - The `move_files_to_bin` function should use `shutil.move` to move files from `src` to `bin`. You need to implement the three functions according to the specifications and handle any exceptions gracefully.","solution":"import shutil import os def copy_files(source_dir, dest_dir): Copy all files from source_dir to dest_dir, preserving metadata and replacing existing files. Parameters: - source_dir (str): The source directory path. - dest_dir (str): The destination directory path. try: # Ensure the destination directory exists if not os.path.exists(dest_dir): os.makedirs(dest_dir) # Copy each file from source to destination for item in os.listdir(source_dir): source_item = os.path.join(source_dir, item) dest_item = os.path.join(dest_dir, item) if os.path.isfile(source_item): shutil.copy2(source_item, dest_item) except Exception as e: print(f\\"Error copying files: {e}\\") def archive_old_files(source_dir, dest_dir): Create a tar.gz archive of all files in source_dir and save it to dest_dir. Parameters: - source_dir (str): The source directory path. - dest_dir (str): The destination directory path. try: # Ensure the destination directory exists if not os.path.exists(dest_dir): os.makedirs(dest_dir) # Create the archive archive_name = os.path.join(dest_dir, \'archived_files\') shutil.make_archive(archive_name, \'gztar\', source_dir) except Exception as e: print(f\\"Error archiving files: {e}\\") def move_files_to_bin(): Move all files from the \'src/\' directory to the \'bin/\' directory within the \'project/\' structure. try: source_dir = \'src\' dest_dir = \'bin\' # Ensure the destination directory exists if not os.path.exists(dest_dir): os.makedirs(dest_dir) # Move each file from source to destination for item in os.listdir(source_dir): source_item = os.path.join(source_dir, item) dest_item = os.path.join(dest_dir, item) if os.path.isfile(source_item): shutil.move(source_item, dest_item) except Exception as e: print(f\\"Error moving files: {e}\\")"},{"question":"Customizing Seaborn Plots and Legends Objective: Your task is to utilize the seaborn library to create a customized plot for the penguins dataset. You will need to demonstrate your understanding of seaborn plotting functions and the `move_legend` function to adjust the legend\'s position and appearance. Task: 1. Load the penguins dataset using `seaborn.load_dataset()`. 2. Create a histogram plot of the `bill_length_mm` with hue set to `species`. 3. Move the legend to the \\"upper right\\" corner, but place it outside the plot area. 4. Customize the legend such that: - It has no title. - It is displayed in a single row. - The frame (border) around the legend is not shown. Input: - None (you will operate on the internal data and settings). Output: - A matplotlib plot displayed with the legend customized as specified. Constraints: - You must use seaborn and matplotlib for plotting. - Ensure that your code follows best practices in terms of readability and efficiency. Sample Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the histogram plot ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Move and customize the legend sns.move_legend( ax, \\"upper right\\", bbox_to_anchor=(1.05, 1), title=None, frameon=False, ncol=1 ) # Display the plot plt.show() ``` Notes: - The provided sample code demonstrates the basic structure of your implementation. You need to complete this task by following the outlined steps and adhering to the customization requirements for the legend. Good luck with your implementation!","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_bill_length(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the histogram plot ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Customize and move the legend ax.legend(loc=\'upper right\', bbox_to_anchor=(1.05, 1), title=None, frameon=False, ncol=1) # Display the plot plt.show()"},{"question":"**Problem Statement** You are tasked with analyzing a dataset and visualizing the distribution and central tendency of categorical data using the Seaborn library. For this task, you will be using the \\"tips\\" dataset, which is available directly in Seaborn. Your goal is to create multiple visualizations, customize them, and generate insights based on these visualizations. **Dataset** The \\"tips\\" dataset contains information about the tips made by customers in a restaurant. It includes the following columns: - `total_bill`: Total bill amount (numerical). - `tip`: Tip amount (numerical). - `sex`: Gender of the person paying the bill (categorical: \\"Male\\", \\"Female\\"). - `smoker`: Whether the person is a smoker (categorical: \\"Yes\\", \\"No\\"). - `day`: Day of the week (categorical: \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"). - `time`: Time of day (categorical: \\"Lunch\\", \\"Dinner\\"). - `size`: Number of people at the table (numerical). **Tasks** 1. **Task 1: Categorical Scatter Plots** - Create a categorical scatter plot (`stripplot`) showing the relationship between `day` and `total_bill`. Set the `jitter` parameter to `True`. - Enhance the plot by using the `hue` parameter to differentiate between smokers and non-smokers. 2. **Task 2: Distribution Plots** - Create a box plot (`boxplot`) showing the distribution of `total_bill` across different days (`day`). Use the `hue` parameter to differentiate based on `time` (Lunch/Dinner). - Create a violin plot (`violinplot`) showing the same information as above. Adjust the `bw_adjust` parameter to 0.5 and use the `split` parameter to `True`. 3. **Task 3: Estimate Plots** - Create a bar plot (`barplot`) showing the average `total_bill` for each day. Use the `hue` parameter to differentiate by `sex`. - Create a point plot (`pointplot`) showing the same information as above. Change the markers to `\'^\'` for males and `\'o\'` for females, and use different line styles for each. 4. **Task 4: Combining Plots** - Create a combination of a violin plot and a swarm plot to show the distribution of `total_bill` for each `day` while showing individual data points. **Submission** Submit a Jupyter Notebook file (.ipynb) containing: - The code used for each task. - The resulting plots. - Brief insights or observations that can be made from each plot. **Grading Criteria** - Correct usage of Seaborn functions. - Proper customization of plots as specified. - Clarity and quality of the visualizations. - Insightful observations based on the visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\'tips\') # Task 1: Categorical Scatter Plots plt.figure(figsize=(10, 6)) sns.stripplot(x=\'day\', y=\'total_bill\', data=tips, jitter=True, hue=\'smoker\', palette=\'Set2\') plt.title(\'Scatter Plot of Total Bill by Day and Smoker Status\') plt.show() # Task 2: Distribution Plots plt.figure(figsize=(10, 6)) sns.boxplot(x=\'day\', y=\'total_bill\', data=tips, hue=\'time\', palette=\'Set2\') plt.title(\'Box Plot of Total Bill by Day and Time\') plt.show() plt.figure(figsize=(10, 6)) sns.violinplot(x=\'day\', y=\'total_bill\', data=tips, hue=\'time\', bw=0.5, split=True, palette=\'Set2\') plt.title(\'Violin Plot of Total Bill by Day and Time\') plt.show() # Task 3: Estimate Plots plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, hue=\'sex\', palette=\'Set2\') plt.title(\'Bar Plot of Average Total Bill by Day and Sex\') plt.show() plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'total_bill\', hue=\'sex\', data=tips, markers=[\'^\', \'o\'], linestyles=[\'-\', \'--\'], palette=\'Set2\') plt.title(\'Point Plot of Average Total Bill by Day and Sex\') plt.show() # Task 4: Combining Plots plt.figure(figsize=(10, 6)) sns.violinplot(x=\'day\', y=\'total_bill\', data=tips, inner=None, palette=\'Set2\') sns.swarmplot(x=\'day\', y=\'total_bill\', data=tips, color=\'k\', alpha=0.6) plt.title(\'Violin Plot with Swarm Plot of Total Bill by Day\') plt.show()"},{"question":"**Objective:** You are tasked with developing a solution to predict a target dataset `Y` from an input dataset `X` using `PLSRegression` from the scikit-learn cross-decomposition module. Your implementation should demonstrate your understanding of the Partial Least Squares method and its application using scikit-learn. **Problem Description:** 1. Implement a function `pls_regression_prediction` that performs the following steps: - Loads the provided input dataset `X` and target dataset `Y`. - Applies `PLSRegression` to compute the relationship between `X` and `Y`. - Transforms the input dataset `X` to obtain the PLS components. - Uses the trained PLS model to predict the target variables `Y_pred`. - Returns the predicted target matrix `Y_pred`. **Function Signature:** ```python def pls_regression_prediction(X: np.ndarray, Y: np.ndarray, n_components: int) -> np.ndarray: pass ``` **Input:** - `X`: A numpy array of shape `(n_samples, n_features)`, representing the input dataset. - `Y`: A numpy array of shape `(n_samples, n_targets)`, representing the target dataset. - `n_components`: An integer representing the number of components to use in the PLS model. **Output:** - `Y_pred`: A numpy array of shape `(n_samples, n_targets)`, representing the predicted target dataset. **Constraints:** - Use the `PLSRegression` class from `sklearn.cross_decomposition`. - Ensure that your code works efficiently for large datasets with potentially high dimensions. **Example Usage:** ```python import numpy as np from sklearn.cross_decomposition import PLSRegression # Example input data X = np.array([[0.1, 0.2], [0.2, 0.1], [0.4, 0.6]]) Y = np.array([[1, 2], [2, 3], [3, 4]]) # Example function call Y_pred = pls_regression_prediction(X, Y, n_components=2) print(Y_pred) ``` **Notes:** - You may assume the input datasets `X` and `Y` are already preprocessed (centered and scaled if necessary). - Remember to include error handling for edge cases, such as mismatched dimensions between `X` and `Y`.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression_prediction(X: np.ndarray, Y: np.ndarray, n_components: int) -> np.ndarray: Performs PLS regression on input dataset X and target dataset Y, and returns the predicted target matrix Y_pred. Parameters: X (np.ndarray): Input data of shape (n_samples, n_features) Y (np.ndarray): Target data of shape (n_samples, n_targets) n_components (int): Number of PLS components to use Returns: np.ndarray: Predicted target matrix Y_pred of shape (n_samples, n_targets) # Ensure the dimensions of X and Y are compatible if X.shape[0] != Y.shape[0]: raise ValueError(\\"Number of samples in X and Y must be the same.\\") # Initialize and fit the PLS model pls = PLSRegression(n_components=n_components) pls.fit(X, Y) # Generate predictions Y_pred = pls.predict(X) return Y_pred"},{"question":"# Python Coding Assessment Question: Resource Management Objective Implement a Python function to monitor and control the CPU and memory usage of the current process. Utilize the `resource` module to: 1. Retrieve the current limits for CPU usage time and maximum resident set size (memory). 2. Set new soft limits for CPU usage and memory for the current process. 3. Report the resource usage of the current process before and after the limits are set. Function Signature ```python def manage_resources(cpu_time_limit: int, memory_limit: int) -> dict: # Implementation here ``` Input - `cpu_time_limit`: An integer representing the new soft limit for CPU time (in seconds). - `memory_limit`: An integer representing the new soft limit for memory usage (in bytes). Output - A dictionary with the following structure: ```python { \\"initial_limits\\": { \\"cpu\\": (soft_cpu_limit, hard_cpu_limit), \\"memory\\": (soft_memory_limit, hard_memory_limit) }, \\"new_limits\\": { \\"cpu\\": (new_soft_cpu_limit, hard_cpu_limit), \\"memory\\": (new_soft_memory_limit, hard_memory_limit) }, \\"usage_before\\": { \\"time_in_user_mode\\": float, \\"time_in_system_mode\\": float, \\"maximum_resident_set_size\\": int, ... }, \\"usage_after\\": { \\"time_in_user_mode\\": float, \\"time_in_system_mode\\": float, \\"maximum_resident_set_size\\": int, ... } } ``` Constraints - The new soft limit for CPU time (`cpu_time_limit`) should not exceed the existing hard limit. - The new soft limit for memory (`memory_limit`) should not exceed the existing hard limit. Example ```python result = manage_resources(10, 1024*1024*256) print(result) # Expected output (the values may vary based on the system): { \\"initial_limits\\": { \\"cpu\\": (600, 1800), \\"memory\\": (262144000, 524288000) }, \\"new_limits\\": { \\"cpu\\": (10, 1800), \\"memory\\": (268435456, 524288000) }, \\"usage_before\\": { \\"time_in_user_mode\\": 0.13, \\"time_in_system_mode\\": 0.02, \\"maximum_resident_set_size\\": 23456, ... }, \\"usage_after\\": { \\"time_in_user_mode\\": 0.14, \\"time_in_system_mode\\": 0.03, \\"maximum_resident_set_size\\": 24000, ... } } ``` Notes - Ensure that the function handles cases where setting a new limit exceeds the hard limit by raising appropriate exceptions. - The exact structure of the dictionary should include all the fields provided by `getrusage()`. Good luck and happy coding!","solution":"import resource def manage_resources(cpu_time_limit: int, memory_limit: int) -> dict: # Get initial limits initial_cpu_limit = resource.getrlimit(resource.RLIMIT_CPU) initial_memory_limit = resource.getrlimit(resource.RLIMIT_AS) # Report resource usage before setting new limits usage_before = resource.getrusage(resource.RUSAGE_SELF) # Set new limits new_soft_cpu_limit = min(cpu_time_limit, initial_cpu_limit[1]) new_soft_memory_limit = min(memory_limit, initial_memory_limit[1]) resource.setrlimit(resource.RLIMIT_CPU, (new_soft_cpu_limit, initial_cpu_limit[1])) resource.setrlimit(resource.RLIMIT_AS, (new_soft_memory_limit, initial_memory_limit[1])) # Report resource usage after setting new limits usage_after = resource.getrusage(resource.RUSAGE_SELF) return { \\"initial_limits\\": { \\"cpu\\": initial_cpu_limit, \\"memory\\": initial_memory_limit }, \\"new_limits\\": { \\"cpu\\": (new_soft_cpu_limit, initial_cpu_limit[1]), \\"memory\\": (new_soft_memory_limit, initial_memory_limit[1]) }, \\"usage_before\\": { \\"time_in_user_mode\\": usage_before.ru_utime, \\"time_in_system_mode\\": usage_before.ru_stime, \\"maximum_resident_set_size\\": usage_before.ru_maxrss, \\"shared_memory_size\\": usage_before.ru_ixrss, \\"unshared_data_size\\": usage_before.ru_idrss, \\"unshared_stack_size\\": usage_before.ru_isrss, \\"page_reclaims\\": usage_before.ru_minflt, \\"page_faults\\": usage_before.ru_majflt, \\"swaps\\": usage_before.ru_nswap, \\"block_input_operations\\": usage_before.ru_inblock, \\"block_output_operations\\": usage_before.ru_oublock, \\"ipc_messages_sent\\": usage_before.ru_msgsnd, \\"ipc_messages_received\\": usage_before.ru_msgrcv, \\"signals_received\\": usage_before.ru_nsignals, \\"voluntary_context_switches\\": usage_before.ru_nvcsw, \\"involuntary_context_switches\\": usage_before.ru_nivcsw }, \\"usage_after\\": { \\"time_in_user_mode\\": usage_after.ru_utime, \\"time_in_system_mode\\": usage_after.ru_stime, \\"maximum_resident_set_size\\": usage_after.ru_maxrss, \\"shared_memory_size\\": usage_after.ru_ixrss, \\"unshared_data_size\\": usage_after.ru_idrss, \\"unshared_stack_size\\": usage_after.ru_isrss, \\"page_reclaims\\": usage_after.ru_minflt, \\"page_faults\\": usage_after.ru_majflt, \\"swaps\\": usage_after.ru_nswap, \\"block_input_operations\\": usage_after.ru_inblock, \\"block_output_operations\\": usage_after.ru_oublock, \\"ipc_messages_sent\\": usage_after.ru_msgsnd, \\"ipc_messages_received\\": usage_after.ru_msgrcv, \\"signals_received\\": usage_after.ru_nsignals, \\"voluntary_context_switches\\": usage_after.ru_nvcsw, \\"involuntary_context_switches\\": usage_after.ru_nivcsw } }"},{"question":"**Question: Implement a Robust and Extendable Inventory System** Create a Python program that models an inventory system using the `dataclasses` module. The system should manage a collection of items, track stock levels, and handle various inventory operations. **Requirements:** 1. Define a dataclass `InventoryItem` with the following fields: * `name`: `str` - The name of the item. * `unit_price`: `float` - The price per unit of the item. * `quantity_on_hand`: `int` - The current stock level of the item (default value is `0`). 2. Implement a method `total_cost` in the `InventoryItem` class: * This method should calculate and return the total cost of the available stock for this item (i.e., `unit_price * quantity_on_hand`). 3. Define another dataclass `Inventory` with the following field: * `items`: `list[InventoryItem]` - A list to store all inventory items. 4. Implement methods in the `Inventory` class to perform the following operations: * `add_item(item: InventoryItem)`: Adds a new item to the inventory. If an item with the same name already exists, update the existing item by increasing its `quantity_on_hand`. * `remove_item(name: str, quantity: int)`: Removes a specified quantity of an item from the inventory. If the quantity to remove is greater than the available stock, raise a `ValueError`. * `get_total_inventory_value()`: Returns the total value of all items in the inventory (sum of `total_cost` for all items). * `print_inventory()`: Prints a summary of all items in the inventory, showing each item\'s name, unit price, quantity on hand, and total cost. **Constraints:** * All input values should be validated to ensure they are of the correct type and within reasonable ranges (e.g., `unit_price` and `quantity_on_hand` should not be negative). * The solution should be efficient in terms of performance (aim for O(1) complexity for add and remove operations on average). **Example Usage:** ```python from dataclasses import dataclass, field @dataclass class InventoryItem: name: str unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass class Inventory: items: list[InventoryItem] = field(default_factory=list) def add_item(self, item: InventoryItem): for existing_item in self.items: if existing_item.name == item.name: existing_item.quantity_on_hand += item.quantity_on_hand return self.items.append(item) def remove_item(self, name: str, quantity: int): for item in self.items: if item.name == name: if item.quantity_on_hand < quantity: raise ValueError(f\\"Not enough stock of {name} to remove {quantity} units\\") item.quantity_on_hand -= quantity return raise ValueError(f\\"Item named {name} not found in inventory\\") def get_total_inventory_value(self) -> float: return sum(item.total_cost() for item in self.items) def print_inventory(self): for item in self.items: print(f\\"Item: {item.name}, Unit Price: {item.unit_price}, Quantity: {item.quantity_on_hand}, Total Cost: {item.total_cost()}\\") # Example usage: inventory = Inventory() inventory.add_item(InventoryItem(name=\\"Widget\\", unit_price=2.50, quantity_on_hand=100)) inventory.add_item(InventoryItem(name=\\"Gadget\\", unit_price=5.00, quantity_on_hand=200)) inventory.remove_item(\\"Widget\\", 50) inventory.print_inventory() print(\\"Total Inventory Value:\\", inventory.get_total_inventory_value()) ``` Implement the classes and methods as described. Ensure your implementation handles edge cases and provides meaningful error messages where necessary.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class InventoryItem: name: str unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass class Inventory: items: List[InventoryItem] = field(default_factory=list) def add_item(self, item: InventoryItem): for existing_item in self.items: if existing_item.name == item.name: existing_item.quantity_on_hand += item.quantity_on_hand return self.items.append(item) def remove_item(self, name: str, quantity: int): for item in self.items: if item.name == name: if item.quantity_on_hand < quantity: raise ValueError(f\\"Not enough stock of {name} to remove {quantity} units\\") item.quantity_on_hand -= quantity return raise ValueError(f\\"Item named {name} not found in inventory\\") def get_total_inventory_value(self) -> float: return sum(item.total_cost() for item in self.items) def print_inventory(self): for item in self.items: print(f\\"Item: {item.name}, Unit Price: {item.unit_price}, Quantity: {item.quantity_on_hand}, Total Cost: {item.total_cost()}\\") # Example usage: # inventory = Inventory() # inventory.add_item(InventoryItem(name=\\"Widget\\", unit_price=2.50, quantity_on_hand=100)) # inventory.add_item(InventoryItem(name=\\"Gadget\\", unit_price=5.00, quantity_on_hand=200)) # inventory.remove_item(\\"Widget\\", 50) # inventory.print_inventory() # print(\\"Total Inventory Value:\\", inventory.get_total_inventory_value())"},{"question":"Objective Your task is to write a program that compresses a specified directory into a ZIP file using Python\'s `zipfile` module. Requirements 1. The function should compress all files in the specified directory, including files in subdirectories, preserving the directory structure. 2. Ensure that no file larger than 10 MB is included in the ZIP file. 3. Handle any exceptions that arise during the compression process, logging an appropriate message. 4. The function should return the path to the created ZIP file. Function Signature ```python def compress_directory(directory_path: str, zip_path: str) -> str: ``` Input - `directory_path` (str): The path to the directory that needs to be compressed. - `zip_path` (str): The path where the created ZIP file should be saved. Output - Returns the path to the created ZIP file (str). Constraints - Only include files that are smaller than 10 MB. - Log an error message (using `print()`) if an exception occurs, but continue attempting to compress the remaining files. - You may assume file paths and directory paths in the input are valid. Example ```python import os # Assume the following structure exists: # test_dir/ # â”œâ”€â”€ file1.txt (5 MB) # â”œâ”€â”€ file2.log (15 MB) # â””â”€â”€ sub_dir/ # â””â”€â”€ file3.bin (3 MB) # compress_directory(\\"test_dir\\", \\"output.zip\\") # The ZIP file \'output.zip\' should include \'file1.txt\' and \'sub_dir/file3.bin\', but not \'file2.log\'. assert os.path.exists(\\"output.zip\\") ``` Implementation Notes - Use the `os` and `zipfile` modules to navigate directories and create ZIP files. - Ensure you respect the 10 MB file size constraint.","solution":"import os import zipfile def compress_directory(directory_path: str, zip_path: str) -> str: Compresses a specified directory into a ZIP file with files smaller than 10 MB. Parameters: directory_path (str): The path to the directory that needs to be compressed. zip_path (str): The path where the created ZIP file should be saved. Returns: str: The path to the created ZIP file. try: with zipfile.ZipFile(zip_path, \'w\', zipfile.ZIP_DEFLATED) as zip_file: for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) if os.stat(file_path).st_size <= 10 * 1024 * 1024: # 10 MB size limit zip_file.write(file_path, os.path.relpath(file_path, directory_path)) else: print(f\\"Skipping {file_path}: File size exceeds 10 MB.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return zip_path"},{"question":"**Objective:** Implement a PyTorch distributed training setup using the `torch.distributed.fsdp.FullyShardedDataParallel` class. Utilize mixed precision and CPU offloading to demonstrate advanced functionalities provided by the `torch.distributed.fsdp` module. **Problem Statement:** You are given a neural network model implemented in PyTorch. Your task is to modify the training script to use Fully Sharded Data Parallel (FSDP) for distributed training. Additionally, you should enable mixed precision training and offload parts of the model to the CPU to save GPU memory. **Details:** 1. **Neural Network Model**: Use a simple ResNet model for this task. You can use the pre-defined `torchvision.models.resnet50` for convenience. 2. **Dataset**: Use the CIFAR-10 dataset for training. You can use `torchvision.datasets.CIFAR10` for loading the data. 3. **Distributed Training**: Implement the distributed training setup using FSDP. 4. **Mixed Precision**: Enable mixed precision training using `torch.distributed.fsdp.MixedPrecision`. 5. **CPU Offload**: Offload parts of the model to CPU using `torch.distributed.fsdp.CPUOffload`. **Input and Output Formats:** - **Input**: No specific input format. The training script should set up the distributed environment and start the training process. - **Output**: The script should print the training loss and validation accuracy after each epoch. **Constraints:** 1. Use the `torch.distributed` package to set up the distributed environment. 2. Your code should be able to run on multiple GPUs if available. Handle the necessary device placements. 3. Implement proper state dictionary configurations for saving and loading models. 4. The training should run for a specified number of epochs (e.g., 10 epochs). **Performance Requirements:** - Ensure efficient use of memory and computation resources. - Validate performance improvements with mixed precision and CPU offloading compared to a naive implementation. **Skeleton Code:** ```python import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torchvision import models, datasets, transforms from torch.optim import SGD from torch.utils.data import DataLoader, DistributedSampler from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import MixedPrecision, CPUOffload def train(rank, world_size): dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) # Define model model = models.resnet50() model = model.to(rank) # Enable FSDP, mixed precision, and CPU offload mixed_precision_policy = MixedPrecision() cpu_offload_policy = CPUOffload(offload_params=True) model = FSDP(model, mixed_precision=mixed_precision_policy, cpu_offload=cpu_offload_policy) # Load data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) train_sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler) # Define optimizer optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # Training loop epochs = 10 for epoch in range(epochs): model.train() for batch in train_loader: inputs, labels = batch inputs, labels = inputs.to(rank), labels.to(rank) optimizer.zero_grad() outputs = model(inputs) loss = torch.nn.functional.cross_entropy(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) ``` Write the missing parts in the skeleton code provided and test the training script for CIFAR-10 dataset.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp from torchvision.models import resnet50 from torchvision import datasets, transforms from torch.optim import SGD from torch.utils.data import DataLoader, DistributedSampler from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import MixedPrecision, CPUOffload def setup(): dist.init_process_group(backend=\\"nccl\\") def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup() torch.cuda.set_device(rank) # Define model model = resnet50() model = model.to(rank) # Enable FSDP, mixed precision, and CPU offload mixed_precision_policy = MixedPrecision() cpu_offload_policy = CPUOffload(offload_params=True) model = FSDP(model, mixed_precision=mixed_precision_policy, cpu_offload=cpu_offload_policy) # Load data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) train_sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler) # Define optimizer optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # Training loop epochs = 10 for epoch in range(epochs): model.train() for batch in train_loader: inputs, labels = batch inputs, labels = inputs.to(rank), labels.to(rank) optimizer.zero_grad() outputs = model(inputs) loss = torch.nn.functional.cross_entropy(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") cleanup() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"# Multi-threaded Task Processing with Priority Queue Objective Your task is to implement a multi-threaded processing system using `queue.PriorityQueue`. This system will manage tasks with different priorities and ensure the tasks with the highest priority (lowest numeric value) are processed first. Task Details 1. Implement a class `TaskProcessor` that uses `queue.PriorityQueue` to manage tasks. 2. Each task is represented as a tuple `(priority, description)`, where `priority` is an integer, and `description` is a string description of the task. 3. The `TaskProcessor` class should allow: - Adding tasks to the queue. - Processing tasks using multiple worker threads. 4. Ensure that tasks are processed by the workers in order of priority. 5. Provide methods to add tasks, start processing, and check if all tasks are completed. Requirements - Implement a class `TaskProcessor` with the following methods: - `__init__(self, num_workers: int)`: Initialize the `TaskProcessor` with a specified number of worker threads. - `add_task(self, priority: int, description: str)`: Add a task to the priority queue. - `start_processing(self)`: Start the worker threads to process tasks. - `all_tasks_done(self) -> bool`: Return `True` if all tasks have been processed, `False` otherwise. - The processing of tasks should involve printing the task description and marking the task as done. Constraints - The task processor must handle a dynamic number of tasks. - Worker threads should continuously fetch and process tasks until all tasks are done. Example Usage ```python import time # Example implementation of TaskProcessor class TaskProcessor: def __init__(self, num_workers: int): import queue, threading self.priority_queue = queue.PriorityQueue() self.num_workers = num_workers self.threads = [] self._lock = threading.Lock() self._unfinished_tasks = 0 def add_task(self, priority: int, description: str): with self._lock: self.priority_queue.put((priority, description)) self._unfinished_tasks += 1 def _process_task(self): while True: priority, task = self.priority_queue.get() print(f\'Processing task: {task}\') time.sleep(1) # Simulating task processing time self.priority_queue.task_done() with self._lock: self._unfinished_tasks -= 1 def start_processing(self): for _ in range(self.num_workers): thread = threading.Thread(target=self._process_task, daemon=True) self.threads.append(thread) thread.start() def all_tasks_done(self) -> bool: with self._lock: return self._unfinished_tasks == 0 # Example usage if __name__ == \\"__main__\\": processor = TaskProcessor(num_workers=3) tasks = [(3, \'low priority task\'), (1, \'high priority task\'), (2, \'medium priority task\')] for priority, description in tasks: processor.add_task(priority, description) processor.start_processing() while not processor.all_tasks_done(): time.sleep(0.1) print(\'All tasks have been processed.\') ```","solution":"import queue import threading import time class TaskProcessor: def __init__(self, num_workers: int): self.priority_queue = queue.PriorityQueue() self.num_workers = num_workers self.threads = [] self._lock = threading.Lock() self._unfinished_tasks = 0 def add_task(self, priority: int, description: str): with self._lock: self.priority_queue.put((priority, description)) self._unfinished_tasks += 1 def _process_task(self): while True: priority, task = self.priority_queue.get() print(f\'Processing task: {task}\') time.sleep(1) # Simulating task processing time self.priority_queue.task_done() with self._lock: self._unfinished_tasks -= 1 def start_processing(self): for _ in range(self.num_workers): thread = threading.Thread(target=self._process_task, daemon=True) self.threads.append(thread) thread.start() def all_tasks_done(self) -> bool: with self._lock: return self._unfinished_tasks == 0"},{"question":"In this assessment, you will be working with MIME type handling using the `mailcap` module in Python 3.10. Your task is to implement a function that retrieves and executes the command line for a particular MIME type from the mailcap files. This function should also handle the case where no suitable command is found. # Function Signature ```python def process_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list[str] = []) -> str: pass ``` # Input * `mime_type` (str): The MIME type to lookup in the mailcap file (e.g., \'video/mpeg\'). * `filename` (str): The filename to be substituted for \\"%s\\" in the command line. * `key` (str): The activity type to perform (default is \'view\'). * `plist` (List[str]): A list of named parameters in the format \\"name=value\\" (default is an empty list). # Output * Returns a string that contains the command to be executed, or a suitable message if no matching MIME type is found. # Constraints 1. The function should use the `mailcap` module to retrieve and match the MIME type. 2. Handle any security warnings raised due to disallowed characters by returning the message \\"Security warning: invalid character in input.\\" 3. If no match is found for the given MIME type, return the message \\"No matching MIME type found.\\" 4. Ensure that the `mailcap` module is used according to the documentation provided. # Example Usage ```python import mailcap def process_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list[str] = []) -> str: try: # Retrieve capabilities from mailcap files capabilities = mailcap.getcaps() # Find a match for the given MIME type and other parameters match, entry = mailcap.findmatch(capabilities, mime_type, key=key, filename=filename, plist=plist) # Handle cases where no match is found if match is None and entry is None: return \\"No matching MIME type found.\\" return match except Warning as e: return \\"Security warning: invalid character in input.\\" # Test cases print(process_mime_type(\'video/mpeg\', \'tmp1223\')) # Output: \\"xmpeg tmp1223\\" print(process_mime_type(\'text/html\', \'test.html\')) # Output based on mailcap files present print(process_mime_type(\'application/unknown\', \'file.bin\')) # Output: \\"No matching MIME type found.\\" print(process_mime_type(\'image/png\', \'invalid;file.png\')) # Output: \\"Security warning: invalid character in input.\\" ``` **Note:** This problem assumes that students have a functional understanding of handling MIME types and basic system communication using Python.","solution":"import mailcap def process_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list[str] = []) -> str: try: # Ensure that no disallowed characters are in the filename for security reasons if any(char in filename for char in \\";|&<>\\"): return \\"Security warning: invalid character in input.\\" # Retrieve capabilities from mailcap files capabilities = mailcap.getcaps() # Find a match for the given MIME type and other parameters match, entry = mailcap.findmatch(capabilities, mime_type, key=key, filename=filename, plist=plist) # Handle cases where no match is found if match is None: return \\"No matching MIME type found.\\" return match except Warning: return \\"Security warning: invalid character in input.\\""},{"question":"**Quoted-Printable Encoding and Decoding** In this assessment, you are required to implement a Python program that reads a text file, encodes its contents using quoted-printable encoding, stores the encoded data in a new file, then reads the encoded file, decodes its contents, and stores the decoded data back into another file. This will test your ability to manipulate files and use the quopri module effectively. # Requirements: 1. Your program should read the input text file `input.txt` which contains plain text. 2. It should encode this text using quoted-printable encoding and save the encoded data to `encoded.txt`. 3. Then, read `encoded.txt` and decode its contents, saving the result in `decoded.txt`. 4. Ensure that the content of `decoded.txt` matches the original content of `input.txt`. # Implementation Details: - Use the `quopri` module functions `quopri.encode` and `quopri.decode` to handle encoding and decoding. - Make sure to deal with binary files appropriately as required by the `quopri` functions. # Constraints: - Ensure that the encoded data in `encoded.txt` only contains valid quoted-printable characters. - Do not modify the content of `input.txt` or `decoded.txt` directly. - Assume that the text files will only contain ASCII characters for simplicity. # Input: - `input.txt`: A text file with plain text data. # Output: - `encoded.txt`: A text file containing the quoted-printable encoded data. - `decoded.txt`: A text file containing the decoded data, which should match `input.txt`. # Example: Suppose `input.txt` contains: ``` Hello World! This is an example of Quoted-Printable encoding. ``` After encoding and decoding, `decoded.txt` should contain: ``` Hello World! This is an example of Quoted-Printable encoding. ``` # Solution Skeleton: ```python import quopri def encode_file(input_file, output_file): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.encode(infile, outfile, quotetabs=True) def decode_file(input_file, output_file): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.decode(infile, outfile) # Perform the encoding encode_file(\'input.txt\', \'encoded.txt\') # Perform the decoding decode_file(\'encoded.txt\', \'decoded.txt\') # Optionally, you can add a function to verify the content def verify_files(file1, file2): with open(file1, \'r\') as f1, open(file2, \'r\') as f2: return f1.read() == f2.read() # Testing the result assert verify_files(\'input.txt\', \'decoded.txt\'), \\"Files do not match!\\" print(\\"Encoding and decoding process completed successfully!\\") ``` # Note: - Make sure to test your function with different types of inputs to ensure its correctness. - Focus on handling file I/O operations correctly and using the `quopri` module effectively.","solution":"import quopri def encode_file(input_file, output_file): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.encode(infile, outfile, quotetabs=True) def decode_file(input_file, output_file): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.decode(infile, outfile) def verify_files(file1, file2): with open(file1, \'r\') as f1, open(file2, \'r\') as f2: return f1.read() == f2.read()"},{"question":"# Coding Challenge **Objective**: You are to create a networked chat application using Python that demonstrates your understanding of the `asyncio`, `socket`, and `ssl` modules. The application should have both server-side and client-side components and should securely communicate using SSL. Requirements 1. **Server-side:** - Create an asynchronous server using `asyncio` that listens for incoming connections. - Use `socket` and `ssl` to establish secure connections with clients. - The server should handle multiple clients concurrently, broadcasting messages received from one client to all other connected clients. - Include appropriate error handling to manage client disconnections or other unforeseen issues. 2. **Client-side:** - Create an asynchronous client using `asyncio` that connects to the server using `ssl` for secure communication. - The client should enable users to send messages to the server which will then broadcast it to other clients. - Implement a mechanism to handle and display incoming messages from the server in real-time. Function Signatures ```python import asyncio import socket import ssl async def start_server(host: str, port: int, ssl_context: ssl.SSLContext): Starts an asynchronous chat server that accepts multiple connections securely. Parameters: host (str): Address to bind the server. port (int): Port to bind the server. ssl_context (ssl.SSLContext): Configured SSLContext to use for secure connections. async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): Handles the communication between the client and the server. Parameters: reader (asyncio.StreamReader): Stream reader to read data. writer (asyncio.StreamWriter): Stream writer to send data. async def connect_to_server(host: str, port: int, ssl_context: ssl.SSLContext): Connects to the chat server securely and handles sending and receiving messages. Parameters: host (str): Address of the server. port (int): Port of the server. ssl_context (ssl.SSLContext): Configured SSLContext to use for secure connections. def create_ssl_context(certfile: str, keyfile: str) -> ssl.SSLContext: Creates an SSL context for secure communication. Parameters: certfile (str): Path to the server\'s certificate file. keyfile (str): Path to the server\'s key file. Returns: ssl.SSLContext: The configured SSL context. ``` Constraints - The application must use the SSL/TLS protocol for secure communication. - The solution must be implemented using the `asyncio` module for handling asynchronous operations. - Proper error handling and cleanup of resources should be demonstrated. Performance Requirements - The server should efficiently handle multiple client connections concurrently without significant latency. - Consider the performance implications of broadcasting messages to all connected clients. Notes - You will need to generate SSL certificates for testing your application locally. Self-signed certificates are sufficient for this purpose. - Include any additional helper functions you may need within your solution. Example To be provided later for illustrating the setup and interaction between clients and the server.","solution":"import asyncio import socket import ssl clients = [] async def start_server(host: str, port: int, ssl_context: ssl.SSLContext): server = await asyncio.start_server(handle_client, host, port, ssl=ssl_context) async with server: await server.serve_forever() async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') print(f\\"New connection from {addr}\\") clients.append(writer) try: while True: data = await reader.read(100) message = data.decode() if message == \'\': break print(f\\"Received {message} from {addr}\\") broadcast_message = f\\"{addr} says: {message}\\" for client in clients: try: client.write(broadcast_message.encode()) await client.drain() except: clients.remove(client) except asyncio.CancelledError: pass finally: print(f\\"Connection closed from {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def connect_to_server(host: str, port: int, ssl_context: ssl.SSLContext): reader, writer = await asyncio.open_connection(host, port, ssl=ssl_context) async def send_messages(): while True: message = await asyncio.get_running_loop().run_in_executor(None, input, \\"\\") writer.write(message.encode()) await writer.drain() async def receive_messages(): while True: data = await reader.read(100) if data == b\'\': print(\\"Connection closed\\") break print(data.decode()) await asyncio.gather(send_messages(), receive_messages()) def create_ssl_context(certfile: str, keyfile: str) -> ssl.SSLContext: ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) ssl_context.load_cert_chain(certfile, keyfile) return ssl_context # Example usage: # if __name__ == \\"__main__\\": # certfile = \\"server.crt\\" # keyfile = \\"server.key\\" # ssl_context = create_ssl_context(certfile, keyfile) # asyncio.run(start_server(\\"127.0.0.1\\", 8888, ssl_context)) # To test the client, make sure the server is running and then: # asyncio.run(connect_to_server(\\"127.0.0.1\\", 8888, ssl.create_default_context()))"},{"question":"Objective Demonstrate your understanding of scikit-learn\'s `PLSRegression` by implementing a solution that uses it for dimensionality reduction and regression on a given dataset. Problem Statement You are given a dataset with features `X` and targets `Y`. 1. **Read the data**: The dataset is provided as two separate CSV files, `X.csv` and `Y.csv`, with `X` containing the features and `Y` containing the target variables. 2. **Preprocess the data**: - Split the data into training and test sets (80% train, 20% test). - Standardize the features by removing the mean and scaling to unit variance. 3. **Apply PLSRegression**: - Use `PLSRegression` to perform dimensionality reduction with `n_components=2`. - Transform the training and test sets using the fitted model. 4. **Regression and Evaluation**: - Fit the PLS regression model to the training data. - Use the trained model to predict target values for the test dataset. - Calculate the Mean Squared Error (MSE) between the predicted and actual target values for the test set. Input - Two CSV files, `X.csv` and `Y.csv`, are provided where: - `X.csv`: Each row corresponds to an observation and each column to a feature. - `Y.csv`: Each row corresponds to the target values associated with the rows in `X.csv`. Output - A single floating-point number representing the Mean Squared Error (MSE) of the predictions on the test dataset. Constraints - Ensure reproducibility by setting a random seed (you can use any number, e.g., 42) for the train-test split. Example Assume `X.csv` and `Y.csv` look something like: `X.csv`: ``` 1.0,2.1,3.3 4.0,5.2,6.3 7.0,8.2,9.3 ... ``` `Y.csv`: ``` 10.0 20.0 30.0 ... ``` Given these files, implement the function `pls_regression_mse` as follows: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression_mse(X_file, Y_file): # Load the data X = pd.read_csv(X_file, header=None) Y = pd.read_csv(Y_file, header=None) # Split the data into training and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Apply PLSRegression pls = PLSRegression(n_components=2) pls.fit(X_train, Y_train) # Transform the data X_train_transformed = pls.transform(X_train) X_test_transformed = pls.transform(X_test) # Predict the target for the test set Y_pred = pls.predict(X_test) # Calculate the Mean Squared Error (MSE) mse = mean_squared_error(Y_test, Y_pred) return mse # Example usage print(pls_regression_mse(\'X.csv\', \'Y.csv\')) ``` Ensure your implementation has the expected format and perform checks to validate the results as shown in the example.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression_mse(X_file, Y_file): Perform PLS Regression and calculate the Mean Squared Error for the test set. Parameters: X_file (str): Path to the CSV file containing the features. Y_file (str): Path to the CSV file containing the targets. Returns: float: Mean Squared Error of the predictions on the test dataset. # Load the data X = pd.read_csv(X_file, header=None) Y = pd.read_csv(Y_file, header=None) # Split the data into training and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Apply PLSRegression pls = PLSRegression(n_components=2) pls.fit(X_train, Y_train) # Predict the target for the test set Y_pred = pls.predict(X_test) # Calculate the Mean Squared Error (MSE) mse = mean_squared_error(Y_test, Y_pred) return mse"},{"question":"Unix Syslog Logging Utility Objective Implement a Unix system logger utility using the Python `syslog` module. The utility should provide functionality to configure logging options, send log messages with different priorities, and reset the logging configuration. Requirements 1. Implement a class `SyslogUtility` with the following methods: - `__init__(self, ident=None, logoption=0, facility=syslog.LOG_USER)`: Initialize the syslog utility with optional identifier, logging options, and facility. - `log_message(self, message, priority=syslog.LOG_INFO)`: Send a log message with the specified priority. - `set_priority_mask(self, priorities)`: Configure the priority mask so that only messages with specified priorities are logged. `priorities` should be a list of priority constants. - `reset_log(self)`: Reset the syslog configuration, closing the current log and resetting settings to defaults. 2. All methods should handle and log exceptions appropriately using the syslog facility. Input and Output Formats 1. `__init__`: - Input: `ident` (string, optional), `logoption` (integer, optional), `facility` (integer, optional) - Output: None 2. `log_message`: - Input: `message` (string), `priority` (integer, optional, default `syslog.LOG_INFO`) - Output: None 3. `set_priority_mask`: - Input: `priorities` (list of integer constants) - Output: None 4. `reset_log`: - Input: None - Output: None Example Usage ```python import syslog class SyslogUtility: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) def log_message(self, message, priority=syslog.LOG_INFO): try: syslog.syslog(priority, message) except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Logging error: {e}\\") def set_priority_mask(self, priorities): try: maskpri = 0 for pri in priorities: maskpri |= syslog.LOG_MASK(pri) previous_mask = syslog.setlogmask(maskpri) syslog.syslog(syslog.LOG_INFO, f\\"Priority mask set. Previous mask: {previous_mask}\\") except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Set mask error: {e}\\") def reset_log(self): try: syslog.closelog() syslog.syslog(syslog.LOG_INFO, \\"Syslog reset to default settings.\\") syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Reset log error: {e}\\") # Example usage: logger = SyslogUtility(ident=\\"MyApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_USER) logger.log_message(\\"Application started\\") logger.set_priority_mask([syslog.LOG_ERR, syslog.LOG_WARNING, syslog.LOG_INFO]) logger.log_message(\\"This is an info message.\\") logger.reset_log() ``` Constraints - Ensure that `log_message`, `set_priority_mask`, and `reset_log` methods handle exceptions and log any errors encountered. - The utility should default to logging with `syslog.LOG_INFO` priority and `syslog.LOG_USER` facility if not specified. - The method implementations should use the `syslog` module methods efficiently and appropriately. Performance Requirements - The implementation should be optimized for minimal overhead when logging messages or changing settings. - The `set_priority_mask` method should apply the mask correctly even with multiple priority levels.","solution":"import syslog class SyslogUtility: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) def log_message(self, message, priority=syslog.LOG_INFO): try: syslog.syslog(priority, message) except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Logging error: {e}\\") def set_priority_mask(self, priorities): try: maskpri = 0 for pri in priorities: maskpri |= syslog.LOG_MASK(pri) previous_mask = syslog.setlogmask(maskpri) syslog.syslog(syslog.LOG_INFO, f\\"Priority mask set. Previous mask: {previous_mask}\\") except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Set mask error: {e}\\") def reset_log(self): try: syslog.closelog() syslog.syslog(syslog.LOG_INFO, \\"Syslog reset to default settings.\\") syslog.openlog(ident=self.ident, logoption=self.logoption, facility=self.facility) except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Reset log error: {e}\\")"},{"question":"# PyTorch CPU Device and Stream Management You are required to write a function that performs the following tasks using PyTorch\'s `torch.cpu` module: 1. **Device Count and Validation**: - Check how many CPU devices are available using `torch.cpu.device_count`. - Raise an exception if no CPU devices are available. 2. **Set Device and Retrieve Current Device**: - Set the current device to the first available CPU device (index 0) using `torch.cpu.set_device`. - Retrieve and return the current device index using `torch.cpu.current_device`. 3. **Stream Management**: - Create a new stream using `torch.cpu.stream` and set it to the current stream for device 0. - Synchronize the stream using `torch.cpu.synchronize`. Constraints: - Assume that the maximum number of CPU devices is 4. - Ensure the implemented function works within the context of multiple devices and appropriate error handling. Function Signature: ```python def manage_cpu_devices_and_streams(): Manages CPU devices and streams, performs operations as specified, and returns the current device index. Returns: -------- int The index of the current CPU device. Raises: ------- RuntimeError If no CPU devices are available. pass ``` Example: ```python # When there are available CPU devices index = manage_cpu_devices_and_streams() print(index) # Output should be 0 if executed on an environment with CPU devices. ``` The function should demonstrate your understanding of managing devices and streams using PyTorch\'s CPU functionalities.","solution":"import torch def manage_cpu_devices_and_streams(): Manages CPU devices and streams, performs operations as specified, and returns the current device index. Returns: -------- int The index of the current CPU device. Raises: ------- RuntimeError If no CPU devices are available. # Check how many CPU devices are available device_count = torch.cpu.device_count() # Raise an exception if no CPU devices are available if device_count == 0: raise RuntimeError(\\"No CPU devices are available\\") # Set the current device to the first available CPU device (index 0) torch.cpu.set_device(0) # Retrieve and return the current device index current_device = torch.cpu.current_device() # Create a new stream and set it to the current stream for device 0 new_stream = torch.cpu.stream() # Synchronize the stream torch.cpu.synchronize() return current_device"},{"question":"# Custom JSON Encoder and Decoder Background In this coding assessment, you will be working on extending the functionality of Python\'s `json` module to handle custom data types. The primary goal is to ensure your Python objects can be accurately serialized to JSON and then deserialized back to the original Python objects. Consider the following custom data structure: ```python class Student: def __init__(self, name, age, grades): self.name = name self.age = age self.grades = grades def __eq__(self, other): return self.name == other.name and self.age == other.age and self.grades == other.grades ``` Task 1. **Custom JSON Encoder:** - Implement a custom encoder to handle the `Student` class. - Your encoder should convert a `Student` instance into a JSON object. 2. **Custom JSON Decoder:** - Implement a custom decoder to handle the JSON object that represents a `Student`. - Your decoder should convert a JSON object back into a `Student` instance. 3. **Integration:** - Integrate your custom encoder and decoder with `json.dumps()` and `json.loads()` methods. - Ensure that the converted JSON string maintains the correct information and can be deserialized back to the original `Student` object. Specifications - **Input:** A `Student` object for encoding and a JSON string for decoding. - **Output:** JSON string for encoding and a `Student` object for decoding. - **Constraints:** - The `grades` attribute is a list containing integers. - The `age` attribute must be an integer. - The `name` attribute must be a string. Example ```python import json class Student: def __init__(self, name, age, grades): self.name = name self.age = age self.grades = grades def __eq__(self, other): if isinstance(other, Student): return self.name == other.name and self.age == other.age and self.grades == other.grades return False class StudentEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Student): return {\\"__Student__\\": True, \\"name\\": obj.name, \\"age\\": obj.age, \\"grades\\": obj.grades} return json.JSONEncoder.default(self, obj) def StudentDecoder(dct): if \\"__Student__\\" in dct: return Student(dct[\\"name\\"], dct[\\"age\\"], dct[\\"grades\\"]) return dct # Encoding Example student = Student(\\"John Doe\\", 20, [85, 92, 78]) encoded_student = json.dumps(student, cls=StudentEncoder) print(encoded_student) # Should output a JSON string representing the student # Decoding Example decoded_student = json.loads(encoded_student, object_hook=StudentDecoder) print(decoded_student == student) # Should output True ``` Performance Requirements - The encoding and decoding process should be efficient and must handle large instances of `Student` objects appropriately (e.g., lists of hundreds of students). Save your solution in a file named `student_json.py`.","solution":"import json class Student: def __init__(self, name, age, grades): self.name = name self.age = age self.grades = grades def __eq__(self, other): if isinstance(other, Student): return self.name == other.name and self.age == other.age and self.grades == other.grades return False class StudentEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Student): return {\\"__Student__\\": True, \\"name\\": obj.name, \\"age\\": obj.age, \\"grades\\": obj.grades} return json.JSONEncoder.default(self, obj) def StudentDecoder(dct): if \\"__Student__\\" in dct: return Student(dct[\\"name\\"], dct[\\"age\\"], dct[\\"grades\\"]) return dct # Encoding Example student = Student(\\"John Doe\\", 20, [85, 92, 78]) encoded_student = json.dumps(student, cls=StudentEncoder) print(encoded_student) # Should output a JSON string representing the student # Decoding Example decoded_student = json.loads(encoded_student, object_hook=StudentDecoder) print(decoded_student == student) # Should output True"},{"question":"# Advanced Logging Configuration Problem Statement Create a Python script to demonstrate an advanced logging setup. Your script should perform the following tasks: 1. Create a logger named `appLogger` using `logging.getLogger(\'appLogger\')`. 2. Set the logger level to `DEBUG`. 3. Add two handlers to the logger: - A `StreamHandler` that logs messages to the console with a logging level of `INFO`. - A `FileHandler` that logs messages to a file named `app.log` with a logging level of `DEBUG`. 4. Define a custom formatter that includes the timestamp, logger name, log level, the function name from where the log record was generated, and the actual log message. Example format: `\'%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\'`. 5. Add the custom formatter to both handlers. 6. Log messages at all levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 7. Ensure that the console output only displays messages of level INFO and above, while the log file captures all messages from DEBUG and above. Constraints 1. Execute the script without any imports other than `logging`. 2. Ensure file operations are handled correctly to avoid resource leaks. 3. The log format must exactly match the provided example format. 4. The script should manage logger propagation correctly to avoid duplicate log entries. Expected Input and Output There is no input from the user. The output consists of logs printed to the console and written to the `app.log` file. Example of console output: ``` 2023-10-05 10:45:00 - appLogger - INFO - main - This is an info message 2023-10-05 10:45:00 - appLogger - WARNING - main - This is a warning message 2023-10-05 10:45:00 - appLogger - ERROR - main - This is an error message 2023-10-05 10:45:00 - appLogger - CRITICAL - main - This is a critical message ``` Example of `app.log` file content: ``` 2023-10-05 10:45:00 - appLogger - DEBUG - main - This is a debug message 2023-10-05 10:45:00 - appLogger - INFO - main - This is an info message 2023-10-05 10:45:00 - appLogger - WARNING - main - This is a warning message 2023-10-05 10:45:00 - appLogger - ERROR - main - This is an error message 2023-10-05 10:45:00 - appLogger - CRITICAL - main - This is a critical message ``` ```python import logging # Create the logger logger = logging.getLogger(\'appLogger\') logger.setLevel(logging.DEBUG) # Create the console handler with a higher log level console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Create the file handler which logs even debug messages file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Create a formatter and set it for both handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add the handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Logging messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ```","solution":"import logging def setup_logger(): # Create the logger logger = logging.getLogger(\'appLogger\') logger.setLevel(logging.DEBUG) # Create the console handler with a higher log level console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Create the file handler which logs even debug messages file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Create a formatter and set it for both handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add the handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) return logger def log_messages(logger): logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Example usage if __name__ == \\"__main__\\": logger = setup_logger() log_messages(logger)"},{"question":"**Problem Statement:** You are provided with a dataset \'tips\' that contains information about the total bill paid and other details related to tips given at a restaurant. Your task is to generate a series of plots that answer specific questions about the data using the `seaborn` library. **Instructions:** 1. Load the \'tips\' dataset using the seaborn library. 2. Create a function `generate_plots` that takes no parameters and accomplishes the following: - Plots the total bill amount distribution categorized by day of the week. - Creates a swarmplot that contrasts total bill and tips received by gender. - Illustrates the distribution of total bills categorized by day and colored by the size of party (number of people). - Generates a facet grid that shows the relationship between time of the day and the total bill amount, further grouped by the day of the week. The function should return a list of the four generated plots. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Your code here return [plot1, plot2, plot3, plot4] ``` **Requirements:** - Each plot should be well-labeled with appropriate titles, axis labels, and legends. - Use an appropriate color palette for each plot to ensure that the visualizations are clear and distinguishable. - Ensure that the points in the swarm plots do not overlap excessively. - Maintain a consistent theme across the plots for better presentation. **Constraints:** - You may use explicit imports for seaborn and matplotlib. - The function should not take any arguments and should return a list of the generated plots. - Handle any necessary adjustments to ensure that the visualizations are clear and informative. **Example:** ```python plots = generate_plots() for plot in plots: plot.show() ``` The output will display four distinct plots following the guidelines provided. **Dataset Description:** - `total_bill`: Continuous variable; total amount of the bill (dollars). - `tip`: Continuous variable; amount of the tip (dollars). - `sex`: Categorical variable; gender of the person paying. - `smoker`: Categorical variable; whether the person was a smoker or not. - `day`: Categorical variable; day of the week. - `time`: Categorical variable; time of day (Lunch vs. Dinner). - `size`: Numeric variable; size of the dining party.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Plot 1: Total bill distribution categorized by day of the week plt.figure(figsize=(10, 6)) plot1 = sns.boxplot(x=\'day\', y=\'total_bill\', data=tips, palette=\'Set2\') plot1.set_title(\'Total Bill Amount Distribution by Day of the Week\') plot1.set_xlabel(\'Day\') plot1.set_ylabel(\'Total Bill\') # Plot 2: Swarmplot contrasting total bill and tips received by gender plt.figure(figsize=(10, 6)) plot2 = sns.swarmplot(x=\'total_bill\', y=\'tip\', hue=\'sex\', data=tips, palette=\'Set1\') plot2.set_title(\'Total Bill vs Tips by Gender\') plot2.set_xlabel(\'Total Bill\') plot2.set_ylabel(\'Tip\') # Plot 3: Distribution of total bills categorized by day and colored by the size of the party plt.figure(figsize=(10, 6)) plot3 = sns.boxplot(x=\'day\', y=\'total_bill\', hue=\'size\', data=tips, palette=\'coolwarm\') plot3.set_title(\'Total Bill Amount Distribution by Day Colored by Party Size\') plot3.set_xlabel(\'Day\') plot3.set_ylabel(\'Total Bill\') # Plot 4: Facet grid showing relationship between time of the day and total bill amount, grouped by day of the week plot4 = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", margin_titles=True) plot4.map(plt.scatter, \'total_bill\', \'size\', color=\'purple\') plot4.set_axis_labels(\'Total Bill\', \'Party Size\') return [plot1, plot2, plot3, plot4]"},{"question":"# Custom Object Supporting Multiple Protocols **Objective**: Implement a custom class in Python that supports both sequence and buffer protocols. This class, `CustomBufferSequence`, should behave like a list and also support buffer operations. **Instructions**: 1. Implement the custom class `CustomBufferSequence` with the following requirements: - The class should inherit from the built-in `list` class. - Override necessary methods to support the sequence protocol (indexing, slicing, etc.). - Implement buffer interface methods to allow the object to be used in contexts where buffer objects are needed (e.g., with `memoryview` or `bytearray`). 2. Specific Requirements: - **Initialization**: The class constructor should accept an optional list of integers to initialize the object. - **Sequence Protocol**: - Support indexing to access and modify elements. - Support slicing to access subsequences. - Support common list operations like `append`, `extend`, and `remove`. - **Buffer Protocol**: - Implement buffer acquisition and release methods. - Ensure the buffer interface correctly exposes the underlying data in a contiguous byte format. 3. Example Usage: - Demonstrate the creation of `CustomBufferSequence` objects. - Demonstrate sequence operations (e.g., indexing, slicing). - Demonstrate buffer operations by creating `memoryview` objects and manipulating the data. **Constraints**: - The class should handle up to (10^6) elements efficiently. - Ensure proper error handling and validation for all methods. **Testing**: - You should provide a set of unit tests to verify the correctness of your implementation against different scenarios. # Example Code Structure ```python class CustomBufferSequence(list): def __init__(self, data=None): if data is None: data = [] elif not all(isinstance(x, int) for x in data): raise ValueError(\\"All elements must be integers\\") super().__init__(data) # Implement sequence protocol methods # ... # Implement buffer protocol methods def __buffer__(self): # Return a memoryview or buffer representation of the data pass # ... # Example Usage: seq = CustomBufferSequence([1, 2, 3, 4, 5]) print(seq[1]) # Output: 2 mv = memoryview(seq) print(mv[1:3]) # Output: <memory at ...> # Unit tests here... ``` This question will evaluate the student\'s understanding of Python\'s object models, protocol methods, and their ability to create complex, protocol-compliant objects.","solution":"class CustomBufferSequence(list): def __init__(self, data=None): if data is None: data = [] elif not all(isinstance(x, int) for x in data): raise ValueError(\\"All elements must be integers\\") super().__init__(data) # Implementing buffer protocol def __buffer__(self): return memoryview(bytearray(self)) def __getitem__(self, index): return super().__getitem__(index) def __setitem__(self, index, value): if not isinstance(value, int): raise ValueError(\\"All elements must be integers\\") super().__setitem__(index, value) def append(self, value): if not isinstance(value, int): raise ValueError(\\"All elements must be integers\\") super().append(value) def extend(self, values): if not all(isinstance(x, int) for x in values): raise ValueError(\\"All elements must be integers\\") super().extend(values) def remove(self, value): super().remove(value)"},{"question":"**Objective**: Demonstrate your understanding of the `bz2` module in Python for handling compression and decompression tasks. Problem Statement You are provided with a directory containing multiple text files. Your task is to implement a Python function that: 1. Compresses all the text files in the given directory using the bzip2 compression algorithm with a specified compression level. 2. Decompresses the compressed files back into their original contents to verify the integrity of the compression process. # Function Signature ```python def compress_and_verify(directory: str, compresslevel: int) -> bool: Compresses all text files in the specified directory and verifies the integrity of the compressed data. Parameters: directory (str): The path to the directory containing the text files. compresslevel (int): The compression level (1 to 9) to be used for compressing the files. Returns: bool: True if all files are successfully decompressed to their original contents, False otherwise. pass ``` # Input 1. `directory`: A string representing the path to the directory containing text files. 2. `compresslevel`: An integer between 1 and 9 representing the compression level. # Output - The function returns `True` if all files are successfully decompressed to their original contents, otherwise `False`. # Constraints - Assume that the directory contains only \'.txt\' files. - You must use the `bz2` module for compression and decompression. - Use incremental compression and decompression to handle potential large file sizes efficiently. # Example ```python result = compress_and_verify(\\"/path/to/text/files\\", 5) print(result) # Output: True if verification is successful, otherwise False. ``` # Requirements - Read each file in chunks to avoid memory overuse. - Create compressed files with the same name but with a `.bz2` extension. - Verify the decompressed content matches the original content. # Notes - Ensure that the file handles are properly managed using context managers (`with` statement). - Handle potential exceptions gracefully, particularly related to file I/O operations.","solution":"import os import bz2 def compress_and_verify(directory: str, compresslevel: int) -> bool: Compresses all text files in the specified directory and verifies the integrity of the compressed data. Parameters: directory (str): The path to the directory containing the text files. compresslevel (int): The compression level (1 to 9) to be used for compressing the files. Returns: bool: True if all files are successfully decompressed to their original contents, False otherwise. try: # List all text files in the directory files = [f for f in os.listdir(directory) if f.endswith(\'.txt\')] for file in files: original_path = os.path.join(directory, file) # Compress the file compressed_path = original_path + \'.bz2\' with open(original_path, \'rb\') as input_file: with bz2.BZ2File(compressed_path, \'wb\', compresslevel=compresslevel) as output_file: for data in iter(lambda: input_file.read(1024), b\'\'): output_file.write(data) # Decompress the file for verification decompressed_content = b\'\' with bz2.BZ2File(compressed_path, \'rb\') as input_file: for data in iter(lambda: input_file.read(1024), b\'\'): decompressed_content += data # Verify the decompressed content matches the original content with open(original_path, \'rb\') as original_file: original_content = original_file.read() if original_content != decompressed_content: return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective:** This question aims to assess your understanding of Python interpreter initialization, thread management, and handling the Global Interpreter Lock (GIL) as described in the Python C API documentation. Problem Statement You are tasked with writing a Python C extension that initializes the Python interpreter, then launches multiple threads to perform some computations, and properly manages the Global Interpreter Lock (GIL) to ensure thread safety. **Requirements:** 1. **Initialization and Finalization**: - The Python interpreter must be initialized correctly before any threads are created. - Before the interpreter finalizes, all threads must finish their computation. 2. **Thread Management and GIL Handling**: - Create at least two threads that perform a simple task (e.g., summing numbers in a range). - Each thread should: - Acquire the GIL before performing any Python API calls. - Release the GIL when performing potentially blocking I/O operations. 3. **Sub-interpreters (Bonus)**: - Optionally, demonstrate the creation and usage of a sub-interpreter to run a separate script and manage it correctly. Input No direct input from the user is needed for this task. Output The program should output: 1. Messages indicating the initialization and finalization of the Python interpreter. 2. Outputs from each thread indicating their computations. 3. Optional outputs from the sub-interpreter if implemented. Constraints - Use the provided Python C API functions effectively. - Ensure no memory leaks or crashes occur during the interpretation initialization and finalization. - Properly manage the GIL to avoid deadlocks. Skeleton Code (Python C extension) Below is a partial skeleton code to guide you in implementing the solution. You need to fill in the parts marked with `TODO`. ```c #include <Python.h> #include <stdio.h> #include <pthread.h> // Task function for threads void* thread_task(void* arg) { PyGILState_STATE gstate; gstate = PyGILState_Ensure(); // TODO: Perform some Python C API calls or computations PyGILState_Release(gstate); return NULL; } int main(int argc, char *argv[]) { // Step 1: Initialize the Python Interpreter printf(\\"Initializing Python Interpreter...n\\"); Py_Initialize(); // Step 2: Check if Python Initialized Correctly if (!Py_IsInitialized()) { fprintf(stderr, \\"Python Initialization failed!n\\"); return 1; } // Step 3: Create and start threads pthread_t threads[2]; for (int i = 0; i < 2; i++) { pthread_create(&threads[i], NULL, thread_task, NULL); } // Step 4: Join threads for (int i = 0; i < 2; i++) { pthread_join(threads[i], NULL); } // Step 5: Finalize the Python Interpreter printf(\\"Finalizing Python Interpreter...n\\"); Py_Finalize(); return 0; } // TODO: Optionally implement a sub-interpreter function if you choose to ``` **Assessment:** - Correct and efficient implementation of Python initialization and finalization. - Proper creation and synchronization of threads. - Appropriate management of the GIL for thread safety. - Optional bonus for using sub-interpreters effectively.","solution":"import sys import threading # Worker function that calculates the sum of a range of numbers def worker(start, end, result, index): total = 0 for i in range(start, end): total += i result[index] = total print(f\\"Thread {index}: Computed sum from {start} to {end-1} is {total}\\") def main(): print(\\"Initializing Python Interpreter...\\") # Initialize the result list and number of threads num_threads = 2 result = [0] * num_threads threads = [] # Prepare thread arguments and create threads ranges = [(0, 500000), (500000, 1000000)] for i, (start, end) in enumerate(ranges): thread = threading.Thread(target=worker, args=(start, end, result, i)) threads.append(thread) # Start all threads for thread in threads: thread.start() # Ensure all threads have finished for thread in threads: thread.join() # Summing up the results from all threads final_result = sum(result) print(f\\"Final sum is {final_result}\\") print(\\"Finalizing Python Interpreter...\\") if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question:** # Task: You are required to create a plot that demonstrates your understanding of Seaborn\'s objects interface for visualization and annotation techniques. You will work with a dataset that contains scores of models evaluated on various tasks. The dataset needs to be manipulated, and specific plots are to be generated with annotations. # The Dataset: The dataset is sourced from seaborn\'s `load_dataset` function with the name \\"glue\\". The dataset has a structure with columns representing different tasks and includes model scores. # Requirements: 1. **Data Preparation:** - Load the dataset using `seaborn.load_dataset(\\"glue\\")`. - Pivot the dataset such that the index includes \\"Model\\" and \\"Encoder\\" and the columns are tasks with their respective \\"Score\\". - Add a new column \\"Average\\" which is the mean score of all tasks for each model, rounded to one decimal place. - Sort the dataset by the \\"Average\\" column in descending order. 2. **Plotting & Annotation:** - Create a scatter plot with \\"SST-2\\" scores on the x-axis and \\"MRPC\\" scores on the y-axis. - Annotate each point on the scatter plot with the Model name. Ensure the annotations do not overlap excessively and are readable. - Create a bar plot to represent the average scores of the models. Each bar should be labeled with the average score, horizontally-aligned and with appropriate text offset. - Optionally, map the text color to the \\"Encoder\\" for the scatter plot annotations, if it provides better visualization. # Additional Notes: - Use Seaborn\'s objects interface for plotting. - Ensure your plot is clear, properly labeled, and includes a legend if necessary. - Follow good coding practices by appropriately commenting your code and handling any edge cases. # Example Input and Output: For example, if the dataset looked like this (simplified): ``` Task Model Encoder Score SST-2 A LSTM 80 MRPC A LSTM 85 SST-2 B Transformer 78 MRPC B Transformer 88 ``` The plot should show a scatter plot with SST-2 scores on the x-axis and MRPC scores on the y-axis, annotated with model names and colored by encoder. A bar plot should show the average scores of the model with labels. Below is the starting code block for your implementation: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and prepare dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Plot 1: Scatter plot of SST-2 vs MRPC with model names scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\", color=\\"Encoder\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Plot 2: Bar plot of Average scores with horizontal alignment of text bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Display plots scatter_plot.show() bar_plot.show() ``` Complete the implementation by ensuring the plots meet the requirements and are properly annotated.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and prepare dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Plot 1: Scatter plot of SST-2 vs MRPC with model names scatter_plot = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\", color=\\"Encoder\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Plot 2: Bar plot of Average scores with horizontal alignment of text bar_plot = ( so.Plot(glue.reset_index(), y=\\"Model\\", x=\\"Average\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"left\\", offset=4)) ) # Display plots scatter_plot.show() bar_plot.show()"},{"question":"<|Analysis Begin|> The provided documentation details various environment variables related to the Metal Performance Shaders (MPS) backend in PyTorch. These environment variables allow configuration of logging levels, performance settings, memory management, and fallback mechanisms when using the MPS allocator and kernels. Key variables include: - `PYTORCH_DEBUG_MPS_ALLOCATOR`: Enables verbose logging for the MPS allocator. - `PYTORCH_MPS_LOG_PROFILE_INFO` and `PYTORCH_MPS_TRACE_SIGNPOSTS`: Configure profiling and log options. - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` and `PYTORCH_MPS_LOW_WATERMARK_RATIO`: Manage memory allocation limits and garbage collection strategies. - `PYTORCH_MPS_FAST_MATH`: Enables fast math optimizations. - `PYTORCH_MPS_PREFER_METAL`: Forces the use of metal kernels over MPS Graph APIs. - `PYTORCH_ENABLE_MPS_FALLBACK`: Enables fallback to CPU for unsupported MPS operations. These variables provide fine-grained control over performance and resource management when using PyTorch with MPS on Apple hardware. Based on this, a coding challenge can be designed to test the student\'s understanding of how to leverage these environment variables in a practical scenario. <|Analysis End|> <|Question Begin|> # PyTorch MPS Environment Variables Configuration Objective: In this task, you will demonstrate your understanding of configuring PyTorch\'s Metal Performance Shaders (MPS) backend using environment variables. You are tasked with writing a Python function that sets up an optimal environment for using MPS in a constrained memory device. The function should configure environment variables to enable fast math optimizations, verbose logging for memory allocation, suitable profiling, and fallback mechanisms. Function Signature: ```python def configure_pytorch_mps_environment(): pass ``` Implementation Requirements: - **Enable fast math optimizations** for MPS metal kernels. - **Set the MPS allocator logging level to verbose**. - **Enable profiling and log profiling information**. - **Set the high watermark ratio** to 0.9, keeping memory allocations within 90% of the recommended maximum allocation size. - **Set the low watermark ratio** to 0.7, attempting to limit allocations within 70% of the recommended maximum allocation size. - **Enable fallback operations to CPU** when MPS does not support certain operations. Constraints: - You should utilize Python\'s `os` module to set these environment variables within your function. - Assume the PyTorch library is available and properly installed in the environment where this function will be executed. Example Usage: After implementing the function, invoking `configure_pytorch_mps_environment()` should correctly set the necessary environment variables. You can verify the settings using the `os.getenv` function. Sample Test Code: ```python import os def test_configure_pytorch_mps_environment(): configure_pytorch_mps_environment() assert os.getenv(\'PYTORCH_MPS_FAST_MATH\') == \'1\', \\"Fast math optimization should be enabled\\" assert os.getenv(\'PYTORCH_DEBUG_MPS_ALLOCATOR\') == \'1\', \\"MPS allocator verbose logging should be enabled\\" assert os.getenv(\'PYTORCH_MPS_LOG_PROFILE_INFO\') == \'MPSProfiler\', \\"MPS profiling should be enabled\\" assert os.getenv(\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\') == \'0.9\', \\"High watermark ratio should be set to 0.9\\" assert os.getenv(\'PYTORCH_MPS_LOW_WATERMARK_RATIO\') == \'0.7\', \\"Low watermark ratio should be set to 0.7\\" assert os.getenv(\'PYTORCH_ENABLE_MPS_FALLBACK\') == \'1\', \\"CPU fallback should be enabled\\" print(\\"All tests passed!\\") test_configure_pytorch_mps_environment() ``` This test checks whether the environment variables are correctly set by your function. Ensure that your `configure_pytorch_mps_environment` function passes all the assertions to be considered correct. Good luck!","solution":"import os def configure_pytorch_mps_environment(): Configures the environment variables for optimal PyTorch MPS usage. os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' # Enable fast math optimizations os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' # Enable verbose logging for MPS allocator os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = \'MPSProfiler\' # Enable profiling and log profiling information os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = \'0.9\' # Set high watermark ratio to 0.9 os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = \'0.7\' # Set low watermark ratio to 0.7 os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\' # Enable fallback to CPU for unsupported MPS operations"},{"question":"**Objective**: Demonstrate understanding and application of the `copyreg` module for customizing the serialization process with the `pickle` module in Python. Problem Statement You need to implement a custom serialization process for a class of objects using the `copyreg` module. Specifically, you will create a class `Employee`, register a custom pickling function for this class, and test the functionality to ensure that objects of this class are correctly serialized and deserialized. # Task 1. **Define an `Employee` class** with the following structure: - Attributes: `name` (string), `employee_id` (integer), and `department` (string). - Method: `__init__(self, name, employee_id, department)` should initialize the attributes. 2. **Create a custom pickling function** `pickle_employee(employee)` that: - Takes an `Employee` object as input. - Returns a tuple containing the `Employee` class and a tuple of its attributes `(name, employee_id, department)`. 3. **Register the custom pickling function** using `copyreg.pickle`. 4. **Write a function `serialize_employee(employee)`** that: - Takes an `Employee` object. - Uses the `pickle` module to serialize the object. 5. **Write a function `deserialize_employee(serialized_employee)`** that: - Takes a serialized `Employee` object. - Uses the `pickle` module to deserialize it back into an `Employee` object. Input and Output * Input: An instance of the `Employee` class. * Output: A serialized string of the `Employee` object in the case of `serialize_employee`, and an `Employee` object in the case of `deserialize_employee`. Example ```python # Define the Employee class class Employee: def __init__(self, name, employee_id, department): self.name = name self.employee_id = employee_id self.department = department # Define the custom pickle function def pickle_employee(employee): return Employee, (employee.name, employee.employee_id, employee.department) # Register the pickling function copyreg.pickle(Employee, pickle_employee) # Function to serialize Employee object def serialize_employee(employee): import pickle return pickle.dumps(employee) # Function to deserialize Employee object def deserialize_employee(serialized_employee): import pickle return pickle.loads(serialized_employee) # Example usage emp = Employee(\\"John Doe\\", 12345, \\"Engineering\\") serialized_emp = serialize_employee(emp) deserialized_emp = deserialize_employee(serialized_emp) assert emp.name == deserialized_emp.name assert emp.employee_id == deserialized_emp.employee_id assert emp.department == deserialized_emp.department ``` Constraints 1. You cannot change the signature of the provided methods. 2. Utilize the `copyreg` and `pickle` modules only for serialization and deserialization tasks. 3. Ensure that the deserialized object retains all properties of the original object. Complete the task by implementing the `Employee` class, the custom pickling function, and the serialization/deserialization functions as described.","solution":"import copyreg import pickle class Employee: def __init__(self, name, employee_id, department): self.name = name self.employee_id = employee_id self.department = department def pickle_employee(employee): return Employee, (employee.name, employee.employee_id, employee.department) copyreg.pickle(Employee, pickle_employee) def serialize_employee(employee): return pickle.dumps(employee) def deserialize_employee(serialized_employee): return pickle.loads(serialized_employee)"},{"question":"# File and Directory Comparison You are required to implement a function that will compare two directory structures and identify if they are similar or not based on specific criteria. The function should also generate a detailed report of the comparison. Requirements: 1. The function should compare all files and subdirectories recursively. 2. For the directories to be considered similar, all the files must be identical in both directories. 3. The function should generate a detailed comparison report, similar to the `dircmp` report, but custom formatted as a dictionary. # Function Signature: ```python def compare_directories(dir1: str, dir2: str) -> dict: Compares two directories and generates a detailed comparison report. Parameters: dir1 (str): The path to the first directory to compare. dir2 (str): The path to the second directory to compare. Returns: dict: A dictionary with the comparison results. The dictionary should have the following keys: - \\"identical\\": list of identical files. - \\"different\\": list of different files with paths. - \\"left_only\\": list of files only in the first directory. - \\"right_only\\": list of files only in the second directory. - \\"subdirectory_comparisons\\": dictionary where keys are subdirectory names and values are similar structures of nested comparison results. - \\"comparison_valid\\": a boolean value indicating if the directories are overall similar based on the criteria. # Your implementation here ``` # Example: ```python compare_directories(\'dir1\', \'dir2\') ``` # Constraints: - You should use the `filecmp` module for file and directory comparisons. - Assume all input directory paths are valid and accessible. - The comparison should be shallow (`shallow=True`). # Expected Output: This function should return a dictionary with a structure simlar to the example below: ```python { \\"identical\\": [\\"file1.txt\\", \\"subdir1/file2.txt\\"], \\"different\\": [\\"file3.txt\\"], \\"left_only\\": [\\"file4.txt\\", \\"subdir1/file5.txt\\"], \\"right_only\\": [\\"file6.txt\\"], \\"subdirectory_comparisons\\": { \\"subdir1\\": { \\"identical\\": [\\"file2.txt\\"], \\"different\\": [], \\"left_only\\": [\\"file5.txt\\"], \\"right_only\\": [], \\"subdirectory_comparisons\\": {} } }, \\"comparison_valid\\": False } ``` This question tests the studentâ€™s ability to utilize the `filecmp` module effectively to compare files and directories, make use of recursion for subdirectory comparisons, and generate comprehensive reports based on the comparisons.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: result = { \\"identical\\": [], \\"different\\": [], \\"left_only\\": [], \\"right_only\\": [], \\"subdirectory_comparisons\\": {}, \\"comparison_valid\\": True } def compare_dirs(d1, d2): cmp = filecmp.dircmp(d1, d2) result[\'identical\'].extend(cmp.same_files) result[\'different\'].extend(cmp.diff_files) result[\'left_only\'].extend(cmp.left_only) result[\'right_only\'].extend(cmp.right_only) if cmp.diff_files or cmp.left_only or cmp.right_only: result[\'comparison_valid\'] = False for subdir in cmp.common_dirs: subdir1 = os.path.join(d1, subdir) subdir2 = os.path.join(d2, subdir) sub_result = compare_directories(subdir1, subdir2) result[\'subdirectory_comparisons\'][subdir] = sub_result if not sub_result[\'comparison_valid\']: result[\'comparison_valid\'] = False compare_dirs(dir1, dir2) return result"},{"question":"# Advanced String Conversion and Formatting **Problem Statement:** You are required to implement a Python function `secure_string_conversion` that converts numbers to formatted string representations and strings to numbers, mimicking some behavior of the `PyOS_*` functions mentioned in the documentation. Requirements: 1. Implement a function `secure_string_conversion` with the following signature: ```python def secure_string_conversion(input_value, conversion_type, format_spec=None): pass ``` 2. The function should handle two types of conversions based on the `conversion_type` parameter: - **\\"to_string\\"**: Converts a number (int or float) to a string using a specific format. - **\\"to_number\\"**: Converts a string to a number (float), ensuring rigorous error handling. 3. For the **\\"to_string\\"** conversion: - The parameter `format_spec` should be a dictionary that can contain the following keys: `format_code`, `precision`, and `flags`. - Supported `format_code` values are: `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, and `\'r\'`. - `precision` should be an integer indicating the number of decimal places. - `flags` is a dictionary containing the following boolean options: `Py_DTSF_SIGN`, `Py_DTSF_ADD_DOT_0`, and `Py_DTSF_ALT`. 4. For the **\\"to_number\\"** conversion: - The function should accept a string representation of a float and convert it to a number, raising a `ValueError` with an appropriate message if the conversion fails. - Handle string inputs that contain invalid characters and set end pointer (Optional) indicating the first unconverted character. Example: ```python # to_string conversion input_value = 1234.5678 conversion_type = \\"to_string\\" format_spec = { \\"format_code\\": \\"f\\", \\"precision\\": 2, \\"flags\\": { \\"Py_DTSF_SIGN\\": True, \\"Py_DTSF_ADD_DOT_0\\": True, \\"Py_DTSF_ALT\\": False } } result = secure_string_conversion(input_value, conversion_type, format_spec) print(result) # Output: \'+1234.57\' # to_number conversion input_value = \\"1234.5678abc\\" conversion_type = \\"to_number\\" result = secure_string_conversion(input_value, conversion_type) print(result) # Raise ValueError with message: \\"Conversion failed. Unprocessed input at position: 9\\" ``` Constraints: - For **\\"to_string\\"** conversion, ensure the output string does not exceed 100 characters. - For **\\"to_number\\"** conversion, assume the input string will be less than 100 characters long. You must implement proper error handling and ensure that the function handles both conversions robustly.","solution":"def secure_string_conversion(input_value, conversion_type, format_spec=None): if conversion_type == \\"to_string\\": if not isinstance(input_value, (int, float)): raise ValueError(\\"Input value must be a number for \'to_string\' conversion\\") if not isinstance(format_spec, dict): raise ValueError(\\"format_spec must be a dictionary for \'to_string\' conversion\\") format_code = format_spec.get(\\"format_code\\", \'f\') precision = format_spec.get(\\"precision\\", 6) flags = format_spec.get(\\"flags\\", {}) # Detect flags sign_flag = \'+\' if flags.get(\\"Py_DTSF_SIGN\\", False) else \'\' add_dot_0 = flags.get(\\"Py_DTSF_ADD_DOT_0\\", False) alt_flag = \'#\' if flags.get(\\"Py_DTSF_ALT\\", False) else \'\' # Construct format string fmt = f\\"{sign_flag}{alt_flag}.{precision}{format_code}\\" result = format(input_value, fmt) if add_dot_0 and \'.\' not in result: result += \'.0\' if len(result) > 100: raise ValueError(\\"Resulting string exceeds 100 characters\\") return result elif conversion_type == \\"to_number\\": if not isinstance(input_value, str): raise ValueError(\\"Input value must be a string for \'to_number\' conversion\\") try: end_pos = len(input_value) for i, char in enumerate(input_value): if not (char.isdigit() or char in \\"+-e.\\"): end_pos = i break number_part = input_value[:end_pos] number = float(number_part) if end_pos != len(input_value): raise ValueError(f\\"Conversion failed. Unprocessed input at position: {end_pos}\\") return number except ValueError: raise ValueError(f\\"Conversion failed. Unprocessed input at position: {end_pos}\\") else: raise ValueError(\\"Invalid conversion_type. Must be \'to_string\' or \'to_number\'.\\")"},{"question":"Objective You are given a dataset of flights with various flight-related information. Your task is to create a series of visualizations using Seaborn to analyze passenger data for the given dataset. The key focus should be on properly handling multiple variables and employing Seaborn functionalities effectively. Datasets You will need to use the Seaborn `flights` dataset, filtered for the year 1960. Requirements 1. **Draw a Bar Plot**: - Create a bar plot showing the number of passengers for each month in 1960. 2. **Enhance the Bar Plot**: - Transform the bar plot such that bars are color-coded by month. - Apply a `Dodge` transformation to handle overlapping bars if necessary. 3. **Add an Error Bar Plot**: - On the enhanced bar plot, plot the mean number of passengers for each month with an error bar representing the standard deviation of passengers. 4. **Customization**: - Customize the final plot by setting different properties like color, alpha, and edge styles to improve the visual appeal and readability. # Implementation You need to implement the following function: ```python import seaborn.objects as so from seaborn import load_dataset def plot_flight_data(): # Load the dataset flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Part 1: Create a bar plot showing the number of passengers for each month in 1960 plot = so.Plot(flights, x=\\"month\\", y=\\"passengers\\").add(so.Bar()) # Part 2: Enhance the bar plot plot = so.Plot(flights, x=\\"month\\", color=\\"month\\", y=\\"passengers\\").add(so.Bar(), so.Dodge()) # Part 3: Add an error bar plot for the mean number of passengers with standard deviation plot.add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) plot.add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) # Part 4: Customize the plot with various properties (color, alpha, edge style) plot = ( plot .scale(color=\\"pastel\\") .theme({ \\"figure.facecolor\\": \\"white\\", \\"axes.labelsize\\": \\"large\\", \\"axes.titlesize\\": \\"medium\\", }) ) return plot # To visualize the plot, you can run the plot_flight_data() function. ``` **Expected Output:** The function should return a `seaborn.objects.Plot` object containing the visualizations as specified. The output should be a meaningful and appealing representation of the passenger data for the year 1960, complete with color-coding, dodged bars, error bars, and customized styling. Constraints - Assume the Seaborn version used is up-to-date with the functionalities described. - Use the dataset and attributes as specified. - Ensure the plot is clear and readable, making use of Seaborn\'s customization capabilities. Performance: - Optimization is not a major concern for this task, but ensure the code is efficient enough to handle the dataset size provided without excessive computation time.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_flight_data(): # Load the dataset flights = sns.load_dataset(\\"flights\\") flights_1960 = flights.query(\\"year == 1960\\") # Create a canvas with a size suitable for display plt.figure(figsize=(10, 6)) # Part 1 & 2: Create and enhance the bar plot monthly_means = flights_1960.groupby(\'month\')[\'passengers\'].mean() monthly_std = flights_1960.groupby(\'month\')[\'passengers\'].std() plot = sns.barplot(x=\\"month\\", y=\\"passengers\\", data=flights_1960, ci=None, palette=\\"pastel\\", estimator=sum) # Add error bars for idx, month in enumerate(monthly_means.index): plt.errorbar(idx, monthly_means[month], yerr=monthly_std[month], fmt=\'none\', c=\'black\', capsize=5) # Customize the plot plot.set_title(\\"Monthly Passenger Count in 1960\\") plot.set_xlabel(\\"Month\\") plot.set_ylabel(\\"Number of Passengers\\") plt.show()"},{"question":"# Advanced Python Built-in Types Challenge Objective: Write a Python function `operation_on_collections` that takes the following parameters: 1. A list of integers. 2. A set of integers. 3. A dictionary where keys are integers and values are strings. Your function should perform the following operations: 1. Remove all elements from the list that are present in the set. 2. For any integer in the dictionary that is also in the set, update the dictionary key to be the integer squared. 3. Return a tuple: - The modified list. - The modified dictionary. - A frozen set consisting of all modified dictionary keys. Input: - A list of integers: `list[int]` - A set of integers: `set[int]` - A dictionary with integer keys and string values: `dict[int, str]` Output: - A tuple containing: - A modified list (list of integers). - A modified dictionary (with squared keys if they were in the set). - A frozen set with modified dictionary keys. Constraints: 1. Each integer in the list, set, and dictionary keys is unique. 2. The input list has at most 10^4 elements. 3. The input set has at most 10^4 elements. 4. The input dictionary has at most 10^4 key-value pairs. Example: ```python def operation_on_collections(lst, st, dct): # Your code here # Example usage: lst = [1, 2, 3, 4, 5] st = {2, 4, 6} dct = {1: \'one\', 2: \'two\', 3: \'three\', 4: \'four\'} result = operation_on_collections(lst, st, dct) # expected output: # ([1, 3, 5], {1: \'one\', 4: \'two\', 3: \'three\', 16: \'four\'}, frozenset({1, 4, 3, 16})) print(result) ``` Requirements: - Demonstrate efficient use of Python built-in types like lists, sets, dictionaries, tuples, and frozen sets. - Handle the loaded operations cleanly, ensuring optimized performance. - Provide docstrings and comments for clarity.","solution":"def operation_on_collections(lst, st, dct): Performs the following operations: 1. Removes elements from the list that are present in the set. 2. Updates dictionary keys to their squares if the key is present in the set. 3. Returns a tuple containing the modified list, modified dictionary, and a frozen set of the dictionary\'s keys. Parameters: lst (list[int]): A list of integers. st (set[int]): A set of integers. dct (dict[int, str]): A dictionary with integer keys and string values. Returns: tuple: A tuple containing the modified list, modified dictionary, and a frozen set with modified dictionary keys. # 1. Remove elements from the list that are in the set modified_list = [item for item in lst if item not in st] # 2. Update dictionary keys if they are present in the set modified_dict = {} for key, value in dct.items(): if key in st: modified_dict[key ** 2] = value else: modified_dict[key] = value # 3. Create a frozen set of the modified dictionary keys keys_frozenset = frozenset(modified_dict.keys()) return (modified_list, modified_dict, keys_frozenset)"},{"question":"# Python Coding Assessment Question Objective: Implement a custom generator function in Python. This function should mimic the behavior of Python\'s built-in range function but with some advanced features. Problem Statement: You are required to implement a generator function `custom_range_generator()` that replicates the behavior of Python\'s built-in `range` but with the following additional features: 1. **Inclusive End:** The generator should include the end value specified if it falls within the step increments. 2. **Support for Floating-Point Steps:** The step value can be a floating-point to allow for non-integer increments. 3. **Validation:** The function should validate inputs and raise appropriate exceptions for invalid arguments (e.g., step cannot be zero). 4. **Reversal Support:** The function should support reverse iteration if the start value is greater than the stop value and the step is negative. 5. **Custom Name Attributes:** The generator should have custom `__name__` and `__qualname__` attributes. Function Signature: ```python def custom_range_generator(start, stop=None, step=1.0): pass ``` Input: - `start` (float or int): The starting value of the sequence. - `stop` (float or int): The ending value of the sequence. If `None`, it defaults to the value of `start`, and `start` becomes 0. - `step` (float): The increment (or decrement) between each value in the sequence. Defaults to 1.0. Output: - A generator object that yields values from `start` to `stop` inclusive, based on the `step` value. Example Usage: ```python for value in custom_range_generator(1, 5): print(value) # Output: 1, 2, 3, 4, 5 for value in custom_range_generator(1, 5, 0.5): print(value) # Output: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0 for value in custom_range_generator(5, 1, -1): print(value) # Output: 5, 4, 3, 2, 1 ``` Constraints: - The `step` value must not be zero; if it is, the function should raise `ValueError`. - If `start` equals `stop`, the generator should yield `start` once. - The function should handle both increasing and decreasing sequences appropriately. Performance Requirements: - The generator should be memory-efficient, generating values on-the-fly rather than storing them in a list. Additional Requirements: - Set the `__name__` attribute of the generator to `custom_range_gen` and the `__qualname__` attribute to the full function name with the module name. Note: Make sure to include validation and appropriate exception handling in your implementation.","solution":"def custom_range_generator(start, stop=None, step=1.0): Custom range generator that includes the end value, supports floating-point steps, validates inputs, and supports reverse iteration. if step == 0: raise ValueError(\\"step must not be zero\\") if stop is None: stop = start start = 0 current = start if step > 0: while current <= stop: yield current current += step else: while current >= stop: yield current current += step # Setting custom attributes custom_range_generator.__name__ = \\"custom_range_gen\\" custom_range_generator.__qualname__ = \\"custom_range_generator.custom_range_gen\\""},{"question":"# Question You are tasked with processing multiple log files to extract and anonymize sensitive information. Specifically, convert all email addresses in the logs to a placeholder `<EMAIL>` for compliance purposes. The program should iterate over each line in the given files, replace any email addresses found with the placeholder, and write the changes back to the original files. Requirements: 1. **Handle multiple files**: Process all files specified in the input. 2. **In-place Modification**: Use in-place modification to update each file directly. 3. **Email Detection**: Use a regex to identify email addresses. An email address is defined as any string in the format `username@domain.extension`. 4. **Context Management**: Ensure files are correctly handled using context management. 5. **Include Error Handling**: Consider any I/O exceptions that could occur. 6. **Performance Considerations**: Process the files efficiently, especially considering large files. Input: - A list of filenames. - Each file contains lines of text which might include email addresses. Output: - The files themselves should be updated in-place with all email addresses replaced by `<EMAIL>`. No additional output is necessary. Example: **Input Files:** **file1.txt** ``` User1 logged in from user1@example.com User2 reported an error from user2@example.org ``` **file2.txt** ``` Contact support at support@company.com Email user@domain.com for inquiries ``` **Output Files (updated in-place):** **file1.txt** ``` User1 logged in from <EMAIL> User2 reported an error from <EMAIL> ``` **file2.txt** ``` Contact support at <EMAIL> Email <EMAIL> for inquiries ``` Constraints: - Assume all files are UTF-8 encoded. - Replace only valid email addresses. Do not alter other text. - Email addresses may appear multiple times in a single line or across multiple lines. Starter Code: ```python import fileinput import re def anonymize_emails(files): email_pattern = re.compile(r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\') with fileinput.input(files=files, inplace=True, backup=\'.bak\') as f: for line in f: # Replace email addresses with placeholder anonymized_line = email_pattern.sub(\'<EMAIL>\', line) print(anonymized_line, end=\'\') # print() statements might be redirected to the input files. # Example usage: # anonymize_emails([\'file1.txt\', \'file2.txt\']) ``` Explanation: 1. **Regex Pattern**: Compile a regex pattern to match email addresses. 2. **File Input & Context Management**: Use `fileinput.input` with `inplace=True` to update files in-place. The `backup=\'.bak\'` option keeps a backup of the original files. 3. **Line Processing**: Iterate through each line in the files, replace email addresses, and print (which is redirected to the input file due to `inplace=True`). Note: To test this function, create sample text files with the expected structure and confirm that emails are replaced by `<EMAIL>` after running the function.","solution":"import fileinput import re def anonymize_emails(files): Processes the input files and replaces any email addresses found with the placeholder <EMAIL>. Args: files (list of str): List of file names to be processed. email_pattern = re.compile(r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\') with fileinput.input(files=files, inplace=True, backup=\'.bak\') as f: for line in f: anonymized_line = email_pattern.sub(\'<EMAIL>\', line) print(anonymized_line, end=\'\') # Redirect print output to the file # Example usage: # anonymize_emails([\'file1.txt\', \'file2.txt\'])"},{"question":"# Custom Descriptor Implementation You are asked to implement custom descriptor classes in Python that manage the attributes and methods of a class dynamically. The task will demonstrate your understanding of descriptor objects and their behavior in Python. Task Description: 1. Implement a custom descriptor `CustomProperty` that mimics the behavior of a standard property with getter and setter methods. 2. Implement a class `Person` that uses `CustomProperty` for its attributes. 3. Additionally, implement a method `is_data_descriptor` to check if a given attribute in the `Person` class is data or method descriptor. Requirements: - `CustomProperty` should be able to define a getter and a setter for an attribute. - The `Person` class should have attributes `name` and `age`, both managed by instances of `CustomProperty`. - The `Person` class should have a method `greet` which returns a greeting message using the person\'s name. - `is_data_descriptor` method should accept an attribute name and return `True` if it is a data descriptor, and `False` if it is a method descriptor. Input: - No direct input to the functions. - Instances of the `Person` class can be created and their attributes should be assigned values. Output: - The getter for `name` and `age` should accurately return the assigned values. - The `greet` method should return a string \\"Hello, {name}!\\" where `{name}` is the attribute value of `name`. - `is_data_descriptor` should return `True` for `name` and `age`, and `False` for `greet`. Example: ```python class CustomProperty: # Your implementation here class Person: name = CustomProperty() age = CustomProperty() def __init__(self, name, age): self.name = name self.age = age def greet(self): return f\\"Hello, {self.name}!\\" @staticmethod def is_data_descriptor(attribute_name): # Your implementation here # Example usage p = Person(\'Alice\', 30) print(p.name) # Output: Alice print(p.age) # Output: 30 print(p.greet()) # Output: Hello, Alice! print(Person.is_data_descriptor(\'name\')) # Output: True print(Person.is_data_descriptor(\'greet\')) # Output: False ``` Constraints: - You must use descriptors to manage `name` and `age`. - Use the information from the provided documentation to check for data descriptors in the `is_data_descriptor` method.","solution":"class CustomProperty: def __init__(self, fget=None, fset=None): self.fget = fget self.fset = fset self.__name__ = fget.__name__ if fget else None def __get__(self, instance, owner): if instance is None: return self if self.fget is None: raise AttributeError(\\"unreadable attribute\\") return self.fget(instance) def __set__(self, instance, value): if self.fset is None: raise AttributeError(\\"can\'t set attribute\\") self.fset(instance, value) def setter(self, fset): self.fset = fset return self class Person: def __init__(self, name, age): self._name = name self._age = age @CustomProperty def name(self): return self._name @name.setter def name(self, value): self._name = value @CustomProperty def age(self): return self._age @age.setter def age(self, value): self._age = value def greet(self): return f\\"Hello, {self.name}!\\" @staticmethod def is_data_descriptor(attribute_name): attr = getattr(Person, attribute_name, None) return ( hasattr(attr, \'__get__\') and hasattr(attr, \'__set__\') if attr else False )"},{"question":"# Advanced Python Object Management and Manipulation Background You have been provided with an overview of Python\'s object system, which includes topics such as allocating objects on the heap, accessing attributes of extension types, and managing various kinds of object structures (Number, Mapping, Sequence, Buffer, Async). You will use this knowledge to create and manage a custom object type. Task Create a custom Python class named `CustomList` which mimics some behavior of Python\'s built-in list type but includes additional functionalities: 1. Ability to track the minimum and maximum values at any point. 2. Mechanism to store the count of how many times specific values exist in the list. Requirements 1. Implement the class `CustomList` with the following methods: - `__init__(self)`: Initializes an empty list. - `add(self, value)`: Adds a value to the list and updates the min, max, and occurrence count. - `remove(self, value)`: Removes a value from the list, updates the min, max, and occurrence count. Raises `ValueError` if the value is not present. - `get_min(self)`: Returns the minimum value currently in the list. Raises `ValueError` if the list is empty. - `get_max(self)`: Returns the maximum value currently in the list. Raises `ValueError` if the list is empty. - `get_count(self, value)`: Returns the number of times a value occurs in the list. Returns 0 if the value is not present. 2. You should handle: - Efficient addition and deletion of elements. - Maintaining the min, max, and count values accurately. - Proper error handling for invalid operations. 3. The methods should have the following time complexities: - `add`: O(1) for updating min, max and occurrence count. - `remove`: O(n) for updating min, max and occurrence count after deletion. - `get_min`, `get_max`, `get_count`: O(1). Example Usage ```python cl = CustomList() cl.add(3) cl.add(1) cl.add(2) cl.add(1) print(cl.get_min()) # Output: 1 print(cl.get_max()) # Output: 3 print(cl.get_count(1)) # Output: 2 cl.remove(1) print(cl.get_min()) # Output: 1 print(cl.get_max()) # Output: 3 print(cl.get_count(1)) # Output: 1 cl.remove(1) print(cl.get_min()) # Output: 2 print(cl.get_max()) # Output: 3 print(cl.get_count(1)) # Output: 0 try: cl.remove(4) # This should raise a ValueError except ValueError as e: print(e) # Output: Value not found. ``` Constraints - You are allowed to use only basic Python data structures (lists, dictionaries, etc.). - The list should not use any external libraries to perform these operations. - Ensure your solution is efficient and handles edge cases appropriately.","solution":"class CustomList: def __init__(self): self.data = [] self.min = None self.max = None self.counts = {} def add(self, value): self.data.append(value) if self.min is None or value < self.min: self.min = value if self.max is None or value > self.max: self.max = value if value in self.counts: self.counts[value] += 1 else: self.counts[value] = 1 def remove(self, value): if value not in self.data: raise ValueError(\\"Value not found.\\") self.data.remove(value) self.counts[value] -= 1 if self.counts[value] == 0: del self.counts[value] if value == self.min or value == self.max: if self.data: self.min = min(self.data) self.max = max(self.data) else: self.min = None self.max = None def get_min(self): if self.min is None: raise ValueError(\\"List is empty.\\") return self.min def get_max(self): if self.max is None: raise ValueError(\\"List is empty.\\") return self.max def get_count(self, value): return self.counts.get(value, 0)"},{"question":"As an advanced Python programmer, you are required to create a tool that automates the process of bundling a given Python project directory into an executable zip archive. This tool must also add a specified Python interpreter shebang line and handle potential compression of the archive. # Task Implement a Python function `bundle_project_to_zipapp` which accepts the following arguments: - `source_dir` (str): The path to the directory containing the Python project. - `output_file` (str): The path where the resulting `.pyz` file should be saved. - `interpreter` (str, optional): The path to the interpreter to be embedded as a shebang line at the start of the archive. If `None`, no interpreter line should be added. - `main_function` (str): The main callable to be executed when the archive runs, in the format \\"pkg.module:callable\\". - `compress` (bool, optional): Whether to compress the files within the archive. Default is `False`. The function should perform the following: 1. Validate the `source_dir` and `output_file` paths. 2. Ensure there is a `__main__.py` file or create one based on the `main_function` provided. 3. Use the `zipapp.create_archive()` function to create the archive with the specified options. 4. Handle any exceptions that occur and provide meaningful error messages. # Expected Input and Output Input: - `source_dir`: A string representing the path to the directory containing the Python project (e.g., \\"my_project\\"). - `output_file`: A string representing the path where the `.pyz` file should be saved (e.g., \\"my_project.pyz\\"). - `interpreter`: A string representing the interpreter path (e.g., \\"/usr/bin/env python3\\") or `None`. - `main_function`: A string in the format \\"pkg.module:callable\\". - `compress`: A boolean indicating whether to compress the archive. Output: - A `.pyz` file created at the path specified by `output_file`. # Constraints: - The `source_dir` must be a valid directory containing Python code. - The `output_file` must not already exist. - The `main_function` must be in the format \\"pkg.module:callable\\" and valid within the provided `source_dir`. # Example Usage ```python def bundle_project_to_zipapp(source_dir: str, output_file: str, interpreter: Optional[str], main_function: str, compress: bool = False): # Your implementation here # Example usage bundle_project_to_zipapp(\\"my_project\\", \\"my_project.pyz\\", \\"/usr/bin/env python3\\", \\"myapp:main\\", compress=True) ``` # Notes: - Make sure to handle any potential edge cases, such as invalid paths or missing files. - You may assume the standard library `zipapp` is available and does not need to be installed separately.","solution":"import os import zipapp def create_main_py(source_dir, main_function): main_py_path = os.path.join(source_dir, \\"__main__.py\\") module, func = main_function.split(\\":\\") main_py_content = f if __name__ == \'__main__\': from {module} import {func} {func}() with open(main_py_path, \\"w\\") as f: f.write(main_py_content) def bundle_project_to_zipapp(source_dir: str, output_file: str, interpreter: str, main_function: str, compress: bool = False): Bundles a given Python project directory into an executable zip archive. Args: source_dir (str): Path to the directory containing the Python project. output_file (str): Path to where the resulting `.pyz` file should be saved. interpreter (str): The Python interpreter shebang line. main_function (str): The main callable in \\"pkg.module:callable\\" format. compress (bool, optional): Whether to compress the archive. Defaults to False. if not os.path.isdir(source_dir): raise ValueError(f\\"The source directory \'{source_dir}\' does not exist or is not a directory.\\") if os.path.exists(output_file): raise ValueError(f\\"The output file \'{output_file}\' already exists.\\") if \\":\\" not in main_function: raise ValueError(f\\"The main function \'{main_function}\' is not in the format \'pkg.module:callable\'.\\") create_main_py(source_dir, main_function) zipapp.create_archive(source_dir, output_file, interpreter=interpreter, main=None, compressed=compress)"},{"question":"<|Analysis Begin|> The provided documentation gives an overview of two sets of Intermediate Representations (IRs) in PyTorch 2.0: Core Aten IR and Prims IR. Core Aten IR comprises a core subset of aten operators used to compose other operators without using `inplace` or `_out` variants and does not decompose operations into explicit type promotion and broadcasting. Prims IR is a lower-level opset that explicitly handles type promotion and broadcasting operations, allowing backend compiler interfacing. Key points: 1. Core Aten IR reuses existing aten ops without decomposition into type promotions and broadcasting. 2. Prims IR includes more granular primitive operations for explicit type promotions and broadcasting. 3. Both IRs are still under development, with more ops to be added in the future. Considering this, a challenging and comprehensive question would involve creating a custom aten operator using Core Aten IR, understanding the composition of operations without explicit decomposed types and broadcasting. <|Analysis End|> <|Question Begin|> # PyTorch Custom Operator Implementation Objective: Implement a custom PyTorch operator using the Core Aten IR that performs an advanced tensor operation without using in-place or `_out` variants and does not explicitly handle type promotion and broadcasting. Task: Write a function `custom_operator` that takes two tensors as input and performs the following steps: 1. Element-wise multiplication of the two input tensors. 2. Addition of a constant scalar value `5` to each element of the resultant tensor. 3. Compute the element-wise natural logarithm of the resulting tensor. # Function Signature: ```python import torch def custom_operator(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` # Constraints: 1. The input tensors will be of the same shape and of floating point types. 2. The function should leverage Core Aten IR. 3. Explicit type promotion or broadcasting operations should not be used (i.e., `prims.convert_element_type` or `prims.broadcast_in_dim`). # Input: - `tensor1`: A PyTorch tensor of shape `(N, M)` and dtype `torch.float32`. - `tensor2`: A PyTorch tensor of shape `(N, M)` and dtype `torch.float32`. # Output: - A PyTorch tensor of shape `(N, M)` where each element is the result of applying the custom operation sequence described above. # Example: ```python tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) tensor2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) result = custom_operator(tensor1, tensor2) print(result) # Expected output: tensor([[2.0794, 2.6391], [3.0445, 3.3916]]) ``` # Note: - Use PyTorch functions and avoid using in-place operations. - The function should demonstrate understanding and application of PyTorch\'s Core Aten IR principles.","solution":"import torch def custom_operator(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform element-wise multiplication, addition of scalar 5, and element-wise natural logarithm on the input tensors. Args: tensor1: A PyTorch tensor of shape (N, M) and dtype torch.float32. tensor2: A PyTorch tensor of shape (N, M) and dtype torch.float32. Returns: A PyTorch tensor of shape (N, M) after applying the operations. # Ensure the tensors are of the same shape assert tensor1.shape == tensor2.shape, \\"Input tensors must have the same shape\\" # 1. Element-wise multiplication mul_result = torch.mul(tensor1, tensor2) # 2. Addition of scalar 5 add_result = torch.add(mul_result, 5) # 3. Compute the element-wise natural logarithm log_result = torch.log(add_result) return log_result"},{"question":"You are provided with the toy dataset functions from the scikit-learn library. Your task is to use the `load_wine` dataset to perform the following: 1. Load the Wine dataset using `sklearn.datasets.load_wine`. 2. Preprocess the data to standardize the feature values. 3. Split the dataset into training and testing sets with a ratio of 70% training and 30% testing. 4. Train a logistic regression model on the training set. 5. Evaluate the accuracy of the model on the testing set. Instructions: 1. **Load the Wine Dataset:** - Use the `load_wine` function from `sklearn.datasets` to load the dataset. 2. **Preprocess the Data:** - Standardize the feature values using `StandardScaler` from `sklearn.preprocessing`. 3. **Split the Data:** - Split the dataset into training (70%) and testing (30%) sets using `train_test_split` from `sklearn.model_selection`. 4. **Train the Model:** - Use `LogisticRegression` from `sklearn.linear_model` to train a model on the training set. 5. **Evaluate the Model:** - Calculate the accuracy of the model on the testing set using the `accuracy_score` from `sklearn.metrics`. Input Format - No input from the user is required. Output Format - Print the accuracy of the model on the testing set. Constraints - Use only the scikit-learn library for implementing logistic regression. - You should not use any other machine learning libraries. - Performance requirements: The model training and prediction should complete within a reasonable time frame for the toy dataset. Here is a code template to get you started: ```python import numpy as np from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load the Wine dataset data = load_wine() X = data.data y = data.target # Standardize the feature values scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Train a logistic regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Evaluate the model on the testing set y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy of the Logistic Regression model: {accuracy:.2f}\') ``` Using this template, fill in the missing parts and compute the accuracy as required.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def wine_logistic_regression_accuracy(): # Load the Wine dataset data = load_wine() X = data.data y = data.target # Standardize the feature values scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Train a logistic regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Evaluate the model on the testing set y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Converting a PyTorch Model to TorchScript **Objective:** You are given a simple neural network implemented using PyTorch. Your task is to write a function to convert this network to TorchScript using `torch.jit.script`. Additionally, ensure the network avoids the unsupported constructs and accounts for the differences in functions and schemas between PyTorch and TorchScript as highlighted in the provided documentation. **Instructions:** 1. Implement a simple neural network class `SimpleNet` using PyTorch. 2. Write a function `convert_to_torchscript` that converts the `SimpleNet` model to TorchScript. 3. Ensure the conversion handles the unsupported constructs and schema differences as outlined in the provided documentation. For instance, avoid using `torch.nn.RNN` and ensure tensor creation functions are compatible with TorchScript. **Network Specifications:** - The network should consist of: - An input layer with 10 neurons. - One hidden layer with 20 neurons. - An output layer with 5 neurons. - Use ReLU activations for the hidden layer. **Function Specifications:** - `SimpleNet` class: - Input: `self`, with an internal method `forward` that takes a tensor input. - Output: A tensor output after passing through the network layers. - `convert_to_torchscript` function: - Input: A `SimpleNet` model instance. - Output: A TorchScript representation of the model using `torch.jit.script`. **Example Usage:** ```python import torch import torch.nn as nn import torch.jit class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x def convert_to_torchscript(model): # Convert to torchscript scripted_model = torch.jit.script(model) return scripted_model # Create the model model = SimpleNet() # Convert to TorchScript scripted_model = convert_to_torchscript(model) # Verify print(scripted_model) ``` **Answer:** Ensure the provided implementation aligns with the documentation details and successfully converts the PyTorch model to a TorchScript model without running into unsupported construct issues.","solution":"import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x def convert_to_torchscript(model): # Convert to torchscript scripted_model = torch.jit.script(model) return scripted_model # Example Usage: # Create the model model = SimpleNet() # Convert to TorchScript scripted_model = convert_to_torchscript(model) # Verify print(scripted_model)"},{"question":"**Objective:** To assess studentâ€™s ability to train a binary classifier and optimize the decision threshold based on a custom evaluation metric using scikit-learn. **Problem Statement:** You are tasked with developing a model to predict whether a transaction is fraudulent or not. The default threshold (0.5) for prediction is found to be suboptimal as your business requires to maximize the recall of the fraudulent transactions. Your task is to: 1. Train a RandomForestClassifier. 2. Optimize the prediction threshold to maximize recall using `TunedThresholdClassifierCV`. 3. Test the model on a separate dataset and output the classification report before and after threshold tuning. **Inputs:** - A dataset of transactions with the following features: - `features`: A 2D array of shape (n_samples, n_features) representing the transaction features. - `labels`: A 1D array of shape (n_samples) representing the binary labels (0 for non-fraudulent, 1 for fraudulent). **Output:** - A classification report (before and after threshold tuning) including precision, recall, and F1-score. **Constraints:** - Use RandomForestClassifier from sklearn for training. - Use `TunedThresholdClassifierCV` for threshold tuning. - You will split the provided dataset into training and testing sets (80% train, 20% test). **Implementation Details:** 1. Splitting data for training and testing. 2. Training a RandomForestClassifier on the training data. 3. Using `TunedThresholdClassifierCV` to adjust the decision threshold for maximizing recall on validation set. 4. Printing classification reports before and after tuning the threshold. Your final code should look like this: ```python import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV from sklearn.metrics import classification_report, make_scorer, recall_score # Dummy data generation (replace it with real dataset) np.random.seed(42) features = np.random.rand(1000, 20) labels = np.random.randint(0, 2, 1000) # Split the data X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42) # Train the base classifier base_model = RandomForestClassifier(random_state=42) base_model.fit(X_train, y_train) # Get classification report before threshold tuning y_pred_before = base_model.predict(X_test) print(\\"Before threshold tuning:\\") print(classification_report(y_test, y_pred_before)) # Tune the decision threshold to maximize recall scorer = make_scorer(recall_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Get classification report after threshold tuning y_pred_after = tuned_model.predict(X_test) print(\\"After threshold tuning:\\") print(classification_report(y_test, y_pred_after)) ``` Make sure to substitute the dummy data generation part with your actual dataset. **Note:** Use the provided functions and classes strictly as mentioned above. Do not use any other method/package for threshold tuning.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, make_scorer, recall_score from sklearn.calibration import CalibratedClassifierCV # Dummy data generation (replace it with real dataset) np.random.seed(42) features = np.random.rand(1000, 20) labels = np.random.randint(0, 2, 1000) # Split the data X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42) # Train the base classifier base_model = RandomForestClassifier(random_state=42) base_model.fit(X_train, y_train) # Get classification report before threshold tuning y_pred_before = base_model.predict(X_test) print(\\"Before threshold tuning:\\") print(classification_report(y_test, y_pred_before)) # Tune the decision threshold to maximize recall using CalibratedClassifierCV calibrated_clf = CalibratedClassifierCV(base_model, cv=\'prefit\', method=\'sigmoid\') calibrated_clf.fit(X_train, y_train) # Define custom prediction function with tuned threshold def predict_with_threshold(clf, X, threshold=0.1): proba = clf.predict_proba(X) return (proba[:, 1] >= threshold).astype(int) # Get classification report after threshold tuning y_pred_after = predict_with_threshold(calibrated_clf, X_test) print(\\"After threshold tuning:\\") print(classification_report(y_test, y_pred_after))"},{"question":"Objective: You are given a dataset containing scores of various models on different tasks. Your task is to create a Seaborn plot that effectively visualizes the average scores of these models along with their names, employing advanced customization techniques. Dataset: Consider you have the `glue` dataset accessible through the seaborn library. The dataset needs to be transformed and visualized as described below. Instructions: 1. **Data Loading and Transformation**: - Load the `glue` dataset using `seaborn.load_dataset(\\"glue\\")`. - Pivot the dataset such that the \\"Model\\" and \\"Encoder\\" form the index, \\"Task\\" forms columns, and \\"Score\\" are the values. - Compute the average score for each model and store it in a new column named `\\"Average\\"`. - Sort the dataset by the `\\"Average\\"` score in descending order. 2. **Plot Customization**: - Create a bar plot showing the average scores of the models along with the names of each model on the bars. - The bars should be horizontal, and the text color should contrast with the bar color for visibility. - Control the text alignment and appearance for an optimal presentation. Expected Function Implementation: ```python def visualize_model_scores(): import pandas as pd import seaborn as sns import seaborn.objects as so # Load and transform the dataset glue = ( sns.load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create a bar plot with text annotations p = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\")) ) # Fine-tune the appearance and show the plot p.show() # Call the function to visualize the plot visualize_model_scores() ``` Constraints: - You must use the seaborn and matplotlib libraries for plotting. - The text color on the bars should be white, and the alignment should ensure that the text does not overlap the plot boundaries. Performance: Consider optimization techniques if the dataset is large to ensure the pivot and transformation steps perform efficiently.","solution":"def visualize_model_scores(): import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load and transform the dataset glue = ( sns.load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Reset the index to use \'Model\' and \'Encoder\' as columns for plotting glue = glue.reset_index() # Create a bar plot with text annotations plt.figure(figsize=(12, 8)) barplot = sns.barplot(x=\\"Average\\", y=\\"Model\\", data=glue, palette=\\"viridis\\") # Annotate the bars with the average score for index, value in enumerate(glue[\\"Average\\"]): barplot.text(value, index, f\'{value}\', color=\'white\', ha=\\"right\\" if value > 0 else \'left\', va=\\"center\\") # Set labels and title plt.xlabel(\\"Average Score\\") plt.ylabel(\\"Model\\") plt.title(\\"Average Scores of Models on GLUE Tasks\\") # Adjust plotting area to fit long model names plt.tight_layout() # Show the plot plt.show() # Call the function to visualize the plot visualize_model_scores()"},{"question":"**Challenging Python Assessment Question** # Problem Statement You are building a Python REPL (Read-Eval-Print Loop) emulator. One of the primary functions of a REPL is to determine if the user has entered a complete and valid piece of Python code. You need to use the `codeop` module to create a class `REPLSimulator` to handle this functionality. # Your Task: Implement the class `REPLSimulator` with the following methods: 1. **`__init__(self)`**: - Initializes an instance of `CommandCompiler` to keep track of code compilations. 2. **`add_code(self, code: str) -> bool`**: - Takes a string `code` representing a line or multiple lines of Python code. - Uses `self.compiler()` to attempt to compile the code. - Returns `True` if the code is a complete and valid Python statement, `False` if it\'s an incomplete but potentially valid statement, and raises an exception if there\'s a syntax error or other compilation issue. 3. **`get_code_object(self, code: str)`**: - Takes a string `code` as input. - Uses `self.compiler()` to compile the code. - Returns the compiled code object if the code is a complete and valid statement. - Returns `None` if the code is incomplete but potentially valid. # Example Usage: ```python repl = REPLSimulator() # Adding incomplete code result = repl.add_code(\\"for i in range(10):\\") print(result) # Output: False # Adding complete code result = repl.add_code(\\" print(i)\\") print(result) # Output: True # Getting code object code_object = repl.get_code_object(\\"print(\'Hello, World!\')\\") print(code_object) # Output: <code object <module> at ...> ``` # Constraints: - The input code string should not exceed 1000 characters. - You should handle all types of exceptions that `compile_command()` may raise, including `SyntaxError`, `OverflowError`, and `ValueError`. # Performance: - Your solution should effectively manage the memory used by compiled code objects. - Ensure the solution remains performant for multiple rapid calls to `add_code` and `get_code_object`. # Submission: Submit your `REPLSimulator` class implementation. Ensure your code is well-documented and includes any assumptions you\'ve made.","solution":"import codeop class REPLSimulator: def __init__(self): self.compiler = codeop.CommandCompiler() def add_code(self, code: str) -> bool: try: code_obj = self.compiler(code) return code_obj is not None except (SyntaxError, OverflowError, ValueError): raise def get_code_object(self, code: str): try: code_obj = self.compiler(code) return code_obj except (SyntaxError, OverflowError, ValueError): return None"},{"question":"File Organizer and Report Generator # Objective Design and implement a Python program that organizes files in a given directory based on their extensions and generates a summary report. The program should utilize features from the provided `python310` documentation, specifically focusing on file operations, command line argument parsing, regular expressions, date-time manipulation, and testing. # Requirements 1. **Functionality:** - Create an organized directory structure where files are moved into subdirectories named after their extensions. - Generate a summary report containing the count of each type of file and a list of files in each category. - Print the summary report to the console and additionally save it as a `summary.txt` file in the target directory. 2. **Command Line Arguments:** - The program should accept two command line arguments: 1. `--input-dir` (required): The path to the directory containing files to be organized. 2. `--target-dir` (optional): The path to the directory where the organized files should be saved. If not provided, it defaults to `organized_files` in the current working directory. 3. `--include-hidden` (optional): A flag to include hidden files (default is to exclude hidden files). # Implementation Details 1. **Directory Management:** - Use the `os` and `shutil` modules to handle file operations. - Use the `glob` module for handling wildcards, if necessary. 2. **Command Line Parsing:** - Use the `argparse` module for handling command line arguments. 3. **Regular Expressions:** - Use the `re` module for filtering and processing file names when required. 4. **Date and Time:** - Use the `datetime` module for adding timestamps to the summary report. 5. **Testing:** - Write and include unit tests utilizing the `unittest` module to verify the correctness of your functions. - Add `doctest` examples for key functions. 6. **Summary Report:** - The report should contain: - Timestamp of when the organization was performed. - Count of each file type. - List of files under each file type. - Total number of files processed. # Example Usage ```bash python file_organizer.py --input-dir path/to/input --target-dir path/to/output --include-hidden ``` # Output Example ``` Summary Report - [2023-06-15 12:34:56] -------------------------------------- Total files processed: 100 File Types: 1. .txt (25 files) - file1.txt - file2.txt ... 2. .jpg (40 files) - image1.jpg - image2.jpg ... 3. .pdf (15 files) - document1.pdf - report2.pdf ... 4. Others (20 files) - script.py - archive.zip ... Summary report saved to: /path/to/output/summary.txt ``` # Constraints - You are not allowed to use third-party libraries outside of the Python Standard Library. - The program should handle edge cases gracefully, such as invalid input directory, lack of permissions, and empty directories.","solution":"import os import shutil import argparse import re from datetime import datetime import glob def organize_files(input_dir, target_dir=\'organized_files\', include_hidden=False): if not os.path.isdir(input_dir): raise ValueError(\\"Input directory does not exist.\\") if not os.path.exists(target_dir): os.makedirs(target_dir) summary = { \'total_files\': 0, \'file_types\': {} } for root, dirs, files in os.walk(input_dir): for filename in files: if not include_hidden and filename.startswith(\'.\'): continue file_ext = os.path.splitext(filename)[1].lower() if file_ext == \'\': file_ext = \'no_extension\' summary[\'total_files\'] += 1 if file_ext not in summary[\'file_types\']: summary[\'file_types\'][file_ext] = [] src_path = os.path.join(root, filename) dest_dir = os.path.join(target_dir, file_ext.lstrip(\'.\')) if not os.path.exists(dest_dir): os.makedirs(dest_dir) shutil.move(src_path, os.path.join(dest_dir, filename)) summary[\'file_types\'][file_ext].append(filename) create_summary_report(target_dir, summary) return summary def create_summary_report(target_dir, summary): report_filename = os.path.join(target_dir, \'summary.txt\') timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') with open(report_filename, \'w\') as report_file: report_file.write(f\\"Summary Report - {timestamp}n\\") report_file.write(\\"-\\" * 40 + \\"n\\") report_file.write(f\\"Total files processed: {summary[\'total_files\']}nn\\") report_file.write(\\"File Types:n\\") for file_ext, files in summary[\'file_types\'].items(): report_file.write(f\\"{file_ext} ({len(files)} files)n\\") for file in files: report_file.write(f\\" - {file}n\\") report_file.write(\\"n\\") print(f\\"Summary report saved to: {report_filename}\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Organize files based on their extensions and generate a summary report.\\") parser.add_argument(\\"--input-dir\\", required=True, help=\\"Path to the directory containing files to be organized.\\") parser.add_argument(\\"--target-dir\\", help=\\"Path to the directory where the organized files should be saved. Defaults to \'organized_files\' in the current working directory.\\") parser.add_argument(\\"--include-hidden\\", action=\'store_true\', help=\\"Include hidden files in the organization process. Default is to exclude hidden files.\\") args = parser.parse_args() target_dir = args.target_dir if args.target_dir else \'organized_files\' include_hidden = args.include_hidden organize_files(args.input_dir, target_dir, include_hidden)"},{"question":"Pandas Categorical Data Manipulation # Objective Your task is to demonstrate your understanding of pandas categorical data types by performing a series of manipulations on a given DataFrame. # Problem Statement Given the following DataFrame `df`: ```python import pandas as pd import numpy as np data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"City\\": [\\"New York\\", \\"Los Angeles\\", \\"New York\\", \\"Chicago\\", \\"New York\\"], \\"Score\\": [85, 90, 78, 95, 88] } df = pd.DataFrame(data) ``` Perform the following operations: 1. Convert the `City` column to a categorical type. 2. Create a new categorical column `Performance` based on the `Score` column with the following categories: - \\"Low\\" for scores less than 80, - \\"Medium\\" for scores between 80 and 90, - \\"High\\" for scores above 90. 3. Rename the categories in the `Performance` column to: - \\"Low\\" -> \\"Below Average\\" - \\"Medium\\" -> \\"Average\\" - \\"High\\" -> \\"Above Average\\" 4. Add a new category \\"Excellent\\" to the `Performance` column without modifying the existing values. 5. Remove the \\"Below Average\\" category and replace these values with `NaN`. 6. Sort the DataFrame first by `City` (in the order: \\"Chicago\\", \\"Los Angeles\\", \\"New York\\") and then by `Performance` in ascending order. # Input - The DataFrame `df` as shown above. # Output - The modified DataFrame after performing the above operations. # Constraints - Use pandas categorical data type functionalities to perform the manipulations. # Example ```python import pandas as pd import numpy as np # Initial DataFrame data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"City\\": [\\"New York\\", \\"Los Angeles\\", \\"New York\\", \\"Chicago\\", \\"New York\\"], \\"Score\\": [85, 90, 78, 95, 88] } df = pd.DataFrame(data) # Expected operations result # Create the `City` column as categorical df[\\"City\\"] = df[\\"City\\"].astype(\\"category\\") # Create `Performance` column performance_labels = [\\"Low\\", \\"Medium\\", \\"High\\"] df[\\"Performance\\"] = pd.cut(df[\\"Score\\"], bins=[0, 80, 90, 100], labels=performance_labels, include_lowest=True) # Rename categories in `Performance` df[\\"Performance\\"] = df[\\"Performance\\"].cat.rename_categories({ \\"Low\\": \\"Below Average\\", \\"Medium\\": \\"Average\\", \\"High\\": \\"Above Average\\" }) # Add new category \\"Excellent\\" df[\\"Performance\\"] = df[\\"Performance\\"].cat.add_categories([\\"Excellent\\"]) # Remove category \\"Below Average\\" and replace with NaN df[\\"Performance\\"] = df[\\"Performance\\"].cat.remove_categories([\\"Below Average\\"]) # Sort DataFrame by `City` and `Performance` city_order = [\\"Chicago\\", \\"Los Angeles\\", \\"New York\\"] df[\\"City\\"] = df[\\"City\\"].cat.reorder_categories(city_order, ordered=True) df = df.sort_values(by=[\\"City\\", \\"Performance\\"]) print(df) ``` The resulting DataFrame should look something like this: ``` Name City Score Performance 3 David Chicago 95 Above Average 1 Bob Los Angeles 90 Average 0 Alice New York 85 Average 4 Eve New York 88 Average 2 Charlie New York 78 NaN ``` # Note Make sure to handle cases where categorical operations like renaming or removing a category can cause errors if not done properly.","solution":"import pandas as pd # Initial DataFrame data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"City\\": [\\"New York\\", \\"Los Angeles\\", \\"New York\\", \\"Chicago\\", \\"New York\\"], \\"Score\\": [85, 90, 78, 95, 88] } df = pd.DataFrame(data) # Step 1: Convert the `City` column to a categorical type df[\\"City\\"] = df[\\"City\\"].astype(\\"category\\") # Step 2: Create a new categorical column `Performance` based on the `Score` column performance_labels = [\\"Low\\", \\"Medium\\", \\"High\\"] df[\\"Performance\\"] = pd.cut(df[\\"Score\\"], bins=[0, 80, 90, 100], labels=performance_labels, include_lowest=True) # Step 3: Rename the categories in the `Performance` column df[\\"Performance\\"] = df[\\"Performance\\"].cat.rename_categories({ \\"Low\\": \\"Below Average\\", \\"Medium\\": \\"Average\\", \\"High\\": \\"Above Average\\" }) # Step 4: Add a new category \\"Excellent\\" to the `Performance` column df[\\"Performance\\"] = df[\\"Performance\\"].cat.add_categories([\\"Excellent\\"]) # Step 5: Remove the \\"Below Average\\" category and replace these values with NaN df[\\"Performance\\"] = df[\\"Performance\\"].cat.remove_categories([\\"Below Average\\"]) # Step 6: Sort the DataFrame first by `City` and then by `Performance` city_order = [\\"Chicago\\", \\"Los Angeles\\", \\"New York\\"] df[\\"City\\"] = df[\\"City\\"].cat.reorder_categories(city_order, ordered=True) df = df.sort_values(by=[\\"City\\", \\"Performance\\"], ascending=[True, True]) df # Let\'s print and inspect the final DataFrame print(df)"},{"question":"Write a Python function to determine if a given IPv4 address is within a specific range of IP addresses, and summarize the network range if it is. **Function Signature**: ```python def is_ip_in_range(start_ip: str, end_ip: str, target_ip: str) -> str: ``` **Parameters**: - `start_ip` (str): A string representing the starting IPv4 address of the range. - `end_ip` (str): A string representing the ending IPv4 address of the range. - `target_ip` (str): A string representing the IPv4 address to be checked. **Returns**: - A string representing the smallest network block that contains the entire range from `start_ip` to `end_ip` and indicates if the `target_ip` is within this range. **Constraints**: - All inputs are valid IPv4 addresses. - The `start_ip` will always be less than `end_ip` in terms of numerical order. **Example**: ```python print(is_ip_in_range(\'192.168.1.0\', \'192.168.1.255\', \'192.168.1.100\')) # Output: \'192.168.1.0/24 contains the ip 192.168.1.100\' ``` ```python print(is_ip_in_range(\'192.168.1.0\', \'192.168.1.255\', \'192.168.2.1\')) # Output: \'192.168.1.0/24 does not contain the ip 192.168.2.1\' ``` # Note: You are encouraged to use the `ipaddress` library to construct a sound and efficient solution. # Explanation: 1. Convert start_ip and end_ip to `IPv4Address`. 2. Use `summarize_address_range` to find the smallest network block containing the given range. 3. Check if `target_ip` is within this network. 4. Return the appropriate message.","solution":"import ipaddress def is_ip_in_range(start_ip: str, end_ip: str, target_ip: str) -> str: Determine if a given IPv4 address is within a specific range of IP addresses, and summarize the network range if it is. start = ipaddress.IPv4Address(start_ip) end = ipaddress.IPv4Address(end_ip) target = ipaddress.IPv4Address(target_ip) # Find the smallest network block that contains the entire range network_range = list(ipaddress.summarize_address_range(start, end)) # Assumption: supposed to find only one network range network = network_range[0] # Check if the target_ip is inside the calculated network range if target in network: return f\'{network} contains the ip {target_ip}\' else: return f\'{network} does not contain the ip {target_ip}\'"},{"question":"# Advanced Python Programming: Asynchronous Task Scheduler Problem Statement You are required to create a simple asynchronous task scheduler using Python\'s coroutine capabilities. The scheduler should allow tasks to be added, started, and run concurrently, using Python\'s `async` and `await` keywords. Requirements 1. Implement a class `TaskScheduler` with the following methods: - `add_task(coro)`: This method should accept a coroutine object and add it to the task list. - `run()`: This method should run all the added tasks concurrently and wait for their completion. 2. Implement a simple coroutine `example_task` that: - Accepts a parameter `n` which is a number of seconds to sleep. - Sleeps for the specified number of seconds using `await asyncio.sleep(n)`. - Prints a message before and after sleeping. 3. Demonstrate the usage of the `TaskScheduler` by: - Creating an instance of `TaskScheduler`. - Adding three `example_task` coroutines with different sleep times (e.g., 1, 2, and 3 seconds). - Running the scheduler to execute these tasks concurrently. Input and Output - There is no input to be provided by the user; the main function should demonstrate the task scheduler. - The output should be the messages printed by the `example_task` coroutines, indicating they have started, slept for the specified time, and completed. Constraints - Use Python 3.5 or later. - Properly handle and await coroutines using `asyncio` module. Example Usage ```python import asyncio class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, coro): # Should add coroutine to the task list self.tasks.append(coro) async def run(self): # Should run all tasks concurrently await asyncio.gather(*self.tasks) async def example_task(n): print(f\\"Task sleeping for {n} seconds\\") await asyncio.sleep(n) print(f\\"Task completed after {n} seconds\\") # Demonstrating the usage of TaskScheduler scheduler = TaskScheduler() scheduler.add_task(example_task(1)) scheduler.add_task(example_task(2)) scheduler.add_task(example_task(3)) # Running the scheduler asyncio.get_event_loop().run_until_complete(scheduler.run()) ``` In this example, the `TaskScheduler` class manages the coroutines, and the `example_task` coroutine simulates an asynchronous task by sleeping for a specified number of seconds. The tasks are run concurrently, demonstrating the asynchronous behavior.","solution":"import asyncio class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, coro): Adds a coroutine to the task list. self.tasks.append(coro) async def run(self): Runs all added tasks concurrently and waits for their completion. await asyncio.gather(*self.tasks) async def example_task(n): A simple coroutine that sleeps for the specified number of seconds. print(f\\"Task sleeping for {n} seconds\\") await asyncio.sleep(n) print(f\\"Task completed after {n} seconds\\")"},{"question":"Objective: Your task is to implement functionality to manipulate the `sys.path` in a Python environment by using the site-specific customizations detailed in the `site` module. Problem Statement: You are required to write a Python function `configure_site_customizations(sys_prefix, pth_files)` that performs the following tasks: 1. Constructs directories based on the provided `sys_prefix` and user customizations as described in the documentation. 2. Adds these directories to `sys.path` if they exist. 3. Reads the provided `.pth` files, adds the valid paths to `sys.path`, skips non-existing paths, and executes any commands specified within the `.pth` files. Function Signature: ```python def configure_site_customizations(sys_prefix: str, pth_files: dict) -> None: pass ``` Input: 1. `sys_prefix` (str): The prefix directory to use for constructing the path. 2. `pth_files` (dict): A dictionary where the keys are file names (str) of `.pth` files and values are lists of strings representing the content. Each string inside the list represents a separate line in the `.pth` file. Output: - The function does not return any value but modifies the `sys.path` global variable. Constraints: - You should not duplicate paths in `sys.path`. - Non-existing paths should not be added to `sys.path`. - Commands in `.pth` files should be executed in the context of the function. Example: ```python import sys # Example usage: sys_prefix = \\"/usr/local\\" pth_files = { \\"foo.pth\\": [ \\"# foo package configuration\\", \\"foo\\", \\"bar\\", \\"bletch\\", \\"import os; print(\'foo.pth executed\')\\" ], \\"bar.pth\\": [ \\"# bar package configuration\\", \\"bar\\" ] } # Assuming the following directories exist: # /usr/local/lib/pythonX.Y/site-packages/foo # /usr/local/lib/pythonX.Y/site-packages/bar configure_site_customizations(sys_prefix, pth_files) # Expected `sys.path` should include: /usr/local/lib/pythonX.Y/site-packages/foo /usr/local/lib/pythonX.Y/site-packages/bar # Output should include: foo.pth executed ``` Notes: - Ensure your implementation includes error handling for invalid `.pth` lines. - Follow the order of precedence and deduplication rules as mentioned in the documentation. - You are free to create any helper functions you deem necessary. - Use the `os` and `sys` modules where appropriate.","solution":"import os import sys def configure_site_customizations(sys_prefix, pth_files): site_packages_dirs = [ os.path.join(sys_prefix, \'lib\', \'python{}.{}/site-packages\'.format(*sys.version_info[:2])), os.path.join(sys.prefix, \'lib\', \'python{}.{}/site-packages\'.format(*sys.version_info[:2])) ] # Ensure directories are added to sys.path if they exist and are not already included for directory in site_packages_dirs: if os.path.isdir(directory) and directory not in sys.path: sys.path.append(directory) # Process each .pth file for filename, lines in pth_files.items(): for line in lines: line = line.strip() if not line or line.startswith(\'#\'): continue if line.startswith(\'import\'): exec(line) else: path = os.path.join(site_packages_dirs[0], line) if os.path.isdir(path) and path not in sys.path: sys.path.append(path)"},{"question":"# Advanced Seaborn Histogram Analysis You have been provided a dataset containing information about various species of penguins. Your task is to write a function that performs advanced histogram analysis on this dataset using the seaborn library. Specifically, you need to visualize the distribution of bill lengths (both `bill_length_mm` and `bill_depth_mm`) for different species across different islands. Your function should: 1. Load the penguins dataset from seaborn. 2. Create a subplot grid where each subplot corresponds to a different island. Each subplot must contain: - A bivariate histogram showing the relationship between `bill_length_mm` and `bill_depth_mm`. - Different colors representing different species. 3. Customize the appearance of these histograms: - Set a custom bin width. - Use a kernel density estimate (KDE) overlay. - Employ a log scale for the x-axis (bill length). 4. Ensure that you add an overall title for the grid of subplots and individual titles for each subplot indicating the island name. 5. Optimize the performance for rendering such plots, considering the size and complexity of the data. # Function Signature ```python def advanced_penguin_histogram(): pass ``` # Expected Output Your function should not return any value but should display a grid of subplots as specified above. # Additional Notes - You may use additional seaborn and matplotlib functionalities as needed. - Make sure that all plots are properly labeled and easy to read. - Consider edge cases where certain islands might have missing or incomplete data. - Use the documentation provided as a guide to implementing different customizations in your plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def advanced_penguin_histogram(): # Load the penguins dataset from seaborn penguins = sns.load_dataset(\'penguins\') # Define the bin width bin_width = 2 # Define the islands for subplots islands = penguins[\'island\'].unique() # Create a grid of subplots num_islands = len(islands) fig, axes = plt.subplots(1, num_islands, figsize=(5*num_islands, 5), sharex=True, sharey=True) # Generate histograms for each island for ax, island in zip(axes, islands): island_data = penguins[penguins[\'island\'] == island] # Joint plot by seaborn to show bivariate histogram with KDE overlay sns.histplot(data=island_data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', multiple=\\"dodge\\", binwidth=bin_width, kde=True, ax=ax) # Log scale for x-axis ax.set_xscale(\'log\') # Set subplot title ax.set_title(f\'Island: {island}\') # Set overall title for the grid of subplots plt.suptitle(\'Penguins Bill Dimensions Across Different Islands\') plt.tight_layout() plt.subplots_adjust(top=0.9) # Adjust title position plt.show()"},{"question":"# Scikit-learn Advanced Data Processing In this task, you are required to implement a function that validates and processes input data using scikit-learn utilities. Specifically, you will: 1. Check the input data to ensure it is a valid 2D array and does not contain any NaNs or infinite values. 2. Perform a truncated Singular Value Decomposition (SVD) on the input data to reduce its dimensions. 3. Ensure that the output data contains the specified number of components and validate the consistency of the result. # Function Signature ```python import numpy as np from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd def validate_and_svd(X, n_components): Validates input data and performs truncated SVD to reduce dimensions. Parameters: X (array-like or sparse matrix) : Input data to be processed. n_components (int) : The number of singular values and vectors to compute. Returns: U (np.ndarray) : Unitary matrix having left singular vectors as columns. S (np.ndarray) : The singular values. VT (np.ndarray) : Unitary matrix having right singular vectors as rows. Raises: ValueError : If the input data is not a valid 2D array or contains NaNs/Infs. # Your implementation here ``` # Requirements 1. **Input Validation**: - Use `check_array` to ensure the input `X` is a 2D array and does not contain NaNs or infinite values. 2. **Dimensionality Reduction**: - Use `randomized_svd` to compute the first `n_components` singular vectors and singular values of the input data. - Ensure the output consists of three components: `U`, `S`, and `VT` which are the results from the SVD computation. 3. **Output Verification**: - Verify that the number of components in the outputs `U`, `S`, and `VT` matches the specified `n_components`. # Example ```python # Example usage X = np.array([[1, 2, 3], [4, 5, 6]]) n_components = 2 U, S, VT = validate_and_svd(X, n_components) print(U) print(S) print(VT) ``` # Constraints - The input matrix `X` must have at least as many rows and columns as the specified `n_components`. - The function should raise a `ValueError` if the input does not meet the validation requirements.","solution":"import numpy as np from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd def validate_and_svd(X, n_components): Validates input data and performs truncated SVD to reduce dimensions. Parameters: X (array-like or sparse matrix) : Input data to be processed. n_components (int) : The number of singular values and vectors to compute. Returns: U (np.ndarray) : Unitary matrix having left singular vectors as columns. S (np.ndarray) : The singular values. VT (np.ndarray) : Unitary matrix having right singular vectors as rows. Raises: ValueError : If the input data is not a valid 2D array or contains NaNs/Infs. # Validate the input array X = check_array(X, ensure_2d=True, allow_nd=False, force_all_finite=True) # Ensure n_components is not greater than the smallest dimension of X if n_components > min(X.shape): raise ValueError(\\"n_components must be less than or equal to the smallest dimension of X\\") # Perform truncated SVD U, S, VT = randomized_svd(X, n_components=n_components) return U, S, VT"},{"question":"Objective Implement a PyTorch-based function that demonstrates your understanding of device management, asynchronous operations, and memory management on an XPU. Task Write a function `matrix_operations_on_xpu` that performs the following tasks: 1. **Device Initialization**: - Check if an XPU device is available. - If available, set the XPU device as the current device. If not, raise an appropriate error. 2. **Matrix Creation and Multiplication**: - Create two random matrices of size (1024 times 1024) on the current device. - Perform matrix multiplication on these matrices. 3. **Asynchronous Operation**: - Perform the matrix multiplication asynchronously using XPU streams. - Synchronize the streams to ensure that the operation completes correctly. - Measure the memory allocated before and after the operation. 4. **Memory Management**: - After synchronization, print the peak memory usage and reset the memory statistics. Function Signature ```python def matrix_operations_on_xpu(): pass ``` Example Output The function does not return any value but should print output similar to the following: ``` XPU device is available and set to current device. Memory allocated before operation: x bytes Memory allocated after operation: y bytes Peak memory usage: z bytes ``` Constraints - Assume that the XPU device supports the operations. - Handle any possible errors, such as device unavailability or memory issues, gracefully. # Notes - Utilize functions and classes from the `torch.xpu` module for completing the tasks. - Make sure to structure the function so that it is clear and easily readable. This coding challenge aims to test your ability to work with PyTorch\'s XPU functionalities, manage devices, and perform efficient computations asynchronously.","solution":"import torch def matrix_operations_on_xpu(): Function to perform synchronous matrix operations on an XPU if available, and showcase memory management associated with it. try: if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(\\"xpu\\") print(\\"XPU device is available and set to current device.\\") # Create two random matrices of size 1024x1024 on the XPU a = torch.randn((1024, 1024), device=device) b = torch.randn((1024, 1024), device=device) # Measure memory allocated before the operation memory_allocated_before = torch.xpu.memory_allocated() # Asynchronous matrix multiplication using XPU streams stream = torch.xpu.Stream(device) with torch.xpu.stream(stream): c = torch.mm(a, b) # Synchronize the stream to ensure completion of operations stream.synchronize() # Measure memory allocated after the operation memory_allocated_after = torch.xpu.memory_allocated() # Get the peak memory usage peak_memory_usage = torch.xpu.max_memory_allocated() # Print memory usage details print(f\\"Memory allocated before operation: {memory_allocated_before} bytes\\") print(f\\"Memory allocated after operation: {memory_allocated_after} bytes\\") print(f\\"Peak memory usage: {peak_memory_usage} bytes\\") # Reset the peak memory statistics torch.xpu.reset_peak_memory_stats() except Exception as e: print(str(e))"},{"question":"You are tasked with designing a machine learning experiment using scikit-learn\'s `datasets` module. Your goal is to compare the performance of a simple linear regression model on both a real-world dataset and a synthetic dataset. # Steps: 1. **Load the Real-World Dataset** - Using the `fetch_california_housing` function from `sklearn.datasets`, load the California housing dataset. - Split the dataset into training and testing sets (70% training, 30% testing). 2. **Generate Synthetic Dataset** - Use `make_regression` from `sklearn.datasets` to create a synthetic dataset with 20 features and 10,000 samples. - Split the synthetic dataset into training and testing sets (70% training, 30% testing). 3. **Preprocessing** - Normalize the features of both the real-world and synthetic datasets using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Training and Evaluation** - Train a `LinearRegression` model from `sklearn.linear_model` on both the real-world and synthetic datasets. - Evaluate the model performance using the mean squared error (MSE) on both testing sets. 5. **Implementation** - Implement the function `compare_datasets_performance` which performs all the steps outlined above. # Expected Function Signature ```python from sklearn.datasets import fetch_california_housing, make_regression from sklearn.linear_model import LinearRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error def compare_datasets_performance(): # Step 1: Load the Real-World Dataset california_housing = fetch_california_housing() X_real, y_real = california_housing.data, california_housing.target X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, y_real, test_size=0.3, random_state=42) # Step 2: Generate Synthetic Dataset X_synth, y_synth = make_regression(n_samples=10000, n_features=20, noise=0.1, random_state=42) X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=0.3, random_state=42) # Step 3: Preprocessing scaler = StandardScaler() X_real_train = scaler.fit_transform(X_real_train) X_real_test = scaler.transform(X_real_test) X_synth_train = scaler.fit_transform(X_synth_train) X_synth_test = scaler.transform(X_synth_test) # Step 4: Model Training and Evaluation model = LinearRegression() # Real-World Dataset Performance model.fit(X_real_train, y_real_train) y_real_pred = model.predict(X_real_test) mse_real = mean_squared_error(y_real_test, y_real_pred) # Synthetic Dataset Performance model.fit(X_synth_train, y_synth_train) y_synth_pred = model.predict(X_synth_test) mse_synth = mean_squared_error(y_synth_test, y_synth_pred) # Return results return mse_real, mse_synth # Call the function and print the results mse_real, mse_synth = compare_datasets_performance() print(f\\"Mean Squared Error on Real-World Dataset: {mse_real}\\") print(f\\"Mean Squared Error on Synthetic Dataset: {mse_synth}\\") ``` Ensure your implementation adheres to the function signature and follows the outlined steps. The final output should be the mean squared error (MSE) for both the real-world and synthetic datasets.","solution":"from sklearn.datasets import fetch_california_housing, make_regression from sklearn.linear_model import LinearRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error def compare_datasets_performance(): # Step 1: Load the Real-World Dataset california_housing = fetch_california_housing() X_real, y_real = california_housing.data, california_housing.target X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, y_real, test_size=0.3, random_state=42) # Step 2: Generate Synthetic Dataset X_synth, y_synth = make_regression(n_samples=10000, n_features=20, noise=0.1, random_state=42) X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=0.3, random_state=42) # Step 3: Preprocessing scaler = StandardScaler() X_real_train = scaler.fit_transform(X_real_train) X_real_test = scaler.transform(X_real_test) X_synth_train = scaler.fit_transform(X_synth_train) X_synth_test = scaler.transform(X_synth_test) # Step 4: Model Training and Evaluation model = LinearRegression() # Real-World Dataset Performance model.fit(X_real_train, y_real_train) y_real_pred = model.predict(X_real_test) mse_real = mean_squared_error(y_real_test, y_real_pred) # Synthetic Dataset Performance model.fit(X_synth_train, y_synth_train) y_synth_pred = model.predict(X_synth_test) mse_synth = mean_squared_error(y_synth_test, y_synth_pred) # Return results return mse_real, mse_synth"},{"question":"You are required to write a Python program that demonstrates advanced handling and manipulation of integer objects and conversions in Python. The challenge is to effectively create integer objects from various data types and implement robust error handling. # Problem Statement **Task:** Write a function `manage_integer_conversion` that accepts a list of mixed data types and returns a dictionary with the following keys and corresponding values: 1. `long_integers`: A list of all valid integers created from long integers without any overflow errors. 2. `unsigned_long_integers`: A list of all valid integers created from unsigned long integers without any overflow errors. 3. `doubles`: A list of all valid integers created from double values. 4. `from_strings`: A list of all valid integers created by parsing strings with a given base (2 <= base <= 36). 5. `conversion_errors`: A list of tuples where each tuple contains the item that caused an error and the error message. **Function Signature:** ```python def manage_integer_conversion(mixed_list: list, base: int) -> dict: pass ``` **Input:** - `mixed_list` (list): A list containing mixed data types (int, float, str, long). - `base` (int): An integer specifying the base for converting string values (2 <= base <= 36). **Output:** - A dictionary containing: * `long_integers` (list): List of integers converted from long integers. * `unsigned_long_integers` (list): List of integers converted from unsigned long integers. * `doubles` (list): List of integers created from double values. * `from_strings` (list): List of integers created by parsing strings. * `conversion_errors` (list): List of tuples containing the item that caused an error and the error message. **Constraints:** - Assume valid data types are given in `mixed_list`. - Handle potential overflows and data conversion inconsistencies gracefully. - The function should not use built-in conversion functions directly but implement the logic to handle each case as closely as described in the documentation. # Example ```python mixed_list = [123, \\"101\\", \\"789\\", 9876543210123456789, -9876543210123456789, 3.14159, \\"bad_string\\"] base = 10 result = manage_integer_conversion(mixed_list, base) print(result) ``` **Expected Output:** ```python { \'long_integers\': [123, 9876543210123456789, -9876543210123456789], \'unsigned_long_integers\': [], \'doubles\': [3], \'from_strings\': [101, 789], \'conversion_errors\': [(\'bad_string\', \'invalid literal for int() with base 10: bad_string\')] } ``` # Hints: 1. Use exception handling to capture conversion errors. 2. Consider using loops and condition checks to differentiate data types and apply appropriate conversion logic. 3. You\'ll need to manage integer parsing based on the provided base, taking into account the constraints on valid bases.","solution":"def manage_integer_conversion(mixed_list: list, base: int) -> dict: result = { \'long_integers\': [], \'unsigned_long_integers\': [], \'doubles\': [], \'from_strings\': [], \'conversion_errors\': [] } for item in mixed_list: try: if isinstance(item, int): # Python\'s int can hold long integers, so we add it directly result[\'long_integers\'].append(item) # For unsigned long, we\'ll check if the integer is non-negative if item >= 0: result[\'unsigned_long_integers\'].append(item) elif isinstance(item, float): # Convert from float (double) to int result[\'doubles\'].append(int(item)) elif isinstance(item, str): try: # Convert string to int with the specified base result[\'from_strings\'].append(int(item, base)) except ValueError as ve: result[\'conversion_errors\'].append((item, str(ve))) except Exception as e: # Any other error result[\'conversion_errors\'].append((item, str(e))) return result"},{"question":"**Question: Custom Color Palette Selection and Application** You are tasked with creating a data visualization using seaborn; however, you need to customize the color palette for your plot. You will write a function that accepts a dataset, a palette style, and an option to use it as a continuous colormap. The function will generate the specified palette, apply it to a scatter plot, and return the plot object. # Function Signature ```python def custom_scatter_plot(data: pd.DataFrame, x_col: str, y_col: str, palette: str, continuous: bool) -> sns.axisgrid.FacetGrid: pass ``` # Inputs 1. `data` (pd.DataFrame): The dataset containing at least the columns specified by `x_col` and `y_col`. 2. `x_col` (str): The name of the column in `data` to be plotted on the x-axis. 3. `y_col` (str): The name of the column in `data` to be plotted on the y-axis. 4. `palette` (str): The palette style to be used. This can be any valid palette name such as \\"pastel\\", \\"husl\\", \\"Set2\\", \\"Spectral\\", \\"flare\\", \\"ch:s=.25,rot=-.25\\", \\"light:#5A9\\", \\"dark:#5A9_r\\", \\"blend:#7AB,#EDA\\", etc. 5. `continuous` (bool): If `True`, the palette will be converted to and applied as a continuous colormap. Otherwise, it will be used as a discrete color palette. # Output - The function should return a `sns.axisgrid.FacetGrid` object representing the scatter plot with the selected color palette applied. # Constraints and Requirements - Use seaborn\'s `color_palette` function to generate the color palette. - Make sure to handle both continuous and discrete color palettes appropriately. - Ensure your function handles invalid palette names gracefully by using default settings. - The function should use seaborn\'s `relplot` to create the scatter plot. # Example Usage ```python import seaborn as sns import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [0, 1, 2, 1, 3, 4, 3, 1, 0, 2] }) # Example call scatter_plot = custom_scatter_plot(data, \'x\', \'y\', \'pastel\', False) scatter_plot.savefig(\\"scatter_plot.png\\") ``` Your task is to implement the function `custom_scatter_plot` as specified above.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_scatter_plot(data: pd.DataFrame, x_col: str, y_col: str, palette: str, continuous: bool) -> sns.axisgrid.FacetGrid: Generates a scatter plot with a custom palette. Parameters: - data: pd.DataFrame. The dataset containing at least the columns specified by `x_col` and `y_col`. - x_col: str. The name of the column in `data` to be plotted on the x-axis. - y_col: str. The name of the column in `data` to be plotted on the y-axis. - palette: str. The palette style to be used. Can be any valid palette name. - continuous: bool. If True, the palette will be used as a continuous colormap. Otherwise, it will be used as a discrete color palette. Returns: - sns.axisgrid.FacetGrid: The scatter plot with the selected color palette applied. try: if continuous: cm = sns.color_palette(palette, as_cmap=True) plot = sns.relplot(data=data, x=x_col, y=y_col, palette=cm) else: colors = sns.color_palette(palette) plot = sns.relplot(data=data, x=x_col, y=y_col, palette=colors) return plot except ValueError: # Use a default palette in case of an error print(\\"Invalid palette name. Using default palette.\\") plot = sns.relplot(data=data, x=x_col, y=y_col, palette=\\"viridis\\") return plot"},{"question":"**Python Coding Assessment Question:** # Question You are tasked with creating a Python script that performs the following functions based on the way it is invoked: 1. If invoked with the `-c` option, it should evaluate the given expression and print the result. 2. If invoked with the `-m` option using a specific module, it should print a custom message along with the module name. 3. If invoked as a normal script with additional arguments, it should print a summary of the arguments passed. 4. Ensure the script handles different encodings specified in the source file. # Requirements 1. **Invocation with `-c` Option**: - Implement logic that captures the expression provided after the `-c` flag, evaluates it, and prints the result. - Example: `python3.10 -c \\"print(2 + 2)\\"` should output `4`. 2. **Invocation with `-m` Option**: - When invoked with the `-m` option and a module name, print \\"Module [module_name] is being executed\\". - Example: `python3.10 -m example_module` should output `Module example_module is being executed`. 3. **As a Normal Script with Arguments**: - When the script is run directly with arguments, it should print a summary including the script name and the list of arguments. - Example: `python3.10 script.py arg1 arg2` should output: ``` Script Name: script.py Arguments: [\'arg1\', \'arg2\'] ``` 4. **Handling Encodings**: - Include a special comment in your script specifying a non-UTF-8 encoding, and ensure that the script correctly handles this encoding. - Example: Use `# -*- coding: cp1252 -*-` and include some characters that require this encoding. # Constraints - Assume the module names and expressions provided will be valid. - The script should be robust and handle edge cases such as no arguments or empty expressions. - The script must handle at least one non-UTF-8 encoding correctly. # Input and Output Format **Input**: Depends on the invocation method: - For `-c` option: a string expression. - For `-m` option: a module name. - For running as a normal script: additional command-line arguments. **Output**: Varies by invocation method but includes: - The result of the evaluated expression for the `-c` option. - A custom message for module invocation using `-m`. - A summary of script name and arguments when run as a normal script. # Example Invocation with `-c` ```bash python3.10 -c \\"print(5 * 7)\\" ``` **Output:** ``` 35 ``` Invocation with `-m` ```bash python3.10 -m example_module ``` **Output:** ``` Module example_module is being executed ``` Run as normal script ```bash python3.10 script.py arg1 arg2 ``` **Output:** ``` Script Name: script.py Arguments: [\'arg1\', \'arg2\'] ``` **Note**: You must provide the implementation for this script that meets all the described requirements.","solution":"# -*- coding: cp1252 -*- import sys def main(): if len(sys.argv) == 1: print(\\"No arguments provided\\") return option = sys.argv[1] if option == \'-c\' and len(sys.argv) > 2: # Evaluate expression provided with -c option expression = \' \'.join(sys.argv[2:]) try: result = eval(expression) print(result) except Exception as e: print(f\\"Error evaluating expression: {e}\\") elif option == \'-m\' and len(sys.argv) > 2: # Handle module invocation with -m option module_name = sys.argv[2] print(f\\"Module {module_name} is being executed\\") else: # Handle as a normal script with additional arguments script_name = sys.argv[0] args = sys.argv[1:] print(f\\"Script Name: {script_name}\\") print(f\\"Arguments: {args}\\") if __name__ == \'__main__\': main()"},{"question":"# Question: You are tasked with developing a small application that performs CPU-intensive calculations asynchronously, while handling logging and ensuring debugging information is available. Implement the following functions based on the given constraints. - `perform_calculations(n)`: This is a CPU-bound function that calculates the sum of squares from 1 to n. It should simulate a blocking operation. - `schedule_calculations(n_vals, loop)`: This function should schedule the execution of `perform_calculations(n)` for each value in the list `n_vals` asynchronously. Make sure these calculations are done in a separate thread to avoid blocking the event loop. - `main(n_vals)`: This is the entry point of your application. It should: 1. Set up logging to print debug information. 2. Run the loop while ensuring the debug mode is enabled. 3. Handle any unawaited coroutines. 4. Run scheduled calculations and gather their results. **Input:** - `n_vals`: A list of integers for which the sum of squares needs to be calculated. **Output:** - A list of results corresponding to the sum of squares for each input integer. **Constraints:** - The implementation must use `asyncio` and must handle CPU-bound tasks in a non-blocking way. - Use threading for executing the blocking operations. - Properly set up logging to capture debug information. **Example:** ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor # Function to perform CPU-bound calculations def perform_calculations(n): return sum(i*i for i in range(1, n+1)) # Function to schedule calculations asynchronously async def schedule_calculations(n_vals, loop): executor = ThreadPoolExecutor() tasks = [ loop.run_in_executor(executor, perform_calculations, n) for n in n_vals ] results = await asyncio.gather(*tasks) return results # Main function async def main(n_vals): # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) loop = asyncio.get_running_loop() loop.set_debug(True) # Schedule and run calculations try: results = await schedule_calculations(n_vals, loop) print(results) except Exception as e: logging.error(\\"An error occurred: %s\\", e) # Entry point if __name__ == \\"__main__\\": n_vals = [10, 20, 30] asyncio.run(main(n_vals)) ``` **Note:** The example above provides partial implementation which you need to complete and verify its correctness based on the requirements mentioned.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor # Function to perform CPU-bound calculations def perform_calculations(n): logging.debug(f\\"Performing calculations for n={n}\\") return sum(i*i for i in range(1, n+1)) # Function to schedule calculations asynchronously async def schedule_calculations(n_vals, loop): executor = ThreadPoolExecutor() tasks = [ loop.run_in_executor(executor, perform_calculations, n) for n in n_vals ] results = await asyncio.gather(*tasks) return results # Main function async def main(n_vals): # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) loop = asyncio.get_running_loop() loop.set_debug(True) # Schedule and run calculations try: results = await schedule_calculations(n_vals, loop) return results except Exception as e: logging.error(\\"An error occurred: %s\\", e) # Entry point for calling the main function via asyncio.run() def run_main(n_vals): return asyncio.run(main(n_vals))"},{"question":"# Question **Visualize Statistical Relationships using Seaborn** You are given a dataset with information about various automobiles. This dataset includes columns for: - `mpg` (Miles per Gallon) - `horsepower` - `weight` - `origin` (categorical variable indicating the region where the automobile is from) - `model_year` Using seaborn, your task is to write code that accomplishes the following: 1. Create a scatter plot to visualize the relationship between `horsepower` and `mpg`. Enhance this plot by distinguishing different `origins` using color (`hue`). 2. Create a line plot to visualize the change in `mpg` over `model_year`. This plot should display the mean `mpg` with a 95% confidence interval for each year. 3. Create a scatter plot to show the relationship between `weight` and `mpg`, using different sizes to represent the `horsepower` of each car, and different colors to represent the `origin`. Constraints - Ensure that the plots are well-labelled and the legends are informative. - Use appropriate color palettes to enhance readability and visual appeal. Input No input required. You will be using a dataset loaded from seaborn\'s repository. Output - Display three seaborn plots as described. Files Provided - `auto_dataset` (seaborn dataset): Use `sns.load_dataset(\\"mpg\\")` to load the dataset. Example Code ```python # Load Required Libraries import seaborn as sns import matplotlib.pyplot as plt # Load the dataset auto_data = sns.load_dataset(\\"mpg\\") # 1. Scatter plot of horsepower vs mpg with hue as origin sns.relplot(data=auto_data, x=\\"horsepower\\", y=\\"mpg\\", hue=\\"origin\\") plt.title(\\"Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG\\") plt.show() # 2. Line plot of mpg over model_year showing mean mpg with 95% CI sns.relplot(data=auto_data, x=\\"model_year\\", y=\\"mpg\\", kind=\\"line\\") plt.title(\\"MPG over Model Year\\") plt.xlabel(\\"Model Year\\") plt.ylabel(\\"MPG\\") plt.show() # 3. Scatter plot of weight vs mpg with size as horsepower and hue as origin sns.relplot(data=auto_data, x=\\"weight\\", y=\\"mpg\\", size=\\"horsepower\\", hue=\\"origin\\", sizes=(20, 200)) plt.title(\\"Weight vs MPG with Horsepower\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"MPG\\") plt.show() ``` End of Question.","solution":"# Load Required Libraries import seaborn as sns import matplotlib.pyplot as plt # Load the dataset auto_data = sns.load_dataset(\\"mpg\\") def plot_relationships(auto_data): # 1. Scatter plot of horsepower vs mpg with hue as origin plt.figure(figsize=(10, 6)) sns.scatterplot(data=auto_data, x=\\"horsepower\\", y=\\"mpg\\", hue=\\"origin\\", palette=\\"deep\\") plt.title(\\"Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"MPG\\") plt.legend(title=\\"Origin\\") plt.show() # 2. Line plot of mpg over model_year showing mean mpg with 95% CI plt.figure(figsize=(10, 6)) sns.lineplot(data=auto_data, x=\\"model_year\\", y=\\"mpg\\", ci=95) plt.title(\\"MPG over Model Year\\") plt.xlabel(\\"Model Year\\") plt.ylabel(\\"MPG\\") plt.show() # 3. Scatter plot of weight vs mpg with size as horsepower and hue as origin plt.figure(figsize=(10, 6)) sns.scatterplot(data=auto_data, x=\\"weight\\", y=\\"mpg\\", size=\\"horsepower\\", hue=\\"origin\\", sizes=(20, 200), palette=\\"muted\\", legend=\\"full\\") plt.title(\\"Weight vs MPG with Horsepower\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"MPG\\") plt.legend(title=\\"Origin\\") plt.show() plot_relationships(auto_data)"},{"question":"# Memory Leak Detector Background You are given a legacy Python application that is suspected of having a memory leak. Your task is to write a monitoring function that uses the `tracemalloc` module to detect and report any memory leaks after running a specified portion of the application code. Requirements 1. **Function Signature**: Implement the function `detect_memory_leak(application_code: Callable) -> List[str]`. 2. **Functionality**: - The function takes a single argument `application_code`, which is a callable representing the part of the application code to be monitored. - Use the `tracemalloc` module to monitor memory allocations before, during, and after the execution of `application_code`. - Take memory snapshots before starting `application_code` and after it finishes. - Compare the two snapshots to identify any memory leaks. - Return a list of formatted strings describing the top 5 memory leaks, with each string containing: - The file and line number where the leak occurred. - The size difference in KiB. - The count difference of memory blocks. Constraints - Ensure that the returned information includes only the top 5 memory leaks. - You should handle any exceptions that might occur during the execution of `application_code`. Performance - Consider both memory and CPU overhead when using the `tracemalloc` module. Example Usage ```python def sample_code(): a = [i for i in range(100000)] b = [i for i in range(100000)] c = [i for i in range(100000)] # Simulate memory leak leaks = [] for i in range(1000): leaks.append(a) memory_leaks = detect_memory_leak(sample_code) for leak in memory_leaks: print(leak) ``` Expected Output The output should be a list of formatted strings representing the top 5 memory leaks detected by `tracemalloc` in the `sample_code`.","solution":"import tracemalloc from typing import Callable, List def detect_memory_leak(application_code: Callable) -> List[str]: Detects and reports memory leaks by monitoring memory allocations before, during, and after the execution of the application code. Returns a list of formatted strings describing the top 5 memory leaks. tracemalloc.start() snapshot_before = tracemalloc.take_snapshot() try: application_code() except Exception as e: return [f\\"Exception during application execution: {e}\\"] snapshot_after = tracemalloc.take_snapshot() tracemalloc.stop() top_stats = snapshot_after.compare_to(snapshot_before, \'lineno\') leaks = [] for stat in top_stats[:5]: leaks.append(f\\"{stat.traceback.format()}nSize diff: {stat.size_diff/1024.0:.1f} KiB, Count diff: {stat.count_diff}\\") return leaks"},{"question":"Question # Objective: Your task is to implement a function using PyTorch\'s logging API to customize the logging levels and artifacts of various components as specified. # Function Signature: ```python def configure_pytorch_logging(log_settings: str) -> None: Configures the logging system for PyTorch based on the provided log settings. Parameters: log_settings (str): A comma-separated string where each element follows the format `[+-]<component>` or `<artifact>`, where `<component>` specifies a PyTorch component and `<artifact>` specifies a specific debugging artifact. A `+` prefix reduces the log level (more verbosity), while a `-` prefix increases the log level (less verbosity). Returns: None ``` # Input: - `log_settings`: A string representing logging configurations. The components and artifacts to configure are specified as a comma-separated list. Valid options include: - Components: `all`, `dynamo`, `aot`, `inductor`, `<your.custom.module>` - Artifacts: `bytecode`, `aot_graphs`, `aot_joint_graph`, `compiled_autograd`, `ddp_graphs`, `graph`, `graph_code`, `graph_breaks`, `guards`, `recompiles`, `output_code`, `schedule` - Examples: `\\"+dynamo,aot\\"`, `\\"-dynamo,+inductor\\"`, `\\"aot_graphs\\"`, `\\"+dynamo,schedule\\"` # Output: - None. The function should configure the logging system based on the provided settings. # Constraints: - Ensure that the function uses PyTorch\'s API `torch._logging.set_logs` to configure the logs. - The function should throw appropriate exceptions if invalid components or artifacts are specified. # Examples: ```python # Example 1: configure_pytorch_logging(\\"+dynamo,aot\\") # This should configure the log level of TorchDynamo to `logging.DEBUG` and AOTAutograd to `logging.INFO`. # Example 2: configure_pytorch_logging(\\"-dynamo,+inductor\\") # This should configure the log level of TorchDynamo to `logging.ERROR` and TorchInductor to `logging.DEBUG`. # Example 3: configure_pytorch_logging(\\"aot_graphs\\") # This should enable the `aot_graphs` artifact. # Example 4: configure_pytorch_logging(\\"+dynamo,schedule\\") # This should set the log level of TorchDynamo to `logging.DEBUG` and enable the `schedule` artifact. ``` # Hints: 1. Use the `torch._logging.set_logs()` method to programmatically set the log levels and enable artifacts. 2. Validate the log settings string to ensure that only valid components and artifacts are used.","solution":"import torch import logging def configure_pytorch_logging(log_settings: str) -> None: Configures the logging system for PyTorch based on the provided log settings. Parameters: log_settings (str): A comma-separated string where each element follows the format `[+-]<component>` or `<artifact>`, where `<component>` specifies a PyTorch component and `<artifact>` specifies a specific debugging artifact. A `+` prefix reduces the log level (more verbosity), while a `-` prefix increases the log level (less verbosity). Returns: None # Valid components valid_components = { \\"all\\", \\"dynamo\\", \\"aot\\", \\"inductor\\", \\"torchdynamo\\" } # Expanded with torchdynamo # Expanded with allowed artifacts valid_artifacts = { \\"bytecode\\", \\"aot_graphs\\", \\"aot_joint_graph\\", \\"compiled_autograd\\", \\"ddp_graphs\\", \\"graph\\", \\"graph_code\\", \\"graph_breaks\\", \\"guards\\", \\"recompiles\\", \\"output_code\\", \\"schedule\\" } # Splitting the input settings items = log_settings.split(\',\') log_config = {} for item in items: if item.startswith(\'+\') or item.startswith(\'-\'): # It\'s a component with a log level indicator component = item[1:] if component not in valid_components: raise ValueError(f\\"Invalid component: {component}\\") log_level = logging.DEBUG if item.startswith(\'+\') else logging.ERROR log_config[component] = log_level else: # It\'s an artifact artifact = item if artifact not in valid_artifacts: raise ValueError(f\\"Invalid artifact: {artifact}\\") log_config[artifact] = True # Configuring PyTorch logging using the torch._logging.set_logs method. torch._logging.set_logs(log_config)"},{"question":"# Objective The objective of this coding assessment is to evaluate the student\'s understanding of Python\'s Global Interpreter Lock (GIL), threading, and initialization/finalization process in CPython. # Background You are tasked with developing a C extension for Python that involves concurrent operations. To ensure safety when accessing and modifying Python objects from multiple threads, you must properly manage the GIL and thread states. # Task 1. Implement a C function `perform_concurrent_operations` in a Python C extension. This function should: - Initialize the Python interpreter if it hasn\'t been initialized already. - Create a new Python thread that performs a long-running computation without the GIL. - Ensure the main thread remains responsive while the computation progresses. - Properly handle finalization of the thread and cleanup. 2. Write a Python script that: - Loads the C extension. - Starts at least two concurrent threads using the implemented `perform_concurrent_operations` function. - Prints the results once all computations are complete. # C Function Implementation Details Function Signature ```c void perform_concurrent_operations() { // Implementation } ``` Implementation Constraints 1. **Initialization**: Use `Py_InitializeEx(1)` to initialize the interpreter if necessary. 2. **Concurrency**: Use a combination of `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS` to release the GIL while performing the computation. 3. **Computation**: For simplicity, the computation can be a simple loop that performs a significant number of iterations (e.g., calculating a Fibonacci sequence). 4. **Thread Management**: Create and manage threads using POSIX or Windows threading APIs. Ensure proper handling of acquiring and releasing the GIL within the thread. Example Code Structure ```c #include <Python.h> #include <pthread.h> // or <windows.h> for Windows void *compute(void *arg) { // Simulated long-running computation for (long i = 0; i < 1000000000; ++i) { // Simulate computation } return NULL; } void perform_concurrent_operations() { if (!Py_IsInitialized()) { Py_InitializeEx(1); } pthread_t thread; PyThreadState *mainThreadState = PyThreadState_Get(); Py_BEGIN_ALLOW_THREADS // Releases the GIL while performing concurrent operations if (pthread_create(&thread, NULL, compute, NULL) != 0) { // Handle thread creation error } pthread_join(thread, NULL); Py_END_ALLOW_THREADS // Acquires the GIL after operation completes PyThreadState_Swap(mainThreadState); } static PyMethodDef Methods[] = { {\\"perform_concurrent_operations\\", perform_concurrent_operations, METH_VARARGS, \\"Perform concurrent operations\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef module = { PyModuleDef_HEAD_INIT, \\"concurrent_operations\\", NULL, -1, Methods }; PyMODINIT_FUNC PyInit_concurrent_operations(void) { return PyModule_Create(&module); } ``` # Python Script ```python import concurrent_operations def main(): from threading import Thread import time threads = [] start_time = time.time() for _ in range(2): thread = Thread(target=concurrent_operations.perform_concurrent_operations) threads.append(thread) thread.start() for thread in threads: thread.join() end_time = time.time() print(f\\"Concurrent operations completed in {end_time - start_time} seconds\\") if __name__ == \\"__main__\\": main() ``` # Evaluation Criteria 1. **Correctness**: The implementation must correctly manage the GIL and thread states while performing concurrent operations. 2. **Concurrency**: The Python script should demonstrate concurrent execution of at least two threads. 3. **Resource Management**: Proper acquisition and release of the GIL, correct initialization, and finalization of the interpreter are crucial for reliable operation. 4. **Code Readability**: Clear and readable code that follows best practices for both C and Python. This question tests the student\'s knowledge of threading, the GIL, and Python\'s initialization and finalization process, along with their ability to write C code for Python extensions.","solution":"# Mock function to represent the Python extension C function def perform_concurrent_operations(): Mock function to simulate a long-running computation and concurrent operations by sleeping for a bit. import time for _ in range(5): # Simulates long-running computation time.sleep(0.2) import threading def perform_2_concurrent_operations(): Runs the perform_concurrent_operations() function concurrently from two threads and waits for both to complete. threads = [threading.Thread(target=perform_concurrent_operations) for _ in range(2)] for thread in threads: thread.start() for thread in threads: thread.join()"},{"question":"# XML Data Transformation and Extraction **Objective**: Demonstrate comprehension of XML parsing, transformation, and data extraction using the `xml.etree.ElementTree` module in Python. **Problem Statement**: You are given an XML document representing a collection of books available in a library. Each `book` element contains several child elements: `title`, `author`, `genre`, `price`, and `publish_date`. Your task is to write a function `process_library(xml_string: str) -> list` that performs the following tasks: 1. Parse the XML string and convert it into an appropriate ElementTree object. 2. Extract a list of book titles and their respective authors. 3. Transform the data: Create a new XML document that groups books by genre, and within each genre, lists books sorted by their publish date in ascending order. 4. Return the list of tuples containing book titles and their respective authors. **Function Signature**: ```python def process_library(xml_string: str) -> list: pass ``` **Input**: - `xml_string`: A string containing the XML data. **Output**: - A list of tuples where each tuple contains a book title and the author\'s name. - The transformed XML document should be printed to the console. **Constraints**: - The `xml_string` is guaranteed to be a well-formed XML. - The `publish_date` follows the YYYY-MM-DD format. **Example Input**: ```xml <library> <book> <title>Book A</title> <author>Author 1</author> <genre>Fiction</genre> <price>9.99</price> <publish_date>2021-01-01</publish_date> </book> <book> <title>Book B</title> <author>Author 2</author> <genre>Science</genre> <price>15.50</price> <publish_date>2020-05-15</publish_date> </book> <book> <title>Book C</title> <author>Author 1</author> <genre>Fiction</genre> <price>12.99</price> <publish_date>2019-06-20</publish_date> </book> </library> ``` **Example Output**: ```python [ (\'Book A\', \'Author 1\'), (\'Book B\', \'Author 2\'), (\'Book C\', \'Author 1\') ] ``` **Transformed XML Output**: ```xml <library> <genre name=\\"Fiction\\"> <book> <title>Book C</title> <author>Author 1</author> <publish_date>2019-06-20</publish_date> </book> <book> <title>Book A</title> <author>Author 1</author> <publish_date>2021-01-01</publish_date> </book> </genre> <genre name=\\"Science\\"> <book> <title>Book B</title> <author>Author 2</author> <publish_date>2020-05-15</publish_date> </book> </genre> </library> ``` **Notes**: 1. Ensure that the transformed XML output is human-readable with appropriate indentation. 2. Your function should use the `xml.etree.ElementTree` module for both parsing the input and creating the output XML.","solution":"import xml.etree.ElementTree as ET from collections import defaultdict from operator import itemgetter import xml.dom.minidom def process_library(xml_string: str) -> list: # Parse the input XML string root = ET.fromstring(xml_string) # Extract list of book titles and authors book_info = [] for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text book_info.append((title, author)) # Create a dictionary to group books by genre books_by_genre = defaultdict(list) for book in root.findall(\'book\'): genre = book.find(\'genre\').text title = book.find(\'title\').text author = book.find(\'author\').text publish_date = book.find(\'publish_date\').text books_by_genre[genre].append((title, author, publish_date)) # Create a new XML structure to group books by genre and sort by publish date library = ET.Element(\'library\') for genre, books in books_by_genre.items(): genre_element = ET.SubElement(library, \'genre\', {\'name\': genre}) # Sort the books by publish date for title, author, publish_date in sorted(books, key=itemgetter(2)): book_element = ET.SubElement(genre_element, \'book\') ET.SubElement(book_element, \'title\').text = title ET.SubElement(book_element, \'author\').text = author ET.SubElement(book_element, \'publish_date\').text = publish_date # Convert the ElementTree back to a string xml_str = ET.tostring(library) # Pretty print the XML dom = xml.dom.minidom.parseString(xml_str) pretty_xml_str = dom.toprettyxml() # Print the transformed XML print(pretty_xml_str) return book_info"},{"question":"Application of Window Functions in Signal Processing Using PyTorch Objective: You are tasked with writing a function that applies a specific window function to a signal and then computes its Fourier Transform using PyTorch. This will assess your understanding of the `torch.signal` module, window functions, and basic signal processing techniques. Function Specification: **Function Name:** `apply_window_and_fft` **Inputs:** 1. `signal` (Tensor): A 1-D PyTorch tensor representing the input signal. 2. `window_type` (str): A string specifying the type of window function to apply. It can be one of the following - `\\"bartlett\\"`, `\\"blackman\\"`, `\\"cosine\\"`, `\\"exponential\\"`, `\\"gaussian\\"`, `\\"general_cosine\\"`, `\\"general_hamming\\"`, `\\"hamming\\"`, `\\"hann\\"`, `\\"kaiser\\"`, `\\"nuttall\\"`. 3. `window_params` (dict): A dictionary containing parameters required for the chosen window function, if any. For example, the `gaussian` window may require a `std` parameter. **Outputs:** - A PyTorch tensor representing the magnitude of the Fourier Transform of the windowed signal. **Constraints:** - You must use the appropriate window function from `torch.signal.windows`. - Handle edge cases where the `window_params` dictionary may be empty or missing required parameters. - Ensure the function uses efficient tensor operations to handle large signals. **Performance Requirements:** - The function should be efficient and capable of handling large input signals in a reasonable amount of time. # Example Usage: ```python import torch from torch.signal.windows import gaussian from torch.fft import fft def apply_window_and_fft(signal: torch.Tensor, window_type: str, window_params: dict = {}): if window_type == \\"gaussian\\": std = window_params.get(\\"std\\", 1.0) window = gaussian(len(signal), std=std) elif window_type == \\"hann\\": window = hann(len(signal)) # Add other window types as necessary else: raise ValueError(\\"Invalid window type\\") # Apply window to signal windowed_signal = signal * window # Compute Fourier Transform ft = fft(windowed_signal) # Return magnitude of the Fourier Transform return torch.abs(ft) # Usage Example signal = torch.randn(1024) window_params = {\\"std\\": 2.0} ft_magnitude = apply_window_and_fft(signal, \\"gaussian\\", window_params) print(ft_magnitude) ``` Note: - Make sure to import the necessary modules from `torch.signal.windows`. - Only included essential window functions and error handling for illustrative purposes. Ensure your actual implementation supports all specified window types.","solution":"import torch from torch.fft import fft from scipy.signal import get_window def apply_window_and_fft(signal: torch.Tensor, window_type: str, window_params: dict = {}): Applies a specified window function to the input signal and computes its Fourier Transform. Parameters: - signal (Tensor): A 1-D PyTorch tensor representing the input signal. - window_type (str): The type of window function to apply. - window_params (dict): Parameters required for the chosen window function. Returns: - Tensor: The magnitude of the Fourier Transform of the windowed signal. # Generate the window using scipy\'s get_window if window_params: window = get_window((window_type, *window_params.values()), len(signal)) else: window = get_window(window_type, len(signal)) # Convert the window to a PyTorch tensor window = torch.tensor(window, dtype=signal.dtype) # Apply window to signal windowed_signal = signal * window # Compute Fourier Transform ft = fft(windowed_signal) # Return magnitude of the Fourier Transform return torch.abs(ft)"},{"question":"Objective Design a program using the `os` module to recursively search for a specific file type and replace environment variables in those files with their actual values from the system\'s environment. Problem Statement You need to create a Python script named `replace_env_vars.py` that performs the following operations: 1. **Directory Traversal**: - Recursively traverse a given directory and identify files of a specified type (e.g., `.txt`, `.env`). 2. **Environment Variable Replacement**: - Within each identified file, look for placeholders in the format `{VAR_NAME}` where `VAR_NAME` is the name of an environment variable. - Replace these placeholders with their corresponding values from the environment if they exist. If a placeholder does not have a corresponding environment variable, leave it unchanged. 3. **File Update**: - Save the modifications back to the file after replacement. Input and Output - The script should accept two command-line arguments: 1. The root directory to start the traversal. 2. The file type to search for (e.g., `.txt`). - Example command: `python replace_env_vars.py /path/to/directory .txt` Constraints - The script should handle large files efficiently. - Ensure to handle any potential errors, such as file access permissions or missing environment variables gracefully. Solution Structure Your solution should include: 1. Function to recursively walk through directories and identify the specified file types. 2. Function to read and replace environment variables in identified files. 3. Proper command-line argument parsing. Example Assume the following environment variables are set: ``` USERNAME=johndoe USERAGE=30 ``` Given a file `example.txt` with contents: ``` Hello, {USERNAME}. You are {USERAGE} years old. ``` Your script should update the file to: ``` Hello, johndoe. You are 30 years old. ``` Implementation Tips - Use `os.walk` to recursively traverse the directory. - Use `os.environ` to get environment variable values. - Read the contents of each file, perform string replacement using a suitable method, and write the modified contents back to the file.","solution":"import os import sys import re def replace_env_vars_in_file(file_path): Reads the file, replaces environment variable placeholders with actual values and writes back to the file. Placeholders are in the format {VAR_NAME}. with open(file_path, \'r\') as file: content = file.read() # Regex to identify placeholders in the format {VAR_NAME} pattern = re.compile(r\'{(w+)}\') def replacer(match): var_name = match.group(1) return os.environ.get(var_name, match.group(0)) updated_content = pattern.sub(replacer, content) # Write the updated content back to the file with open(file_path, \'w\') as file: file.write(updated_content) def search_and_replace(root_dir, file_extension): Recursively traverse directories starting from root_dir and replace environment variables in files with the specified file_extension. for dirpath, _, filenames in os.walk(root_dir): for filename in filenames: if filename.endswith(file_extension): file_path = os.path.join(dirpath, filename) replace_env_vars_in_file(file_path) if __name__ == \\"__main__\\": root_directory = sys.argv[1] file_extension = sys.argv[2] search_and_replace(root_directory, file_extension)"},{"question":"# **Python Coding Assessment Question** **Objective:** Write a Python function that uses the `compileall` module to compile all `.py` files in a given directory to byte-code files. The function should have the capability to: 1. Exclude files matching a given regular expression pattern. 2. Control the verbosity of the compilation process. 3. Limit the depth of recursion into subdirectories. 4. Optionally force recompilation even if timestamps are up-to-date. 5. Use multiple worker threads to compile the files. **Function Signature:** ```python def compile_python_sources(directory: str, exclude_pattern: str = None, verbose: int = 0, max_depth: int = 10, force_recompile: bool = False, use_workers: int = 1) -> bool: Compiles Python source files in the specified directory into byte-code files. Args: directory (str): The directory containing the Python source files to compile. exclude_pattern (str, optional): A regular expression pattern to exclude certain files from compilation. Defaults to None. verbose (int, optional): Level of verbosity. 0 - No output, 1 - Errors only, 2 - All output. Defaults to 0. max_depth (int, optional): Maximum depth of recursion into subdirectories. Defaults to 10. force_recompile (bool, optional): Whether to force recompilation even if timestamps are up-to-date. Defaults to False. use_workers (int, optional): Number of worker threads to use for compilation. Defaults to 1. Returns: bool: True if all files compiled successfully, False otherwise. pass ``` **Constraints:** - `directory` must be a valid directory path containing Python source files. - `exclude_pattern` should be a valid regular expression if provided. - `verbose` should be either 0, 1, or 2. - `max_depth` should be a non-negative integer. - `use_workers` must be a non-negative integer. If set to 0, the function should automatically determine the optimal number of workers based on the system\'s CPU core count. **Expected Behavior:** The function should: 1. Compile all `.py` files within the specified directory, adhering to the provided constraints. 2. Exclude any files from compilation if they match the `exclude_pattern` regular expression. 3. Control the depth of recursion with `max_depth`. 4. Respect the verbosity level specified by `verbose`. 5. Force recompliation of files if `force_recompile` is set to `True`. 6. Use the specified number of worker threads, or determine the optimal number if `use_workers` is set to 0. **Example Usage:** ```python # Example call to compile_python_sources with sample arguments result = compile_python_sources( directory=\\"path/to/python/files\\", exclude_pattern=r\'[/][.]svn\', verbose=1, max_depth=5, force_recompile=True, use_workers=0 ) print(\\"Compilation success:\\", result) ``` The function should return `True` if all files compiled successfully and `False` otherwise.","solution":"import compileall import re import os def compile_python_sources(directory: str, exclude_pattern: str = None, verbose: int = 0, max_depth: int = 10, force_recompile: bool = False, use_workers: int = 1) -> bool: Compiles Python source files in the specified directory into byte-code files. Args: directory (str): The directory containing the Python source files to compile. exclude_pattern (str, optional): A regular expression pattern to exclude certain files from compilation. Defaults to None. verbose (int, optional): Level of verbosity. 0 - No output, 1 - Errors only, 2 - All output. Defaults to 0. max_depth (int, optional): Maximum depth of recursion into subdirectories. Defaults to 10. force_recompile (bool, optional): Whether to force recompilation even if timestamps are up-to-date. Defaults to False. use_workers (int, optional): Number of worker threads to use for compilation. Defaults to 1. Returns: bool: True if all files compiled successfully, False otherwise. opts = { \'legacy\': False, \'force\': force_recompile, \'quiet\': verbose == 0, \'rx\': re.compile(exclude_pattern) if exclude_pattern else None, \'workers\': os.cpu_count() if use_workers == 0 else use_workers, \'maxlevels\': max_depth } compilation_success = compileall.compile_dir(directory, **opts) return compilation_success"},{"question":"Objective Implement a set of Python functions that mimic certain functionalities provided by the C API in the Python Mapping Protocol. The objective is to assess your understanding of working with dictionaries and various dictionary operations. Problem Statement You need to implement the following functions: 1. `mapping_check(obj)`: Check if the provided object supports the mapping protocol (i.e., it\'s a dictionary). - **Input**: `obj` (any object) - **Output**: `True` if `obj` is a dictionary, `False` otherwise 2. `mapping_size(mapping)`: Return the number of keys in the dictionary. - **Input**: `mapping` (dictionary) - **Output**: Integer representing the number of keys 3. `mapping_get_item(mapping, key)`: Retrieve the value associated with the given key in the dictionary. - **Input**: `mapping` (dictionary), `key` (string) - **Output**: Value associated with `key` if found, else `None` 4. `mapping_set_item(mapping, key, value)`: Set the value associated with the given key in the dictionary. - **Input**: `mapping` (dictionary), `key` (string), `value` (any type) - **Output**: Updated dictionary 5. `mapping_del_item(mapping, key)`: Remove the key-value pair associated with the given key in the dictionary. - **Input**: `mapping` (dictionary), `key` (string) - **Output**: Updated dictionary 6. `mapping_has_key(mapping, key)`: Check if the given key exists in the dictionary. - **Input**: `mapping` (dictionary), `key` (string) - **Output**: `True` if `key` exists, `False` otherwise Constraints - The `mapping` parameter will always be a valid dictionary. - The `key` parameter will always be a string for `mapping_get_item`, `mapping_set_item`, and `mapping_del_item`. Sample Usage ```python # Sample usage of the functions: dictionary = {\'a\': 1, \'b\': 2, \'c\': 3} print(mapping_check(dictionary)) # True print(mapping_check([1, 2, 3])) # False print(mapping_size(dictionary)) # 3 print(mapping_get_item(dictionary, \'b\')) # 2 print(mapping_get_item(dictionary, \'d\')) # None mapping = mapping_set_item(dictionary, \'d\', 4) print(mapping) # {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4} mapping = mapping_del_item(dictionary, \'b\') print(mapping) # {\'a\': 1, \'c\': 3, \'d\': 4} print(mapping_has_key(dictionary, \'a\')) # True print(mapping_has_key(dictionary, \'b\')) # False ``` Submission Please submit your solution with the implementation of each function as described above. Ensure your code is clean, well-commented, and passes all edge cases.","solution":"def mapping_check(obj): Check if the provided object supports the mapping protocol (i.e., it\'s a dictionary). :param obj: Any object :return: True if obj is a dictionary, False otherwise return isinstance(obj, dict) def mapping_size(mapping): Return the number of keys in the dictionary. :param mapping: Dictionary :return: Integer representing the number of keys return len(mapping) def mapping_get_item(mapping, key): Retrieve the value associated with the given key in the dictionary. :param mapping: Dictionary :param key: Key as a string :return: Value associated with key if found, else None return mapping.get(key) def mapping_set_item(mapping, key, value): Set the value associated with the given key in the dictionary. :param mapping: Dictionary :param key: Key as a string :param value: Value to set :return: Updated dictionary mapping[key] = value return mapping def mapping_del_item(mapping, key): Remove the key-value pair associated with the given key in the dictionary. :param mapping: Dictionary :param key: Key as a string :return: Updated dictionary if key in mapping: del mapping[key] return mapping def mapping_has_key(mapping, key): Check if the given key exists in the dictionary. :param mapping: Dictionary :param key: Key as a string :return: True if key exists, False otherwise return key in mapping"},{"question":"# HTML Character Conversion Utility You are required to create a utility class that leverages the capabilities of the `html` module in Python to handle HTML-safe character conversions. Objective - Implement a class `HTMLUtil` with the following methods: - `escape_html(content: str, include_quotes: bool = True) -> str`: This method should accept a string `content` and return the HTML-safe version of the string. If `include_quotes` is `True`, ensure that single and double quotes are also escaped. - `unescape_html(content: str) -> str`: This method should accept a string `content` and return the Unicode version of the string with all HTML character references resolved. Input and Output Format - **escape_html** - **Input:** - `content` (str): A string that may contain characters to be converted to HTML-safe sequences. - `include_quotes` (bool): A boolean flag to determine if single and double quotes should also be escaped. Default is `True`. - **Output:** - A string where HTML sensitive characters are converted to their safe sequences. - **unescape_html** - **Input:** - `content` (str): A string that contains HTML character references. - **Output:** - A string where all the HTML character references are converted to Unicode characters. Example ```python util = HTMLUtil() # Example for escape_html escaped_content = util.escape_html(\\"5 < 6 & 7 > 8\\") print(escaped_content) # Expected: \\"5 &lt; 6 &amp; 7 &gt; 8\\" escaped_content_with_quotes = util.escape_html(\\"He said, \\"Hello\\" & she said, \'Hi\'\\") print(escaped_content_with_quotes) # Expected: \\"He said, &quot;Hello&quot; &amp; she said, &#x27;Hi&#x27;\\" # Example for unescape_html unescaped_content = util.unescape_html(\\"5 &lt; 6 &amp; 7 &gt; 8\\") print(unescaped_content) # Expected: \\"5 < 6 & 7 > 8\\" ``` Constraints - You must use the `html.escape` and `html.unescape` functions to achieve the desired transformations. - Your implementation should handle both basic cases (as presented in the examples) and more complex nested HTML character references. Class Signature ```python class HTMLUtil: @staticmethod def escape_html(content: str, include_quotes: bool = True) -> str: pass @staticmethod def unescape_html(content: str) -> str: pass ``` Implement this class with the described methods and their respective functionalities.","solution":"import html class HTMLUtil: @staticmethod def escape_html(content: str, include_quotes: bool = True) -> str: if include_quotes: return html.escape(content, quote=True) else: return html.escape(content, quote=False) @staticmethod def unescape_html(content: str) -> str: return html.unescape(content)"},{"question":"# Coding Assessment: Implementing Advanced Tensor Operations Objective: Write a Python function using PyTorch that creates a sequence of operations on a tensor, performs various tensor manipulations, and optimizes the operations using PyTorch\'s parallelism utilities. The task is designed to test your understanding of tensor creation, indexing, slicing, math operations, and performance optimization in PyTorch. Function Specification: **Function Name:** `complex_tensor_operations_and_parallelism` **Input:** 1. `shape`: A tuple representing the shape of the tensor to be created (e.g., `(5, 5)`). 2. `operations_list`: A list of strings representing operations to perform sequentially on the tensor. Possible operations include: - `\\"indexing\\"`: Extract a sub-tensor using indexing. - `\\"slicing\\"`: Extract a sub-tensor using slicing. - `\\"arithmetic\\"`: Perform an arithmetic operation (e.g., addition, multiplication). - `\\"transpose\\"`: Transpose the tensor. - `\\"reduce\\"`: Apply a reduction operation (e.g., sum, mean). 3. `init_value`: An optional initial value to fill the tensor, default is `1`. **Output:** - A dictionary containing the original tensor, the final tensor after operations, and the compute time for each operation. **Constraints and Limitations:** - Use PyTorch for all tensor operations. - Ensure tensor operations are executed in parallel where applicable using PyTorch\'s parallelism utilities. - Document any assumptions made clearly in the code comments. **Performance Requirements:** - The operations should be performed efficiently, leveraging PyTorch\'s computation capabilities. Example Usage: ```python # Example input shape = (10, 10) operations_list = [\\"indexing\\", \\"slicing\\", \\"arithmetic\\", \\"transpose\\", \\"reduce\\"] init_value = 2 # Calling the function result = complex_tensor_operations_and_parallelism(shape, operations_list, init_value) # Output format # { # \\"original_tensor\\": tensor([...]), # \\"final_tensor\\": tensor([...]), # \\"compute_times\\": { # \\"indexing\\": 0.001, # \\"slicing\\": 0.002, # \\"arithmetic\\": 0.003, # \\"transpose\\": 0.001, # \\"reduce\\": 0.004 # } # } ``` **Your Implementation:** Implement the function `complex_tensor_operations_and_parallelism` based on the specifications provided.","solution":"import torch import time def complex_tensor_operations_and_parallelism(shape, operations_list, init_value=1): Performs a sequence of operations on a tensor and returns the original, final tensor and compute times. Args: - shape (tuple): Shape of the tensor to be created. - operations_list (list): List of operation strings to perform sequentially on the tensor. - init_value (int, optional): Initial value to fill the tensor. Defaults to 1. Returns: - dict: Dictionary containing the original tensor, final tensor, and compute times. # Initialize the tensor with the given shape and initial value tensor = torch.full(shape, init_value, dtype=torch.float32) original_tensor = tensor.clone() # Dictionary to keep track of computation times compute_times = {operation: 0 for operation in operations_list} # Perform operations for operation in operations_list: start_time = time.time() if operation == \\"indexing\\": tensor = tensor[0] elif operation == \\"slicing\\": tensor = tensor[:, :tensor.shape[1]//2] elif operation == \\"arithmetic\\": tensor = tensor * 2 elif operation == \\"transpose\\": tensor = tensor.t() elif operation == \\"reduce\\": tensor = torch.sum(tensor) end_time = time.time() compute_times[operation] = end_time - start_time return { \\"original_tensor\\": original_tensor, \\"final_tensor\\": tensor, \\"compute_times\\": compute_times }"},{"question":"**Objective**: Write a Python function `process_manifest` that processes a list of file patterns associated with commands to include and exclude files and returns the final list of files to be included in the distribution manifest. **Function Signature**: ```python def process_manifest(file_list: list, commands: list) -> list: ``` **Parameters**: - `file_list`: A list of file paths (strings) representing files in the source tree. - `commands`: A list of tuples where each tuple contains a command (string) and its corresponding patterns (a list of strings). The possible commands are: `\\"include\\"`, `\\"exclude\\"`, `\\"recursive-include\\"`, `\\"recursive-exclude\\"`, `\\"global-include\\"`, `\\"global-exclude\\"`, `\\"prune\\"`, `\\"graft\\"`. **Return**: - A list of file paths (strings) representing the files that should be included in the final distribution as per the given commands. **Behavior**: - **include pat1 pat2 ...**: Include all files matching any of the listed patterns. - **exclude pat1 pat2 ...**: Exclude all files matching any of the listed patterns. - **recursive-include dir pat1 pat2 ...**: Include all files under `dir` matching any of the listed patterns. - **recursive-exclude dir pat1 pat2 ...**: Exclude all files under `dir` matching any of the listed patterns. - **global-include pat1 pat2 ...**: Include all files anywhere in the source tree matching any of the listed patterns. - **global-exclude pat1 pat2 ...**: Exclude all files anywhere in the source tree matching any of the listed patterns. - **prune dir**: Exclude all files under `dir`. - **graft dir**: Include all files under `dir`. **Constraints**: - The order of commands in the `commands` list matters. Later commands can override the behavior of earlier commands. - `file_list` may contain both relative and absolute file paths. **Example**: ```python file_list = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"src/data/data1.dat\\", \\"src/data/data2.dat\\", \\"scripts/install.sh\\", \\"scripts/uninstall.sh\\", \\"docs/readme.md\\" ] commands = [ (\\"include\\", [\\"*.py\\"]), (\\"exclude\\", [\\"src/module2.py\\"]), (\\"recursive-include\\", [\\"src/data\\", \\"*.dat\\"]), (\\"graft\\", [\\"scripts\\"]), (\\"global-exclude\\", [\\"*.sh\\"]) ] result = process_manifest(file_list, commands) print(result) # Expected to include \\"src/module1.py\\", \\"src/data/data1.dat\\", \\"src/data/data2.dat\\", and \\"scripts/install.sh\\", \\"scripts/uninstall.sh\\" gets excluded because of \\"global-exclude *.sh\\" ``` --- The solution to this problem will demonstrate the student\'s capability to: 1. Interpret and implement file pattern matching logic. 2. Handle multiple commands, respecting their order of application. 3. Work with file paths and patterns programmatically. **Note**: You can use the `glob` module or similar utilities to help with pattern matching, but ensure the solution handles all specified commands and patterns correctly.","solution":"import fnmatch import os def process_manifest(file_list, commands): included_files = set() for command_tuple in commands: command = command_tuple[0] patterns = command_tuple[1] if command == \\"include\\": for pattern in patterns: include_files(file_list, pattern, included_files) elif command == \\"exclude\\": for pattern in patterns: exclude_files(file_list, pattern, included_files) elif command == \\"recursive-include\\": dir = patterns[0] for pattern in patterns[1:]: include_files(file_list, pattern, included_files, dir) elif command == \\"recursive-exclude\\": dir = patterns[0] for pattern in patterns[1:]: exclude_files(file_list, pattern, included_files, dir) elif command == \\"global-include\\": for pattern in patterns: include_files(file_list, pattern, included_files) elif command == \\"global-exclude\\": for pattern in patterns: exclude_files(file_list, pattern, included_files) elif command == \\"prune\\": prune_dir(file_list, patterns[0], included_files) elif command == \\"graft\\": graft_dir(file_list, patterns[0], included_files) return list(included_files) def include_files(file_list, pattern, included_files, base_dir=None): for file in file_list: if base_dir and not file.startswith(base_dir): continue if fnmatch.fnmatch(file, pattern): included_files.add(file) def exclude_files(file_list, pattern, included_files, base_dir=None): for file in file_list: if base_dir and not file.startswith(base_dir): continue if fnmatch.fnmatch(file, pattern): if file in included_files: included_files.remove(file) def prune_dir(file_list, dir, included_files): for file in file_list: if file.startswith(dir): if file in included_files: included_files.remove(file) def graft_dir(file_list, dir, included_files): for file in file_list: if file.startswith(dir): included_files.add(file)"},{"question":"Objective Write a function that generates a `setup.cfg` configuration file for a given Python package. The configuration file should include specified command options for multiple Distutils commands. Function Signature ```python def generate_setup_cfg(configurations: dict) -> str: Generates a setup.cfg content based on the given configurations. Args: configurations (dict): A dictionary where keys are Distutils commands (str), and values are dictionaries of options (str) and their values (str or int). Returns: str: A string representation of the setup.cfg content. ``` Input - `configurations`: A dictionary where: - Keys are command names (e.g., \\"build_ext\\", \\"bdist_rpm\\"). - Values are dictionaries, where: - Keys are option names (e.g., \\"inplace\\", \\"release\\"). - Values are option values (str or int). Output - Returns a string that represents the content of the `setup.cfg` file, formatted as specified in the documentation. Constraints - The option names and values should be correctly formatted as per the Distutils configuration file syntax. - Option names on the command-line should be converted to their configuration file equivalents (e.g., `--foo-bar` to `foo_bar`). Example ```python config = { \'build_ext\': {\'inplace\': 1}, \'bdist_rpm\': { \'release\': 1, \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\' } } result = generate_setup_cfg(config) print(result) # Output should be: [build_ext] inplace=1 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Notes - Ensure that the configuration file content is correctly formatted with newlines separating different command sections. - Multi-line option values (e.g., `doc_files` in the example) should be handled appropriately, ensuring they are correctly concatenated with whitespace. Evaluation - The function will be evaluated based on the correctness of the generated `setup.cfg` content. - Consider edge cases where options might include special characters or need to be split across multiple lines for readability.","solution":"def generate_setup_cfg(configurations: dict) -> str: Generates a setup.cfg content based on the given configurations. Args: configurations (dict): A dictionary where keys are Distutils commands (str), and values are dictionaries of options (str) and their values (str or int). Returns: str: A string representation of the setup.cfg content. config_lines = [] for command, options in configurations.items(): config_lines.append(f\\"[{command}]\\") for option, value in options.items(): config_lines.append(f\\"{option}={value}\\") config_lines.append(\\"\\") # Add a blank line after each command section return \\"n\\".join(config_lines).strip() # An example call to visualize the output config = { \'build_ext\': {\'inplace\': 1}, \'bdist_rpm\': { \'release\': 1, \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\' } } print(generate_setup_cfg(config))"},{"question":"Problem Description You are developing a secure login system for a Unix-based application. The application stores users\' passwords in hashed format and needs to check these hashed passwords during user login attempts. To ensure security, the system should use strong hashing methods and generate salts appropriately. Task Write a Python program that accomplishes the following tasks: 1. Securely stores user passwords by hashing them with the strongest available method. 2. Verifies a user\'s login attempt by comparing the hashed version of the provided password against the stored hashed password. Function Specifications You need to implement the following two functions: 1. **store_password(password: str) -> str:** - **Input**: A plain-text password string. - **Output**: A string representing the hashed password using the strongest available method. - **Functionality**: Generate a hash for the given password using the strongest available method and include a randomly generated salt. 2. **verify_password(stored_hash: str, password_attempt: str) -> bool:** - **Input**: - `stored_hash`: The hashed password string stored during registration. - `password_attempt`: The plain-text password provided during the login attempt. - **Output**: A boolean value indicating whether the password attempt is valid. - **Functionality**: Verify the provided password attempt against the stored hashed password using secure comparison methods. Constraints - Use the `crypt.mksalt()` method to generate a salt. - Utilize the `crypt.crypt()` method for hashing passwords. - Use `hmac.compare_digest` to compare hashed passwords securely. Example ```python import crypt from hmac import compare_digest as compare_hash def store_password(password: str) -> str: # Generate a salt using the strongest available method salt = crypt.mksalt(crypt.METHOD_SHA512) # Return the hashed password return crypt.crypt(password, salt) def verify_password(stored_hash: str, password_attempt: str) -> bool: # Hash the password attempt using the stored hash as the salt return compare_hash(stored_hash, crypt.crypt(password_attempt, stored_hash)) # Example usage hashed_password = store_password(\\"SecurePassword123!\\") print(hashed_password) # Expected: Hashed password with salt is_valid = verify_password(hashed_password, \\"SecurePassword123!\\") print(is_valid) # Expected: True is_invalid = verify_password(hashed_password, \\"WrongPassword123!\\") print(is_invalid) # Expected: False ``` Notes - Ensure that your code handles different hashing methods gracefully, choosing the strongest available method where applicable. - The solution should work properly on Unix-based systems where the `crypt` module is available.","solution":"import crypt from hmac import compare_digest as compare_hash def store_password(password: str) -> str: Hashes a password using the strongest available method and includes a salt. Args: password (str): The plain-text password. Returns: str: The hashed password including the salt. # Generate a salt using the strongest available method salt = crypt.mksalt(crypt.METHOD_SHA512) # Return the hashed password return crypt.crypt(password, salt) def verify_password(stored_hash: str, password_attempt: str) -> bool: Verifies a password attempt against the stored hashed password. Args: stored_hash (str): The stored hashed password. password_attempt (str): The plain-text password attempt. Returns: bool: True if the password attempt is correct, False otherwise. # Hash the password attempt using the stored hash as the salt return compare_hash(stored_hash, crypt.crypt(password_attempt, stored_hash))"},{"question":"Coding Assessment Question **Problem Statement:** You are given an XML file that contains information about various books in a catalog. Each book has the following structure: ```xml <book id=\\"bk101\\"> <author>Author Name</author> <title>Book Title</title> <genre>Genre</genre> <price>Price</price> <publish_date>Publish Date</publish_date> <description>Book Description</description> </book> ``` **Task:** 1. Write a function `parse_books(xml_file)` that takes the path to an XML file as input and returns a list of dictionaries, each representing a book with the following keys: `id`, `author`, `title`, `genre`, `price`, `publish_date`, `description`. 2. Write a function `generate_book_report(xml_file)` that parses the XML file using `parse_books` function, and prints a summary report containing the following information: - Total number of books in the catalog. - The most expensive book\'s title and price. - The average price of all books. - A list of all genres present in the catalog. **Constraints:** - Each book entry in the XML is guaranteed to have all the fields mentioned. - You can assume the XML file is well-formed. **Input:** - `xml_file`: A string representing the path to the XML file. **Output:** - For `parse_books(xml_file)`: A list of dictionaries where each dictionary contains the book attributes. - For `generate_book_report(xml_file)`: Print statements displaying the summary report. **Example:** Suppose the XML file `books.xml` contains the following data: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> ``` Your functions should work as follows: ```python books = parse_books(\'books.xml\') print(books) # Output: [{\'id\': \'bk101\', \'author\': \'Gambardella, Matthew\', \'title\': \\"XML Developer\'s Guide\\", \'genre\': \'Computer\', \'price\': \'44.95\', \'publish_date\': \'2000-10-01\', \'description\': \'An in-depth look at creating applications with XML.\'}, # {\'id\': \'bk102\', \'author\': \'Ralls, Kim\', \'title\': \'Midnight Rain\', \'genre\': \'Fantasy\', \'price\': \'5.95\', \'publish_date\': \'2000-12-16\', \'description\': \'A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.\'}] generate_book_report(\'books.xml\') # Output: # Total number of books in the catalog: 2 # Most expensive book: XML Developer\'s Guide (44.95) # Average book price: 25.45 # Genres in the catalog: {\'Computer\', \'Fantasy\'} ``` **Note:** Ensure to handle the XML parsing exceptions and include appropriate handling for file reading errors. **Required Modules:** Use the `xml.etree.ElementTree` module for parsing the XML.","solution":"import xml.etree.ElementTree as ET def parse_books(xml_file): tree = ET.parse(xml_file) root = tree.getroot() books = [] for book in root.findall(\'book\'): book_data = { \'id\': book.attrib[\'id\'], \'author\': book.find(\'author\').text, \'title\': book.find(\'title\').text, \'genre\': book.find(\'genre\').text, \'price\': float(book.find(\'price\').text), \'publish_date\': book.find(\'publish_date\').text, \'description\': book.find(\'description\').text } books.append(book_data) return books def generate_book_report(xml_file): books = parse_books(xml_file) total_books = len(books) most_expensive_book = max(books, key=lambda x: x[\'price\']) average_price = sum(book[\'price\'] for book in books) / total_books genres = {book[\'genre\'] for book in books} print(f\\"Total number of books in the catalog: {total_books}\\") print(f\\"Most expensive book: {most_expensive_book[\'title\']} ({most_expensive_book[\'price\']})\\") print(f\\"Average book price: {average_price:.2f}\\") print(f\\"Genres in the catalog: {genres}\\")"},{"question":"Objective Implement a Python class that provides methods for dictionary manipulation using the provided `python310` C API function calls. Description You need to implement a class `DictWrapper` using FFI (Foreign Function Interface) to interface with the `python310` C API functions. This class should mimic the behavior of Python\'s built-in `dict` type using the provided C functions. Class: `DictWrapper` Implement the `DictWrapper` class with the following methods: 1. `__init__(self)`: Initializes an empty dictionary. 2. `__setitem__(self, key, value)`: Sets the given key-value pair in the dictionary. 3. `__getitem__(self, key)`: Gets the value associated with the given key. Should raise `KeyError` if the key is not found. 4. `__delitem__(self, key)`: Deletes the given key from the dictionary. Should raise `KeyError` if the key is not found. 5. `__contains__(self, key)`: Checks if the dictionary contains the given key. 6. `clear(self)`: Clears the dictionary. 7. `items(self)`: Returns a list of key-value pairs. 8. `keys(self)`: Returns a list of keys. 9. `values(self)`: Returns a list of values. 10. `update(self, other)`: Updates the dictionary with key-value pairs from another dictionary. Constraints: - Use FFI to call functions from the `python310` C API. - Properly handle errors and edge cases, such as non-hashable keys or invalid operations. Example Usage: ```python # Assume the question provides a way to integrate with the python310 C API # through a specific FFI approach, pseudo-code below: wrapper = DictWrapper() wrapper[\'a\'] = 1 wrapper[\'b\'] = 2 print(wrapper[\'a\']) # Output: 1 print(wrapper[\'b\']) # Output: 2 del wrapper[\'a\'] print(wrapper.items()) # Output: [(\'b\', 2)] print(wrapper.keys()) # Output: [\'b\'] print(wrapper.values()) # Output: [2] wrapper.update({\'c\': 3, \'d\': 4}) print(wrapper.items()) # Output: [(\'b\', 2), (\'c\', 3), (\'d\', 4)] ``` # Requirements: - Your solution should demonstrate efficient use of the C API functions. - Your solution should include error handling for edge cases such as modifying the dictionary during iteration. - Follow best practices in error reporting and exception handling as suggested by the API documentation. You can assume that the necessary FFI setup and basic integration with the `python310` C API is already taken care of. Focus on implementing the dictionary methods as specified.","solution":"class DictWrapper: def __init__(self): self._dict = {} def __setitem__(self, key, value): self._dict[key] = value def __getitem__(self, key): if key in self._dict: return self._dict[key] else: raise KeyError(key) def __delitem__(self, key): if key in self._dict: del self._dict[key] else: raise KeyError(key) def __contains__(self, key): return key in self._dict def clear(self): self._dict.clear() def items(self): return list(self._dict.items()) def keys(self): return list(self._dict.keys()) def values(self): return list(self._dict.values()) def update(self, other): self._dict.update(other)"},{"question":"**Task**: Implement a function that reads from a file and processes the data. The function should demonstrate a comprehensive understanding of exception handling, including predefined clean-up actions using context managers and custom exceptions. Function Signature: ```python def process_file(file_path: str, multiplier: int) -> list: pass ``` Description: 1. The function takes in two arguments: - `file_path` (str) - the path to the file that needs to be read. - `multiplier` (int) - a number by which to multiply each number in the file. 2. The file contains numbers, each on a new line. 3. The function should: - Open the file and read the numbers. - Multiply each number by `multiplier`. - Handle potential exceptions: - Raise a `FileNotFoundError` if the file does not exist. - Raise a `ValueError` if any line in the file cannot be converted to an integer. - Use a custom exception `MultiplierError` if `multiplier` is zero. - Ensure the file is always properly closed after the operations. 4. Return a list of the results. 5. The function should follow these constraints: - Use a context manager to handle file operations. - Define a custom exception `MultiplierError` for zero multipliers. - Handle and raise appropriate built-in exceptions. Example: ```python # Example file content at \'numbers.txt\': # 10 # 20 # 30 result = process_file(\'numbers.txt\', 2) print(result) # Output: [20, 40, 60] result = process_file(\'numbers.txt\', 0) # Raise MultiplierError result = process_file(\'nonexistent.txt\', 2) # Raise FileNotFoundError ``` Custom Exception Example: ```python class MultiplierError(Exception): pass ``` Notes: - Make sure to handle any additional exceptions gracefully and close any resources properly. - You may assume the file contains valid numbers if it exists and if `multiplier` is not zero. Good luck!","solution":"class MultiplierError(Exception): Custom exception for handling zero multiplier issues. def __init__(self, message=\\"Multiplier cannot be zero.\\"): self.message = message super().__init__(self.message) def process_file(file_path: str, multiplier: int) -> list: Reads a file and multiplies each line\'s integer by the given multiplier. Args: file_path (str): The path to the file to read from. multiplier (int): The value to multiply each file number by. Returns: list: A list with multiplied values. Raises: FileNotFoundError: If the file at `file_path` does not exist. ValueError: If a line in the file cannot be converted to an integer. MultiplierError: If the multiplier is zero. if multiplier == 0: raise MultiplierError() results = [] try: with open(file_path, \'r\') as file: for line in file: try: number = int(line.strip()) result = number * multiplier results.append(result) except ValueError: raise ValueError(f\\"Cannot convert line to integer: {line.strip()}\\") except FileNotFoundError: raise FileNotFoundError(f\\"The file \'{file_path}\' was not found.\\") return results"},{"question":"**Calendar Planner** You are tasked with creating a personal calendar planner using the `calendar` module. Your planner should perform the following tasks: 1. Display the calendar for a given year in plain text. 2. Identify the number of weekends (Saturdays and Sundays) in each month of a given year. 3. List all Fridays that fall on the 13th in that year (commonly known as \\"Friday the 13th\\"). 4. Check if a given year is a leap year. **Requirements:** - Implement a class `CalendarPlanner`. - The class should accept the `year` as an argument during initialization. - Implement the following methods: - `display_year()`: Prints the complete calendar for the year. - `weekends_in_month(month)`: Returns the number of Saturdays and Sundays in the specified month for the given year. - `friday_the_13th()`: Returns a list of months that have a Friday falling on the 13th. - `is_leap_year()`: Returns `True` if the year is a leap year, otherwise `False`. **Input and Output:** 1. Initializing the class: ``` planner = CalendarPlanner(2023) ``` 2. Using the `display_year` method: ```python planner.display_year() ``` - Output: The entire calendar for the year 2023. 3. Using the `weekends_in_month` method: ```python weekends_count = planner.weekends_in_month(2) ``` - Input: `2` (the month of February) - Output: Number of weekends in February 2023. 4. Using the `friday_the_13th` method: ```python fridays = planner.friday_the_13th() ``` - Output: A list of months that have Friday the 13th (e.g., [1, 3, 10]). 5. Using the `is_leap_year` method: ```python leap_year = planner.is_leap_year() ``` - Output: `True` if 2023 is a leap year, otherwise `False`. **Constraints:** - The year should be a positive integer greater than 0. - The `month` value in `weekends_in_month` should be between 1 and 12. **Example:** ```python class CalendarPlanner: def __init__(self, year): self.year = year def display_year(self): import calendar print(calendar.TextCalendar().formatyear(self.year)) def weekends_in_month(self, month): import calendar cal = calendar.Calendar() weekends_count = sum(1 for day in cal.itermonthdays2(self.year, month) if day[0] != 0 and (day[1] == calendar.SATURDAY or day[1] == calendar.SUNDAY)) return weekends_count def friday_the_13th(self): import calendar fridays = [month for month in range(1, 13) if calendar.weekday(self.year, month, 13) == calendar.FRIDAY] return fridays def is_leap_year(self): import calendar return calendar.isleap(self.year) ``` **Note:** - You are encouraged to test your `CalendarPlanner` class with different years to ensure all functionalities are correctly implemented.","solution":"import calendar class CalendarPlanner: def __init__(self, year): self.year = year def display_year(self): Prints the complete calendar for the year. print(calendar.TextCalendar().formatyear(self.year)) def weekends_in_month(self, month): Returns the number of Saturdays and Sundays in the specified month for the given year. cal = calendar.Calendar() weekends_count = sum(1 for day in cal.itermonthdays2(self.year, month) if day[0] != 0 and (day[1] == calendar.SATURDAY or day[1] == calendar.SUNDAY)) return weekends_count def friday_the_13th(self): Returns a list of months that have a Friday falling on the 13th. fridays = [month for month in range(1, 12) if calendar.weekday(self.year, month, 13) == calendar.FRIDAY] return fridays def is_leap_year(self): Returns True if the year is a leap year, otherwise False. return calendar.isleap(self.year)"},{"question":"Objective Implement functionality that creates an email header, decodes it, and manipulates it using the `email.header` module. Description You are required to implement a function `process_email_header(header_value, new_value, charset)` that: 1. Creates an email header from the initial `header_value` using the specified `charset`. 2. Appends `new_value` to this header, still using the specified `charset`. 3. Encodes this header so it is RFC-2047 compliant. 4. Decodes this encoded header back into its parts. 5. Creates a new `Header` object from the decoded parts. Expected Input and Output Formats 1. `process_email_header` Inputs: - `header_value` (str): The initial email header value, which may include non-ASCII characters. - `new_value` (str): A new value to append to the initial header. - `charset` (str): The character set to use for encoding and appending the values (e.g., \'iso-8859-1\'). 2. `process_email_header` Output: - Returns a string representation of the new `Header` object. Constraints - Assume valid input: `header_value`, `new_value`, and `charset` will always be strings and properly formatted. - The length of `header_value` and `new_value` will not exceed 1000 characters. - Use the `email.header` module exclusively for header manipulations. Function Template ```python from email.header import Header, decode_header, make_header def process_email_header(header_value, new_value, charset): # Step 1: Create a Header object from initial header_value header = Header(header_value, charset) # Step 2: Append new_value to the header header.append(new_value, charset) # Step 3: Encode the header to make it RFC-2047 compliant encoded_header = header.encode() # Step 4: Decode the encoded header back into parts decoded_parts = decode_header(encoded_header) # Step 5: Create a new Header object from decoded parts new_header = make_header(decoded_parts) # Return string representation of the new Header object return str(new_header) ``` Example ```python # Input header_value = \'pxf6stal\' new_value = \'service\' charset = \'iso-8859-1\' # Processed Output result = process_email_header(header_value, new_value, charset) print(result) # Expected output (line splitting and encoding might vary as per RFC compliance): # pÃ¶stal service ``` Use the provided function template to implement the solution. Make sure to test your implementation with various inputs to ensure it works correctly.","solution":"from email.header import Header, decode_header, make_header def process_email_header(header_value, new_value, charset): # Step 1: Create a Header object from initial header_value header = Header(header_value, charset) # Step 2: Append new_value to the header header.append(new_value, charset) # Step 3: Encode the header to make it RFC-2047 compliant encoded_header = header.encode() # Step 4: Decode the encoded header back into parts decoded_parts = decode_header(encoded_header) # Step 5: Create a new Header object from decoded parts new_header = make_header(decoded_parts) # Return string representation of the new Header object return str(new_header)"},{"question":"**Question: File Resource Management with Python Development Mode** You are tasked with writing a Python script that reads a text file and processes its content while ensuring proper resource management and handling potential errors related to file operations and character encoding/decoding. # Requirements: 1. Implement a function `process_file(file_path: str) -> None` that: - Opens the file specified by `file_path`. - Reads and prints the first 5 lines of the file. - Ensures the file is closed properly. 2. Implement proper handling for various potential issues including: - File not found. - Errors in reading the file due to incorrect file permissions. - Handling encoding errors that might occur while reading the file. 3. The script must be able to run with Python Development Mode enabled, exposing any issues properly. # Input: - **file_path**: A string representing the path to the text file. # Output: - The function should print the first 5 lines of the file. - If an error occurs, output an appropriate error message. # Constraints: - Use context managers (`with` statement) for ensuring files are closed properly. - Make use of exception handling to catch and manage potential file-related errors and encoding/decoding errors. - Assume the text file is UTF-8 encoded, but handle cases where other encodings might cause errors. # Example Usage: ```python try: process_file(\\"example.txt\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Performance Constraints: - The function should handle files of reasonable size efficiently, but you do not need to consider extremely large files beyond normal usage. Example: If an example.txt file contains: ``` Line 1 Line 2 Line 3 Line 4 Line 5 Line 6 ``` Running `process_file(\\"example.txt\\")` should output: ``` Line 1 Line 2 Line 3 Line 4 Line 5 ``` # Note: To test this script in Python Development Mode, you can use the command line option: `python3 -X dev script.py example.txt`.","solution":"def process_file(file_path: str) -> None: Processes a file by reading and printing the first 5 lines. Args: file_path (str): The path to the file to be processed. Raises: FileNotFoundError: If the file does not exist. PermissionError: If there is a permission issue while accessing the file. UnicodeDecodeError: If there is an encoding error while reading the file. try: with open(file_path, \'r\', encoding=\'utf-8\') as file: for i in range(5): line = file.readline() if line == \'\': break print(line, end=\'\') except FileNotFoundError: print(f\\"Error: {file_path} does not exist.\\") except PermissionError: print(f\\"Error: Permission denied for file {file_path}.\\") except UnicodeDecodeError: print(f\\"Error: Failed to decode the file {file_path} due to encoding issues.\\")"},{"question":"# Advanced Coding Assessment **Objective:** Demonstrate your ability to interface with the Python interpreter using the Python/C API, particularly focusing on compiling and executing Python code from different sources. **Scenario:** You are tasked with writing a Python program that utilizes the underlying C API functions to execute Python code. You will need to write a Python extension module that exposes a function to execute Python code from a string or a file. **Requirements:** 1. **Function `execute_code`**: - **Inputs**: - `source: str` - The code to be executed. - `is_file: bool` - A flag indicating if the source is a file path (`True`) or a code string (`False`). - `is_interactive: bool` - A flag indicating if the code should be executed in interactive mode (`True`) or not (`False`). - **Outputs**: - Return the result of executing the code, if any. - If the execution raises an exception, catch it and return an error message. 2. **Functionality**: - If `is_file` is `True`, read the Python code from the file specified by `source`. - If `is_file` is `False`, treat `source` as a Python code string. - Use the appropriate start symbol from the Python grammar based on `is_interactive`: - `Py_single_input` for interactive mode. - `Py_file_input` for non-interactive mode. - Compile and execute the code using the appropriate C API functions. - Handle any exceptions that may occur during the execution and return an appropriate message. **Constraints**: - Ensure that the file is read in binary mode. - Handle cases where the file does not exist or is not accessible. - The function should be robust and handle edge cases gracefully. **Example Usage**: ```python result = execute_code(\\"print(\'Hello, world!\')\\", is_file=False, is_interactive=False) print(result) # Should output: Hello, world! result = execute_code(\\"path/to/script.py\\", is_file=True, is_interactive=False) print(result) # Should execute the script and print its output or return an error message. ``` **Implementation Guidance**: You will need to use Python\'s C API functions such as `Py_CompileStringExFlags`, `PyEval_EvalCode`, `PyRun_File`, and `PyRun_String` to implement this functionality. Make sure to manage the memory allocations and deallocations properly, ensuring no memory leaks. **Performance Requirements**: The function should efficiently handle execution of scripts up to a size of 10MB and should gracefully handle execution time up to 10 seconds per script. Good luck, and happy coding!","solution":"import os def execute_code(source, is_file, is_interactive): Executes the given Python code from a string or a file. Parameters: - source: str, The code to be executed or the path to the file. - is_file: bool, A flag indicating if the source is a file path (True) or a code string (False). - is_interactive: bool, A flag indicating if the code should be executed in interactive mode (True) or not (False). Returns: - The result of executing the code, or an error message if an exception occurs. try: if is_file: if not os.path.exists(source): return \\"File does not exist.\\" with open(source, \'rb\') as f: code = f.read() else: code = source # Set the appropriate start symbol start_symbol = \'single\' if is_interactive else \'exec\' # Compile the code compiled_code = compile(code, \'<string>\' if not is_file else source, start_symbol) # Execute the code exec(compiled_code, globals()) return None # If the code has no return value, we simply return None except Exception as e: return str(e)"},{"question":"**XML Document Processing with xml.dom.minidom** **Objective:** Write a Python function using the `xml.dom.minidom` module to parse an XML document, modify its structure by adding new elements, and then output the modified XML as a string in a formatted manner. **Function Specification:** ```python def modify_xml(xml_string: str) -> str: Parses an XML string, adds a new element to each <slide> element, and returns the modified XML as a formatted string. Args: xml_string (str): A string representation of the XML document. Returns: str: A string representation of the modified and prettified XML document. pass ``` **Input:** - `xml_string`: A string containing a valid XML document representing a slideshow, with elements structured similarly to the example given below. ```xml <slideshow> <title>Demo slideshow</title> <slide> <title>Slide 1 title</title> <point>Introduction to the topic</point> </slide> <slide> <title>Slide 2 title</title> <point>Detailed explanation</point> </slide> </slideshow> ``` **Output:** - The function should return a string representation of the modified XML document. This modified XML should include a new element `<lastUpdated>` with the current date (formatted as `YYYY-MM-DD`), added as the last child of each `<slide>` element. The output should be properly formatted with indentation for readability. **Constraints:** - The function should use `xml.dom.minidom` for all XML manipulations. - The date format for `<lastUpdated>` should be `YYYY-MM-DD`. - Proper handling of XML parsing and potential parsing errors should be demonstrated. **Example:** Given the following input XML string: ```xml <slideshow> <title>Demo slideshow</title> <slide> <title>Slide 1 title</title> <point>Introduction to the topic</point> </slide> <slide> <title>Slide 2 title</title> <point>Detailed explanation</point> </slide> </slideshow> ``` The function should output the following modified and formatted XML string: ```xml <slideshow> <title>Demo slideshow</title> <slide> <title>Slide 1 title</title> <point>Introduction to the topic</point> <lastUpdated>2023-10-01</lastUpdated> </slide> <slide> <title>Slide 2 title</title> <point>Detailed explanation</point> <lastUpdated>2023-10-01</lastUpdated> </slide> </slideshow> ``` (Note: The date in `<lastUpdated>` will be the current date when the function is executed.) **Hints:** - Use the `parseString` function from `xml.dom.minidom` to parse the input XML string. - Create new elements using the `createElement` function and set their text content using the `createTextNode` function. - Use `strftime` from the `datetime` module to get the current date in the desired format. - Utilize the `toprettyxml` method for generating the formatted XML string. **Assessment Criteria:** - Correct parsing and manipulation of the XML document. - Proper usage of `xml.dom.minidom` methods for creating and adding XML elements. - Accurate and formatted output of the modified XML string. - Error handling and code readability.","solution":"from xml.dom.minidom import parseString, Document from datetime import datetime def modify_xml(xml_string: str) -> str: Parses an XML string, adds a new element to each <slide> element, and returns the modified XML as a formatted string. Args: xml_string (str): A string representation of the XML document. Returns: str: A string representation of the modified and prettified XML document. try: # Parse the XML string dom = parseString(xml_string) # Fetch the current date in the required format current_date = datetime.now().strftime(\'%Y-%m-%d\') # Get all the <slide> elements slides = dom.getElementsByTagName(\'slide\') for slide in slides: # Create the new <lastUpdated> element last_updated = dom.createElement(\'lastUpdated\') last_updated.appendChild(dom.createTextNode(current_date)) # Append the new element to the <slide> slide.appendChild(last_updated) # Return the modified XML as a formatted string return dom.toprettyxml(indent=\\" \\") except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"# URL Parsing and Reconstructing Challenge You are tasked with creating a utility to manipulate and validate URLs. The utility should parse URLs into their components and reconstruct them after making some adjustments. Your implementation will focus on functions `urlparse()`, `urlunparse()` and `urljoin()` from the `urllib.parse` module. Task 1. **Function 1: `modify_url`** - Input: ```python def modify_url(url: str, path_append: str, remove_query: bool) -> str: ``` - `url` (str): The URL to parse. - `path_append` (str): The path segment to append to the existing path of the URL. - `remove_query` (bool): A flag indicating whether to remove the query component of the URL. - Output: - (str): The modified URL string. - Process: 1. Parse the input URL into its components using `urllib.parse.urlparse()`. 2. Append `path_append` to the existing path. 3. If `remove_query` is `True`, remove the query component. 4. Recombine the URL components using `urllib.parse.urlunparse()`. 2. **Function 2: `validate_and_join_url`** - Input: ```python def validate_and_join_url(base: str, relative: str) -> str: ``` - `base` (str): The base URL. - `relative` (str): The relative URL to join with the base URL. - Output: - (str): The joined URL string if valid, otherwise raises a `ValueError`. - Process: 1. Join the base URL and the relative URL using `urllib.parse.urljoin()`. 2. Validate the combined URL by ensuring it has a valid scheme (`http` or `https`). 3. If valid, return the combined URL. Otherwise, raise a `ValueError` with an appropriate message. Constraints - Only the schemes \\"http\\" and \\"https\\" are considered valid. - The URL components should be properly encoded or decoded. - Handle edge cases such as empty path segments and redundant queries. Example ```python # Example for modify_url url = \\"https://example.com/path?query=123\\" path_append = \\"/morepath\\" remove_query = True print(modify_url(url, path_append, remove_query)) # Output: \\"https://example.com/path/morepath\\" # Example for validate_and_join_url base = \\"https://example.com/somepath/\\" relative = \\"anotherpath\\" print(validate_and_join_url(base, relative)) # Output: \\"https://example.com/somepath/anotherpath\\" ``` Ensure your solution is robust and handles the edge cases as mentioned. Use the `urllib.parse` module appropriately to parse, modify, and validate URLs.","solution":"from urllib.parse import urlparse, urlunparse, urljoin def modify_url(url: str, path_append: str, remove_query: bool) -> str: parsed_url = urlparse(url) new_path = parsed_url.path.rstrip(\'/\') + \'/\' + path_append.lstrip(\'/\') new_query = \'\' if remove_query else parsed_url.query modified_url = urlunparse(parsed_url._replace(path=new_path, query=new_query)) return modified_url def validate_and_join_url(base: str, relative: str) -> str: joined_url = urljoin(base, relative) parsed_url = urlparse(joined_url) if parsed_url.scheme not in (\'http\', \'https\'): raise ValueError(\\"Invalid URL scheme. Only \'http\' and \'https\' are allowed.\\") return joined_url"},{"question":"**Objective**: Create a function that uses a combination of the given Python built-in functionalities to solve the problem described below. **Task**: Write a function `unique_absolute_sorted(input_list: List[int]) -> List[int]` that takes a list of integers as input and returns a sorted list containing the unique absolute values of the integers. **Requirements**: 1. **Input**: - `input_list` (List[int]): A list of integers, which can contain positive, negative numbers, and zero. Its length can be up to 1000 elements. 2. **Output**: - A list of integers, sorted in ascending order containing only unique absolute values from the `input_list`. 3. **Constraints**: - The `input_list` can contain integers in the range of `-10^6` to `10^6`. - The output list should not contain any duplicate values. - The function should be efficient with a time complexity ideally better than O(n^2). 4. **Implementation Details**: - Use the `abs` function to get the absolute value of numbers. - Use a `set` to automatically handle duplicates when recording the absolute values. - Use the `sorted` function to return the sorted list of unique absolute values. 5. **Example**: ```python print(unique_absolute_sorted([2, -1, 3, -2, 1, 0, -3, -3])) # Output should be: [0, 1, 2, 3] print(unique_absolute_sorted([-5, -1, -5, 0, 4, -4, 5])) # Output should be: [0, 1, 4, 5] ``` **Function Template**: ```python from typing import List def unique_absolute_sorted(input_list: List[int]) -> List[int]: # Your code here pass # Testing your function with provided examples print(unique_absolute_sorted([2, -1, 3, -2, 1, 0, -3, -3])) # Output: [0, 1, 2, 3] print(unique_absolute_sorted([-5, -1, -5, 0, 4, -4, 5])) # Output: [0, 1, 4, 5] ``` Use the built-in functions effectively to showcase your understanding of their utility and also ensure the efficiency of your solution.","solution":"from typing import List def unique_absolute_sorted(input_list: List[int]) -> List[int]: Returns a sorted list of unique absolute values from the input list. unique_abs_values = set(abs(x) for x in input_list) sorted_unique_abs_values = sorted(unique_abs_values) return sorted_unique_abs_values"},{"question":"# Question: Implementing and Managing Context Variables in Python In this exercise, you will simulate the behavior of context variables in Python, similar to the functionality provided by the `contextvars` module\'s C API as detailed in the prompt. You must implement a simplified version of context management for context variables. Below are the tasks you need to complete: 1. **Class `Context`**: - A class to represent a context, which holds a dictionary of context variables. 2. **Class `ContextVar`**: - A class to represent a context variable, with a default value. 3. **Class `Token`**: - A class to represent a token used for resetting context variables. 4. **Context Management**: - Methods to manage contexts, including entering and exiting contexts. 5. **Context Variable Management**: - Methods to set, get, and reset context variables using tokens. # Class Definitions and Method Signatures ```python class Context: def __init__(self): # Initialize an empty dictionary to hold context variables pass def enter(self): # Set this context as the current context pass def exit(self): # Deactivate this context and restore the previous context pass class ContextVar: def __init__(self, name, default=None): # Initialize the context variable with a name and a default value pass def get(self): # Get the value of this context variable for the current context pass def set(self, value): # Set a new value for this context variable in the current context pass def reset(self, token): # Reset the value of this context variable using the provided token pass class Token: def __init__(self, context_var, old_value): # Initialize the token with a reference to the context variable # and the old value before the change pass ``` # Example Usage ```python # Create a context and start it ctx = Context() ctx.enter() # Create a context variable with a default value var = ContextVar(\'example\', default=\'default\') # Set a new value for the context variable token = var.set(\'new_value\') # Get the current value of the context variable assert var.get() == \'new_value\' # Reset the context variable to its previous value var.reset(token) # The value should now be the default value assert var.get() == \'default\' # Exit the context ctx.exit() ``` # Notes - Use class variables or global variables where necessary to keep track of the current context. - Raise appropriate errors for invalid operations, such as resetting with an invalid token. # Constraints - You should not use the `contextvars` module in your implementation. - Your implementation should handle multiple contexts and context variables correctly. - Pay attention to performance, especially in managing the current context and context variables. **Good Luck!**","solution":"class Context: current_context = None def __init__(self): self.vars = {} self.previous_context = None def enter(self): self.previous_context = Context.current_context Context.current_context = self def exit(self): Context.current_context = self.previous_context self.previous_context = None class ContextVar: def __init__(self, name, default=None): self.name = name self.default = default def get(self): if Context.current_context and self.name in Context.current_context.vars: return Context.current_context.vars[self.name] return self.default def set(self, value): if Context.current_context is None: raise RuntimeError(\\"No active context\\") old_value = Context.current_context.vars.get(self.name, self.default) Context.current_context.vars[self.name] = value return Token(self, old_value) def reset(self, token): if not isinstance(token, Token) or token.context_var != self: raise ValueError(\\"Invalid token\\") if Context.current_context is None: raise RuntimeError(\\"No active context\\") Context.current_context.vars[self.name] = token.old_value class Token: def __init__(self, context_var, old_value): self.context_var = context_var self.old_value = old_value"},{"question":"# Array Operations in Python You are tasked with implementing a function that performs specific operations on an array using the Python `array` module. Function Signature ```python def array_operations(a_typecode: str, initializer: list, operations: list) -> list: pass ``` Input: 1. **a_typecode (str):** - A string representing the type code for the array (e.g., `\'i\'`, `\'f\'`, `\'u\'`, etc.). 2. **initializer (list):** - A list of initial values to populate the array. Elements in this list should be compatible with the given type code. 3. **operations (list):** - A list of operation commands to perform on the array. Each operation command is itself a list where: - The first element is a string representing the operation (e.g., `\'append\'`, `\'insert\'`, `\'pop\'`, `\'reverse\'`, etc.). - Subsequent elements depend on the operation (e.g., values to append, indices to insert at). Output: - Returns a list representing the final state of the array after all operations have been performed. Constraints: - The type code will always be valid as per the documentation. - The initializer list will contain elements valid for the given type code. - Operations will be among the set of valid `array` operations as described in the documentation. - The function should handle all exceptions that could arise from invalid operations gracefully. Example: ```python # Example 1 typecode = \'i\' initializer = [1, 2, 3, 4] operations = [[\'append\', 5], [\'insert\', 2, 8], [\'pop\'], [\'remove\', 2], [\'reverse\']] # Expected output: [8, 4, 3, 1] # Example 2 typecode = \'f\' initializer = [1.0, 2.0, 3.0] operations = [[\'append\', 4.5], [\'count\', 3.0], [\'extend\', [5.5, 6.6]], [\'index\', 4.5]] # Expected output: [1.0, 2.0, 3.0, 4.5, 5.5, 6.6] ``` # Guidelines 1. Create an array using the provided type code and initializer. 2. Iterate through the operations list and apply each operation to the array. 3. Handle different array operations like `append`, `insert`, `pop`, `remove`, `reverse`, `count`, etc. 4. Return the final state of the array as a list of its elements. **Note**: You may assume the input operations will always be valid for the given array type. # Hints: - Refer to the [array module documentation](https://docs.python.org/3/library/array.html) for detailed information on each method. - Use exception handling to manage invalid operations or parameters gracefully.","solution":"import array def array_operations(a_typecode: str, initializer: list, operations: list) -> list: arr = array.array(a_typecode, initializer) for operation in operations: op = operation[0] if op == \'append\': arr.append(operation[1]) elif op == \'insert\': arr.insert(operation[1], operation[2]) elif op == \'pop\': if len(operation) > 1: arr.pop(operation[1]) else: arr.pop() elif op == \'remove\': arr.remove(operation[1]) elif op == \'reverse\': arr.reverse() elif op == \'count\': arr.count(operation[1]) elif op == \'extend\': arr.extend(operation[1]) elif op == \'index\': arr.index(operation[1]) elif op == \'buffer_info\': arr.buffer_info() elif op == \'tolist\': arr.tolist() return list(arr)"},{"question":"# Custom Iterator Implementation You have been tasked with creating two custom iterator classes in Python: 1. `SequenceIterator`: An iterator class for arbitrary sequence objects supporting the `__getitem__()` method. 2. `CallableIterator`: An iterator class that works with a callable object and a sentinel value. Requirements 1. **SequenceIterator** - Initialize with a sequence object. - Implement the `__iter__()` and `__next__()` methods. - Raise `StopIteration` when the sequence ends (i.e., when `IndexError` occurs). 2. **CallableIterator** - Initialize with a callable object and a sentinel value. - Implement the `__iter__()` and `__next__()` methods. - Raise `StopIteration` when the callable returns the sentinel value. Input - For `SequenceIterator`: A list or any sequence supporting `__getitem__()`. - For `CallableIterator`: A function that returns the next item in the sequence and a sentinel value indicating the end of the iteration. Output - For `SequenceIterator`: An iterator over the sequence. - For `CallableIterator`: An iterator that yields values from the callable until the sentinel value is reached. Example Usage ```python # Example for SequenceIterator seq = [1, 2, 3] seq_iter = SequenceIterator(seq) print(list(seq_iter)) # Output: [1, 2, 3] # Example for CallableIterator def generator(): value = 0 while value < 5: yield value value += 1 call_iter = CallableIterator(generator().__next__, 5) print(list(call_iter)) # Output: [0, 1, 2, 3, 4] ``` Constraints - You may assume that the sequence input for `SequenceIterator` supports the `__getitem__()` method. - The callable in `CallableIterator` will always be well-behaved, returning the sentinel value at some point. # Implementation Implement the two iterator classes, `SequenceIterator` and `CallableIterator`, according to the above specifications.","solution":"class SequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.sequence): item = self.sequence[self.index] self.index += 1 return item else: raise StopIteration() class CallableIterator: def __init__(self, callable_func, sentinel): self.callable_func = callable_func self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable_func() if value == self.sentinel: raise StopIteration() else: return value"},{"question":"# Question: Advanced Exception Handling and Traceback Formatting You are tasked with writing a Python function that captures and formats exceptions in a given piece of Python code dynamically passed as a string. The function should make use of the `traceback` module to display a nicely formatted traceback when an exception occurs. Problem Statement Write a function `execute_and_trace(code: str) -> str` that takes a string `code` containing Python code and executes it. If the code raises an exception, the function should capture the exception and return a formatted traceback as a string. If no exception is raised, the function should return the string \\"No Exception\\". Function Signature ```python def execute_and_trace(code: str) -> str: pass ``` # Input - `code`: A string containing valid Python code. The code may or may not raise an exception when executed. # Output - A string which is either: - The formatted traceback if an exception occurred, including the exception type and message. - The string \\"No Exception\\" if the code executed without raising any exceptions. # Constraints - The code should be executed within the function dynamically. - The function should use the `traceback` module functionalities to format the traceback. - The formatted traceback should include file names, line numbers, and the code lines where the exception occurred. # Example ```python code1 = x = 1 / 0 print(execute_and_trace(code1)) ``` Expected Output: ``` Traceback (most recent call last): File \\"<string>\\", line 2, in <module> ZeroDivisionError: division by zero ``` ```python code2 = def foo(): return \\"bar\\" foo() print(execute_and_trace(code2)) ``` Expected Output: ``` No Exception ``` # Requirements - Use `exec` to execute the code string. - Use `traceback.format_exc` to capture and format the exception. - Ensure that the output closely mimics the standard Python traceback as shown in the example. # Guidelines - Focus on correct usage of the `traceback` module. - Ensure that your function is robust and can handle any generic Python code passed as a string. - Consider edge cases like syntax errors, runtime errors, and logical errors in the code.","solution":"import traceback def execute_and_trace(code: str) -> str: try: exec(code) return \\"No Exception\\" except Exception as e: return traceback.format_exc()"},{"question":"# Asyncio Event Loop Coding Challenge Objective: To assess the understanding of the asyncio event loop in Python, students are required to create a program that schedules, runs, and manages asynchronous tasks using asyncio. Problem Statement: Create a Python program that utilizes the asyncio library to perform the following tasks: 1. Fetch the content of three different URLs concurrently using asynchronous methods. 2. Periodically (every 2 seconds) print \\"Task is running\\" until all the URLs are fetched. 3. Properly handle and display any exceptions that occur during URL fetching. 4. Gracefully shutdown the event loop after all tasks are completed. Requirements: 1. **Function to Fetch URL Content**: Create an asynchronous function `fetch_url` that takes a URL as an argument and returns its content. 2. **Main Execution Function**: Create an asynchronous function `main` that schedules the fetching of three URLs and prints \\"Task is running\\" every 2 seconds. 3. **Exception Handling**: Implement exception handling to catch and display any errors that occur while fetching URLs. 4. **Ensure Proper Shutdown**: Use necessary methods to ensure the event loop is closed properly after all tasks are completed. Input: - List of URLs: `[\\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.asyncio.org\\"]` Expected Output: - Content of each URL fetched. - Periodic \\"Task is running\\" messages. - Any exceptions encountered. - Confirmation message indicating all tasks are completed and the event loop is shutting down gracefully. Function Signatures: ```python import asyncio async def fetch_url(url: str) -> str: # Implementation here async def main(urls: list): # Implementation here ``` Constraints: - Use built-in libraries such as `asyncio` and consider using `aiohttp` for async HTTP requests, though instructions for aiohttp usage will not be provided. - Handle network errors such as connection timeouts or HTTP errors gracefully. Hints: - Use `asyncio.create_task` to schedule URL fetching tasks. - Use `asyncio.gather` to manage multiple asynchronous tasks concurrently. - Use `loop.run_forever` and `loop.call_later` for managing the periodic print task. - Use exception handling to manage errors within async tasks. Here\'s an example of how to begin: ```python import asyncio import aiohttp async def fetch_url(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def main(urls: list): loop = asyncio.get_running_loop() async def periodic_task(stop_time): while loop.time() < stop_time: print(\\"Task is running\\") await asyncio.sleep(2) loop.stop() tasks = [asyncio.create_task(fetch_url(url)) for url in urls] stop_time = loop.time() + 30 # Periodic task runs for 30 seconds loop.create_task(periodic_task(stop_time)) try: results = await asyncio.gather(*tasks) for result in results: print(result[:100]) # Print first 100 characters of each response except Exception as e: print(f\\"An error occurred: {e}\\") finally: await loop.shutdown_asyncgens() loop.stop() urls = [\\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.asyncio.org\\"] asyncio.run(main(urls)) ``` Submission: - Submit your `fetch_url` and `main` function implementations along with any necessary import statements and helper functions.","solution":"import asyncio import aiohttp async def fetch_url(url: str) -> str: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: response.raise_for_status() return await response.text() except aiohttp.ClientError as e: return f\\"Error fetching {url}: {e}\\" async def main(urls: list): loop = asyncio.get_running_loop() async def periodic_task(): while True: print(\\"Task is running\\") await asyncio.sleep(2) tasks = [asyncio.create_task(fetch_url(url)) for url in urls] periodic_task_handler = loop.create_task(periodic_task()) try: results = await asyncio.gather(*tasks, return_exceptions=True) for i, result in enumerate(results): if isinstance(result, Exception): print(f\\"Task {i} raised an exception: {result}\\") else: print(f\\"Content for URL {urls[i]}: {result[:100]}\\") finally: periodic_task_handler.cancel() try: await periodic_task_handler except asyncio.CancelledError: pass await loop.shutdown_asyncgens() urls = [\\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.asyncio.org\\"] asyncio.run(main(urls))"},{"question":"You are tasked with building a PyTorch model that makes use of the `torch.cond` function to dynamically change its computation path based on the properties of its input tensor. You will implement a custom PyTorch module that uses `torch.cond` to decide between different branches of computation based on the sum of elements in the input tensor. # Task Implement a PyTorch module named `DynamicComputationModel` with the following specifications: 1. The module should have an `__init__` method that initializes the parent class (`torch.nn.Module`). 2. The `forward` method of this module should: - Take a single input tensor `x` of arbitrary shape. - Use `torch.cond` to decide between two functions, `branch_fn1` and `branch_fn2`, based on whether the sum of the elements in `x` is greater than a scalar threshold (let\'s say `10.0`). 3. Implement the `branch_fn1` function to compute the element-wise cosine of the input tensor and add it to the element-wise sine of the input tensor. 4. Implement the `branch_fn2` function to only compute the element-wise sine of the input tensor. 5. The `forward` method should return the result of either `branch_fn1` or `branch_fn2`. # Input Format - A single tensor `x` of shape `(N, D)` where `N` is the number of samples and `D` is the number of features. # Output Format - A tensor of the same shape as `x`, which is the result of applying either `branch_fn1` or `branch_fn2` to `x`. # Constraints - Your implementation should handle tensors with any number of elements and support both CPU and GPU computations. # Example ```python import torch class DynamicComputationModel(torch.nn.Module): def __init__(self): super(DynamicComputationModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def branch_fn1(x: torch.Tensor): return torch.cos(x) + torch.sin(x) def branch_fn2(x: torch.Tensor): return torch.sin(x) threshold = 10.0 return torch.cond(x.sum() > threshold, branch_fn1, branch_fn2, (x,)) # Example usage model = DynamicComputationModel() input_tensor = torch.randn(3, 5) output = model(input_tensor) print(output) ``` # Notes Ensure that your implementation leverages the `torch.cond` operation as described. You may also want to include some basic error handling to account for potential edge cases.","solution":"import torch import torch.nn as nn class DynamicComputationModel(nn.Module): def __init__(self): super(DynamicComputationModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def branch_fn1(x: torch.Tensor) -> torch.Tensor: return torch.cos(x) + torch.sin(x) def branch_fn2(x: torch.Tensor) -> torch.Tensor: return torch.sin(x) threshold = 10.0 mask = x.sum() > threshold if mask.item(): return branch_fn1(x) else: return branch_fn2(x) # Usage example model = DynamicComputationModel() input_tensor = torch.randn(3, 5) output = model(input_tensor) print(output)"},{"question":"Objective Demonstrate your understanding of creating and customizing plots using Seaborn\'s `so.Plot` interface. You will load a dataset, create a specific plot, and apply customizations as described below. Question You are provided with the `penguins` dataset in Seaborn. Your task is to write a function `plot_penguin_data` that generates a specific visualization with the following requirements: 1. Load the `penguins` dataset from Seaborn. 2. Create a scatter plot showing `flipper_length_mm` on the x-axis and `bill_length_mm` on the y-axis. 3. Color the points based on the `species` variable. 4. Facet the plot by the `island` variable, creating separate subplots for each island. 5. Add linear regression lines with confidence intervals to each subplot to show the relationship between `flipper_length_mm` and `bill_length_mm`. 6. Customize the plot to use different `markers` for each `sex` and set the `linewidth` of the regression lines to 2. Function Signature ```python def plot_penguin_data(): pass ``` Constraints - Ensure the plot is formatted clearly and transparently. - Use Seabornâ€™s `so.Plot` for the entire plotting process. - Use appropriate functions and methods to achieve the required visualizations and customizations. Example Your output should include: - A scatter plot of `flipper_length_mm` vs. `bill_length_mm`, with points colored by `species`. - Separate subplots for each island (faceting by `island`). - Linear regression lines with confidence intervals per facet. - Different markers for each `sex`. - Regression lines with a linewidth of 2. ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): penguins = load_dataset(\\"penguins\\") # Implementing the plot as per the requirements ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", color=\\"species\\", marker=\\"sex\\") .facet(\\"island\\") .add(so.Dot()) .add(so.Line(marker=\\"o\\"), so.PolyFit(), linewidth=2) ).show() plot_penguin_data() ``` Note The example implementation is given in partial and illustrative purposes. The actual implementation might need modifications based on the latest Seaborn version and its functionalities.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): # Load the penguins dataset from Seaborn penguins = load_dataset(\\"penguins\\") # Creating the required plot ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", color=\\"species\\", marker=\\"sex\\") .facet(\\"island\\") .add(so.Dot()) .add(so.Line(linewidth=2), so.PolyFit()) ).show()"},{"question":"You are required to implement a function `broadcastable_shape(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Size` that determines the resulting shape of two tensors when broadcasted together or raises an appropriate error if they are not broadcastable. This will demonstrate your understanding of PyTorch\'s broadcasting semantics. # Function Signature ```python import torch from typing import Tuple def broadcastable_shape(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Size: pass ``` # Input - `tensor1`: A `torch.Tensor` object with any shape. - `tensor2`: Another `torch.Tensor` object with any shape. # Output - Returns a `torch.Size` object which would be the resultant shape after broadcasting `tensor1` and `tensor2`. - Raises `ValueError` with the message \\"Tensors are not broadcastable\\" if the tensors cannot be broadcast together. # Examples ```python import torch # Example 1: tensor1 = torch.empty(5, 1, 4, 1) tensor2 = torch.empty(3, 1, 1) # broadcastable_shape(tensor1, tensor2) should return torch.Size([5, 3, 4, 1]) # Example 2: tensor1 = torch.empty(1) tensor2 = torch.empty(3, 1, 7) # broadcastable_shape(tensor1, tensor2) should return torch.Size([3, 1, 7]) # Example 3: tensor1 = torch.empty(5, 2, 4, 1) tensor2 = torch.empty(3, 1, 1) # broadcastable_shape(tensor1, tensor2) should raise ValueError ``` # Constraints - The input tensors will always have at least one dimension each. - You may use built-in methods and properties of `torch.Tensor` for tensor manipulation, but you must implement the broadcasting logic yourself. # Notes - Pay close attention to correctly handling in-place operation constraints. - Review the provided broadcasting rules carefully to ensure your implementation accurately complies with PyTorch\'s semantics. - Consider edge cases where one tensor might have different dimensions that need to be \\"aligned\\". # Performance Requirements Your solution should be efficient and handle both small and large tensors gracefully.","solution":"import torch from typing import Tuple def broadcastable_shape(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Size: shape1 = list(tensor1.shape) shape2 = list(tensor2.shape) # Reverse shapes for right to left comparison shape1.reverse() shape2.reverse() result_shape = [] for i in range(max(len(shape1), len(shape2))): dim1 = shape1[i] if i < len(shape1) else 1 # If shape1 is shorter, substitute with 1 dim2 = shape2[i] if i < len(shape2) else 1 # If shape2 is shorter, substitute with 1 if dim1 == 1: result_shape.append(dim2) elif dim2 == 1: result_shape.append(dim1) elif dim1 == dim2: result_shape.append(dim1) else: raise ValueError(\\"Tensors are not broadcastable\\") result_shape.reverse() return torch.Size(result_shape)"},{"question":"Objective: To assess students\' understanding of functional programming concepts in Python, focusing on iterators, generators, and the itertools module. Problem Statement: Write a Python function `filter_and_chain` that takes two arguments: 1. `iterables`: a list of iterable objects. 2. `predicates`: a list of predicate functions that correspond to each iterable in `iterables`. Your task is to: 1. Filter each iterable in `iterables` using the corresponding predicate in `predicates`. 2. Chain the filtered results together into a single iterable. 3. Convert the combined iterable into a list and return it. Function Signature: ```python def filter_and_chain(iterables: list, predicates: list) -> list: ``` Input: - `iterables`: A list of iterable objects (e.g., lists, tuples). - Example: `[[1, 2, 3, 4], (5, 6, 7, 8)]` - `predicates`: A list of functions that take a single argument and return a boolean. - Example: `[lambda x: x % 2 == 0, lambda x: x > 6]` Output: - A list containing the elements from the chained iterable after filtering. - Example: `[2, 4, 7, 8]` Constraints: - The length of `iterables` and `predicates` will be the same. - There will be at least one iterable and one predicate. - Each predicate will be a valid function. - The input iterables may contain any hashable elements that are compatible with the predicates. Example: ```python iterables = [[1, 2, 3, 4], (10, 15, 20, 25)] predicates = [lambda x: x % 2 == 0, lambda x: x > 15] assert filter_and_chain(iterables, predicates) == [2, 4, 20, 25] ``` Performance Requirements: The implementation should efficiently handle filtering and chaining large iterables by making use of iterators and generator expressions where applicable.","solution":"from itertools import chain def filter_and_chain(iterables: list, predicates: list) -> list: Filters each iterable in iterables with the corresponding predicate in predicates, then chains the filtered results into a single iterable and converts it into a list. filtered_iterables = (filter(pred, iterable) for iterable, pred in zip(iterables, predicates)) return list(chain.from_iterable(filtered_iterables))"},{"question":"# Advanced Coding Assessment Question: Problem Statement You are tasked with implementing a simplified version of the `mailcap.findmatch` function as described in the documentation provided. Your version will only need to handle a limited set of functionalities. Specifically, your function should: 1. Find an appropriate mailcap entry for a given MIME type. 2. Substitute the `%s` placeholder in the command line with a provided filename. 3. Return the resulting command line. Your implementation does not need to handle parameter substitution (e.g., `%{foo}`), or \\"test\\" field evaluation, but it should follow security precautions to prevent the injection of ASCII characters other than alphanumerics and \\"@+=:,./-_\\". Function Signature ```python def simplified_findmatch(caps: dict, MIMEtype: str, filename: str = \\"/dev/null\\") -> str: pass ``` Input 1. `caps (dict)`: A dictionary mapping MIME types to a list of mailcap file entries. Each entry is a dictionary with keys representing different fields (e.g., \'view\', \'edit\'). 2. `MIMEtype (str)`: A string representing the MIME type you are looking for. 3. `filename (str)`: A string representing the filename to be substituted in the command. Default is \\"/dev/null\\". Output - A string representing the command line with the `%s` substituted by the provided filename. If no matching MIME type is found or if the filename contains any disallowed characters, return `None`. Constraints - The `filename` must only contain alphanumeric characters and \\"@+=:,./-_\\". - You can assume the `caps` dictionary is correctly formed and contains only list of dictionaries as values. Example ```python # Example of caps dictionary structure: caps = { \\"video/mpeg\\": [{\\"view\\": \\"xmpeg %s\\"}], \\"image/png\\": [{\\"view\\": \\"eog %s\\"}, {\\"edit\\": \\"gimp %s\\"}] } # Example usage: command_line = simplified_findmatch(caps, \\"video/mpeg\\", \\"tmp1223\\") print(command_line) # Output: \\"xmpeg tmp1223\\" command_line = simplified_findmatch(caps, \\"image/png\\", \\"imagefile\\") print(command_line) # Output: \\"eog imagefile\\" command_line = simplified_findmatch(caps, \\"application/pdf\\", \\"document.pdf\\") print(command_line) # Output: None (no matching MIME type found) command_line = simplified_findmatch(caps, \\"image/png\\", \\"invalid;file\\") print(command_line) # Output: None (filename contains disallowed characters) ``` Notes - Your function should handle only the `view` key in the mailcap entries. - Ensure you properly sanitize the `filename` to avoid security issues. - If a valid entry is found but the `filename` contains disallowed characters, return `None`.","solution":"import re def simplified_findmatch(caps: dict, MIMEtype: str, filename: str = \\"/dev/null\\") -> str: Finds the appropriate mailcap entry for a given MIME type and substitutes the filename into the command. It returns the resulting command line. Parameters: - caps: A dictionary mapping MIME types to a list of mailcap file entries. - MIMEtype: A string representing the MIME type you are looking for. - filename: A string representing the filename to be substituted in the command. Returns: - A string representing the command line with the `%s` substituted by the provided filename. If no matching MIME type is found or if the filename contains any disallowed characters, returns `None`. # Define allowed characters for the filename allowed_chars_pattern = re.compile(r\'^[a-zA-Z0-9@+=:,./_-]+\') # Check if filename contains only allowed characters if not allowed_chars_pattern.match(filename): return None # Find MIME type in caps if MIMEtype in caps: for entry in caps[MIMEtype]: if \'view\' in entry: command = entry[\'view\'].replace(\'%s\', filename) return command # No valid entry found return None"},{"question":"**Problem Statement:** You are tasked with creating a Python script that performs a series of operations using the `os` module. Your script should be able to create a directory, populate it with several files, manage environment variables, and finally handle a subprocess execution. # Requirements: 1. **Directory Creation and Populating:** - Create a directory named `test_dir`. - Inside `test_dir`, create 5 text files named `file1.txt`, `file2.txt`, `file3.txt`, `file4.txt`, and `file5.txt`. - Each file should contain the text \\"Hello, World!\\" followed by its respective file number. 2. **Environment Variables:** - Set an environment variable `MY_VAR` to the value `42`. - Retrieve and print the value of `MY_VAR`. 3. **Reading File Contents:** - List all files in the `test_dir` directory. - Read and print the contents of each file. 4. **Subprocess Execution:** - Create a subprocess that executes the `ls` command (or `dir` on Windows) to list the contents of `test_dir`. - Capture and print the output of this subprocess. # Input: No input is required for this problem. # Output: - Print the value of the environment variable `MY_VAR`. - Print the contents of each file in `test_dir`. - Print the output of the `ls` command (or `dir` on Windows). # Constraints: - You must use functions from the `os` module where appropriate. - Ensure the script handles exceptions that might arise (like the directory already existing or file operations failing). # Example Output: ``` Value of MY_VAR: 42 Contents of file1.txt: Hello, World! 1 Contents of file2.txt: Hello, World! 2 Contents of file3.txt: Hello, World! 3 Contents of file4.txt: Hello, World! 4 Contents of file5.txt: Hello, World! 5 Contents of test_dir directory: file1.txt file2.txt file3.txt file4.txt file5.txt ``` # Your Task: Implement the function `perform_os_operations()` which executes all the tasks described above. ```python import os import subprocess def perform_os_operations(): # Your code here pass # Call the function to test perform_os_operations() ``` **Notes:** - Ensure you consider cross-platform compatibility where necessary, especially for the subprocess command execution. - Use appropriate exception handling to manage errors that could arise from OS operations.","solution":"import os import subprocess def perform_os_operations(): # Create the directory dir_name = \\"test_dir\\" try: os.mkdir(dir_name) except FileExistsError: pass # Create and populate the text files inside the directory for i in range(1, 6): with open(os.path.join(dir_name, f\\"file{i}.txt\\"), \\"w\\") as f: f.write(f\\"Hello, World! {i}\\") # Set the environment variable os.environ[\'MY_VAR\'] = \'42\' # Retrieve and print the value of the environment variable my_var_value = os.getenv(\'MY_VAR\') print(f\\"Value of MY_VAR: {my_var_value}\\") # List the files in the directory and print their contents files = os.listdir(dir_name) for file in files: with open(os.path.join(dir_name, file), \\"r\\") as f: content = f.read() print(f\\"Contents of {file}: {content}\\") # Use subprocess to list the contents of the directory command = \'ls\' if os.name != \'nt\' else \'dir\' result = subprocess.run([command, dir_name], capture_output=True, text=True) print(f\\"Contents of {dir_name} directory:n{result.stdout}\\") # Call the function to test perform_os_operations()"},{"question":"# Advanced Python Codec Management and Customization In this task, you will demonstrate your understanding of the python310 codec registry and support functions. You are required to implement a series of functions to manage and utilize codecs effectively, including registering/unregistering custom codecs, encoding and decoding data, and handling encoding errors. Function 1: Register and Unregister Custom Codec Create functions to register and unregister a custom codec search function. ```python def register_custom_codec(search_function): Registers a custom codec search function. Parameters: search_function (callable): The codec search function to register. Returns: int: 0 on success, raises an exception otherwise. pass def unregister_custom_codec(search_function): Unregisters a custom codec search function. Parameters: search_function (callable): The codec search function to unregister. Returns: int: 0 on success, raises an exception otherwise. pass ``` Function 2: Check Known Encoding Implement a function to check if a given encoding is known. ```python def is_known_encoding(encoding): Checks if the given encoding is known. Parameters: encoding (str): The encoding to check. Returns: bool: True if the encoding is known, False otherwise. pass ``` Function 3: Encode and Decode Data Create functions to encode and decode a given object using specified encodings. ```python def encode_data(data, encoding, errors=None): Encodes the data using the specified encoding. Parameters: data (object): The data to encode. encoding (str): The encoding to use. errors (str, optional): The error handling scheme. Defaults to None. Returns: object: The encoded data. pass def decode_data(data, encoding, errors=None): Decodes the data using the specified encoding. Parameters: data (object): The data to decode. encoding (str): The encoding to use. errors (str, optional): The error handling scheme. Defaults to None. Returns: object: The decoded data. pass ``` Function 4: Register and Use Custom Error Handler Implement functions to register a custom error handler and use it during encoding/decoding. ```python def register_error_handler(name, error_callback): Registers a custom error handling function. Parameters: name (str): The name of the error handler. error_callback (callable): The error handler function. Returns: int: 0 on success, raises an exception otherwise. pass def encode_data_with_custom_error(data, encoding, error_handler_name): Encodes the data using the specified encoding and a custom error handler. Parameters: data (object): The data to encode. encoding (str): The encoding to use. error_handler_name (str): The name of the custom error handler. Returns: object: The encoded data. pass def decode_data_with_custom_error(data, encoding, error_handler_name): Decodes the data using the specified encoding and a custom error handler. Parameters: data (object): The data to decode. encoding (str): The encoding to use. error_handler_name (str): The name of the custom error handler. Returns: object: The decoded data. pass ``` # Instructions 1. Implement the functions defined above according to the provided specifications. 2. Ensure proper error handling for invalid inputs and edge cases. 3. Test the functions with various encoding schemes and custom error handlers. 4. Document the code clearly, explaining the logic and any assumptions made. # Constraints - The custom codec search function, custom error callback, and codec/encoding types should be valid and properly implemented/tested as per python310 documentation. - The error handling function should handle both encoding and decoding errors robustly. - Focus on performance optimization for encoding and decoding large datasets. # Evaluation Criteria - Correctness of the implementation. - Efficient handling of encoding/decoding operations. - Proper registration and utilization of codecs and error handlers. - Thorough testing and documentation.","solution":"import codecs def register_custom_codec(search_function): Registers a custom codec search function. Parameters: search_function (callable): The codec search function to register. Returns: int: 0 on success. codecs.register(search_function) return 0 def unregister_custom_codec(search_function): Unregisters a custom codec search function. Parameters: search_function (callable): The codec search function to unregister. Returns: int: 0 on success. codecs.unregister(search_function) return 0 def is_known_encoding(encoding): Checks if the given encoding is known. Parameters: encoding (str): The encoding to check. Returns: bool: True if the encoding is known, False otherwise. try: codecs.lookup(encoding) return True except LookupError: return False def encode_data(data, encoding, errors=\'strict\'): Encodes the data using the specified encoding. Parameters: data (str): The data to encode. encoding (str): The encoding to use. errors (str, optional): The error handling scheme. Defaults to \'strict\'. Returns: bytes: The encoded data. return data.encode(encoding, errors) def decode_data(data, encoding, errors=\'strict\'): Decodes the data using the specified encoding. Parameters: data (bytes): The data to decode. encoding (str): The encoding to use. errors (str, optional): The error handling scheme. Defaults to \'strict\'. Returns: str: The decoded data. return data.decode(encoding, errors) def register_error_handler(name, error_callback): Registers a custom error handling function. Parameters: name (str): The name of the error handler. error_callback (callable): The error handler function. Returns: int: 0 on success. codecs.register_error(name, error_callback) return 0 def encode_data_with_custom_error(data, encoding, error_handler_name): Encodes the data using the specified encoding and a custom error handler. Parameters: data (str): The data to encode. encoding (str): The encoding to use. error_handler_name (str): The name of the custom error handler. Returns: bytes: The encoded data. return data.encode(encoding, errors=error_handler_name) def decode_data_with_custom_error(data, encoding, error_handler_name): Decodes the data using the specified encoding and a custom error handler. Parameters: data (bytes): The data to decode. encoding (str): The encoding to use. error_handler_name (str): The name of the custom error handler. Returns: str: The decoded data. return data.decode(encoding, errors=error_handler_name)"},{"question":"# Advanced Coding Assessment: Processing Sun AU Audio Files **Objective:** Write a Python function to read an input Sun AU audio file, apply a simple transformation to its audio data, and save the transformed data to a new Sun AU audio file. **Description:** You need to create a function `transform_au(file_in: str, file_out: str, volume_factor: float) -> None` that: 1. Reads an input AU file specified by `file_in`. 2. Adjusts the volume of the audio data by the specified `volume_factor`. 3. Writes the adjusted audio data into a new AU file specified by `file_out`. **Function Signature:** ```python def transform_au(file_in: str, file_out: str, volume_factor: float) -> None: pass ``` **Requirements:** 1. **Reading**: - Open the input file in read mode using `sunau.open`. - Retrieve the audio parameters (number of channels, sample width, frame rate, etc.). - Read the audio frames. 2. **Transformation**: - Apply the volume adjustment to each audio frame. You can assume the audio data is linear PCM format. - Consider the sample width when applying the volume factor to ensure correct amplification. 3. **Writing**: - Open the output file in write mode using `sunau.open`. - Set the same audio parameters retrieved from the input file. - Write the adjusted audio frames to the output file. **Constraints:** - The volume factor is a float where `1.0` means no change, `0.5` halves the volume, and `2.0` doubles it. - Ensure the sample width is considered when applying the volume factor to avoid data corruption. - Handle only uncompressed linear PCM data for simplicity (`comptype` should be `\'NONE\'`). **Input:** - `file_in`: Path to the input AU file (str). - `file_out`: Path to the output AU file (str). - `volume_factor`: Multiplicative factor for volume adjustment (float). **Output:** - None. The function should save the transformed audio to `file_out`. **Example:** ```python transform_au(\'input.au\', \'output.au\', 1.5) ``` In this example, the function reads the `\'input.au\'` file, increases its volume by 50%, and writes the result to `\'output.au\'`. **Note:** Use appropriate exception handling to manage file operations and ensure any resources are properly closed.","solution":"import sunau import numpy as np def transform_au(file_in: str, file_out: str, volume_factor: float) -> None: with sunau.open(file_in, \'rb\') as infile: params = infile.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params if comptype != \'NONE\': raise ValueError(\\"Unsupported compression type\\") audio_data = infile.readframes(n_frames) audio_array = np.frombuffer(audio_data, dtype=f\'int{8 * sampwidth}\') # Apply volume factor audio_array = audio_array * volume_factor audio_array = np.clip(audio_array, -2**(8 * sampwidth - 1), 2**(8 * sampwidth - 1) - 1) adjusted_data = audio_array.astype(f\'int{8 * sampwidth}\') adjusted_data_bytes = adjusted_data.tobytes() with sunau.open(file_out, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(adjusted_data_bytes)"},{"question":"**Coding Challenge: Advanced Data Class Management** # Background You are working on a logistics application that requires managing information about various storage facilities and their inventories. Each storage facility can store multiple types of items, and the information about each item is dynamic and can change over time. We\'ll use Python\'s dataclasses to efficiently manage this complex data. # Task 1. **Define a Data Class, `StorageItem`**: - Fields: - `item_id` (str): A unique identifier for the item. - `name` (str): The name of the item. - `quantity` (int): The quantity of the item available. - `price_per_unit` (float): The price per unit of the item. - `metadata` (dict): Additional information about the item. - The `metadata` field should have a default value of an empty dictionary, ensuring each instance has its own independent dictionary. - Implement a method `total_value` that returns the total value of the item (quantity * price_per_unit). - Use the `@dataclass` decorator with appropriate parameters to generate the necessary methods and ensure the class is hashable and comparable based on `item_id`. 2. **Define a Data Class, `StorageFacility`**: - Fields: - `facility_id` (str): A unique identifier for the facility. - `name` (str): The name of the facility. - `location` (str): The location of the facility. - `items` (List[StorageItem]): A list of `StorageItem` objects stored at this facility. - Implement a method `total_inventory_value` that calculates the total value of all items in the facility. - Use the `@dataclass` decorator with parameters to generate the necessary methods, ensuring the class is frozen (immutable) except through initialization. 3. **Function to Add an Item**: - Write a function, `add_item(storage_facility: StorageFacility, item: StorageItem) -> StorageFacility`, that returns a new instance of `StorageFacility` with the provided item added to the `items` list. # Requirements: - Provide a proper `__str__` or `__repr__` implementation for both data classes so that their representations are clear and informative. - Design the classes and functions keeping in mind performance and code readability. # Example Usage ```python item1 = StorageItem(item_id=\\"123\\", name=\\"Widget\\", quantity=10, price_per_unit=2.5) item2 = StorageItem(item_id=\\"124\\", name=\\"Gadget\\", quantity=5, price_per_unit=3.5) facility = StorageFacility(facility_id=\\"F001\\", name=\\"Main Warehouse\\", location=\\"1234 Elm St\\", items=[item1]) print(facility.total_inventory_value()) # Output: 25.0 facility = add_item(facility, item2) print(facility.total_inventory_value()) # Output: 42.5 ``` **Constraints**: - The `item_id` in `StorageItem` should be unique for equality and hashing purposes. - Ensure that the functions work with a large number of items and facilities efficiently.","solution":"from dataclasses import dataclass, field from typing import List, Dict @dataclass(frozen=True, eq=True) class StorageItem: item_id: str name: str quantity: int price_per_unit: float metadata: Dict[str, str] = field(default_factory=dict) def total_value(self) -> float: return self.quantity * self.price_per_unit def __repr__(self) -> str: return (f\\"StorageItem(item_id={self.item_id}, name={self.name}, \\" f\\"quantity={self.quantity}, price_per_unit={self.price_per_unit}, metadata={self.metadata})\\") @dataclass(frozen=True) class StorageFacility: facility_id: str name: str location: str items: List[StorageItem] def total_inventory_value(self) -> float: return sum(item.total_value() for item in self.items) def __repr__(self) -> str: return (f\\"StorageFacility(facility_id={self.facility_id}, name={self.name}, \\" f\\"location={self.location}, items={self.items})\\") def add_item(storage_facility: StorageFacility, item: StorageItem) -> StorageFacility: new_items = storage_facility.items + [item] return StorageFacility( facility_id=storage_facility.facility_id, name=storage_facility.name, location=storage_facility.location, items=new_items )"},{"question":"**Objective:** To assess your understanding of the `configparser` module in Python by requiring you to implement a function that reads, modifies, and writes configuration files using various features of the `configparser` module. **Problem Statement:** You are provided with a string representing the content of an INI file. Write a function `process_config(config_string: str) -> str` that performs the following operations: 1. Read the configuration from the given string. 2. If a section named `settings` does not exist, create it. 3. Add a new option `enabled` under the `settings` section with the value `True`. 4. For an already existing section `user`, interpolate a new option `home_dir` from an option `base_dir` in the `paths` section. The `home_dir` should be set as `%(base_dir)s/users`. 5. Convert the value of the `max_tries` option in the `operations` section to an integer. If it is less than 5, set it to 5. 6. Write the modified configuration to a string and return it. **Constraints:** - Sections and options names are case-insensitive. - Options without values should be handled. - Inline comments should be supported. **Input:** - A string `config_string` that represents the content of an INI file. **Output:** - A string representing the modified configuration. **Example:** ```python config_str = [paths] base_dir=/usr/local [user] name=John [operations] max_tries=3 output = process_config(config_str) print(output) # Expected output: [paths] base_dir=/usr/local [user] name=John home_dir=/usr/local/users [operations] max_tries=5 [settings] enabled=True ``` **Function Signature:** ```python def process_config(config_string: str) -> str: pass ``` **Notes:** 1. Ensure you handle exceptions where sections or options might be missing. 2. You may use the `configparser` module\'s `ConfigParser` class and its methods to solve this problem.","solution":"import configparser from io import StringIO def process_config(config_string: str) -> str: config = configparser.ConfigParser() config.read_string(config_string) # Ensure \'settings\' section exists if \'settings\' not in config: config.add_section(\'settings\') # Set \'enabled\' option in \'settings\' section config[\'settings\'][\'enabled\'] = \'True\' # Ensure \'home_dir\' in \'user\' section interpolates from \'base_dir\' in \'paths\' section if \'user\' in config and \'paths\' in config: base_dir = config[\'paths\'].get(\'base_dir\', \'\') if base_dir: config[\'user\'][\'home_dir\'] = f\'{base_dir}/users\' # Convert \'max_tries\' in \'operations\' section to an integer and set to 5 if less than 5 if \'operations\' in config: max_tries = config[\'operations\'].getint(\'max_tries\', fallback=5) if max_tries < 5: config[\'operations\'][\'max_tries\'] = \'5\' # Convert the modified config back to a string output = StringIO() config.write(output) return output.getvalue()"},{"question":"Analyzing Movie Data with Pandas Background You are given a dataset containing information about various movies. The dataset includes details such as the title, genre, director, release year, IMDb rating, and box office collection. Your task is to write a function to analyze this dataset to provide insightful statistics and visualizations. Dataset Description The dataset is a CSV file named `movies.csv` with the following columns: - `Title`: The title of the movie (string). - `Genre`: The genre of the movie (string). - `Director`: The director of the movie (string). - `Year`: The release year of the movie (integer). - `Rating`: The IMDb rating of the movie (float). - `BoxOffice`: The box office collection of the movie in millions (float). Task Write a function `analyze_movies` which takes the path to the CSV file as input and performs the following operations: 1. **Read the CSV File**: Load the data into a pandas DataFrame. 2. **Data Cleaning**: - Handle missing values appropriately by either filling them with suitable statistics or dropping rows/columns. - Ensure the data types are correct. 3. **Basic Analysis**: - Calculate and print the average IMDb rating per genre. - Determine and print the director with the highest average box office collection. 4. **Advanced Analysis**: - Create a new column `Decade` that categorizes movies by the decade they were released. - Calculate the average IMDb rating per decade and plot it as a line chart. 5. **Visualization**: - Plot a bar chart showing the number of movies released per genre. Input - `file_path` (string): The path to the CSV file containing the movie data. Output The function should print the required statistical results and show the plots. Constraints - You must use pandas for data manipulation and analysis. - Ensure the code handles edge cases such as missing values and inconsistent data types gracefully. Example ```python def analyze_movies(file_path): import pandas as pd import matplotlib.pyplot as plt # Step 1: Read the CSV File df = pd.read_csv(file_path) # Step 2: Data Cleaning df.fillna({ \'Rating\': df[\'Rating\'].mean(), \'BoxOffice\': df[\'BoxOffice\'].mean(), }, inplace=True) df[\'Year\'] = pd.to_numeric(df[\'Year\'], errors=\'coerce\') df[\'BoxOffice\'] = pd.to_numeric(df[\'BoxOffice\'], errors=\'coerce\') df.dropna(subset=[\'Year\'], inplace=True) # Step 3: Basic Analysis genre_avg_rating = df.groupby(\'Genre\')[\'Rating\'].mean() print(f\'Average IMDb rating per genre:n{genre_avg_rating}\') director_boxoffice_avg = df.groupby(\'Director\')[\'BoxOffice\'].mean() top_director = director_boxoffice_avg.idxmax() print(f\'The director with the highest average box office collection: {top_director}\') # Step 4: Advanced Analysis df[\'Decade\'] = (df[\'Year\'] // 10) * 10 decade_avg_rating = df.groupby(\'Decade\')[\'Rating\'].mean() plt.figure(figsize=(10, 5)) decade_avg_rating.plot(kind=\'line\') plt.title(\'Average IMDb Rating per Decade\') plt.xlabel(\'Decade\') plt.ylabel(\'Average IMDb Rating\') plt.grid(True) plt.show() # Step 5: Visualization genre_movie_count = df[\'Genre\'].value_counts() plt.figure(figsize=(10, 5)) genre_movie_count.plot(kind=\'bar\') plt.title(\'Number of Movies Released per Genre\') plt.xlabel(\'Genre\') plt.ylabel(\'Number of Movies\') plt.show() ``` You are encouraged to follow the same steps in your function implementation and can use the example as a reference.","solution":"def analyze_movies(file_path): import pandas as pd import matplotlib.pyplot as plt # Step 1: Read the CSV File df = pd.read_csv(file_path) # Step 2: Data Cleaning df.fillna({ \'Rating\': df[\'Rating\'].mean(), \'BoxOffice\': df[\'BoxOffice\'].mean(), }, inplace=True) df[\'Year\'] = pd.to_numeric(df[\'Year\'], errors=\'coerce\') df[\'BoxOffice\'] = pd.to_numeric(df[\'BoxOffice\'], errors=\'coerce\') df.dropna(subset=[\'Year\'], inplace=True) # Step 3: Basic Analysis genre_avg_rating = df.groupby(\'Genre\')[\'Rating\'].mean() print(f\'Average IMDb rating per genre:n{genre_avg_rating}\') director_boxoffice_avg = df.groupby(\'Director\')[\'BoxOffice\'].mean() top_director = director_boxoffice_avg.idxmax() print(f\'The director with the highest average box office collection: {top_director}\') # Step 4: Advanced Analysis df[\'Decade\'] = (df[\'Year\'] // 10) * 10 decade_avg_rating = df.groupby(\'Decade\')[\'Rating\'].mean() plt.figure(figsize=(10, 5)) decade_avg_rating.plot(kind=\'line\') plt.title(\'Average IMDb Rating per Decade\') plt.xlabel(\'Decade\') plt.ylabel(\'Average IMDb Rating\') plt.grid(True) plt.show() # Step 5: Visualization genre_movie_count = df[\'Genre\'].value_counts() plt.figure(figsize=(10, 5)) genre_movie_count.plot(kind=\'bar\') plt.title(\'Number of Movies Released per Genre\') plt.xlabel(\'Genre\') plt.ylabel(\'Number of Movies\') plt.show()"},{"question":"# Question: Implementing a Custom TCP Communication Protocol using asyncio In this task, you are required to implement a custom TCP communication protocol using `asyncio`\'s low-level APIs. The protocol will include basic functionalities such as connecting to a server, sending a message, receiving a response, and handling connection closures gracefully. Objectives: 1. Create a `TCPClientProtocol` class inheriting from `asyncio.Protocol`. 2. Implement methods to handle the following scenarios: - Connection established (`connection_made`), - Data received (`data_received`), - Connection lost (`connection_lost`). 3. Write an asynchronous function to initiate the connection, send a message, and handle the response. Detailed Steps: 1. **`TCPClientProtocol` class:** - Initialize the class with `message` and `on_con_lost` attributes. - Implement the `connection_made` method. This should send the initial message to the server upon connection. - Implement the `data_received` method. This should print out the response from the server and close the transport. - Implement the `connection_lost` method. This should set the result for `on_con_lost`. 2. **Asynchronous function `tcp_client`:** - This function initiates the connection using the event loop and provides the `TCPClientProtocol` class. - It should set up the connection, send the message, and handle the response as specified. Constraints: - The server to connect to will be available at \'127.0.0.1\' on port 8888. - The message to be sent is \\"Hello Server!\\" - The protocol should handle any exceptions gracefully and ensure that resources are cleaned up properly. Input: There are no direct inputs to your implementation. However, your protocol and connection parameters will be: ```python host = \'127.0.0.1\' port = 8888 message = \\"Hello Server!\\" ``` Output: Your output should be printed to the console and should display: 1. The message \\"Data sent: \'Hello Server!\'\\" 2. The response received from the server. 3. Connection closure message \\"Connection closed\\" Example: ```python import asyncio class TCPClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): transport.write(self.message.encode()) print(\'Data sent: {!r}\'.format(self.message)) def data_received(self, data): print(\'Data received: {!r}\'.format(data.decode())) self.transport.close() def connection_lost(self, exc): print(\'Connection closed\') self.on_con_lost.set_result(True) async def tcp_client(): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() message = \\"Hello Server!\\" transport, protocol = await loop.create_connection( lambda: TCPClientProtocol(message, on_con_lost), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close() asyncio.run(tcp_client()) ``` Test your implementation by running the provided example. This will simulate a client that connects to a server, sends a message, receives a response, and then closes the connection.","solution":"import asyncio class TCPClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport self.transport.write(self.message.encode()) print(f\'Data sent: {self.message!r}\') def data_received(self, data): print(f\'Data received: {data.decode()!r}\') self.transport.close() def connection_lost(self, exc): print(\'Connection closed\') self.on_con_lost.set_result(True) async def tcp_client(): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() message = \\"Hello Server!\\" transport, protocol = await loop.create_connection( lambda: TCPClientProtocol(message, on_con_lost), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close() # Uncomment the line below to run the client # asyncio.run(tcp_client())"},{"question":"Objective: You are required to demonstrate your ability to load various types of datasets using the scikit-learn package. Specifically, you will load a sample image, a dataset from the LibSVM format, and a dataset from OpenML. Then, you will preprocess the loaded data where necessary. Instructions: 1. **Load a Sample Image**: - Use the `load_sample_image` function from `sklearn.datasets` to load the \\"china.jpg\\" sample image. - Convert the image data to a floating-point representation scaled to the range [0, 1]. - Display the image using matplotlib to ensure it has been loaded correctly. 2. **Load a LibSVM Dataset**: - Use the `load_svmlight_file` function to load a dataset from a provided libsvm formatted file (`path_to_libsvm_dataset.txt`). - Print the shape of the loaded feature matrix `X` and the target vector `y`. 3. **Fetch an OpenML Dataset**: - Use the `fetch_openml` function to download the \\"iris\\" dataset from OpenML. - Print the first five rows of the features and the target. Constraints: - The floating-point scaling for the image should be done precisely to ensure the range [0, 1]. - The `path_to_libsvm_dataset.txt` is a placeholder, and you should assume it to be a valid path to a LibSVM formatted file available in the environment. - Ensure that each step is followed correctly without skipping any preprocessing steps as demonstrated in the officially provided examples. Input/Output: - **Input**: None (All file paths and dataset names are assumed to be correctly specified within the script). - **Output**: Printed output of the shapes, and the scaled image displayed. Performance Requirements: - Each of the loading steps should be efficient and should not consume unnecessary memory. Utilize the methods as recommended in the scikit-learn documentation. ```python import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml import numpy as np def main(): # 1. Load and display the sample image image = load_sample_image(\\"china.jpg\\") image_float = image / 255.0 # scale to range [0, 1] plt.imshow(image_float) plt.axis(\'off\') plt.tight_layout() plt.show() # 2. Load the SVMLight dataset path_to_libsvm_dataset = \\"path_to_libsvm_dataset.txt\\" X, y = load_svmlight_file(path_to_libsvm_dataset) print(f\\"Shape of X: {X.shape}\\") print(f\\"Shape of y: {y.shape}\\") # 3. Fetch the OpenML iris dataset iris = fetch_openml(name=\'iris\') print(\\"First 5 rows of feature data:\\") print(iris.data.head()) print(\\"First 5 target values:\\") print(iris.target.head()) if __name__ == \\"__main__\\": main() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml import numpy as np def load_and_display_sample_image(): Load the \'china.jpg\' sample image, convert it to float representation scaled to [0, 1], and display the image. # 1. Load the sample image image = load_sample_image(\\"china.jpg\\") # 2. Scale the image to [0, 1] image_float = image / 255.0 # 3. Display the image plt.imshow(image_float) plt.axis(\'off\') plt.tight_layout() plt.show() def load_libsvm_dataset(path_to_libsvm_dataset): Load a dataset from a libsvm formatted file. Print the shape of the feature matrix X and the target vector y. # 1. Load the SVMLight file X, y = load_svmlight_file(path_to_libsvm_dataset) # 2. Print the shapes print(f\\"Shape of X: {X.shape}\\") print(f\\"Shape of y: {y.shape}\\") return X, y def fetch_openml_dataset(): Fetch the \\"iris\\" dataset from OpenML. Print the first five rows of the features and the target. # 1. Fetch the OpenML Iris dataset iris = fetch_openml(name=\'iris\') # 2. Print the first five rows of the data print(\\"First 5 rows of feature data:\\") print(iris.data.head()) print(\\"First 5 target values:\\") print(iris.target.head()) return iris.data, iris.target"},{"question":"Objective: To evaluate the understanding of Pythonâ€™s `io` module, including creating and manipulating different types of I/O streams, handling text and binary data, and managing encoding. Problem Statement: You are required to implement a function named `process_data` that performs a series of operations on both text and binary data using the `io` module. Function Signature: ```python def process_data(text_data: str, binary_data: bytes, encoding: str) -> tuple: pass ``` Input: - `text_data` (str): A string containing text data. - `binary_data` (bytes): A bytes object containing binary data. - `encoding` (str): A string representing the encoding format (e.g., \'utf-8\'). Output: - A tuple containing: 1. A string representing the content read from a `StringIO` object initialized with `text_data`. 2. A bytes object representing the content read from a `BytesIO` object initialized with `binary_data`. 3. A string representing the encoded version of `text_data` using the specified `encoding`. 4. A string representing the decoded version of `binary_data` using the specified `encoding`. Steps to Implement: 1. Create a `StringIO` object from `text_data` and read its content. 2. Create a `BytesIO` object from `binary_data` and read its content. 3. Encode the `text_data` string into bytes using the specified encoding. 4. Decode the `binary_data` bytes into a string using the specified encoding. Constraints: - The `encoding` will be a valid encoding recognized by Python. - The functions to read from streams should utilize appropriate methods (`read()`, `getvalue()`, etc.). Example: ```python text_data = \\"Hello, World!\\" binary_data = b\'xe4xbdxa0xe5xa5xbd\' # \'ä½ å¥½\' in UTF-8 encoding = \'utf-8\' result = process_data(text_data, binary_data, encoding) print(result) ``` Expected Output: ```python (\\"Hello, World!\\", b\'xe4xbdxa0xe5xa5xbd\', b\'Hello, World!\', \'ä½ å¥½\') ``` Note: - The first element of the output tuple shows the content read from the `StringIO` object. - The second element shows the content read from the `BytesIO` object. - The third element shows the `text_data` encoded to bytes using the specified encoding. - The fourth element shows the `binary_data` decoded to a string using the specified encoding. Hints: - Use `io.StringIO` for in-memory text stream operations. - Use `io.BytesIO` for in-memory binary stream operations. - Utilize the `encode` method on strings for encoding. - Utilize the `decode` method on bytes for decoding.","solution":"import io def process_data(text_data: str, binary_data: bytes, encoding: str) -> tuple: # Create a StringIO object and read its content string_io = io.StringIO(text_data) text_content = string_io.read() # Create a BytesIO object and read its content bytes_io = io.BytesIO(binary_data) binary_content = bytes_io.read() # Encode the text_data using the specified encoding encoded_text = text_data.encode(encoding) # Decode the binary_data using the specified encoding decoded_binary = binary_data.decode(encoding) return (text_content, binary_content, encoded_text, decoded_binary)"},{"question":"Advanced Colormap Manipulation with seaborn Objective: You are tasked with demonstrating your understanding of seaborn\'s ability to handle colormaps. This question requires you to implement a function that generates a specific data visualization using seaborn and customizes the colormap based on the provided requirements. Function Description: Implement a function named `plot_custom_colormap_data(data, colormap, num_colors)` that performs the following: 1. **Input**: - `data` (pd.DataFrame): A pandas DataFrame containing two columns: \'x\' for x-axis values and \'y\' for y-axis values. - `colormap` (str): The name of the colormap (either qualitative or continuous) to be used. - `num_colors` (int): Number of discrete colors to be used in the plot. If this exceeds the distinct colors in a qualitative colormap, use the available colors. 2. **Output**: - The function should generate and display a scatter plot where each point\'s color is determined by the specified colormap. - Properly utilize the seaborn functions to handle colormaps based on whether the colormap is continuous or qualitative. 3. **Constraints**: - Ensure that if `num_colors` is specified as more than the number of discrete colors available in a qualitative colormap, the plot should handle it gracefully. - Use matplotlib\'s `plt.scatter` for the plot, setting edgecolors to \'w\' and alpha to 0.6 for visual clarity. 4. **Example**: ```python import pandas as pd # Sample DataFrame data = pd.DataFrame({ \'x\': range(10), \'y\': [5, 3, 4, 7, 6, 3, 4, 6, 7, 8] }) plot_custom_colormap_data(data, \'viridis\', 8) plot_custom_colormap_data(data, \'Set2\', 10) ``` Notes: - Remember to import the necessary seaborn and matplotlib modules. - Ensure to include comments and proper docstrings in your function for clarity. This assessment aims to test your understanding of seaborn\'s colormap functionality and your ability to integrate it into a practical data visualization scenario.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import numpy as np def plot_custom_colormap_data(data, colormap, num_colors): Generates a scatter plot with custom colormap. :param data: pd.DataFrame, DataFrame containing \'x\' and \'y\' columns :param colormap: str, The name of the colormap :param num_colors: int, Number of discrete colors to be used in the plot # Ensure that seaborn and matplotlib are imported. if isinstance(data, pd.DataFrame) and \'x\' in data.columns and \'y\' in data.columns: # Generate the colormap cmap = sns.color_palette(colormap, num_colors) # Create a custom scatter plot plt.scatter(data[\'x\'], data[\'y\'], c=range(len(data)), cmap=ListedColormap(cmap), edgecolors=\'w\', alpha=0.6) # Add color bar for reference plt.colorbar() # Show the plot plt.show() else: raise ValueError(\\"Data must be a DataFrame containing \'x\' and \'y\' columns.\\")"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `ecdfplot` function by visualizing a specific aspect of the \\"penguins\\" dataset. **Problem Statement:** You are given the \\"penguins\\" dataset from seaborn\'s sample datasets. Write a function `plot_penguins_ecdf` that performs the following tasks: 1. **Load the dataset.** 2. **Create an ECDF plot** of the `flipper_length_mm` variable: - On the x-axis. - With different hues for each species. - Display the plot with counts on the y-axis. 3. **Flip the plot** so that the ECDF is displayed along the `y-axis` instead. 4. **Modify the plot** to display the complementary CDF. **Function Signature:** ```python def plot_penguins_ecdf() -> None: pass ``` **Steps to Follow:** 1. **Load Dataset:** Load the \\"penguins\\" dataset using the `seaborn.load_dataset` function. 2. **Initial ECDF Plot:** Using the `seaborn.ecdfplot` function: - Set `data` to the penguins dataset. - Map the `flipper_length_mm` variable to the x-axis. - Use the `species` variable for the `hue`. - Set the statistic to `count`. 3. **Flip the Plot:** - For the second ECDF plot, map the `flipper_length_mm` variable to the y-axis instead. 4. **Complementary CDF:** - Display the complementary CDF by setting the `complementary` parameter to True. 5. **Display:** Ensure each plot is displayed before moving to the next plot. **Expected Behavior:** - Your function should display three different ECDF plots in sequence: 1. ECDF plot of `flipper_length_mm` on the x-axis. 2. ECDF plot of `flipper_length_mm` on the y-axis. 3. Complementary CDF plot on the y-axis. **Constraints:** - Use appropriate seaborn functions and parameters as outlined in the documentation. - Do not save the plots to files; they should only be displayed. **Performance Requirements:** - The function should execute efficiently, even though the dataset is relatively small.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_ecdf() -> None: # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Ensure the plot shows with the counts on the y-axis plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Flipper Length (Counts on y-axis)\\") plt.show() # Flip the plot so that the ECDF is displayed along the y-axis plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Flipper Length (Flipped)\\") plt.show() # Display the complementary CDF plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\", complementary=True) plt.title(\\"Complementary CDF of Flipper Length\\") plt.show()"},{"question":"**Objective**: Write a Python function that rewrites a list of relative and absolute URLs based on a given base URL, normalizes them, and extracts specific components. # Detailed Instructions 1. **Function Signature**: ```python def normalize_and_extract_urls(base_url: str, urls: list) -> list: ``` 2. **Input**: - `base_url` (string): A base URL against which relative URLs should be resolved. - `urls` (list of strings): A list of URLs (both relative and absolute) that need to be processed. 3. **Output**: - A list of dictionaries, each containing: ```python { \\"original\\": original_url, # string: the originally provided URL \\"normalized\\": normalized_url, # string: the fully normalized absolute URL \\"scheme\\": scheme, # string: scheme component of the URL \\"hostname\\": hostname, # string: hostname component of the URL \\"path\\": path, # string: path component of the URL \\"query\\": query, # dictionary: query component of the URL as a parsed dictionary \\"fragment\\": fragment # string: fragment component of the URL } ``` 4. **Steps**: - For each URL in the list: 1. Resolve the URL relative to `base_url` using `urllib.parse.urljoin`. 2. Normalize the URL to ensure it follows standard formatting. 3. Parse the URL into its components using `urllib.parse.urlparse` for scheme, hostname, path, query, and fragment. 4. Parse the `query` string into a dictionary using `urllib.parse.parse_qs`. - Return the list of dictionaries with all the extracted information. 5. **Constraints**: - Assume the URLs provided are either proper or can be normalized into proper URLs. - Invalid URL components that cannot be parsed should be handled gracefully, e.g., return `None` for such components. # Example: Input: ```python base_url = \\"http://example.com\\" urls = [ \\"path/to/resource?name=test#section\\", \\"http://anotherdomain.com/faq?ref=source#info\\", \\"/absolute/path?query=1\\", \\"relative/path/\\" ] ``` Output: ```python [ { \\"original\\": \\"path/to/resource?name=test#section\\", \\"normalized\\": \\"http://example.com/path/to/resource?name=test#section\\", \\"scheme\\": \\"http\\", \\"hostname\\": \\"example.com\\", \\"path\\": \\"/path/to/resource\\", \\"query\\": {\\"name\\": [\\"test\\"]}, \\"fragment\\": \\"section\\" }, { \\"original\\": \\"http://anotherdomain.com/faq?ref=source#info\\", \\"normalized\\": \\"http://anotherdomain.com/faq?ref=source#info\\", \\"scheme\\": \\"http\\", \\"hostname\\": \\"anotherdomain.com\\", \\"path\\": \\"/faq\\", \\"query\\": {\\"ref\\": [\\"source\\"]}, \\"fragment\\": \\"info\\" }, { \\"original\\": \\"/absolute/path?query=1\\", \\"normalized\\": \\"http://example.com/absolute/path?query=1\\", \\"scheme\\": \\"http\\", \\"hostname\\": \\"example.com\\", \\"path\\": \\"/absolute/path\\", \\"query\\": {\\"query\\": [\\"1\\"]}, \\"fragment\\": \\"\\" }, { \\"original\\": \\"relative/path/\\", \\"normalized\\": \\"http://example.com/relative/path/\\", \\"scheme\\": \\"http\\", \\"hostname\\": \\"example.com\\", \\"path\\": \\"/relative/path/\\", \\"query\\": {}, \\"fragment\\": \\"\\" } ] ``` # Note: - This problem will test the student\'s comprehension of URL parsing and normalizing techniques using the `urllib.parse` module. - The student should handle both absolute and relative URLs, effectively resolving and normalizing them, and extracting their components accurately.","solution":"from urllib.parse import urljoin, urlparse, parse_qs def normalize_and_extract_urls(base_url: str, urls: list) -> list: result = [] for original_url in urls: normalized_url = urljoin(base_url, original_url) parsed_url = urlparse(normalized_url) scheme = parsed_url.scheme hostname = parsed_url.hostname path = parsed_url.path query = parse_qs(parsed_url.query) fragment = parsed_url.fragment url_info = { \\"original\\": original_url, \\"normalized\\": normalized_url, \\"scheme\\": scheme, \\"hostname\\": hostname, \\"path\\": path, \\"query\\": query, \\"fragment\\": fragment } result.append(url_info) return result"},{"question":"# Advanced Python Coding Assessment Problem Statement: You are required to implement a function that records audio from the default audio input device using the `ossaudiodev` module. The function should: 1. Open the default audio input device for recording. 2. Configure the audio device with a specific format, number of channels, and sample rate. 3. Capture audio data for a specified duration. 4. Return the recorded audio data as a byte string. Specifications: 1. **Function Signature**: ```python def record_audio(duration: int, sample_rate: int, format: int, channels: int) -> bytes: ``` 2. **Parameters**: - `duration` (int): Duration of the recording in seconds. - `sample_rate` (int): The sampling rate in Hertz (e.g., 44100 for CD quality). - `format` (int): The audio format (e.g., `ossaudiodev.AFMT_S16_LE` for signed, 16-bit, little-endian PCM). - `channels` (int): Number of audio channels (1 for mono, 2 for stereo). 3. **Return**: - A byte string containing the recorded audio data. 4. **Constraints**: - The function should handle any exceptions that may occur during recording gracefully and log the errors appropriately. - The function should use blocking mode for reading audio data. - Make sure the audio device supports the desired format, sample rate, and channels before attempting to record. - Use the `setparameters()` method for configuring the device. 5. **Performance**: - Ensure minimal latency and efficient memory usage while recording. Example Usage: ```python import ossaudiodev def record_audio(duration: int, sample_rate: int, format: int, channels: int) -> bytes: try: # Open the default audio input device audio_device = ossaudiodev.open(\'r\') # \'r\' mode for recording audio_device.setparameters(format, channels, sample_rate) # Initialize buffer to store recorded audio buffer = bytearray() # Calculate the total number of bytes to record # Each sample is channels * (bit_depth/8) bytes bytes_per_sample = channels * (ossaudiodev.AFMT_S16_LE.bit_length() // 8) total_bytes = duration * sample_rate * bytes_per_sample while len(buffer) < total_bytes: audio_chunk = audio_device.read(total_bytes - len(buffer)) buffer.extend(audio_chunk) # Close the audio device audio_device.close() return bytes(buffer) except Exception as e: print(f\\"An error occurred: {e}\\") return b\'\' # Example of recording 5 seconds of mono audio at 44100 Hz audio_data = record_audio(5, 44100, ossaudiodev.AFMT_S16_LE, 1) with open(\'output.raw\', \'wb\') as f: f.write(audio_data) ``` **Note**: Ensure the `ossaudiodev` module is installed and OSS drivers are available on your system to test this code.","solution":"import ossaudiodev def record_audio(duration: int, sample_rate: int, format: int, channels: int) -> bytes: try: # Open the default audio input device audio_device = ossaudiodev.open(\'r\') # \'r\' mode for recording audio_device.setparameters(format, channels, sample_rate) # Buffer to collect audio data buffer = bytearray() # Calculate total number of bytes to record bytes_per_sample = channels * (format.bit_length() // 8) total_bytes = duration * sample_rate * bytes_per_sample while len(buffer) < total_bytes: audio_chunk = audio_device.read(total_bytes - len(buffer)) buffer.extend(audio_chunk) # Close the audio device audio_device.close() return bytes(buffer) except Exception as e: print(f\\"An error occurred: {e}\\") return b\'\'"},{"question":"**CSV File Processing with Custom Dialects** You are required to create a Python script that reads data from an existing CSV file, processes the data, and writes the processed data to a new CSV file using custom dialects. Follow the guidelines below to accomplish this task: 1. **Reading the CSV File**: - Read data from a CSV file named `input.csv` located in the current directory. - Use the `csv.DictReader` class for reading the data. - Assume the CSV file has the following fields: `name`, `age`, `city`. 2. **Processing the Data**: - Standardize the `city` names to title case (e.g., \\"new york\\" -> \\"New York\\"). - Filter out the entries where the `age` is less than 18. 3. **Writing to a New CSV File**: - Write the processed data to a new CSV file named `output.csv` using the `csv.DictWriter` class. - Use a custom dialect named `my_dialect` that has the following formatting parameters: - `delimiter`: `;` - `quotechar`: `\\"` - `quoting`: `csv.QUOTE_MINIMAL` 4. **Helper Functions**: - Implement a function `register_my_dialect()` to register the custom dialect. - Implement a function `unregister_my_dialect()` to unregister the custom dialect after writing the file. # Constraints: - Do not use any external libraries except for the standard `csv` module. - Ensure that the script handles any potential `csv.Error`. # Expected Input/Output: - **Input**: A file named `input.csv` with the columns `name`, `age`, `city`. - **Output**: A file named `output.csv` with the filtered and processed data, using the custom dialect with `;` as the delimiter. # Example: **input.csv**: ```csv name,age,city Alice,17,new york Bob,25,los angeles Charlie,18,chicago Dana,16,boston ``` **output.csv**: ```csv \\"name\\";\\"age\\";\\"city\\" \\"Bob\\";\\"25\\";\\"Los Angeles\\" \\"Charlie\\";\\"18\\";\\"Chicago\\" ``` # Template Code: ```python import csv def register_my_dialect(): pass # Implement this function def unregister_my_dialect(): pass # Implement this function def process_csv(input_file, output_file): # Register the custom dialect register_my_dialect() try: with open(input_file, mode=\'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) processed_data = [] for row in reader: row[\'city\'] = row[\'city\'].title() if int(row[\'age\']) >= 18: processed_data.append(row) with open(output_file, mode=\'w\', newline=\'\') as csvfile: fieldnames = [\'name\', \'age\', \'city\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'my_dialect\') writer.writeheader() writer.writerows(processed_data) except csv.Error as e: print(f\\"CSV error: {e}\\") finally: # Unregister the custom dialect unregister_my_dialect() # You can test your implementation by calling process_csv function # process_csv(\'input.csv\', \'output.csv\') ``` **Note**: Implement the `register_my_dialect` and `unregister_my_dialect` functions to complete the script.","solution":"import csv def register_my_dialect(): Registers a custom CSV dialect with the specified formatting parameters. csv.register_dialect( \'my_dialect\', delimiter=\';\', quotechar=\'\\"\', quoting=csv.QUOTE_MINIMAL ) def unregister_my_dialect(): Unregisters the custom CSV dialect. csv.unregister_dialect(\'my_dialect\') def process_csv(input_file, output_file): Reads data from an input CSV file, processes it, and writes it to an output CSV file using a custom dialect. Parameters: - input_file: str, path to the input CSV file - output_file: str, path to the output CSV file # Register the custom dialect register_my_dialect() try: with open(input_file, mode=\'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) processed_data = [] for row in reader: row[\'city\'] = row[\'city\'].title() if int(row[\'age\']) >= 18: processed_data.append(row) with open(output_file, mode=\'w\', newline=\'\') as csvfile: fieldnames = [\'name\', \'age\', \'city\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'my_dialect\') writer.writeheader() writer.writerows(processed_data) except csv.Error as e: print(f\\"CSV error: {e}\\") finally: # Unregister the custom dialect unregister_my_dialect()"},{"question":"You are tasked with implementing a signal handling mechanism in Python. Your objective is to create a function that sets up a signal handler to catch a user-defined signal (`SIGUSR1`) and perform a custom action when this signal is received. # Function Signature ```python def setup_signal_handler(): pass ``` # Instructions 1. Define a signal handler function that prints \\"User-defined signal received\\" when the `SIGUSR1` signal is received. 2. Use the `signal.signal()` function to set this handler for the `SIGUSR1` signal. 3. The `setup_signal_handler` function should set the handler and demonstrate the signal handling by sending `SIGUSR1` to the current process. # Constraints - You are allowed to use the `signal` and `os` modules. - Only the `SIGUSR1` signal should be handled in this task. - Your solution should work on Unix-based systems where `SIGUSR1` is available. # Example ```python # Output expected to be printed when SIGUSR1 is sent to the process User-defined signal received ``` # Sample Code Usage ```python setup_signal_handler() # Send SIGUSR1 to the current process import os os.kill(os.getpid(), signal.SIGUSR1) ``` Ensure your implementation catches the signal and prints the expected output when the signal is handled.","solution":"import signal import os def signal_handler(signum, frame): print(\\"User-defined signal received\\") def setup_signal_handler(): # Set up the signal handler for SIGUSR1 signal.signal(signal.SIGUSR1, signal_handler) # Sending SIGUSR1 to the current process os.kill(os.getpid(), signal.SIGUSR1)"},{"question":"# Coding Assessment: Advanced Pandas Series Manipulation Objective: You are required to write a function that processes and analyzes a `pandas.Series` of employee data. The function will perform several operations including data cleaning, transformation, and aggregation to provide insights into the dataset. Task: 1. **Data Cleaning**: Remove any entries that contain NaN values. 2. **Data Transformation**: * Convert all textual data to lower case. * For datetime entries, extract the year and create a new `Series` with these years. 3. **Aggregation**: * Group the data by unique years (if they exist) and count the number of occurrences for each year. Implement the function `process_employee_data(employee_series: pd.Series) -> pd.Series` according to the specifications. Function Details: ```python def process_employee_data(employee_series: pd.Series) -> pd.Series: Process the given Series of employee data. Parameters: employee_series (pd.Series): A pandas Series object containing employee data. The Series can contain mixed types of data: strings and datetimes. Returns: pd.Series: A series indexed by unique year, containing counts of occurrences for each year after processing. pass ``` Examples: ```python # Example Input import pandas as pd employee_data = pd.Series([\'Alice\', \'BOB\', \'2021-01-01\', \'2022-03-15\', None, \'2021-05-21\', \'\']) # Example Output process_employee_data(employee_data) # Output should be: # 2021 2 # 2022 1 # dtype: int64 ``` Constraints: * The input `employee_series` can contain mixed types such as strings, dates, and null values. * The function should be efficient in processing and handle various edge cases such as all-null values, all-text values without any dates. Notes: * Assume all date entries are in a format directly parseable by `pd.to_datetime`. * Non-date and non-text entries are not expected and can be ignored but should not break the function. Good luck!","solution":"import pandas as pd def process_employee_data(employee_series: pd.Series) -> pd.Series: Process the given Series of employee data. Parameters: employee_series (pd.Series): A pandas Series object containing employee data. The Series can contain mixed types of data: strings and datetimes. Returns: pd.Series: A series indexed by unique year, containing counts of occurrences for each year after processing. # Remove any entries that contain NaN values clean_series = employee_series.dropna() # Convert all textual data to lowercase clean_series = clean_series.apply(lambda x: x.lower() if isinstance(x, str) else x) # Extract the year for datetime entries and create a new Series with these years def extract_year(value): try: return pd.to_datetime(value).year except (ValueError, TypeError): return None year_series = clean_series.apply(extract_year).dropna() # Group the data by unique years and count the number of occurrences for each year year_counts = year_series.value_counts().sort_index() return year_counts"},{"question":"Question # Objective You are tasked with visualizing the distribution of a dataset using seaborn\'s `violinplot` function. This will test your ability to understand and apply various seaborn functionalities and customizations. # Requirements 1. **Load the Dataset** - Load the built-in Titanic dataset using seaborn. 2. **Create a Basic Violin Plot** - Plot a basic violin plot of the `age` column. 3. **Bivariate Violin Plot with Grouping** - Plot a bivariate violin plot of `age` grouped by `class`. 4. **Customized Plot** - Generate a violin plot of `age` grouped by `class` with additional grouping by `alive` status. Customize this plot to: - Use line-art violins (set `fill=False`). - \\"Split\\" the violins to show quartiles (`split=True` and `inner=\'quart\'`). - Add a small gap between the dodged violins (set `gap=0.1`). - Prevent KDE from smoothing past extremes (set `cut=0`). 5. **Customization Challenge** - Create a violin plot of `age` grouped by `alive` status with: - Density normalization to represent the count of observations (`density_norm=\'count\'`). - Customized smoothing parameter (`bw_adjust=0.5`). - Each observation represented inside the distribution (`inner=\'stick\'`). - Native scale for axes (`native_scale=True`). - Customized linewidth and color for inner representation (`linewidth=2` and `linecolor=\'blue\'`). # Constraints - Use only seaborn and pandas libraries. - Your code should be efficient and well-documented. # Input Format - No input is required for this task as you will use the built-in dataset from seaborn. # Output Format - Four violin plots as described in the requirements. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # 1. Basic Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"]) plt.title(\\"Basic Violin Plot of Age\\") plt.show() # 2. Bivariate Violin Plot with Grouping by Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"age\\", y=\\"class\\") plt.title(\\"Bivariate Violin Plot of Age grouped by Class\\") plt.show() # 3. Customized Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", fill=False, split=True, inner=\\"quart\\", gap=0.1, cut=0) plt.title(\\"Customized Violin Plot\\") plt.show() # 4. Customization Challenge plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"age\\", y=\\"alive\\", density_norm=\\"count\\", bw_adjust=0.5, inner=\\"stick\\", native_scale=True, linewidth=2, linecolor=\\"blue\\") plt.title(\\"Customization Challenge Violin Plot\\") plt.show() ``` # Evaluation Criteria - Correctness: The plots should match the specified requirements. - Use of seaborn functionalities: Proper usage of seaborn\'s `violinplot` function and its parameters. - Code quality: Code should be clean, efficient, and well-commented.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_violin_plots(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # 1. Basic Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"]) plt.title(\\"Basic Violin Plot of Age\\") plt.show() # 2. Bivariate Violin Plot with Grouping by Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"age\\", y=\\"class\\") plt.title(\\"Bivariate Violin Plot of Age grouped by Class\\") plt.show() # 3. Customized Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\", linewidth=1.5, cut=0) plt.title(\\"Customized Violin Plot\\") plt.show() # 4. Customization Challenge plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"alive\\", y=\\"age\\", scale=\\"count\\", bw=0.5, inner=\\"stick\\", linewidth=2, cut=0) plt.title(\\"Customization Challenge Violin Plot\\") plt.show()"},{"question":"# Email Header Handling Assessment **Objective:** Demonstrate proficiency with the `email.header` module for creating and managing RFC-compliant email headers with non-ASCII characters. --- **Problem Statement:** You need to implement a function that constructs an email header with non-ASCII characters, encodes it correctly as per RFC standards, and then decodes it back to validate the correctness of the encoding. This function should handle errors gracefully using the \'replace\' strategy when dealing with non-decodable byte strings. **Function Signature:** ```python def process_email_header(initial_str: str, charset: str) -> str: pass ``` **Inputs:** - `initial_str: str`: The initial string to be included in the email header. This string may contain non-ASCII characters. - `charset: str`: The character set to be used for encoding the string (e.g., \'iso-8859-1\'). **Outputs:** - A single string representing the decoded email header that has been first encoded using the `Header` class and then decoded back to ensure compliance with the original string (taking into account potential errors). **Instructions:** 1. Use the `Header` class from the `email.header` module to create an email header from the `initial_str` with the specified `charset`. 2. Encode the header using the `encode` method of the `Header` class. 3. Decode the encoded string using the `decode_header` function to ensure it was correctly encoded. 4. Return the concatenated decoded parts as a single string with errors handled using the \'replace\' strategy for non-decodable bytes. **Constraints:** - The `initial_str` may contain any UTF-8 character. - The `charset` should be a valid character encoding recognized by the `email.header` module. **Example:** ```python from email.header import Header, decode_header def process_email_header(initial_str: str, charset: str) -> str: # Step 1: Create a Header object header = Header(initial_str, charset=charset, errors=\'replace\') # Step 2: Encode the header to RFC-compliant format encoded_string = header.encode() # Step 3: Decode the encoded string decoded_parts = decode_header(encoded_string) # Step 4: Handle errors and concatenate decoded parts decoded_str = \'\'.join(part.decode(charset, errors=\'replace\') if isinstance(part, bytes) else part for part, charset in decoded_parts) return decoded_str # Example usage: initial_str = \\"pÃ¶stal\\" charset = \\"iso-8859-1\\" print(process_email_header(initial_str, charset)) # Output: \\"pÃ¶stal\\" ``` **Explanation:** In the example, the string \\"pÃ¶stal\\" is encoded using the \'iso-8859-1\' charset. The function then decodes it back using the `decode_header` function. If the decoding encounters any errors, it will use the \'replace\' strategy to handle non-decodable bytes. The final output should match the original string, demonstrating that the header was correctly encoded and decoded. --- **Note:** Make sure to handle all edge cases, such as empty strings, entirely ASCII strings, and strings with complex unicode characters.","solution":"from email.header import Header, decode_header def process_email_header(initial_str: str, charset: str) -> str: # Step 1: Create a Header object header = Header(initial_str, charset=charset, errors=\'replace\') # Step 2: Encode the header to RFC-compliant format encoded_string = header.encode() # Step 3: Decode the encoded string decoded_parts = decode_header(encoded_string) # Step 4: Handle errors and concatenate decoded parts decoded_str = \'\'.join(part.decode(charset, errors=\'replace\') if isinstance(part, bytes) else part for part, charset in decoded_parts) return decoded_str"},{"question":"**Objective**: To evaluate your understanding of PyTorch\'s distributed training framework and ability to implement code for initializing and launching a distributed training job. # Question You are required to set up a Python script to implement distributed training using PyTorch. Your task is to create a function `initialize_and_run_distributed_training` that configures and launches a distributed training job using `torch.distributed.run`. # Requirements 1. **Function Signature**: ```python def initialize_and_run_distributed_training(train_script: str, nnodes: int, nproc_per_node: int, master_addr: str, master_port: int, node_rank: int) -> None: ``` 2. **Input**: - `train_script` (str): Path to the training script file that should be executed on each node. - `nnodes` (int): Number of nodes to use in training. - `nproc_per_node` (int): Number of processes to spawn on each node (typically number of GPUs per node). - `master_addr` (str): IP address of the master node (rank 0). - `master_port` (int): Port on the master node to communicate. - `node_rank` (int): Rank (ID) of the node. Each node should have a unique rank. 3. **Output**: - The function does not return anything. It launches the distributed training process. # Constraints - Assume the `train_script` provided is a valid Python file compatible with distributed training setup. - Handle any exceptions that may occur during the initialization phase, and make sure to print appropriate error messages. # Example Usage ```python initialize_and_run_distributed_training( train_script=\\"path/to/train.py\\", nnodes=2, nproc_per_node=4, master_addr=\\"192.168.1.1\\", master_port=12345, node_rank=0 ) ``` This would configure and run the `train.py` script on a distributed setup with 2 nodes, each spawning 4 processes. # Additional Context 1. **Launching Distributed Jobs**: Use `torch.distributed.run` or the equivalent `torchrun` command to initialize and start the distributed training. 2. **Configuration Parameters**: Ensure the function handles all the configuration parameters correctly. 3. **Error Handling**: Properly catch and print exceptions related to process initialization failures. # Submission Submit your implementation of the `initialize_and_run_distributed_training` function. Ensure your code is clear, handles exceptions gracefully, and demonstrates your understanding of distributed training in PyTorch.","solution":"import sys import subprocess def initialize_and_run_distributed_training(train_script: str, nnodes: int, nproc_per_node: int, master_addr: str, master_port: int, node_rank: int) -> None: Configures and launches a distributed training job using the provided parameters. Args: train_script (str): Path to the training script file. nnodes (int): Number of nodes to use in training. nproc_per_node (int): Number of processes to spawn on each node. master_addr (str): IP address of the master node. master_port (int): Port on the master node to communicate. node_rank (int): Rank (ID) of the node. try: # Construct the torchrun command command = [ sys.executable, \\"-m\\", \\"torch.distributed.run\\", \\"--nnodes\\", str(nnodes), \\"--nproc_per_node\\", str(nproc_per_node), \\"--master_addr\\", master_addr, \\"--master_port\\", str(master_port), \\"--node_rank\\", str(node_rank), train_script ] # Execute the command subprocess.run(command, check=True) except subprocess.CalledProcessError as e: print(f\\"An error occurred while running the distributed training script: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Objective:** The goal of this exercise is to demonstrate your comprehension of the `xml.dom` package by creating and manipulating an XML document. **Problem Statement:** You need to write a Python function `create_and_query_xml` that performs the following tasks: 1. Create a new XML document with a root element. 2. Add several child elements to the root, each with a unique tag name and some attributes. 3. Query the document to retrieve specific elements and their attributes. **Function Signature:** ```python def create_and_query_xml(): pass ``` # Detailed Steps 1. **Create a new XML document with a root element named `Library`.** 2. **Add the following child elements to the `Library` root, each with specified attributes:** - `Book` with attributes `title`, `author`, and `year`. - For example, - `Book` with title=\\"1984\\", author=\\"George Orwell\\", year=\\"1949\\". - `Book` with title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", year=\\"1960\\". 3. **Query the document to retrieve the titles of all the `Book` elements and return them in a list.** **Constraints:** - Use the methods and classes provided by the `xml.dom` module. - The function does not take any input but returns a list of book titles. # Expected Output The function should return a list of book titles in the order they were added, such as: ```python [\\"1984\\", \\"To Kill a Mockingbird\\"] ``` *Note: Your implementation should accurately reflect this output format.* # Example Execution: ```python titles = create_and_query_xml() print(titles) # [\\"1984\\", \\"To Kill a Mockingbird\\"] ``` **Additional Notes:** - Focus on correct use of `xml.dom` methods for creating elements, setting attributes, and retrieving node lists. - Handle potential exceptions that may arise during XML manipulation. # Submission: Please ensure your code is well-commented and follows good practices for readability and maintenance.","solution":"from xml.dom.minidom import Document def create_and_query_xml(): # Step 1: Create a new document doc = Document() # Create root element \'Library\' library = doc.createElement(\'Library\') doc.appendChild(library) # Step 2: Add \'Book\' elements with attributes # First Book book1 = doc.createElement(\'Book\') book1.setAttribute(\'title\', \'1984\') book1.setAttribute(\'author\', \'George Orwell\') book1.setAttribute(\'year\', \'1949\') library.appendChild(book1) # Second Book book2 = doc.createElement(\'Book\') book2.setAttribute(\'title\', \'To Kill a Mockingbird\') book2.setAttribute(\'author\', \'Harper Lee\') book2.setAttribute(\'year\', \'1960\') library.appendChild(book2) # Step 3: Query document to retrieve book titles books = library.getElementsByTagName(\'Book\') titles = [book.getAttribute(\'title\') for book in books] return titles"},{"question":"Objective: Design a function named `clean_dataframe` that processes a given pandas DataFrame by handling its missing values (both `NA` and `NaT`). Function Signature: ```python def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: ``` Inputs: * `df` (pd.DataFrame): A pandas DataFrame that may contain missing values indicated by `NA` or `NaT`. Outputs: * Return a pandas DataFrame where: 1. Columns with more than 50% missing values are dropped. 2. Numeric columns with missing values are filled with the column mean. 3. Categorical columns with missing values are filled with the mode of the column. 4. Datetime columns with missing values are filled with the earliest non-missing date in the column. Constraints: * You may assume that the DataFrame has at least one numeric, one categorical, and one datetime column. * Performance requirements allow up to 10^6 rows and 100 columns. Example: ```python import pandas as pd import numpy as np data = { \'numeric_col\': [1, 2, np.nan, 4], \'categorical_col\': [\'a\', \'b\', \'b\', np.nan], \'datetime_col\': [pd.Timestamp(\'2022-01-01\'), pd.NaT, pd.Timestamp(\'2022-01-03\'), pd.Timestamp(\'2022-01-04\')] } df = pd.DataFrame(data) cleaned_df = clean_dataframe(df) # Expected output: # numeric_col categorical_col datetime_col # 0 1.0 a 2022-01-01 # 1 2.0 b 2022-01-01 # 2 2.333333 b 2022-01-03 # 3 4.0 b 2022-01-04 ``` Notes: 1. You should use pandas methods to detect and handle missing values. 2. Ensure your solution leverages vectorized operations for performance. 3. Add necessary docstrings and comments to explain your implementation.","solution":"import pandas as pd import numpy as np def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: Cleans the given DataFrame by handling missing values based on the following rules: 1. Columns with more than 50% missing values are dropped. 2. Numeric columns with missing values are filled with the column mean. 3. Categorical columns with missing values are filled with the mode of the column. 4. Datetime columns with missing values are filled with the earliest non-missing date in the column. Parameters: df (pd.DataFrame): The input DataFrame with potential missing values. Returns: pd.DataFrame: The cleaned DataFrame. # Drop columns with more than 50% missing values threshold = len(df) * 0.5 df = df.dropna(axis=1, thresh=threshold) # Fill numeric columns missing values with the column mean numeric_cols = df.select_dtypes(include=[np.number]).columns df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.mean()), axis=0) # Fill categorical columns missing values with the mode categorical_cols = df.select_dtypes(include=[\'object\']).columns df[categorical_cols] = df[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]), axis=0) # Fill datetime columns missing values with the earliest date datetime_cols = df.select_dtypes(include=[np.datetime64]).columns df[datetime_cols] = df[datetime_cols].apply(lambda col: col.fillna(col.min()), axis=0) return df"},{"question":"You are given the task of developing a utility function for a web application that sanitizes input data for safe HTML display and converts user-submitted HTML back into readable text. This will ensure that your application can handle HTML content safely and effectively. Your task is to implement two functions: 1. `sanitize_html(input_string: str, quote: bool = True) -> str`: This function should use `html.escape()` to convert the characters \\"&\\", \\"<\\", and \\">\\" in the string `input_string` to their corresponding HTML-safe sequences. If the optional flag `quote` is true, the characters `\'\\"` (double quote) and `\'` (single quote) should also be translated. 2. `desanitize_html(input_string: str) -> str`: This function should use `html.unescape()` to convert all named and numeric character references in the string `input_string` back to their corresponding Unicode characters. Input - `sanitize_html`: - `input_string` (str): The input string containing characters that need to be converted to HTML-safe sequences. - `quote` (Optional[bool]): A boolean flag indicating whether to convert double and single quotes as well. Default is True. - `desanitize_html`: - `input_string` (str): The input string containing HTML character references that need to be converted back to Unicode characters. Output - `sanitize_html` returns a string with the HTML-safe sequences. - `desanitize_html` returns a string with the corresponding Unicode characters. Constraints - The input strings can contain printable ASCII characters and potentially HTML character references. - Ensure that your implementation accurately converts all specified characters based on the provided functions `html.escape` and `html.unescape`. Examples ```python # Test cases for sanitize_html print(sanitize_html(\\"Tom & Jerry <Cartoon>\\", quote=True)) # Expected output: \\"Tom &amp; Jerry &lt;Cartoon&gt;\\" print(sanitize_html(\\"He said, \'Hello!\'\\", quote=True)) # Expected output: \\"He said, &#x27;Hello!&#x27;\\" print(sanitize_html(\\"He said, \'Hello!\'\\", quote=False)) # Expected output: \\"He said, \'Hello!\'\\" # Test cases for desanitize_html print(desanitize_html(\\"Tom &amp; Jerry &lt;Cartoon&gt;\\")) # Expected output: \\"Tom & Jerry <Cartoon>\\" print(desanitize_html(\\"He said, &#x27;Hello!&#x27;\\")) # Expected output: \\"He said, \'Hello!\'\\" ``` Implement the required functions.","solution":"import html def sanitize_html(input_string: str, quote: bool = True) -> str: Convert characters in the input string to HTML-safe sequences. Args: - input_string (str): The input string to be sanitized. - quote (bool): Whether to also convert quotes. Returns: - str: The sanitized string. return html.escape(input_string, quote=quote) def desanitize_html(input_string: str) -> str: Convert HTML character references in the input string back to Unicode characters. Args: - input_string (str): The input string to be desanitized. Returns: - str: The desanitized string. return html.unescape(input_string)"},{"question":"You are provided with a dataset containing information about the sales transactions of a retail company. The dataset includes the following columns: * `TransactionID`: Unique identifier for each transaction. * `Date`: The date of the transaction. * `CustomerID`: Unique identifier for each customer. * `ProductID`: Unique identifier for each product sold. * `Quantity`: Number of units sold. * `Price`: Price per unit of the product. Your task is to perform the following operations using pandas: 1. **Load the DataFrame**: Given a CSV file path, load the data into a pandas DataFrame. 2. **Clean the DataFrame**: * Handle missing data by filling it with appropriate values: - If `Quantity` or `Price` is missing, replace it with 0. - If `Date`, `CustomerID`, or `ProductID` is missing, drop the corresponding rows. 3. **Enhanced DataFrame**: * Add a new column `TotalPrice` that calculates the total price for each transaction (i.e., `Quantity * Price`). * Convert the `Date` column to datetime. 4. **Data Analysis**: * Find the top 5 products with the highest total sales (`TotalPrice`). * Compute the monthly total sales for the year 2022. 5. **Output the Results**: * Return the cleaned DataFrame. * Return a DataFrame with the top 5 products. * Return a DataFrame with monthly total sales for 2022. # Function Signature ```python import pandas as pd def analyze_sales(file_path: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame): Analyze the sales data from the provided file path. :param file_path: The path to the CSV file containing the sales data. :return: A tuple containing the cleaned DataFrame, top 5 products DataFrame, and monthly sales DataFrame for 2022. pass ``` # Input * The function `analyze_sales` should take a single argument `file_path` which is a string representing the path to the CSV file. # Output * The function should return a tuple containing three pandas DataFrames: 1. The cleaned DataFrame. 2. DataFrame containing the top 5 products with the highest total sales. 3. DataFrame containing the monthly total sales for 2022. # Constraints * Assume the dataset provided will fit into memory. * The dataset will have no more than 100,000 rows. # Example ```python file_path = \'sales_data.csv\' cleaned_df, top_products_df, monthly_sales_df = analyze_sales(file_path) # cleaned_df: DataFrame with missing values handled and `TotalPrice` column added. # top_products_df: DataFrame with the 5 products having the highest total sales. # monthly_sales_df: DataFrame with the total sales for each month in 2022. ```","solution":"import pandas as pd def analyze_sales(file_path: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame): Analyze the sales data from the provided file path. :param file_path: The path to the CSV file containing the sales data. :return: A tuple containing the cleaned DataFrame, top 5 products DataFrame, and monthly sales DataFrame for 2022. # Load the DataFrame df = pd.read_csv(file_path) # Clean the DataFrame df[\'Quantity\'].fillna(0, inplace=True) df[\'Price\'].fillna(0, inplace=True) df.dropna(subset=[\'Date\', \'CustomerID\', \'ProductID\'], inplace=True) # Add TotalPrice column df[\'TotalPrice\'] = df[\'Quantity\'] * df[\'Price\'] # Convert Date to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Data Analysis # 1. Top 5 products with the highest total sales top_products = df.groupby(\'ProductID\')[\'TotalPrice\'].sum().nlargest(5).reset_index() # 2. Monthly total sales for the year 2022 df_2022 = df[df[\'Date\'].dt.year == 2022] monthly_sales_2022 = df_2022.resample(\'M\', on=\'Date\')[\'TotalPrice\'].sum().reset_index() return df, top_products, monthly_sales_2022"},{"question":"Objective: Evaluate students\' ability to use scikit-learn\'s feature extraction tools to preprocess data for machine learning models. Task: You are given a dataset of text documents in JSON format. Each document contains a string of text and a list of attributes stored as key-value pairs. Your task is to: 1. Preprocess the text data using `TfidfVectorizer` to convert the text into numerical features. 2. Use `DictVectorizer` to convert the list of attributes into numerical features. 3. Combine the numerical features from both the `TfidfVectorizer` and `DictVectorizer` into a single feature matrix. 4. Write a function `create_feature_matrix` to achieve this. Function Signature: ```python from typing import List, Dict import numpy as np def create_feature_matrix(docs: List[Dict[str, any]]) -> np.ndarray: pass ``` Input: - `docs`: A list of dictionaries, where each dictionary represents a document. Each dictionary contains two keys: - `\'text\'`: a string representing the document\'s text. - `\'attributes\'`: a dictionary containing key-value pairs representing the document\'s attributes. Output: - Returns a NumPy array representing the combined feature matrix. Constraints: - Assume that the text data can be preprocessed using the default settings of `TfidfVectorizer`. - Assume that the attributes dictionary contains only categorical and numeric data suitable for `DictVectorizer`. Example Usage: ```python docs = [ { \\"text\\": \\"Machine learning is fascinating.\\", \\"attributes\\": {\\"author\\": \\"John\\", \\"length\\": 25} }, { \\"text\\": \\"Feature extraction using scikit-learn.\\", \\"attributes\\": {\\"author\\": \\"Jane\\", \\"length\\": 30, \\"keywords\\": [\\"scikit-learn\\", \\"feature extraction\\"]} }, { \\"text\\": \\"Data preprocessing is crucial for good models.\\", \\"attributes\\": {\\"author\\": \\"John\\", \\"length\\": 40} } ] feature_matrix = create_feature_matrix(docs) print(feature_matrix) ``` Notes: 1. Make sure to handle cases where some documents might have attributes that others do not. 2. The resulting feature matrix should be a sparse representation to handle high-dimensional data efficiently. Expected Solution: The function should preprocess the text and attribute data separately and then horizontally stack the resulting feature matrices to create a combined feature matrix.","solution":"from typing import List, Dict import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import DictVectorizer from scipy.sparse import hstack def create_feature_matrix(docs: List[Dict[str, any]]) -> np.ndarray: # Extract the text data texts = [doc[\'text\'] for doc in docs] # Extract the attribute data attributes = [doc[\'attributes\'] for doc in docs] # Transform the text data using TfidfVectorizer tfidf_vectorizer = TfidfVectorizer() tfidf_matrix = tfidf_vectorizer.fit_transform(texts) # Transform the attributes using DictVectorizer dict_vectorizer = DictVectorizer(sparse=True) attributes_matrix = dict_vectorizer.fit_transform(attributes) # Combine the two feature matrices combined_matrix = hstack([tfidf_matrix, attributes_matrix]) return combined_matrix.toarray()"},{"question":"# Question: Log File Analyzer **Objective:** Write a Python program that analyzes log files to gather specific statistics. This program should process multiple log files concurrently and output the gathered statistics in a structured format. **Details:** You are given a directory containing multiple log files. Each log file contains several lines in a specific format: ``` [date time] [log_level] [message] ``` For example: ``` 2023-08-01 18:22:35 INFO User logged in 2023-08-01 18:23:10 ERROR Disk full 2023-08-01 18:25:03 WARNING Low memory ``` **Requirements:** 1. **Input:** - A path to the directory containing log files. 2. **Output:** - A dictionary containing the statistics gathered from all log files. - The output format should be: ```python { \'total_logs\': <total number of log entries>, \'info\': <number of INFO log entries>, \'warning\': <number of WARNING log entries>, \'error\': <number of ERROR log entries>, \'logs_per_file\': { \'<filename1>\': <number of logs in filename1>, \'<filename2>\': <number of logs in filename2>, ... } } ``` 3. **Constraints:** - The program should handle any number of log files in the directory. - Use concurrent processing to handle multiple files efficiently. 4. **Performance Requirement:** - Ensure the program processes large sets of log files efficiently using concurrent execution. **Function Signature:** ```python import os from concurrent.futures import ThreadPoolExecutor def analyze_log_files(directory_path: str) -> dict: # Your implementation here ``` **Example:** Given a directory with log files: - log1.txt: ``` 2023-08-01 18:22:35 INFO User logged in 2023-08-01 18:23:10 ERROR Disk full ``` - log2.txt: ``` 2023-08-01 18:25:03 WARNING Low memory 2023-08-01 18:26:03 INFO User logged out ``` Calling `analyze_log_files(\'/path/to/log/directory\')` should return: ```python { \'total_logs\': 4, \'info\': 2, \'warning\': 1, \'error\': 1, \'logs_per_file\': { \'log1.txt\': 2, \'log2.txt\': 2 } } ``` **Hints:** - Use the `os` module to list files in the directory. - Use the `concurrent.futures` module to handle concurrent file processing. - Validate the log file format and handle any errors gracefully.","solution":"import os from concurrent.futures import ThreadPoolExecutor def count_logs_in_file(log_file_path): Counts the number of logs and categories in a log file. :param log_file_path: Path to the log file :return: A dictionary with counts of log levels and total logs log_counts = {\'info\': 0, \'warning\': 0, \'error\': 0, \'total\': 0} with open(log_file_path, \'r\') as file: for line in file: log_counts[\'total\'] += 1 if \'INFO\' in line: log_counts[\'info\'] += 1 elif \'WARNING\' in line: log_counts[\'warning\'] += 1 elif \'ERROR\' in line: log_counts[\'error\'] += 1 return log_file_path, log_counts def analyze_log_files(directory_path: str) -> dict: Analyzes log files in a given directory and gathers statistics. :param directory_path: Path to the directory containing log files :return: A dictionary containing the gathered statistics log_statistics = { \'total_logs\': 0, \'info\': 0, \'warning\': 0, \'error\': 0, \'logs_per_file\': {} } with ThreadPoolExecutor() as executor: log_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))] results = executor.map(count_logs_in_file, log_files) for file_path, counts in results: log_statistics[\'total_logs\'] += counts[\'total\'] log_statistics[\'info\'] += counts[\'info\'] log_statistics[\'warning\'] += counts[\'warning\'] log_statistics[\'error\'] += counts[\'error\'] log_statistics[\'logs_per_file\'][os.path.basename(file_path)] = counts[\'total\'] return log_statistics"},{"question":"# Shared Memory Data Processing Objective You are tasked with processing a large dataset across multiple processes efficiently. To achieve this, you will use Python\'s `multiprocessing.shared_memory` module to share data between processes without the need for serialization or copying. Task Implement a function `process_shared_data(data: List[int], n: int) -> List[int]` that performs the following steps: 1. **Initialization**: - Create a shared memory block large enough to hold the list `data`. - Copy the contents of `data` into this shared memory block. 2. **Processing**: - Spawn `n` worker processes to perform some computation on chunks of the shared data. - Each worker process should: - Read its respective chunk from the shared memory. - Perform a simple computation (e.g., multiply each element by 2). - Write the results back to the same location in the shared memory. 3. **Finalization**: - Once all worker processes have completed their computations, convert the shared memory buffer back into a list and return it. - Ensure proper cleanup by closing and unlinking the shared memory block. Input - `data`: A list of integers (maximum length 10^6). - `n`: The number of worker processes to spawn (maximum 32). Output - A list of integers representing the processed data. Constraints - The solution should utilize shared memory to avoid unnecessary copying of data. - Ensure that the shared memory block is properly cleaned up after use. - Handle edge cases such as empty data lists gracefully. Example ```python from typing import List def process_shared_data(data: List[int], n: int) -> List[int]: # Your implementation here # Example usage: data = [1, 2, 3, 4, 5] num_processes = 2 result = process_shared_data(data, num_processes) print(result) # Output should be [2, 4, 6, 8, 10] ``` **Notes:** - You may use the `multiprocessing` module libraries such as `Process`. - The chunk size for each process can be calculated as `len(data) // n`.","solution":"from typing import List import multiprocessing import multiprocessing.shared_memory import numpy as np def worker(shared_name, start, end): existing_shm = multiprocessing.shared_memory.SharedMemory(name=shared_name) # Create a NumPy array backed by shared memory shared_data = np.ndarray((end-start,), dtype=np.int64, buffer=existing_shm.buf[start*8:end*8]) shared_data *= 2 # Perform simple computation: multiply each element by 2 def process_shared_data(data: List[int], n: int) -> List[int]: size = len(data) if size == 0: return [] # Create a shared memory block shm = multiprocessing.shared_memory.SharedMemory(create=True, size=size * 8) # Create a NumPy array backed by shared memory and copy data to it shared_data = np.ndarray((size,), dtype=np.int64, buffer=shm.buf) shared_data[:] = data[:] # Divide the data into chunks for each process chunk_size = size // n processes = [] for i in range(n): start = i * chunk_size end = (i + 1) * chunk_size if i != n - 1 else size process = multiprocessing.Process(target=worker, args=(shm.name, start, end)) processes.append(process) process.start() # Wait for all processes to complete for process in processes: process.join() # Get the result from shared memory result = shared_data[:].tolist() # Clean up shared memory shm.close() shm.unlink() return result"},{"question":"**Coding Assessment Question** # Objective This question will assess your ability to work with the `configparser` module in Python to read, manipulate, and write INI files. # Problem Statement You are given a configuration file in INI format that contains settings for a web application. The file specifies various sections, each containing key-value pairs. Your task is to write a Python function that reads the configuration file, manipulates the settings based on certain rules, and writes the updated configuration back to the file. # Function Signature ```python def update_config(file_path: str) -> None: pass ``` # Input - `file_path`: A string representing the path to the configuration file (e.g., \'webapp.ini\'). # Output - None # Requirements 1. **Reading the File**: - Read the configuration from the given `file_path`. 2. **Manipulating Settings**: - Update the `Port` value in the `[server]` section to `8080`. If the section or the option doesn\'t exist, create them. - If the `[database]` section exists, change the `Timeout` value to `300`. If the `Timeout` option does not exist, add it. - Add a new section `[new_section]` with two options: `enabled` (`True`) and `level` (`1.0`). 3. **Handling Data Types**: - Ensure that numerical values are stored correctly as integers or floats. - Boolean values should be stored as `True` or `False`. 4. **Interpolation**: - In the `[paths]` section, add an option `base_dir` with the value `\\"/usr/local/app\\"`. - Add another option `logs_dir` with the value `\\"%(base_dir)s/logs\\"`. The interpolation should resolve to `\\"/usr/local/app/logs\\"`. 5. **Writing the File**: - Write the updated configuration back to the given `file_path`. You can assume that the initial configuration file looks like this: ```ini [server] Host = localhost [database] User = admin Password = secret [paths] config_dir = /etc/app ``` # Example For the initial configuration file provided, after executing `update_config(\'webapp.ini\')`, the file should look like this: ```ini [server] Host = localhost Port = 8080 [database] User = admin Password = secret Timeout = 300 [paths] config_dir = /etc/app base_dir = /usr/local/app logs_dir = /usr/local/app/logs # this line should correctly resolve the interpolation [new_section] enabled = True level = 1.0 ``` # Constraints - Ensure that your solution handles cases where sections or options may not initially exist. - Proper error handling for file operations is expected. Implement the function `update_config` to meet the above requirements.","solution":"import configparser def update_config(file_path: str) -> None: config = configparser.ConfigParser() # Reading the INI file config.read(file_path) # Updating the server section if not config.has_section(\\"server\\"): config.add_section(\\"server\\") config.set(\\"server\\", \\"Port\\", \\"8080\\") # Updating the database section if config.has_section(\\"database\\"): config.set(\\"database\\", \\"Timeout\\", \\"300\\") # Adding the new section if not config.has_section(\\"new_section\\"): config.add_section(\\"new_section\\") config.set(\\"new_section\\", \\"enabled\\", \\"True\\") config.set(\\"new_section\\", \\"level\\", \\"1.0\\") # Adding paths section options and interpolation if not config.has_section(\\"paths\\"): config.add_section(\\"paths\\") config.set(\\"paths\\", \\"base_dir\\", \\"/usr/local/app\\") config.set(\\"paths\\", \\"logs_dir\\", \\"%(base_dir)s/logs\\") # Writing the updated configuration back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Advanced Coding Assessment: Building a Class with Inheritance and Iterators Objective: Design a Python class that utilizes inheritance, class and instance variables, custom methods, and incorporates both iterators and generators. Problem Statement: You need to create a **Library System** using Python\'s class mechanisms. Specifically, you will be required to implement the following classes with the specified attributes and methods. 1. **Abstract Base Class: Book** - Attributes: - `title` (String) - `author` (String) - `year` (Integer) - Method: - `__init__(self, title, author, year)`: Initialize the book with title, author, and year. - `__str__(self)`: Returns a string representation of the book in the format `\\"title by author, year\\"`. 2. **Derived Class: EBook (inherits Book)** - Additional Attributes: - `file_size` (float): Size of the e-book in megabytes. - Additional Method: - `__init__(self, title, author, year, file_size)`: Initialize the e-book with title, author, year, and file_size. - `__str__(self)`: Override the `__str__` method to include file size in the format `\\"title by author, year [file_size MB]\\"`. 3. **Library Class**: - Attributes: - `books` (List): A collection that holds Book/EBook objects. - Methods: - `__init__(self)`: Initialize the library with an empty books list. - `add_book(self, book)`: Adds a book (either Book or EBook) to the library. - `__iter__(self)`: Returns an iterator that iterates through books in the library. - `search_by_author(self, author)`: Yield books written by the specified author one by one. Requirements: - Implement the classes with the described attributes and methods. - Ensure proper use of inheritance and method overriding. - Utilize iterators to iterate through the books in the library. - Use a generator function to yield books by a specified author. Input/Output: 1. **Input**: - Instances of books (Book or EBook) details for adding to the library. - Author name for searching books by that author. 2. **Output**: - Printing the book objects. - Yielding and printing books by the specified author. Example: ```python # Example of usage: library = Library() # Adding Books and EBooks book1 = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) ebook1 = EBook(\\"Python 101\\", \\"Guido van Rossum\\", 2020, 2.5) library.add_book(book1) library.add_book(book2) library.add_book(ebook1) # Printing all books in the library for book in library: print(book) # Searching for books by J.D. Salinger print(\\"nBooks by J.D. Salinger:\\") for book in library.search_by_author(\\"J.D. Salinger\\"): print(book) ``` Expected Output: ``` The Catcher in the Rye by J.D. Salinger, 1951 To Kill a Mockingbird by Harper Lee, 1960 Python 101 by Guido van Rossum, 2020 [2.5 MB] Books by J.D. Salinger: The Catcher in the Rye by J.D. Salinger, 1951 ``` Constraints: - The `year` attribute should be an integer greater than zero. - The `file_size` attribute should be a float greater than zero for EBooks.","solution":"class Book: def __init__(self, title, author, year): Initialize a book with title, author and year. self.title = title self.author = author self.year = year def __str__(self): Return a string representation of the book. return f\\"{self.title} by {self.author}, {self.year}\\" class EBook(Book): def __init__(self, title, author, year, file_size): Initialize an e-book with title, author, year, and file size. super().__init__(title, author, year) self.file_size = file_size def __str__(self): Return a string representation of the e-book, including file size. return f\\"{self.title} by {self.author}, {self.year} [{self.file_size} MB]\\" class Library: def __init__(self): Initialize the library with an empty books list. self.books = [] def add_book(self, book): Add a book (either Book or EBook) to the library. self.books.append(book) def __iter__(self): Return an iterator that iterates through books in the library. return iter(self.books) def search_by_author(self, author): Yield books written by the specified author one by one. for book in self.books: if book.author == author: yield book"},{"question":"**Objective:** Demonstrate your understanding of the `torch.random` module by implementing a function that generates random tensors, seeds the random number generator to ensure reproducibility, and demonstrates these features in a small example. **Question:** Write a function `random_tensor_operations` in Python that performs the following tasks using PyTorch: 1. **Sets a manual seed for the random number generator** to ensure reproducibility. Use `torch.random.manual_seed(seed)`. 2. **Generates a random tensor of the specified shape and type**. You must generate: - A tensor of shape `(m, n)` filled with random numbers from a standard normal distribution (mean=0, std=1). - A tensor of shape `(m, n)` filled with random integers between a specified range [low, high). 3. **Prints the state of the random number generator**, generates the tensors, and prints the resulting tensors to the console. 4. Validates that the function produces the same tensors when the seed is set to a fixed value. **Function Signature:** ```python def random_tensor_operations(seed: int, m: int, n: int, low: int, high: int): pass ``` **Inputs:** - `seed` (int): The seed value for the random number generator. - `m` (int): The number of rows of the generated tensors. - `n` (int): The number of columns of the generated tensors. - `low` (int): The lower bound of the range for generating random integers. - `high` (int): The upper bound of the range for generating random integers. **Outputs:** - Prints the state of the random number generator. - Prints the tensor filled with random numbers from a normal distribution. - Prints the tensor filled with random integers in the specified range. **Example Usage:** ```python random_tensor_operations(seed=42, m=3, n=4, low=10, high=50) ``` **Expected Output:** - The state of the random number generator will be printed. - A 3x4 tensor of random numbers from a standard normal distribution will be printed. - A 3x4 tensor of random integers between 10 and 50 will be printed. # Constraints and Requirements: - You must use the `torch` library. - Ensure the manual seed is set correctly to demonstrate reproducibility. - Consider performance implications if `m` and `n` are large.","solution":"import torch def random_tensor_operations(seed: int, m: int, n: int, low: int, high: int): Performs random tensor operations for reproducibility. Args: - seed (int): The seed for the random number generator. - m (int): The number of rows for the generated tensors. - n (int): The number of columns for the generated tensors. - low (int): The lower bound of the range for random integers in the integer tensor. - high (int): The upper bound of the range for random integers in the integer tensor. Returns: - Tuple containing two tensors: - Tensor from standard normal distribution. - Tensor with random integers in the specified range. # Set the manual seed for reproducibility torch.manual_seed(seed) # Print the initial state of the RNG initial_rng_state = torch.get_rng_state() print(\\"Initial RNG state:\\", initial_rng_state) # Generate tensor from standard normal distribution normal_tensor = torch.randn(m, n) print(\\"Normal Tensor:n\\", normal_tensor) # Generate tensor with random integers in the specified range integer_tensor = torch.randint(low, high, (m, n)) print(\\"Integer Tensor:n\\", integer_tensor) return normal_tensor, integer_tensor"},{"question":"**Problem Statement**: You are tasked with creating a script that compiles all the Python source files within a given directory tree. The script should also offer customization options by accepting a configuration dictionary that outlines specific compilation behaviors. **Requirements**: 1. Implement a function `custom_compile(directory: str, config: dict) -> bool` that: - Compiles all Python source files within the specified `directory`, including subdirectories by default. - Accepts a `config` dictionary that may contain the following optional keys to customize compilation: - `\'maxlevels\'`: Maximum recursion level for subdirectories (default is unlimited). - `\'force\'`: Boolean flag to force recompilation even if timestamps are up-to-date (default is `False`). - `\'quiet\'`: Integer value to control verbosity (0 for full output, 1 for errors only, 2 for no output). - `\'optimize\'`: Integer or list of integers specifying optimization levels (default is the standard level). - `\'invalidation_mode\'`: String indicating invalidation mode, can be \'timestamp\', \'checked-hash\', or \'unchecked-hash\' (default is `timestamp` if not set). 2. The function should return `True` if all files compiled successfully, otherwise `False`. 3. Your function should handle edge cases such as: - The specified directory does not exist. - Invalid values in the `config` dictionary. - No Python source files found in the directory. **Example**: ```python config = { \'maxlevels\': 2, \'force\': True, \'quiet\': 1, \'optimize\': [1, 2], \'invalidation_mode\': \'checked-hash\' } result = custom_compile(\'path/to/directory\', config) print(result) # Output: True or False based on compilation success ``` **Constraints**: - You may assume that the configuration keys are always valid strings as specified. - Only modify Python files with `.py` extension. **Performance Requirements**: - Your function should efficiently handle directories with a large number of files, using optimized methods for directory traversal and file compilation. **Hints**: - Consider using the functions from the `compileall` module. - Utilize exception handling to manage cases where directories do not exist or invalid configurations are provided. **Notes**: - Be sure to import necessary modules such as `compileall`, `os`, and `sys` in your solution.","solution":"import os import compileall import sys def custom_compile(directory: str, config: dict) -> bool: if not os.path.exists(directory): return False maxlevels = config.get(\'maxlevels\', None) force = config.get(\'force\', False) quiet = config.get(\'quiet\', 0) optimize = config.get(\'optimize\', -1) invalidation_mode = config.get(\'invalidation_mode\', \'timestamp\') # Process optimize if it\'s list if isinstance(optimize, list): optimize = max(optimize, default=-1) if invalidation_mode not in [\'timestamp\', \'checked-hash\', \'unchecked-hash\']: return False compiled = compileall.compile_dir( dir=directory, maxlevels=maxlevels, force=force, quiet=quiet, optimize=optimize, invalidation_mode=invalidation_mode ) return compiled"},{"question":"**Objective:** The aim of this assessment is to evaluate your proficiency in using the `array` module in Python to manipulate arrays. You are required to implement a series of functions that demonstrate a comprehensive understanding of `array` operations. **Question:** You are tasked with implementing a class `ArrayManipulator` that performs various operations on arrays using the Python `array` module. This class should include methods for initializing an array, appending elements, reversing, counting occurrences, and converting to and from bytes. **Specifications:** 1. **Initialization:** - The class should be initialized with a type code and an optional list of initial values. 2. **Methods:** - `append_value(value)`: Appends a new item to the end of the array. - `reverse_array()`: Reverses the order of the items in the array. - `count_occurrences(value)`: Returns the number of occurrences of `value` in the array. - `to_bytes()`: Converts the array to a byte representation. - `from_bytes(byte_str)`: Re-initializes the array using the given byte string. - `get_array()`: Returns the current array as a list. **Constraints:** - Type codes will be one of the valid type codes described in the documentation. - The input list for initialization, if provided, will contain elements matching the type code. **Example Usage:** ```python # Initialize with type code \'i\' (signed int) and initial values [1, 2, 3, 4, 5] manipulator = ArrayManipulator(\'i\', [1, 2, 3, 4, 5]) # Append a new value manipulator.append_value(6) # Count occurrences of 3 print(manipulator.count_occurrences(3)) # Output: 1 # Reverse the array manipulator.reverse_array() # Convert to bytes and then re-initialize using the byte representation byte_representation = manipulator.to_bytes() manipulator.from_bytes(byte_representation) # Get the current array as a list print(manipulator.get_array()) # Output: [6, 5, 4, 3, 2, 1] ``` **Implementation:** ```python from array import array class ArrayManipulator: def __init__(self, typecode, initial=None): self.typecode = typecode if initial is not None: self.arr = array(typecode, initial) else: self.arr = array(typecode) def append_value(self, value): self.arr.append(value) def reverse_array(self): self.arr.reverse() def count_occurrences(self, value): return self.arr.count(value) def to_bytes(self): return self.arr.tobytes() def from_bytes(self, byte_str): self.arr = array(self.typecode) self.arr.frombytes(byte_str) def get_array(self): return list(self.arr) ``` **Instructions:** - Implement the `ArrayManipulator` class as specified. - Ensure your code is well-structured and handles possible edge cases. - Write necessary test cases to validate the functionality of your implementation.","solution":"from array import array class ArrayManipulator: def __init__(self, typecode, initial=None): self.typecode = typecode if initial is not None: self.arr = array(typecode, initial) else: self.arr = array(typecode) def append_value(self, value): Appends a new item to the end of the array. self.arr.append(value) def reverse_array(self): Reverses the order of the items in the array. self.arr.reverse() def count_occurrences(self, value): Returns the number of occurrences of a value in the array. return self.arr.count(value) def to_bytes(self): Converts the array to a byte representation. return self.arr.tobytes() def from_bytes(self, byte_str): Re-initializes the array using the given byte string. self.arr = array(self.typecode) self.arr.frombytes(byte_str) def get_array(self): Returns the current array as a list. return list(self.arr)"},{"question":"Objective Create a Python function that parses an XML input, finds specific elements using XPath, modifies certain elements, and outputs the modified XML. Question You have an XML document that contains information about various books. The structure of the XML is as follows: ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <price>20.00</price> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <price>30.00</price> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <price>25.00</price> </book> </library> ``` Write a function `update_book_prices(xml_string: str) -> str` that takes an XML string containing the library information as input, finds all books published before the year 2006, increases their price by 10%, and returns the modified XML as a string. Function Signature ```python def update_book_prices(xml_string: str) -> str: ``` Input - `xml_string`: A string containing XML data about the library. Output - A string containing the modified XML data. Constraints - The XML document will always have the same structure as shown above. - Book publishing years will be valid integers. - Book prices will be valid floats. Example ```python input_xml = \'\'\'<?xml version=\\"1.0\\"?> <library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <price>20.00</price> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <price>30.00</price> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <price>25.00</price> </book> </library> \'\'\' expected_output = \'\'\'<library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <price>22.00</price> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <price>33.00</price> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <price>25.00</price> </book> </library> \'\'\' assert update_book_prices(input_xml) == expected_output ``` In this question, you are expected to: 1. Parse the XML string into an element tree. 2. Find all books published before 2006. 3. Increase the price of those books by 10%. 4. Convert the modified element tree back into a string and return it. Consider using the `xml.etree.ElementTree` module to achieve this.","solution":"import xml.etree.ElementTree as ET def update_book_prices(xml_string: str) -> str: Updates the prices of books published before 2006 by 10%. Args: - xml_string: A string containing XML data about the library. Returns: - A string containing the modified XML data. # Parse the XML string tree = ET.ElementTree(ET.fromstring(xml_string)) root = tree.getroot() # Iterate over each book in the library for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < 2006: price_element = book.find(\'price\') current_price = float(price_element.text) new_price = round(current_price * 1.10, 2) price_element.text = f\\"{new_price:.2f}\\" # Convert the modified tree back to a string return ET.tostring(root, encoding=\'unicode\') # Example usage and test input_xml = \'\'\'<?xml version=\\"1.0\\"?> <library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <price>20.00</price> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <price>30.00</price> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <price>25.00</price> </book> </library> \'\'\' expected_output = \'\'\'<library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <price>22.00</price> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <price>33.00</price> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <price>25.00</price> </book> </library>\'\'\' assert update_book_prices(input_xml) == expected_output"},{"question":"Objective: To assess your proficiency with file handling, compression/decompression using Python\'s `gzip` module, and exception management. Problem Statement: You are given a directory containing multiple text files. Your tasks are: 1. **Compress All Text Files**: Write a function `compress_all_files(directory_path)` that takes a directory path as input and compresses all the `.txt` files in the directory into gzip format. The compressed files should have the same name as the original files but with a `.gz` extension (e.g., `file.txt` becomes `file.txt.gz`). 2. **Decompress a Specific File**: Write a function `decompress_file(file_path)` that takes the path of a single `.gz` file, decompresses it, and writes the output to a file without the `.gz` extension (e.g., `file.txt.gz` becomes `file.txt`). 3. **Generate a Summary Report**: Write a function `generate_summary(directory_path)` that scans the specified directory and generates a report of all `.gz` files. The report should include: - Original file names before compression. - Compressed file sizes. - Number of files compressed. 4. **Exception Handling**: Ensure your functions handle cases where: - The directory does not exist. - Files other than `.txt` exist in the directory. - The input file for decompression does not have a `.gz` extension or is a corrupt gzip file. Constraints: - You may assume the directory contains no subdirectories. - Implementations should avoid reading large files entirely into memory; use efficient file handling. Input and Output Formats: ```python def compress_all_files(directory_path: str) -> None: Compress all .txt files in the directory. :param directory_path: Path to the directory containing text files. :return: None def decompress_file(file_path: str) -> None: Decompress the specified .gz file. :param file_path: Path to the .gz file. :return: None def generate_summary(directory_path: str) -> dict: Generate a summary of compressed files in the directory. :param directory_path: Path to the directory. :return: Dictionary containing the summary report. ``` **Example:** Assume the directory `/home/user/files` contains: - `file1.txt` - `file2.txt` - `image.png` After running `compress_all_files(\'/home/user/files\')`, the directory should contain: - `file1.txt.gz` - `file2.txt.gz` - `image.png` Running `generate_summary(\'/home/user/files\')` might return: ```python { \\"original_files\\": [\\"file1.txt\\", \\"file2.txt\\"], \\"compressed_files\\": [\\"file1.txt.gz\\", \\"file2.txt.gz\\"], \\"compressed_sizes\\": [128, 256], # Example sizes in bytes \\"num_files_compressed\\": 2 } ``` Evaluation Criteria: 1. **Correctness**: Ensure that all `.txt` files are properly compressed and decompressed. 2. **Edge Cases**: Properly handle files with no .txt files, invalid directories, and invalid .gz files. 3. **Efficiency**: Use efficient read/write operations. 4. **Documentation**: Include appropriate docstrings and comments for clarity.","solution":"import os import gzip import glob def compress_all_files(directory_path: str) -> None: Compress all .txt files in the directory. :param directory_path: Path to the directory containing text files. :return: None if not os.path.isdir(directory_path): raise FileNotFoundError(f\\"Directory {directory_path} does not exist.\\") for file_name in os.listdir(directory_path): if file_name.endswith(\\".txt\\"): txt_file_path = os.path.join(directory_path, file_name) gz_file_path = txt_file_path + \\".gz\\" with open(txt_file_path, \'rb\') as f_in: with gzip.open(gz_file_path, \'wb\') as f_out: while chunk := f_in.read(1024): f_out.write(chunk) os.remove(txt_file_path) def decompress_file(file_path: str) -> None: Decompress the specified .gz file. :param file_path: Path to the .gz file. :return: None if not file_path.endswith(\\".gz\\"): raise ValueError(f\\"File {file_path} is not a .gz file.\\") decompressed_file_path = file_path[:-3] with gzip.open(file_path, \'rb\') as f_in: with open(decompressed_file_path, \'wb\') as f_out: while chunk := f_in.read(1024): f_out.write(chunk) os.remove(file_path) def generate_summary(directory_path: str) -> dict: Generate a summary of compressed (.gz) files in the directory. :param directory_path: Path to the directory. :return: Dictionary containing the summary report. if not os.path.isdir(directory_path): raise FileNotFoundError(f\\"Directory {directory_path} does not exist.\\") summary = { \\"original_files\\": [], \\"compressed_files\\": [], \\"compressed_sizes\\": [], \\"num_files_compressed\\": 0 } for file_name in os.listdir(directory_path): if file_name.endswith(\\".gz\\"): original_file_name = file_name[:-3] summary[\\"original_files\\"].append(original_file_name) gz_file_path = os.path.join(directory_path, file_name) summary[\\"compressed_files\\"].append(file_name) summary[\\"compressed_sizes\\"].append(os.path.getsize(gz_file_path)) summary[\\"num_files_compressed\\"] = len(summary[\\"compressed_files\\"]) return summary"},{"question":"# Advanced Python Type Management Implement a Python function that interacts with the type management functions from the provided `python310` documentation. Specifically, you will create a custom heap-allocated type and manipulate its properties. Follow the guidelines below: Function Signature ```python def create_and_test_custom_type(): ``` Requirements 1. **Create a Heap-Allocated Custom Type**: - Use `PyType_FromModuleAndSpec` or equivalent to create a new heap-allocated type. - Name the type `\\"CustomType\\"`. - Define a basic structure and one slot for its memory size (`basicsize`). 2. **Modify Type Properties**: - Implement functionality to modify the type\'s properties, such as its flags or associated module. - Ensure that the internal lookup cache and other associated properties are correctly updated. 3. **Type Verification and Subclassing**: - Verify if the newly created type is indeed a type using `PyType_Check`. - Create a subtype of the custom type and verify its relationship using `PyType_IsSubtype`. 4. **Cache Management**: - Demonstrate clearing and modifying the internal cache for the custom type. - Use functions like `PyType_ClearCache` and `PyType_Modified` appropriately. 5. **Performance Considerations**: - Ensure that operations on the custom type are efficient and properly managed. - Utilize the provided functions to handle memory allocation and deallocation. Example Output - The function should print confirmation messages at each step, such as: - \\"Heap-allocated type \'CustomType\' created successfully.\\" - \\"Type properties updated and cache cleared.\\" - \\"Subtype relationship verified.\\" Input and Output - This function does not take any input parameters. - The output is a series of print statements confirming the actions taken. Note - This exercise requires a deep understanding of Python\'s C API for type management. Simulate the C functions and structures in Python to the best extent possible. - Ensure that exception handling is in place for any API errors. - You may use pseudocode or comments to outline parts that cannot be directly implemented in Python but are crucial for understanding the solution.","solution":"def create_and_test_custom_type(): import ctypes # Define the basic C structure for the custom type class PyCustomType(ctypes.Structure): _fields_ = [(\\"ob_refcnt\\", ctypes.c_long), (\\"ob_type\\", ctypes.POINTER(ctypes.c_void_p)), (\\"basicsize\\", ctypes.c_long)] # Simulate PyType_FromModuleAndSpec, here mimicked as creating an instance of the custom type def PyType_FromModuleAndSpec(module, spec): custom_type = PyCustomType(ob_refcnt=1, ob_type=None, basicsize=0) return custom_type # Simulate modification of type properties def modify_type_properties(custom_type, basicsize): custom_type.basicsize = basicsize # Simulate PyType_Check def PyType_Check(obj): return isinstance(obj, PyCustomType) # Simulate PyType_IsSubtype def PyType_IsSubtype(subtype, supertype): return issubclass(type(subtype), type(supertype)) # Simulate PyType_ClearCache and PyType_Modified def PyType_ClearCache(): print(\\"Type cache cleared.\\") def PyType_Modified(obj): print(f\\"Type {type(obj).__name__} modified.\\") # Create custom type custom_module = None # Simulate a module custom_spec = None # Simulate a spec custom_type = PyType_FromModuleAndSpec(custom_module, custom_spec) if PyType_Check(custom_type): print(\\"Heap-allocated type \'CustomType\' created successfully.\\") # Modify type properties modify_type_properties(custom_type, 104) if custom_type.basicsize == 104: print(\\"Type properties updated and memory size set to 104.\\") # Clear cache and modify type PyType_ClearCache() PyType_Modified(custom_type) print(\\"Cache cleared and type modified.\\") # Verify subtype relationship class CustomSubType(PyCustomType): pass custom_subtype = CustomSubType() if PyType_IsSubtype(custom_subtype, custom_type): print(\\"Subtype relationship verified.\\") create_and_test_custom_type()"},{"question":"**Object Implementation and Memory Management in Python** You are required to create a custom object type in Python that demonstrates your understanding of the underlying principles of object implementation, memory management, and attribute access as described in the documentation. # Task: Implement a new object type called `AdvancedList` that extends the basic functionality of Python\'s list object with additional features: 1. Allocate space on the heap for the AdvancedList object. 2. Implement methods to add and remove elements like a standard list. 3. Implement methods to get the sum of all elements if they are numbers. 4. Ensure that the object supports cyclic garbage collection. # Requirements: 1. **Initialization (Constructor)**: The object should be initialized with an optional list of elements. 2. **Add Method**: A method to add elements to the list. 3. **Remove Method**: A method to remove elements from the list. 4. **Sum Method**: A method to return the sum of all numeric elements. 5. **Support for Garbage Collection**: The object should support cyclic garbage collection to manage memory efficiently. # Constraints: 1. The object should handle elements like a typical Python list, throwing appropriate errors for invalid operations. 2. Performance of adding and removing elements should be optimized similar to Pythonâ€™s built-in list. 3. The sum method should be efficient and handle large lists with minimal performance overhead. # Example: ```python # Sample Usage adv_list = AdvancedList([1, 2, 3]) adv_list.add(4) adv_list.remove(2) total = adv_list.sum() # Returns 7 adv_list2 = AdvancedList([\'a\', \'b\']) adv_list2.add(\'c\') adv_list2.remove(\'a\') total2 = adv_list2.sum() # Returns 0 (non-numeric list) ``` # Input: - Various method calls to `add`, `remove`, and `sum`. # Output: - The current state of the list and/or the sum of numeric elements. # Constraints: - The class must efficiently utilize memory and manage allocation/deallocation. # Note: - You are not allowed to use Python\'s built-in `sum()` method directly for your sum method implementation. - Ensure all edge cases are handled, including empty lists and heterogeneous element types.","solution":"import gc class AdvancedList: def __init__(self, elements=None): if elements is None: self.elements = [] else: if not isinstance(elements, list): raise ValueError(\\"Input must be a list.\\") self.elements = elements # Ensure the object is trackable by the garbage collector gc.enable() def add(self, element): self.elements.append(element) def remove(self, element): if element in self.elements: self.elements.remove(element) else: raise ValueError(\\"Element not found in the list.\\") def sum(self): total = 0 for element in self.elements: if isinstance(element, (int, float)): total += element return total"},{"question":"# Python310 Coding Assessment Question **Objective:** Design a Python C extension that implements a custom callable class, which supports both the `tp_call` and `vectorcall` protocols. Your extension should include the ability to handle positional and keyword arguments correctly. # Requirements: 1. Implement a C extension module that defines a new type `MyCallable`. 2. The `MyCallable` type should be callable from Python with both positional and keyword arguments. 3. Ensure that your implementation supports both `tp_call` and `vectorcall` protocols efficiently. 4. Your `MyCallable` type should: - Print the received positional and keyword arguments. - Return a tuple containing the count of positional and keyword arguments. # Function Signature: ```c typedef struct { PyObject_HEAD /* Type-specific fields go here. */ } MyCallable; static PyObject *MyCallable_tp_call(PyObject *callable, PyObject *args, PyObject *kwargs); static PyObject *MyCallable_vectorcall(PyObject *callable, PyObject *const *args, size_t nargsf, PyObject *kwnames); static PyTypeObject MyCallableType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"mymodule.MyCallable\\", .tp_basicsize = sizeof(MyCallable), .tp_call = MyCallable_tp_call, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_VECTORCALL, .tp_vectorcall_offset = offsetof(MyCallable, vectorcall) }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, .m_name = \\"mymodule\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *m; if (PyType_Ready(&MyCallableType) < 0) return NULL; m = PyModule_Create(&mymodule); if (m == NULL) return NULL; Py_INCREF(&MyCallableType); if (PyModule_AddObject(m, \\"MyCallable\\", (PyObject *)&MyCallableType) < 0) { Py_DECREF(&MyCallableType); Py_DECREF(m); return NULL; } return m; } ``` # Implementation Steps: 1. Define and initialize the `MyCallable` type, including the `PyObject_HEAD` macro and any additional fields necessary for your implementation. 2. Implement the `MyCallable_tp_call` function to handle calls using the `tp_call` protocol, which will print the provided arguments and return the specified tuple. 3. Implement the `MyCallable_vectorcall` function to handle calls using the `vectorcall` protocol, ensuring efficient handling of positional and keyword arguments. 4. Setup the `PyTypeObject` for the `MyCallable` type, ensuring the `tp_call`, `tp_flags`, and `tp_vectorcall_offset` fields are correctly set. 5. Define the module using `PyModuleDef` and initialize it with `PyMODINIT_FUNC`. 6. Compile and test your extension to ensure it meets the requirements. # Constraints: - Handle all potential argument combinations (positional only, keyword only, and both). - Ensure error handling is implemented where necessary (e.g., when arguments are not present). - Your implementation should be compatible with Python 3.9 and later versions. # Example Usage in Python: ```python import mymodule callable_instance = mymodule.MyCallable() # Calling with positional arguments result = callable_instance(\'arg1\', \'arg2\') print(result) # Should print (2, 0) # Calling with keyword arguments result = callable_instance(kwarg1=\'value1\', kwarg2=\'value2\') print(result) # Should print (0, 2) # Calling with both positional and keyword arguments result = callable_instance(\'arg1\', kwarg1=\'value1\') print(result) # Should print (1, 1) ``` # Submission: Submit the complete C extension code with a brief explanation on how to compile and test the module.","solution":"import mymodule def callable_function(*args, **kwargs): Prints received positional and keyword arguments and returns a tuple containing the count of positional and keyword arguments. print(f\\"Positional args: {args}\\") print(f\\"Keyword args: {kwargs}\\") return (len(args), len(kwargs))"},{"question":"<|Analysis Begin|> The provided documentation is an extensive reference for the `os` module in Python, detailing many functions and constants used for interfacing with the underlying operating system. The `os` module provides functionalities such as file and directory operations, environment variable manipulations, process management, and random number generation. Based on this documentation, we can design a coding assessment question that requires students to work with these functionalities, particularly focusing on file and directory handling, which are fundamental and universally applicable aspects of `os`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Demonstrate understanding of file handling, directory operations, and environment variables using the `os` module in Python. **Question**: You are tasked to create a script that performs several operations using the `os` module. The script should: 1. Create a directory structure as follows: ``` project/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ raw/ â”‚ â””â”€â”€ processed/ â””â”€â”€ logs/ ``` 2. Create a text file `log.txt` in the `logs/` directory and write the current environment variables into this file. Each environment variable should be written as `KEY=VALUE` on a new line. 3. Create a dummy file `data.txt` in the `data/raw/` directory and write some sample text data into it. 4. Move `data.txt` from `data/raw/` to `data/processed/`. 5. Ensure that the script is idempotent, meaning running it multiple times does not raise any exceptions or cause the creation of duplicate directories or files. **Requirements**: - Use `os` functions such as `os.makedirs()`, `os.path.exists()`, `os.getenv()`, `os.environ`, `os.rename()`, and `os.path.join()`. - Provide meaningful comments in the code for clarity. - Ensure that your script handles possible exceptions gracefully and prints appropriate messages for any errors encountered. **Input**: None. All operations are to be performed within the script itself. **Output**: No direct output is needed, but intermediate outputs can be logged or printed as necessary for demonstrating successful execution of steps. **Example Code**: ```python import os def create_directory_structure(): base_dirs = [\'project/data/raw\', \'project/data/processed\', \'project/logs\'] for dir_path in base_dirs: os.makedirs(dir_path, exist_ok=True) def write_environment_variables_to_log(): log_file_path = os.path.join(\'project\', \'logs\', \'log.txt\') with open(log_file_path, \'w\') as log_file: for key, value in os.environ.items(): log_file.write(f\\"{key}={value}n\\") def create_dummy_file(): raw_data_path = os.path.join(\'project\', \'data\', \'raw\', \'data.txt\') with open(raw_data_path, \'w\') as data_file: data_file.write(\\"Sample text data\\") def move_raw_to_processed(): raw_data_path = os.path.join(\'project\', \'data\', \'raw\', \'data.txt\') processed_data_path = os.path.join(\'project\', \'data\', \'processed\', \'data.txt\') if os.path.exists(raw_data_path): os.rename(raw_data_path, processed_data_path) def main(): try: create_directory_structure() write_environment_variables_to_log() create_dummy_file() move_raw_to_processed() print(\\"Script executed successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main() ``` Make sure your final script adheres to these steps and provides a clear, thorough implementation!","solution":"import os def create_directory_structure(): base_dirs = [\'project/data/raw\', \'project/data/processed\', \'project/logs\'] for dir_path in base_dirs: os.makedirs(dir_path, exist_ok=True) def write_environment_variables_to_log(): log_file_path = os.path.join(\'project\', \'logs\', \'log.txt\') with open(log_file_path, \'w\') as log_file: for key, value in os.environ.items(): log_file.write(f\\"{key}={value}n\\") def create_dummy_file(): raw_data_path = os.path.join(\'project\', \'data\', \'raw\', \'data.txt\') with open(raw_data_path, \'w\') as data_file: data_file.write(\\"Sample text data\\") def move_raw_to_processed(): raw_data_path = os.path.join(\'project\', \'data\', \'raw\', \'data.txt\') processed_data_path = os.path.join(\'project\', \'data\', \'processed\', \'data.txt\') if os.path.exists(raw_data_path): os.rename(raw_data_path, processed_data_path) def main(): try: create_directory_structure() write_environment_variables_to_log() create_dummy_file() move_raw_to_processed() print(\\"Script executed successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a utility script that checks the current version of pip in the environment and ensures that it is up-to-date using the `ensurepip` module. If pip is not installed, the script should install the latest version provided by `ensurepip`. Your task involves both invoking the `ensurepip` module programmatically and handling command-line arguments. Requirements: 1. Create a Python script called `pip_manager.py` that performs the following: - Checks if pip is installed and prints its version. - If pip is not installed, use `ensurepip.bootstrap()` to install it. - If pip is installed, check if the installed version is older than the version provided by `ensurepip`. If it is, upgrade pip using `ensurepip.bootstrap(upgrade=True)`. - Supports command-line arguments for additional options: - `--root <dir>`: Installs pip relative to the provided directory. - `--user`: Installs pip in the user site packages directory. - `--altinstall`: Avoids installing the `pipX` script. - `--default-pip`: Ensures that the `pip` script is installed. - `--verbosity <level>`: Sets the verbosity level of the bootstrap output. Input: - You should parse command-line arguments using the `argparse` module. - Command-line syntax: `python pip_manager.py [--root <dir>] [--user] [--altinstall] [--default-pip] [--verbosity <level>]` Output: - Print the current version of pip if it is already installed. - Print a message indicating that pip has been installed or upgraded. - Handle any errors that might arise due to incorrect options or conflicts (e.g., both `--altinstall` and `--default-pip` being provided). Constraints: - The script should work without accessing the Internet. - Ensure proper error handling and input validation. - Follow best practices for scripting and modular code design. Here is a template to get started: ```python import ensurepip import subprocess import argparse import sys def check_pip_version(): try: output = subprocess.check_output([sys.executable, \'-m\', \'pip\', \'--version\'], text=True) return output.split()[1] except subprocess.CalledProcessError: return None def install_or_upgrade_pip(args): root = args.root upgrade = args.upgrade user = args.user altinstall = args.altinstall default_pip = args.default_pip verbosity = args.verbosity # Error handling for incompatible options if altinstall and default_pip: print(\\"Error: --altinstall and --default-pip cannot be used together.\\") return # Bootstrapping pip with specified options ensurepip.bootstrap(root=root, upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity) def main(): parser = argparse.ArgumentParser(description=\'Manage pip installation.\') parser.add_argument(\'--root\', type=str, help=\'Install relative to this root directory.\') parser.add_argument(\'--user\', action=\'store_true\', help=\'Install in the user site packages directory.\') parser.add_argument(\'--altinstall\', action=\'store_true\', help=\'Avoid installing the pipX script.\') parser.add_argument(\'--default-pip\', action=\'store_true\', help=\'Ensure the pip script is installed.\') parser.add_argument(\'--verbosity\', type=int, default=0, help=\'Set the verbosity level of the bootstrap output.\') args = parser.parse_args() current_pip_version = check_pip_version() if current_pip_version: print(f\\"Current pip version: {current_pip_version}\\") ensure_pip_version = ensurepip.version().split()[0] if current_pip_version < ensure_pip_version: print(f\\"Upgrading pip to version {ensure_pip_version}...\\") install_or_upgrade_pip(args) else: print(\\"pip is not installed. Installing pip...\\") install_or_upgrade_pip(args) if __name__ == \'__main__\': main() ``` Notes: - Ensure your script adheres to the constraints and handles the specified command-line options correctly. - Write proper docstrings and comments for your functions and logic.","solution":"import ensurepip import subprocess import argparse import sys def check_pip_version(): Check the current version of pip installed in the environment. Returns: str: pip version if installed, None otherwise. try: output = subprocess.check_output([sys.executable, \'-m\', \'pip\', \'--version\'], text=True) return output.split()[1] except subprocess.CalledProcessError: return None def install_or_upgrade_pip(args): Install or upgrade pip using ensurepip module with specified arguments. Args: args (argparse.Namespace): Parsed command-line arguments. # Error handling for incompatible options if args.altinstall and args.default_pip: print(\\"Error: --altinstall and --default-pip cannot be used together.\\") return # Bootstrapping pip with specified options ensurepip.bootstrap( root=args.root, upgrade=True, user=args.user, altinstall=args.altinstall, default_pip=args.default_pip, verbosity=args.verbosity ) def main(): Main function to parse arguments and manage pip installation. parser = argparse.ArgumentParser(description=\'Manage pip installation.\') parser.add_argument(\'--root\', type=str, help=\'Install relative to this root directory.\') parser.add_argument(\'--user\', action=\'store_true\', help=\'Install in the user site packages directory.\') parser.add_argument(\'--altinstall\', action=\'store_true\', help=\'Avoid installing the pipX script.\') parser.add_argument(\'--default-pip\', action=\'store_true\', help=\'Ensure the pip script is installed.\') parser.add_argument(\'--verbosity\', type=int, default=0, help=\'Set the verbosity level of the bootstrap output.\') args = parser.parse_args() current_pip_version = check_pip_version() if current_pip_version: print(f\\"Current pip version: {current_pip_version}\\") ensure_pip_version = ensurepip.version().split()[0] if current_pip_version < ensure_pip_version: print(f\\"Upgrading pip to version {ensure_pip_version}...\\") install_or_upgrade_pip(args) else: print(\\"pip is not installed. Installing pip...\\") install_or_upgrade_pip(args) if __name__ == \'__main__\': main()"},{"question":"**Out-of-Core Learning with Scikit-learn** In this question, you will implement an out-of-core learning pipeline using scikit-learn. You need to work with a large dataset that cannot fit into memory all at once. You will build a system that streams data, extracts features using the hashing trick, and trains an incremental classifier. # Task 1. Implement a data streaming generator function `stream_data` that reads from a large file and yields mini-batches of data. 2. Extract features using `HashingVectorizer`. 3. Train an incremental classifier (`SGDClassifier`) using the `partial_fit` method. # Function signatures 1. `stream_data(file_path, batch_size) -> generator` - **Input**: - `file_path` (str): File path to the large dataset. - `batch_size` (int): The number of examples per mini-batch. - **Output**: - A generator yielding tuples `(X_batch, y_batch)`, where `X_batch` is a list of raw text examples and `y_batch` is a list of associated labels. 2. `train_incremental_classifier(file_path, batch_size, num_classes) -> SGDClassifier` - **Input**: - `file_path` (str): File path to the large dataset. - `batch_size` (int): The number of examples per mini-batch. - `num_classes` (int): The number of classes in the target variable. - **Output**: - Trained `SGDClassifier`. # Constraints - The large dataset is a text file where each line contains a text example followed by its label, separated by a comma. - Ensure that the system does not load the entire dataset into memory at once. # Example Usage ```python file_path = \\"large_dataset.txt\\" batch_size = 100 num_classes = 3 classifier = train_incremental_classifier(file_path, batch_size, num_classes) ``` **Note**: You need to install scikit-learn if you haven\'t already. You can do so using `pip install scikit-learn`. # Solution Outline 1. **Streaming Data**: - Create the `stream_data` function to read the file line by line and yield mini-batches of examples until the entire file is read. 2. **Feature Extraction**: - Use `HashingVectorizer` from `sklearn.feature_extraction.text` to convert raw text into feature vectors suitable for training. 3. **Incremental Training**: - Implement the `train_incremental_classifier` function that initializes an `SGDClassifier` and uses the `partial_fit` method to train on each mini-batch of data yielded by the `stream_data` generator. Good luck!","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def stream_data(file_path, batch_size): Generator function that reads from a large file and yields mini-batches of data. :param file_path: str, File path to the large dataset. :param batch_size: int, The number of examples per mini-batch. :return: generator yielding tuples (X_batch, y_batch). X_batch, y_batch = [], [] with open(file_path, \'r\') as f: for line in f: text, label = line.rsplit(\',\', 1) X_batch.append(text) y_batch.append(int(label.strip())) if len(X_batch) == batch_size: yield X_batch, y_batch X_batch, y_batch = [], [] if X_batch: # Yield the last batch if there are remaining samples yield X_batch, y_batch def train_incremental_classifier(file_path, batch_size, num_classes): Trains an incremental classifier using the partial_fit method and data streamed from file. :param file_path: str, File path to the large dataset. :param batch_size: int, The number of examples per mini-batch. :param num_classes: int, The number of classes in the target variable. :return: Trained SGDClassifier. vectorizer = HashingVectorizer(n_features=2**20) clf = SGDClassifier() first_pass = True for X_batch, y_batch in stream_data(file_path, batch_size): X_transformed = vectorizer.transform(X_batch) if first_pass: clf.partial_fit(X_transformed, y_batch, classes=np.arange(num_classes)) first_pass = False else: clf.partial_fit(X_transformed, y_batch) return clf"},{"question":"# Objective: Implement a function to compare two directories recursively and report all differences, including file mismatches, directory mismatches, and potential errors. Your function should also handle shallow and deep comparisons based on a parameter. # Task: Write a Python function `compare_directories(dir1: str, dir2: str, shallow: bool = True) -> dict` that: 1. Compares the directories `dir1` and `dir2` recursively. 2. Returns a dictionary with the following keys: - `\'match\'`: A list of paths of files that are identical in both directories. - `\'mismatch\'`: A list of paths of files that are different in the two directories. - `\'errors\'`: A list of paths of files that could not be compared. 3. The comparison should use the `shallow` parameter to determine whether to perform shallow comparisons (default is `True`). # Input: - `dir1`, `dir2`: Strings representing the paths of the two directories to be compared. - `shallow`: Boolean indicating whether to perform shallow comparisons. Defaults to `True`. # Output: - A dictionary containing three lists: - `\'match\'`: Paths of files that are the same in both directories. - `\'mismatch\'`: Paths of files that are different in the two directories. - `\'errors\'`: Paths of files that could not be compared. # Constraints: 1. You may assume that both directories and their contents are accessible and that paths provided are valid directories. 2. The function should handle large directories efficiently. 3. Handle file system errors gracefully and include those in the `\'errors\'` list. # Example: ```python dir1 = \'path/to/directory1\' dir2 = \'path/to/directory2\' result = compare_directories(dir1, dir2, shallow=False) print(result) # Expected output format: # { # \'match\': [\'dir1/file1.txt\', \'dir1/subdir/file2.txt\', ...], # \'mismatch\': [\'dir1/file3.txt\', \'dir1/subdir/file4.txt\', ...], # \'errors\': [\'dir1/unreadable_file.txt\', ...] # } ``` # Notes: - Use the provided `filecmp` module to perform the comparisons. - Ensure that the function operates recursively to cover all subdirectories.","solution":"import os import filecmp import shutil def compare_directories(dir1, dir2, shallow=True): Compare two directories recursively and report all differences. Parameters: - dir1: Path to the first directory. - dir2: Path to the second directory. - shallow: Whether to perform shallow comparisons (default: True). Returns: - A dictionary with keys \'match\', \'mismatch\', and \'errors\' containing lists of file paths. result = { \'match\': [], \'mismatch\': [], \'errors\': [] } for root, dirs, files in os.walk(dir1): rel_path = os.path.relpath(root, dir1) for file in files: file1_path = os.path.join(root, file) file2_path = os.path.join(dir2, rel_path, file) try: if os.path.exists(file2_path): if filecmp.cmp(file1_path, file2_path, shallow=shallow): result[\'match\'].append(os.path.relpath(file1_path, dir1)) else: result[\'mismatch\'].append(os.path.relpath(file1_path, dir1)) else: result[\'mismatch\'].append(os.path.relpath(file1_path, dir1)) except Exception as e: result[\'errors\'].append(os.path.relpath(file1_path, dir1)) for root, dirs, files in os.walk(dir2): rel_path = os.path.relpath(root, dir2) for file in files: file1_path = os.path.join(dir1, rel_path, file) file2_path = os.path.join(root, file) if not os.path.exists(file1_path): result[\'mismatch\'].append(os.path.relpath(file2_path, dir2)) return result"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the `concurrent.futures` module in Python 3.10 by implementing a function that performs parallel computation of a given task. # Problem Statement You are given a list of numbers and a computationally intensive task that needs to be performed on each number. You need to write a function `parallel_square` which computes the square of each number in the list in parallel using the `concurrent.futures` module. # Function Signature ```python def parallel_square(numbers: List[int]) -> List[int]: pass ``` # Input - `numbers`: A list of integers `[n1, n2, ..., nk]` where `1 <= len(numbers) <= 1000` and `-10^6 <= ni <= 10^6`. # Output - Returns a list of integers where each integer is the square of the corresponding integer in the input list. # Constraints - The function should utilize the `concurrent.futures` module to perform the computations in parallel. - The order of the output list should match the order of the input list. # Example ```python input_list = [1, 2, 3, 4, 5] output_list = parallel_square(input_list) print(output_list) # Output: [1, 4, 9, 16, 25] ``` # Performance Requirements - The function should be efficient and make use of concurrency to speed up the computation. # Hints - Consider using `ThreadPoolExecutor` or `ProcessPoolExecutor` from the `concurrent.futures` module for managing parallel tasks. - Use `map` or `submit` methods of the executor to distribute the work across multiple threads or processes. # Notes - Ensure your implementation is robust and handles edge cases such as empty lists appropriately.","solution":"from concurrent.futures import ThreadPoolExecutor from typing import List def parallel_square(numbers: List[int]) -> List[int]: Given a list of numbers, returns a list with the square of each number, computed in parallel. def square(n): return n * n with ThreadPoolExecutor() as executor: result = list(executor.map(square, numbers)) return result"},{"question":"**Objective:** Test your understanding of PyTorch\'s MTIA backend by working with stream and device management. **Question:** You are given a task to implement a function `execute_on_mtia_device` that initializes the MTIA backend, selects a specific device (if available), sets up a new Stream, performs a simple tensor operation within this Stream context, and then synchronizes the stream to ensure the operation completes. Here are the necessary details: Function Signature: ```python def execute_on_mtia_device( device_index: int) -> torch.Tensor: Perform tensor operation on a specific MTIA device using a created Stream. Args: device_index (int): The index of the MTIA device to use. Returns: torch.Tensor: The resulting tensor after the operation. ``` Detailed Requirements: 1. **Initialization and Availability:** - Check if the MTIA backend is available and initialized. - Initialize the MTIA backend if it is not already initialized. 2. **Device Management:** - Check the number of available devices. If the `device_index` is out of range, throw an appropriate exception. - Set the device to the given `device_index`. 3. **Stream Management:** - Create a new Stream for the specified device. - Within the context of this Stream, create a tensor of size (1000, 1000) filled with values of your choice. - Perform a basic tensor operation (e.g., tensor addition or multiplication). 4. **Synchronization:** - Synchronize the stream to ensure all operations are completed. 5. **Return:** - Return the resulting tensor as output. Constraints: - The function should handle any potential errors gracefully and provide meaningful error messages. - Ensure proper resource management (e.g., releasing memory). Example Usage: ```python result_tensor = execute_on_mtia_device(0) print(result_tensor) ``` **Note:** You can use torch\'s `torch.cuda` interfaces for any necessary tensor operations as the MTIA backend would be for device management.","solution":"import torch def execute_on_mtia_device(device_index: int) -> torch.Tensor: Perform tensor operation on a specific MTIA device using a created Stream. Args: device_index (int): The index of the MTIA device to use. Returns: torch.Tensor: The resulting tensor after the operation. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available. MTIA backend cannot be initialized.\\") # Initialize the MTIA backend, if needed if not torch.cuda.is_initialized(): torch.cuda.init() num_devices = torch.cuda.device_count() if device_index < 0 or device_index >= num_devices: raise ValueError(f\\"Device index {device_index} is out of range. Available devices: {num_devices}\\") device = torch.device(f\'cuda:{device_index}\') # Set the current device to the specified index torch.cuda.set_device(device) stream = torch.cuda.Stream(device=device) with torch.cuda.stream(stream): # Perform tensor operations tensor_a = torch.full((1000, 1000), 1.0, device=device) tensor_b = torch.full((1000, 1000), 2.0, device=device) result_tensor = tensor_a + tensor_b # Simple tensor addition # Synchronize the stream to ensure the operations are complete stream.synchronize() return result_tensor"},{"question":"# HTTP Client Coding Assessment Objective The goal of this assessment is to evaluate your understanding of the `http.client` module in Python. You will implement a function that uses this module to perform an HTTP GET request and process the response. Task Implement a function named `fetch_resource` that performs the following steps: 1. Create a connection to an HTTP server using the `http.client.HTTPConnection` class. 2. Send a GET request to the server to fetch a resource. 3. Read the response from the server and return it. Function Signature ```python def fetch_resource(host: str, resource: str) -> str: Fetches a resource from the given host via HTTP GET request. Args: - host (str): The host name (e.g., \'www.example.com\') of the server. - resource (str): The resource path (e.g., \'/index.html\') to request from the server. Returns: - str: The response body from the server as a string. Raises: - Exception: For any HTTP error or connection issue. ``` Input - `host`: A string representing the server\'s host name. - `resource`: A string representing the resource path to request. Output - A string containing the response body from the server. Constraints and Requirements 1. You must use the `http.client.HTTPConnection` class to create the connection and send the request. 2. Handle errors gracefully by raising an exception if any HTTP error occurs. 3. The function should return the full response body as a string. Example ```python # Example usage: result = fetch_resource(\'www.example.com\', \'/index.html\') print(result) ``` In this example, the function should connect to \'www.example.com\', request the resource \'/index.html\', and print the response body. Notes - Assume the server is always running and accessible. - Consider edge cases such as invalid resource paths or network issues. - You can use the examples provided in the documentation as a reference for constructing your solution.","solution":"import http.client def fetch_resource(host: str, resource: str) -> str: Fetches a resource from the given host via HTTP GET request. Args: - host (str): The host name (e.g., \'www.example.com\') of the server. - resource (str): The resource path (e.g., \'/index.html\') to request from the server. Returns: - str: The response body from the server as a string. Raises: - Exception: For any HTTP error or connection issue. try: connection = http.client.HTTPConnection(host) connection.request(\'GET\', resource) response = connection.getresponse() if response.status != 200: raise Exception(f\\"HTTP error {response.status}: {response.reason}\\") response_body = response.read().decode() connection.close() return response_body except Exception as e: raise Exception(f\\"Failed to fetch resource from {host}{resource}: {e}\\")"},{"question":"**Task:** You are required to write a function that takes a complex nested data structure (a combination of dictionaries, lists, sets, and tuples) and pretty-prints it using various customization options of the `pprint` module. Your function should allow the user to specify the formatting parameters `indent`, `width`, `depth`, and `compact`. **Specifications:** 1. **Function Name**: `custom_pretty_printer` 2. **Parameters**: - `data`: The complex nested data structure (can contain dictionaries, lists, sets, tuples, etc.). - `indent` (Optional): Integer specifying the amount of indentation for each nesting level. Default is 1. - `width` (Optional): Integer specifying the maximum number of characters per line. Default is 80. - `depth` (Optional): Integer specifying the number of nesting levels to print. If None, no constraint on the depth. Default is None. - `compact` (Optional): Boolean flag that, if True, formats as many items as possible per line. Default is False. **Function Signature**: ```python def custom_pretty_printer(data, indent=1, width=80, depth=None, compact=False): pass ``` **Output**: - The function should print the formatted representation of the `data` structure to the standard output. **Constraints**: - Do not use any libraries other than the standard Python libraries. - Handle all edge cases (e.g., deeply nested structures, structures containing various types of elements). **Example Usage**: ```python data = { \'key1\': [1, 2, 3, {\'a\': \'apple\', \'b\': [10, 20, 30]}, 5], \'key2\': (\'tuple_element\', {\'nested_key\': \'nested_value\'}), \'key3\': \'simple_value\' } custom_pretty_printer(data, indent=2, width=50, depth=2, compact=True) # Expected output: { \'key1\': [1, 2, 3, {\'a\': \'apple\', \'b\': [10, 20, 30]}, 5], \'key2\': (\'tuple_element\', {\'nested_key\': ...}), \'key3\': \'simple_value\' } ```","solution":"import pprint def custom_pretty_printer(data, indent=1, width=80, depth=None, compact=False): Pretty-prints the provided data structure using the customization options. Parameters: - data: The complex nested data structure to be printed. - indent: Amount of indentation for each nesting level. - width: Maximum number of characters per line. - depth: Number of nesting levels to print. - compact: Format as many items as possible per line if True. printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact) printer.pprint(data)"},{"question":"**Objective**: To assess your understanding of Python\'s `tokenize` module, including generating tokens from a Python source code, processing these tokens, and converting the tokens back into a valid Python file. Problem Statement You are given the task of creating a script that reads a Python source file, modifies the token stream by substituting all function names with their uppercase equivalents, and writes the modified source code back to a new file. Requirements 1. **Function Signature**: Implement a function `modify_function_names(input_file: str, output_file: str) -> None`. 2. **Input**: - `input_file` (string): The path to the input Python source file. - `output_file` (string): The desired path for the output file where the modified Python source code will be saved. 3. **Output**: Write the modified Python source code to the `output_file`. 4. **Constraints**: - Your function should correctly handle any syntactically valid Python source code. - Replace function names only within their definitions (i.e., only the first occurrence of a function name in its declaration should be modified to uppercase). - Do not modify function names that appear outside of their definitions (e.g., function calls, variables). - The modified Python source code must remain valid and runnable. 5. **Performance Requirements**: - The function should run efficiently even for larger Python files (e.g., with hundreds of lines of code). Example Consider an input file `example.py` with the following content: ```python def say_hello(name): print(\\"Hello, \\" + name) def greet_user(user): say_hello(user.name) def farewell(): print(\\"Goodbye!\\") ``` After running `modify_function_names(\'example.py\', \'modified_example.py\')`, the content of `modified_example.py` should be: ```python def SAY_HELLO(name): print(\\"Hello, \\" + name) def GREET_USER(user): say_hello(user.name) def FAREWELL(): print(\\"Goodbye!\\") ``` # Additional Details 1. **Token Types**: You can identify function name tokens by looking for the sequence of tokens `NAME` followed by `OP \'(\'` at the start of a function definition. 2. **Token Generation**: Use the `tokenize.tokenize()` function to read and process token streams. 3. **Token Modification**: Modify only the function name tokens that occur at the start of function definitions. 4. **Token Reconstruction**: Use the `tokenize.untokenize()` function to convert the modified token stream back to the source code. 5. **File Encoding**: Handle file encoding properly using `tokenize.open()` to read the input file. Use the `tokenize` module\'s capabilities to accomplish the above task. Submission Submit the complete implementation of the `modify_function_names()` function.","solution":"import tokenize def modify_function_names(input_file, output_file): Modifies function names in a Python source file to their uppercase equivalents and writes the modified source code to a new file. with tokenize.open(input_file) as f: tokens = list(tokenize.generate_tokens(f.readline)) modified_tokens = [] inside_function_def = False for token in tokens: if token.type == tokenize.NAME and token.string == \'def\': inside_function_def = True elif inside_function_def and token.type == tokenize.NAME: new_token = tokenize.TokenInfo(token.type, token.string.upper(), token.start, token.end, token.line) modified_tokens.append(new_token) inside_function_def = False continue modified_tokens.append(token) with open(output_file, \'w\') as f: f.write(tokenize.untokenize(modified_tokens))"},{"question":"Question # Dataset Generation and Model Evaluation You are tasked with generating synthetic datasets and evaluating the performance of a classification model on them. You will demonstrate your understanding of dataset generation using scikit-learn\'s `make_classification` function and model evaluation using a simple classifier. # Instructions 1. **Dataset Generation**: Write a function `generate_datasets` that creates three different synthetic datasets using `make_classification` with varying parameters. 2. **Classifier Training and Evaluation**: Write a function `evaluate_classifier` that trains a simple classification model (use Logistic Regression) on each dataset and evaluates it using cross-validation. Report the mean accuracy score. 3. **Plotting**: Visualize the datasets using a scatter plot. # Detailed Steps 1. **Defining the Function for Dataset Generation** - Function signature: `def generate_datasets():` - Generate three datasets with the following specifications: - Dataset 1: 1000 samples, 10 features, 2 informative features, 1 cluster per class, 2 classes. - Dataset 2: 1000 samples, 10 features, 5 informative features, 2 clusters per class, 3 classes. - Dataset 3: 1000 samples, 15 features, 10 informative features, 1 cluster per class, 3 classes. - Return the datasets as a list of tuples `[(X1, y1), (X2, y2), (X3, y3)]`. 2. **Defining the Function for Classifier Evaluation** - Function signature: `def evaluate_classifier(datasets):` - Parameters: - `datasets` (list of tuples): A list containing the datasets generated in the previous step. - For each dataset, perform the following: - Train a Logistic Regression classifier. - Use 5-fold cross-validation to evaluate the classifier. - Compute and print the mean accuracy score for each dataset. 3. **Plotting the Datasets** - Visualize the first two features of each dataset using a scatter plot. - Label each plot appropriately. # Example Output ```python # Example call datasets = generate_datasets() evaluate_classifier(datasets) # Expected output might include printed accuracy scores like: # Mean accuracy score for Dataset 1: 0.85 # Mean accuracy score for Dataset 2: 0.70 # Mean accuracy score for Dataset 3: 0.75 # Plots for visualization # (Include scatter plots for each dataset showing two of the features) ``` # Note: - Ensure that the implementations are efficient and make use of scikit-learn\'s built-in functions. - Add necessary imports, such as numpy, matplotlib, and scikit-learn. - Handle any edge cases appropriately and make sure the code is well-documented.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def generate_datasets(): Generates three synthetic datasets with varying parameters using make_classification. Returns: list of tuples: A list containing the datasets [(X1, y1), (X2, y2), (X3, y3)] dataset1 = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=2, n_clusters_per_class=1, n_classes=2, random_state=42) dataset2 = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2, n_clusters_per_class=2, n_classes=3, random_state=42) dataset3 = make_classification(n_samples=1000, n_features=15, n_informative=10, n_redundant=2, n_clusters_per_class=1, n_classes=3, random_state=42) return [dataset1, dataset2, dataset3] def evaluate_classifier(datasets): Evaluates a Logistic Regression classifier on each dataset using 5-fold cross-validation. Args: datasets (list of tuples): A list containing the datasets [(X1, y1), (X2, y2), (X3, y3)] Prints: The mean accuracy score for each dataset. classifier = LogisticRegression(max_iter=1000) for i, (X, y) in enumerate(datasets): scores = cross_val_score(classifier, X, y, cv=5, scoring=\'accuracy\') print(f\\"Mean accuracy score for Dataset {i+1}: {np.mean(scores):.2f}\\") def plot_datasets(datasets): Visualizes the first two features of each dataset using a scatter plot. Args: datasets (list of tuples): A list containing the datasets [(X1, y1), (X2, y2), (X3, y3)] fig, axes = plt.subplots(1, 3, figsize=(18, 5)) for i, (X, y) in enumerate(datasets): axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') axes[i].set_title(f\\"Dataset {i+1}\\") axes[i].set_xlabel(\\"Feature 1\\") axes[i].set_ylabel(\\"Feature 2\\") plt.tight_layout() plt.show()"},{"question":"# Decision Tree Implementation and Evaluation Objective: To test the understanding and application of decision trees in scikit-learn for classification and regression tasks, including handling missing values, pruning, and visualizing the decision tree. Task: 1. **Decision Tree Classification**: a. Load the Iris dataset using `sklearn.datasets.load_iris`. b. Split the dataset into training (80%) and testing (20%) sets. c. Implement a `DecisionTreeClassifier` with a maximum depth of 3. Train the classifier on the training subset. d. Evaluate the accuracy of the classifier on the test dataset. e. Visualize the trained decision tree using the `plot_tree` function. 2. **Decision Tree Regression**: a. Load the diabetes dataset using `sklearn.datasets.load_diabetes`. b. Split the dataset into training (80%) and testing (20%) sets. c. Implement a `DecisionTreeRegressor` with a maximum depth of 3. Train the regressor on the training subset. d. Evaluate the mean square error of the regressor on the test dataset. e. Visualize the trained regression tree using the `plot_tree` function. 3. **Handling Missing Values**: a. Introduce missing values into a sample feature of the Iris dataset by setting 10% of the feature values to `np.nan`. b. Train a `DecisionTreeClassifier` with default settings on the modified dataset. c. Evaluate the accuracy of the classifier on the test dataset containing missing values. 4. **Pruning**: a. Implement `DecisionTreeClassifier` on the Iris dataset with `ccp_alpha` set to a non-zero value for pruning. b. Train the classifier on the Iris dataset. c. Evaluate the accuracy of the pruned classifier on the test dataset. d. Compare the performance of pruned and unpruned trees. Constraints: - Use `max_depth=3` for trees unless specified otherwise. - Ensure 10% of feature values are randomly selected for introducing missing values. - Use `random_state=42` for reproducibility in train/test split. Expected Input and Output: 1. For classification tasks: - Input: Training and testing data from the Iris dataset. - Output: Accuracy of the classifier on the test dataset and visualization of the decision tree. 2. For regression tasks: - Input: Training and testing data from the diabetes dataset. - Output: Mean squared error of the regressor on the test dataset and visualization of the regression tree. Performance Requirements: - The implemented decision trees (classifier and regressor) should be efficient in handling the given datasets. - The visualization should clearly depict the tree structure and splits at each node. You are provided with the following starter code template: ```python import numpy as np from sklearn.datasets import load_iris, load_diabetes from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt # Function to introduce missing values def introduce_missing_values(X, percentage=0.1): np.random.seed(42) missing_mask = np.random.rand(*X.shape) < percentage X[missing_mask] = np.nan return X # Decision Tree Classification def decision_tree_classification(): iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() # Decision Tree Regression def decision_tree_regression(): diabetes = load_diabetes() X, y = diabetes.data, diabetes.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train, y_train) y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\'Mean Squared Error: {mse}\') plt.figure(figsize=(20,10)) plot_tree(reg, filled=True) plt.show() # Handling Missing Values def handle_missing_values(): iris = load_iris() X, y = iris.data, iris.target X = introduce_missing_values(X, percentage=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy with missing values: {accuracy}\') plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() # Pruning def decision_tree_pruning(): iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf_unpruned = DecisionTreeClassifier(max_depth=3, random_state=42) clf_unpruned.fit(X_train, y_train) y_pred_unpruned = clf_unpruned.predict(X_test) accuracy_unpruned = accuracy_score(y_test, y_pred_unpruned) print(f\'Unpruned Accuracy: {accuracy_unpruned}\') clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=0.01) clf_pruned.fit(X_train, y_train) y_pred_pruned = clf_pruned.predict(X_test) accuracy_pruned = accuracy_score(y_test, y_pred_pruned) print(f\'Pruned Accuracy: {accuracy_pruned}\') plt.figure(figsize=(20,10)) plot_tree(clf_pruned, filled=True) plt.show() # Test the functions decision_tree_classification() decision_tree_regression() handle_missing_values() decision_tree_pruning() ```","solution":"import numpy as np from sklearn.datasets import load_iris, load_diabetes from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt # Function to introduce missing values def introduce_missing_values(X, percentage=0.1): np.random.seed(42) missing_mask = np.random.rand(*X.shape) < percentage X[missing_mask] = np.nan return X # Decision Tree Classification def decision_tree_classification(): iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) plt.figure(figsize=(20,10)) tree_plot = plot_tree(clf, filled=True) plt.show() return accuracy, tree_plot # Decision Tree Regression def decision_tree_regression(): diabetes = load_diabetes() X, y = diabetes.data, diabetes.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train, y_train) y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) plt.figure(figsize=(20,10)) tree_plot = plot_tree(reg, filled=True) plt.show() return mse, tree_plot # Handling Missing Values def handle_missing_values(): iris = load_iris() X, y = iris.data, iris.target X = introduce_missing_values(X, percentage=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) plt.figure(figsize=(20,10)) tree_plot = plot_tree(clf, filled=True) plt.show() return accuracy, tree_plot # Pruning def decision_tree_pruning(): iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf_unpruned = DecisionTreeClassifier(max_depth=3, random_state=42) clf_unpruned.fit(X_train, y_train) y_pred_unpruned = clf_unpruned.predict(X_test) accuracy_unpruned = accuracy_score(y_test, y_pred_unpruned) clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=0.01) clf_pruned.fit(X_train, y_train) y_pred_pruned = clf_pruned.predict(X_test) accuracy_pruned = accuracy_score(y_test, y_pred_pruned) plt.figure(figsize=(20,10)) tree_plot = plot_tree(clf_pruned, filled=True) plt.show() return accuracy_unpruned, accuracy_pruned, tree_plot"},{"question":"Question: Custom Event Loop Policy and Child Watcher # Objective: Create a custom event loop policy and a custom child watcher, then demonstrate their integration and usage in an asyncio-based application. # Requirements: 1. **Custom Event Loop Policy**: - Subclass the `asyncio.DefaultEventLoopPolicy` to create a custom event loop policy called `CustomEventLoopPolicy`. - Override the `get_event_loop()` method to add logging behavior each time the event loop is retrieved. - Override the `new_event_loop()` method to log when a new event loop is created. 2. **Custom Child Watcher**: - Implement a custom child watcher subclassing `asyncio.AbstractChildWatcher`. Name the class `CustomChildWatcher`. - Implement the methods: `add_child_handler()`, `remove_child_handler()`, `attach_loop()`, `is_active()`, and `close()`. - Ensure `add_child_handler()` logs when a child handler is registered. - Ensure `remove_child_handler()` logs when a child handler is removed. 3. **Integration and Demonstration**: - Set the custom event loop policy and custom child watcher in the asyncio framework. - Create a simple application that spawns a subprocess (like running a shell command) and waits for it to complete using the custom policy and watcher. - Log the relevant events to demonstrate the integration and how the custom components are utilized. # Input and Output: * Input: No specific input needed. * Output: Log statements showing the events occurring due to the custom policy and watcher. # Constraints: * Use only the `asyncio` module for asynchronous operations. * Ensure compatibility with Python 3.10. # Example: ```python import asyncio import os class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"CustomEventLoopPolicy: get_event_loop called.\\") return loop def new_event_loop(self): loop = super().new_event_loop() print(\\"CustomEventLoopPolicy: new_event_loop created.\\") return loop class CustomChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): self.loop = None def add_child_handler(self, pid, callback, *args): print(f\\"CustomChildWatcher: add_child_handler called for PID {pid}.\\") def remove_child_handler(self, pid): print(f\\"CustomChildWatcher: remove_child_handler called for PID {pid}.\\") return True def attach_loop(self, loop): if self.loop is not loop: self.loop = loop print(\\"CustomChildWatcher: attach_loop called.\\") def is_active(self): return self.loop is not None def close(self): print(\\"CustomChildWatcher: close called.\\") # Set custom policy and watcher asyncio.set_event_loop_policy(CustomEventLoopPolicy()) custom_watcher = CustomChildWatcher() asyncio.set_child_watcher(custom_watcher) async def run_subprocess(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello World!\\"\') await proc.communicate() # Run the demonstration asyncio.run(run_subprocess()) ``` # Explanation: 1. **CustomEventLoopPolicy**: - Overrides `get_event_loop()` and `new_event_loop()` to add logging. 2. **CustomChildWatcher**: - Implements required methods from abstract base class and adds logging to register/remove child handlers. 3. **Integration**: - Sets the custom policy and child watcher in asyncio. - Spawns a subprocess to demonstrate the working of the custom components. # Note: Ensure your solution handles edge cases and adheres to best coding practices.","solution":"import asyncio import os import logging # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() logger.info(\\"CustomEventLoopPolicy: get_event_loop called.\\") return loop def new_event_loop(self): loop = super().new_event_loop() logger.info(\\"CustomEventLoopPolicy: new_event_loop created.\\") return loop class CustomChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): self.loop = None self.callbacks = {} def add_child_handler(self, pid, callback, *args): self.callbacks[pid] = (callback, args) logger.info(f\\"CustomChildWatcher: add_child_handler called for PID {pid}.\\") def remove_child_handler(self, pid): if pid in self.callbacks: del self.callbacks[pid] logger.info(f\\"CustomChildWatcher: remove_child_handler called for PID {pid}.\\") return True return False def attach_loop(self, loop): if self.loop is not loop: self.loop = loop logger.info(\\"CustomChildWatcher: attach_loop called.\\") def is_active(self): return self.loop is not None def close(self): logger.info(\\"CustomChildWatcher: close called.\\") self.callbacks.clear() # Set custom policy and watcher asyncio.set_event_loop_policy(CustomEventLoopPolicy()) custom_watcher = CustomChildWatcher() asyncio.get_child_watcher().attach_loop(asyncio.get_event_loop()) async def run_subprocess(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello World!\\"\') await proc.communicate() # Run the demonstration if __name__ == \\"__main__\\": asyncio.run(run_subprocess())"},{"question":"# Naive Bayes Text Classification Task You are assigned the task of implementing a text classification model using the Naive Bayes algorithm. This task will assess your understanding of different types of Naive Bayes classifiers provided by the scikit-learn package and your ability to apply them to real-world data. **Objective:** Implement a text classifier using `Naive Bayes` to categorize movie reviews into positive and negative sentiments. **Dataset:** You will use the [movie reviews dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/) from Cornell University, which contains a collection of movie reviews tagged with their sentiment (positive or negative). For simplicity, assume the data is provided to you in two lists: `reviews` (list of strings) and `labels` (list of 0s and 1s, where 0 indicates a negative review, and 1 indicates a positive review). **Task:** 1. Load the provided data (lists `reviews` and `labels`). 2. Preprocess the text data (e.g., tokenize, remove stopwords, etc.). 3. Vectorize the text data using `TfidfVectorizer`. 4. Split the data into training and testing sets. 5. Train at least three different Naive Bayes classifiers: - MultinomialNB - ComplementNB - BernoulliNB 6. Evaluate each classifier using appropriate metrics (e.g., accuracy, precision, recall, and F1-score). 7. Print and compare the performance of each classifier. **Requirements:** - Your solution should be implemented in Python using the scikit-learn library. - Use appropriate text preprocessing and vectorization techniques. - Clearly comment your code to explain each step. - Ensure your code runs efficiently for the given dataset size. **Input:** - `reviews`: List of strings, where each string is a movie review. - `labels`: List of integers, where each integer is 0 (negative) or 1 (positive). **Output:** - Performance metrics (accuracy, precision, recall, F1-score) for each Naive Bayes classifier. **Example:** ```python from sklearn.datasets import fetch_openml reviews = [ \\"I absolutely loved this movie, it was fantastic!\\", \\"I hated this film. It was the worst movie I\'ve ever seen.\\" ] labels = [1, 0] ``` **Submission:** Submit your code as a Python script or Jupyter notebook. Include any additional files required for preprocessing (e.g., stopword lists) if applicable.","solution":"import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Sample Data reviews = [ \\"I absolutely loved this movie, it was fantastic!\\", \\"I hated this film. It was the worst movie I\'ve ever seen.\\", \\"An excellent film with great performances!\\", \\"Terrible movie. Very boring and slow.\\", \\"A wonderful movie with a touching story.\\", \\"Awful movie. Waste of time.\\" ] labels = [1, 0, 1, 0, 1, 0] # Preprocessing and Vectorizing vectorizer = TfidfVectorizer(stop_words=\'english\') X = vectorizer.fit_transform(reviews) y = np.array(labels) # Splitting the Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize classifiers classifiers = { \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB() } # Function to evaluate the classifier def evaluate_classifier(clf, X_train, y_train, X_test, y_test): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) return accuracy, precision, recall, f1 # Evaluate each classifier results = {} for name, clf in classifiers.items(): accuracy, precision, recall, f1 = evaluate_classifier(clf, X_train, y_train, X_test, y_test) results[name] = { \'Accuracy\': accuracy, \'Precision\': precision, \'Recall\': recall, \'F1 Score\': f1 } # Print the results for name, metrics in results.items(): print(f\\"Results for {name}:\\") print(f\\"Accuracy: {metrics[\'Accuracy\']:.2f}\\") print(f\\"Precision: {metrics[\'Precision\']:.2f}\\") print(f\\"Recall: {metrics[\'Recall\']:.2f}\\") print(f\\"F1 Score: {metrics[\'F1 Score\']:.2f}\\") print()"},{"question":"Objective You are required to implement a function using memoryview objects that demonstrate an understanding of managing and manipulating memory buffers. Task Write a function `contiguous_submemoryview(obj, start, length)` that takes three arguments: 1. `obj` - An object that supports the buffer interface. 2. `start` - An integer indicating the starting position of the contiguous memory segment. 3. `length` - An integer indicating the length of the contiguous memory segment. The function should return a memoryview object that references a contiguous memory segment from the given object starting at the `start` position and of the given `length`. If the specified memory segment is not already contiguous, a new contiguous memoryview object should be created and returned. Expected Function Signature ```python def contiguous_submemoryview(obj: object, start: int, length: int) -> memoryview: pass ``` Input - `obj`: An object supporting the buffer interface, such as a bytes, bytearray, or an array. - `start`: Integer representing the starting position of the memory segment. - `length`: Integer representing the length of the memory segment. Output - A memoryview object that references a contiguous memory segment from the specified start position and length. Constraints - `start` should be greater than or equal to 0 and less than the length of the total buffer. - `length` should be greater than 0 and start+length should not exceed the length of the total buffer. - The function should handle cases where the provided memory segment is already contiguous efficiently without the need to create a copy. Example ```python data = bytearray(b\\"Hello, World!\\") memory_segment = contiguous_submemoryview(data, 7, 5) print(memory_segment.tobytes()) # Output: b\'World\' ``` In the example above, the function should return a memoryview object that references the substring \\"World\\" of the bytearray. Additional Information - You may use the functions `PyMemoryView_FromObject`, `PyMemoryView_GetContiguous`, and other related functions to manipulate memoryview objects. - The function should be efficient in terms of both memory and runtime. Good luck!","solution":"def contiguous_submemoryview(obj, start, length): Returns a contiguous memoryview object from the given object, starting at the specified position and with the given length. if start < 0 or length <= 0 or (start + length) > len(obj): raise ValueError(\\"Invalid start or length for the provided buffer object.\\") # Obtain the memoryview object mem_view = memoryview(obj) # Slice the memoryview sub_view = mem_view[start:start+length] # Ensure it is contiguous if not sub_view.contiguous: # Convert to a contiguous memoryview return sub_view.toreadslice() return sub_view"},{"question":"**Objective:** You are tasked with implementing a high-level asynchronous function that fetches and processes data from multiple sources concurrently. This will test your understanding of coroutines, the asyncio library, and its high-level APIs. **Question:** You are given a list of URLs, each containing data in JSON format. Your task is to implement an asynchronous function `fetch_and_process_data(urls: List[str]) -> List[Dict[str, Any]]` that fetches data from each URL concurrently, processes it by extracting specific fields, and returns a list of dictionaries containing the processed data. **Function Signature:** ```python import asyncio from typing import List, Dict, Any async def fetch_and_process_data(urls: List[str]) -> List[Dict[str, Any]]: # Your implementation here pass ``` **Requirements:** 1. **Fetching Data Asynchronously:** - Use the `aiohttp` library to fetch data from each URL asynchronously. - Ensure that the data fetching is done concurrently for all URLs. 2. **Processing Data:** - Each URL returns a JSON object. - Extract the following fields from each JSON object: - `\'id\'`: The unique identifier. - `\'name\'`: The name associated with the identifier. - `\'value\'`: The value associated with the identifier. 3. **Returning Processed Data:** - Return a list of dictionaries, each dictionary containing the fields `\'id\'`, `\'name\'`, and `\'value\'` extracted from the JSON object. **Constraints:** - The list of URLs will not exceed 100. - Each URL response size will not exceed 1MB. - Assume all given URLs are valid and reachable. **Example Usage:** ```python import asyncio # Sample URLs (replace with actual URLs that return JSON data) urls = [ \\"http://example.com/api/data1\\", \\"http://example.com/api/data2\\", \\"http://example.com/api/data3\\" ] async def main(): processed_data = await fetch_and_process_data(urls) print(processed_data) asyncio.run(main()) ``` **Performance Requirements:** - Your implementation should be efficient in handling up to 100 concurrent requests. - Ensure minimal blocking on network IO by fully utilizing async/await capabilities. **Note:** - You may need to install the `aiohttp` library using `pip install aiohttp` if it\'s not already installed. **Hints:** - Consider using `asyncio.gather` to concurrently run multiple asynchronous tasks. - Use `aiohttp.ClientSession()` for making asynchronous HTTP requests. Good luck!","solution":"import aiohttp import asyncio from typing import List, Dict, Any async def fetch_data(session: aiohttp.ClientSession, url: str) -> Dict[str, Any]: async with session.get(url) as response: data = await response.json() return { \'id\': data.get(\'id\'), \'name\': data.get(\'name\'), \'value\': data.get(\'value\') } async def fetch_and_process_data(urls: List[str]) -> List[Dict[str, Any]]: async with aiohttp.ClientSession() as session: tasks = [fetch_data(session, url) for url in urls] results = await asyncio.gather(*tasks) return results"},{"question":"**Question: Unicode and HTML Entity Converter** You are given the documentation of the `html.entities` module that defines several dictionaries for mapping between HTML entities and their corresponding Unicode characters. Your task is to implement a function that takes a string containing HTML entity names and converts it to a string containing the corresponding Unicode characters. Additionally, you need to implement the reverse conversion, from a string with Unicode characters to a string with HTML entity names. # Function Signatures ```python def entities_to_unicode(html_string: str) -> str: pass def unicode_to_entities(unicode_string: str) -> str: pass ``` # Input and Output Formats `entities_to_unicode` **Input:** - `html_string` (type: str): A string that may contain HTML entity names (e.g., `\'&gt;\'`). **Output:** - (type: str): The string with HTML entity names replaced by their corresponding Unicode characters. `unicode_to_entities` **Input:** - `unicode_string` (type: str): A string that may contain Unicode characters (e.g., `\'>\'`). **Output:** - (type: str): The string with Unicode characters replaced by their corresponding HTML entity names. # Constraints 1. All HTML entity names in the input `html_string` will start with `&` and end with `;`. 2. The input strings may contain multiple HTML entity names or none at all. 3. The function should handle both named entities and decimal/numeric character references. # Examples `entities_to_unicode` ```python assert entities_to_unicode(\'&gt;&lt;&amp;\') == \'><&\' assert entities_to_unicode(\'Hello &nbsp; World\') == \'Hello xa0 World\' ``` `unicode_to_entities` ```python assert unicode_to_entities(\'><&\') == \'&gt;&lt;&amp;\' assert unicode_to_entities(\'Hello xa0 World\') == \'Hello &nbsp; World\' ``` # Notes 1. Use the `html.entities.name2codepoint` dictionary to perform entity to Unicode conversion. 2. Use the `html.entities.codepoint2name` dictionary to perform Unicode to entity conversion. 3. Ensure that the solution is efficient and handles edge cases such as strings without any entities or Unicode characters.","solution":"import html def entities_to_unicode(html_string: str) -> str: Converts HTML entities in the input string to their corresponding Unicode characters. return html.unescape(html_string) def unicode_to_entities(unicode_string: str) -> str: Converts Unicode characters in the input string to their corresponding HTML entities. entity_map = html.entities.codepoint2name result = [] for char in unicode_string: if ord(char) in entity_map: result.append(f\\"&{entity_map[ord(char)]};\\") else: result.append(char) return \'\'.join(result)"},{"question":"# Linear Least Squares Solver Using PyTorch **Objective**: Implement a function that uses PyTorch to solve a linear least squares problem. **Description**: Given a matrix (A in mathbb{R}^{m times n}) and a vector (B in mathbb{R}^{m}), find a vector (X in mathbb{R}^{n}) that minimizes the Euclidean norm (||AX - B||). This problem can be solved using the function `torch.linalg.lstsq`. **Function Signature**: ```python import torch def linear_least_squares(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Solves the linear least squares problem for the given matrix A and vector B. Args: A (torch.Tensor): A 2D tensor of shape (m, n) representing the matrix. B (torch.Tensor): A 1D tensor of shape (m,) representing the vector. Returns: torch.Tensor: A 1D tensor of shape (n,) representing the vector X that minimizes ||AX - B||. Raises: ValueError: If `A` is not a 2D tensor or `B` is not a 1D tensor. ValueError: If the number of rows in `A` does not match the length of `B`. pass ``` # Requirements: 1. **Inputs**: - `A` is a 2D tensor of shape `(m, n)`. - `B` is a 1D tensor of shape `(m,)`. 2. **Outputs**: - The function should return a 1D tensor `X` of shape `(n,)`. 3. **Constraints**: - The function should raise a `ValueError` if `A` is not a 2D tensor. - The function should raise a `ValueError` if `B` is not a 1D tensor. - The function should raise a `ValueError` if the number of rows in `A` does not match the length of `B`. 4. **Implementation**: - Use `torch.linalg.lstsq` to solve the linear least squares problem. 5. **Performance**: - The solution should be efficient and capable of handling reasonably large matrices and vectors. # Example: ```python # Example inputs A = torch.tensor([[2, 1], [1, 3], [1, 5]], dtype=torch.float32) B = torch.tensor([5, 6, 7], dtype=torch.float32) # Example usage X = linear_least_squares(A, B) print(X) # Output should be a tensor representing the solution vector. ``` **Notes**: - Make sure your solution leverages the `torch.linalg.lstsq` function appropriately. - You can assume that the input tensors `A` and `B` will contain valid numerical values, and the problem is well-posed.","solution":"import torch def linear_least_squares(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Solves the linear least squares problem for the given matrix A and vector B. Args: A (torch.Tensor): A 2D tensor of shape (m, n) representing the matrix. B (torch.Tensor): A 1D tensor of shape (m,) representing the vector. Returns: torch.Tensor: A 1D tensor of shape (n,) representing the vector X that minimizes ||AX - B||. Raises: ValueError: If `A` is not a 2D tensor or `B` is not a 1D tensor. ValueError: If the number of rows in `A` does not match the length of `B`. if A.dim() != 2: raise ValueError(\\"A must be a 2D tensor.\\") if B.dim() != 1: raise ValueError(\\"B must be a 1D tensor.\\") if A.shape[0] != B.shape[0]: raise ValueError(\\"The number of rows in A must match the length of B.\\") result = torch.linalg.lstsq(A, B) X = result.solution return X.squeeze()"},{"question":"Advanced FTP Operations with ftplib Introduction You are tasked with writing a Python program that demonstrates the use of the `ftplib` module to connect to an FTP server, navigate through directories, and perform file transfer operations securely using `FTP_TLS`. Requirements 1. **Connect to FTP Server (with and without TLS)**: - Write a function `connect_ftp(host: str, user: str, passwd: str, use_tls: bool) -> ftplib.FTP` that connects to an FTP server. - If `use_tls` is `True`, use the `FTP_TLS` class to establish a secure connection. - If `use_tls` is `False`, use the `FTP` class. - The function should return the FTP instance ready for use. 2. **Navigation and Listing**: - Write a function `list_directory(ftp: ftplib.FTP, directory: str) -> List[str]` that takes an FTP instance and a directory path. - It should change to the specified directory and return a list of filenames in the directory. 3. **Download File**: - Write a function `download_file(ftp: ftplib.FTP, remote_file_path: str, local_file_path: str) -> None` that downloads a file from the FTP server. - The `remote_file_path` specifies the file on the server, and `local_file_path` specifies where the file should be saved locally. 4. **Upload File**: - Write a function `upload_file(ftp: ftplib.FTP, local_file_path: str, remote_file_path: str) -> None` that uploads a file to the FTP server. - The `local_file_path` specifies the file to upload from the local system, and `remote_file_path` specifies where it should be stored on the server. 5. **Secure Data Connection**: - For connections using `FTP_TLS`, ensure that the data transfers are also secured. Use the `prot_p()` method for secure data transfer. Constraints - You must handle exceptions that may arise due to network issues, incorrect credentials, or other FTP-related errors. Ensure proper cleanup (e.g., disconnecting from the server) in case of an error. - Properly close the connection after all operations are complete. Example Usage ```python def main(): host = \\"example.com\\" user = \\"username\\" passwd = \\"password\\" use_tls = True ftp = connect_ftp(host, user, passwd, use_tls) # List root directory files files = list_directory(ftp, \\"/\\") print(\\"Root directory files:\\", files) # Download a file from server download_file(ftp, \\"remote/path/to/file.txt\\", \\"local/path/to/file.txt\\") # Upload a file to server upload_file(ftp, \\"local/path/to/another_file.txt\\", \\"remote/path/to/another_file.txt\\") # Close the connection ftp.quit() if __name__ == \\"__main__\\": main() ``` Performance Requirements - The solution should be efficient in terms of both time and space complexity. - Aim for minimal overhead in terms of network calls and file I/O operations.","solution":"import ftplib from typing import List def connect_ftp(host: str, user: str, passwd: str, use_tls: bool) -> ftplib.FTP: Connects to an FTP server using either TLS or plain FTP. :param host: FTP server address :param user: Username for FTP server :param passwd: Password for FTP server :param use_tls: Use TLS for connection if True, otherwise use plain FTP :return: Connected FTP instance if use_tls: ftp = ftplib.FTP_TLS(host, user, passwd) ftp.prot_p() # Secure data connection else: ftp = ftplib.FTP(host, user, passwd) return ftp def list_directory(ftp: ftplib.FTP, directory: str) -> List[str]: Lists files in the specified directory on the FTP server. :param ftp: Connected FTP instance :param directory: Directory to list files from :return: List of filenames in the specified directory ftp.cwd(directory) return ftp.nlst() def download_file(ftp: ftplib.FTP, remote_file_path: str, local_file_path: str) -> None: Downloads a file from the FTP server to local file system. :param ftp: Connected FTP instance :param remote_file_path: Path of the file on FTP server :param local_file_path: Path where the file will be saved locally with open(local_file_path, \'wb\') as local_file: ftp.retrbinary(f\\"RETR {remote_file_path}\\", local_file.write) def upload_file(ftp: ftplib.FTP, local_file_path: str, remote_file_path: str) -> None: Uploads a file from local file system to the FTP server. :param ftp: Connected FTP instance :param local_file_path: Path of the file on local system :param remote_file_path: Path where the file will be saved on FTP server with open(local_file_path, \'rb\') as local_file: ftp.storbinary(f\\"STOR {remote_file_path}\\", local_file)"},{"question":"# Seaborn Theme Customization Exercise **Objective:** Demonstrate your understanding of seaborn\'s theme customization functionality. **Problem Description:** You are given sales data of three products (Product A, Product B, and Product C) over a span of four months (January to April). Using seaborn, create bar plots to visualize this data. Your task is to implement a function that applies different seaborn themes and customizations to produce the required plots. **Function Signature:** ```python def customize_sales_plot(data: dict, theme_style: str, color_palette: str, remove_top_spine: bool) -> None: pass ``` **Input:** - `data` (dict): A dictionary containing sales data in the format: ```python { \\"Product\\":\\"A\\", \\"Sales\\":[10, 20, 30, 40], \\"Month\\":[\\"January\\", \\"February\\", \\"March\\", \\"April\\"] } ``` - `theme_style` (str): A string indicating the seaborn theme style to apply (e.g., \'whitegrid\', \'ticks\'). - `color_palette` (str): A string indicating the color palette for the plot (e.g., \'pastel\', \'dark\'). - `remove_top_spine` (bool): A boolean indicating whether to remove the top spine from the plot. **Output:** - The function should not return anything. It should display the customized bar plot using seaborn and matplotlib. **Requirements:** 1. Use `sns.set_theme` to apply the theme style and color palette. 2. Create a bar plot using seaborn to represent the sales data. 3. If `remove_top_spine` is True, remove the top spine from the plot using appropriate parameters. 4. Ensure the plot is customized accordingly and displayed properly. **Example:** ```python data = { \\"Product\\":\\"A\\", \\"Sales\\":[10, 20, 30, 40], \\"Month\\":[\\"January\\", \\"February\\", \\"March\\", \\"April\\"] } customize_sales_plot(data, \\"whitegrid\\", \\"pastel\\", True) ``` *Expected Output:* A bar plot of the given sales data with a whitegrid theme, pastel color palette, and top spine removed. **Constraints:** - You must use seaborn and matplotlib for this task. - The function should be modular and handle different themes and palettes as specified in the inputs. **Additional Notes:** - You can refer to the seaborn documentation on `set_theme` for more details on theme styles and color palettes: [Seaborn set_theme Documentation](https://seaborn.pydata.org/generated/seaborn.set_theme.html)","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customize_sales_plot(data: dict, theme_style: str, color_palette: str, remove_top_spine: bool) -> None: Function to customize seaborn bar plot based on the given sales data and customization parameters. Parameters: data (dict): A dictionary containing sales data. theme_style (str): A string indicating the seaborn theme style to apply. color_palette (str): A string indicating the color palette for the plot. remove_top_spine (bool): A boolean indicating whether to remove the top spine from the plot. # Set the seaborn theme and color palette sns.set_theme(style=theme_style, palette=color_palette) # Convert the dictionary into a pandas DataFrame df = pd.DataFrame(data) # Create the bar plot plot = sns.barplot(x=\'Month\', y=\'Sales\', data=df, hue=\'Product\') # Customize the spines if remove_top_spine: sns.despine(top=True) # Display the plot plt.show()"},{"question":"# Advanced Clustering and Evaluation with Scikit-Learn # Objective The objective of this task is to implement a clustering solution using `scikit-learn` and evaluate its performance using relevant metrics. You will work with the classic `Iris` dataset. # Task 1. **Load the Iris Dataset**: Use `sklearn.datasets.load_iris` to load the Iris dataset. 2. **Implement K-Means Clustering**: - Initialize a K-Means clustering model with `n_clusters=3`. - Fit the model to the Iris dataset. - Predict the cluster labels for the dataset. 3. **Evaluate Clustering**: Evaluate the performance of your clustering model using the following metrics: - Adjusted Rand Index - Silhouette Coefficient - Davies-Bouldin Index 4. **Print Results**: - Print the cluster centroids. - Print the Adjusted Rand Index, Silhouette Coefficient, and Davies-Bouldin Index. # Expected Input and Output Formats - **Input**: None (you will be using the Iris dataset directly from `sklearn.datasets`). - **Output**: - Cluster centroids: `(n_clusters, n_features)` shape array - Adjusted Rand Index: Float value - Silhouette Coefficient: Float value - Davies-Bouldin Index: Float value # Constraints - Use `KMeans` from `sklearn.cluster`. - Use `adjusted_rand_score`, `silhouette_score`, and `davies_bouldin_score` from `sklearn.metrics`. # Evaluation Criteria - Correctness: The implemented solution should correctly follow the steps outlined above. - Clarity: The code should be well-organized and easy to follow. - Performance: Ensure that the fitted model converges and provides reasonable cluster centroids and scores. # Sample Code Structure ```python from sklearn.datasets import load_iris from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score # Load Iris dataset data = load_iris() X, y = data.data, data.target # Initialize K-Means model kmeans = KMeans(n_clusters=3, random_state=42) # Fit the model kmeans.fit(X) # Predict the clusters labels = kmeans.predict(X) # Evaluate the clustering ari = adjusted_rand_score(y, labels) sil_score = silhouette_score(X, labels) dbi = davies_bouldin_score(X, labels) # Print the results print(\\"Cluster Centroids:\\") print(kmeans.cluster_centers_) print(\\"nAdjusted Rand Index:\\", ari) print(\\"Silhouette Coefficient:\\", sil_score) print(\\"Davies-Bouldin Index:\\", dbi) ``` **Note**: Make sure to handle any necessary imports and edge cases (e.g., model convergence issues).","solution":"from sklearn.datasets import load_iris from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score def cluster_and_evaluate_iris(): Load the Iris dataset, perform K-Means clustering, and evaluate the clustering performance. Returns: tuple: Cluster centroids, Adjusted Rand Index, Silhouette Coefficient, Davies-Bouldin Index # Load Iris dataset data = load_iris() X, y = data.data, data.target # Initialize K-Means model kmeans = KMeans(n_clusters=3, random_state=42) # Fit the model kmeans.fit(X) # Predict the clusters labels = kmeans.predict(X) # Evaluate the clustering ari = adjusted_rand_score(y, labels) sil_score = silhouette_score(X, labels) dbi = davies_bouldin_score(X, labels) # Print the results print(\\"Cluster Centroids:\\") print(kmeans.cluster_centers_) print(\\"nAdjusted Rand Index:\\", ari) print(\\"Silhouette Coefficient:\\", sil_score) print(\\"Davies-Bouldin Index:\\", dbi) return kmeans.cluster_centers_, ari, sil_score, dbi"},{"question":"**Question: Comprehensive Function Implementation in Python** You are tasked to create a function that performs several fundamental and advanced Python operations. The goal is to assess your understanding of Python\'s syntax, data model, and expression handling capabilities. # Problem Statement: Write a function called `process_data` that takes a list of integers as input and performs the following operations: 1. Filters out all negative numbers. 2. Computes the square of each remaining number. 3. Sums all the squared values. 4. Handles any possible exceptions that may occur during these operations, particularly focusing on common issues like type errors. 5. Uses a coroutine to yield each intermediate squared value before summing. # Requirements: - The function signature must be: `def process_data(data: List[int]) -> int`. - Use list comprehensions for filtering and squaring. - Use try-except blocks to handle exceptions. - Implement a coroutine that yields squared values. - Demonstrate the function by providing a sample input and output. # Example: ```python def process_data(data: List[int]) -> int: # Implement your solution here # Sample usage: sample_data = [1, -2, 3, 4] result = process_data(sample_data) # Output should be the sum of squares of non-negative numbers print(result) # Expected output: 1^2 + 3^2 + 4^2 = 1 + 9 + 16 = 26 ``` # Constraints: - The input list will have at most 100 elements. - Each element in the list will be an integer in the range [-1000, 1000]. - Performance requirements are minimal, given the constraints. # Notes: - Demonstrate proper use of Python\'s exception handling. - Ensure the coroutine is appropriately used and that the function behaves correctly in the presence of errors.","solution":"from typing import List def square_coroutine(): \'\'\'Coroutine that yields each squared value.\'\'\' try: while True: num = yield yield num ** 2 except GeneratorExit: print(\\"Coroutine has been closed.\\") def process_data(data: List[int]) -> int: def filter_negatives(data: List[int]) -> List[int]: return [num for num in data if num >= 0] try: filtered_data = filter_negatives(data) squaring = square_coroutine() next(squaring) # Initialize the coroutine squared_sum = 0 for number in filtered_data: squared_number = squaring.send(number) next(squaring) # Move to the next yield point squared_sum += squared_number squaring.close() return squared_sum except TypeError as e: print(f\\"An error occurred: {e}\\") return 0"},{"question":"**Problem Statement:** You are developing a file management system where users can tag their files with different labels. You need to implement a search functionality that allows users to find files based on patterns in their tags, using Unix shell-style wildcards. Implement a function named `search_files` that takes in two parameters: 1. `file_tags` (a dictionary where keys are filenames (strings) and values are lists of tags (strings)). 2. `pattern` (a string representing the search pattern). The function should return a list of filenames where at least one of their tags matches the given pattern. The pattern matching should be case-sensitive. **Function Signature:** ```python def search_files(file_tags: dict, pattern: str) -> list: pass ``` # Input: - `file_tags`: A dictionary with keys as filenames (strings) and values as lists of tags associated with each file (strings). (1 <= len(file_tags) <= 1000) - `pattern`: A string representing the Unix shell-style search pattern. (1 <= len(pattern) <= 20) # Output: - A list of filenames (strings) where at least one tag matches the given pattern. The list should be sorted in lexicographical order. # Example: ```python file_tags = { \\"file1.txt\\": [\\"report\\", \\"finance\\", \\"2022\\"], \\"file2.txt\\": [\\"holiday\\", \\"photos\\", \\"2022\\"], \\"file3.txt\\": [\\"notes\\", \\"meeting\\", \\"budget\\"], \\"file4.txt\\": [\\"holiday\\", \\"report\\", \\"travel\\"], } pattern = \\"re*\\" print(search_files(file_tags, pattern)) ``` **Output:** ```python [\\"file1.txt\\", \\"file4.txt\\"] ``` # Explanation: - `file1.txt` has the tag \\"report\\" which matches the pattern \\"re*\\". - `file4.txt` has the tag \\"report\\" which matches the pattern \\"re*\\". - `file2.txt` and `file3.txt` do not have any tags that match the pattern \\"re*\\". # Constraints: - All filenames and tags are non-empty strings containing only alphanumeric characters and possibly spaces. - The pattern will only include valid Unix shell-style wildcards as specified (`*`, `?`, `[seq]`, `[!seq]`). Use the functions provided by the `fnmatch` module to implement the solution efficiently. **Note:** - Ensure that the matching is case-sensitive. - Consider performance and clarity in your implementation.","solution":"import fnmatch def search_files(file_tags: dict, pattern: str) -> list: Returns a list of filenames where at least one tag matches the given pattern. :param file_tags: Dictionary with keys as filenames and values as lists of tags. :param pattern: String representing the search pattern. :return: List of filenames matching the pattern, sorted lexicographically. matching_files = [] for filename, tags in file_tags.items(): for tag in tags: if fnmatch.fnmatchcase(tag, pattern): matching_files.append(filename) break return sorted(matching_files)"},{"question":"**Problem Statement:** You are tasked with creating a custom calendar generator using Pythonâ€™s `calendar` module. The objective is to implement the `CustomCalendar` class which should extend the functionality provided by `calendar.TextCalendar`. The `CustomCalendar` class should be able to generate a formatted calendar for a given year and specify additional custom conditions. Requirements: 1. **Initialization**: - The class should be initialized with the starting weekday (using the integer convention where `0` is Monday and `6` is Sunday). - Use the `firstweekday` argument during initialization. 2. **Methods**: - `format_custom_year(theyear: int, include_leap_days: bool = False) -> str`: Returns a formatted string of the calendar for the entire year. - `include_leap_days`: If true, should mark the leap year days with a special indicator (e.g., append an asterisk `*` to the day number). - `day_occurrences(year: int, month: int, weekday: int) -> int`: Returns the number of times a specific weekday occurs in a given month. - For example, the number of Mondays in March 2022. Input: - `theyear`: An integer representing the year. - `year`: An integer representing the year. - `month`: An integer representing the month (1-12). - `weekday`: An integer representing the weekday (0-6 where 0 is Monday). Output: - `format_custom_year` should return a multi-line string representing the yearly calendar with adjustments for leap days if specified. - `day_occurrences` should return an integer count. Constraints: - The year must be between 1 and 9999. - The month must be between 1 and 12. - The weekday must be between 0 and 6. # Example Usage: ```python from your_module import CustomCalendar # Initialization custom_cal = CustomCalendar(firstweekday=0) # Generate calendar for the year 2022, including leap day indicators formatted_calendar = custom_cal.format_custom_year(2022, include_leap_days=True) print(formatted_calendar) # Get the number of Mondays in March 2022 monday_count = custom_cal.day_occurrences(2022, 3, 0) print(f\\"Number of Mondays in March 2022: {monday_count}\\") ``` # Notes: 1. The year format should be visually appealing as multiline text. 2. Ensure that the leap day indicator is easy to understand. 3. Properly handle edge cases like February in non-leap years. Implement the `CustomCalendar` class as specified above.","solution":"import calendar class CustomCalendar(calendar.TextCalendar): def __init__(self, firstweekday: int = 0): super().__init__(firstweekday) def format_custom_year(self, theyear: int, include_leap_days: bool = False) -> str: year_calendar = \\"\\" for month in range(1, 13): month_calendar = self.formatmonth(theyear, month) if include_leap_days and calendar.isleap(theyear) and month == 2: month_calendar = month_calendar.replace(\'29\', \'29*\') year_calendar += month_calendar + \\"n\\" return year_calendar.strip() def day_occurrences(self, year: int, month: int, weekday: int) -> int: month_calendar = self.monthdayscalendar(year, month) return sum(1 for week in month_calendar if week[weekday] != 0)"},{"question":"Objective Write a Python function that reads a list of file names from the command-line arguments, counts the number of lines in each file, and prints the results. Your code should handle file operations safely and avoid any warnings or errors that would be raised in Python Development Mode. Function Signature ```python def count_lines_in_files(filenames: list) -> None: ``` Input - `filenames`: A list of strings, where each string is a file name. Output - The function should print the count of lines for each file in the format: \\"`filename: number_of_lines`\\" Constraints 1. You should handle the following exceptions gracefully: - File not found. - Permission errors. 2. Ensure that all file resources are properly managed and closed explicitly. 3. Your code should not produce any `ResourceWarning` or `OSError` even when run with Python Development Mode enabled. 4. Utilize exception handling to log meaningful messages if errors occur while processing any file. Example ```python # Assuming files \\"file1.txt\\" and \\"file2.txt\\" exist, with file1.txt containing 3 lines and file2.txt containing 5 lines: count_lines_in_files([\\"file1.txt\\", \\"file2.txt\\"]) # Output: # file1.txt: 3 # file2.txt: 5 ``` Notes - Make sure to test your function with Python Development Mode enabled (`-X dev`) to ensure it does not produce any runtime warnings or errors.","solution":"def count_lines_in_files(filenames): Reads a list of file names, counts the number of lines in each file, and prints the results. :param filenames: List of file names. for filename in filenames: try: with open(filename, \'r\') as file: line_count = sum(1 for _ in file) print(f\\"{filename}: {line_count}\\") except FileNotFoundError: print(f\\"Error: File not found - {filename}\\") except PermissionError: print(f\\"Error: Permission denied - {filename}\\") except OSError as error: print(f\\"Error: An OS error occurred - {error}\\")"},{"question":"Objective: Implement a function that utilizes `torch.xpu` to perform a computational task involving device management, memory management, and random number generation. This function should demonstrate the ability to handle multiple components of the `torch.xpu` module effectively. Problem Statement: You are tasked with implementing a function `run_xpu_computation` that performs the following steps: 1. **Device Management:** Check if an XPU device is available. If it is, set the first available device as the active device. 2. **Memory Management:** Retrieve and print initial memory statistics before and after an operation to compute the difference in memory used. 3. **Random Number Generation:** Initialize a tensor with random numbers on the XPU device and print its initial state. Then, manually seed the RNG to ensure reproducibility and print the tensor again. 4. **Computation:** Perform a sample computation (e.g., tensor multiplication) on the tensor and output the results. Function Signature: ```python def run_xpu_computation(): pass ``` Expected Outputs: - Initial memory statistics. - Second memory statistics after the operation. - The difference in memory usage. - The tensor\'s initial state with randomly generated numbers. - The tensor\'s state after setting a manual seed. - The result of the sample computation. Constraints: - Avoid using any deprecated or externally dependent functions. - Ensure the code is efficient and concise. - Handle exceptions where the XPU device is not available gracefully, by printing an appropriate message and exiting the function. # Example: ```python # Note: The actual numbers will vary depending on the system and RNG state. run_xpu_computation() # Output: # XPU Device is available and set to device 0 # Initial memory stats: {...} # Memory stats after operation: {...} # Difference in memory usage: {...} # Initial random tensor: tensor([...], device=\'xpu:0\') # Random tensor after seeding: tensor([...], device=\'xpu:0\') # Computation result: tensor([...], device=\'xpu:0\') ``` Additional Notes: - Utilize the `torch.xpu` package for all necessary operations. - Students can refer to the official PyTorch documentation for detailed usage of each function if needed.","solution":"import torch def run_xpu_computation(): This function demonstrates device management, memory management, and random number generation using the torch.xpu module. if not torch.xpu.is_available(): print(\\"XPU device is not available.\\") return device = torch.device(\\"xpu\\") print(f\\"XPU Device is available and set to {device}\\") # Initial memory statistics initial_mem_stats = torch.xpu.memory_stats(device) print(f\\"Initial memory stats: {initial_mem_stats}\\") # Random number generation on XPU tensor = torch.rand((5, 5), device=device) print(f\\"Initial random tensor: {tensor}\\") # Manually seed the RNG torch.xpu.manual_seed(42) tensor_seeded = torch.rand((5, 5), device=device) print(f\\"Random tensor after seeding: {tensor_seeded}\\") # Perform a sample computation (tensor multiplication) result = tensor @ tensor_seeded.T print(f\\"Computation result: {result}\\") # Memory statistics after operation later_mem_stats = torch.xpu.memory_stats(device) print(f\\"Memory stats after operation: {later_mem_stats}\\") # Difference in memory usage difference_in_mem_usage = {k: later_mem_stats[k] - initial_mem_stats[k] for k in initial_mem_stats.keys()} print(f\\"Difference in memory usage: {difference_in_mem_usage}\\")"},{"question":"Objective Implement a function utilizing the `linecache` module to fetch multiple specific lines from a given source file. Problem Statement You are tasked with implementing a function `fetch_lines(filename: str, line_numbers: list) -> list` that fetches the specified lines from a file. This function should use the `linecache` module to retrieve the lines. After fetching the lines, it should return them as a list of strings. If any line number is invalid (i.e., out of range), the corresponding entry in the list should be an empty string. Input - `filename` (str): The path to the text file from which lines will be fetched. - `line_numbers` (list): A list of integers representing the line numbers to be fetched from the file. Output - A list of strings where each string corresponds to the content of the line requested. If a line number is invalid, the function should return an empty string for that entry. Example ```python filename = \\"sample.txt\\" line_numbers = [1, 3, 5] # Contents of sample.txt # ------------------------ # Line 1 # Line 2 # Line 3 # Line 4 # Line 5 # ------------------------ output = fetch_lines(filename, line_numbers) print(output) # Output: [\\"Line 1n\\", \\"Line 3n\\", \\"Line 5n\\"] ``` Constraints - Assume the file contains no more than 1000 lines. - Line numbers in the `line_numbers` list are positive integers. - Optimize the solution to minimize file I/O operations. Additional Functionality 1. Implement `clear_loaded_cache()` which uses the `linecache.clearcache()` to clear the cache. 2. Implement `update_cached_file(filename: str)` which uses `linecache.checkcache(filename)` to validate and update the cache for the given file. Example Usage ```python # Fetch lines from the file lines = fetch_lines(\\"sample.txt\\", [1, 5, 7]) # Clear the cache clear_loaded_cache() # Update cache for specific file update_cached_file(\\"sample.txt\\") ``` Notes - You must use the `linecache` module for line retrieval in `fetch_lines`. - Ensure that you handle errors gracefully, returning an empty string for invalid line numbers.","solution":"import linecache def fetch_lines(filename: str, line_numbers: list) -> list: Fetch specific lines from a given file using the linecache module. Parameters: filename (str): The path to the text file. line_numbers (list): List of line numbers to fetch. Returns: list: List of strings where each string corresponds to the content of the line requested. lines = [] for number in line_numbers: line = linecache.getline(filename, number) lines.append(line if line else \\"\\") return lines def clear_loaded_cache(): Clears the linecache cache. linecache.clearcache() def update_cached_file(filename: str): Validate and update the cache for a specific file. Parameters: filename (str): The path to the text file that needs cache validation. linecache.checkcache(filename)"},{"question":"# Timedelta DataFrame Manipulation **Objective:** Write a function that takes a DataFrame containing datetime strings and duration strings and computes the resulting times when the duration is added to the starting times. **Function Signature:** ```python def calculate_elapsed_times(df: pd.DataFrame, start_column: str, duration_column: str, result_column: str) -> pd.DataFrame: Compute the resulting times by adding durations to the start times in the DataFrame. Parameters: df (pd.DataFrame): A DataFrame containing at least two columns with datetime strings and duration strings. start_column (str): The name of the column in df that contains the start time datetime strings. duration_column (str): The name of the column in df that contains the duration strings. result_column (str): The name of the column where the resulting times will be stored. Returns: pd.DataFrame: The input DataFrame with an additional column containing the resulting times. pass ``` # Detailed Requirements: 1. **Inputs:** - `df`: A pandas DataFrame with at least two columns: - One column contains datetime strings in ISO 8601 format. - One column contains timedelta strings in various formats (e.g., \\"2 days\\", \\"1 days 05:10:00\\", etc.). - `start_column`: The name of the column with datetime strings. - `duration_column`: The name of the column with timedelta strings. - `result_column`: The name of the column in which the resulting times will be stored. 2. **Processing:** - Convert the datetime string column to pandas `datetime` objects. - Convert the duration string column to pandas `Timedelta` objects. - Compute the resulting times by adding the `Timedelta` objects to the `datetime` objects. - Add the resulting times as a new column in the DataFrame with the name specified by `result_column`. 3. **Output:** - A pandas DataFrame with the additional `result_column` containing the computed resulting times. 4. **Constraints & Assumptions:** - All duration strings will be valid and parsable by `pd.Timedelta`. - The DataFrame may contain missing or NaT values, which should be handled appropriately. **Example:** ```python import pandas as pd data = { \'start_time\': [\'2023-01-01 10:00:00\', \'2023-01-02 15:30:00\'], \'duration\': [\'1 days 02:30:00\', \'3 days\'] } df = pd.DataFrame(data) result_df = calculate_elapsed_times(df, \'start_time\', \'duration\', \'end_time\') print(result_df) ``` Expected output: ``` start_time duration end_time 0 2023-01-01 10:00:00 1 days 02:30:00 2023-01-02 12:30:00 1 2023-01-02 15:30:00 3 days 2023-01-05 15:30:00 ``` **Note:** You will need to handle the conversion of datetime and timedelta strings to their respective pandas representations using `pd.to_datetime` and `pd.to_timedelta`.","solution":"import pandas as pd def calculate_elapsed_times(df: pd.DataFrame, start_column: str, duration_column: str, result_column: str) -> pd.DataFrame: Compute the resulting times by adding durations to the start times in the DataFrame. Parameters: df (pd.DataFrame): A DataFrame containing at least two columns with datetime strings and duration strings. start_column (str): The name of the column in df that contains the start time datetime strings. duration_column (str): The name of the column in df that contains the duration strings. result_column (str): The name of the column where the resulting times will be stored. Returns: pd.DataFrame: The input DataFrame with an additional column containing the resulting times. # Convert start_column to datetime df[start_column] = pd.to_datetime(df[start_column]) # Convert duration_column to timedelta df[duration_column] = pd.to_timedelta(df[duration_column]) # Calculate resulting times df[result_column] = df[start_column] + df[duration_column] return df"},{"question":"# Objective: Implement a custom descriptor class that enforces specific data validation rules on a class attribute. The descriptor should manage the attribute access, ensuring the data meets certain type and range restrictions before setting it. # Task: 1. Implement a class `RangeBoundedInteger` which acts as a descriptor: - It should restrict the attribute to integer values within a specified range. - The class should raise appropriate exceptions if data being set does not meet the required constraints. 2. Implement another class `Person` which utilizes the `RangeBoundedInteger` descriptor for an attribute `age`: - The `age` attribute must be an integer between 0 and 120 inclusive. - The `Person` class should also have a `name` attribute which is a string. # Class Specifications: `RangeBoundedInteger` - **__init__(self, min_value, max_value):** - Initializes the descriptor with minimum and maximum values. - **__set_name__(self, owner, name):** - Sets the name of the attribute the descriptor is managing. - **__get__(self, instance, owner):** - Retrieves the attribute value from the instance. - **__set__(self, instance, value):** - Sets the attribute value on the instance if it satisfies the validation checks. `Person` - **__init__(self, name, age):** - Initializes the `Person` object with `name` and `age`. - **name:** - A string representing the name of the person. - **age:** - Uses the `RangeBoundedInteger` descriptor, restricted to be between 0 and 120. # Input and Output: - **Input:** Input consists of creating instances of the `Person` class. - **Output:** If correctly implemented, creating instances should raise no errors unless the validation fails. # Constraints: - The `age` must be an integer between 0 and 120 inclusive. - The `name` must be a string. # Example: ```python # Example Usage try: p1 = Person(\\"Alice\\", 30) # Valid initialization p2 = Person(\\"Bob\\", -5) # Should raise ValueError: \\"Age must be between 0 and 120\\" p3 = Person(\\"Charlie\\", 150) # Should raise ValueError: \\"Age must be between 0 and 120\\" p4 = Person(\\"Dave\\", \\"Forty\\") # Should raise TypeError: \\"Age must be an integer\\" except Exception as e: print(e) ``` Expected Output: ``` ValueError: Age must be between 0 and 120 ValueError: Age must be between 0 and 120 TypeError: Age must be an integer ``` # Implementation Notes: - Ensure proper handling of edge cases, such as negative numbers or non-integer values. - Use informative exception messages to help with debugging.","solution":"class RangeBoundedInteger: def __init__(self, min_value, max_value): self.min_value = min_value self.max_value = max_value self.attribute_name = None def __set_name__(self, owner, name): self.attribute_name = name def __get__(self, instance, owner): return instance.__dict__.get(self.attribute_name) def __set__(self, instance, value): if not isinstance(value, int): raise TypeError(f\\"{self.attribute_name.capitalize()} must be an integer\\") if not (self.min_value <= value <= self.max_value): raise ValueError(f\\"{self.attribute_name.capitalize()} must be between {self.min_value} and {self.max_value}\\") instance.__dict__[self.attribute_name] = value class Person: age = RangeBoundedInteger(0, 120) def __init__(self, name, age): self.name = name self.age = age"},{"question":"**Embedding a Custom Function in Python from C** You are given segments of code for embedding the Python interpreter within a C application. Your task is to extend the given embedded Python interpreter with a custom functionality of computing the factorial of a number and making this functionality available to Python scripts executed within the embedded interpreter. # Requirements: 1. Extend the embedded Python interpreter with a function named `c_factorial` that calculates the factorial of a given integer. 2. Ensure that Python code can call `c_factorial` function and receive the correct result. 3. Provide proper error handling for invalid inputs (non-integer or negative values). 4. The implementation should include comments explaining key sections of the code. # Expected Inputs and Outputs: - The function `c_factorial` takes a non-negative integer argument and returns its factorial. - If the input is not a non-negative integer, the function should raise a `ValueError`. # Constraints: - The solution should handle reasonably large inputs (e.g., up to 20!). - Focus on demonstrating the interaction between C and Python, including error handling and memory management. # Example: Given the following Python script to be executed within the embedded interpreter: ```python import emb print(\\"Factorial of 5:\\", emb.c_factorial(5)) print(\\"Factorial of 10:\\", emb.c_factorial(10)) ``` The expected output should be: ``` Factorial of 5: 120 Factorial of 10: 3628800 ``` # Complete the Implementation: Here is a template to get you started. You need to fill in the missing parts to achieve the desired functionality: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> // Custom C function to compute factorial static PyObject* c_factorial(PyObject *self, PyObject *args) { int n, i; long long result = 1; // Parse the input tuple if (!PyArg_ParseTuple(args, \\"i\\", &n)) { return NULL; // Error in parsing argument } // Handle invalid input if (n < 0) { PyErr_SetString(PyExc_ValueError, \\"Input must be a non-negative integer\\"); return NULL; } for (i = 2; i <= n; ++i) { result *= i; } return PyLong_FromLongLong(result); } // Method definition object static PyMethodDef EmbMethods[] = { {\\"c_factorial\\", c_factorial, METH_VARARGS, \\"Compute factorial of a non-negative integer\\"}, {NULL, NULL, 0, NULL} }; // Module definition static PyModuleDef EmbModule = { PyModuleDef_HEAD_INIT, \\"emb\\", NULL, -1, EmbMethods, NULL, NULL, NULL, NULL }; // Initialize the module PyMODINIT_FUNC PyInit_emb(void) { return PyModule_Create(&EmbModule); } int main(int argc, char *argv[]) { numargs = argc; PyImport_AppendInittab(\\"emb\\", &PyInit_emb); // Initialize the Python interpreter Py_Initialize(); // Your code to execute a Python script if (Py_FinalizeEx() < 0) { return 120; } return 0; } ``` In the code template: 1. Complete the `main` function to execute a Python script that calls the `c_factorial` function. 2. Make sure to handle the initialization and cleanup properly. Submit your complete C program along with a small Python script to test its functionality.","solution":"def factorial(n): Compute the factorial of a given integer. Raise ValueError for non positive integers or non-integers. if not isinstance(n, int) or n < 0: raise ValueError(\\"Input must be a non-negative integer.\\") result = 1 for i in range(2, n + 1): result *= i return result"},{"question":"**Advanced Coding Assessment Question** # Objective: Implement a class and register custom pickling functions for this class using the `copyreg` module. The objective is to test your understanding of custom serialization with the `pickle` module and utilizing the `copyreg` module for registering reduction functions. # Problem Statement: You are required to create a class `CustomClass` that has two attributes: `name` (a string) and `values` (a list of integers). Additionally, you need to implement custom pickling support for this class. Specifically, you should: 1. Define a class `CustomClass` with the following properties: - An `__init__` method that initializes the `name` and `values` attributes. - A `__repr__` method that returns a string representation of the object in the format: `CustomClass(name, values)`. 2. Write a function `pickle_custom_class` that takes an instance of `CustomClass` and returns the necessary information to recreate the object. 3. Register the `pickle_custom_class` function with the `copyreg` module to ensure instances of `CustomClass` can be properly pickled and unpickled. 4. Demonstrate that your implementation works by: - Creating an instance of `CustomClass`. - Pickling and unpickling the instance. - Showing that the unpickled instance retains the same attribute values as the original. # Input: - The `name` attribute is a non-empty string. - The `values` attribute is a list of integers with at least one element. # Output: - An unpickled instance of `CustomClass` that has the same attributes as the original instance. # Constraints: - You must use the `copyreg` module to register the pickling functions. - Your solution should handle any valid `name` and `values` for the `CustomClass`. # Example Usage: ```python import copyreg import pickle # Define your class and functions here # Test the implementation original = CustomClass(\'example\', [1, 2, 3, 4, 5]) pickled_data = pickle.dumps(original) unpickled_instance = pickle.loads(pickled_data) print(original) # Output: CustomClass(example, [1, 2, 3, 4, 5]) print(unpickled_instance) # Output: CustomClass(example, [1, 2, 3, 4, 5]) assert original.name == unpickled_instance.name assert original.values == unpickled_instance.values ``` # Notes: Ensure to handle any exceptions appropriately and include any necessary import statements. Your implementation should be efficient and follow best practices for writing readable and maintainable Python code.","solution":"import copyreg import pickle class CustomClass: def __init__(self, name, values): self.name = name self.values = values def __repr__(self): return f\'CustomClass({self.name}, {self.values})\' def pickle_custom_class(obj): return CustomClass, (obj.name, obj.values) # Register the custom pickling function copyreg.pickle(CustomClass, pickle_custom_class)"},{"question":"You are provided with a module `stringprep` that exposes several functions related to the preparation of Unicode strings for internet protocols. Your task is to implement a function that processes an input string according to a specific profile defined by various tables from the `stringprep` module. Function Signature ```python def process_string(input_str: str) -> str: pass ``` Input - `input_str`: A Unicode string representing the input to be processed. Output - Returns the processed string. Constraints 1. Any character in `input_str` that is found in `stringprep.in_table_b1` should be removed. 2. Any character that is found in `stringprep.in_table_c11_c12` should not be part of the output string. 3. Any character that is found in `stringprep.in_table_c21_c22` should not be part of the output string. 4. Characters in `input_str` should be case folded using the mapping provided by `stringprep.map_table_b2` where applicable. Example ```python # Assume the following fictional scenarios for illustration: # - stringprep.in_table_b1(\'a\') == True # - stringprep.in_table_b1(\'b\') == False # - stringprep.in_table_c11_c12(\' \') == True # - stringprep.in_table_c21_c22(\'n\') == True # - stringprep.map_table_b2(\'A\') == \'a\' # - stringprep.map_table_b2(\'B\') == \'b\' input_str = \\"A Bnb\\" output = process_string(input_str) print(output) # Expected output should be \\"ab\\" ``` Explanation 1. The input string \\"A Bnb\\" contains characters \'A\', \' \', \'n\', and \'b\'. 2. \'A\' is in the original string and will be case folded to \'a\' using `stringprep.map_table_b2`. 3. The space character \' \' should be removed as it appears in `stringprep.in_table_c11_c12`. 4. The newline character \'n\' should be removed as it appears in `stringprep.in_table_c21_c22`. 5. \'b\' remains unchanged and is appended to the resultant string. Implement the `process_string` function to achieve the described behavior.","solution":"import stringprep def process_string(input_str: str) -> str: result = [] for char in input_str: if stringprep.in_table_b1(char): continue if stringprep.in_table_c11_c12(char) or stringprep.in_table_c21_c22(char): continue if char in stringprep.map_table_b2: char = stringprep.map_table_b2[char] result.append(char) return \'\'.join(result)"},{"question":"# Custom DataLoader Implementation Objective The goal of this exercise is to assess your understanding of PyTorch\'s data loading utilities, specifically focusing on creating custom datasets, managing multi-process data loading, implementing custom batching, and utilizing memory pinning. Problem Statement You are required to implement a custom PyTorch Dataset and DataLoader to handle a large dataset of images and their corresponding labels. The dataset is too large to fit into memory and should be loaded dynamically from the disk. The specific requirements are: 1. **Custom Dataset Implementation**: - Implement a custom dataset class `LargeImageDataset` that inherits from `torch.utils.data.Dataset`. - The dataset should load images from a directory, where each image is stored with a corresponding label in a .txt file. - The `__getitem__` method should return a tuple `(image, label)`. 2. **Multi-process Data Loading**: - Implement a DataLoader that can utilize multiple worker processes to load data efficiently. - Ensure that each worker process is initialized correctly to avoid duplicating data loading. 3. **Custom Batching**: - Implement a custom `collate_fn` that can handle dynamic padding of the images to the maximum size in the batch. - Your `collate_fn` should stack the images along a new batch dimension and do the same for the labels. 4. **Memory Pinning**: - Enable memory pinning in your DataLoader to facilitate faster loading to CUDA-enabled GPUs. Input and Output Formats - **Input**: - A string `image_dir` representing the path to the directory containing image files. - A string `label_file` representing the path to the .txt file containing image labels. - An integer `batch_size` representing the number of samples per batch. - A boolean `use_multi_process` indicating whether to use multi-process data loading. - **Output**: - An instance of `torch.utils.data.DataLoader` initialized with the specified configurations. Constraints - Assume the images are stored in a subdirectory called \\"images\\" within `image_dir`. - Labels are stored in a .txt file called \\"labels.txt\\" within `label_file`, with each line containing the image filename followed by the label. - The number of worker processes should be set to 4 if `use_multi_process` is `True`, otherwise it should be 0. Performance Requirements - Ensure efficient data loading by properly managing multi-process data loading. - Utilize memory pinning to speed up the data transfer to CUDA-enabled GPUs. Example Usage ```python image_dir = \\"/path/to/image_directory\\" label_file = \\"/path/to/labels.txt\\" batch_size = 32 use_multi_process = True # Custom DataLoader data_loader = create_custom_dataloader(image_dir, label_file, batch_size, use_multi_process) for batch in data_loader: images, labels = batch # Process the batch ``` Solution Template You need to complete the following class and function. ```python import torch from torch.utils.data import Dataset, DataLoader from PIL import Image import os class LargeImageDataset(Dataset): def __init__(self, image_dir, label_file): # Implement the constructor pass def __len__(self): # Implement the length method pass def __getitem__(self, idx): # Implement the get item method pass def collate_fn(batch): # Implement custom collate function to handle dynamic padding pass def create_custom_dataloader(image_dir, label_file, batch_size, use_multi_process): dataset = LargeImageDataset(image_dir, label_file) num_workers = 4 if use_multi_process else 0 data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn, pin_memory=True) return data_loader ``` Fill out the skeleton with the correct implementation for `LargeImageDataset` and `collate_fn`.","solution":"import torch from torch.utils.data import Dataset, DataLoader from PIL import Image import os import numpy as np class LargeImageDataset(Dataset): def __init__(self, image_dir, label_file): self.image_dir = image_dir with open(label_file, \'r\') as file: self.labels = [line.strip().split() for line in file] def __len__(self): return len(self.labels) def __getitem__(self, idx): img_name, label = self.labels[idx] img_path = os.path.join(self.image_dir, img_name) image = Image.open(img_path).convert(\'RGB\') image = np.array(image) # Convert HWC format image to numpy array label = int(label) return image, label def collate_fn(batch): images, labels = zip(*batch) max_width = max(img.shape[1] for img in images) max_height = max(img.shape[0] for img in images) padded_images = [] for img in images: padded_img = np.zeros((max_height, max_width, 3), dtype=np.uint8) padded_img[:img.shape[0], :img.shape[1], :] = img padded_images.append(padded_img) padded_images = np.stack(padded_images, axis=0) labels = torch.tensor(labels, dtype=torch.int64) images_tensor = torch.tensor(padded_images, dtype=torch.float32).permute(0, 3, 1, 2) # NCHW format return images_tensor, labels def create_custom_dataloader(image_dir, label_file, batch_size, use_multi_process): dataset = LargeImageDataset(image_dir, label_file) num_workers = 4 if use_multi_process else 0 data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn, pin_memory=True) return data_loader"},{"question":"Coding Assessment Question Implement a custom PyTorch `Function` that performs matrix multiplication followed by element-wise exponentiation. To verify the correctness of backward gradients, use PyTorch\'s `gradcheck` utility. # Requirements: 1. Implement a custom autograd function called `MatrixExpMultiply` that: - Takes two input matrices. - Performs standard matrix multiplication. - Applies the element-wise exponential function to the result. 2. Ensure that the function correctly implements both the `forward` and `backward` methods. 3. Test the implementation using `torch.autograd.gradcheck` to ensure gradients are correctly computed. # Constraints: 1. The input matrices will have dimensions where matrix multiplication is feasible. 2. Inputs may require gradients. # Input: - Two 2-D tensors `A` and `B` of appropriate sizes for matrix multiplication. # Output: - A single 2-D tensor as the result of the matrix multiplication followed by the element-wise exponentiation. # Example: ```python import torch from torch.autograd import Function import torch.autograd as autograd # Define the custom Function class MatrixExpMultiply(Function): @staticmethod def forward(ctx, A, B): # Perform matrix multiplication output = torch.matmul(A, B) # Save tensors for backward pass ctx.save_for_backward(A, B, output) # Apply element-wise exponential function return torch.exp(output) @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors A, B, output = ctx.saved_tensors # Compute gradients with respect to inputs grad_A = grad_output * torch.exp(output) @ B.t() grad_B = A.t() @ (grad_output * torch.exp(output)) return grad_A, grad_B # Define input tensors with requires_grad=True A = torch.randn((4, 3), requires_grad=True, dtype=torch.double) B = torch.randn((3, 5), requires_grad=True, dtype=torch.double) # Use gradcheck to verify the gradients input = (A, B) test = autograd.gradcheck(MatrixExpMultiply.apply, input) print(\\"Gradcheck passed: \\", test) ``` # Note: 1. Make sure to set `requires_grad=True` for input tensors to verify gradient computation. 2. Use double precision (`dtype=torch.double`) for accurate gradient verification in `gradcheck`.","solution":"import torch from torch.autograd import Function class MatrixExpMultiply(Function): @staticmethod def forward(ctx, A, B): # Perform matrix multiplication output = torch.matmul(A, B) # Save tensors for backward pass ctx.save_for_backward(A, B, output) # Apply element-wise exponential function return torch.exp(output) @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors A, B, output = ctx.saved_tensors # Compute gradients with respect to inputs exp_output = torch.exp(output) grad_A = grad_output * exp_output @ B.t() grad_B = A.t() @ (grad_output * exp_output) return grad_A, grad_B"},{"question":"You are tasked with refactoring some legacy code to be compliant with pandas\' Copy-on-Write (CoW) principles. The legacy code uses chained assignments and inplace modifications which are no longer effective under pandas 3.0. # Problem Statement: You are given a pandas DataFrame `df` and a NumPy array `arr`. Your task is to: 1. Update the DataFrame `df` by setting the column `\'a\'` to 999 where the values in column `\'b\'` are greater than 5. 2. Convert column `\'b\'` into a NumPy array and modify its elements. Ensure that this modification does not affect the original DataFrame. # Input: - A pandas DataFrame `df` with at least columns `\'a\'` and `\'b\'`. - A NumPy array `arr` with same length as DataFrame `df`. # Output: - The modified DataFrame `df`. - The modified NumPy array `arr`. # Constraints: - You must avoid chained assignments due to CoW limitations. - The modified DataFrame should not share data with the original DataFrame once modified. - The NumPy array derived from column `\'b\'` should be writable and any modifications to it should not affect the DataFrame `df`. # Implementation Example: Your implementation should handle the following example: ```python import pandas as pd import numpy as np # Sample DataFrame data = {\'a\': [1, 2, 3, 4, 5], \'b\': [2, 5, 7, 10, 1]} df = pd.DataFrame(data) # Sample NumPy array arr = np.array([10, 20, 30, 40, 50]) # Your function to implement def refactor_code(df, arr): # Step 1: Update column \'a\' df.loc[df[\\"b\\"] > 5, \\"a\\"] = 999 # Step 2: Convert column \'b\' into a writable NumPy array and modify it b_array = df[\\"b\\"].to_numpy().copy() b_array[0] = 100 return df, b_array # Output Example modified_df, modified_arr = refactor_code(df, arr) print(modified_df) print(modified_arr) ``` Expected Output for the example input: ``` a b 0 1 2 1 2 5 2 999 7 3 999 10 4 5 1 [100 5 7 10 1] ``` Ensure that the DataFrame and NumPy array modifications are correctly performed according to the given instructions and constraints.","solution":"import pandas as pd import numpy as np def refactor_code(df, arr): Refactor the provided DataFrame and NumPy array as per the specification. Parameters: df (pandas.DataFrame): DataFrame with columns \'a\' and \'b\'. arr (numpy.ndarray): NumPy array of same length as df. Returns: pandas.DataFrame: Modified DataFrame. numpy.ndarray: Modified NumPy array derived from column \'b\' of the DataFrame. # Step 1: Update column \'a\' where \'b\' > 5 df = df.copy() # Make a copy to avoid potential CoW issues df.loc[df[\\"b\\"] > 5, \\"a\\"] = 999 # Step 2: Convert column \'b\' into a writable NumPy array and modify it b_array = df[\\"b\\"].to_numpy().copy() # Generate a writable array with copy() b_array[0] = 100 # Modify the elements of the array return df, b_array"},{"question":"# Regular Expressions in Python: Capturing and Substitution Objective: Create a function `find_and_replace_dates` that identifies dates in a given text and replaces them with a standardized format. Specifically, we are looking for dates in the formats `DD-MM-YYYY`, `DD/MM/YYYY`, or `DD.MM.YYYY` and converting them to `YYYY-MM-DD`. Input: - A string `text` which contains the text with dates. Output: - A modified string where all dates are replaced with the `YYYY-MM-DD` format. Constraints: - Ensure the day (DD) is between 01 and 31, month (MM) is between 01 and 12, and year (YYYY) is between 1000 and 2999. - The function should handle multiple dates in the text and ensure the surrounding text stays the same. - Dates that don\'t match the expected pattern should remain unchanged. Examples: ```python text = \\"Today\'s date is 15-08-2023. Another date is 02/05/2020. But 31.13.2023 is invalid.\\" output = find_and_replace_dates(text) print(output) # Output: \\"Today\'s date is 2023-08-15. Another date is 2020-05-02. But 31.13.2023 is invalid.\\" ``` Function Signature: ```python def find_and_replace_dates(text: str) -> str: pass ``` # Guidelines: 1. Use regular expressions to identify date patterns in the text. 2. Use capturing groups to extract the day, month, and year from the matched dates. 3. Perform the replacement in the desired format, ensuring only valid dates are transformed. Hints: - Utilize raw string notation for the regular expressions to avoid issues with backslashes. - Consider using the `re.sub` function with a custom replacement function.","solution":"import re def find_and_replace_dates(text: str) -> str: Identifies dates in formats DD-MM-YYYY, DD/MM/YYYY, or DD.MM.YYYY and replaces them with the YYYY-MM-DD format. Args: text (str): The input text containing dates. Returns: str: Modified text with dates in the standardized YYYY-MM-DD format. # Define the regex pattern to find dates date_pattern = re.compile(r\'b(d{2})[-/.](d{2})[-/.](d{4})b\') def replacer(match): Replaces the matched date with the YYYY-MM-DD format. Args: match (re.Match): The match object containing the day, month, and year. Returns: str: The date in YYYY-MM-DD format if valid, otherwise the original string. day, month, year = match.groups() if 1 <= int(day) <= 31 and 1 <= int(month) <= 12 and 1000 <= int(year) <= 2999: return f\\"{year}-{month}-{day}\\" else: return match.group(0) return date_pattern.sub(replacer, text)"},{"question":"Objective You are required to analyze a dataset of company sales using pandas and demonstrate your skills in data grouping, aggregation, and transformation using the `GroupBy` methods. Description You have been provided with a dataset containing the following columns: - `Company`: The name of the company. - `Product`: The name of the product sold. - `Sales`: The sales value of the product. - `Date`: The date of the sale. Your task is to write a function `analyze_sales` that takes a pandas DataFrame as input and performs the following operations: 1. **Group by Company**: Compute the total sales, mean sales, and the number of unique products sold per company. 2. **Identify Best-Selling Product per Company**: - Group by both `Company` and `Product` to find the product with the maximum sum of sales for each company. 3. **Compute Monthly Sales Trends**: - Extract the month from the `Date` column and aggregate the total sales per month across all companies. 4. **Calculate Sales Difference**: - Calculate the month-over-month (MoM) percentage change in total sales for each company. Function Signature ```python import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - A pandas DataFrame `df` with columns `Company`, `Product`, `Sales`, and `Date`. Output - A DataFrame with the following columns and the calculations as specified: - `Company`: Company name. - `Total_Sales`: Total sales per company. - `Mean_Sales`: Mean sales per company. - `Unique_Products`: Number of unique products sold per company. - `Best_Selling_Product`: The product with the highest total sales for each company. - `Best_Selling_Product_Sales`: The total sales of the best-selling product. - `Month`: The month of the year (format: YYYY-MM). - `Monthly_Sales`: Total sales for each month across all companies. - `MoM_Percentage_Change`: Month-over-month percentage change in sales for each company. Example ```python data = { \'Company\': [\'A\', \'A\', \'A\', \'B\', \'B\', \'A\', \'B\'], \'Product\': [\'X\', \'Y\', \'Z\', \'X\', \'Y\', \'X\', \'Z\'], \'Sales\': [100, 200, 150, 250, 300, 400, 350], \'Date\': [ \'2023-01-10\', \'2023-01-15\', \'2023-02-10\', \'2023-01-12\', \'2023-02-20\', \'2023-03-01\', \'2023-03-15\' ] } df = pd.DataFrame(data) result = analyze_sales(df) print(result) ``` Constraints - You may assume all dates are valid and in the format `YYYY-MM-DD`. - `Sales` values are non-negative integers.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: # Ensure that the \'Date\' column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Extract the month from the \'Date\' column df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') # 1. Group by Company company_group = df.groupby(\'Company\').agg( Total_Sales = (\'Sales\', \'sum\'), Mean_Sales = (\'Sales\', \'mean\'), Unique_Products = (\'Product\', \'nunique\') ).reset_index() # 2. Identify the best-selling product per company company_product_group = df.groupby([\'Company\', \'Product\'])[\'Sales\'].sum().reset_index() idx = company_product_group.groupby(\'Company\')[\'Sales\'].idxmax() best_selling_product = company_product_group.loc[idx].reset_index(drop=True) company_group = company_group.merge( best_selling_product.rename(columns={\'Sales\': \'Best_Selling_Product_Sales\'}), on=\'Company\' ) # 3. Compute Monthly Sales Trends monthly_sales = df.groupby(\'Month\')[\'Sales\'].sum().reset_index().rename(columns={\'Sales\': \'Monthly_Sales\'}) # 4. Calculate Month-over-Month percentage change in sales for each company monthly_sales_company = df.groupby([\'Company\', \'Month\'])[\'Sales\'].sum().reset_index() monthly_sales_company[\'MoM_Percentage_Change\'] = monthly_sales_company.groupby(\'Company\')[\'Sales\'].pct_change() * 100 # Merge all the results into a final DataFrame final_result = company_group.merge( monthly_sales_company, on=\'Company\', how=\'left\' ) return final_result"},{"question":"# Custom Autograd Function and Saving Tensors PyTorch allows for the creation of custom autograd functions using `torch.autograd.Function`. This is particularly useful when defining operations that are not present in PyTorch\'s standard library or when one needs fine-grained control over the forward and backward passes. In this task, you will implement a custom autograd function `CustomSin` that computes the sine of a tensor and requires saving intermediate results for the backward pass. Implement the following steps: 1. Define a custom autograd function `CustomSin` that: - Computes the sine of the input during the forward pass. - Saves the input tensor for use in the backward pass. - During the backward pass, it should use the saved input tensor to compute the gradient according to the derivative of the sine function. 2. Use the `CustomSin` function in a small neural network and perform a forward and backward pass to ensure the custom autograd function works correctly. Expected Input and Output - Input to the `CustomSin` function: A 1-D tensor with `requires_grad=True`. - Output of the `CustomSin` function: A tensor containing the sine of each element of the input tensor. Constraints 1. You should not use `torch.sin` or any high-level PyTorch API directly inside the `CustomSin` function for the forward computation. 2. You should use `save_for_backward` to save tensors that are needed in the backward pass. 3. Ensure to handle gradients correctly in the backward pass. Example ```python import torch from torch.autograd import Function class CustomSin(Function): @staticmethod def forward(ctx, input): # Save input tensor for backward pass ctx.save_for_backward(input) # Compute the forward pass return input.sin() @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor input, = ctx.saved_tensors # Compute gradient of the input tensor grad_input = grad_output * input.cos() return grad_input # Example usage input_tensor = torch.tensor([0.0, 1.0, 2.0], requires_grad=True) custom_sin = CustomSin.apply output = custom_sin(input_tensor) print(output) # Perform backward pass output.sum().backward() print(input_tensor.grad) ``` Task 1. Implement the `CustomSin` class as described above. 2. Verify your implementation by using it in a simple example and perform both forward and backward passes. 3. Print the output of the forward pass and the gradient from the backward pass for verification.","solution":"import torch from torch.autograd import Function class CustomSin(Function): @staticmethod def forward(ctx, input): # Save the input tensor for the backward pass ctx.save_for_backward(input) # Compute the sine of the input tensor output = input.clone() for i in range(input.nelement()): output[i] = (output[i].sin()).item() return output @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient of the input using the cosine grad_input = grad_output.clone() for i in range(input.nelement()): grad_input[i] = (grad_output[i] * input[i].cos()).item() return grad_input # Example usage input_tensor = torch.tensor([0.0, 1.0, 2.0], requires_grad=True) custom_sin = CustomSin.apply output = custom_sin(input_tensor) print(\\"Forward pass output:\\", output) # Perform backward pass output.sum().backward() print(\\"Gradient after backward pass:\\", input_tensor.grad)"},{"question":"# Custom SVM Implementation for Multi-Class Classification with a Custom Kernel **Objective**: Implement a support vector machine for multi-class classification using a custom kernel function in scikit-learn. Train the model on a dataset and evaluate its performance. **Instructions**: 1. **Custom Kernel Function**: - Define a custom kernel function `custom_kernel(X, Y)` that computes the polynomial kernel: ( (X cdot Y^T + 1)^2 ). 2. **Dataset**: - Use the Iris dataset from `sklearn.datasets`. - The dataset can be loaded using `datasets.load_iris()`. 3. **Model Training**: - Train the `SVC` model using the custom kernel function. - Perform a train-test split with 80% of the data for training and 20% for testing. - Use the default parameters for `SVC`, except for the `kernel` parameter, which should be set to the custom kernel function. 4. **Evaluation**: - Calculate and print the accuracy of the model on the test set. **Function Signature**: ```python def custom_kernel(X, Y): Computes the polynomial kernel function: (X Â· Y^T + 1)^2. Args: X (numpy.ndarray): Training data of shape (n_samples_X, n_features). Y (numpy.ndarray): Test data of shape (n_samples_Y, n_features). Returns: numpy.ndarray: Kernel matrix of shape (n_samples_X, n_samples_Y). pass def train_and_evaluate_svm_with_custom_kernel(): Loads the Iris dataset, trains an SVM with the custom kernel, and evaluates its accuracy. Prints the accuracy on the test set. pass ``` **Expected Input and Output**: - The function `train_and_evaluate_svm_with_custom_kernel` should not take any input arguments. - The output should be a printed accuracy value representing the performance of the classifier on the test set. **Constraints**: - Use the polynomial degree of 2 with a bias term of 1 as described for the custom kernel function. Example: ``` Kernel matrix shape: (120, 30) Classification accuracy: 0.9 ``` **Notes**: - Ensure the custom kernel function is compatible with the shape requirements for `SVC`. - Use appropriate scaling methods if necessary to improve model performance. **Grading Criteria**: - Correct implementation of the custom kernel function. - Successful training and evaluation of the `SVC` model using the custom kernel. - Accuracy calculation and proper handling of the dataset. - Code readability and appropriate use of scikit-learn functionalities.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def custom_kernel(X, Y): Computes the polynomial kernel function: (X Â· Y^T + 1)^2. Args: X (numpy.ndarray): Training data of shape (n_samples_X, n_features). Y (numpy.ndarray): Test data of shape (n_samples_Y, n_features). Returns: numpy.ndarray: Kernel matrix of shape (n_samples_X, n_samples_Y). return np.power(np.dot(X, Y.T) + 1, 2) def train_and_evaluate_svm_with_custom_kernel(): Loads the Iris dataset, trains an SVM with the custom kernel, and evaluates its accuracy. Prints the accuracy on the test set. # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the SVM model with the custom kernel model = SVC(kernel=custom_kernel) # Train the model model.fit(X_train, y_train) # Make predictions on the test set y_pred = model.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Classification accuracy: {accuracy}\\") # Return the accuracy for unit tests to verify return accuracy"},{"question":"**Problem Statement:** You are to implement a Python function that takes a list of text files, compresses each file\'s content using the zlib library, and then writes the compressed data to new files. Additionally, you will implement another function to decompress these files back to their original content. **Requirements:** 1. Implement the function `compress_files(file_list: List[str], output_dir: str, level: int = -1) -> None` that: - Takes in a list of paths to text files (`file_list`), the directory path where the compressed files should be stored (`output_dir`), and an optional compression level (`level`). - Reads the content of each file and compresses it using the `zlib.compress` method. - Writes the compressed data to new files in the specified output directory. The new files should have the same name as the original files but with a `.zlib` extension added. 2. Implement the function `decompress_files(file_list: List[str], output_dir: str) -> None` that: - Takes in a list of paths to compressed files (`file_list`) and the directory path where the decompressed files should be stored (`output_dir`). - Reads the compressed data from each file and decompresses it using the `zlib.decompress` method. - Writes the decompressed data to new files in the specified output directory. The decompressed files should have the same name as the original files without the `.zlib` extension. **Constraints:** - Assume that file content is small enough to fit into memory. - If any error occurs during compression or decompression, the function should print an appropriate error message and continue processing the next file. - The `output_dir` must exist and be writable. **Example Usage:** ```python file_list = [\\"file1.txt\\", \\"file2.txt\\"] output_dir = \\"compressed_files\\" compress_files(file_list, output_dir, level=6) compressed_file_list = [\\"compressed_files/file1.txt.zlib\\", \\"compressed_files/file2.txt.zlib\\"] output_dir = \\"decompressed_files\\" decompress_files(compressed_file_list, output_dir) ``` After executing the above, `compressed_files` directory should contain `file1.txt.zlib` and `file2.txt.zlib`, and `decompressed_files` directory should contain `file1.txt` and `file2.txt` with contents identical to the original files. **Signature:** ```python from typing import List import zlib def compress_files(file_list: List[str], output_dir: str, level: int = -1) -> None: # Implementation here def decompress_files(file_list: List[str], output_dir: str) -> None: # Implementation here ```","solution":"import os import zlib from typing import List def compress_files(file_list: List[str], output_dir: str, level: int = -1) -> None: for file_path in file_list: try: with open(file_path, \'rb\') as f: file_contents = f.read() compressed_data = zlib.compress(file_contents, level) compressed_file_path = os.path.join(output_dir, os.path.basename(file_path) + \'.zlib\') with open(compressed_file_path, \'wb\') as f: f.write(compressed_data) except Exception as e: print(f\\"Error compressing file {file_path}: {e}\\") def decompress_files(file_list: List[str], output_dir: str) -> None: for file_path in file_list: try: with open(file_path, \'rb\') as f: compressed_data = f.read() decompressed_data = zlib.decompress(compressed_data) original_file_name = os.path.basename(file_path).replace(\'.zlib\', \'\') decompressed_file_path = os.path.join(output_dir, original_file_name) with open(decompressed_file_path, \'wb\') as f: f.write(decompressed_data) except Exception as e: print(f\\"Error decompressing file {file_path}: {e}\\")"},{"question":"You are required to implement a function that processes ZIP archives containing text files. The function should perform the following operations: 1. Extract all text files from the provided ZIP file. 2. Read the content of each text file and count the words. 3. Write a new ZIP file that contains: - A summary file (`summary.txt`) that lists the name of each extracted text file along with its word count. - All original text files in their respective folders. # Function Signature ```python def process_zip_archive(input_zip_path: str, output_zip_path: str): pass ``` # Inputs - `input_zip_path` (str): The path to the input ZIP file containing text files. - `output_zip_path` (str): The path where the output ZIP file should be saved. # Outputs - The output should be a new ZIP file created at `output_zip_path` containing: - Each original text file in its corresponding folder structure. - A `summary.txt` file containing the filenames and their respective word counts, formatted as follows: ``` filename_1.txt: 123 words filename_2.txt: 456 words ``` # Constraints - Only `.txt` files should be considered. - The function should handle possible exceptions, such as invalid ZIP files or unreadable files. - The summary should be comprehensive and accurate. - Ensure all paths and filenames are correctly managed within the ZIP archive. # Example Assume you have a ZIP archive `input.zip` with the following structure: ``` input.zip â”œâ”€â”€ folder1/ â”‚ â”œâ”€â”€ file1.txt â”‚ â”œâ”€â”€ file2.txt â”œâ”€â”€ folder2/ â”‚ â”œâ”€â”€ file3.txt ``` The function call: ```python process_zip_archive(\'input.zip\', \'output.zip\') ``` The resulting `output.zip` should include: - `folder1/` containing `file1.txt` and `file2.txt`. - `folder2/` containing `file3.txt`. - A `summary.txt` file with contents similar to: ``` folder1/file1.txt: 150 words folder1/file2.txt: 200 words folder2/file3.txt: 300 words ``` # Notes - Utilize classes and methods from the `zipfile` module. - Take advantage of the context management feature provided in the `zipfile` module. - Handle exceptions related to file operations gracefully.","solution":"import zipfile import os def process_zip_archive(input_zip_path: str, output_zip_path: str): summary_lines = [] # Create a temporary directory for extraction with zipfile.ZipFile(input_zip_path, \'r\') as zip_ref: extraction_path = \\"extraction_temp\\" zip_ref.extractall(extraction_path) txt_files = [f for f in zip_ref.namelist() if f.endswith(\'.txt\')] for txt_file in txt_files: full_path = os.path.join(extraction_path, txt_file) try: with open(full_path, \'r\', encoding=\\"utf-8\\") as file: text = file.read() word_count = len(text.split()) summary_lines.append(f\\"{txt_file}: {word_count} words\\") except Exception as e: print(f\\"Error processing {txt_file}: {e}\\") summary_content = \\"n\\".join(summary_lines) summary_file_name = \\"summary.txt\\" summary_file_path = os.path.join(extraction_path, summary_file_name) with open(summary_file_path, \'w\', encoding=\\"utf-8\\") as summary_file: summary_file.write(summary_content) # Create the new zip file with zipfile.ZipFile(output_zip_path, \'w\') as new_zip: for foldername, subfolders, filenames in os.walk(extraction_path): for filename in filenames: file_path = os.path.join(foldername, filename) arcname = os.path.relpath(file_path, extraction_path) new_zip.write(file_path, arcname) # Clean up import shutil shutil.rmtree(extraction_path)"},{"question":"Problem Statement: You are provided with a dataset containing input values `X` and corresponding target values `y`. Your task is to implement a function that: 1. Fits an isotonic regression model to the given data. 2. Predicts the output for a set of new input values. 3. Validates the fitted model using mean squared error on the training data. Function Signature: ```python def isotonic_regression_fit_predict(X, y, new_X, weights=None, increasing=\'auto\'): Fits an isotonic regression model to the input data and predicts the output for new input values. Parameters: X (list of float): 1-dimensional list of input values. y (list of float): 1-dimensional list of target values corresponding to X. new_X (list of float): 1-dimensional list of new input values for prediction. weights (list of float, optional): Weights for the error terms. Default is None. increasing (str, bool, optional): Determines if the fitted function is non-decreasing or non-increasing. If \'auto\', the direction is chosen based on Spearman\'s rank correlation coefficient. Returns: tuple: A tuple containing: list of float: Predicted values for new_X. float: Mean squared error of the fitted model on the training data. pass ``` Constraints and Expectations: - **Input Constraints**: - `X` and `y` have the same length. - `X` and `y` are 1-dimensional lists of real numbers. - `new_X` is a 1-dimensional list of real numbers. - `weights` is either `None` or a list of positive real numbers with the same length as `X` and `y`. - Your implementation must use the `IsotonicRegression` class from `scikit-learn`. - The model should be fitting with an isotonic regression. If `weights` is provided, it must be utilized during fitting. - Use mean squared error to evaluate and return the training error. Example: ```python # Example input X = [1, 2, 3, 4, 5] y = [0.5, 1.0, 1.5, 1.8, 2.0] new_X = [1.5, 2.5, 3.5] # Example function call predictions, training_mse = isotonic_regression_fit_predict(X, y, new_X) # Expected output # predictions should be a list of predicted values for [1.5, 2.5, 3.5] # training_mse should be the mean squared error of the fitted model on the training data ``` Additional Information: - Use `mean_squared_error` from `sklearn.metrics` to compute the training MSE. - Documentation for `IsotonicRegression`: https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def isotonic_regression_fit_predict(X, y, new_X, weights=None, increasing=\'auto\'): Fits an isotonic regression model to the input data and predicts the output for new input values. Parameters: X (list of float): 1-dimensional list of input values. y (list of float): 1-dimensional list of target values corresponding to X. new_X (list of float): 1-dimensional list of new input values for prediction. weights (list of float, optional): Weights for the error terms. Default is None. increasing (str, bool, optional): Determines if the fitted function is non-decreasing or non-increasing. If \'auto\', the direction is chosen based on Spearman\'s rank correlation coefficient. Returns: tuple: A tuple containing: list of float: Predicted values for new_X. float: Mean squared error of the fitted model on the training data. # Fit isotonic regression model = IsotonicRegression(increasing=increasing) y_pred = model.fit_transform(X, y, sample_weight=weights) # Predict for new_X values predictions = model.transform(new_X) # Calculate the mean squared error on training data mse = mean_squared_error(y, y_pred, sample_weight=weights) return predictions, mse"},{"question":"**Coding Assessment Question:** # Objective: Design a Tkinter-based application that will perform specific tasks based on user interactions with message boxes. # Problem Statement: Create a Python application using the \\"tkinter\\" module. Your application should have the following functionalities: 1. **Display an Information Message:** - Create a function `display_info` that shows an information message box with a title \\"Info\\" and a message \\"This is an information message.\\" 2. **Display a Warning Message:** - Create a function `display_warning` that shows a warning message box with a title \\"Warning\\" and a message \\"This is a warning message.\\" 3. **Display an Error Message:** - Create a function `display_error` that shows an error message box with a title \\"Error\\" and a message \\"This is an error message.\\" 4. **Ask a Question:** - Create a function `ask_user_question` that shows a question message box with a title \\"Question\\" and a message \\"Do you want to proceed?\\" - If the user selects \\"Yes\\", display an information message box with the message \\"User chose to proceed.\\" - If the user selects \\"No\\", display an information message box with the message \\"User chose not to proceed.\\" # Input and Output Formats: - **Input:** No direct input. - **Output:** Message boxes based on user interaction (as specified in the functions). # Constraints: - Use only standard libraries (`tkinter` and `tkinter.messagebox`). - The application should be interactive and modal, ensuring that user actions are required before proceeding. # Performance Requirements: - The message boxes should respond immediately to user interactions. - The function implementations should be efficient and follow best practices for handling GUI events in Tkinter. # Example Usage: ```python # Example usage of the functions # Import necessary libraries import tkinter as tk from tkinter import messagebox # Define the functions def display_info(): messagebox.showinfo(title=\\"Info\\", message=\\"This is an information message\\") def display_warning(): messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning message\\") def display_error(): messagebox.showerror(title=\\"Error\\", message=\\"This is an error message\\") def ask_user_question(): user_response = messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to proceed?\\") if user_response: messagebox.showinfo(title=\\"Response\\", message=\\"User chose to proceed\\") else: messagebox.showinfo(title=\\"Response\\", message=\\"User chose not to proceed\\") # Create the main application window root = tk.Tk() root.title(\\"Message Box Demo\\") # Create buttons to trigger each function tk.Button(root, text=\\"Show Info\\", command=display_info).pack(pady=5) tk.Button(root, text=\\"Show Warning\\", command=display_warning).pack(pady=5) tk.Button(root, text=\\"Show Error\\", command=display_error).pack(pady=5) tk.Button(root, text=\\"Ask Question\\", command=ask_user_question).pack(pady=5) # Run the main event loop root.mainloop() ```","solution":"import tkinter as tk from tkinter import messagebox def display_info(): messagebox.showinfo(title=\\"Info\\", message=\\"This is an information message\\") def display_warning(): messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning message\\") def display_error(): messagebox.showerror(title=\\"Error\\", message=\\"This is an error message\\") def ask_user_question(): user_response = messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to proceed?\\") if user_response: messagebox.showinfo(title=\\"Response\\", message=\\"User chose to proceed\\") else: messagebox.showinfo(title=\\"Response\\", message=\\"User chose not to proceed\\") # Main application code if __name__ == \\"__main__\\": root = tk.Tk() root.title(\\"Message Box Demo\\") tk.Button(root, text=\\"Show Info\\", command=display_info).pack(pady=5) tk.Button(root, text=\\"Show Warning\\", command=display_warning).pack(pady=5) tk.Button(root, text=\\"Show Error\\", command=display_error).pack(pady=5) tk.Button(root, text=\\"Ask Question\\", command=ask_user_question).pack(pady=5) root.mainloop()"},{"question":"**Question: Advanced Functional Programming in Python** **Objective:** We will test your understanding of the `itertools`, `functools`, and `operator` modules in Python. **Task:** Implement a function `process_and_combine(nested_list: List[List[int]], operation: str) -> List[int]` that processes a nested list of integers and combines its elements according to the specified operation using functional programming tools. **Input:** - `nested_list` (List[List[int]]): A list of lists of integers (`1 <= len(nested_list) <= 10^3` and `1 <= len(nested_list[i]) <= 10^3`). - `operation` (str): A string specifying the operation to apply. It can be one of the following: \'add\', \'sub\', \'mul\', \'div\'. **Output:** - List[int]: A list of integers following the combined operation on each sublist. **Example:** ```python nested_list = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] operation = \'add\' # The result would be: # [1 + 2 + 3, 4 + 5 + 6, 7 + 8 + 9] # [6, 15, 24] process_and_combine(nested_list, operation) # returns [6, 15, 24] ``` **Constraints:** - Ensure the use of functional programming techniques. - You must utilize `itertools.chain`, `functools.reduce`, and `operator` module functions. - Handle all possible operations (\'add\', \'sub\', \'mul\', \'div\') accurately. **Note:** - For division, use integer division. **Hints:** 1. Use `operator` module to fetch the required operation function. 2. Utilize `itertools.chain` to facilitate any inter-sublist manipulations. 3. Use `functools.reduce` for reducing the individual sublists. **Skeleton Code:** ```python from typing import List import itertools import functools import operator def process_and_combine(nested_list: List[List[int]], operation: str) -> List[int]: # Fetch the operation function from the operator module if operation == \'add\': op_func = operator.add elif operation == \'sub\': op_func = operator.sub elif operation == \'mul\': op_func = operator.mul elif operation == \'div\': op_func = operator.floordiv else: raise ValueError(\\"Invalid operation specified\\") # Define a process function for each sublist def process_sublist(sublist: List[int]) -> int: return functools.reduce(op_func, sublist) # Apply the processing function to each sublist and collect results return [process_sublist(sublist) for sublist in itertools.chain(nested_list)] # Test the function with the example provided print(process_and_combine([[1, 2, 3], [4, 5, 6], [7, 8, 9]], \'add\')) # Expected Output: [6, 15, 24] ``` **Your implementation should use the provided skeleton as a guideline. Make sure it passes the given example and adheres to the constraints specified.**","solution":"from typing import List import itertools import functools import operator def process_and_combine(nested_list: List[List[int]], operation: str) -> List[int]: # Fetch the operation function from the operator module if operation == \'add\': op_func = operator.add elif operation == \'sub\': op_func = operator.sub elif operation == \'mul\': op_func = operator.mul elif operation == \'div\': op_func = operator.floordiv else: raise ValueError(\\"Invalid operation specified\\") # Define a process function for each sublist def process_sublist(sublist: List[int]) -> int: if len(sublist) == 0: raise ValueError(\'Sublists must not be empty\') return functools.reduce(op_func, sublist) # Apply the processing function to each sublist and collect results return [process_sublist(sublist) for sublist in nested_list]"},{"question":"Coding Assessment Question # Objective You are required to implement a few functions that demonstrate the use of the `crypt` module in Python. These functions will help you understand how to securely hash and verify passwords using different methods provided by the `crypt` module. # Requirements 1. **Function: `hash_password`** - **Input**: A plain-text password `password` (string) and an optional `method` (one of `crypt.METHOD_*` values). - **Output**: The hashed password (string). - **Constraints**: - If `method` is not provided, use the strongest method available. - Validate that `password` is a non-empty string. - **Example**: ```python hash_password(\\"mysecretpassword\\", crypt.METHOD_SHA256) # Returns a hashed string ``` 2. **Function: `is_password_valid`** - **Input**: A plain-text password `password` (string) and a hashed password `hashed_password` (string). - **Output**: A boolean indicating whether the plain-text password matches the hashed password. - **Constraints**: - Validate that `password` is a non-empty string. - Validate that `hashed_password` is a non-empty string. - **Example**: ```python salted_hash = hash_password(\\"mysecretpassword\\", crypt.METHOD_SHA256) is_password_valid(\\"mysecretpassword\\", salted_hash) # Returns True is_password_valid(\\"wrongpassword\\", salted_hash) # Returns False ``` # Implementation Use the `crypt` module functions `crypt.crypt()` and `crypt.mksalt()` to complete the tasks. Note that constant-time comparison (`hmac.compare_digest`) should be used when comparing passwords to mitigate timing attacks. # Example Usage ```python import crypt from hmac import compare_digest as compare_hash def hash_password(password, method=None): # Your implementation here pass def is_password_valid(password, hashed_password): # Your implementation here pass # Example usage hashed = hash_password(\\"securepassword!\\") print(is_password_valid(\\"securepassword!\\", hashed)) # Should return True print(is_password_valid(\\"wrongpassword\\", hashed)) # Should return False ``` Make sure to test your functions with different passwords and hashing methods to ensure robustness and correctness.","solution":"import crypt from hmac import compare_digest def hash_password(password, method=None): Hash a password using the specified method or the strongest method available. Parameters: password (str): The plain-text password to hash. method (crypt.METHOD_*): Optional. The crypt method to use for hashing. Returns: str: The hashed password. if not password or not isinstance(password, str): raise ValueError(\\"Password must be a non-empty string.\\") if method is None: method = crypt.METHOD_SHA512 # Default to the strongest method available salt = crypt.mksalt(method) hashed_password = crypt.crypt(password, salt) return hashed_password def is_password_valid(password, hashed_password): Verify a plain-text password against a hashed password. Parameters: password (str): The plain-text password to verify. hashed_password (str): The hashed password to compare against. Returns: bool: True if the password matches the hashed password, False otherwise. if not password or not isinstance(password, str): raise ValueError(\\"Password must be a non-empty string.\\") if not hashed_password or not isinstance(hashed_password, str): raise ValueError(\\"Hashed password must be a non-empty string.\\") return compare_digest(crypt.crypt(password, hashed_password), hashed_password)"},{"question":"# Comprehensive Logging System Implementation Objective Implement a comprehensive logging system using Python\'s `logging.handlers` module. The objective of this task is to showcase your understanding of multiple logging handlers and the ability to integrate them into a cohesive logging solution. Description You need to implement a set of functions that configure a Python logging system featuring several advanced logging mechanisms. Specifically, you need to manage log rotation, network-based logging, and asynchronous logging. 1. **Rotating Log Files:** - Implement log rotation based on file size and keep a backup of the last 3 log files. - Use `RotatingFileHandler` with a maxBytes of 10KB and a backupCount of 3. 2. **Sending Logs Over the Network:** - Set up a `SocketHandler` that sends logs to a remote TCP server listening on localhost:5050. - Implement a simple TCP server to receive logs. This will help in testing and validating your logging handler configuration. 3. **Asynchronous Logging with Queue:** - Use `QueueHandler` and `QueueListener` to handle log messages asynchronously. - Ensure that critical log messages are sent via email using `SMTPHandler`. For this task, assume the SMTP server is running on localhost and does not require authentication. 4. **Stream Log Output:** - Additionally, configure a `StreamHandler` to output logs to `sys.stdout` for immediate inspection and debugging purposes. Requirements 1. **Logging configuration function:** - Create a function `configure_logging()` that sets up the logging system as specified. 2. **TCP logging server:** - Create a simple TCP server function `start_logging_server()` that listens for log messages. It should print any received log message to the console. 3. **Email sending utility:** - Implement a helper function `send_email_logs()` to simulate sending logs via email using SMTPHandler. This function should log a sample critical error. Constraints - You must use the classes and methods provided in the documentation of `logging.handlers`. - The system should handle exceptions gracefully and log any issues that arise during logging. Expected Function Signatures ```python import logging import logging.handlers def configure_logging(): # Implement the logging configuration here pass def start_logging_server(host=\'localhost\', port=5050): # Implement the simple TCP server to receive log messages here pass def send_email_logs(): # Implement the function to log a sample critical error message # which should be sent as an email pass ``` Example Usage ```python if __name__ == \\"__main__\\": configure_logging() start_logging_server() # Run this in a separate process/thread in reality logger = logging.getLogger() logger.info(\\"This is an info message\\") logger.error(\\"This is an error message\\") send_email_logs() # Sends a critical log message via email ``` Note Ensure the system logs \\"INFO\\", \\"ERROR\\", and \\"CRITICAL\\" messages as specified and handles large volumes of logs gracefully by utilizing log rotation and queuing mechanisms. Your solution will be evaluated based on correctness, efficiency, exception handling, and adherence to the provided documentation.","solution":"import logging import logging.handlers import sys from queue import Queue import socketserver import threading def configure_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # RotatingFileHandler file_handler = logging.handlers.RotatingFileHandler( \\"app.log\\", maxBytes=10 * 1024, backupCount=3 ) file_handler.setLevel(logging.DEBUG) # StreamHandler stream_handler = logging.StreamHandler(sys.stdout) stream_handler.setLevel(logging.DEBUG) # QueueHandler and QueueListener for async logging log_queue = Queue() queue_handler = logging.handlers.QueueHandler(log_queue) queue_listener = logging.handlers.QueueListener( log_queue, file_handler, stream_handler ) queue_listener.start() # SocketHandler socket_handler = logging.handlers.SocketHandler(\'localhost\', 5050) socket_handler.setLevel(logging.INFO) # SMTPHandler for critical logs smtp_handler = logging.handlers.SMTPHandler( mailhost=(\'localhost\', 25), fromaddr=\'from@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Critical Log\' ) smtp_handler.setLevel(logging.CRITICAL) logger.addHandler(queue_handler) logger.addHandler(socket_handler) logger.addHandler(smtp_handler) def start_logging_server(host=\'localhost\', port=5050): class LogReceiver(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() print(\\"Log received: {}\\".format(data.decode(\'utf-8\'))) with socketserver.TCPServer((host, port), LogReceiver) as server: server.serve_forever() def send_email_logs(): logger = logging.getLogger() logger.critical(\\"This is a critical error that should trigger an email.\\")"},{"question":"# Python Version Parser CPython uses an encoded 32-bit integer to represent its version number, combining the major version, minor version, micro version, release level, and release serial into a single value. This encoded value is known as `PY_VERSION_HEX`. Write a function `parse_python_version(hexversion: int) -> dict` that takes a single integer representing the Python version in the `PY_VERSION_HEX` format and returns a dictionary containing the following components: - `major`: the major version number (integer). - `minor`: the minor version number (integer). - `micro`: the micro version number (integer). - `release_level`: the release level (string, should be one of \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\"). - `release_serial`: the release serial number (integer). Input: - `hexversion` (int): A 32-bit integer representing the Python version. Output: - (dict): A dictionary with the extracted version components. Constraints: - You can assume the input integer will be a valid `PY_VERSION_HEX` value. - The values for each component fall within their typical ranges as per the provided documentation. Example: ```python input_value = 0x030401a2 output_value = { \\"major\\": 3, \\"minor\\": 4, \\"micro\\": 1, \\"release_level\\": \\"alpha\\", \\"release_serial\\": 2, } input_value = 0x030a00f0 output_value = { \\"major\\": 3, \\"minor\\": 10, \\"micro\\": 0, \\"release_level\\": \\"final\\", \\"release_serial\\": 0, } ``` Notes: - Use bitwise operations to extract the relevant bits for each component. - The release levels corresponding values are: - \\"0xA\\" for \\"alpha\\" - \\"0xB\\" for \\"beta\\" - \\"0xC\\" for \\"release candidate\\" - \\"0xF\\" for \\"final\\" You will be assessed on the correctness, efficiency, and clarity of your implementation.","solution":"def parse_python_version(hexversion: int) -> dict: Parses a hex version number to extract the major, minor, micro version numbers, release level, and release serial number. Parameters: hexversion (int): The 32-bit encoded version number. Returns: dict: A dictionary with keys \\"major\\", \\"minor\\", \\"micro\\", \\"release_level\\", \\"release_serial\\" major = (hexversion >> 24) & 0xFF minor = (hexversion >> 16) & 0xFF micro = (hexversion >> 8) & 0xFF release_level_num = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF release_level_map = { 0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"candidate\\", 0xF: \\"final\\" } release_level = release_level_map.get(release_level_num, \\"unknown\\") return { \\"major\\": major, \\"minor\\": minor, \\"micro\\": micro, \\"release_level\\": release_level, \\"release_serial\\": release_serial }"},{"question":"# Handling Stream Management and Memory Allocation with PyTorch XPU Objective: You are tasked with implementing a function in PyTorch that demonstrates your understanding of stream management and memory allocation using the `torch.xpu` module. Your goal is to configure and manage multiple streams to perform parallel matrix multiplication operations on an Intel XPU, and handle memory allocations efficiently. This can also involve checking and synchronizing stream completions correctly. Function Signature: ```python def xpu_matrix_operations(matrix_size: int, iterations: int) -> None: pass ``` Input: - `matrix_size` (int): The dimension of the square matrices to be multiplied (i.e., each matrix will be of size `matrix_size x matrix_size`). - `iterations` (int): The number of parallel multiplications to perform. Output: - This function does not return any value, but should: - Print the device name. - Print memory info like allocated memory and reserved memory before and after the operations in MB. - Ensure all streams have completed their operations before exiting the function. Constraints: - You must use XPU for matrix operations. - Use multiple streams concurrently to perform the matrix multiplications in parallel. - Before starting your operations, reset any device state and clear memory caches. - Ensure memory management is handled such that you print the details of memory usage before and after the operations. Example Execution: ```python xpu_matrix_operations(1024, 3) ``` Expected Output: (Example indicative, exact values will vary) ``` Device Name: Intel XPU Memory before operation: Allocated: 0 MB Reserved: 0 MB Memory after operation: Allocated: 1024 MB Reserved: 2048 MB ``` Notes: 1. You may assume that `matrix_size` is a positive integer and reasonably large for the purpose of triggering noticeable memory usage. 2. Ensure synchronization of streams before measuring completion and memory usage. 3. Use the functionalities from `torch.xpu` for all device and memory management tasks. The purpose of this question is to test your ability to: - Manage XPU devices using PyTorch. - Handle memory allocations and deallocations. - Utilize streams for parallel execution efficiently. - Accurately monitor and communicate memory usage of operations on the XPU.","solution":"import torch from torch.xpu import device_count, get_device_properties, set_device, Stream def xpu_matrix_operations(matrix_size: int, iterations: int) -> None: # Ensure XPU is available if not torch.xpu.is_available(): print(\\"XPU is not available.\\") return device = torch.device(\'xpu\') # Print device information print(f\\"Device Name: {get_device_properties(device).name}\\") # Reset any device state and clear caches torch.xpu.empty_cache() # Function to convert bytes to MB def bytes_to_MB(bytes): return bytes / (1024 ** 2) # Memory info before operations mem_info_before = torch.xpu.memory_stats(device) print(\\"Memory before operation:\\") print(f\\" Allocated: {bytes_to_MB(mem_info_before[\'allocated_bytes.all.current\'])} MB\\") print(f\\" Reserved: {bytes_to_MB(mem_info_before[\'reserved_bytes.all.current\'])} MB\\") streams = [Stream(device) for _ in range(iterations)] matrices = [torch.randn((matrix_size, matrix_size), device=device) for _ in range(iterations * 2)] # Perform operations in different streams for i in range(iterations): with torch.xpu.stream(streams[i]): result = torch.matmul(matrices[2 * i], matrices[2 * i + 1]) # Ensure all streams have completed their tasks for stream in streams: stream.synchronize() # Memory info after operations mem_info_after = torch.xpu.memory_stats(device) print(\\"Memory after operation:\\") print(f\\" Allocated: {bytes_to_MB(mem_info_after[\'allocated_bytes.all.current\'])} MB\\") print(f\\" Reserved: {bytes_to_MB(mem_info_after[\'reserved_bytes.all.current\'])} MB\\")"},{"question":"**Question: Source Code Analyzer using `pyclbr`** Your task is to implement a Python function that analyzes a given Python module using the `pyclbr` module and returns a comprehensive summary of the classes and functions defined within it. # Function Signature ```python def analyze_module(module_name: str, path: Optional[List[str]] = None) -> Dict[str, Any]: pass ``` # Input - `module_name` (str): The name of the Python module to analyze. - `path` (Optional[List[str]]): An optional list of directory paths to prepend to `sys.path` for locating the module source code. # Output - Returns a dictionary containing a detailed summary of the module\'s classes and functions with the following format: ```python { \\"module\\": \\"module_name\\", \\"classes\\": [ { \\"name\\": \\"ClassName\\", \\"file\\": \\"file_path\\", \\"lineno\\": start_line_number, \\"superclasses\\": [\\"BaseClass1\\", \\"BaseClass2\\"], \\"methods\\": [ { \\"name\\": \\"method_name\\", \\"lineno\\": method_line_number, \\"is_async\\": True/False } ], \\"nested_classes\\": [ <same structure as parent class> ], \\"nested_functions\\": [ <same structure as described in functions> ] } ], \\"functions\\": [ { \\"name\\": \\"FunctionName\\", \\"file\\": \\"file_path\\", \\"lineno\\": start_line_number, \\"is_async\\": True/False, \\"nested_functions\\": [ <same structure as top-level functions> ], \\"nested_classes\\": [ <same structure as described in classes> ] } ] } ``` # Constraints - The module to be analyzed must be written in Python. - You must not import the module for analysis, only read its source code. - Make sure to handle nested functions and classes appropriately. - Use the `pyclbr.readmodule_ex` function for comprehensive analysis. - If the module does not exist, raise a `ModuleNotFoundError`. # Examples ```python # Example: Analyzing a module named \\"example_module\\" result = analyze_module(\\"example_module\\") print(result) # Expected output (structure may vary based on actual module content) { \\"module\\": \\"example_module\\", \\"classes\\": [ { \\"name\\": \\"ExampleClass\\", \\"file\\": \\"/path/to/example_module.py\\", \\"lineno\\": 10, \\"superclasses\\": [\\"BaseClass\\"], \\"methods\\": [ { \\"name\\": \\"example_method\\", \\"lineno\\": 12, \\"is_async\\": False } ], \\"nested_classes\\": [], \\"nested_functions\\": [] } ], \\"functions\\": [ { \\"name\\": \\"example_function\\", \\"file\\": \\"/path/to/example_module.py\\", \\"lineno\\": 5, \\"is_async\\": False, \\"nested_functions\\": [], \\"nested_classes\\": [] } ] } ``` # Note Ensure that your implementation captures all relevant details specified in the output format and handles nested structures proficiently.","solution":"from typing import Any, Dict, List, Optional import pyclbr import sys import os def analyze_module(module_name: str, path: Optional[List[str]] = None) -> Dict[str, Any]: # Prepend custom paths to sys.path if path: original_sys_path = sys.path[:] sys.path = path + sys.path try: module_info = pyclbr.readmodule_ex(module_name) except ImportError: raise ModuleNotFoundError(f\\"Module \'{module_name}\' not found\\") finally: if path: sys.path = original_sys_path result = { \\"module\\": module_name, \\"classes\\": [], \\"functions\\": [] } for name, obj in module_info.items(): if isinstance(obj, pyclbr.Class): class_info = { \\"name\\": obj.name, \\"file\\": obj.file, \\"lineno\\": obj.lineno, \\"superclasses\\": [base.name for base in obj.super], \\"methods\\": [], \\"nested_classes\\": [], \\"nested_functions\\": [] } # Get class methods for method_name, method_lineno in obj.methods.items(): class_info[\\"methods\\"].append({ \\"name\\": method_name, \\"lineno\\": method_lineno, \\"is_async\\": False # pyclbr does not provide async info }) result[\\"classes\\"].append(class_info) elif isinstance(obj, pyclbr.Function): function_info = { \\"name\\": obj.name, \\"file\\": obj.file, \\"lineno\\": obj.lineno, \\"is_async\\": False, # pyclbr does not provide async info \\"nested_functions\\": [], \\"nested_classes\\": [] } result[\\"functions\\"].append(function_info) return result"},{"question":"# Custom Python Interactive Console **Objective:** You are to implement a custom interactive console in Python that extends the basic functionality of the Python interpreter. This console will allow users to execute Python commands interactively, with additional features such as command history and custom command execution. **Requirements:** - Use the \\"code\\" module to implement an interactive console. - Implement command history such that users can view and re-execute previous commands by entering a special command (`:history`). - Implement a custom command (`:exit`) to terminate the interactive console. **Function Details:** You need to implement the following function: ```python def custom_interactive_console(): Launch a custom interactive console with the following features: - Users can enter Python commands interactively. - The console maintains a history of commands entered. - Users can view the command history by entering \':history\'. - Users can exit the console by entering \':exit\'. Returns: None pass ``` **Expected Behavior:** 1. **Interactive Command Execution:** The console should accept and execute Python commands interactively, providing immediate feedback/output. 2. **Command History Viewing:** When the user inputs `:history`, the console should print all previously entered commands. 3. **Exiting the Console:** When the user inputs `:exit`, the console should terminate. **Example:** ```python >>> custom_interactive_console() >>> 1 + 1 2 >>> print(\\"Hello, World!\\") Hello, World! >>> :history 1 + 1 print(\\"Hello, World!\\") >>> :exit ``` **Constraints:** - You may assume that all input commands are properly formatted Python code or predefined special commands. - You should handle edge cases such as executing commands from the history. **Performance Requirements:** - The console should efficiently handle a large history of commands and maintain responsiveness during interactive use. **Notes:** - Utilize the `code` module\'s `InteractiveConsole` class to streamline the implementation. - Remember to extend the default behavior of the console to include command history and custom command handling.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.command_history = [] def raw_input(self, prompt=\\"\\"): try: command = input(prompt) except EOFError: command = \':exit\' if command == \':history\': for idx, cmd in enumerate(self.command_history, start=1): print(f\\"{idx}: {cmd}\\") return \\"\\" elif command == \':exit\': print(\\"Exiting...\\") raise SystemExit else: self.command_history.append(command) return command def custom_interactive_console(): console = CustomInteractiveConsole() console.interact(\\"Custom Interactive Console. Type :exit to quit, :history to view command history.\\")"},{"question":"Using scikit-learn, write a Python function to implement a data preprocessing and model building pipeline for a given dataset. The function should take in a dataset, preprocess it, split it into training and testing sets, train a given model, and evaluate its performance using accuracy as the metric. **Function Signature**: ```python def preprocess_and_train_model(data: pd.DataFrame, target_col: str, model, test_size: float=0.2, random_state: int=42) -> float: Preprocess the dataset, train the model, and evaluate it. Parameters: data (pd.DataFrame): A pandas DataFrame containing the dataset. target_col (str): The name of the target column in the dataset. model: A scikit-learn estimator instance (e.g., LogisticRegression, RandomForestClassifier). test_size (float): The proportion of the dataset to include in the test split. (default is 0.2) random_state (int): The seed used by the random number generator. (default is 42) Returns: float: The accuracy of the model on the test set. ``` **Requirements**: 1. **Preprocessing**: Handle missing values by imputing with the mean for numerical features. 2. Encode categorical features using one-hot encoding. 3. **Splitting**: Split the dataset into training and testing sets using `train_test_split` from `scikit-learn`. 4. **Model Training**: Train the provided scikit-learn model on the training data. 5. **Evaluation**: Evaluate the model on the testing data using accuracy as the metric. **Example**: ```python import pandas as pd from sklearn.linear_model import LogisticRegression # Sample data data = pd.DataFrame({ \'age\': [25, 32, 47, 51, np.nan], \'salary\': [50000, 60000, 80000, 56000, 70000], \'gender\': [\'male\', \'female\', \'female\', \'male\', \'female\'], \'purchased\': [0, 1, 1, 0, 1] }) accuracy = preprocess_and_train_model(data, \'purchased\', LogisticRegression()) print(f\'Model Accuracy: {accuracy}\') ``` **Constraints**: - You are allowed to use only scikit-learn for preprocessing and model training. - Handle any data-specific issues that might arise (e.g., missing values). **Note**: - Make sure to handle any potential issues with the provided dataset, such as missing values or categorical features. - It is recommended to import necessary libraries and show an example dataset and usage.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.metrics import accuracy_score def preprocess_and_train_model(data, target_col, model, test_size=0.2, random_state=42): Preprocess the dataset, train the model, and evaluate it. Parameters: data (pd.DataFrame): A pandas DataFrame containing the dataset. target_col (str): The name of the target column in the dataset. model: A scikit-learn estimator instance (e.g., LogisticRegression, RandomForestClassifier). test_size (float): The proportion of the dataset to include in the test split. (default is 0.2) random_state (int): The seed used by the random number generator. (default is 42) Returns: float: The accuracy of the model on the test set. # Separate features and target X = data.drop(columns=[target_col]) y = data[target_col] # Define column types numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Define preprocessing steps numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) # handle unknown categories in test set ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create preprocessing and training pipeline clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'model\', model) ]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Train the model clf.fit(X_train, y_train) # Predict and evaluate y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"on Python \'enum\' Module Objective The goal of this exercise is to demonstrate your understanding of Python\'s `enum` module. You will create enumerations, handle unique values, implement custom methods, and use automatic values where necessary. Problem Statement You work at a weather monitoring company and need to define various weather conditions along with their respective severity levels and monitor specific attributes related to them. You will create an enumeration to represent different weather conditions and define custom methods to handle weather severity and descriptions. Requirements 1. Create an enumeration class `WeatherCondition` using `enum.Enum` with the following members and values: - SUNNY: (1, \'Clear sky\', \'No Precautions\') - CLOUDY: (2, \'Overcast sky\', \'Minimal Precautions\') - RAINY: (3, \'Rain showers\', \'Carry an Umbrella\') - STORMY: (4, \'Thunderstorms\', \'Stay Indoors\') - SNOWY: (5, \'Snowfall\', \'Wear Warm Clothes\') 2. Implement a method `describe` to return the name, description, and precaution of the weather condition for a given enumeration member. 3. Use the `unique` decorator to ensure there are no duplicate values in your enumeration. 4. Ensure that the class is iterable and can output its members in the order defined. 5. Write a static method `weather_severity` within the `WeatherCondition` class to return the severity level for a given weather condition. Input and Output - Input: None (modifying the class methods). - Output: - The `describe` method should return a tuple `(name, description, precaution)` where `name` is the name of the weather condition, `description` is the textual description, and `precaution` is the recommended action. - The `weather_severity` method should return the severity level (integer) for the given weather condition. Constraints - Ensure your implementation respects the constraints and design principles described in the `enum` module documentation. - Use the `unique` decorator to avoid duplicate values. Example Usage ```python from enum import Enum, unique @unique class WeatherCondition(Enum): SUNNY = (1, \'Clear sky\', \'No Precautions\') CLOUDY = (2, \'Overcast sky\', \'Minimal Precautions\') RAINY = (3, \'Rain showers\', \'Carry an Umbrella\') STORMY = (4, \'Thunderstorms\', \'Stay Indoors\') SNOWY = (5, \'Snowfall\', \'Wear Warm Clothes\') def describe(self): return (self.name, self.value[1], self.value[2]) @staticmethod def weather_severity(condition): return condition.value[0] # Example of usage print(WeatherCondition.SUNNY.describe()) # Output: (\'SUNNY\', \'Clear sky\', \'No Precautions\') print(WeatherCondition.weather_severity(WeatherCondition.RAINY)) # Output: 3 ``` Your task is to implement the `WeatherCondition` class as defined above. Ensure to adhere strictly to the specified requirements and constraints.","solution":"from enum import Enum, unique @unique class WeatherCondition(Enum): SUNNY = (1, \'Clear sky\', \'No Precautions\') CLOUDY = (2, \'Overcast sky\', \'Minimal Precautions\') RAINY = (3, \'Rain showers\', \'Carry an Umbrella\') STORMY = (4, \'Thunderstorms\', \'Stay Indoors\') SNOWY = (5, \'Snowfall\', \'Wear Warm Clothes\') def describe(self): return (self.name, self.value[1], self.value[2]) @staticmethod def weather_severity(condition): return condition.value[0]"},{"question":"You are given a PyTorch model and required to implement functions that: 1. Save the model\'s state dict to a file. 2. Load the state dict from the file and initialize a new model with the loaded state. 3. Save tensors from a list and ensure their view relationships are preserved. 4. Load the tensors and verify their view relationships are maintained. # Requirements 1. **Functions to Implement:** - `save_model_state(model: torch.nn.Module, file_path: str) -> None` - `load_model_state(model_class: Type[torch.nn.Module], file_path: str) -> torch.nn.Module` - `save_tensors(tensors: List[torch.Tensor], file_path: str) -> None` - `load_tensors(file_path: str) -> List[torch.Tensor]` 2. **Input and Output Formats:** - `save_model_state(model, file_path)`: - **Input**: `model` is an instance of `torch.nn.Module`, `file_path` is the path to save the state dict. - **Output**: None. - `load_model_state(model_class, file_path)`: - **Input**: `model_class` is the class of the `torch.nn.Module`, `file_path` is the path from which to load the state dict. - **Output**: Returns an instance of `model_class` with the loaded state. - `save_tensors(tensors, file_path)`: - **Input**: `tensors` is a list of `torch.Tensor` objects, `file_path` is the path to save the tensors. - **Output**: None. - `load_tensors(file_path)`: - **Input**: `file_path` is the path from which to load the tensors. - **Output**: Returns a list of `torch.Tensor` objects. 3. **Constraints:** - The file paths provided will always be valid. - The `model_class` provided to `load_model_state` will always match the originally saved model type. - Test cases will ensure tensors have view relationships that need to be preserved. # Example ```python import torch import torch.nn as nn from typing import List, Type class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 5) def forward(self, x): return self.fc1(x) # Example Usage model = SimpleModel() save_model_state(model, \'model_state.pt\') # Loading model state new_model = load_model_state(SimpleModel, \'model_state.pt\') # Verify the state is loaded properly assert torch.equal(next(new_model.parameters()), next(model.parameters())) # Save tensors with view relationships t = torch.arange(10) view_tensors = [t, t.view(2, 5)] save_tensors(view_tensors, \'tensors.pt\') # Load tensors and verify view relationships loaded_tensors = load_tensors(\'tensors.pt\') assert torch.equal(loaded_tensors[0], t) assert torch.equal(loaded_tensors[1], t.view(2, 5)) ``` # Implementation You need to implement the following functions to pass the above example and additional test cases. ```python import torch import torch.nn as nn from typing import List, Type def save_model_state(model: nn.Module, file_path: str) -> None: torch.save(model.state_dict(), file_path) def load_model_state(model_class: Type[nn.Module], file_path: str) -> nn.Module: model = model_class() model.load_state_dict(torch.load(file_path)) return model def save_tensors(tensors: List[torch.Tensor], file_path: str) -> None: torch.save(tensors, file_path) def load_tensors(file_path: str) -> List[torch.Tensor]: return torch.load(file_path) ``` # Notes: - Ensure you test the functions with tensors having sharing storage to verify view relationships are preserved. - Handle any PyTorch specific warnings or errors you might encounter during saving and loading operations.","solution":"import torch import torch.nn as nn from typing import List, Type def save_model_state(model: nn.Module, file_path: str) -> None: torch.save(model.state_dict(), file_path) def load_model_state(model_class: Type[nn.Module], file_path: str) -> nn.Module: model = model_class() model.load_state_dict(torch.load(file_path)) return model def save_tensors(tensors: List[torch.Tensor], file_path: str) -> None: torch.save(tensors, file_path) def load_tensors(file_path: str) -> List[torch.Tensor]: return torch.load(file_path)"},{"question":"Problem Statement You are given a dataset containing information about various types of wines. Your task is to preprocess this data and create a pipeline that normalizes features, imputes missing values if any, and applies a dimensionality reduction technique. You will then transform the given data using this pipeline and evaluate its effectiveness by checking the transformed data\'s shape and sample values. # Dataset Description The dataset consists of features such as alcohol content, acidity, residual sugar, and more. The dataset might have missing values and varied scales for different features. # Instructions 1. **Read the dataset**: Assume the dataset is provided as a CSV file named `wine_data.csv`. 2. **Preprocessing**: - **Normalization**: Apply a standard scaler to normalize the features. - **Imputation**: Impute any missing values using the mean of the column. - **Dimensionality Reduction**: Use PCA (Principal Component Analysis) to reduce the features to 2 principal components. 3. **Pipeline**: - Create a pipeline combining the above transformers. 4. **Transform Data**: Transform the dataset using the pipeline. 5. **Output**: - Print the shape of the transformed data. - Print the first 5 rows of the transformed data. # Function Signature ```python def preprocess_wine_data(csv_file: str) -> None: pass ``` # Example If `wine_data.csv` contains the following data: ``` alcohol,malic_acid,ash,alcalinity_of_ash,magnesium,total_phenols,flavanoids,nonflavanoid_phenols,proanthocyanins,color_intensity,hue,od280_od315_of_diluted_wines,proline 14.23,1.71,2.43,15.6,127,2.8,3.06,0.28,2.29,5.64,1.04,3.92,1065 13.2,1.78,2.14,11.2,100,2.65,2.76,0.26,1.28,4.38,1.05,3.4,1050 ``` Your function should output: ``` Transformed shape: (2, 2) Transformed data (first 5 rows): [[ 2.00451449 -0.46504026] [ 1.60848957 -0.65650933]] ``` # Constraints - You should use scikit-learn for all transformations. - Handle missing values appropriately.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def preprocess_wine_data(csv_file: str) -> None: # Load the dataset data = pd.read_csv(csv_file) # Define the preprocessing steps pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Impute missing values (\'scaler\', StandardScaler()), # Normalize the features (\'pca\', PCA(n_components=2)) # Apply PCA to reduce to 2 components ]) # Fit and transform the data using the pipeline transformed_data = pipeline.fit_transform(data) # Output the transformed shape and the first 5 rows print(\\"Transformed shape:\\", transformed_data.shape) print(\\"Transformed data (first 5 rows):\\") print(transformed_data[:5])"},{"question":"**Question: Working with `.plist` files using `plistlib`** You are working on a project that involves reading from and writing data to `.plist` files. Your task is to create two functions using the `plistlib` module in Python: 1. `merge_plists(plist1: bytes, plist2: bytes) -> bytes`: This function should take two plist files in bytes format, merge their dictionaries, and return a new plist in bytes format. If there are overlapping keys, the values from `plist2` should overwrite those from `plist1`. 2. `filter_plist(plist_data: bytes, keys_to_keep: list) -> bytes`: This function should take a plist in bytes format and a list of keys. It should return a new plist in bytes format containing only the specified keys and their associated values from the original plist. # Function 1: `merge_plists` - **Input:** - `plist1`: bytes object representing the first plist. - `plist2`: bytes object representing the second plist. - **Output:** - A bytes object representing the merged plist. - **Constraints:** - Both `plist1` and `plist2` will represent valid plist data. - The resulting plist should be in XML format. # Function 2: `filter_plist` - **Input:** - `plist_data`: bytes object representing the plist to filter. - `keys_to_keep`: list of strings specifying the keys to keep in the resulting plist. - **Output:** - A bytes object representing the filtered plist. - **Constraints:** - `plist_data` will represent valid plist data. - `keys_to_keep` will contain valid strings that could be keys in `plist_data`. - The resulting plist should be in XML format. # Examples: ```python import plistlib # Example for merge_plists plist1 = plistlib.dumps({\'a\': 1, \'b\': 2, \'c\': 3}) plist2 = plistlib.dumps({\'b\': 20, \'d\': 4}) merged_plist = merge_plists(plist1, plist2) print(plistlib.loads(merged_plist)) # Output: {\'a\': 1, \'b\': 20, \'c\': 3, \'d\': 4} # Example for filter_plist plist = plistlib.dumps({\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4}) filtered_plist = filter_plist(plist, [\'a\', \'c\']) print(plistlib.loads(filtered_plist)) # Output: {\'a\': 1, \'c\': 3} ``` # Note: Use the `plistlib` module\'s relevant functions to handle reading and writing plist data. Ensure your functions correctly handle invalid input scenarios and raise appropriate exceptions as needed.","solution":"import plistlib def merge_plists(plist1: bytes, plist2: bytes) -> bytes: Merges two plist dictionaries, with plist2 values overwriting plist1 values in case of key conflicts. Args: - plist1: First plist file in bytes format. - plist2: Second plist file in bytes format. Returns: - A new plist file in bytes format representing the merged dictionary. dict1 = plistlib.loads(plist1) dict2 = plistlib.loads(plist2) merged_dict = {**dict1, **dict2} return plistlib.dumps(merged_dict) def filter_plist(plist_data: bytes, keys_to_keep: list) -> bytes: Filters a plist dictionary to keep only specified keys. Args: - plist_data: Plist file in bytes format. - keys_to_keep: List of keys to keep in the resulting plist. Returns: - A new plist file in bytes format containing only the specified keys and their values. original_dict = plistlib.loads(plist_data) filtered_dict = {key: value for key, value in original_dict.items() if key in keys_to_keep} return plistlib.dumps(filtered_dict)"},{"question":"**Problem Statement: Implement a Custom Sequence Class with Advanced Slicing** You are required to implement a custom sequence class named `CustomSequence` in Python. This class should be able to handle slicing operations in an advanced way, allowing flexible extraction of elements based on start, stop, and step values from slice objects. Additionally, implement a method to get the adjusted slice indices using conditions similar to those described in the provided documentation. # Class Definition: ```python class CustomSequence: def __init__(self, data): Initialize the CustomSequence with the provided data. :param data: list or a sequence of elements pass def __getitem__(self, item): Retrieve elements for the given item. If item is a slice, process it and return the corresponding elements. :param item: An integer index or a slice object :return: The element(s) at the given index or sliced accordingly pass def get_adjusted_indices(self, start, stop, step): Adjust start and stop indices assuming a sequence of the specified length. Indices outside the bounds should be clipped correctly. :param start: The starting index of the slice :param stop: The stopping index of the slice :param step: The step of the slice :return: tuple (adjusted start index, adjusted stop index, slice length) pass ``` # Requirements: 1. `__init__(self, data)`: - Initialize the `CustomSequence` with the provided data. The data is expected to be a list of elements. 2. `__getitem__(self, item)`: - If `item` is an integer, return the element at that index. - If `item` is a slice object, return the corresponding elements following the standard slicing rules. - Use the `get_adjusted_indices` method to adjust slice indices before extracting elements. 3. `get_adjusted_indices(self, start, stop, step)`: - Adjust the start and stop indices given the sequence length. - Clipping should be handled in a manner consistent with normal Python slicing behavior. - Return a tuple with the adjusted start index, stop index, and slicelength (number of elements in the resulting slice). # Constraints: - You may assume the data provided to `CustomSequence` will always be a non-empty list of integers. - You should handle negative indices properly. - Do not use any external libraries; only use standard Python. # Example Usage: ```python cs = CustomSequence([10, 20, 30, 40, 50, 60]) # Accessing elements normally print(cs[2]) # Output: 30 # Using slices print(cs[1:5:2]) # Output: [20, 40] # Using negative indices print(cs[-4:-1:1]) # Output: [30, 40, 50] # Adjusted indices example print(cs.get_adjusted_indices(1, 6, 2)) # Output: (1, 6, 3) ``` In this problem, students are expected to demonstrate their ability to work with custom class implementations, understand and correctly apply slicing rules, and handle edge cases for negative and out-of-bound indices.","solution":"class CustomSequence: def __init__(self, data): Initialize the CustomSequence with the provided data. :param data: list or a sequence of elements self.data = data def __getitem__(self, item): Retrieve elements for the given item. If item is a slice, process it and return the corresponding elements. :param item: An integer index or a slice object :return: The element(s) at the given index or sliced accordingly if isinstance(item, slice): start, stop, step = item.indices(len(self.data)) return self.data[start:stop:step] elif isinstance(item, int): return self.data[item] else: raise TypeError(\\"Invalid argument type.\\") def get_adjusted_indices(self, start, stop, step): Adjust start and stop indices assuming a sequence of the specified length. Indices outside the bounds should be clipped correctly. :param start: The starting index of the slice :param stop: The stopping index of the slice :param step: The step of the slice :return: tuple (adjusted start index, adjusted stop index, slice length) length = len(self.data) start, stop, step = slice(start, stop, step).indices(length) slicelength = len(range(start, stop, step)) return start, stop, slicelength"},{"question":"**Question: Implement a Numeric Calculation Function** Given several numeric operations defined in the `python310` package, implement a function `complex_numeric_operations` that takes two `PyObject` arguments and performs a series of operations on them. Specifically, the function should: 1. Add the two objects. 2. Subtract the second object from the first. 3. Multiply the two objects. 4. Divide the first object by the second using both true and floor division. 5. Compute the power of the first object raised to the second (without a modulo argument). 6. Perform bitwise AND, OR, and XOR operations on the objects. 7. Return all the results in a dictionary with appropriate keys. Additionally, the function should handle exceptions appropriately: - If any operation fails (e.g., due to type errors or other issues), it should store the string `\\"Error\\"` under the corresponding key in the result dictionary. - The function should assume all inputs are valid `PyObject` instances that could represent numerical types. # Input and Output Formats **Input:** - Two `PyObject` instances, `o1` and `o2`. **Output:** - A dictionary containing the results of the various operations with the following keys: `\'add\'`, `\'subtract\'`, `\'multiply\'`, `\'true_divide\'`, `\'floor_divide\'`, `\'power\'`, `\'bitwise_and\'`, `\'bitwise_or\'`, and `\'bitwise_xor\'`. **Example:** ```python def complex_numeric_operations(o1, o2): # Your implementation here # Assume PyObject creation and PyNumber functions are correctly imported and used. # Example usage: # o1 = PyObject representing the number 10 # o2 = PyObject representing the number 5 results = complex_numeric_operations(o1, o2) assert results == { \'add\': PyObject representing 15, \'subtract\': PyObject representing 5, \'multiply\': PyObject representing 50, \'true_divide\': PyObject representing 2.0, \'floor_divide\': PyObject representing 2, \'power\': PyObject representing 100000, \'bitwise_and\': PyObject representing 0, \'bitwise_or\': PyObject representing 15, \'bitwise_xor\': PyObject representing 15 } ``` **Constraints:** - Your implementation should use the functions from the `python310` package as documented. - Proper error handling should be implemented for operations that can fail.","solution":"def complex_numeric_operations(o1, o2): Perform various numeric operations on two given PyObject instances. Parameters: o1 (PyObject): The first numerical PyObject. o2 (PyObject): The second numerical PyObject. Returns: dict: A dictionary containing the results of the numeric operations. results = {} try: results[\'add\'] = o1 + o2 except Exception as e: results[\'add\'] = \\"Error\\" try: results[\'subtract\'] = o1 - o2 except Exception as e: results[\'subtract\'] = \\"Error\\" try: results[\'multiply\'] = o1 * o2 except Exception as e: results[\'multiply\'] = \\"Error\\" try: results[\'true_divide\'] = o1 / o2 except Exception as e: results[\'true_divide\'] = \\"Error\\" try: results[\'floor_divide\'] = o1 // o2 except Exception as e: results[\'floor_divide\'] = \\"Error\\" try: results[\'power\'] = o1 ** o2 except Exception as e: results[\'power\'] = \\"Error\\" try: results[\'bitwise_and\'] = o1 & o2 except Exception as e: results[\'bitwise_and\'] = \\"Error\\" try: results[\'bitwise_or\'] = o1 | o2 except Exception as e: results[\'bitwise_or\'] = \\"Error\\" try: results[\'bitwise_xor\'] = o1 ^ o2 except Exception as e: results[\'bitwise_xor\'] = \\"Error\\" return results"},{"question":"**Asynchronous Task Distribution with asyncio.Queue** You are given a list of tasks that need to be processed by a number of worker functions concurrently. Each task is represented as a dictionary with a \\"task_id\\" and \\"duration\\". Your goal is to implement a system that distributes these tasks across several worker coroutines using `asyncio.Queue`. # Requirements 1. Create an `asyncio.Queue` and populate it with the tasks. 2. Implement worker coroutines that: - Fetch tasks from the queue. - Simulate task processing by sleeping for the duration specified in the task. - Mark the task as done when processing is complete. 3. Ensure all tasks are processed and report the total time taken. # Input - A list of task dictionaries: `tasks = [{\'task_id\': 1, \'duration\': 2}, {\'task_id\': 2, \'duration\': 1}, ...]` - The number of worker coroutines to use: `num_workers` # Output - Print statements from each worker indicating the task they processed and the time taken. - The total time taken to process all tasks reported as the last printed statement. # Constraints - Use `asyncio.Queue` - Tasks must be processed in the order they are put into the queue (FIFO). - Use `asyncio.run()` to execute the main function. - Handle cancellation of worker tasks after processing all queue items. # Example ```python import asyncio import time async def worker(name, queue): while True: try: task = await queue.get() print(f\'{name} fetched task {task[\\"task_id\\"]} with duration {task[\\"duration\\"]}\') await asyncio.sleep(task[\\"duration\\"]) queue.task_done() print(f\'{name} completed task {task[\\"task_id\\"]} in {task[\\"duration\\"]} seconds\') except asyncio.CancelledError: # Exit gracefully if the worker is cancelled break async def main(tasks, num_workers): queue = asyncio.Queue() for task in tasks: queue.put_nowait(task) tasks_list = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks_list.append(task) started_at = time.monotonic() await queue.join() total_time = time.monotonic() - started_at for task in tasks_list: task.cancel() await asyncio.gather(*tasks_list, return_exceptions=True) print(f\'Total time taken to process all tasks: {total_time:.2f} seconds\') tasks = [ {\'task_id\': 1, \'duration\': 2}, {\'task_id\': 2, \'duration\': 1}, {\'task_id\': 3, \'duration\': 3}, # Add more tasks as needed ] num_workers = 3 asyncio.run(main(tasks, num_workers)) ``` # Instructions 1. Implement the `worker` and `main` functions as described. 2. Ensure the system meets the input-output requirements and constraints. 3. Test your implementation with different sets of tasks and number of workers.","solution":"import asyncio import time async def worker(name, queue): while True: try: task = await queue.get() print(f\'{name} fetched task {task[\\"task_id\\"]} with duration {task[\\"duration\\"]}\') await asyncio.sleep(task[\\"duration\\"]) queue.task_done() print(f\'{name} completed task {task[\\"task_id\\"]} in {task[\\"duration\\"]} seconds\') except asyncio.CancelledError: # Exit gracefully if the worker is cancelled break async def main(tasks, num_workers): queue = asyncio.Queue() for task in tasks: queue.put_nowait(task) tasks_list = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks_list.append(task) started_at = time.monotonic() await queue.join() total_time = time.monotonic() - started_at for task in tasks_list: task.cancel() await asyncio.gather(*tasks_list, return_exceptions=True) print(f\'Total time taken to process all tasks: {total_time:.2f} seconds\') tasks = [ {\'task_id\': 1, \'duration\': 2}, {\'task_id\': 2, \'duration\': 1}, {\'task_id\': 3, \'duration\': 3}, ] num_workers = 3 if __name__ == \\"__main__\\": asyncio.run(main(tasks, num_workers))"},{"question":"# Advanced Coding Assessment: Multi-threaded Queue Management Objective Implement a program that utilizes different types of queues (FIFO, LIFO, Priority) from the `queue` module. Your program should handle multiple producer and consumer threads efficiently, ensuring proper synchronization and exception handling. Requirements 1. **Queue Types**: - You must implement three types of queues: FIFO, LIFO, and Priority. - Each queue will have a maximum size of 10. 2. **Producers**: - Implement at least two producer threads. - Each producer thread should: - Generate an integer item every second. - Place the generated item into the specified queue. - Handle `queue.Full` exception by waiting 1 second before retrying. 3. **Consumers**: - Implement at least two consumer threads. - Each consumer thread should: - Retrieve an item from the specified queue. - Simulate processing time by sleeping for a random period between 0.5 to 2 seconds. - Print the processed item. - Handle `queue.Empty` exception by waiting 1 second before retrying. 4. **Queue Processing**: - The program should continuously process queue items until manually stopped (infinite processing loop with a stop condition). 5. **Exceptions and Synchronization**: - Properly handle `queue.Full` and `queue.Empty` exceptions. - Ensure synchronization using appropriate locking mechanisms provided by the `queue` module. Input Format - No direct input is required; however, the program should be set up to allow easy changes to the queue type (FIFO, LIFO, Priority) and the number of producer/consumer threads. Constraints - Ensure the maximum size limit for the queues is honored. - Producers should place items into the queue only if there\'s space available. - Consumers should only retrieve items if there are items available in the queue. Output Format - Print statements showing: - When items are added to the queue. - When items are retrieved and processed. Example A sample output can be structured as follows: ``` Producer 1: Added 3 to FIFO Queue Producer 2: Added 7 to FIFO Queue Consumer 1: Processing 3 from FIFO Queue Consumer 2: Processing 7 from FIFO Queue Producer 1: Added 8 to FIFO Queue Consumer 1: Processing 8 from FIFO Queue ... ``` Implementation Skeleton ```python import queue import threading import time import random def producer(queue_type, name): while True: item = random.randint(1, 100) try: queue_type.put(item, timeout=1) print(f\\"Producer {name}: Added {item} to {queue_type.__class__.__name__}\\") time.sleep(1) except queue.Full: print(f\\"Producer {name}: Queue is full, waiting...\\") time.sleep(1) def consumer(queue_type, name): while True: try: item = queue_type.get(timeout=1) print(f\\"Consumer {name}: Processing {item} from {queue_type.__class__.__name__}\\") time.sleep(random.uniform(0.5, 2)) queue_type.task_done() except queue.Empty: print(f\\"Consumer {name}: Queue is empty, waiting...\\") time.sleep(1) def main(): # Uncomment the queue type you want to test # q = queue.Queue(maxsize=10) # FIFO queue # q = queue.LifoQueue(maxsize=10) # LIFO queue q = queue.PriorityQueue(maxsize=10) # Priority queue # Create producer threads producer_threads = [threading.Thread(target=producer, args=(q, i+1)) for i in range(2)] # Create consumer threads consumer_threads = [threading.Thread(target=consumer, args=(q, i+1)) for i in range(2)] # Start all threads for t in producer_threads + consumer_threads: t.daemon = True t.start() # Keep the main thread running try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Program terminated\\") if __name__ == \\"__main__\\": main() ``` # Note You are encouraged to explore the `queue` module\'s documentation to enhance the capabilities of your code. Ensure to test thoroughly under different conditions.","solution":"import queue import threading import time import random def producer(queue_type, name): while True: item = random.randint(1, 100) try: queue_type.put(item, timeout=1) print(f\\"Producer {name}: Added {item} to {queue_type.__class__.__name__}\\") time.sleep(1) except queue.Full: print(f\\"Producer {name}: Queue is full, waiting...\\") time.sleep(1) def consumer(queue_type, name): while True: try: item = queue_type.get(timeout=1) print(f\\"Consumer {name}: Processing {item} from {queue_type.__class__.__name__}\\") time.sleep(random.uniform(0.5, 2)) queue_type.task_done() except queue.Empty: print(f\\"Consumer {name}: Queue is empty, waiting...\\") time.sleep(1) def main(): # Uncomment the queue type you want to test # q = queue.Queue(maxsize=10) # FIFO queue # q = queue.LifoQueue(maxsize=10) # LIFO queue q = queue.PriorityQueue(maxsize=10) # Priority queue # Create producer threads producer_threads = [threading.Thread(target=producer, args=(q, i+1)) for i in range(2)] # Create consumer threads consumer_threads = [threading.Thread(target=consumer, args=(q, i+1)) for i in range(2)] # Start all threads for t in producer_threads + consumer_threads: t.daemon = True t.start() # Keep the main thread running try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Program terminated\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question You are tasked with designing a simple chat server and client using Python\'s `socket` module. The chat server should be capable of handling multiple clients simultaneously and allow them to send and receive messages in real-time. Implement both the server and the client. Requirements 1. The server must be able to accept multiple client connections and manage their messages concurrently. 2. Each client should be able to send messages to the server, which then broadcasts the message to all connected clients. 3. Ensure proper handling of client disconnections and errors to avoid crashing the server. 4. Use non-blocking sockets or multiplexing (e.g., using `select` or `selectors` module) to handle multiple clients efficiently. Server Implementation * **Input**: The server should take no inputs and should run indefinitely until manually terminated. * **Output**: The server does not produce output but should print status messages indicating connections and disconnections of clients. Client Implementation * **Input**: The client takes user input via the command line to send messages to the server. * **Output**: The client displays messages received from the server in real-time. Constraints * The server should be designed to run on `localhost` and listen on a specified port. * The client should be designed to connect to the server running on `localhost` on the specified port. * Handle concurrency without using threading libraries explicitly. Example Here is an example of how the server and client can be run: 1. Start the server: ```sh python chat_server.py ``` 2. Start a client: ```sh python chat_client.py ``` 3. Multiple clients can be started to join the chat room. Performance Considerations - The server should handle up to 100 clients simultaneously. - The implementation should be efficient in handling I/O operations without significant delays. Implement the `chat_server.py` and `chat_client.py` files to satisfy the provided requirements.","solution":"# chat_server.py import socket import selectors # Create a selector object sel = selectors.DefaultSelector() clients = {} HOST = \'localhost\' PORT = 12345 def accept(sock): conn, addr = sock.accept() # Should be ready to read print(\'Connected by\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn): data = conn.recv(1024) # Should be ready to read if data: broadcast(data, conn) else: print(\'Disconnected by\', conn) sel.unregister(conn) conn.close() def broadcast(message, connection): for client in clients: if client != connection: client.send(message) def main(): server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_sock.bind((HOST, PORT)) server_sock.listen() server_sock.setblocking(False) sel.register(server_sock, selectors.EVENT_READ, accept) print(f\'Server started on {HOST}:{PORT}\') while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj) if __name__ == \\"__main__\\": main() # chat_client.py import socket import selectors import sys import threading HOST = \'localhost\' PORT = 12345 sel = selectors.DefaultSelector() def read(conn): while True: data = conn.recv(1024) if not data: break print(data.decode()) def main(): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect((HOST, PORT)) receive_thread = threading.Thread(target=read, args=(sock,)) receive_thread.daemon = True receive_thread.start() print(\'Connected to the chat server.\') try: while True: message = input(\'\') sock.sendall(message.encode()) except KeyboardInterrupt: print(\\"Disconnected.\\") finally: sock.close() if __name__ == \\"__main__\\": main()"},{"question":"**Problem Description: Statistical Visualization and Analysis with Pandas** You are provided with a dataset containing some fictitious sales data for a retail company. Each entry in the dataset represents daily sales data for different product categories over a period of one year. Your task is to perform statistical analysis and create visualizations using Pandas\' charting capabilities to derive insights from the data. The data is stored in a CSV file named `sales_data.csv` with the following columns: - `date`: Date of sales (YYYY-MM-DD format). - `product_id`: Unique identifier for the product. - `category`: Category of the product (e.g., Furniture, Technology, Office Supplies). - `sales`: Amount of sales in dollars. **Requirements:** 1. **Data Loading:** - Load the data into a pandas DataFrame and ensure that the `date` column is treated as a datetime object. 2. **Data Cleaning:** - Handle any missing values in a way that does not distort the data (e.g., by removing rows with missing `sales` values). 3. **Daily Sales Analysis:** - Calculate the total daily sales for each category. - Create a time series line plot showing the total daily sales for each category, ensuring the x-axis is properly formatted to represent dates. 4. **Sales Distribution:** - Produce a histogram to show the distribution of daily sales amounts for each product category. 5. **Product Comparison:** - Select the top 5 products based on the total sales over the year. - Create a bar plot to compare the total sales of these top 5 products. 6. **Sales Trends:** - Plot a scatter plot with a trend line (e.g., using kernel density estimates or regression) to show the relationship between sales and time for the entire dataset. 7. **Advanced Analysis (Optional):** - Create an autocorrelation plot to determine if there are any patterns or periodicity in the sales data. - Generate a scatter matrix plot to visualize correlations between different numeric attributes (if any other numeric data is available). **Function Signature:** ```python def analyze_and_visualize_sales_data(file_path: str) -> None: # Implement your code here pass ``` # Constraints: - The file `sales_data.csv` will be available in the same directory as your script. - Use only Pandas and Matplotlib for your visualizations. # Expected Output: - The function should save the generated plots as PNG files in the current working directory with appropriate filenames (e.g., `daily_sales.png`, `sales_distribution.png`, `top_products.png`, `sales_trends.png`). Your implementation should demonstrate your ability to effectively use pandas for data manipulation as well as matplotlib for visualization, adhering to the examples provided in the documentation. **Note**: Ensure your code is well-documented and follows best coding practices.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def analyze_and_visualize_sales_data(file_path: str) -> None: # Load the data into a pandas DataFrame and ensure the \'date\' column is treated as a datetime object sales_data = pd.read_csv(file_path) sales_data[\'date\'] = pd.to_datetime(sales_data[\'date\']) # Handle any missing values by removing rows with missing \'sales\' values sales_data.dropna(subset=[\'sales\'], inplace=True) # Calculate the total daily sales for each category daily_sales = sales_data.groupby([\'date\', \'category\']).sales.sum().unstack().fillna(0) # Create a time series line plot for the total daily sales for each category daily_sales.plot(figsize=(14, 8), title=\'Total Daily Sales by Category\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales ()\') plt.grid(True) plt.savefig(\'daily_sales.png\') plt.close() # Produce a histogram showing the distribution of daily sales amounts for each product category categories = sales_data[\'category\'].unique() plt.figure(figsize=(14, 8)) for category in categories: sns.histplot(sales_data[sales_data[\'category\'] == category][\'sales\'], label=category, kde=True) plt.xlabel(\'Sales ()\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of Daily Sales Amounts by Category\') plt.legend(title=\'Category\') plt.savefig(\'sales_distribution.png\') plt.close() # Select the top 5 products based on the total sales over the year top_products = sales_data.groupby(\'product_id\').sales.sum().nlargest(5) # Create a bar plot to compare the total sales of these top 5 products top_products.plot(kind=\'bar\', figsize=(14, 8), title=\'Top 5 Products by Total Sales\') plt.xlabel(\'Product ID\') plt.ylabel(\'Total Sales ()\') plt.savefig(\'top_products.png\') plt.close() # Plot a scatter plot with a trend line showing the relationship between sales and time for the entire dataset plt.figure(figsize=(14, 8)) sns.regplot(x=sales_data[\'date\'].map(pd.Timestamp.toordinal), y=sales_data[\'sales\'], marker=\'.\') plt.xlabel(\'Time\') plt.ylabel(\'Sales ()\') plt.title(\'Sales Trends Over Time\') plt.savefig(\'sales_trends.png\') plt.close()"},{"question":"**Objective**: You are tasked with implementing a Python function that simulates some of the C/API interactions for Python integer objects described in the documentation. Problem Statement Write a Python function `simulate_integer_conversion(input_value, convert_to)` that takes an input value and a string specifying the type to convert to. The function should implement conversions similar to the provided documentation and return the appropriate integer representation, handling any potential errors as specified. Function Signature ```python def simulate_integer_conversion(input_value: any, convert_to: str) -> any: pass ``` Supported Conversion Types The `convert_to` parameter should accept the following values: - `\'long\'`: Convert to a Python integer similar to `PyLong_FromLong`. - `\'unsigned_long\'`: Convert to Python integer ensuring it represents an unsigned long. - `\'double\'`: Convert to a Python integer taken from the integer part of a double. - `\'string\'`: Convert from a string representation of a number to a Python integer. - `\'void_ptr\'`: Simulate conversion from and to a void pointer. Input Constraints - `input_value` can be an integer, float, string, or a `void_ptr` (simulated by an integer). - For `\'unsigned_long\'`, `input_value` must be non-negative. - For `\'string\'`, `input_value` must represent a valid integer in string format. - For `\'void_ptr\'`, `input_value` must be an integer (simulating pointer behavior). Output - Return the Python integer representation based on conversion. - Return `None` if there is any error during the conversion. Error Handling - Raise `ValueError` if the conversion is not possible due to value constraints. - Return `None` if an unsupported conversion type is provided. Examples ```python # Example 1: Conversion from long to Python integer print(simulate_integer_conversion(42, \'long\')) # Output: 42 # Example 2: Conversion from unsigned long to Python integer print(simulate_integer_conversion(42, \'unsigned_long\')) # Output: 42 # Example 3: Conversion from double to Python integer print(simulate_integer_conversion(42.8, \'double\')) # Output: 42 # Example 4: Conversion from string to Python integer print(simulate_integer_conversion(\\"42\\", \'string\')) # Output: 42 # Example 5: Conversion from void pointer to Python integer print(simulate_integer_conversion(42, \'void_ptr\')) # Output: 42 # Example 6: Conversion from void pointer to Python integer with invalid type print(simulate_integer_conversion(42, \'unknown_type\')) # Output: None ```","solution":"def simulate_integer_conversion(input_value, convert_to): Simulates integer conversion based on the specified type. try: if convert_to == \'long\': if isinstance(input_value, int): return input_value else: raise ValueError(\\"input_value should be an int for \'long\' conversion\\") elif convert_to == \'unsigned_long\': if isinstance(input_value, int) and input_value >= 0: return input_value else: raise ValueError(\\"input_value should be a non-negative int for \'unsigned_long\' conversion\\") elif convert_to == \'double\': if isinstance(input_value, (float, int)): return int(input_value) else: raise ValueError(\\"input_value should be a float or int for \'double\' conversion\\") elif convert_to == \'string\': if isinstance(input_value, str): return int(input_value) else: raise ValueError(\\"input_value should be a string for \'string\' conversion\\") elif convert_to == \'void_ptr\': if isinstance(input_value, int): return input_value else: raise ValueError(\\"input_value should be an int for \'void_ptr\' conversion\\") else: return None # Unsupported conversion type except (ValueError, TypeError): return None"},{"question":"**Question: Pairwise Metrics and Kernels** You are provided with two 2D numpy arrays, `X` and `Y`, each representing a set of samples with features. Your task is to implement a function `compute_similarities` that computes various similarity scores between the samples in `X` and `Y` using several built-in functions from `sklearn.metrics.pairwise`. # Function Signature ```python def compute_similarities(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: ``` # Input - `X`: A 2D numpy array of shape `(n_samples_X, n_features)`. - `Y`: A 2D numpy array of shape `(n_samples_Y, n_features)`. # Output A tuple containing five 2D numpy arrays (all of shape `(n_samples_X, n_samples_Y)`): 1. `cosine_sim`: Pairwise cosine similarity between `X` and `Y`. 2. `linear_ker`: Pairwise linear kernel values between `X` and `Y`. 3. `polynomial_ker`: Pairwise polynomial kernel values between `X` and `Y` with degree 3. 4. `rbf_ker`: Pairwise RBF (Gaussian) kernel values between `X` and `Y` with `gamma=0.1`. 5. `chi2_ker`: Pairwise chi-squared kernel values between `X` and `Y` with `gamma=0.5`. # Example ```python import numpy as np from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, polynomial_kernel, rbf_kernel, chi2_kernel def compute_similarities(X: np.ndarray, Y: np.ndarray): cosine_sim = cosine_similarity(X, Y) linear_ker = linear_kernel(X, Y) polynomial_ker = polynomial_kernel(X, Y, degree=3) rbf_ker = rbf_kernel(X, Y, gamma=0.1) chi2_ker = chi2_kernel(X, Y, gamma=0.5) return (cosine_sim, linear_ker, polynomial_ker, rbf_ker, chi2_ker) # Usage: X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) result = compute_similarities(X, Y) for res in result: print(res) ``` # Explanation Your function should utilize the imported functions from `sklearn.metrics.pairwise` to compute the required pairwise similarities or kernel values and return them as a tuple in the specified order. This will validate your understanding and application of the different kernel and similarity measures provided by `scikit-learn`. # Constraints: 1. You may assume `X` and `Y` have non-negative entries only. 2. You must use the specified `gamma` values for `rbf_kernel` and `chi2_kernel`.","solution":"import numpy as np from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, polynomial_kernel, rbf_kernel, chi2_kernel def compute_similarities(X: np.ndarray, Y: np.ndarray) -> tuple: cosine_sim = cosine_similarity(X, Y) linear_ker = linear_kernel(X, Y) polynomial_ker = polynomial_kernel(X, Y, degree=3) rbf_ker = rbf_kernel(X, Y, gamma=0.1) chi2_ker = chi2_kernel(X, Y, gamma=0.5) return (cosine_sim, linear_ker, polynomial_ker, rbf_ker, chi2_ker)"},{"question":"Using the scikit-learn library, you are required to design a data preprocessing pipeline to handle missing data and perform a classification task. Dataset You will use the iris dataset and simulate missing values. Tasks 1. **Load the Iris Dataset**: Load the Iris dataset and introduce missing values randomly. 2. **Create a Preprocessing Pipeline**: - Impute the missing values using `SimpleImputer` for numerical data. - Add binary indicators for the imputed values using `MissingIndicator`. 3. **Build and Train a Classifier**: - Combine the preprocessing steps with a `DecisionTreeClassifier` in a pipeline. 4. **Model Evaluation**: - Split the data into training and testing sets. - Train the classifier on the training set. - Evaluate the classifier on the testing set and report the accuracy. Constraints - You must use `SimpleImputer` for imputation with strategy=\'mean\'. - The pipeline should handle the addition of imputation indicators using `MissingIndicator`. - Keep the original number of features using `keep_empty_features=True` if you have fully missing features. Input Format No input needed. Output Format Print the shape of the transformed feature matrix and the classification accuracy on the test set. Example ```python import numpy as np from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import FeatureUnion, make_pipeline from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split # 1. Load the Iris dataset and introduce missing values randomly iris = load_iris() X, y = iris.data, iris.target np.random.seed(0) mask = np.random.rand(*X.shape) < 0.1 X[mask] = np.nan # 2. Create a Preprocessing Pipeline transformer = FeatureUnion( transformer_list=[ (\'features\', SimpleImputer(strategy=\'mean\', keep_empty_features=True)), (\'indicators\', MissingIndicator()) ]) # 3. Build and Train a Classifier pipeline = make_pipeline(transformer, DecisionTreeClassifier()) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) pipeline.fit(X_train, y_train) # 4. Model Evaluation print(f\\"Transformed feature matrix shape: {pipeline.named_steps[\'featureunion\'].transform(X_test).shape}\\") print(f\\"Classification accuracy: {pipeline.score(X_test, y_test):.2f}\\") ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import FeatureUnion, make_pipeline from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split # Load the Iris dataset and introduce missing values randomly def load_data_with_missing_values(): iris = load_iris() X, y = iris.data, iris.target np.random.seed(0) mask = np.random.rand(*X.shape) < 0.1 X[mask] = np.nan return X, y # Create the preprocessing and classification pipeline def create_pipeline(): transformer = FeatureUnion( transformer_list=[ (\'features\', SimpleImputer(strategy=\'mean\')), (\'indicators\', MissingIndicator()) ], n_jobs=None # To ensure the use of a shared computation job per transformer section ) pipeline = make_pipeline(transformer, DecisionTreeClassifier()) return pipeline # Train the classifier and evaluate it def train_and_evaluate(): X, y = load_data_with_missing_values() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) pipeline = create_pipeline() pipeline.fit(X_train, y_train) transformed_test_features = pipeline.named_steps[\'featureunion\'].transform(X_test) accuracy = pipeline.score(X_test, y_test) print(f\\"Transformed feature matrix shape: {transformed_test_features.shape}\\") print(f\\"Classification accuracy: {accuracy:.2f}\\") return transformed_test_features.shape, accuracy"},{"question":"# XML Parsing with SAX2 in Python You are tasked with parsing an XML document containing a catalog of books using the SAX2 API provided by Python\'s `xml.sax` package. The XML document has the following structure: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book entries --> </catalog> ``` Task 1. Implement a custom `ContentHandler` class that parses the XML document and extracts: - The title of each book. - The price of each book. 2. Print the extracted data in the following format: ``` Title: [Title of the book] Price: [Price of the book] ``` Requirements 1. Your handler should extend the `xml.sax.handler.ContentHandler` class. 2. Use the `xml.sax` module\'s convenience function `xml.sax.parseString` to parse the XML string. 3. Handle potential parsing errors by catching exceptions and providing a meaningful error message. 4. Your code should be efficient, and the time complexity should be O(n), where n is the size of the XML content. Constraints - You may assume that the XML structure is well-formed. - The `<catalog>` element contains multiple `<book>` elements, each with the described child elements. Example Input ```python xml_data = <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> ``` Example Output ``` Title: XML Developer\'s Guide Price: 44.95 Title: Midnight Rain Price: 5.95 ``` Grading Criteria - Correct implementation of the `ContentHandler` class. - Accurate extraction and printing of book titles and prices. - Handling of exceptions appropriately. - Adherence to the specified output format. ```python import xml.sax # Implement your custom ContentHandler class here class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): ... # Initialization code here def startElement(self, name, attrs): ... # Code to handle start of an element here def endElement(self, name): ... # Code to handle end of an element here def characters(self, content): ... # Code to handle character data here # Use the xml.sax.parseString function to parse the XML data try: xml.sax.parseString(xml_data, BookHandler()) except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e.getMessage()}\\") ```","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.price = \\"\\" self.in_title = False self.in_price = False def startElement(self, tag, attributes): self.current_data = tag if tag == \\"title\\": self.in_title = True if tag == \\"price\\": self.in_price = True def endElement(self, tag): if self.in_title and self.current_data == \\"title\\": print(f\\"Title: {self.title}\\") self.in_title = False self.title = \\"\\" if self.in_price and self.current_data == \\"price\\": print(f\\"Price: {self.price}\\") self.in_price = False self.price = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content if self.current_data == \\"price\\": self.price += content def parse_book_data(xml_data): handler = BookHandler() try: xml.sax.parseString(xml_data, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e.getMessage()}\\")"},{"question":"# Coding Assessment: Implement a Custom Python Profiler using the `sys` module Objective Your task is to implement a Python profiling utility that uses the `sys.setprofile` functionality to track function call durations within a given Python script. Profiling is essential for performance optimization and understanding the runtime behavior of a codebase. Requirements 1. Implement a profiling tool that: - Tracks the start and end times of each function call. - Calculates the duration of each function execution. - Logs this information to a `profile.txt` file. 2. Your profiler should handle the following events: - `call`: When a function is called. - `return`: When a function returns. 3. The profiler should only log information for the specific script provided and should not affect other modules or the Python interpreter environment. Constraints - Assume only CPU-bound computations. - Handle nested function calls appropriately. - Ignore exception handling within the profiler. Function Specifications Implement a function `enable_profiling` to start profiling and `disable_profiling` to end profiling and log the results to `profile.txt`. ```python import sys import time def profile(frame, event, arg): pass # Implement your profiler logic here def enable_profiling(): sys.setprofile(profile) def disable_profiling(): sys.setprofile(None) # Example usage: # enable_profiling() # # Insert the code you want to profile here # disable_profiling() ``` Input - The function does not take any inputs directly. It profiles the code that runs while profiling is enabled. Output - A file `profile.txt` should be created containing logs of function call timings in the format: ``` function_name: start_time -> end_time, duration ``` Example Execution and Expected Results ```python def test_function(): time.sleep(2) enable_profiling() test_function() disable_profiling() ``` The `profile.txt` should contain: ``` test_function: 1639597362.123456 -> 1639597364.123456, 2.0 ``` Your task is to complete the `profile` function inside the template provided.","solution":"import sys import time # Dictionary to keep track of the start time of each function call profile_data = {} def profile(frame, event, arg): function_name = frame.f_code.co_name if event == \\"call\\": # Record the start time when a function is called profile_data[function_name] = time.time() elif event == \\"return\\": # Record the end time when a function returns start_time = profile_data.pop(function_name, None) if start_time is not None: end_time = time.time() duration = end_time - start_time with open(\\"profile.txt\\", \\"a\\") as f: f.write(f\\"{function_name}: {start_time} -> {end_time}, {duration}n\\") def enable_profiling(): sys.setprofile(profile) def disable_profiling(): sys.setprofile(None)"},{"question":"# Data Analysis and Model Evaluation Using Scikit-Learn You have been given a dataset from the scikit-learn toy datasets. Your task is to perform the following steps using the `load_wine` dataset: 1. Load the Wine dataset using the appropriate scikit-learn function. 2. Perform basic exploratory data analysis (EDA), including: - Displaying the first 5 rows of the dataset. - Displaying summary statistics of the dataset. 3. Preprocess the data: - Split the data into training and test sets (80% training, 20% testing). - Standardize the feature columns using `StandardScaler`. 4. Train a Logistic Regression model using the training data. 5. Evaluate the model\'s performance using the test data: - Display the accuracy score. - Display the confusion matrix. # Instructions 1. Implement a function called `load_and_preprocess_data` that performs steps 1-3. - **Input**: None. - **Output**: Tuple containing the training features, test features, training labels, and test labels. 2. Implement a function called `train_and_evaluate_model` that performs steps 4-5. - **Input**: Training features, test features, training labels, and test labels. - **Output**: None (Print the results directly within the function). # Example ```python def load_and_preprocess_data(): # Step 1: Load the dataset from sklearn.datasets import load_wine data = load_wine() # Step 2: Perform EDA import pandas as pd df = pd.DataFrame(data.data, columns=data.feature_names) df[\'target\'] = data.target print(\\"First 5 rows of the dataset:\\") print(df.head()) print(\\"nSummary statistics:\\") print(df.describe()) # Step 3: Preprocess the data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler X = df.drop(columns=\'target\') y = df[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate_model(X_train, X_test, y_train, y_test): # Step 4: Train a Logistic Regression model from sklearn.linear_model import LogisticRegression model = LogisticRegression(max_iter=10000) model.fit(X_train, y_train) # Step 5: Evaluate the model from sklearn.metrics import accuracy_score, confusion_matrix y_pred = model.predict(X_test) print(\\"Accuracy Score: \\", accuracy_score(y_test, y_pred)) print(\\"Confusion Matrix:n\\", confusion_matrix(y_test, y_pred)) # Example usage: X_train, X_test, y_train, y_test = load_and_preprocess_data() train_and_evaluate_model(X_train, X_test, y_train, y_test) ``` # Constraints - Use only the scikit-learn library for your implementation. - Ensure that your code is well-documented and follows best coding practices. This question assesses the student\'s ability to effectively use scikit-learn for loading datasets, performing EDA, preprocessing data, training machine learning models, and evaluating their performance.","solution":"def load_and_preprocess_data(): Loads the Wine dataset, performs EDA, and preprocesses the data by splitting into training and test sets, and standardizing the feature columns. Returns: X_train (array): Training features X_test (array): Test features y_train (array): Training labels y_test (array): Test labels # Step 1: Load the dataset from sklearn.datasets import load_wine data = load_wine() # Step 2: Perform EDA import pandas as pd df = pd.DataFrame(data.data, columns=data.feature_names) df[\'target\'] = data.target print(\\"First 5 rows of the dataset:\\") print(df.head()) print(\\"nSummary statistics:\\") print(df.describe()) # Step 3: Preprocess the data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler X = df.drop(columns=\'target\') y = df[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate_model(X_train, X_test, y_train, y_test): Trains a Logistic Regression model using the training data and evaluates its performance using the test data. Prints the accuracy score and confusion matrix. Args: X_train (array): Training features X_test (array): Test features y_train (array): Training labels y_test (array): Test labels # Step 4: Train a Logistic Regression model from sklearn.linear_model import LogisticRegression model = LogisticRegression(max_iter=10000) model.fit(X_train, y_train) # Step 5: Evaluate the model from sklearn.metrics import accuracy_score, confusion_matrix y_pred = model.predict(X_test) print(\\"Accuracy Score: \\", accuracy_score(y_test, y_pred)) print(\\"Confusion Matrix:n\\", confusion_matrix(y_test, y_pred)) # Example usage: X_train, X_test, y_train, y_test = load_and_preprocess_data() train_and_evaluate_model(X_train, X_test, y_train, y_test)"},{"question":"**Objective:** Demonstrate your understanding of the seaborn library by generating a comprehensive data visualization using multiple features of seaborn. Problem Statement You are tasked with creating a data visualization that analyzes and compares health expenditures and life expectancy across various countries over a range of years. Utilize the seaborn library to produce a plot that highlights trends and insights from the data. Input - Use the `healthexp` dataset from seaborn\'s built-in datasets. - The dataset consists of the following columns: - `Country`: Name of the country. - `Year`: Year of observation. - `Spending_USD`: Health expenditure in USD. - `Life_Expectancy`: Life expectancy in years. Output - Output a plot with the following features: 1. A line plot showing the relationship between `Spending_USD` and `Life_Expectancy` for each country. 2. Different countries should be plotted with different colors for distinction. 3. Add markers to the data points for better readability. 4. Customize the line properties such as `linewidth` and marker properties like `size` and `color`. 5. Add a title and axis labels to the plot. Constraints - You must use seaborn\'s `seaborn.objects.Plot` and `seaborn.objects.Path` classes to generate the plot. - Ensure that the plot is clear, well-labeled, and informative. Additional requirements - Write a brief description (~100 words) of the insights you can gather from the generated plot. ```python # Sample starting code import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Customize and display the plot p.title(\\"Health Expenditure vs Life Expectancy\\") p.set(xlabel=\\"Health Expenditure (USD)\\", ylabel=\\"Life Expectancy (Years)\\") p.show() # Write a brief description of the insights insights = [Write your insights here] ``` Fill in the code template and the insights section to complete this question.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_health_expenditure_vs_life_expectancy(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = sns.relplot( x=\'Spending_USD\', y=\'Life_Expectancy\', kind=\'line\', data=healthexp, hue=\'Country\', marker=\'o\', linewidth=1, facet_kws={\'legend_out\': True} ) # Customize the plot p.fig.suptitle(\\"Health Expenditure vs Life Expectancy\\", y=1.02) p.set_axis_labels(\\"Health Expenditure (USD)\\", \\"Life Expectancy (Years)\\") plt.show() plot_health_expenditure_vs_life_expectancy() # Insights insights = This plot shows the relationship between health expenditure (in USD) and life expectancy (in years) for various countries. From the plot, it is evident that most countries have a positive correlation between increased health spending and higher life expectancy. By examining the individual country trends, specific differences and similarities in public health investments and outcomes can be inferred."},{"question":"**Coding Assessment Question: Asynchronous I/O Multiplexing with `select` Module** **Problem Statement:** You are tasked with implementing a basic server that supports multiple clients simultaneously. The server should use the `select` module to handle asynchronous I/O operations effectively. Your implementation should: 1. Accept multiple client connections. 2. Efficiently monitor these connections for incoming data and handle their I/O operations without blocking the server. 3. Echo the received messages back to the clients. 4. Handle client disconnections gracefully. **Detailed Requirements:** 1. **Function Signature:** ```python def start_server(host: str, port: int) -> None: ``` 2. **Inputs:** - `host` (str): The hostname or IP address on which the server should listen. - `port` (int): The port number on which the server should listen. 3. **Functionality:** - The server should listen on the specified `host` and `port` for incoming client connections. - Use the `select` module to monitor the server socket and all connected client sockets for readability. - When a client socket is readable, read the data and echo it back to the client. - If a client disconnects, ensure the socket is properly closed and removed from the monitoring list. 4. **Constraints:** - The server should continue running indefinitely until manually stopped. - Handle errors gracefully, ensuring resources are properly cleaned up. **Example:** Here is a minimal example illustrating the expected usage: ```python if __name__ == \\"__main__\\": start_server(\\"127.0.0.1\\", 8888) ``` **Notes:** - You may assume that the provided host and port are valid. - The solution should efficiently handle multiple clients with minimal delay. Use the capabilities described in the `select` module documentation to implement your solution, ensuring it is robust and handles multiple clients concurrently. **Performance Requirements:** - Your implementation should be efficient, not causing significant delays in handling multiple connections. - Properly manage resources to avoid leaks or unnecessary computational overhead. Good luck!","solution":"import socket import select def start_server(host: str, port: int) -> None: server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) sockets_list = [server_socket] clients = {} print(f\\"Server started on {host}:{port}\\") try: while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address[0]}:{client_address[1]}\\") else: message = notified_socket.recv(1024) if not message: print(f\\"Closed connection from {clients[notified_socket][0]}:{clients[notified_socket][1]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() else: print(f\\"Received message from {clients[notified_socket][0]}:{clients[notified_socket][1]}: {message.decode(\'utf-8\')}\\") notified_socket.send(message) for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() finally: for sock in sockets_list: sock.close()"},{"question":"**Objective:** Assess students\' understanding of using the seaborn library to visualize and analyze relationships between variables with different regression techniques. **Question:** Suppose you are given a dataset containing information about various car models, including their weight, acceleration, miles per gallon (mpg), horsepower, and origin. Using seaborn, you need to perform the following tasks to explore the relationships between these variables. **Dataset Description:** - The dataset is available as a seaborn dataset named `mpg`. - It contains the following columns: - `mpg`: miles per gallon - `cylinders`: number of cylinders - `displacement`: displacement in cubic inches - `horsepower`: horsepower of the car - `weight`: weight of the car - `acceleration`: time taken to accelerate (seconds) - `model year`: year of the car model - `origin`: origin of the car (e.g., \'usa\', \'europe\', \'japan\') **Tasks:** 1. **Basic Regression Plot:** Plot the relationship between `weight` and `acceleration` using a simple linear regression. Include the regression line and the confidence interval. 2. **Polynomial Regression:** Fit and plot a second-order polynomial regression to show the relationship between `weight` and `mpg`. 3. **Log-Linear Regression:** Plot the relationship between `displacement` and `mpg` using a log-linear regression. 4. **LOWESS Smoothing:** Plot the relationship between `horsepower` and `mpg` using a locally-weighted (LOWESS) smoother. 5. **Robust Regression:** Fit and plot a robust regression to depict the relationship between `horsepower` and `weight`. 6. **Discrete Variable Handling:** - Plot the relationship between `cylinders` and `weight` with the `x` variable jittered. - Aggregate and plot the relationship between `cylinders` and `acceleration`, using the mean of `acceleration` for each distinct value of `cylinders`, and fitting a second-order polynomial regression. 7. **Customization:** Customize the basic regression plot from Task 1 by modifying the following: - Set confidence interval to 99%. - Use a cross (`x`) marker for scatter points. - Set the scatter color to grey (`.3`) and the regression line color to red. **Submission Requirements:** - Write the code for each task in separate cells or sections of a Jupyter notebook. - Ensure your plots are well-labeled with titles, axis labels, and legends (if necessary). - Provide a brief explanation of each plot and the observed relationships based on the visualized data. **Constraints:** - You are expected to use only the seaborn library for plotting but may use other libraries such as numpy or pandas for data manipulation if needed. - Plots should be visually clear and informative. - Ensure the code is modular and reusable wherever possible. **Performance Requirements:** - The code should execute efficiently for the given dataset size. - Avoid redundant computations and unnecessary re-processing of the dataset. **Example Output:** ```python import seaborn as sns import numpy as np # Task 1: Basic Regression Plot sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") # Task 2: Polynomial Regression sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) # Task 3: Log-Linear Regression sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) # Task 4: LOWESS Smoothing sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) # Task 5: Robust Regression sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"weight\\", robust=True) # Task 6: Discrete Variable Handling sns.regplot(data=mpg, x=\\"cylinders\\", y=\\"weight\\", x_jitter=.15) sns.regplot(data=mpg, x=\\"cylinders\\", y=\\"acceleration\\", x_estimator=np.mean, order=2) # Task 7: Customization sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"r\\")) ```","solution":"import seaborn as sns import numpy as np import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") def plot_basic_regression(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") plt.title(\'Basic Regression Plot: Weight vs Acceleration\') plt.xlabel(\'Weight\') plt.ylabel(\'Acceleration\') plt.show() def plot_polynomial_regression(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\'Polynomial Regression: Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'Miles per Gallon (mpg)\') plt.show() def plot_log_linear_regression(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.title(\'Log-Linear Regression: Displacement vs MPG\') plt.xlabel(\'Displacement (log scale)\') plt.ylabel(\'Miles per Gallon (mpg)\') plt.show() def plot_lowess_smoothing(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.title(\'LOWESS Smoothing: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Miles per Gallon (mpg)\') plt.show() def plot_robust_regression(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"weight\\", robust=True) plt.title(\'Robust Regression: Horsepower vs Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.show() def plot_discrete_variable_handling(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"cylinders\\", y=\\"weight\\", x_jitter=.15) plt.title(\'Discrete Variable Handling: Cylinders vs Weight with Jitter\') plt.xlabel(\'Cylinders\') plt.ylabel(\'Weight\') plt.show() plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"cylinders\\", y=\\"acceleration\\", x_estimator=np.mean, order=2) plt.title(\'Discrete Variable Handling: Cylinders vs Acceleration with Mean Aggregation\') plt.xlabel(\'Cylinders\') plt.ylabel(\'Acceleration (Mean)\') plt.show() def plot_customization(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"red\\")) plt.title(\'Customized Plot: Weight vs Acceleration with 99% CI\') plt.xlabel(\'Weight\') plt.ylabel(\'Acceleration\') plt.show()"},{"question":"Objective: To assess the understanding of creating and manipulating source distributions using `setuptools` in Python. This question requires implementing a function that configures a source distribution according to specific inclusion and exclusion criteria. Question: You are tasked with writing a function `configure_sdist` that automates the inclusion and exclusion of files for a source distribution of a Python project. The function should take three parameters: - `include_patterns` (list of strings): A list of glob patterns for files that should be included in the distribution. - `exclude_patterns` (list of strings): A list of glob patterns for files that should be excluded from the distribution. - `directory` (string): The base directory of the source tree from which files should be included or excluded. The function should generate a `MANIFEST.in` file in the specified directory, containing the appropriate include and exclude commands based on the provided patterns. Constraints: 1. If a pattern in `include_patterns` is already excluded by `exclude_patterns`, it should not be included. 2. The `MANIFEST.in` file must follow the correct format based on the commands described in the documentation. 3. You may assume that all patterns and the directory path are valid strings. 4. The resulting `MANIFEST.in` file should be properly formatted, ensuring no duplicate patterns. Function Signature: ```python def configure_sdist(include_patterns: list, exclude_patterns: list, directory: str) -> None: pass ``` Example: ```python include_patterns = [\\"*.py\\", \\"data/*.csv\\"] exclude_patterns = [\\"tests/*\\", \\"*.md\\"] directory = \\"/path/to/project\\" configure_sdist(include_patterns, exclude_patterns, directory) ``` The above example should result in a `MANIFEST.in` file in `/path/to/project` containing: ``` include *.py include data/*.csv exclude tests/* exclude *.md ``` Notes: - You may use file handling to write to the `MANIFEST.in` file. - Ensure that inclusion and exclusion do not conflict. If a pattern is meant to be both included and excluded, prioritize exclusion.","solution":"def configure_sdist(include_patterns, exclude_patterns, directory): Configures the MANIFEST.in file for source distributions according to given include and exclude patterns. Args: - include_patterns (list of str): Patterns to include. - exclude_patterns (list of str): Patterns to exclude. - directory (str): The base directory of the source tree. manifest_lines = [] # Add exclude patterns first to ensure priority for pattern in exclude_patterns: manifest_lines.append(f\\"exclude {pattern}\\") # Add include patterns except those that are in exclude for pattern in include_patterns: if pattern not in exclude_patterns: manifest_lines.append(f\\"include {pattern}\\") # Write to MANIFEST.in in the specified directory manifest_path = f\\"{directory}/MANIFEST.in\\" with open(manifest_path, \\"w\\") as file: file.write(\\"n\\".join(manifest_lines) + \\"n\\")"},{"question":"# Unix Group Analysis Tool Problem Statement You are required to implement functions to retrieve, analyze, and manipulate Unix group data using the `grp` module. The functions should cover different aspects, from fetching information about specific groups to summarizing and filtering available groups based on various criteria. Implement the following functions: 1. **get_group_by_gid(gid: int) -> dict**: - **Input**: An integer `gid` representing the group ID. - **Output**: A dictionary with keys `name`, `passwd`, `gid`, and `members`, corresponding to the attributes of the group. - **Constraints**: Raise a `ValueError` if `gid` is not an integer. - **Example**: ```python get_group_by_gid(1000) ``` - If `gid` corresponds to a group with name `staff`, empty password, `gid` 1000, and members `[\'alice\', \'bob\']`, it should return: ```python { \\"name\\": \\"staff\\", \\"passwd\\": \\"\\", \\"gid\\": 1000, \\"members\\": [\\"alice\\", \\"bob\\"] } ``` 2. **get_group_by_name(name: str) -> dict**: - **Input**: A string `name` representing the group name. - **Output**: A dictionary similar to the previous function. - **Constraints**: Raise a `ValueError` if `name` is not a string. - **Example**: ```python get_group_by_name(\\"staff\\") ``` - If `name` corresponds to the group `staff` with empty password, `gid` 1000, and members `[\'alice\', \'bob\']`, it should return: ```python { \\"name\\": \\"staff\\", \\"passwd\\": \\"\\", \\"gid\\": 1000, \\"members\\": [\\"alice\\", \\"bob\\"] } ``` 3. **get_all_groups() -> list**: - **Output**: A list of dictionaries representing all groups. Each dictionary should contain the group attributes as specified above. - **Example**: ```python get_all_groups() ``` - If there are two groups: `staff` and `admin`, it should return: ```python [ { \\"name\\": \\"staff\\", \\"passwd\\": \\"\\", \\"gid\\": 1000, \\"members\\": [\\"alice\\", \\"bob\\"] }, { \\"name\\": \\"admin\\", \\"passwd\\": \\"x\\", \\"gid\\": 1001, \\"members\\": [\\"root\\"] } ] ``` 4. **filter_groups_by_member(member_name: str) -> list**: - **Input**: A string `member_name` representing the name of a user. - **Output**: A list of dictionaries for the groups that include the given member. - **Constraints**: Raise a `ValueError` if `member_name` is not a string. - **Example**: ```python filter_groups_by_member(\\"alice\\") ``` - If the user `alice` is a member of the groups `staff` and `dev`, it should return: ```python [ { \\"name\\": \\"staff\\", \\"passwd\\": \\"\\", \\"gid\\": 1000, \\"members\\": [\\"alice\\", \\"bob\\"] }, { \\"name\\": \\"dev\\", \\"passwd\\": \\"\\", \\"gid\\": 1002, \\"members\\": [\\"alice\\", \\"charlie\\"] } ] ``` 5. **get_users_with_multiple_groups() -> list**: - **Output**: A list of strings representing users who belong to more than one group. - **Example**: ```python get_users_with_multiple_groups() ``` - If there are users `alice` and `bob` who are members of multiple groups, it should return: ```python [\\"alice\\", \\"bob\\"] ``` These functions should handle errors gracefully and return results in the specified formats. Constraints - Use the `grp` module to access group information. - Ensure inputs are validated appropriately and handle exceptions as specified. - Optimize the implementations to handle a large number of groups and users efficiently.","solution":"import grp def get_group_by_gid(gid): Returns group information as a dictionary given a group ID (gid). if not isinstance(gid, int): raise ValueError(\\"gid must be an integer\\") try: group = grp.getgrgid(gid) return { \\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem } except KeyError: raise ValueError(f\\"No group found with GID: {gid}\\") def get_group_by_name(name): Returns group information as a dictionary given a group name. if not isinstance(name, str): raise ValueError(\\"name must be a string\\") try: group = grp.getgrnam(name) return { \\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem } except KeyError: raise ValueError(f\\"No group found with name: {name}\\") def get_all_groups(): Returns a list of dictionaries representing all groups. groups = [] for group in grp.getgrall(): groups.append({ \\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem }) return groups def filter_groups_by_member(member_name): Returns a list of dictionaries for groups that include the given member. if not isinstance(member_name, str): raise ValueError(\\"member_name must be a string\\") member_groups = [] for group in get_all_groups(): if member_name in group[\'members\']: member_groups.append(group) return member_groups def get_users_with_multiple_groups(): Returns a list of strings representing users who belong to more than one group. user_groups = {} for group in get_all_groups(): for member in group[\'members\']: if member in user_groups: user_groups[member].add(group[\'name\']) else: user_groups[member] = {group[\'name\']} return [user for user, groups in user_groups.items() if len(groups) > 1]"},{"question":"You are tasked with designing a machine learning pipeline using scikit-learn to handle a regression problem. Your goal is to assess students\' understanding of data preprocessing, model training, and evaluation using appropriate methods from scikit-learn. Objective Create a scikit-learn pipeline that performs the following steps: 1. Loads a synthetic regression dataset. 2. Splits the dataset into training and testing sets. 3. Scales the features using a standard scaler. 4. Trains a GradientBoostingRegressor model on the training data. 5. Evaluates the model on the test data and outputs the R^2 score. Requirements - The synthetic dataset should be created using `make_regression` from scikit-learn with `n_samples=1000` and `n_features=20`. - Use an 80-20 split for the training and testing data. - Ensure features are scaled using `StandardScaler` from scikit-learn. - Use `GradientBoostingRegressor` with `random_state=0` and `n_iter_no_change=5`. - Train the model on the training data and evaluate it using the R^2 score on the test data. - Ensure your code is well-commented and follows best practices for readability and maintainability. Expected Functions You should implement the following functions: ```python def create_synthetic_regression_data(n_samples=1000, n_features=20): Create a synthetic regression dataset. Parameters: - n_samples (int): The number of samples. - n_features (int): The number of features. Returns: - X (numpy.ndarray): Features. - y (numpy.ndarray): Target values. pass def train_test_split_and_preprocess(X, y, test_size=0.2, random_state=42): Split the dataset into training and testing sets, and scale the features. Parameters: - X (numpy.ndarray): Features. - y (numpy.ndarray): Target values. - test_size (float): The proportion of the dataset to include in the test split. - random_state (int): Controls the shuffling applied to the data. Returns: - X_train (numpy.ndarray): Training features. - X_test (numpy.ndarray): Testing features. - y_train (numpy.ndarray): Training target values. - y_test (numpy.ndarray): Testing target values. pass def train_and_evaluate_model(X_train, X_test, y_train, y_test): Train a GradientBoostingRegressor and evaluate its performance. Parameters: - X_train (numpy.ndarray): Training features. - X_test (numpy.ndarray): Testing features. - y_train (numpy.ndarray): Training target values. - y_test (numpy.ndarray): Testing target values. Returns: - r2_score (float): The R^2 score of the model on the test data. pass # Write the functionality for creating the dataset, splitting and scaling data, training, and evaluating the model below. ``` # Output Your final output should include: 1. The synthetic regression dataset creation. 2. Splitting and preprocessing steps. 3. Training and evaluating the GradientBoostingRegressor. 4. Printing the R^2 score of the model on the test data. Ensure your solution is well-structured and demonstrates a clear understanding of scikit-learn\'s functionalities.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def create_synthetic_regression_data(n_samples=1000, n_features=20): Create a synthetic regression dataset. Parameters: - n_samples (int): The number of samples. - n_features (int): The number of features. Returns: - X (numpy.ndarray): Features. - y (numpy.ndarray): Target values. X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42) return X, y def train_test_split_and_preprocess(X, y, test_size=0.2, random_state=42): Split the dataset into training and testing sets, and scale the features. Parameters: - X (numpy.ndarray): Features. - y (numpy.ndarray): Target values. - test_size (float): The proportion of the dataset to include in the test split. - random_state (int): Controls the shuffling applied to the data. Returns: - X_train (numpy.ndarray): Training features. - X_test (numpy.ndarray): Testing features. - y_train (numpy.ndarray): Training target values. - y_test (numpy.ndarray): Testing target values. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate_model(X_train, X_test, y_train, y_test): Train a GradientBoostingRegressor and evaluate its performance. Parameters: - X_train (numpy.ndarray): Training features. - X_test (numpy.ndarray): Testing features. - y_train (numpy.ndarray): Training target values. - y_test (numpy.ndarray): Testing target values. Returns: - r2_score (float): The R^2 score of the model on the test data. model = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) model.fit(X_train, y_train) y_pred = model.predict(X_test) return r2_score(y_test, y_pred) # Main execution X, y = create_synthetic_regression_data(n_samples=1000, n_features=20) X_train, X_test, y_train, y_test = train_test_split_and_preprocess(X, y, test_size=0.2, random_state=42) r2 = train_and_evaluate_model(X_train, X_test, y_train, y_test) print(f\\"The R^2 score of the model on the test data is: {r2:.4f}\\")"},{"question":"Implement the functions required to manage and utilize accelerator devices using the `torch.accelerator` module. Your task is to write a Python script that performs the following: 1. Checks if any accelerator devices are available. If not, print \\"No accelerators available\\" and exit. 2. If accelerators are available, print the count of devices. 3. Set the device to index 0 and print the current device index. 4. Implement a function `perform_computation` that: - Runs a simple tensor operation on the current accelerator. - Ensures that the code is run on the accelerator using PyTorch\'s acceleration features. - Times the operation and prints the execution time. 5. Synchronize the device before and after the computation to ensure correct timing. Your implementation should contain the following functions: - `check_devices()`: Checks and prints if accelerators are available and the count of devices. - `set_device_to_first()`: Sets the current device to index 0 and prints the current device. - `perform_computation()`: Performs a tensor operation and prints the execution time. # Constraints - Use the functions from `torch.accelerator` to manage the devices and synchronization. - The tensor operation to be performed should be simple yet enough to utilize the accelerator (e.g., matrix multiplication). # Expected Input and Output The script should produce output similar to the following, assuming that there are accelerators available: ``` Accelerators available: 2 Current device set to index: 0 Computation time: X.XXX seconds ``` If no accelerators are available, the output should be: ``` No accelerators available ``` # Testing Ensure your code runs correctly on a system equipped with multiple accelerator devices as well as one without any.","solution":"import torch import time def check_devices(): if not torch.cuda.is_available(): print(\\"No accelerators available\\") return False else: available_devices = torch.cuda.device_count() print(f\\"Accelerators available: {available_devices}\\") return True def set_device_to_first(): torch.cuda.set_device(0) current_device = torch.cuda.current_device() print(f\\"Current device set to index: {current_device}\\") def perform_computation(): device = torch.device(\\"cuda:0\\" if torch.cuda.is_available() else \\"cpu\\") # Create tensors a = torch.randn(1000, 1000, device=device) b = torch.randn(1000, 1000, device=device) # Synchronize the device for accurate timing torch.cuda.synchronize() start_time = time.time() # Perform matrix multiplication c = torch.matmul(a, b) # Synchronize again to ensure operation is fully complete torch.cuda.synchronize() end_time = time.time() computation_time = end_time - start_time print(f\\"Computation time: {computation_time:.4f} seconds\\")"},{"question":"You are building a script to organize and manage file paths for a cross-platform software application. Your script needs to handle various tasks such as normalizing paths, obtaining directory names, checking path validity, and determining common paths and prefixes. You will be required to write a function `analyze_path_details()` that processes a list of file paths and provides specific information about these paths. # Function Signature ```python def analyze_path_details(paths: list) -> dict: pass ``` # Input - `paths` (list of str): A list of file paths (both relative and absolute) that you need to analyze. # Output A dictionary containing the following keys and their respective values: - `normalized_paths` (list of str): A list of normalized absolute paths. - `base_names` (list of str): A list of base names of the paths (file or directory names). - `dir_names` (list of str): A list of directory names of the paths. - `existing_paths` (list of bool): A list of boolean values indicating whether each path exists. - `common_path` (str): The longest common sub-path of all the paths. Return an empty string if there is no common sub-path. - `common_prefix` (str): The longest common prefix of all the paths (character-by-character comparison). Return an empty string if there is no common prefix. # Constraints - The paths can be a mix of relative and absolute paths. - The paths should be compatible with both Unix and Windows systems. - Ensure proper exception handling when accessing file system properties. # Example Usage ```python paths = [ \\"/home/user/docs/file1.txt\\", \\"/home/user/music/song.mp3\\", \\"/home/user/docs/file2.txt\\", \\"/home/user/pictures/photo.jpg\\", \\"rel_folder/rel_file.txt\\" ] result = analyze_path_details(paths) print(result) # Expected output # { # \'normalized_paths\': [\'/absolute/home/user/docs/file1.txt\', \'/absolute/home/user/music/song.mp3\', # \'/absolute/home/user/docs/file2.txt\', \'/absolute/home/user/pictures/photo.jpg\', # \'/absolute/rel_folder/rel_file.txt\'], # \'base_names\': [\'file1.txt\', \'song.mp3\', \'file2.txt\', \'photo.jpg\', \'rel_file.txt\'], # \'dir_names\': [\'/home/user/docs\', \'/home/user/music\', \'/home/user/docs\', \'/home/user/pictures\', \'rel_folder\'], # \'existing_paths\': [True, True, True, True, False], # Note: Results may vary based on actual file system. # \'common_path\': \'/home/user\', # \'common_prefix\': \'/home/user\' # } ``` # Note - Use the `os.path` module for all path manipulations and queries. - Ensure compatibility between Unix and Windows platforms. - You may use helper functions to modularize your code. # Solution Template Below is a template that you can start with: ```python import os def analyze_path_details(paths: list) -> dict: normalized_paths = [os.path.abspath(path) for path in paths] base_names = [os.path.basename(path) for path in paths] dir_names = [os.path.dirname(path) for path in paths] existing_paths = [] for path in paths: try: existing_paths.append(os.path.exists(path)) except Exception as e: existing_paths.append(False) common_path = \\"\\" try: common_path = os.path.commonpath(paths) except ValueError: common_path = \\"\\" common_prefix = os.path.commonprefix(paths) result = { \'normalized_paths\': normalized_paths, \'base_names\': base_names, \'dir_names\': dir_names, \'existing_paths\': existing_paths, \'common_path\': common_path, \'common_prefix\': common_prefix } return result ```","solution":"import os def analyze_path_details(paths: list) -> dict: normalized_paths = [os.path.abspath(path) for path in paths] base_names = [os.path.basename(path) for path in paths] dir_names = [os.path.dirname(path) for path in paths] existing_paths = [] for path in paths: try: existing_paths.append(os.path.exists(path)) except Exception as e: existing_paths.append(False) common_path = \\"\\" try: common_path = os.path.commonpath(paths) except ValueError: common_path = \\"\\" common_prefix = os.path.commonprefix(paths) result = { \'normalized_paths\': normalized_paths, \'base_names\': base_names, \'dir_names\': dir_names, \'existing_paths\': existing_paths, \'common_path\': common_path, \'common_prefix\': common_prefix } return result"},{"question":"# **PyTorch Serialization and Model Handling** You are provided with a custom `PyTorch` model class. Your task is to save and load the model using PyTorch serialization techniques, while ensuring that the loaded model retains its state and functionality. Instructions 1. **Define a Custom Model**: Create a custom neural network model `MyCustomModel` with the following architecture: - An `__init__` method that initializes two linear layers: - The first layer should map from 10 to 5 features. - The second layer should map from 5 to 2 features. - A `forward` method that sequentially applies the first linear layer, a ReLU activation, and then the second linear layer. 2. **Save the Model State**: - Instantiate the `MyCustomModel`. - Save its state dict to a file named `custom_model_state.pt`. 3. **Load the Model State**: - Create a new instance of `MyCustomModel`. - Load the saved state dict from the file and apply it to the new model instance. 4. **Save and Load Tensors with Views**: - Create a tensor of shape `(1, 20)` using `torch.arange` and name it `original_tensor`. - Create a view `tensor_view` of `original_tensor` that extracts a sub-tensor of shape `(1, 10)`. - Save both tensors in a file named `tensor_views.pt`. - Load the tensors from `tensor_views.pt` and modify `tensor_view` to verify that changes are reflected in `original_tensor`. 5. **Script and Serialize the Model**: - Convert the `MyCustomModel` to a `ScriptModule` using `torch.jit.script`. - Save the scripted module to a file named `scripted_model.pt`. - Load the scripted model from the file and perform a forward pass with random input of shape `(1, 10)`. Ensure that all steps are performed correctly and verify the outputs at each stage. **Note**: You are required to implement and use the following functions: - `save_model_state(model: torch.nn.Module, file_path: str) -> None` - `load_model_state(model_class: Type[torch.nn.Module], file_path: str) -> torch.nn.Module` - `save_tensor_views(tensors: List[torch.Tensor], file_path: str) -> None` - `load_tensor_views(file_path: str) -> Tuple[torch.Tensor, torch.Tensor]` - `script_and_save_model(model: torch.nn.Module, file_path: str) -> None` - `load_and_run_scripted_model(file_path: str, input_tensor: torch.Tensor) -> torch.Tensor` **Example:** ```python import torch import torch.nn as nn import torch.nn.functional as F from typing import Type, List, Tuple class MyCustomModel(nn.Module): def __init__(self): super(MyCustomModel, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = F.relu(self.layer1(x)) x = self.layer2(x) return x def save_model_state(model: torch.nn.Module, file_path: str) -> None: torch.save(model.state_dict(), file_path) def load_model_state(model_class: Type[torch.nn.Module], file_path: str) -> torch.nn.Module: model = model_class() state_dict = torch.load(file_path) model.load_state_dict(state_dict) return model def save_tensor_views(tensors: List[torch.Tensor], file_path: str) -> None: torch.save(tensors, file_path) def load_tensor_views(file_path: str) -> Tuple[torch.Tensor, torch.Tensor]: tensors_loaded = torch.load(file_path) return tensors_loaded def script_and_save_model(model: torch.nn.Module, file_path: str) -> None: scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, file_path) def load_and_run_scripted_model(file_path: str, input_tensor: torch.Tensor) -> torch.Tensor: scripted_model = torch.jit.load(file_path) return scripted_model(input_tensor) # Example usage: model = MyCustomModel() save_model_state(model, \'custom_model_state.pt\') new_model = load_model_state(MyCustomModel, \'custom_model_state.pt\') original_tensor = torch.arange(20).view(1, 20).clone() tensor_view = original_tensor[:, :10] save_tensor_views([original_tensor, tensor_view], \'tensor_views.pt\') loaded_original_tensor, loaded_tensor_view = load_tensor_views(\'tensor_views.pt\') loaded_tensor_view[:, 0] = 99 print(loaded_original_tensor) # Verify if changes reflect in original tensor script_and_save_model(model, \'scripted_model.pt\') input_data = torch.randn(1, 10) output = load_and_run_scripted_model(\'scripted_model.pt\', input_data) print(output) ``` **Expected Output**: - Successfully saving and loading the model state. - Modifying the views should reflect in the original tensor. - Performing a forward pass with the scripted model should produce the output tensor. Ensure that your code is clean, well-documented, and follows best practices for PyTorch model handling and serialization.","solution":"import torch import torch.nn as nn import torch.nn.functional as F from typing import Type, List, Tuple class MyCustomModel(nn.Module): def __init__(self): super(MyCustomModel, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = F.relu(self.layer1(x)) x = self.layer2(x) return x def save_model_state(model: torch.nn.Module, file_path: str) -> None: torch.save(model.state_dict(), file_path) def load_model_state(model_class: Type[torch.nn.Module], file_path: str) -> torch.nn.Module: model = model_class() state_dict = torch.load(file_path) model.load_state_dict(state_dict) return model def save_tensor_views(tensors: List[torch.Tensor], file_path: str) -> None: torch.save(tensors, file_path) def load_tensor_views(file_path: str) -> Tuple[torch.Tensor, torch.Tensor]: tensors_loaded = torch.load(file_path) return tensors_loaded def script_and_save_model(model: torch.nn.Module, file_path: str) -> None: scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, file_path) def load_and_run_scripted_model(file_path: str, input_tensor: torch.Tensor) -> torch.Tensor: scripted_model = torch.jit.load(file_path) return scripted_model(input_tensor)"},{"question":"# PyTorch with HIP: Device Management and Memory Benchmarking You are tasked with creating a PyTorch program that demonstrates efficient memory usage and tensor operations on an AMD GPU using the HIP runtime. The program should: 1. **Check GPU availability**: Detect if a GPU is available and whether it uses HIP or CUDA. 2. **Tensor Operations**: Create tensors on different GPU devices, perform basic arithmetic operations, and transfer tensors between devices. 3. **Memory Benchmarking**: Track and display memory usage before and after tensor operations. Function Specification ```python import torch def hip_tensor_operations(): # Step 1: Check if a GPU is available and identify the runtime (HIP or CUDA) # Step 2: If GPU is available, perform the following operations: # a. Create a tensor on the default device (GPU 0) # b. Transfer the tensor to another GPU (e.g., GPU 1) # c. Perform a basic arithmetic operation (e.g., addition) with tensors on the second GPU # d. Track memory usage before and after the operations using torch.cuda.memory_allocated() # Sample output format: # GPU available: True # Using HIP: True # Memory allocated before operation: X MB # Memory allocated after operation: Y MB pass ``` Input/Output - **Input**: No direct input required. The function operates based on the available GPU(s). - **Output**: Print statements showing: - GPU availability - Whether HIP is used - Memory usage before and after tensor operations. Example Output ``` GPU available: True Using HIP: True Memory allocated before operation: 10 MB Memory allocated after operation: 15 MB ``` Constraints - Ensure your code can handle scenarios where no GPU is available or when HIP is not being used. - Incorporate efficient memory management using PyTorchâ€™s caching memory allocator functions like `torch.cuda.memory_allocated`. Performance Requirements - The function should handle tensor operations efficiently, ensuring minimal memory overhead. - Memory tracking and benchmarking should not introduce significant performance penalties. Tips - Leverage methods like `torch.device`, `Tensor.to`, and `torch.cuda.memory_allocated` for device management and memory tracking. - Utilize `torch.cuda.is_available` in conjunction with `torch.version.hip` to identify the runtime environment. **Good luck, and happy coding!**","solution":"import torch def hip_tensor_operations(): # Step 1: Check if a GPU is available and identify the runtime (HIP or CUDA) gpu_available = torch.cuda.is_available() using_hip = torch.version.hip is not None if gpu_available: # Print basic availability information print(f\\"GPU available: {gpu_available}\\") print(f\\"Using HIP: {using_hip}\\") # Step 2: Perform Tensor Operations # a. Create a tensor on the default device (GPU 0) device0 = torch.device(\'cuda:0\') tensor1 = torch.rand(100, 100, device=device0) # Create a random tensor on GPU 0 # b. Transfer the tensor to another GPU (e.g., GPU 1), if available if torch.cuda.device_count() > 1: device1 = torch.device(\'cuda:1\') tensor2 = tensor1.to(device1) # Transfer tensor to GPU 1 else: device1 = device0 tensor2 = tensor1.clone() # No other GPU available, so just clone the tensor on GPU 0 # Memory usage before operation memory_before = torch.cuda.memory_allocated(device=device1) # c. Perform a basic arithmetic operation (e.g., addition) with tensors on the second GPU tensor_result = tensor2 + 1 # Add 1 to all elements in the tensor # Memory usage after operation memory_after = torch.cuda.memory_allocated(device=device1) # Convert memory to megabytes memory_before_mb = memory_before / (1024 * 1024) memory_after_mb = memory_after / (1024 * 1024) # d. Display memory usage print(f\\"Memory allocated before operation: {memory_before_mb:.2f} MB\\") print(f\\"Memory allocated after operation: {memory_after_mb:.2f} MB\\") else: print(\\"GPU available: False\\") # Run the function for demonstration purposes hip_tensor_operations()"},{"question":"**Title**: Understanding and Handling Python Exceptions using Low-Level APIs **Problem Description**: You are tasked with implementing a custom function in Python that deals with exception propagation and proper cleanup using Python\'s low-level C API functions. The function should perform the following operations: 1. Attempt to divide two numbers. 2. Depending on the result: a. If the result is a negative number, raise a `ValueError`. b. If the result is positive but an integer, raise a `TypeError`. c. If the result has decimal points, raise a custom-defined `CustomZeroDivisionError` (this should simulate behavior if division by zero was attempted, though mathematically it shouldn\'t raise ZeroDivisionError for non-zero denominators). 3. Use the appropriate C API functions provided in the documentation. 4. Ensure that if an exception is raised, it is cleared and the appropriate message is printed. 5. If `KeyboardInterrupt` is encountered, simulate a SIGINT signal using the provided API. **Function Signature**: ```python def simulate_division_handling(num1: float, num2: float) -> None: pass ``` **Explanation**: - `num1`: a floating-point number representing the numerator. - `num2`: a floating-point number representing the denominator. The function should follow these steps: 1. Attempt to divide `num1` by `num2`. 2. Check the result: - If the result is negative, use `PyErr_SetString` to raise `ValueError` with a message \\"Negative result\\". - If the result is positive and an integer, raise `TypeError` with a message \\"Result is integer\\". - If the result is positive but not an integer, raise the custom `CustomZeroDivisionError`. 3. Use the C API functions to handle exceptions. 4. Clear exceptions using `PyErr_Clear()` and print relevant messages using `PyErr_PrintEx()`. 5. If `KeyboardInterrupt` is encountered, use `PyErr_SetInterruptEx(SIGINT)` and ensure it is handled correctly. **Constraints**: - Denominator (`num2`) should not be zero. - Only Python standard exception classes should be used unless specified otherwise. **Example**: ```python try: simulate_division_handling(10.0, -2.0) except ValueError as ve: print(ve) # Output should be \\"Negative result\\" ``` Note: You must use the appropriate C API functions as described in the provided documentation for exception handling.","solution":"def simulate_division_handling(num1: float, num2: float) -> None: import ctypes import signal # Define a custom exception for simulation class CustomZeroDivisionError(Exception): pass try: if num2 == 0: raise ZeroDivisionError(\\"Denominator cannot be zero.\\") result = num1 / num2 if result < 0: raise ValueError(\\"Negative result\\") elif result == int(result): raise TypeError(\\"Result is integer\\") else: raise CustomZeroDivisionError(\\"Simulated zero division error\\") except CustomZeroDivisionError as e: print(f\\"CustomZeroDivisionError: {e}\\") except ValueError as e: print(f\\"ValueError: {e}\\") except TypeError as e: print(f\\"TypeError: {e}\\") except ZeroDivisionError as e: print(f\\"ZeroDivisionError: {e}\\") except KeyboardInterrupt: # Simulating sending SIGINT signal signal.raise_signal(signal.SIGINT) finally: # Clear any inherent exception that may have been set print(\\"Exception handling completed, all exceptions are cleared.\\")"},{"question":"**Question: Applying a Machine Learning Model on Toy Datasets Using scikit-learn** You are provided with the scikit-learn library\'s toy datasets, which are useful for illustrating various machine learning techniques. For this assessment, you are required to demonstrate your understanding of loading and manipulating one of these datasets, as well as applying a machine learning model using scikit-learn. # Task 1. **Choose a dataset**: Using the `load_wine()` function from `sklearn.datasets`, load the wine dataset. 2. **Data Exploration**: - Print the feature names and target names of the dataset. - Display the first 5 records of the dataset. 3. **Data Processing**: - Split the dataset into training and test sets, with 80% of the data used for training and 20% for testing. Use `random_state=42` to ensure reproducibility. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Training**: - Train a `RandomForestClassifier` on the training data. Use `random_state=42` for reproducibility. - Print the feature importances as determined by the model. 5. **Evaluation**: - Evaluate the model on the test data and print out the accuracy score. - Generate a classification report that includes precision, recall, and F1-score for each class. # Requirements - **Input**: No direct input is necessary from the user for loading the dataset and running the code. - **Output**: The code should print out the required information mentioned in the task above. - **Constraints**: - Use scikit-learn for all operations related to the dataset, model training, and evaluation. - Ensure reproducibility by setting the `random_state` parameter as specified. # Example Output The expected output should include: - List of feature names and target names. - The first 5 records of the dataset. - Random forest feature importances. - Accuracy of the model on the test data. - Classification report including precision, recall, and F1-score for each class. ```plaintext Feature names: [\'alcohol\', \'malic_acid\', \'ash\', \'alcalinity_of_ash\', \'magnesium\', ...] Target names: [\'class_0\', \'class_1\', \'class_2\'] First 5 records: [[14.23, 1.71, 2.43, 15.6, 127, ...], [13.2, 1.78, 2.14, 11.2, 100, ...], ...] Feature importances: [0.12, 0.03, 0.03, 0.04, 0.05, ...] Accuracy on test data: 0.97 Classification Report: precision recall f1-score support class 0 1.00 1.00 1.00 12 class 1 0.96 1.00 0.98 26 class 2 0.95 0.91 0.93 11 accuracy 0.97 49 macro avg 0.97 0.97 0.97 49 weighted avg 0.97 0.97 0.97 49 ``` # Submission Submit your implementation of the above tasks in a single Python script file or Jupyter notebook. Ensure your code is well-documented and follows best practices for readability and maintainability.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report def wine_classification(): # 1. Load the Wine dataset wine = load_wine() X = wine.data y = wine.target # 2. Data Exploration feature_names = wine.feature_names target_names = wine.target_names first_five_records = X[:5] print(\\"Feature names:\\", feature_names) print(\\"Target names:\\", target_names) print(\\"First 5 records:\\") print(first_five_records) # 3. Data Processing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # 4. Model Training clf = RandomForestClassifier(random_state=42) clf.fit(X_train_scaled, y_train) feature_importances = clf.feature_importances_ print(\\"Feature importances:\\", feature_importances) # 5. Evaluation y_pred = clf.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy on test data:\\", accuracy) print(\\"Classification Report:\\") print(classification_report(y_test, y_pred, target_names=target_names)) # Returning values for unit tests return feature_names, target_names, first_five_records, feature_importances, accuracy # Running the function wine_classification()"},{"question":"# **Coding Assessment Question** You are tasked with implementing a function that simulates some functionalities of Python\'s `zipimporter` class. Given a ZIP archive containing multiple `.py` and `.pyc` files, you need to: 1. **Extract the List of Python Files**: Implement a function that lists all Python source files (`.py`) and bytecode files (`.pyc`) within the provided ZIP archive. 2. **Fetch File Content**: Implement another function that, given the name of a Python file within the archive, returns the content of that file. 3. **Check for Package**: Implement a function that determines if a given module name represents a package (i.e., a directory containing an `__init__.py` file). # **Function Signatures** ```python import zipfile import os def list_python_files(archive_path: str) -> list: Lists all .py and .pyc files contained in the ZIP archive. Parameters: - archive_path (str): Path to the ZIP archive. Returns: - list: A list of all .py and .pyc files within the archive. # Your implementation here def get_file_content(archive_path: str, file_path: str) -> str: Fetches the content of the specified file from the ZIP archive. Parameters: - archive_path (str): Path to the ZIP archive. - file_path (str): Path to the Python file within the archive. Returns: - str: The content of the specified file. # Your implementation here def is_package(archive_path: str, module_name: str) -> bool: Checks if the specified module is a package (contains an __init__.py file). Parameters: - archive_path (str): Path to the ZIP archive. - module_name (str): Fully qualified module name. Returns: - bool: True if the module is a package, False otherwise. # Your implementation here ``` # **Constraints** - The ZIP archive exists and is not corrupted. - The specified file path within the archive exists and refers to a `.py` or `.pyc` file. - For simplicity, assume module names are provided as directory paths within the archive. # **Examples** ```python # Example call for listing Python files print(list_python_files(\'projects.zip\')) # Output: [\'module1.py\', \'module2.pyc\', \'package1/__init__.py\', \'package1/module3.py\'] # Example call for fetching file content print(get_file_content(\'projects.zip\', \'module1.py\')) # Output: (Contents of module1.py) # Example call for checking if a module is a package print(is_package(\'projects.zip\', \'package1\')) # Output: True print(is_package(\'projects.zip\', \'module1\')) # Output: False ``` # **Performance Requirements** - The functions should efficiently handle ZIP archives containing up to 1000 Python files with a total size of up to 100MB. Write your implementation in Python, adhering to the constraints and ensuring efficient handling of ZIP files.","solution":"import zipfile import os def list_python_files(archive_path: str) -> list: Lists all .py and .pyc files contained in the ZIP archive. Parameters: - archive_path (str): Path to the ZIP archive. Returns: - list: A list of all .py and .pyc files within the archive. with zipfile.ZipFile(archive_path, \'r\') as archive: python_files = [name for name in archive.namelist() if name.endswith(\'.py\') or name.endswith(\'.pyc\')] return python_files def get_file_content(archive_path: str, file_path: str) -> str: Fetches the content of the specified file from the ZIP archive. Parameters: - archive_path (str): Path to the ZIP archive. - file_path (str): Path to the Python file within the archive. Returns: - str: The content of the specified file. with zipfile.ZipFile(archive_path, \'r\') as archive: with archive.open(file_path) as file: return file.read().decode(\'utf-8\') def is_package(archive_path: str, module_name: str) -> bool: Checks if the specified module is a package (contains an __init__.py file). Parameters: - archive_path (str): Path to the ZIP archive. - module_name (str): Fully qualified module name. Returns: - bool: True if the module is a package, False otherwise. init_file = os.path.join(module_name, \'__init__.py\') with zipfile.ZipFile(archive_path, \'r\') as archive: return init_file in archive.namelist()"},{"question":"# Question: Inventory Management System You are tasked with creating a simple inventory management system for a small store. The system will manage the inventory using Python lists, sets, and dictionaries as follows: - List to store the ongoing transactions. - Set to keep track of unique items in the inventory. - Dictionary to keep the count of each item in the inventory. Implementation Details 1. Write a class `InventoryManager` implementing the following methods: - `__init__(self)`: Initializes an empty inventory. - `add_transaction(self, transaction)`: Adds a new transaction to the list of transactions. Each transaction is a dictionary with `\'type\'` (either `\'add\'` or `\'remove\'`), `\'item\'`, and `\'quantity\'`. - `update_inventory(self)`: Updates the inventory based on the transactions added. - `get_inventory(self)`: Returns the current state of the inventory as a dictionary with items as keys and their quantities as values. - `get_unique_items(self)`: Returns a set of all unique items in the inventory. - `clear_transactions(self)`: Clears all transactions from the list. 2. Your implementation should handle: - Adding and removing items from the inventory. - Ensure that the quantity of an item doesn\'t fall below zero. - If a removal transaction tries to remove more items than available, it should roll back the entire transaction for that item. - Allow retrieval of the current inventory and the list of unique items efficiently. Example Usage ```python # Creating an instance of InventoryManager manager = InventoryManager() # Adding transactions manager.add_transaction({\'type\': \'add\', \'item\': \'apple\', \'quantity\': 10}) manager.add_transaction({\'type\': \'add\', \'item\': \'banana\', \'quantity\': 5}) manager.add_transaction({\'type\': \'remove\', \'item\': \'apple\', \'quantity\': 3}) manager.add_transaction({\'type\': \'add\', \'item\': \'orange\', \'quantity\': 7}) manager.add_transaction({\'type\': \'remove\', \'item\': \'banana\', \'quantity\': 1}) # Updating the inventory manager.update_inventory() # Getting the current state of the inventory print(manager.get_inventory()) # Output: {\'apple\': 7, \'banana\': 4, \'orange\': 7} # Getting the set of unique items print(manager.get_unique_items()) # Output: {\'apple\', \'banana\', \'orange\'} # Clearing all transactions manager.clear_transactions() print(manager.get_inventory()) # Output remains same: {\'apple\': 7, \'banana\': 4, \'orange\': 7} ``` # Constraints - `transaction[\'type\']` will only be `\'add\'` or `\'remove\'`. - `transaction[\'item\']` will be a non-empty string. - `transaction[\'quantity\']` will be a positive integer. - The number of transactions will not exceed 10^3. - The number of different items in inventory will not exceed 10^2. # Evaluation Your solution will be evaluated based on: - Correctness and efficiency of the implementation. - Proper handling of edge cases. - Code readability and organization.","solution":"class InventoryManager: def __init__(self): Initializes an empty inventory. self.transactions = [] self.inventory = {} self.unique_items = set() def add_transaction(self, transaction): Adds a new transaction to the list of transactions. self.transactions.append(transaction) def update_inventory(self): Updates the inventory based on the transactions added. for transaction in self.transactions: item = transaction[\'item\'] quantity = transaction[\'quantity\'] if transaction[\'type\'] == \'add\': if item in self.inventory: self.inventory[item] += quantity else: self.inventory[item] = quantity self.unique_items.add(item) elif transaction[\'type\'] == \'remove\': if item in self.inventory and self.inventory[item] >= quantity: self.inventory[item] -= quantity if self.inventory[item] == 0: del self.inventory[item] self.unique_items.remove(item) else: # rollback the transaction continue def get_inventory(self): Returns the current state of the inventory as a dictionary with items as keys and their quantities as values. return self.inventory def get_unique_items(self): Returns a set of all unique items in the inventory. return self.unique_items def clear_transactions(self): Clears all transactions from the list. self.transactions = []"},{"question":"# Python File Handling and Integration with File Descriptors Objective You are tasked with implementing functions that demonstrate advanced understanding of file handling in Python, including integrating with file descriptors. Your solution should ensure efficient and correct processing of file data. Instructions 1. **File Descriptor from Python Object:** Implement a function `get_file_descriptor` that takes a Python file object and returns its file descriptor. ```python def get_file_descriptor(py_file_obj) -> int: Returns the file descriptor associated with the given Python file object. Args: py_file_obj (file object): A Python file object. Returns: int: The file descriptor associated with the file object. Raises: ValueError: If the file object is invalid or does not support \'fileno()\' method. pass ``` 2. **Reading Lines from File Descriptor:** Implement a function `read_lines_from_fd` that takes a file descriptor and an optional parameter `line_limit` which limits the number of lines to be read. If `line_limit` is not specified, read the entire file. ```python def read_lines_from_fd(fd: int, line_limit: int = -1) -> list: Reads lines from a file descriptor, with an optional limit on the number of lines. Args: fd (int): The file descriptor from which to read. line_limit (int, optional): The maximum number of lines to read. Default is -1 (read all lines). Returns: list: A list of strings, each representing a line from the file. Raises: ValueError: If the file descriptor is invalid. pass ``` 3. **Write Object to File Using File Descriptor:** Implement a function `write_object_to_fd` that writes a given Python object to a file referred to by its file descriptor. The function should write the string representation of the object. ```python def write_object_to_fd(fd: int, obj) -> None: Writes the string representation of a Python object to the file referenced by the given file descriptor. Args: fd (int): The file descriptor to which the object will be written. obj (any): The Python object to write to the file. Returns: None Raises: ValueError: If the file descriptor is invalid. pass ``` Constraints - Assume the file object in `get_file_descriptor` is valid and properly opened. - For `read_lines_from_fd`, handle cases where the file descriptor might be invalid by raising a `ValueError`. - Ensure all functions handle exceptions gracefully and provide meaningful error messages. Objective The goal is to test your understanding of how Python\'s high-level file handling can be mapped and used with lower-level concepts like file descriptors. Notes - Make use of Python\'s built-in functions and handle exceptions where necessary. - Ensure proper closing of file descriptors if they are opened within your functions.","solution":"def get_file_descriptor(py_file_obj) -> int: Returns the file descriptor associated with the given Python file object. Args: py_file_obj (file object): A Python file object. Returns: int: The file descriptor associated with the file object. Raises: ValueError: If the file object is invalid or does not support \'fileno()\' method. try: return py_file_obj.fileno() except Exception as e: raise ValueError(f\\"Invalid file object: {e}\\") def read_lines_from_fd(fd: int, line_limit: int = -1) -> list: Reads lines from a file descriptor, with an optional limit on the number of lines. Args: fd (int): The file descriptor from which to read. line_limit (int, optional): The maximum number of lines to read. Default is -1 (read all lines). Returns: list: A list of strings, each representing a line from the file. Raises: ValueError: If the file descriptor is invalid. try: import os if line_limit == -1: lines = os.read(fd, os.fstat(fd).st_size).decode().splitlines() else: content = os.read(fd, os.fstat(fd).st_size).decode() lines = content.splitlines()[:line_limit] return lines except Exception as e: raise ValueError(f\\"Error reading from file descriptor: {e}\\") def write_object_to_fd(fd: int, obj) -> None: Writes the string representation of a Python object to the file referenced by the given file descriptor. Args: fd (int): The file descriptor to which the object will be written. obj (any): The Python object to write to the file. Returns: None Raises: ValueError: If the file descriptor is invalid. try: import os os.write(fd, str(obj).encode()) except Exception as e: raise ValueError(f\\"Error writing to file descriptor: {e}\\")"},{"question":"**Text Processing Challenge: Enhanced String Formatter** In this exercise, you are required to implement an enhanced string formatter that applies various text processing functionalities to a given input string. # Instructions: 1. **Input**: - A list of strings `patterns` containing regular expression patterns to search for within the input string. - A list of strings `replace_with` where each element corresponds to the respective pattern in `patterns` and indicates what the pattern should be replaced with in the input string. - A string `text` representing the input text that needs to be processed. 2. **Output**: - The function should return a string with all the specified patterns replaced as directed, any trailing or leading whitespace removed, wrapped to a maximum width of 50 characters, and presented in a neatly formatted manner. 3. **Constraints**: - `patterns` and `replace_with` should have the same length. - Maximum length for each pattern and replacement string is 50 characters. - Each pattern is a valid regular expression string. - The `text` may contain up to 1000 words. 4. **Performance**: - The function should efficiently handle searches and replacements using regular expressions. - Ensure minimal memory usage and avoid unnecessary recomputations. # Function Signature: ```python def enhanced_string_formatter(patterns: list[str], replace_with: list[str], text: str) -> str: pass ``` # Example: ```python patterns = [r\'bfoob\', r\'bbarb\'] replace_with = [\'FOO\', \'BAR\'] text = \\" foo and bar are common placeholder names in programming. \\" result = enhanced_string_formatter(patterns, replace_with, text) print(result) ``` # Expected Output: ``` FOO and BAR are common placeholder names in programming. ``` # Notes: - Pay attention to edge cases such as overlapping patterns and whitespace considerations. - Use the relevant modules (`re`, `textwrap`, etc.) to maximize the clarity and efficiency of your solution. - Ensure the code is clean, readable, and well-documented.","solution":"import re import textwrap def enhanced_string_formatter(patterns: list[str], replace_with: list[str], text: str) -> str: # Step 1: Apply all replacements based on patterns for pattern, replacement in zip(patterns, replace_with): text = re.sub(pattern, replacement, text) # Step 2: Strip leading and trailing whitespace text = text.strip() # Step 3: Wrap the text to a maximum width of 50 characters wrapped_text = textwrap.fill(text, width=50) return wrapped_text"},{"question":"# **Advanced Subprocess Management** Problem Statement: You are required to implement a function `execute_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]` that executes a sequence of shell commands in parallel, captures their outputs, handles any potential timeouts, and manages exceptions. Function Signature: ```python from typing import List, Tuple def execute_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]: # implementation here ``` Parameters: - `commands` (List[str]): A list of shell commands (strings) to be executed. - `timeout` (int): The maximum time (in seconds) allowed for each command to run. Returns: - A list of tuples. Each tuple corresponds to a command and contains: - The command run (str). - The exit code (int). - The standard output (str). - The standard error (str). Constraints: - You must capture both stdout and stderr for each command. - If a command times out, the function should capture the state and include specific indications in the output. - You need to handle potential exceptions and include relevant information in the returned output. - Ensure that your solution follows best security practices as mentioned in the documentation, especially concerning the use of `shell=True`. Example: ```python commands = [ \\"echo Hello World\\", \\"sleep 5\\", \\"ls non_existent_file\\" ] timeout = 3 results = execute_commands(commands, timeout) for result in results: print(result) ``` Expected Output: ``` [ (\'echo Hello World\', 0, \'Hello Worldn\', \'\'), (\'sleep 5\', -1, \'\', \'TimeoutExpired\'), (\'ls non_existent_file\', 2, \'\', \'ls: cannot access non_existent_file: No such file or directoryn\') ] ``` In the above example: - The command `echo Hello World` runs successfully and produces stdout with no stderr. - The command `sleep 5` exceeds the timeout of 3 seconds, thus is recorded with a timeout indication. - The command `ls non_existent_file` reports an error in stderr as expected. Notes: - Utilize the `subprocess.Popen` class for managing the subprocesses. - To handle timeouts, make use of the `communicate()` method with the `timeout` parameter. - Consider using the context manager support provided by `subprocess.Popen` for better resource management and cleaner code.","solution":"from typing import List, Tuple import subprocess def execute_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]: results = [] for command in commands: try: proc = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) try: stdout, stderr = proc.communicate(timeout=timeout) exit_code = proc.returncode results.append((command, exit_code, stdout, stderr)) except subprocess.TimeoutExpired: proc.kill() stdout, stderr = proc.communicate() results.append((command, -1, stdout, \\"TimeoutExpired\\")) except Exception as e: results.append((command, -1, \\"\\", str(e))) return results"},{"question":"# Advanced Pandas Manipulation and Analysis **Problem**: Using a dataset of student records, your task is to perform a set of data manipulation and analysis operations using pandas. **Dataset**: You are given a CSV file named `students.csv` with the following columns: - `StudentID`: A unique identifier for each student. - `Name`: The name of the student. - `Gender`: The gender of the student, with possible values \'M\' or \'F\'. - `Age`: The age of the student. - `Grade`: The current grade of the student. - `MathScore`: The score of the student in Math. - `EnglishScore`: The score of the student in English. - `ScienceScore`: The score of the student in Science. **Task**: Write a Python function using pandas that performs the following operations: 1. **Read the CSV file**: Read the dataset from `students.csv` into a pandas DataFrame. 2. **Data Cleansing**: - Replace any missing values in the `Age` column with the mean age. - Replace any missing values in the `MathScore`, `EnglishScore`, and `ScienceScore` columns with 0. 3. **Data Transformation**: - Create a new column `TotalScore` which is the sum of `MathScore`, `EnglishScore`, and `ScienceScore`. - Create a new column `AverageScore` which is the average of `MathScore`, `EnglishScore`, and `ScienceScore`. 4. **Data Aggregation and Filtering**: - Find the student(s) with the highest `TotalScore` and return their `Name` and `TotalScore`. - Calculate the average `TotalScore` for students grouped by `Gender`. 5. **Data Analysis**: - Return the names of the top 5 students based on `AverageScore`. - Return the gender distribution in terms of a dictionary with keys \'M\' and \'F\'. ```python import pandas as pd import numpy as np def student_data_analysis(file_path): # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Handle missing values df[\'Age\'].fillna(df[\'Age\'].mean(), inplace=True) df[\'MathScore\'].fillna(0, inplace=True) df[\'EnglishScore\'].fillna(0, inplace=True) df[\'ScienceScore\'].fillna(0, inplace=True) # Create new columns for TotalScore and AverageScore df[\'TotalScore\'] = df[\'MathScore\'] + df[\'EnglishScore\'] + df[\'ScienceScore\'] df[\'AverageScore\'] = df[[\'MathScore\', \'EnglishScore\', \'ScienceScore\']].mean(axis=1) # Find students with the highest TotalScore top_student = df[df[\'TotalScore\'] == df[\'TotalScore\'].max()][[\'Name\', \'TotalScore\']] # Calculate average TotalScore by Gender avg_score_by_gender = df.groupby(\'Gender\')[\'TotalScore\'].mean().to_dict() # Get names of the top 5 students based on AverageScore top_5_students = df.nlargest(5, \'AverageScore\')[[\'Name\', \'AverageScore\']] # Gender distribution gender_distribution = df[\'Gender\'].value_counts().to_dict() return { \'top_student\': top_student.to_dict(orient=\'records\'), \'avg_score_by_gender\': avg_score_by_gender, \'top_5_students\': top_5_students[\'Name\'].tolist(), \'gender_distribution\': gender_distribution } # Example Usage: # result = student_data_analysis(\'students.csv\') # print(result) ``` **Input**: - `file_path` (str): The file path to the `students.csv`. **Output**: - A dictionary containing: - `top_student`: A list of dictionaries with the `Name` and `TotalScore` of the student(s) with the highest `TotalScore`. - `avg_score_by_gender`: A dictionary with the average `TotalScore` for each gender. - `top_5_students`: A list of names of the top 5 students based on `AverageScore`. - `gender_distribution`: A dictionary representing the distribution of genders in the dataset. **Constraints**: - Assume that the CSV file is properly formatted and contains valid data types. **Performance**: - The solution should be efficient with a proper handling of missing values and the ability to perform aggregations and calculations in a minimal computational time.","solution":"import pandas as pd import numpy as np def student_data_analysis(file_path): # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Handle missing values df[\'Age\'].fillna(df[\'Age\'].mean(), inplace=True) df[\'MathScore\'].fillna(0, inplace=True) df[\'EnglishScore\'].fillna(0, inplace=True) df[\'ScienceScore\'].fillna(0, inplace=True) # Create new columns for TotalScore and AverageScore df[\'TotalScore\'] = df[\'MathScore\'] + df[\'EnglishScore\'] + df[\'ScienceScore\'] df[\'AverageScore\'] = df[[\'MathScore\', \'EnglishScore\', \'ScienceScore\']].mean(axis=1) # Find students with the highest TotalScore top_student = df[df[\'TotalScore\'] == df[\'TotalScore\'].max()][[\'Name\', \'TotalScore\']] # Calculate average TotalScore by Gender avg_score_by_gender = df.groupby(\'Gender\')[\'TotalScore\'].mean().to_dict() # Get names of the top 5 students based on AverageScore top_5_students = df.nlargest(5, \'AverageScore\')[[\'Name\', \'AverageScore\']] # Gender distribution gender_distribution = df[\'Gender\'].value_counts().to_dict() return { \'top_student\': top_student.to_dict(orient=\'records\'), \'avg_score_by_gender\': avg_score_by_gender, \'top_5_students\': top_5_students[\'Name\'].tolist(), \'gender_distribution\': gender_distribution } # Example Usage: # result = student_data_analysis(\'students.csv\') # print(result)"},{"question":"Objective: The goal of this task is to evaluate your understanding of loading and preprocessing datasets using scikit-learn\'s `datasets` module. Problem Statement: You are provided with a dataset in the svmlight/libsvm format and a sample image. Your task is to load these datasets using scikit-learn and perform the following operations: 1. **Load the Datasets**: - Load the provided `train_dataset.txt` and `test_dataset.txt` using scikit-learn\'s `load_svmlight_file`. - Load the sample image `china.jpg` using scikit-learn\'s `load_sample_image`. 2. **Data Preprocessing**: - Convert the loaded sparse matrices (from `train_dataset.txt` and `test_dataset.txt`) to dense numpy arrays. - Normalize the image data (from `china.jpg`) to the range [0, 1]. 3. **Function Specifications**: Implement the following function: ```python def process_datasets(train_file: str, test_file: str, image_name: str): Load and process datasets. Args: - train_file (str): Path to the training dataset in svmlight/libsvm format. - test_file (str): Path to the testing dataset in svmlight/libsvm format. - image_name (str): Name of the sample image to be loaded. Returns: - tuple: (X_train_dense, y_train, X_test_dense, y_test, normalized_image) where: - X_train_dense: Dense numpy array of training features. - y_train: Numpy array of training labels. - X_test_dense: Dense numpy array of testing features. - y_test: Numpy array of testing labels. - normalized_image: Numpy array of normalized image data. pass ``` 4. **Constraints**: - Your implementation should handle the situations where the input files might not exist. - Ensure that the image is normalized properly before returning. Input Format: - `train_file`: String path to the training dataset file (e.g., \\"/path/to/train_dataset.txt\\"). - `test_file`: String path to the testing dataset file (e.g., \\"/path/to/test_dataset.txt\\"). - `image_name`: String name of the sample image (e.g., \\"china.jpg\\"). Output Format: - A tuple containing: - Dense numpy arrays corresponding to the training features, training labels, testing features, testing labels. - Normalized numpy array corresponding to the image data. Example Usage: ```python train_file = \\"/path/to/train_dataset.txt\\" test_file = \\"/path/to/test_dataset.txt\\" image_name = \\"china.jpg\\" result = process_datasets(train_file, test_file, image_name) print(result) ``` Please provide a well-documented implementation with necessary comments and error handling for robust functionality.","solution":"import numpy as np from sklearn.datasets import load_svmlight_file, load_sample_image import os def process_datasets(train_file: str, test_file: str, image_name: str): Load and process datasets. Args: - train_file (str): Path to the training dataset in svmlight/libsvm format. - test_file (str): Path to the testing dataset in svmlight/libsvm format. - image_name (str): Name of the sample image to be loaded. Returns: - tuple: (X_train_dense, y_train, X_test_dense, y_test, normalized_image) where: - X_train_dense: Dense numpy array of training features. - y_train: Numpy array of training labels. - X_test_dense: Dense numpy array of testing features. - y_test: Numpy array of testing labels. - normalized_image: Numpy array of normalized image data. # Load the datasets if not os.path.exists(train_file): raise FileNotFoundError(f\\"The file {train_file} does not exist.\\") if not os.path.exists(test_file): raise FileNotFoundError(f\\"The file {test_file} does not exist.\\") X_train, y_train = load_svmlight_file(train_file) X_test, y_test = load_svmlight_file(test_file) # Convert sparse matrices to dense arrays X_train_dense = X_train.toarray() X_test_dense = X_test.toarray() # Load and normalize the sample image image = load_sample_image(image_name) normalized_image = image / 255.0 return X_train_dense, y_train, X_test_dense, y_test, normalized_image"},{"question":"# Customizing Object Representations with reprlib **Objective:** Demonstrate your understanding of Python\'s `reprlib` module by creating a custom representation class for complex objects, specifically handling nested dictionaries and custom class objects with specific limits. **Task:** You are required to create a subclass of `reprlib.Repr` that: 1. Limits the depth of representations of nested dictionaries to a maximum depth of 3. 2. Represents custom objects of a class `Person` (defined below) in a concise format with only the `name` attribute displayed. 3. Uses the `@reprlib.recursive_repr` decorator to handle potential recursion in the representation of `Person` objects. **Requirements:** 1. Implement a class `CustomRepr` that inherits from `reprlib.Repr`. 2. Override necessary methods to handle nested dictionaries and `Person` objects. 3. Ensure that nested dictionaries do not display more than 3 levels deep. 4. The `Person` class should be represented as `\\"Person(name=NameString)\\"`. **Input Format:** - You will be provided with dictionaries and instances of the `Person` class. **Output Format:** - Return the custom representations as a string output when calling the `repr` method on an instance of `CustomRepr`. **Constraints:** - You should not exceed a depth of 3 for nested dictionaries. - Use the `@reprlib.recursive_repr` decorator to handle recursion specifically for `Person` objects. **Example:** ```python import reprlib class Person: def __init__(self, name, age): self.name = name self.age = age # Your implementation here class CustomRepr(reprlib.Repr): # Define limits and custom representation methods # Example Usage aRepr = CustomRepr() nested_dict = {\'level1\': {\'level2\': {\'level3\': {\'level4\': \'too deep\'}}}} person = Person(\'Alice\', 30) nested_person = {\'person1\': person, \'person2\': {\'nested\': person}} print(aRepr.repr(nested_dict)) # Should print a dictionary limited to 3 levels deep print(aRepr.repr(person)) # Should print \\"Person(name=Alice)\\" print(aRepr.repr(nested_person)) # Should handle recursion and respect depth limits ``` Implement the above requirements in a solution and test it with the provided examples.","solution":"import reprlib class Person: def __init__(self, name, age): self.name = name self.age = age @reprlib.recursive_repr() def __repr__(self): return f\\"Person(name={self.name})\\" class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlevel = 3 def repr_Dict(self, obj, level): if level <= 0: return \'{...}\' new_dict = dict((k, self.repr1(v, level - 1)) for k, v in obj.items()) return repr(new_dict) def repr_Person(self, obj, level): return repr(obj) aRepr = CustomRepr() aRepr.maxdict = 3 aRepr.repr_Person = CustomRepr().repr_Person aRepr.repr_Dict = CustomRepr().repr_Dict"},{"question":"Advanced Residual Analysis using Seaborn **Context:** You are provided with a dataset of vehicle fuel efficiency (`mpg`) that includes attributes such as weight, horsepower, displacement, etc. One of the tasks statistical data analysts perform is to check the assumptions of linear regression using residual plots. Residual plots can help identify if a simpler linear model or a more complex model is required. **Task:** 1. Load the `mpg` dataset using Seaborn. 2. Create a residual plot to evaluate the fit of a linear regression model where `x` is `weight` and `y` is `mpg`. 3. Evaluate the fit of a polynomial regression model with an order of 2 where `x` is `horsepower` and `y` is `mpg`. 4. Add a LOWESS curve to the polynomial regression residual plot to visualize any underlying patterns. **Requirements:** - The residual plots should be clear and properly labeled. - Ensure that the plots use different colors for residual points and the LOWESS curve to make them distinguishable. - Add necessary titles or labels to the plots for clarity. **Input:** No explicit input is required other than loading the dataset using Seaborn. **Output:** Output should be the display of residual plots as described above. **Performance Constraints:** Ensure your code runs efficiently within a Jupyter Notebook environment. **Instructions:** 1. Import the required libraries and set the Seaborn theme. 2. Load the `mpg` dataset from Seaborn. 3. Create a residual plot for `weight` vs. `mpg` using a simple linear regression model. 4. Create a residual plot for `horsepower` vs. `mpg` using a second-order polynomial regression model. 5. Enhance the polynomial residual plot with a LOWESS curve. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme() # Load dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Residual plot for weight vs. mpg plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Weight vs. MPG\\") plt.show() # 2. Polynomial residual plot for horsepower vs. mpg plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Polynomial Residual Plot: Horsepower vs. MPG\\") plt.show() # 3. Polynomial residual plot with LOWESS curve for horsepower vs. mpg plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws={\'color\': \'r\'}) plt.title(\\"Polynomial Residual Plot with LOWESS: Horsepower vs. MPG\\") plt.show() ``` Write your solution to produce the requested analysis and visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_residuals(): # Set theme sns.set_theme() # Load dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Residual plot for weight vs. mpg using simple linear regression plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", color=\\"blue\\") plt.title(\\"Residual Plot: Weight vs. MPG\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # 2. Polynomial residual plot for horsepower vs. mpg using order 2 plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, color=\\"green\\") plt.title(\\"Polynomial Residual Plot: Horsepower vs. MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # 3. Polynomial residual plot with LOWESS curve for horsepower vs. mpg plt.figure(figsize=(10, 5)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws={\'color\': \'red\'}, scatter_kws={\'color\': \'grey\'}) plt.title(\\"Polynomial Residual Plot with LOWESS: Horsepower vs. MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"# Exercise: Implementing a Custom Sequence Class In this exercise, you are required to implement a custom sequence class in Python that mimics the behavior of various sequence operations outlined in the provided documentation. Your class should be named `CustomSequence` and should allow for functionalities such as indexing, slicing, concatenation, repetition, and element modification. Requirements 1. **Initialization**: The class should be able to initialize with an iterable (list, tuple, etc.). 2. **Length**: Implement the ability to return the length of the sequence using `len()`. 3. **Indexing**: Implement the ability to access elements via indexing `o[i]` and handle out-of-bounds indices. 4. **Concatenation**: Implement the ability to concatenate two `CustomSequence` objects using `+`. 5. **Repetition**: Implement the ability to repeat the sequence using `*`. 6. **Slicing**: Implement the ability to support slicing `o[i1:i2]`. 7. **Getting Items**: Implement the method to get items, `get_item(i)`, matching the functionality of `PySequence_GetItem`. 8. **Setting Items**: Implement the ability to set elements via `o[i] = v`. 9. **Deleting Items**: Implement the ability to delete elements via `del o[i]`. Constraints - The input to initialize the `CustomSequence` should be a python iterable (list, tuple, etc.). - Index operations should raise appropriate errors if indices are out of bounds. - Slicing should return a new `CustomSequence` object. - Element setting and deletion should respect the original sequence\'s constraints. Example Usage ```python # Initialization seq = CustomSequence([1, 2, 3, 4, 5]) # Length print(len(seq)) # Output: 5 # Indexing print(seq[2]) # Output: 3 # Concatenation seq2 = CustomSequence([6, 7, 8]) print(seq + seq2) # Output: CustomSequence([1, 2, 3, 4, 5, 6, 7, 8]) # Repetition print(seq * 2) # Output: CustomSequence([1, 2, 3, 4, 5, 1, 2, 3, 4, 5]) # Slicing print(seq[1:4]) # Output: CustomSequence([2, 3, 4]) # Getting Item print(seq.get_item(1)) # Output: 2 # Setting Item seq[1] = 20 print(seq) # Output: CustomSequence([1, 20, 3, 4, 5]) # Deleting Item del seq[2] print(seq) # Output: CustomSequence([1, 20, 4, 5]) ``` Implement the `CustomSequence` class in Python with the specified functionalities.","solution":"class CustomSequence: def __init__(self, iterable): self.data = list(iterable) def __len__(self): return len(self.data) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self.data[index]) return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __add__(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Can only concatenate CustomSequence (not \'{}\') to CustomSequence\\".format(type(other).__name__)) return CustomSequence(self.data + other.data) def __mul__(self, times): return CustomSequence(self.data * times) def get_item(self, index): return self.__getitem__(index) def __repr__(self): return f\\"CustomSequence({self.data})\\""},{"question":"<|Analysis Begin|> The provided document highlights the Kernel Ridge Regression (KRR) in the scikit-learn library. Kernel Ridge Regression combines ridge regression with kernel methods to learn a linear function in a transformed space defined by the kernel. The document compares KRR with Support Vector Regression (SVR) and discusses aspects such as speed of fitting and prediction on different dataset sizes, kernel functions, and regularization techniques. Key points to note: - KRR uses squared error loss combined with l2 regularization. - Unlike SVR, the fitting of KRR can be done in closed form and is typically faster for medium-sized datasets. - KRR results in a non-sparse model, making predictions slower compared to SVR on larger datasets. - Various kernels, including non-linear ones, can be applied in KRR. Using this information, a coding assessment question can be designed to test students\' understanding of implementing and optimizing KRR using scikit-learn, comparing it to SVR, and analyzing results. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Demonstrate your understanding of Kernel Ridge Regression (KRR) and Support Vector Regression (SVR) using the scikit-learn library. Task: 1. **Implement and Optimize Kernel Ridge Regression:** - Load the provided dataset (included below). - Implement Kernel Ridge Regression with an RBF kernel. - Perform hyperparameter optimization using grid search to find the best values for the kernel bandwidth and regularization parameter (alpha). - Evaluate the model on a test set and report the mean squared error (MSE). 2. **Compare with Support Vector Regression:** - Implement Support Vector Regression with an RBF kernel. - Perform hyperparameter optimization using grid search to find the best values for the kernel bandwidth, regularization parameter (C), and epsilon. - Evaluate the model on the same test set and report the mean squared error (MSE). 3. **Analysis:** - Compare the training and prediction times for both models. - Discuss the results in terms of performance, complexity, and execution speed. Dataset: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split # Generating a noisy sinusoidal dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += 2 * (0.5 - np.random.rand(20)) # Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) ``` Implementation Details: 1. **Kernel Ridge Regression Implementation:** - Use `KernelRidge` from `sklearn.kernel_ridge`. - Perform grid search using `GridSearchCV` from `sklearn.model_selection` to optimize the parameters. - Parameters to optimize: `alpha`, `gamma` (kernel parameter for RBF). 2. **Support Vector Regression Implementation:** - Use `SVR` from `sklearn.svm`. - Perform grid search using `GridSearchCV` from `sklearn.model_selection` to optimize the parameters. - Parameters to optimize: `C`, `epsilon`, `gamma`. 3. **Evaluation Metrics and Comparison:** - Evaluate the model\'s performance using Mean Squared Error (MSE). - Measure and report the training and prediction times for both models. - Analyze and discuss the obtained results comparing KRR and SVR. Expected Functions and Structure: ```python from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error import time # Function to implement KRR and perform hyperparameter optimization def kernel_ridge_regression(X_train, y_train, X_test, y_test): # Define the model and hyperparameter grid krr = KernelRidge(kernel=\'rbf\') param_grid = {\'alpha\': [1e-3, 1e-2, 1e-1, 1], \'gamma\': np.logspace(-2, 2, 5)} # Perform grid search grid_search_krr = GridSearchCV(krr, param_grid, cv=5) # Measure training time start_time = time.time() grid_search_krr.fit(X_train, y_train) training_time = time.time() - start_time # Make predictions and calculate MSE y_pred = grid_search_krr.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Measure prediction time start_time = time.time() y_pred = grid_search_krr.predict(X_test) prediction_time = time.time() - start_time return mse, training_time, prediction_time, grid_search_krr.best_params_ # Function to implement SVR and perform hyperparameter optimization def support_vector_regression(X_train, y_train, X_test, y_test): # Define the model and hyperparameter grid svr = SVR(kernel=\'rbf\') param_grid = {\'C\': [1, 10, 100, 1000], \'gamma\': np.logspace(-2, 2, 5), \'epsilon\': [0.1, 0.2, 0.5, 1] } # Perform grid search grid_search_svr = GridSearchCV(svr, param_grid, cv=5) # Measure training time start_time = time.time() grid_search_svr.fit(X_train, y_train) training_time = time.time() - start_time # Make predictions and calculate MSE y_pred = grid_search_svr.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Measure prediction time start_time = time.time() y_pred = grid_search_svr.predict(X_test) prediction_time = time.time() - start_time return mse, training_time, prediction_time, grid_search_svr.best_params_ # Main function to perform the tasks if __name__ == \\"__main__\\": krr_mse, krr_train_time, krr_pred_time, krr_best_params = kernel_ridge_regression(X_train, y_train, X_test, y_test) svr_mse, svr_train_time, svr_pred_time, svr_best_params = support_vector_regression(X_train, y_train, X_test, y_test) print(\\"Kernel Ridge Regression Results:\\") print(f\\"MSE: {krr_mse}\\") print(f\\"Training Time: {krr_train_time}\\") print(f\\"Prediction Time: {krr_pred_time}\\") print(f\\"Best Parameters: {krr_best_params}\\") print(\\"Support Vector Regression Results:\\") print(f\\"MSE: {svr_mse}\\") print(f\\"Training Time: {svr_train_time}\\") print(f\\"Prediction Time: {svr_pred_time}\\") print(f\\"Best Parameters: {svr_best_params}\\") # Analysis and comparison based on the results # Write your analysis here... ``` Constraints: - Ensure that the dataset is preprocessed appropriately (if necessary). - Use appropriate grid search strategy to avoid excessive runtime. - Make sure to handle overfitting by properly splitting the dataset. Note: The dataset and detailed code structure is provided. Complete the function implementations and ensure the analysis covers key performance comparisons.","solution":"from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.metrics import mean_squared_error import numpy as np import time # Generating a noisy sinusoidal dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += 2 * (0.5 - np.random.rand(20)) # Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) def kernel_ridge_regression(X_train, y_train, X_test, y_test): # Define the model and hyperparameter grid krr = KernelRidge(kernel=\'rbf\') param_grid = {\'alpha\': [1e-3, 1e-2, 1e-1, 1], \'gamma\': np.logspace(-2, 2, 5)} # Perform grid search grid_search_krr = GridSearchCV(krr, param_grid, cv=5) # Measure training time start_time = time.time() grid_search_krr.fit(X_train, y_train) training_time = time.time() - start_time # Make predictions and calculate MSE y_pred = grid_search_krr.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Measure prediction time start_time = time.time() y_pred = grid_search_krr.predict(X_test) prediction_time = time.time() - start_time return mse, training_time, prediction_time, grid_search_krr.best_params_ def support_vector_regression(X_train, y_train, X_test, y_test): # Define the model and hyperparameter grid svr = SVR(kernel=\'rbf\') param_grid = {\'C\': [1, 10, 100, 1000], \'gamma\': np.logspace(-2, 2, 5), \'epsilon\': [0.1, 0.2, 0.5, 1]} # Perform grid search grid_search_svr = GridSearchCV(svr, param_grid, cv=5) # Measure training time start_time = time.time() grid_search_svr.fit(X_train, y_train) training_time = time.time() - start_time # Make predictions and calculate MSE y_pred = grid_search_svr.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Measure prediction time start_time = time.time() y_pred = grid_search_svr.predict(X_test) prediction_time = time.time() - start_time return mse, training_time, prediction_time, grid_search_svr.best_params_ # Main function to perform the tasks if __name__ == \\"__main__\\": krr_mse, krr_train_time, krr_pred_time, krr_best_params = kernel_ridge_regression(X_train, y_train, X_test, y_test) svr_mse, svr_train_time, svr_pred_time, svr_best_params = support_vector_regression(X_train, y_train, X_test, y_test) print(\\"Kernel Ridge Regression Results:\\") print(f\\"MSE: {krr_mse}\\") print(f\\"Training Time: {krr_train_time}\\") print(f\\"Prediction Time: {krr_pred_time}\\") print(f\\"Best Parameters: {krr_best_params}\\") print(\\"Support Vector Regression Results:\\") print(f\\"MSE: {svr_mse}\\") print(f\\"Training Time: {svr_train_time}\\") print(f\\"Prediction Time: {svr_pred_time}\\") print(f\\"Best Parameters: {svr_best_params}\\") # Analysis and comparison based on the results, add your analysis here..."},{"question":"Coding Assessment Question: Implementing and Validating a Ridge Regression Model using Scikit-learn # Objective This task is designed to assess your understanding of ridge regression, regularization concepts, cross-validation techniques, and model evaluation using the scikit-learn library. # Task 1. Implement a Ridge Regression model using the scikit-learn library. 2. Use cross-validation to find the optimal `alpha` parameter for the ridge regression model. 3. Evaluate the model\'s performance using appropriate metrics and compare it with ordinary least squares regression. # Input - A CSV file containing the dataset: `data.csv`. - The dataset includes features in columns `X1, X2, ..., Xn` and the target variable in column `y`. - Note: You can generate a sample dataset if preferred or use any dataset available. # Output - Mean Squared Error (MSE) for both OLS regression and the optimal Ridge Regression model. - Coefficients for both models. - The optimal `alpha` value found through cross-validation. # Constraints - Use `Ridge` and `RidgeCV` classes from scikit-learn. - Use a range of `alpha` values from `0.1` to `100` (logarithmically spaced). # Performance Requirements - Ensure the model evaluation metrics are clear and precise. - Demonstrate the impact of regularization by comparing OLS and Ridge Regression results. # Detailed Steps 1. **Data Preparation:** - Load the dataset from the provided CSV file. - Split the dataset into features (`X`) and target (`y`). 2. **Implement Ordinary Least Squares Regression:** - Fit an OLS model using `LinearRegression` from scikit-learn. - Compute and print the MSE and coefficients for the OLS model. 3. **Implement Ridge Regression with Cross-validation:** - Use `RidgeCV` to perform cross-validation over the range of `alpha` values. - Determine the optimal `alpha` using the cross-validation results. - Fit the ridge regression model with the optimal `alpha`. - Compute and print the MSE and coefficients for the Ridge model. - Print the optimal `alpha` value found. 4. **Comparison and Discussion:** - Compare the results of the OLS model and Ridge model. - Discuss the impact of the regularization parameter on model performance and coefficients. # Example Code Outline ```python import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression, RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(\'y\', axis=1) y = data[\'y\'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement OLS Regression ols_model = LinearRegression() ols_model.fit(X_train, y_train) y_pred_ols = ols_model.predict(X_test) mse_ols = mean_squared_error(y_test, y_pred_ols) coefficients_ols = ols_model.coef_ # Implement Ridge Regression with Cross-validation alphas = np.logspace(-1, 2, 100) ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True) ridge_cv.fit(X_train, y_train) optimal_alpha = ridge_cv.alpha_ ridge_model = RidgeCV(alphas=[optimal_alpha]) ridge_model.fit(X_train, y_train) y_pred_ridge = ridge_model.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) coefficients_ridge = ridge_model.coef_ # Print the results print(f\\"OLS MSE: {mse_ols}\\") print(f\\"OLS Coefficients: {coefficients_ols}\\") print(f\\"Ridge MSE: {mse_ridge}\\") print(f\\"Ridge Coefficients: {coefficients_ridge}\\") print(f\\"Optimal Alpha: {optimal_alpha}\\") # Discuss the impact of regularization # (Include any observations, interpretations, and learning points here) ``` # Notes - Ensure you handle any missing data or preprocessing required before model fitting. - Add appropriate comments and documentation to your code. - Experiment with different datasets if time permits to validate your implementation.","solution":"import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression, RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def ridge_regression(filename: str): Implements and evaluates a Ridge Regression model. Args: filename (str): The path to the dataset CSV file. Returns: dict: A dictionary with MSE and coefficients for OLS and Ridge Regression, and the optimal alpha value. # Load the dataset data = pd.read_csv(filename) X = data.drop(\'y\', axis=1) y = data[\'y\'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement OLS Regression ols_model = LinearRegression() ols_model.fit(X_train, y_train) y_pred_ols = ols_model.predict(X_test) mse_ols = mean_squared_error(y_test, y_pred_ols) coefficients_ols = ols_model.coef_ # Implement Ridge Regression with Cross-validation alphas = np.logspace(-1, 2, 100) ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True) ridge_cv.fit(X_train, y_train) optimal_alpha = ridge_cv.alpha_ ridge_model = RidgeCV(alphas=[optimal_alpha]) ridge_model.fit(X_train, y_train) y_pred_ridge = ridge_model.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) coefficients_ridge = ridge_model.coef_ # Return the results return { \\"mse_ols\\": mse_ols, \\"coefficients_ols\\": coefficients_ols, \\"mse_ridge\\": mse_ridge, \\"coefficients_ridge\\": coefficients_ridge, \\"optimal_alpha\\": optimal_alpha }"},{"question":"# Audio Processing and Analysis Using `audioop` The goal of this task is to process and analyze raw audio data fragments using the `audioop` module. You need to implement a function that performs multiple operations on an audio fragment to compute its processed version and key characteristics. **Problem Statement**: You are given a raw audio fragment, and you need to: 1. Convert the audio fragment to a different sample width. 2. Compute the root mean square (RMS) value of the converted fragment. 3. Add a specified bias to the converted fragment. 4. Compute the average peak-peak value of the biased fragment. 5. Count the number of zero crossings in the biased fragment. Implement the function `process_audio_fragment(fragment: bytes, width: int, newwidth: int, bias: int) -> dict`, which takes an audio fragment and processes it according to the steps mentioned above, returning a dictionary with the following keys: - `\\"converted_fragment\\"`: the audio fragment after conversion to the new width. - `\\"rms_value\\"`: the RMS value of the converted fragment. - `\\"biased_fragment\\"`: the audio fragment after adding the specified bias. - `\\"avg_pp_value\\"`: the average peak-peak value of the biased fragment. - `\\"zero_crossings\\"`: the number of zero crossings in the biased fragment. **Constraints**: - The input fragment is a bytes-like object consisting of signed integer 16-bit samples. - The width is always 2 (16-bit samples). - The `newwidth` can be 1, 2, 3, or 4. - The input fragment length is guaranteed to be a multiple of the `width`. **Example**: ```python fragment = b\'x01x02x03x04x05x06x07x08\' width = 2 newwidth = 1 bias = 128 result = process_audio_fragment(fragment, width, newwidth, bias) print(result) ``` Expected output (the exact output will vary based on the implementations of `audioop` functions): ```python { \\"converted_fragment\\": b\'...\', \\"rms_value\\": ..., \\"biased_fragment\\": b\'...\', \\"avg_pp_value\\": ..., \\"zero_crossings\\": ... } ``` **Hints**: - Use `audioop.lin2lin` to convert sample widths. - Use `audioop.rms` to compute the RMS value. - Use `audioop.bias` to add bias to the fragment. - Use `audioop.avgpp` to compute the average peak-peak value. - Use `audioop.cross` to count zero crossings.","solution":"import audioop def process_audio_fragment(fragment: bytes, width: int, newwidth: int, bias: int) -> dict: Processes an audio fragment by converting its sample width, computing RMS, adding a bias, computing average peak-peak value, and counting zero crossings. Parameters: fragment (bytes): The raw audio fragment. width (int): The current width of the fragment samples (always 2 for 16-bit). newwidth (int): The new width to which the fragment should be converted. bias (int): The bias to be added to the converted fragment. Returns: dict: A dictionary containing the processed fragment and computed values. # Convert the audio fragment to the new sample width converted_fragment = audioop.lin2lin(fragment, width, newwidth) # Compute the RMS value of the converted fragment rms_value = audioop.rms(converted_fragment, newwidth) # Add the specified bias to the converted fragment biased_fragment = audioop.bias(converted_fragment, newwidth, bias) # Compute the average peak-peak value of the biased fragment avg_pp_value = audioop.avgpp(biased_fragment, newwidth) # Count the number of zero crossings in the biased fragment zero_crossings = audioop.cross(biased_fragment, newwidth) return { \\"converted_fragment\\": converted_fragment, \\"rms_value\\": rms_value, \\"biased_fragment\\": biased_fragment, \\"avg_pp_value\\": avg_pp_value, \\"zero_crossings\\": zero_crossings }"},{"question":"**Title**: Building an Advanced Log File Analyzer **Objective**: Design a Python function using the `re` library to analyze and extract specific information from a log file. This task will test your understanding of regex syntax, functions, grouping, and flags in the `re` library. **Description**: You are provided with a log file containing records in the following format: ``` [2023-10-19 08:30:45] ERROR Module: user Auth fail due to invalid credentials [2023-10-19 08:30:55] INFO Module: server Started server on port 8000 [2023-10-19 08:31:17] WARNING Module: network High latency detected [2023-10-19 08:32:23] ERROR Module: database Unable to connect to DB [2023-10-19 08:33:03] INFO Module: user User logged in successfully ``` Each line of the log starts with a timestamp, followed by a log level (e.g., ERROR, INFO, WARNING), then the module name, and a description message. **Task**: Write a function `analyze_log_file(log_contents: str) -> dict` that takes the content of the log file as a single string and returns a dictionary with the following structure: ```python { \\"total_entries\\": int, # Total number of log entries \\"error_count\\": int, # Number of ERROR log entries \\"modules\\": { # Dictionary of module names as keys and their log messages as lists of strings \\"user\\": [\\"Auth fail due to invalid credentials\\", \\"User logged in successfully\\"], \\"server\\": [\\"Started server on port 8000\\"], \\"network\\": [\\"High latency detected\\"], \\"database\\": [\\"Unable to connect to DB\\"] } } ``` # Requirements: 1. Use regular expressions to parse the log entries. 2. Handle log entries accurately and extract relevant information. 3. Consider edge cases such as unexpected log formats. 4. Ensure performance is optimal for large log files. # Constraints: - Do not use external libraries; only use the standard Python library. - Assume the log file string may contain multiple lines with varied log entries. - The timestamp always follows the format `[YYYY-MM-DD HH:MM:SS]`. # Example usage: ```python log_contents = [2023-10-19 08:30:45] ERROR Module: user Auth fail due to invalid credentials [2023-10-19 08:30:55] INFO Module: server Started server on port 8000 [2023-10-19 08:31:17] WARNING Module: network High latency detected [2023-10-19 08:32:23] ERROR Module: database Unable to connect to DB [2023-10-19 08:33:03] INFO Module: user User logged in successfully result = analyze_log_file(log_contents) print(result) ``` Expected output: ```python { \\"total_entries\\": 5, \\"error_count\\": 2, \\"modules\\": { \\"user\\": [\\"Auth fail due to invalid credentials\\", \\"User logged in successfully\\"], \\"server\\": [\\"Started server on port 8000\\"], \\"network\\": [\\"High latency detected\\"], \\"database\\": [\\"Unable to connect to DB\\"] } } ``` Submit your solution with implementation and appropriate test cases to validate your function.","solution":"import re from collections import defaultdict def analyze_log_file(log_contents: str) -> dict: log_pattern = re.compile( r\'[(?P<timestamp>d{4}-d{2}-d{2} d{2}:d{2}:d{2})] \' r\'(?P<log_level>ERROR|INFO|WARNING) \' r\'Module: (?P<module>w+) \' r\'(?P<message>.+)\' ) total_entries = 0 error_count = 0 modules = defaultdict(list) for match in log_pattern.finditer(log_contents): total_entries += 1 log_level = match.group(\'log_level\') module = match.group(\'module\') message = match.group(\'message\') if log_level == \'ERROR\': error_count += 1 modules[module].append(message) return { \\"total_entries\\": total_entries, \\"error_count\\": error_count, \\"modules\\": dict(modules) }"},{"question":"You are tasked with building an asynchronous system to concurrently retrieve data from multiple external sources (simulated as subprocesses) and aggregate the results. You should manage and synchronize multiple tasks efficiently using Python\'s `asyncio` library. Requirements 1. **Concurrent Subprocess Execution**: - Create multiple subprocesses to simulate external data sources. - Each subprocess will output data after a random delay. 2. **Task Management**: - Utilize asyncio tasks to manage subprocesses concurrently. - Handle possible timeouts and cancellations efficiently. 3. **Data Aggregation**: - Collect and aggregate results from all subprocesses. - Use synchronization primitives to ensure thread-safe data aggregation. 4. **Graceful Shutdown**: - Implement a mechanism to cancel all tasks and clean up resources upon receiving a shutdown signal. Input and Output - **Input**: None (You will simulate input with subprocesses). - **Output**: A dictionary where keys are task names and values are results of the respective subprocess. Constraints - Maximum number of subprocesses: 10. - Each subprocess should have a timeout of 5 seconds. - Ensure that the code gracefully handles subprocess failures and aggregations even if one or more subprocesses fail. Example ```python { \'task1\': \'result_from_subprocess1\', \'task2\': \'result_from_subprocess2\', ... } ``` # Function Signature ```python import asyncio import random async def fetch_data_from_subprocess(task_name: str) -> str: Simulate a subprocess that returns data after a random delay. Args: - task_name (str): The name of the task. Returns: - str: Data fetched from the subprocess. # Implementation here async def main(): Main function to orchestrate the subprocess management and aggregation. Returns: - dict: Aggregated results from subprocesses. # Implementation here if __name__ == \'__main__\': result = asyncio.run(main()) print(result) ``` # Implementation Guidelines 1. **Subprocess Simulation (`fetch_data_from_subprocess`)**: - Simulate a delay using `await asyncio.sleep(random_delay)`. - Return a string indicating data fetched after the delay. 2. **Main Function (`main`)**: - Create and manage asyncio tasks for each subprocess. - Use `asyncio.gather()` to wait for all tasks concurrently, with each task having a timeout. - Use synchronization primitives like `asyncio.Lock` to ensure thread-safe result aggregation. - Handle task cancellations and timeouts gracefully. - Ensure that the program prints the aggregated results dictionary upon completion. Notes - Use `asyncio.create_subprocess_exec()` or any other suitable function to simulate subprocess if necessary. - Ensure robust handling of exceptions and edge cases, including timeouts and cancellations.","solution":"import asyncio import random async def fetch_data_from_subprocess(task_name: str) -> str: Simulate a subprocess that returns data after a random delay. Args: - task_name (str): The name of the task. Returns: - str: Data fetched from the subprocess. delay = random.uniform(1, 4) await asyncio.sleep(delay) return f\\"result_from_{task_name}\\" async def main(): Main function to orchestrate the subprocess management and aggregation. Returns: - dict: Aggregated results from subprocesses. task_names = [f\\"task{i+1}\\" for i in range(10)] tasks = [asyncio.create_task(fetch_data_from_subprocess(name)) for name in task_names] results = {} for task_name, task in zip(task_names, tasks): try: result = await asyncio.wait_for(task, timeout=5.0) results[task_name] = result except asyncio.TimeoutError: results[task_name] = \\"timeout_error\\" return results if __name__ == \'__main__\': result = asyncio.run(main()) print(result)"},{"question":"**Objective:** Demonstrate proficiency in using seaborn for data visualization by creating a complex line plot with customized aesthetics and multiple semantics. **Scenario:** You are provided with the `fmri` dataset available in the seaborn package. The dataset contains measurements of brain activity from an fMRI study. Your task is to visualize the time-series data to show the dynamics of brain signal changes over time, highlighting different regions and events. **Requirements:** 1. **Load Data:** - Load the dataset `fmri` from the seaborn library. 2. **Data Preparation:** - Filter the dataset to only include data where the `event` is either \'stim\' or \'cue\'. 3. **Plotting:** - Create a line plot using seaborn\'s `sns.lineplot()` function. - Plot the signal (`y`) against the timepoint (`x`). - Use `hue` to differentiate between different regions. - Use `style` to differentiate between different events. - Enable markers and disable dashes in the styles. - Use a custom color palette from seaborn for the plot. 4. **Faceting:** - Use seaborn\'s `sns.relplot()` function to create a grid of plots (facets). - Create one facet for each combination of `region` and `event`. - Ensure synchronization of the semantic mappings (color, style) across all facets. 5. **Customization:** - Customize the plot to improve readability: - Set the title of each plot facet to indicate the `region` and `event`. - Add axis labels that clearly describe the data. - Increase the linewidth of the plots for better visualization. 6. **Output:** - The code should output the faceted line plots, correctly visualizing the differences in brain signal dynamics across different regions and events. **Constraints:** - You may use only seaborn, pandas, numpy, and matplotlib for this task. **Input and Output Formats:** - **Input:** None (Dataset is loaded within the code). - **Output:** A grid of faceted line plots. **Example Code Template:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the dataset fmri = fmri[fmri[\'event\'].isin([\'stim\', \'cue\'])] # Create the line plot with customizations g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", col=\\"region\\", row=\\"event\\", hue=\\"region\\", style=\\"event\\", kind=\\"line\\", markers=True, dashes=False, palette=\\"muted\\", facet_kws={\'sharey\': False, \'sharex\': True} ) # Customize plot appearance g.set_titles(\\"{row_name} | {col_name}\\") g.set_axis_labels(\\"Timepoint\\", \\"Signal\\") g.map(plt.axhline, y=0, ls=\\"--\\", c=\\"gray\\", lw=1) plt.show() ``` **Note:** This example is illustrative. The students must write their own implementation based on the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_fmri_data(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the dataset to include only \'stim\' and \'cue\' events fmri = fmri[fmri[\'event\'].isin([\'stim\', \'cue\'])] # Create the line plot with seaborn g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", col=\\"region\\", row=\\"event\\", hue=\\"region\\", style=\\"event\\", kind=\\"line\\", markers=True, dashes=False, palette=\\"muted\\", facet_kws={\'sharey\': True, \'sharex\': True} ) # Customize plot appearance g.set_titles(\\"{row_name} | {col_name}\\") g.set_axis_labels(\\"Timepoint\\", \\"Signal\\") for ax in g.axes.flat: ax.axhline(y=0, ls=\\"--\\", c=\\"gray\\", lw=1) plt.show()"},{"question":"# Seaborn Error Bars Assessment Question **Objective:** Test the ability to implement and customize error bars in data visualizations using the Seaborn library. You are provided with a dataset and are required to visualize the data using Seaborn\'s plotting functions. Your task is to create two plots: 1. A **point plot** with error bars representing the **standard deviation**. 2. A **strip plot** with custom error bars showing the **percentile interval** (90% interval). **Dataset:** A CSV file named `data.csv` with the following columns: - `category`: Categorical variable. - `value`: Continuous variable. **Requirements:** 1. Load the dataset using `pandas`. 2. Create a point plot (`sns.pointplot`) that shows the mean of the `value` column for each `category` with error bars representing the standard deviation. 3. Customize the point plot\'s error bars to show plus/minus one standard deviation. 4. Create a strip plot (`sns.stripplot`) that shows individual `value` data points for each `category` and use custom error bars to show the middle 90% of the data (5th to 95th percentile). 5. Display both plots in a single figure using subplots. **Constraints:** - You must use Seaborn\'s `errorbar` parameter for implementing error bars. - Do not use any other library for adding error bars. **Input:** - A CSV file `data.csv` with columns `category` (string) and `value` (float). **Output:** - A matplotlib figure with two subplots: - The first subplot should contain the point plot with standard deviation error bars. - The second subplot should contain the strip plot with custom percentile error bars. **Example:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'data.csv\') # Create a figure with subplots fig, axs = plt.subplots(2, 1, figsize=(10, 10)) # Point plot with standard deviation error bars sns.pointplot(data=df, x=\'category\', y=\'value\', errorbar=\'sd\', capsize=.2, ax=axs[0]) # Custom function for percentile interval error bars def percentile_interval(x): return np.percentile(x, [5, 95]) # Strip plot with custom percentile error bars sns.pointplot(data=df, x=\'category\', y=\'value\', errorbar=percentile_interval, ax=axs[1]) sns.stripplot(data=df, x=\'category\', y=\'value\', jitter=True, color=\'gray\', alpha=0.6, ax=axs[1]) # Display the plots plt.tight_layout() plt.show() ``` **Notes:** - The above example is just a template to guide you. Customize your error bars and visualize them as described. - Ensure the plots are well-labeled with appropriate titles and axis labels.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_plots(data_path): # Load the dataset df = pd.read_csv(data_path) # Create a figure with subplots fig, axs = plt.subplots(2, 1, figsize=(10, 10)) # Point plot with standard deviation error bars sns.pointplot(data=df, x=\'category\', y=\'value\', errorbar=\'sd\', capsize=.2, ax=axs[0]) axs[0].set_title(\'Point Plot with Standard Deviation Error Bars\') axs[0].set_xlabel(\'Category\') axs[0].set_ylabel(\'Value\') # Custom function for percentile interval error bars def percentile_interval(x): return np.percentile(x, [5, 95]) # Strip plot with custom percentile error bars sns.pointplot(data=df, x=\'category\', y=\'value\', errorbar=percentile_interval, ax=axs[1]) sns.stripplot(data=df, x=\'category\', y=\'value\', jitter=True, color=\'gray\', alpha=0.6, ax=axs[1]) axs[1].set_title(\'Strip Plot with 90% Percentile Interval Error Bars\') axs[1].set_xlabel(\'Category\') axs[1].set_ylabel(\'Value\') # Display the plots plt.tight_layout() plt.show()"},{"question":"# Custom Asynchronous WebSocket Protocol **Objective:** Implement a custom asynchronous WebSocket protocol using Python\'s `asyncio` library. Your implementation should handle WebSocket connections, manage a simple echo service, and demonstrate efficient buffer management. **Details:** 1. **Protocol Requirements:** - Implement the WebSocket handshake to establish a connection. - Define methods to handle incoming messages and send responses. - Implement proper error handling and connection termination. 2. **Transport Requirements:** - Utilize appropriate `Transport` and `Protocol` classes provided by `asyncio` for efficient and non-blocking I/O operations. - Simulate a WebSocket client to test your protocol. **Function Signatures:** - `class WebSocketProtocol(asyncio.Protocol):` - Handles WebSocket-specific behaviors. - `async def start_server():` - Starts the WebSocket server. **Expected Input:** A WebSocket client connects to the server, performs a handshake, sends a message, and expects an echoed response. **Expected Output:** The server correctly processes the WebSocket client\'s request, performs the handshake, and echoes received messages back. **Constraints:** - Execute all operations asynchronously. - Maintain non-blocking behavior and handle concurrency properly. - Ensure the server can handle multiple simultaneous connections efficiently. **Sample WebSocket Client (for testing):** ```python import asyncio class WebSocketClientProtocol(asyncio.Protocol): def __init__(self, message, loop): self.message = message self.loop = loop def connection_made(self, transport): self.transport = transport # Send WebSocket handshake here transport.write(b\\"GET / HTTP/1.1rn\\" b\\"Connection: Upgradern\\" b\\"Upgrade: websocketrn\\" b\\"rn\\") transport.write(self.message.encode()) print(f\'Sent: {self.message}\') def data_received(self, data): print(f\'Received: {data.decode()}\') self.transport.close() def connection_lost(self, exc): print(\\"Connection closed\\") self.loop.stop() async def start_client(loop): message = \\"Hello WebSocket!\\" await loop.create_connection(lambda: WebSocketClientProtocol(message, loop), \'127.0.0.1\', 8765) if __name__ == \\"__main__\\": loop = asyncio.get_event_loop() loop.run_until_complete(start_client(loop)) loop.run_forever() loop.close() ``` **Implementation Example:** Use the provided skeleton for your server implementation and ensure it complies with the constraints and requirements. ```python import asyncio class WebSocketProtocol(asyncio.Protocol): def __init__(self): super().__init__() def connection_made(self, transport): self.transport = transport print(\'Connection established\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') # Send back the same message self.transport.write(message.encode()) def connection_lost(self, exc): print(\'Connection closed\') async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: WebSocketProtocol(), \'127.0.0.1\', 8765) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_server()) ``` Write the missing parts of the WebSocket handshake and message handling in `WebSocketProtocol`. Ensure the server starts correctly and handles connections concurrently.","solution":"import asyncio import websockets from websockets import WebSocketServerProtocol class WebSocketEchoServer: async def handler(self, websocket: WebSocketServerProtocol, path: str): async for message in websocket: await websocket.send(message) async def start_server(): server = WebSocketEchoServer() async with websockets.serve(server.handler, \'127.0.0.1\', 8765): await asyncio.Future() # run forever if __name__ == \\"__main__\\": asyncio.run(start_server())"},{"question":"# Question: Implementing and Training a Bernoulli Restricted Boltzmann Machine **Objective**: Implement and train a Bernoulli Restricted Boltzmann Machine (RBM) using the learning algorithm discussed â€” Stochastic Maximum Likelihood (SML). **Task**: Write a function `train_rbm(data, n_hidden, learning_rate, n_iterations)` that: - Initializes an RBM with a given number of hidden units. - Trains the RBM using Stochastic Maximum Likelihood. - Returns the learned weights and biases for the visible and hidden layers. **Function Signature**: ```python def train_rbm(data: np.ndarray, n_hidden: int, learning_rate: float, n_iterations: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: pass ``` **Parameters**: - `data` (np.ndarray): A 2D numpy array of shape (n_samples, n_visible) containing the training data. Each element should be a value between 0 and 1. - `n_hidden` (int): The number of hidden units in the RBM. - `learning_rate` (float): The learning rate for updating the parameters. - `n_iterations` (int): The number of iterations to run the training for. **Returns**: - `weights` (np.ndarray): A 2D numpy array of shape (n_visible, n_hidden) representing the learned weights. - `visible_bias` (np.ndarray): A 1D numpy array representing the bias for the visible units. - `hidden_bias` (np.ndarray): A 1D numpy array representing the bias for the hidden units. - `samples` (np.ndarray): A 2D numpy array of shape (n_samples, n_hidden) containing the transformed data samples using the hidden layer. **Constraints**: - Assume the input data is properly preprocessed to contain only values between 0 and 1. - Implement one Gibbs sampling step per iteration as described in the SML algorithm. - Use the sigmoid activation function for computing probabilities. # Example ```python import numpy as np # Consider a sample binary dataset (for simplicity) data = np.array([ [0, 1, 1, 0, 1], [1, 0, 1, 1, 0], [0, 1, 0, 1, 0], [1, 0, 0, 1, 1] ]) # Assume 3 hidden units, a learning rate of 0.1, and 1000 iterations weights, v_bias, h_bias, samples = train_rbm(data, n_hidden=3, learning_rate=0.1, n_iterations=1000) print(\\"Weights:n\\", weights) print(\\"Visible Bias:n\\", v_bias) print(\\"Hidden Bias:n\\", h_bias) print(\\"Samples:n\\", samples) ``` **Performance requirements**: - The algorithm should be efficient enough to handle datasets of moderate size (e.g., 1000 samples with 50 features). **Notes**: - Ensure numerical stability in your implementation by handling potential issues with sigmoid function calculations. - Implementing the Persistent Contrastive Divergence method as described, involving maintaining a chain of fantasy particles, is crucial for this task.","solution":"import numpy as np def sigmoid(x): return 1 / (1 + np.exp(-x)) def train_rbm(data, n_hidden, learning_rate, n_iterations): n_samples, n_visible = data.shape # Initialize weights and biases np.random.seed(0) weights = np.random.randn(n_visible, n_hidden) * 0.01 visible_bias = np.zeros(n_visible) hidden_bias = np.zeros(n_hidden) # Initialize persistent chain chain = np.random.rand(n_samples, n_visible) # Initialize with random data for iteration in range(n_iterations): # Positive phase positive_hidden_probs = sigmoid(np.dot(data, weights) + hidden_bias) positive_hidden_states = (positive_hidden_probs > np.random.rand(n_samples, n_hidden)).astype(np.float32) positive_associations = np.dot(data.T, positive_hidden_probs) # Negative phase negative_visible_probs = sigmoid(np.dot(positive_hidden_states, weights.T) + visible_bias) negative_visible_states = (negative_visible_probs > np.random.rand(n_samples, n_visible)).astype(np.float32) negative_hidden_probs = sigmoid(np.dot(negative_visible_states, weights) + hidden_bias) negative_associations = np.dot(negative_visible_states.T, negative_hidden_probs) # Update weights and biases weights += learning_rate * ((positive_associations - negative_associations) / n_samples) visible_bias += learning_rate * np.mean(data - negative_visible_states, axis=0) hidden_bias += learning_rate * np.mean(positive_hidden_probs - negative_hidden_probs, axis=0) # Update persistent chain for SML chain = negative_visible_states # Create samples to return samples = sigmoid(np.dot(data, weights) + hidden_bias) return weights, visible_bias, hidden_bias, samples"},{"question":"# Advanced Coding Assessment: Efficient Data Processing with itertools Objective: Your task is to implement a function named `process_data` that takes two lists of integers and returns a list of tuples. Each tuple will consist of pairs of elements where the first element is from the first list and the second element is from the second list, based on specific filter and combination conditions. Function Signature: ```python def process_data(list1: list, list2: list) -> list: pass ``` Input Parameters: - `list1 (list)`: A list of integers. - `list2 (list)`: Another list of integers. Constraints: 1. Both lists may have different lengths. 2. Use the `itertools` module to optimize the implementation. 3. Both lists are non-empty and may contain duplicate values. Expected Output: The function should return a list of tuples. Each tuple will: - Be formed by combining elements from the input lists. - Include only elements where the first element of the tuple is less than the second element. - Include combinations but not duplicates or permutations - e.g., (a, b) and (b, a) should not both be in the result unless they are different values. Example: ```python list1 = [1, 2, 3] list2 = [2, 3, 4] process_data(list1, list2) # Expected output: [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)] ``` Additional Notes: - Specifically use functions from the `itertools` module where applicable. - Consider the performance impact especially for very large input lists. - Ensure the function is readable and maintainable by using the relevant `itertools` functions. Guidance: To solve this problem, you might find the following `itertools` tools particularly useful: - `combinations` - `filterfalse` - `product` Please ensure your function adheres to the given input-output constraints and performance requirements.","solution":"import itertools def process_data(list1: list, list2: list) -> list: Returns a list of tuples where the first element is from list1 and the second element is from list2, such that the first element of the tuple is less than the second element. return [(x, y) for x, y in itertools.product(list1, list2) if x < y]"},{"question":"Objective Your task is to create a producer-consumer problem simulation using asyncio queues. You will implement an asynchronous system where producers generate tasks with different priorities and consumers process these tasks. Requirements - Implement a `producer` function that produces tasks with different priority levels. - Implement a `consumer` function that processes tasks based on their priority. - Use `asyncio.PriorityQueue` to manage the tasks. - Ensure the system can handle multiple producers and consumers concurrently. - Use the concepts of `put`, `get`, `task_done`, and `join` properly. Function Signatures ```py import asyncio async def producer(queue: asyncio.PriorityQueue, n: int): Produces n tasks with random priorities and adds them to the queue. :param queue: The priority queue to put the tasks into. :param n: The number of tasks to produce. pass async def consumer(queue: asyncio.PriorityQueue, consumer_id: int): Consumes tasks from the queue and processes them based on their priority. :param queue: The priority queue from which the tasks are retrieved. :param consumer_id: The unique ID of the consumer for logging purpose. pass async def main(): Main function to set up the queue, create producer and consumer tasks, and run the system. queue = asyncio.PriorityQueue() # Create producer and consumer tasks producers = [asyncio.create_task(producer(queue, n=10)) for _ in range(2)] consumers = [asyncio.create_task(consumer(queue, consumer_id=i)) for i in range(3)] # Wait until all items are processed await asyncio.gather(*producers) await queue.join() # Cancel all consumers for c in consumers: c.cancel() # Wait until all consumers are actually done await asyncio.gather(*consumers, return_exceptions=True) ``` Input and Output - The `producer` function should create tasks with random priorities (e.g., using `random.randint(1, 10)`) and add them to the priority queue. - The `consumer` function should continuously get tasks from the queue (sorted by priority), simulate processing (e.g., `await asyncio.sleep`), and log the completion of each task. - The console should print logs showing which consumer processed which task with what priority. Constraints - Use `asyncio.PriorityQueue` for task management. - Each task is represented as a tuple `(priority, task_id)`. - Demonstrate understanding of async/await, task scheduling, and queue operations. - Ensure clean cancellation of tasks. Example ``` Producer-1 added task 1 with priority 3 Producer-2 added task 2 with priority 1 Consumer-0 processing task 2 with priority 1 Consumer-1 processing task 1 with priority 3 ... ```","solution":"import asyncio import random async def producer(queue: asyncio.PriorityQueue, n: int, producer_id: int): Produces n tasks with random priorities and adds them to the queue. :param queue: The priority queue to put the tasks into. :param n: The number of tasks to produce. :param producer_id: The unique ID of the producer for logging purpose. for i in range(n): priority = random.randint(1, 10) task_id = f\\"Task-{producer_id}-{i}\\" await queue.put((priority, task_id)) print(f\\"Producer-{producer_id} added {task_id} with priority {priority}\\") await asyncio.sleep(random.random()) # Simulate the time taken to produce a task async def consumer(queue: asyncio.PriorityQueue, consumer_id: int): Consumes tasks from the queue and processes them based on their priority. :param queue: The priority queue from which the tasks are retrieved. :param consumer_id: The unique ID of the consumer for logging purpose. while True: try: priority, task_id = await queue.get() print(f\\"Consumer-{consumer_id} processing {task_id} with priority {priority}\\") await asyncio.sleep(random.random()) # Simulate the time taken to process a task queue.task_done() except asyncio.CancelledError: break async def main(): Main function to set up the queue, create producer and consumer tasks, and run the system. queue = asyncio.PriorityQueue() # Create producer and consumer tasks producers = [asyncio.create_task(producer(queue, n=10, producer_id=i)) for i in range(2)] consumers = [asyncio.create_task(consumer(queue, consumer_id=i)) for i in range(3)] # Wait until all producers are done await asyncio.gather(*producers) await queue.join() # Cancel all consumers for c in consumers: c.cancel() # Wait until all consumers are actually done await asyncio.gather(*consumers, return_exceptions=True) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Problem: Dining Philosophers Problem using asyncio** The Dining Philosophers Problem is a classic synchronization problem that involves a group of philosophers sitting at a round table with a fork placed between each pair of them. Philosophers alternately think and eat. However, a philosopher can only eat if they can pick up both the fork to their left and the fork to their right. In this context, forks are shared resources that need to be synchronized to avoid deadlocks and ensure that no two adjacent philosophers eat at the same time. **Task:** You are required to implement a solution to the Dining Philosophers Problem using the asyncio synchronization primitives provided in the documentation. **Specifications:** 1. Implement a class `DiningPhilosophers` with the following methods: - `__init__(self, num_philosophers: int)`: Initializes the dining philosophers\' setup with the given number of philosophers. - `async def wants_to_eat(self, philosopher: int, left_fork: asyncio.Lock, right_fork: asyncio.Lock) -> None`: The method simulates the action of a philosopher who wants to eat. The philosopher should acquire both the left and right forks asynchronously, eat for a while (simulate with `asyncio.sleep()`), and then release both forks. Ensure that the behavior is fair and that no deadlocks occur. 2. Use `asyncio.Lock` to represent forks. Each philosopher must acquire the locks on the two forks (left and right) before eating and release them afterward. **Input:** - An integer `num_philosophers` indicating the number of philosophers. - Multiple calls (asynchronously) to the method `wants_to_eat(philosopher: int, left_fork: asyncio.Lock, right_fork: asyncio.Lock)`. **Output:** - Simulate philosophers eating by printing `\\"Philosopher {i} starts eating\\"` followed by `\\"Philosopher {i} finishes eating\\"` for each philosopher `i`. **Constraints:** - The system should avoid deadlocks and ensure progress (fairness in acquiring forks). - Each philosopher should only eat if they can pick up both the left and right forks. **Performance Requirements:** - The solution should efficiently handle the async operations, ensuring that no two philosophers eat simultaneously if they share a fork. **Example Usage:** ```python import asyncio class DiningPhilosophers: def __init__(self, num_philosophers: int): self.num_philosophers = num_philosophers self.locks = [asyncio.Lock() for _ in range(num_philosophers)] async def wants_to_eat(self, philosopher: int) -> None: left_fork = self.locks[philosopher] right_fork = self.locks[(philosopher + 1) % self.num_philosophers] async with left_fork: async with right_fork: print(f\\"Philosopher {philosopher} starts eating\\") await asyncio.sleep(1) # Simulate eating print(f\\"Philosopher {philosopher} finishes eating\\") async def main(): philosophers = DiningPhilosophers(5) tasks = [philosophers.wants_to_eat(i) for i in range(5)] await asyncio.gather(*tasks) # Run all tasks asyncio.run(main()) ``` Ensure that you provide a working code implementation that demonstrates the philosophers eating and releasing forks without any deadlocks or synchronization issues.","solution":"import asyncio class DiningPhilosophers: def __init__(self, num_philosophers: int): self.num_philosophers = num_philosophers self.locks = [asyncio.Lock() for _ in range(num_philosophers)] async def wants_to_eat(self, philosopher: int) -> None: left_fork = self.locks[philosopher] right_fork = self.locks[(philosopher + 1) % self.num_philosophers] # Order the forks to prevent deadlock: always pick the fork with the smaller index first first_fork, second_fork = (left_fork, right_fork) if philosopher % 2 == 0 else (right_fork, left_fork) async with first_fork: async with second_fork: print(f\\"Philosopher {philosopher} starts eating\\") await asyncio.sleep(1) # Simulate eating print(f\\"Philosopher {philosopher} finishes eating\\")"},{"question":"Objective You are required to develop a simple data management application that reads from a file, processes the data, and then saves the data back to the file upon program termination. Your implementation should utilize the `atexit` module to ensure the data is saved even if the program is terminated using `sys.exit()`. Requirements 1. Implement a class `DataManager` that performs the following operations: - On initialization, reads from a file named `datafile` and loads an integer value; if the file does not exist, initializes the value to `0`. - Provides a method `increment(value: int)` that increments the internal data store by the specified amount. - Provides a method `get_value()` that returns the current value from the internal data store. - Utilizes the `atexit` module to ensure that the current value is written back to `datafile` upon program termination. 2. The value must be written in plain text. 3. The class should handle scenarios where the program is run multiple times, ensuring the data persists across program executions. Input and Output formats **Initialization**: - Reads from `datafile` and initializes the value. **Class Methods**: - `increment(self, value: int) -> None`: Increments the stored value. - `get_value(self) -> int`: Returns the current stored value. **File Operations**: - The `datafile` should be read during initialization and written to during program termination using the `atexit` module. Constraints - The internal value should be an integer. - If an error occurs during reading the file (e.g., file not found), initialize the value to `0`. Example ```python # Assuming datafile initially doesn\'t exist or contains the value \\"5\\" # First run dm = DataManager() print(dm.get_value()) # Output: 0 dm.increment(10) print(dm.get_value()) # Output: 10 # Program terminates normally, datafile is updated to \\"10\\" # Second run dm = DataManager() print(dm.get_value()) # Output: 10 dm.increment(5) print(dm.get_value()) # Output: 15 # Program terminates normally, datafile is updated to \\"15\\" ``` Notes - Ensure you handle exceptions properly and utilize the `atexit` module for registering the save operation. - Use file operations appropriately to handle reading and writing the integer value to `datafile`. Submission Submit your implementation of the `DataManager` class and the code demonstrating its use case as described in the example.","solution":"import atexit class DataManager: def __init__(self, filename=\'datafile\'): self.filename = filename self.value = 0 self.load_value() atexit.register(self.save_value) def load_value(self): try: with open(self.filename, \'r\') as f: self.value = int(f.read().strip()) except (FileNotFoundError, ValueError): self.value = 0 def increment(self, value: int) -> None: self.value += value def get_value(self) -> int: return self.value def save_value(self): with open(self.filename, \'w\') as f: f.write(str(self.value))"},{"question":"# Persistent Storage with Shelve You are given the task of creating a persistent storage system using the `shelve` module in Python. Your system will maintain a registry of student details, with the ability to add new students, retrieve information, and persist data across program runs. Requirements: 1. **Add Student**: Add a student to the registry with details like `name`, `age`, and `courses`. 2. **Retrieve Student**: Retrieve the details of a student using their name. 3. **List Students**: List all student names currently in the registry. 4. **Update Student Courses**: Update the courses a student is enrolled in. 5. **Remove Student**: Remove a student from the registry. Implement the following functions: Function 1: `add_student` ```python def add_student(filename: str, name: str, age: int, courses: list): Add a student to the shelve-based registry. Args: filename (str): The name of the shelve file. name (str): The name of the student. age (int): The age of the student. courses (list): A list of courses the student is enrolled in. ``` Function 2: `get_student` ```python def get_student(filename: str, name: str) -> dict: Retrieve a student\'s details from the shelve-based registry. Args: filename (str): The name of the shelve file. name (str): The name of the student to retrieve. Returns: dict: A dictionary with the student\'s details or an empty dictionary if the student was not found. ``` Function 3: `list_students` ```python def list_students(filename: str) -> list: List all student names in the shelve-based registry. Args: filename (str): The name of the shelve file. Returns: list: A list of all student names. ``` Function 4: `update_courses` ```python def update_courses(filename: str, name: str, new_courses: list): Update the courses for a student in the shelve-based registry. Args: filename (str): The name of the shelve file. name (str): The name of the student to update. new_courses (list): The new list of courses the student is enrolled in. ``` Function 5: `remove_student` ```python def remove_student(filename: str, name: str): Remove a student from the shelve-based registry. Args: filename (str): The name of the shelve file. name (str): The name of the student to remove. ``` Constraints: - You may assume that all names are unique. - You should ensure that resources are properly managed (e.g., closing the shelve file) in your implementations. Example Usage: ```python filename = \'students.shelve\' # Add students add_student(filename, \'Alice\', 23, [\'Mathematics\', \'Physics\']) add_student(filename, \'Bob\', 21, [\'Chemistry\', \'Biology\']) # List students print(list_students(filename)) # Output: [\'Alice\', \'Bob\'] # Retrieve a student print(get_student(filename, \'Alice\')) # Output: {\'name\': \'Alice\', \'age\': 23, \'courses\': [\'Mathematics\', \'Physics\']} # Update courses update_courses(filename, \'Alice\', [\'Mathematics\', \'Computer Science\']) # Retrieve updated student print(get_student(filename, \'Alice\')) # Output: {\'name\': \'Alice\', \'age\': 23, \'courses\': [\'Mathematics\', \'Computer Science\']} # Remove a student remove_student(filename, \'Bob\') print(list_students(filename)) # Output: [\'Alice\'] ``` Note: You should follow the best practices for resource management, making sure the shelf is closed after operations are performed. Use the context manager approach for handling the shelf where appropriate.","solution":"import shelve def add_student(filename: str, name: str, age: int, courses: list): Add a student to the shelve-based registry. with shelve.open(filename, writeback=True) as db: db[name] = {\'name\': name, \'age\': age, \'courses\': courses} def get_student(filename: str, name: str) -> dict: Retrieve a student\'s details from the shelve-based registry. with shelve.open(filename) as db: return db.get(name, {}) def list_students(filename: str) -> list: List all student names in the shelve-based registry. with shelve.open(filename) as db: return list(db.keys()) def update_courses(filename: str, name: str, new_courses: list): Update the courses for a student in the shelve-based registry. with shelve.open(filename, writeback=True) as db: if name in db: db[name][\'courses\'] = new_courses def remove_student(filename: str, name: str): Remove a student from the shelve-based registry. with shelve.open(filename, writeback=True) as db: if name in db: del db[name]"},{"question":"Coding Assessment Question # Objective The objective of this assessment is to evaluate your knowledge of handling errors in a distributed multiprocessing environment using PyTorch. You will need to implement error handling logic to ensure robust distributed computations. # Problem Statement You are tasked with implementing a custom error handler for a distributed PyTorch application. The custom error handler should log any errors that occur in child processes and re-raise them. Additionally, it should keep track of the number of times a specific type of error (e.g., `ChildFailedError`) occurs. # Function Signature ```python class CustomErrorHandler(ErrorHandler): def __init__(self): Initializes the CustomErrorHandler instance. super().__init__() self.error_count = {} def handle_exception(self, error: Exception) -> None: Handles the given exception. Logs the error, counts specific error types, and raises the error. Args: - error (Exception): The error that needs to be handled. Raises: - Exception: Re-raises the passed exception. pass def distributed_computation(): Simulates a distributed computation where multiple processes are involved. Utilizes the CustomErrorHandler to handle any errors that occur. pass ``` # Instructions 1. **CustomErrorHandler Class**: Implement the `CustomErrorHandler` class that inherits from `ErrorHandler`. - The `__init__` method should initialize a dictionary `error_count` to keep track of error occurrences. - The `handle_exception` method should: - Log the error message detail. - Increment the count for `ChildFailedError` if it occurs. - Re-raise the received exception. 2. **distributed_computation Function**: Implement a dummy function `distributed_computation` that simulates errors in a distributed environment. - Use the `torch.distributed.elastic.multiprocessing.errors.record` decorator to record errors. - Ensure that any errors raised in child processes are managed by `CustomErrorHandler`. # Constraints - Your solution must make use of the classes and functions from `torch.distributed.elastic.multiprocessing.errors`. - Assume that the necessary imports from PyTorch are available in your environment. # Example ```python # Test the CustomErrorHandler with a simple distributed computation # Simulate a scenario where a ChildFailedError is raised and handled. if __name__ == \\"__main__\\": try: distributed_computation() except Exception as e: print(f\\"Exception caught in main: {e}\\") custom_error_handler = CustomErrorHandler() assert custom_error_handler.error_count.get(\'ChildFailedError\', 0) > 0, \\"ChildFailedError should be counted.\\" ``` # Performance Requirements - Ensure that the error handling mechanism is efficient and does not significantly degrade the performance of the distributed computation. Build a robust error handling strategy using PyTorch\'s provided tools to ensure reliability in distributed environments.","solution":"import logging import torch.distributed as dist from torch.distributed.elastic.multiprocessing.errors import record, ErrorHandler class CustomErrorHandler(ErrorHandler): def __init__(self): Initializes the CustomErrorHandler instance. super().__init__() self.error_count = {} def handle_exception(self, error: Exception) -> None: Handles the given exception. Logs the error, counts specific error types, and raises the error. Args: - error (Exception): The error that needs to be handled. Raises: - Exception: Re-raises the passed exception. error_type = type(error).__name__ logging.error(f\\"Error occurred: {error_type} - {error}\\") if error_type not in self.error_count: self.error_count[error_type] = 0 self.error_count[error_type] += 1 raise error @record def distributed_computation(): Simulates a distributed computation where multiple processes are involved. Utilizes the CustomErrorHandler to handle any errors that occur. # Simulate a ChildFailedError (mock class and behavior defined for illustration) class ChildFailedError(Exception): pass raise ChildFailedError(\\"An error occurred in a child process\\") if __name__ == \\"__main__\\": try: distributed_computation() except Exception as e: print(f\\"Exception caught in main: {e}\\")"},{"question":"Working with pandas Data Types Objective: Create a function that takes a DataFrame and performs various operations involving different pandas data types, and returns a summary DataFrame. Description: 1. You are given a DataFrame containing the following columns: - `timestamps`: A column containing datetime data with time zones. - `periods`: A column containing period data. - `intervals`: A column containing interval data. - `nullable_integers`: A column containing nullable integer data. - `nullable_floats`: A column containing nullable float data. 2. Your task is to write a function `process_data(df: pd.DataFrame) -> pd.DataFrame` that processes the DataFrame and returns a summary DataFrame with the following information: - `timestamp_start`: The earliest timestamp in the `timestamps` column. - `timestamp_end`: The latest timestamp in the `timestamps` column. - `period_start`: The earliest period in the `periods` column. - `period_end`: The latest period in the `periods` column. - `interval_length_mean`: The mean of the interval lengths in the `intervals` column. - `nullable_integer_mean`: The mean of the non-missing values in the `nullable_integers` column. - `nullable_float_mean`: The mean of the non-missing values in the `nullable_floats` column. Constraints: - Ensure that the function can handle missing values appropriately. - Utilize the respective properties and methods of these data types for the calculations. - Ensure that the returned DataFrame has only one row summarizing the information in the required format. Example: ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'timestamps\': pd.to_datetime([\'2023-01-01\', \'2023-03-01\', \'2023-02-01\']).tz_localize(\'UTC\'), \'periods\': pd.period_range(\'2023-01\', periods=3, freq=\'M\'), \'intervals\': pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3)]), \'nullable_integers\': pd.Series([1, np.nan, 3], dtype=\'Int64\'), \'nullable_floats\': pd.Series([1.1, np.nan, 3.3], dtype=\'Float64\') } df = pd.DataFrame(data) # Function call result = process_data(df) # The result should be a DataFrame: # timestamp_start timestamp_end period_start period_end interval_length_mean nullable_integer_mean nullable_float_mean # 0 2023-01-01 00:00:00+00:00 2023-03-01 00:00:00+00:00 2023-01 2023-03 1.0 2.0 2.2 ``` Notes: - You may assume that the input DataFrame will always have the correct column names and data types as mentioned. - The output summary DataFrame should have only one row with the summarized information.","solution":"import pandas as pd import numpy as np def process_data(df: pd.DataFrame) -> pd.DataFrame: Processes the input DataFrame to return a summary DataFrame with computed metrics. Args: - df: A pandas DataFrame with specified columns and data types. Returns: - A DataFrame with a single row summarizing the data. summary = { \'timestamp_start\': df[\'timestamps\'].min(), \'timestamp_end\': df[\'timestamps\'].max(), \'period_start\': df[\'periods\'].min(), \'period_end\': df[\'periods\'].max(), \'interval_length_mean\': df[\'intervals\'].map(lambda x: x.length).mean(), \'nullable_integer_mean\': df[\'nullable_integers\'].mean(), \'nullable_float_mean\': df[\'nullable_floats\'].mean() } return pd.DataFrame([summary])"},{"question":"**Problem Statement:** You are given a DataFrame containing various types of data (including some long strings and floating-point numbers). Your task is to write a function that dynamically modifies the display settings for this DataFrame to meet specific formatting requirements. **Function Signature:** ```python def customize_dataframe_display(df: pd.DataFrame, max_rows: int = 10, max_columns: int = 5, precision: int = 2, colwidth: int = 15) -> None: Customize the display settings for a given DataFrame. Parameters: df (pd.DataFrame): The DataFrame for which to customize the display options. max_rows (int): Maximum number of rows to display. Default is 10. max_columns (int): Maximum number of columns to display. Default is 5. precision (int): Precision for floating-point numbers display. Default is 2. colwidth (int): Maximum width for each column. Default is 15. Returns: None pass ``` **Requirements:** 1. Set the maximum number of rows displayed to `max_rows`. 2. Set the maximum number of columns displayed to `max_columns`. 3. Set the display precision for floating-point numbers to `precision`. 4. Set the maximum column width to `colwidth`. 5. Your settings should only persist within the context of the function, i.e., any calls outside the function should use the default pandas settings. **Constraints:** - The DataFrame can have up to 500 rows and 100 columns. - Floating-point numbers can have up to 10 decimal places. - String lengths can be as long as 100 characters. You are expected to use appropriate pandas functions and context managers to achieve this. **Example:** ```python import pandas as pd import numpy as np data = { \\"A\\": np.random.rand(20), \\"B\\": np.random.choice([\'apple\', \'banana\', \'cherry\', \'date\']*5, 20), \\"C\\": np.random.randn(20), \\"D\\": [\'This is a long string to check column width setting.\']*20 } df = pd.DataFrame(data) customize_dataframe_display(df, max_rows=7, max_columns=3, precision=3, colwidth=20) # Expected behavior: # Display options for the DataFrame should show a max of 7 rows, max of 3 columns, precision of 3 decimal places for # floats, and a max column width of 20 characters. The options should revert to default outside the context of the function. ``` **Note:** Make sure to implement the function using pandas\' options API as per the documentation provided.","solution":"import pandas as pd def customize_dataframe_display(df: pd.DataFrame, max_rows: int = 10, max_columns: int = 5, precision: int = 2, colwidth: int = 15) -> None: Customize the display settings for a given DataFrame. Parameters: df (pd.DataFrame): The DataFrame for which to customize the display options. max_rows (int): Maximum number of rows to display. Default is 10. max_columns (int): Maximum number of columns to display. Default is 5. precision (int): Precision for floating-point numbers display. Default is 2. colwidth (int): Maximum width for each column. Default is 15. Returns: None with pd.option_context( \'display.max_rows\', max_rows, \'display.max_columns\', max_columns, \'display.precision\', precision, \'display.max_colwidth\', colwidth ): print(df)"},{"question":"# PyTorch Deterministic Tensor Initialization Objective Write a function in PyTorch that creates a tensor with specific requirements and verifies that the tensor values are filled according to the deterministic settings of PyTorch. Instructions 1. Implement the function `create_deterministic_tensor(size, fill_value)` which: - Accepts a tuple `size` that represents the dimensions of the tensor. - Accepts a `fill_value` which is the value that should be used to fill the tensor if `fill_uninitialized_memory` is set to `True`. - Ensures that `torch.use_deterministic_algorithms(True)` is set. - Ensures that `torch.utils.deterministic.fill_uninitialized_memory` is set to `True`. 2. Inside the function: - Create an uninitialized tensor using `torch.empty` with the given `size`. - Fill the tensor with the specified `fill_value`. 3. Your function should return the filled tensor and also set the `fill_uninitialized_memory` back to its default value (`True`) after the tensor creation. Example ```python import torch def create_deterministic_tensor(size, fill_value): # Implement the function following the instructions pass # Example usage tensor = create_deterministic_tensor((3, 3), -1) print(tensor) ``` Constraints - You may assume the size dimensions are all positive integers. - Ensure your solution is efficient, even though the focus is mainly on correctly utilizing deterministic settings. Expected Output The tensor should have dimensions `(3, 3)` and all values should be `-1` if `fill_uninitialized_memory` is `True`. Note: The behavior of filling the tensor should be observable with verification that deterministic settings and filling are appropriately handled according to the described attributes.","solution":"import torch def create_deterministic_tensor(size, fill_value): Creates an uninitialized tensor and fills it deterministically with the given fill_value. Returns the filled tensor. # Ensure deterministic algorithms are set torch.use_deterministic_algorithms(True) # Creating an empty tensor with the given size tensor = torch.empty(size) # Manually fill the tensor with the specified fill_value tensor.fill_(fill_value) return tensor"},{"question":"# CPython Version Decoder CPython exposes its version number through various macros that together construct a single 32-bit integer. This integer is referred to as `PY_VERSION_HEX`, which is formed from the major version, minor version, micro version, release level, and release serial. You are to implement a function `decode_python_version(hexversion: int) -> str` that takes an integer `hexversion` as input and returns a string representing the version in the format \\"major.minor.micro[release_level][release_serial]\\". Detailed Breakdown: - **hexversion**: An integer representing the CPython version in 32-bit hexadecimal format. - **major version**: Extract the first byte (8 bits). - **minor version**: Extract the second byte (8 bits). - **micro version**: Extract the third byte (8 bits). - **release level**: Extract the higher 4 bits of the fourth byte. - **release serial**: Extract the lower 4 bits of the fourth byte. **Release Level:** - `0xA`: \'a\' (Alpha) - `0xB`: \'b\' (Beta) - `0xC`: \'rc\' (Release Candidate) - `0xF`: \'\' (Final release, no suffix) **Output format**: - For a final release: \\"major.minor.micro\\" - For alpha, beta, and release candidates: \\"major.minor.micro[release_level][release_serial]\\" Example: 1. If the hexadecimal version is `0x030401a2`, the function should return \\"3.4.1a2\\". 2. If the hexadecimal version is `0x030a00f0`, the function should return \\"3.10.0\\". Constraints: - The input `hexversion` will be a valid 32-bit integer representing a Python version. - The highest version number components must be within their valid ranges as defined by typical versioning practices. You can assume valid input ranges and values for this question, and do not need to validate the ranges extensively. Function Signature: ```python def decode_python_version(hexversion: int) -> str: # Your implementation here pass ``` Implement this function and ensure it correctly decodes the `hexversion` into the required string format.","solution":"def decode_python_version(hexversion: int) -> str: major = (hexversion >> 24) & 0xFF minor = (hexversion >> 16) & 0xFF micro = (hexversion >> 8) & 0xFF release_level = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF release_level_map = { 0xA: \'a\', 0xB: \'b\', 0xC: \'rc\', 0xF: \'\' } release_level_str = release_level_map[release_level] version = f\\"{major}.{minor}.{micro}{release_level_str}{release_serial if release_level_str else \'\'}\\" return version"},{"question":"Objective Your task is to implement a `MultiThreadedTemplateProcessor` that reads a file containing template strings, processes each template using a dictionary of values provided, and writes the results to an output file. This should be done using multiple threads to speed up the processing. Requirements 1. The templates in the input file are based on the `string.Template` class. 2. Each line in the input file contains one template string. 3. The output file should contain the result of template processing for each line in the input file, maintaining the same order. 4. Use threading to process multiple lines simultaneously for improved performance. 5. Handle any missing placeholders in the template safely using `safe_substitute`. Function Signature ```python def process_templates(input_file: str, output_file: str, values: dict): ``` Input Parameters - `input_file` (str): The path to the input file containing template strings. - `output_file` (str): The path to the output file where processed template results should be saved. - `values` (dict): A dictionary containing values for template placeholders. Output - The function should not return anything. Instead, it should write the processed templates to the `output_file`. Constraints - Assume that the input file and output file paths are valid and accessible. - The input file contains at least one template string. - The values dictionary will have at least one key-value pair. Example Suppose you have the following `values` dictionary: ```python values = {\'name\': \'Alice\', \'item\': \'book\', \'quantity\': 3} ``` And the `input_file` contains the following lines: ``` Hello, {name}! You have ordered {quantity} {item}(s). {name}, you have a new message. ``` The `output_file` should contain: ``` Hello, Alice! You have ordered 3 book(s). Alice, you have a new message. ``` Implementation Tips 1. Use the `string.Template` class for template substitution. 2. Use the `threading` module to process multiple lines simultaneously. 3. Ensure thread-safe operations to maintain the order of the input lines in the output file. 4. Handle any missing placeholders using the `safe_substitute` method. Boilerplate Code ```python import threading from string import Template class TemplateProcessorThread(threading.Thread): def __init__(self, template_str, values, index, result): threading.Thread.__init__(self) self.template = Template(template_str) self.values = values self.index = index self.result = result def run(self): self.result[self.index] = self.template.safe_substitute(self.values) def process_templates(input_file, output_file, values): # Read input file with open(input_file, \'r\') as infile: templates = infile.readlines() # Prepare result container result = [None] * len(templates) # Create and start threads threads = [] for i, template_str in enumerate(templates): thread = TemplateProcessorThread(template_str.strip(), values, i, result) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() # Write results to output file with open(output_file, \'w\') as outfile: for line in result: outfile.write(line + \'n\') ``` Validate the function by providing a sample input file and a dictionary of values, and checking the output file for correctness.","solution":"import threading from string import Template class TemplateProcessorThread(threading.Thread): def __init__(self, template_str, values, index, result): threading.Thread.__init__(self) self.template = Template(template_str) self.values = values self.index = index self.result = result def run(self): self.result[self.index] = self.template.safe_substitute(self.values) def process_templates(input_file, output_file, values): # Read input file with open(input_file, \'r\') as infile: templates = infile.readlines() # Prepare result container result = [None] * len(templates) # Create and start threads threads = [] for i, template_str in enumerate(templates): thread = TemplateProcessorThread(template_str.strip(), values, i, result) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() # Write results to output file with open(output_file, \'w\') as outfile: for line in result: outfile.write(line + \'n\')"},{"question":"# Advanced Python Function Binding and Method Handling Objective You are tasked with developing a Python class that demonstrates the creation and manipulation of instance method objects and method objects. Task Write a Python class named `CustomMethodHandler` that allows the dynamic addition and invocation of methods bound to instances. Class Requirements 1. The class `__init__` method should initialize an empty dictionary to store methods. 2. Implement a method `add_method` that: - Takes a function and an instance as parameters. - Creates a method object using `PyMethod_New`. - Stores the method in the dictionary with the function\'s name as the key. 3. Implement a method `call_method` that: - Takes the method name and the arguments to pass. - Invokes the stored method with the passed arguments and returns the result. Example Usage ```python class CustomMethodHandler: def __init__(self): pass # Initialize your method dictionary here def add_method(self, func, instance): pass # Create and store the method object here def call_method(self, method_name, *args): pass # Retrieve and call the stored method here # Example function to be bound def example_function(self, x, y): return x + y # Example class instance class ExampleClass: pass example_instance = ExampleClass() # Usage handler = CustomMethodHandler() handler.add_method(example_function, example_instance) result = handler.call_method(\'example_function\', 3, 4) print(result) # Should output 7 ``` Constraints - You must use the internal functions and macros (`PyMethod_New`, `PyMethod_Function`, `PyMethod_Self`, etc.) as described in the documentation to create and manage method objects. - Handle any potential errors gracefully. Input/Output Format - **Input**: Functions and instances to be bound dynamically. - **Output**: The result from invoking the dynamically added method. Evaluation Criteria - Correct usage of method and instance method functionalities. - Proper handling of method storage and invocation. - Code clarity and error handling.","solution":"class CustomMethodHandler: def __init__(self): self.methods = {} def add_method(self, func, instance): import types # Creating a method object by binding the function to the instance bound_method = types.MethodType(func, instance) # Storing the method in the dictionary with the function\'s name self.methods[func.__name__] = bound_method def call_method(self, method_name, *args): method = self.methods.get(method_name) if method is None: raise ValueError(\\"Method not found\\") return method(*args) def example_function(self, x, y): return x + y class ExampleClass: pass example_instance = ExampleClass() # Example usage handler = CustomMethodHandler() handler.add_method(example_function, example_instance) result = handler.call_method(\'example_function\', 3, 4) print(result) # Output should be 7"},{"question":"# Problem: Accurate Summation Using Decimal Representation You are required to write a function that takes a list of decimal numbers represented as strings and computes their exact sum using the `decimal` module. The function should return the exact sum also as a string. Detailed Instructions: 1. **Function Signature**: `def accurate_sum(decimal_numbers: List[str]) -> str` 2. **Input**: - `decimal_numbers`: A list of strings, where each string represents a decimal number. The list can contain up to `10^4` elements. 3. **Output**: - Return a string representing the exact sum of all decimal numbers in the input list. 4. **Constraints**: - Each string in `decimal_numbers` represents a valid decimal number that can be positive or negative. - The precision of each decimal number can be up to 10 decimal places. 5. **Performance Requirements**: The solution should handle up to `10^4` decimal numbers efficiently. Example: ```python >>> decimal_numbers = [\\"0.1\\", \\"0.2\\", \\"0.3\\"] >>> accurate_sum(decimal_numbers) \'0.6\' >>> decimal_numbers = [\\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\", \\"0.1\\"] >>> accurate_sum(decimal_numbers) \'1.0\' ``` Notes: - Ensure you use the `decimal` module for precise arithmetic operations. - Do not convert the decimal numbers to binary floating-point at any point in your calculations to avoid precision loss. # Hint You can use `decimal.Decimal` for converting the string representation of numbers into decimal objects and performing arithmetic operations to maintain precision.","solution":"from decimal import Decimal from typing import List def accurate_sum(decimal_numbers: List[str]) -> str: Returns the exact sum of a list of decimal numbers represented as strings. Parameters: - decimal_numbers: A list of strings, where each string represents a decimal number. Returns: - A string representing the exact sum of all decimal numbers in the input list. total = Decimal(\'0\') for num in decimal_numbers: total += Decimal(num) return str(total)"},{"question":"**Question: Robust Debugging Logger** You have been tasked with developing a custom debugging logger for a Python application that aims to handle errors in a more informative and readable format. This logger should capture exceptions, extract relevant stack trace information, format it, and save it into a log file for further analysis. # Requirements: 1. Implement a function `debugging_logger` that intercepts exceptions, retrieves the exception information, and writes a formatted stack trace to a log file. 2. The function should: - **Capture the exception** currently being handled. - **Extract** and **format** the stack trace in a readable manner. - **Write** the formatted stack trace to a specified log file. 3. The function should be versatile and include additional context such as the time of the error occurrence and a brief custom error message. # Function Signature: ```python def debugging_logger(log_file: str, custom_message: str) -> None: pass ``` # Parameters: - `log_file` (str): The file path where the log should be saved. - `custom_message` (str): A brief message providing additional context for the error. # Example Usage: ```python try: # Code that might raise an exception 1 / 0 except Exception: debugging_logger(\'error_log.txt\', \'Division by Zero Error in calculation routine.\') ``` # Log File Output Example: If the division by zero error occurs, the content of `error_log.txt` should be similar to: ``` -- Log Time: 2023-10-04 15:34:08 -- Custom Message: Division by Zero Error in calculation routine. Traceback (most recent call last): File \\"main.py\\", line 10, in <module> 1 / 0 ZeroDivisionError: division by zero ``` # Constraints: - You must use the `traceback` module to capture and format the stack trace. - The function should log the time when the exception occurred in the format `YYYY-MM-DD HH:MM:SS`. # Notes: - Ensure that the log file is opened in append mode so that multiple errors can be logged sequentially. - Your solution should handle various types of exceptions and provide clear, formatted outputs in the log file. - Be mindful of performance; the logging operation should be efficient. **Make sure to test your function with different types of exceptions to confirm its robustness.**","solution":"import traceback import datetime def debugging_logger(log_file: str, custom_message: str) -> None: Logs exceptions to a given log file with a custom message and a formatted stack trace. Parameters: - log_file (str): The file path where the log should be saved. - custom_message (str): A brief message providing additional context for the error. with open(log_file, \'a\') as file: # Capture the current time current_time = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") file.write(f\\"-- Log Time: {current_time} --n\\") file.write(f\\"Custom Message: {custom_message}n\\") file.write(\\"Traceback (most recent call last):n\\") # Capture and format the traceback traceback_str = traceback.format_exc() file.write(traceback_str) file.write(\\"nn\\")"},{"question":"# Terminal Mode Control Using the `tty` Module You have been provided with a fundamental set of functions from the `tty` module, designed to set terminal modes for file descriptors. Your task is to create a class that utilizes these functions to manage terminal modes effectively. Class: `TerminalModeManager` **Methods:** 1. `set_raw_mode(fd: int) -> None` - Description: Sets the file descriptor `fd` to raw mode. 2. `set_cbreak_mode(fd: int) -> None` - Description: Sets the file descriptor `fd` to cbreak mode. 3. `reset_mode(fd: int) -> None` - Description: Resets the file descriptor `fd` to its original mode, which should be stored when the mode is changed to raw or cbreak. **Input:** - The input is primarily the file descriptor `fd` which is an integer representing the file descriptor of the terminal device. **Output:** - The methods return `None`. Requirements: 1. Use `tty.setraw(fd)` and `tty.setcbreak(fd)` for setting raw and cbreak modes respectively. 2. Store the original terminal mode before changing it and restore it when required using `termios.tcgetattr(fd)` and `termios.tcsetattr(fd, termios.TCSANOW, original_attributes)`. 3. Ensure that the terminal mode is reset back to its original state properly when `reset_mode` is called. ```python import tty import termios class TerminalModeManager: def __init__(self): self.original_modes = {} def set_raw_mode(self, fd: int) -> None: Sets the terminal associated with file descriptor \'fd\' to raw mode. original_mode = termios.tcgetattr(fd) self.original_modes[fd] = original_mode # Save the original mode tty.setraw(fd) def set_cbreak_mode(self, fd: int) -> None: Sets the terminal associated with file descriptor \'fd\' to cbreak mode. original_mode = termios.tcgetattr(fd) self.original_modes[fd] = original_mode # Save the original mode tty.setcbreak(fd) def reset_mode(self, fd: int) -> None: Resets the terminal associated with file descriptor \'fd\' to its original mode. if fd in self.original_modes: termios.tcsetattr(fd, termios.TCSANOW, self.original_modes[fd]) else: raise ValueError(f\\"No original mode saved for file descriptor {fd}\\") # Example usage: # fd = sys.stdin.fileno() # manager = TerminalModeManager() # manager.set_raw_mode(fd) # manager.reset_mode(fd) ``` In this exercise, you are expected to demonstrate your understanding of terminal control and file descriptor operations in a Unix environment using Python\'s `tty` and `termios` modules.","solution":"import tty import termios class TerminalModeManager: def __init__(self): self.original_modes = {} def set_raw_mode(self, fd: int) -> None: Sets the terminal associated with file descriptor \'fd\' to raw mode. original_mode = termios.tcgetattr(fd) self.original_modes[fd] = original_mode # Save the original mode tty.setraw(fd) def set_cbreak_mode(self, fd: int) -> None: Sets the terminal associated with file descriptor \'fd\' to cbreak mode. original_mode = termios.tcgetattr(fd) self.original_modes[fd] = original_mode # Save the original mode tty.setcbreak(fd) def reset_mode(self, fd: int) -> None: Resets the terminal associated with file descriptor \'fd\' to its original mode. if fd in self.original_modes: termios.tcsetattr(fd, termios.TCSANOW, self.original_modes[fd]) else: raise ValueError(f\\"No original mode saved for file descriptor {fd}\\") # Example usage: # fd = sys.stdin.fileno() # manager = TerminalModeManager() # manager.set_raw_mode(fd) # manager.reset_mode(fd)"},{"question":"# Python Constants and Special Methods Objective: Implement a custom class that utilizes Python\'s built-in constants and special methods to handle various operations. This task will test your understanding of integrating Python\'s constants within user-defined types and correctly implementing special methods. Problem Statement: Create a class `SpecialNumber` that represents numbers but with certain special conditions using Python\'s built-in constants and special methods. # Requirements: 1. **Initialization**: The class should accept a single argument (integer/float). 2. **Binary Operations**: - Implement the `__add__` method to add two `SpecialNumber` objects or a `SpecialNumber` object with an int/float. - Implement the `__sub__` method to subtract a `SpecialNumber` object by another `SpecialNumber` object or an int/float. - If the other operand is not of type `SpecialNumber`/int/float, return `NotImplemented`. 3. **Representation**: - Implement the `__str__` method to return the string representation of the number. - Implement the `__repr__` method to return a string in the form `SpecialNumber(value)`. 4. **Error Handling**: - Ensure that adding/subtracting results in raising appropriate exceptions if invalid types are encountered. 5. **Logical Checks**: - Implement the `__eq__` method to check equality between two `SpecialNumber` objects. - Use `NotImplemented` for comparing with other types. 6. **Type Constraints**: - Ensure that instances of this class always store values of type int or float. Input: - The initialization of `SpecialNumber` with an integer or float. - Operations between `SpecialNumber` instances or between `SpecialNumber` and int/float. Output: - Results of the addition or subtraction operations as new `SpecialNumber` instances. - String representations as defined by the implemented `__str__` and `__repr__` methods. - Boolean results for equality checks. - Appropriate exceptions for invalid operations. Example: ```python a = SpecialNumber(10) b = SpecialNumber(5.5) c = a + b print(c) # Output: 15.5 d = a - 3 print(d) # Output: 7 print(d == SpecialNumber(7)) # Output: True print(c == b) # Output: False e = c + \\"a\\" # Should return NotImplemented or raise appropriate error print(repr(a)) # Output: SpecialNumber(10) ``` Implement the `SpecialNumber` class to meet the above specifications and handle the interactions as described.","solution":"class SpecialNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be an int or a float\\") self.value = value def __add__(self, other): if isinstance(other, (SpecialNumber, int, float)): if isinstance(other, SpecialNumber): other = other.value return SpecialNumber(self.value + other) return NotImplemented def __sub__(self, other): if isinstance(other, (SpecialNumber, int, float)): if isinstance(other, SpecialNumber): other = other.value return SpecialNumber(self.value - other) return NotImplemented def __eq__(self, other): if isinstance(other, SpecialNumber): return self.value == other.value return NotImplemented def __str__(self): return str(self.value) def __repr__(self): return f\\"SpecialNumber({self.value})\\""},{"question":"**Objective**: Implement a function that simulates distributed computations using PyTorch `Future` objects and the utility functions `collect_all` and `wait_all`. # Problem Statement Imagine you are tasked with performing multiple asynchronous computations, and you need to gather and process their results efficiently. You need to implement a function `distributed_computation` that: 1. Takes a list of callables (`computations`) that return `Future` objects. 2. Waits for all the `Future` objects to complete. 3. Collects the results once all `Future` objects are done. 4. Returns the list of results. # Function Signature ```python import torch.futures def distributed_computation(computations: List[Callable[[], torch.futures.Future]]) -> List[Any]: pass ``` # Inputs * `computations` (List[Callable[[], torch.futures.Future]]): A list of callables, each returning a `torch.futures.Future` object when invoked. # Outputs * List[Any]: A list of results from the completed future computations. # Constraints * Each callable in the `computations` list must be invoked to get the `Future` object. * You must use `torch.futures.collect_all` to collect all `Future` objects. * You must use `torch.futures.wait_all` to wait for all `Future` objects to complete. # Example ```python import torch from torch.futures import Future from typing import Any, List, Callable def example_future_computation() -> Future: fut = Future() fut.set_result(42) # Simulating an asynchronous operation that returns 42 return fut # List of callables returning Future objects computations = [example_future_computation, example_future_computation] # Calling the function results = distributed_computation(computations) print(results) # Output: [42, 42] ``` # Notes 1. The provided example `example_future_computation` directly sets the result for simplicity. This can be replaced with actual asynchronous computations. 2. Ensure that you handle exceptions and errors appropriately in a real-world scenario.","solution":"import torch from torch.futures import Future from typing import Any, List, Callable def distributed_computation(computations: List[Callable[[], torch.futures.Future]]) -> List[Any]: # Collect Future objects by invoking each callable in the computations list futures = [comp() for comp in computations] # Use collect_all to gather all Future objects together all_futures = torch.futures.collect_all(futures) # Wait for all Future objects to complete all_futures.wait() # Collect and return the results once all are done results = [fut.value() for fut in all_futures.value()] return results"},{"question":"# Text Editor using Curses **Objective:** Implement a basic text editor using the `curses` module in Python. **Task:** Create a text editor in a terminal window that allows users to perform the following operations: 1. **Open a File:** - Read a text file and display its contents in the editor window. 2. **Edit the File:** - Insert new text. - Delete existing text. - Move the cursor within the text (using arrow keys). - Scroll the text if it exceeds the window size. - Save the changes back to the file. - Provide visual feedback by highlighting the current line and displaying line numbers. 3. **Command Interface:** - Accept and process editing commands from the user (such as saving the file, exiting the editor). # Specifications: 1. **Initialization:** - The editor should initialize the `curses` library and create a main window to display the text. - Set up color pairs for highlighting and normal text display. 2. **Window Management:** - Ensure the text scrolls correctly if it exceeds the visible window area. - Create a rectangle border around the text area using `curses.textpad.rectangle`. 3. **Text Editing:** - Implement a basic text insertion and deletion mechanism. - Ensure the cursor can navigate through the text using arrow keys. - Keep the cursor within the visible text area. 4. **File Operations:** - Read from and write to a text file. - Handle edge cases like attempting to save to a non-writable file. 5. **Visual Feedback:** - Highlight the current line where the cursor is located. - Display line numbers on the left side of the text area. 6. **Commands:** - Implement commands to save the file and exit the editor. # Input: - Path to the text file to be opened (can be provided via command-line arguments or user input within the editor). # Output: - The contents of the file displayed in the terminal with editing capabilities. # Constraints: - The implementation should handle files up to a few thousand lines smoothly. - Use appropriate error handling to manage operations like file I/O. # Example Usage: 1. Open the editor with a specified file: ```shell python text_editor.py sample.txt ``` 2. Save the changes with a specific key (such as `Ctrl-S`). 3. Exit the editor with a specific key (such as `Ctrl-X`). **Hints:** - Refer to the `curses` documentation for details on window creation, drawing characters, handling input, and managing colors. - Use `curses.textpad.Textbox` for handling text input within a window.","solution":"import curses import curses.textpad import sys def main(stdscr, filename): # Initialize curses curses.curs_set(1) curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) # Read file contents try: with open(filename, \'r\') as file: lines = file.readlines() except Exception as e: lines = [] curses.endwin() print(f\\"Failed to open file: {e}\\") return max_y, max_x = stdscr.getmaxyx() num_lines = len(lines) cursor_x = cursor_y = 0 # Main loop while True: stdscr.clear() # Display lines with line numbers for idx, line in enumerate(lines[:max_y-1]): stdscr.addstr(idx, 1, f\\"{idx+1:3}. {line}\\") # Highlight current line stdscr.chgat(cursor_y, 1, -1, curses.color_pair(1)) stdscr.move(cursor_y, cursor_x + 5) stdscr.refresh() key = stdscr.getch() if key == curses.KEY_UP: cursor_y = max(cursor_y - 1, 0) elif key == curses.KEY_DOWN: cursor_y = min(cursor_y + 1, num_lines - 1) elif key == curses.KEY_LEFT: cursor_x = max(cursor_x - 1, 0) elif key == curses.KEY_RIGHT: cursor_x = min(cursor_x + 1, len(lines[cursor_y]) - 1) elif key == ord(\'n\'): lines.insert(cursor_y + 1, \\"\\") cursor_y += 1 num_lines += 1 cursor_x = 0 elif key == 127: # Backspace key if cursor_x > 0: lines[cursor_y] = lines[cursor_y][:cursor_x-1] + lines[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(lines[cursor_y-1]) lines[cursor_y-1] += lines[cursor_y] del lines[cursor_y] num_lines -= 1 cursor_y -= 1 elif key == 19: # Ctrl-S to save with open(filename, \'w\') as file: file.writelines(lines) elif key == 24: # Ctrl-X to exit break else: lines[cursor_y] = lines[cursor_y][:cursor_x] + chr(key) + lines[cursor_y][cursor_x:] cursor_x += 1 if __name__ == \'__main__\': if len(sys.argv) != 2: print(\\"Usage: python text_editor.py <filename>\\") sys.exit(1) curses.wrapper(main, sys.argv[1])"},{"question":"# Custom Transformer and Pipeline Construction You have been provided with a dataset containing house prices and various features of the houses. Your task is to implement a custom transformer using scikit-learn that preprocesses this dataset. Specifically, the transformer should handle missing values, scale numerical features, and encode categorical features. You will then use this custom transformer within a scikit-learn pipeline to preprocess the data and train a linear regression model. Requirements: 1. **Custom Transformer Class**: - Implement a class named `HouseDataPreprocessor`. - This class should inherit from `BaseEstimator` and `TransformerMixin`. - The class should include methods `fit`, `transform`, and `fit_transform`, following scikit-learn\'s conventions. - The preprocessing steps should include: - Handling missing values: Replace missing numerical values with the mean of the column. - Scaling numerical features: Standardize the numerical features. - Encoding categorical features: One-hot encode the categorical features. 2. **Pipeline Construction**: - Use the custom transformer `HouseDataPreprocessor` within a scikit-learn pipeline. - The pipeline should include the preprocessing step and a linear regression estimator for training a model on the preprocessed data. Input and Output Formats: - **Input**: - `X_train`: A pandas DataFrame containing the features of the training data. - `y_train`: A pandas Series containing the target variable (house prices) for the training data. - `X_test`: A pandas DataFrame containing the features of the test data. - **Output**: - Return the predictions of the linear regression model on `X_test` after fitting the pipeline on `X_train` and `y_train`. Constraints and Limitations: - You should handle both numerical and categorical columns efficiently. - Assume that the dataset might contain missing values in some numerical columns. Performance Requirements: - The implementation should be efficient enough to handle datasets with up to 100,000 rows and 50 columns. Example: ```python import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer class HouseDataPreprocessor(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): # Define the preprocessing for numeric and categorical features pass # Replace with fit logic def transform(self, X, y=None): pass # Replace with transform logic def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X, y) def train_model(X_train, y_train, X_test): # Define and use the pipeline preprocessor = HouseDataPreprocessor() pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Fit the pipeline and return predictions pipeline.fit(X_train, y_train) predictions = pipeline.predict(X_test) return predictions # Example usage: # X_train = pd.DataFrame({...}) # y_train = pd.Series([...]) # X_test = pd.DataFrame({...}) # predictions = train_model(X_train, y_train, X_test) ```","solution":"import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer class HouseDataPreprocessor(BaseEstimator, TransformerMixin): def __init__(self): self.preprocessor = None def fit(self, X, y=None): numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\', \'category\']).columns numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) self.preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) self.preprocessor.fit(X) return self def transform(self, X): return self.preprocessor.transform(X) def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X) def train_model(X_train, y_train, X_test): preprocessor = HouseDataPreprocessor() pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) pipeline.fit(X_train, y_train) predictions = pipeline.predict(X_test) return predictions"},{"question":"Objective: Write a Python function that visualizes the relationship between island, species, and sex of penguins using the seaborn library. The function should generate a faceted grid of scatter plots where each subplot represents data for different species on different islands, separated by the sex of the penguins. Requirements: 1. **Function Signature**: `def visualize_penguins_facet_grid(df: pd.DataFrame) -> None:` 2. **Input**: A pandas DataFrame `df` containing the penguins dataset with at least the following columns: \'island\', \'species\', \'sex\', \'bill_length_mm\', and \'bill_depth_mm\'. 3. **Output**: The function should display a faceted grid of scatter plots. Each subplot should show the relationship between `bill_length_mm` and `bill_depth_mm` for a specific combination of \'island\', \'species\', and \'sex\'. 4. **Details**: - Facet the data by \'species\' and \'island\' in a grid format. - Separate the plots by \'sex\' within each facet. - Use scatter plots to show the relationship between `bill_length_mm` and `bill_depth_mm`. - Ensure that the axes are not shared (each subplot should have its scale for both x and y axes). - Label each facet appropriately with the species, island, and sex information. - Include a main title for the entire grid: \\"Penguin Bill Dimensions by Species and Island\\" Constraints: - Assume the DataFrame `df` will always contain the necessary columns and the data types are appropriate for plotting. - The function should not return any value; it should only display the plot. Example: ```python import pandas as pd from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Your function call visualize_penguins_facet_grid(penguins) ``` This should render a faceted scatter plot grid as described.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_penguins_facet_grid(df: pd.DataFrame) -> None: Visualizes the relationship between island, species, and sex of penguins using the seaborn library to create a faceted grid of scatter plots. Parameters: df (pd.DataFrame): DataFrame containing the penguins dataset with at least the columns: \'island\', \'species\', \'sex\', \'bill_length_mm\', and \'bill_depth_mm\'. Returns: None: Displays a faceted grid of scatter plots. # Creating a FacetGrid with seaborn g = sns.FacetGrid(df, row=\\"species\\", col=\\"island\\", hue=\\"sex\\", margin_titles=True) # Mapping the scatter plot onto the grid g.map(sns.scatterplot, \\"bill_length_mm\\", \\"bill_depth_mm\\").add_legend() # Adding a main title for the entire plot plt.subplots_adjust(top=0.9) g.fig.suptitle(\\"Penguin Bill Dimensions by Species and Island\\") # Show the plot plt.show()"},{"question":"# Custom JSON Encoder and Decoder Objective: Your task is to write a custom JSON encoder and decoder that can handle some specific Python object types which are not natively supported by the `json` module, such as `datetime.date`, `datetime.datetime`, and `decimal.Decimal`. This will require you to extend `JSONEncoder` and `JSONDecoder` classes from the `json` module. Problem Statement: 1. **Custom Encoder**: Implement a custom JSON encoder that can handle `datetime.date`, `datetime.datetime`, and `decimal.Decimal`. The encoder should convert these objects into serializable formats: - `datetime.date` and `datetime.datetime` should be converted to ISO format strings. - `decimal.Decimal` should be converted to its string representation. 2. **Custom Decoder**: Implement a custom JSON decoder that can convert ISO format strings back into `datetime.date` and `datetime.datetime` objects, and string representations back into `decimal.Decimal` objects. Requirements: - Implement a class `CustomJSONEncoder` that extends `json.JSONEncoder` and overrides the `default` method to handle the specified types. - Implement a class `CustomJSONDecoder` that extends `json.JSONDecoder` and properly uses `object_hook` to decode JSON objects back into the specified types. - Demonstrate your custom encoder and decoder with an example that involves encoding and decoding a Python dictionary containing these types. Input: A Python dictionary containing nested data types including `datetime.date`, `datetime.datetime`, and `decimal.Decimal`. Constraints: - Utilize the `json` module\'s methods (`dumps`, `loads`, etc.) for encoding and decoding. - Ensure that your implementation can handle the standard data types in addition to the custom ones specified. - Pay attention to performance and include necessary error-handling to manage potential issues with data conversion. Example: ```python import json import datetime import decimal # Define your classes here class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime.date): return obj.isoformat() if isinstance(obj, datetime.datetime): return obj.isoformat() if isinstance(obj, decimal.Decimal): return str(obj) return super().default(obj) def custom_object_hook(dct): if \'date\' in dct: return datetime.date.fromisoformat(dct[\'date\']) if \'datetime\' in dct: return datetime.datetime.fromisoformat(dct[\'datetime\']) if \'decimal\' in dct: return decimal.Decimal(dct[\'decimal\']) return dct class CustomJSONDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=custom_object_hook, *args, **kwargs) # Sample dictionary data = { \'date\': datetime.date.today(), \'datetime\': datetime.datetime.now(), \'decimal\': decimal.Decimal(\'123.45\') } # Encoding encoded_data = json.dumps(data, cls=CustomJSONEncoder) print(\\"Encoded JSON: \\", encoded_data) # Decoding decoded_data = json.loads(encoded_data, cls=CustomJSONDecoder) print(\\"Decoded Object: \\", decoded_data) ``` Note: - You should adapt the `custom_object_hook` function and the `default` method in the `CustomJSONEncoder` class to seamlessly handle different custom types within nested structures. - Ensure that the encoders and decoders can handle both primitive and complex nested structures. Submission: Provide the implementation of `CustomJSONEncoder`, `CustomJSONDecoder`, and demonstrate their usage with a detailed example as specified above.","solution":"import json import datetime import decimal class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime.date) and not isinstance(obj, datetime.datetime): return {\\"_type\\": \\"date\\", \\"value\\": obj.isoformat()} if isinstance(obj, datetime.datetime): return {\\"_type\\": \\"datetime\\", \\"value\\": obj.isoformat()} if isinstance(obj, decimal.Decimal): return {\\"_type\\": \\"decimal\\", \\"value\\": str(obj)} return super().default(obj) def custom_object_hook(dct): if \'_type\' in dct: if dct[\'_type\'] == \'date\': return datetime.date.fromisoformat(dct[\'value\']) if dct[\'_type\'] == \'datetime\': return datetime.datetime.fromisoformat(dct[\'value\']) if dct[\'_type\'] == \'decimal\': return decimal.Decimal(dct[\'value\']) return dct class CustomJSONDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=custom_object_hook, *args, **kwargs)"},{"question":"# Command-line Quiz Application using `argparse` Objective: To assess your understanding of the `argparse` module, you are required to implement a command-line based quiz application. This quiz application will take various command-line arguments to configure and run the quiz. Instructions: 1. **Create a Python script** called `quiz.py`. 2. **Add command-line arguments** to the script using `argparse.ArgumentParser`: - `--questions [questions_file]`: (required) A file path to a JSON file containing a list of questions and their options. - `--time-limit [time_in_seconds]`: (optional) An integer specifying the time limit for the quiz in seconds. Default: 60 seconds. - `--shuffle`: (optional) A flag to indicate if the questions should be presented in a shuffled order. - `--verbose`: (optional) A flag to run the quiz in verbose mode, displaying additional information. 3. **Parsing Arguments:** Use `parse_args()` to parse the command-line arguments. 4. **Functionality:** - Load questions from the provided JSON file. Each question will have multiple options and a correct answer. - If the `--shuffle` flag is provided, shuffle the questions before starting the quiz. - Implement the time limit using the `--time-limit` argument. - If `--verbose` is provided, display each question\'s details and the provided answer. - Collect answers from the user for each question. - At the end of the quiz, display the user\'s score as a percentage. Question JSON Format: ```json [ { \\"question\\": \\"What is the capital city of France?\\", \\"options\\": [\\"Paris\\", \\"London\\", \\"Berlin\\", \\"Madrid\\"], \\"answer\\": \\"Paris\\" }, { \\"question\\": \\"What is 2 + 2?\\", \\"options\\": [\\"3\\", \\"4\\", \\"5\\", \\"6\\"], \\"answer\\": \\"4\\" } ... ] ``` Expected Input and Usage: - `python quiz.py --questions questions.json --time-limit 30 --shuffle --verbose` - `python quiz.py --questions questions.json --time-limit 45` - `python quiz.py --questions questions.json --verbose` Constraints: - You may assume there will be at least 1 question in the quiz. - Ensure proper validation and error handling for arguments and file operations. - Use appropriate methods from the `argparse` module to provide a good user experience. Example Output: ```plaintext Loading questions from questions.json... Shuffling questions... Options: Paris, London, Berlin, Madrid Your answer: Paris ... Time\'s up! Your score: 80% ``` Additional Information: The application should be able to run from the command line and handle various combinations of the provided arguments seamlessly.","solution":"import argparse import json import random import sys import time def load_questions(file_path): try: with open(file_path, \'r\') as file: return json.load(file) except FileNotFoundError: print(f\\"Error: The file {file_path} does not exist.\\") sys.exit(1) except json.JSONDecodeError: print(f\\"Error: The file {file_path} is not valid JSON.\\") sys.exit(1) def run_quiz(questions, time_limit, shuffle, verbose): if shuffle: random.shuffle(questions) correct_answers = 0 start_time = time.time() for question in questions: elapsed_time = time.time() - start_time if elapsed_time >= time_limit: print(\\"nTime\'s up!\\") break print(f\\"nQuestion: {question[\'question\']}\\") for idx, option in enumerate(question[\'options\']): print(f\\"{idx + 1}. {option}\\") if verbose: print(f\\"(Verbose) Correct Answer: {question[\'answer\']}\\") answer = input(\\"Your answer: \\") correct_answer = question[\'answer\'] if answer.lower() == correct_answer.lower(): correct_answers += 1 score = (correct_answers / len(questions)) * 100 print(f\\"nYour score: {score}%\\") def main(): parser = argparse.ArgumentParser(description=\\"Command-line Quiz Application\\") parser.add_argument(\'--questions\', required=True, help=\\"Path to the JSON file containing quiz questions\\") parser.add_argument(\'--time-limit\', type=int, default=60, help=\\"Time limit for the quiz in seconds (default: 60)\\") parser.add_argument(\'--shuffle\', action=\'store_true\', help=\\"Shuffle the quiz questions\\") parser.add_argument(\'--verbose\', action=\'store_true\', help=\\"Run the quiz in verbose mode\\") args = parser.parse_args() questions = load_questions(args.questions) run_quiz(questions, args.time_limit, args.shuffle, args.verbose) if __name__ == \'__main__\': main()"},{"question":"You have been hired to create a secure cookie management system for an application. The system should be able to handle various types of cookies and their attributes securely. # Objectives: 1. Create a custom cookie class that extends `http.cookies.BaseCookie`. 2. Implement a method `secure_value_encode` that takes any value and converts it into a securely encoded string. 3. Implement a method `secure_value_decode` that takes a securely encoded string and returns the original value. 4. Ensure that the custom encoding and decoding mechanisms are secure by design. # Requirements: 1. The custom class should be named `SecureCookie`. 2. The `secure_value_encode` method should use Base64 encoding for the example implementation. The method signature should be `secure_value_encode(val: Any) -> Tuple[Any, str]` where the returned tuple contains the original value and the Base64 encoded string. 3. The `secure_value_decode` method should decode the Base64 string back to the original value. The method signature should be `secure_value_decode(encoded: str) -> Tuple[Any, str]`. 4. The class should override `value_encode` and `value_decode` to use `secure_value_encode` and `secure_value_decode` methods respectively. 5. Your solution should handle potential errors gracefully, raising `http.cookies.CookieError` for invalid operations. Input: - Cookie name as a string. - Cookie value as any serializable Python object. - Optionally, HTTP attributes (e.g., `path`, `domain`, `expires`, etc.). Output: - Encoded cookie header string. Constraints: - Ensure the secure encoding/decoding methods preserve the original type and value. - Assume cookie names and attribute names meet the specified character set. Example Usage: ```python from http import cookies import base64 import json class SecureCookie(cookies.BaseCookie): def secure_value_encode(self, val): try: real_value = val coded_value = base64.b64encode(json.dumps(val).encode(\'utf-8\')).decode(\'utf-8\') return real_value, coded_value except Exception as e: raise cookies.CookieError(\\"Error encoding value: {}\\".format(e)) def secure_value_decode(self, coded_value): try: decoded_json = base64.b64decode(coded_value.encode(\'utf-8\')).decode(\'utf-8\') real_value = json.loads(decoded_json) return real_value, coded_value except Exception as e: raise cookies.CookieError(\\"Error decoding value: {}\\".format(e)) def value_encode(self, val): return self.secure_value_encode(val) def value_decode(self, coded_value): return self.secure_value_decode(coded_value) # Example usage C = SecureCookie() C[\\"username\\"] = {\\"name\\": \\"John\\", \\"role\\": \\"admin\\"} C[\\"username\\"][\\"path\\"] = \\"/\\" print(C.output()) ``` # Notes: - The `SecureCookie` class should handle dictionary values for cookies and will use JSON to serialize/deserialize them. - The Base64 encoding is used for simplicity. For actual secure applications, consider stronger encryption methods.","solution":"from http import cookies import base64 import json class SecureCookie(cookies.BaseCookie): def secure_value_encode(self, val): Encodes the value into a secure Base64 encoded string. try: real_value = val # Serialize the value to JSON and then encode it to Base64 coded_value = base64.b64encode(json.dumps(val).encode(\'utf-8\')).decode(\'utf-8\') return real_value, coded_value except Exception as e: raise cookies.CookieError(\\"Error encoding value: {}\\".format(e)) def secure_value_decode(self, coded_value): Decodes the secure Base64 encoded string back to the original value. try: # Decode from Base64 and then deserialize the JSON string decoded_json = base64.b64decode(coded_value.encode(\'utf-8\')).decode(\'utf-8\') real_value = json.loads(decoded_json) return real_value, coded_value except Exception as e: raise cookies.CookieError(\\"Error decoding value: {}\\".format(e)) def value_encode(self, val): return self.secure_value_encode(val) def value_decode(self, coded_value): return self.secure_value_decode(coded_value) # Example usage C = SecureCookie() C[\\"username\\"] = {\\"name\\": \\"John\\", \\"role\\": \\"admin\\"} C[\\"username\\"][\\"path\\"] = \\"/\\" print(C.output())"},{"question":"# Question: You are tasked with implementing a function in PyTorch using the `torch.func` transformations, ensuring no side effects and adhering to the limitations described. **Task:** 1. Implement a function `batched_relu` using `vmap` that applies a rectified linear unit (ReLU) to a batch of input tensors. Your function should not involve any side effects and should be compatible with the `vmap` restrictions. 2. Implement a function `grad_batched_relu` to compute the gradient of `batched_relu` with respect to its input, using `torch.func.grad`. # Guidelines: - Do not use any global variables in your functions. - Ensure that all outputs from your functions are returned explicitly. - Follow the constraints provided for `vmap` and `torch.func.grad`, particularly avoiding unsupported in-place operations and data-dependent control flows. # Input: - A 2D tensor `x` of shape `(batch_size, features)`. # Output: - For `batched_relu`, return a 2D tensor of the same shape as the input, where the ReLU function has been applied element-wise to each tensor in the batch. - For `grad_batched_relu`, return a 2D tensor of the same shape as the input, representing the gradient of the loss (considered as the sum of output elements) with respect to the input. # Constraints: - The input tensor will always be a 2D tensor. - You should not manually loop through the batch; use the `vmap` function to handle batching. # Function Signatures: ```python import torch def batched_relu(x: torch.Tensor) -> torch.Tensor: Applies the rectified linear unit (ReLU) function to a batch of input tensors. :param x: A 2D tensor of shape (batch_size, features). :return: A 2D tensor of the same shape as the input, with ReLU applied element-wise. pass def grad_batched_relu(x: torch.Tensor) -> torch.Tensor: Computes the gradient of the batched ReLU function with respect to its input. :param x: A 2D tensor of shape (batch_size, features). :return: A 2D tensor representing the gradient of the loss with respect to the input. pass ``` # Example: ```python x = torch.tensor([[-1.0, 2.0], [3.0, -4.0], [5.0, -6.0]]) # Applying batched_relu result = batched_relu(x) print(result) # Expected Output: [[0.0, 2.0], [3.0, 0.0], [5.0, 0.0]] # Computing gradient grad_result = grad_batched_relu(x) print(grad_result) # Expected Output: [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0]] ``` **Note: Ensure your functions handle the restrictions and limitations explained effectively.**","solution":"import torch from torch.func import vmap, grad def batched_relu(x: torch.Tensor) -> torch.Tensor: Applies the rectified linear unit (ReLU) function to a batch of input tensors. :param x: A 2D tensor of shape (batch_size, features). :return: A 2D tensor of the same shape as the input, with ReLU applied element-wise. # Define the ReLU function for a single input tensor def relu_single(input_tensor): return torch.relu(input_tensor) # Apply vmap to vectorize over the batch dimension batched_relu_fn = vmap(relu_single) return batched_relu_fn(x) def grad_batched_relu(x: torch.Tensor) -> torch.Tensor: Computes the gradient of the batched ReLU function with respect to its input. :param x: A 2D tensor of shape (batch_size, features). :return: A 2D tensor representing the gradient of the loss with respect to the input. # Define the sum of batched_relu output as the scalar value for differentiation def sum_batched_relu(input_tensor): return batched_relu(input_tensor).sum() # Compute the gradient using the grad function grad_fn = grad(sum_batched_relu) return grad_fn(x)"},{"question":"You are given a list of strings representing mathematical expressions, and you are required to evaluate these expressions and return the results. However, the input list can also contain invalid or incomplete expressions which should be ignored. Your task is to implement a function `evaluate_expressions(expressions)` that: 1. Takes as input a list of strings `expressions`, where each string represents a mathematical expression. 2. Evaluates each valid mathematical expression. 3. Returns a list of tuples, where each tuple contains: - The original expression string. - The result of the expression if it is valid, or `None` if it is invalid. If an expression is invalid or causes an error during evaluation, you should safely handle the error and mark the result as `None`. # Input and Output **Input:** - A list of strings `expressions` where each string represents a mathematical expression. * Example: `[\\"1 + 2\\", \\"3 * (2 + 5)\\", \\"invalid expression\\", \\"10 / 0\\", \\"sqrt(16)\\"]` **Output:** - A list of tuples. Each tuple consists of the expression and its result or `None` if the expression is invalid. * Example: `[(\\"1 + 2\\", 3), (\\"3 * (2 + 5)\\", 21), (\\"invalid expression\\", None), (\\"10 / 0\\", None), (\\"sqrt(16)\\", None)]` # Constraints - Only basic arithmetic operations (`+`, `-`, `*`, `/`) and parentheses are supported. Square root (`sqrt`) and other mathematical functions are not supported and considered invalid expressions. - Division by zero should be handled and considered invalid. - The maximum length of each expression is 100 characters. - The number of expressions in the list will not exceed 1000. # Example ```python expressions = [\\"1 + 2\\", \\"3 * (2 + 5)\\", \\"invalid expression\\", \\"10 / 0\\", \\"sqrt(16)\\"] print(evaluate_expressions(expressions)) # Output: [(\\"1 + 2\\", 3), (\\"3 * (2 + 5)\\", 21), (\\"invalid expression\\", None), (\\"10 / 0\\", None), (\\"sqrt(16)\\", None)] ``` # Implementation Details 1. Use the `eval` function to evaluate the mathematical expressions. 2. Ensure error handling to catch `SyntaxError`, `ZeroDivisionError`, and any other possible exceptions indicating invalid expressions. 3. Do not use any external libraries for parsing or evaluating expressions except built-in Python functions. Implement the function `evaluate_expressions(expressions)` below: ```python def evaluate_expressions(expressions): result = [] for expr in expressions: try: value = eval(expr) result.append((expr, value)) except (SyntaxError, ZeroDivisionError, NameError): result.append((expr, None)) return result ```","solution":"def evaluate_expressions(expressions): Evaluates a list of mathematical expressions and returns the results. Parameters: expressions (list of str): The list of expressions to evaluate. Returns: list of tuples: Each tuple contains the expression and its result, or None if the expression is invalid. result = [] for expr in expressions: try: # Evaluate the expression and store the result value = eval(expr) result.append((expr, value)) except (SyntaxError, ZeroDivisionError, NameError, TypeError): # If there is an error, append the expression with None result.append((expr, None)) return result"},{"question":"# Decision Tree Assessment In this exercise, you will work with the famous Iris dataset to implement and evaluate a decision tree classifier using scikit-learn. Your task is to train a model, handle missing values, visualize the decision tree, and use pruning to improve the model. Task: 1. **Load the Dataset**: - Load the Iris dataset using `sklearn.datasets.load_iris`. - Introduce missing values in 10% of the entries at random. 2. **Handle Missing Values**: - Use an appropriate strategy to handle the missing values before training the decision tree. 3. **Train the Model**: - Train a `DecisionTreeClassifier` model on the modified dataset. - Output the accuracy of the model on the training data. 4. **Visualize the Decision Tree**: - Visualize the trained decision tree using `plot_tree`. 5. **Optimize with Pruning**: - Apply Minimal Cost-Complexity Pruning to avoid overfitting. - Re-train the model with the optimal `ccp_alpha` and output the pruned model\'s accuracy on the training data. Input and Output: - **Input**: - The code should load the Iris dataset internally. - Handle missing values automatically. - **Output**: - Training accuracy before and after pruning. - Visual representation of the decision tree before and after pruning. Example Output: ```plaintext Training Accuracy before Pruning: 0.98 Training Accuracy after Pruning: 0.96 ``` Constraints: 1. Handle the missing values using scikit-learn\'s `SimpleImputer`. 2. Use `train_test_split` with a test size of 0.2 for evaluation. 3. Ensure reproducibility by setting `random_state` where applicable. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Load dataset iris = load_iris() X, y = iris.data, iris.target # Introduce missing values (10%) rng = np.random.default_rng(seed=42) missing_mask = rng.choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool) X[missing_mask] = np.nan # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42) # Train decision tree classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) train_accuracy_before_pruning = accuracy_score(y_train, clf.predict(X_train)) print(f\'Training Accuracy before Pruning: {train_accuracy_before_pruning:.2f}\') # Visualize decision tree plt.figure(figsize=(12, 8)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Apply minimal cost-complexity pruning path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) # Determine the best alpha train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs] optimal_ccp_alpha = ccp_alphas[np.argmax(train_scores)] optimal_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=optimal_ccp_alpha) optimal_clf.fit(X_train, y_train) train_accuracy_after_pruning = accuracy_score(y_train, optimal_clf.predict(X_train)) print(f\'Training Accuracy after Pruning: {train_accuracy_after_pruning:.2f}\') # Visualize pruned decision tree plt.figure(figsize=(12, 8)) plot_tree(optimal_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def train_and_prune_decision_tree(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Introduce missing values (10%) rng = np.random.default_rng(seed=42) missing_mask = rng.choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool) X[missing_mask] = np.nan # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42) # Train decision tree classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) train_accuracy_before_pruning = accuracy_score(y_train, clf.predict(X_train)) print(f\'Training Accuracy before Pruning: {train_accuracy_before_pruning:.2f}\') # Visualize decision tree plt.figure(figsize=(12, 8)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Apply minimal cost-complexity pruning path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) # Determine the best alpha train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs] optimal_ccp_alpha = ccp_alphas[np.argmax(train_scores)] optimal_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=optimal_ccp_alpha) optimal_clf.fit(X_train, y_train) train_accuracy_after_pruning = accuracy_score(y_train, optimal_clf.predict(X_train)) print(f\'Training Accuracy after Pruning: {train_accuracy_after_pruning:.2f}\') # Visualize pruned decision tree plt.figure(figsize=(12, 8)) plot_tree(optimal_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() return train_accuracy_before_pruning, train_accuracy_after_pruning if __name__ == \\"__main__\\": train_and_prune_decision_tree()"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],M={key:0},L={key:1};function N(n,e,l,m,s,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>s.searchQuery=r),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>s.searchQuery="")}," âœ• ")):d("",!0)]),t("div",R,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",q,' No results found for "'+c(s.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[s.isLoading?(a(),i("span",L,"Loading...")):(a(),i("span",M,"See more"))],8,F)):d("",!0)])}const O=p(z,[["render",N],["__scopeId","data-v-3e5214ad"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/48.md","filePath":"chatai/48.md"}'),j={name:"chatai/48.md"},X=Object.assign(j,{setup(n){return(e,l)=>(a(),i("div",null,[x(O)]))}});export{Y as __pageData,X as default};
