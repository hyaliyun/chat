import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-8299c5a9"]]),I=JSON.parse('[{"question":"Objective You are required to implement a function that fetches data from multiple URLs. The function should handle HTTP GET and POST requests, manage headers, and handle potential errors gracefully. Problem Statement Implement a function `fetch_multiple_urls(urls, data=None, headers=None)` that takes a list of URLs and makes HTTP requests to fetch data from those URLs. 1. **URLs (list of str)**: List of URLs to fetch data from. 2. **data (dict, optional)**: Dictionary containing data to be sent via a POST request. If `None`, a GET request should be made. 3. **headers (dict, optional)**: Dictionary containing additional headers to be included in the request. If `None`, no extra headers are added. Function Signature ```python import urllib.request import urllib.parse def fetch_multiple_urls(urls, data=None, headers=None): # your code here ``` Expected Behavior 1. The function should iterate over the list of URLs and fetch their content. 2. If `data` is provided, it should be sent as a POST request. The dictionary should be encoded using `urllib.parse.urlencode` and `data.encode(\'ascii\')`. 3. If `headers` are provided, they should be included in the request. 4. The function should handle exceptions such as `URLError` and `HTTPError` and return appropriate error messages. 5. The function should return a dictionary where keys are URLs and values are either the content retrieved or the error message in case of failure. Example ```python urls = [ \'http://www.example.com\', \'http://www.nonexistentwebsite.xyz\' ] data = { \'name\': \'John Doe\', \'location\': \'Earth\' } headers = { \'User-Agent\': \'Mozilla/5.0\' } result = fetch_multiple_urls(urls, data, headers) ``` **Expected Output:** ```python { \'http://www.example.com\': \'<response content>\', \'http://www.nonexistentwebsite.xyz\': \'Error: <appropriate error message>\' } ``` Constraints 1. The function should handle the case where some URLs might not be reachable or return errors. 2. The response content can be large, ensure to handle large responses efficiently. Tips - Use the `urllib.request.Request` class for building the requests. - Utilize `urlopen` to make the requests and handle exceptions using `try-except` blocks. - Use the `response.read()` method to get the content of the response. - Remember to close the response object after reading from it. Additional Note For advanced students, include handling of different HTTP status codes in the error handling.","solution":"import urllib.request import urllib.parse import socket def fetch_multiple_urls(urls, data=None, headers=None): results = {} for url in urls: try: if data: data_encoded = urllib.parse.urlencode(data).encode(\'ascii\') request = urllib.request.Request(url, data=data_encoded) else: request = urllib.request.Request(url) if headers: for key, value in headers.items(): request.add_header(key, value) with urllib.request.urlopen(request, timeout=10) as response: content = response.read().decode(\'utf-8\') results[url] = content except urllib.error.HTTPError as e: results[url] = f\'Error: HTTPError {e.code} {e.reason}\' except urllib.error.URLError as e: results[url] = f\'Error: URLError {e.reason}\' except socket.timeout as e: results[url] = \'Error: Timeout\' except Exception as e: results[url] = f\'Error: {str(e)}\' return results"},{"question":"# PyTorch IR Programming Challenge PyTorch 2.0 introduces two sets of intermediate representations (IRs) called Core Aten IR and Prims IR. These IRs are used for composing operations that can interface with backend compilers. The Core Aten IR includes a subset of functional operators, while the Prims IR includes primitive operators with explicit type promotion and broadcasting. In this coding challenge, you will implement a custom operation that consists of a composition of both Core Aten IR and Prims IR operators. Your task is to create a function that computes the element-wise power of a tensor and then scales the result by a scalar factor using these IRs. Function Signature ```python import torch def custom_operation(tensor: torch.Tensor, exponent: float, scale_factor: float) -> torch.Tensor: pass ``` Requirements 1. **Input:** - `tensor`: A `torch.Tensor` of any shape. - `exponent`: A `float` representing the power to which each element of the tensor should be raised. - `scale_factor`: A `float` representing the factor by which the result should be scaled. 2. **Output:** - A `torch.Tensor` of the same shape as the input tensor with each element raised to the specified exponent and then scaled by the scale factor. 3. **Constraints:** - You must utilize both Core Aten IR and Prims IR in your implementation. - The function should handle tensors of any shape and value ranges. Example ```python tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) exponent = 2.0 scale_factor = 0.5 result = custom_operation(tensor, exponent, scale_factor) print(result) # Expected output: tensor([[0.5, 2.0], [4.5, 8.0]]) ``` Guidelines 1. Use `Core Aten IR` for performing the exponentiation. 2. Use `Prims IR` for applying the type promotion and scaling the tensor. 3. Please ensure the function is efficient and handles edge cases gracefully. Tips - Review the `Core Aten IR` and `Prims IR` operations available in the provided CSV files to identify the necessary operators. - Ensure proper type promotion and broadcasting where applicable. Good luck, and happy coding!","solution":"import torch def custom_operation(tensor: torch.Tensor, exponent: float, scale_factor: float) -> torch.Tensor: Performs an element-wise power on the input tensor and then scales the result by the given factor. Parameters: tensor (torch.Tensor): Input tensor. exponent (float): The exponent to which each element is raised. scale_factor (float): The scale factor by which the result is multiplied. Returns: torch.Tensor: The resultant tensor after the operations. # Core Aten IR for exponentiation tensor_exp = torch.pow(tensor, exponent) # Prims IR for scaling # Unfortunately, we need to use standard multiplication as the specific Prims IR function is not directly accessible # in standard PyTorch distributions as of the current PyTorch IRs. In a real environment, you should replace this # with the corresponding primitive operation. result = tensor_exp * scale_factor return result"},{"question":"# Secure File Integrity Verification You are tasked with designing a mechanism to ensure the integrity and security of files transferred over a network. For this, you will use the Python cryptographic services. Specifically, you need to: 1. Compute secure hashes of files using the `hashlib` module. 2. Implement a keyed-hashing mechanism using the `hmac` module to ensure the integrity and authenticity of the file. 3. Generate a secure token using the `secrets` module to unique identify each file during the transmission. Requirements 1. **Function 1**: `compute_file_hash(filepath: str) -> str` - Compute and return the SHA-256 hash of the file located at `filepath`. - **Input**: `filepath` (str) — Path to the file. - **Output**: (str) — The SHA-256 hash of the file in hexadecimal format. 2. **Function 2**: `compute_hmac(filepath: str, key: bytes) -> str` - Compute and return the HMAC (using SHA-256) of the file located at `filepath` with the given `key`. - **Input**: 1. `filepath` (str) — Path to the file. 2. `key` (bytes) — The key to use for the HMAC computation. - **Output**: (str) — The computed HMAC in hexadecimal format. 3. **Function 3**: `generate_secure_token() -> str` - Generate and return a secure random token using the `secrets` module. - **Input**: None - **Output**: (str) — A hexadecimal token of at least 16 bytes. Constraints 1. Assume that the file size will not exceed 100MB. 2. You must handle file read/write exceptions and other potential errors gracefully. 3. Ensure that your functions are efficient and utilize appropriate cryptographic standards. Example ```python # Let\'s assume you have a file named \'example.txt\' # Function 1: Compute file hash hash_value = compute_file_hash(\'example.txt\') print(hash_value) # Outputs the SHA-256 hash of the file # Function 2: Compute HMAC key = b\'secret_key\' hmac_value = compute_hmac(\'example.txt\', key) print(hmac_value) # Outputs the HMAC of the file # Function 3: Generate Secure Token token = generate_secure_token() print(token) # Outputs a random secure token ``` Please implement these three functions to meet the described requirements. Ensure you follow best practices and proper error handling.","solution":"import hashlib import hmac import secrets def compute_file_hash(filepath: str) -> str: Compute and return the SHA-256 hash of the file located at filepath. try: sha256_hash = hashlib.sha256() with open(filepath, \\"rb\\") as f: for byte_block in iter(lambda: f.read(4096), b\\"\\"): sha256_hash.update(byte_block) return sha256_hash.hexdigest() except Exception as e: raise RuntimeError(f\\"Error computing file hash: {e}\\") def compute_hmac(filepath: str, key: bytes) -> str: Compute and return the HMAC (using SHA-256) of the file located at filepath with the given key. try: hmac_hash = hmac.new(key, digestmod=hashlib.sha256) with open(filepath, \\"rb\\") as f: for byte_block in iter(lambda: f.read(4096), b\\"\\"): hmac_hash.update(byte_block) return hmac_hash.hexdigest() except Exception as e: raise RuntimeError(f\\"Error computing HMAC: {e}\\") def generate_secure_token() -> str: Generate and return a secure random token. return secrets.token_hex(16)"},{"question":"**Problem Statement:** You have been provided with a dataset containing information about passengers from the Titanic. Your task is to create a comprehensive visual analysis using seaborn to gain insights into the passengers\' demographics and survival rates. **Dataset:** Use the Titanic dataset provided by seaborn: ```python df = sns.load_dataset(\\"titanic\\") ``` **Requirements:** 1. **Strip Plot:** - Create a strip plot of passengers\' ages for each class, colored by their sex. Ensure that the jitter effect is enabled for better visualization. 2. **Box Plot:** - Generate a box plot that compares the age distribution across different classes. Separate the data by the \'sex\' variable. 3. **Violin Plot:** - Create a violin plot to compare the distribution of ages across different classes. Adjust the bandwidth and trim the violin plot tails. Ensure that the violins are split by the \'sex\' variable. 4. **Bar Plot with Subplots:** - Construct bar plots to visualize the survival rate for each class. Create subplots for \'sex\' to show separate bar plots for male and female passengers. Adjust the subplot size and aspect ratio. 5. **Combined Plot:** - Generate a combined plot where a violin plot shows the age distribution (with a light color and without inner marks), and overlay a swarmplot on top to show individual age data points colored by class. Ensure the swarmplot points are small enough to be distinct. 6. **Customization:** - Fine-tune one of the plots (e.g., the bar plot with subplots) by customizing the axes labels, tick labels, titles, setting the y-axis limit between 0 and 1, and removing the spine on the left side. **Constraints:** - Use seaborn and matplotlib.pyplot for your visualizations. - Ensure your plots are easy to read and interpret. - Reuse the dataset for all visualizations. **Expected Output:** - Multiple visualizations as described in the requirements. - Each plot should be displayed and labeled appropriately. **Performance Requirements:** - The code should execute without errors and generate the required visualizations efficiently. **Note:** Include comments in your code to describe each step and part of the visualizations. Ensure that your code is well-structured and adheres to best practices in data visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data(): Perform a comprehensive visual analysis on the Titanic dataset. # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Strip plot of passengers\' ages for each class, colored by sex plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df, jitter=True, palette=\\"Set1\\") plt.title(\\"Strip Plot of Ages for Each Class by Sex\\") plt.show() # Box plot comparing the age distribution across different classes, separated by sex plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df, palette=\\"Set2\\") plt.title(\\"Box Plot of Age Distribution Across Classes by Sex\\") plt.show() # Violin plot comparing the distribution of ages across different classes plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df, palette=\\"Set3\\", split=True, bw=0.2, cut=0) plt.title(\\"Violin Plot of Age Distribution Across Classes by Sex\\") plt.show() # Bar Plot with Subplots: Survival rate for each class by sex g = sns.catplot(x=\\"class\\", hue=\\"survived\\", col=\\"sex\\", data=df, kind=\\"count\\", height=5, aspect=1.2, palette=\\"Set1\\") g.set_axis_labels(\\"Class\\", \\"Survival Count\\") g.fig.suptitle(\\"Survival Rate by Class and Sex\\", y=1.05) plt.show() # Fine-tuning the subplot grid for ax in g.axes.flat: ax.set_ylim(0, 150) ax.set_yticks(range(0, 150, 30)) ax.spines[\'left\'].set_visible(False) # Combined plot: Violin plot and swarmplot for age distribution and individual data points by class plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", data=df, color=\\"lightgrey\\", cut=0) sns.swarmplot(x=\\"class\\", y=\\"age\\", hue=\\"class\\", data=df, palette=\\"Set1\\", size=3, dodge=True) plt.title(\\"Combined Plot of Age Distribution by Class\\") plt.show()"},{"question":"# Email Address Parsing and Formatting In this exercise, you will write a Python function that takes a list of email addresses in raw string format and converts each address into its properly formatted version as per RFC standards using the `email.utils` module. Function Signature ```python def format_email_addresses(raw_addresses: list) -> list: Formats a list of raw email addresses into RFC compliant email addresses. ``` Input - `raw_addresses`: A list of strings, where each string is an email address in raw format. The format can vary and may include unnecessary spaces, improperly quoted names, or missing angle brackets. Output - A list of strings, where each string is a properly formatted email address suitable for use in email headers. Constraints - Each email address string in `raw_addresses` will contain at least an email address. The presence of a real name is optional. - Ensure that the output is formatted according to the RFC 2822 standard for email headers. - Handle edge cases where the raw email address string may be improperly formatted. Example ```python input_addresses = [ \' John Doe <john.doe@example.com> \', \'\\"Jane Doe\\" <jane.doe@example.com>\', \'alice@example.com\', \' Bob Smith < bob.smith@example.com>\' ] formatted_addresses = format_email_addresses(input_addresses) print(formatted_addresses) ``` Expected output: ```python [ \'John Doe <john.doe@example.com>\', \'Jane Doe <jane.doe@example.com>\', \'alice@example.com\', \'Bob Smith <bob.smith@example.com>\' ] ``` Notes - To achieve proper formatting, you will need to use the `email.utils.parseaddr()` and `email.utils.formataddr()` functions. - Focus on stripping unnecessary spaces, ensuring proper quotation, and ensuring each address is correctly encapsulated with angle brackets if needed. - The implementation should handle both naive and aware datetime objects while formatting the output correctly.","solution":"import email.utils def format_email_addresses(raw_addresses): formatted_addresses = [] for address in raw_addresses: name, addr = email.utils.parseaddr(address) formatted_address = email.utils.formataddr((name, addr)).strip() formatted_addresses.append(formatted_address) return formatted_addresses"},{"question":"You are working on a project where you need to process different types of multimedia files based on their MIME types. The `mailcap` module provides a mechanism to retrieve and handle such configurations through mailcap files present on the system. Your task is to implement a function that utilizes the `mailcap` module to determine the command line to execute for a given MIME type and filename. Additionally, if the MIME type is not supported, the function should return a custom error message. Function Signature ```python def get_command_for_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: pass ``` Input - `mime_type` (string): The MIME type of the file. - `filename` (string): The name of the file to process. - `key` (string, optional): The activity to perform on the file (default is \'view\'). - `plist` (list, optional): Named parameters for the command (default is an empty list). Output - (string): The command line to be executed for the given MIME type and filename. If no matching MIME type can be found, the function should return the string `\\"Error: Unsupported MIME type\\"`. Constraints - The `filename` should not contain any disallowed characters (ASCII characters other than alphanumerics and \\"@+=:,./-_\\"). - If any disallowed characters are present in `mime_type` or `plist` values, entries using those values should be ignored. - The function should use the `mailcap` module\'s `getcaps` and `findmatch` functions to derive the command. Example Suppose the system mailcap file contains an entry for `video/mpeg` as follows: ``` video/mpeg; xmpeg %s ``` ```python # Example usage result = get_command_for_mime_type(\\"video/mpeg\\", \\"sample_video.mpg\\") print(result) # Output: \\"xmpeg sample_video.mpg\\" result = get_command_for_mime_type(\\"image/png\\", \\"image_file.png\\") print(result) # Output: \\"Error: Unsupported MIME type\\" ``` # Notes - Make sure to handle security constraints with respect to disallowed characters. - Utilize the `mailcap.getcaps()` function to retrieve mailcap entries. - Use the `mailcap.findmatch()` function to determine the appropriate command line. # Implementation Provide your implementation of the `get_command_for_mime_type` function below: ```python import mailcap import re def get_command_for_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: # Your implementation here pass ```","solution":"import mailcap import re def get_command_for_mime_type(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: # Define a regex pattern to check for disallowed characters disallowed_char_pattern = re.compile(r\'[^a-zA-Z0-9@+=:,./-_]\') # Check for disallowed characters in filename, mime_type, and plist values if disallowed_char_pattern.search(filename) or disallowed_char_pattern.search(mime_type) or any(disallowed_char_pattern.search(str(param)) for param in plist): return \\"Error: Unsupported MIME type\\" # Get mailcap entries caps = mailcap.getcaps() # Find a match for the given mime_type and key command, _ = mailcap.findmatch(caps, mime_type, key, filename=filename, plist=plist) # If no command is found, return the error message if command: return command else: return \\"Error: Unsupported MIME type\\""},{"question":"**Question: Implement and Track a Custom Neural Network** Your task is to implement a custom neural network and use the `torch.utils.module_tracker.ModuleTracker` to track the position inside the network\'s hierarchy. This exercise is designed to test your understanding of PyTorch\'s module system and your ability to use utility classes for monitoring and tracking. # Input: 1. Define a custom neural network class inheriting from `torch.nn.Module`. 2. Write a function `track_model_hierarchy` that takes an instance of your custom neural network and uses `torch.utils.module_tracker.ModuleTracker` to print the current position within the module hierarchy. # Constraints: - The custom neural network must have at least 3 different types of layers (e.g., `Conv2d`, `Linear`, `ReLU`). - Use at least one nested sub-module within your custom network. - The `track_model_hierarchy` function should traverse through the network and print positions within the hierarchy using the `ModuleTracker`. # Example: Consider the following custom neural network: ```python import torch import torch.nn as nn import torch.utils.module_tracker as mt class CustomNetwork(nn.Module): def __init__(self): super(CustomNetwork, self).__init__() self.layer1 = nn.Conv2d(1, 20, 5) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(500, 10) self.sub_module = nn.Sequential( nn.Conv2d(20, 50, 5), nn.ReLU(), nn.Linear(800, 100) ) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x.view(-1, 500)) x = self.sub_module(x.view(-1, 800)) return x ``` Implement the function `track_model_hierarchy`: ```python def track_model_hierarchy(model): tracker = mt.ModuleTracker() # Initialize tracker for name, module in model.named_modules(): if name: # Ignore the parent module itself tracker.enter_module(name) # Enter the module print(\'Current module position:\', tracker) # Print position tracker.exit_module() # Exit the module # Example usage: model = CustomNetwork() track_model_hierarchy(model) ``` # Expected Output: The output should reflect the current position inside the module hierarchy, such as: ``` Current module position: CustomNetwork.layer1 Current module position: CustomNetwork.layer2 Current module position: CustomNetwork.layer3 Current module position: CustomNetwork.sub_module.0 Current module position: CustomNetwork.sub_module.1 Current module position: CustomNetwork.sub_module.2 ``` Implement the function `track_model_hierarchy` and define a custom `CustomNetwork` class as specified. Ensure your function produces the expected output.","solution":"import torch import torch.nn as nn class CustomNetwork(nn.Module): def __init__(self): super(CustomNetwork, self).__init__() self.layer1 = nn.Conv2d(1, 20, 5) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(500, 10) self.sub_module = nn.Sequential( nn.Conv2d(20, 50, 5), nn.ReLU(), nn.Linear(800, 100) ) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x.view(-1, 500)) x = self.sub_module(x.view(-1, 800)) return x def track_model_hierarchy(model): def helper(module, prefix): for name, sub_module in module.named_children(): current_prefix = f\\"{prefix}.{name}\\" if prefix else name print(\'Current module position:\', current_prefix) helper(sub_module, current_prefix) helper(model, \'\') # Example usage: model = CustomNetwork() track_model_hierarchy(model)"},{"question":"Background The `pty` module in Python enables interactions with pseudo-terminals, making it possible to programmatically manage terminal sessions. This can be useful for various applications, such as automated testing of terminal-based applications, creating terminal emulators, or logging terminal sessions. Objective Implement a Python script that mimics a simplified version of a Unix utility for running a command in a pseudo-terminal and logging its output. Task 1. Write a function `run_and_log(command: str, log_file: str, use_shell: bool = False) -> int` that: - Runs the provided command in a pseudo-terminal. - Logs all input and output of the session to the specified log file. - Returns the exit status of the command. 2. The function should handle the following: - The `command` parameter is the command to be executed. If `use_shell` is `True`, the command should be executed using the user\'s default shell. - The `log_file` parameter specifies the file where all input and output should be logged. - If `use_shell` is `True`, determine the user\'s shell from the environment variable `SHELL`. 3. Using the `pty.spawn()` function, manage the pseudo-terminal session and handle I/O redirection to the log file. 4. Ensure proper error handling and resource management, such as closing file descriptors. Constraints - You must use the `pty` module to handle the pseudo-terminal. - Your solution should be compatible with Unix-based systems (Linux, macOS). - The function should raise an appropriate exception if the command cannot be executed. Example ```python import os import pty def run_and_log(command, log_file, use_shell=False): shell = os.environ.get(\'SHELL\', \'sh\') if use_shell else command mode = \'wb\' with open(log_file, mode) as script: def read(fd): data = os.read(fd, 1024) script.write(data) return data script.write((\'Command started on %sn\' % time.asctime()).encode()) exit_status = pty.spawn(shell, None if not use_shell else read) script.write((\'Command done on %sn\' % time.asctime()).encode()) return exit_status # Example usage exit_status = run_and_log(\'ls -l\', \'session.log\', use_shell=True) print(f\\"Exit status: {exit_status}\\") ``` Notes - Ensure your function handles both direct execution of commands and execution through the shell. - Demonstrate the function with various commands and log the outputs to a file named `session.log`. Submission Submit your code as a Python script or Jupyter notebook, along with example outputs from running the provided example commands.","solution":"import os import pty import time def run_and_log(command: str, log_file: str, use_shell: bool = False) -> int: Runs the provided command in a pseudo-terminal and logs input and output to the specified log file. Parameters: - command: The command to be executed. - log_file: The file where all input and output should be logged. - use_shell: If True, execute the command using the user\'s default shell. Returns: - The exit status of the command. def read(fd): data = os.read(fd, 1024) script.write(data) return data shell = command if use_shell: shell = os.environ.get(\'SHELL\', \'sh\') mode = \'wb\' with open(log_file, mode) as script: script.write((\'Command started on %sn\' % time.asctime()).encode()) exit_status = pty.spawn(shell if not use_shell else [shell, \'-c\', command], read) script.write((\'Command done on %sn\' % time.asctime()).encode()) return exit_status"},{"question":"**Coding Assessment Question: Advanced Task Scheduling and Error Handling in asyncio** # Objective Implement a Python function using the `asyncio` package to create and manage multiple asynchronous tasks, handle scheduling of callbacks, and incorporate custom error handling in an asyncio event loop. # Problem Statement You are required to create a function `run_event_loop_with_task_handling` that does the following: 1. **Create and run multiple asynchronous tasks**: - Create 3 tasks that each print a message (including the task number) after sleeping for a varying amount of time (1 second, 2 seconds, and 3 seconds respectively). 2. **Schedule a callback to run immediately after the last task is completed**: - The callback should print \\"All tasks completed!\\". 3. **Handle Exceptions**: - Implement a custom exception handler for the event loop that logs exceptions occurring within tasks or callbacks to the console. # Input The function `run_event_loop_with_task_handling` will take no parameters. # Expected behavior: The function should follow these steps: 1. Create the event loop. 2. Define and create 3 asynchronous tasks. 3. Schedule a callback that runs after all tasks are completed. 4. Implement a custom exception handler that prints the exception details to the console. 5. Run the event loop until all tasks and the scheduled callback are completed. # Example Output ```plaintext Task 1 completed after 1 seconds Task 2 completed after 2 seconds Task 3 completed after 3 seconds All tasks completed! Custom Exception Handler: exception in task <Task> ``` # Constraints - Ensure that all tasks are awaited properly and the callback runs only after all tasks are done. - Your function should handle exceptions gracefully using the custom exception handler. # Notes - Use `asyncio.get_running_loop()` to get the current running event loop. - Use `loop.run_until_complete()` or `loop.run_forever()` to manage the event loop\'s execution. - Use `asyncio.sleep()` to simulate the task delays. - Define your callback and schedule it using `loop.call_soon()`. - Implement the custom exception handler using `loop.set_exception_handler()`. # Function Signature ```python import asyncio def run_event_loop_with_task_handling(): # Function implementation # Run the function if this script is the entry point if __name__ == \\"__main__\\": run_event_loop_with_task_handling() ```","solution":"import asyncio async def task(number, delay): await asyncio.sleep(delay) print(f\\"Task {number} completed after {delay} seconds\\") def all_tasks_done_callback(): print(\\"All tasks completed!\\") def custom_exception_handler(loop, context): exception = context.get(\'exception\') print(f\\"Custom Exception Handler: {exception}\\") def run_event_loop_with_task_handling(): async def main(): loop = asyncio.get_running_loop() tasks = [ loop.create_task(task(1, 1)), loop.create_task(task(2, 2)), loop.create_task(task(3, 3)) ] await asyncio.gather(*tasks) loop.call_soon(all_tasks_done_callback) loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) loop.set_exception_handler(custom_exception_handler) loop.run_until_complete(main()) loop.close() if __name__ == \\"__main__\\": run_event_loop_with_task_handling()"},{"question":"<|Analysis Begin|> The provided documentation details the functionality of the \'hmac\' module, which implements the HMAC algorithm according to RFC 2104. The key features and functions include: 1. **hmac.new(key, msg=None, digestmod=\'\')**: Creates a new HMAC object. 2. **hmac.digest(key, msg, digest)**: Returns the HMAC digest for a given key, message, and digest type. 3. **HMAC.update(msg)**: Updates the HMAC object with more message data. 4. **HMAC.digest()**: Returns the binary digest of the accumulated message data. 5. **HMAC.hexdigest()**: Returns the hexadecimal digest of the accumulated message data. 6. **HMAC.copy()**: Returns a copy of the HMAC object. 7. **hmac.compare_digest(a, b)**: Compares two digests in a timing-attack resistant way. The documentation specifies that the \'key\', \'msg\', and \'digestmod\' parameters can be of varying types and that certain functions have constraints and performance optimizations. <|Analysis End|> <|Question Begin|> **Objective:** Demonstrate your understanding and ability to implement and utilize the HMAC (Keyed-Hashing for Message Authentication) algorithm using Python\'s \'hmac\' module. **Task:** Write a Python function `generate_hmac_key` that generates and verifies an HMAC for a message under given constraints. **Function Signature:** ```python def generate_hmac_key(key: bytes, message: str, digestmod: str) -> str: # your code here ``` **Input:** - `key`: A `bytes` object that represents the secret key. The length of the key should be at least 16 bytes and at most 64 bytes. - `message`: A `string` that represents the message to be hashed. - `digestmod`: A `string` representing the hash algorithm to be used. This should be a valid algorithm name suitable for `hashlib.new()` (e.g., \'sha256\', \'sha512\'). **Output:** - Returns a `string` that represents the hexadecimal digest of the generated HMAC of the supplied message. **Constraints:** - The `key` length must be between 16 and 64 bytes. - The `digestmod` must be a valid hash algorithm name supported by the `hashlib` module. - The function should raise a `ValueError` if the `key` length or `digestmod` is invalid. **Example:** ```python # Example 1 key = b\'secretkey1234567\' message = \\"This is a secret message\\" digestmod = \'sha256\' print(generate_hmac_key(key, message, digestmod)) # Output: (will be a 64-character hex digest, as hexadecimal digits are used to represent binary data that is 256 bits long) # Example 2 key = b\'anothersecretkeywithlength32bytes1234\' message = \\"Another secret message\\" digestmod = \'sha1\' print(generate_hmac_key(key, message, digestmod)) # Output: (will be a 40-character hex digest, as hexadecimal digits are used to represent binary data that is 160 bits long) ``` **Note:** Ensure you use the `hmac` and `hashlib` modules efficiently to achieve the task. Raise appropriate errors for invalid inputs.","solution":"import hmac import hashlib def generate_hmac_key(key: bytes, message: str, digestmod: str) -> str: Generates an HMAC for the given message using the specified key and hash algorithm. Args: key (bytes): The secret key to use for HMAC. Must be between 16 and 64 bytes. message (str): The message to hash. digestmod (str): The hash algorithm to use (e.g., \'sha256\', \'sha512\'). Returns: str: The hexadecimal digest of the generated HMAC. Raises: ValueError: If the key length is not within the specified bounds or if the digestmod is invalid. if not (16 <= len(key) <= 64): raise ValueError(\\"Key length must be between 16 and 64 bytes.\\") try: hashlib.new(digestmod) except ValueError: raise ValueError(\\"Invalid digestmod specified.\\") hmac_obj = hmac.new(key, message.encode(), digestmod) return hmac_obj.hexdigest()"},{"question":"**Task**: Using the scikit-learn `random_projection` module, implement a Python function that performs the following steps: 1. Generates a synthetic dataset with `n_samples` samples and `n_features` features. The data should be drawn from a standard normal distribution. 2. Applies both a Gaussian random projection and a sparse random projection to reduce the dimensionality of the dataset to `n_components`. 3. Computes the inverse transformation for both projections. 4. Compares the inversely transformed datasets against the original dataset by calculating the mean squared error (MSE) for each. **Function Signature**: ```python def random_projection_comparison(n_samples: int, n_features: int, n_components: int) -> dict: pass ``` **Inputs**: - `n_samples` (int): Number of samples in the synthetic dataset. - `n_features` (int): Number of features in the synthetic dataset. - `n_components` (int): Number of components for dimensionality reduction. **Outputs**: - `result` (dict): A dictionary with the following key-value pairs: - `\'mse_gaussian\'`: Mean squared error of the original data and the data inversely transformed from its Gaussian random projection. - `\'mse_sparse\'`: Mean squared error of the original data and the data inversely transformed from its sparse random projection. **Constraints**: - The function should handle cases where `n_components < n_features`. **Performance Requirements**: - The function should execute efficiently even for large `n_samples` and `n_features`. **Example**: ```python result = random_projection_comparison(n_samples=100, n_features=10000, n_components=3947) print(result) # Expected output: {\'mse_gaussian\': <some-float-value>, \'mse_sparse\': <some-float-value>} ``` **Notes**: - You may assume that all necessary libraries (numpy, scikit-learn, etc.) are already installed and imported. **Instructions**: Implement the `random_projection_comparison` function as specified. Ensure your solution is properly documented and handles edge cases gracefully.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def random_projection_comparison(n_samples: int, n_features: int, n_components: int) -> dict: Generates a synthetic dataset, applies Gaussian and sparse random projections to reduce dimensionality, computes their inverse transforms, and calculates the mean squared error with the original dataset. Args: n_samples (int): Number of samples in the synthetic dataset. n_features (int): Number of features in the synthetic dataset. n_components (int): Number of components for dimensionality reduction. Returns: dict: A dictionary with mean squared errors for Gaussian and sparse random projections. # Step 1: Generate a synthetic dataset X = np.random.randn(n_samples, n_features) # Step 2: Apply Gaussian Random Projection grp = GaussianRandomProjection(n_components=n_components) X_gaussian_projected = grp.fit_transform(X) # Step 3: Apply Sparse Random Projection srp = SparseRandomProjection(n_components=n_components) X_sparse_projected = srp.fit_transform(X) # Step 4: Inverse transform the projections X_gaussian_inverse = grp.inverse_transform(X_gaussian_projected) X_sparse_inverse = srp.inverse_transform(X_sparse_projected) # Step 5: Compute the MSE for both projections mse_gaussian = mean_squared_error(X, X_gaussian_inverse) mse_sparse = mean_squared_error(X, X_sparse_inverse) return { \'mse_gaussian\': mse_gaussian, \'mse_sparse\': mse_sparse }"},{"question":"# Python Coding Assessment Question **Title: Task Scheduler** **Objective:** Implement a task scheduler to manage a list of tasks with different priorities and completion states. **Problem Description:** You are required to write a Python class `TaskScheduler` that manages a collection of tasks. Each task is represented as a dictionary with the following keys: - `\'name\'`: A `str` indicating the name of the task. - `\'priority\'`: An `int` indicating the priority of the task (higher value means higher priority). - `\'completed\'`: A `bool` indicating whether the task is completed or not. The `TaskScheduler` class should support the following methods: 1. `add_task(name: str, priority: int) -> None`: Adds a new task with the given name and priority. By default, the task is not completed. 2. `remove_task(name: str) -> None`: Removes the task with the given name. If the task does not exist, raise a `ValueError` with the message \\"Task not found\\". 3. `complete_task(name: str) -> None`: Marks the task with the given name as completed. If the task does not exist, raise a `ValueError` with the message \\"Task not found\\". 4. `get_tasks(priority: Optional[int] = None, completed: bool = None) -> List[Dict[str, Any]]`: Returns a list of tasks filtered by the given priority and/or completion status. If no filters are provided, all tasks are returned sorted by priority in descending order. 5. `clear_completed_tasks() -> None`: Removes all tasks that are marked as completed. **Constraints:** - Task names are unique. Adding a task with an existing name should raise a `ValueError` with the message \\"Task already exists\\". - Assume that there are no more than 100 tasks at any given time. **Example:** ```python scheduler = TaskScheduler() scheduler.add_task(\\"Do laundry\\", 3) scheduler.add_task(\\"Buy groceries\\", 1) scheduler.add_task(\\"Clean house\\", 2) scheduler.complete_task(\\"Do laundry\\") print(scheduler.get_tasks()) # Output: [{\'name\': \'Do laundry\', \'priority\': 3, \'completed\': True}, # {\'name\': \'Clean house\', \'priority\': 2, \'completed\': False}, # {\'name\': \'Buy groceries\', \'priority\': 1, \'completed\': False}] print(scheduler.get_tasks(completed=False)) # Output: [{\'name\': \'Clean house\', \'priority\': 2, \'completed\': False}, # {\'name\': \'Buy groceries\', \'priority\': 1, \'completed\': False}] scheduler.clear_completed_tasks() print(scheduler.get_tasks()) # Output: [{\'name\': \'Clean house\', \'priority\': 2, \'completed\': False}, # {\'name\': \'Buy groceries\', \'priority\': 1, \'completed\': False}] ``` **Implementation Requirements:** - You are required to use list methods and comprehensions where appropriate. - Ensure efficient and readable code. - Include appropriate error handling.","solution":"class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, name: str, priority: int) -> None: if any(task[\'name\'] == name for task in self.tasks): raise ValueError(\\"Task already exists\\") self.tasks.append({\'name\': name, \'priority\': priority, \'completed\': False}) def remove_task(self, name: str) -> None: for task in self.tasks: if task[\'name\'] == name: self.tasks.remove(task) return raise ValueError(\\"Task not found\\") def complete_task(self, name: str) -> None: for task in self.tasks: if task[\'name\'] == name: task[\'completed\'] = True return raise ValueError(\\"Task not found\\") def get_tasks(self, priority: int = None, completed: bool = None): filtered_tasks = self.tasks if priority is not None: filtered_tasks = [task for task in filtered_tasks if task[\'priority\'] == priority] if completed is not None: filtered_tasks = [task for task in filtered_tasks if task[\'completed\'] == completed] return sorted(filtered_tasks, key=lambda x: x[\'priority\'], reverse=True) def clear_completed_tasks(self) -> None: self.tasks = [task for task in self.tasks if not task[\'completed\']]"},{"question":"# Bytecode Analysis using the `dis` Module Objective: Your task is to analyze a Python function using the `dis` module, generate the bytecode, and extract specific details from the disassembled output. Problem Statement: Implement a function `analyze_function_bytecode` that takes a single argument `func`, which is a Python function object. The function should return a dictionary containing the following information extracted from the bytecode of `func`: 1. `line_numbers`: A list of all the line numbers where bytecode instructions are generated. 2. `opnames`: A list of the opnames (operation names) of the bytecode instructions in the same order they appear. 3. `jump_targets`: A list of instruction offsets that are jump targets. # Input: - `func` (function): A Python function object to be analyzed. # Output: - A dictionary containing the keys `line_numbers`, `opnames`, and `jump_targets` with their corresponding values as described. # Example: ```python def example_function(x): if x > 0: return x * 2 else: return -x result = analyze_function_bytecode(example_function) print(result) ``` Expected output format: ```python { \'line_numbers\': [1, 2, 3, 4], \'opnames\': [\'LOAD_FAST\', \'LOAD_CONST\', \'COMPARE_OP\', \'POP_JUMP_IF_FALSE\', ...], \'jump_targets\': [10, 18] } ``` # Constraints: - The function to be analyzed will be a valid Python function with up to 100 lines of code. - Your implementation should correctly handle functions with varying complexity, including conditionals and loops. # Performance Requirements: - Your implementation should efficiently use the functionalities provided by the `dis` module to extract the required details from the bytecode. Function Signature: ```python def analyze_function_bytecode(func: callable) -> dict: ``` Good luck!","solution":"import dis def analyze_function_bytecode(func): Analyzes the bytecode of a given function and returns a dictionary containing line numbers, opnames, and jump targets. Args: func (callable): The function to analyze. Returns: dict: A dictionary with keys \'line_numbers\', \'opnames\', and \'jump_targets\'. bytecode = dis.Bytecode(func) line_numbers = [] opnames = [] jump_targets = set() for instruction in bytecode: line_numbers.append(instruction.starts_line if instruction.starts_line is not None else line_numbers[-1]) opnames.append(instruction.opname) if instruction.is_jump_target: jump_targets.add(instruction.offset) return { \'line_numbers\': line_numbers, \'opnames\': opnames, \'jump_targets\': list(jump_targets) }"},{"question":"# Audio Signal Processing with `audioop` **Objective:** Implement a function that processes an audio fragment to adjust its volume, apply a bias, and convert it to a specific encoding format. **Function Signature:** ```python def process_audio(fragment: bytes, width: int, volume_factor: float, bias: int, encoding: str) -> bytes: Processes an audio fragment by adjusting its volume, applying a bias, and converting it to a specified encoding format. Parameters: fragment (bytes): The input audio fragment containing signed integer samples. width (int): The sample width in bytes, either 1, 2, 3, or 4. volume_factor (float): The factor by which to adjust the volume. For example, a factor of 1.2 increases the volume by 20%. bias (int): The bias to apply to each sample. encoding (str): The target encoding format. It can be one of \\"alaw\\", \\"ulaw\\", or \\"adpcm\\". Returns: bytes: The processed audio fragment in the specified encoding format. Raises: ValueError: If the encoding is not supported. ``` **Instructions:** 1. Adjust the volume of the audio fragment by multiplying each sample by the given `volume_factor`. This can be achieved using `audioop.mul`. 2. Apply a bias to each sample in the fragment using `audioop.bias`. 3. Convert the audio fragment to the specified encoding format. You need to handle three encoding formats: - \\"alaw\\": Use `audioop.lin2alaw`. - \\"ulaw\\": Use `audioop.lin2ulaw`. - \\"adpcm\\": Use `audioop.lin2adpcm`. Note that this also requires managing the coder state. 4. Ensure your function handles invalid encoding formats by raising a `ValueError`. **Constraints:** - You can assume that `fragment` is non-empty and its length is a multiple of `width`. - The `width` will always be one of 1, 2, 3, or 4. - The `encoding` will be one of \\"alaw\\", \\"ulaw\\", or \\"adpcm\\". **Example:** ```python fragment = b\'x01x02x03x04\' width = 2 volume_factor = 1.5 bias = 100 encoding = \'alaw\' processed_fragment = process_audio(fragment, width, volume_factor, bias, encoding) print(processed_fragment) # Expected output: Processed audio fragment in a-LAW encoding ``` **Notes:** - Utilize the appropriate functions from the `audioop` module to perform each step. - Make sure to handle the necessary state management for the ADPCM encoding.","solution":"import audioop def process_audio(fragment: bytes, width: int, volume_factor: float, bias: int, encoding: str) -> bytes: Processes an audio fragment by adjusting its volume, applying a bias, and converting it to a specified encoding format. Parameters: fragment (bytes): The input audio fragment containing signed integer samples. width (int): The sample width in bytes, either 1, 2, 3, or 4. volume_factor (float): The factor by which to adjust the volume. For example, a factor of 1.2 increases the volume by 20%. bias (int): The bias to apply to each sample. encoding (str): The target encoding format. It can be one of \\"alaw\\", \\"ulaw\\", or \\"adpcm\\". Returns: bytes: The processed audio fragment in the specified encoding format. Raises: ValueError: If the encoding is not supported. # Adjust the volume of the fragment adjusted_fragment = audioop.mul(fragment, width, volume_factor) # Apply the bias to the fragment biased_fragment = audioop.bias(adjusted_fragment, width, bias) # Convert the fragment to the specified encoding if encoding == \'alaw\': processed_fragment = audioop.lin2alaw(biased_fragment, width) elif encoding == \'ulaw\': processed_fragment = audioop.lin2ulaw(biased_fragment, width) elif encoding == \'adpcm\': processed_fragment, _ = audioop.lin2adpcm(biased_fragment, width, None) else: raise ValueError(f\\"Unsupported encoding format: {encoding}\\") return processed_fragment"},{"question":"**Objective**: To assess your understanding and ability to utilize `scikit-learn`\'s dataset loading functionality, process the data, and apply a machine learning model for classification. Description: You are provided with a task to load the Olivetti faces dataset, preprocess the data, and perform a classification task to distinguish between different individuals using a Support Vector Machine (SVM) classifier. The Olivetti faces dataset consists of 400 images of 40 individuals (10 images per individual). Each image is grayscale with a resolution of 64x64 pixels. Requirements: 1. Write a function `fetch_and_classify_olivetti_faces` that performs the following steps: - Fetch the Olivetti faces dataset using the `fetch_olivetti_faces` function from `sklearn.datasets`. - Standardize the pixel values across all images to have zero mean and unit variance. - Split the dataset into training (80%) and testing (20%) subsets. - Train an SVM classifier with a linear kernel on the training data. - Evaluate the classifier on the testing data and return the accuracy. # Function Signature: ```python def fetch_and_classify_olivetti_faces() -> float: ``` # Input: - The function does not take any input. # Output: - Return a floating-point number representing the accuracy of the SVM classifier on the test data. # Constraints and Notes: - Use `StandardScaler` from `sklearn.preprocessing` for standardizing the data. - For splitting the dataset, use `train_test_split` from `sklearn.model_selection` with `random_state=42`. - Use the `SVC` class from `sklearn.svm` to create the Support Vector Machine classifier. ```python # Example solution structure from sklearn.datasets import fetch_olivetti_faces from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def fetch_and_classify_olivetti_faces() -> float: # Step 1: Fetch the dataset data = fetch_olivetti_faces() X, y = data[\'data\'], data[\'target\'] # Step 2: Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the SVM classifier svm_clf = SVC(kernel=\'linear\') svm_clf.fit(X_train, y_train) # Step 5: Predict and calculate accuracy y_pred = svm_clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy ``` Submit your implemented function. Ensure the function performs all required steps and returns the accuracy as specified.","solution":"from sklearn.datasets import fetch_olivetti_faces from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def fetch_and_classify_olivetti_faces() -> float: # Step 1: Fetch the dataset data = fetch_olivetti_faces() X, y = data[\'data\'], data[\'target\'] # Step 2: Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the SVM classifier svm_clf = SVC(kernel=\'linear\') svm_clf.fit(X_train, y_train) # Step 5: Predict and calculate accuracy y_pred = svm_clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Question: Binary Data Processing and Transformation You are provided with a binary file containing a sequence of records. Each record consists of the following fields in the given order: 1. An unsigned integer (4 bytes) indicating a unique identifier. 2. A signed short integer (2 bytes) indicating the score. 3. A string of fixed length 10 bytes representing a username, encoded in ASCII. # Task 1. **Read the File**: - Write a function `read_binary_file(file_path: str) -> List[Tuple[int, int, str]]` that reads the binary file and returns a list of tuples. Each tuple should contain three elements corresponding to the fields of a record: `id` (int), `score` (int), and `username` (str). The username string should be stripped of any trailing null bytes (`x00`). 2. **Write to a New File**: - Write a function `write_binary_file(file_path: str, data: List[Tuple[int, int, str]]) -> None` that writes the list of tuples back into a new binary file, maintaining the original binary structure and encoding format. - Ensure that the username is re-encoded to 10 bytes, padded with null bytes (`x00`) if necessary. # Input - The `read_binary_file` function will take a string `file_path`, which is the path to the input binary file. - The `write_binary_file` function will take a string `file_path` for the output file and a list of tuples `data` representing the read records. # Output - `read_binary_file` should return a list of tuples, with each tuple containing the fields as described (id, score, username). - `write_binary_file` has no return but should write the binary data correctly formatted to a new file. # Example Given a binary file structured as specified: - Record 1: id=1, score=100, username=\\"user1\\" - Record 2: id=2, score=90, username=\\"admin\\" After reading and writing back using the provided functions, the file\'s binary structure should remain consistent with the original format. # Constraints - Assume the binary file is well-formed and contains an integer number of records as specified. - The username is always ASCII encoded. # Functions Signatures ```python def read_binary_file(file_path: str) -> List[Tuple[int, int, str]]: pass def write_binary_file(file_path: str, data: List[Tuple[int, int, str]]) -> None: pass ``` # Notes - Use the `struct` module to pack and unpack binary data. - Use proper exception handling for file operations.","solution":"import struct from typing import List, Tuple def read_binary_file(file_path: str) -> List[Tuple[int, int, str]]: records = [] # Define the format for struct.unpack record_struct = struct.Struct(\'Ih10s\') with open(file_path, \'rb\') as file: while True: record_bytes = file.read(record_struct.size) if not record_bytes: break id, score, username = record_struct.unpack(record_bytes) username = username.decode(\'ascii\').rstrip(\'x00\') records.append((id, score, username)) return records def write_binary_file(file_path: str, data: List[Tuple[int, int, str]]) -> None: # Define the format for struct.pack record_struct = struct.Struct(\'Ih10s\') with open(file_path, \'wb\') as file: for record in data: id, score, username = record username = username.encode(\'ascii\')[:10].ljust(10, b\'x00\') record_bytes = record_struct.pack(id, score, username) file.write(record_bytes)"},{"question":"# Pandas Options Mastery Challenge **Objective:** Demonstrate your ability to dynamically manipulate pandas settings to optimize DataFrame display based on specific requirements. **Context:** Suppose you are working on a large dataset where the default pandas display settings are not ideal. You need to adjust these settings to make data analysis more efficient and readable. You will work with the following DataFrame: ```python import pandas as pd import numpy as np df = pd.DataFrame({ \'A\': np.random.randn(50), \'B\': np.random.randint(0, 100, size=50), \'C\': np.random.choice([\'foo\', \'bar\', \'baz\'], size=50), \'D\': np.random.random(size=50) }) ``` **Task:** 1. **Initial Setup:** Write a function `configure_pandas_display()` that sets the following pandas display options: - Maximum number of rows to display: 10 - Maximum width of each column: 20 - Display precision for floating-point numbers: 4 decimal places 2. **Temporarily Adjust Settings:** Write a function `temporary_display_settings()` that utilizes `option_context` to temporarily set the following options while executing the provided code block: - Maximum number of rows to display: 5 - Display precision for floating-point numbers: 2 decimal places Within this function, print the DataFrame `df`. 3. **Reset to Default:** Write a function `reset_to_default_settings()` that resets all pandas display options to their default values. **Expected Input and Output:** - **Function 1:** `configure_pandas_display()` - **Input:** None - **Output:** None (However, verify by printing `pd.get_option` values for `display.max_rows`, `display.max_colwidth`, and `display.precision`) - **Function 2:** `temporary_display_settings()` - **Input:** None - **Output:** A printed DataFrame with the temporary settings applied. - **Function 3:** `reset_to_default_settings()` - **Input:** None - **Output:** None (However, verify by printing `pd.get_option` values to ensure they are reset) **Constraints:** - Ensure that the changes to pandas settings do not unintentionally affect subsequent code executions. - Use the functions `get_option`, `set_option`, `reset_option`, and `option_context` where applicable. # Example Usage: ```python configure_pandas_display() print(f\\"Max rows: {pd.get_option(\'display.max_rows\')}\\") print(f\\"Max colwidth: {pd.get_option(\'display.max_colwidth\')}\\") print(f\\"Precision: {pd.get_option(\'display.precision\')}\\") temporary_display_settings() reset_to_default_settings() print(f\\"Max rows (reset): {pd.get_option(\'display.max_rows\')}\\") print(f\\"Max colwidth (reset): {pd.get_option(\'display.max_colwidth\')}\\") print(f\\"Precision (reset): {pd.get_option(\'display.precision\')}\\") ```","solution":"import pandas as pd def configure_pandas_display(): Sets the pandas display options. pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_colwidth\', 20) pd.set_option(\'display.precision\', 4) def temporary_display_settings(df): Temporarily sets pandas display options and prints the DataFrame. with pd.option_context(\'display.max_rows\', 5, \'display.precision\', 2): print(df) def reset_to_default_settings(): Resets all pandas display options to their default values. pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_colwidth\') pd.reset_option(\'display.precision\')"},{"question":"**Objective:** Write a function that assesses students\' understanding of the `sndhdr` module and involves processing sound file information. **Task:** You are tasked with writing a function `collect_sound_file_info(filepaths: List[str]) -> Dict[str, Any]` that takes in a list of file paths pointing to various sound files. Your function should process each file using the `sndhdr.whathdr()` function and return a dictionary with the following keys and values: - **\'file_count\'**: The total number of files processed. - **\'valid_files\'**: A list of file paths where the file type was successfully determined. - **\'invalid_files\'**: A list of file paths where the file type could not be determined. - **\'file_info\'**: A dictionary where each key is the file path of a valid sound file and the corresponding value is another dictionary containing the extracted details: filetype, framerate, nchannels, nframes, and sampwidth. **Input:** - `filepaths` (List[str]): A list of strings representing the file paths to the sound files. **Output:** - A dictionary with keys \'file_count\', \'valid_files\', \'invalid_files\', and \'file_info\' structured as described above. **Constraints:** - Handle any exceptions that might arise due to file I/O operations and ensure the function doesn\'t crash. - Ensure that the namedtuples returned by `sndhdr.whathdr()` are properly converted to dictionaries before storing them in the \'file_info\' dictionary. **Example:** ```python from sndhdr import whathdr from typing import List, Dict, Any def collect_sound_file_info(filepaths: List[str]) -> Dict[str, Any]: result = { \'file_count\': 0, \'valid_files\': [], \'invalid_files\': [], \'file_info\': {} } for filepath in filepaths: try: sound_info = whathdr(filepath) result[\'file_count\'] += 1 if sound_info: result[\'valid_files\'].append(filepath) result[\'file_info\'][filepath] = sound_info._asdict() else: result[\'invalid_files\'].append(filepath) except Exception as e: result[\'invalid_files\'].append(filepath) return result # Example usage: filepaths = [\'test1.wav\', \'test2.aiff\', \'test3.invalid\'] print(collect_sound_file_info(filepaths)) # Output structure: # { # \'file_count\': 3, # \'valid_files\': [\'test1.wav\', \'test2.aiff\'], # \'invalid_files\': [\'test3.invalid\'], # \'file_info\': { # \'test1.wav\': {\'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 1024, \'sampwidth\': 16}, # \'test2.aiff\': {\'filetype\': \'aiff\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 1024, \'sampwidth\': 16} # } # } ``` **Note:** - You need to ensure that the `sndhdr` module is available in your environment. - Remember to handle cases where `whathdr` might return `None`.","solution":"from sndhdr import whathdr from typing import List, Dict, Any def collect_sound_file_info(filepaths: List[str]) -> Dict[str, Any]: result = { \'file_count\': 0, \'valid_files\': [], \'invalid_files\': [], \'file_info\': {} } for filepath in filepaths: try: sound_info = whathdr(filepath) result[\'file_count\'] += 1 if sound_info: result[\'valid_files\'].append(filepath) result[\'file_info\'][filepath] = { \'filetype\': sound_info.filetype, \'framerate\': sound_info.framerate, \'nchannels\': sound_info.nchannels, \'nframes\': sound_info.nframes, \'sampwidth\': sound_info.sampwidth } else: result[\'invalid_files\'].append(filepath) except Exception: result[\'invalid_files\'].append(filepath) return result"},{"question":"**Objective:** Implement a function in Python using the hashlib module that securely hashes a given password with a specified salt and iteration count using the `pbkdf2_hmac` method. Your function should also demonstrate the ability to update the hash with additional data, retrieve the hex digest of the hash, and utilize personalization for different hashing contexts. **Function Signature:** ```python def secure_password_hash(password: str, salt: bytes, iterations: int, context: str) -> str: Securely hash a password with a given salt and iteration count, with context-dependent personalization. Parameters: - password (str): The password to be hashed. - salt (bytes): The salt to be used for hashing. - iterations (int): The number of iterations to be used in the pbkdf2_hmac method. - context (str): A string representing the context for personalization. Returns: - str: The hexadecimal digest of the hashed password. pass ``` **Input and Output:** - The function takes a password (string), salt (bytes), iterations (integer), and context (string) as inputs. - The function returns the hexadecimal digest of the hashed password (string). **Constraints:** - `password` should be a non-empty string. - `salt` should be a byte string of length >= 16. - `iterations` should be a positive integer with a preferred value of at least 100,000. - `context` should be a non-empty string. **Performance Requirements:** - The function should handle typical password hashing scenarios efficiently, with a focus on security and avoiding collisions. **Example Usage:** ```python password = \\"my_secure_password\\" salt = b\\"x8ex94xd7xadx1ex56xd5xa6xc9x79x3f\\" iterations = 150000 context = \\"login\\" hex_digest = secure_password_hash(password, salt, iterations, context) print(hex_digest) # Expected output: a 64-character hexadecimal string ``` **Notes:** - Use the `pbkdf2_hmac` method from hashlib for password hashing. - Incorporate context-based personalization in the hash function. - Ensure that the hashing process iteratively updates the hash with byte-formatted password input. **Hints:** - The `pbkdf2_hmac` method requires a hash name, the password, salt, number of iterations, and the derived key length (`dklen` as optional). - For personalization, consider combining the context with the original password or salt prior to hashing. - The `hex_digest` method will help convert the binary digest to hex format suitable for display or storage.","solution":"import hashlib def secure_password_hash(password: str, salt: bytes, iterations: int, context: str) -> str: Securely hash a password with a given salt and iteration count, with context-dependent personalization. Parameters: - password (str): The password to be hashed. - salt (bytes): The salt to be used for hashing. - iterations (int): The number of iterations to be used in the pbkdf2_hmac method. - context (str): A string representing the context for personalization. Returns: - str: The hexadecimal digest of the hashed password. if not password: raise ValueError(\\"Password cannot be empty\\") if not isinstance(salt, bytes) or len(salt) < 16: raise ValueError(\\"Salt must be a byte string of length at least 16\\") if iterations <= 0: raise ValueError(\\"Iterations must be a positive integer\\") if not context: raise ValueError(\\"Context cannot be empty\\") # Combine the context and password for personalization personalized_data = context.encode() + password.encode() # Generate the hash using pbkdf2_hmac dk = hashlib.pbkdf2_hmac(\'sha256\', personalized_data, salt, iterations) # Convert the derived key to a hex string hex_digest = dk.hex() return hex_digest"},{"question":"**Cryptographic Services Assessment** You are required to implement a secure authentication system using Python\'s cryptographic services. The system needs to perform the following tasks: 1. **Password Hashing and Verification**: - Implement a function to securely hash a user\'s password using the `hashlib` module. - Implement a function to verify a user\'s password against the stored hash. 2. **Token Generation**: - Implement a function to generate a secure token using the `secrets` module. This token will be used for session management or password recovery. 3. **Message Authentication**: - Implement a function to authenticate messages using HMAC with a secret key. Use the `hmac` module. # Detailed Requirements 1. **Password Hashing and Verification** - `hash_password(password: str) -> str`: This function takes a plain-text password as input and returns a securely hashed password. - `verify_password(password: str, hashed_password: str) -> bool`: This function takes a plain-text password and a hashed password, and returns `True` if the password matches the hash, `False` otherwise. 2. **Token Generation** - `generate_token(length: int) -> str`: This function generates a secure token of the specified length. 3. **Message Authentication** - `authenticate_message(message: str, key: str) -> str`: This function takes a message and a secret key, and returns the HMAC of the message using the given key. - `verify_message(message: str, key: str, hmac_to_verify: str) -> bool`: This function takes a message, a secret key, and an HMAC to verify. It returns `True` if the HMAC is valid, `False` otherwise. # Constraints and Considerations - Use the `hashlib` module\'s SHA256 algorithm for password hashing. - Use the `secrets` module to generate a secure random number for the token. - Use the `hmac` module for message authentication. - Ensure that the token generated is URL-safe. - Handle exceptions and edge cases appropriately. # Example ```python # Password hashing and verification hashed = hash_password(\'secure_password\') assert verify_password(\'secure_password\', hashed) == True assert verify_password(\'wrong_password\', hashed) == False # Token generation token = generate_token(16) assert isinstance(token, str) and len(token) == 16 # Message authentication key = \'supersecretkey\' message = \'This is a secure message\' hmac_value = authenticate_message(message, key) assert verify_message(message, key, hmac_value) == True assert verify_message(message, \'wrongkey\', hmac_value) == False ```","solution":"import hashlib import secrets import hmac def hash_password(password: str) -> str: Hash the given password using SHA256. hash_object = hashlib.sha256(password.encode()) hashed_password = hash_object.hexdigest() return hashed_password def verify_password(password: str, hashed_password: str) -> bool: Verify the given password against the hashed password. return hash_password(password) == hashed_password def generate_token(length: int) -> str: Generate a secure random token of specified length. token = secrets.token_urlsafe(length) # Ensuring the token length matches the specified length. return token[:length] def authenticate_message(message: str, key: str) -> str: Authenticate the given message using HMAC with the provided key. hmac_digest = hmac.new(key.encode(), message.encode(), hashlib.sha256).hexdigest() return hmac_digest def verify_message(message: str, key: str, hmac_to_verify: str) -> bool: Verify the HMAC of the given message with the provided key. return hmac.compare_digest(authenticate_message(message, key), hmac_to_verify)"},{"question":"**Objective:** To test the understanding of the `glob` module\'s functionality and efficiently utilize its features to handle complex directory searches and pattern matching. **Problem Statement:** You are given a root directory path containing several subdirectories and files. You need to write a function that will search through the directory tree from the given root path and return a list of files that match specific patterns. **Function Signature:** ```python def find_matching_files(root_path: str, pattern: str, recursive: bool = False) -> list: Searches for files that match a given pattern starting from the root_path. Parameters: - root_path (str): The directory path from which the search begins. - pattern (str): The Unix style pattern to use for matching files. - recursive (bool): If True, perform a recursive search; else, search only in the root_path directory. Returns: - list: A list of paths matching the given pattern. ``` **Input:** - `root_path`: A string representing the path of the root directory. - `pattern`: A string containing the pattern to match file names. - `recursive`: A boolean flag indicating whether the search should be recursive. **Output:** - A list of file paths (strings) starting from `root_path` that match the given `pattern`. **Constraints:** 1. The search should respect hidden files (starting with \'.\') rules as per the `glob` module. 2. The result should include broken symlinks if they match the pattern. 3. Performance should be considered, especially when performing recursive searches. **Example:** Assuming the directory structure is as follows: ``` root/ |-- file1.txt |-- .hiddenfile |-- subdir/ |-- file2.txt |-- file3.gif |-- .hiddenfile2 |-- another_subdir/ |-- file4.txt |-- link_to_file1 -> ../file1.txt ``` **Sample Calls:** ```python # Non-recursive search print(find_matching_files(\'root\', \'*.txt\')) # Output: [\'root/file1.txt\'] # Recursive search print(find_matching_files(\'root\', \'*.txt\', recursive=True)) # Output: [\'root/file1.txt\', \'root/subdir/file2.txt\', \'root/another_subdir/file4.txt\'] # Pattern matching hidden files print(find_matching_files(\'root\', \'.*\', recursive=True)) # Output: [\'root/.hiddenfile\', \'root/subdir/.hiddenfile2\'] ``` **Notes:** - Utilize the `glob.glob()` or `glob.iglob()` function from the `glob` module within your solution. - Ensure edge cases like empty directories and no matches are handled gracefully. - Do not use any other third-party libraries. **Evaluation Criteria:** - Correctness: The function should return accurate and expected results for different patterns and recursive settings. - Efficiency: The function should be able to handle large directory trees efficiently. - Code Quality: The code should be clean, well-documented, and follow Python best practices.","solution":"import glob import os def find_matching_files(root_path: str, pattern: str, recursive: bool = False) -> list: Searches for files that match a given pattern starting from the root_path. Parameters: - root_path (str): The directory path from which the search begins. - pattern (str): The Unix style pattern to use for matching files. - recursive (bool): If True, perform a recursive search; else, search only in the root_path directory. Returns: - list: A list of paths matching the given pattern. if recursive: search_path = os.path.join(root_path, \'**\', pattern) return glob.glob(search_path, recursive=True) else: search_path = os.path.join(root_path, pattern) return glob.glob(search_path)"},{"question":"# Kernel Approximation Coding Task **Objective:** Implement an efficient machine learning pipeline using scikit-learn\'s kernel approximation methods to classify a given dataset. **Problem Statement:** You are given a labeled dataset `X` (feature matrix) and `y` (labels). Your task is to: 1. Apply kernel approximation techniques to transform the input data `X`. 2. Train a linear classifier on the transformed data. 3. Evaluate the classifier\'s performance using accuracy. **Steps to Follow:** 1. **Choose a Kernel Approximation Method:** Select one of the following kernel approximation methods: - Nystroem Method - Radial Basis Function (RBF) Sampler - Additive Chi Squared Kernel - Skewed Chi Squared Kernel - Polynomial Kernel Approximation 2. **Transform the Data:** - Use the chosen kernel approximation method to fit and transform the input data `X`. 3. **Train a Classifier:** - Train an `SGDClassifier` (Stochastic Gradient Descent Classifier) on the transformed data. 4. **Evaluate Performance:** - Evaluate the accuracy of the classifier on the transformed data. **Input Format:** - `X`: A 2D list or numpy array of shape (n_samples, n_features) representing the feature matrix. - `y`: A list or numpy array of shape (n_samples,) representing the labels. **Output Format:** - Return the accuracy of the classifier as a float. **Constraints:** - You must use `n_components` parameter when applicable. - You must handle any randomness (e.g., using `random_state`). **Example:** ```python from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation_pipeline(X, y): # Apply RBF Sampler for kernel approximation rbf_feature = RBFSampler(gamma=1, random_state=1) X_features = rbf_feature.fit_transform(X) # Train SGD Classifier clf = SGDClassifier(max_iter=5, random_state=1) clf.fit(X_features, y) # Make predictions and evaluate accuracy predictions = clf.predict(X_features) accuracy = accuracy_score(y, predictions) return accuracy # Example Usage X = [[0, 0], [1, 1], [1, 0], [0, 1]] y = [0, 0, 1, 1] print(kernel_approximation_pipeline(X, y)) # Output: 1.0 ``` **Note:** This is a simplified example using RBF Sampler. You are required to implement a similar pipeline using your chosen kernel approximation method and return the accuracy.","solution":"from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation_pipeline(X, y): Transforms the input data X using the RBF Sampler kernel approximation method, trains an SGD classifier on the transformed data, and returns the accuracy. # Apply RBF Sampler for kernel approximation rbf_feature = RBFSampler(gamma=1, random_state=1) X_features = rbf_feature.fit_transform(X) # Train SGD Classifier clf = SGDClassifier(max_iter=5, random_state=1) clf.fit(X_features, y) # Make predictions and evaluate accuracy predictions = clf.predict(X_features) accuracy = accuracy_score(y, predictions) return accuracy"},{"question":"**Question: Analysis of Memory Usage with `tracemalloc`** You are tasked with evaluating the memory usage pattern of a function that processes data. Your goal is to identify memory leaks or unusually high memory consumption areas within the function. To achieve this, you will use the `tracemalloc` module to capture and compare memory snapshots before and after the function execution. Additionally, you\'ll need to generate a detailed report showing the top lines of code responsible for memory allocation. **Requirements:** 1. **Function Definitions:** - Implement the function `analyze_memory_usage(func, *args, **kwargs)` that performs the following steps: - Starts memory tracing. - Takes a snapshot before the function execution. - Executes the given function `func` with the provided `args` and `kwargs`. - Takes a snapshot after the function execution. - Stops memory tracing. - Analyzes the snapshots to identify the top 10 lines of code responsible for memory allocation. - Returns a formatted string report of the analysis. 2. **Report Format:** - The report should include the filename and line number of each top memory allocation, the size in KiB, and the corresponding source code line. **Function Signature:** ```python def analyze_memory_usage(func, *args, **kwargs) -> str: pass ``` **Example Usage:** ```python # Define a sample function to analyze def sample_function(): large_list = [i for i in range(100000)] # Simulates high memory usage small_list = [i for i in range(1000)] return sum(large_list) + sum(small_list) # Analyze memory usage of the sample function report = analyze_memory_usage(sample_function) print(report) ``` **Expected Output:** ``` Top 10 lines #1: <filename>:<lineno>: <size> KiB <source code line> #2: <filename>:<lineno>: <size> KiB <source code line> ... Total allocated size: <total size> KiB ``` **Constraints:** - Assume `func` is a Python function with no side effects. - The function should handle cases where the module fails to take snapshots or report statistics gracefully. - Use the appropriate `tracemalloc` methods and classes to implement the required functionality. Implement the `analyze_memory_usage` function and provide a detailed analysis of a sample function\'s memory usage based on the above guidelines.","solution":"import tracemalloc def analyze_memory_usage(func, *args, **kwargs) -> str: Analyzes the memory usage of the given function using tracemalloc and returns a formatted report. # Start tracing memory allocations tracemalloc.start() # Take a snapshot before the function execution snapshot_before = tracemalloc.take_snapshot() # Execute the function func(*args, **kwargs) # Take a snapshot after the function execution snapshot_after = tracemalloc.take_snapshot() # Stop tracing memory allocations tracemalloc.stop() # Compare the snapshots stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Format the report report_lines = [\\"Top 10 lines\\"] for i, stat in enumerate(stats[:10], start=1): report_lines.append(f\\"#{i}: {stat.traceback.format()}\\") report_lines.append(f\\" Size: {stat.size / 1024:.1f} KiB\\") total_allocated = sum(stat.size for stat in stats) report_lines.append(f\\"Total allocated size: {total_allocated / 1024:.1f} KiB\\") # Join the report lines into a single string report = \\"n\\".join(report_lines) return report"},{"question":"# Question You have been given a dataset containing monthly sales data of a retail store in two different locations (Location A and Location B). Your task is to create visualizations using seaborn that will help analyze the sales trends effectively. Dataset ```python import pandas as pd data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Location A\\": [1200, 1500, 1600, 1400, 1700, 1800, 1900, 2000, 2100, 2300, 2200, 2400], \\"Location B\\": [1100, 1300, 1500, 1600, 1700, 1750, 1850, 1950, 2050, 2150, 2250, 2350] } df = pd.DataFrame(data) ``` Tasks 1. **Create a bar plot** to visualize the monthly sales for both locations. The x-axis should represent the months, and the y-axis should represent sales amounts. 2. **Apply a \'darkgrid\' theme** to the bar plot. 3. **Create a line plot** to visualize the sales trend over the months for both locations. To make the plot more informative: - Use distinct colors for each location. - Add markers to each data point. 4. **Customize the plot using seaborn parameters** to remove the top and right spines of the plot. Expected Outputs 1. A bar plot comparing monthly sales. 2. A line plot showing sales trends over the months with proper customizations. Your final code should look like this: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Dataset data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Location A\\": [1200, 1500, 1600, 1400, 1700, 1800, 1900, 2000, 2100, 2300, 2200, 2400], \\"Location B\\": [1100, 1300, 1500, 1600, 1700, 1750, 1850, 1950, 2050, 2150, 2250, 2350] } df = pd.DataFrame(data) # Task 1: Bar plot for monthly sales sns.set_theme(style=\\"darkgrid\\") sns.barplot(x=\\"Month\\", y=\\"value\\", hue=\\"variable\\", data=pd.melt(df, [\'Month\'])) plt.title(\'Monthly Sales Comparison\') plt.ylabel(\'Sales ()\') plt.xlabel(\'Month\') plt.show() # Task 2: Line plot with customizations custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"whitegrid\\", rc=custom_params) sns.lineplot(data=df, x=\\"Month\\", y=\\"Location A\\", marker=\\"o\\", color=\\"blue\\", label=\\"Location A\\") sns.lineplot(data=df, x=\\"Month\\", y=\\"Location B\\", marker=\\"o\\", color=\\"red\\", label=\\"Location B\\") plt.title(\'Sales Trend Over Months\') plt.ylabel(\'Sales ()\') plt.xlabel(\'Month\') plt.legend(title=\'Location\') plt.show() ``` Constraints - Make sure the plots are properly labeled. - Ensure that the custom seaborn parameters are applied correctly.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_sales_plots(): # Dataset data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Location A\\": [1200, 1500, 1600, 1400, 1700, 1800, 1900, 2000, 2100, 2300, 2200, 2400], \\"Location B\\": [1100, 1300, 1500, 1600, 1700, 1750, 1850, 1950, 2050, 2150, 2250, 2350] } df = pd.DataFrame(data) # Task 1: Bar plot for monthly sales sns.set_theme(style=\\"darkgrid\\") plt.figure(figsize=(10, 6)) melted_df = pd.melt(df, [\'Month\']) ax1 = sns.barplot(x=\\"Month\\", y=\\"value\\", hue=\\"variable\\", data=melted_df) ax1.set_title(\'Monthly Sales Comparison\') ax1.set_ylabel(\'Sales ()\') ax1.set_xlabel(\'Month\') plt.legend(title=\'Location\') plt.show() # Task 2: Line plot with customizations custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"whitegrid\\", rc=custom_params) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\\"Month\\", y=\\"Location A\\", marker=\\"o\\", color=\\"blue\\", label=\\"Location A\\") sns.lineplot(data=df, x=\\"Month\\", y=\\"Location B\\", marker=\\"o\\", color=\\"red\\", label=\\"Location B\\") plt.title(\'Sales Trend Over Months\') plt.ylabel(\'Sales ()\') plt.xlabel(\'Month\') plt.legend(title=\'Location\') plt.show() # Call the function to generate the plots generate_sales_plots()"},{"question":"Coding Assessment Question **Objective**: To assess the understanding of Python\'s data model, special method names, attribute access customization, and the creation and usage of custom classes and metaclasses. # Problem Statement You are tasked with creating a custom class `SpecialList` that mimics Python\'s built-in list but with additional constraints and custom behaviors. The class should implement the following features: 1. A custom string representation using `__repr__`. 2. Specific access controls for certain indices using `__getitem__`, `__setitem__`, and `__delitem__`. 3. The ability to filter duplicates automatically upon insertion. 4. Implementation of a custom metaclass to alter the class creation process and initialize certain attributes. # Requirements 1. **Custom list behavior**: - Implement a class `SpecialList` that stores its elements in a private list attribute. - Override the `__getitem__`, `__setitem__`, and `__delitem__` methods to control access to indices `0` and `-1`. Accessing, setting, or deleting these indices should raise an `IndexError`. - Only one unique element should be present in the list at any instance (no duplicates). This check should occur whenever an element is added to the list. 2. **String representation**: - Override the `__repr__` method to return a string in the format: `SpecialList(<elements>)`. 3. **Metaclass functionality**: - Implement a custom metaclass `ListMeta` that initializes an attribute `creation_time` in the `SpecialList` class, storing the datetime when the class is created. # Constraints 1. You should ensure that `SpecialList` cannot be instantiated with a type other than `int`. 2. The list should maintain insertion order. # Function Signature ```python class ListMeta(type): def __new__(cls, name, bases, dct): dct[\'creation_time\'] = datetime.datetime.now() return super().__new__(cls, name, bases, dct) class SpecialList(metaclass=ListMeta): def __init__(self, *args: int): self._list = [] for item in args: self.__append_unique(item) def __append_unique(self, value: int): if value not in self._list: self._list.append(value) def __getitem__(self, index: int): if index == 0 or index == -1: raise IndexError(\\"Access to index 0 or -1 is not allowed.\\") return self._list[index] def __setitem__(self, index: int, value: int): if index == 0 or index == -1: raise IndexError(\\"Setting value at index 0 or -1 is not allowed.\\") if value not in self._list: self._list[index] = value def __delitem__(self, index: int): if index == 0 or index == -1: raise IndexError(\\"Deleting value at index 0 or -1 is not allowed.\\") del self._list[index] def __repr__(self): return f\\"SpecialList({self._list})\\" @property def creation_time(self): return self.__class__.creation_time # Example Usage: # Validations # sl = SpecialList(1, 2, 2, 3, 4) # Should create list without duplicates: [1, 2, 3, 4] # print(sl) # Should output: SpecialList([1, 2, 3, 4]) # print(sl.creation_time) # Should output the datetime when the class was created. # sl.append(5) # Should add the element if it is unique and not 0 or -1 # sl[1] # Should access the element at index 1 # sl[0] # Should raise IndexError ``` # Notes - You can use any standard libraries if necessary. - Ensure that the `SpecialList` class passes all validation checks and correctly implements access control for the specific indices and uniqueness constraints.","solution":"import datetime class ListMeta(type): def __new__(cls, name, bases, dct): dct[\'creation_time\'] = datetime.datetime.now() return super().__new__(cls, name, bases, dct) class SpecialList(metaclass=ListMeta): def __init__(self, *args: int): self._list = [] for item in args: if not isinstance(item, int): raise TypeError(\\"SpecialList can only contain integers.\\") self.__append_unique(item) def __append_unique(self, value: int): if value not in self._list: self._list.append(value) def __getitem__(self, index: int): if index == 0 or index == -1: raise IndexError(\\"Access to index 0 or -1 is not allowed.\\") return self._list[index] def __setitem__(self, index: int, value: int): if index == 0 or index == -1: raise IndexError(\\"Setting value at index 0 or -1 is not allowed.\\") if not isinstance(value, int): raise TypeError(\\"SpecialList can only contain integers.\\") if value not in self._list: self._list[index] = value def __delitem__(self, index: int): if index == 0 or index == -1: raise IndexError(\\"Deleting value at index 0 or -1 is not allowed.\\") del self._list[index] def __repr__(self): return f\\"SpecialList({self._list})\\" @property def creation_time(self): return self.__class__.creation_time"},{"question":"# Advanced Python Typing and Generic Programming **Objective:** Demonstrate your understanding of Python\'s typing module by implementing a generic data structure with type checking and a protocol to enforce method requirements. **Task:** 1. **Define a Generic Stack Class:** * Implement a generic `Stack` class that can hold elements of any type. * The class should include methods to push an element onto the stack, pop an element from the stack, and check if the stack is empty. 2. **Define a Protocol for Serializing:** * Create a protocol `Serializable` that enforces a method `serialize` which returns a string representation of the object. 3. **Extend the Stack Class with Serialization:** * Extend the `Stack` class to create a `SerializableStack` that can only store elements which conform to the `Serializable` protocol. * Ensure type safety such that only objects that implement the `Serializable` protocol can be added to the stack. **Constraints:** * Focus on proper usage of the `typing` module features (e.g., `TypeVar`, `Generic`, `Protocol`). * Follow Python naming conventions and ensure code readability. **Input and Output Formats:** 1. **Generic Stack Class:** ```python from typing import TypeVar, Generic, List T = TypeVar(\'T\') class Stack(Generic[T]): def __init__(self): self._stack: List[T] = [] def push(self, item: T) -> None: self._stack.append(item) def pop(self) -> T: if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self) -> bool: return len(self._stack) == 0 ``` 2. **Serializable Protocol:** ```python from typing import Protocol class Serializable(Protocol): def serialize(self) -> str: ... ``` 3. **SerializableStack Class:** ```python from typing import TypeVar, Generic, List T = TypeVar(\'T\', bound=Serializable) class SerializableStack(Generic[T]): def __init__(self): self._stack: List[T] = [] def push(self, item: T) -> None: self._stack.append(item) def pop(self) -> T: if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self) -> bool: return len(self._stack) == 0 def serialize_all(self) -> str: return \\", \\".join(item.serialize() for item in self._stack) ``` **Example Usage:** ```python class MySerializableClass: def __init__(self, value: int): self.value = value def serialize(self) -> str: return f\\"MySerializableClass({self.value})\\" # Create a stack for elements that implement Serializable stack = SerializableStack[MySerializableClass]() obj1 = MySerializableClass(10) obj2 = MySerializableClass(20) stack.push(obj1) stack.push(obj2) print(stack.serialize_all()) # Output: MySerializableClass(10), MySerializableClass(20) stack.pop() print(stack.serialize_all()) # Output: MySerializableClass(10) ``` **Evaluate:** Your implementation will be evaluated based on: * Correct implementation of the generic stack and protocol. * Proper use of type hints and ensuring type safety. * Correct handling of error cases, such as popping from an empty stack. * Code readability and adherence to Python conventions.","solution":"from typing import TypeVar, Generic, List, Protocol T = TypeVar(\'T\') class Stack(Generic[T]): def __init__(self): self._stack: List[T] = [] def push(self, item: T) -> None: self._stack.append(item) def pop(self) -> T: if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self) -> bool: return len(self._stack) == 0 class Serializable(Protocol): def serialize(self) -> str: ... TSerializable = TypeVar(\'TSerializable\', bound=Serializable) class SerializableStack(Generic[TSerializable]): def __init__(self): self._stack: List[TSerializable] = [] def push(self, item: TSerializable) -> None: self._stack.append(item) def pop(self) -> TSerializable: if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self) -> bool: return len(self._stack) == 0 def serialize_all(self) -> str: return \\", \\".join(item.serialize() for item in self._stack) # Example usage: class MySerializableClass: def __init__(self, value: int): self.value = value def serialize(self) -> str: return f\\"MySerializableClass({self.value})\\" # Create a stack for elements that implement Serializable stack = SerializableStack[MySerializableClass]() obj1 = MySerializableClass(10) obj2 = MySerializableClass(20) stack.push(obj1) stack.push(obj2) # Serialize all elements print(stack.serialize_all()) # Output: MySerializableClass(10), MySerializableClass(20) stack.pop() print(stack.serialize_all()) # Output: MySerializableClass(10)"},{"question":"**Objective**: Demonstrate your understanding of scikit-learn\'s `SGDClassifier`, feature scaling, pipelines, and model evaluation. # Problem Statement You are provided with a dataset containing features and class labels. Your task is to: 1. Load and preprocess the data. 2. Implement a machine learning pipeline using scikit-learn\'s `SGDClassifier`. 3. Train the model and evaluate its performance on a test set. # Dataset Assume you have a CSV file named `data.csv` with the following structure: - The first column contains class labels (0 or 1). - The remaining columns contain feature values. # Instructions 1. **Read the dataset**: - Load the dataset from `data.csv`. 2. **Preprocess the data**: - Split the data into training and testing sets (80% training, 20% testing). - Standardize the feature values to ensure they have a mean of 0 and a variance of 1. 3. **Pipeline implementation**: - Create a pipeline that includes feature scaling and an `SGDClassifier`. - Use the following parameters for `SGDClassifier`: `loss=\\"log_loss\\"`, `penalty=\\"elasticnet\\"`, `max_iter=1000`, `tol=1e-3`. 4. **Model training and evaluation**: - Train the pipeline on the training data. - Evaluate the model\'s performance using accuracy score on the test set. # Requirements - Implement your solution in Python. - Use scikit-learn functions and classes for pipeline, classification, scaling, and evaluation. - Ensure your code is clean and well-documented. # Example ```python import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score # 1. Read the dataset data = pd.read_csv(\'data.csv\') X = data.iloc[:, 1:].values y = data.iloc[:, 0].values # 2. Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Create a pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", max_iter=1000, tol=1e-3)) # 4. Train the model pipeline.fit(X_train, y_train) # 5. Evaluate the model y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") ``` # Constraints - You must use scikit-learn\'s `make_pipeline` function for creating the pipeline. - Ensure the model training has a convergence tolerance (`tol`) of `1e-3`. # Submission Submit: - Your Python code in a single script or Jupyter Notebook. - A text file containing the accuracy of the trained model on the test set.","solution":"import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def load_and_preprocess_data(file_path): Loads the dataset and preprocesses it by splitting into train and test sets and scaling the feature values. # Read the dataset data = pd.read_csv(file_path) X = data.iloc[:, 1:].values y = data.iloc[:, 0].values # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def create_pipeline(): Creates a machine learning pipeline with feature scaling and SGDClassifier. pipeline = make_pipeline( StandardScaler(), SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", max_iter=1000, tol=1e-3) ) return pipeline def train_and_evaluate_model(file_path): Loads the data, creates the pipeline, trains the model, and evaluates it. # Load and preprocess the data X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) # Create the pipeline pipeline = create_pipeline() # Train the model pipeline.fit(X_train, y_train) # Predict on the test set y_pred = pipeline.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage: # accuracy = train_and_evaluate_model(\'data.csv\') # print(f\\"Accuracy: {accuracy:.2f}\\")"},{"question":"# Multi-threading and Synchronization in Python Objective: You are required to implement a Python function that uses low-level threading features provided by the `_thread` module. The task focuses on creating multiple threads to perform operations concurrently and ensuring thread synchronization using locks. Problem Statement: Write a Python function `calculate_sum_concurrently(n: int, num_threads: int) -> int` that calculates the sum of the first `n` natural numbers using `num_threads` threads. Each thread should be responsible for adding a range of numbers to a shared total sum. Ensure that the access to the shared sum is synchronized to prevent data races. Function Signature: ```python def calculate_sum_concurrently(n: int, num_threads: int) -> int: pass ``` Input: - `n` (int): The upper limit of natural numbers to sum up. (1 <= n <= 10^6) - `num_threads` (int): The number of threads to use. (1 <= num_threads <= 20) Output: - Returns the total sum of natural numbers from 1 to `n`. Constraints: - You must use the `_thread` module for thread management. - Proper synchronization is required using lock objects from `_thread`. - All threads should contribute to the sum without overlapping ranges. Example: ```python # Example 1 result = calculate_sum_concurrently(10, 2) print(result) # Output: 55 # Example 2 result = calculate_sum_concurrently(100, 4) print(result) # Output: 5050 ``` Implementation Details: 1. Divide the range of numbers (1 to `n`) as equally as possible among the `num_threads` threads. 2. Use a lock to synchronize access to the shared total sum variable. 3. Start the threads and ensure they complete their task before returning the final sum. Notes: - Make sure that the function is thread-safe and handles synchronization correctly. - Consider edge cases such as `n = 1`, `num_threads = 1`, or situations where `n` is not perfectly divisible by `num_threads`. Hints: - You can use `_thread.allocate_lock()` to create a lock and `lock.acquire()`/`lock.release()` to synchronize. - Use `_thread.start_new_thread()` to start a new thread with a target function that computes a part of the sum. - Utilize `_thread.get_ident()` to verify that threads are working correctly if needed. Good luck and happy coding!","solution":"import _thread import threading def calculate_sum_concurrently(n: int, num_threads: int) -> int: total_sum = 0 lock = _thread.allocate_lock() def worker(start: int, end: int): nonlocal total_sum local_sum = sum(range(start, end + 1)) lock.acquire() total_sum += local_sum lock.release() threads = [] step = (n + num_threads - 1) // num_threads for i in range(num_threads): start = i * step + 1 end = min((i + 1) * step, n) if start <= end: thread = threading.Thread(target=worker, args=(start, end)) threads.append(thread) thread.start() for thread in threads: thread.join() return total_sum"},{"question":"# Bytearray Manipulation in Python310 As a developer, your task is to create a Python class that interacts with bytearray objects using some functionality described in the Python C API (reflected now in a high-level Python API for bytearrays). Requirements: 1. Implement the class `BytearrayManipulator` that has the following methods: - `from_object`: Takes any object that implements the buffer protocol and returns a new bytearray. - `from_string_and_size`: Takes a string and its length, creating a new bytearray. - `concat`: Takes two bytearrays and returns a new bytearray that is the concatenation of both. - `get_size`: Accepts a bytearray and returns its size. - `as_string`: Accepts a bytearray and returns its content as a string, ensuring a null-terminator at the end. - `resize`: Resizes a given bytearray to the specified length. Each method should be implemented efficiently and correctly representing the behavior specified in the documentation. Example Usage: ```python class BytearrayManipulator: @staticmethod def from_object(obj): # Implement this method pass @staticmethod def from_string_and_size(string, length): # Implement this method pass @staticmethod def concat(ba1, ba2): # Implement this method pass @staticmethod def get_size(ba): # Implement this method pass @staticmethod def as_string(ba): # Implement this method pass @staticmethod def resize(ba, size): # Implement this method pass # Example usage: buffer_object = b\\"hello, world\\" ba1 = BytearrayManipulator.from_object(buffer_object) print(ba1) # Output: bytearray(b\'hello, world\') string = \\"example\\" ba2 = BytearrayManipulator.from_string_and_size(string, len(string)) print(ba2) # Output: bytearray(b\'example\') ba3 = BytearrayManipulator.concat(ba1, ba2) print(ba3) # Output: bytearray(b\'hello, worldexample\') size = BytearrayManipulator.get_size(ba3) print(size) # Output: 21 string_representation = BytearrayManipulator.as_string(ba3) print(string_representation) # Output: \'hello, worldexamplex00\' BytearrayManipulator.resize(ba3, 10) print(ba3) # Output: bytearray(b\'hello, wor\') ``` Constraints: 1. Focus on achieving functionality akin to the C API described but using pure Python equivalents. 2. Handle edge cases such as invalid inputs or attempts to resize the bytearray to negative lengths appropriately.","solution":"class BytearrayManipulator: @staticmethod def from_object(obj): Takes any object that implements the buffer protocol and returns a new bytearray. return bytearray(obj) @staticmethod def from_string_and_size(string, length): Takes a string and its length, creating a new bytearray. return bytearray(string[:length], \'utf-8\') @staticmethod def concat(ba1, ba2): Takes two bytearrays and returns a new bytearray that is the concatenation of both. return ba1 + ba2 @staticmethod def get_size(ba): Accepts a bytearray and returns its size. return len(ba) @staticmethod def as_string(ba): Accepts a bytearray and returns its content as a string, ensuring a null-terminator at the end. return ba.decode(\'utf-8\') + \'x00\' @staticmethod def resize(ba, size): Resizes a given bytearray to the specified length. if size < 0: raise ValueError(\\"Size must be non-negative\\") current_size = len(ba) if size < current_size: del ba[size:] elif size > current_size: ba.extend([0] * (size - current_size))"},{"question":"Problem Statement: You are required to design a utility function that handles errors using the `errno` module. This function should take an integer error code and return a detailed error description. Specifically, the function should do the following: 1. If the error code is present in `errno.errorcode`, return a string in the format: ``` \\"Error Code: [code], Error Name: [error_name], Error Message: [error_message]\\" ``` 2. If the error code is not present in `errno.errorcode`, return a string: ``` \\"Unknown error code: [code]\\" ``` 3. The function should translate the error code to a human-readable error message using `os.strerror()`. 4. Your solution should handle any potential exceptions that may arise from the use of `os.strerror()`. Function Signature: ```python import os import errno def describe_error_code(code: int) -> str: pass ``` Input: - An integer `code` (representing error code). Output: - A string describing the error in the specified format. Constraints: - The input integer can be any valid integer. - The function should handle all possible error codes and unknown error codes gracefully. Example: ```python # Example 1 print(describe_error_code(errno.EPERM)) # Output: \\"Error Code: 1, Error Name: EPERM, Error Message: Operation not permitted\\" # Example 2 print(describe_error_code(12345)) # Output: \\"Unknown error code: 12345\\" ``` Additional Requirements: - You must use the `errno` and `os` modules in Python. - Ensure your function is efficient and handle edge cases (like invalid error codes that might cause exceptions).","solution":"import os import errno def describe_error_code(code: int) -> str: try: if code in errno.errorcode: error_name = errno.errorcode[code] error_message = os.strerror(code) return f\\"Error Code: {code}, Error Name: {error_name}, Error Message: {error_message}\\" else: return f\\"Unknown error code: {code}\\" except ValueError: return f\\"Unknown error code: {code}\\""},{"question":"You are required to implement a small concurrent application that fetches data from multiple URLs and processes it asynchronously. The goal is to efficiently handle multiple HTTP requests and aggregate the results using asyncio. Requirements: 1. Implement an asynchronous function `fetch_url(session, url)` that takes an `aiohttp.ClientSession` object and a URL string, performs an HTTP GET request to the URL, and returns the response text. 2. Implement an asynchronous function `process_data(data)` that simulates processing by sleeping asynchronously for a random time (between 1 and 3 seconds) and then returns the length of the data. 3. Create an asynchronous function `fetch_and_process(urls)` that: - Takes a list of URLs. - Creates an aiohttp ClientSession. - Fetches data concurrently from all the URLs using `fetch_url`. - Processes each response using `process_data`. - Returns a dictionary where the keys are the URLs and the values are the lengths of the data after processing. Input: - A list of URL strings. Output: - A dictionary where the keys are the URLs and the values are integers representing the length of the processed data. Constraints: - Use the `aiohttp` library for HTTP requests. You can install it via `pip install aiohttp`. - You should handle any potential exceptions during HTTP requests gracefully, ensuring that the program continues running even if some requests fail. Example: ```python import asyncio import aiohttp import random async def fetch_url(session, url): # Your implementation here async def process_data(data): # Your implementation here async def fetch_and_process(urls): # Your implementation here # Example usage urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] result = asyncio.run(fetch_and_process(urls)) print(result) # Example output: {\'http://example.com\': 5000, \'http://example.org\': 4500, \'http://example.net\': 5300} ``` Note: - Do not hard-code the URLs; the function should work with any list of URLs provided. - Ensure the final implementation is efficient and makes full use of asyncio\'s concurrent capabilities.","solution":"import asyncio import aiohttp import random async def fetch_url(session, url): Asynchronously fetches the content of a URL. try: async with session.get(url) as response: return await response.text() except Exception as e: print(f\\"Failed to fetch {url}: {e}\\") return None async def process_data(data): Simulates data processing by waiting for 1 to 3 seconds. await asyncio.sleep(random.randint(1, 3)) return len(data) async def fetch_and_process(urls): Fetches and processes data from a list of URLs concurrently. async with aiohttp.ClientSession() as session: fetch_tasks = [fetch_url(session, url) for url in urls] fetched_data = await asyncio.gather(*fetch_tasks) process_tasks = [process_data(data) for data in fetched_data if data is not None] processed_data_lengths = await asyncio.gather(*process_tasks) return {url: length for url, length in zip(urls, processed_data_lengths) if length is not None} # Example usage (for testing purpose) # urls = [ # \\"http://example.com\\", # \\"http://example.org\\", # \\"http://example.net\\" # ] # result = asyncio.run(fetch_and_process(urls)) # print(result)"},{"question":"**Title:** Data Validation and Preprocessing using scikit-learn Utilities **Objective:** To assess your understanding of data validation and preprocessing using scikit-learn utility functions. **Problem Statement:** You are given a dataset represented as a 2D NumPy array `X` and a 1D NumPy array `y`. Your task is to implement a function `validate_and_preprocess_data(X, y, random_state=None)` that performs the following steps: 1. Validate that `X` is a 2D array and `y` is a 1D array. 2. Ensure that `X` and `y` have the same length. 3. Check that `X` does not contain any NaNs or Infs. 4. Convert `X` to a float array. 5. Shuffle `X` and `y` consistently according to the given `random_state`. 6. Return the preprocessed `X` and `y`. **Function Signature:** ```python import numpy as np from sklearn.utils import check_X_y, assert_all_finite, as_float_array, shuffle, check_random_state def validate_and_preprocess_data(X: np.ndarray, y: np.ndarray, random_state=None) -> (np.ndarray, np.ndarray): # Your implementation here pass ``` **Input:** - `X` (2D np.ndarray): The feature matrix. - `y` (1D np.ndarray): The target array. - `random_state` (int, RandomState instance or None, optional): Determines the shuffling of data. **Output:** - Tuple containing: - Preprocessed `X` (2D np.ndarray). - Preprocessed `y` (1D np.ndarray). **Constraints:** 1. `X` and `y` should have consistent lengths. 2. `X` should be free from NaNs or Infs. **Example:** ```python X = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) random_state = 42 X_processed, y_processed = validate_and_preprocess_data(X, y, random_state=random_state) # Expected output: # X_processed and y_processed would be shuffled consistently with the same random_state ``` **Additional Notes:** - You should utilize scikit-learn utility functions such as `check_X_y`, `assert_all_finite`, `as_float_array`, `shuffle`, and `check_random_state`.","solution":"import numpy as np from sklearn.utils import check_X_y, assert_all_finite, as_float_array, shuffle, check_random_state def validate_and_preprocess_data(X: np.ndarray, y: np.ndarray, random_state=None) -> (np.ndarray, np.ndarray): # Validate X and y X, y = check_X_y(X, y, ensure_2d=True, allow_nd=False, dtype=None) # Ensure that there are no NaNs or Infs in X assert_all_finite(X) # Convert X to float array X = as_float_array(X) # Random state check random_state = check_random_state(random_state) # Shuffle X and y consistently X, y = shuffle(X, y, random_state=random_state) return X, y"},{"question":"# Coding Exercise: Recursive Directory Search with File Extension Filtering Objective: Write a Python function that recursively searches through a given directory for files with a specified extension and returns a list of the absolute paths to these files. Requirements: 1. Use recursive function calls. 2. Implement appropriate control flow mechanisms to handle different cases (e.g., directories, file extensions). 3. Manage default and optional arguments to allow the user to specify the file extension and the starting directory. Function Signature: ```python def find_files(start_dir: str, extension: str = \'txt\') -> list: Recursively searches for files with a specified extension starting from a given directory. Args: start_dir (str): The directory from which the search begins. extension (str): The file extension to search for (default is \'txt\'). Returns: list: A list of absolute paths to the files that match the specified extension. ``` Input: - `start_dir` (str): The starting directory for the search. The input string must be a valid directory path. - `extension` (str): The file extension to search for. Defaults to \'txt\'. Output: - list: A list of absolute paths (strings) to the files that match the specified extension. Constraints: - Use Python\'s `os` module to interact with the filesystem. Example: ```python # Example directory structure: # /test # ├── file1.txt # ├── file2.csv # ├── subdir1 # │ ├── file3.txt # │ └── file4.csv # └── subdir2 # ├── file5.txt # └── file6.pdf result = find_files(\'/test\') # Output: [\'/test/file1.txt\', \'/test/subdir1/file3.txt\', \'/test/subdir2/file5.txt\'] ``` Note: - Ensure that your code handles edge cases, such as an empty directory or non-existing paths. - The function should be robust and able to manage large directory structures efficiently. Additional Challenge: - Optimize the function to handle directories with a large number of files and subdirectories efficiently.","solution":"import os def find_files(start_dir: str, extension: str = \'txt\') -> list: Recursively searches for files with a specified extension starting from a given directory. Args: start_dir (str): The directory from which the search begins. extension (str): The file extension to search for (default is \'txt\'). Returns: list: A list of absolute paths to the files that match the specified extension. found_files = [] for root, dirs, files in os.walk(start_dir): for file in files: if file.endswith(f\'.{extension}\'): found_files.append(os.path.join(root, file)) return found_files"},{"question":"Problem Statement You are implementing a file system utility to analyze and manipulate paths. The goal is to create functions that combine multiple functionalities provided by the `os.path` module. These utility functions will help in normalizing paths, verifying path properties, and comparing paths. Task Implement the following functions: 1. **normalize_and_abs_path(path: str) -> str**: - Normalize and return the absolute version of the given path. - Use `os.path.normpath()` and `os.path.abspath()`. 2. **is_valid_file(path: str) -> bool**: - Check if the given path is a valid existing file. - Use `os.path.exists()` and `os.path.isfile()`. 3. **get_common_subdirectory(paths: list) -> str**: - Return the longest common sub-path of each pathname in the provided list of paths. - Raise a `ValueError` if the paths contain both absolute and relative pathnames or if the list is empty. - Use `os.path.commonpath()`. 4. **compare_last_modification_time(path1: str, path2: str) -> str**: - Compare the last modification time of two files. - Return `\'path1\'`, `\'path2\'`, or `\'same\'` based on which file has a later modification time. - Use `os.path.getmtime()`. Input and Output Format 1. **normalize_and_abs_path(path: str) -> str** - **Input**: A string representing a path. - **Output**: A string representing the normalized absolute path. 2. **is_valid_file(path: str) -> bool** - **Input**: A string representing a path. - **Output**: A boolean indicating if the path is a valid existing file. 3. **get_common_subdirectory(paths: list) -> str** - **Input**: A list of strings representing paths. - **Output**: A string representing the longest common sub-path. - Raise `ValueError` with a message if: - The list is empty. - Contains both absolute and relative pathnames. 4. **compare_last_modification_time(path1: str, path2: str) -> str** - **Input**: Two strings representing paths. - **Output**: A string `\'path1\'` if the first path has a later modification time, `\'path2\'` if the second path has a later modification time, or `\'same\'` if both have the same modification time. Constraints - You can assume that the paths provided are valid strings. - Ensure the functions handle cases of missing files, inaccessible files, and invalid/unrepresentable paths gracefully. Example ```python # Example usage: print(normalize_and_abs_path(\\"./test/../file.txt\\")) # Should output the absolute path equivalent. print(is_valid_file(\\"/some/path/to/file.txt\\")) # Should return True if file exists and is a regular file. print(get_common_subdirectory([\\"/usr/lib\\", \\"/usr/local/lib\\"])) # Should return \'/usr\' print(compare_last_modification_time(\\"/path/to/file1.txt\\", \\"/path/to/file2.txt\\")) # Should return the file with the later modification time or \'same\'. ```","solution":"import os def normalize_and_abs_path(path: str) -> str: Normalize and return the absolute version of the given path. return os.path.abspath(os.path.normpath(path)) def is_valid_file(path: str) -> bool: Check if the given path is a valid existing file. return os.path.exists(path) and os.path.isfile(path) def get_common_subdirectory(paths: list) -> str: Return the longest common sub-path of each pathname in the provided list of paths. Raise a ValueError if the paths contain both absolute and relative pathnames or if the list is empty. if not paths: raise ValueError(\\"The list of paths is empty.\\") abs_paths = [os.path.isabs(path) for path in paths] if any(abs_paths) and not all(abs_paths): raise ValueError(\\"Paths contain both absolute and relative pathnames.\\") try: return os.path.commonpath(paths) except ValueError: raise ValueError(\\"Cannot determine common subdirectory for the given paths.\\") def compare_last_modification_time(path1: str, path2: str) -> str: Compare the last modification time of two files. Return \'path1\', \'path2\', or \'same\' based on which file has a later modification time. if not os.path.exists(path1) or not os.path.isfile(path1): raise ValueError(f\\"The path {path1} is not a valid file.\\") if not os.path.exists(path2) or not os.path.isfile(path2): raise ValueError(f\\"The path {path2} is not a valid file.\\") mtime1 = os.path.getmtime(path1) mtime2 = os.path.getmtime(path2) if mtime1 > mtime2: return \'path1\' elif mtime2 > mtime1: return \'path2\' else: return \'same\'"},{"question":"You are responsible for managing a leaderboard system for a game. Players\' scores are recorded in real-time, and you must maintain the leaderboard in sorted order without re-sorting the entire list after each insertion. Instead, the leaderboard should always remain sorted after each player\'s score is recorded. Implement a `Leaderboard` class that supports the following functionalities using the `bisect` module functions: 1. `add_score(player_id: int, score: int) -> None`: Adds or updates the player’s score. If the player is already on the leaderboard, their score should be updated to the new score. 2. `top(n: int) -> List[Tuple[int, int]]`: Returns a list of the top `n` players and their scores in descending order of scores. If there are fewer than `n` players on the leaderboard, return all of them. Your implementation should make effective use of `bisect` functions to ensure efficient insertion and maintenance of sorted order. Example Usage: ```python lb = Leaderboard() lb.add_score(1, 100) lb.add_score(2, 200) lb.add_score(3, 150) # After adding scores: # Leaderboard: [(2, 200), (3, 150), (1, 100)] print(lb.top(2)) # Output: [(2, 200), (3, 150)] lb.add_score(1, 250) # After updating score: # Leaderboard: [(1, 250), (2, 200), (3, 150)] print(lb.top(2)) # Output: [(1, 250), (2, 200)] ``` Constraints: 1. Player IDs are guaranteed to be unique integers. 2. Scores are non-negative integers. 3. `top` function calls will always request a positive number of players. 4. The total number of players is not extremely large (you can assume it\'s in the range of a few hundred players). Performance Requirements: Your implementation should be efficient in terms of both time and space complexity, utilizing the bisect functions to maintain order without repeatedly sorting the leaderboard.","solution":"import bisect class Leaderboard: def __init__(self): self.scores = [] self.player_map = {} def add_score(self, player_id: int, score: int) -> None: if player_id in self.player_map: # Find the old position of the player old_score_entry = (self.player_map[player_id], player_id) pos = bisect.bisect_left(self.scores, old_score_entry) # Remove the old entry del self.scores[pos] # Update score in the player_map self.player_map[player_id] = score new_score_entry = (score, player_id) # Insert the new entry maintaining sorted order bisect.insort_left(self.scores, new_score_entry) def top(self, n: int) -> list: # Retrieve the top n scores in descending order return sorted(self.scores, key=lambda x: (-x[0], x[1]))[:n]"},{"question":"# Question: FileEncoder Utility Using UUencode You are required to implement a utility called `FileEncoder` that provides functionalities to encode and decode files using the UUencode format. This utility should interface with the deprecated `uu` module functions provided in the Python standard library. Requirements: 1. Implement the class `FileEncoder` with the following methods: - `encode_file(self, input_path: str, output_path: str, backtick: bool = False) -> None` - `decode_file(self, input_path: str, output_path: str, quiet: bool = False) -> None` 2. The `encode_file` method should: - Read a file from `input_path`. - Encode it using UUencode. - Write the encoded content to `output_path`. - Use the `backtick` parameter to determine if zeros should be represented by a backtick or not. 3. The `decode_file` method should: - Read a uuencoded file from `input_path`. - Decode it. - Write the decoded content to `output_path`. - Ensure any warnings are either printed to STDERR or silenced according to the `quiet` parameter. - Handle `uu.Error` exception in case the input file is malformed or cannot be decoded. 4. Both methods should raise a custom `FileEncoderError` exception for any issues related to file I/O (e.g., file not found, permission issues). 5. Include a main section for the script to allow command-line execution: - Example usage: - ` python file_encoder.py encode input.txt output.uue` - ` python file_encoder.py decode input.uue output.txt` - Here, `encode` and `decode` will be commands that correspond to the appropriate methods in the `FileEncoder` class. Constraints: - You must use the `uu` module for encoding and decoding. - You must handle any file-related exceptions gracefully. - Provide helpful error messages for issues encountered during file operations or encoding/decoding. - Adhere to Python file handling best practices, ensuring files are properly closed after operations. Example: Here\'s how you might use the `FileEncoder` class: ```python encoder = FileEncoder() encoder.encode_file(\'input.txt\', \'output.uue\', backtick=True) encoder.decode_file(\'output.uue\', \'decoded.txt\', quiet=True) ``` This question tests your ability to work with file operations, use an external module for encoding/decoding, handle exceptions, and create a user-friendly command-line interface.","solution":"import uu import os class FileEncoderError(Exception): Custom exception for file encoder errors. pass class FileEncoder: def encode_file(self, input_path: str, output_path: str, backtick: bool = False) -> None: Encode a file using UUencode format. Args: input_path (str): Path to the input file. output_path (str): Path to the output file where encoded content will be written. backtick (bool): If True, use backtick for zero bytes, otherwise use space. try: with open(input_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=os.path.basename(input_path), mode=os.stat(input_path).st_mode, backtick=backtick) except FileNotFoundError: raise FileEncoderError(f\\"File not found: {input_path}\\") except PermissionError: raise FileEncoderError(f\\"Permission denied: {input_path}\\") except Exception as ex: raise FileEncoderError(f\\"An error occurred while encoding the file: {ex}\\") def decode_file(self, input_path: str, output_path: str, quiet: bool = False) -> None: Decode a UUencoded file. Args: input_path (str): Path to the input uuencoded file. output_path (str): Path to the output file where decoded content will be written. quiet (bool): If True, suppresses output to STDERR. try: with open(input_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.decode(in_file, out_file, quiet=quiet) except FileNotFoundError: raise FileEncoderError(f\\"File not found: {input_path}\\") except PermissionError: raise FileEncoderError(f\\"Permission denied: {input_path}\\") except uu.Error as ex: raise FileEncoderError(f\\"Error during decoding. The file might be malformed: {ex}\\") except Exception as ex: raise FileEncoderError(f\\"An error occurred while decoding the file: {ex}\\")"},{"question":"**Objective:** You are required to parse and process an XML document securely using Python’s `xml.etree.ElementTree` module. The document contains a list of books with details such as title, author, and year. **Problem Statement:** Write a Python function `parse_books_xml(xml_str: str) -> List[Dict[str, Any]]` that takes an XML string `xml_str` containing the book details and returns a list of dictionaries where each dictionary represents a book with its details. **Details:** 1. The XML string will follow this structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> </book> ... </library> ``` 2. Each dictionary in the list should have keys `title`, `author`, and `year`. The value for `year` should be an integer. 3. If any field is missing in a `<book>` element (like `<author>` or `<year>`), the corresponding key should have the value `None`. **Constraints:** - The XML string can have up to 10,000 book entries. - Avoid potential vulnerabilities by ensuring the parsing process does not expand external entities. **Function Signature:** ```python from typing import List, Dict, Any def parse_books_xml(xml_str: str) -> List[Dict[str, Any]]: pass ``` **Example:** ```python xml_data = <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> </book> </library> print(parse_books_xml(xml_data)) # Output should be: # [{\'title\': \'Book Title 1\', \'author\': \'Author 1\', \'year\': 2001}, # {\'title\': \'Book Title 2\', \'author\': \'Author 2\', \'year\': 2005}] ``` **Notes:** - You should use `xml.etree.ElementTree` for XML parsing. - Ensure that your implementation does not expand external entities by setting the appropriate parser properties or using Python\'s safe defaults.","solution":"from typing import List, Dict, Any import xml.etree.ElementTree as ET def parse_books_xml(xml_str: str) -> List[Dict[str, Any]]: Parse the given XML string containing book details and return a list of dictionaries. Each dictionary represents a book with details such as title, author, and year. books = [] # Parse XML string root = ET.fromstring(xml_str) # Iterate through each \'book\' element for book_elem in root.findall(\'book\'): book = {} # Retrieve title title_elem = book_elem.find(\'title\') book[\'title\'] = title_elem.text if title_elem is not None else None # Retrieve author author_elem = book_elem.find(\'author\') book[\'author\'] = author_elem.text if author_elem is not None else None # Retrieve year year_elem = book_elem.find(\'year\') if year_elem is not None: try: book[\'year\'] = int(year_elem.text) except ValueError: book[\'year\'] = None # Handle case where year is not a valid integer else: book[\'year\'] = None books.append(book) return books"},{"question":"Objective You are tasked with creating a utility that processes email messages. You will write a class `EmailUtility` with specific methods to manipulate email headers and payloads using the `email.message.Message` class. Requirements 1. **Class Definition**: Define a class `EmailUtility` that will encapsulate the `email.message.Message` object. 2. **Initialization**: The constructor should initialize the `Message` object. ```python class EmailUtility: def __init__(self): pass ``` 3. **Method 1: add_header**: Implement a method `add_header` that adds a header to the email message. ```python def add_header(self, name: str, value: str, params: dict = {}): Adds a header to the email message. Args: name (str): The name of the header. value (str): The value of the header. params (dict): Additional parameters for the header. Example: email_util.add_header(\'Content-Disposition\', \'attachment\', {\'filename\': \'file.txt\'}) pass ``` 4. **Method 2: get_header**: Implement a method `get_header` to get the value of a specified header field. ```python def get_header(self, name: str) -> str: Returns the value of the specified header field. Args: name (str): The name of the header. Returns: str: The value of the header. Example: value = email_util.get_header(\'Subject\') pass ``` 5. **Method 3: remove_header**: Implement a method `remove_header` that removes all occurrences of a header with a specified name from the email message. ```python def remove_header(self, name: str): Removes all occurrences of the header with the specified name. Args: name (str): The name of the header to remove. Example: email_util.remove_header(\'Subject\') pass ``` 6. **Method 4: set_payload**: Implement a method `set_payload` to set the payload of the email message. ```python def set_payload(self, payload: str): Sets the payload of the email message. Args: payload (str): The payload to set. Example: email_util.set_payload(\'This is the body of the email.\') pass ``` 7. **Method 5: get_payload**: Implement a method `get_payload` to get the payload of the email message. ```python def get_payload(self) -> str: Returns the payload of the email message. Returns: str: The payload of the email. Example: payload = email_util.get_payload() pass ``` 8. **Method 6: as_string**: Implement a method `as_string` to return the entire email message as a string. ```python def as_string(self) -> str: Returns the entire email message flattened as a string. Returns: str: The flattened email message. Example: message_str = email_util.as_string() pass ``` Constraints - Email headers must comply with **RFC 5322**. - Payloads can be either simple text or binary data. - Ensure that all header names are ASCII characters. Example Usage ```python # Initialize the utility email_util = EmailUtility() # Add headers email_util.add_header(\'Subject\', \'Test Email\') email_util.add_header(\'From\', \'sender@example.com\') email_util.add_header(\'To\', \'receiver@example.com\') # Set payload email_util.set_payload(\'This is a test email.\') # Get headers print(email_util.get_header(\'Subject\')) # Output: Test Email # Remove a header email_util.remove_header(\'To\') # Get payload print(email_util.get_payload()) # Output: This is a test email. # Get the entire email as string print(email_util.as_string()) ``` Your task is to implement the `EmailUtility` class with the specified methods.","solution":"from email.message import EmailMessage class EmailUtility: def __init__(self): self.message = EmailMessage() def add_header(self, name: str, value: str, params: dict = {}): Adds a header to the email message. Args: name (str): The name of the header. value (str): The value of the header. params (dict): Additional parameters for the header. Example: email_util.add_header(\'Content-Disposition\', \'attachment\', {\'filename\': \'file.txt\'}) self.message.add_header(name, value, **params) def get_header(self, name: str) -> str: Returns the value of the specified header field. Args: name (str): The name of the header. Returns: str: The value of the header. Example: value = email_util.get_header(\'Subject\') return self.message.get(name) def remove_header(self, name: str): Removes all occurrences of the header with the specified name. Args: name (str): The name of the header to remove. Example: email_util.remove_header(\'Subject\') del self.message[name] def set_payload(self, payload: str): Sets the payload of the email message. Args: payload (str): The payload to set. Example: email_util.set_payload(\'This is the body of the email.\') self.message.set_payload(payload) def get_payload(self) -> str: Returns the payload of the email message. Returns: str: The payload of the email. Example: payload = email_util.get_payload() return self.message.get_payload() def as_string(self) -> str: Returns the entire email message flattened as a string. Returns: str: The flattened email message. Example: message_str = email_util.as_string() return self.message.as_string()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Python\'s slice objects by implementing a function that extracts sublists from a given list using custom-defined slice parameters. # Problem Statement You are tasked with implementing a function `extract_slices(sequence, slices)` that takes a sequence (a list of integers) and a list of slice parameters, and returns a list of sublists according to the provided slices. The `slices` parameter is a list of tuples, where each tuple contains three values representing `start`, `stop`, and `step` for slicing. These values correspond to the slice object attributes in Python. # Function Signature ```python def extract_slices(sequence: list[int], slices: list[tuple[int, int, int]]) -> list[list[int]]: pass ``` # Input 1. `sequence` - A list of integers (`-10^4 <= sequence[i] <= 10^4`). 2. `slices` - A list of tuples, where each tuple contains three integer values `(start, stop, step)`. # Output - A list of sublists extracted according to the provided slice parameters. # Constraints - The length of the `sequence` will be at most `10^4`. - The number of slice tuples in `slices` will be at most `100`. - The `start`, `stop`, and `step` values will be within the bounds handled by Python slices. # Example ```python sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] slices = [(1, 5, 1), (2, 8, 2), (0, -1, 1)] result = extract_slices(sequence, slices) print(result) # Output: [[2, 3, 4, 5], [3, 5, 7], [1, 2, 3, 4, 5, 6, 7, 8, 9]] ``` # Notes - You must handle different boundary conditions and negative indices properly. - Using the provided slice parameters, create slice objects and apply them to the sequence to obtain the required sublists. - Assume valid input will be provided (i.e., slices are meaningful within the context of the sequence length). # Solution Template ```python def extract_slices(sequence: list[int], slices: list[tuple[int, int, int]]) -> list[list[int]]: result = [] for start, stop, step in slices: slice_obj = slice(start, stop, step) result.append(sequence[slice_obj]) return result ```","solution":"def extract_slices(sequence: list[int], slices: list[tuple[int, int, int]]) -> list[list[int]]: result = [] for start, stop, step in slices: slice_obj = slice(start, stop, step) result.append(sequence[slice_obj]) return result"},{"question":"Objective: Implement a Python function that performs multiple system-level operations showing the understanding and application of certain functions documented above. Task: Write a Python function `system_utilities_demo()` that simulates some system-level interactions using the provided Python C API functions. Your implementation should: 1. **Check if a given file path is interactive**: Create a file named `demo_file.txt`, check and print whether this file is interactive or not using the `Py_FdIsInteractive()` function. 2. **Get and Set System Signal Handlers**: - Retrieve the current signal handler for `SIGINT` (using `PyOS_getsig`). - Set a new signal handler for `SIGINT` that prints `\\"SIGINT received\\"` when triggered (using `PyOS_setsig`). - Restore the original signal handler for `SIGINT`. 3. **Handle File System Path Representations**: Given a file path, convert it to its file system representation using the `PyOS_FSPath()` function and print the result. 4. **Exit the Program Cleanly**: Ensure the program exits cleanly using `Py_Exit()` with a status code of `0`. Input: - A string representing a file path. Output: - Print the results of various operations as described in the task. - Exit the program cleanly. Constraints: - Assume the path provided as input is valid. - Handle potential errors gracefully, ensuring the program does not crash. Example: ```python def system_utilities_demo(path): # Your implementation here # Example usage system_utilities_demo(\'demo_file.txt\') ``` This function should demonstrate understanding and proper use of the Python C API system functions as described in the documentation. Notes: - For handling signals, you may use Python\'s `signal` module to facilitate setting signal handlers. - Creating and managing files should be done using Python\'s built-in `open` function. - Ensure to run the code with proper permissions, especially when handling system-level functions. Performance Requirements: - The function should perform all its tasks efficiently within reasonable time limits, considering the system calls involved.","solution":"import signal import os import sys def system_utilities_demo(path): # Step 1: Check if the file path is interactive by creating the file. with open(path, \'w\') as f: f.write(\\"Testing interactivity and other system utilities.\\") # Check if the file descriptor is interactive fd = os.open(path, os.O_RDONLY) is_interactive = os.isatty(fd) print(f\\"Is file \'{path}\' interactive? {is_interactive}\\") os.close(fd) # Step 2: Get and Set System Signal Handlers for SIGINT def new_sigint_handler(signum, frame): print(\\"SIGINT received\\") original_sigint_handler = signal.getsignal(signal.SIGINT) print(f\\"Original SIGINT handler: {original_sigint_handler}\\") # Set new SIGINT handler signal.signal(signal.SIGINT, new_sigint_handler) # Restore original SIGINT handler signal.signal(signal.SIGINT, original_sigint_handler) print(\\"SIGINT handler restored to original.\\") # Step 3: Handle File System Path Representations using the file system representation fs_path = os.fsdecode(path) print(f\\"File system path representation: {fs_path}\\") # Step 4: Exit the Program Cleanly sys.exit(0) # Example usage if __name__ == \\"__main__\\": try: system_utilities_demo(\'demo_file.txt\') except SystemExit as e: print(f\\"Exited with code {e.code}\\")"},{"question":"You are provided with the Unix group database through Python\'s `grp` module. Your task is to implement a function `fetch_group_members(group_names)` that takes in a list of group names and returns a dictionary where each key is a group name from the input list and the corresponding value is a list of all member usernames in that group. The function should adhere to the following requirements: 1. The function should raise a `KeyError` if any group name in the input list does not exist in the group database. 2. If a group entry has no members, the value should be an empty list. 3. The input list can be of arbitrary length, but may include a maximum of 1000 group names. Function Signature ```python def fetch_group_members(group_names: list[str]) -> dict[str, list[str]]: pass ``` Input - `group_names`: A list of strings representing group names. Constraints: 1 <= len(group_names) <= 1000. Output - A dictionary where each key is a group name and the value is a list of usernames belonging to the group. Example ```python groupnames = [\\"group1\\", \\"admin\\", \\"users\\"] fetch_group_members(groupnames) ``` Output: ```python { \\"group1\\": [\\"user1\\", \\"user2\\"], \\"admin\\": [\\"admin1\\", \\"admin2\\"], \\"users\\": [] } ``` Additional Notes - Use the `grp.getgrnam(name)` function to fetch the group database entry for each group name. - Handle potential `KeyError` exceptions appropriately within your function. - Ensure that your solution is efficient and can handle the maximum input size.","solution":"import grp def fetch_group_members(group_names: list[str]) -> dict[str, list[str]]: Fetches the group members for the given list of group names. Parameters: group_names (list[str]): A list of group names. Returns: dict[str, list[str]]: A dictionary where keys are group names and values are lists of members in those groups. Raises: KeyError: If any group name does not exist in the system. result = {} for group_name in group_names: try: group_info = grp.getgrnam(group_name) result[group_name] = group_info.gr_mem except KeyError: raise KeyError(f\\"The group {group_name} does not exist.\\") return result"},{"question":"# Advanced Coding Assessment Question: Optimizing PyTorch Operations with CUDA Objective Design and implement a function to optimize PyTorch operations on CUDA utilizing CUDA streams and graphs for better performance. The operations to be performed include matrix multiplication and element-wise addition on large tensors, and you must maintain dynamic memory management throughout the process. Problem Statement You are required to implement a class `CudaOptimizer` with methods that: 1. Initialize tensors on specified CUDA devices. 2. Perform matrix multiplication and element-wise addition with CUDA streams. 3. Capture and replay these operations using CUDA graph for optimized execution. 4. Record and report the time taken for these operations with and without CUDA graph optimizations. Instructions 1. **Initialization Method:** - `initialize_tensors`: Accepts the tensor sizes and CUDA device IDs, and initializes the tensors on specified devices. 2. **CUDA Stream Operations Method:** - `stream_operations`: Performs matrix multiplication and element-wise addition using CUDA streams. 3. **CUDA Graph Optimization Method:** - `capture_graph_operations`: Captures the operations into a CUDA graph and replays them. 4. **Time Measurement Method:** - `measure_operations`: Measures the execution time for operations with and without CUDA graphs. Input and Output 1. **Input:** - `tensor_sizes` (tuple): A tuple of integers representing the sizes of the tensors. - `cuda_devices` (list): A list of CUDA device IDs indicating on which devices the tensors should be initialized. 2. **Output:** - A dictionary with two keys: `no_graph_time` and `graph_time`, representing the time taken for operations without CUDA graph optimization and with CUDA graph optimization respectively. Constraints - You can assume that the provided CUDA device IDs are valid and CUDA is available on the system. - The tensor sizes should be large enough to observe significant performance benefits from optimization. Example ```python # Example class usage optimizer = CudaOptimizer() tensor_sizes = (10240, 10240) cuda_devices = [0, 1] # Initialize tensors optimizer.initialize_tensors(tensor_sizes, cuda_devices) # Measure performance timing = optimizer.measure_operations() print(timing) ``` Implementation The `CudaOptimizer` class should be implemented as follows: ```python import torch import time class CudaOptimizer: def __init__(self): self.tensors = {} self.streams = {} self.graphs = {} def initialize_tensors(self, tensor_sizes, cuda_devices): assert len(cuda_devices) == 2, \\"Two CUDA devices are required.\\" self.tensors[\'A\'] = torch.randn(tensor_sizes, device=f\'cuda:{cuda_devices[0]}\') self.tensors[\'B\'] = torch.randn(tensor_sizes, device=f\'cuda:{cuda_devices[1]}\') def stream_operations(self): cuda0 = torch.device(\'cuda:0\') cuda1 = torch.device(\'cuda:1\') self.streams[\'s1\'] = torch.cuda.Stream(device=cuda0) self.streams[\'s2\'] = torch.cuda.Stream(device=cuda1) with torch.cuda.stream(self.streams[\'s1\']): C = self.tensors[\'A\'] @ self.tensors[\'A\'].t() with torch.cuda.stream(self.streams[\'s2\']): D = self.tensors[\'B\'] + self.tensors[\'B\'] torch.cuda.synchronize(cuda0) torch.cuda.synchronize(cuda1) def capture_graph_operations(self): self.graphs[\'g\'] = torch.cuda.CUDAGraph() static_input_A = self.tensors[\'A\'] static_input_B = self.tensors[\'B\'] torch.cuda.synchronize() with torch.cuda.graph(self.graphs[\'g\']): C = static_input_A @ static_input_A.t() D = static_input_B + static_input_B self.graphs[\'g\'].replay() def measure_operations(self): # Measure without CUDA Graph optimization start_time = time.time() self.stream_operations() no_graph_time = time.time() - start_time # Measure with CUDA Graph optimization start_time = time.time() self.capture_graph_operations() graph_time = time.time() - start_time return {\'no_graph_time\': no_graph_time, \'graph_time\': graph_time} ``` Your task is to complete and test the `CudaOptimizer` class to ensure it meets the requirements.","solution":"import torch import time class CudaOptimizer: def __init__(self): self.tensors = {} self.streams = {} self.graphs = {} def initialize_tensors(self, tensor_sizes, cuda_devices): assert len(cuda_devices) == 2, \\"Two CUDA devices are required.\\" self.tensors[\'A\'] = torch.randn(tensor_sizes, device=f\'cuda:{cuda_devices[0]}\') self.tensors[\'B\'] = torch.randn(tensor_sizes, device=f\'cuda:{cuda_devices[1]}\') def stream_operations(self): cuda0 = torch.device(\'cuda:0\') cuda1 = torch.device(\'cuda:1\') self.streams[\'s1\'] = torch.cuda.Stream(device=cuda0) self.streams[\'s2\'] = torch.cuda.Stream(device=cuda1) with torch.cuda.stream(self.streams[\'s1\']): C = self.tensors[\'A\'] @ self.tensors[\'A\'].t() with torch.cuda.stream(self.streams[\'s2\']): D = self.tensors[\'B\'] + self.tensors[\'B\'] torch.cuda.synchronize(cuda0) torch.cuda.synchronize(cuda1) def capture_graph_operations(self): self.graphs[\'g\'] = torch.cuda.CUDAGraph() static_input_A = self.tensors[\'A\'] static_input_B = self.tensors[\'B\'] torch.cuda.synchronize() with torch.cuda.graph(self.graphs[\'g\']): C = static_input_A @ static_input_A.t() D = static_input_B + static_input_B self.graphs[\'g\'].replay() def measure_operations(self): # Measure without CUDA Graph optimization start_time = time.time() self.stream_operations() no_graph_time = time.time() - start_time # Measure with CUDA Graph optimization start_time = time.time() self.capture_graph_operations() graph_time = time.time() - start_time return {\'no_graph_time\': no_graph_time, \'graph_time\': graph_time}"},{"question":"You are required to build a functionality that utilizes Python’s `mimetypes` module to identify MIME types and manage custom MIME type mappings. You will implement a class `MimeTypeManager` that provides the following functionalities: 1. **Load MIME types from a file:** ```python def load_mime_types(self, filename: str) -> bool: ``` - **Input:** - `filename` (str): The path to the MIME types file. - **Output:** - Returns `True` if the file was successfully loaded, `False` otherwise. 2. **Guess MIME type from a URL or filename:** ```python def guess_mime_type(self, url: str) -> Tuple[Optional[str], Optional[str]]: ``` - **Input:** - `url` (str): The URL or filename whose MIME type needs to be guessed. - **Output:** - Returns a tuple `(type, encoding)`. The `type` is a string representing the MIME type, and `encoding` is the encoding type if any. `None` is returned for both if the type can\'t be guessed. 3. **Guess file extension from a MIME type:** ```python def guess_file_extension(self, mime_type: str) -> Optional[str]: ``` - **Input:** - `mime_type` (str): The MIME type for which to guess the file extension. - **Output:** - Returns a string representing the file extension including the leading dot (e.g., \'.txt\'). Returns `None` if no extension can be guessed. 4. **Add a custom MIME type to file extension mapping:** ```python def add_custom_type(self, mime_type: str, extension: str) -> None: ``` - **Input:** - `mime_type` (str): The MIME type to be added. - `extension` (str): The file extension to be mapped to the MIME type. - **Output:** - None. # Constraints 1. You may assume that the `filename` provided in `load_mime_types` refers to a valid file formatted similarly to standard `mime.types` files. 2. The length of `url` and `mime_type` strings will not exceed 255 characters. 3. Extensions will always start with a leading dot. # Example ```python manager = MimeTypeManager() # Load MIME types from a file manager.load_mime_types(\'mime.types\') # Returns True if successfully loaded. # Guess MIME type from a URL or filename print(manager.guess_mime_type(\'example.txt\')) # (\'text/plain\', None) # Guess file extension from a MIME type print(manager.guess_file_extension(\'text/plain\')) # \'.txt\' # Add a custom MIME type to file extension mapping manager.add_custom_type(\'application/example\', \'.ex\') print(manager.guess_file_extension(\'application/example\')) # \'.ex\' ``` Implement the `MimeTypeManager` class with the above functionalities using the `mimetypes` module.","solution":"import mimetypes from typing import Optional, Tuple class MimeTypeManager: def __init__(self): self.custom_types = {} def load_mime_types(self, filename: str) -> bool: try: mimetypes.init() mimetypes.read_mime_types(filename) return True except Exception: return False def guess_mime_type(self, url: str) -> Tuple[Optional[str], Optional[str]]: mime_type, encoding = mimetypes.guess_type(url) return mime_type, encoding def guess_file_extension(self, mime_type: str) -> Optional[str]: # Check custom types first if mime_type in self.custom_types: return self.custom_types[mime_type] extension = mimetypes.guess_extension(mime_type) return extension def add_custom_type(self, mime_type: str, extension: str) -> None: self.custom_types[mime_type] = extension"},{"question":"Custom Plotting Context and Visualization **Objective**: To assess the student\'s understanding of seaborn\'s plotting contexts and their ability to create and manage plot visualizations. **Problem Statement**: You are required to write a function called `custom_plotting_context`, which takes a dataset, a predefined seaborn style, and a dictionary of custom parameters. Your function should: 1. Apply the predefined-style context. 2. Temporarily customize the plotting parameters using the provided dictionary. 3. Create a plot using the seaborn\'s `lineplot` to visualize data. 4. Return the final plot object. **Function Signature**: ```python def custom_plotting_context(data: dict, style: str, custom_params: dict): pass ``` **Input**: * `data` (dict): A dictionary with `x` and `y` keys corresponding to the data points for the x and y axes respectively. Example: ```python { \\"x\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"y\\": [10, 20, 15, 25] } ``` * `style` (str): A predefined seaborn style context, e.g., \\"talk\\", \\"paper\\", \\"notebook\\", etc. * `custom_params` (dict): A dictionary of custom plotting parameters. Example: ```python { \\"font.size\\": 15, \\"axes.labelsize\\": 14 } ``` **Output**: * Returns a seaborn plot object (lineplot). **Constraints**: * The function should be self-contained with no external dependencies except for seaborn and matplotlib. * Only use seaborn methods as demonstrated in the provided documentation. **Example**: ```python data = { \\"x\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"y\\": [10, 20, 15, 25] } style = \\"talk\\" custom_params = { \\"font.size\\": 15, \\"axes.labelsize\\": 14 } plot = custom_plotting_context(data, style, custom_params) plot.show() ``` # Requirements: 1. The function should handle invalid input gracefully, raising appropriate exceptions. 2. Ensure that any temporary changes to the plotting context do not affect subsequent plots outside the function. # Evaluation Criteria: 1. Correct implementation of plotting contexts. 2. Proper application of custom parameters. 3. Quality and readability of the resulting plot. 4. Code organization and comments.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plotting_context(data, style, custom_params): Applies a predefined seaborn style and custom parameters to create a line plot. Parameters: - data (dict): A dictionary with \'x\' and \'y\' keys containing the data points. - style (str): A predefined seaborn style context. - custom_params (dict): A dictionary of custom plotting parameters. Returns: - plot: The seaborn line plot object. # Validate input if not isinstance(data, dict) or \'x\' not in data or \'y\' not in data: raise ValueError(\\"Data must be a dictionary with \'x\' and \'y\' keys.\\") if not isinstance(style, str): raise ValueError(\\"Style must be a string.\\") if not isinstance(custom_params, dict): raise ValueError(\\"Custom parameters must be a dictionary.\\") # Apply the predefined style context sns.set_context(style) # Customize plotting parameters temporarily with sns.plotting_context(rc=custom_params): # Create the line plot plot = sns.lineplot(x=data[\'x\'], y=data[\'y\']) return plot"},{"question":"You are given a list of tasks, each with a priority and a description. Your task is to implement a priority queue using the `heapq` module, which supports the following operations: 1. Adding a new task with a given priority. 2. Removing a task with a given description. 3. Pop and return the task with the highest priority. 4. Changing the priority of an existing task. Implement the following functions: 1. `add_task(priority_queue, task, priority)`: - **Input**: - `priority_queue` (list): The priority queue. - `task` (str): The task description. - `priority` (int): The priority of the task. - **Output**: None 2. `remove_task(priority_queue, task, task_finder, removed_marker)`: - **Input**: - `priority_queue` (list): The priority queue. - `task` (str): The task description to remove. - `task_finder` (dict): A dictionary mapping task descriptions to their entries in the priority queue. - `removed_marker` (str): A marker indicating that the task has been removed. - **Output**: None 3. `pop_task(priority_queue, task_finder, removed_marker)`: - **Input**: - `priority_queue` (list): The priority queue. - `task_finder` (dict): A dictionary mapping task descriptions to their entries in the priority queue. - `removed_marker` (str): A marker indicating that the task has been removed. - **Output**: - Returns the description of the task with the highest priority. 4. `change_task_priority(priority_queue, task, priority, task_finder, removed_marker)`: - **Input**: - `priority_queue` (list): The priority queue. - `task` (str): The task description whose priority is to be changed. - `priority` (int): The new priority of the task. - `task_finder` (dict): A dictionary mapping task descriptions to their entries in the priority queue. - `removed_marker` (str): A marker indicating that the task has been removed. - **Output**: None **Example**: ```python priority_queue = [] task_finder = {} REMOVED = \'<removed-task>\' add_task(priority_queue, \'task1\', 1) add_task(priority_queue, \'task2\', 2) add_task(priority_queue, \'task3\', 3) # priority_queue now contains tasks in the order: task1, task2, task3 remove_task(priority_queue, \'task2\', task_finder, REMOVED) # \'task2\' is marked as removed and priority_queue is updated popped_task = pop_task(priority_queue, task_finder, REMOVED) # should return \'task1\' change_task_priority(priority_queue, \'task3\', 0, task_finder, REMOVED) # \'task3\' priority is changed and priority_queue is updated ``` **Constraints**: 1. Task descriptions are unique. 2. Priorities are integers. 3. The priority queue should be implemented using the `heapq` module. 4. Ensure that adding and removing tasks, as well as changing task priorities, maintain the heap invariant.","solution":"import heapq def add_task(priority_queue, task, priority): Adds a new task with a given priority. entry = [priority, task] heapq.heappush(priority_queue, entry) def remove_task(priority_queue, task, task_finder, removed_marker): Marks a task as removed in the priority queue by using a unique marker. entry = task_finder.pop(task) entry[-1] = removed_marker def pop_task(priority_queue, task_finder, removed_marker): Pops and returns the task with the highest priority. while priority_queue: priority, task = heapq.heappop(priority_queue) if task != removed_marker: del task_finder[task] return task return None def change_task_priority(priority_queue, task, priority, task_finder, removed_marker): Changes the priority of an existing task. remove_task(priority_queue, task, task_finder, removed_marker) add_task(priority_queue, task, priority)"},{"question":"**Coding Assessment Question** # Objective: You are tasked to write a Python function that mimics a subset of functionalities provided by the `sdist` command, specifically those related to include/exclude patterns for files in a directory tree. You should be able to parse a set of commands and apply them to select the appropriate files. # Problem Statement: You need to implement a function `apply_sdist_commands(directory: str, commands: List[str]) -> List[str]` that takes a directory and a list of commands, and returns a list of file paths that match the include/exclude criteria specified by the commands. # Input: - **directory (str)**: The base directory to apply the commands on. - **commands (List[str])**: A list of commands (strings) where each command is of the format described, such as `include`, `exclude`, `recursive-include`, `recursive-exclude`, etc. # Output: - **List[str]**: A list of file paths (relative to the base directory) that should be included after applying the commands. # Constraints: 1. The patterns use Unix-style \\"glob\\" patterns: \\"`*`\\" matches any sequence of characters, \\"`?`\\" matches any single character, and \\"`[range]`\\" matches any character in the range (e.g., \\"a-z\\", \\"a-zA-Z\\", \\"a-f0-9_.\\"). 2. You must handle both inclusion and exclusion criteria. 3. The order of the commands affects the result (later commands can override earlier ones). 4. Assume that all paths use Unix-style forward slashes \\"/\\" for directories. # Example: ```python directory = \\"project\\" commands = [ \\"include *.py\\", \\"exclude test_*.py\\", \\"recursive-include src *.py *.md\\", \\"recursive-exclude src __pycache__\\" ] print(apply_sdist_commands(directory, commands)) ``` # Sample Output Explanation: This should output a list of file paths considering the commands provided, applying first the includes, then the excludes, followed by recursive pattern matching within directories as instructed. # Notes: - You may use standard Python libraries such as `os`, `fnmatch`, and `glob` to help with file system and pattern matching tasks. - Consider edge cases where no files match the criteria or when a path is excluded after being included. # Solution: Implement the function with appropriate parsing, matching, and overrides to ensure accurate results. ```python import os import fnmatch def apply_sdist_commands(directory: str, commands: List[str]) -> List[str]: included_files = set() excluded_files = set() for command in commands: parts = command.split() cmd_type = parts[0] patterns = parts[1:] if cmd_type == \\"include\\": for pattern in patterns: for root, dirs, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): included_files.add(os.path.join(root, filename).replace(directory + \\"/\\", \\"\\")) elif cmd_type == \\"exclude\\": for pattern in patterns: for root, dirs, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): excluded_files.add(os.path.join(root, filename).replace(directory + \\"/\\", \\"\\")) elif cmd_type == \\"recursive-include\\": dir = patterns[0] patterns = patterns[1:] for pattern in patterns: for root, dirs, files in os.walk(os.path.join(directory, dir)): for filename in fnmatch.filter(files, pattern): included_files.add(os.path.join(root, filename).replace(directory + \\"/\\", \\"\\")) elif cmd_type == \\"recursive-exclude\\": dir = patterns[0] patterns = patterns[1:] for pattern in patterns: for root, dirs, files in os.walk(os.path.join(directory, dir)): for filename in fnmatch.filter(files, pattern): excluded_files.add(os.path.join(root, filename).replace(directory + \\"/\\", \\"\\")) # The final files should be those included but not excluded result_files = [file for file in included_files if file not in excluded_files] return sorted(result_files) ``` Make sure to test your function with various combinations of commands and directory structures to ensure it works as expected.","solution":"import os import fnmatch from typing import List def apply_sdist_commands(directory: str, commands: List[str]) -> List[str]: included_files = set() excluded_files = set() for command in commands: parts = command.split() cmd_type = parts[0] patterns = parts[1:] if cmd_type == \\"include\\": for pattern in patterns: for root, dirs, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): included_files.add(os.path.relpath(os.path.join(root, filename), directory)) elif cmd_type == \\"exclude\\": for pattern in patterns: for root, dirs, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): excluded_files.add(os.path.relpath(os.path.join(root, filename), directory)) elif cmd_type == \\"recursive-include\\": dir = patterns[0] patterns = patterns[1:] for root, dirs, files in os.walk(os.path.join(directory, dir)): for pattern in patterns: for filename in fnmatch.filter(files, pattern): included_files.add(os.path.relpath(os.path.join(root, filename), directory)) elif cmd_type == \\"recursive-exclude\\": dir = patterns[0] patterns = patterns[1:] for root, dirs, files in os.walk(os.path.join(directory, dir)): for pattern in patterns: for filename in fnmatch.filter(files, pattern): excluded_files.add(os.path.relpath(os.path.join(root, filename), directory)) # The final files should be those included but not excluded result_files = [file for file in included_files if file not in excluded_files] return sorted(result_files)"},{"question":"Objective: Implement a function that performs various operations on a mixed collection of data types. Your solution should demonstrate your understanding of Python\'s built-in types, their methods, and various operations, including those on sequences, numerics, and mappings. Problem Statement: You need to write a function `analyze_collection(data)` that takes in a list called `data`, which contains elements of different data types like integers, floats, strings, tuples, lists, and dictionaries. The function should perform the following operations and return a dictionary with the specified details: 1. **Data Types Count**: Count the number of elements of each data type (int, float, str, list, tuple, dict) in the `data` list. 2. **Truth Value Testing**: Create a list of the truth values of the elements in the `data` list. 3. **Numeric Operations**: Calculate the sum of all numeric (int and float) elements in the `data` list. 4. **String Manipulations**: Concatenate all the string elements in the `data` list. 5. **Sequence Lengths**: Create a dictionary with the lengths of all sequence elements (str, list, tuple) in the `data` list. 6. **Key Existence in Dictionaries**: For each dictionary in the `data` list, check if a given key (the key to check is \'key\') exists, and count how many dictionaries contain this key. Function Signature: ```python def analyze_collection(data: list) -> dict: pass ``` Input: - `data`: A list containing elements of various data types (e.g., `[1, 2.0, \'hello\', [1, 2], (3, 4), {\'key\': \'value\'}, \'world\']`). Output: - A dictionary containing results with keys: - `\'type_counts\'`: A dictionary with the count of each data type. - `\'truth_values\'`: A list of boolean values indicating the truth value of each element. - `\'numeric_sum\'`: The sum of all numeric elements. - `\'concatenated_str\'`: A single string that is the result of concatenating all string elements. - `\'sequence_lengths\'`: A dictionary with the lengths of each sequence element. - `\'key_existence_count\'`: An integer indicating how many dictionaries contain the key `\'key\'`. Example: ```python data = [1, 2.0, \'hello\', [1, 2], (3, 4), {\'key\': \'value\'}, \'world\'] result = analyze_collection(data) print(result) # Expected output: { \'type_counts\': {\'int\': 1, \'float\': 1, \'str\': 2, \'list\': 1, \'tuple\': 1, \'dict\': 1}, \'truth_values\': [True, True, True, True, True, True, True], \'numeric_sum\': 3.0, \'concatenated_str\': \'helloworld\', \'sequence_lengths\': {\'hello\': 5, \'world\': 5, \'[1, 2]\': 2, \'(3, 4)\': 2}, \'key_existence_count\': 1 } ``` Constraints: - The list might contain nested collections, but for this problem, only consider the top-level elements. - Assume all elements are valid and no need for error handling of invalid types. **Note**: Ensure your solution is efficient and avoids unnecessary computations.","solution":"def analyze_collection(data): Analyzes a mixed collection of data types. type_counts = {\'int\': 0, \'float\': 0, \'str\': 0, \'list\': 0, \'tuple\': 0, \'dict\': 0} truth_values = [] numeric_sum = 0 concatenated_str = \\"\\" sequence_lengths = {} key_existence_count = 0 for item in data: # Count types if isinstance(item, int): type_counts[\'int\'] += 1 numeric_sum += item elif isinstance(item, float): type_counts[\'float\'] += 1 numeric_sum += item elif isinstance(item, str): type_counts[\'str\'] += 1 concatenated_str += item sequence_lengths[item] = len(item) elif isinstance(item, list): type_counts[\'list\'] += 1 sequence_lengths[str(item)] = len(item) elif isinstance(item, tuple): type_counts[\'tuple\'] += 1 sequence_lengths[str(item)] = len(item) elif isinstance(item, dict): type_counts[\'dict\'] += 1 if \'key\' in item: key_existence_count += 1 # Truth value testing truth_values.append(bool(item)) return { \'type_counts\': type_counts, \'truth_values\': truth_values, \'numeric_sum\': numeric_sum, \'concatenated_str\': concatenated_str, \'sequence_lengths\': sequence_lengths, \'key_existence_count\': key_existence_count }"},{"question":"# Advanced Coding Assessment: Customizing Histograms with Seaborn **Objective:** Your task is to demonstrate your understanding of creating and customizing univariate and bivariate histograms using the seaborn `histplot` function. # Instructions: 1. **Univariate Histogram**: Load the \\"penguins\\" dataset and create a univariate histogram for the `flipper_length_mm` column. Customize the histogram by setting the number of bins to 20 and adding a kernel density estimate (KDE). 2. **Multiple Distributions**: Using the same \\"penguins\\" dataset, create a histogram for `flipper_length_mm`, but this time plot the distributions for each species in different colors using the `hue` parameter. 3. **Bivariate Histogram**: Load the \\"planets\\" dataset and create a bivariate histogram for the `year` (x-axis) and `distance` (y-axis) columns. Ensure the y-axis uses a logarithmic scale. Also, add a color bar to annotate the colormap. 4. **Normalization and Log Scale**: For the \\"tips\\" dataset, create a histogram for the `total_bill` column. Use the `stat` parameter to normalize the histogram to show the probability. Set the bins in log space. Requirements: 1. For each task, include descriptive titles and labels for axes. 2. The histograms should be visually distinct and demonstrate a clear understanding of customization options in seaborn. 3. Your code should be efficient and well-documented. # Expected Output: 1. A univariate histogram for `flipper_length_mm` with KDE and customized bins. 2. A histogram of `flipper_length_mm` for each species, color-coded. 3. A bivariate histogram of `year` vs. `distance` on a log scale, with a color bar. 4. A normalized histogram of `total_bill` on a log scale showing probabilities. # Example Solution: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Univariate Histogram with KDE penguins = sns.load_dataset(\\"penguins\\") plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\\"Univariate Histogram of Flipper Length with KDE\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Task 2: Multiple Distributions with Overlay plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"layer\\") plt.title(\\"Histogram of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Task 3: Bivariate Histogram with Log Scale and Color Bar planets = sns.load_dataset(\\"planets\\") plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\\"Bivariate Histogram of Year vs. Distance\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Distance (log scale)\\") plt.show() # Task 4: Normalized Histogram with Log Scale for Bins tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(10, 6)) sns.histplot(data=tips, x=\\"total_bill\\", stat=\\"probability\\", bins=30, log_scale=True) plt.title(\\"Normalized Histogram of Total Bill with Log Scale\\") plt.xlabel(\\"Total Bill (log scale)\\") plt.ylabel(\\"Probability\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_univariate_histogram_with_kde(): penguins = sns.load_dataset(\\"penguins\\") plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\\"Univariate Histogram of Flipper Length with KDE\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() def plot_histogram_multiple_distributions(): penguins = sns.load_dataset(\\"penguins\\") plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"layer\\") plt.title(\\"Histogram of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() def plot_bivariate_histogram(): planets = sns.load_dataset(\\"planets\\") plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\\"Bivariate Histogram of Year vs. Distance\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Distance (log scale)\\") plt.show() def plot_normalized_histogram_log_scale(): tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(10, 6)) sns.histplot(data=tips, x=\\"total_bill\\", stat=\\"probability\\", bins=30, log_scale=True) plt.title(\\"Normalized Histogram of Total Bill with Log Scale\\") plt.xlabel(\\"Total Bill (log scale)\\") plt.ylabel(\\"Probability\\") plt.show()"},{"question":"Given the focus on environment variables in the \\"posix\\" module, the following task is designed to assess the student\'s understanding of operating system environment manipulations using the \\"os\\" module in Python. # Task: Environment Variable Manipulation **Background:** Environment variables are a set of dynamic named values that can affect the way running processes will behave on a computer. The \\"os\\" module in Python provides a way to interact with these environment variables. **Objective:** Write a Python function called `modify_environment` that takes two parameters: 1. `var_name` (string): The name of the environment variable. 2. `new_value` (string or None): The new value to set for the environment variable. If `new_value` is None, the function should delete the environment variable if it exists. The function should perform the following actions: 1. If `new_value` is provided (not None): - Update or add the environment variable `var_name` with the value `new_value`. 2. If `new_value` is None: - Delete the environment variable `var_name` if it exists. If the variable does not exist, do nothing. After modifying the environment, your function should return the updated environment variables as a dictionary using `os.environ`. # Constraints: - Assume the function will only be called in environments where the \\"os\\" module is available. - The function should handle potential exceptions gracefully and log error messages where applicable. # Function Signature: ```python import os def modify_environment(var_name: str, new_value: str | None) -> dict: # Your implementation here ``` # Example Usage: ```python # Adding a new environment variable print(modify_environment(\\"MY_VAR\\", \\"my_value\\")) # Expected output: The dictionary of environment variables with \\"MY_VAR\\" set to \\"my_value\\". # Updating the existing environment variable print(modify_environment(\\"MY_VAR\\", \\"new_value\\")) # Expected output: The dictionary of environment variables with \\"MY_VAR\\" updated to \\"new_value\\". # Deleting the environment variable print(modify_environment(\\"MY_VAR\\", None)) # Expected output: The dictionary of environment variables without \\"MY_VAR\\". ``` **Note:** Your implementation should ensure compatibility with both Unix and Windows environments, making use of the \\"os\\" module in Python.","solution":"import os def modify_environment(var_name: str, new_value: str | None) -> dict: Modify the given environment variable. Parameters: var_name (str): The name of the environment variable. new_value (str | None): The new value to set for the environment variable. If None, delete the variable. Returns: dict: The updated environment variables dictionary. if new_value is not None: os.environ[var_name] = new_value else: if var_name in os.environ: del os.environ[var_name] return dict(os.environ)"},{"question":"Objective Write a function that converts a string containing HTML entities to its respective Unicode characters. The function should leverage the dictionaries in the `html.entities` module to handle the transformations. Function Signature ```python def decode_html_entities(html_string: str) -> str: pass ``` Input - `html_string` (str): A string containing HTML entity names (e.g., \\"Hello &amp; Welcome to the world of &lt;Python&gt;!\\") or numeric character references (e.g., \\"&#60;3\\"). Output - (str): The decoded string with all HTML entities replaced by their corresponding Unicode characters. Constraints - The input string can be up to 10^5 characters long. - The function should handle both named entities (like `&amp;`, `&gt;`, `&lt;`) and numeric character references (like `&#60;`, `&#x3C;`) appropriately. - If an entity is not recognized, it should remain unchanged in the output string. Performance Requirements - The function should run efficiently with a time complexity of O(n), where n is the length of the input string. Examples ```python decode_html_entities(\\"Hello &amp; World!\\") # Should return: \\"Hello & World!\\" decode_html_entities(\\"100 &gt; 20 &amp; 2 &lt; 3\\") # Should return: \\"100 > 20 & 2 < 3\\" decode_html_entities(\\"&#60;div&#62;Hello&#60;/div&#62;\\") # Should return: \\"<div>Hello</div>\\" decode_html_entities(\\"5 &le; 10\\") # Should return: \\"5 ≤ 10\\" decode_html_entities(\\"Use the &copy; symbol.\\") # Should return: \\"Use the © symbol.\\" ``` Note Use the `html.entities` module\'s dictionaries to map named HTML entities to their respective Unicode characters. For numeric character references, you can use Python\'s built-in `chr()` function to convert Unicode code points to their corresponding characters.","solution":"from html import unescape def decode_html_entities(html_string: str) -> str: Converts a string containing HTML entities to its respective Unicode characters. This function handles both named entities (like &amp;, &gt;, &lt;) and numeric character references (like &#60;, &#x3C;). Args: - html_string (str): A string containing HTML entity names or numeric character references. Returns: - str: The decoded string with all HTML entities replaced by their corresponding Unicode characters. return unescape(html_string)"},{"question":"**Coding Assessment Question: Analysis of Customer Transactions** You are provided with a dataset containing customer transactions. Your task is to write a function that reads this data, performs several data manipulations, and outputs a summary DataFrame. # Function Signature: ```python import pandas as pd def analyze_customer_transactions(file_path: str) -> pd.DataFrame: pass ``` # Input: - `file_path`: A string representing the path to a CSV file. The CSV file contains the following columns: - `customer_id`: A unique identifier for each customer. - `transaction_date`: The date of the transaction. - `amount`: The transaction amount in dollars. # Output: - A pandas DataFrame with the following columns: - `customer_id`: Unique customer identifier. - `total_transactions`: Total number of transactions per customer. - `total_amount`: Total transaction amount per customer. - `average_transaction_amount`: Average transaction amount per customer. - `first_transaction_date`: The date of the first transaction per customer. - `last_transaction_date`: The date of the last transaction per customer. # Steps: 1. Read the CSV file into a pandas DataFrame. 2. Perform data manipulations to calculate the required summary metrics. 3. Ensure the DataFrame is sorted by `customer_id`. 4. Return the resulting summary DataFrame. # Example: If the CSV file contains the following data: ``` customer_id,transaction_date,amount 1,2023-01-01,100.0 1,2023-01-15,200.0 2,2023-02-01,300.0 2,2023-02-02,150.0 3,2023-03-01,50.0 ``` The output DataFrame should look like: ``` customer_id total_transactions total_amount average_transaction_amount first_transaction_date last_transaction_date 0 1 2 300.0 150.0 2023-01-01 2023-01-15 1 2 2 450.0 225.0 2023-02-01 2023-02-02 2 3 1 50.0 50.0 2023-03-01 2023-03-01 ``` # Constraints: - Do not use any private pandas modules or functions. - Assume the input CSV file is well-formed and does not contain any missing or invalid data. - The function will be tested with various CSV files having different sizes ranging from small to very large datasets. # Notes: - Focus on writing clean, efficient, and readable code. - Make use of appropriate pandas functions and methods to achieve the desired results.","solution":"import pandas as pd def analyze_customer_transactions(file_path: str) -> pd.DataFrame: # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Convert transaction_date to datetime df[\'transaction_date\'] = pd.to_datetime(df[\'transaction_date\']) # Group by customer_id and calculate required metrics grouped_df = df.groupby(\'customer_id\').agg( total_transactions=(\'transaction_date\', \'count\'), total_amount=(\'amount\', \'sum\'), average_transaction_amount=(\'amount\', \'mean\'), first_transaction_date=(\'transaction_date\', \'min\'), last_transaction_date=(\'transaction_date\', \'max\') ).reset_index() # Ensure the DataFrame is sorted by customer_id grouped_df = grouped_df.sort_values(by=\'customer_id\').reset_index(drop=True) return grouped_df"},{"question":"**Title: Analyzing and Manipulating Iterators with `itertools`** **Objective:** Write a function that processes a list of numbers through various stages of transformation using functions from the `itertools` module. **Problem Statement:** Given a list of integers, perform the following transformations step-by-step using the `itertools` module: 1. **Filter even numbers**: First, ignore all even numbers using `itertools.filterfalse`. 2. **Accumulate odd sums**: Compute an accumulated sum of the remaining numbers using `itertools.accumulate`. 3. **Pairwise summation**: Using the result of the accumulated sums, create a list of pairwise summed tuples. 4. **Chain additional numbers**: Chain the results with another list that repeats a specified number of times. 5. **Return everything**: Return the list of all the results. **Function Signature:** ```python def transform_numbers(numbers: list, extra: list, repeat: int) -> tuple: pass ``` **Inputs:** 1. `numbers`: A list of integers. 2. `extra`: A list of integers for chaining. 3. `repeat`: An integer indicating how many times the `extra` list should be repeated using `itertools.repeat`. **Outputs:** A tuple containing three lists: 1. The filtered list with even numbers removed. 2. The accumulated sum list. 3. A list of pairwise summed tuples. 4. The final chained list. **Example:** ```python numbers = [1, 2, 3, 4, 5, 6] extra = [7, 8] repeat = 2 filtered, accumulated, pairwise_sums, final_chain = transform_numbers(numbers, extra, repeat) assert filtered == [1, 3, 5] assert accumulated == [1, 4, 9] assert pairwise_sums == [(1, 4), (4, 9)] assert final_chain == [1, 4, 9, 7, 8, 7, 8] ``` **Constraints:** - The function should utilize the `itertools` module\'s functions where specified. - Assume all inputs are valid and non-empty. **Notes:** - The function should handle large inputs efficiently, ensuring that the operations using `itertools` are optimized for performance. **Hints:** - Use `itertools.filterfalse` to filter out the even numbers. - Use `itertools.accumulate` for computing the accumulated sums. - Use `itertools.pairwise` for creating overlapping pairs. - Use `itertools.chain` and `itertools.repeat` to generate the final chained list. **Bonus Challenge:** Improve the function to handle cases where the list of numbers can be empty, returning appropriate empty lists for each stage.","solution":"import itertools def transform_numbers(numbers: list, extra: list, repeat: int): Transforms the given list of numbers through various stages using itertools. Args: - numbers: a list of integers. - extra: a list of integers to be chained. - repeat: an integer indicating how many times to repeat the \'extra\' list. Returns: A tuple containing: - The filtered list with even numbers removed. - The accumulated sum list. - A list of pairwise summed tuples. - The final chained list. # Filter out even numbers filtered = list(itertools.filterfalse(lambda x: x % 2 == 0, numbers)) # Compute accumulated sums accumulated = list(itertools.accumulate(filtered)) # Create list of pairwise summed tuples paired = list(itertools.pairwise(accumulated)) # Chain the accumulated sums with extra list repeated \'repeat\' times chained = list(itertools.chain(accumulated, itertools.chain.from_iterable(itertools.repeat(extra, repeat)))) return filtered, accumulated, paired, chained"},{"question":"# Advanced Coding Assessment Question Objective Design a Python function that reads multiple text files, processes each line according to specific rules, and writes the processed content either back to the original files (in-place) or to new output files. Your solution should demonstrate a strong understanding of Python\'s `fileinput` module. Task You are required to implement a function `process_files()` that processes multiple input files, either outputting the modified contents to new files or replacing the original files with their modified versions. It should support handling compressed files and should be implemented using the `fileinput` module. Function Signature ```python def process_files(files: list, in_place: bool = False, backup_ext: str = \'.bak\', encoding: str = \'utf-8\', compress: bool = False) -> None: pass ``` Inputs - `files` (list): A list of file paths to process. - `in_place` (bool): If `True`, modify the input files in place. If `False`, create new files with the modified content (appending `_processed` to the original file names). - `backup_ext` (str): The extension to use for backup files when `in_place` is `True` (default: `.bak`). Ignored if `in_place` is `False`. - `encoding` (str): The encoding to use for reading and writing files (default: `\'utf-8\'`). - `compress` (bool): If `True`, handle `.gz` and `.bz2` compressed files. Processing Rules 1. **Remove Empty Lines**: All empty lines should be removed. 2. **Trim Whitespace**: Leading and trailing whitespace should be trimmed from each line. 3. **Convert to Uppercase**: Convert each line to uppercase. Outputs There is no return value. The function should create or modify files as described in the task. Constraints - You must use the `fileinput` module for reading and writing files. - Your solution needs to handle large files efficiently. - You can assume that the provided list of files is valid and non-empty. Example ```python # Assume \'file1.txt\' contains: # Hello World # # This is a test file. # # and \'file2.txt\' contains: # Python Programming # # is fun! # # Usage: process_files([\'file1.txt\', \'file2.txt\'], in_place=True) # After calling the function, \'file1.txt\' should be modified to: # HELLO WORLD # THIS IS A TEST FILE. # # and \'file2.txt\' should be modified to: # PYTHON PROGRAMMING # IS FUN! process_files([\'file1.txt\', \'file2.txt\'], in_place=False) # This should create \'file1_processed.txt\' and \'file2_processed.txt\' with the same contents as described above. ``` Ensure that your implementation efficiently handles file reading and writing operations, and test it to confirm that it behaves as expected for different scenarios.","solution":"import fileinput import os import shutil def process_files(files, in_place=False, backup_ext=\'.bak\', encoding=\'utf-8\', compress=False): def process_line(line): return line.strip().upper() if line.strip() else None if in_place: for file in files: with fileinput.FileInput(file, inplace=True, backup=backup_ext, mode=\'r\', encoding=encoding) as f: for line in f: processed_line = process_line(line) if processed_line is not None: print(processed_line) else: for file in files: base, ext = os.path.splitext(file) output_file = base + \\"_processed\\" + ext with open(output_file, \'w\', encoding=encoding) as out_f: with fileinput.FileInput(file, mode=\'r\', encoding=encoding) as f: for line in f: processed_line = process_line(line) if processed_line is not None: out_f.write(processed_line + \'n\')"},{"question":"# Question: **Title:** Implementing a Custom Generic Container with Type Hinting **Problem Statement:** You are required to implement a custom generic container class `MyContainer` in Python that simulates type hinting using the `GenericAlias` functionality described in the documentation. The container should hold elements of any type but should enforce the type when adding and retrieving items. This simulation helps to understand how type hinting can be applied dynamically using Python\'s type system. Your task is to: 1. Implement the class `MyContainer` which accepts a type parameter during initialization. 2. Ensure that the container can only accept items of the specified type. 3. Provide methods to add an item to the container and retrieve an item from the container, while enforcing the type checks. 4. Use the type hinting provided by the `GenericAlias`. **Function Signatures:** ```python class MyContainer: def __init__(self, elem_type: type): Initializes the container with the specified element type. :param elem_type: The type of elements that the container will hold. def add_item(self, item): Adds an item to the container, enforcing type check. :param item: The item to add, which must be of type `elem_type`. :raises TypeError: If the item is not of type `elem_type`. def get_item(self, index: int): Retrieves an item from the container by index. :param index: The index of the item to retrieve. :return: The item at the specified index. ``` **Example Usage:** ```python int_container = MyContainer(int) int_container.add_item(1) # Should work int_container.add_item(\\"a\\") # Should raise TypeError str_container = MyContainer(str) str_container.add_item(\\"hello\\") # Should work ``` **Constraints:** - Enforce type checking strictly. - Use the `GenericAlias` feature to implement the type checking mechanism. - Provide necessary error handling for type mismatches. **Additional Notes:** - You can assume only basic types will be used (e.g., int, str, etc.). - Aim for a clear, concise implementation and ensure your code passes all example cases provided.","solution":"class MyContainer: def __init__(self, elem_type: type): Initializes the container with the specified element type. :param elem_type: The type of elements that the container will hold. self.elem_type = elem_type self.items = [] def add_item(self, item): Adds an item to the container, enforcing type check. :param item: The item to add, which must be of type `elem_type`. :raises TypeError: If the item is not of type `elem_type`. if not isinstance(item, self.elem_type): raise TypeError(f\\"Item must be of type {self.elem_type.__name__}\\") self.items.append(item) def get_item(self, index: int): Retrieves an item from the container by index. :param index: The index of the item to retrieve. :return: The item at the specified index. return self.items[index]"},{"question":"Student Grades Analysis You are tasked with writing a program that takes a list of student grades and performs various operations to summarize and analyze the data. Specifically, you\'ll need to implement functions to: 1. **Normalize Grades**: Normalize the grades so that they fall within the range of 0 to 100. If a grade is below 0, set it to 0. If a grade is above 100, set it to 100. 2. **Grade Summary**: Create a summary of grades, showing the number of students who received grades in specific ranges. The ranges are: 0-59, 60-69, 70-79, 80-89, and 90-100. 3. **Top N Students**: Identify the top N students with the highest grades. If there is a tie, include all students with the same grade in the result. Function Specifications: 1. `normalize_grades(grades: List[float]) -> List[float]` - **Input**: A list of float numbers representing student grades. - **Output**: A list of float numbers where each grade is normalized to be within the range 0 to 100. 2. `grade_summary(grades: List[float]) -> Dict[str, int]` - **Input**: A list of float numbers representing student grades. - **Output**: A dictionary with the following keys and corresponding counts: - \'0-59\': Number of grades in the range of 0 to 59. - \'60-69\': Number of grades in the range of 60 to 69. - \'70-79\': Number of grades in the range of 70 to 79. - \'80-89\': Number of grades in the range of 80 to 89. - \'90-100\': Number of grades in the range of 90 to 100. 3. `top_n_students(grades: List[float], n: int) -> List[float]` - **Input**: A list of float numbers representing student grades and an integer N. - **Output**: A list of float numbers representing the top N grades, sorted in descending order. If there is a tie, include all students with the same grade. Constraints: - The input grades list will have at least one grade. - Grades can be positive, negative, or beyond the 100 mark. - The value of N will be a positive integer not greater than the number of students. Example: ```python grades = [105, 85, 67, -3, 95, 73, 46, 88, 92, 88] n = 3 normalized_grades = normalize_grades(grades) # normalized_grades should be [100, 85, 67, 0, 95, 73, 46, 88, 92, 88] summary = grade_summary(normalized_grades) # summary should be {\'0-59\': 2, \'60-69\': 1, \'70-79\': 1, \'80-89\': 3, \'90-100\': 3} top_students = top_n_students(normalized_grades, n) # top_students should be [100, 95, 92, 88, 88] ``` Implement the functions within a single Python file and ensure the solution handles the constraints efficiently.","solution":"from typing import List, Dict def normalize_grades(grades: List[float]) -> List[float]: return [min(max(grade, 0), 100) for grade in grades] def grade_summary(grades: List[float]) -> Dict[str, int]: summary = {\'0-59\': 0, \'60-69\': 0, \'70-79\': 0, \'80-89\': 0, \'90-100\': 0} for grade in grades: if grade <= 59: summary[\'0-59\'] += 1 elif grade <= 69: summary[\'60-69\'] += 1 elif grade <= 79: summary[\'70-79\'] += 1 elif grade <= 89: summary[\'80-89\'] += 1 else: summary[\'90-100\'] += 1 return summary def top_n_students(grades: List[float], n: int) -> List[float]: sorted_grades = sorted(grades, reverse=True) top_grades = sorted_grades[:n] if len(sorted_grades) > n: last_top_grade = top_grades[-1] top_grades.extend(grade for grade in sorted_grades[n:] if grade == last_top_grade) return top_grades"},{"question":"Objective Create a custom XML parser using the `xml.sax.xmlreader` module that processes XML data incrementally and extracts specific information from the XML structure. Problem Statement You are tasked with building a custom SAX parser that parses an XML document containing information about a collection of books. Each book has a title, author, publication year, and ISBN number. The XML document is too large to be processed in one go, so your parser should handle the data incrementally. Implement a class `BookIncrementalParser` inheriting from `xml.sax.xmlreader.IncrementalParser` which processes the XML document in chunks and extracts a list of dictionaries where each dictionary contains details of a book. Requirements 1. **Input Format**: - The parser will accept chunks of XML data as strings. 2. **Output Format**: - The output should be a list of dictionaries, where each dictionary is structured as follows: ```python { \'title\': \'Book Title\', \'author\': \'Author Name\', \'year\': 2022, \'isbn\': \'1234567890\' } ``` 3. **Constraints**: - The parser should utilize the `feed`, `close`, and `reset` methods to handle the XML data incrementally. - Implement at least the `startElement`, `endElement`, and `characters` methods from the `ContentHandler` interface to manage the SAX events. 4. **Performance**: - Your implementation should efficiently handle XML data in chunks without running into memory issues. Example Usage ```python from xml.sax import make_parser from xml.sax.xmlreader import InputSource from xml.sax.handler import ContentHandler class BookIncrementalParser(xml.sax.xmlreader.IncrementalParser, ContentHandler): # Implement the required methods based on the SAX parser structure pass # Example usage: parser = BookIncrementalParser() input_data = [ \'<books>\', \'<book><title>Book One</title><author>Author A</author><year>2021</year><isbn>1234567890</isbn></book>\', \'<book><title>Book Two</title><author>Author B</author><year>2022</year><isbn>0987654321</isbn></book>\', \'</books>\' ] for chunk in input_data: parser.feed(chunk) parser.close() books = parser.get_books() print(books) # Expected output: [{\'title\': \'Book One\', \'author\': \'Author A\', \'year\': 2021, \'isbn\': \'1234567890\'}, {\'title\': \'Book Two\', \'author\': \'Author B\', \'year\': 2022, \'isbn\': \'0987654321\'}] ``` Instructions 1. Implement the `BookIncrementalParser` class inheriting from `xml.sax.xmlreader.IncrementalParser`. 2. Ensure that your parser correctly handles the SAX events to extract book details. 3. Provide a method `get_books` that returns the list of dictionaries containing book details. 4. Test your implementation with various XML data chunks to ensure it handles incremental parsing correctly.","solution":"import xml.sax from xml.sax.handler import ContentHandler class BookIncrementalParser(xml.sax.xmlreader.IncrementalParser, ContentHandler): def __init__(self): super().__init__() self.books = [] self.current_book = {} self.current_element = \\"\\" self.content = \\"\\" self.parser = xml.sax.make_parser() self.parser.setContentHandler(self) def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def reset(self): self.books = [] self.current_book = {} self.current_element = \\"\\" self.content = \\"\\" def get_books(self): return self.books def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.current_book = {} def endElement(self, name): if name == \\"book\\": self.books.append(self.current_book) elif name in [\\"title\\", \\"author\\", \\"year\\", \\"isbn\\"]: if name == \\"year\\": self.current_book[name] = int(self.content) else: self.current_book[name] = self.content self.content = \\"\\" def characters(self, content): if self.current_element: self.content += content"},{"question":"**Assignment: Complex Data Selection Using pandas** You are provided with a dataset representing sales data for a retail store. This dataset contains information about transactions carried out over a particular timeframe. # Dataset Description: - `transaction_id`: Unique identifier for each transaction. - `date`: Date of the transaction. - `customer_id`: Unique identifier for each customer. - `amount`: Amount of money spent in the transaction. - `product`: Name of the product purchased. - `quantity`: Quantity of the product purchased. # Instructions: Implement a function `filter_sales_data(data: pd.DataFrame) -> pd.DataFrame` that performs the following operations on the input DataFrame `data`: 1. **Filter by Date:** - Select transactions that occurred between `2021-01-01` and `2021-12-31` inclusive. 2. **Spending Threshold:** - Exclude transactions where the amount spent is less than 10. 3. **Product Selection:** - From the remaining data, select transactions that involve only the products \'Laptop\', \'Smartphone\', or \'Tablet\'. 4. **Top Customers:** - Identify the top 5 customers who have spent the most in total during the specified timeframe. - Return only the transactions involving these top 5 customers. # Function Signature: ```python import pandas as pd def filter_sales_data(data: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass ``` # Constraints: - Use `pandas` DataFrame indexing and selection methods (`loc`, `iloc`, boolean indexing, `query`, etc.) as covered in the provided documentation. - Ensure that the function is efficient and handles large datasets within a reasonable time. # Example: Given the following sample DataFrame: ```python data = pd.DataFrame({ \'transaction_id\': [1, 2, 3, 4, 5, 6], \'date\': [\'2021-01-01\', \'2021-05-15\', \'2021-07-20\', \'2021-11-30\', \'2022-01-10\', \'2021-03-25\'], \'customer_id\': [101, 102, 101, 103, 104, 105], \'amount\': [50, 200, 5, 150, 300, 20], \'product\': [\'Laptop\', \'Smartphone\', \'Keyboard\', \'Laptop\', \'Tablet\', \'Smartphone\'], \'quantity\': [1, 2, 1, 1, 3, 1] }) ``` The function `filter_sales_data(data)` should return: ```python # Assuming the example data set is larger and includes multiple transactions per customer, # this will be an example result based on the described criteria. ``` **Note:** The provided example and expected outcome may not cover all edge cases. Write your implementation to handle general cases as described in the instructions.","solution":"import pandas as pd def filter_sales_data(data: pd.DataFrame) -> pd.DataFrame: # Step 1: Filter by Date filtered_data = data[(data[\'date\'] >= \'2021-01-01\') & (data[\'date\'] <= \'2021-12-31\')] # Step 2: Exclude transactions where the amount spent is less than 10 filtered_data = filtered_data[filtered_data[\'amount\'] >= 10] # Step 3: Select transactions that involve only the products \'Laptop\', \'Smartphone\', or \'Tablet\' filtered_data = filtered_data[filtered_data[\'product\'].isin([\'Laptop\', \'Smartphone\', \'Tablet\'])] # Step 4: Identify the top 5 customers who have spent the most in total during the specified timeframe top_customers = filtered_data.groupby(\'customer_id\')[\'amount\'].sum().nlargest(5).index # Return only the transactions involving these top 5 customers result = filtered_data[filtered_data[\'customer_id\'].isin(top_customers)] return result"},{"question":"**Advanced Pandas Coding Question** **Objective:** The goal of this exercise is to assess your understanding of advanced pandas functionalities, including data manipulation, selection, aggregation, and merging. **Task:** You are given two datasets in CSV format that record information about employees and their performance reviews respectively. 1. **employees.csv**: - **Columns**: - `employee_id` (unique identifier for each employee) - `first_name` - `last_name` - `department` - `salary` 2. **reviews.csv**: - **Columns**: - `review_id` (unique identifier for each review) - `employee_id` (identifier to which employee the review is for) - `review_date` (date of the review) - `rating` (rating score out of 10) **Requirements:** 1. Load both datasets into pandas DataFrames. 2. Merge the two datasets on `employee_id`. 3. Perform the following operations: - Filter out reviews with a rating below 5. - Create a new DataFrame that shows the average rating per `department`. - For each `department`, calculate the correlation between the `salary` and the `rating`. - Create a pivot table showing the average rating by `first_name` and `last_name`. 4. Write the final DataFrame with the average rating per department to a new CSV file named `avg_ratings_per_department.csv`. **Constraints:** - Assume both CSV files exist in the current directory. - Ensure your solution is efficient and handles large datasets gracefully. **Input:** - `employees.csv` - `reviews.csv` **Output:** - `avg_ratings_per_department.csv` **Implementation:** ```python import pandas as pd # Load the datasets employees = pd.read_csv(\'employees.csv\') reviews = pd.read_csv(\'reviews.csv\') # Merge the datasets merged_df = pd.merge(employees, reviews, on=\'employee_id\') # Filter out reviews with a rating below 5 filtered_df = merged_df[merged_df[\'rating\'] >= 5] # Create a DataFrame that shows the average rating per department avg_ratings = filtered_df.groupby(\'department\')[\'rating\'].mean().reset_index() # For each department, calculate the correlation between the salary and the rating correlations = {} for department, group in filtered_df.groupby(\'department\'): correlations[department] = group[\'salary\'].corr(group[\'rating\']) # Create a pivot table showing the average rating by first_name and last_name pivot_table = pd.pivot_table(filtered_df, values=\'rating\', index=[\'first_name\', \'last_name\'], aggfunc=\'mean\') # Write the average rating per department to a CSV file avg_ratings.to_csv(\'avg_ratings_per_department.csv\', index=False) # (Optional) Print the results print(\\"Average ratings per department:\\") print(avg_ratings) print(\\"nCorrelations between salary and rating per department:\\") print(correlations) print(\\"nPivot table of average ratings by first and last name:\\") print(pivot_table) ``` Ensure your code handles possible missing values and edge cases effectively.","solution":"import pandas as pd def process_employee_reviews(): # Load the datasets employees = pd.read_csv(\'employees.csv\') reviews = pd.read_csv(\'reviews.csv\') # Merge the datasets on `employee_id` merged_df = pd.merge(employees, reviews, on=\'employee_id\') # Filter out reviews with a rating below 5 filtered_df = merged_df[merged_df[\'rating\'] >= 5] # Create a DataFrame that shows the average rating per `department` avg_ratings_per_department = filtered_df.groupby(\'department\')[\'rating\'].mean().reset_index() # For each `department`, calculate the correlation between `salary` and `rating` correlations = {} for department, group in filtered_df.groupby(\'department\'): correlations[department] = group[\'salary\'].corr(group[\'rating\']) # Create a pivot table showing the average rating by `first_name` and `last_name` pivot_table = pd.pivot_table(filtered_df, values=\'rating\', index=[\'first_name\', \'last_name\'], aggfunc=\'mean\') # Write the average rating per department to a new CSV file avg_ratings_per_department.to_csv(\'avg_ratings_per_department.csv\', index=False) return avg_ratings_per_department, correlations, pivot_table if __name__ == \\"__main__\\": avg_ratings_per_department, correlations, pivot_table = process_employee_reviews() print(\\"Average ratings per department:n\\", avg_ratings_per_department) print(\\"nCorrelations between salary and rating per department:n\\", correlations) print(\\"nPivot table of average ratings by first and last name:n\\", pivot_table)"},{"question":"# Seaborn Heatmap Customization Challenge You are tasked with analyzing a dataset and producing a well-customized heatmap using the `seaborn` library. The dataset contains the scores of different models on various tasks. Your goal is to create a heatmap that includes several customizations to make the visualization informative and clear. Requirements: 1. **Load the Dataset**: - Load the built-in `glue` dataset from `seaborn` and pivot it such that models are the rows, tasks are the columns, and values are the scores. 2. **Heatmap Customization**: - Create a heatmap with the following customizations: - Annotate the heatmap with the scores formatted to one decimal place. - Use a colormap of your choice that enhances the readability of the heatmap. - Add lines between the cells of the heatmap for better separation. - Normalize the colormap such that the minimum value corresponds to 50 and the maximum to 100. 3. **Additional Customization**: - Modify the axes labels to remove the default labels. - Position the x-axis at the top of the heatmap instead of the bottom. 4. **Output**: - Display the final heatmap. **Input**: - No direct input is required from the user; you will use the built-in `seaborn` dataset `glue`. **Output**: - A seaborn heatmap with the specified customizations. Constraints: - Use only `seaborn` and `pandas` libraries for data manipulation and visualization. - Ensure that the heatmap is properly formatted and visually informative. Example: Your final code should look something like this: ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn sns.set_theme() # Load and pivot the dataset glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create the heatmap with the specified customizations ax = sns.heatmap(glue, annot=True, fmt=\\".1f\\", cmap=\\"crest\\", linewidth=.5, vmin=50, vmax=100) # Further customize the appearance ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the heatmap plt.show() ``` Ensure your code implements all the required customizations and produces a clear, informative heatmap.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_heatmap(): Creates and displays a seaborn heatmap of the \'glue\' dataset with specified customizations. # Set the theme for seaborn sns.set_theme() # Load and pivot the dataset glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create the heatmap with the specified customizations ax = sns.heatmap(glue, annot=True, fmt=\\".1f\\", cmap=\\"crest\\", linewidths=.5, vmin=50, vmax=100) # Further customize the appearance ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the heatmap plt.show()"},{"question":"HTTP and IP Manipulation Task **Objective:** Your task is to write a Python script that performs the following: 1. **Fetch Data from a URL:** - Use the `urllib.request` module to make an HTTP GET request to a given web URL. - Parse and extract all IPv4 addresses from the text/data returned by the URL. 2. **Process the IP Addresses:** - Use the `ipaddress` module to validate the extracted IP addresses. - Filter out and retain only the valid IPv4 addresses. - Classify these IP addresses into different network classes (A, B, C, D, or E). 3. **Output the Results:** - Print the list of valid IPv4 addresses. - Print the count of IPs in each network class. **Function Specifications:** - Function Name: `fetch_and_classify_ips` - **Input:** - `url` (str): The URL from which to fetch data. - **Output:** - A tuple containing: - A list of valid IPv4 addresses (List of strings). - A dictionary with the count of IPs in each network class {\'A\': int, \'B\': int, \'C\': int, \'D\': int, \'E\': int}. **Constraints:** - Assume the input string can contain any text with potential embedded IP addresses. - The function should handle HTTP errors gracefully. **Example Usage:** ```python url = \\"http://example.com/data\\" valid_ips, ip_classes_count = fetch_and_classify_ips(url) # Sample Output # valid_ips = [\'192.168.0.1\', \'10.0.0.1\'] # ip_classes_count = {\'A\': 1, \'B\': 0, \'C\': 1, \'D\': 0, \'E\': 0} ``` **Notes:** 1. Use regular expressions to find potential IP addresses in the fetched data. 2. Handle exceptions in cases where data cannot be fetched from the URL by returning empty lists/dictionaries. 3. Remember to use appropriate methods from the `urllib.request` and `ipaddress` modules as needed. Good luck, and happy coding!","solution":"import urllib.request import re import ipaddress def fetch_and_classify_ips(url): try: # Fetch data from the URL response = urllib.request.urlopen(url) data = response.read().decode(\'utf-8\') # Extract potential IPs using regex potential_ips = re.findall(r\'b(?:[0-9]{1,3}.){3}[0-9]{1,3}b\', data) valid_ips = [] ip_classes_count = {\'A\': 0, \'B\': 0, \'C\': 0, \'D\': 0, \'E\': 0} # Validate IPs and classify for ip in potential_ips: try: ip_obj = ipaddress.IPv4Address(ip) valid_ips.append(ip) # Determine the network class first_octet = int(ip.split(\'.\')[0]) if 1 <= first_octet <= 126: ip_classes_count[\'A\'] += 1 elif 128 <= first_octet <= 191: ip_classes_count[\'B\'] += 1 elif 192 <= first_octet <= 223: ip_classes_count[\'C\'] += 1 elif 224 <= first_octet <= 239: ip_classes_count[\'D\'] += 1 elif 240 <= first_octet <= 255: ip_classes_count[\'E\'] += 1 except ipaddress.AddressValueError: # Invalid IP addresses are skipped continue return valid_ips, ip_classes_count except urllib.error.URLError: # In case of a connection error, return empty results return [], {\'A\': 0, \'B\': 0, \'C\': 0, \'D\': 0, \'E\': 0}"},{"question":"<|Analysis Begin|> The provided documentation for the Python codec registry and support functions can be broken down into several key components: 1. **Codec Registration and Lookup**: - Functions to register (`PyCodec_Register`) and unregister (`PyCodec_Unregister`) codec search functions. - Querying known encodings with `PyCodec_KnownEncoding`. - Encoding and decoding operations with `PyCodec_Encode` and `PyCodec_Decode`. 2. **Codec Lookup API**: - Functions to retrieve encoder (`PyCodec_Encoder`), decoder (`PyCodec_Decoder`), incremental encoder (`PyCodec_IncrementalEncoder`), incremental decoder (`PyCodec_IncrementalDecoder`), stream reader (`PyCodec_StreamReader`), and stream writer (`PyCodec_StreamWriter`) for a given encoding. 3. **Registry API for Unicode Encoding Error Handlers**: - Functions to register (`PyCodec_RegisterError`) and lookup (`PyCodec_LookupError`) error handling callbacks. - Specific error handling mechanisms like strict errors (`PyCodec_StrictErrors`), ignoring errors (`PyCodec_IgnoreErrors`), replacement errors (`PyCodec_ReplaceErrors`), XML character reference replacement (`PyCodec_XMLCharRefReplaceErrors`), backslash replace errors (`PyCodec_BackslashReplaceErrors`), and name replace errors (`PyCodec_NameReplaceErrors`). Based on this, we can design a coding challenge that involves the creation, registration, and usage of custom encoding error handlers and encoding/decoding routines. <|Analysis End|> <|Question Begin|> **Custom Encoding and Error Handling in Python** In this exercise, you are required to demonstrate your understanding of the codec registry and support functions, particularly focusing on encoding and decoding with custom error handling. You will need to implement a Python function that registers a custom encoding error handler and then uses it to encode and decode a given string. # Instructions 1. **Custom Error Handler**: - Implement a custom error handling callback named `custom_error_handler` that will: - Replace any unencodable characters with the string `\\"!!ERROR!!\\"`. - Return the modified string and the position where encoding should continue. 2. **Register Error Handler**: - Register the `custom_error_handler` using `PyCodec_RegisterError` under the name \\"custom\\". 3. **Encoding and Decoding**: - Implement a function `custom_encode_decode` that: - Takes a string `input_str` and an encoding `encoding` as input. - Encodes the `input_str` using the specified `encoding` and your custom error handler. - Decodes the encoded string back using the same `encoding`. 4. **Function Signature**: ```python def custom_error_handler(exc: UnicodeError) -> tuple: # Your implementation here def custom_encode_decode(input_str: str, encoding: str) -> str: # Your implementation here ``` # Example Usage ```python # Register your custom error handler custom_error_handler_registration = custom_error_handler(UnicodeEncodeError) print(custom_encode_decode(\\"Hello, world! 😀\\", \\"ascii\\")) # Output should be: \'Hello, world! !!ERROR!!\' print(custom_encode_decode(\\"Python 3.10\\", \\"utf-8\\")) # Output should be: \'Python 3.10\' ``` # Constraints - The `custom_error_handler` should handle `UnicodeEncodeError` and `UnicodeDecodeError`. - You may assume the encoding used will always be either \'ascii\' or \'utf-8\'. - Your implementation should use the appropriate functions from the provided codec registry and support functions documentation. - Ensure your error handler is registered only once and reused for subsequent encoding/decoding operations. # Performance Requirements - The solution should handle input strings up to 10,000 characters efficiently. - The custom error handler should be optimized to minimize string operations. Please write your solution keeping these constraints and requirements in mind.","solution":"import codecs def custom_error_handler(exc): Custom error handling callback. Replace unencodable characters with \'!!ERROR!!\'. if isinstance(exc, UnicodeEncodeError): return \'!!ERROR!!\', exc.start + 1 else: return \'\', exc.end # Register the custom error handler codecs.register_error(\'custom\', custom_error_handler) def custom_encode_decode(input_str, encoding): Encode the input_str using the specified encoding and custom error handler. Decode it back using the same encoding. encoded_str = input_str.encode(encoding, errors=\'custom\') decoded_str = encoded_str.decode(encoding, errors=\'custom\') return decoded_str"},{"question":"# PyTorch Advanced Configuration Challenge **Objective:** You are tasked with designing a system that configures and manages the behavior of module parameters during the conversion of models in PyTorch. Specifically, you need to implement a class that uses the `torch.__future__` functions to set and get conversion settings, and also to demonstrate its practical use in a model conversion scenario. **Task:** 1. Implement a class `PyTorchFutureConfig` that has methods to: - Set the overwrite module parameters conversion setting. - Get the current overwrite module parameters conversion setting. - Set the swap module parameters conversion setting. - Get the current swap module parameters conversion setting. 2. Demonstrate the use of this class in a scenario where you set both settings before a model conversion and get the settings after conversion. **Specifications:** 1. **Class Definition:** - The class should be named `PyTorchFutureConfig`. - It should have the following methods: ```python def set_overwrite_params(self, value: bool) -> None: Sets the overwrite module parameters conversion setting. :param value: Boolean indicating whether to overwrite module parameters on conversion. def get_overwrite_params(self) -> bool: Gets the current overwrite module parameters conversion setting. :return: Boolean value of the current setting. def set_swap_params(self, value: bool) -> None: Sets the swap module parameters conversion setting. :param value: Boolean indicating whether to swap module parameters on conversion. def get_swap_params(self) -> bool: Gets the current swap module parameters conversion setting. :return: Boolean value of the current setting. ``` 2. **Model Conversion Scenario:** - Create a simple PyTorch model or use a pre-defined one. - Utilize the `PyTorchFutureConfig` class to set the configurations for overwrite and swap settings. - Simulate a model conversion process (this can be a dummy conversion as the focus is on the configuration part). - Retrieve and print the conversion settings after the conversion. **Example:** ```python class PyTorchFutureConfig: def set_overwrite_params(self, value: bool) -> None: import torch.__future__ torch.__future__.set_overwrite_module_params_on_conversion(value) def get_overwrite_params(self) -> bool: import torch.__future__ return torch.__future__.get_overwrite_module_params_on_conversion() def set_swap_params(self, value: bool) -> None: import torch.__future__ torch.__future__.set_swap_module_params_on_conversion(value) def get_swap_params(self) -> bool: import torch.__future__ return torch.__future__.get_swap_module_params_on_conversion() # Example usage: config = PyTorchFutureConfig() config.set_overwrite_params(True) config.set_swap_params(False) # Simulate model conversion # [Conversion logic here - focus is on configuration management] print(\\"Overwrite Params Setting:\\", config.get_overwrite_params()) print(\\"Swap Params Setting:\\", config.get_swap_params()) ``` **Constraints:** - You should not implement the actual model conversion but just a placeholder for it. - Ensure that your class methods interact correctly with the `torch.__future__` module functions. **Performance Requirements:** - The methods in the `PyTorchFutureConfig` class should efficiently call and retrieve the current settings without any unnecessary overhead.","solution":"class PyTorchFutureConfig: def set_overwrite_params(self, value: bool) -> None: import torch.__future__ torch.__future__.set_overwrite_module_params_on_conversion(value) def get_overwrite_params(self) -> bool: import torch.__future__ return torch.__future__.get_overwrite_module_params_on_conversion() def set_swap_params(self, value: bool) -> None: import torch.__future__ torch.__future__.set_swap_module_params_on_conversion(value) def get_swap_params(self) -> bool: import torch.__future__ return torch.__future__.get_swap_module_params_on_conversion() # Example usage: config = PyTorchFutureConfig() config.set_overwrite_params(True) config.set_swap_params(False) # Simulate model conversion # [Conversion logic here - focus is on configuration management] print(\\"Overwrite Params Setting:\\", config.get_overwrite_params()) print(\\"Swap Params Setting:\\", config.get_swap_params())"},{"question":"# Question: You are tasked with designing a Python utility that can encode and decode data using various encoding schemes provided by the `base64` module. The utility should be capable of handling multiple encoding methods and should provide functionality to encode or decode data from a file and write the result to another file. **Requirements:** 1. Implement a class named `Base64Utility` with the following methods: - `encode_to_file(self, input_file: str, output_file: str, encoding: str, **kwargs)`: This method should read binary data from `input_file`, encode it using the specified `encoding` method (e.g., \'b64\', \'b32\', \'b16\', \'a85\', \'b85\'), and write the encoded data to `output_file`. - `decode_to_file(self, input_file: str, output_file: str, encoding: str, **kwargs)`: This method should read encoded data from `input_file`, decode it using the specified `encoding` method, and write the decoded binary data to `output_file`. - `encode(self, data: bytes, encoding: str, **kwargs) -> bytes`: This method should encode the given binary `data` using the specified `encoding` method and return the encoded bytes. - `decode(self, data: bytes, encoding: str, **kwargs) -> bytes`: This method should decode the given encoded `data` using the specified `encoding` method and return the decoded bytes. **Input:** - `input_file` and `output_file` in `encode_to_file` and `decode_to_file` are paths to the respective files. - `data` in `encode` and `decode` is a bytes object. - `encoding` is a string specifying the encoding method. Accepted values are \'b64\', \'urlsafe_b64\', \'standard_b64\', \'b32\', \'b32hex\', \'b16\', \'a85\', and \'b85\'. - `**kwargs` can include additional parameters such as `altchars`, `validate`, `casefold`, `map01`, `foldspaces`, `wrapcol`, `pad`, `adobe`, and `ignorechars` as applicable to the specific encoding or decoding method. **Output:** - The `encode` method returns the encoded bytes. - The `decode` method returns the decoded bytes. - The `encode_to_file` method writes the encoded data to `output_file`. - The `decode_to_file` method writes the decoded data to `output_file`. **Constraints:** - Assume the input data files are in a valid format and properly padded for decoding. - Assume that any optional parameters passed via `**kwargs` are appropriate for the encoding/decoding function being called. - Raise appropriate exceptions if the encoding method is not supported or if any required parameters are missing. **Example:** ```python utility = Base64Utility() # Encoding and decoding data encoded_data = utility.encode(b\'data to encode\', \'b64\') decoded_data = utility.decode(encoded_data, \'b64\') # Encoding and decoding from file utility.encode_to_file(\'input.bin\', \'output.txt\', \'b64\') utility.decode_to_file(\'output.txt\', \'decoded.bin\', \'b64\') ``` **Performance Requirements:** - The implemented methods should efficiently handle files up to 100MB in size and should properly manage memory usage during read and write operations. Implement the `Base64Utility` class to meet the above requirements.","solution":"import base64 class Base64Utility: ENCODING_MAP = { \'b64\': (base64.b64encode, base64.b64decode), \'urlsafe_b64\': (base64.urlsafe_b64encode, base64.urlsafe_b64decode), \'standard_b64\': (base64.standard_b64encode, base64.standard_b64decode), \'b32\': (base64.b32encode, base64.b32decode), \'b32hex\': (base64.b32hexencode, base64.b32hexdecode), \'b16\': (base64.b16encode, base64.b16decode), \'a85\': (base64.a85encode, base64.a85decode), \'b85\': (base64.b85encode, base64.b85decode), } def encode(self, data: bytes, encoding: str, **kwargs) -> bytes: if encoding not in self.ENCODING_MAP: raise ValueError(f\\"Unsupported encoding: {encoding}\\") encode_func = self.ENCODING_MAP[encoding][0] return encode_func(data, **kwargs) def decode(self, data: bytes, encoding: str, **kwargs) -> bytes: if encoding not in self.ENCODING_MAP: raise ValueError(f\\"Unsupported encoding: {encoding}\\") decode_func = self.ENCODING_MAP[encoding][1] return decode_func(data, **kwargs) def encode_to_file(self, input_file: str, output_file: str, encoding: str, **kwargs): with open(input_file, \'rb\') as infile: data = infile.read() encoded_data = self.encode(data, encoding, **kwargs) with open(output_file, \'wb\') as outfile: outfile.write(encoded_data) def decode_to_file(self, input_file: str, output_file: str, encoding: str, **kwargs): with open(input_file, \'rb\') as infile: data = infile.read() decoded_data = self.decode(data, encoding, **kwargs) with open(output_file, \'wb\') as outfile: outfile.write(decoded_data)"},{"question":"# Email Parsing Exception Handling You are tasked with implementing a robust email parsing function that uses custom exception handling. This function will parse an email message and raise appropriate exceptions when specific issues are encountered. Task: Implement a function `parse_email` that takes an email message as input and performs the following: 1. **Check for Boundary Issues**: - If an email message claims to be multipart but has no boundary parameter, raise a `NoBoundaryInMultipartDefect` exception. - If the start boundary specified in the Content-Type header is not found in the message body, raise a `StartBoundaryNotFoundDefect` exception. 2. **Header Issues**: - If the first header line is a continuation line, raise a `FirstHeaderLineIsContinuationDefect` exception. - If a line in the header section is found without a leading whitespace and without a colon, raise a `MissingHeaderBodySeparatorDefect` exception. 3. **Base64 Decoding Issues**: - If base64 decoding finds characters outside the base64 alphabet, raise an `InvalidBase64CharactersDefect` exception. - If base64 decoding finds incorrect padding, raise an `InvalidBase64PaddingDefect` exception. 4. **Add appropriate custom messages** to each exception to describe the exact issue. Input: - A string `email_message` representing the raw email content. Output: - The function should return \\"Parsing completed\\" if the email is successfully parsed without any issues. - The function should raise appropriate exceptions with meaningful messages if any of the specified defects are encountered. Constraints: - Assume that the email content string passed to the function is not empty. - Do not use any third-party libraries for email parsing. Here is the function signature: ```python def parse_email(email_message: str) -> str: # your code here ``` Example: ```python email_message = Content-Type: multipart/mixed; boundary=\\"boundary\\" From: sender@example.com To: recipient@example.com --boundary Content-Type: text/plain Hello, this is the body of the email. --boundary Content-Type: application/pdf Content-Disposition: attachment; filename=\\"file.pdf\\" JVBERi0xLjQKJeLjz9MKMyAwIG9iago8PC9MZW5ndGggNzAvRmlsdGVy.... --boundary-- print(parse_email(email_message)) # Output: Parsing completed faulty_email_message = Content-Type: multipart/mixed From: sender@example.com To: recipient@example.com --boundary Content-Type: text/plain Hello, this is the body of the email. --boundary Content-Type: application/pdf Content-Disposition: attachment; filename=\\"file.pdf\\" JVBERi0xLjQKJeLjz9MKMyAwIG9iago8PC9MZW5ndGggNzAvRmlsdGVy.... --boundary-- print(parse_email(faulty_email_message)) # Raises: NoBoundaryInMultipartDefect(\\"The email claims to be multipart but has no boundary parameter.\\") ```","solution":"class NoBoundaryInMultipartDefect(Exception): pass class StartBoundaryNotFoundDefect(Exception): pass class FirstHeaderLineIsContinuationDefect(Exception): pass class MissingHeaderBodySeparatorDefect(Exception): pass class InvalidBase64CharactersDefect(Exception): pass class InvalidBase64PaddingDefect(Exception): pass def parse_email(email_message: str) -> str: import base64 def check_headers(headers): # Check if the first header line is a continuation line if headers[0].startswith((\' \', \'t\')): raise FirstHeaderLineIsContinuationDefect( \\"The first header line is a continuation line.\\" ) for header in headers: # Check for missing colon in header lines if \':\' not in header and not header.startswith((\' \', \'t\')): raise MissingHeaderBodySeparatorDefect( \\"A line in the header section is missing a colon.\\" ) def check_multipart_boundary(content_type_header, body): if \\"boundary=\\" not in content_type_header: raise NoBoundaryInMultipartDefect( \\"The email claims to be multipart but has no boundary parameter.\\" ) boundary = content_type_header.split(\\"boundary=\\")[1].strip().strip(\'\\"\') if f\\"--{boundary}\\" not in body: raise StartBoundaryNotFoundDefect( \\"The specified boundary was not found in the message body.\\" ) headers, body = email_message.split(\\"nn\\", 1) headers = headers.split(\\"n\\") # Checking headers check_headers(headers) content_type_header = None for header in headers: if header.lower().startswith(\\"content-type:\\"): content_type_header = header break if content_type_header and \\"multipart\\" in content_type_header: check_multipart_boundary(content_type_header, body) # Check base64 encoding issues assuming the content-type is base64 lines = body.split(\\"n\\") for line in lines: if line.startswith(\'--\'): continue try: base64.b64decode(line, validate=True) except base64.binascii.Error as e: if \\"non-valid characters\\" in str(e): raise InvalidBase64CharactersDefect( \\"Base64 decoding found invalid characters.\\" ) if \\"Incorrect padding\\" in str(e): raise InvalidBase64PaddingDefect( \\"Base64 decoding found incorrect padding.\\" ) return \\"Parsing completed\\""},{"question":"# Question: Implement a POP3 Email Client Your task is to implement a function `fetch_emails(host, username, password)` that connects to a given POP3 mail server, authenticates using the provided username and password, and retrieves the latest email message. The function should also handle potential errors gracefully and print an appropriate message for each error scenario. Requirements: 1. Use the `poplib.POP3` class to establish a connection to the server. 2. Authenticate the user with `user` and `pass_` methods. 3. Retrieve the latest email message using the `retr` method. 4. Print all the lines of the latest email message. 5. Include error handling for the following scenarios: - Network issues (e.g., cannot connect to the server) - Authentication failures (e.g., incorrect username or password) - POP3 protocol errors (e.g., `poplib.error_proto`) 6. Ensure the mailbox is unlocked and the connection is closed properly before the function exits using the `quit` method. Function Signature: ```python def fetch_emails(host: str, username: str, password: str) -> None: pass ``` Example: ```python fetch_emails(\'pop.example.com\', \'user@example.com\', \'password123\') ``` This should output the content of the latest email message or an appropriate error message if one of the specified error scenarios occurs. Constraints: - Assume the email server supports the basic POP3 protocol on port 110. - Use only the standard `poplib` module without external libraries. - Handle edge cases where there might be no messages in the mailbox.","solution":"import poplib import sys def fetch_emails(host: str, username: str, password: str) -> None: try: # Connect to the server pop_conn = poplib.POP3(host) pop_conn.getwelcome() # Authenticate pop_conn.user(username) pop_conn.pass_(password) # Get the number of messages num_messages = len(pop_conn.list()[1]) if num_messages == 0: print(\\"No emails found.\\") else: # Retrieve the latest email response, lines, octets = pop_conn.retr(num_messages) for line in lines: print(line.decode(\'utf-8\')) # Disconnect from the server pop_conn.quit() except poplib.error_proto as e: print(f\\"POP3 protocol error: {str(e)}\\") except Exception as e: print(f\\"Error: {str(e)}\\")"},{"question":"# Constructing a MIME Email with Multiple Attachments **Problem Statement:** You are required to write a function `construct_email` that constructs a MIME email with multiple types of attachments using the `email.mime` package. The function should exhibit the use of various `email.mime` classes such as `MIMEText`, `MIMEImage`, `MIMEApplication`, and `MIMEMultipart`. **Function Signature:** ```python def construct_email(subject: str, sender: str, recipient: str, text_body: str, image_path: str, application_data: bytes) -> email.message.Message: pass ``` **Input:** - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `text_body` (str): The text content of the email. - `image_path` (str): The file path of an image to attach. - `application_data` (bytes): The binary data for an application file to attach. **Output:** - Returns an `email.message.Message` object representing the constructed MIME email. **Constraints:** - The email should be constructed as a `MIMEMultipart` message with at least: - A `MIMEText` part for the text body. - A `MIMEImage` part for the attached image. - A `MIMEApplication` part for the attached application data. - The email should have appropriate headers including `Subject`, `From`, and `To`. **Details:** - Use `MIMEMultipart` for the main email message. - Use `MIMEText` for the text body of the email. - Use `MIMEImage` for the image attachment. You can use Python\'s `imghdr` module to help determine the image type. - Use `MIMEApplication` for the application data attachment. - Ensure that the appropriate encoding is applied to each MIME part. **Example Usage:** ```python email_message = construct_email( subject=\\"Project Update\\", sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", text_body=\\"Here is the latest update on the project.\\", image_path=\\"/path/to/image.jpg\\", application_data=b\\"x00x01x02x03\\" ) # The email_message object can now be used to send the email using smtplib or other email sending libraries. ``` Your function will be evaluated on correctness and adherence to MIME standards described in the `email.mime` package documentation. **Note:** - This problem assumes that you have prior knowledge of how to read image files in binary mode and handling file paths in Python. - Ensure to handle exceptions where necessary, such as file not found for images.","solution":"import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders def construct_email(subject: str, sender: str, recipient: str, text_body: str, image_path: str, application_data: bytes): # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Attach the text body msg.attach(MIMEText(text_body, \'plain\')) # Attach the image with open(image_path, \'rb\') as img_file: img_data = img_file.read() img_type = os.path.splitext(image_path)[1][1:] # Extract the image type mime_image = MIMEImage(img_data, _subtype=img_type) mime_image.add_header(\'Content-Disposition\', \'attachment\', filename=os.path.basename(image_path)) msg.attach(mime_image) # Attach the application data mime_application = MIMEApplication(application_data) mime_application.add_header(\'Content-Disposition\', \'attachment\', filename=\'application.bin\') msg.attach(mime_application) return msg"},{"question":"**Objective:** The goal of this question is to assess your understanding and application of the `grp` module in Python for accessing the Unix group database. **Problem Statement:** You are required to write a Python function that analyzes the Unix group database and provides specific details about groups and their members. The function must: 1. List all groups that have no members. 2. List the number of groups each user belongs to. 3. List all groups sorted by their `gr_gid` in ascending order. **Function Signature:** ```python def analyze_group_database() -> dict: Returns a dictionary with the following keys and values: - \'groups_without_members\': A list of group names with no members. - \'user_group_counts\': A dictionary where keys are usernames and values are the number of groups the user belongs to. - \'groups_sorted_by_gid\': A list of tuples, each containing the group name and gid, sorted by gid in ascending order. pass ``` **Expected Output:** The function should return a dictionary with the required information. **Constraints:** - Use the `grp` module to interact with the Unix group database. - The function should handle cases where no groups or users exist. **Example Output:** ```python { \\"groups_without_members\\": [\\"nogroup\\", \\"adm\\"], \\"user_group_counts\\": {\\"root\\": 1, \\"user1\\": 2, \\"user2\\": 3}, \\"groups_sorted_by_gid\\": [(\\"staff\\", 50), (\\"users\\", 100), (\\"admins\\", 101)] } ``` **Guidelines:** 1. Utilize the `grp.getgrall()` function to retrieve all group entries. 2. Iterate through the group entries to process and gather the necessary information. 3. Ensure your function handles possible edge cases and errors gracefully. Good luck!","solution":"import grp def analyze_group_database(): Returns a dictionary with the following keys and values: - \'groups_without_members\': A list of group names with no members. - \'user_group_counts\': A dictionary where keys are usernames and values are the number of groups the user belongs to. - \'groups_sorted_by_gid\': A list of tuples, each containing the group name and gid, sorted by gid in ascending order. group_entries = grp.getgrall() groups_without_members = [group.gr_name for group in group_entries if not group.gr_mem] user_group_counts = {} for group in group_entries: for member in group.gr_mem: if member in user_group_counts: user_group_counts[member] += 1 else: user_group_counts[member] = 1 groups_sorted_by_gid = sorted([(group.gr_name, group.gr_gid) for group in group_entries], key=lambda x: x[1]) return { \'groups_without_members\': groups_without_members, \'user_group_counts\': user_group_counts, \'groups_sorted_by_gid\': groups_sorted_by_gid, }"},{"question":"Coding Assessment Question **Objective:** You are required to demonstrate your understanding of the `os` module in Python for handling environment variables. This includes creating, modifying, and using these variables to control execution flow. **Problem Statement:** Implement a Python function called `environment_manager` that will: 1. Set a new environment variable \\"MY_VAR\\" to the value \\"HelloWorld\\". 2. Retrieve the value of \\"MY_VAR\\" and log it. 3. Modify the value of \\"MY_VAR\\" to \\"PythonRocks\\". 4. Delete \\"MY_VAR\\" from the environment variables. Then, for the purpose of verifying your function, write a `main` function that: 1. Checks if the environment variable \\"MY_VAR\\" exists at every step. 2. Logs the results at each step, including the initial state (before any operations), the setting of the variable, the modification, and the deletion. **Input and Output Formats:** - There is no input for the `environment_manager` function. - The function should output logs with the following information at each step: 1. Initial environment state with respect to \\"MY_VAR\\". 2. State after setting \\"MY_VAR\\". 3. Retrieved value after setting \\"MY_VAR\\". 4. State after modifying \\"MY_VAR\\". 5. Retrieved value after modifying \\"MY_VAR\\". 6. State after deleting \\"MY_VAR\\". **Constraints and Requirements:** - You must use the `os` module to handle environment variables. - The function should gracefully handle the absence of \\"MY_VAR\\" during initial checks and ensure no exceptions are raised unnecessarily. - Your solution will be tested in a Unix-like environment where environment variable values are represented as strings. **Performance Requirements:** - The solution should efficiently manage environment variables with minimal performance overhead. ```python import os def environment_manager(): # Check initial state of \\"MY_VAR\\" initial_state = os.environ.get(\\"MY_VAR\\") print(f\\"Initial state of MY_VAR: {initial_state}\\") # Set \\"MY_VAR\\" to \\"HelloWorld\\" os.environ[\\"MY_VAR\\"] = \\"HelloWorld\\" value_after_setting = os.environ.get(\\"MY_VAR\\") print(f\\"State after setting MY_VAR: {value_after_setting}\\") # Retrieve and log the value of \\"MY_VAR\\" retrieved_value = os.environ[\\"MY_VAR\\"] print(f\\"Retrieved value after setting MY_VAR: {retrieved_value}\\") # Modify \\"MY_VAR\\" to \\"PythonRocks\\" os.environ[\\"MY_VAR\\"] = \\"PythonRocks\\" value_after_modifying = os.environ.get(\\"MY_VAR\\") print(f\\"State after modifying MY_VAR: {value_after_modifying}\\") # Retrieve and log the modified value of \\"MY_VAR\\" modified_value = os.environ[\\"MY_VAR\\"] print(f\\"Retrieved value after modifying MY_VAR: {modified_value}\\") # Delete \\"MY_VAR\\" del os.environ[\\"MY_VAR\\"] state_after_deletion = os.environ.get(\\"MY_VAR\\") print(f\\"State after deleting MY_VAR: {state_after_deletion}\\") def main(): # Execute the environment_manager function to verify the expected behavior environment_manager() if __name__ == \\"__main__\\": main() ``` **Note:** Ensure that your function works correctly by testing it in a shell environment where you have permissions to modify environment variables.","solution":"import os def environment_manager(): Manages environment variable \\"MY_VAR\\" by setting, modifying, retrieving, and deleting it. Logs the state at each step. # Check initial state of \\"MY_VAR\\" initial_state = os.environ.get(\\"MY_VAR\\") print(f\\"Initial state of MY_VAR: {initial_state}\\") # Set \\"MY_VAR\\" to \\"HelloWorld\\" os.environ[\\"MY_VAR\\"] = \\"HelloWorld\\" value_after_setting = os.environ.get(\\"MY_VAR\\") print(f\\"State after setting MY_VAR: {value_after_setting}\\") # Retrieve and log the value of \\"MY_VAR\\" retrieved_value = os.environ[\\"MY_VAR\\"] print(f\\"Retrieved value after setting MY_VAR: {retrieved_value}\\") # Modify \\"MY_VAR\\" to \\"PythonRocks\\" os.environ[\\"MY_VAR\\"] = \\"PythonRocks\\" value_after_modifying = os.environ.get(\\"MY_VAR\\") print(f\\"State after modifying MY_VAR: {value_after_modifying}\\") # Retrieve and log the modified value of \\"MY_VAR\\" modified_value = os.environ[\\"MY_VAR\\"] print(f\\"Retrieved value after modifying MY_VAR: {modified_value}\\") # Delete \\"MY_VAR\\" del os.environ[\\"MY_VAR\\"] state_after_deletion = os.environ.get(\\"MY_VAR\\") print(f\\"State after deleting MY_VAR: {state_after_deletion}\\") def main(): # Execute the environment_manager function to verify the expected behavior environment_manager() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Challenge: Persistance using Python C API **Objective:** Demonstrate your understanding of the Python marshalling functions to serialize and deserialize Python objects using the C API exposed functions in Python 3.10. You will write a Python wrapper for these C functions. **Task:** You are required to implement a Python module `marshaller` that provides the following functions: 1. **`write_object_to_file(obj: Any, filename: str, version: int) -> None`**: Serialize the given Python object `obj` to a file with name `filename` using the specified version. 2. **`read_object_from_file(filename: str) -> Any`**: Deserialize and return the Python object stored in the file `filename`. 3. **`write_object_to_bytes(obj: Any, version: int) -> bytes`**: Serialize the given Python object `obj` to a bytes object using the specified version. 4. **`read_object_from_bytes(data: bytes) -> Any`**: Deserialize and return the Python object from the provided binary data. **Input:** - For `write_object_to_file`: - `obj`: Any valid Python object. - `filename`: A string representing the path where the object will be stored. - `version`: An integer (0, 1, or 2) indicating the version of the marshalling format. - For `read_object_from_file`: - `filename`: A string representing the path from which the object will be read. - For `write_object_to_bytes`: - `obj`: Any valid Python object. - `version`: An integer (0, 1, or 2) indicating the version of the marshalling format. - For `read_object_from_bytes`: - `data`: A byte string containing the serialized object. **Output:** - For `write_object_to_file`: Write the given object to the specified file. - For `read_object_from_file`: Return the deserialized Python object from the specified file. - For `write_object_to_bytes`: Return the object serialized as a bytes object. - For `read_object_from_bytes`: Return the deserialized Python object from the given bytes object. **Constraints:** 1. The C code must handle errors and raise appropriate Python exceptions (`EOFError`, `ValueError`, `TypeError`) on failures. 2. The files involved should be operated on in binary mode. 3. The wrapper functions must be implemented using the aforementioned C API functions for marshalling. **Performance Requirements:** - The functions should be efficient in terms of both time and memory usage. Consider using `PyMarshal_ReadLastObjectFromFile` where applicable for optimized memory usage. **Implementation Notes:** - You may need to use the `ctypes` or a similar library to load and use the C functions within Python. - Ensure to include necessary error handling to capture and raise Python exceptions when things go wrong. - Document and test your module thoroughly. **Example Usage:** ```python import marshaller # Example object to serialize example_obj = {\'key\': \'value\', \'num\': 42, \'list\': [1, 2, 3]} # Serialization to file marshaller.write_object_to_file(example_obj, \'example.dat\', version=2) # Deserialization from file obj_from_file = marshaller.read_object_from_file(\'example.dat\') print(obj_from_file) # Serialization to bytes obj_bytes = marshaller.write_object_to_bytes(example_obj, version=2) # Deserialization from bytes obj_from_bytes = marshaller.read_object_from_bytes(obj_bytes) print(obj_from_bytes) ```","solution":"import marshal def write_object_to_file(obj, filename, version): Serialize the given Python object to a file with name filename using the specified version. with open(filename, \'wb\') as file: marshal.dump(obj, file, version) def read_object_from_file(filename): Deserialize and return the Python object stored in the file filename. with open(filename, \'rb\') as file: return marshal.load(file) def write_object_to_bytes(obj, version): Serialize the given Python object to a bytes object using the specified version. return marshal.dumps(obj, version) def read_object_from_bytes(data): Deserialize and return the Python object from the provided binary data. return marshal.loads(data)"},{"question":"Objective: Demonstrate your proficiency in using PyTorch\'s Export IR to create and manipulate a computational graph. Question: You are given a PyTorch `nn.Module` which performs a basic computation. Your task is to: 1. Export the module\'s computational graph using `torch.export.export`. 2. Modify the resulting graph to insert a new operation. 3. Ensure that the modified graph is valid and can be executed. Specifications: 1. **Input:** A PyTorch `nn.Module` performing a basic computation (e.g., addition, multiplication). 2. **Output:** The modified `ExportedProgram` including the additional operation. Requirements: 1. Define and export the initial model using `torch.export.export`. 2. Insert a new operation (e.g., `torch.ops.aten.mul.Tensor`) into the graph after the initial operation. 3. Ensure the graph maintains its validity and can be executed. Constraints: - You must use `torch.export.export` to export the module\'s graph. - The new operation should be inserted appropriately in the sequence of operations. - Preserve the original inputs and outputs while adding the new operation. Example: ```python import torch from torch import nn # Define the initial module class InitialModule(nn.Module): def forward(self, x, y): return x + y # Define a function to perform the required tasks def modify_and_export_module(module, example_args): # Export the initial module exported_program = torch.export.export(module, example_args) # Access the graph and its nodes graph = exported_program.graph # Insert a new multiplication operation after the addition with graph.inserting_after(graph.nodes[2]): # Assuming the addition is node[2] new_node = graph.create_node(\'call_function\', torch.ops.aten.mul.Tensor, args=(graph.nodes[2], graph.nodes[1])) # Modify the final output node to include the new multiplication result output_node = list(graph.nodes)[-1] output_node.args = (new_node,) return exported_program # Instantiate the module and example inputs module = InitialModule() example_args = (torch.randn(1), torch.randn(1)) # Call the function to modify and export the module modified_exported_program = modify_and_export_module(module, example_args) # Print the modified graph print(modified_exported_program.graph) ``` Ensure that your solution demonstrates the understanding and proper manipulation of the `ExportedProgram` and its graph in PyTorch.","solution":"import torch from torch import nn from torch.onnx import export from torch.fx import symbolic_trace # Define the initial module class InitialModule(nn.Module): def forward(self, x, y): return x + y def modify_and_export_module(module, example_args): Export the module\'s computational graph, modify it to include a new multiplication operation, and return the modified graph. # Export the module\'s computational graph using PyTorch FX tracing traced_graph = symbolic_trace(module) # Get the nodes from the graph graph_nodes = list(traced_graph.graph.nodes) # Lookup for the addition node, which is the second node (first is input) add_node = graph_nodes[2] # Insert a new multiplication node after the addition node with traced_graph.graph.inserting_after(add_node): new_node = traced_graph.graph.create_node( \'call_function\', torch.mul, args=(add_node, graph_nodes[1]), # Multiply the addition result by y name=\'mul_node\' ) # Modify the output node to include the new multiplication result output_node = graph_nodes[-1] output_node.args = (new_node,) # Recompile the modified graph traced_graph.recompile() return traced_graph # Instantiate the module and example inputs module = InitialModule() example_args = (torch.randn(1), torch.randn(1)) # Call the function to modify and export the module modified_graph = modify_and_export_module(module, example_args) # Function to execute the modified graph def execute_modified_graph(graph, *inputs): return graph.forward(*inputs) # Print the modified graph print(modified_graph)"},{"question":"Title: Evaluating Regression Models with Scikit-learn Objective: You are tasked with implementing and comparing the performance of different regression models from `scikit-learn` to predict the target values of a given dataset. You will need to apply Ordinary Least Squares (OLS) Regression, Ridge Regression, and Lasso Regression, then evaluate their performance using appropriate metrics. Dataset: For this assessment, use the built-in `diabetes` dataset provided by `scikit-learn`: ```python from sklearn.datasets import load_diabetes data = load_diabetes() X, y = data.data, data.target ``` Instructions: 1. **Data Preparation:** - Split the dataset into training and testing sets using an 80-20 split. 2. **Model Implementation:** - Implement the following regression models: - Ordinary Least Squares (OLS) Regression - Ridge Regression (with `alpha=1.0`) - Lasso Regression (with `alpha=0.1`) - Ensure to use `scikit-learn`\'s regression classes for each model: - `LinearRegression` for OLS - `Ridge` for Ridge Regression - `Lasso` for Lasso Regression 3. **Model Training:** - Fit each model on the training set. 4. **Model Evaluation:** - Predict the target values for the testing set using each model. - Calculate and print the following performance metrics for each model: - Mean Squared Error (MSE) - Coefficient of Determination (R²) 5. **Code Implementation:** - Ensure your code is organized, properly commented, and adheres to best practices. Expected Input and Output: - **Input:** - You are not required to take any extra input from the user. Use the provided dataset directly in your script. - **Output:** - Print the Mean Squared Error (MSE) and the R² score for each model in a tabular format. - Example: ``` Model | Mean Squared Error | R^2 Score ------------------------------------------------- OLS | 2903.53 | 0.51 Ridge | 2910.62 | 0.51 Lasso | 3027.34 | 0.49 ``` Constraints: - You must use the `scikit-learn` library for model implementation and evaluation. - Ensure reproducibility of results by setting a random state wherever applicable. Performance Considerations: - The performance will be primarily evaluated based on the accuracy of the models and the correctness of the implementation. - Ensure that the code executes in a reasonable time frame. Good luck with your assessment!","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error, r2_score def evaluate_regression_models(): # Load the dataset data = load_diabetes() X, y = data.data, data.target # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the regression models models = { \'OLS\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=0.1) } # Train and evaluate each model results = [] for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) results.append((name, mse, r2)) # Print the results in tabular format print(f\\"{\'Model\':<10} | {\'Mean Squared Error\':<18} | {\'R^2 Score\':<10}\\") print(\\"-\\" * 47) for name, mse, r2 in results: print(f\\"{name:<10} | {mse:<18.2f} | {r2:<10.2f}\\") # Execute the function to print the results evaluate_regression_models()"},{"question":"**Objective**: Implement a custom logging system using the Python `logging` module. The implementation should demonstrate understanding of loggers, handlers, and formatters. **Description**: You need to create a Python module that sets up a logging system with the following specifications: 1. **Logger Setup**: - Create a logger named `customLogger` at the root level. - The logger should log messages at the `DEBUG` level. 2. **Handlers**: - Add a `StreamHandler` that logs messages to the console. The messages should be formatted to show the log level, logger name, and message. - Add a `FileHandler` that logs messages to a file named `application.log`. It should rotate logs after the file reaches 1MB, keeping up to 3 backup files. - Add a `TimedRotatingFileHandler` that rotates logs every midnight, keeping logs for up to 7 days. 3. **Formatters**: - The `StreamHandler` should format messages as `LEVEL - LOGGER_NAME - MESSAGE`. - The `FileHandler` should include the date and time in the format `YYYY-MM-DD HH:MM:SS - MESSAGE`. - The `TimedRotatingFileHandler` should use the default format. **Input and Output**: - There should be no function inputs required. - The module should log messages at different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) to all handlers showing that the handlers and formatters are working as intended. **Performance Requirements**: - Ensure the logging system is efficient and capable of handling multiple log entries without significant performance degradation. **Constraints**: - Use only the standard `logging` module for this task. - Ensure that the log file and backup files do not exceed the specified sizes and durations. **Example Output**: ```plaintext DEBUG - customLogger - This is a debug message. INFO - customLogger - This is an info message. WARNING - customLogger - This is a warning message. ERROR - customLogger - This is an error message. CRITICAL - customLogger - This is a critical message. ``` **File Output in `application.log`**: ```plaintext 2023-10-01 10:00:00 - This is a debug message. 2023-10-01 10:00:01 - This is an info message. 2023-10-01 10:00:02 - This is a warning message. 2023-10-01 10:00:03 - This is an error message. 2023-10-01 10:00:04 - This is a critical message. ``` **Implementation**: ```python import logging from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler # Create logger logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # Create console handler and set level to debug console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler which logs debug messages to a file and rotates after 1MB file_handler = RotatingFileHandler(\'application.log\', maxBytes=1 * 1024 * 1024, backupCount=3) file_handler.setLevel(logging.DEBUG) # Create timed rotating file handler which rotates daily and keeps logs for 7 days timed_handler = TimedRotatingFileHandler(\'timed_application.log\', when=\'midnight\', interval=1, backupCount=7) timed_handler.setLevel(logging.DEBUG) # Create formatters and add them to handlers console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') file_formatter = logging.Formatter(\'%(asctime)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(timed_handler) # Log messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` **Task**: 1. Implement the above logging setup in a Python module named `custom_logging.py`. 2. Run the logging commands to ensure that the console output and log files display the messages with the specified formats. 3. Verify that the `RotatingFileHandler` respects the size limits and the `TimedRotatingFileHandler` respects the time-based rotation and retention period.","solution":"import logging from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler def setup_custom_logger(): # Create logger logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # Create console handler and set level to debug console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler which logs debug messages to a file and rotates after 1MB file_handler = RotatingFileHandler( \'application.log\', maxBytes=1 * 1024 * 1024, backupCount=3) file_handler.setLevel(logging.DEBUG) # Create timed rotating file handler which rotates daily and keeps logs for 7 days timed_handler = TimedRotatingFileHandler( \'timed_application.log\', when=\'midnight\', interval=1, backupCount=7) timed_handler.setLevel(logging.DEBUG) # Create formatters and add them to handlers console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') file_formatter = logging.Formatter(\'%(asctime)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(timed_handler) return logger # Instantiate custom logger logger = setup_custom_logger() # Log messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\')"},{"question":"Objective Implement a custom PyTorch transformation using `torch.fx`, where the goal is to replace every instance of a `torch.nn.ReLU` activation in a model with a custom-defined function `custom_relu`. Problem Statement You are provided with a base PyTorch model that may contain several ReLU activation functions. Your task is to implement a transformation function `replace_relu_with_custom_relu` that replaces all ReLU activations with a custom activation function `custom_relu`. This custom ReLU should perform an element-wise maximum between each element in the input tensor and zero, followed by element-wise addition of a small number (epsilon) to ensure the output is never exactly zero. Requirements: 1. Implement the function `custom_relu(x)` that follows the behavior described. 2. Implement the function `replace_relu_with_custom_relu(model: torch.nn.Module) -> torch.nn.Module` that uses `torch.fx` to replace all instances of `torch.ReLU` in the model with `custom_relu`. 3. Ensure that the transformed model produces outputs that incorporate the behavior of `custom_relu` instead of `torch.nn.ReLU`. Input The input to your function will be a PyTorch model (`torch.nn.Module`) which may have multiple ReLU activations. Output The output should be a transformed PyTorch model with all ReLU activations replaced by `custom_relu`. Constraints 1. Your implementation should appropriately handle nested modules. 2. The input model can have any architecture including CNNs, RNNs, or Transformers. Performance Your solution should maintain the original model structure and ensure minimal runtime overhead beyond the custom activation function. Example Usage ```python import torch import torch.fx from torch.fx import Tracer class BaseModel(torch.nn.Module): def __init__(self): super(BaseModel, self).__init__() self.layer1 = torch.nn.Linear(10, 10) self.relu = torch.nn.ReLU() self.layer2 = torch.nn.Linear(10, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) return self.layer2(x) def custom_relu(x): epsilon = 1e-6 return torch.max(x, torch.tensor(0.0)) + epsilon def replace_relu_with_custom_relu(model: torch.nn.Module) -> torch.nn.Module: # Implement the graph transformation here class CustomTracer(Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: if isinstance(m, torch.nn.ReLU): return True return super().is_leaf_module(m, module_qualified_name) def transform(m: torch.nn.Module, tracer_class: type = CustomTracer) -> torch.nn.Module: graph: torch.fx.Graph = tracer_class().trace(m) for node in graph.nodes: if node.op == \'call_module\' and isinstance(getattr(m, node.target), torch.nn.ReLU): node.replace_all_uses_with(graph.call_function(custom_relu, (node.args[0],))) graph.lint() return torch.fx.GraphModule(m, graph) return transform(model) # Instantiate and use the transformed model base_model = BaseModel() transformed_model = replace_relu_with_custom_relu(base_model) # Test transformed model on sample data input_data = torch.rand(1, 10) output_data = transformed_model(input_data) print(output_data) ``` Notes - Ensure you test the transformed model both functionally and perform validation checks. - Pay attention to preserve the original model operations outside the scope of directly activating ReLU.","solution":"import torch import torch.fx from torch.fx import Tracer def custom_relu(x): Custom ReLU activation function. Returns element-wise maximum between x and 0, plus a small epsilon. epsilon = 1e-6 return torch.max(x, torch.tensor(0.0)) + epsilon def replace_relu_with_custom_relu(model: torch.nn.Module) -> torch.nn.Module: Replaces all instances of torch.nn.ReLU in a model with custom_relu. Args: - model (torch.nn.Module): The input model containing ReLU activations. Returns: - torch.nn.Module: Transformed model with custom ReLU activations. class CustomTracer(Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: if isinstance(m, torch.nn.ReLU): return True return super().is_leaf_module(m, module_qualified_name) def transform(m: torch.nn.Module, tracer_class: type = CustomTracer) -> torch.nn.Module: graph: torch.fx.Graph = tracer_class().trace(m) for node in graph.nodes: if node.op == \'call_module\' and isinstance(getattr(m, node.target), torch.nn.ReLU): with graph.inserting_before(node): new_node = graph.call_function(custom_relu, args=(node.args[0],)) node.replace_all_uses_with(new_node) graph.erase_node(node) graph.lint() return torch.fx.GraphModule(m, graph) return transform(model)"},{"question":"# Question: Advanced Data Persistence with `pickle` and `sqlite3` You are tasked with creating a Python module that combines the usage of `pickle` and `sqlite3` to implement a simple yet efficient persistent storage system for Python objects. Part 1: Serialization and Deserialization 1. **Function: `serialize_object(file_path: str, obj: Any) -> None`** - **Inputs**: - `file_path`: String, the path to the file where the object will be serialized. - `obj`: Any, the Python object to serialize. - **Outputs**: None - **Description**: Serialize the given Python object and save it to the specified file using `pickle`. - **Constraints**: Ensure the function handles exceptions where the object cannot be serialized and logs an appropriate error message using Python\'s logging module. 2. **Function: `deserialize_object(file_path: str) -> Any`** - **Inputs**: - `file_path`: String, the path to the file where the serialized object is stored. - **Outputs**: Any, the deserialized Python object. - **Description**: Read the serialized object from the specified file and deserialize it using `pickle`. - **Constraints**: Ensure the function handles exceptions where the file cannot be read or the object cannot be deserialized, logging an appropriate error message. Part 2: Persistent Storage with SQLite 3. **Function: `store_object_in_db(db_path: str, obj: Any, table_name: str = \'persisted_objects\') -> None`** - **Inputs**: - `db_path`: String, the path to the SQLite database file. - `obj`: Any, the Python object to serialize and store. - `table_name`: String, the table name where the object will be stored. Default is \'persisted_objects\'. - **Outputs**: None - **Description**: Serialize the given Python object using `pickle` and store it in the specified SQLite database table. The table should have two columns: an `id` (INTEGER PRIMARY KEY) and `data` (BLOB). - **Constraints**: Ensure the table is created if it does not exist. Handle exceptions for database operations and logging appropriate error messages. 4. **Function: `retrieve_object_from_db(db_path: str, object_id: int, table_name: str = \'persisted_objects\') -> Any`** - **Inputs**: - `db_path`: String, the path to the SQLite database file. - `object_id`: Integer, the ID of the object to be retrieved. - `table_name`: String, the table name from where the object will be retrieved. Default is \'persisted_objects\'. - **Outputs**: Any, the deserialized Python object retrieved from the database. - **Description**: Retrieve the serialized object with the given ID from the SQLite database, deserialize it using `pickle`, and return the object. - **Constraints**: Handle exceptions for database operations and deserialization, logging appropriate error messages. Part 3: Performance Considerations 5. **Function: `store_large_objects_in_db(db_path: str, objects: List[Any], table_name: str = \'persisted_objects\') -> None`** - **Inputs**: - `db_path`: String, the path to the SQLite database file. - `objects`: List of Any, multiple Python objects to serialize and store. - `table_name`: String, the table name where these objects will be stored. Default is \'persisted_objects\'. - **Outputs**: None - **Description**: Serialize and store multiple Python objects in a single transaction to optimize performance and reduce I/O operations. - **Constraints**: Ensure that the operation is atomic and handles rollback in case of failure. Log all error messages appropriately. # Submission - Implement the five functions as specified. - Ensure that your code is well-documented and adheres to PEP 8 guidelines. - Provide a set of unit tests to validate the functionality of each function.","solution":"import pickle import sqlite3 import logging from typing import Any, List # Configure logging logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) def serialize_object(file_path: str, obj: Any) -> None: try: with open(file_path, \'wb\') as file: pickle.dump(obj, file) except Exception as e: logger.error(f\\"Serialization error: {e}\\") def deserialize_object(file_path: str) -> Any: try: with open(file_path, \'rb\') as file: return pickle.load(file) except Exception as e: logger.error(f\\"Deserialization error: {e}\\") return None def store_object_in_db(db_path: str, obj: Any, table_name: str = \'persisted_objects\') -> None: try: conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(f CREATE TABLE IF NOT EXISTS {table_name} ( id INTEGER PRIMARY KEY, data BLOB ) ) serialized_obj = pickle.dumps(obj) cursor.execute(f\\"INSERT INTO {table_name} (data) VALUES (?)\\", (serialized_obj,)) conn.commit() except Exception as e: logger.error(f\\"Database store error: {e}\\") finally: conn.close() def retrieve_object_from_db(db_path: str, object_id: int, table_name: str = \'persisted_objects\') -> Any: try: conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(f\\"SELECT data FROM {table_name} WHERE id = ?\\", (object_id,)) row = cursor.fetchone() if row: return pickle.loads(row[0]) else: logger.error(f\\"Object with ID {object_id} not found.\\") return None except Exception as e: logger.error(f\\"Database retrieve error: {e}\\") return None finally: conn.close() def store_large_objects_in_db(db_path: str, objects: List[Any], table_name: str = \'persisted_objects\') -> None: try: conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(f CREATE TABLE IF NOT EXISTS {table_name} ( id INTEGER PRIMARY KEY, data BLOB ) ) serialized_objects = [(pickle.dumps(obj),) for obj in objects] cursor.executemany(f\\"INSERT INTO {table_name} (data) VALUES (?)\\", serialized_objects) conn.commit() except Exception as e: logger.error(f\\"Database bulk store error: {e}\\") conn.rollback() finally: conn.close()"},{"question":"Objective The task is to write a function that processes tensors deterministically by using the `torch.utils.deterministic` module\'s `fill_uninitialized_memory` attribute. You will be given two tensor shapes and a seed value for reproducibility. Your function should create two tensors with uninitialized memory of the given shapes, fill them with known values if required, and perform an element-wise addition on these tensors. Requirements 1. Use `torch.empty` to create tensors with uninitialized memory for given shapes. 2. Ensure reproducibility using `torch.use_deterministic_algorithms()`. 3. Use `fill_uninitialized_memory` to fill the tensors with known values before performing operations. 4. Perform element-wise addition on the two tensors and return the resulting tensor. # Specifications 1. **Input:** - `shape1` (tuple of ints): The shape of the first tensor. - `shape2` (tuple of ints): The shape of the second tensor (should be broadcastable to `shape1`). - `seed` (int): The seed value to ensure reproducibility. 2. **Output:** - A tensor resulting from the element-wise addition of the two tensors. 3. **Constraints:** - Use the `torch.utils.deterministic` module to ensure deterministic operations. - Use `fill_uninitialized_memory` to fill the tensors with known values if required. - Assume `shape1` and `shape2` are always valid and broadcastable. # Example ```python def deterministic_tensor_addition(shape1, shape2, seed): # Your implementation here # Example usage tensor_add = deterministic_tensor_addition((2, 3), (3,), 42) print(tensor_add) ``` Expected output tensor will be a deterministic result based on filled uninitialized memory and the provided seed. # Additional Notes - Ensure that you use the given seed to initialize random number generation for reproducibility. - Test the function to make sure it consistently produces the same output for the same inputs.","solution":"import torch def deterministic_tensor_addition(shape1, shape2, seed): Create two tensors with uninitialized memory of the given shapes, fill them with known values if required, and perform element-wise addition. Parameters: shape1 (tuple of ints): The shape of the first tensor. shape2 (tuple of ints): The shape of the second tensor (should be broadcastable to `shape1`). seed (int): The seed value to ensure reproducibility. Returns: torch.Tensor: The resulting tensor from the element-wise addition. torch.manual_seed(seed) torch.use_deterministic_algorithms(True) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False a = torch.empty(shape1) b = torch.empty(shape2) # Optional step - fill tensors if necessary. For uninitialized memory, this step is assumed. # torch.utils.deterministic.fill_uninitialized_memory(a) # torch.utils.deterministic.fill_uninitialized_memory(b) # Assuming we are required to fill the uninitialized tensors with some known deterministic values. a.fill_(1) b.fill_(1) return a + b"},{"question":"# Python Keywords and Soft Keywords Checker Python provides built-in support to manage keywords and soft keywords through the `keyword` module. Keywords are reserved words, while soft keywords have specific meanings based on context. Your task is to implement a function `classify_words(words: List[str]) -> Dict[str, List[str]]` that takes a list of strings, `words`, and classifies each word as a keyword, soft keyword, or neither. Return a dictionary with keys \\"keywords\\", \\"soft_keywords\\", and \\"neither\\", where each key maps to a list containing the corresponding words. # Input - A list of strings, `words`, containing 1 to 1000 strings. - Each string `word` has a length between 1 and 20 characters and contains only alphabetic characters. # Output - A dictionary with three keys: \\"keywords\\", \\"soft_keywords\\", and \\"neither\\". - The value for \\"keywords\\" is a list of words that are Python keywords. - The value for \\"soft_keywords\\" is a list of words that are Python soft keywords. - The value for \\"neither\\" is a list of words that are neither keywords nor soft keywords. # Example ```python from keyword import iskeyword, kwlist, issoftkeyword, softkwlist def classify_words(words): keyword_list = [] soft_keyword_list = [] neither_list = [] for word in words: if iskeyword(word): keyword_list.append(word) elif issoftkeyword(word): soft_keyword_list.append(word) else: neither_list.append(word) return { \\"keywords\\": keyword_list, \\"soft_keywords\\": soft_keyword_list, \\"neither\\": neither_list } # Example usage: words = [\\"if\\", \\"def\\", \\"match\\", \\"loop\\"] print(classify_words(words)) # Output: # { # \\"keywords\\": [\\"if\\", \\"def\\"], # \\"soft_keywords\\": [\\"match\\"], # \\"neither\\": [\\"loop\\"] # } ``` # Constraints - The function should be efficient even with the maximum input size. - The solution must correctly leverage the `keyword` module\'s functions and lists. Implement the `classify_words` function ensuring to fulfill the requirements above.","solution":"from keyword import iskeyword, issoftkeyword def classify_words(words): Classifies words into \'keywords\', \'soft_keywords\', and \'neither\'. Args: words (list of str): List of words to classify. Returns: dict: Dictionary with keys \'keywords\', \'soft_keywords\', and \'neither\'. result = { \\"keywords\\": [], \\"soft_keywords\\": [], \\"neither\\": [] } for word in words: if iskeyword(word): result[\\"keywords\\"].append(word) elif issoftkeyword(word): result[\\"soft_keywords\\"].append(word) else: result[\\"neither\\"].append(word) return result"},{"question":"You are working with an extension module in Python that uses PyCapsule objects to encapsulate pointers to C functions. Your task is to write Python code to interact with these PyCapsule objects, ensuring their creation and proper validation. To complete this task, you need to write a Python function that will: 1. Create a PyCapsule object with a given pointer, name, and destructor. 2. Check if the created PyCapsule object is valid. 3. Retrieve the pointer, name, and destructor from the PyCapsule if it is valid. 4. Update the name, context, and pointer inside the PyCapsule object. Use the given C functions provided by the documentation to achieve this. # Function Signature ```python def manage_pycapsule(pointer: int, name: str, destructor: \\"PyCapsule_Destructor\\") -> dict: ``` # Input - `pointer`: An integer representing the pointer to be encapsulated (for this exercise, you can use any integer as a mock pointer). - `name`: A string representing the name to be associated with the capsule. - `destructor`: A mock function or `None` to act as the destructor. # Output A dictionary with the following keys and their corresponding values: - \\"is_valid\\": A boolean indicating whether the created PyCapsule is valid. - \\"pointer_value\\": The pointer retrieved from the PyCapsule. - \\"name_value\\": The name retrieved from the PyCapsule. - \\"destructor_value\\": The destructor function retrieved from the PyCapsule. # Constraints - The pointer must not be `NULL`. - The name can be `NULL`. - The destructor can be `NULL`. # Example ```python def destructor_example(capsule): print(\\"Destructor called\\") # Function usage result = manage_pycapsule(42, \\"example_module.attribute\\", destructor_example) assert result[\\"is_valid\\"] == True assert result[\\"pointer_value\\"] == 42 assert result[\\"name_value\\"] == \\"example_module.attribute\\" assert result[\\"destructor_value\\"] == destructor_example ``` # Note - You may use a mock library or dummy values to simulate interaction with the C API in Python. - Assume that the necessary functions from the Python C API are available for use within a suitable testing framework.","solution":"class MockPyCapsule: Mock PyCapsule for simulation purposes def __init__(self, pointer, name, destructor): self.pointer = pointer self.name = name self.destructor = destructor self.is_valid = pointer is not None @staticmethod def Check(capsule): return isinstance(capsule, MockPyCapsule) and capsule.is_valid def GetPointer(self): if self.is_valid: return self.pointer return None def GetName(self): if self.is_valid: return self.name return None def GetDestructor(self): if self.is_valid: return self.destructor return None def manage_pycapsule(pointer: int, name: str, destructor): Manages a PyCapsule object by creating it, validating it, and retrieving its attributes. capsule = MockPyCapsule(pointer, name, destructor) is_valid = MockPyCapsule.Check(capsule) pointer_value = capsule.GetPointer() name_value = capsule.GetName() destructor_value = capsule.GetDestructor() return { \\"is_valid\\": is_valid, \\"pointer_value\\": pointer_value, \\"name_value\\": name_value, \\"destructor_value\\": destructor_value }"},{"question":"# Pandas Sparse Data Structures: Analyzing Large Sparse Datasets Objective: You are given a large dataset containing mostly missing values (NaNs). The goal is to efficiently store and analyze this data using pandas\' sparse data structures. You need to implement functions to convert the dataset to a sparse format, perform operations on it, and convert it back to a dense format if necessary. Problem Statement: Implement the following three functions: 1. **`convert_to_sparse(df: pd.DataFrame, fill_value: float = np.nan) -> pd.DataFrame`**: - **Input**: A pandas DataFrame `df` containing mostly NaN values, and an optional `fill_value` (default is `np.nan`). - **Output**: A DataFrame converted to a sparse format using `SparseDtype` with the specified fill value. - **Constraints**: Ensure that memory usage is efficiently reduced due to sparsity. 2. **`sparse_analysis(sparse_df: pd.DataFrame) -> dict`**: - **Input**: A pandas DataFrame in sparse format. - **Output**: A dictionary containing the following analysis results: - `\'density\'`: The density of the sparse DataFrame. - `\'memory_usage\'`: A tuple with the memory usage comparison between the original dense DataFrame and the sparse DataFrame (in bytes). - **Constraints**: Ensure the density calculation accurately reflects the proportion of stored (non-fill value) data. 3. **`convert_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame`**: - **Input**: A pandas DataFrame in sparse format. - **Output**: The same DataFrame converted back to a dense format. - **Constraints**: Ensure that the conversion correctly transforms the sparse DataFrame back to a dense DataFrame with the original values restored. Example Usage: ```python import pandas as pd import numpy as np # Example Dense DataFrame data = {\'A\': [1, np.nan, np.nan, 4, np.nan], \'B\': [np.nan, 2, np.nan, np.nan, 5]} df = pd.DataFrame(data) # 1. Convert to Sparse DataFrame sparse_df = convert_to_sparse(df) print(sparse_df) # 2. Perform Analysis on Sparse DataFrame analysis_results = sparse_analysis(sparse_df) print(analysis_results) # 3. Convert Sparse DataFrame back to Dense dense_df = convert_to_dense(sparse_df) print(dense_df) ``` Notes: - The provided functions should utilize pandas\' `SparseArray`, `SparseDtype`, and `.sparse` accessor methods as needed. - Performance in terms of memory usage should be demonstrably improved when converting to sparse. - The `sparse_analysis` function should provide a clear comparison between the dense and sparse formats. Submission: Please submit the implementation of the three functions (`convert_to_sparse`, `sparse_analysis`, and `convert_to_dense`) in a single Python script or Jupyter notebook.","solution":"import pandas as pd import numpy as np def convert_to_sparse(df: pd.DataFrame, fill_value: float = np.nan) -> pd.DataFrame: Converts a DataFrame to sparse format using SparseDtype with the specified fill value. sparse_df = df.astype(pd.SparseDtype(\\"float\\", fill_value)) return sparse_df def sparse_analysis(sparse_df: pd.DataFrame) -> dict: Analyzes a sparse DataFrame and returns a dictionary with density and memory usage comparison. dense_df = sparse_df.sparse.to_dense() density = sparse_df.size / dense_df.size memory_usage_dense = dense_df.memory_usage(deep=True).sum() memory_usage_sparse = sparse_df.memory_usage(deep=True).sum() return { \'density\': density, \'memory_usage\': (memory_usage_dense, memory_usage_sparse) } def convert_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame: Converts a sparse DataFrame back to a dense format. return sparse_df.sparse.to_dense()"},{"question":"# PyTorch Coding Assessment **Objective:** Implement a function that creates a sequence of tensors, performs specific manipulations, and finally validates the results using PyTorch assert functions. **Task:** Write a function `validate_tensor_operations()` that performs the following steps: 1. Creates two tensors `a` and `b` of size `(3, 3)` with elements drawn from a normal distribution using `torch.testing.make_tensor`. 2. Adds tensor `a` to tensor `b` and stores the result in tensor `c`. 3. Creates a new tensor `expected_c` which is the element-wise sum of tensors `a` and `b`. Ensure this tensor is created manually without directly adding `a` and `b`. 4. Validates that tensor `c` and `expected_c` are close by using the `torch.testing.assert_close` function. 5. Also validate the closeness using `torch.testing.assert_allclose`. The function should not return anything but should raise an assertion error if the validation fails. **Function Signature:** ```python import torch from torch.testing import assert_close, make_tensor, assert_allclose def validate_tensor_operations(): # Step 1: Create tensors a and b a = make_tensor((3, 3), device=torch.device(\'cpu\'), dtype=torch.float) b = make_tensor((3, 3), device=torch.device(\'cpu\'), dtype=torch.float) # Step 2: Add tensors a and b to get tensor c c = a + b # Step 3: Create tensor expected_c manually expected_c = torch.zeros_like(a) for i in range(3): for j in range(3): expected_c[i, j] = a[i, j] + b[i, j] # Step 4: Validate tensor c and expected_c using assert_close assert_close(c, expected_c) # Step 5: Validate using assert_allclose assert_allclose(c, expected_c) ``` **Constraints:** - You may assume that the random values generated by `make_tensor` are within a reasonable range for standard normal distribution. - The function should not use any print statements or return values. - The function should raise an assertion error if the tensors are not close within the default tolerances. **Example:** No explicit example is necessary since this function does not return any value, and its correctness is based on passing the assertions.","solution":"import torch from torch.testing import assert_close, make_tensor, assert_allclose def validate_tensor_operations(): # Step 1: Create tensors a and b a = make_tensor((3, 3), device=torch.device(\'cpu\'), dtype=torch.float) b = make_tensor((3, 3), device=torch.device(\'cpu\'), dtype=torch.float) # Step 2: Add tensors a and b to get tensor c c = a + b # Step 3: Create tensor expected_c manually expected_c = torch.zeros_like(a) for i in range(3): for j in range(3): expected_c[i, j] = a[i, j] + b[i, j] # Step 4: Validate tensor c and expected_c using assert_close assert_close(c, expected_c) # Step 5: Validate using assert_allclose assert_allclose(c, expected_c)"},{"question":"You are tasked with developing a multi-threaded task processing system using Python\'s `queue` module. The system should use different types of queues (`Queue`, `LifoQueue`, `PriorityQueue`, and `SimpleQueue`) to manage tasks executed by multiple worker threads. Each type of queue will have specific requirements regarding how tasks are processed. **Requirements:** 1. **Task Management**: - Define a `Task` class with attributes `task_id` (string) and `priority` (integer, with lower values indicating higher priority). - Implement the `Task` class with appropriate methods for comparison based on priority. 2. **Worker Threads**: - Implement worker threads that will continuously fetch tasks from the queues and execute them. - Each worker thread should: - Print the `task_id` of the task it is processing. - Call `task_done()` on the queue after finishing the task. 3. **Queue Types**: - Instantiate and populate four different queues (`Queue`, `LifoQueue`, `PriorityQueue`, and `SimpleQueue`) with `Task` instances. - For `PriorityQueue`, ensure tasks are added as `(priority, task)` tuples. 4. **Main Program**: - Initialize each queue with a different set of tasks. - Start worker threads for each queue type. - Ensure the main program waits until all tasks in all queues are processed before exiting. **Constraints:** - Each queue should have at least 5 tasks. - Use at least 2 worker threads per queue. - Include error handling for empty queues. **Input/Output:** - The program does not take direct input. It should initialize queues and tasks within the code. - The output should display which worker is processing which task. # Example: ```python from queue import Queue, LifoQueue, PriorityQueue, SimpleQueue import threading # Define the Task class here class Task: # your code here # Define worker function here def worker(queue): # your code here # Define the main function to initialize queues and tasks def main(): # your code here if __name__ == \\"__main__\\": main() ``` Implement the `Task` class, the `worker` function, and the `main` function based on the requirements above. Ensure proper synchronization and queue management throughout the implementation.","solution":"from queue import Queue, LifoQueue, PriorityQueue, SimpleQueue, Empty import threading class Task: def __init__(self, task_id, priority=0): self.task_id = task_id self.priority = priority def __lt__(self, other): return self.priority < other.priority def __repr__(self): return f\\"Task(task_id={self.task_id}, priority={self.priority})\\" def worker(queue): while True: try: task = queue.get(timeout=1) print(f\\"Worker {threading.current_thread().name} processing {task.task_id}\\") queue.task_done() except Empty: break def main(): q1 = Queue() q2 = LifoQueue() q3 = PriorityQueue() q4 = SimpleQueue() tasks = [ Task(\\"task1\\", 1), Task(\\"task2\\", 2), Task(\\"task3\\", 3), Task(\\"task4\\", 4), Task(\\"task5\\", 5) ] for task in tasks: q1.put(task) q2.put(task) q3.put((task.priority, task)) q4.put(task) queues = [q1, q2, q3, q4] threads = [] for q in queues: for _ in range(2): t = threading.Thread(target=worker, args=(q,)) t.start() threads.append(t) for t in threads: t.join() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question: Create a Python function that handles and processes raw binary data using Python\'s buffer protocol. You are required to simulate reading from and writing to a memory buffer, similar to how C extensions would manage memory in the given old buffer protocol functions. The task involves implementing a class that can read and write to byte buffers. Class Specification: Define a class `BufferManager` with the following methods: - `__init__(self, initial_data: bytes)`: Initializes the buffer manager with the initial data provided as a bytes object. - `read(self, size: int) -> bytes`: Reads `size` bytes from the buffer and returns them. If `size` is greater than the available bytes, return all the bytes left in the buffer. - `write(self, data: bytes) -> None`: Writes the given bytes `data` to the buffer. - `get_buffer(self) -> memoryview`: Returns a memory view object of the current buffer content for efficient slicing and manipulation. Input and Output Formats: - `__init__(self, initial_data: bytes)`: `initial_data` is a bytes object containing the initial data for the buffer. - `read(self, size: int) -> bytes`: `size` is an integer indicating the number of bytes to read from the buffer. Returns a bytes object containing the read data. - `write(self, data: bytes) -> None`: `data` is a bytes object to be written to the buffer. - `get_buffer(self) -> memoryview`: Returns a memoryview object of the buffer\'s current state. Example Usage: ```python # Initialize buffer manager with initial data buffer_manager = BufferManager(initial_data=b\\"Hello, World!\\") # Read 5 bytes from the buffer print(buffer_manager.read(5)) # Output: b\'Hello\' # Write additional data to the buffer buffer_manager.write(b\\" Welcome!\\") # Get the memory view of the current buffer state buffer_view = buffer_manager.get_buffer() print(buffer_view.tobytes()) # Output: b\', World! Welcome!\' ``` Constraints: - You must handle memory efficiently using Python\'s buffer protocol. - Use `memoryview` for any operations that can benefit from memory sharing and slicing. Notes: - Focus on using Python\'s `memoryview` and buffer interface capabilities. - Ensure that the buffer handles edge cases, such as reading more bytes than available or writing empty data.","solution":"class BufferManager: def __init__(self, initial_data: bytes): self.buffer = bytearray(initial_data) self.pointer = 0 def read(self, size: int) -> bytes: if self.pointer + size > len(self.buffer): size = len(self.buffer) - self.pointer start = self.pointer end = self.pointer + size self.pointer += size return bytes(self.buffer[start:end]) def write(self, data: bytes) -> None: self.buffer.extend(data) def get_buffer(self) -> memoryview: return memoryview(self.buffer)"},{"question":"**Objective:** Implement a Python class that uses the iterator protocol, including handling of synchronous and asynchronous iterations. Problem Statement You are required to implement a Python class called `CustomIterator` that demonstrates both synchronous and asynchronous iteration protocols. The class should manage a list of elements and provide capabilities to iterate over them synchronously or asynchronously. Requirements 1. **Synchronous Iteration:** - Implement an `__iter__` method to return an iterator object. - Implement a `__next__` method to return the next item from the list. 2. **Asynchronous Iteration:** - Implement an `__aiter__` method to return an asynchronous iterator object. - Implement an `__anext__` method to return the next item from the list asynchronously. Use `await asyncio.sleep(0)` to simulate asynchrony. 3. **Class Initialization:** - The class should be initialized with a list of elements (e.g., `data`). 4. **Edge Cases:** - Handle the end of iteration by raising `StopIteration` for synchronous iteration. - Handle the end of iteration by raising `StopAsyncIteration` for asynchronous iteration. Constraints - The list of elements will have a maximum length of `1000`. - The elements within the list can be of any data type. Example Usage ```python import asyncio class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): self.index = 0 return self def __next__(self): if self.index >= len(self.data): raise StopIteration value = self.data[self.index] self.index += 1 return value async def __aiter__(self): self.index = 0 return self async def __anext__(self): if self.index >= len(self.data): raise StopAsyncIteration value = self.data[self.index] self.index += 1 await asyncio.sleep(0) return value # Example synchronous iteration it = CustomIterator([1, 2, 3]) for item in it: print(item) # Output: 1, 2, 3 # Example asynchronous iteration async def async_iter_test(): ait = CustomIterator([1, 2, 3]) async for item in ait: print(item) # Output: 1, 2, 3 asyncio.run(async_iter_test()) ``` Implement the `CustomIterator` class based on the above example. Ensure that your implementation correctly follows the synchronous and asynchronous iteration protocols. Input Format - Data provided during class initialization as a list. Output Format - The class should provide iterable objects both synchronously and asynchronously, returning elements from the list in order.","solution":"import asyncio class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): self.index = 0 return self def __next__(self): if self.index >= len(self.data): raise StopIteration value = self.data[self.index] self.index += 1 return value async def __aiter__(self): self.index = 0 return self async def __anext__(self): if self.index >= len(self.data): raise StopAsyncIteration value = self.data[self.index] self.index += 1 await asyncio.sleep(0) return value"},{"question":"**Objective:** Demonstrate your understanding of the `mailcap` module, dictionary manipulation, and security-conscious programming. **Question:** Your task is to write a Python function `safe_mailcap_command` that will use the `mailcap` module to safely return the command line associated with a given MIME type. The function should: 1. Retrieve the current mailcap entries using `mailcap.getcaps()`. 2. Use `mailcap.findmatch()` to find the appropriate command for the provided MIME type. 3. Ensure that the returned command is secure by verifying that the filename does not contain any disallowed ASCII characters. 4. Raise appropriate exceptions for the following cases: - No matching MIME type entry found. - The filename contains disallowed ASCII characters. The returned command should conform to the following: - Only alphanumeric characters and the symbols `@+=:,./-_` are allowed in the filename. - If a disallowed character is found in the filename, the function should return `None` and raise a `ValueError` exception with the message \\"Invalid filename\\". **Function Signature:** ```python import mailcap def safe_mailcap_command(MIMEtype: str, filename: str) -> str: # Your implementation here ``` **Input:** - `MIMEtype`: A string representing the MIME type (e.g., \\"video/mpeg\\"). - `filename`: A string representing the filename to be used in the command. **Output:** - A string representing the safe command line to execute. - If the operation fails due to disallowed characters in the filename, return `None` and raise a `ValueError` exception. **Constraints:** - You must use the `mailcap` module\'s `getcaps()` and `findmatch()` functions. - Do not use any additional libraries for processing. **Example:** ```python import mailcap def safe_mailcap_command(MIMEtype: str, filename: str) -> str: # Check if filename contains disallowed characters allowed_chars = set(\\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789@+=:,./-_\\") if not all(c in allowed_chars for c in filename): raise ValueError(\\"Invalid filename\\") # Retrieve mailcap entries caps = mailcap.getcaps() # Find the matching command command, _ = mailcap.findmatch(caps, MIMEtype, filename=filename) if command is None: return None return command # Example usage print(safe_mailcap_command(\'video/mpeg\', \'tmp1223\')) # \'xmpeg tmp1223\' ``` **Note:** You may encounter issues running deprecated modules in some environments; ensure your development environment supports the `mailcap` module for testing your solution.","solution":"import mailcap def safe_mailcap_command(MIMEtype: str, filename: str) -> str: Safely retrieves the command line associated with a given MIME type and filename using the mailcap module. Parameters: - MIMEtype: A string representing the MIME type (e.g., \\"video/mpeg\\"). - filename: A string representing the filename to be used in the command. Returns: - A string representing the safe command line to execute. Raises: - ValueError: If the filename contains disallowed characters or if no matching MIME type entry is found. # Check if filename contains disallowed characters allowed_chars = set(\\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789@+=:,./-_\\") if not all(c in allowed_chars for c in filename): raise ValueError(\\"Invalid filename\\") # Retrieve mailcap entries caps = mailcap.getcaps() # Find the matching command command, _ = mailcap.findmatch(caps, MIMEtype, filename=filename) if command is None: raise ValueError(\\"No matching MIME type entry found\\") return command"},{"question":"# Custom Import Finder and Loader In this coding assessment, you will demonstrate your understanding of Python’s import system by implementing a custom meta path finder and loader. Your task is to design a system that allows importing modules from a dictionary. This dictionary will map module names (as strings) to their corresponding code (also as strings). This would effectively simulate importing modules from an in-memory source rather than from the filesystem. Task 1. Implement a custom meta path finder class `DictMetaFinder` that searches for modules in a given dictionary. 2. Implement a custom loader class `DictLoader` that uses the content of the dictionary to create and load the module. 3. Register the meta path finder to modify Python\'s import system. 4. Demonstrate the functionality by importing a sample module from the dictionary. Requirements - **Input:** - A dictionary named `module_dict` where the keys are module names (e.g., `mymodule`) and the values are the module code as strings. - A module name to import from this dictionary (e.g., `mymodule`). - **Output:** - The resulting module should be imported and its functionality should be demonstrated by calling a function from the module. - **Constraints:** - The module name should be a valid Python module name. - The module code should be valid Python code. Example **Given:** ```python module_dict = { \\"mymodule\\": \'\'\' def hello(): print(\\"Hello from mymodule!\\") \'\'\' } ``` **Then, an example expected output after importing `mymodule` and calling its `hello` function:** ```python \\"Hello from mymodule!\\" ``` Implementation Steps 1. Define a class `DictMetaFinder` that implements the `find_spec` method. This method should check if a module name exists in `module_dict`. 2. Define a class `DictLoader` that implements the `exec_module` method. This method should execute the module code in a new module object. 3. Insert an instance of `DictMetaFinder` into `sys.meta_path`. 4. Write a test case to import `mymodule` from `module_dict` and invoke its `hello` function. Code Template ```python import sys import importlib.util import types # Step 1: Custom Meta Path Finder class DictMetaFinder: def __init__(self, module_dict): self.module_dict = module_dict def find_spec(self, fullname, path, target=None): if fullname in self.module_dict: return importlib.util.spec_from_loader(fullname, DictLoader(self.module_dict)) return None # Step 2: Custom Loader class DictLoader: def __init__(self, module_dict): self.module_dict = module_dict def exec_module(self, module): module_code = self.module_dict[module.__name__] exec(module_code, module.__dict__) # Step 3: Register the Meta Path Finder module_dict = { \\"mymodule\\": \'\'\' def hello(): print(\\"Hello from mymodule!\\") \'\'\' } sys.meta_path.insert(0, DictMetaFinder(module_dict)) # Step 4: Test the Custom Import System import mymodule mymodule.hello() ```","solution":"import sys import importlib.util import types # Step 1: Custom Meta Path Finder class DictMetaFinder: def __init__(self, module_dict): self.module_dict = module_dict def find_spec(self, fullname, path, target=None): if fullname in self.module_dict: return importlib.util.spec_from_loader(fullname, DictLoader(self.module_dict)) return None # Step 2: Custom Loader class DictLoader: def __init__(self, module_dict): self.module_dict = module_dict def create_module(self, spec): return types.ModuleType(spec.name) def exec_module(self, module): module_code = self.module_dict[module.__name__] exec(module_code, module.__dict__) # Step 3: Register the Meta Path Finder module_dict = { \\"mymodule\\": \'\'\' def hello(): print(\\"Hello from mymodule!\\") \'\'\' } sys.meta_path.insert(0, DictMetaFinder(module_dict)) # Step 4: Test the Custom Import System # This code demonstrates the usage and is not part of the actual solution that would be tested. if __name__ == \\"__main__\\": import mymodule mymodule.hello()"},{"question":"Objective: The goal of this assessment is to test your understanding of the Python `sys` module, specifically focusing on command-line arguments and tracing functions for debugging purposes. Task: Write a Python script that performs the following tasks: 1. **Command-Line Argument Parsing**: - The script should accept three command-line arguments: - `--name` (a string representing the user\'s name) - `--age` (an integer representing the user\'s age) - `--greet` (a boolean flag; if provided, the script should print a greeting message) 2. **Debugging Tracer Function**: - Implement a tracer function that logs each function call and return within the script. The tracer function should: - Print the function name and line number when a function is called. - Print the function name and line number when a function returns. Input: - Command-line arguments. Example: `python script.py --name Alice --age 30 --greet`. Output: - Depending on the command-line arguments, the output should include: - A greeting message if the `--greet` flag is provided. - Debugging logs for all function calls and returns. Example: For the command-line arguments `python script.py --name Alice --age 30 --greet`, the script should output: ``` Hello, Alice! Alice is 30 years old. Function call: main at line 60 Function return: main at line 80 ... ``` Constraints: - Ensure that the script handles invalid inputs gracefully. - Your tracer function should be efficient and should not cause significant performance overhead. Implementation: You should use the `sys.argv` list for parsing command-line arguments and `sys.settrace` for setting up the tracer function. Below is the initial implementation structure to help you get started: ```python import sys def tracer(frame, event, arg): if event == \'call\': print(f\\"Function call: {frame.f_code.co_name} at line {frame.f_lineno}\\") elif event == \'return\': print(f\\"Function return: {frame.f_code.co_name} at line {frame.f_lineno}\\") return tracer def main(name, age, greet): if greet: print(f\\"Hello, {name}!\\") print(f\\"{name} is {age} years old.\\") if __name__ == \\"__main__\\": # Parsing command-line arguments args = sys.argv[1:] name = None age = None greet = False for i, arg in enumerate(args): if arg == \'--name\': name = args[i + 1] elif arg == \'--age\': age = int(args[i + 1]) elif arg == \'--greet\': greet = True if name and age: sys.settrace(tracer) main(name, age, greet) sys.settrace(None) else: print(\\"Usage: script.py --name <name> --age <age> [--greet]\\") ``` Modify and complete this implementation as needed to meet the task requirements.","solution":"import sys def tracer(frame, event, arg): if event == \'call\': print(f\\"Function call: {frame.f_code.co_name} at line {frame.f_lineno}\\") elif event == \'return\': print(f\\"Function return: {frame.f_code.co_name} at line {frame.f_lineno}\\") return tracer def main(name, age, greet): if greet: print(f\\"Hello, {name}!\\") print(f\\"{name} is {age} years old.\\") if __name__ == \\"__main__\\": # Parsing command-line arguments args = sys.argv[1:] name = None age = None greet = False for i, arg in enumerate(args): if arg == \'--name\' and i + 1 < len(args): name = args[i + 1] elif arg == \'--age\' and i + 1 < len(args): try: age = int(args[i + 1]) except ValueError: print(\\"Error: --age must be followed by an integer number\\") sys.exit(1) elif arg == \'--greet\': greet = True if name and age is not None: sys.settrace(tracer) main(name, age, greet) sys.settrace(None) else: print(\\"Usage: script.py --name <name> --age <age> [--greet]\\")"},{"question":"# Advanced Serialization and Deserialization with the `pickle` Module Objective Design a class that mimics a filesystem directory structure and demonstrate custom serialization and deserialization using the `pickle` module. The class should handle pickling of directories and files, where the directories can be nested. Description You are required to implement a `Directory` class to represent a directory structure. Each directory can contain other directories and files. Then, serialize and deserialize a complex directory structure using the `pickle` module. You need to ensure that the directory and file objects are reconstructed correctly with their hierarchical relationships intact. # Class Details 1. **Directory**: - Attributes: ```python - name (str): The name of the directory. - contents (list): List of sub-Directory objects and file names (strings). ``` - Methods: ```python - add_subdirectory(subdir): Add a sub-directory (Directory object) to this directory. - add_file(filename): Add a file (string) to this directory. - list_contents(): Return a list of all immediate sub-directory names and file names in this directory. - __str__(): Provide a string representation of the directory, including subdirectories and files. ``` Task 1. Implement the `Directory` class as specified. 2. Add custom serialization (pickling) and deserialization (unpickling) methods for the `Directory` class to: - Serialize the `Directory` object to a byte stream. - Deserialize the byte stream back into a `Directory` object. 3. Demonstrate the module by creating a directory structure, serializing it to a byte stream, modifying the original structure, and then deserializing it back to confirm the integrity of the serialization process. Example Usage ```python if __name__ == \\"__main__\\": root = Directory(\\"root\\") subdir1 = Directory(\\"subdir1\\") subdir2 = Directory(\\"subdir2\\") root.add_subdirectory(subdir1) root.add_subdirectory(subdir2) subdir1.add_file(\\"file1.txt\\") subdir1.add_file(\\"file2.txt\\") subdir2.add_file(\\"fileA.txt\\") # Serialize the directory structure serialized_data = root.serialize() # Modify the directory structure subdir1.add_file(\\"newfile.txt\\") # Deserialize back to original structure deserialized_root = Directory.deserialize(serialized_data) print(deserialized_root) ``` # Constraints - Use protocol 4 for pickling. - Ensure that the serialized output is not corrupted and can be deserialized multiple times. - Handle potential errors that could occur during serialization and deserialization. # Grading Criteria - Correct implementation of the `Directory` class and its methods. - Accurate custom `serialize` and `deserialize` methods using the `pickle` module. - Demonstration of the usage example, showing the integrity of the serialized and deserialized objects.","solution":"import pickle class Directory: def __init__(self, name): self.name = name self.contents = [] def add_subdirectory(self, subdir): Adds a subdirectory to the contents. if isinstance(subdir, Directory): self.contents.append(subdir) def add_file(self, filename): Adds a file to the contents. if isinstance(filename, str): self.contents.append(filename) def list_contents(self): Lists all contents (subdirectories and files). return [item.name if isinstance(item, Directory) else item for item in self.contents] def __str__(self): Provides a string representation of the directory. def helper(directory, depth): result = \\" \\" * depth + f\\"Directory: {directory.name}n\\" for item in directory.contents: if isinstance(item, Directory): result += helper(item, depth + 1) else: result += \\" \\" * (depth + 1) + f\\"File: {item}n\\" return result return helper(self, 0) def serialize(self): Serializes the directory structure. return pickle.dumps(self, protocol=4) @staticmethod def deserialize(data): Deserializes the directory structure. return pickle.loads(data)"},{"question":"Design a function that utilizes the Kernel Ridge Regression model to fit a dataset and make predictions. Your task is to showcase an understanding of the key concepts and implementation details of Kernel Ridge Regression as provided in the documentation. # Function Signature: ```python def kernel_ridge_regression(X_train, y_train, X_test, alpha=1.0, kernel=\'linear\', gamma=None): Fit a Kernel Ridge Regression model to the training data and use it to make predictions on the test data. Parameters: - X_train (numpy.ndarray): The training data features, shape (n_samples, n_features). - y_train (numpy.ndarray): The training data targets, shape (n_samples,). - X_test (numpy.ndarray): The test data features, shape (m_samples, n_features). - alpha (float): Regularization parameter, default is 1.0. - kernel (string): The kernel to be used (\'linear\', \'poly\', \'rbf\', \'sigmoid\', \'precomputed\'), default is \'linear\'. - gamma (float): Kernel coefficient for \'rbf\', \'poly\', and \'sigmoid\'. If None, defaults to 1/n_features. Returns: - y_pred (numpy.ndarray): The predicted values for the test data, shape (m_samples,). pass ``` # Constraints: - The input arrays `X_train` and `X_test` are expected to be two-dimensional arrays of shape (n_samples, n_features) and (m_samples, n_features), respectively. - The target array `y_train` is expected to be a one-dimensional array of shape (n_samples,). - You should use the `KernelRidge` class from the `sklearn.kernel_ridge` module. - The function should handle different kernel types as specified by the `kernel` parameter. - You are required to handle the `gamma` parameter appropriately based on the kernel type. - Make sure to provide a default value for all optional parameters as per the function signature. - You should assume that the provided data arrays are well-formed and do not contain missing values. # Example: ```python import numpy as np # Example training data X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) y_train = np.array([1, 2, 3, 4]) # Example test data X_test = np.array([[2, 3], [6, 7]]) # Calling the kernel ridge regression function y_pred = kernel_ridge_regression(X_train, y_train, X_test, alpha=0.5, kernel=\'rbf\', gamma=0.1) print(\\"Predicted values:\\", y_pred) ``` In this task, you are required to: 1. Implement the `kernel_ridge_regression` function. 2. Demonstrate the function with the provided example. You may refer to the scikit-learn documentation for additional guidance on using the `KernelRidge` class.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge def kernel_ridge_regression(X_train, y_train, X_test, alpha=1.0, kernel=\'linear\', gamma=None): Fit a Kernel Ridge Regression model to the training data and use it to make predictions on the test data. Parameters: - X_train (numpy.ndarray): The training data features, shape (n_samples, n_features). - y_train (numpy.ndarray): The training data targets, shape (n_samples,). - X_test (numpy.ndarray): The test data features, shape (m_samples, n_features). - alpha (float): Regularization parameter, default is 1.0. - kernel (string): The kernel to be used (\'linear\', \'poly\', \'rbf\', \'sigmoid\', \'precomputed\'), default is \'linear\'. - gamma (float): Kernel coefficient for \'rbf\', \'poly\', and \'sigmoid\'. If None, defaults to 1/n_features. Returns: - y_pred (numpy.ndarray): The predicted values for the test data, shape (m_samples,). if gamma is None and kernel in [\'rbf\', \'poly\', \'sigmoid\']: gamma = 1.0 / X_train.shape[1] model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma) model.fit(X_train, y_train) y_pred = model.predict(X_test) return y_pred"},{"question":"**Coding Problem: Implement a Custom Serialization and Deserialization Class** # Objective Implement a class `CustomMarshaller` in Python that uses the marshal library to serialize and deserialize Python objects. This class should demonstrate your understanding of reading and writing serialized data in both files and byte strings. # Requirements 1. **Class Definition**: `CustomMarshaller` 2. **Methods**: - `serialize_to_file(self, value, file_path, version)`: This method should take a Python object (`value`), a file path (`file_path`), and an integer version (`version`). It should serialize the object and write it to the file in binary mode using the specified version format. - `deserialize_from_file(self, file_path)`: This method should take a file path (`file_path`) and deserialize the contents of the file back into a Python object. - `serialize_to_string(self, value, version)`: This method should take a Python object (`value`) and an integer version (`version`) and return the serialized object as a byte string. - `deserialize_from_string(self, byte_string)`: This method should take a byte string (`byte_string`) and return the deserialized Python object. 3. **Constraints**: - The file operations should handle exceptions gracefully, e.g., file not found, read/write errors. - Assume *version* can be `0`, `1`, or `2`. # Example Usage ```python import marshal class CustomMarshaller: def serialize_to_file(self, value, file_path, version): with open(file_path, \'wb\') as file: marshal.dump(value, file, version) def deserialize_from_file(self, file_path): with open(file_path, \'rb\') as file: return marshal.load(file) def serialize_to_string(self, value, version): return marshal.dumps(value, version) def deserialize_from_string(self, byte_string): return marshal.loads(byte_string) # Example usage marshaller = CustomMarshaller() obj = {\'a\': 1, \'b\': [1, 2, 3], \'c\': {\'nested\': \'dict\'}} # Serialize to file marshaller.serialize_to_file(obj, \'data.marshal\', 2) # Deserialize from file deserialized_obj = marshaller.deserialize_from_file(\'data.marshal\') print(deserialized_obj) # Serialize to string byte_string = marshaller.serialize_to_string(obj, 2) print(byte_string) # Deserialize from string deserialized_from_string_obj = marshaller.deserialize_from_string(byte_string) print(deserialized_from_string_obj) ``` # Notes - Ensure to import the marshal module at the beginning of your file. - Your implementation should pass provided example usage to demonstrate correctness. # Performance Requirements - The implementation should be efficient in terms of read/write operations, leveraging `marshal` built-in optimizations where applicable. # Evaluation Criteria - Correctness: Does the implementation meet the requirements and handle edge cases as described? - Code Quality: Is the code clean, well-documented, and readable? - Exception Handling: Does the implementation gracefully handle errors during file operations?","solution":"import marshal class CustomMarshaller: def serialize_to_file(self, value, file_path, version): Serializes a Python object to a file in binary mode using the specified version format. :param value: The Python object to serialize :param file_path: The file path to write the serialized object to :param version: The version format to use (0, 1, or 2) try: with open(file_path, \'wb\') as file: marshal.dump(value, file, version) except (FileNotFoundError, IOError) as e: print(f\\"Error writing to file {file_path}: {e}\\") def deserialize_from_file(self, file_path): Deserializes the contents of the file back into a Python object. :param file_path: The file path to read the serialized object from :return: The deserialized Python object try: with open(file_path, \'rb\') as file: return marshal.load(file) except (FileNotFoundError, IOError) as e: print(f\\"Error reading from file {file_path}: {e}\\") return None def serialize_to_string(self, value, version): Serializes a Python object and returns it as a byte string using the specified version format. :param value: The Python object to serialize :param version: The version format to use (0, 1, or 2) :return: The serialized object as a byte string return marshal.dumps(value, version) def deserialize_from_string(self, byte_string): Deserializes a byte string back into a Python object. :param byte_string: The byte string containing the serialized object :return: The deserialized Python object return marshal.loads(byte_string)"},{"question":"You are given two WAV files: `input1.wav` and `input2.wav`. Your task is to create a new WAV file `output.wav` containing the mixed audio of the two input files. You must implement the following function: ```python import wave def mix_wav_files(input_file1, input_file2, output_file): Mixes two WAV files and writes the result to a new WAV file. Parameters: input_file1 (str): The path to the first input WAV file. input_file2 (str): The path to the second input WAV file. output_file (str): The path to the output WAV file where the mixed audio will be saved. Returns: None pass # Your implementation here ``` # Detailed Requirements 1. **Reading Input Files**: - Open `input_file1` and `input_file2` in read mode using the `wave` module. - Ensure that both input files have the same parameters (number of channels, sample width, and frame rate). If they do not, raise a `ValueError`. 2. **Mixing Audio Data**: - Read the audio frames from both input files. - Mix the frames by averaging the sample values. Note that you may need to handle sample width when mixing the frames. 3. **Writing Output File**: - Use the parameters from the input files to create a new `Wave_write` object for `output_file`. - Write the mixed frames to `output_file`. # Constraints - The input files are guaranteed to have the same length. - The input files are guaranteed to be PCM-encoded. # Example Usage ```python mix_wav_files(\'input1.wav\', \'input2.wav\', \'output.wav\') ``` **Notes**: - Handle file closing properly, preferably using the `with` statement. - You may assume that the input WAV files are small enough to be read entirely into memory. This function should demonstrate your understanding of the `wave` module, file operations, audio processing, and handling bytes data efficiently.","solution":"import wave import struct import numpy as np def mix_wav_files(input_file1, input_file2, output_file): Mixes two WAV files and writes the result to a new WAV file. Parameters: input_file1 (str): The path to the first input WAV file. input_file2 (str): The path to the second input WAV file. output_file (str): The path to the output WAV file where the mixed audio will be saved. Returns: None with wave.open(input_file1, \'rb\') as wav1, wave.open(input_file2, \'rb\') as wav2: params1 = wav1.getparams() params2 = wav2.getparams() if params1[:4] != params2[:4]: raise ValueError(\\"Input WAV files must have the same parameters\\") frames1 = wav1.readframes(params1.nframes) frames2 = wav2.readframes(params2.nframes) samples1 = np.frombuffer(frames1, dtype=np.int16) samples2 = np.frombuffer(frames2, dtype=np.int16) mixed_samples = ((samples1 + samples2) / 2).astype(np.int16) with wave.open(output_file, \'wb\') as wav_out: wav_out.setparams(params1) wav_out.writeframes(mixed_samples.tobytes())"},{"question":"# Unsupervised Learning with scikit-learn Objective You are required to implement a function that performs clustering and outlier detection on a given dataset and then reduces the dimensionality of the dataset for visualization purposes. Task Implement the function `analyze_data_with_unsupervised_learning(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]` to perform the following tasks: 1. **Clustering**: - Use the K-means algorithm to cluster the data into 3 clusters. - Return the cluster labels for the dataset. 2. **Outlier Detection**: - Use the Isolation Forest algorithm to detect outliers in the dataset. - Return the outlier labels (1 for inliers, -1 for outliers). 3. **Dimensionality Reduction**: - Reduce the dimensionality of the dataset to 2 dimensions using PCA. - Return the transformed 2D data. Input - `data`: A 2D numpy array of shape `(n_samples, n_features)` representing the dataset. Output - A tuple containing: 1. A numpy array of shape `(n_samples,)` representing cluster labels. 2. A numpy array of shape `(n_samples,)` representing outlier labels. 3. A numpy array of shape `(n_samples, 2)` representing the data transformed into 2 dimensions using PCA. Constraints - The dataset will have at least 100 samples and the number of features will be between 5 and 50. Example ```python import numpy as np data = np.random.rand(100, 10) cluster_labels, outlier_labels, reduced_data = analyze_data_with_unsupervised_learning(data) print(cluster_labels.shape) # Output: (100,) print(outlier_labels.shape) # Output: (100,) print(reduced_data.shape) # Output: (100, 2) ``` # Performance Requirements - The implementation should perform clustering, outlier detection, and dimensionality reduction efficiently. - Ensure that the K-means and PCA algorithms are correctly initialized and fitted. # Implementation Notes - You may assume all necessary imports from sklearn are available. - Example imports: `from sklearn.cluster import KMeans`, `from sklearn.ensemble import IsolationForest`, `from sklearn.decomposition import PCA`. # Evaluation Your implementation will be evaluated based on: - Correctness of the clustering results. - Accuracy of detecting outliers. - Proper transformation and dimensionality reduction using PCA. - Code efficiency and readability.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.ensemble import IsolationForest from sklearn.decomposition import PCA from typing import Tuple def analyze_data_with_unsupervised_learning(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Perform clustering, outlier detection, and dimensionality reduction on the dataset. Parameters ---------- data : np.ndarray A 2D numpy array of shape (n_samples, n_features). Returns ------- Tuple[np.ndarray, np.ndarray, np.ndarray] - Cluster labels for the dataset. - Outlier labels (1 for inliers, -1 for outliers). - Data transformed into 2 dimensions using PCA. # Clustering with K-means kmeans = KMeans(n_clusters=3, random_state=42) cluster_labels = kmeans.fit_predict(data) # Outlier detection with Isolation Forest isolation_forest = IsolationForest(random_state=42) outlier_labels = isolation_forest.fit_predict(data) # Dimensionality reduction with PCA pca = PCA(n_components=2, random_state=42) reduced_data = pca.fit_transform(data) return cluster_labels, outlier_labels, reduced_data"},{"question":"# Objective: You are required to implement a function that makes use of the `py_compile` module to compile a given list of Python source files to bytecode. Additionally, provide an option for error handling and optimization level settings. Finally, design a command-line interface for batch file compilation utilizing your function. # Problem: Implement a function `compile_py_files(file_list: list, optimize: int = -1, quiet: int = 0) -> list`, which: 1. Takes a list of Python source files (`file_list`) as input. 2. Compiles each file to a byte-compiled file, handling errors based on the `quiet` parameter: - If `quiet` is 0 or 1, errors should be printed to stderr but do not raise exceptions. - If `quiet` is 2, suppress all error messages. 3. Uses the specified optimization level (`optimize`). 4. Returns a list of paths to the generated bytecode files. Furthermore, you should implement a command-line interface that: 1. Accepts multiple Python source file paths as arguments. 2. Provides options for setting the optimization level (`-o` or `--optimize`) and quiet mode (`-q` or `--quiet`). 3. Prints the paths of the compiled files or suppresses this output based on the quiet level. # Function Signature: ```python def compile_py_files(file_list: list, optimize: int = -1, quiet: int = 0) -> list: pass ``` # Detailed Requirements: 1. **Function `compile_py_files`:** - **Inputs:** - `file_list`: A list of strings representing the paths to Python source files. - `optimize`: An integer representing the optimization level (default is -1). - `quiet`: An integer representing the error handling level (default is 0). - **Outputs:** - A list of strings representing the paths to the generated bytecode files. - **Constraints:** - All files in `file_list` will be valid paths to existing Python files. - Optimization levels should comply with Python\'s `compile()` function requirements. 2. **Command-Line Interface:** - **Usage:** ```sh python script_name.py <file1> <file2> ... [-o <optimize>] [-q <quiet>] ``` - **Options:** - `-o`, `--optimize`: Specify optimization level. - `-q`, `--quiet`: Specify quiet level. # Example: For a set of Python files: ```sh python script_name.py example1.py example2.py -o 2 -q 1 ``` The function should compile these files with optimization level 2 and print errors if any occurred but do not raise exceptions. # Notes: - Ensure to handle potential exceptions and edge cases, such as non-regular files or symlinks as specified by `py_compile`. - You are allowed to import necessary standard libraries for argument parsing and file handling. You should submit the code implementation along with example usages and explanations of how it satisfies the given requirements.","solution":"import py_compile import sys import os def compile_py_files(file_list: list, optimize: int = -1, quiet: int = 0) -> list: Compiles a given list of Python source files to bytecode with specified optimization and quiet options. Args: - file_list: List of Python source file paths to compile. - optimize: Optimization level (default is -1, equivalent to system default). - quiet: Quiet level, controlling error reporting (default is 0). Returns: - List of compiled bytecode file paths. compiled_files = [] for file_path in file_list: try: compiled_path = py_compile.compile(file_path, cfile=None, dfile=None, doraise=True, optimize=optimize) compiled_files.append(compiled_path) except py_compile.PyCompileError as e: if quiet < 2: sys.stderr.write(f\\"Error compiling {file_path}: {e.msg}n\\") if quiet == 0: raise return compiled_files if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Batch compile Python source files to bytecode.\\") parser.add_argument(\\"files\\", metavar=\\"FILE\\", type=str, nargs=\\"+\\", help=\\"Python source files to compile\\") parser.add_argument(\\"-o\\", \\"--optimize\\", type=int, default=-1, help=\\"Optimization level (0, 1, or 2)\\") parser.add_argument(\\"-q\\", \\"--quiet\\", type=int, default=0, choices=[0, 1, 2], help=\\"Quiet level (0, 1, or 2)\\") args = parser.parse_args() compiled_files = compile_py_files(args.files, optimize=args.optimize, quiet=args.quiet) if args.quiet < 2: for file in compiled_files: print(f\\"Compiled {file}\\")"},{"question":"Objective: Demonstrate your understanding of interacting with Unix system databases and error handling. Problem Statement: You need to implement a function `find_user_info` that retrieves user information based on either their numeric user ID or their login name. The function signature is as follows: ```python def find_user_info(identifier: Union[int, str]) -> str: Retrieves and returns user information for a given numeric user ID or login name. Parameters: identifier (int or str): The numeric user ID or login name. Returns: str: A formatted string containing user information, or a message indicating that the user was not found. Raises: TypeError: If the identifier is neither an integer nor a string. ``` Input: - `identifier` (int or str): The numeric user ID or the login name of the user. Output: - `str`: A formatted string containing the user information in the following format: ``` Login Name: <pw_name> User ID: <pw_uid> Group ID: <pw_gid> Full Name: <pw_gecos> Home Directory: <pw_dir> Shell: <pw_shell> ``` If the user is not found, the function should return the message: ``` User not found. ``` Constraints: 1. If the `identifier` is neither an integer nor a string, raise a `TypeError` with the message: ``` Identifier must be an integer (UID) or a string (login name). ``` 2. Handle cases where the user is not found gracefully without raising an exception. Notes: - Make sure to handle different data types for the `identifier` within the function. - Utilize the `pwd` module provided by the system. Example: ```python print(find_user_info(1000)) # Expected Output: # Login Name: johnsmith # User ID: 1000 # Group ID: 1000 # Full Name: John Smith # Home Directory: /home/johnsmith # Shell: /bin/bash print(find_user_info(\\"johnsm\\")) # Expected Output: # User not found. print(find_user_info(\\"johnsmith\\")) # Expected Output: # Login Name: johnsmith # User ID: 1000 # Group ID: 1000 # Full Name: John Smith # Home Directory: /home/johnsmith # Shell: /bin/bash print(find_user_info(9999)) # Expected Output: # User not found. print(find_user_info([\\"example\\"])) # Expected Output: # Traceback (most recent call last): # ... # TypeError: Identifier must be an integer (UID) or a string (login name). ``` Good luck!","solution":"import pwd from typing import Union def find_user_info(identifier: Union[int, str]) -> str: Retrieves and returns user information for a given numeric user ID or login name. Parameters: identifier (int or str): The numeric user ID or login name. Returns: str: A formatted string containing user information, or a message indicating that the user was not found. Raises: TypeError: If the identifier is neither an integer nor a string. if not isinstance(identifier, (int, str)): raise TypeError(\\"Identifier must be an integer (UID) or a string (login name).\\") try: if isinstance(identifier, int): pw_record = pwd.getpwuid(identifier) else: pw_record = pwd.getpwnam(identifier) return (f\\"Login Name: {pw_record.pw_name}n\\" f\\"User ID: {pw_record.pw_uid}n\\" f\\"Group ID: {pw_record.pw_gid}n\\" f\\"Full Name: {pw_record.pw_gecos}n\\" f\\"Home Directory: {pw_record.pw_dir}n\\" f\\"Shell: {pw_record.pw_shell}\\") except KeyError: return \\"User not found.\\""},{"question":"# Question: Create a Custom Script to Ensure `pip` is Installed In this task, you are required to write a Python function utilizing the `ensurepip` package. The function will determine if `pip` needs to be installed or upgraded and will perform the necessary action accordingly. We\'ll call the function `manage_pip_installation`. Function Specification **Function Name:** ```python manage_pip_installation(root: Optional[str], upgrade: bool, user: bool, altinstall: bool, default_pip: bool, verbosity: int) -> str ``` **Parameters:** 1. `root` (Optional[str]): A string specifying an alternative root directory to install `pip` relative to. If `None`, the default install location is used. 2. `upgrade` (bool): A boolean flag indicating whether to upgrade `pip` if it is already installed. 3. `user` (bool): A boolean flag indicating whether to install `pip` using the user scheme rather than globally. 4. `altinstall` (bool): A boolean flag that, if set, will prevent the installation of the `pipX` script. 5. `default_pip` (bool): A boolean flag that, if set, will install the `pip` script in addition to `pipX` and `pipX.Y`. 6. `verbosity` (int): An integer controlling the amount of output from the bootstrapping operation. **Returns:** - A string message indicating the result of the operation. For example, \\"pip installed\\", \\"pip upgraded\\", or \\"pip is already up-to-date.\\" **Constraints:** - If both `altinstall` and `default_pip` are set to `True`, raise a `ValueError` with the message \\"Cannot set both altinstall and default_pip to True\\". - The installation should be performed only if necessary based on the presence and version of `pip`. **Performance Requirements:** - The function should handle different scenarios efficiently and provide clear output messages. Example Usage ```python result = manage_pip_installation( root=None, upgrade=False, user=False, altinstall=False, default_pip=True, verbosity=1 ) print(result) # Expected output: \\"pip installed\\", \\"pip upgraded\\", or \\"pip is already up-to-date.\\" ``` **Note:** - Make sure to handle exceptions and edge cases properly. - Consider checking the current version of `pip` installed, if any, before deciding to install or upgrade.","solution":"import ensurepip import subprocess import sys from typing import Optional def manage_pip_installation(root: Optional[str] = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = True, verbosity: int = 1) -> str: Ensures that pip is installed or upgraded. Args: root (Optional[str]): Alternative root directory to install pip. upgrade (bool): Whether to upgrade pip if it is already installed. user (bool): Whether to install pip using the user scheme. altinstall (bool): Prevent installation of the pipX script. default_pip (bool): Install the pip script in addition to pipX and pipX.Y. verbosity (int): Controls the amount of output from the bootstrapping operation. Returns: str: Message indicating the result of the operation. if altinstall and default_pip: raise ValueError(\\"Cannot set both altinstall and default_pip to True\\") # Check the current version of pip try: result = subprocess.run([sys.executable, \'-m\', \'pip\', \'--version\'], capture_output=True, text=True) pip_installed = result.returncode == 0 except Exception: pip_installed = False if pip_installed: if upgrade: ensurepip.bootstrap(upgrade=True, root=root, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity) return \\"pip upgraded\\" else: return \\"pip is already up-to-date\\" else: ensurepip.bootstrap(root=root, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity) return \\"pip installed\\""},{"question":"# Mapping Operations Assessment Using the information provided about the `PyMapping` protocol, implement a Python class that mimics some of the low-level functionalities of this protocol for dictionary-like objects. Your task is to create a class `MappingOperations` that provides the following methods: 1. **__init__:** - Initialize the class with an empty dictionary. 2. **check_mapping(obj):** - Return `True` if `obj` supports the mapping protocol (i.e., is a dictionary-like object), otherwise `False`. 3. **size():** - Return the number of keys in the internal dictionary. 4. **get_item(key):** - Given a string `key`, return the corresponding value from the internal dictionary, or return `None` if the key does not exist. 5. **set_item(key, value):** - Set the `value` for a given string `key` in the internal dictionary. 6. **del_item(key):** - Remove the mapping for the given string `key` from the internal dictionary. If the key does not exist, do nothing. 7. **has_key(key):** - Return `True` if the internal dictionary contains the string `key`, otherwise `False`. 8. **keys():** - Return a list of keys in the internal dictionary. 9. **values():** - Return a list of values in the internal dictionary. 10. **items():** - Return a list of key-value pairs in the internal dictionary as tuples. # Performance Requirement - All dictionary operations should have an average time complexity of (O(1)). # Example Usage ```python mapping = MappingOperations() mapping.set_item(\\"key1\\", \\"value1\\") print(mapping.get_item(\\"key1\\")) # Output: value1 print(mapping.size()) # Output: 1 print(mapping.has_key(\\"key1\\")) # Output: True print(mapping.keys()) # Output: [\'key1\'] print(mapping.values()) # Output: [\'value1\'] print(mapping.items()) # Output: [(\'key1\', \'value1\')] mapping.del_item(\\"key1\\") print(mapping.has_key(\\"key1\\")) # Output: False ``` Your implementation should correctly handle these scenarios and pass all the example cases.","solution":"class MappingOperations: def __init__(self): self._dict = {} def check_mapping(self, obj): Return True if obj supports the mapping protocol (is a dictionary-like object), otherwise False. return isinstance(obj, dict) def size(self): Return the number of keys in the internal dictionary. return len(self._dict) def get_item(self, key): Given a string key, return the corresponding value from the internal dictionary, or return None if the key does not exist. return self._dict.get(key, None) def set_item(self, key, value): Set the value for a given string key in the internal dictionary. self._dict[key] = value def del_item(self, key): Remove the mapping for the given string key from the internal dictionary. If the key does not exist, do nothing. if key in self._dict: del self._dict[key] def has_key(self, key): Return True if the internal dictionary contains the string key, otherwise False. return key in self._dict def keys(self): Return a list of keys in the internal dictionary. return list(self._dict.keys()) def values(self): Return a list of values in the internal dictionary. return list(self._dict.values()) def items(self): Return a list of key-value pairs in the internal dictionary as tuples. return list(self._dict.items())"},{"question":"# PyTorch Meta Device Assessment Problem Statement: You are given a custom neural network model. Your task is to: 1. Create an instance of this model on the \'meta\' device. 2. Transfer this model to the CPU without data initialization. 3. Manually initialize the model parameters on the CPU. Detailed Steps: 1. **Model Definition**: ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 5) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x ``` 2. **Create an instance on the meta device**: - Load the model on a \'meta\' device. - Print the model to confirm its device and parameter state. 3. **Convert the model to CPU (uninitialized)**: - Transfer the model to the CPU using the `to_empty` method, ensuring the parameters are not initialized. - Print the model to validate it is moved to CPU without initialization. 4. **Manually initialize the model parameters**: - Write a function to initialize the parameters using a normal distribution with a mean of 0 and a standard deviation of 0.01. Constraints: - Use the provided `SimpleNet` class without modifications. - Do not directly set tensor data. Expected Input and Output: - **Input**: No input parameters. - **Output**: Print the model after each significant step as described. Performance Requirements: - Ensure the solution is efficient but correctness and understanding are the primary focus. Sample Execution: ```python # Step 1: Load on meta device meta_model = create_model_on_meta() print(meta_model) # Step 2: Convert to CPU uninitialized cpu_model_uninitialized = convert_to_cpu_uninitialized(meta_model) print(cpu_model_uninitialized) # Step 3: Manually initialize cpu_model_initialized = initialize_parameters(cpu_model_uninitialized) print(cpu_model_initialized) ``` Implement the required functions to achieve the steps outlined. Here is the function template you need to complete: ```python def create_model_on_meta(model_class): Create an instance of the model_class on the \'meta\' device. with torch.device(\'meta\'): model = model_class() return model def convert_to_cpu_uninitialized(meta_model): Convert the meta model to CPU, leaving parameters uninitialized. cpu_model = meta_model.to_empty(device=\\"cpu\\") return cpu_model def initialize_parameters(model): Manually initialize the parameters of the model. for param in model.parameters(): nn.init.normal_(param, mean=0.0, std=0.01) return model # Usage: meta_model = create_model_on_meta(SimpleNet) print(meta_model) cpu_model_uninitialized = convert_to_cpu_uninitialized(meta_model) print(cpu_model_uninitialized) cpu_model_initialized = initialize_parameters(cpu_model_uninitialized) print(cpu_model_initialized) ```","solution":"import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 5) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def create_model_on_meta(model_class): Create an instance of the model_class on the \'meta\' device. with torch.device(\'meta\'): model = model_class() return model def convert_to_cpu_uninitialized(meta_model): Convert the meta model to CPU, leaving parameters uninitialized. cpu_model = meta_model.to_empty(device=\\"cpu\\") return cpu_model def initialize_parameters(model): Manually initialize the parameters of the model. for param in model.parameters(): nn.init.normal_(param, mean=0.0, std=0.01) return model # Example Usage: if __name__ == \\"__main__\\": meta_model = create_model_on_meta(SimpleNet) print(meta_model) cpu_model_uninitialized = convert_to_cpu_uninitialized(meta_model) print(cpu_model_uninitialized) cpu_model_initialized = initialize_parameters(cpu_model_uninitialized) print(cpu_model_initialized)"},{"question":"# Seaborn Advanced Rugplot Visualization You are required to write a function `create_combined_plot` that generates a complex seaborn plot incorporating various aspects of rugplot usage as demonstrated in the provided documentation. Function Signature ```python def create_combined_plot(tips_csv: str, diamonds_csv: str) -> None: ``` Input - `tips_csv`: A string representing the file path of a `tips` dataset in CSV format. - `diamonds_csv`: A string representing the file path of a `diamonds` dataset in CSV format. Output - The function should not return anything. It should generate and display the required seaborn plots. Requirements 1. **Load the datasets**: - Load the `tips` dataset from the provided `tips_csv` file path. - Load the `diamonds` dataset from the provided `diamonds_csv` file path. 2. **Plot the `tips` dataset**: - Create a KDE plot for the `total_bill` column. - Add a rug plot for the `total_bill` column. 3. **Plot the `tips` dataset with scatter plot and rug plot**: - Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Add a rug plot for `total_bill` and `tip`. 4. **Enhance the scatter plot with hue mapping**: - Add hue mapping to distinguish data points by the `time` column (Lunch/Dinner). 5. **Customize rug marks**: - Increase the height of the rug marks to 10% of the axis. - Create a new rug plot with the marks positioned outside the axes and slightly offset. 6. **Visualizer for a larger dataset**: - Using the `diamonds` dataset, create a scatter plot with `carat` on the x-axis and `price` on the y-axis. - Add a rug plot using thinner lines and apply alpha blending for better density representation. Constraints - You can assume the CSV files are properly formatted and contain the necessary columns (`total_bill`, `tip`, `time` for `tips` and `carat`, `price` for `diamonds`). Below is an example of how your function\'s output should be structured: ```python create_combined_plot(\'path_to_tips.csv\', \'path_to_diamonds.csv\') ``` This should generate a series of visualizations as required by the steps mentioned.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_combined_plot(tips_csv: str, diamonds_csv: str) -> None: # Load the datasets tips = pd.read_csv(tips_csv) diamonds = pd.read_csv(diamonds_csv) # Create a KDE plot for the `total_bill` column with a rug plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\'total_bill\', fill=True) sns.rugplot(data=tips, x=\'total_bill\') plt.title(\\"KDE plot with Rug plot for `total_bill`\\") plt.show() # Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis with rug plots plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\') sns.rugplot(data=tips, x=\'total_bill\', y=\'tip\') plt.title(\\"Scatter plot with rug plot for `total_bill` vs `tip`\\") plt.show() # Scatter plot with hue mapping to distinguish by `time` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') sns.rugplot(data=tips, x=\'total_bill\', y=\'tip\') plt.title(\\"Scatter plot with rug plot for `total_bill` vs `tip` with hue\\") plt.show() # Customized rug marks plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') sns.rugplot(data=tips, x=\'total_bill\', y=\'tip\', height=0.1) plt.title(\\"Customized rug plot for `total_bill` vs `tip` with hue\\") plt.show() # Visualizer for a larger dataset using `diamonds` plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\'carat\', y=\'price\', alpha=0.3) sns.rugplot(data=diamonds, x=\'carat\', y=\'price\', height=0.01, lw=0.5, alpha=0.3) plt.title(\\"Scatter plot with rug plot for `carat` vs `price`\\") plt.show()"},{"question":"Problem Statement: Persistent Dictionary and Data Update You are given a task to manage a simple database using the `shelve` module from Python. In this task, you need to create a persistent dictionary to store user profiles. Each user profile consists of fields like username, email, and age. You will need to implement functions to: 1. Add a new user. 2. Retrieve an existing user profile. 3. Update an existing user profile. 4. Delete a user profile. 5. List all user profiles in the database. Requirements: 1. Implement a class `UserProfileDB` with the following methods: - `__init__(self, filename: str)`: Initializes the database. - `add_user(self, username: str, email: str, age: int)`: Adds a new user to the database. - `get_user(self, username: str) -> dict`: Retrieves the user profile corresponding to the given username. - `update_user(self, username: str, email: Optional[str] = None, age: Optional[int] = None)`: Updates the email and/or age for an existing user. - `delete_user(self, username: str)`: Deletes the user profile with the given username. - `list_users(self) -> List[dict]`: Returns a list of all user profiles in the database. 2. Ensure that the database is properly closed after operations to prevent data corruption. Use the database as a context manager where appropriate. Constraints: - The username will always be unique. - The database file used should be named `user_profiles.db` in the current working directory. - Use `pickle.DEFAULT_PROTOCOL` for serialization. Example Usage: ```python # Initialize the user profile database db = UserProfileDB(\'user_profiles.db\') # Add users db.add_user(\\"john_doe\\", \\"john@example.com\\", 30) db.add_user(\\"jane_doe\\", \\"jane@example.com\\", 25) # Retrieve a user print(db.get_user(\\"john_doe\\")) # Output: {\'username\': \'john_doe\', \'email\': \'john@example.com\', \'age\': 30} # Update a user db.update_user(\\"jane_doe\\", age=26) # List all users print(db.list_users()) # Output: [{\'username\': \'john_doe\', \'email\': \'john@example.com\', \'age\': 30}, {\'username\': \'jane_doe\', \'email\': \'jane@example.com\', \'age\': 26}] # Delete a user db.delete_user(\\"john_doe\\") ``` Additional Notes: - Make use of the `writeback` parameter appropriately to ensure data consistency when updating mutable objects. - Handle potential exceptions (e.g., KeyError) gracefully and provide meaningful error messages.","solution":"import shelve from typing import Optional, List class UserProfileDB: def __init__(self, filename: str): self.filename = filename def add_user(self, username: str, email: str, age: int): with shelve.open(self.filename, writeback=True) as db: if username in db: raise ValueError(\\"User already exists\\") db[username] = {\\"username\\": username, \\"email\\": email, \\"age\\": age} def get_user(self, username: str) -> dict: with shelve.open(self.filename, writeback=True) as db: if username not in db: raise KeyError(\\"User not found\\") return db[username] def update_user(self, username: str, email: Optional[str] = None, age: Optional[int] = None): with shelve.open(self.filename, writeback=True) as db: if username not in db: raise KeyError(\\"User not found\\") if email is not None: db[username][\\"email\\"] = email if age is not None: db[username][\\"age\\"] = age def delete_user(self, username: str): with shelve.open(self.filename, writeback=True) as db: if username not in db: raise KeyError(\\"User not found\\") del db[username] def list_users(self) -> List[dict]: with shelve.open(self.filename, writeback=True) as db: return [db[user] for user in db]"},{"question":"# Question In this task, you are going to demonstrate and implement seaborn functionality to customize plot contexts. Requirements You need to implement the following functions: 1. `set_plot_context(context_name, font_scale, rc_params)`: This function should set the plotting context using seaborn and apply the custom configurations provided. The input parameters are as follows: - `context_name` (string): The name of the context to set. It can be one of the following: `paper`, `notebook`, `talk`, `poster`. - `font_scale` (float): A value for scaling the size of the fonts relative to the default size. - `rc_params` (dictionary): A dictionary of parameters to override the default seaborn settings. Keys are parameter names and values are the parameter values. 2. `create_custom_plot(x, y, context_name=\\"notebook\\", font_scale=1, rc_params=None)`: This function should: - Use the `set_plot_context` function to apply the context settings. - Create a line plot using seaborn\'s `lineplot` function with provided `x` and `y` data. - Return the seaborn line plot object. Input - The `set_plot_context` function parameters: - `context_name` (string): Context for seaborn plots (`paper`, `notebook`, `talk`, `poster`). - `font_scale` (float): Scale for font size. - `rc_params` (dictionary): Dictionary with seaborn parameter customizations. - The `create_custom_plot` function parameters: - `x` and `y` (lists of numbers): Data for the x and y axes, respectively. - `context_name` (string, optional): Context name to set for the plot. Default is `notebook`. - `font_scale` (float, optional): Font scaling factor. Default is 1. - `rc_params` (dictionary, optional): Dictionary with seaborn parameter customizations. Default is None. Output - The `set_plot_context` function does not return anything. It sets the context for seaborn plots. - The `create_custom_plot` function returns the seaborn line plot object created with the specified settings. Constraints - `context_name` must be one of the allowed seaborn contexts (`paper`, `notebook`, `talk`, `poster`). - `font_scale` must be a positive float. Example ```python import seaborn as sns # sample data x = [0, 1, 2] y = [1, 3, 2] # Example usage create_custom_plot(x, y, context_name=\\"talk\\", font_scale=1.5, rc_params={\\"lines.linewidth\\": 2}) ``` This should create a line plot with the talk context, font scale of 1.5, and line width of 2.","solution":"import seaborn as sns def set_plot_context(context_name, font_scale=1, rc_params=None): Sets the plotting context using seaborn. :param context_name: The name of the context to set. It can be one of the following: \'paper\', \'notebook\', \'talk\', \'poster\'. :param font_scale: A value for scaling the size of the fonts relative to the default size. :param rc_params: A dictionary of parameters to override the default seaborn settings. if context_name not in [\'paper\', \'notebook\', \'talk\', \'poster\']: raise ValueError(f\\"Invalid context_name: {context_name}. Choose from \'paper\', \'notebook\', \'talk\', \'poster\'.\\") if font_scale <= 0: raise ValueError(f\\"font_scale must be a positive float. Got {font_scale} instead.\\") sns.set_context(context=context_name, font_scale=font_scale, rc=rc_params) def create_custom_plot(x, y, context_name=\'notebook\', font_scale=1, rc_params=None): Creates a line plot using seaborn with customized context settings. :param x: Data for the x axis. :param y: Data for the y axis. :param context_name: Context name to set for the plot. Default is \'notebook\'. :param font_scale: Font scaling factor. Default is 1. :param rc_params: Dictionary with seaborn parameter customizations. Default is None. :return: The seaborn line plot object. set_plot_context(context_name, font_scale, rc_params) return sns.lineplot(x=x, y=y)"},{"question":"**Question: Implementing a Multi-output Decision Tree Regressor with Missing Values Handling and Pruning** **Objective:** You are required to implement a multi-output decision tree regressor using the scikit-learn library. Your solution should handle missing values appropriately and utilize minimal cost-complexity pruning to avoid overfitting. **Details:** 1. **Data Preparation:** - Generate a synthetic dataset with features and target values. Let one of the features include some missing values. - Ensure the target is multi-dimensional, i.e., a multi-output regression problem. 2. **Model Implementation:** - Use `DecisionTreeRegressor` to train a decision tree on the generated dataset. - Handle missing values during both training and prediction phases. - Implement minimal cost-complexity pruning to avoid overfitting. 3. **Evaluation:** - Evaluate the model on a test dataset. - Use appropriate metrics for regression problems to assess the performance. **Input:** - `X_train`: 2D array of shape `(n_samples, n_features)` containing training data. - `y_train`: 2D array of shape `(n_samples, n_outputs)` containing training target values. - `X_test`: 2D array of shape `(n_samples, n_features)` containing test data. **Output:** - `y_pred`: 2D array of shape `(n_samples, n_outputs)` containing predicted values for the test data. **Constraints:** - Handle missing values by assigning them to either left or right nodes based on training splits. - Use `ccp_alpha` parameter for minimal cost-complexity pruning. **Performance Requirements:** - Ensure the implementation is efficient in terms of both time and space. - The prediction latency should be acceptable for real-time applications. **Hint:** Refer to the scikit-learn documentation for `DecisionTreeRegressor`, `ccp_alpha` parameter for pruning, and handling missing values. **Example:** ```python from sklearn.tree import DecisionTreeRegressor import numpy as np # Generate synthetic dataset X_train = np.array([[0, np.nan], [1, 1], [2, np.nan], [6, 3], [4, np.nan]]) y_train = np.array([[0.5, 1.0], [1.5, 2.0], [2.5, 3.0], [4.5, 5.0], [3.5, 4.0]]) X_test = np.array([[3, np.nan], [5, np.nan]]) # Implement the DecisionTreeRegressor with missing values handling and pruning # Fill in the implementation details clf = DecisionTreeRegressor(random_state=0, ccp_alpha=0.01) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(y_pred) ``` **Note:** Your implementation should be robust to handle various scenarios and edge cases, especially with missing values in the input data.","solution":"from sklearn.tree import DecisionTreeRegressor from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline import numpy as np def train_and_evaluate(X_train, y_train, X_test, ccp_alpha=0.01): Train a DecisionTreeRegressor on provided data and predict on test data. X_train: np.ndarray : Training features y_train: np.ndarray : Training targets X_test: np.ndarray : Test features ccp_alpha: float : Complexity parameter used for Minimal Cost-Complexity Pruning Returns: y_pred: np.ndarray : Predicted values for X_test # Define a pipeline with an imputer and decision tree regressor clf = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Strategy to handle missing values (\'regressor\', DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)) ]) # Fit the model clf.fit(X_train, y_train) # Predict on test data y_pred = clf.predict(X_test) return y_pred"},{"question":"**Objective:** Demonstrate your understanding of Python 3.10 built-in functions by solving a complex problem using these functions. **Problem Statement:** You are given a list of strings. Some of these strings might represent integers or floating-point numbers, and some might contain non-digit characters. Your task is to create a function called `process_strings` that processes this list of strings. The function should: 1. Convert all valid numeric strings (both integer and floating-point) to their respective numeric types. 2. For numeric values, compute the absolute value. 3. For non-numeric strings, convert all the characters to their corresponding ASCII value characters by using the `ascii()` function. 4. Return a list containing the processed values. **Function Signature:** ```python def process_strings(strings: list) -> list: pass ``` **Input:** - `strings`: A list of strings. (1 ≤ len(strings) ≤ 1000) - Example: `[\\"123\\", \\"-45.67\\", \\"hello\\", \\"world\\", \\"78\\"]` **Output:** - A list of processed values where numeric strings are converted to their absolute numeric value and non-numeric strings have their characters converted using the `ascii()` function. - Example: `[123, 45.67, \\"\'hello\'\\", \\"\'world\'\\", 78]` **Constraints:** - You should not use any external libraries. Rely on Python\'s built-in functions only. - Ensure your solution is efficient, with a focus on minimizing runtime. **Example:** ```python print(process_strings([\\"123\\", \\"-45.67\\", \\"hello\\", \\"world\\", \\"78\\"])) # Output: [123, 45.67, \\"\'hello\'\\", \\"\'world\'\\", 78] ``` # Explanation: - The string `\\"123\\"` is a valid integer, so it is converted to `123`. - The string `\\"-45.67\\"` is a valid floating-point number, so it is converted to `45.67`. - The string `\\"hello\\"` is not a number, so each character is processed by the `ascii()` function resulting in the string `\\"\'hello\'\\"`. - Similarly, the string `\\"world\\"` is processed using the `ascii()` function resulting in the string `\\"\'world\'\\"`. - The string `\\"78\\"` is a valid integer, so it is converted to `78`. **Implementation:** ```python def process_strings(strings: list) -> list: processed = [] for s in strings: try: # Try to convert to integer num = int(s) processed.append(abs(num)) except ValueError: try: # Try to convert to float if not an integer num = float(s) processed.append(abs(num)) except ValueError: # If neither integer nor float, convert using ascii() processed.append(ascii(s)) return processed ```","solution":"def process_strings(strings: list) -> list: Process a list of strings, converting numeric strings to their absolute numeric values and non-numeric strings to their ASCII representation. Parameters: strings (list): A list of strings to process. Returns: list: A list with numeric values and processed non-numeric strings. processed = [] for s in strings: try: # Try to convert to integer num = int(s) processed.append(abs(num)) except ValueError: try: # Try to convert to float if not an integer num = float(s) processed.append(abs(num)) except ValueError: # If neither integer nor float, convert using ascii() processed.append(ascii(s)) return processed"},{"question":"**Problem Statement:** You are required to create a comprehensive analysis using the `seaborn` library, incorporating multiple functionalities to plot various visualizations from available datasets. Your task is to ensure an understanding of the variety of parameters seaborn offers for bar plots, as well as its interaction with pandas dataframes. Use the provided datasets `penguins` and `flights` as specified. # Task: 1. Load the `penguins` and `flights` datasets using seaborn. 2. Create a facet grid plot using `sns.catplot` to show a bar plot with the following specifications: - Set the facet\'s columns to be the different species in the `penguins` dataset. - Use `sex` for the x-axis and `body_mass_g` for the y-axis. - Height should be 4 and the aspect ratio 0.5. 3. Create a bar plot showing the total number of passengers (`passengers` column) per year (`year` column) in the `flights` dataset. - Aggregate the passengers per year using `sum`. - Customize to hide error bars and annotate each bar with its total value. 4. Create a bar plot grouping by both `island` and `sex` using the `penguins` dataset. - Plot `body_mass_g` on the y-axis and `island` on the x-axis. - Use `hue` to differentiate between `sex`. - Customize the error bars to show the standard deviation. 5. Create a wide-form dataset for `flights` by pivoting the `index` to be `year`, columns as `month`, and values as `passengers`. - Plot a bar plot using this wide-form dataset. - Customize the appearance of the bars using `linewidth=2.5`, `edgecolor=\\".5\\"` and `facecolor=(0, 0, 0, 0)`. # Constraints: - You should use seaborn version >= 0.11. - Ensure all plots are properly annotated and labeled for clarity. - Handle potential missing values in the datasets if necessary. # Input: No input required from the user; the datasets should be loaded directly from seaborn. # Output: You need to display the following plots: 1. Faceted bar plot of `penguins` dataset. 2. Bar plot of total passengers per year in `flights` dataset with annotations. 3. Grouped bar plot by `island` and `sex` in the `penguins` dataset. 4. Customized appearance bar plot using wide-form `flights` dataset. Illustrate your solution with corresponding plots and any relevant code comments.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_datasets(): Load the penguins and flights datasets from seaborn. Returns: tuple: A tuple containing two datasets - penguins and flights penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") return penguins, flights def plot_facet_grid(penguins): Create a facet grid plot using sns.catplot with the specified parameters. Displays the plot. g = sns.catplot(data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", col=\\"species\\", height=4, aspect=0.5) g.fig.suptitle(\\"Body Mass by Sex and Species in Penguins\\", y=1.05) plt.show() def plot_total_passengers_per_year(flights): Create a bar plot of total passengers per year in the flights dataset with annotations. Displays the plot. flights_per_year = flights.groupby(\\"year\\")[\\"passengers\\"].sum().reset_index() ax = sns.barplot(data=flights_per_year, x=\\"year\\", y=\\"passengers\\", ci=None) for index, row in flights_per_year.iterrows(): ax.text(row.name, row.passengers, row.passengers, color=\'black\', ha=\\"center\\") ax.set_title(\\"Total Passengers Per Year\\") ax.set_xlabel(\\"Year\\") ax.set_ylabel(\\"Total Passengers\\") plt.show() def plot_grouped_by_island_and_sex(penguins): Create a grouped bar plot by island and sex in the penguins dataset. Displays the plot. ax = sns.barplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\") ax.set_title(\\"Body Mass by Island and Sex in Penguins\\") ax.set_xlabel(\\"Island\\") ax.set_ylabel(\\"Body Mass (g)\\") plt.show() def plot_wide_form_flights(flights): Create a bar plot using the wide-form of flights dataset. Displays the plot. flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") ax = flights_wide.plot(kind=\\"bar\\", linewidth=2.5, edgecolor=\\".5\\", facecolor=(0, 0, 0, 0)) ax.set_title(\\"Monthly Passengers Trends Over Years\\") ax.set_xlabel(\\"Year\\") ax.set_ylabel(\\"Passengers\\") plt.show() def main(): penguins, flights = load_datasets() plot_facet_grid(penguins) plot_total_passengers_per_year(flights) plot_grouped_by_island_and_sex(penguins) plot_wide_form_flights(flights) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Implement a function that performs the following tasks: 1. Parses a list of arguments provided as strings, ensuring they match expected types. 2. Imports specified modules dynamically based on the parsed arguments. 3. Uses reflection to invoke methods from the imported modules with the parsed arguments. # Function Signature ```python def dynamic_function_execute(argument_list: list[str]) -> Any: Execute a function dynamically by parsing arguments, importing the module, and invoking the specified function. :param argument_list: A list of strings where: - The first element is the module name. - The second element is the function name within the module. - The subsequent elements are arguments for the function. :return: The result of the dynamic function invocation. pass ``` # Detailed Requirements: 1. **Input Format:** - A list of strings `argument_list`. - Example: `[\\"math\\", \\"sqrt\\", \\"16\\"]` - The first element is the module name to be imported. - The second element is the function name from the module to be invoked. - The remaining elements are arguments for the function, which should be parsed appropriately. 2. **Output Format:** - The return value of the invoked function. 3. **Constraints/Assumptions:** - Only standard library modules will be used. - The arguments for the function will either be able to be converted to standard types (like int, float, str) or they will have `eval`-like representation (ensuring they are safe to use with `eval`). - Error handling for parsing, importing, and invocation should be properly managed and informative. 4. **Example:** ```python result = dynamic_function_execute([\\"math\\", \\"pow\\", \\"2\\", \\"3\\"]) print(result) # Output: 8.0 result = dynamic_function_execute([\\"os\\", \\"getcwd\\"]) print(result) # Output: (current working directory path as string) ``` 5. **Performance Requirements:** - The function should efficiently import the necessary module and invoke the function. # Hints: - Utilize Python\'s `importlib` library for dynamic imports. - Use `getattr` for fetching the function from the module. - Be cautious while handling and converting argument types. Implement the `dynamic_function_execute` function that showcases your understanding of Python\'s dynamic functionalities and utility handling as discussed.","solution":"import importlib def dynamic_function_execute(argument_list): Execute a function dynamically by parsing arguments, importing the module, and invoking the specified function. :param argument_list: A list of strings where: - The first element is the module name. - The second element is the function name within the module. - The subsequent elements are arguments for the function. :return: The result of the dynamic function invocation. if not argument_list: raise ValueError(\\"The argument list cannot be empty.\\") module_name = argument_list[0] function_name = argument_list[1] function_args = argument_list[2:] # Import the module dynamically module = importlib.import_module(module_name) # Get the function from the module func = getattr(module, function_name) # Convert arguments to their appropriate types parsed_args = [eval(arg) for arg in function_args] # Call the function with the parsed arguments result = func(*parsed_args) return result"},{"question":"Objective: Implement a Python program that demonstrates a deep understanding of the `multiprocessing.shared_memory` module by solving a real-world problem using shared memory for inter-process communication. Problem Statement: You are given the task of processing a large dataset by splitting it across multiple processes. Implement a program that: 1. Creates a shared memory block to hold a large NumPy array. 2. Spawns multiple processes to calculate the sum of different segments of the array concurrently. 3. Aggregates the partial results from all processes to compute the total sum of the array. Requirements: 1. **Input Format**: - The program should generate a large NumPy array of random integers. The array size should be at least 10^6 elements. - The number of worker processes should be determined based on the CPU core count. 2. **Output Format**: - Print the total sum of the array after all processes have completed. 3. **Constraints**: - Use the `multiprocessing.shared_memory` module to avoid unnecessary copying of data. - Properly handle the creation, attachment, and cleanup of shared memory blocks. 4. **Function Implementation**: - Implement a function `process_segment` that each worker process will run to compute the sum of its segment. - Implement a function `main` to coordinate the creation of shared memory, spawning of processes, and aggregation of results. Example: Consider using a shared memory block to store an array of size 10^6. If the CPU has 4 cores, split the array into 4 segments and spawn 4 processes to compute the sum of each segment. Finally, aggregate the partial sums and print the total sum. ```python import numpy as np import multiprocessing as mp from multiprocessing import shared_memory def process_segment(shm_name, segment_start, segment_end, result_queue): existing_shm = shared_memory.SharedMemory(name=shm_name) array = np.ndarray((segment_end - segment_start,), dtype=np.int64, buffer=existing_shm.buf[segment_start * 8:segment_end * 8]) segment_sum = np.sum(array) result_queue.put(segment_sum) existing_shm.close() def main(): # Create a large NumPy array large_array = np.random.randint(0, 100, size=10**6, dtype=np.int64) # Create a shared memory block shm = shared_memory.SharedMemory(create=True, size=large_array.nbytes) shm_array = np.ndarray(large_array.shape, dtype=large_array.dtype, buffer=shm.buf) np.copyto(shm_array, large_array) num_cores = mp.cpu_count() segment_size = large_array.size // num_cores processes = [] result_queue = mp.Queue() for i in range(num_cores): segment_start = i * segment_size # Ensure the last segment goes to the end of the array segment_end = (i + 1) * segment_size if i != num_cores - 1 else large_array.size p = mp.Process(target=process_segment, args=(shm.name, segment_start, segment_end, result_queue)) processes.append(p) p.start() total_sum = 0 for p in processes: total_sum += result_queue.get() p.join() print(f\'Total Sum: {total_sum}\') # Clean up shm.close() shm.unlink() if __name__ == \'__main__\': main() ``` Notes: - Ensure the shared memory block is properly cleaned up after the computation is done. - Handle the creation, reading, and modification of the shared memory block carefully to avoid memory access errors. - Test the program on a machine with multiple CPU cores to observe the performance benefit.","solution":"import numpy as np import multiprocessing as mp from multiprocessing import shared_memory def process_segment(shm_name, segment_start, segment_end, result_queue): existing_shm = shared_memory.SharedMemory(name=shm_name) array = np.ndarray((segment_end - segment_start,), dtype=np.int64, buffer=existing_shm.buf[segment_start * 8:segment_end * 8]) segment_sum = np.sum(array) result_queue.put(segment_sum) existing_shm.close() def main(): # Create a large NumPy array large_array = np.random.randint(0, 100, size=10**6, dtype=np.int64) # Create a shared memory block shm = shared_memory.SharedMemory(create=True, size=large_array.nbytes) shm_array = np.ndarray(large_array.shape, dtype=large_array.dtype, buffer=shm.buf) np.copyto(shm_array, large_array) num_cores = mp.cpu_count() segment_size = large_array.size // num_cores processes = [] result_queue = mp.Queue() for i in range(num_cores): segment_start = i * segment_size # Ensure the last segment goes to the end of the array segment_end = (i + 1) * segment_size if i != num_cores - 1 else large_array.size p = mp.Process(target=process_segment, args=(shm.name, segment_start, segment_end, result_queue)) processes.append(p) p.start() total_sum = 0 for p in processes: total_sum += result_queue.get() p.join() print(f\'Total Sum: {total_sum}\') # Clean up shm.close() shm.unlink() if __name__ == \'__main__\': main()"},{"question":"Employee Data Processor You are given a text file containing data of employees in the following format: ``` Name,Age,Department,Salary John Doe,29,Engineering,60000 Jane Smith, 34, Marketing,75000 Fred Brown,45, HR,50000 Alice Green,28,Engineering,70000 ... ``` Write a Python function `process_employee_data(file_path: str, min_age: int) -> dict` that reads the employee data from the given file and computes the following: 1. The average salary of employees older than `min_age`. 2. The total number of employees in each department. Your function should return a dictionary with two keys: `\\"average_salary\\"` and `\\"department_counts\\"`, where: - `\\"average_salary\\"` maps to the average salary of employees older than `min_age` (rounded to 2 decimal places). - `\\"department_counts\\"` maps to another dictionary where the keys are department names and the values are the counts of employees in those departments. If there are no employees older than `min_age`, the function should return `None` for the `\\"average_salary\\"`. # Constraints - Assume the file is well-formed and each line contains valid data. - The `department` field in the file will not be empty. - Salary values will be positive integers. - Age and `min_age` are positive integers. # Example Suppose the content of `employees.txt` is: ``` Name,Age,Department,Salary John Doe,29,Engineering,60000 Jane Smith,34,Marketing,75000 Fred Brown,45,HR,50000 Alice Green,28,Engineering,70000 ``` Calling `process_employee_data(\'employees.txt\', 30)` should return: ```python { \\"average_salary\\": 62500.00, \\"department_counts\\": { \\"Engineering\\": 2, \\"Marketing\\": 1, \\"HR\\": 1 } } ``` # Function Signature ```python def process_employee_data(file_path: str, min_age: int) -> dict: # Your code here ``` # Notes: - Make sure to handle file reading exceptions. - Use Python\'s built-in libraries and features effectively. - The solution should be efficient in terms of time and space complexity.","solution":"import csv from collections import defaultdict from typing import Dict def process_employee_data(file_path: str, min_age: int) -> Dict[str, any]: Processes employee data from a CSV file and returns a dictionary with the average salary of employees older than `min_age` and the count of employees in each department. Parameters: - file_path: str: Path to the employee data file - min_age: int: Minimum age to consider for average salary calculation Returns: - Dict[str, any]: A dictionary with keys \\"average_salary\\" and \\"department_counts\\" \\"average_salary\\" maps to the average salary of employees older than `min_age` \\"department_counts\\" maps to a dictionary with department names as keys and the counts of employees as values total_salary = 0 count = 0 department_counts = defaultdict(int) try: with open(file_path, mode=\'r\') as file: reader = csv.DictReader(file) for row in reader: age = int(row[\'Age\'].strip()) salary = int(row[\'Salary\'].strip()) department = row[\'Department\'].strip() department_counts[department] += 1 if age > min_age: total_salary += salary count += 1 average_salary = round(total_salary / count, 2) if count > 0 else None return { \\"average_salary\\": average_salary, \\"department_counts\\": department_counts } except (FileNotFoundError, IOError): # Handle file not found or IO error return { \\"average_salary\\": None, \\"department_counts\\": {} }"},{"question":"**Title:** Advanced Date and Time Manipulation with Timezone Handling **Objective:** The objective of this task is to assess your understanding of Python\'s `datetime` module, particularly focusing on creating, manipulating datetime objects, and handling timezones. **Question:** You are developing a part of a software system that schedules meetings across different timezones. The system should allow creating meetings, converting meeting times to different timezones, and calculating the duration between them. 1. **Create a Meeting Class:** Implement a class `Meeting` that has the following attributes: - `title` (str): The title of the meeting. - `start_datetime` (datetime): The start date and time of the meeting. - `end_datetime` (datetime): The end date and time of the meeting. - `timezone` (timezone): The timezone of the meeting. Implement the following methods in the `Meeting` class: - `__init__(self, title, start_datetime, end_datetime, timezone)`: Constructor to initialize the meeting attributes. - `convert_to_timezone(self, new_timezone)`: Converts the meeting\'s `start_datetime` and `end_datetime` to `new_timezone`. - `get_duration(self)`: Returns the duration of the meeting in minutes. 2. **Ensure Constraints:** - The `end_datetime` should always be after `start_datetime`. - The duration should be calculated as the total minutes between the two `datetime` values. # Input Format - The constructor of the `Meeting` class takes: - A `title` (string) representing the title of the meeting. - A `start_datetime` and `end_datetime` (both datetime objects) representing the start and end times of the meeting. - A `timezone` (timezone object) in which the meeting is scheduled. # Output Format - The `convert_to_timezone(self, new_timezone)` method should return None but modify the `start_datetime` and `end_datetime`. - The `get_duration(self)` method should return an integer representing the duration of the meeting in minutes. # Example ```python from datetime import datetime, timezone, timedelta # Example initialization and usage mt = Meeting( title=\\"Team Sync\\", start_datetime=datetime(2023, 10, 3, 9, 0, tzinfo=timezone.utc), end_datetime=datetime(2023, 10, 3, 10, 0, tzinfo=timezone.utc), timezone=timezone.utc ) # Convert meeting timezone to US/Eastern eastern_timezone = timezone(timedelta(hours=-5)) mt.convert_to_timezone(eastern_timezone) # Calculate duration of the meeting duration = mt.get_duration() print(duration) # Output should be 60 minutes ``` Notes: - Use the `timezone` class for handling timezones. - Assume the `start_datetime` and `end_datetime` provided are always valid. Implement the `Meeting` class to complete the exercise.","solution":"from datetime import datetime, timedelta, timezone class Meeting: def __init__(self, title, start_datetime, end_datetime, meeting_timezone): Initialize the Meeting with title, start_datetime, end_datetime, and timezone. self.title = title self.start_datetime = start_datetime self.end_datetime = end_datetime self.timezone = meeting_timezone # Ensure end_datetime is after start_datetime if self.end_datetime <= self.start_datetime: raise ValueError(\\"end_datetime must be after start_datetime\\") def convert_to_timezone(self, new_timezone): Convert the meeting\'s start_datetime and end_datetime to new_timezone. self.start_datetime = self.start_datetime.astimezone(new_timezone) self.end_datetime = self.end_datetime.astimezone(new_timezone) self.timezone = new_timezone def get_duration(self): Return the duration of the meeting in minutes. duration = self.end_datetime - self.start_datetime return int(duration.total_seconds() // 60)"},{"question":"**Problem Statement:** You are tasked with enhancing the `webbrowser` module by implementing a function that registers a new browser type and selectively opens URLs in specific browsers based on the conditions provided. To achieve this, you\'ll need to understand how to leverage the `webbrowser.register`, `webbrowser.get`, and `webbrowser.open` functions. **Requirements:** 1. Implement the function `register_and_open(url: str, browser_name: str, user_preference: str) -> bool` which performs the following: - Registers a new browser with the specified `browser_name`. - Based on the `user_preference`, either opens the URL in the newly registered browser or in the default browser. 2. The function should: - Return `True` if the URL is successfully opened in the desired browser. - Return `False` if an error occurs or the URL cannot be opened. **Detailed Function Behavior:** - **register_and_open(url: str, browser_name: str, user_preference: str) -> bool** - Parameters: - `url` (str): The URL to be opened. - `browser_name` (str): The name of the new browser type to register. The browser type should be valid and match one of the predefined types in the `webbrowser` module. - `user_preference` (str): Indicates the preferred browser for opening the URL. It can be `\\"default\\"` or `\\"new_browser\\"`. - Returns: - `True` if the URL is opened successfully. - `False` if an error occurs or the URL cannot be opened. **Example Usage:** ```python result = register_and_open(\'http://example.com\', \'firefox\', \'new_browser\') # Firefox should be registered as a browser, and the URL should be opened in Firefox. # The function should return True if successful, False otherwise. ``` **Constraints:** - You may assume a stable internet connection. - The `browser_name` provided should be one of the valid types listed in the documentation. - You should handle exceptions appropriately to ensure the function returns `False` in case of errors. Implement the `register_and_open` function and ensure it meets the above requirements. Use your understanding of the `webbrowser` module\'s functionalities to complete this task.","solution":"import webbrowser def register_and_open(url: str, browser_name: str, user_preference: str) -> bool: Registers a new browser and opens a URL based on user preference. Parameters: - url (str): The URL to be opened. - browser_name (str): The name of the new browser type to register. - user_preference (str): Indicates the preferred browser for opening the URL (\\"default\\" or \\"new_browser\\"). Returns: - bool: True if the URL is opened successfully, False otherwise. try: webbrowser.register(browser_name, None, webbrowser.get(browser_name)) if user_preference == \\"new_browser\\": browser = webbrowser.get(browser_name) else: browser = webbrowser.get() browser.open(url) return True except Exception as e: print(f\\"Error: {e}\\") return False"},{"question":"Objective The objective of this question is to assess the student\'s understanding of PyTorch\'s attention mechanisms and the use of block masks within these mechanisms. The tasks involve creating block masks and integrating them into a simplified attention layer. Problem Statement You are required to implement a simplified version of an attention mechanism using PyTorch. Specifically, you will need to use block masks to control the scope of attention. 1. **Create a Block Mask**: Implement a function `generate_block_mask` that creates a block mask using the `create_block_mask` utility. The function should take the shape of the mask as input arguments. 2. **Attention Layer**: Implement a class `SimpleAttention` that integrates the block masks into its forward pass. The attention mechanism should weigh values by their importance, restricted by the block mask. 3. **Integration with PyTorch**: Ensure your implementation leverages PyTorch\'s capabilities effectively and adheres to best practices for defining models and layers. Function Signatures ```python def generate_block_mask(shape: tuple) -> torch.Tensor: Generates a block mask with the given shape using `create_block_mask`. Args: shape (tuple): A tuple (height, width) representing the dimensions of the mask. Returns: torch.Tensor: The block mask tensor. pass class SimpleAttention(nn.Module): def __init__(self, embed_dim: int): Initialize the SimpleAttention layer. Args: embed_dim (int): The dimension of the input embeddings. super(SimpleAttention, self).__init__() self.embed_dim = embed_dim self.query = nn.Linear(embed_dim, embed_dim) self.key = nn.Linear(embed_dim, embed_dim) self.value = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, mask: torch.Tensor): Forward pass for the SimpleAttention layer. Args: query (torch.Tensor): The query tensor. key (torch.Tensor): The key tensor. value (torch.Tensor): The value tensor. mask (torch.Tensor): The block mask tensor limiting the attention scope. Returns: torch.Tensor: The output of the attention mechanism after applying the block mask. pass ``` Constraints - The `generate_block_mask` function must use the `create_block_mask` utility. - The `SimpleAttention` class must incorporate the generated block mask to filter the attention weights. - Your code should be optimized for computational efficiency and should make good use of PyTorch\'s capabilities. Instructions 1. Implement the `generate_block_mask` function. 2. Implement the `SimpleAttention` class with the specified methods and functionality. 3. Test your implementation to ensure correctness. Example ```python # Example usage of generate_block_mask and SimpleAttention # Generate a block mask of shape (3, 3) mask = generate_block_mask((3, 3)) # Define a SimpleAttention layer with embed_dim of 4 attention_layer = SimpleAttention(embed_dim=4) # Define some dummy data query = key = value = torch.randn(5, 8, 4) # Batch of 5, sequence length 8, embedding dimension 4 # Pass the data through the attention layer output = attention_layer(query, key, value, mask) ```","solution":"import torch import torch.nn as nn def create_block_mask(size): # This is a simplified placeholder for the create_block_mask utility function. # In a real scenario, you would import this from the utility module. return torch.ones(size) def generate_block_mask(shape: tuple) -> torch.Tensor: Generates a block mask with the given shape using `create_block_mask`. Args: shape (tuple): A tuple (height, width) representing the dimensions of the mask. Returns: torch.Tensor: The block mask tensor. return create_block_mask(shape) class SimpleAttention(nn.Module): def __init__(self, embed_dim: int): Initialize the SimpleAttention layer. Args: embed_dim (int): The dimension of the input embeddings. super(SimpleAttention, self).__init__() self.embed_dim = embed_dim self.query = nn.Linear(embed_dim, embed_dim) self.key = nn.Linear(embed_dim, embed_dim) self.value = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, mask: torch.Tensor): Forward pass for the SimpleAttention layer. Args: query (torch.Tensor): The query tensor of shape (batch_size, seq_len, embed_dim). key (torch.Tensor): The key tensor of shape (batch_size, seq_len, embed_dim). value (torch.Tensor): The value tensor of shape (batch_size, seq_len, embed_dim). mask (torch.Tensor): The block mask tensor limiting the attention scope, shape (seq_len, seq_len). Returns: torch.Tensor: The output of the attention mechanism after applying the block mask. # Step 1: Linear projections Q = self.query(query) K = self.key(key) V = self.value(value) # Step 2: Scaled dot-product attention mechanism scores = torch.matmul(Q, K.transpose(-2, -1)) / self.embed_dim**0.5 # Step 3: Apply the mask (making sure mask broadcasting is correct) masked_scores = scores.masked_fill(mask == 0, float(\'-inf\')) # Step 4: Softmax attention_weights = torch.softmax(masked_scores, dim=-1) # Step 5: Weighted sum of values output = torch.matmul(attention_weights, V) return output"},{"question":"Problem: XML Document Parsing and Modification with ElementTree You are provided with an XML document containing information about a catalogue of books. Each book has attributes like `id`, `author`, `genre`, `price`, and `publish_date`, and there are sub-elements like `title` and `description`. Your task is to write Python functions to parse this XML, query specific information, modify certain attributes, and output the modified XML to a file. # 1. Parse the XML Document Write a function `parse_catalog(xml_string: str) -> ET.Element` that accepts a string containing the XML data and returns the root element of the parsed XML tree. # 2. Query Books by Genre Write a function `get_books_by_genre(root: ET.Element, genre: str) -> List[str]` that accepts the root element and a genre string. The function should return a list of book titles that belong to the specified genre. # 3. Increase Prices by Percentage Write a function `increase_prices(root: ET.Element, percentage: float) -> None` that accepts the root element and a percentage value. The function should increase the price of each book by the specified percentage. # 4. Output Modified XML Write a function `write_modified_xml(root: ET.Element, file_path: str) -> None` that accepts the root element of the modified XML tree and a file path. The function should write the modified XML document to the specified file using pretty-printing. # Input and Output **Input:** - `xml_string`: A string containing the XML data. - `root`: The root element of the parsed XML. - `genre`: A string representing the genre of books to query. - `percentage`: A float representing the percentage by which to increase book prices. - `file_path`: A string representing the path of the output file. **Output:** - For `parse_catalog`, return the root element of the parsed XML. - For `get_books_by_genre`, return a list of book titles as strings. - For `increase_prices` and `write_modified_xml`, modify the XML data and write to a file, respectively. # Example ```python xml_data = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies...</description> </book> </catalog> root = parse_catalog(xml_data) # Query books by genre books = get_books_by_genre(root, \\"Computer\\") print(books) # Output: [\'XML Developer\'s Guide\'] # Increase prices by 10% increase_prices(root, 10.0) # Write the modified XML to a file write_modified_xml(root, \\"modified_catalog.xml\\") ``` # Constraints - Use the `xml.etree.ElementTree` module only. - Ensure proper handling and parsing of the XML data. - Handle cases where no books match the specified genre gracefully. Complete the functions to solve the problem as outlined above.","solution":"import xml.etree.ElementTree as ET from typing import List def parse_catalog(xml_string: str) -> ET.Element: Parses the input XML string and returns the root element. tree = ET.ElementTree(ET.fromstring(xml_string)) return tree.getroot() def get_books_by_genre(root: ET.Element, genre: str) -> List[str]: Returns a list of book titles that belong to the specified genre. books = root.findall(f\\".//book[genre=\'{genre}\']\\") return [book.find(\'title\').text for book in books] def increase_prices(root: ET.Element, percentage: float) -> None: Increases the price of each book by the specified percentage. books = root.findall(\'.//book\') for book in books: price = book.find(\'price\') new_price = float(price.text) * (1 + percentage / 100) price.text = f\\"{new_price:.2f}\\" def write_modified_xml(root: ET.Element, file_path: str) -> None: Writes the modified XML to the specified file path using pretty-printing. import xml.dom.minidom rough_string = ET.tostring(root, \'utf-8\') reparsed = xml.dom.minidom.parseString(rough_string) with open(file_path, \\"w\\") as file: file.write(reparsed.toprettyxml(indent=\\" \\"))"},{"question":"Objective: Create a custom codec that can encode and decode a specific transformation in text data and register this codec using the `codecs` module. Additionally, write a function to apply this encoding and another function to decode it. Problem Statement: You are tasked with creating a custom codec named \\"reverse_codec\\" which encodes text by reversing it and decodes it by reversing it back. You will then register this codec using the `codecs` module and write two functions `encode_reverse` and `decode_reverse` utilizing your custom codec. Requirements: 1. Implement a function `reverse_encode(input, errors=\'strict\')` which reverses the input string. 2. Implement a function `reverse_decode(input, errors=\'strict\')` which reverses the input to get back the original string. 3. Define a Codec class `ReverseCodec` with `reverse_encode` and `reverse_decode`. 4. Register the custom codec using `codecs.register()`. 5. Implement a function `encode_reverse(text: str) -> str` that uses the custom codec to encode the given text. 6. Implement a function `decode_reverse(encoded_text: str) -> str` that uses the custom codec to decode the given encoded text. Input: - A string for the `encode_reverse` function. - A reversed (encoded) string for the `decode_reverse` function. Output: - The reversed string as a result of `encode_reverse`. - The original string as a result of `decode_reverse`. Constraints: - The input text for encoding or decoding will only contain printable characters and no special characters or whitespace. - Performance should be efficient for input strings up to 10^5 characters. Example: ```python text = \\"hello\\" encoded_text = encode_reverse(text) # \\"olleh\\" decoded_text = decode_reverse(encoded_text) # \\"hello\\" assert encoded_text == \\"olleh\\" assert decoded_text == \\"hello\\" ``` # Instructions: 1. Create a class `ReverseCodec` with methods for encoding and decoding as described. 2. Register the codec so it can be used with the `codecs` module methods. 3. Implement the `encode_reverse` and `decode_reverse` functions. 4. Ensure all functions adhere to the specified input and output formats. Note: Make sure your solution handles the encoding and decoding processes efficiently and correctly registers the codec with Python\'s `codecs` registry.","solution":"import codecs def reverse_encode(input, errors=\'strict\'): Encodes the input string by reversing it. return input[::-1], len(input) def reverse_decode(input, errors=\'strict\'): Decodes the input string by reversing it back to the original. return input[::-1], len(input) class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): return reverse_encode(input, errors) def decode(self, input, errors=\'strict\'): return reverse_decode(input, errors) def reverse_codec_search_function(encoding): if encoding == \'reverse_codec\': return codecs.CodecInfo( name=\'reverse_codec\', encode=reverse_encode, decode=reverse_decode, incrementalencoder=None, incrementaldecoder=None, streamreader=None, streamwriter=None, ) return None codecs.register(reverse_codec_search_function) def encode_reverse(text: str) -> str: Encodes the given text using the custom reverse codec. return codecs.encode(text, \'reverse_codec\') def decode_reverse(encoded_text: str) -> str: Decodes the given text using the custom reverse codec. return codecs.decode(encoded_text, \'reverse_codec\')"},{"question":"**Coding Assessment Question** **Objective:** The objective of this task is to assess your understanding of Seaborn\'s color palette customization using the `sns.husl_palette` function. You will create a plot with a customized palette and demonstrate your ability to adjust the attributes of the palette. **Problem Statement:** Write a Python function called `custom_palette_plot` that performs the following tasks: 1. Generates a bar plot using a dataset of your choice (create a small sample dataset if needed). 2. Customizes the color palette using Seaborn\'s `sns.husl_palette` function with the following specifications: - Number of colors: 10 - Lightness: 0.5 - Saturation: 0.8 - Returns a continuous colormap 3. Applies the customized palette to the bar plot. 4. Adds appropriate titles and labels to the plot. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def custom_palette_plot(): # Your code here pass ``` **Input:** - None (You will create a dataset within the function) **Output:** - A bar plot displaying the desired customizations. **Constraints:** - You must use the Seaborn library for plotting. - Ensure the plot is displayed using `plt.show()`. **Example:** Your function should create and display a plot similar to the one below: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_palette_plot(): # Sample dataset data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\', \'J\'], \'Values\': [3, 4, 2, 5, 7, 6, 5, 4, 3, 5] }) # Customizing the palette palette = sns.husl_palette(n_colors=10, l=0.5, s=0.8, as_cmap=True) # Creating the plot sns.set_theme(style=\\"whitegrid\\") bar_plot = sns.barplot(x=\'Category\', y=\'Values\', data=data, palette=palette) # Adding titles and labels bar_plot.set_title(\'Customized Color Palette Bar Plot\') bar_plot.set_xlabel(\'Category\') bar_plot.set_ylabel(\'Values\') # Display the plot plt.show() # Calling the function to display the plot custom_palette_plot() ``` Ensure that your solution adheres to the constraints, and the plot correctly reflects the custom palette settings.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_palette_plot(): # Sample dataset data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\', \'J\'], \'Values\': [3, 4, 2, 5, 7, 6, 5, 4, 3, 5] }) # Customizing the palette palette = sns.husl_palette(n_colors=10, l=0.5, s=0.8) # Creating the plot sns.set_theme(style=\\"whitegrid\\") bar_plot = sns.barplot(x=\'Category\', y=\'Values\', data=data, palette=palette) # Adding titles and labels bar_plot.set_title(\'Customized Color Palette Bar Plot\') bar_plot.set_xlabel(\'Category\') bar_plot.set_ylabel(\'Values\') # Display the plot plt.show()"},{"question":"# Coding Assessment: Support Vector Machines with scikit-learn **Objective:** This assessment will test your ability to implement and utilize Support Vector Machines (SVMs) using the scikit-learn library. You will work with a dataset to perform both classification and regression tasks. # Part 1: Classification **Problem Statement:** Implement a function that trains an SVM classifier on the Iris dataset and predicts the class of new samples. Function Signature ```python def train_and_predict_iris_svm(C: float, kernel: str, gamma: str, new_samples: List[List[float]]) -> List[int]: Trains an SVM classifier on the Iris dataset and predicts the class of new samples. Parameters: - C (float): Regularization parameter. - kernel (str): Specifies the kernel type to be used in the algorithm (\'linear\', \'poly\', \'rbf\', \'sigmoid\'). - gamma (str): Kernel coefficient for \'rbf\', \'poly\' and \'sigmoid\'. - new_samples (List[List[float]]): A list of samples to predict the class for. Returns: - List[int]: Predicted classes for the new samples. ``` Requirements 1. Load the Iris dataset from `sklearn.datasets`. 2. Train an SVM classifier using the specified parameters (`C`, `kernel`, and `gamma`). 3. Use the trained model to predict the classes for the `new_samples`. # Part 2: Regression **Problem Statement:** Implement a function that trains an SVM regressor on a synthetic dataset and predicts values for new data points. Function Signature ```python def train_and_predict_svr(C: float, kernel: str, epsilon: float, new_samples: List[List[float]]) -> List[float]: Trains an SVR model on a synthetic dataset and predicts values for new samples. Parameters: - C (float): Regularization parameter. - kernel (str): Specifies the kernel type to be used in the algorithm (\'linear\', \'poly\', \'rbf\', \'sigmoid\'). - epsilon (float): Epsilon parameter in the epsilon-insensitive loss function. - new_samples (List[List[float]]): A list of samples to predict the values for. Returns: - List[float]: Predicted values for the new samples. ``` Requirements 1. Create a synthetic dataset using `sklearn.datasets.make_regression`. 2. Train an SVR model using the specified parameters (`C`, `kernel`, and `epsilon`). 3. Use the trained model to predict values for the `new_samples`. # Constraints and Performance Requirements - Ensure the solution handles datasets with up to 150 samples efficiently. - Use appropriate data preprocessing techniques if necessary (standardization, normalization). - Validate input parameters and handle exceptions gracefully. Example Usage ```python # Example for classification C = 1.0 kernel = \'rbf\' gamma = \'scale\' new_samples = [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.7, 1.5], [7.0, 3.2, 4.8, 1.8]] print(train_and_predict_iris_svm(C, kernel, gamma, new_samples)) # Output: [0, 1, 1] # Example for regression C = 1.0 kernel = \'rbf\' epsilon = 0.1 new_samples = [[0.5], [1.5], [2.5]] print(train_and_predict_svr(C, kernel, epsilon, new_samples)) # Output: [0.47, 1.56, 2.54] ``` Submit the implemented function definitions for evaluation. Ensure that your code is clean, well-documented, and follows best coding practices.","solution":"from typing import List from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, SVR def train_and_predict_iris_svm(C: float, kernel: str, gamma: str, new_samples: List[List[float]]) -> List[int]: # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) new_samples = scaler.transform(new_samples) # Split the data for training and testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the SVM model svm = SVC(C=C, kernel=kernel, gamma=gamma) svm.fit(X_train, y_train) # Predict the classes for new samples predicted_classes = svm.predict(new_samples).tolist() return predicted_classes def train_and_predict_svr(C: float, kernel: str, epsilon: float, new_samples: List[List[float]]) -> List[float]: # Create a synthetic regression dataset X, y = datasets.make_regression(n_samples=150, n_features=1, noise=0.1, random_state=42) # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) new_samples = scaler.transform(new_samples) # Split the data for training and testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the SVR model svr = SVR(C=C, kernel=kernel, epsilon=epsilon) svr.fit(X_train, y_train) # Predict the values for new samples predicted_values = svr.predict(new_samples).tolist() return predicted_values"},{"question":"# Time Series Analysis with Pandas Objective Implement a set of functions to demonstrate your mastery of pandas time series manipulation, focusing on creating time series, applying time zones, resampling data, and handling custom business days. Problem Statement You are given sales data for a company which operates from Monday to Friday, recorded every hour. The sales data need to be processed and analyzed for business operations. Implement the following functions: 1. **`generate_time_series(start: str, end: str, freq: str, tz: str) -> pd.Series`**: - Create a time series starting from `start` date to `end` date with a frequency of `freq` and localize it to the time zone `tz`. - **Inputs**: - `start` (str): Start date in the format `YYYY-MM-DD` - `end` (str): End date in the format `YYYY-MM-DD` - `freq` (str): Frequency string as accepted by `pd.date_range` - `tz` (str): Time zone string. - **Output**: - `pd.Series`: Time series with random sales data for the given date range and frequency, localized to the given time zone. 2. **`resample_and_aggregate(ts: pd.Series, freq: str, method: str) -> pd.Series`**: - Resample and aggregate the given time series `ts` using the specified `freq` and aggregation method `method`. - **Inputs**: - `ts` (pd.Series): Input time series with time indexes. - `freq` (str): Frequency string for resampling. - `method` (str): Aggregation method (`\'sum\'`, `\'mean\'`, `\'max\'`, `\'min\'`, `\'ohlc\'`). - **Output**: - `pd.Series`: Resampled and aggregated time series. 3. **`apply_custom_business_days(ts: pd.Series, weekmask: str, holidays: List[str]) -> pd.Series`**: - Adjust the given time series `ts` to respect a custom business day calendar. - **Inputs**: - `ts` (pd.Series): Input time series with time indexes. - `weekmask` (str): Weekmask string indicating valid business days (e.g., \'Mon Tue Wed Thu Fri\'). - `holidays` (List[str]): List of holiday dates in the format `YYYY-MM-DD`. - **Output**: - `pd.Series`: Time series adjusted to respect the custom business day calendar. 4. **`analyze_sales(ts: pd.Series, start: str, end: str, method: str) -> pd.DataFrame`**: - Analyze the given time series `ts`, resampling it to daily frequency and then summarizing data between `start` and `end` dates using the provided aggregation method. - **Inputs**: - `ts` (pd.Series): Input time series with time indexes. - `start` (str): Start date for the analysis period in the format `YYYY-MM-DD`. - `end` (str): End date for the analysis period in the format `YYYY-MM-DD`. - `method` (str): Aggregation method (`\'sum\'`, `\'mean\'`, `\'max\'`, `\'min\'`, `\'ohlc\'`). - **Output**: - `pd.DataFrame`: DataFrame with daily summarized data for the given date range and aggregation method. The sample sales data generation and analysis should reflect the following structure and constraints: 1. **Sales data generation**: Create evenly distributed random sales data over the given time frame. 2. **Timezone adjustments**: Handle the provided timezone correctly in localization and conversion. 3. **Given Constraints**: - `start` < `end`. - The frequency `freq` is valid. - The aggregation method `method` is one of `\'sum\'`, `\'mean\'`, `\'max\'`, `\'min\'`, `\'ohlc\'`. - The weekmask is a valid combination of weekdays. Example Usage ```python # Generate a time series sales_ts = generate_time_series(\\"2023-01-01\\", \\"2023-01-31\\", \\"1H\\", \\"America/New_York\\") # Resample and aggregate sales data to daily totals daily_sales = resample_and_aggregate(sales_ts, \\"1D\\", \\"sum\\") # Apply custom business days criteria custom_sales = apply_custom_business_days(daily_sales, \'Mon Tue Wed Thu Fri\', [\'2023-01-02\', \'2023-01-16\']) # Analyze the sales data for a specific period analysis_result = analyze_sales(custom_sales, \'2023-01-01\', \'2023-01-31\', \'mean\') print(analysis_result) ``` This problem is intended to assess the candidate’s ability to work with time series data in pandas, handle time zone conversions, resample data, and respect business calendars.","solution":"import pandas as pd import numpy as np from pandas.tseries.offsets import CustomBusinessDay from typing import List def generate_time_series(start: str, end: str, freq: str, tz: str) -> pd.Series: Create a time series with random sales data from `start` to `end` at a specified frequency and time zone. date_rng = pd.date_range(start=start, end=end, freq=freq, tz=tz) sales_data = np.random.randint(1, 100, size=len(date_rng)) return pd.Series(sales_data, index=date_rng) def resample_and_aggregate(ts: pd.Series, freq: str, method: str) -> pd.Series: Resample and aggregate the time series data. if method == \'sum\': return ts.resample(freq).sum() elif method == \'mean\': return ts.resample(freq).mean() elif method == \'max\': return ts.resample(freq).max() elif method == \'min\': return ts.resample(freq).min() elif method == \'ohlc\': return ts.resample(freq).ohlc() else: raise ValueError(\\"Method should be one of \'sum\', \'mean\', \'max\', \'min\', \'ohlc\'.\\") def apply_custom_business_days(ts: pd.Series, weekmask: str, holidays: List[str]) -> pd.Series: Adjust the time series to custom business days. holidays = pd.to_datetime(holidays) custom_bday = CustomBusinessDay(weekmask=weekmask, holidays=holidays) return ts.asfreq(custom_bday) def analyze_sales(ts: pd.Series, start: str, end: str, method: str) -> pd.DataFrame: Analyze the sales data, resampling to daily frequency and summarizing between `start` and `end` dates. ts = ts[start:end] return resample_and_aggregate(ts, \'1D\', method)"},{"question":"**Question:** Implement a function `custom_seaborn_plot` that creates a seaborn scatter plot with customized color palettes and saves the plot as an image file. # Function Signature: ```python def custom_seaborn_plot(data: pd.DataFrame, x_col: str, y_col: str, palette_type: str, palette_options: dict, output_file: str) -> None: pass ``` # Inputs: 1. **data**: A pandas DataFrame containing the data to be plotted. 2. **x_col**: A string representing the name of the column to be used for the x-axis. 3. **y_col**: A string representing the name of the column to be used for the y-axis. 4. **palette_type**: A string indicating the type of color palette to be used. Valid values include: \\"categorical\\", \\"diverging\\", \\"sequential_light\\", \\"sequential_dark\\", \\"blend\\". 5. **palette_options**: A dictionary containing additional options for customizing the selected palette. The keys should be specific to the chosen palette type. For example: - For \\"categorical\\": {\'name\': str} - For \\"diverging\\": {\'name\': str, \'as_cmap\': bool} - For \\"sequential_light\\": {\'color\': str, \'as_cmap\': bool} - For \\"sequential_dark\\": {\'color\': str, \'as_cmap\': bool} - For \\"blend\\": {\'start\': str, \'end\': str, \'as_cmap\': bool} 6. **output_file**: A string representing the file path where the plot image should be saved. # Outputs: - The function does not return anything. It should save the generated plot as an image file to the specified `output_file` path. # Constraints: - The input DataFrame must have the specified `x_col` and `y_col`. - The palette customization should strictly adhere to the options detailed above. - The output image format should be automatically inferred from the file extension provided in `output_file`. # Example Usage: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [i**2 for i in range(10)] }) # Custom palette options palette_options = {\'color\': \'#5A9\', \'as_cmap\': True} # Create and save scatter plot custom_seaborn_plot(data, \'x\', \'y\', \'sequential_light\', palette_options, \'output.png\') ``` **Notes:** - Include necessary error handling for invalid inputs. - Ensure that the color palette application aligns with the seaborn documentation provided. - Make the plot visually appealing and ensure the chosen color palette has a clear impact on the plot\'s appearance.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot(data: pd.DataFrame, x_col: str, y_col: str, palette_type: str, palette_options: dict, output_file: str) -> None: if x_col not in data.columns or y_col not in data.columns: raise ValueError(f\\"Columns \'{x_col}\' or \'{y_col}\' not found in DataFrame.\\") palette = None if palette_type == \\"categorical\\": palette = sns.color_palette(palette_options.get(\'name\', \'deep\')) elif palette_type == \\"diverging\\": palette = sns.color_palette(palette_options.get(\'name\', \'coolwarm\'), as_cmap=palette_options.get(\'as_cmap\', False)) elif palette_type == \\"sequential_light\\": palette = sns.light_palette(palette_options.get(\'color\', \'blue\'), as_cmap=palette_options.get(\'as_cmap\', False)) elif palette_type == \\"sequential_dark\\": palette = sns.dark_palette(palette_options.get(\'color\', \'blue\'), as_cmap=palette_options.get(\'as_cmap\', False)) elif palette_type == \\"blend\\": palette = sns.blend_palette([palette_options.get(\'start\', \'blue\'), palette_options.get(\'end\', \'green\')], as_cmap=palette_options.get(\'as_cmap\', False)) else: raise ValueError(\\"Invalid palette type provided.\\") plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=x_col, y=y_col, palette=palette) plt.savefig(output_file) plt.close()"},{"question":"**Question: Efficient Sorted List Management with Bisect** You are developing a system to track and manage a sorted list of events, where each event is represented as a dictionary with keys \'event_name\', \'start_time\', and \'duration\'. The `start_time` is given in seconds since the epoch (January 1st, 1970). To keep the list sorted by `start_time` while minimizing the time complexity of insertions and searches, you must implement functions that use the `bisect` module to manage the events. **Tasks:** 1. Write a function `add_event(events, new_event)` that takes a sorted list of events and a new event to insert into the list while maintaining the order. Use appropriate functions from the `bisect` module. 2. Write a function `find_next_event(events, current_time)` that returns the details of the first event that starts after `current_time`. If there are no events after `current_time`, return `None`. 3. Write a function `find_events_in_range(events, start_time, end_time)` that returns a list of events that are scheduled to start within the specified time range `[start_time, end_time)`, inclusive of `start_time` and exclusive of `end_time`. **Constraints:** - The list `events` is always sorted by `start_time`. - Each event is a dictionary with keys \'event_name\', \'start_time\', and \'duration\'. - You may assume the input list `events` and `new_event` are valid and `start_time` and `end_time` are integers representing seconds since the epoch. - Aim for a time complexity of O(log n) for searching and O(n) for insertion, where `n` is the number of events in the list. **Function Signatures:** ```python def add_event(events: List[Dict[str, Union[str, int]]], new_event: Dict[str, Union[str, int]]) -> None: pass def find_next_event(events: List[Dict[str, Union[str, int]]], current_time: int) -> Optional[Dict[str, Union[str, int]]]: pass def find_events_in_range(events: List[Dict[str, Union[str, int]]], start_time: int, end_time: int) -> List[Dict[str, Union[str, int]]]: pass ``` **Example Usage:** ```python events = [ {\'event_name\': \'Event1\', \'start_time\': 1620000000, \'duration\': 3600}, {\'event_name\': \'Event2\', \'start_time\': 1620007200, \'duration\': 1800}, ] new_event = {\'event_name\': \'Event3\', \'start_time\': 1620003600, \'duration\': 7200} add_event(events, new_event) # Now events should have \'Event3\' inserted in the correct position. next_event = find_next_event(events, 1620001000) # Should return the details of \'Event3\' range_events = find_events_in_range(events, 1620000000, 1620009000) # Should return the details of \'Event1\' and \'Event3\' ``` **Note:** - Make sure to write efficient implementations taking advantage of the `bisect` module functions.","solution":"from bisect import bisect_left, bisect_right from typing import List, Dict, Union, Optional def add_event(events: List[Dict[str, Union[str, int]]], new_event: Dict[str, Union[str, int]]) -> None: Inserts a new event into the sorted list of events while maintaining the order by start_time. index = bisect_left([event[\'start_time\'] for event in events], new_event[\'start_time\']) events.insert(index, new_event) def find_next_event(events: List[Dict[str, Union[str, int]]], current_time: int) -> Optional[Dict[str, Union[str, int]]]: Finds the first event that starts after the current_time. index = bisect_right([event[\'start_time\'] for event in events], current_time) if index < len(events): return events[index] return None def find_events_in_range(events: List[Dict[str, Union[str, int]]], start_time: int, end_time: int) -> List[Dict[str, Union[str, int]]]: Returns a list of events that start within the given time range [start_time, end_time). left_index = bisect_left([event[\'start_time\'] for event in events], start_time) right_index = bisect_left([event[\'start_time\'] for event in events], end_time) return events[left_index:right_index]"},{"question":"**Seaborn Plotting Challenge** You are tasked with visualizing a dataset using Seaborn\'s `objects` interface to demonstrate your understanding of advanced plotting techniques. The dataset `tips` is provided within the seaborn library and contains data about tips received in a restaurant based on various factors. Using the `tips` dataset, create a multi-faceted scatter plot that visualizes the relationship between `total_bill` and `tip`, while differentiating by `day` and `sex`. Also, add error bars representing the standard error. # Specific Requirements: 1. **Plot**: Create a plot object with `total_bill` on the x-axis and `tip` on the y-axis. 2. **Faceting**: Facet the plot by `day`. 3. **Dots and Styles**: - Add dots (`so.Dot()`) with edge color white. - Add jitter to reduce overlap. 4. **Differentiation**: Use different color markers based on `sex`. 5. **Error Bars**: Include error bars representing the standard error of the mean tip for each total bill. # Expected Input and Output - **Input**: `None` (you will use the `tips` dataset from seaborn). - **Output**: A Seaborn plot that satisfies the above requirements. # Constraints - Ensure your plot is aesthetically pleasing. - Use only `seaborn.objects` submodule and relevant methods. # Example of the Required Output ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add faceting, dots, jitter, and error bars (p .facet(\\"day\\", wrap=2) .add(so.Dot(edgecolor=\\"w\\")) .add(so.Jitter(.2)) .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) .scale(color=\\"sex\\", marker=[\\"o\\", \\"s\\"]) ) ``` **Note**: The example provides a template to get you started but does not include everything required by the question. You need to complete the plot as per the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_seaborn_plot(): # Load dataset tips = load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(.2)) .add(so.Range(), so.Est(errorbar=\\"se\\")) ) p.show()"},{"question":"# Coding Assessment: Exploring Relationship Between Parameters with Seaborn\'s JointGrid **Objective:** Create a Python function using the `seaborn` library that visualizes the relationship between two numerical variables from the penguins dataset and includes additional customizations as specified. **Function Signature:** ```python def plot_penguins_relationship(height: float, ratio: float, space: float, joint_func, marginal_func, joint_kwargs: dict, marginal_kwargs: dict): pass ``` **Input:** 1. `height` (float): Height of the `JointGrid`. 2. `ratio` (float): Ratio between the joint and marginal axes. 3. `space` (float): Amount of space between plots. 4. `joint_func` (callable): Seaborn function for the joint plot (e.g., `sns.scatterplot`). 5. `marginal_func` (callable): Seaborn function for the marginal plots (e.g., `sns.histplot`). 6. `joint_kwargs` (dict): Dictionary of additional keyword arguments to pass to `joint_func`. 7. `marginal_kwargs` (dict): Dictionary of additional keyword arguments to pass to `marginal_func`. **Output:** - A `JointGrid` plot visualizing the relationship between `bill_length_mm` and `bill_depth_mm` with the specified customizations. **Constraints:** - Ensure that the joint and marginal plots use the same dataset (`penguins` dataset). - The function should handle cases where the keyword arguments dictionaries are empty. - Use appropriate parameter names in the plotting functions according to Seaborn’s documentation. **Instructions:** 1. Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. 2. Create a `JointGrid` plot with the given `height`, `ratio`, and `space`. 3. Use the provided `joint_func` with `joint_kwargs` to plot the joint plot. 4. Use the provided `marginal_func` with `marginal_kwargs` to plot the marginal histograms. 5. Ensure the resulting plot shows the relationship between the `bill_length_mm` and `bill_depth_mm`. 6. Add suitable titles and axis labels to the plots. **Example Usage:** ```python plot_penguins_relationship( height=6, ratio=2, space=0.1, joint_func=sns.scatterplot, marginal_func=sns.histplot, joint_kwargs={\\"color\\": \\"blue\\", \\"alpha\\": 0.5}, marginal_kwargs={\\"kde\\": True, \\"color\\": \\"blue\\"} ) ``` **Notes:** - Ensure that your implementation offers flexibility in terms of the seaborn functions and keyword arguments it accepts. - Provide an explanation or inline comments in your code for better clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_relationship(height: float, ratio: float, space: float, joint_func, marginal_func, joint_kwargs: dict, marginal_kwargs: dict): Creates a JointGrid plot visualizing the relationship between bill_length_mm and bill_depth_mm in the penguins dataset. Parameters: - height (float): Height of the JointGrid. - ratio (float): Ratio between the joint and marginal axes. - space (float): Amount of space between plots. - joint_func (callable): Seaborn function for the joint plot. - marginal_func (callable): Seaborn function for the marginal plots. - joint_kwargs (dict): Dictionary of additional keyword arguments for joint_func. - marginal_kwargs (dict): Dictionary of additional keyword arguments for marginal_func. # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a JointGrid with the specified height, ratio, and space grid = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=height, ratio=ratio, space=space) # Use the provided joint_func and marginal_func with the given **kwargs joint_func(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, ax=grid.ax_joint, **joint_kwargs) marginal_func(x=\\"bill_length_mm\\", data=penguins, ax=grid.ax_marg_x, **marginal_kwargs) marginal_func(y=\\"bill_depth_mm\\", data=penguins, ax=grid.ax_marg_y, **marginal_kwargs) # Set titles and labels grid.ax_joint.set_title(\\"Relationship between Bill Length and Bill Depth\\") grid.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") plt.show()"},{"question":"# Custom HTML Parser Implementation **Objective:** Create a custom HTML parser that extracts and processes specific patterns from a given HTML document. This will test your understanding of the `HTMLParser` class, including subclassing, method overriding, and custom behavior implementation. **Task:** 1. Create a subclass of `HTMLParser` named `CustomHTMLParser`. 2. Override the following methods to meet the specified requirements: - `handle_starttag(self, tag, attrs)`: Populate a dictionary where each key is a tag name and the value is a list of its attributes. - `handle_endtag(self, tag)`: Print a message each time an end tag is encountered. - `handle_data(self, data)`: Append any data enclosed by `<h1>` tags to a list. - `handle_comment(self, data)`: Count the number of HTML comments encountered. 3. Implement a method `get_h1_texts(self)` that returns the list of texts enclosed by `<h1>` tags. 4. Implement a method `get_comment_count(self)` that returns the total number of comments encountered. 5. Implement a method `get_tags_with_attrs(self)` that returns the dictionary of tags with their attributes. **Input:** - A string `html_content` containing the HTML document to be parsed. **Output:** - Using `get_h1_texts()`, `get_comment_count()`, and `get_tags_with_attrs()` methods, return the following: - A list of all texts enclosed by `<h1>` tags. - The total number of HTML comments. - A dictionary of all tags with their attributes. **Example:** ```python html_content = <html> <head> <title>Test Document</title> </head> <body> <!-- This is a comment --> <h1>First Heading</h1> <p>Some text here.</p> <div id=\\"container\\"> <!-- Another comment --> <h1>Second Heading</h1> <a href=\\"http://example.com\\">Link</a> </div> </body> </html> parser = CustomHTMLParser() parser.feed(html_content) # Expected Outputs print(parser.get_h1_texts()) # [\'First Heading\', \'Second Heading\'] print(parser.get_comment_count()) # 2 print(parser.get_tags_with_attrs()) # {\'div\': [(\'id\', \'container\')], \'a\': [(\'href\', \'http://example.com\')]} ``` **Constraints:** - You may assume existing HTML content is well-formed for simplicity, despite the parser being capable of handling some invalid HTML. - You should handle case-insensitivity appropriately as HTML tags are case-insensitive. - Your implementation should handle unexpected edge cases gracefully and should not raise errors for unseen events. You are encouraged to test the provided sample input to verify your implementation works as required.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tags_with_attrs = {} self.h1_texts = [] self.comment_count = 0 self.current_tag = None def handle_starttag(self, tag, attrs): if tag not in self.tags_with_attrs: self.tags_with_attrs[tag.lower()] = [] self.tags_with_attrs[tag.lower()].extend(attrs) self.current_tag = tag.lower() def handle_endtag(self, tag): print(f\\"Encountered end tag: {tag}\\") self.current_tag = None def handle_data(self, data): if self.current_tag == \'h1\': self.h1_texts.append(data.strip()) def handle_comment(self, data): self.comment_count += 1 def get_h1_texts(self): return self.h1_texts def get_comment_count(self): return self.comment_count def get_tags_with_attrs(self): return self.tags_with_attrs # Usage example: # html_content = # <html> # <head> # <title>Test Document</title> # </head> # <body> # <!-- This is a comment --> # <h1>First Heading</h1> # <p>Some text here.</p> # <div id=\\"container\\"> # <!-- Another comment --> # <h1>Second Heading</h1> # <a href=\\"http://example.com\\">Link</a> # </div> # </body> # </html> # # parser = CustomHTMLParser() # parser.feed(html_content) # print(parser.get_h1_texts()) # [\'First Heading\', \'Second Heading\'] # print(parser.get_comment_count()) # 2 # print(parser.get_tags_with_attrs()) # {\'div\': [(\'id\', \'container\')], \'a\': [(\'href\', \'http://example.com\')]}"},{"question":"You are provided with a pandas DataFrame which might contain duplicate indices (row labels) and/or column labels. Your task is to implement a function `process_and_secure_df` which performs the following operations: 1. **Detect and Report Duplicate Labels:** - Check for duplicate row labels and column labels. - Print a message indicating whether duplicates were found in row labels and/or column labels. 2. **Remove Duplicate Indices:** - For duplicate row labels, keep only the first occurrence. - For duplicate column labels, keep only the first occurrence. 3. **Disallow Duplicate Labels:** - Ensure that the DataFrame disallows any future introduction of duplicate labels. Expected Function Signature ```python import pandas as pd def process_and_secure_df(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` Input - `df`: A pandas DataFrame that may contain duplicate row and/or column labels. Output - Return a pandas DataFrame that: - No longer has any duplicate row or column labels (only the first occurrence of duplicates is kept). - Disallows any future introduction of duplicate labels. Example ```python import pandas as pd # Example DataFrame with duplicate row and column labels data = { \\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6], \\"A\\": [7, 8, 9], } index = [\\"x\\", \\"y\\", \\"x\\"] df = pd.DataFrame(data, index=index) # Process and secure the DataFrame result_df = process_and_secure_df(df) print(result_df) # Output DataFrame should not have duplicate labels and should disallow duplicates ``` Constraints 1. The function should handle DataFrame of any size. 2. The function should only keep the first occurrence when removing duplicates. 3. Use the pandas library functions to implement the solution.","solution":"import pandas as pd def process_and_secure_df(df: pd.DataFrame) -> pd.DataFrame: # Check for duplicate row labels and column labels duplicate_row_labels = df.index.duplicated(keep=\'first\') duplicate_col_labels = df.columns.duplicated(keep=\'first\') # Reporting duplicates if any(duplicate_row_labels): print(\\"Duplicate row labels found.\\") if any(duplicate_col_labels): print(\\"Duplicate column labels found.\\") # Remove duplicate row labels and keep only the first occurrence df_no_duplicate_rows = df[~df.index.duplicated(keep=\'first\')] # Remove duplicate column labels and keep only the first occurrence df_no_duplicate_cols = df_no_duplicate_rows.loc[:, ~df_no_duplicate_rows.columns.duplicated(keep=\'first\')] # Disallow any future introduction of duplicate labels df_no_duplicate_cols.index.name = \'index\' df_no_duplicate_cols.columns.name = \'columns\' return df_no_duplicate_cols"},{"question":"# Managing Duplicate Labels in DataFrames You are tasked with writing a function to clean and manage duplicate labels in a pandas DataFrame. The function should: 1. Identify and report any duplicate row or column labels. 2. Remove any duplicate row labels by keeping only the first occurrence. 3. Disallow duplicates going forward after the cleanup. Function Signature ```python def clean_and_manage_duplicates(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df`: A pandas DataFrame which may contain duplicate row and/or column labels. Output - Returns a pandas DataFrame that: - Does not contain duplicate row labels. - Disallows duplicate row and column labels going forward. Constraints - You cannot use any third-party libraries apart from pandas and numpy. - You should not modify the input DataFrame directly; instead, return a new DataFrame with the desired properties. Example ```python import pandas as pd df = pd.DataFrame( {\\"A\\": [0, 1, 2, 3], \\"B\\": [4, 5, 6, 7]}, index=[\\"x\\", \\"y\\", \\"x\\", \\"z\\"] ) new_df = clean_and_manage_duplicates(df) ``` The output `new_df` should: - Not contain duplicate row labels (`\\"x\\"` in this case). - Disallow any new duplicates in row or column labels. Use the following test case to ensure the function works as expected: ```python import pandas as pd df = pd.DataFrame( {\\"A\\": [0, 1, 2, 3], \\"B\\": [4, 5, 6, 7]}, index=[\\"x\\", \\"y\\", \\"x\\", \\"z\\"] ) new_df = clean_and_manage_duplicates(df) assert not new_df.index.duplicated().any(), \\"There should be no duplicate row labels\\" assert not new_df.flags.allows_duplicate_labels, \\"Duplicate labels should be disallowed\\" ``` Hint Use pandas methods like `Index.duplicated()`, `DataFrame.groupby()`, and `DataFrame.set_flags()` to accomplish the task.","solution":"import pandas as pd def clean_and_manage_duplicates(df: pd.DataFrame) -> pd.DataFrame: Cleans the DataFrame by removing duplicate row labels and ensuring no duplicate labels are allowed in the resulting DataFrame. Parameters: df (pd.DataFrame): The input DataFrame which may contain duplicate row and/or column labels. Returns: pd.DataFrame: A new DataFrame with no duplicate row labels and disallowed duplicate labels. # Remove duplicate row labels by keeping only the first occurrence df_cleaned = df.loc[~df.index.duplicated(keep=\'first\')] # Disallow duplicates for the future DataFrame df_cleaned.flags.allows_duplicate_labels = False return df_cleaned"},{"question":"Objective: Your task is to implement a Python function using the `ossaudiodev` module to: 1. Open an audio device. 2. Set the audio format, number of channels, and sampling rate. 3. Write a sample audio data to the device. 4. Open a mixer device and set the volume. 5. Ensure proper error handling and resource management. Function Signature: ```python def setup_and_play_audio(format: int, nchannels: int, samplerate: int, volume: int, audio_data: bytes) -> bool: Sets up the audio device with specified format, channels, and sample rate, then plays the given audio data. Also, sets the master volume using the mixer device. :param format: The audio format (e.g., ossaudiodev.AFMT_S16_LE) :param nchannels: Number of audio channels (e.g., 1 for mono, 2 for stereo) :param samplerate: Sampling rate in Hz (e.g., 44100 for CD quality) :param volume: Volume level (0 to 100) :param audio_data: The audio data to be played as bytes :return: True if successful, False otherwise pass ``` Expected Behavior: 1. Open the default audio device for writing. 2. Set the audio parameters: - Format - Number of channels - Sample rate 3. Write the provided audio data to the device. 4. Open the default mixer device. 5. Set the master volume to the provided level. 6. Properly handle any exceptions and ensure that all opened devices are closed. 7. Return `True` if all operations were successful, otherwise return `False`. Constraints: - You should handle any errors gracefully and ensure that devices are closed even if an error occurs. - Make sure the audio device is in blocking mode (default mode). - Use the appropriate constants for the audio format and mixer control. Example: ```python audio_data = b\'x00x01x02...\' # Example audio data result = setup_and_play_audio(ossaudiodev.AFMT_S16_LE, 2, 44100, 75, audio_data) print(result) # Should print True if successful ``` > Note: You may need to import the `ossaudiodev` module and use the constants defined in it.","solution":"import ossaudiodev def setup_and_play_audio(format: int, nchannels: int, samplerate: int, volume: int, audio_data: bytes) -> bool: Sets up the audio device with specified format, channels, and sample rate, then plays the given audio data. Also, sets the master volume using the mixer device. :param format: The audio format (e.g., ossaudiodev.AFMT_S16_LE) :param nchannels: Number of audio channels (e.g., 1 for mono, 2 for stereo) :param samplerate: Sampling rate in Hz (e.g., 44100 for CD quality) :param volume: Volume level (0 to 100) :param audio_data: The audio data to be played as bytes :return: True if successful, False otherwise try: # Open the audio device in write mode audio_device = ossaudiodev.open(\'w\') # Set the audio format, channels, and rate audio_device.setfmt(format) audio_device.channels(nchannels) audio_device.speed(samplerate) # Write the audio data to the device audio_device.write(audio_data) # Open the mixer device mixer_device = ossaudiodev.openmixer() # Set the master volume mixer_device.set(ossaudiodev.SOUND_MIXER_VOLUME, (volume, volume)) # Close the devices audio_device.close() mixer_device.close() return True except Exception as e: # Ensure devices are closed if they were opened try: audio_device.close() except: pass try: mixer_device.close() except: pass print(f\'An error occurred: {e}\') return False"},{"question":"**Title**: Implement Platform-Aware Asynchronous I/O Operations **Objective**: Demonstrate the ability to understand and utilize Python\'s \\"asyncio\\" module with considerations for platform-specific limitations and functionalities. **Problem Statement**: You are required to implement a function `read_socket_data` that asynchronously reads data from a given socket. Your function should handle platform-specific differences using `asyncio`. Ensure that your implementation: - Uses the appropriate event loop based on the platform it is running on (Windows vs. other platforms). - Incorporates the necessary platform-specific checks and functionality. - Handles exceptions gracefully and provides meaningful error messages. **Function Signature**: ```python import asyncio import socket async def read_socket_data(sock: socket.socket) -> str: pass ``` **Input**: - `sock`: A `socket.socket` object which you need to read data from. **Output**: - Returns the read data as a string. **Constraints**: - You must use `asyncio` for asynchronous I/O operations. - Your solution should distinguish between different event loops used on Windows and other platforms. - Handle I/O limitations and provide alternative solutions or error messages where specific functionalities are unsupported. **Examples**: ```python import socket import asyncio # Example socket setup (pseudo-code, actual implementation may vary) sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect((\'localhost\', 8080)) # Read data using the function data = asyncio.run(read_socket_data(sock)) print(data) ``` **Notes**: 1. Review the platform-based restrictions in the original documentation while implementing your solution. 2. You may need to simulate or mock some behaviors if running cross-platform code. **Performance Requirements**: - Your solution should efficiently handle asynchronous I/O operations without blocking the event loop. **Hints**: - Use `asyncio.get_event_loop()` to retrieve the current event loop. - Handle different event loop types such as `SelectorEventLoop` and `ProactorEventLoop`. - Look for platform-specific modules or methods mentioned in the documentation (`Lib/asyncio/windows_events.py`, `selectors.KqueueSelector` for macOS, etc.)","solution":"import asyncio import socket import platform async def read_socket_data(sock: socket.socket) -> str: loop = asyncio.get_running_loop() if platform.system() == \'Windows\': return await loop.sock_recv(sock, 4096) else: return await sock.recv(4096).decode(\'utf-8\') # Note: The `loop.sock_recv` method is available only in Windows-specific event loop. # On Unix-like systems the normal socket.recv should be awaited directly."},{"question":"# Memory Leak Detection using `tracemalloc` Module **Objective:** Write a Python script that identifies memory leaks by comparing memory allocation snapshots before and after a sequence of operations in a given application. **Problem Statement:** You are provided with a sample Python function that performs several operations. Your task is to use the `tracemalloc` module to detect potential memory leaks by keeping track of memory allocations before and after the function execution. You should output the top 10 differences in memory allocations. **Function to analyze:** ```python def sample_function(): # Simulate a memory-intensive operation data = [i ** 2 for i in range(10000)] return sum(data) ``` **Task:** 1. Start tracing memory allocations. 2. Take a snapshot of the memory status before executing `sample_function()`. 3. Execute the `sample_function()`. 4. Take another snapshot of memory status after executing the function. 5. Compare both snapshots to identify the top 10 differences in memory allocations. 6. Filter out any allocations from the `tracemalloc` and `<frozen importlib._bootstrap>` modules. 7. Output the results in a readable format. **Input and Output:** - **Input:** No input required. The function `sample_function` should be analyzed by directly incorporating it into your script. - **Output:** Print the output as follows: ``` [ Top 10 differences ] Filename:LineNumber : Increase in size in KiB, Increase in count of blocks ... ``` **Code Template:** You may use the following structure as a starting point: ```python import tracemalloc def sample_function(): # Simulate a memory-intensive operation data = [i ** 2 for i in range(10000)] return sum(data) def main(): # Step 1: Start tracing memory allocations tracemalloc.start() # Step 2: Take a snapshot before executing the function snapshot1 = tracemalloc.take_snapshot() # Step 3: Execute the function sample_function() # Step 4: Take another snapshot after the function execution snapshot2 = tracemalloc.take_snapshot() # Step 5: Compare both snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Step 6: Filter out unwanted traces top_stats = [stat for stat in top_stats if \'tracemalloc\' not in str(stat.traceback) and \'<frozen importlib._bootstrap>\' not in str(stat.traceback)] # Step 7: Output the top 10 differences print(\\"[ Top 10 differences ]\\") for stat in top_stats[:10]: print(f\\"{stat.traceback[0].filename}:{stat.traceback[0].lineno} : \\" f\\"{stat.size_diff / 1024:.1f} KiB, {stat.count_diff} blocks\\") if __name__ == \\"__main__\\": main() ``` **Constraints:** - Analyze only the `sample_function` provided. - Ensure that the printed results are clear and formatted correctly. - Consider memory and performance constraints while calling `sample_function`. --- **Note:** This task will test your understanding of memory management and the `tracemalloc` module, covering aspects such as starting and stopping memory tracing, taking memory snapshots, comparing snapshots, and filtering memory trace results.","solution":"import tracemalloc def sample_function(): # Simulate a memory-intensive operation data = [i ** 2 for i in range(10000)] return sum(data) def main(): # Step 1: Start tracing memory allocations tracemalloc.start() # Step 2: Take a snapshot before executing the function snapshot1 = tracemalloc.take_snapshot() # Step 3: Execute the function sample_function() # Step 4: Take another snapshot after the function execution snapshot2 = tracemalloc.take_snapshot() # Step 5: Compare both snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Step 6: Filter out unwanted traces top_stats = [stat for stat in top_stats if \'tracemalloc\' not in str(stat.traceback) and \'<frozen importlib._bootstrap>\' not in str(stat.traceback)] # Step 7: Output the top 10 differences print(\\"[ Top 10 differences ]\\") for stat in top_stats[:10]: print(f\\"{stat.traceback[0].filename}:{stat.traceback[0].lineno} : \\" f\\"{stat.size_diff / 1024:.1f} KiB, {stat.count_diff} blocks\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a Python extension module in C, named `calc`, that provides a function to compute the factorial of a non-negative integer. The function will be callable from Python using the signature: `calc.factorial(n)`, where `n` is a non-negative integer. The module should handle the following: 1. Raise a `TypeError` if the argument is not an integer. 2. Raise a `ValueError` if the argument is a negative integer. 3. Properly manage reference counts to ensure no memory leaks or use of freed memory. Your task is to implement the `factmodule.c` file that contains: 1. The factorial function implementation. 2. Error handling for invalid inputs. 3. A method table defining the name and address of the factorial function. 4. The module initialization function. Below is the structure of the file you need to implement: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> // Function to compute factorial static long compute_factorial(int n) { if (n == 0 || n == 1) { return 1; } return n * compute_factorial(n - 1); } // Wrapper function for the factorial function callable from Python static PyObject* calc_factorial(PyObject* self, PyObject* args) { int n; // Parse input arguments if (!PyArg_ParseTuple(args, \\"i\\", &n)) { // Argument is not an integer PyErr_SetString(PyExc_TypeError, \\"The argument must be an integer\\"); return NULL; } // Check for non-negative value if (n < 0) { PyErr_SetString(PyExc_ValueError, \\"The argument must be a non-negative integer\\"); return NULL; } // Compute factorial and return it as a Python long object long result = compute_factorial(n); return PyLong_FromLong(result); } // Method table static PyMethodDef CalcMethods[] = { {\\"factorial\\", calc_factorial, METH_VARARGS, \\"Compute the factorial of a non-negative integer.\\"}, {NULL, NULL, 0, NULL} // Sentinel }; // Module definition static struct PyModuleDef calcmodule = { PyModuleDef_HEAD_INIT, \\"calc\\", // name of module NULL, // module documentation, may be NULL -1, // size of per-interpreter state of the module CalcMethods // the method table }; // Module initialization function PyMODINIT_FUNC PyInit_calc(void) { return PyModule_Create(&calcmodule); } ``` # Instructions 1. Implement the `factmodule.c` file according to the structure provided above. 2. Ensure error handling for invalid input types and negative integers. 3. Manage the reference counts appropriately. 4. After implementing, compile and link the module to create a shared object file. 5. Test the module in Python by writing a script that imports `calc` and calls `calc.factorial(n)` with various test cases. # Compilation and Testing To compile and link the module: ```sh gcc -Wall -shared -o calc.so -fPIC factmodule.c -I/usr/include/python3.10 ``` To test the module in Python: ```python import calc print(calc.factorial(5)) # Expected output: 120 print(calc.factorial(0)) # Expected output: 1 print(calc.factorial(-1)) # Should raise a ValueError print(calc.factorial(1.5)) # Should raise a TypeError ``` Submit your `factmodule.c` file and a Python script that demonstrates the testing of the module.","solution":"def factorial(n): Returns the factorial of a non-negative integer n. Raises TypeError if n is not an integer. Raises ValueError if n is negative. if not isinstance(n, int): raise TypeError(\\"The argument must be an integer\\") if n < 0: raise ValueError(\\"The argument must be a non-negative integer\\") if n == 0 or n == 1: return 1 else: result = 1 for i in range(2, n + 1): result *= i return result"},{"question":"# EmailMessage Coding Task In this task, you will demonstrate your understanding of the `email.message.EmailMessage` class through the following steps: 1. **Create a Class Method to Compose an Email:** Implement a method named `compose_email` that takes the following arguments: - `subject` (a string): The subject of the email. - `sender` (a string): The email address of the sender. - `recipient` (a string): The email address of the recipient. - `body` (a string): The body content of the email. - `attachments` (a list of tuples): Each tuple consists of the filename (string) and file content (bytes). 2. **Handle Headers and Content:** - Use the `EmailMessage` class to create and set headers (`Subject`, `From`, `To`). - Set the `Content-Type` of the email to `text/plain` for the body. - Add each attachment to the email with the appropriate `Content-Disposition` header as `attachment`. 3. **Serialize the Message:** - Implement the `serialize_email` method that returns the serialized version of the email as a string. # Requirements: - Utilize the provided `EmailMessage` methods such as `add_header`, `set_content`, `attach`, and others as needed. - Ensure the email message adheres to proper email format standards. - Include exception handling to manage possible errors (e.g., missing required information). # Example Function Signatures: ```python from email.message import EmailMessage class EmailComposer: def compose_email(self, subject, sender, recipient, body, attachments): # Implementation here def serialize_email(self, email_message): # Implementation here ``` # Example Usage: ```python composer = EmailComposer() email_message = composer.compose_email( subject=\\"Project Update\\", sender=\\"user@example.com\\", recipient=\\"colleague@example.com\\", body=\\"Here is the update on the project...\\", attachments=[(\\"report.pdf\\", b\\"%PDF-1.4...\\")] ) serialized_string = composer.serialize_email(email_message) print(serialized_string) ``` # Constraints: - The subject, sender, and recipient are mandatory non-empty strings. - The body is a non-empty string. - If attachments are provided, each attachment\'s filename should be non-empty, and the content should be non-empty bytes. # Performance: - The function should be able to handle messages with up to 10 attachments efficiently. Implement this in Python using the `email.message.EmailMessage` class.","solution":"from email.message import EmailMessage import mimetypes class EmailComposer: def compose_email(self, subject, sender, recipient, body, attachments): # Validate the mandatory fields are non-empty if not all([subject, sender, recipient, body]): raise ValueError(\\"Subject, sender, recipient, and body are mandatory and must be non-empty.\\") # Create the email message msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg.set_content(body) # Add each attachment for filename, file_content in attachments: if not filename or not file_content: raise ValueError(\\"Each attachment must have a non-empty filename and non-empty content.\\") # Guess the mime type of the file mime_type, _ = mimetypes.guess_type(filename) mime_type = mime_type or \'application/octet-stream\' main_type, sub_type = mime_type.split(\'/\', 1) msg.add_attachment(file_content, maintype=main_type, subtype=sub_type, filename=filename) return msg def serialize_email(self, email_message): Serialize the email message to a string. if not isinstance(email_message, EmailMessage): raise TypeError(\\"Expected an EmailMessage object.\\") return email_message.as_string()"},{"question":"**Objective:** Design a function to demonstrate the comprehension of handling various asyncio exceptions. This question focuses on simulating realistic scenarios in asynchronous operations and managing the exceptions raised effectively. **Question:** You are required to implement an asynchronous function `fetch_data(urls: List[str], timeout: float) -> List[str]`. This function should attempt to fetch data from multiple URLs within a specified timeout and handle specific exceptions that may arise during the process. The function should adhere to the following guidelines: 1. Use the `aiohttp` package to perform asynchronous HTTP requests to the given URLs. 2. Handle the following `asyncio` exceptions: - `asyncio.TimeoutError`: When an operation exceeds the given deadline. - `asyncio.CancelledError`: When an operation is canceled. - `asyncio.InvalidStateError`: When there is an invalid internal state in a task or future. - `asyncio.IncompleteReadError`: When the requested read operation is not completed fully. **Function Specifications:** - **Function Name:** `fetch_data` - **Inputs:** - `urls`: A list of strings, where each string is a URL to fetch data from. - `timeout`: A float value representing the maximum time in seconds allowed for the entire operation. - **Outputs:** - A list of strings, each string containing the response data for the corresponding URL. - **Constraints:** - Use `asyncio.gather()` to run multiple requests concurrently. - If an exception is raised for a specific URL, handle it appropriately and append an error message to the result list for that URL. - The function should return a list with the same length as the input list `urls`. **Function Signature:** ```python from typing import List import aiohttp async def fetch_data(urls: List[str], timeout: float) -> List[str]: pass ``` **Performance Requirements:** - The function should handle up to 100 URLs with a timeout of 10 seconds efficiently. - Properly manage network and I/O-bound tasks to optimize performance. **Example Usage:** ```python urls = [\\"http://example.com\\", \\"http://example.org\\"] timeout = 5.0 result = await fetch_data(urls, timeout) print(result) # Output could be something like [\\"<html>...</html>\\", \\"TimeoutError: Operation exceeded the given deadline\\"] ``` **Notes:** - Ensure to install the `aiohttp` package before implementing the solution. - Each URL fetch operation should be wrapped in a try-except block to appropriately capture and handle the specified exceptions.","solution":"from typing import List import aiohttp import asyncio async def fetch_url(session, url, timeout): try: async with session.get(url, timeout=timeout) as response: return await response.text() except asyncio.TimeoutError: return f\\"TimeoutError: Operation exceeded the given deadline\\" except asyncio.CancelledError: return f\\"CancelledError: Operation was cancelled\\" except asyncio.InvalidStateError: return f\\"InvalidStateError: Invalid internal state in task or future\\" except asyncio.IncompleteReadError: return f\\"IncompleteReadError: Read operation not completed fully\\" except Exception as e: return f\\"Error: {str(e)}\\" async def fetch_data(urls: List[str], timeout: float) -> List[str]: async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url, timeout) for url in urls] return await asyncio.gather(*tasks)"},{"question":"# Seaborn Heatmap Customization Assessment You are given a dataset `tips` loaded from Seaborn\'s datasets which contains information about restaurant bills. Your task is to create multiple heatmaps to analyze various aspects of this dataset. # Dataset Description The `tips` dataset includes the following columns: - **total_bill**: Total bill amount (numeric) - **tip**: Tip amount (numeric) - **sex**: Gender of the payer (Male/Female) - **smoker**: Whether the payer is a smoker (Yes/No) - **day**: Day of the week (Thur/Fri/Sat/Sun) - **time**: Time of day (Lunch/Dinner) - **size**: Size of the party (numeric) # Tasks 1. **Load the dataset and pivot**: Load the `tips` dataset and create a pivot table showing the average `total_bill` for each combination of `day` and `time`. 2. **Generate a basic heatmap**: Using the pivot table from Task 1, create a basic heatmap to visualize the average `total_bill`. 3. **Heatmap with annotations**: Enhance the heatmap by adding annotations that show the average `total_bill` in each cell, formatted to 1 decimal place. 4. **Customized colormap and linewidth**: Apply a different colormap of your choice and set the linewidth between cells to 0.5. 5. **Normalized colormap range**: Adjust the colormap of the heatmap range to be between 10 and 30. 6. **Rank annotation**: Create a new heatmap using the pivot table, but this time, annotate each cell with the rank of the average total bill within each column. 7. **Axes adjustment**: Further customize the final heatmap by modifying the axes labels and moving the ticks to the top. # Constraints * You can only use Pandas for data manipulation and Seaborn for plotting. * The pivot table must retain the appropriate format for generating the heatmaps (indexes as row labels and columns as column labels). # Expected Input and Output **Input**: - Loading dataset from Seaborn directly. **Output**: - Multiple Seaborn heatmaps as described in the tasks. # Example Code Here’s a brief skeleton to get you started: ```python import seaborn as sns import pandas as pd # 1. Load the dataset and create a pivot table tips = sns.load_dataset(\\"tips\\") pivot_table = tips.pivot_table(values=\\"total_bill\\", index=\\"day\\", columns=\\"time\\", aggfunc=\\"mean\\") # 2. Basic heatmap sns.heatmap(pivot_table) # 3. Heatmap with annotations sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\") # 4. Customized colormap and linewidth sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=0.5) # 5. Normalized colormap range sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=0.5, vmin=10, vmax=30) # 6. Rank annotation sns.heatmap(pivot_table, annot=pivot_table.rank(axis=\\"columns\\")) # 7. Axes adjustment ax = sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=0.5, vmin=10, vmax=30) ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_pivot_data(): Load the \'tips\' dataset and create a pivot table showing the average total_bill for each combination of day and time. tips = sns.load_dataset(\\"tips\\") pivot_table = tips.pivot_table(values=\\"total_bill\\", index=\\"day\\", columns=\\"time\\", aggfunc=\\"mean\\") return pivot_table def basic_heatmap(pivot_table): Generate a basic heatmap to visualize the average total_bill. plt.figure() sns.heatmap(pivot_table) plt.show() def heatmap_with_annotations(pivot_table): Enhance the heatmap by adding annotations that show the average total_bill in each cell, formatted to 1 decimal place. plt.figure() sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\") plt.show() def customized_heatmap(pivot_table, cmap=\\"coolwarm\\", linewidths=0.5): Apply a different colormap and set the linewidth between cells. plt.figure() sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=cmap, linewidths=linewidths) plt.show() def normalized_heatmap(pivot_table, vmin=10, vmax=30): Adjust the colormap range between 10 and 30. plt.figure() sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=0.5, vmin=vmin, vmax=vmax) plt.show() def rank_annotation_heatmap(pivot_table): Annotate each cell with the rank of the average total bill within each column. plt.figure() sns.heatmap(pivot_table, annot=pivot_table.rank(axis=\\"columns\\")) plt.show() def axes_adjustment_heatmap(pivot_table): Further customize the heatmap by modifying the axes labels and moving the ticks to the top. plt.figure() ax = sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=0.5, vmin=10, vmax=30) ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() plt.show()"},{"question":"You have been provided with a brief snippet about the use of seaborn\'s `Plot` class from the `seaborn.objects` module. Based on this information, please complete the following task: # Task Write a Python function `create_custom_plot` using seaborn that: 1. Takes three arguments: - `x`: a list of x-coordinates, - `y`: a list of y-coordinates, - `limits`: a dictionary including two tuples, `x_range` and `y_range`, representing the range of the x and y axis respectively. Defaults to `None`. 2. Uses seaborn `Plot` object to plot the data points `(x, y)` as a line plot where each point is marked with an \'o\'. 3. Sets the axis limits according to `limits`. If `limits` is `None`, use seaborn\'s default limit setting. 4. Returns the created seaborn `Plot` object. # Example Usage: ```python x_coords = [1, 2, 3] y_coords = [1, 3, 2] limits = {\'x_range\': (0, 4), \'y_range\': (-1, 6)} plot = create_custom_plot(x_coords, y_coords, limits) ``` # Constraints: - `x` and `y` will always be lists of integers or floats of the same length. - `limits` if provided, will always contain valid `x_range` and `y_range`. # Notes: - Make sure to handle the case where `limits` are not provided (or are `None`) by using seaborn\'s default limit settings. - Test your function to ensure it correctly applies the axis limits if provided, and represents the data as expected. # Performance Requirements: - Ensure the function executes efficiently with a reasonable time complexity for typical input sizes. # Expected Output: The function should return a seaborn `Plot` object that can be further used for rendering or manipulation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x, y, limits=None): Create a custom seaborn line plot with specified x and y axis limits. Parameters: - x: list of x-coordinates - y: list of y-coordinates - limits: dictionary with \'x_range\' and \'y_range\', or None Returns: - sns_plot: seaborn Plot object # Create a seaborn Plot object sns_plot = sns.relplot(x=x, y=y, kind=\\"line\\", marker=\\"o\\") # Apply axis limits if provided if limits: if \'x_range\' in limits: plt.xlim(limits[\'x_range\']) if \'y_range\' in limits: plt.ylim(limits[\'y_range\']) return sns_plot"},{"question":"# Seaborn Coding Assessment Question Objective Demonstrate your ability to create and customize plots using the `seaborn.objects` module. Task You are required to create a series of plots using the `anscombe` dataset provided by seaborn. Then, apply different customizations to the plot themes. Dataset - Use the `anscombe` dataset available in seaborn. Instructions 1. Load the `anscombe` dataset. 2. Create a `so.Plot` object with: - `x` as the x-axis variable. - `y` as the y-axis variable. - Different colors for each `dataset`. 3. Add a linear regression line (`PolyFit` with order 1). 4. Include a scatter plot (`Dot`). 5. Modify the plot theme to have: - White background for axes (`axes.facecolor` should be \\"white\\"). - Slategray color for axis edges (`axes.edgecolor` should be \\"slategray\\"). - Line width of 4 for plot lines. 6. Apply the `ticks` style using `seaborn` for the plot theme. 7. Save the plot as `anscombe_custom_plot.png`. Your final plot should include: - A facet grid for each `dataset`. - Properly customized theme settings. - Different colors for datasets. - Linear regression lines and scatter plots. Implementation ```python import seaborn.objects as so from seaborn import load_dataset, axes_style # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the base plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Modify the theme settings p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) # Apply seaborn ticks style p.theme(axes_style(\\"ticks\\")) # Save the plot p.save(\\"anscombe_custom_plot.png\\") ``` Submission Submit the Python script that meets the above requirements along with the generated `anscombe_custom_plot.png` file.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style def create_custom_anscombe_plot(output_file): Creates a customized plot for the Anscombe dataset and saves it as an image file. Parameters: output_file (str): The path of the file to save the plot. # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the base plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Modify the theme settings p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) # Apply seaborn ticks style p.theme(axes_style(\\"ticks\\")) # Save the plot p.save(output_file)"},{"question":"As a data scientist at a retail company, you are working with a dataset that contains information about transactions made over a period of time. Your task is to visualize the transaction data using Seaborn to gain insights into the patterns of spending and tipping behavior. **Dataset:** - Use the \\"tips\\" dataset included within seaborn. **Requirements:** 1. Create a scatter plot to visualize the relationship between `total_bill` and `tip`. 2. Overlay a rug plot along both axes to provide additional detail. 3. Use hue mapping to add a third dimension to your plot by differentiating between `time` (Lunch/Dinner). 4. Customize the rug plot: - Set the height of the rug plot to `0.1`. - Position the rugs outside the axes with a height offset of `-0.02` (specifying `clip_on=False`). - Adjust the rug plot\'s line width to `1` and set the alpha blending to `0.5`. Your function should **display** the plot, not return any values. **Function Signature:** ```python def visualize_tips_data(): pass ``` # Constraints - You are not allowed to use any other visualization library apart from seaborn and matplotlib. - The code should be self-contained and reproducible using only the \\"tips\\" dataset from seaborn. # Example: ```python def visualize_tips_data(): import seaborn as sns sns.set_theme() tips = sns.load_dataset(\\"tips\\") # Create scatter plot sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Overlay rug plot with customizations sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=0.1, clip_on=False, lw=1, alpha=0.5) # Call the function to visualize the plot visualize_tips_data() ``` # Performance Requirements: - The function should execute within reasonable time limits for the given dataset size.","solution":"def visualize_tips_data(): import seaborn as sns import matplotlib.pyplot as plt # Set Seaborn theme sns.set_theme() # Load the \\"tips\\" dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Create scatter plot ax = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Overlay rug plot with customizations sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=0.1, clip_on=False, lw=1, alpha=0.5) # Display the plot plt.show()"},{"question":"**Objective**: Test the students\' ability to create and manage Python virtual environments and handle package dependencies. Question You are asked to set up a Python project that uses a virtual environment and specific package versions. Your task is to create a script that accomplishes the following steps: 1. **Create a Virtual Environment**: - Create a virtual environment named `myenv` in the current directory. 2. **Activate the Virtual Environment**: - Automatically activate the created virtual environment within the script. 3. **Install Packages**: - Install the following packages with the specified versions: - `requests` version `2.23.0` - `numpy` version `1.18.1` 4. **Verify the Installation**: - Print the list of installed packages along with their versions. 5. **Output Package Information**: - For the package `requests`, display detailed information including its name, version, and summary. Requirements - The script should be executable on Unix-based systems (Linux/MacOS). - You need to handle all necessary imports within the script. - Ensure that the script will run correctly even if it is executed multiple times, handling any existing virtual environment or installed packages. Constraints - You should not use any external tools or scripts outside of Python\'s standard library and `pip`. - The virtual environment directory (`myenv`) should not be visible using normal directory listing commands. Input and Output Format - **Input**: The script does not require any input from the user. - **Output**: The script should output the list of installed packages with their versions and detailed information for the `requests` package. Example Output ``` Installed packages: requests==2.23.0 numpy==1.18.1 Package Information for `requests`: Name: requests Version: 2.23.0 Summary: Python HTTP for Humans. Home-page: http://python-requests.org Author: Kenneth Reitz Author-email: me@kennethreitz.com License: Apache 2.0 Location: /absolute/path/to/myenv/lib/python3.8/site-packages Requires: ``` Note - Make sure your script handles the creation and activation of the virtual environment programmatically and does not require manual intervention. Good luck!","solution":"import os import subprocess import sys import pkg_resources def create_virtual_env(env_name=\'myenv\'): # Create Virtual Environment subprocess.run([sys.executable, \'-m\', \'venv\', env_name], check=True) def activate_virtual_env(env_name=\'myenv\'): # Enable the virtual environment activate_script = os.path.join(env_name, \'bin\', \'activate\') command = f\\". {activate_script} && {subprocess.list2cmdline(sys.argv)}\\" subprocess.run(command, shell=True, executable=\\"/bin/bash\\", check=True) def install_packages(env_name=\'myenv\', packages=None): if packages is None: packages = [\'requests==2.23.0\', \'numpy==1.18.1\'] # Install specific packages for package in packages: subprocess.run([os.path.join(env_name, \'bin\', \'pip\'), \'install\', package], check=True) def list_installed_packages(env_name=\'myenv\'): # List installed packages exec_path = os.path.join(env_name, \'bin\', \'pip\') result = subprocess.run([exec_path, \'freeze\'], check=True, text=True, capture_output=True) installed_packages = result.stdout.strip().split(\'n\') print(\\"Installed packages:\\") for package in installed_packages: print(package) return installed_packages def display_package_info(package_name, env_name=\'myenv\'): # Get package information for `requests` exec_path = os.path.join(env_name, \'bin\', \'pip\') result = subprocess.run([exec_path, \'show\', package_name], check=True, text=True, capture_output=True) package_info = result.stdout.strip().split(\'n\') print(f\\"nPackage Information for `{package_name}`:\\") for info in package_info: print(info) def main(): env_name = \'myenv\' # Create virtual environment if it does not exist if not os.path.exists(env_name): create_virtual_env(env_name) activate_virtual_env(env_name) packages = [\'requests==2.23.0\', \'numpy==1.18.1\'] install_packages(env_name, packages) list_installed_packages(env_name) display_package_info(\'requests\', env_name) if __name__ == \\"__main__\\": main()"},{"question":"**XML Parsing and Handling with `xml.parsers.expat`** # Problem Statement: You are provided with a string representing an XML document. Your task is to write a function that parses this XML document, captures specific elements, and extracts data based on given conditions. Specifically, you need to: 1. Count the number of occurrences of a specific element. 2. Extract all distinct values of a particular attribute within those elements. 3. Aggregate character data found within these elements. # Requirements: - Use the `xml.parsers.expat` module for parsing the XML. - Implement the following function: ```python def parse_xml_and_extract(xml_string: str, target_element: str, target_attribute: str) -> dict: Parses the given XML string and extracts information based on specified element and attribute. Args: xml_string (str): The XML document as a string. target_element (str): The element whose occurrences need to be counted and processed. target_attribute (str): The attribute whose distinct values need to be extracted. Returns: dict: A dictionary with the following structure: { \\"count\\": int, # Number of target_element occurrences \\"attributes\\": set, # Set of unique attribute values \\"data\\": str # Concatenated character data within target elements } ``` # Input: - `xml_string` (str): A valid XML document string. - `target_element` (str): The name of the XML element to be processed. - `target_attribute` (str): The name of the attribute within the target elements. # Output: - A dictionary containing: - `count` (int): Number of times `target_element` occurs in the XML. - `attributes` (set): Set of distinct values for `target_attribute` within `target_element`. - `data` (str): Concatenated character data found within `target_element`. # Example: ```python xml_data = <root> <item category=\\"fruit\\">Apple</item> <item category=\\"fruit\\">Banana</item> <item category=\\"vegetable\\">Carrot</item> <item category=\\"vegetable\\">Peas</item> </root> result = parse_xml_and_extract(xml_data, \\"item\\", \\"category\\") print(result) # Expected output: {\'count\': 4, \'attributes\': {\'fruit\', \'vegetable\'}, \'data\': \'AppleBananaCarrotPeas\'} ``` # Constraints: - The XML string will be well-formed. - The `target_element` and `target_attribute` will always be provided and valid within the context of the XML. **Note:** Ensure proper error handling for XML parsing issues, and make efficient use of handlers provided by the `xml.parsers.expat` module to achieve the desired result.","solution":"import xml.parsers.expat def parse_xml_and_extract(xml_string: str, target_element: str, target_attribute: str) -> dict: Parses the given XML string and extracts information based on the specified element and attribute. Args: xml_string (str): The XML document as a string. target_element (str): The element whose occurrences need to be counted and processed. target_attribute (str): The attribute whose distinct values need to be extracted. Returns: dict: A dictionary with the following structure: { \\"count\\": int, # Number of target_element occurrences \\"attributes\\": set, # Set of unique attribute values \\"data\\": str # Concatenated character data within target elements } # Initialize variables count = 0 attributes = set() data = [] in_target_element = False # Define the handler functions def start_element(name, attrs): nonlocal count, in_target_element if name == target_element: count += 1 in_target_element = True if target_attribute in attrs: attributes.add(attrs[target_attribute]) def end_element(name): nonlocal in_target_element if name == target_element: in_target_element = False def char_data(content): if in_target_element: data.append(content) # Create a parser object and assign the handler functions parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data # Parse the XML string try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError as e: raise ValueError(\\"Invalid XML provided\\") from e return { \\"count\\": count, \\"attributes\\": attributes, \\"data\\": \'\'.join(data) }"},{"question":"# Challenge: Enhanced Data Visualization with Seaborn **Objective:** Create a detailed data visualization using the `seaborn` library. You will use the `penguins` dataset to demonstrate your understanding of seaborn\'s plotting capabilities and customization options. **Task:** 1. **Data Preparation:** - Load the `penguins` dataset using seaborn\'s `load_dataset` function. - Handle any missing data by dropping rows with `NaN` values (hint: `dropna()`). 2. **Visualization:** - Create a multi-faceted histogram (using `FacetGrid`) that displays the distribution of `bill_length_mm` across different species. - Each histogram should reflect different islands. - Add an overall title for the figure. 3. **Legend Customization:** - For each facet, place the legend at the top center of each subplot. - Do not set the legend outside the figure boundaries to ensure all legends are visible. 4. **Additional Customization:** - Use the `sns.set_theme()` function to apply a theme of your choice. - Customize additional plot elements for readability and aesthetics, such as: - Adding axis labels. - Adjusting tick parameters. - Ensuring the main title is properly formatted. **Constraints:** - Ensure all plots are clearly readable and legends are appropriately placed. - Utilization of at least two different seaborn customization techniques for better representation and clarity. **Input and Output Formats:** *Input*: - No input required, the dataset `penguins` will be directly loaded within the script. *Output*: - A figure containing multiple histograms faceted by island with properly placed and customized legends. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins_data(): # Load and pre-process the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Set theme sns.set_theme(style=\\"whitegrid\\") # Create a FacetGrid for the histogram g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, facet_kws={\'legend_out\': False} ) # Customize grid and move legends g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Bill Length (mm)\\", \\"Count\\") sns.move_legend(g, \\"upper center\\", bbox_to_anchor=(0.5, 1.1), ncol=3, frameon=True) # Add overall title plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Bill Length Distribution of Penguins by Island\') # Show plot plt.show() visualize_penguins_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins_data(): # Load and pre-process the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Set theme sns.set_theme(style=\\"whitegrid\\") # Create a FacetGrid for the histogram g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, facet_kws={\'legend_out\': False} ) # Customize grid and move legends g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Bill Length (mm)\\", \\"Count\\") sns.move_legend(g, \\"upper center\\", bbox_to_anchor=(0.5, 1.05), ncol=1, frameon=True) # Add overall title plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Bill Length Distribution of Penguins by Island\') # Show plot plt.show() visualize_penguins_data()"},{"question":"**Coding Assessment Question** # Objective The goal of this assessment is to evaluate your ability to work with different data structures in seaborn, transform data between long-form and wide-form, and create complex visualizations. # Problem Statement You are given a dataset that records the number of daily website visits for a set of pages over a period of time. This dataset is in wide-form, where each column represents a different page, and each row represents the number of visits on a particular day. You need to: 1. Transform the data from wide-form to long-form. 2. Create visualizations using seaborn to represent trends in the data. 3. Annotate the plots with correct axis labels and legend information. # Input - A wide-form dataset as a pandas DataFrame, where each column represents a page and each row represents the number of visits on a particular day. - The DataFrame has a DateTime index representing the date of each record. # Output - A long-form pandas DataFrame. - Two seaborn plots: 1. A line plot showing the number of visits to each page over time. 2. A bar plot showing the average number of visits per page. # Constraints - The dataset will have at least 100 rows and 5 columns. - The DateTime index will cover a range of several months. # Function Signature ```python import pandas as pd import seaborn as sns def visualize_web_visits(data: pd.DataFrame): Transform the given wide-form dataset to long-form and create visualizations. Args: data (pd.DataFrame): Wide-form DataFrame with DateTime index and columns representing pages. Returns: tuple: A tuple containing the long-form DataFrame and the two seaborn plots. # Transform the data to long-form long_form_data = pd.melt(data.reset_index(), id_vars=[\'index\'], var_name=\'page\', value_name=\'visits\') long_form_data.rename(columns={\'index\': \'date\'}, inplace=True) # Create a line plot of the number of visits to each page over time line_plot = sns.relplot(data=long_form_data, x=\'date\', y=\'visits\', hue=\'page\', kind=\'line\') line_plot.set_axis_labels(\\"Date\\", \\"Number of Visits\\") line_plot.legend.set_title(\\"Page\\") # Create a bar plot showing the average number of visits per page avg_visits = long_form_data.groupby(\'page\')[\'visits\'].mean().reset_index() bar_plot = sns.catplot(data=avg_visits, x=\'page\', y=\'visits\', kind=\'bar\') bar_plot.set_axis_labels(\\"Page\\", \\"Average Number of Visits\\") return long_form_data, line_plot, bar_plot ``` # Example Given the following wide-form DataFrame: ```python import pandas as pd import numpy as np dates = pd.date_range(start=\'2021-01-01\', periods=100) data = pd.DataFrame(np.random.randint(1, 100, size=(100, 5)), index=dates, columns=[\'Page_A\', \'Page_B\', \'Page_C\', \'Page_D\', \'Page_E\']) ``` Executing the function: ```python long_form_data, line_plot, bar_plot = visualize_web_visits(data) ``` Would output: 1. A long-form DataFrame with columns: `date`, `page`, and `visits`. 2. A seaborn line plot showing the number of visits to each page over time. 3. A seaborn bar plot showing the average number of visits per page.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_web_visits(data: pd.DataFrame): Transform the given wide-form dataset to long-form and create visualizations. Args: data (pd.DataFrame): Wide-form DataFrame with DateTime index and columns representing pages. Returns: tuple: A tuple containing the long-form DataFrame and the two seaborn plots. # Transform the data to long-form long_form_data = pd.melt(data.reset_index(), id_vars=[\'index\'], var_name=\'page\', value_name=\'visits\') long_form_data.rename(columns={\'index\': \'date\'}, inplace=True) # Create a line plot of the number of visits to each page over time line_plot = sns.relplot(data=long_form_data, x=\'date\', y=\'visits\', hue=\'page\', kind=\'line\', aspect=2) line_plot.set_axis_labels(\\"Date\\", \\"Number of Visits\\") line_plot._legend.set_title(\\"Page\\") # Create a bar plot showing the average number of visits per page avg_visits = long_form_data.groupby(\'page\')[\'visits\'].mean().reset_index() bar_plot = sns.catplot(data=avg_visits, x=\'page\', y=\'visits\', kind=\'bar\', aspect=1.5) bar_plot.set_axis_labels(\\"Page\\", \\"Average Number of Visits\\") # Return the long-form DataFrame and the handles to the plots return long_form_data, line_plot, bar_plot"},{"question":"**Coding Assessment Question** # Problem: Terminal Control and Resource Management Using `termios` and `resource` In this exercise, you are required to demonstrate your understanding of terminal control and resource management in a Unix environment using Python. You will implement two functions: 1. `configure_terminal()`: This function will modify the terminal settings such that it no longer echoes characters typed by the user (this is commonly used for password input). 2. `resource_usage_report()`: This function will report the current resource usage for the calling process. # Function 1: `configure_terminal` Input - This function takes no input. Output - The function should not return any value, but it will affect the terminal settings. Details - Use the `termios` module to disable the echo feature of the terminal. - Ensure you restore the original settings when the function exits. # Function 2: `resource_usage_report` Input - This function takes no input. Output - This function will return a dictionary containing the following keys: - `user_time`: The amount of CPU time spent in user mode. - `system_time`: The amount of CPU time spent in system mode. - `max_rss`: The maximum resident set size used. - `shared_mem_size`: The amount of shared memory used. - `unshared_mem_size`: The amount of unshared memory used. Details - Use the `resource` module to get the current resource usage of the calling process. - Collect and return the relevant information in a dictionary format. # Example Usage ```python def configure_terminal(): import sys, tty, termios fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: tty.setraw(fd) termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) except: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) def resource_usage_report(): import resource usage = resource.getrusage(resource.RUSAGE_SELF) return { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_rss\': usage.ru_maxrss, \'shared_mem_size\': usage.ru_ixrss, \'unshared_mem_size\': usage.ru_idrss } # Testing configure_terminal() configure_terminal() # Testing resource_usage_report() report = resource_usage_report() print(report) ``` # Constraints - You should handle exceptions gracefully. - Document your code properly. Ensure that your implementation demonstrates a good understanding of Unix-specific system calls and resource management.","solution":"import termios, sys, resource def configure_terminal(): Configures the terminal to disable echoing of characters. Restores the terminal settings when finished. fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: new_settings = termios.tcgetattr(fd) new_settings[3] = new_settings[3] & ~termios.ECHO termios.tcsetattr(fd, termios.TCSADRAIN, new_settings) input(\\"Terminal configured. Press Enter to continue...\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) def resource_usage_report(): Returns the current resource usage of the calling process. :return: a dictionary with keys \'user_time\', \'system_time\', \'max_rss\', \'shared_mem_size\', and \'unshared_mem_size\'. usage = resource.getrusage(resource.RUSAGE_SELF) return { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_rss\': usage.ru_maxrss, \'shared_mem_size\': usage.ru_ixrss, \'unshared_mem_size\': usage.ru_idrss }"},{"question":"<|Analysis Begin|> The provided documentation is an excerpt from \\"The Python Tutorial\\" which covers various fundamental and some advanced topics in Python. It includes sections on basic data types (numbers, strings, lists), control flow (if, for, while, match), function definitions, data structures (lists, tuples, sets, dictionaries), modules, input and output, exceptions, classes, iterators, generators, and a brief tour of the Python Standard Library. Given that the purpose is to design a challenging coding question to assess the students\' understanding of Python concepts, it is important to include various aspects like data structures, control flow, functions, and possibly classes or modules. The question should require students to combine these elements to demonstrate their comprehension and ability to apply what they have learned. We\'ll design a question that involves: - Implementing a complex function. - Utilizing different data structures. - Applying control flow mechanisms. - Using functions, generators, and possibly classes. <|Analysis End|> <|Question Begin|> # Coding Assessment Task **Title: Movie Theater Reservation System** Develop a Python program to manage a movie theater\'s reservation system. The theater has multiple screens, and each screen has a fixed number of rows and seats per row. Objectives: 1. Implement a class `Theater` to manage the overall reservation system. 2. Implement a class `Screen` to manage individual screens within the theater. `Theater` Class: - **Attributes**: - `screens`: A dictionary to store `Screen` objects with screen names as keys. - **Methods**: - `add_screen(name: str, rows: int, seats_per_row: int)`: Adds a new screen to the theater. - `reserve_seat(screen_name: str, row: int, seat: int) -> str`: Reserves a specific seat on the given screen. - `cancel_reservation(screen_name: str, row: int, seat: int) -> str`: Cancels a reservation for a specific seat on the given screen. - `check_availability(screen_name: str) -> List[Tuple[int, List[int]]]`: Returns a list of rows and available seats for each row for the given screen. `Screen` Class: - **Attributes**: - `name`: The name of the screen. - `rows`: Number of rows in the screen. - `seats_per_row`: Number of seats per row in the screen. - `seating_chart`: A dictionary to keep track of reserved and available seats. (key: row number, value: list of seat availability status). - **Methods**: - `reserve_seat(row: int, seat: int) -> str`: Reserves a specific seat. - `cancel_reservation(row: int, seat: int) -> str`: Cancels a reservation for a specific seat. - `get_available_seats() -> List[Tuple[int, List[int]]]`: Returns a list of rows and the available seats in each row. Expected Input and Output: - Input: Sequence of method calls to simulate adding screens, making reservations, canceling reservations, and checking availability. - Output: String messages confirming reservation status, cancellation status, and seat availability status. Constraints: - The seat numbers and row numbers are 1-indexed. - Performance should be optimized for large theaters with multiple screens. Example Usage: ```python theater = Theater() theater.add_screen(\\"Screen1\\", 5, 10) print(theater.reserve_seat(\\"Screen1\\", 2, 5)) # Output: \\"Seat 2-5 on Screen1 reserved successfully.\\" print(theater.cancel_reservation(\\"Screen1\\", 2, 5)) # Output: \\"Reservation for seat 2-5 on Screen1 canceled.\\" print(theater.check_availability(\\"Screen1\\")) # Output: [(1, [1, 2, ..., 10]), (2, [1, 2, ..., 10]), ..., (5, [1, 2, ..., 10])] ``` By completing this task, students will showcase their understanding of class design, data structure utilization, and control flow management in Python.","solution":"from typing import List, Tuple, Dict class Screen: def __init__(self, name: str, rows: int, seats_per_row: int): self.name = name self.rows = rows self.seats_per_row = seats_per_row self.seating_chart = {row: [True] * seats_per_row for row in range(1, rows + 1)} def reserve_seat(self, row: int, seat: int) -> str: if self.seating_chart[row][seat - 1]: self.seating_chart[row][seat - 1] = False return f\\"Seat {row}-{seat} on {self.name} reserved successfully.\\" else: return f\\"Seat {row}-{seat} on {self.name} is already reserved.\\" def cancel_reservation(self, row: int, seat: int) -> str: if not self.seating_chart[row][seat - 1]: self.seating_chart[row][seat - 1] = True return f\\"Reservation for seat {row}-{seat} on {self.name} canceled.\\" else: return f\\"No reservation found for seat {row}-{seat} on {self.name}.\\" def get_available_seats(self) -> List[Tuple[int, List[int]]]: available_seats = [] for row, seats in self.seating_chart.items(): available = [i + 1 for i, available in enumerate(seats) if available] available_seats.append((row, available)) return available_seats class Theater: def __init__(self): self.screens: Dict[str, Screen] = {} def add_screen(self, name: str, rows: int, seats_per_row: int): if name not in self.screens: self.screens[name] = Screen(name, rows, seats_per_row) def reserve_seat(self, screen_name: str, row: int, seat: int) -> str: if screen_name in self.screens: return self.screens[screen_name].reserve_seat(row, seat) else: return f\\"Screen {screen_name} does not exist.\\" def cancel_reservation(self, screen_name: str, row: int, seat: int) -> str: if screen_name in self.screens: return self.screens[screen_name].cancel_reservation(row, seat) else: return f\\"Screen {screen_name} does not exist.\\" def check_availability(self, screen_name: str) -> List[Tuple[int, List[int]]]: if screen_name in self.screens: return self.screens[screen_name].get_available_seats() else: return f\\"Screen {screen_name} does not exist.\\""},{"question":"# Parallel Data Processing with `concurrent.futures` You have been provided with a large dataset consisting of URLs. Each URL points to a file that needs to be downloaded, processed, and summarized. Processing each file involves reading its contents and calculating the number of words. You need to design a Python function using the `concurrent.futures` module to perform these tasks in parallel, aiming to minimize the overall execution time. Function Signature ```python def parallel_url_processing(urls: List[str]) -> Dict[str, int]: ``` Parameters - `urls` (List[str]): A list of URLs (strings) pointing to text files that need to be downloaded and processed. Returns - Dict[str, int]: A dictionary where the keys are the URLs and the values are the corresponding word counts for each downloaded and processed file. Constraints 1. You should use the `ThreadPoolExecutor` from the `concurrent.futures` module to handle downloading and processing tasks concurrently. 2. Implement proper error handling to gracefully manage scenarios where a URL might be unreachable or the download fails. 3. Ensure the solution is efficient and can handle up to 1000 URLs robustly. Example Here’s an example of the function usage: ```python urls = [ \\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", ... ] result = parallel_url_processing(urls) print(result) # Output might be something like: # { # \\"http://example.com/file1.txt\\": 543, # \\"http://example.com/file2.txt\\": 1234, # ... # } ``` Note You do not need to actually perform network requests in this question. You can simulate the download operation and focus on the parallel processing and word count logic. Use the following stub for the download function in your solution: ```python import random import time def download_url(url: str) -> str: # Simulate network delay time.sleep(random.uniform(0.1, 0.3)) # Simulate file content return \\"This is the content of the file at \\" + url ``` Your implementation should integrate `download_url` and process the simulated file content to count the words. Make sure to handle concurrent execution and error handling as required.","solution":"import concurrent.futures from typing import List, Dict import random import time def download_url(url: str) -> str: # Simulate network delay time.sleep(random.uniform(0.1, 0.3)) # Simulate file content return \\"This is the content of the file at \\" + url def count_words(content: str) -> int: Given the content of a file as a string, return the number of words. return len(content.split()) def process_url(url: str) -> (str, int): Downloads the content from a URL and counts the number of words. try: content = download_url(url) word_count = count_words(content) return url, word_count except Exception as e: return url, -1 # or some appropriate error value def parallel_url_processing(urls: List[str]) -> Dict[str, int]: Downloads and processes multiple URLs in parallel to count words in each file. result = {} with concurrent.futures.ThreadPoolExecutor() as executor: future_to_url = {executor.submit(process_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: url, word_count = future.result() result[url] = word_count except Exception as e: result[url] = -1 # or some appropriate error value return result"},{"question":"# Question Title: **Fetching and Parsing Web Content** # Objective: Implement a Python function that fetches web content from a given URL and performs specific parsing tasks on the retrieved HTML content using the `urllib` module. # Problem Statement: You are required to write a function named `fetch_and_parse(url: str) -> dict` that performs the following tasks: 1. Fetches the HTML content of the given `url` using the `urllib` module. 2. Parses the HTML content to extract all unique hyperlinks (URLs) present in the page. 3. Parses the HTML content to extract the content of all `<h1>` tags. The function should return a dictionary with two keys: - `\\"urls\\"`: a list of all unique URLs found in the HTML content. - `\\"h1_texts\\"`: a list of text content of all `<h1>` tags found in the HTML content. # Input: - `url` (str): A string representing the URL of the web page to fetch. # Output: - A dictionary with the keys `\\"urls\\"` and `\\"h1_texts\\"` as described above. # Constraints: - Use only the `urllib` module for making the HTTP request and parsing the URL. Do not use other libraries like BeautifulSoup, requests, etc. - Assume the URL will always return a valid HTML content. - The function needs to handle potential HTTP errors gracefully. # Example: ```python def fetch_and_parse(url: str) -> dict: # Your implementation here # Example Usage: url = \\"http://example.com\\" result = fetch_and_parse(url) print(result) ``` Given a URL such as \\"http://example.com\\", the expected output could be: ```python { \\"urls\\": [\\"http://example.org\\", \\"http://example.net\\"], \\"h1_texts\\": [\\"Example Domain\\"] } ``` # Notes: - You may find the `urllib.parse` module handy for parsing URLs. - The use of regular expressions might be necessary to extract tags and attributes correctly. - Make sure to handle cases where the URL fetching might fail due to network issues or invalid URLs by catching exceptions and returning an appropriate error message. # Performance Considerations: - Ensure that the function operates within reasonable time limits, such as completing within a few seconds for URLs with moderately sized HTML content.","solution":"import urllib.request import urllib.error import re from urllib.parse import urljoin def fetch_and_parse(url: str) -> dict: try: response = urllib.request.urlopen(url) html_content = response.read().decode(\\"utf-8\\") except urllib.error.URLError as e: return {\\"error\\": str(e)} # Regular expressions to extract URLs and H1 tags url_pattern = re.compile(r\'<as+(?:[^>]*?s+)?href=[\\"\'](.*?)[\\"\']\') h1_pattern = re.compile(r\'<h1>(.*?)</h1>\') # Find all URLs in the HTML content urls = set() for match in url_pattern.findall(html_content): absolute_url = urljoin(url, match) urls.add(absolute_url) # Find all H1 tag contents h1_texts = h1_pattern.findall(html_content) return { \\"urls\\": list(urls), \\"h1_texts\\": h1_texts }"},{"question":"# Advanced XML Processing and Manipulation **Objective:** Using the `xml.etree.ElementTree` and `xml.dom.minidom` modules, write a Python program that reads an XML file, manipulates its content, and outputs a modified XML. The goal is to demonstrate your ability to parse, modify, and serialize XML documents using Python. **Task Description:** Write a function `modify_xml(input_file: str, output_file: str, tag_to_modify: str, new_text: str) -> None` that: 1. Reads an XML file from `input_file`. 2. Searches for the first occurrence of a specific tag (`tag_to_modify`). 3. Changes the text content of the found tag to `new_text`. 4. Writes the modified XML to `output_file` with proper indentation and formatting. **Input:** - `input_file` (str): Path to the input XML file. - `output_file` (str): Path to save the modified XML file. - `tag_to_modify` (str): The tag name whose text content needs to be modified. - `new_text` (str): The new text content to be set for the specified tag. **Output:** - The function writes the modified XML content to the `output_file`. **Constraints:** - Assume the XML file is well-formed. - The specified tag (`tag_to_modify`) exists in the XML file. - The input and output file paths are valid. **Example:** Given an input XML file (`input.xml`): ```xml <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> ``` Calling `modify_xml(\'input.xml\', \'output.xml\', \'body\', \'Remember the meeting at 10 AM!\')` will result in the following modified XML saved to `output.xml`: ```xml <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Remember the meeting at 10 AM!</body> </note> ``` **Instructions:** 1. Use `xml.etree.ElementTree` for XML parsing and modification. 2. Use `xml.dom.minidom` for pretty-printing the modified XML. 3. Ensure the XML output file is well-formatted with proper indentation. 4. Add necessary error handling as needed. **Performance Requirements:** - Efficiently handle XML files up to 10MB in size. - Ensure the code runs within reasonable time limits and handles memory usage effectively. ```python import xml.etree.ElementTree as ET from xml.dom import minidom def modify_xml(input_file: str, output_file: str, tag_to_modify: str, new_text: str) -> None: # Implement the function here pass # Example usage: # modify_xml(\'input.xml\', \'output.xml\', \'body\', \'Remember the meeting at 10 AM!\') ```","solution":"import xml.etree.ElementTree as ET from xml.dom import minidom def modify_xml(input_file: str, output_file: str, tag_to_modify: str, new_text: str) -> None: # Parse the input XML file tree = ET.parse(input_file) root = tree.getroot() # Find the first occurrence of the tag_to_modify and change its text element = root.find(tag_to_modify) if element is not None: element.text = new_text # Write the modified XML to a string rough_string = ET.tostring(root, \'utf-8\') reparsed = minidom.parseString(rough_string) pretty_xml_as_string = reparsed.toprettyxml(indent=\\" \\") # Write the pretty-printed XML to the output file with open(output_file, \'w\') as f: f.write(pretty_xml_as_string)"},{"question":"# Python Object Manipulation and Validation You are required to implement a Python function `validate_and_process_objects` that takes a list of different Python objects and returns a dictionary summarizing their types and certain properties. Use type checks and relevant functions from the Python concrete objects API to ensure proper handling and processing. Function Signature ```python def validate_and_process_objects(objects: list) -> dict: pass ``` Input - `objects`: A list of arbitrary Python objects. The objects may be of types: integer, float, boolean, string, list, tuple, dictionary, set, bytes, bytearray, unicode, complex number, and None. Output - A dictionary where each key is a string representing the type of the object (e.g., `\'int\'`, `\'float\'`, `\'bool\'`, `\'str\'`, `\'list\'`, `\'tuple\'`, `\'dict\'`, `\'set\'`, `\'bytes\'`, `\'bytearray\'`, `\'unicode\'`, `\'complex\'`, `\'None\'`). The value corresponding to each key is a list of properties associated with that type. Properties to be Included - For numeric types (integer, float, complex): - `\'value\'`: The value of the number. - `\'is_positive\'`: Boolean indicating if the number is positive. - `\'is_zero\'`: Boolean indicating if the number is zero. - For booleans: - `\'value\'`: The boolean value. - For sequences (string, list, tuple, bytes, bytearray, unicode): - `\'length\'`: The length of the sequence. - `\'first_element_type\'`: The type of the first element in the sequence (if not empty). - For dictionaries: - `\'num_keys\'`: Number of keys in the dictionary. - `\'keys\'`: List of keys in the dictionary. - `\'values\'`: List of values in the dictionary. - For sets: - `\'num_elements\'`: Number of elements in the set. - For None: - No additional properties. Example ```python objects = [ 42, 3.14, True, \\"hello\\", [1, 2, 3], (4, 5), {\\"key\\": \\"value\\"}, {1, 2, 3}, b\\"bytes\\", bytearray(b\\"bytearray\\"), u\\"unicode string\\", complex(2,3), None ] result = validate_and_process_objects(objects) # Example of what the result might contain: print(result[\'int\']) # Output: # [{\'value\': 42, \'is_positive\': True, \'is_zero\': False }] ``` Constraints - Implement type checks as mentioned in the documentation instead of using Python\'s built-in `isinstance`. - Ensure that NULL objects (None) are properly handled and do not cause memory access violations. - Optimize for performance to handle large lists of objects efficiently. Notes - This question requires a solid understanding of Python object types and their properties. - Assume the input list will only contain valid Python object types as specified.","solution":"def validate_and_process_objects(objects): summary = { \'int\': [], \'float\': [], \'bool\': [], \'str\': [], \'list\': [], \'tuple\': [], \'dict\': [], \'set\': [], \'bytes\': [], \'bytearray\': [], \'unicode\': [], \'complex\': [], \'None\': [] } for obj in objects: obj_type = type(obj) if obj_type == int: summary[\'int\'].append({ \'value\': obj, \'is_positive\': obj > 0, \'is_zero\': obj == 0 }) elif obj_type == float: summary[\'float\'].append({ \'value\': obj, \'is_positive\': obj > 0, \'is_zero\': obj == 0 }) elif obj_type == bool: summary[\'bool\'].append({\'value\': obj}) elif obj_type == str: summary[\'str\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type == list: summary[\'list\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type == tuple: summary[\'tuple\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type == dict: summary[\'dict\'].append({ \'num_keys\': len(obj), \'keys\': list(obj.keys()), \'values\': list(obj.values()) }) elif obj_type == set: summary[\'set\'].append({\'num_elements\': len(obj)}) elif obj_type == bytes: summary[\'bytes\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type == bytearray: summary[\'bytearray\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type.__name__ == \'unicode\': summary[\'unicode\'].append({ \'length\': len(obj), \'first_element_type\': type(obj[0]).__name__ if obj else None }) elif obj_type == complex: summary[\'complex\'].append({ \'value\': obj, \'is_positive\': obj.real > 0, \'is_zero\': obj == 0 }) elif obj is None: summary[\'None\'].append({}) return {k: v for k, v in summary.items() if v}"},{"question":"# Implementing a Custom Sequence Class # Problem Statement Your task is to implement a custom sequence class `CustomSequence` in Python that mimics the behavior of Python sequence types (e.g., lists, tuples) using fundamental and advanced sequence operations. Your implementation should support the following functionalities: 1. Initialization: Initialize the class with a list of items. 2. Length Calculation: Return the length of the sequence using the `len` function. 3. Concatenation: Concatenate two sequence objects. 4. Repetition: Repeat the sequence a specified number of times. 5. Indexing: Retrieve an item at a given index. 6. Slicing: Retrieve a slice of the sequence. 7. Item Assignment: Assign a new value to an item at a given index. 8. Count Occurrences: Count the number of occurrences of a value in the sequence. 9. Containment Check: Check if a value is contained within the sequence. 10. Convert to List: Convert the sequence to a list. 11. Convert to Tuple: Convert the sequence to a tuple. # Class Definition ```python class CustomSequence: def __init__(self, items): Initialize the CustomSequence object with a list of items. :param items: List of items to initialize the sequence with. pass def __len__(self): Return the number of items in the sequence. :return: Length of the sequence. pass def __add__(self, other): Concatenate this sequence with another sequence. :param other: Another CustomSequence object. :return: A new CustomSequence object with concatenated items. pass def __mul__(self, count): Repeat the sequence \'count\' times. :param count: Number of times to repeat the sequence. :return: A new CustomSequence object with repeated items. pass def __getitem__(self, index): Retrieve the item at the specified index or slice. :param index: Index or slice to retrieve the item(s). :return: Item(s) at the specified index or slice. pass def __setitem__(self, index, value): Assign the value to the specified index. :param index: Index to assign the value. :param value: Value to be assigned. pass def count(self, value): Count the number of occurrences of \'value\' in the sequence. :param value: The value to count occurrences of. :return: The number of occurrences. pass def __contains__(self, value): Check if \'value\' is in the sequence. :param value: The value to check. :return: True if the value is in the sequence, otherwise False. pass def to_list(self): Convert the sequence to a list. :return: A list with the same contents as the sequence. pass def to_tuple(self): Convert the sequence to a tuple. :return: A tuple with the same contents as the sequence. pass ``` # Example Usage ```python # Initialize seq = CustomSequence([1, 2, 3]) # Length print(len(seq)) # Output: 3 # Concatenation seq2 = CustomSequence([4, 5]) seq3 = seq + seq2 print(seq3.to_list()) # Output: [1, 2, 3, 4, 5] # Repetition seq4 = seq * 2 print(seq4.to_list()) # Output: [1, 2, 3, 1, 2, 3] # Indexing print(seq[1]) # Output: 2 # Slicing print(seq[1:]) # Output: [2, 3] # Item Assignment seq[1] = 20 print(seq.to_list()) # Output: [1, 20, 3] # Count Occurrences print(seq.count(20)) # Output: 1 # Containment Check print(20 in seq) # Output: True # Convert to List print(seq.to_list()) # Output: [1, 20, 3] # Convert to Tuple print(seq.to_tuple()) # Output: (1, 20, 3) ``` **Constraints:** 1. You should not use built-in Python sequence classes directly in your class (e.g., list, tuple). **Performance Requirements:** 1. Your implementation should handle sequences with at least 100,000 elements efficiently. Make sure to test your `CustomSequence` class thoroughly to ensure all functionalities are working correctly.","solution":"class CustomSequence: def __init__(self, items): Initialize the CustomSequence object with a list of items. :param items: List of items to initialize the sequence with. self.items = items def __len__(self): Return the number of items in the sequence. :return: Length of the sequence. return len(self.items) def __add__(self, other): Concatenate this sequence with another sequence. :param other: Another CustomSequence object. :return: A new CustomSequence object with concatenated items. return CustomSequence(self.items + other.items) def __mul__(self, count): Repeat the sequence \'count\' times. :param count: Number of times to repeat the sequence. :return: A new CustomSequence object with repeated items. return CustomSequence(self.items * count) def __getitem__(self, index): Retrieve the item at the specified index or slice. :param index: Index or slice to retrieve the item(s). :return: Item(s) at the specified index or slice. return self.items[index] def __setitem__(self, index, value): Assign the value to the specified index. :param index: Index to assign the value. :param value: Value to be assigned. self.items[index] = value def count(self, value): Count the number of occurrences of \'value\' in the sequence. :param value: The value to count occurrences of. :return: The number of occurrences. return self.items.count(value) def __contains__(self, value): Check if \'value\' is in the sequence. :param value: The value to check. :return: True if the value is in the sequence, otherwise False. return value in self.items def to_list(self): Convert the sequence to a list. :return: A list with the same contents as the sequence. return self.items def to_tuple(self): Convert the sequence to a tuple. :return: A tuple with the same contents as the sequence. return tuple(self.items)"},{"question":"You are provided with the seaborn library and two datasets, `fmri` and `seaice`. # Objective: Create a composite visualization that provides meaningful insights from the datasets using Seaborn\'s objects interface. # Instructions: 1. Load both `fmri` and `seaice` datasets from seaborn. 2. Preprocess the `seaice` dataset to filter data from the years 1980 and 2019, and reshape it to have days of the year as rows and extent values as columns. 3. Query the `fmri` dataset to include only the \'parietal\' region. 4. Create a composite plot that includes: a. A `Band` plot representing the interval of sea ice extent between 1980 and 2019. b. Two visualizations from the `fmri` dataset: one showing the average signal over time with a line plot, and another showing error intervals with a band plot. 5. Apply appropriate customizations to ensure the visualizations are clear and informative. # Example Solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create composite plots plot_seaice = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\").add(so.Band(alpha=.5, edgewidth=2)) plot_fmri = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Display plots plot_seaice.show() plot_fmri.show() ``` # Requirements: 1. Your solution should load and preprocess the data correctly. 2. The final output should include both the sea ice extent band plot and the composite fmri visualization. 3. Customize the appearance of the plots to make them easily interpretable. This could include adjusting line widths, colors, transparency, and any other relevant properties. 4. Ensure all visualizations are displayed clearly.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load datasets fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year in [1980, 2019]\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .dropna() .reset_index() ) # Create composite plots plot_seaice = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\").add(so.Band(alpha=0.5, edgewidth=2)).scale(x=\'log\') plot_fmri = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Display plots plot_seaice.show() plot_fmri.show()"},{"question":"You are provided with the seaborn objects module documentation snippet that demonstrates loading a dataset and various plotting functionalities. Your task is to write a function to generate a comparative plot using seaborn that compares the distribution of \'price\' across different \'cut\' categories in the \'diamonds\' dataset. The function should cover the following requirements: 1. **Generate a plot object**: Use `so.Plot` to create a base plot. 2. **Apply logarithmic scaling**: Apply logarithmic scaling to the y-axis. 3. **Add percentiles**: Include dots in the plot to represent the 10th, 50th, and 90th percentiles for each \'cut\' category. 4. **Add range intervals**: - Overlay a range mark showing the 25th and 75th percentiles with a slight vertical shift. - Adjust the size and transparency of points to ensure clarity. 5. **Format the plot**: - Ensure proper labeling of axes and add a title \\"Price Distribution by Diamond Cut\\". - Customize the plot for better readability. Constraints: - You must use the seaborn `objects` module functions and methods as shown in the provided documentation snippet. - The function should handle any typical errors that might occur (e.g. dataset not loaded properly). Input: - None Output: - The function should display the generated plot. Function Signature: ```python def plot_diamond_cut_price_distribution(): pass ``` Example of Expected Visualization: The output should be a Seaborn plot similar to the examples provided in the documentation, with appropriate customization to meet the requirements. ```python from seaborn import load_dataset def plot_diamond_cut_price_distribution(): import seaborn.objects as so # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the base Plot p = so.Plot(diamonds, \\"cut\\", \\"price\\").scale(y=\\"log\\") # Add the main percentiles p.add(so.Dot(), so.Perc([10, 50, 90])) # Overlay range intervals with a slight vertical shift p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Customize points size and transparency p.add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) # Add title and labels for clarity p._title = \\"Price Distribution by Diamond Cut\\" # Display the plot p.show() # Call the function to display the plot plot_diamond_cut_price_distribution() ``` The function `plot_diamond_cut_price_distribution()` should generate a Seaborn plot object following the described steps and display it.","solution":"def plot_diamond_cut_price_distribution(): import seaborn as sns import seaborn.objects as so # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the base Plot and apply logarithmic scaling to the y-axis p = so.Plot(diamonds, x=\\"cut\\", y=\\"price\\").scale(y=\\"log\\") # Add the main percentiles (10th, 50th, 90th) p.add(so.Dot(), so.Perc([10, 50, 90])) # Overlay range intervals showing the 25th and 75th percentiles with vertical shift p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Customize points size and transparency for clarity p.add(so.Dot(pointsize=2, alpha=0.5), so.Jitter(width=0.2)) # Ensure proper labeling and add a title p = p.label(title=\\"Price Distribution by Diamond Cut\\", x=\\"Diamond Cut\\", y=\\"Price (log scale)\\") # Display the plot p.show() # Call the function to display the plot plot_diamond_cut_price_distribution()"},{"question":"**Objective:** Design a function using the `pathlib` module that navigates through a given directory, filters specific file types, counts them, and summarizes their details in a structured way. This task will test your understanding of both path manipulations and file operations using `pathlib`. **Problem Statement:** You are provided with the path to a directory. Write a Python function `summarize_directory` that traverses this directory recursively and summarizes information about all `.txt` and `.py` files contained within it. The function should output the summarized information as a dictionary. **Function Signature:** ```python from pathlib import Path def summarize_directory(directory: str) -> dict: pass ``` **Input:** - `directory`: A string representing the path to the directory that needs to be summarized. **Output:** - A dictionary with the following structure: ```python { \'total_txt_files\': int, \'total_py_files\': int, \'files\': [ { \'name\': str, \'path\': str, \'size\': int }, ... ] } ``` **Behavior:** - `total_txt_files` is an integer representing the total number of `.txt` files found in the directory. - `total_py_files` is an integer representing the total number of `.py` files found in the directory. - `files` is a list of dictionaries, each containing: - `name`: The name of the file. - `path`: The absolute path to the file. - `size`: The size of the file in bytes. **Constraints:** - Your function should handle and exclude symbolic links (you can assume that they should not be followed). - The function should be efficient and should not load the whole content of files into memory. **Example:** Suppose the directory structure is as follows: ```plaintext example_dir/ file1.txt file2.py subdir/ file3.txt file4.py file5.md ``` Then calling: ```python summarize_directory(\'example_dir\') ``` Should return: ```python { \'total_txt_files\': 2, \'total_py_files\': 2, \'files\': [ {\'name\': \'file1.txt\', \'path\': \'/absolute/path/to/example_dir/file1.txt\', \'size\': 123}, {\'name\': \'file2.py\', \'path\': \'/absolute/path/to/example_dir/file2.py\', \'size\': 456}, {\'name\': \'file3.txt\', \'path\': \'/absolute/path/to/example_dir/subdir/file3.txt\', \'size\': 78}, {\'name\': \'file4.py\', \'path\': \'/absolute/path/to/example_dir/subdir/file4.py\', \'size\': 90} ] } ``` **Notes:** - Ensure your code is well-documented and efficient. - Consider edge cases such as an empty directory, a directory without any `.txt` or `.py` files, etc. - You are encouraged to use appropriate methods provided by the `pathlib` module for all path and file operations.","solution":"from pathlib import Path def summarize_directory(directory: str) -> dict: Summarizes information about all .txt and .py files in the given directory and its subdirectories. Args: directory (str): The path to the directory to be summarized. Returns: dict: A dictionary containing the number of .txt and .py files and their details. dir_path = Path(directory) result = { \'total_txt_files\': 0, \'total_py_files\': 0, \'files\': [] } # Traverse the directory recursively for path in dir_path.rglob(\'*\'): if path.is_file() and path.suffix in {\'.txt\', \'.py\'}: file_info = { \'name\': path.name, \'path\': str(path.absolute()), \'size\': path.stat().st_size } result[\'files\'].append(file_info) if path.suffix == \'.txt\': result[\'total_txt_files\'] += 1 elif path.suffix == \'.py\': result[\'total_py_files\'] += 1 return result"},{"question":"**Title:** Customizing Seaborn Plot Contexts and Aesthetics **Objective:** Demonstrate your understanding of the Seaborn library by creating and customizing line plots using various contexts and aesthetic settings. **Task:** You are required to write a Python function `customize_plot()` that takes in two parameters: - `context`: A string representing the Seaborn context (e.g., `\\"notebook\\"`, `\\"paper\\"`, `\\"talk\\"`, `\\"poster\\"`). - `font_scale`: A float representing the scaling factor for the fonts used in the plot. The function should: 1. Set the specified context and font scale using `sns.set_context()`. 2. Generate a line plot using Seaborn with the x-values `[0, 1, 2, 3, 4]` and y-values `[0, 1, 4, 9, 16]`. 3. Save the plot to a file named `customized_plot.png`. Additional constraints: - You should also override the line width to be `2.5`. - Ensure the plot includes titles and labels for the axes that reflect the custom font scale. **Input:** - `context`: A string representing the context to be set for the plot (e.g., `\\"notebook\\"`, `\\"paper\\"`, `\\"talk\\"`, `\\"poster\\"`). - `font_scale`: A float representing the scaling factor for the fonts. **Output:** - A saved file named `customized_plot.png` containing the generated plot. **Example:** ```python customize_plot(\\"talk\\", 1.5) # This should generate a plot with \'talk\' context and fonts scaled by 1.5, save it as \'customized_plot.png\'. ``` **Constraints:** - Validate that `context` is one of the allowed Seaborn contexts (`notebook`, `paper`, `talk`, `poster`). - Validate that `font_scale` is a positive float. **Performance Requirements:** - The function should generate and save the plot efficiently without unnecessary computations or data. **Hints:** - Refer to the Seaborn documentation for details on `sns.set_context()` and how to customize plots. - You can use `plt.title()`, `plt.xlabel()`, and `plt.ylabel()` from the matplotlib library to add and scale titles and labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(context, font_scale): Customizes and saves a Seaborn line plot based on the given context and font scale. Parameters: - context: A string representing the Seaborn context (e.g., \\"notebook\\", \\"paper\\", \\"talk\\", \\"poster\\"). - font_scale: A float representing the scaling factor for the fonts used in the plot. # Validate inputs allowed_contexts = [\\"notebook\\", \\"paper\\", \\"talk\\", \\"poster\\"] if context not in allowed_contexts: raise ValueError(\\"Invalid context. Allowed values are: \'notebook\', \'paper\', \'talk\', \'poster\'.\\") if not isinstance(font_scale, (int, float)) or font_scale <= 0: raise ValueError(\\"Font scale must be a positive float.\\") # Set the context and font scale using Seaborn sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": 2.5}) # Data for the line plot x = [0, 1, 2, 3, 4] y = [0, 1, 4, 9, 16] # Create the plot plt.figure() sns.lineplot(x=x, y=y) plt.title(f\\"Line Plot with {context.capitalize()} Context\\", fontsize=font_scale * 10) plt.xlabel(\\"X-axis\\", fontsize=font_scale * 10) plt.ylabel(\\"Y-axis\\", fontsize=font_scale * 10) # Save the plot plt.savefig(\\"customized_plot.png\\") plt.close()"},{"question":"# Out-of-Core Learning Challenge **Objective:** Implement an out-of-core learning system for a text classification task using scikit-learn. **Problem Statement:** You are given a large dataset of text documents and their associated labels. The dataset is too large to fit into memory, so you must process it in mini-batches. Your task is to implement an out-of-core learning system that reads data in chunks, extracts features using a hashing technique, and performs incremental learning using one of the supported classifiers. **Instructions:** 1. **Dataset Simulation:** Simulate a streaming dataset of text documents and labels. Create a function `stream_data(batch_size)` that yields mini-batches of text documents and their labels. Each batch should contain `batch_size` number of samples. 2. **Feature Extraction:** Use `sklearn.feature_extraction.text.HashingVectorizer` to transform the text documents into numerical feature vectors. 3. **Incremental Learning:** Use `sklearn.linear_model.SGDClassifier` for the incremental learning task. This classifier supports the `partial_fit` method. 4. **Training:** Train the classifier incrementally using data from the `stream_data` function. Ensure to pass the possible classes in the first `partial_fit` call. 5. **Evaluation:** After training, evaluate the classifier on a test set and report the accuracy. **Constraints:** - Each batch processed should be at most 1,000 samples. - You should process at least 10 batches of data. - Use appropriate parameters for `SGDClassifier`. **Function Signatures:** 1. `def stream_data(batch_size: int) -> Tuple[List[str], List[int]]:` 2. `def train_incremental_classifier(num_batches: int, batch_size: int, classes: List[int], test_data: List[str], test_labels: List[int]) -> float:` **Example Usage:** ```python # Simulate streaming data def stream_data(batch_size: int): # This is just an example. Implement with actual data streaming logic. for _ in range(num_batches): texts = [\\"sample text\\"] * batch_size labels = [0] * batch_size # Replace with actual labels yield texts, labels # Definitions of classes and test data classes = [0, 1] test_data = [\\"test sample text\\"] * 1000 test_labels = [0] * 1000 # Replace with actual test labels # Train and evaluate the incremental classifier accuracy = train_incremental_classifier(num_batches=10, batch_size=1000, classes=classes, test_data=test_data, test_labels=test_labels) print(f\\"Test Accuracy: {accuracy}\\") ``` **Expected Output:** The function `train_incremental_classifier` should return the classification accuracy on the test set after processing the specified number of mini-batches.","solution":"from typing import List, Tuple from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(batch_size: int, num_batches: int) -> Tuple[List[str], List[int]]: Simulates streaming data by yielding mini-batches of text documents and labels. for i in range(num_batches): # Generating sample data for demo. Replace this with actual data streaming logic. texts = [f\\"sample text {i}\\" for i in range(batch_size)] labels = [i % 2 for i in range(batch_size)] yield texts, labels def train_incremental_classifier(num_batches: int, batch_size: int, classes: List[int], test_data: List[str], test_labels: List[int]) -> float: Trains an incremental classifier using streamed data and evaluates it on the test set. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() # Process each mini-batch of data for texts, labels in stream_data(batch_size, num_batches): X = vectorizer.transform(texts) classifier.partial_fit(X, labels, classes=classes) # Transform the test data X_test = vectorizer.transform(test_data) # Make predictions and calculate accuracy predictions = classifier.predict(X_test) accuracy = accuracy_score(test_labels, predictions) return accuracy"},{"question":"**Question:** # Resource Management and Debugging in Python Development Mode Python Development Mode introduces additional runtime checks that help developers catch issues early. One of the common mistakes in Python is improper resource management, such as forgetting to close files. When running in Development Mode, Python emits warnings when such issues are detected. In this exercise, you will write a Python function that reads from a file, counts the number of lines, and returns this count. You need to ensure that your code handles file resources properly to avoid any warnings when running in Python Development Mode. Requirements: 1. Write a function `count_lines(filename)` that: - Opens a file whose path is given by the argument `filename`. - Counts the number of lines in the file. - Ensures that the file is properly closed after reading. 2. To check your solution: - Run it normally to ensure it works correctly. - Run it in Python Development Mode using the `-X dev` flag to ensure no warnings (`ResourceWarning`) are emitted. Constraints: - Assume the file is a text file and exists at the given location. - The function should handle any potential exceptions in file operations gracefully, possibly logging an appropriate message. Input: - `filename` (str): The path to the text file. Output: - Returns an integer representing the number of lines in the file. Example: ```python def count_lines(filename): # Your implementation here # Example usage num_lines = count_lines(\\"example.txt\\") print(num_lines) # Should print the number of lines in example.txt ``` Additional Information: - Remember to handle the opening and closing of the file correctly to avoid `ResourceWarning`. - Ensuring proper closing of the file can be done using context managers in Python. **Challenge Upgrade (Optional):** - Modify your function to also log a message if the file could not be opened. This log should include the exception message and the filename. - Test your function with asyncio by writing an async version that uses `aiofiles` to read the file asynchronously and ensure proper resource management in an asynchronous context.","solution":"import logging def count_lines(filename): Counts the number of lines in a file. Parameters: filename (str): The path to the text file. Returns: int: The number of lines in the file. logging.basicConfig(level=logging.ERROR) try: with open(filename, \'r\') as file: return sum(1 for line in file) except Exception as e: logging.error(f\\"Failed to open the file {filename}: {e}\\") return 0"},{"question":"# Question: Implementing a Dimensionality Reduction Pipeline You are given a dataset containing numerous features. Your task is to implement a machine learning pipeline using scikit-learn that reduces the dimensionality of the data using Principal Component Analysis (PCA) and then applies a supervised learning algorithm to classify the data. Instructions: 1. **PCA Dimensionality Reduction**: - Perform PCA on the input data to reduce it to a specified number of components (`n_components`). 2. **Classification**: - Use a RandomForestClassifier to classify the reduced data. 3. **Pipeline**: - Combine the PCA transformation and RandomForestClassifier into a single pipeline. 4. **Evaluation**: - Evaluate the performance of the pipeline using cross-validation. Specifications: - **Input**: - `X_train`: A 2D numpy array or DataFrame of shape (n_samples, n_features) containing the training data. - `y_train`: A 1D numpy array or Series of shape (n_samples,) containing the labels for the training data. - `n_components`: An integer specifying the number of components for PCA. - `cv`: An integer specifying the number of cross-validation folds. - **Output**: - A dictionary containing the cross-validation scores for each fold and the mean accuracy. Example Usage: ```python X_train = [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2], [1.3, 1.4, 1.5, 1.6]] y_train = [0, 1, 0, 1] n_components = 2 cv = 3 results = implement_pipeline(X_train, y_train, n_components, cv) ``` Constraints: - Use `sklearn.decomposition.PCA` for dimensionality reduction. - Use `sklearn.ensemble.RandomForestClassifier` for the classification task. - Use `sklearn.pipeline.Pipeline` to create the pipeline. - Use `sklearn.model_selection.cross_validate` for cross-validation. Function Signature: ```python def implement_pipeline(X_train: Union[np.ndarray, pd.DataFrame], y_train: Union[np.ndarray, pd.Series], n_components: int, cv: int) -> Dict[str, Union(np.ndarray, float]]: pass ``` Use this function to perform the task. Ensure your implementation is efficient and follows scikit-learn\'s recommended practices.","solution":"from typing import Union, Dict import numpy as np import pandas as pd from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_validate def implement_pipeline(X_train: Union[np.ndarray, pd.DataFrame], y_train: Union[np.ndarray, pd.Series], n_components: int, cv: int) -> Dict[str, Union[np.ndarray, float]]: Implement a machine learning pipeline that includes PCA for dimensionality reduction and a RandomForest classifier. Parameters: - X_train: 2D numpy array or DataFrame of shape (n_samples, n_features) - y_train: 1D numpy array or Series of shape (n_samples,) - n_components: Number of components to keep for PCA - cv: Number of cross-validation folds Returns: - Dictionary with the cross-validation scores for each fold and the mean accuracy # Define the PCA and RandomForestClassifier pca = PCA(n_components=n_components) rfc = RandomForestClassifier() # Create a pipeline with PCA and RandomForestClassifier pipeline = Pipeline([ (\'pca\', pca), (\'classifier\', rfc) ]) # Perform cross-validation cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, return_train_score=False) # Prepare results results = { \'test_scores\': cv_results[\'test_score\'], \'mean_accuracy\': np.mean(cv_results[\'test_score\']) } return results"},{"question":"# Custom Iterator Implementation Objective Write a Python module that showcases custom iterator implementation using sequence-based and callable-based iterators. Your task involves creating both types of iterators and demonstrating their usage. Part 1: Sequence Iterator 1. Implement a class `CustomSequenceIterator` that takes a sequence (like a list) during initialization and supports iteration. 2. Use the provided documentation insights to guide your implementation, but don\'t use built-in `iter()` function directly. Part 2: Callable Iterator 1. Implement a class `CustomCallableIterator` that takes a callable and a sentinel value during initialization. 2. The callable should be invoked without any parameters, and iteration should terminate when the callable returns the sentinel value. # Requirements 1. Your classes should provide an iterable interface by implementing `__iter__` and `__next__` methods. 2. Demonstrate your classes by iterating over a sample sequence and a sample callable respectively. Input and Output Formats 1. **Input for CustomSequenceIterator**: - A sequence of any type supporting `__getitem__`. **Output**: - Items of the sequence will be yielded one by one until `IndexError`. 2. **Input for CustomCallableIterator**: - A callable object that returns values and a sentinel value to signify the end of iteration. **Output**: - Items returned by the callable will be yielded one by one until the sentinel value is returned. Example ```python # Example usage for CustomSequenceIterator sequence = [1, 2, 3, 4, 5] seq_iter = CustomSequenceIterator(sequence) for item in seq_iter: print(item) # Output: 1, 2, 3, 4, 5 # Example usage for CustomCallableIterator def callable_generator(): i = 0 while i < 5: yield i i += 1 callable_instance = callable_generator() sentinel = -1 call_iter = CustomCallableIterator(callable_instance.__next__, sentinel) for item in call_iter: print(item) # Output: 0, 1, 2, 3, 4 ``` Constraints 1. Do not use the built-in `iter()` functions for implementing the custom iterators. 2. Ensure your implementations handle typical error cases gracefully.","solution":"class CustomSequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration value = self.sequence[self.index] self.index += 1 return value class CustomCallableIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"**Title: Shared Memory Synchronization and Management** **Objective:** Implement a pair of functions to demonstrate the usage of shared memory blocks and synchronization between processes using the `multiprocessing.shared_memory` module. **Task Description:** Write two functions: 1. `create_and_modify_shared_memory(size, data)`: This function creates a shared memory block of a given size and populates it with initial data. 2. `read_and_modify_shared_memory(name, modify_indices, modify_values)`: This function attaches to an existing shared memory block using its name, reads the initial data, modifies specified indices with new values, and returns the modified data. **Function Specifications:** 1. **`create_and_modify_shared_memory(size, data)`** - **Inputs:** - `size` (int): The size of the shared memory block in bytes. - `data` (list of int): A list of integers to be stored in the shared memory block initially. - **Outputs:** - `shm_name` (str): The unique name of the created shared memory block. - **Constraints:** - The length of `data` should not exceed `size` divided by 4 (assuming 4 bytes per integer). - **Description:** - Create a shared memory block of the specified `size`. - Write the `data` to the shared memory block. - Return the unique name of the shared memory block for access by other processes. 2. **`read_and_modify_shared_memory(name, modify_indices, modify_values)`** - **Inputs:** - `name` (str): The unique name of the existing shared memory block. - `modify_indices` (list of int): Indices of the shared memory block to be modified. - `modify_values` (list of int): New values to write at the corresponding `modify_indices`. - **Outputs:** - `modified_data` (list of int): The list of integers representing the modified data in the shared memory block. - **Constraints:** - The lengths of `modify_indices` and `modify_values` must be equal. - **Description:** - Attach to the shared memory block using the provided `name`. - Read the current data from the shared memory block. - Modify the specified indices with the new values. - Return the modified data as a list of integers. **Example Usage:** ```python # In the first process shm_name = create_and_modify_shared_memory(20, [1, 2, 3, 4, 5]) print(shm_name) # e.g., \'psm_21467_46075\' # In the second process modified_data = read_and_modify_shared_memory(shm_name, [2, 4], [33, 55]) print(modified_data) # [1, 2, 33, 4, 55] ``` **Notes:** - Make sure to handle proper cleanup by calling `close` and `unlink` on the shared memory block after its use. - Assume no concurrent writes to the same indices in the shared memory block.","solution":"from multiprocessing import shared_memory import struct def create_and_modify_shared_memory(size, data): Creates a shared memory block of the given size and populates it with initial data. Arguments: size -- the size of the shared memory block in bytes data -- a list of integers to be stored in the shared memory block initially Returns: shm_name -- the unique name of the created shared memory block assert len(data) * 4 <= size, \\"Data exceeds the allowed size\\" # Create a new shared memory block shm = shared_memory.SharedMemory(create=True, size=size) # Write data into the shared memory block for i, value in enumerate(data): struct.pack_into(\'i\', shm.buf, i * 4, value) return shm.name def read_and_modify_shared_memory(name, modify_indices, modify_values): Attaches to an existing shared memory block, reads the initial data, modifies specified indices with new values, and returns the modified data. Arguments: name -- the unique name of the existing shared memory block modify_indices -- indices of the shared memory block to be modified modify_values -- new values to write at the corresponding modify_indices Returns: modified_data -- the modified data as a list of integers assert len(modify_indices) == len(modify_values), \\"Lengths of modify_indices and modify_values must be equal\\" # Attach to the existing shared memory block shm = shared_memory.SharedMemory(name=name) size = len(shm.buf) data = [] # Read the current data from the shared memory block for i in range(size // 4): value = struct.unpack_from(\'i\', shm.buf, i * 4)[0] data.append(value) # Modify the specified indices with the new values for idx, val in zip(modify_indices, modify_values): struct.pack_into(\'i\', shm.buf, idx * 4, val) data[idx] = val # Cleanup: it\'s the user responsibility to decide when to close and unlink the shared memory # shm.close() return data"},{"question":"Problem Statement Using the \\"curses.ascii\\" module, write a function `process_ascii_string` that takes a string as input and returns a processed version of the string. The function should adhere to the following rules: 1. Replace each control character in the string with its corresponding ASCII mnemonic or caret notation as specified in the `curses.ascii.unctrl` function. 2. Remove any extended ASCII characters (characters with ordinal values above 127). 3. The resulting string should contain only printable ASCII characters and spaces. Function Signature ```python def process_ascii_string(input_string: str) -> str: pass ``` Input and Output Formats - **Input**: A single string `input_string` (1 ≤ len(input_string) ≤ 10^6) containing ASCII characters. - **Output**: A single processed string following the rules specified above. Example ```python # Example 1 input_string = \\"Hellox00Worldx1b!\\" output_string = process_ascii_string(input_string) print(output_string) # Expected Output: \\"Hello^@World^[!\\" # Example 2 input_string = \\"Pythonx7f3.10x80is awesomex1a\\" output_string = process_ascii_string(input_string) print(output_string) # Expected Output: \\"Python^?3.10is awesome^Z\\" ``` Constraints and Notes - Ensure high efficiency, handling up to 1,000,000 characters in the input string. - Use the \\"curses.ascii\\" functions and constants appropriately to manage control characters and constraints.","solution":"import curses.ascii def process_ascii_string(input_string: str) -> str: Processes the input string by converting control characters to mnemonic or caret notation, and removing extended ASCII characters. Args: input_string (str): The string to be processed. Returns: str: The processed string with only printable ASCII characters and spaces. result = [] for char in input_string: if curses.ascii.iscntrl(char): result.append(curses.ascii.unctrl(char)) elif curses.ascii.isascii(char): if curses.ascii.isprint(char) or curses.ascii.isspace(char): result.append(char) return \'\'.join(result)"},{"question":"You are provided with the deprecated `pipes` module. Despite its deprecation in Python 3.11, let\'s explore its functionality through a task. # Task Write a function `transform_file(input_file: str, output_file: str, transformations: list)` that reads content from an `input_file`, applies a sequence of transformations using a `pipes.Template`, and writes the transformed content to an `output_file`. # Function signature ```python def transform_file(input_file: str, output_file: str, transformations: list) -> None: pass ``` # Parameters - `input_file` (str): The path to the input file. - `output_file` (str): The path to the output file. - `transformations` (list of tuples): A list where each tuple contains two elements: 1. `cmd` (str): A valid bourne shell command to apply to the data. 2. `kind` (str): A string of length 2 (e.g., `-f`, `ff`) specifying how the command reads the input and writes the output based on the `append` method\'s conventions. # Output - The function does not need to return anything. It will write the transformed data to the specified `output_file`. # Example You are given an input file `greetings.txt` that contains the following text: ``` hello world foo bar ``` You need to convert all characters to uppercase and then reverse the order of lines. The transformations provided should be: ```python transformations = [(\'tr a-z A-Z\', \'--\'), (\'tac\', \'--\')] ``` After applying these transformations, the content of the `output_file` should be: ``` FOO BAR HELLO WORLD ``` # Constraints - You can assume that the `input_file` will always exist and be readable. - You can assume that `output_file` can be created and written to. - You must ensure that the function handles the sequence of transformations in the order specified. # Notes - Utilize the `pipes.Template` class and its methods (especially `append`, `open`, and `copy`) to accomplish this task. - Error handling around shell command execution and file operations is a plus. Now, implement the `transform_file` function according to the description above.","solution":"import pipes def transform_file(input_file: str, output_file: str, transformations: list) -> None: Applies a sequence of transformations to the content of input_file and writes the result to output_file. :param input_file: The path to the input file. :param output_file: The path to the output file. :param transformations: A list of tuples where each tuple contains a command and its respective kind. template = pipes.Template() for cmd, kind in transformations: template.append(cmd, kind) with template.open(input_file, \'r\') as f_in: with open(output_file, \'w\') as f_out: f_out.write(f_in.read())"},{"question":"# Advanced PyTorch Tensor Views Exercise Objective: Demonstrate knowledge and understanding of PyTorch\'s tensor views by implementing a series of functions that perform specific tensor operations involving views. Description: 1. **Function 1: `reshape_tensor`** - **Input:** - A PyTorch tensor `t` of shape `(N, M)`. - Two integers `new_rows` and `new_cols` such that `new_rows * new_cols == N * M`. - **Output:** - A tensor reshaped to `(new_rows, new_cols)` using the `.view()` method. ```python def reshape_tensor(t: torch.Tensor, new_rows: int, new_cols: int) -> torch.Tensor: pass ``` 2. **Function 2: `transpose_and_check_contiguity`** - **Input:** - A PyTorch tensor `t` of shape `(N, M)`. - **Output:** - A tuple (`transposed_tensor`, `is_contiguous`) where `transposed_tensor` is obtained by transposing `t` using `.transpose(0, 1)`, and `is_contiguous` is a boolean that checks if the transposed tensor is contiguous. ```python def transpose_and_check_contiguity(t: torch.Tensor) -> tuple: pass ``` 3. **Function 3: `modify_view_and_check`** - **Input:** - A PyTorch tensor `t` of shape `(N, M)`. - An integer `(row, col)` pair indicating the element in `t` that should be updated. - A floating-point value `value` that will be set to the selected element. - **Output:** - The base tensor after modification by a view operation. ```python def modify_view_and_check(t: torch.Tensor, row: int, col: int, value: float) -> torch.Tensor: pass ``` Constraints: - You must use PyTorch operations adhering to the view principles described in the documentation. - Assume that provided input dimensions for reshaping are always valid. - Ensure that tensor contiguity checks employ PyTorch\'s `is_contiguous()` method. - Avoid unnecessary data copying where possible. Example Usage: ```python import torch # Example for reshape_tensor t = torch.arange(12).reshape(3, 4) reshaped = reshape_tensor(t, 2, 6) print(reshaped) # Should print tensor of shape (2, 6) # Example for transpose_and_check_contiguity t = torch.arange(6).reshape(2, 3) transposed, is_contig = transpose_and_check_contiguity(t) print(transposed) # Should be a tensor of shape (3, 2) print(is_contig) # Should print a boolean value # Example for modify_view_and_check t = torch.zeros((3, 3)) modified = modify_view_and_check(t, 1, 1, 5.0) print(modified) # Base tensor should reflect the modification ``` This question assesses the understanding of tensor views, tensor manipulation, and the effect of these operations on the underlying data and contiguity.","solution":"import torch def reshape_tensor(t: torch.Tensor, new_rows: int, new_cols: int) -> torch.Tensor: Reshapes the given tensor `t` to shape `(new_rows, new_cols)` using the `.view()` method. :param t: PyTorch tensor of shape `(N, M)` :param new_rows: New number of rows :param new_cols: New number of columns :return: Reshaped tensor of shape `(new_rows, new_cols)` return t.view(new_rows, new_cols) def transpose_and_check_contiguity(t: torch.Tensor) -> tuple: Transposes the given tensor `t` and checks if it is contiguous. :param t: PyTorch tensor of shape `(N, M)` :return: Tuple of transposed tensor and a boolean indicating if the tensor is contiguous transposed_tensor = t.transpose(0, 1) is_contiguous = transposed_tensor.is_contiguous() return transposed_tensor, is_contiguous def modify_view_and_check(t: torch.Tensor, row: int, col: int, value: float) -> torch.Tensor: Modifies the element at position `(row, col)` in the view of the tensor `t` to the given `value`. :param t: PyTorch tensor of shape `(N, M)` :param row: Row index of the element to modify :param col: Column index of the element to modify :param value: New value to set :return: The base tensor after modification view_t = t.view(t.shape) view_t[row, col] = value return t"},{"question":"**Objective:** Using the seaborn library, you are required to create various plots demonstrating your understanding of multi-dimensional plotting and faceting. **Question:** You are given a dataset on car specifications. Your task is to perform the following steps: 1. Load the `mpg` dataset using seaborn\'s `load_dataset` function. 2. Create a plot displaying the relationship between `displacement` and `acceleration` as well as between `weight` and `acceleration`. 3. Create a grid of subplots where each subplot shows the relationship between `mpg` and either `displacement`, `weight`, `horsepower`, or `cylinders`, arranged in a two-dimensional grid with 2 columns. 4. Create a faceted plot showing the relationship between `weight` and both `horsepower` and `acceleration`, with different origins (`col=\'origin\'`). For each plot: - Ensure to use `so.Plot` for plotting. - Add dots to the plots using `add(so.Dots())`. - Customize the labels to be clear and descriptive using the `label` method. **Input:** - No explicit input is required as you will load the dataset within the implementation. **Expected Output:** - The output should consist of three distinct plots displayed inline. **Constraints:** - Use seaborn\'s `so.Plot` class and accompanying methods to create the plots. - Clearly follow the structure and styling conventions provided in the documentation. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Step 1: Load the dataset mpg = load_dataset(\\"mpg\\") # Step 2: Plot displacement vs acceleration and weight vs acceleration ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement\\", x1=\\"Weight\\", y=\\"Acceleration\\") .add(so.Dots()) ) plt.show() # Step 3: Create a grid of subplots for mpg vs various variables ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .label(x0=\\"Displacement\\", x1=\\"Weight\\", x2=\\"Horsepower\\", x3=\\"Cylinders\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plt.show() # Step 4: Faceted plot for weight vs horsepower and acceleration, with facets by origin ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x=\\"Weight\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\") .add(so.Dots()) ) plt.show() ``` **Grading Criteria:** - Correctly using the seaborn `so.Plot`. - Properly displaying the required plots with specified variables. - Appropriate use of labeling and faceting. - Code readability and adherence to Python conventions.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def load_data(): Loads the mpg dataset from seaborn\'s collection. return sns.load_dataset(\\"mpg\\") def plot_displacement_weight_vs_acceleration(data): Plots displacement vs acceleration and weight vs acceleration. ( so.Plot(data, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement\\", x1=\\"Weight\\", y=\\"Acceleration\\") .add(so.Dots()) ) plt.show() def grid_plot_mpg_vs_variables(data): Creates a grid of subplots showing mpg vs displacement, weight, horsepower, and cylinders. ( so.Plot(data, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .label(x0=\\"Displacement\\", x1=\\"Weight\\", x2=\\"Horsepower\\", x3=\\"Cylinders\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plt.show() def faceted_plot_weight_vs_horsepower_acceleration(data): Creates a faceted plot showing weight vs horsepower and acceleration, with facets by origin. ( so.Plot(data, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x=\\"Weight\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\") .add(so.Dots()) ) plt.show()"},{"question":"# Python Coding Assessment Question **Objective**: Implement a function utilizing the \\"secrets\\" module to generate a secure random password following specific criteria. **Problem Statement**: You\'re tasked with writing a function `generate_secure_password(length: int, include_special_chars: bool=False) -> str` that generates a secure random password. This password must meet the following requirements: 1. The password length should be at least 12 characters. 2. The password must contain at least: - One lowercase letter. - One uppercase letter. - One digit. 3. If `include_special_chars` is `True`, the password must also contain at least one special character from the set `!@#%^&*()-_=+`. **Function Signature**: ```python def generate_secure_password(length: int, include_special_chars: bool=False) -> str: pass ``` **Input**: - `length` (int): The desired length of the password (must be at least 12). - `include_special_chars` (bool): A flag indicating whether to include special characters in the password. **Output**: - A string representing the generated secure password. **Constraints**: - `length` should be an integer greater than or equal to 12. - If `include_special_chars` is `True`, the generated password must include at least one special character. - The password should be generated using the cryptographically secure random functions provided by the \\"secrets\\" module. **Examples**: 1. `generate_secure_password(12)` might return `gG3kdj29Sle0` 2. `generate_secure_password(16, include_special_chars=True)` might return `As12!Ge0@Hjp5Mtr` Implement the required function `generate_secure_password` and ensure it correctly generates passwords adhering to the specified criteria. **Additional Notes**: - Ensure that the password generation logic includes a check to enforce the inclusion of required characters. - You may use the `secrets` and `string` modules to facilitate the generation of the password. Good luck, and encrypt your future with strong passwords!","solution":"import secrets import string def generate_secure_password(length: int, include_special_chars: bool=False) -> str: Generates a secure random password with the specified length. Includes at least one lowercase letter, one uppercase letter, and one digit. If include_special_chars is True, includes at least one special character. if length < 12: raise ValueError(\\"Password length must be at least 12 characters\\") alphabet = string.ascii_letters + string.digits if include_special_chars: special_chars = \\"!@#%^&*()-_=+\\" alphabet += special_chars while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password) and (not include_special_chars or any(c in special_chars for c in password))): return password"},{"question":"# Question on UUIDs and Manipulations **Objective**: Implement a set of functions using Python\'s `uuid` module to generate and manipulate UUIDs in specified ways. Requirements: 1. **Function** `generate_uuids(n: int) -> List[str]` - Generates `n` random UUIDs using `uuid.uuid4()`. - Returns a list of these UUIDs as strings. 2. **Function** `uuid_analysis(uuids: List[str]) -> Dict[str, Any]` - Accepts a list of UUID strings. - Analyzes the list to return: - The total number of UUIDs. - A set of unique UUID variants found (possible values: `uuid.RFC_4122`, `uuid.RESERVED_NCS`, `uuid.RESERVED_MICROSOFT`, `uuid.RESERVED_FUTURE`). - A count of each version of the UUIDs (depending on `uuid.version` attribute, which is meaningful only if `uuid.variant` is `uuid.RFC_4122`). 3. **Function** `generate_namespaced_uuid(namespace: str, name_str: str, hash_method: str) -> str` - Generates a UUID based on the provided `namespace` and `name_str`, and hash method specified. - The `namespace` should be one of the predefined namespaces: `NAMESPACE_DNS`, `NAMESPACE_URL`, `NAMESPACE_OID`, or `NAMESPACE_X500`. - The `hash_method` should be either \\"MD5\\" or \\"SHA1\\". - Uses `uuid.uuid3()` for MD5 and `uuid.uuid5()` for SHA-1. - Returns the generated UUID as a string. Constraints: - Assume `n` is a non-negative integer and `uuids` is a non-empty list of valid UUID strings. - `namespace` and `hash_method` inputs will be valid strings (as specified in the requirements). Example Usage: ```python import uuid # Generating a list of 5 random UUIDs random_uuids = generate_uuids(5) # Analyzing a list of UUIDs analysis_result = uuid_analysis(random_uuids) # analysis_result should be a dictionary containing the details of the UUIDs in the list. # Generating a namespaced UUID namespaced_uuid = generate_namespaced_uuid(\'NAMESPACE_DNS\', \'example.com\', \'SHA1\') ``` Notes: - The functions should utilize Python\'s `uuid` module efficiently. - Exception handling for invalid inputs is not required. - Focus on using the attributes and methods provided by the `uuid` module to achieve the tasks.","solution":"import uuid from typing import List, Dict, Any def generate_uuids(n: int) -> List[str]: Generates `n` random UUIDs using uuid.uuid4(). Parameters: - n (int): Number of UUIDs to generate. Returns: - List[str]: List of generated UUIDs as strings. return [str(uuid.uuid4()) for _ in range(n)] def uuid_analysis(uuids: List[str]) -> Dict[str, Any]: Analyzes a list of UUID strings to return details about the UUIDs. Parameters: - uuids (List[str]): List of UUID strings. Returns: - Dict[str, Any]: Dictionary containing analysis of UUIDs. total_uuids = len(uuids) unique_variants = set() version_counts = {} for uuid_str in uuids: u = uuid.UUID(uuid_str) unique_variants.add(u.variant) if u.variant == uuid.RFC_4122: version_counts[u.version] = version_counts.get(u.version, 0) + 1 return { \\"total_uuids\\": total_uuids, \\"unique_variants\\": unique_variants, \\"version_counts\\": version_counts } def generate_namespaced_uuid(namespace: str, name_str: str, hash_method: str) -> str: Generates a UUID based on the provided `namespace`, `name_str`, and `hash_method`. Parameters: - namespace (str): The namespace for the UUID (one of the predefined namespaces). - name_str (str): The name string for the UUID. - hash_method (str): The hashing method (\\"MD5\\" or \\"SHA1\\"). Returns: - str: The generated UUID as a string. namespaces = { \\"NAMESPACE_DNS\\": uuid.NAMESPACE_DNS, \\"NAMESPACE_URL\\": uuid.NAMESPACE_URL, \\"NAMESPACE_OID\\": uuid.NAMESPACE_OID, \\"NAMESPACE_X500\\": uuid.NAMESPACE_X500 } if hash_method == \\"MD5\\": return str(uuid.uuid3(namespaces[namespace], name_str)) elif hash_method == \\"SHA1\\": return str(uuid.uuid5(namespaces[namespace], name_str)) else: raise ValueError(\\"Invalid hash method. Use \'MD5\' or \'SHA1\'.\\")"},{"question":"# Tensor Views and Contiguity in PyTorch **Objective**: You are required to implement a function that demonstrates the use of tensor views in PyTorch. Your function should: 1. Create a base tensor. 2. Perform a series of view operations on this tensor. 3. Verify if views share data with the base tensor. 4. Check the contiguity of the resulting views. 5. Return results that reflect these operations and checks. **Function Signature**: ```python import torch def tensor_view_operations(): pass ``` **Expected Tasks**: 1. Create a 4x4 tensor of random values. 2. Create at least three different views of this tensor using methods such as `view`, `transpose`, `narrow`, etc. 3. Modify an element in one of the views and demonstrate that this change is reflected in the base tensor. 4. Check and return whether each view shares the same data pointer (`storage().data_ptr()`) as the base tensor. 5. Check and return whether each view is contiguous using `is_contiguous()`. **Constraints**: - The random values in the base tensor should be floating-point numbers between 0 and 1. - You may use any of the view operations listed in the provided PyTorch documentation. **Example Output**: The function should return a dictionary with keys indicating the view operations performed and values being tuples of: 1. Boolean indicating data pointer sharing with the base tensor. 2. Boolean indicating whether the tensor is contiguous. For instance: ```python { \\"view\\": (True, True), \\"transpose\\": (True, False), \\"narrow\\": (True, True), # Additional views as per implementation } ``` You are required to provide code that demonstrates the understanding and manipulation of tensor views, contiguous checks, and the implications of views sharing underlying data with their base tensors.","solution":"import torch def tensor_view_operations(): # Create a 4x4 tensor of random values base_tensor = torch.rand((4, 4)) # Create different views of this tensor view_1 = base_tensor.view(-1) transpose_1 = base_tensor.t() narrow_1 = base_tensor.narrow(0, 1, 3) # Modify an element in one of the views view_1[0] = -1 # this should reflect in the base tensor # Verification if views share data with the base tensor checks = { \\"view\\": (view_1.storage().data_ptr() == base_tensor.storage().data_ptr(), view_1.is_contiguous()), \\"transpose\\": (transpose_1.storage().data_ptr() == base_tensor.storage().data_ptr(), transpose_1.is_contiguous()), \\"narrow\\": (narrow_1.storage().data_ptr() == base_tensor.storage().data_ptr(), narrow_1.is_contiguous()), } return checks"},{"question":"Question: Functional Programming Challenge # Objective: Your task is to implement a function that processes a list of numbers using various functional programming libraries to compute various metrics on the list. This will involve utilizing `itertools`, `functools`, and `operator`. # Function Signature: ```python def functional_metrics(numbers: List[int]) -> Dict[str, float]: ``` # Input: - A list of integers `numbers` with `1 <= len(numbers) <= 10^6`. # Output: - A dictionary with keys as `[\\"average\\", \\"max_diff\\", \\"sub_sum_half\\"]` and their corresponding values with the following meanings: - `average`: The average of the numbers in the list. - `max_diff`: The maximum difference between adjacent numbers in the sorted list. - `sub_sum_half`: The sum of every second number in the list, starting from the first one. # Constraints: - You must use `itertools`, `functools`, and `operator` in the implementation of your function. - The function should be optimized for performance given the input size constraint. # Hints: - Use `itertools` for efficient looping and iteration. - Use `functools` for reducing or partial function creation. - Use `operator` to map standard operations. # Example: ```python from typing import List, Dict from itertools import tee, islice from functools import reduce from operator import sub, add def functional_metrics(numbers: List[int]) -> Dict[str, float]: if not numbers: return {\\"average\\": 0.0, \\"max_diff\\": 0.0, \\"sub_sum_half\\": 0.0} # Calculate average total_sum = reduce(add, numbers) average = total_sum / len(numbers) # Calculate max_diff sorted_numbers = sorted(numbers) pairwise_diff = map(sub, islice(sorted_numbers, 1, None), sorted_numbers) max_diff = max(pairwise_diff) # Calculate sub_sum_half sub_sum_half = reduce(add, islice(numbers, 0, None, 2)) return {\\"average\\": average, \\"max_diff\\": max_diff, \\"sub_sum_half\\": sub_sum_half} # Example usage print(functional_metrics([1, 3, 5, 7, 9])) # Output should be {\\"average\\": 5.0, \\"max_diff\\": 2.0, \\"sub_sum_half\\": 16.0} ``` # Explanation: - **Average**: The total sum of the list divided by the length. - **Max Difference**: Sort the list and find the maximum difference between consecutive elements. - **Subset Sum Half**: Sum the values at every second index in the list using slicing. Ensure you understand how to utilize `itertools`, `functools`, and `operator` to implement these requirements.","solution":"from typing import List, Dict from itertools import tee, islice from functools import reduce from operator import sub, add def functional_metrics(numbers: List[int]) -> Dict[str, float]: if not numbers: return {\\"average\\": 0.0, \\"max_diff\\": 0.0, \\"sub_sum_half\\": 0.0} # Calculate average total_sum = reduce(add, numbers) average = total_sum / len(numbers) # Calculate max_diff sorted_numbers = sorted(numbers) it1, it2 = tee(sorted_numbers) next(it2, None) pairwise_diff = map(sub, it2, it1) max_diff = max(pairwise_diff, default=0) # Calculate sub_sum_half sub_sum_half = reduce(add, islice(numbers, 0, None, 2)) return {\\"average\\": average, \\"max_diff\\": max_diff, \\"sub_sum_half\\": sub_sum_half}"},{"question":"# Question: Visualizing Automobile Characteristics with Seaborn You are provided with the \\"mpg\\" dataset, which contains information on the fuel economy of various cars. Your task is to create a comprehensive visualization using seaborn that meets the following criteria: 1. Load the \\"mpg\\" dataset from seaborn. 2. Create a scatter plot showing the relationship between \\"horsepower\\" (on the x-axis) and \\"mpg\\" (on the y-axis). 3. Color the dots by the \\"origin\\" variable. 4. Use different markers to differentiate between the number of cylinders in each car. Specifically, use: - \'o\' for 4 cylinders - \'x\' for 6 cylinders - Any other marker (e.g., a diamond) for 8 cylinders. 5. Add jitter to the x-axis values to help visualize the density of points. 6. Ensure that the fill color of the markers indicates \\"weight\\" using a quantitative color scale (e.g., \\"viridis\\"). Implement the function `plot_mpg_data()` that will produce the described plot. Expected Input and Output Formats: - **Input:** None (the function should internally load the dataset). - **Output:** None (the function should display the plot directly). Constraints: - You should use seaborn and its `objects` module for plotting. - Ensure your function is efficiently coded and avoids unnecessary computations. Example: The function call `plot_mpg_data()` should produce a scatter plot as per the specifications above. Code: ```python import seaborn.objects as so from seaborn import load_dataset def plot_mpg_data(): mpg = load_dataset(\\"mpg\\") plot = ( so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") .add(so.Dots(), so.Jitter(x=0.25), color=\\"origin\\") .add(so.Dots(), marker=so.Parameter(\\"cylinders\\"), fillcolor=\\"weight\\") .scale(fillcolor=\\"viridis\\", marker={\\"4\\": \\"o\\", \\"6\\": \\"x\\", \\"8\\": \\"d\\"}) ) plot.show() ``` Given the constraints and requirements, your task is to implement the function `plot_mpg_data()`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_mpg_data(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Create jitter for horsepower to avoid overplotting jittered_hp = mpg[\\"horsepower\\"] + np.random.normal(0, 1, len(mpg)) # Define marker style based on the number of cylinders markers = { 4: \'o\', 6: \'x\', 8: \'D\' } # Initialize the figure and axis plt.figure(figsize=(12, 8)) ax = sns.scatterplot( x=jittered_hp, y=\\"mpg\\", hue=\\"origin\\", style=mpg[\\"cylinders\\"].map(markers), size=\\"weight\\", sizes=(20, 200), palette=\'viridis\', data=mpg ) # Set plot labels and title ax.set_xlabel(\'Horsepower\') ax.set_ylabel(\'Miles per Gallon (MPG)\') ax.set_title(\'MPG vs Horsepower, Colored by Origin, Marker Style by Cylinders, Size by Weight\') # Show plot plt.show()"},{"question":"# Question: Multi-Type Object Manipulation and Compatibility You are tasked with writing a function `analyze_and_convert` that takes a list of various Python objects and performs the following tasks: 1. **Type Checking:** Verify the type of each object in the list. - If the object is an integer, convert it to its hexadecimal string representation. - If the object is a float, round it to two decimal places. - If the object is a string, convert it to a bytes object using UTF-8 encoding. - If the object is a dictionary, extract all keys as a list. - If the object is a list, concatenate all string elements within the list. Ignore non-string elements. - Ignore any other types of objects. 2. **Output Construction:** Create a dictionary where each key is the original type name and the value is a list of processed objects of that type. **Function Signature:** ```python def analyze_and_convert(objects: list) -> dict: ``` **Input:** - `objects` (list): A list containing objects of various types (integers, floats, strings, dictionaries, lists). **Output:** - A dictionary where keys are type names (`\'int\'`, `\'float\'`, `\'str\'`, `\'dict\'`, `\'list\'`) and values are lists of processed objects as per the rules above. **Constraints:** - You may assume the given list will contain at least one object. - You are not required to handle nested lists or dictionaries. **Example:** ```python input_data = [123, 3.14159, \\"Hello\\", {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}, [1, 2, \\"world\\", \\"Python\\"], True] output = analyze_and_convert(input_data) ``` **Expected Output:** ```python { \'int\': [\'0x7b\'], \'float\': [3.14], \'str\': [b\'Hello\'], \'dict\': [[\'key1\', \'key2\']], \'list\': [\'worldPython\'] } ``` Write the function `analyze_and_convert` that accomplishes the above tasks while ensuring type safety and correct object manipulations.","solution":"def analyze_and_convert(objects: list) -> dict: result = {\'int\': [], \'float\': [], \'str\': [], \'dict\': [], \'list\': []} for obj in objects: if isinstance(obj, int): result[\'int\'].append(hex(obj)) elif isinstance(obj, float): result[\'float\'].append(round(obj, 2)) elif isinstance(obj, str): result[\'str\'].append(obj.encode(\'utf-8\')) elif isinstance(obj, dict): result[\'dict\'].append(list(obj.keys())) elif isinstance(obj, list): concatenated_str = \'\'.join([item for item in obj if isinstance(item, str)]) result[\'list\'].append(concatenated_str) return {k: v for k, v in result.items() if v} # Example usage input_data = [123, 3.14159, \\"Hello\\", {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}, [1, 2, \\"world\\", \\"Python\\"], True] output = analyze_and_convert(input_data) print(output)"},{"question":"**Coding Assessment Question: Pandas ExtensionArray Implementation** # Objective Your task is to implement a custom pandas ExtensionArray that represents an array of hexadecimal (base-16) values. This custom array should be capable of converting between decimal and hexadecimal formats, along with supporting several core array operations. # Problem Statement Implement a custom pandas ExtensionArray to handle arrays of hexadecimal values. Your implementation should include: 1. **HexExtensionDtype**: - A custom pandas ExtensionDtype to represent hexadecimal data type. 2. **HexExtensionArray**: - A custom pandas ExtensionArray to store and manipulate hexadecimal values. # Requirements 1. **Class: HexExtensionDtype** - Must inherit from `pandas.api.extensions.ExtensionDtype`. - Should define the necessary properties: `name`, `type`, and `na_value`. 2. **Class: HexExtensionArray** - Must inherit from `pandas.api.extensions.ExtensionArray`. - Should define the necessary methods: `__init__`, `__getitem__`, `__len__`, `_from_sequence`, `_concat_same_type`, `to_numpy`, `dtype`, `nbytes`, `isna`, `take`, and `copy`. - The array should internally store hexadecimal values as strings but allow conversion from and to decimal integers. # Example ```python import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class HexExtensionDtype(ExtensionDtype): name = \\"hex\\" type = str na_value = None @classmethod def construct_array_type(cls): return HexExtensionArray class HexExtensionArray(ExtensionArray): def __init__(self, values): self._data = [str(hex(val)) if isinstance(val, int) else val for val in values] def __getitem__(self, idx): return self._data[idx] def __len__(self): return len(self._data) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls([str(val) for val in scalars]) def _concat_same_type(self, to_concat): return HexExtensionArray([item for sublist in to_concat for item in sublist]) def to_numpy(self): return self._data @property def dtype(self): return HexExtensionDtype() @property def nbytes(self): return sum(len(x) for x in self._data) def isna(self): return [x is None for x in self._data] def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else self.dtype.na_value result = [self._data[idx] if idx != -1 else fill_value for idx in indices] else: result = [self._data[idx] for idx in indices] return HexExtensionArray(result) def copy(self): return HexExtensionArray(self._data.copy()) ``` # Input Format - There are no specific inputs for this task. You must implement the classes as described above. # Output Format - No explicit output. Ensure the customized `HexExtensionArray` and `HexExtensionDtype` work seamlessly with pandas Series and DataFrames. # Constraints - You must ensure that your implementation adheres to the pandas extension array API. - Implement all necessary methods to handle common operations required by pandas ExtensionArray. # Notes - Custom extension arrays should be able to integrate with pandas data structures such as `Series` and `DataFrame`. - Proper handling of missing values (`na_value`) is essential.","solution":"import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class HexExtensionDtype(ExtensionDtype): name = \\"hex\\" type = str na_value = None @classmethod def construct_array_type(cls): return HexExtensionArray class HexExtensionArray(ExtensionArray): def __init__(self, values): self._data = [str(hex(val)) if isinstance(val, int) else val for val in values] def __getitem__(self, idx): return self._data[idx] def __len__(self): return len(self._data) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls([str(hex(val)) if isinstance(val, int) else val for val in scalars]) def _concat_same_type(self, to_concat): return HexExtensionArray([item for sublist in to_concat for item in sublist]) def to_numpy(self, dtype=None, copy=False): return self._data @property def dtype(self): return HexExtensionDtype() @property def nbytes(self): return sum(len(x.encode(\'utf-8\')) for x in self._data) def isna(self): return [x == self.dtype.na_value for x in self._data] def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else self.dtype.na_value result = [self._data[idx] if idx != -1 else fill_value for idx in indices] else: result = [self._data[idx] for idx in indices] return HexExtensionArray(result) def copy(self): return HexExtensionArray(self._data.copy())"},{"question":"Coding Assessment Question # Task You are required to implement a Python function for parsing arguments and building values, which simulates the behavior of a subset of the argument parsing mechanisms described. This function should be able to: 1. **Parse Command-Line Style Arguments**: Given a list of arguments (similar to `sys.argv`), the function should recognize and handle different types of arguments: positional arguments, optional flags, and options with values. 2. **Build and Return a Dictionary**: The parsed arguments should be returned as a Python dictionary, with the argument names as keys and the corresponding values (or `None` for flags) as values. # Detailed Requirements - **Input**: A list of strings representing command-line arguments. - **Output**: A dictionary with argument names as keys and their corresponding values (or `None` for flags). # Specifications 1. **Positional Arguments**: These are arguments that are expected to be found in a specific order. For instance, `[\'input.txt\', \'output.txt\']` should be parsed as `{\'positional\': [\'input.txt\', \'output.txt\']}`. 2. **Optional Flags**: These are arguments typically prefixed with `-` or `--` and do not take any value. For example, `[\'-v\', \'--help\']` should be parsed as `{\'v\': None, \'help\': None}`. 3. **Options with Values**: These are arguments that take a value. They are usually separated from their value by a space or `=`. For instance, `[\'--file=input.txt\', \'--output\', \'output.txt\']` should be parsed as `{\'file\': \'input.txt\', \'output\': \'output.txt\'}`. # Constraints - The function should handle edge cases such as no arguments, missing values for options with values, and repeated flags or options. - The function should ignore any unknown flags or options and only return the recognized ones. - Positional arguments should be collected into a list and returned under the key `\'positional\'`. # Function Signature ```python def parse_arguments(arguments: list) -> dict: pass ``` # Example ```python assert parse_arguments([\'input.txt\', \'output.txt\', \'-v\', \'--file=input.txt\', \'--output\', \'output.txt\']) == { \'positional\': [\'input.txt\', \'output.txt\'], \'v\': None, \'file\': \'input.txt\', \'output\': \'output.txt\' } assert parse_arguments([\'input.txt\', \'--help\']) == { \'positional\': [\'input.txt\'], \'help\': None } assert parse_arguments([]) == { \'positional\': [] } ``` # Performance Requirements The solution should be efficient enough to handle a large number of arguments (up to 1000) within a reasonable time frame (e.g., 1 second).","solution":"def parse_arguments(arguments: list) -> dict: Parses command-line style arguments into a dictionary. Positional arguments are put into a list under the \'positional\' key, optional flags are set as keys with `None` as their values, and options with values are set as key-value pairs. result = {\'positional\': []} i = 0 while i < len(arguments): arg = arguments[i] if arg.startswith(\'--\'): if \'=\' in arg: key, value = arg[2:].split(\'=\', 1) result[key] = value else: if i + 1 < len(arguments) and not arguments[i + 1].startswith(\'-\'): result[arg[2:]] = arguments[i + 1] i += 1 else: result[arg[2:]] = None elif arg.startswith(\'-\'): result[arg[1:]] = None else: result[\'positional\'].append(arg) i += 1 return result"},{"question":"# Question: Implementing a Log Compression and Decompression Utility Objective: You are required to implement a utility that compresses and decompresses log files using the bzip2 algorithm. This utility will help users save storage space by compressing large log files and decompressing them when needed. Requirements: 1. **Function `compress_log_file(file_path: str, compressed_file_path: str, compresslevel: int = 9) -> None`:** - **Input**: - `file_path` (str): The path to the log file that needs to be compressed. - `compressed_file_path` (str): The destination path for the compressed file. - `compresslevel` (int, optional): The level of compression (1-9), with 9 being the highest. Defaults to 9. - **Output**: - None - **Description**: - Reads the log file located at `file_path`. - Compresses its contents using bzip2 compression with the specified `compresslevel`. - Writes the compressed data to `compressed_file_path`. 2. **Function `decompress_log_file(compressed_file_path: str, decompressed_file_path: str) -> None`:** - **Input**: - `compressed_file_path` (str): The path to the compressed log file. - `decompressed_file_path` (str): The destination path for the decompressed log file. - **Output**: - None - **Description**: - Reads the compressed log file located at `compressed_file_path`. - Decompresses its contents. - Writes the decompressed data to `decompressed_file_path`. Constraints: 1. The utility should handle large log files efficiently. 2. The utility should validate the inputs and raise appropriate errors if the files cannot be read or written. 3. You should use the incremental (de)compression classes provided by the `bz2` module to manage memory usage efficiently. Example Usage: ```python # Compress a log file compress_log_file(\'server.log\', \'server.log.bz2\') # Decompress the log file decompress_log_file(\'server.log.bz2\', \'server_decompressed.log\') # Verify the decompressed log file matches the original with open(\'server.log\', \'rb\') as original, open(\'server_decompressed.log\', \'rb\') as decompressed: assert original.read() == decompressed.read() ``` # Constraints: - Ensure the original file and the decompressed file are identical. - Use exception handling to manage possible I/O errors. - Optimize for memory usage using incremental compression and decompression techniques. Submission: Submit the implementation along with a set of test cases to validate the correct functionality of both compression and decompression functions. Ensure that your test cases cover different file sizes and verify the integrity of the decompressed files.","solution":"import bz2 def compress_log_file(file_path: str, compressed_file_path: str, compresslevel: int = 9) -> None: Compresses the log file at file_path using bzip2 and writes the compressed data to compressed_file_path. try: with open(file_path, \'rb\') as input_file, bz2.BZ2File(compressed_file_path, \'wb\', compresslevel=compresslevel) as output_file: for data in iter(lambda: input_file.read(1024 * 1024), b\'\'): output_file.write(data) except Exception as e: raise IOError(f\\"An error occurred while compressing the file: {e}\\") def decompress_log_file(compressed_file_path: str, decompressed_file_path: str) -> None: Decompresses the bzip2 compressed log file at compressed_file_path and writes the decompressed data to decompressed_file_path. try: with bz2.BZ2File(compressed_file_path, \'rb\') as input_file, open(decompressed_file_path, \'wb\') as output_file: for data in iter(lambda: input_file.read(1024 * 1024), b\'\'): output_file.write(data) except Exception as e: raise IOError(f\\"An error occurred while decompressing the file: {e}\\")"},{"question":"# File Locking and Concurrent Access Control in Python Objective Implement a Python function to handle file locking and concurrent access using the `fcntl` module. Function Signature ```python def manage_file_access(file_path: str, operation: str, lock_type: str): pass ``` Description You need to implement the `manage_file_access` function, which aims to manage file access using file locks to ensure safe concurrent access. The function should perform the following operations based on the parameters provided: 1. **File Path (`file_path`)**: - A string representing the path of the file to be managed. 2. **Operation (`operation`)**: - A string specifying the file operation to be performed. It can be one of the following values: - `\\"read\\"`: Read the contents of the file. - `\\"write\\"`: Write a predefined string \\"Hello, World!\\" to the file. 3. **Lock Type (`lock_type`)**: - A string specifying the type of lock to be used. It can be one of the following values: - `\\"shared\\"`: Acquire a shared lock (`LOCK_SH`). - `\\"exclusive\\"`: Acquire an exclusive lock (`LOCK_EX`). - `\\"unlock\\"`: Release the lock (`LOCK_UN`). The function should: - Open the file specified by `file_path`. - Apply the appropriate lock described by `lock_type` using `fcntl.flock`. - Perform the file operation specified by `operation`. - Handle any exceptions that might arise and ensure proper release of locks. Constraints - The function should handle large files without loading the entire file into memory. - Ensure that file operations do not lead to deadlocks. - Properly handle exceptions to avoid leaving the file in a locked state. Example ```python # Example file path file_path = \\"example.txt\\" # Perform a write operation with an exclusive lock manage_file_access(file_path, \\"write\\", \\"exclusive\\") # Perform a read operation with a shared lock manage_file_access(file_path, \\"read\\", \\"shared\\") # Unlock the file manage_file_access(file_path, \\"\\", \\"unlock\\") ``` Expected Output - The `\\"write\\"` operation will write \\"Hello, World!\\" into the file. - The `\\"read\\"` operation will return the contents of the file. - The `\\"unlock\\"` operation will release any locks on the file. You should aim for robust error handling and efficient resource management in your implementation.","solution":"import fcntl import os def manage_file_access(file_path: str, operation: str, lock_type: str): Manages file access with locking. Parameters: - file_path: Path to the file. - operation: Operation to perform, \'read\' or \'write\'. - lock_type: Type of lock, \'shared\', \'exclusive\', or \'unlock\'. lock_map = { \'shared\': fcntl.LOCK_SH, \'exclusive\': fcntl.LOCK_EX, \'unlock\': fcntl.LOCK_UN } try: with open(file_path, \'a+\' if operation == \'write\' else \'r\') as file: if lock_type in lock_map: fcntl.flock(file, lock_map[lock_type]) if operation == \'read\': file.seek(0) # Go to the beginning of the file data = file.read() print(data) # Only for demonstration; in actual use, we might return data elif operation == \'write\': file.write(\\"Hello, World!n\\") file.flush() # Ensure data is written to disk if lock_type == \'unlock\': fcntl.flock(file, fcntl.LOCK_UN) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective Your task is to implement a Python function that makes an HTTP GET request to a specified URL, handles any redirects, and manages cookies appropriately. The function should also add custom headers and handle the response accordingly. # Function Signature ```python def fetch_url(url: str, headers: dict, cookies: dict) -> dict: Fetch the URL and return a dictionary with the response data. Parameters: url (str): The URL to fetch. headers (dict): A dictionary of HTTP headers to send with the request. cookies (dict): A dictionary of cookies to include with the request. Returns: dict: A dictionary containing the response \'status\', \'headers\', and \'body\'. ``` # Input - `url`: A string, the URL to fetch. - `headers`: A dictionary, key-value pairs of headers to include in the request. - `cookies`: A dictionary, key-value pairs of cookies to include in the request. # Output - A dictionary containing: - `status`: An integer, the HTTP status code of the response. - `headers`: A dictionary, the headers received in the response. - `body`: A string, the body of the response decoded using `utf-8`. # Constraints 1. Handle cookies using the `http.cookiejar.CookieJar` class. 2. Follow a maximum of 5 redirects. If more than 5 redirects occur, raise an exception. 3. Handle only HTTP and HTTPS URLs. 4. If the request fails due to any reason (network issues, HTTP error, etc.), catch the exception and return a dictionary with the response \'status\' as `None` and an appropriate error message in the \'body\'. # Example Usage ```python url = \\"http://example.com\\" headers = { \\"User-Agent\\": \\"my-application/1.0\\" } cookies = { \\"sessionid\\": \\"123456789\\" } response = fetch_url(url, headers, cookies) print(response) ``` # Example Output ```json { \\"status\\": 200, \\"headers\\": { \\"Content-Type\\": \\"text/html; charset=UTF-8\\", ... }, \\"body\\": \\"<!DOCTYPE html><html>...</html>\\" } ``` # Notes - Use the `urllib.request` module to implement this functionality. - Ensure that exceptions are handled gracefully and informative messages are returned. - Make sure to manage HTTP cookies and follow redirects correctly. - Custom headers should be included in the request.","solution":"import urllib.request import urllib.error import http.cookiejar def fetch_url(url: str, headers: dict, cookies: dict) -> dict: Fetch the URL and return a dictionary with the response data. Parameters: url (str): The URL to fetch. headers (dict): A dictionary of HTTP headers to send with the request. cookies (dict): A dictionary of cookies to include with the request. Returns: dict: A dictionary containing the response \'status\', \'headers\', and \'body\'. cj = http.cookiejar.CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) # Set custom headers request = urllib.request.Request(url, headers=headers) # Add cookies to the request for cookie_name, cookie_value in cookies.items(): cj.set_cookie(http.cookiejar.Cookie( version=0, name=cookie_name, value=cookie_value, port=None, port_specified=False, domain=\'\', domain_specified=False, domain_initial_dot=False, path=\'/\', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={\'HttpOnly\': None}, rfc2109=False )) try: response = opener.open(request, timeout=10) body = response.read().decode(\'utf-8\') response_headers = dict(response.info()) status = response.getcode() return { \\"status\\": status, \\"headers\\": response_headers, \\"body\\": body } except urllib.error.HTTPError as e: return { \\"status\\": e.code, \\"headers\\": dict(e.headers), \\"body\\": str(e) } except urllib.error.URLError as e: return { \\"status\\": None, \\"headers\\": {}, \\"body\\": str(e) } except Exception as e: return { \\"status\\": None, \\"headers\\": {}, \\"body\\": str(e) }"},{"question":"# Advanced Python Programming: Asyncio Task Management You are an engineer at a company that processes large sets of data asynchronously. You are required to process different datasets concurrently while ensuring that: 1. Each dataset is processed using a coroutine. 2. The processing should follow a hierarchy, meaning a certain dataset should be processed only after another dataset completes. 3. You need to handle scenarios where processing might get timed out or a task might require protection from cancellation. Your task is to implement a class `DatasetProcessor` with the following functionalities: Class: DatasetProcessor # Methods: 1. **`__init__(self, datasets: List[str], timeout: float):`** - Initializes the `datasets` (a list of dataset names) and a `timeout` value for processing each dataset. 2. **`async def process_dataset(self, dataset: str) -> str:`** - Simulate processing a dataset, where processing involves sleeping for a random time between 1 to 5 seconds. If the processing time exceeds the timeout, a `TimeoutError` should be raised, and the processing should stop. 3. **`async def run(self) -> Dict[str, Union[str, Exception]]:`** - Start processing all datasets concurrently. Ensure that: - Datasets are processed in the given order. - If any dataset results in a `TimeoutError`, add `\\"Timeout\\"` as the result for that dataset. - Return a dictionary with dataset names as keys and either the result or the exception as the value. Constraints: - Use the `asyncio` module for managing coroutines and tasks. - Each dataset may only be processed after the previous dataset completes. - Handle the shutdown gracefully, ensuring any running task does not get abruptly cancelled. Here is a template to get you started: ```python import asyncio import random from typing import List, Dict, Union class DatasetProcessor: def __init__(self, datasets: List[str], timeout: float): self.datasets = datasets self.timeout = timeout async def process_dataset(self, dataset: str) -> str: # Simulate random processing time processing_time = random.randint(1, 5) try: result = await asyncio.wait_for(asyncio.sleep(processing_time), timeout=self.timeout) return f\\"Processed {dataset} in {processing_time} seconds\\" except asyncio.TimeoutError: return \\"Timeout\\" async def run(self) -> Dict[str, Union[str, Exception]]: results = {} for dataset in self.datasets: try: result = await self.process_dataset(dataset) results[dataset] = result except Exception as e: results[dataset] = e return results # Example usage: datasets = [\\"dataset1\\", \\"dataset2\\", \\"dataset3\\"] processor = DatasetProcessor(datasets, 3.0) results = asyncio.run(processor.run()) print(results) ``` Explanation: - `process_dataset` method simulates processing of a dataset. - `run` method ensures each dataset is processed in the given order, handles timeouts, and returns results. Make sure to thoroughly test your implementation with different timeouts and dataset configurations to cover edge cases.","solution":"import asyncio import random from typing import List, Dict, Union class DatasetProcessor: def __init__(self, datasets: List[str], timeout: float): self.datasets = datasets self.timeout = timeout async def process_dataset(self, dataset: str) -> str: # Simulate random processing time processing_time = random.randint(1, 5) try: await asyncio.wait_for(asyncio.sleep(processing_time), timeout=self.timeout) return f\\"Processed {dataset} in {processing_time} seconds\\" except asyncio.TimeoutError: return \\"Timeout\\" async def run(self) -> Dict[str, Union[str, Exception]]: results = {} for dataset in self.datasets: try: result = await self.process_dataset(dataset) results[dataset] = result except Exception as e: results[dataset] = e return results # Example usage: datasets = [\\"dataset1\\", \\"dataset2\\", \\"dataset3\\"] processor = DatasetProcessor(datasets, 3.0) results = asyncio.run(processor.run()) print(results)"},{"question":"# Coding Exercise: Preprocessing Labels for Supervised Learning Objective Your task is to implement a function that preprocesses the labels of a given dataset using the LabelBinarizer, MultiLabelBinarizer, and LabelEncoder from the `sklearn.preprocessing` module. You will then use the preprocessed labels in a simple supervised learning task using Logistic Regression. Instructions 1. **LabelBinarizer Transformation** - Fit a `LabelBinarizer` to the provided list of integer labels. - Transform the list of labels using the `LabelBinarizer`. 2. **MultiLabelBinarizer Transformation** - Fit a `MultiLabelBinarizer` to a provided list of sets of integer labels. - Transform the list of sets using the `MultiLabelBinarizer`. 3. **LabelEncoder Transformation** - Fit a `LabelEncoder` to the provided list of string labels. - Transform the list of labels using the `LabelEncoder`. 4. **Supervised Learning Task** - Create and fit a `LogisticRegression` model from `sklearn.linear_model` using the original integer labels and a provided feature matrix. - Create and fit a separate `LogisticRegression` model using the transformed labels from the `LabelBinarizer`. - Print the accuracy of both models on a provided test set. Function Signature ```python import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder from sklearn.linear_model import LogisticRegression def preprocess_labels_and_fit_model( int_labels: list, multi_labels: list, string_labels: list, features: np.ndarray, test_features: np.ndarray, test_labels: np.ndarray ): Preprocesses labels using LabelBinarizer, MultiLabelBinarizer, and LabelEncoder, then fits Logistic Regression models on the original and transformed labels. Parameters: - int_labels (list): List of integer labels for LabelBinarizer. - multi_labels (list): List of sets of integer labels for MultiLabelBinarizer. - string_labels (list): List of string labels for LabelEncoder. - features (np.ndarray): Feature matrix for training the Logistic Regression models. - test_features (np.ndarray): Feature matrix for testing the Logistic Regression models. - test_labels (np.ndarray): True integer labels for the test set. Returns: - None. Prints the accuracy of Logistic Regression models. # LabelBinarizer transformation lb = LabelBinarizer() lb.fit(int_labels) transformed_int_labels = lb.transform(int_labels) # MultiLabelBinarizer transformation mlb = MultiLabelBinarizer() mlb.fit(multi_labels) transformed_multi_labels = mlb.transform(multi_labels) # LabelEncoder transformation le = LabelEncoder() le.fit(string_labels) transformed_string_labels = le.transform(string_labels) # Logistic Regression Model with original integer labels model_original = LogisticRegression() model_original.fit(features, int_labels) accuracy_original = model_original.score(test_features, test_labels) # Logistic Regression Model with LabelBinarizer transformed labels (Note: This step is not typical in practice but serves educational purpose) # Since this implies a binary/multilabel classification, the logic of output labels may vary. # Transform binarized labels back for a fitting purpose. Taking argmax as a simple consideration. model_lb = LogisticRegression() model_lb.fit(features, np.argmax(transformed_int_labels, axis=1)) lb_test_predictions = model_lb.predict(test_features) lb_test_labels = lb.inverse_transform(lb_test_predictions) lb_accuracy = np.mean(lb_test_labels == test_labels) print(f\\"Accuracy with original integer labels: {accuracy_original}\\") print(f\\"Accuracy with LabelBinarizer transformed labels: {lb_accuracy}\\") # Example usage: # int_labels = [1, 2, 6, 4, 2] # multi_labels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] # string_labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] # features = np.random.rand(5, 10) # Example feature matrix # test_features = np.random.rand(2, 10) # Example test feature matrix # test_labels = [1, 6] # Example true test labels # preprocess_labels_and_fit_model(int_labels, multi_labels, string_labels, features, test_features, test_labels) ``` Constraints - Use the `sklearn.preprocessing` module for binarization and encoding. - Use the `LogisticRegression` model from `sklearn.linear_model`. - Assume that the feature matrix is a NumPy array with shape `(n_samples, n_features)`. Performance Requirements - The solution should have a time complexity of O(n), where n is the number of samples, for each preprocessing step. - The Logistic Regression fitting should be efficient and converge within reasonable iterations given the size of data. Notes - This problem will test your understanding of label preprocessing techniques and their application in a supervised learning context. Ensure that your solution handles different types of labels appropriately and integrates them with machine learning models effectively.","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder from sklearn.linear_model import LogisticRegression def preprocess_labels_and_fit_model( int_labels: list, multi_labels: list, string_labels: list, features: np.ndarray, test_features: np.ndarray, test_labels: int ): Preprocesses labels using LabelBinarizer, MultiLabelBinarizer, and LabelEncoder, then fits Logistic Regression models on the original and transformed labels. Parameters: - int_labels (list): List of integer labels for LabelBinarizer. - multi_labels (list): List of sets of integer labels for MultiLabelBinarizer. - string_labels (list): List of string labels for LabelEncoder. - features (np.ndarray): Feature matrix for training the Logistic Regression models. - test_features (np.ndarray): Feature matrix for testing the Logistic Regression models. - test_labels (list): True integer labels for the test set. Returns: - None. Prints the accuracy of Logistic Regression models. # LabelBinarizer transformation lb = LabelBinarizer() lb.fit(int_labels) transformed_int_labels = lb.transform(int_labels) # MultiLabelBinarizer transformation mlb = MultiLabelBinarizer() mlb.fit(multi_labels) transformed_multi_labels = mlb.transform(multi_labels) # LabelEncoder transformation le = LabelEncoder() le.fit(string_labels) transformed_string_labels = le.transform(string_labels) # Logistic Regression Model with original integer labels model_original = LogisticRegression(max_iter=500) model_original.fit(features, int_labels) accuracy_original = model_original.score(test_features, test_labels) # Logistic Regression Model with LabelBinarizer transformed labels # Since this typically implies a multi-class classification problem, # we directly use transformed labels which are binarized for fitting. model_lb = LogisticRegression(max_iter=500) model_lb.fit(features, transformed_int_labels.argmax(axis=1)) lb_accuracy = model_lb.score(test_features, test_labels) print(f\\"Accuracy with original integer labels: {accuracy_original}\\") print(f\\"Accuracy with LabelBinarizer transformed labels: {lb_accuracy}\\")"},{"question":"**Objective:** Evaluate the student\'s ability to utilize seaborn to create and apply color palettes in data visualization. **Question:** Implement a function `create_custom_palette_and_plot` that generates a seaborn color palette based on the given parameters and uses it to create a scatter plot. The function should take the following inputs: - `palette_type` (str): Type of the palette (e.g., \\"pastel\\", \\"husl\\", \\"Set2\\", \\"Spectral\\", etc.) - `palette_params` (dict): Additional parameters for the palette (e.g., number of hues, `as_cmap=True`, etc.) - `x` (list of int): X-coordinates of the points. - `y` (list of int): Y-coordinates of the points. - `hue` (list of str): Values to use for coloring the points. The function should output: - A scatter plot using seaborn\'s `relplot`. **Constraints:** - You should use `sns.color_palette` to create the color palette. - Ensure the plot uses the specified palette correctly. - Handle exceptions where the palette type or parameters might be invalid. **Example:** ```python def create_custom_palette_and_plot(palette_type, palette_params, x, y, hue): import seaborn as sns import matplotlib.pyplot as plt try: palette = sns.color_palette(palette_type, **palette_params) with sns.color_palette(palette): sns.relplot(x=x, y=y, hue=hue, s=100, legend=False) plt.show() except ValueError as e: print(f\\"Error: {e}\\") # Example usage x = list(range(10)) y = [2]*10 hue = list(map(str, x)) create_custom_palette_and_plot(\\"husl\\", {\\"n_colors\\": 9}, x, y, hue) ``` Expected Output: A scatter plot with points from coordinates `x`, `y`, and colored based on `hue` using a \\"husl\\" palette. **Note:** Be sure to test your function with various palette types and parameters to ensure robustness.","solution":"def create_custom_palette_and_plot(palette_type, palette_params, x, y, hue): import seaborn as sns import matplotlib.pyplot as plt try: palette = sns.color_palette(palette_type, **palette_params) with sns.color_palette(palette): sns.relplot(x=x, y=y, hue=hue, s=100, palette=palette, legend=False) plt.show() except ValueError as e: print(f\\"Error: {e}\\") except TypeError as e: print(f\\"Error: {e}\\")"},{"question":"You are tasked with implementing a custom label encoder class that can handle both single-label and multi-label classification tasks. This class should combine functionalities similar to `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` from the sklearn.preprocessing module. # Class Specification Class Name: - `CustomLabelEncoder` Methods: 1. **fit(self, y)** - **Input**: A list of labels `y`, where each label can be a single item or a collection of items (for multilabel data). - **Operation**: Learn the unique classes from the supplied data. 2. **transform(self, y)** - **Input**: A list of labels `y`, where each label can be a single item or a collection of items. - **Output**: Return the transformed labels into a binary matrix if multi-label, or an array of encoded integers if single-label. - **Constraints**: All labels must be from the set of classes learned in the `fit` method. 3. **inverse_transform(self, y)** - **Input**: Transformed labels from the previous step (either a binary matrix or an array of integer encoded labels). - **Output**: Return the original labels corresponding to the encoded form. 4. **classes_** (attribute) - **Output**: Return the list of classes learned during fitting. # Example Usage ```python # Example with single-label classification labels_single = [1, 2, 2, 6] encoder = CustomLabelEncoder() encoder.fit(labels_single) encoded_single = encoder.transform([1, 1, 2, 6]) # encoded_single should output something like array([0, 0, 1, 2]) original_single = encoder.inverse_transform(encoded_single) # original_single should output array([1, 1, 2, 6]) # Example with multi-label classification labels_multi = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] encoder.fit(labels_multi) encoded_multi = encoder.transform([[2, 3], [0, 1, 4]]) # encoded_multi should output something like array([[0, 0, 1, 1, 0], # [1, 1, 0, 0, 1]]) original_multi = encoder.inverse_transform(encoded_multi) # original_multi should output [[2, 3], [0, 1, 4]] ``` # Constraints - The input `y` for `fit` and `transform` methods should be either a list of single items or a list with sublists containing multiple items (for multilabel). - Add appropriate error handling for cases where `transform` is called before `fit` or with unknown labels. Make sure your implementation handles both the single-label and multi-label cases appropriately. You are not allowed to directly use the `LabelBinarizer`, `MultiLabelBinarizer`, or `LabelEncoder` classes from sklearn in your solution.","solution":"import numpy as np from collections import defaultdict class CustomLabelEncoder: def __init__(self): self.class_to_index = {} self.index_to_class = {} self.classes_ = [] def fit(self, y): unique_classes = set() if not y: raise ValueError(\\"Input labels are empty.\\") for label in y: if isinstance(label, (list, tuple)): unique_classes.update(label) else: unique_classes.add(label) self.classes_ = sorted(unique_classes) self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes_)} self.index_to_class = {idx: cls for idx, cls in enumerate(self.classes_)} def _encode_single_label(self, label): return self.class_to_index[label] def _decode_single_label(self, index): return self.index_to_class[index] def transform(self, y): if not self.classes_: raise ValueError(\\"The fit method must be called before transform.\\") if isinstance(y[0], (list, tuple)): binary_matrix = np.zeros((len(y), len(self.classes_)), dtype=int) for i, label_set in enumerate(y): for label in label_set: binary_matrix[i, self._encode_single_label(label)] = 1 return binary_matrix else: encoded_labels = [self._encode_single_label(label) for label in y] return np.array(encoded_labels) def inverse_transform(self, y): if isinstance(y[0], (list, np.ndarray)): original_labels = [] for row in y: labels = [self._decode_single_label(idx) for idx in range(len(self.classes_)) if row[idx] == 1] original_labels.append(labels) return original_labels else: original_labels = [self._decode_single_label(index) for index in y] return original_labels"},{"question":"Objective Implement a function that takes several parameters describing a Python package and generates the appropriate setup script call using the `distutils.core.setup()` function. Description Create a function `generate_setup_script` that assembles the necessary arguments and calls the `distutils.core.setup()` function. The function should be able to handle a variety of input parameters for the package metadata and configuration. Requirements 1. The function should take in the following parameters: - `name` (str): Name of the package. - `version` (str): Version of the package. - `description` (str): Short description of the package. - `long_description` (str): Long description of the package. - `author` (str): Name of the package author. - `author_email` (str): Email address of the package author. - `url` (str): URL for the package (homepage). - `packages` (list of str): A list of Python packages included in the distribution. - `scripts` (list of str): A list of standalone script files to be built and installed. - `classifiers` (list of str): A list of categories for the package. - `keywords` (list of str): A list of keywords to describe the package. - `install_requires` (list of str): A list of dependencies required by the package. 2. The function should generate a call to `distutils.core.setup()` with the provided parameters. 3. The function should be named `generate_setup_script`. 4. You do not need to handle all possible parameters of `setup()`, but the function should be able to handle at least the ones listed in the requirements. # Example ```python def generate_setup_script(name, version, description, long_description, author, author_email, url, packages, scripts, classifiers, keywords, install_requires): from distutils.core import setup setup( name=name, version=version, description=description, long_description=long_description, author=author, author_email=author_email, url=url, packages=packages, scripts=scripts, classifiers=classifiers, keywords=keywords, install_requires=install_requires, ) # Sample call generate_setup_script( name=\\"mypackage\\", version=\\"1.0\\", description=\\"This is a sample package\\", long_description=\\"This package provides an example of a distutils setup script.\\", author=\\"John Doe\\", author_email=\\"john.doe@example.com\\", url=\\"https://example.com/mypackage\\", packages=[\\"mypackage\\"], scripts=[\\"scripts/myscript.py\\"], classifiers=[\\"Programming Language :: Python :: 3\\", \\"License :: OSI Approved :: MIT License\\"], keywords=[\\"example\\", \\"setup\\", \\"distutils\\"], install_requires=[\\"requests\\", \\"numpy\\"] ) ``` # Constraints - You can assume that all string inputs are valid and properly formatted. - List inputs for packages, scripts, classifiers, keywords, and install_requires will be non-empty lists of valid strings. - You do not need to handle nested parameters or more complex structures beyond the provided lists and strings.","solution":"def generate_setup_script(name, version, description, long_description, author, author_email, url, packages, scripts, classifiers, keywords, install_requires): from distutils.core import setup setup( name=name, version=version, description=description, long_description=long_description, author=author, author_email=author_email, url=url, packages=packages, scripts=scripts, classifiers=classifiers, keywords=keywords, install_requires=install_requires, )"},{"question":"Coding Assessment Question # Objective The objective of this question is to assess your understanding of pandas functionalities. You will be required to create, manipulate, and perform operations on pandas Series and DataFrame objects. # Problem Statement You are provided with data regarding the sales of different products in a store. You need to perform various operations to analyze this data. # Data You have the following data: 1. **Product Sales Data**: - A dictionary containing the sales figures of 5 products over a week: ```python sales_data = { \'ProductA\': [150, 160, 170, 180, 190], \'ProductB\': [210, 220, 230, 240, 250], \'ProductC\': [300, 310, 320, 330, 340], \'ProductD\': [400, 410, 420, 430, 440], \'ProductE\': [500, 510, 520, 530, 540] } ``` 2. **Week Days**: - A list representing the days of the week: ```python week_days = [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\'] ``` # Tasks 1. **DataFrame Creation**: - Create a pandas DataFrame from the `sales_data` dictionary. The rows should be indexed by `week_days`. 2. **Data Analysis**: - Calculate the total sales for each product for the week. - Calculate the average sales per day across all products. - Identify the day with the highest total sales. - Add a new column to the DataFrame with the percentage increase in sales of each product compared to the previous day. 3. **Operations and Calculations**: - Create a pandas Series called `total_sales_by_day` which contains the total sales for each day. - Subtract the minimum sales value of `ProductC` from the total sales of each product. 4. **Handle Missing Data**: - Assume `ProductB` did not have sales data recorded for \'Thursday\'. Update the DataFrame accordingly and fill the missing value with the mean sales of `ProductB`. # Expected Output - A DataFrame with the total and average sales information. - Statistical analysis results for each product and for overall days. - Handling missing values correctly. # Function Definitions Implement the following functions: ```python import pandas as pd def create_data_frame(sales_data, week_days): Create a pandas DataFrame from sales_data and week_days. Parameters: sales_data (dict): Dictionary containing sales data for products. week_days (list): List containing the days of the week. Returns: DataFrame: pandas DataFrame with sales data, indexed by week_days. # Your code here def analyze_sales(dataframe): Perform analysis on the sales DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: tuple: A tuple containing: - Series: Total sales for each product. - Series: Average sales per day across all products. - str: The day with the highest total sales. - DataFrame: The original DataFrame with an additional column for percentage increase in sales. # Your code here def operations_and_calculations(dataframe): Perform operations and calculations on the sales DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: tuple: A tuple containing: - Series: Total sales by day. - DataFrame: Updated DataFrame after subtracting minimum sales of ProductC from all products. # Your code here def handle_missing_data(dataframe): Handle missing data in the DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: DataFrame: Updated DataFrame with missing data handled. # Your code here ``` # Example Usage ```python sales_data = { \'ProductA\': [150, 160, 170, 180, 190], \'ProductB\': [210, 220, 230, 240, 250], \'ProductC\': [300, 310, 320, 330, 340], \'ProductD\': [400, 410, 420, 430, 440], \'ProductE\': [500, 510, 520, 530, 540] } week_days = [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\'] # Step 1: Create DataFrame df = create_data_frame(sales_data, week_days) # Step 2: Analyze Sales total_sales, avg_sales_per_day, highest_sales_day, df_with_percentage = analyze_sales(df) # Step 3: Operations and Calculations total_sales_by_day, df_updated = operations_and_calculations(df) # Step 4: Handle Missing Data df_with_missing_data = handle_missing_data(df) ``` # Constraints - You may assume the input data is always correctly formatted. - Use pandas functionalities wherever possible for efficiency.","solution":"import pandas as pd def create_data_frame(sales_data, week_days): Create a pandas DataFrame from sales_data and week_days. Parameters: sales_data (dict): Dictionary containing sales data for products. week_days (list): List containing the days of the week. Returns: DataFrame: pandas DataFrame with sales data, indexed by week_days. df = pd.DataFrame(sales_data, index=week_days) return df def analyze_sales(dataframe): Perform analysis on the sales DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: tuple: A tuple containing: - Series: Total sales for each product. - Series: Average sales per day across all products. - str: The day with the highest total sales. - DataFrame: The original DataFrame with an additional column for percentage increase in sales. total_sales = dataframe.sum() avg_sales_per_day = dataframe.mean(axis=1) highest_sales_day = avg_sales_per_day.idxmax() percentage_increase = dataframe.pct_change() df_with_percentage = dataframe.copy() df_with_percentage[\'Percentage Increase\'] = percentage_increase.mean(axis=1).fillna(0) * 100 return total_sales, avg_sales_per_day, highest_sales_day, df_with_percentage def operations_and_calculations(dataframe): Perform operations and calculations on the sales DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: tuple: A tuple containing: - Series: Total sales by day. - DataFrame: Updated DataFrame after subtracting minimum sales of ProductC from all products. total_sales_by_day = dataframe.sum(axis=1) min_sales_productC = dataframe[\'ProductC\'].min() df_updated = dataframe - min_sales_productC return total_sales_by_day, df_updated def handle_missing_data(dataframe): Handle missing data in the DataFrame. Parameters: dataframe (DataFrame): The pandas DataFrame containing sales data. Returns: DataFrame: Updated DataFrame with missing data handled. adjacency_dataframe = dataframe.copy() adjacency_dataframe.at[\'Thursday\', \'ProductB\'] = None mean_productB_sales = adjacency_dataframe[\'ProductB\'].mean() adjacency_dataframe[\'ProductB\'] = adjacency_dataframe[\'ProductB\'].fillna(mean_productB_sales) return adjacency_dataframe"},{"question":"# Seaborn Theme Customization and Bar Plot Creation **Objective:** Demonstrate your understanding of seaborn\'s theme customization capabilities by creating a bar plot with specific customizations. **Problem Statement:** Write a function `create_custom_barplot` that takes in three parameters: 1. `x_data`: A list of category names (strings) for the x-axis. 2. `y_data`: A list of numerical values corresponding to `x_data` for the y-axis. 3. `hue_order`: A list of custom theme parameters (dictionary) that you will use to customize the seaborn plot. The function should: 1. Set a seaborn theme with the provided custom parameters. 2. Create a bar plot using the provided `x_data` and `y_data`. 3. Customize the bar plot to have: - A white grid style for the background. - A pastel color palette. 4. Remove the top and right spines from the plot. 5. Display the plot. **Input:** - `x_data`: List of strings, example: `[\\"Category A\\", \\"Category B\\", \\"Category C\\"]` - `y_data`: List of integers/floats, example: `[10, 20, 15]` - `hue_order`: Dictionary of custom theme parameters, example: `{\\"axes.spines.right\\": False, \\"axes.spines.top\\": False}` **Output:** - The function should display a customized bar plot. **Constraints:** - Assume `x_data` and `y_data` are always of the same length and non-empty. - The function should handle cases where `hue_order` might be empty. **Example Usage:** ```python x_data = [\\"A\\", \\"B\\", \\"C\\"] y_data = [1, 3, 2] hue_order = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} create_custom_barplot(x_data, y_data, hue_order) ``` This should produce a bar plot with a white grid background, pastel color palette, and the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_barplot(x_data, y_data, hue_order): Creates and displays a customized bar plot using seaborn. Parameters: x_data (list): A list of category names for the x-axis. y_data (list): A list of numerical values corresponding to x_data for the y-axis. hue_order (dict): A dictionary of custom theme parameters for seaborn. # Set seaborn theme with custom parameters sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\", rc=hue_order) # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=x_data, y=y_data) # Customize the plot\'s spines sns.despine() # Display the plot plt.show()"},{"question":"Objective The main goal of this exercise is to demonstrate your understanding of loading, manipulating, and performing basic machine learning tasks using scikit-learn\'s toy datasets. Problem Statement Using the provided Scikit-learn toy datasets, create a model to classify the **wine** dataset using any classification algorithm of your choice from scikit-learn. Instructions 1. **Loading the Dataset** - Load the wine dataset using the `load_wine` function from `sklearn.datasets`. 2. **Data Preprocessing** - Split the data into training and testing sets. Use 70% for training and 30% for testing. 3. **Model Training** - Choose an appropriate classification algorithm from scikit-learn and train your model using the training set. 4. **Model Evaluation** - Evaluate the performance of your model using appropriate metrics and the testing set. 5. **Performance Requirements** - Your solution should calculate and print the following metrics: - Accuracy - Precision - Recall - F1 Score Restrictions - You are required to use scikit-learn for all tasks. - Do not use external libraries other than scikit-learn for the preprocessing or model training tasks. Expected Input and Output - **Input**: There is no direct input to the function except the data loaded using `load_wine`. - **Output**: - Accuracy of the model - Precision of the model - Recall of the model - F1 Score of the model Example ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the dataset data = load_wine() X = data.data y = data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Preprocess the data (Standardization) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the model model = LogisticRegression() model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate and print the model performance accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") ``` This is just a sample to guide you on the structure. Ensure you understand and implement the relevant code to achieve the task.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def classify_wine(): # Load the dataset data = load_wine() X = data.data y = data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Preprocess the data (Standardization) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the model model = RandomForestClassifier(random_state=0) model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate and print the model performance accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') return accuracy, precision, recall, f1"},{"question":"**Coding Assessment Question** **Title:** Efficient Graph Adjacency Matrix Operations with Sparse Tensors **Objective:** Your task is to work with a graph adjacency matrix stored as a dense tensor. You will convert this dense tensor into various sparse tensor formats, perform specific operations, and convert the results back to dense format for validation. **Problem Statement:** 1. Given a dense adjacency matrix `A` of a directed graph, convert it into COO and CSR sparse tensor formats. 2. Perform matrix-vector multiplication on both the COO and CSR representations with a given vector `v`. 3. Convert the results back to dense format. 4. Verify that the outputs from both formats are equivalent to the result of directly multiplying the dense matrix `A` with the vector `v`. **Function Signature:** ```python import torch def sparse_tensor_operations(A: torch.Tensor, v: torch.Tensor) -> bool: Given a dense adjacency matrix A and a vector v, convert A to COO and CSR formats, perform matrix-vector multiplication, and verify the results. Parameters: - A (torch.Tensor): Dense adjacency matrix of shape (N, N) - v (torch.Tensor): Vector of shape (N,) Returns: - bool: True if results from sparse formats are equivalent to the dense multiplication, False otherwise. pass ``` **Input Format:** - `A`: A 2D dense tensor of shape (N, N) representing the adjacency matrix. - `v`: A 1D dense tensor of shape (N,) representing the vector to be multiplied. **Output Format:** - Return a boolean value indicating whether the output from sparse tensor operations matches the output from dense tensor multiplication. **Constraints:** - `N` ≥ 1 **Example:** ```python A = torch.tensor([[0., 2.], [3., 0.]]) v = torch.tensor([1., 0.]) print(sparse_tensor_operations(A, v)) # Expected: True ``` **Instructions:** 1. Convert the dense tensor `A` to COO and CSR formats. 2. Perform the matrix-vector multiplication for both COO and CSR formatted tensors. 3. Convert the results back to dense format. 4. Compare the results with the multiplication result of the dense tensor `A` and vector `v`. 5. Ensure to handle any edge cases and ensure the formats preserve the properties of the input tensors. **Evaluation Criteria:** - Correctness of sparse tensor conversions. - Accuracy of matrix-vector multiplication using sparse formats. - Verification against the dense tensor multiplication result. - Handling of different sizes and values within the matrices and vectors.","solution":"import torch def sparse_tensor_operations(A: torch.Tensor, v: torch.Tensor) -> bool: Given a dense adjacency matrix A and a vector v, convert A to COO and CSR formats, perform matrix-vector multiplication, and verify the results. Parameters: - A (torch.Tensor): Dense adjacency matrix of shape (N, N) - v (torch.Tensor): Vector of shape (N,) Returns: - bool: True if results from sparse formats are equivalent to the dense multiplication, False otherwise. # Convert A to COO format A_coo = A.to_sparse_coo() # Convert A to CSR format A_csr = A.to_sparse_csr() # Perform matrix-vector multiplication using COO format result_coo = torch.sparse.mm(A_coo, v.unsqueeze(1)).squeeze(1) # Perform matrix-vector multiplication using CSR format result_csr = torch.sparse.mm(A_csr, v.unsqueeze(1)).squeeze(1) # Perform matrix-vector multiplication using dense format result_dense = A @ v # Verify if the results from COO and CSR formats match the dense result is_coo_correct = torch.allclose(result_dense, result_coo) is_csr_correct = torch.allclose(result_dense, result_csr) # Return True if both COO and CSR results are correct return is_coo_correct and is_csr_correct"},{"question":"# Question: Utilizing PyTorch Backends for Optimized Computation **Objective:** Write a Python function using PyTorch that toggles specific backend features and checks the availability of certain backend capabilities to ensure that the environment is set for optimized computation using CUDA and cuDNN. **Function Signature:** ```python import torch def optimize_and_check_backends(): This function should: 1. Enable TensorFloat-32 (TF32) usage on CUDA for matrix multiplications. 2. Enable reduced precision reduction for FP16 and BF16 GEMMs on CUDA. 3. Enable cuDNN benchmark mode to select the fastest convolution algorithm. 4. Check if the essential backends (CUDA, cuDNN) are available. Returns: dict: A dictionary containing the availability status of CUDA and cuDNN. Example format: {\'cuda_available\': True, \'cudnn_available\': True} pass ``` **Requirements:** 1. **Enable TF32 Usage on CUDA:** - `torch.backends.cuda.matmul.allow_tf32 = True` 2. **Enable Reduced Precision Reduction for FP16 and BF16 GEMMs on CUDA:** - `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True` - `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True` 3. **Enable cuDNN Benchmark Mode:** - `torch.backends.cudnn.benchmark = True` 4. **Check and Return Availability of CUDA and cuDNN:** - Use `torch.backends.cuda.is_built()` to check if CUDA is built. - Use `torch.backends.cudnn.is_available()` to check if cuDNN is available. **Example Usage:** ```python result = optimize_and_check_backends() print(result) # Output might be: {\'cuda_available\': True, \'cudnn_available\': True} ``` **Constraints:** - The function should only modify the backend settings and query their status. No additional computation or data manipulation is required. **Performance Requirements:** - The function should execute in a reasonable time frame given the operations are primarily setting attributes and querying available backends. **Note:** - Ensure that you have an environment with CUDA and cuDNN installed for testing this function; otherwise, the availability checks will return `False`.","solution":"import torch def optimize_and_check_backends(): This function should: 1. Enable TensorFloat-32 (TF32) usage on CUDA for matrix multiplications. 2. Enable reduced precision reduction for FP16 and BF16 GEMMs on CUDA. 3. Enable cuDNN benchmark mode to select the fastest convolution algorithm. 4. Check if the essential backends (CUDA, cuDNN) are available. Returns: dict: A dictionary containing the availability status of CUDA and cuDNN. Example format: {\'cuda_available\': True, \'cudnn_available\': True} # Enable TF32 usage on CUDA for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Enable reduced precision reduction for FP16 and BF16 GEMMs on CUDA torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True # Enable cuDNN benchmark mode torch.backends.cudnn.benchmark = True # Check if the essential backends (CUDA, cuDNN) are available cuda_available = torch.backends.cuda.is_built() cudnn_available = torch.backends.cudnn.is_available() return {\'cuda_available\': cuda_available, \'cudnn_available\': cudnn_available}"},{"question":"# PyTorch GPU Tuning Assessment As a software engineer at a company developing high-performance machine learning algorithms, you have been tasked with optimizing the GPU performance of certain operations using PyTorch\'s `torch.cuda.tunable` module. Your teammate has started the process but left the task unfinished. Your job is to complete the implementation. Objective Write a function `configure_and_tune_gemm` that performs the following steps: 1. Enable the GPU tuning feature. 2. Set the maximum tuning duration to 5000 milliseconds and the maximum tuning iterations to 100. 3. Ensure that untuned operations are recorded. 4. Perform tuning for a GEMM operation by reading from and writing to a specified file. 5. Retrieve and return the tuning results. Function Signature ```python def configure_and_tune_gemm(filename: str) -> dict: Configure the GPU tuning parameters and perform GEMM tuning operations. Args: - filename (str): The path to the file for reading/writing GEMM tuning configurations and results. Returns: - dict: The tuning results. ``` Input - `filename` (str): A string representing the path to the file to be used for GEMM tuning. Output - A dictionary containing the tuning results. Constraints - You can assume the specified file exists and has necessary read/write permissions. - Handle all intermediate steps to ensure tuning operations are successful. Example Suppose the file \\"gemm_tuning.config\\" is used as follows: ```python results = configure_and_tune_gemm(\\"gemm_tuning.config\\") print(results) ``` This should activate tuning, configure the parameters, perform the tuning, and output the tuning results as a dictionary. # Solution Template Here is a template to get you started: ```python import torch.cuda.tunable as tunable def configure_and_tune_gemm(filename: str) -> dict: # Enable tuning tunable.enable(True) # Set maximum tuning duration and iterations tunable.set_max_tuning_duration(5000) tunable.set_max_tuning_iterations(100) # Record untuned operations tunable.record_untuned_enable(True) # Perform GEMM tuning tunable.tune_gemm_in_file(filename) # Retrieve and return tuning results results = tunable.get_results() return results ``` Use the function signature and example provided to implement the required functionality successfully. Ensure you manage the GPU tuning parameters and operations correctly.","solution":"import torch.cuda.tunable as tunable def configure_and_tune_gemm(filename: str) -> dict: Configure the GPU tuning parameters and perform GEMM tuning operations. Args: - filename (str): The path to the file for reading/writing GEMM tuning configurations and results. Returns: - dict: The tuning results. # Enable tuning tunable.enable(True) # Set maximum tuning duration and iterations tunable.set_max_tuning_duration(5000) tunable.set_max_tuning_iterations(100) # Record untuned operations tunable.record_untuned_enable(True) # Perform GEMM tuning by reading from and writing to the specified file tunable.tune_gemm_in_file(filename) # Retrieve and return tuning results results = tunable.get_results() return results"},{"question":"Problem Statement You are given a list of students with their scores in multiple subjects. Your task is to perform various operations such as adding new scores, updating existing scores, calculating the average score for each student, and performing some advanced queries and transformations. **Objective:** Implement the following functions: 1. `add_score(data: dict, student: str, subject: str, score: float) -> None` 2. `update_score(data: dict, student: str, subject: str, score: float) -> None` 3. `delete_score(data: dict, student: str, subject: str) -> None` 4. `average_score(data: dict) -> dict` 5. `top_student(data: dict) -> str` 6. `subject_topper(data: dict, subject: str) -> str` 7. `students_with_minimum_score(data: dict, pass_mark: float) -> list` 8. `transpose_scores(data: dict) -> dict` # Detailed Function Descriptions 1. **add_score** - **Input:** `data` (dict): Dictionary containing student data, `student` (str): Name of the student, `subject` (str): Name of the subject, `score` (float): Score of the student in that subject. - **Output:** None - **Description:** Adds a new score for a student in a given subject. If the student doesn\'t exist, create a new entry for the student. 2. **update_score** - **Input:** `data` (dict): Dictionary containing student data, `student` (str): Name of the student, `subject` (str): Name of the subject, `score` (float): New score to update. - **Output:** None - **Description:** Updates the score of a student in a given subject. 3. **delete_score** - **Input:** `data` (dict): Dictionary containing student data, `student` (str): Name of the student, `subject` (str): Name of the subject. - **Output:** None - **Description:** Deletes the score of a student in a given subject. If the subject does not exist for the student, do nothing. 4. **average_score** - **Input:** `data` (dict): Dictionary containing student data. - **Output:** `dict`: Dictionary where keys are student names and values are their average scores. - **Description:** Calculates the average score for each student. 5. **top_student** - **Input:** `data` (dict): Dictionary containing student data. - **Output:** `str`: Name of the student with the highest average score. - **Description:** Returns the name of the student with the highest average score. If multiple students have the highest average, return any one of them. 6. **subject_topper** - **Input:** `data` (dict): Dictionary containing student data, `subject` (str): Name of the subject. - **Output:** `str`: Name of the student with the highest score in the specified subject. - **Description:** Returns the name of the student with the highest score in the specified subject. If multiple students have the highest score, return any one of them. 7. **students_with_minimum_score** - **Input:** `data` (dict): Dictionary containing student data, `pass_mark` (float): Minimum passing score. - **Output:** `list`: List of student names who have any subject below the given pass mark. - **Description:** Returns a list of names of students who have scored below the given pass mark in any subject. 8. **transpose_scores** - **Input:** `data` (dict): Dictionary containing student data. - **Output:** `dict`: Dictionary where keys are subjects and values are dictionaries with student names as keys and their scores as values. - **Description:** Transposes the scores data to switch the role of students and subjects. # Constraints - Use list comprehensions where appropriate. - Use appropriate data structures like tuples and sets where necessary. - The operations should efficiently handle the provided tasks. # Example ```python data = { \\"Alice\\": {\\"Math\\": 90, \\"Science\\": 85}, \\"Bob\\": {\\"Math\\": 70, \\"History\\": 80}, \\"Charlie\\": {\\"Science\\": 100, \\"Math\\": 90} } add_score(data, \\"Alice\\", \\"History\\", 75) update_score(data, \\"Alice\\", \\"Math\\", 95) delete_score(data, \\"Bob\\", \\"History\\") print(average_score(data)) # {\'Alice\': 85.0, \'Bob\': 70.0, \'Charlie\': 95.0} print(top_student(data)) # Charlie (or any student with the highest average) print(subject_topper(data, \\"Math\\")) # Alice or Charlie print(students_with_minimum_score(data, 80)) # [\'Bob\'] print(transpose_scores(data)) # {\'Math\': {\'Alice\': 95, \'Bob\': 70, \'Charlie\': 90}, # \'Science\': {\'Alice\': 85, \'Charlie\': 100}, # \'History\': {\'Alice\': 75}} ``` Implement the functions as described. Ensure proper handling of edge cases and efficient computation.","solution":"def add_score(data, student, subject, score): if student not in data: data[student] = {} data[student][subject] = score def update_score(data, student, subject, score): if student in data and subject in data[student]: data[student][subject] = score def delete_score(data, student, subject): if student in data and subject in data[student]: del data[student][subject] def average_score(data): result = {} for student, scores in data.items(): result[student] = sum(scores.values()) / len(scores) return result def top_student(data): avg_scores = average_score(data) return max(avg_scores, key=avg_scores.get) def subject_topper(data, subject): max_score = -float(\'inf\') topper = None for student, scores in data.items(): if subject in scores and scores[subject] > max_score: max_score = scores[subject] topper = student return topper def students_with_minimum_score(data, pass_mark): result = [] for student, scores in data.items(): if any(score < pass_mark for score in scores.values()): result.append(student) return result def transpose_scores(data): transposed = {} for student, scores in data.items(): for subject, score in scores.items(): if subject not in transposed: transposed[subject] = {} transposed[subject][student] = score return transposed"},{"question":"Objective: Implement a Python script that uses the `argparse` module to create a command-line utility for managing a to-do list. This exercise will test your understanding of the `argparse` module, including its ability to handle positional and optional arguments, sub-commands, and custom help messages. Description: Your script should provide functionality to add, remove, list, and clear to-do items. Each to-do item should have a description and an optional due date. The script should support the following sub-commands: - `add`: Adds a new to-do item. - `remove`: Removes an existing to-do item by its index. - `list`: Lists all to-do items. - `clear`: Clears all to-do items. Requirements: 1. **Adding a To-Do Item**: - Command: `add` - Arguments: - `description`: A string representing the to-do description (positional argument). - `--due`: An optional string representing the due date in the format `YYYY-MM-DD`. 2. **Removing a To-Do Item**: - Command: `remove` - Arguments: - `index`: An integer representing the index of the item to remove (positional argument). 3. **Listing To-Do Items**: - Command: `list` - Arguments: None. 4. **Clearing All To-Do Items**: - Command: `clear` - Arguments: None. 5. **General**: - The script should support a global `--help` flag to display help information for all sub-commands. Function Specifications: You should implement the following functions to handle the sub-commands: 1. `add_todo_item(description: str, due: Optional[str])` -> None 2. `remove_todo_item(index: int)` -> None 3. `list_todo_items()` -> None 4. `clear_todo_items()` -> None These functions should manipulate a global list called `todo_list` to store the to-do items. Example Usage: ```sh python todo.py add \\"Buy groceries\\" --due 2023-12-01 python todo.py add \\"Prepare for meeting\\" python todo.py list 1. Buy groceries (due: 2023-12-01) 2. Prepare for meeting python todo.py remove 1 python todo.py list 1. Prepare for meeting python todo.py clear python todo.py list No to-do items found. ``` Constraints: - You can assume the input to be valid. - You should handle cases where the index for removal is out of bounds by displaying an appropriate message. - The script should handle the absence of optional arguments gracefully. Performance Requirements: - The to-do list operations should handle a reasonable number of items within a limit of 1000 items efficiently. Implement the script below: ```python import argparse from typing import Optional todo_list = [] def add_todo_item(description: str, due: Optional[str]) -> None: # Implement your code here pass def remove_todo_item(index: int) -> None: # Implement your code here pass def list_todo_items() -> None: # Implement your code here pass def clear_todo_items() -> None: # Implement your code here pass if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Manage your to-do list.\\") subparsers = parser.add_subparsers(dest=\\"command\\", required=True) add_parser = subparsers.add_parser(\\"add\\", help=\\"Add a new to-do item.\\") add_parser.add_argument(\\"description\\", type=str, help=\\"Description of the to-do item.\\") add_parser.add_argument(\\"--due\\", type=str, help=\\"Due date for the to-do item (YYYY-MM-DD).\\") remove_parser = subparsers.add_parser(\\"remove\\", help=\\"Remove a to-do item by index.\\") remove_parser.add_argument(\\"index\\", type=int, help=\\"Index of the to-do item to remove.\\") list_parser = subparsers.add_parser(\\"list\\", help=\\"List all to-do items.\\") clear_parser = subparsers.add_parser(\\"clear\\", help=\\"Clear all to-do items.\\") args = parser.parse_args() if args.command == \\"add\\": add_todo_item(args.description, args.due) elif args.command == \\"remove\\": remove_todo_item(args.index) elif args.command == \\"list\\": list_todo_items() elif args.command == \\"clear\\": clear_todo_items() ``` You should fill in the functions with the appropriate logic to manage the `todo_list`.","solution":"import argparse from typing import Optional todo_list = [] def add_todo_item(description: str, due: Optional[str]) -> None: item = {\\"description\\": description, \\"due\\": due} todo_list.append(item) print(f\'Added to-do item: \\"{description}\\" with due date: {due}\') def remove_todo_item(index: int) -> None: if 0 <= index < len(todo_list): removed_item = todo_list.pop(index) print(f\'Removed to-do item: \\"{removed_item[\\"description\\"]}\\"\') else: print(f\'Error: Index {index} is out of bounds.\') def list_todo_items() -> None: if not todo_list: print(\\"No to-do items found.\\") else: for i, item in enumerate(todo_list, start=1): due = item[\\"due\\"] or \\"no due date\\" print(f\'{i}. {item[\\"description\\"]} (due: {due})\') def clear_todo_items() -> None: todo_list.clear() print(\\"All to-do items have been cleared.\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Manage your to-do list.\\") subparsers = parser.add_subparsers(dest=\\"command\\", required=True) add_parser = subparsers.add_parser(\\"add\\", help=\\"Add a new to-do item.\\") add_parser.add_argument(\\"description\\", type=str, help=\\"Description of the to-do item.\\") add_parser.add_argument(\\"--due\\", type=str, help=\\"Due date for the to-do item (YYYY-MM-DD).\\") remove_parser = subparsers.add_parser(\\"remove\\", help=\\"Remove a to-do item by index.\\") remove_parser.add_argument(\\"index\\", type=int, help=\\"Index of the to-do item to remove.\\") list_parser = subparsers.add_parser(\\"list\\", help=\\"List all to-do items.\\") clear_parser = subparsers.add_parser(\\"clear\\", help=\\"Clear all to-do items.\\") args = parser.parse_args() if args.command == \\"add\\": add_todo_item(args.description, args.due) elif args.command == \\"remove\\": remove_todo_item(args.index - 1) # Adjust from 1-based to 0-based index elif args.command == \\"list\\": list_todo_items() elif args.command == \\"clear\\": clear_todo_items()"},{"question":"Objective Demonstrate your understanding of PyTorch\'s error propagation in a distributed multiprocessing environment by implementing a distributed computation with appropriate error handling. Problem Statement You are given a task to execute a series of computations in parallel using distributed multiprocessing in PyTorch. However, in such an environment, it is crucial to handle possible errors appropriately using PyTorch\'s error handling utilities. Implement a Python function `parallel_computation_with_error_handling` that performs parallel computations and records any errors that occur during the process using `torch.distributed.elastic.multiprocessing.errors.record`. Requirements 1. You must use the `torch.multiprocessing` module to handle parallel execution. 2. In case of any exception during the processes\' execution, you should use `torch.distributed.elastic.multiprocessing.errors.record` to record the error. 3. Implement custom error handling for the processes using `ChildFailedError`, `ErrorHandler`, and `ProcessFailure`. Input - `tasks`: A list of callables, each representing a task to be executed in parallel. - `args`: A list of argument tuples, each corresponding to arguments for the given tasks. Output - A list of results where each result corresponds to the output of the respective task. If an error occurs for a task, its result should be `None`. Constraints - Each task in `tasks` must be a callable that accepts the corresponding arguments from `args`. Performance Requirements - The function should efficiently handle multiple tasks in parallel. - The error handling mechanism should ensure that the program continues execution even if some tasks fail. Function Signature ```python import torch.multiprocessing as mp from typing import List, Callable, Any def parallel_computation_with_error_handling(tasks: List[Callable], args: List[tuple]) -> List[Any]: pass ``` Example Usage ```python def task1(x): return x * x def task2(x): if x == 0: raise ValueError(\\"Invalid input\\") return 10 / x tasks = [task1, task2, task1] args = [(5,), (0,), (2,)] results = parallel_computation_with_error_handling(tasks, args) print(results) # Should print: [25, None, 4] ``` # Instructions - Define the `parallel_computation_with_error_handling` function to execute the given tasks in parallel. - Use the error propagation and handling mechanisms from `torch.distributed.elastic.multiprocessing.errors` to manage any exceptions that occur during the execution of the tasks. - Ensure that the function returns a list of results where failed tasks have a result of `None`.","solution":"import torch.multiprocessing as mp from torch.distributed.elastic.multiprocessing.errors import record from torch.distributed.elastic.multiprocessing.errors import ErrorHandler, ChildFailedError from typing import List, Callable, Any def worker_function(task_index, task, argument, return_dict): try: result = task(*argument) return_dict[task_index] = result except Exception as e: with record(f\\"Process {task_index} failed\\"): raise e def parallel_computation_with_error_handling(tasks: List[Callable], args: List[tuple]) -> List[Any]: manager = mp.Manager() return_dict = manager.dict() processes = [] for i, (task, arg) in enumerate(zip(tasks, args)): p = mp.Process(target=worker_function, args=(i, task, arg, return_dict)) p.start() processes.append(p) for p in processes: p.join() results = [return_dict.get(i, None) for i in range(len(tasks))] return results"},{"question":"**Objective:** Demonstrate your understanding of tensor operations and interoperability by utilizing PyTorch and DLPack. **Problem Statement:** You are provided with a function that generates a DLPack tensor. Your task is to define two functions: 1. `convert_to_pytorch(dlpack_tensor: \\"dlpack\\") -> \\"torch.Tensor\\"`: This function takes a DLPack tensor as an input and converts it into a PyTorch tensor. 2. `convert_to_dlpack(pytorch_tensor: \\"torch.Tensor\\") -> \\"dlpack\\"`: This function takes a PyTorch tensor as an input and converts it into a DLPack tensor. Additionally, you should perform the following steps in a single function `verify_conversion(dlpack_tensor: \\"dlpack\\") -> bool`: 1. Convert the DLPack tensor to a PyTorch tensor using `convert_to_pytorch`. 2. Perform a simple PyTorch operation (e.g., add a scalar value) on the PyTorch tensor. 3. Convert the modified PyTorch tensor back to a DLPack tensor using `convert_to_dlpack`. 4. Verify that the final DLPack tensor has been correctly updated with the performed operation, returning `True` if the conversion was successful and the operations were correctly executed, otherwise `False`. **Constraints:** - The DLPack format should be compatible with the operations performed. - Ensure efficient memory handling during conversions to avoid unnecessary copies. - PyTorch and DLPack versions should be compatible with one another. **Function Specifications:** 1. `convert_to_pytorch(dlpack_tensor: \\"dlpack\\") -> \\"torch.Tensor\\"` - **Input:** A DLPack tensor. - **Output:** A PyTorch tensor. 2. `convert_to_dlpack(pytorch_tensor: \\"torch.Tensor\\") -> \\"dlpack\\"` - **Input:** A PyTorch tensor. - **Output:** A DLPack tensor. 3. `verify_conversion(dlpack_tensor: \\"dlpack\\") -> bool` - **Input:** A DLPack tensor. - **Output:** A boolean indicating the success of the conversion and operation. ```python import torch from torch.utils.dlpack import from_dlpack, to_dlpack import dlpack def convert_to_pytorch(dlpack_tensor: \\"dlpack\\") -> torch.Tensor: # Your code here pass def convert_to_dlpack(pytorch_tensor: torch.Tensor) -> \\"dlpack\\": # Your code here pass def verify_conversion(dlpack_tensor: \\"dlpack\\") -> bool: # Your code here pass # Example Usage (assuming `example_dlpack_tensor` is a predefined DLPack tensor): # result = verify_conversion(example_dlpack_tensor) # print(result) # This should print True if conversion and operations are successful. ``` **Note:** Ensure you handle any exceptions or edge cases that may arise during the tensor operations and conversions.","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def convert_to_pytorch(dlpack_tensor: \\"dlpack\\") -> torch.Tensor: Converts a DLPack tensor to a PyTorch tensor. return from_dlpack(dlpack_tensor) def convert_to_dlpack(pytorch_tensor: torch.Tensor) -> \\"dlpack\\": Converts a PyTorch tensor to a DLPack tensor. return to_dlpack(pytorch_tensor) def verify_conversion(dlpack_tensor: \\"dlpack\\") -> bool: Verifies the conversion operations between DLPack and PyTorch tensors. try: # Convert DLPack tensor to PyTorch tensor pytorch_tensor = convert_to_pytorch(dlpack_tensor) # Perform an operation on the PyTorch tensor pytorch_tensor += 1 # Convert the modified PyTorch tensor back to DLPack tensor modified_dlpack_tensor = convert_to_dlpack(pytorch_tensor) # Verify that the tensor was modified correctly final_pytorch_tensor = from_dlpack(modified_dlpack_tensor) return torch.equal(pytorch_tensor, final_pytorch_tensor) except Exception as e: print(f\\"Error during verification: {e}\\") return False"},{"question":"**Objective**: Demonstrate comprehension of codec-based encoding and decoding in Python, along with custom error handling. **Problem Statement**: You are tasked with creating a utility that processes text data using specified encodings and custom error handling mechanisms. Your utility should be capable of encoding a given string into a byte sequence and subsequently decoding it back into a string, using various encodings. Additionally, you should handle any encoding or decoding errors using a custom error handler. # Requirements: 1. Implement a function `custom_encode(data: str, encoding: str, errors: str = \'strict\') -> bytes` that encodes a given string `data` using the specified `encoding` and error strategy `errors`. 2. Implement a function `custom_decode(data: bytes, encoding: str, errors: str = \'strict\') -> str` that decodes a given byte sequence `data` using the specified `encoding` and error strategy `errors`. 3. Implement a custom error handling callback function `custom_error_handler(exc)`, and register this handler under the name `\'custom\'`. 4. Modify the `encoding` and `decoding` functions to use the custom error handler when `errors` is set to `\'custom\'`. # Input: - `data` (str) or (bytes): The data to be encoded or decoded. - `encoding` (str): The encoding to use (e.g., \'utf-8\', \'ascii\'). - `errors` (str): The error handling strategy to use (\'strict\', \'ignore\', \'replace\', or \'custom\'). # Output: - For `custom_encode`: The encoded byte sequence. - For `custom_decode`: The decoded string. # Constraints: - You must use `PyCodec_Encode` and `PyCodec_Decode` functions for encoding and decoding operations. - You must use `PyCodec_RegisterError` to register the custom error handler. # Example: ```python def custom_encode(data: str, encoding: str, errors: str = \'strict\') -> bytes: # Your implementation here pass def custom_decode(data: bytes, encoding: str, errors: str = \'strict\') -> str: # Your implementation here pass def custom_error_handler(exc): # Your implementation here pass # Register the custom error handler # PyCodec_RegisterError(\'custom\', custom_error_handler) # Example usage try: encoded_data = custom_encode(\\"Sample text\\", \\"ascii\\", \\"custom\\") decoded_data = custom_decode(encoded_data, \\"ascii\\", \\"custom\\") print(decoded_data) except Exception as e: print(f\\"Error occurred: {e}\\") ``` # Notes: - Your custom error handler should replace problematic characters with a placeholder character (e.g., \'?\'). - Ensure that your functions handle various encoding types and error handling strategies appropriately. - You may define additional helper functions if needed.","solution":"import codecs def custom_encode(data: str, encoding: str, errors: str = \'strict\') -> bytes: Encodes a given string `data` using the specified `encoding` and error strategy `errors`. return codecs.encode(data, encoding, errors) def custom_decode(data: bytes, encoding: str, errors: str = \'strict\') -> str: Decodes a given byte sequence `data` using the specified `encoding` and error strategy `errors`. return codecs.decode(data, encoding, errors) def custom_error_handler(exc): Custom error handler that replaces problematic characters with \'?\'. if isinstance(exc, (UnicodeEncodeError, UnicodeDecodeError)): # Replace with the character \'?\' and try to continue processing return (\'?\', exc.start + 1) else: raise TypeError(\\"Don\'t know how to handle this type of error\\") # Register the custom error handler codecs.register_error(\'custom\', custom_error_handler) # Example usage try: encoded_data = custom_encode(\\"Sample text with unicode: ñ\\", \\"ascii\\", \\"custom\\") decoded_data = custom_decode(encoded_data, \\"ascii\\", \\"custom\\") print(decoded_data) except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"# Question: You are tasked with creating a function in Python that extracts the version details from a 32-bit integer that encodes the Python version number. This function should return a dictionary with the major, minor, micro, release level, and release serial parts of the version. Detailed Specifications: 1. The function should be named `extract_python_version`. 2. The function should take a single integer input representing the encoded version number. 3. The function should return a dictionary in the following format: ```python { \\"major\\": <major_version>, \\"minor\\": <minor_version>, \\"micro\\": <micro_version>, \\"release_level\\": <release_level>, \\"release_serial\\": <release_serial> } ``` - `major`: An integer representing the major version (e.g., 3 in \\"3.10.0\\"). - `minor`: An integer representing the minor version (e.g., 10 in \\"3.10.0\\"). - `micro`: An integer representing the micro version (e.g., 1 in \\"3.10.1\\"). - `release_level`: A string representing the release level, which could be \'alpha\', \'beta\', \'candidate\', or \'final\'. - `release_serial`: An integer representing the release serial number (e.g., 2 in \\"3.10.0a2\\"). Considerations: 1. You will use bitwise operations to extract each part of the version number from the 32-bit integer. 2. The `release_level` should be mapped from the hexadecimal value to the appropriate string: - 0xA to \'alpha\' - 0xB to \'beta\' - 0xC to \'candidate\' - 0xF to \'final\' 3. Here are some example encoded version numbers and their corresponding outputs: ```python # Example 1: encoded_version = 0x030401a2 output = { \\"major\\": 3, \\"minor\\": 4, \\"micro\\": 1, \\"release_level\\": \\"alpha\\", \\"release_serial\\": 2 } # Example 2: encoded_version = 0x030a00f0 output = { \\"major\\": 3, \\"minor\\": 10, \\"micro\\": 0, \\"release_level\\": \\"final\\", \\"release_serial\\": 0 } ``` Function Signature: ```python def extract_python_version(version_hex: int) -> dict: pass ``` # Constraints: 1. The input integer will always be a valid 32-bit encoded Python version. 2. Performance should be optimal, but given the small size of the operation, special performance optimization is not required.","solution":"def extract_python_version(version_hex: int) -> dict: Extracts the Python version details from a 32-bit integer. Args: version_hex (int): A 32-bit integer representing the encoded version number. Returns: dict: A dictionary containing the major, minor, micro, release level, and release serial parts of the version. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF release_level_code = (version_hex >> 4) & 0xF release_serial = version_hex & 0xF release_level_map = { 0xA: \'alpha\', 0xB: \'beta\', 0xC: \'candidate\', 0xF: \'final\' } release_level = release_level_map.get(release_level_code, \'unknown\') return { \\"major\\": major, \\"minor\\": minor, \\"micro\\": micro, \\"release_level\\": release_level, \\"release_serial\\": release_serial }"},{"question":"You are provided with several real-world datasets available in the `sklearn.datasets` module. Your task is to load one of these datasets, perform exploratory data analysis (EDA), preprocess the data, and train a simple machine learning model. Follow the instructions below: Instructions 1. **Dataset Loading**: - Use the `fetch_california_housing` function from `sklearn.datasets` to load the California housing dataset. 2. **Exploratory Data Analysis (EDA)**: - Display the first five rows of the dataset. - Show the statistical summary (mean, std, min, 25%, 50%, 75%, max) of the dataset. - Identify and display any missing values in the dataset. 3. **Data Preprocessing**: - Handle any missing values if present. - Split the dataset into training and testing sets (80% training, 20% testing). 4. **Model Training**: - Train a linear regression model using `LinearRegression` from `sklearn.linear_model` on the training dataset. 5. **Model Evaluation**: - Evaluate the model on the testing dataset and report the R² score. Expected Input and Output - **Input**: - Use the California housing dataset. - **Output**: - Display first five rows of the dataset. - Display the statistical summary of the dataset. - A report of any missing values in the dataset. - R² score of the linear regression model on the test set. Constraints - Use only Scikit-learn functionalities for dataset operations, preprocessing, and model training. - Ensure proper handling of any missing values. Example Code Template ```python from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import pandas as pd # 1. Load the dataset data = fetch_california_housing(as_frame=True) df = data.frame # 2. EDA # Display the first five rows print(\\"First five rows of the dataset:\\") print(df.head()) # Display statistical summary print(\\"nStatistical summary:\\") print(df.describe()) # Identify missing values print(\\"nMissing values in the dataset:\\") print(df.isnull().sum()) # 3. Data Preprocessing # Handle missing values if any # (Assuming no missing values for simplicity, but typically you would handle them here) # Split the dataset X = df.drop(\'MedHouseVal\', axis=1) y = df[\'MedHouseVal\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 4. Model Training model = LinearRegression() model.fit(X_train, y_train) # 5. Model Evaluation r2_score = model.score(X_test, y_test) print(\\"nR² score of the linear regression model on the test set:\\") print(r2_score) ``` Use this template and follow the instructions to complete your assessment.","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import pandas as pd def load_dataset(): data = fetch_california_housing(as_frame=True) return data.frame def exploratory_data_analysis(df): eda_results = { \\"first_five_rows\\": df.head(), \\"statistical_summary\\": df.describe(), \\"missing_values\\": df.isnull().sum() } return eda_results def preprocess_data(df): X = df.drop(\'MedHouseVal\', axis=1) y = df[\'MedHouseVal\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_model(X_train, y_train): model = LinearRegression() model.fit(X_train, y_train) return model def evaluate_model(model, X_test, y_test): r2_score = model.score(X_test, y_test) return r2_score # Loading the dataset df = load_dataset() # Performing EDA eda_results = exploratory_data_analysis(df) print(\\"First five rows of the dataset:\\") print(eda_results[\\"first_five_rows\\"]) print(\\"nStatistical summary:\\") print(eda_results[\\"statistical_summary\\"]) print(\\"nMissing values in the dataset:\\") print(eda_results[\\"missing_values\\"]) # Preprocessing the data X_train, X_test, y_train, y_test = preprocess_data(df) # Training the model model = train_model(X_train, y_train) # Evaluating the model r2_score = evaluate_model(model, X_test, y_test) print(\\"nR² score of the linear regression model on the test set:\\") print(r2_score)"},{"question":"**Coding Assessment Question:** You are given the task of implementing two functions that mimic the working of `quopri` module\'s encoding and decoding features for quoted-printable data. Your implementation should read from and write to binary files and should handle spaces and tabs according to the specifications. # Task 1: `custom_encodestring` Write a function `custom_encodestring(s: bytes, quotetabs: bool = False, header: bool = False) -> bytes` that encodes a given bytes string `s` into its quoted-printable form. # Task 2: `custom_decodestring` Write a function `custom_decodestring(s: bytes, header: bool = False) -> bytes` that decodes a given quoted-printable bytes string `s` back to its original form. # Specifications: 1. **custom_encodestring(s: bytes, quotetabs: bool = False, header: bool = False) -> bytes**: - **Input**: A bytes string `s`, a boolean `quotetabs` indicating whether to encode embedded spaces and tabs, and a boolean `header` indicating if spaces should be encoded as underscores. - **Output**: Returns the quoted-printable encoded bytes string. - **Constraints**: - If `quotetabs` is `True`, both tabs and spaces should be encoded in the output. - If `header` is `True`, spaces should be encoded as underscores. 2. **custom_decodestring(s: bytes, header: bool = False) -> bytes**: - **Input**: A quoted-printable encoded bytes string `s` and a boolean `header` indicating if underscores should be decoded as spaces. - **Output**: Returns the original decoded binary data as bytes. - **Constraints**: - If `header` is `True`, underscores in the input should be decoded back to spaces. # Example ```python original = b\\"Hello, World! Hi there!\\" # spaces and alphanumeric characters encoded = custom_encodestring(original, quotetabs=True, header=True) decoded = custom_decodestring(encoded, header=True) assert decoded == original original = b\\"Tabt and spaces are here.\\" encoded = custom_encodestring(original, quotetabs=True, header=False) decoded = custom_decodestring(encoded, header=False) assert decoded == original ``` # Notes: - Pay attention to RFC 1521 and RFC 1522 guidelines regarding encoding rules and transformations. - Ensure that the implementation works efficiently with large inputs.","solution":"def custom_encodestring(s: bytes, quotetabs: bool = False, header: bool = False) -> bytes: result = bytearray() for b in s: if b == ord(\' \'): if header: result.extend(b\'_\') elif quotetabs: result.extend(b\'=20\') else: result.append(b) elif b == ord(\'t\'): if quotetabs: result.extend(b\'=09\') else: result.append(b) elif 33 <= b <= 60 or 62 <= b <= 126: # ASCII printable characters except \'=\' result.append(b) else: result.extend(f\'={b:02X}\'.encode()) return bytes(result) def custom_decodestring(s: bytes, header: bool = False) -> bytes: result = bytearray() i = 0 while i < len(s): b = s[i] if header and b == ord(\'_\'): result.append(ord(\' \')) elif b == ord(\'=\'): hex_str = s[i+1:i+3].decode() result.append(int(hex_str, 16)) i += 2 else: result.append(b) i += 1 return bytes(result)"},{"question":"# Custom Extension Type - PolyNumber Background: You are asked to implement a custom extension type in Python called `PolyNumber`. This type represents a polynomial number and supports basic operations and attribute management. The polynomial number is represented by its coefficients in increasing order of powers. For example, the polynomial `3 + 2x + x^2` would be represented as `[3, 2, 1]` in the coefficients attribute. Task: You need to create a custom extension type in Python that supports: 1. Initialization from a list of coefficients. 2. String representation of the polynomial. 3. Attribute access and modification. 4. Addition of two `PolyNumber` objects. Requirements: 1. Implement the following type methods in C: - `tp_init` for initializing with a list of coefficients. - `tp_str` for providing a human-readable string representation. - `tp_getattro` and `tp_setattro` for managing attributes (`coefficients`). - `tp_add` for adding two polynomial numbers. 2. The string representation for the polynomial `3 + 2x + x^2` should be in the format `\\"3 + 2x + 1x^2\\"`. 3. Support the following attributes: - `coefficients`: A list of integers representing the polynomial coefficients. 4. Addition (`+`) of two `PolyNumber` objects should sum their coefficients. Input/Output: - **Input:** A list of integers representing the coefficients. - **Output:** A string for `tp_str`, addition results in a new `PolyNumber` object. Constraints: - Coefficients should be non-negative integers. - Ensure proper memory management in your implementation. Implementation Tip: Below is a skeleton of your implementation to get started: ```c #include <Python.h> typedef struct { PyObject_HEAD PyObject *coefficients; /* List of coefficients */ } PolyNumberObject; static int PolyNumber_init(PolyNumberObject *self, PyObject *args, PyObject *kwds) { PyObject *coefficients = NULL; if (!PyArg_ParseTuple(args, \\"O\\", &coefficients)) { return -1; } if (!PyList_Check(coefficients)) { PyErr_SetString(PyExc_TypeError, \\"The coefficient attribute value must be a list\\"); return -1; } Py_INCREF(coefficients); self->coefficients = coefficients; return 0; } static PyObject * PolyNumber_str(PolyNumberObject *self) { // Your implementation to create string representation } static PyObject * PolyNumber_getattro(PolyNumberObject *self, PyObject *attr) { // Your implementation to manage attributes } static int PolyNumber_setattro(PolyNumberObject *self, PyObject *attr, PyObject *value) { // Your implementation to manage attributes } static PyObject * PolyNumber_add(PyObject *self, PyObject *other) { // Your implementation to add two PolyNumber objects } static PyTypeObject PolyNumberType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"PolyNumber\\", .tp_basicsize = sizeof(PolyNumberObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_init = (initproc)PolyNumber_init, .tp_str = (reprfunc)PolyNumber_str, .tp_getattro = (getattrofunc)PolyNumber_getattro, .tp_setattro = (setattrofunc)PolyNumber_setattro, .tp_as_number = &PolyNumber_as_number, // Additional methods if any }; static PyNumberMethods PolyNumber_as_number = { .nb_add = (binaryfunc)PolyNumber_add, // Additional number methods if any }; static PyModuleDef polynumbermodule = { PyModuleDef_HEAD_INIT, .m_name = \\"polynumber\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_polynumber(void) { PyObject *m; if (PyType_Ready(&PolyNumberType) < 0) return NULL; m = PyModule_Create(&polynumbermodule); if (m == NULL) return NULL; Py_INCREF(&PolyNumberType); PyModule_AddObject(m, \\"PolyNumber\\", (PyObject *)&PolyNumberType); return m; } ``` Implement the missing functions and initialize the module in Python.","solution":"class PolyNumber: def __init__(self, coefficients): if not all(isinstance(c, int) and c >= 0 for c in coefficients): raise ValueError(\\"All coefficients must be non-negative integers.\\") self.coefficients = coefficients def __str__(self): terms = [] for i, coeff in enumerate(self.coefficients): if coeff != 0: if i == 0: terms.append(f\\"{coeff}\\") elif i == 1: terms.append(f\\"{coeff}x\\") else: terms.append(f\\"{coeff}x^{i}\\") return \\" + \\".join(terms) if terms else \\"0\\" def __add__(self, other): if not isinstance(other, PolyNumber): return NotImplemented new_coeffs = [ a + b for a, b in zip(self.coefficients, other.coefficients) ] + self.coefficients[len(other.coefficients):] + other.coefficients[len(self.coefficients):] return PolyNumber(new_coeffs)"},{"question":"**Advanced Coding Assessment: Manipulating Python Function Objects** # Objective Demonstrate your understanding of Python function objects and their attributes, by using the CPython function handling API exposed through Python\'s `ctypes` library. # Description You are required to implement a Python class `FunctionManipulator` that uses the `ctypes` library to interact with Python function objects at a lower level, utilizing the functions detailed in the provided documentation. Your class should provide methods to: 1. Create a new function object. 2. Retrieve various attributes of a function object. 3. Modify the default arguments of a function. # Requirements Your `FunctionManipulator` class must implement the following methods: 1. **`create_function(code: types.CodeType, globals_dict: dict) -> types.FunctionType`** - **Input:** - `code` is a code object created using `compile()`. - `globals_dict` is a dictionary of global variables accessible to the function. - **Output:** Returns a new function object using `PyFunction_New`. 2. **`get_function_attributes(func: types.FunctionType) -> dict`** - **Input:** - `func` is a Python function object. - **Output:** Returns a dictionary with keys `\'code\'`, `\'globals\'`, `\'module\'`, `\'defaults\'`, `\'closure\'`, and `\'annotations\'` containing the respective attributes of the function. 3. **`set_function_defaults(func: types.FunctionType, defaults: tuple) -> None`** - **Input:** - `func` is a Python function object. - `defaults` is a tuple containing the new default argument values. - **Output:** Sets the function\'s default arguments to the values in `defaults`. # Constraints - You should use the `ctypes` library to access the C API functions described in the documentation. - Handle any errors that might arise from using these C API functions appropriately. - Ensure that your solution works with standard library functions and custom functions. # Example ```python import ctypes import types class FunctionManipulator: def __init__(self): # Load the Python C API library self.libpython = ctypes.PyDLL(None) def create_function(self, code: types.CodeType, globals_dict: dict) -> types.FunctionType: # Implement to create a function object using ctypes and PyFunction_New pass def get_function_attributes(self, func: types.FunctionType) -> dict: # Implement to get function attributes using ctypes and respective C API functions pass def set_function_defaults(self, func: types.FunctionType, defaults: tuple) -> None: # Implement to set default arguments using ctypes and PyFunction_SetDefaults pass # Example usage code = compile(\\"def _(): return x\\", \\"<string>\\", \\"exec\\").co_consts[0] globals_dict = {\'x\': 42} manipulator = FunctionManipulator() new_func = manipulator.create_function(code, globals_dict) assert new_func() == 42 attributes = manipulator.get_function_attributes(new_func) print(attributes) manipulator.set_function_defaults(new_func, (100,)) assert new_func() == 100 ``` Implement the above methods ensuring that all function attributes can be accurately retrieved, and defaults can be correctly set. Your solution should appropriately handle any errors that arise from invalid operations or inputs.","solution":"import ctypes import types class FunctionManipulator: def __init__(self): # Load the Python C API library try: self.libpython = ctypes.PyDLL(None) except Exception as e: raise RuntimeError(\\"Failed to load the Python C API: \\" + str(e)) def create_function(self, code: types.CodeType, globals_dict: dict) -> types.FunctionType: try: PyFunction_New = self.libpython.PyFunction_New PyFunction_New.restype = ctypes.py_object PyFunction_New.argtypes = [ctypes.py_object, ctypes.py_object] function = PyFunction_New(code, globals_dict) return function except Exception as e: raise RuntimeError(\\"Error creating function object: \\" + str(e)) def get_function_attributes(self, func: types.FunctionType) -> dict: try: attributes = { \'code\': func.__code__, \'globals\': func.__globals__, \'module\': func.__module__, \'defaults\': func.__defaults__, \'closure\': func.__closure__, \'annotations\': func.__annotations__, } return attributes except AttributeError as e: raise RuntimeError(\\"Error retrieving function attributes: \\" + str(e)) def set_function_defaults(self, func: types.FunctionType, defaults: tuple) -> None: try: func.__defaults__ = defaults except TypeError as e: raise RuntimeError(\\"Error setting function default arguments: \\" + str(e)) # Example usage if __name__ == \\"__main__\\": code = compile(\\"def _(): return x\\", \\"<string>\\", \\"exec\\").co_consts[0] globals_dict = {\'x\': 42} manipulator = FunctionManipulator() new_func = manipulator.create_function(code, globals_dict) assert new_func() == 42 attributes = manipulator.get_function_attributes(new_func) print(attributes) manipulator.set_function_defaults(new_func, (100,)) assert new_func() == 100"},{"question":"# Question You are tasked with creating a Python script that simulates a simple custom logging and error-handling system using functionalities from the `sys` module. The script should demonstrate your understanding of some key aspects of the `sys` module. Requirements: 1. **Custom Exception Handling**: - Implement a custom exception handler that overrides the default `sys.excepthook`. - The custom handler should log the exception information (type, value, traceback) into a file named `exceptions.log`. - The handler should also print a user-friendly error message such as \\"An error occurred. Please check exceptions.log for details.\\" 2. **Command Line Argument Processing**: - Process command line arguments passed to the script and provide a simple help message if the `--help` flag is present. - If no arguments are provided, print \\"No arguments provided.\\". - Log all the command line arguments to a file named `arguments.log`. 3. **Performance Monitoring**: - Use `sys.getallocatedblocks` and `sys.getrecursionlimit` to check and log the current memory usage and recursion limit to a file named `performance.log`. 4. **Dynamic Function Execution with Tracing**: - Implement a tracing function that logs each line of code being executed to a file named `trace.log`. - Use `sys.settrace` to set this tracing function and dynamically execute a simple function to demonstrate tracing. Input Format: - The script may receive several command line arguments. - The script should be executable without any arguments, handling them elegantly. Output Format: - The script should generate the following three log files: - `exceptions.log`: Logs details of any exceptions using the custom exception handler. - `arguments.log`: Logs all command line arguments. - `performance.log`: Logs memory usage and recursion limits. - `trace.log`: Logs each line of code executed during the traced function. Constraints: - Ensure file operations handle exceptions properly to avoid runtime errors. - Clearly comment on each part of the script to explain its functionality. Example Usage: ```sh python custom_sys_script.py --help ``` ```sh python custom_sys_script.py arg1 arg2 ``` Example File Contents: - `arguments.log`: ``` Command Line Arguments: [\'arg1\', \'arg2\'] ``` - `performance.log`: ``` Allocated blocks: 10025 Recursion limit: 3000 ``` - `exceptions.log`: ``` Exception type: <class \'ValueError\'> Exception value: An example error Traceback: (most recent call last): File \\"custom_sys_script.py\\", line 42, in <module> raise ValueError(\\"An example error\\") ``` - `trace.log`: ``` Tracing line: print(\\"Tracing this function\\") ``` Implement the described functionalities in your Python script.","solution":"import sys import traceback def custom_exception_handler(exc_type, exc_value, exc_traceback): with open(\'exceptions.log\', \'a\') as f: f.write(f\\"Exception type: {exc_type}n\\") f.write(f\\"Exception value: {exc_value}n\\") f.write(\\"Traceback:n\\") traceback.print_tb(exc_traceback, file=f) print(\\"An error occurred. Please check exceptions.log for details.\\") sys.excepthook = custom_exception_handler def process_command_line_arguments(): if \'--help\' in sys.argv: print(\\"Usage: python custom_sys_script.py [arguments]\\") elif len(sys.argv) == 1: print(\\"No arguments provided.\\") with open(\'arguments.log\', \'w\') as f: f.write(f\\"Command Line Arguments: {sys.argv[1:]}n\\") def log_performance_metrics(): with open(\'performance.log\', \'w\') as f: f.write(f\\"Allocated blocks: {sys.getallocatedblocks()}n\\") f.write(f\\"Recursion limit: {sys.getrecursionlimit()}n\\") def tracefunc(frame, event, arg): if event == \\"line\\": with open(\'trace.log\', \'a\') as f: f.write(f\\"Tracing line: {frame.f_lineno} - {frame.f_code.co_name}n\\") return tracefunc def trace_me(): print(\\"Tracing this function.\\") x = 10 y = x + 5 print(f\\"Result: {y}\\") if __name__ == \\"__main__\\": process_command_line_arguments() log_performance_metrics() sys.settrace(tracefunc) trace_me() sys.settrace(None)"},{"question":"**Pathname Multi-Tool Function** **Objective:** You need to implement a Python function that simulates a multi-tool for pathname manipulations by utilizing various functionalities provided by the `os.path` module. **Function Signature:** ```python def pathname_multi_tool(paths, operation, start_path=None): Perform various pathname manipulations based on the specified operation. Parameters: paths (list): A list of path-like objects (strings or bytes) to be used as input. operation (str): The operation to be performed. It can be \\"join\\", \\"split\\", \\"normalize\\", \\"basename\\", \\"dirname\\", \\"commonpath\\", \\"commonprefix\\", \\"realpath\\", \\"relpath\\", or \\"exists\\". start_path (str, optional): A starting path for operations like \\"relpath\\" where a start path is needed. Defaults to None. Returns: The result of the operation specified: - \\"join\\": A single path obtained by joining all paths in the list. - \\"split\\": A list of tuples [(head, tail), ...] obtained by splitting each path in the list. - \\"normalize\\": A list of normalized paths. - \\"basename\\": A list of base names for each path in the list. - \\"dirname\\": A list of directory names for each path in the list. - \\"commonpath\\": The longest common sub-path of all paths in the input list. - \\"commonprefix\\": The longest common prefix of all paths in the input list. - \\"realpath\\": A list of canonical paths of each path in the list. - \\"relpath\\": A list of paths relative to the start_path for each path in the list. - \\"exists\\": A list of booleans indicating if each path in the list exists. Raises: ValueError: If the operation is not recognized or if inputs are invalid. pass ``` **Input Constraints:** - `paths` will contain between 1 and 100 path-like objects. - Each path-like object will be a string or bytes and will have a length between 1 and 200 characters. - `operation` will be one of the following strings: \\"join\\", \\"split\\", \\"normalize\\", \\"basename\\", \\"dirname\\", \\"commonpath\\", \\"commonprefix\\", \\"realpath\\", \\"relpath\\", \\"exists\\". - If `operation` is \\"relpath\\", `start_path` will be provided and will be a valid path-like object. **Examples:** 1. **Join Operation:** ```python pathname_multi_tool([\\"/home/user\\", \\"documents\\", \\"file.txt\\"], \\"join\\") ``` **Output:** ```python \'/home/user/documents/file.txt\' ``` 2. **Split Operation:** ```python pathname_multi_tool([\\"/home/user/file.txt\\", \\"/var/log/sys.log\\"], \\"split\\") ``` **Output:** ```python [(\'/home/user\', \'file.txt\'), (\'/var/log\', \'sys.log\')] ``` 3. **Common Path Operation:** ```python pathname_multi_tool([\\"/home/user/docs\\", \\"/home/user/music\\", \\"/home/user/pics\\"], \\"commonpath\\") ``` **Output:** ```python \'/home/user\' ``` 4. **Relative Path Operation:** ```python pathname_multi_tool([\\"/home/user/docs/file.txt\\", \\"/home/user/music/song.mp3\\"], \\"relpath\\", start_path=\\"/home/user\\") ``` **Output:** ```python [\'docs/file.txt\', \'music/song.mp3\'] ``` **Further Guidance:** - Utilize the appropriate `os.path` functions to perform the specified operations. - Ensure error handling by raising a `ValueError` for unrecognized operations and invalid inputs. - Consider edge cases such as empty paths, invalid types, and inconsistent operations. Implement the function `pathname_multi_tool` to handle the described requirements and constraints.","solution":"import os def pathname_multi_tool(paths, operation, start_path=None): Perform various pathname manipulations based on the specified operation. Parameters: paths (list): A list of path-like objects (strings or bytes) to be used as input. operation (str): The operation to be performed. It can be \\"join\\", \\"split\\", \\"normalize\\", \\"basename\\", \\"dirname\\", \\"commonpath\\", \\"commonprefix\\", \\"realpath\\", \\"relpath\\", or \\"exists\\". start_path (str, optional): A starting path for operations like \\"relpath\\" where a start path is needed. Defaults to None. Returns: The result of the operation specified: - \\"join\\": A single path obtained by joining all paths in the list. - \\"split\\": A list of tuples [(head, tail), ...] obtained by splitting each path in the list. - \\"normalize\\": A list of normalized paths. - \\"basename\\": A list of base names for each path in the list. - \\"dirname\\": A list of directory names for each path in the list. - \\"commonpath\\": The longest common sub-path of all paths in the input list. - \\"commonprefix\\": The longest common prefix of all paths in the input list. - \\"realpath\\": A list of canonical paths of each path in the list. - \\"relpath\\": A list of paths relative to the start_path for each path in the list. - \\"exists\\": A list of booleans indicating if each path in the list exists. Raises: ValueError: If the operation is not recognized or if inputs are invalid. if not isinstance(paths, list) or not all(isinstance(p, (str, bytes)) for p in paths): raise ValueError(\\"Paths must be a list of strings or bytes.\\") if operation == \\"join\\": return os.path.join(*paths) elif operation == \\"split\\": return [os.path.split(p) for p in paths] elif operation == \\"normalize\\": return [os.path.normpath(p) for p in paths] elif operation == \\"basename\\": return [os.path.basename(p) for p in paths] elif operation == \\"dirname\\": return [os.path.dirname(p) for p in paths] elif operation == \\"commonpath\\": return os.path.commonpath(paths) elif operation == \\"commonprefix\\": return os.path.commonprefix(paths) elif operation == \\"realpath\\": return [os.path.realpath(p) for p in paths] elif operation == \\"relpath\\": if start_path is None: raise ValueError(\\"start_path must be provided for \'relpath\' operation.\\") return [os.path.relpath(p, start=start_path) for p in paths] elif operation == \\"exists\\": return [os.path.exists(p) for p in paths] else: raise ValueError(\\"Unsupported operation specified.\\")"},{"question":"# Web Browser Controller Customization You are tasked with automating the process of opening URLs using Python\'s `webbrowser` module. Your task is to create a Python function that demonstrates the functionality provided by the module and meets the following requirements: Requirements: 1. **Function Name:** `custom_open_url` 2. **Inputs:** - `url` (string): The URL to be opened. - `open_type` (string): The type of opening, which can be one of the following values: - `\\"new_window\\"` to open the URL in a new window. - `\\"new_tab\\"` to open the URL in a new tab. - `\\"default\\"` to use the default browser settings for opening the URL. 3. **Output:** None 4. **Functionality:** - Based on the `open_type`, the function should open the URL in the appropriate manner (new window, new tab, or default). - The function should handle errors gracefully and print them out using the `webbrowser.Error` exception. - Additionally, the function should check if the environment variable `BROWSER` is set. If it is, use the browser specified in the `BROWSER` variable for opening the URL. 5. **Constraints:** - Use appropriate methods from the `webbrowser` module. - Ensure cross-platform compatibility. Example Usage: ```python # Open in a new window custom_open_url(\\"https://www.example.com\\", \\"new_window\\") # Open in a new tab custom_open_url(\\"https://www.example.com\\", \\"new_tab\\") # Use default opening method custom_open_url(\\"https://www.example.com\\", \\"default\\") ``` Notes: - Ensure your implementation is clean and well-documented. - Handle cases where the environment variable `BROWSER` may not be set. - Make sure to use appropriate exception handling as described in the `webbrowser` module documentation.","solution":"import webbrowser import os def custom_open_url(url, open_type): Opens the given URL based on the specified open type. Parameters: - url (str): The URL to open. - open_type (str): The type of opening (\\"new_window\\", \\"new_tab\\", \\"default\\") Returns: - None try: # Check if BROWSER environment variable is set browser_name = os.environ.get(\'BROWSER\') if browser_name: browser = webbrowser.get(browser_name) else: browser = webbrowser.get() if open_type == \\"new_window\\": browser.open_new(url) elif open_type == \\"new_tab\\": browser.open_new_tab(url) elif open_type == \\"default\\": browser.open(url) else: raise ValueError(\\"Invalid open_type provided. Choose from \'new_window\', \'new_tab\', \'default\'.\\") except webbrowser.Error as e: print(f\\"Failed to open URL: {e}\\") except ValueError as ve: print(f\\"Error: {ve}\\")"},{"question":"Objective: Implement a python script that demonstrates comprehension of both one-shot and incremental compression and decompression using the `bz2` module. Problem Statement: You are tasked with handling large text data compression and decompression. To test your understanding of the `bz2` module, implement the following functionalities: 1. **One-shot Compression/Decompression**: - Write a function `one_shot_compress(data: bytes, compresslevel: int) -> bytes` that compresses the data using the given compression level (1-9). - Write a function `one_shot_decompress(data: bytes) -> bytes` that decompresses the provided compressed data. 2. **Incremental Compression/Decompression**: - Write a function `incremental_compress(data_chunks: list[bytes], compresslevel: int) -> bytes` that incrementally compresses an iterable of data chunks using the given compression level (1-9) and returns the compressed data. - Write a function `incremental_decompress(compressed_data: bytes) -> bytes` that incrementally decompresses the provided compressed data. Detailed Requirements: 1. **One-shot Compression/Decompression** - The `one_shot_compress` function should take a `bytes` object `data` and an integer `compresslevel` (1-9) and return the compressed data as `bytes`. - The `one_shot_decompress` function should take a compressed `bytes` object and return the decompressed data as `bytes`. 2. **Incremental Compression/Decompression** - The `incremental_compress` function should take a list of `bytes` objects `data_chunks` and an integer `compresslevel` (1-9). It should return the compressed data as a single `bytes` object. - The `incremental_decompress` function should take a compressed `bytes` object and return the decompressed data as a single `bytes` object. 3. **Performance and Constraints:** - Ensure your solution efficiently handles large data inputs. - Document any assumptions or decisions made during implementation. Function Signatures: ```python def one_shot_compress(data: bytes, compresslevel: int) -> bytes: pass def one_shot_decompress(data: bytes) -> bytes: pass def incremental_compress(data_chunks: list[bytes], compresslevel: int) -> bytes: pass def incremental_decompress(compressed_data: bytes) -> bytes: pass ``` Example Usage: ```python # Example data for one-shot compression and decompression data = b\\"Example data for testing bzip2 compression and decompression.\\" # One-shot Compression compressed_data = one_shot_compress(data, 9) assert isinstance(compressed_data, bytes) # One-shot Decompression decompressed_data = one_shot_decompress(compressed_data) assert decompressed_data == data # Data chunks for incremental compression and decompression data_chunks = [b\\"part1\\", b\\"part2\\", b\\"part3\\"] # Incremental Compression compressed_chunks = incremental_compress(data_chunks, 9) assert isinstance(compressed_chunks, bytes) # Incremental Decompression decompressed_chunks = incremental_decompress(compressed_chunks) assert decompressed_chunks == b\\"\\".join(data_chunks) ``` Note: - Ensure you handle edge cases such as empty inputs. - Include comments to explain your approach and logic.","solution":"import bz2 def one_shot_compress(data: bytes, compresslevel: int) -> bytes: Compresses the data using the given compression level (1-9). if not isinstance(data, bytes): raise TypeError(\\"data must be bytes\\") if not 1 <= compresslevel <= 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") return bz2.compress(data, compresslevel) def one_shot_decompress(data: bytes) -> bytes: Decompresses the provided compressed data. if not isinstance(data, bytes): raise TypeError(\\"data must be bytes\\") return bz2.decompress(data) def incremental_compress(data_chunks: list[bytes], compresslevel: int) -> bytes: Incrementally compresses an iterable of data chunks using the given compression level (1-9) and returns the compressed data. if not all(isinstance(chunk, bytes) for chunk in data_chunks): raise TypeError(\\"all elements in data_chunks must be bytes\\") if not 1 <= compresslevel <= 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") compressor = bz2.BZ2Compressor(compresslevel) compressed_data = bytearray() for chunk in data_chunks: compressed_data.extend(compressor.compress(chunk)) compressed_data.extend(compressor.flush()) return bytes(compressed_data) def incremental_decompress(compressed_data: bytes) -> bytes: Incrementally decompresses the provided compressed data and returns the decompressed data. if not isinstance(compressed_data, bytes): raise TypeError(\\"compressed_data must be bytes\\") decompressor = bz2.BZ2Decompressor() decompressed_data = decompressor.decompress(compressed_data) return decompressed_data"},{"question":"Advanced Seaborn Plot Customization Objective: Demonstrate your comprehension of Seaborn for creating and customizing advanced plots. Question: You are required to visualize a dataset utilizing Seaborn\'s object-oriented interface. Follow the steps below to preprocess the data and create a series of customized plots. 1. Load the `iris` dataset from Seaborn. 2. Compute the mean of the numerical columns (`sepal_length`, `sepal_width`, `petal_length`, `petal_width`) for each species and assign it to a new column `mean_measurements`. 3. Sort the dataset based on the `mean_measurements` column in descending order. 4. Create a bar plot showing `species` on the y-axis and `mean_measurements` on the x-axis, with bar text annotations displaying the exact mean values. 5. Create a second plot showing a scatter plot with `sepal_length` on the x-axis and `sepal_width` on the y-axis, color-coded by `species`. Annotate each point with its corresponding `species` name. 6. Customize text alignment and appearance in both plots to improve clarity. Input: - No external inputs required; use the Seaborn `iris` dataset. Output: - Two plots with specified customizations. Constraints: - Perform computations using Pandas. - Use Seaborn\'s object-oriented interface for plotting. - Ensure your plots are clear and well-labeled. Example Code Structure: ```python import seaborn as sns import seaborn.objects as so import pandas as pd # Load the iris dataset df = sns.load_dataset(\'iris\') # Step 2: Compute mean measurements for each species mean_measurements = ( df.groupby(\'species\') [[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']] .mean() .mean(axis=1) .reset_index(name=\'mean_measurements\') ) # Step 3: Sort the dataset based on mean_measurements mean_measurements_sorted = mean_measurements.sort_values(by=\'mean_measurements\', ascending=False) # Step 4: Create a bar plot with text annotations bar_plot = ( so.Plot(mean_measurements_sorted, x=\'mean_measurements\', y=\'species\', text=\'mean_measurements\') .add(so.Bar()) .add(so.Text(color=\'black\', halign=\'right\')) ) bar_plot.show() # Step 5: Create a scatter plot with text annotations scatter_plot = ( so.Plot(df, x=\'sepal_length\', y=\'sepal_width\', color=\'species\', text=\'species\') .add(so.Dot()) .add(so.Text(valign=\'bottom\')) ) scatter_plot.show() # Step 6: Customize text alignment and appearance # Already illustrated in the example, students should further improve as needed. ``` Ensure your code is structured correctly with clear comments and explanations for each major step.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the iris dataset df = sns.load_dataset(\'iris\') # Step 2: Compute mean measurements for each species mean_measurements = ( df.groupby(\'species\') [[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']] .mean() .mean(axis=1) .reset_index(name=\'mean_measurements\') ) # Step 3: Sort the dataset based on mean_measurements mean_measurements_sorted = mean_measurements.sort_values(by=\'mean_measurements\', ascending=False) # Step 4: Create a bar plot with text annotations plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=\'mean_measurements\', y=\'species\', data=mean_measurements_sorted, palette=\'viridis\') # Add text annotations for index, value in mean_measurements_sorted.iterrows(): bar_plot.text(value[\'mean_measurements\'] + 0.02, index, round(value[\'mean_measurements\'], 2), color=\'black\') plt.title(\'Mean Measurements per Species\') plt.xlabel(\'Mean Measurements\') plt.ylabel(\'Species\') plt.show() # Step 5: Create a scatter plot with text annotations plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', data=df, palette=\'viridis\') # Add text annotations for each point for index, row in df.iterrows(): scatter_plot.text(row[\'sepal_length\'] + 0.02, row[\'sepal_width\'], row[\'species\'], horizontalalignment=\'left\', size=\'small\', color=\'black\', weight=\'semibold\') plt.title(\'Sepal Measurements by Species\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.show()"},{"question":"**Objective:** To ensure the student understands how to use the `subprocess` module to perform pipeline operations and manage standard input and output. # Problem Statement Create a function `process_pipeline(commands: list, input_text: str) -> str` that simulates a pipeline of shell commands using the `subprocess` module. The function should execute a sequence of shell commands, where the output of one command is the input to the next command. The initial input to the pipeline is provided as a string, and the final output should also be returned as a string. Input: - `commands`: A list of strings, where each string is a shell command to be executed in the pipeline. - `input_text`: A string to be used as the initial input to the pipeline. Output: - The final output of the pipeline as a string. Example: ```python commands = [\'tr a-z A-Z\', \'grep HELLO\'] input_text = \\"hello worldnhello Pythonn\\" result = process_pipeline(commands, input_text) print(result) # Output: \\"HELLO WORLDnHELLO PYTHONn\\" ``` # Constraints: 1. You should use the `subprocess` module to handle the pipelines. 2. Ensure that all processes are properly closed and their outputs managed. 3. Consider edge cases, such as empty input text and commands that do not produce any output. Performance Requirements: - The function should correctly handle large input text efficiently. - The function should be able to handle a reasonable number of commands in the pipeline without significant performance degradation. # Implementation Notes - Utilize `subprocess.Popen` to chain the commands. - Manage the standard input and output streams correctly to ensure the pipeline works as intended. Good luck!","solution":"import subprocess def process_pipeline(commands: list, input_text: str) -> str: Executes a series of shell commands in a pipeline. Args: commands (list): A list of shell command strings. input_text (str): The initial input to the pipeline. Returns: str: The final output of the pipeline. if not commands: return input_text process = [] prev_output = None for index, command in enumerate(commands): if index == 0: # Start the first process with input_text process.append(subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)) stdout, stderr = process[-1].communicate(input=input_text.encode()) else: # Directorly chain with previous command\'s output process.append(subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)) stdout, stderr = process[-1].communicate(input=prev_output) prev_output = stdout return prev_output.decode()"},{"question":"**Question:** You are tasked with creating a utility function that processes a list of IP addresses and returns a summary. This summary should classify addresses based on their IP versions and calculate the number of unique networks each address belongs to. # Function Signature ```python def summarize_ip_addresses(ip_list: list[str]) -> dict: pass ``` # Input - `ip_list`: A list of strings where each item is a valid IP address in either IPv4 or IPv6 format (e.g., `\'192.0.2.1\'`, `\'2001:DB8::1\'`). # Output - A dictionary with the following structure: ```python { \\"IPv4\\": { \\"addresses\\": int, \\"unique_networks\\": int }, \\"IPv6\\": { \\"addresses\\": int, \\"unique_networks\\": int } } ``` - `\\"IPv4\\"` and `\\"IPv6\\"`: Keys representing the respective IP versions. - `\\"addresses\\"`: The total number of addresses for this IP version. - `\\"unique_networks\\"`: The count of unique networks the addresses belong to. An address belongs to a network with a default prefix of `/24` for IPv4 and `/64` for IPv6. # Constraints - Each address in `ip_list` will be a valid string representation of an IP address. - Do not use any additional external libraries besides `ipaddress`. # Example ```python # Example input ip_list = [ \'192.0.2.1\', \'192.0.2.5\', \'192.0.3.1\', \'2001:DB8::1\', \'2001:DB8::5\', \'2001:DB8:0:0:0:0:0:1\' ] # Example output { \\"IPv4\\": { \\"addresses\\": 3, \\"unique_networks\\": 2 }, \\"IPv6\\": { \\"addresses\\": 3, \\"unique_networks\\": 1 } } ``` # Explanation - The input list contains 3 IPv4 addresses and 3 IPv6 addresses. - The 3 IPv4 addresses belong to 2 distinct networks: `192.0.2.0/24` (for the first two addresses) and `192.0.3.0/24` (for the third address). - The 3 IPv6 addresses belong to the same network: `2001:DB8::/64`. Implement the `summarize_ip_addresses` function in Python.","solution":"import ipaddress def summarize_ip_addresses(ip_list): summary = { \\"IPv4\\": { \\"addresses\\": 0, \\"unique_networks\\": set() }, \\"IPv6\\": { \\"addresses\\": 0, \\"unique_networks\\": set() } } for ip in ip_list: ip_obj = ipaddress.ip_address(ip) if ip_obj.version == 4: summary[\\"IPv4\\"][\\"addresses\\"] += 1 network = ipaddress.ip_network(ip + \'/24\', strict=False) summary[\\"IPv4\\"][\\"unique_networks\\"].add(network.network_address) elif ip_obj.version == 6: summary[\\"IPv6\\"][\\"addresses\\"] += 1 network = ipaddress.ip_network(ip + \'/64\', strict=False) summary[\\"IPv6\\"][\\"unique_networks\\"].add(network.network_address) # Convert set to count of unique networks summary[\\"IPv4\\"][\\"unique_networks\\"] = len(summary[\\"IPv4\\"][\\"unique_networks\\"]) summary[\\"IPv6\\"][\\"unique_networks\\"] = len(summary[\\"IPv6\\"][\\"unique_networks\\"]) return summary"},{"question":"# Objective To assess your understanding of Python\'s `logging.config` module, particularly in configuring loggers using dictionary schemas. # You are required to: Write a Python function named `configure_logging` that sets up logging based on a given dictionary configuration using the `logging.config.dictConfig()` method. # Function Signature ```python def configure_logging(config: dict) -> None: ``` # Input - `config`: A dictionary containing the logging configuration as per the schema described in the documentation. # Output - The function should not return anything. Instead it should set up the logger configuration as per the provided dictionary schema. # Constraints 1. The configuration dictionary will always have the key `version` set to 1. 2. The configuration dictionary may include custom formatters, handlers, and loggers. 3. Handle exceptions that may arise from configuring the logging for invalid configuration dictionaries. The function should print an appropriate error message for such exceptions. # Example ```python config_dict = { \'version\': 1, \'formatters\': { \'default\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', \'level\': \'INFO\', }, }, \'loggers\': { \'example_logger\': { \'handlers\': [\'console\'], \'level\': \'INFO\', \'propagate\': False, }, }, \'root\': { \'handlers\': [\'console\'], \'level\': \'WARNING\', } } def configure_logging(config): import logging.config try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\") # Example Usage configure_logging(config_dict) logger = logging.getLogger(\'example_logger\') logger.info(\'This is an info message\') # This should print to console logger.debug(\'This is a debug message\') # This should not print to console ``` # Note: - The function should handle any exceptions that arise from incorrect configurations by printing an error message. - You can assume that the configuration dictionaries provided to the function will follow Python\'s dictionary schema outlined in the documentation.","solution":"import logging.config def configure_logging(config: dict) -> None: Configures logging using a dictionary schema. Args: config (dict): The dictionary configuration for logging. Raises: Logs an error message if any error occurs during configuration. try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError, KeyError) as e: print(f\\"Error configuring logging: {e}\\")"},{"question":"**Objective:** Assess the student\'s understanding and ability to handle normalization layers in PyTorch when dealing with issues related to in-place updates and using functorch\'s vectorized map (vmap). **Problem Statement:** You are given a PyTorch neural network model that uses BatchNorm2d layers. However, due to issues with inplace updates when using vectorized operations with functorch, you need to rewrite parts of the model to use GroupNorm instead of BatchNorm. Your task is to implement a function `convert_batchnorm_to_groupnorm` that takes a model and a specified number of groups for GroupNorm, and returns the modified model where all instances of BatchNorm2d are replaced with GroupNorm. Furthermore, you should ensure that the modified model does not use running stats. **Function Signature:** ```python import torch.nn as nn def convert_batchnorm_to_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: pass ``` **Input:** - `model`: A PyTorch `nn.Module` instance, where some layers are `BatchNorm2d`. - `num_groups`: An integer specifying the number of groups for `GroupNorm`. Ensure that the number of channels in any `BatchNorm2d` layer is divisible by `num_groups`. **Output:** - Returns the modified `nn.Module` instance where all `BatchNorm2d` layers are replaced with `GroupNorm` layers. **Constraints:** - You must ensure that all `BatchNorm2d` layers are replaced correctly, considering that the number of channels should be divisible by the specified number of groups. - Do not use running stats in the GroupNorm layers. **Example:** ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.fc = nn.Linear(32 * 28 * 28, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) x = x.view(x.size(0), -1) x = self.fc(x) return x model = SimpleModel() num_groups = 4 modified_model = convert_batchnorm_to_groupnorm(model, num_groups) # Ensure the modified model has GroupNorm layers instead of BatchNorm2d for layer in modified_model.modules(): if isinstance(layer, nn.BatchNorm2d): print(\\"Test Failed: Model still contains BatchNorm2d layer.\\") elif isinstance(layer, nn.GroupNorm): print(\\"GroupNorm layer found with {} groups.\\".format(layer.num_groups)) ``` **Notes:** - It is essential to verify that the number of groups specified is valid for all `BatchNorm2d` layers being replaced. - The function must handle the modification in place and preserve the overall structure and functionality of the original model.","solution":"import torch.nn as nn def convert_batchnorm_to_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: Convert all BatchNorm2d layers in the model to GroupNorm layers. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features if num_channels % num_groups != 0: raise ValueError(f\\"The number of channels {num_channels} is not divisible by {num_groups}\\") group_norm = nn.GroupNorm(num_groups, num_channels) setattr(model, name, group_norm) else: convert_batchnorm_to_groupnorm(module, num_groups) return model"},{"question":"# Python Version Decoder Python uses a specific encoding scheme for its version numbers. The version string like \\"3.4.1a2\\" can be represented using a hexadecimal number. Your task is to write a function that decodes a given hexadecimal version number into its component parts. Input: - A single integer representing the hex-encoded version number. Output: - A dictionary with the following keys and their corresponding values extracted from the input integer: - `major`: The major version (e.g. `3` from \\"3.4.1a2\\"). - `minor`: The minor version (e.g. `4` from \\"3.4.1a2\\"). - `micro`: The micro version (e.g. `1` from \\"3.4.1a2\\"). - `release_level`: A string indicating the release level. This should be one of `\\"alpha\\"`, `\\"beta\\"`, `\\"release_candidate\\"`, or `\\"final\\"` corresponding to the encoded values `0xA`, `0xB`, `0xC`, and `0xF`, respectively. - `release_serial`: The release serial (e.g. `2` from \\"3.4.1a2\\"). Constraints: - The input integer will always be a valid 32-bit encoded version number. Example: ```python def decode_python_version(hex_version): # Your code here # Example usage: hex_version = 0x030401a2 decoded_version = decode_python_version(hex_version) print(decoded_version) ``` Expected Output: ```python { \\"major\\": 3, \\"minor\\": 4, \\"micro\\": 1, \\"release_level\\": \\"alpha\\", \\"release_serial\\": 2 } ``` Notes: - You might find bitwise operations and masking useful. - Pay attention to how the bits are arranged in the version encoding.","solution":"def decode_python_version(hex_version): Decodes a hex-encoded Python version number into its component parts. Args: hex_version (int): The hex-encoded version number. Returns: dict: A dictionary with keys \'major\', \'minor\', \'micro\', \'release_level\', \'release_serial\'. # Extracting each component using bitwise operations major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level_code = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF # Mapping release_level_code to release level string release_level_map = { 0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"release_candidate\\", 0xF: \\"final\\" } release_level = release_level_map.get(release_level_code, \\"unknown\\") return { \\"major\\": major, \\"minor\\": minor, \\"micro\\": micro, \\"release_level\\": release_level, \\"release_serial\\": release_serial }"},{"question":"In this task, you will implement functions to transform labels for multiclass and multilabel classifications using `scikit-learn` transformers provided in the documentation. You will write code for the following: 1. **Multiclass Label Binarization**: Transform a list of multiclass labels into a binary matrix using `LabelBinarizer`. 2. **Multilabel Binarization**: Transform a list of sets of labels into a binary matrix using `MultiLabelBinarizer`. 3. **Label Encoding and Decoding**: Normalize labels to values between 0 and n_classes - 1 using `LabelEncoder` and retrieve the original labels. # Requirements 1. Implement the following functions in Python: - `multiclass_label_binarization(labels: List[int]) -> Tuple[List[int], np.ndarray]` - `multilabel_binarization(labels: List[Set[int]]) -> np.ndarray` - `label_encoding_decoding(labels: List[Union[int, str]]) -> Tuple[np.ndarray, List[Union[int, str]]]` 2. Each function should work as described below: Function 1: `multiclass_label_binarization` - **Input**: A list of integers representing multiclass labels, e.g., `[1, 2, 6, 4, 2]`. - **Output**: A tuple containing an array of unique classes and the binary matrix obtained by binarizing the input labels. - **Example**: ```python labels = [1, 2, 6, 4, 2] unique_classes, binary_matrix = multiclass_label_binarization(labels) # unique_classes should be [1, 2, 4, 6] # binary_matrix should be: # [[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 1, 0], # [0, 0, 0, 1], # [0, 1, 0, 0]] ``` Function 2: `multilabel_binarization` - **Input**: A list of sets containing integer labels for multilabel classification, e.g., `[{2, 3, 4}, {2}, {0, 1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2}]`. - **Output**: A binary matrix obtained by binarizing the input multilabels. - **Example**: ```python labels = [{2, 3, 4}, {2}, {0, 1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2}] binary_matrix = multilabel_binarization(labels) # binary_matrix should be: # [[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]] ``` Function 3: `label_encoding_decoding` - **Input**: A list of labels, either integers or strings, e.g., `[\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"]`. - **Output**: A tuple containing the array of labels encoded to numeric values and the list of decoded labels back to their original form. - **Example**: ```python labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded_labels, decoded_labels = label_encoding_decoding(labels) # encoded_labels should be [1, 1, 2, 0] # decoded_labels should be [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] ``` # Constraints - You may assume the input list always contains valid labels. - The input labels for each function will not contain any missing values. - The order of the labels in the output must match the order in the input. # Libraries - Use `scikit-learn` for label transformation. - Use `numpy` for array manipulation if necessary. # Performance - The solution should efficiently handle input lists with size up to 10,000 labels. # Submission Submit your implementation as a `.py` file containing the three functions mentioned above.","solution":"from typing import List, Set, Tuple, Union import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def multiclass_label_binarization(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]: lb = LabelBinarizer() binary_matrix = lb.fit_transform(labels) unique_classes = lb.classes_ return unique_classes, binary_matrix def multilabel_binarization(labels: List[Set[int]]) -> np.ndarray: mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(labels) return binary_matrix def label_encoding_decoding(labels: List[Union[int, str]]) -> Tuple[np.ndarray, List[Union[int, str]]]: le = LabelEncoder() encoded_labels = le.fit_transform(labels) decoded_labels = le.inverse_transform(encoded_labels) return encoded_labels, decoded_labels.tolist()"},{"question":"# Out-of-Core Learning with scikit-learn Objective Implement an out-of-core learning system for text classification using the `HashingVectorizer` for feature extraction and the `SGDClassifier` for incremental learning. The system should handle datasets that cannot fit into memory at once by processing data in mini-batches. Input - `data_stream`: A generator function that yields batches of text documents (list of strings). - `target_stream`: A generator function that yields batches of target labels (list of integers). - `mini_batch_size`: An integer specifying the number of examples per mini-batch. - `num_batches`: An integer specifying the number of batches to process. - `classes`: A list of all possible target class labels. Output - The final accuracy score of the `SGDClassifier` on the last mini-batch processed. Constraints - You may not load the entire dataset into memory at once. - You must use `HashingVectorizer` for feature extraction and `SGDClassifier` for incremental learning using `partial_fit`. - Each mini-batch must have the same `mini_batch_size`. Performance Requirements - Your implementation should handle large datasets efficiently, only keeping a small portion of data (one mini-batch) in memory at any given time. Function Signature ```python def out_of_core_text_classification(data_stream, target_stream, mini_batch_size, num_batches, classes): # Your code here ``` # Example Usage ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def data_stream(): # Simulates streaming of text data texts = [ [\\"document one text\\", \\"document two text\\", \\"document three text\\"], [\\"document four text\\", \\"document five text\\"], [\\"document six text\\"] ] for batch in texts: yield batch def target_stream(): # Simulates streaming of text labels labels = [ [0, 1, 0], [1, 0], [0] ] for batch in labels: yield batch mini_batch_size = 3 num_batches = 3 classes = [0, 1] accuracy = out_of_core_text_classification(data_stream(), target_stream(), mini_batch_size, num_batches, classes) print(f\\"Final Accuracy: {accuracy}\\") ``` In this example, the `out_of_core_text_classification` function processes text data and labels in batches and returns the accuracy score of the `SGDClassifier` on the last mini-batch. Notes - The `partial_fit` method should be called with the `classes` parameter during the first call to ensure the classifier is aware of all possible classes. - Ensure that the vectorization and classification steps are efficiently handled by only keeping the data and model for the current mini-batch in memory.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def out_of_core_text_classification(data_stream, target_stream, mini_batch_size, num_batches, classes): vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() for i in range(num_batches): texts = next(data_stream) labels = next(target_stream) X_train = vectorizer.transform(texts) y_train = labels if i == 0: classifier.partial_fit(X_train, y_train, classes=classes) else: classifier.partial_fit(X_train, y_train) final_batch = X_train final_labels = y_train predictions = classifier.predict(final_batch) accuracy = accuracy_score(final_labels, predictions) return accuracy"},{"question":"**Question: Implement a Bzip2 Compression and Decompression Manager** You are tasked with implementing a `Bzip2Manager` class. This class will handle bzip2 compression and decompression of data, and provide an interface to easily compress and decompress text data both incrementally and all at once. You will need to implement the following methods: 1. **`compress_data(data: bytes, compresslevel: int = 9) -> bytes`**: - This method takes a bytes-like object and compresses it at the given compression level (default is 9). - It returns the compressed data as bytes. 2. **`decompress_data(data: bytes) -> bytes`**: - This method takes a bytes-like object that represents compressed data and decompresses it. - It returns the decompressed data as bytes. 3. **`compress_file(input_path: str, output_path: str, compresslevel: int = 9)`**: - This method reads the content of the file specified by `input_path`, compresses it using the given compression level (default is 9), and writes the compressed data to the file specified by `output_path`. - Ensure that the output file is properly closed after writing. 4. **`decompress_file(input_path: str, output_path: str)`**: - This method reads the content of the file specified by `input_path`, decompresses it, and writes the decompressed data to the file specified by `output_path`. - Ensure that the output file is properly closed after writing. 5. **`compress_incrementally(chunks: list[bytes], compresslevel: int = 9) -> bytes`**: - This method accepts a list of bytes-like objects (`chunks`) and compresses them incrementally using the given compression level (default is 9). - The method returns the concatenated compressed data as bytes. 6. **`decompress_incrementally(chunks: list[bytes]) -> bytes`**: - This method accepts a list of bytes-like objects (`chunks`) that represent compressed data and decompresses them incrementally. - The method returns the concatenated decompressed data as bytes. # Constraints: - Your implementation should handle cases of empty inputs and files gracefully. - You are not allowed to use any other external library except `bz2`. # Example Usage ```python bz_manager = Bzip2Manager() # One-shot compression and decompression compressed_data = bz_manager.compress_data(b\\"Hello, World!\\") decompressed_data = bz_manager.decompress_data(compressed_data) assert b\\"Hello, World!\\" == decompressed_data # File-based compression and decompression bz_manager.compress_file(\\"input.txt\\", \\"output.bz2\\") bz_manager.decompress_file(\\"output.bz2\\", \\"decompressed.txt\\") # Incremental compression and decompression chunks = [b\\"chunk1\\", b\\"chunk2\\", b\\"chunk3\\"] compressed_chunks = bz_manager.compress_incrementally(chunks) decompressed_chunks = bz_manager.decompress_incrementally([compressed_chunks]) assert b\\"chunk1chunk2chunk3\\" == decompressed_chunks ``` **Deliverables:** - Implement the `Bzip2Manager` class and its methods as described. - Ensure you test each method to verify its correctness.","solution":"import bz2 class Bzip2Manager: @staticmethod def compress_data(data: bytes, compresslevel: int = 9) -> bytes: return bz2.compress(data, compresslevel) @staticmethod def decompress_data(data: bytes) -> bytes: return bz2.decompress(data) @staticmethod def compress_file(input_path: str, output_path: str, compresslevel: int = 9): with open(input_path, \'rb\') as file_in: data = file_in.read() compressed_data = Bzip2Manager.compress_data(data, compresslevel) with open(output_path, \'wb\') as file_out: file_out.write(compressed_data) @staticmethod def decompress_file(input_path: str, output_path: str): with open(input_path, \'rb\') as file_in: compressed_data = file_in.read() decompressed_data = Bzip2Manager.decompress_data(compressed_data) with open(output_path, \'wb\') as file_out: file_out.write(decompressed_data) @staticmethod def compress_incrementally(chunks: list[bytes], compresslevel: int = 9) -> bytes: compressor = bz2.BZ2Compressor(compresslevel) compressed_data = b\'\' for chunk in chunks: compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() return compressed_data @staticmethod def decompress_incrementally(chunks: list[bytes]) -> bytes: decompressor = bz2.BZ2Decompressor() decompressed_data = b\'\' for chunk in chunks: decompressed_data += decompressor.decompress(chunk) return decompressed_data"},{"question":"Objective This task requires you to create a function that utilizes the `ipaddress` library to perform a series of network-related calculations and return specific results. Testing your function will ensure a thorough comprehension of handling IP addresses and networks in Python. Problem Statement You are tasked with writing a function `network_summary(input_network: str, ip_range: tuple) -> dict` that performs the following: 1. Parse the `input_network`, which is a string representing an IP network (either IPv4 or IPv6). 2. Compute the number of usable host addresses in the given network. 3. Check if the provided `ip_range`, a tuple of two IP addresses (either IPv4 or IPv6) represented as strings, completely falls within the parsed network. 4. Return a dictionary containing: - `\\"num_usable_hosts\\"`: Total number of usable host addresses within the network. - `\\"range_in_network\\"`: A boolean indicating whether the `ip_range` falls within the network. - `\\"summarized_range\\"`: A list of network definitions summarizing the range between `ip_range[0]` and `ip_range[1]`. Function Signature ```python def network_summary(input_network: str, ip_range: (str, str)) -> dict: pass ``` Constraints - You may only use the Python Standard Library. - The `input_network` will be a valid IPv4 or IPv6 network string in CIDR notation. - The IP addresses in `ip_range` will be valid addresses of the same type (both IPv4 or both IPv6). - The first address in `ip_range` will always be less than or equal to the second address. Example ```python network = \'192.168.1.0/24\' ip_range = (\'192.168.1.10\', \'192.168.1.20\') result = network_summary(network, ip_range) print(result) ``` **Sample Output** ```python { \\"num_usable_hosts\\": 254, \\"range_in_network\\": True, \\"summarized_range\\": [\'192.168.1.10/31\', \'192.168.1.12/30\', \'192.168.1.16/30\', \'192.168.1.20/32\'] } ``` Notes - The number of usable hosts excludes the network address and the broadcast address for IPv4 networks (except for /31, /32, /127, /128 prefix lengths). - The `summarized_range` should contain the smallest number of network definitions that cover the entire IP range. --- Your task is to implement the function `network_summary` as specified above.","solution":"import ipaddress def network_summary(input_network: str, ip_range: (str, str)) -> dict: network = ipaddress.ip_network(input_network) ip_start = ipaddress.ip_address(ip_range[0]) ip_end = ipaddress.ip_address(ip_range[1]) # Number of usable hosts num_usable_hosts = (network.num_addresses - 2) if network.prefixlen < (network.max_prefixlen - 1) else network.num_addresses # Range in network range_in_network = ip_start in network and ip_end in network # Summarized range summarized_range = [str(subnet) for subnet in ipaddress.summarize_address_range(ip_start, ip_end)] return { \\"num_usable_hosts\\": num_usable_hosts, \\"range_in_network\\": range_in_network, \\"summarized_range\\": summarized_range }"},{"question":"**Python 3.10 Coding Assessment Question** # Objective The objective of this assessment is to evaluate your ability to implement a function using Python 3.10\'s new pattern matching feature, as well as your ability to work with dictionaries and perform string manipulations. # Task You are required to write a function named `process_command` that processes a command string and performs different actions based on the command using Python 3.10\'s structural pattern matching (introduced in PEP 634). # Specifications 1. **Input:** * The function receives a single input parameter `command` which is a string. * The command string can be one of the following: - `\\"add <key> <value>\\"`: Add a key-value pair to an internal dictionary. - `\\"remove <key>\\"`: Remove a key from the dictionary. - `\\"update <key> <new_value>\\"`: Update the value of an existing key in the dictionary. - `\\"get <key>\\"`: Retrieve the value associated with the key. - `\\"list\\"`: Return all the key-value pairs in the dictionary as a list of tuples. 2. **Output:** * Based on the command, the function should return the appropriate result: - For `\\"add <key> <value>\\"`, return a message `\\"Added key <key> with value <value>\\"`. - For `\\"remove <key>\\"`, return a message `\\"Removed key <key>\\"`. - For `\\"update <key> <new_value>\\"`, return a message `\\"Updated key <key> to new value <new_value>\\"`. - For `\\"get <key>\\"`, return the value associated with the key or `\\"Key <key> not found\\"` if the key does not exist. - For `\\"list\\"`, return a list of tuples representing the key-value pairs in the dictionary. 3. **Constraints:** * The keys and values in the commands are alphanumeric strings. * All add, remove, update, and get commands are to be processed on an internal dictionary that your function should maintain across multiple calls if needed. # Example ```python def process_command(command: str): # Your implementation goes here # Example usage: print(process_command(\\"add name Alice\\")) # Output: \\"Added key name with value Alice\\" print(process_command(\\"add age 30\\")) # Output: \\"Added key age with value 30\\" print(process_command(\\"get name\\")) # Output: \\"Alice\\" print(process_command(\\"update age 31\\")) # Output: \\"Updated key age to new value 31\\" print(process_command(\\"get age\\")) # Output: \\"31\\" print(process_command(\\"remove name\\")) # Output: \\"Removed key name\\" print(process_command(\\"list\\")) # Output: [(\\"age\\", \\"31\\")] print(process_command(\\"get name\\")) # Output: \\"Key name not found\\" ``` # Notes You must use Python 3.10\'s pattern matching feature to implement this function. Make sure to handle potential edge cases, such as attempting to update or remove a non-existent key.","solution":"def process_command(command: str): store = process_command.store if hasattr(process_command, \'store\') else {} match command.split(): case [\\"add\\", key, value]: store[key] = value process_command.store = store return f\\"Added key {key} with value {value}\\" case [\\"remove\\", key]: if key in store: del store[key] process_command.store = store return f\\"Removed key {key}\\" else: return f\\"Key {key} not found\\" case [\\"update\\", key, new_value]: if key in store: store[key] = new_value process_command.store = store return f\\"Updated key {key} to new value {new_value}\\" else: return f\\"Key {key} not found\\" case [\\"get\\", key]: return store.get(key, f\\"Key {key} not found\\") case [\\"list\\"]: return list(store.items()) case _: return \\"Invalid command\\""},{"question":"# Compression and Decompression Pipeline Objective Design a function that takes a large dataset as input, compresses it using `zlib` with different compression settings, and then decompresses it to verify the integrity of the decompressed data. The function should handle any potential errors in the compression/decompression process. Task You are tasked with creating two main functions to achieve the compression-decompression pipeline: 1. **compress_data(data: bytes, level: int = zlib.Z_DEFAULT_COMPRESSION, use_dict: bool = False) -> bytes** - **Input:** - `data` (bytes): The data to be compressed. - `level` (int): Compression level from 0 (no compression) to 9 (maximum compression). Default is `zlib.Z_DEFAULT_COMPRESSION`. - `use_dict` (bool): Flag indicating whether to use a predefined compression dictionary. Default is `False`. - **Output:** - `compressed_data` (bytes): The compressed data. - **Description:** - Compress the input data using `zlib.compressobj`. - If `use_dict` is `True`, create a predefined dictionary and use it during compression. 2. **decompress_data(compressed_data: bytes, use_dict: bool = False) -> bytes** - **Input:** - `compressed_data` (bytes): The data to be decompressed. - `use_dict` (bool): Flag indicating whether to use a predefined compression dictionary. Default is `False`. - **Output:** - `decompressed_data` (bytes): The decompressed data. - **Description:** - Decompress the input data using `zlib.decompressobj`. - If `use_dict` is `True`, use the same predefined dictionary that was used during compression. Requirements - Ensure the decompressed data matches the original data. If the data does not match, raise a `zlib.error` exception. - Handle `zlib.error` exceptions and provide meaningful error messages. - Use appropriate buffer sizes and other parameters to optimize the compression and decompression processes. Constraints - The input data size can be very large, so make sure the functions are memory efficient. - The compression level should be adjustable. Example ```python import zlib def compress_data(data: bytes, level: int = zlib.Z_DEFAULT_COMPRESSION, use_dict: bool = False) -> bytes: try: # Implement the compression logic compressor = zlib.compressobj(level) compressed_data = compressor.compress(data) compressed_data += compressor.flush(zlib.Z_FINISH) except zlib.error as e: raise zlib.error(f\\"Compression failed: {e}\\") return compressed_data def decompress_data(compressed_data: bytes, use_dict: bool = False) -> bytes: try: # Implement the decompression logic decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(compressed_data) decompressed_data += decompressor.flush() if len(decompressed_data) != len(data): raise zlib.error(\\"Decompressed data mismatch with original data size\\") except zlib.error as e: raise zlib.error(f\\"Decompression failed: {e}\\") return decompressed_data # Example usage original_data = b\\"This is a test data to be compressed and decompressed using zlib.\\" compressed = compress_data(original_data, level=5) decompressed = decompress_data(compressed) assert decompressed == original_data print(\\"Compression and Decompression verified successfully\\") ``` Submission Submit your implementation of `compress_data` and `decompress_data` functions. Ensure your functions handle large datasets efficiently and correctly manage different compression settings and error cases.","solution":"import zlib def compress_data(data: bytes, level: int = zlib.Z_DEFAULT_COMPRESSION, use_dict: bool = False) -> bytes: try: # Create a compressor object with the specified compression level compressor = zlib.compressobj(level=level) compressed_data = compressor.compress(data) compressed_data += compressor.flush(zlib.Z_FINISH) except zlib.error as e: raise zlib.error(f\\"Compression failed: {e}\\") return compressed_data def decompress_data(compressed_data: bytes, use_dict: bool = False) -> bytes: try: # Create a decompressor object decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(compressed_data) decompressed_data += decompressor.flush() except zlib.error as e: raise zlib.error(f\\"Decompression failed: {e}\\") return decompressed_data"},{"question":"# Question: Optimizing Prediction Performance with Scikit-learn You have been given a task to optimize the performance of a machine learning model using scikit-learn. The optimization focuses on reducing prediction latency and increasing throughput while maintaining an acceptable level of accuracy. Follow the guidelines and constraints provided to achieve the desired performance improvements. Scenario Suppose you are working with a large dataset, `data.csv`, containing 100,000 instances and 1,000 features. Your task is to: 1. Build and optimize a predictive model using scikit-learn. 2. Implement and compare the performance in terms of prediction latency and throughput when using dense and sparse data representations. 3. Analyze the effect of model complexity on prediction latency and optimize accordingly. Requirements 1. **Data Preparation**: - Load the dataset `data.csv`. - Split the dataset into training (80%) and testing (20%) sets. 2. **Model Building**: - Train a linear model (`LinearRegression`) and a non-linear model (`RandomForestRegressor`). 3. **Optimization**: - Implement predictions in bulk mode and atomic mode. - Convert the data to a sparse format if the sparsity ratio is greater than 90%. - Optimize prediction latency by configuring scikit-learn to reduce validation overhead. 4. **Evaluation**: - Measure and compare the prediction latency and throughput for dense and sparse representations. - Analyze the effect of model complexity by varying regularization parameters for the linear model and the number of estimators for the random forest model. Code Implementation Implement the following steps in the form of functions: 1. **Data Loading and Splitting**: ```python def load_and_split_data(filepath): Load the dataset from the given filepath and split it into training and testing sets. Parameters: - filepath: str, path to the data file. Returns: - X_train, X_test, y_train, y_test: numpy arrays, split data. pass ``` 2. **Sparse Representation Conversion**: ```python def convert_to_sparse(X): Convert the given data to a sparse format if sparsity ratio > 90%. Parameters: - X: numpy array, input data. Returns: - X_sparse: scipy sparse matrix, converted sparse data. pass ``` 3. **Model Training**: ```python def train_models(X_train, y_train): Train a linear model and a non-linear model. Parameters: - X_train: numpy array, training features. - y_train: numpy array, training labels. Returns: - lin_model: trained LinearRegression model. - rf_model: trained RandomForestRegressor model. pass ``` 4. **Prediction and Optimization**: ```python def measure_prediction_performance(models, X_test, atomic_mode=True): Measure prediction performance in terms of latency and throughput. Parameters: - models: list, trained models. - X_test: numpy array/scipy sparse matrix, testing features. - atomic_mode: bool, flag to indicate atomic mode or bulk mode. Returns: - performance_metrics: dict, containing latency and throughput metrics. pass ``` 5. **Main Function**: ```python def main(filepath): Main function to orchestrate loading data, training models, and measuring performance. Parameters: - filepath: str, path to the data file. Returns: - None # Load and split data X_train, X_test, y_train, y_test = load_and_split_data(filepath) # Convert to sparse if applicable X_train_sparse = convert_to_sparse(X_train) X_test_sparse = convert_to_sparse(X_test) # Train models lin_model, rf_model = train_models(X_train, y_train) # Measure performance for dense data dense_performance = measure_prediction_performance([lin_model, rf_model], X_test, atomic_mode=False) # Measure performance for sparse data sparse_performance = measure_prediction_performance([lin_model, rf_model], X_test_sparse, atomic_mode=False) # Output the results print(\\"Dense Performance:\\", dense_performance) print(\\"Sparse Performance:\\", sparse_performance) # Run main function with the provided data file main(\\"data.csv\\") ``` Constraints - Use optimal BLAS/LAPACK libraries for best performance. - Ensure the scikit-learn configuration is set to reduce validation overhead during prediction. - Focus on maintaining a balance between prediction latency and accuracy. Provide the complete implementation of the required functions. Ensure your code is efficient and well-documented.","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor import time def load_and_split_data(filepath): Load the dataset from the given filepath and split it into training and testing sets. Parameters: - filepath: str, path to the data file. Returns: - X_train, X_test, y_train, y_test: numpy arrays, split data. data = pd.read_csv(filepath) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def convert_to_sparse(X): Convert the given data to a sparse format if sparsity ratio > 90%. Parameters: - X: numpy array, input data. Returns: - X_sparse: scipy sparse matrix, converted sparse data. sparsity_ratio = np.mean(X == 0) if sparsity_ratio > 0.9: return csr_matrix(X) return X def train_models(X_train, y_train): Train a linear model and a non-linear model. Parameters: - X_train: numpy array, training features. - y_train: numpy array, training labels. Returns: - lin_model: trained LinearRegression model. - rf_model: trained RandomForestRegressor model. lin_model = LinearRegression() lin_model.fit(X_train, y_train) rf_model = RandomForestRegressor(n_estimators=100, random_state=42) rf_model.fit(X_train, y_train) return lin_model, rf_model def measure_prediction_performance(models, X_test, atomic_mode=True): Measure prediction performance in terms of latency and throughput. Parameters: - models: list, trained models. - X_test: numpy array/scipy sparse matrix, testing features. - atomic_mode: bool, flag to indicate atomic mode or bulk mode. Returns: - performance_metrics: dict, containing latency and throughput metrics. performance_metrics = {} for model in models: start_time = time.time() if atomic_mode: for i in range(X_test.shape[0]): model.predict(X_test[i].reshape(1, -1)) else: model.predict(X_test) end_time = time.time() latency = (end_time - start_time) / X_test.shape[0] throughput = X_test.shape[0] / (end_time - start_time) performance_metrics[model.__class__.__name__] = { \\"latency\\": latency, \\"throughput\\": throughput } return performance_metrics def main(filepath): Main function to orchestrate loading data, training models, and measuring performance. Parameters: - filepath: str, path to the data file. Returns: - None # Load and split data X_train, X_test, y_train, y_test = load_and_split_data(filepath) # Convert to sparse if applicable X_train_sparse = convert_to_sparse(X_train) X_test_sparse = convert_to_sparse(X_test) # Train models lin_model, rf_model = train_models(X_train, y_train) # Measure performance for dense data dense_performance = measure_prediction_performance([lin_model, rf_model], X_test, atomic_mode=False) # Measure performance for sparse data sparse_performance = measure_prediction_performance([lin_model, rf_model], X_test_sparse, atomic_mode=False) # Output the results print(\\"Dense Performance:\\", dense_performance) print(\\"Sparse Performance:\\", sparse_performance) # Run main function with the provided data file if __name__ == \\"__main__\\": main(\\"data.csv\\")"},{"question":"Objective: Implement a function that enhances the volume of a stereo audio fragment by a given factor for both left and right channels independently. The audio fragment can have samples with widths of 1, 2, 3, or 4 bytes. Requirements: 1. **Function signature:** ```python def enhance_stereo_volume(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: ``` 2. **Inputs:** - `fragment`: - Type: `bytes` - Description: A stereo audio fragment represented as bytes. - Constraint: The length of `fragment` is even and corresponds to pairs of left and right channel samples. - `width`: - Type: `int` - Description: The width (in bytes) of each sample, valid values are 1, 2, 3, or 4. - `lfactor`: - Type: `float` - Description: The enhancement factor for the left channel (e.g., `1.5` to increase volume by 50%). - `rfactor`: - Type: `float` - Description: The enhancement factor for the right channel (e.g., `0.8` to decrease volume by 20%). 3. **Output:** - Return: - Type: `bytes` - Description: The audio fragment with enhanced volume for both channels. 4. **Constraints:** - The function must use `audioop` module for intermediate audio operations. - Overflow in sample values should be properly truncated. - Performance should handle fragments up to 10MB in size efficiently. 5. **Example:** ```python example_fragment = b\'x00x01x02x03x04x05x06x07\' width = 2 lfactor = 1.5 rfactor = 0.8 result = enhance_stereo_volume(example_fragment, width, lfactor, rfactor) print(result) # Expected result: Enhanced volume of the given audio fragment in bytes ``` Hint: Consider using `audioop.mul` and `audioop.tostereo` to separate and recombine the channels after processing. Note: You may assume that the input `fragment` always represents a valid stereo audio sample fragment.","solution":"import audioop def enhance_stereo_volume(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: Enhances the volume of a stereo audio fragment for left and right channels independently. Parameters: - fragment (bytes): Stereo audio fragment. - width (int): Width of each sample in bytes (1, 2, 3, or 4). - lfactor (float): Enhancement factor for the left channel. - rfactor (float): Enhancement factor for the right channel. Returns: - bytes: The audio fragment with enhanced volume for both channels. # Separate the left and right channels left_channel = audioop.tomono(fragment, width, 1, 0) right_channel = audioop.tomono(fragment, width, 0, 1) # Adjust the volume for each channel left_channel = audioop.mul(left_channel, width, lfactor) right_channel = audioop.mul(right_channel, width, rfactor) # Recombine the channels into one stereo fragment enhanced_fragment = audioop.tostereo(left_channel, width, 0.5, 0.5) enhanced_fragment = audioop.add( enhanced_fragment, audioop.tostereo(right_channel, width, 0.5, 0.5), width) return enhanced_fragment"},{"question":"Implementing a Custom Data Structure for Project Management You have been asked to implement a custom data structure to manage tasks in a project. Each task has the following attributes: - `task_id` (int): A unique identifier for the task. - `task_name` (str): Name of the task. - `priority` (int): Priority of the task (lower number means higher priority). - `completed` (bool): Status of the task (True if task is completed, else False). You need to implement a class `ProjectManager` with the following methods: 1. `add_task(task_id: int, task_name: str, priority: int) -> None`: Adds a new task to the project. 2. `remove_task(task_id: int) -> None`: Removes a task by its task_id. 3. `mark_task_completed(task_id: int) -> None`: Marks a specific task as completed. 4. `get_task(task_id: int) -> dict`: Returns the task details as a dictionary with keys `task_id`, `task_name`, `priority`, and `completed`. 5. `get_all_tasks() -> list`: Returns a list of all tasks, sorted by priority (highest to lowest). 6. `get_completed_tasks() -> list`: Returns a list of completed tasks. # Constraints: - The method `add_task` should raise a `ValueError` if a task with the same `task_id` is already present. - The method `remove_task` should raise a `KeyError` if a task with the given `task_id` does not exist. - The method `mark_task_completed` should raise a `KeyError` if a task with the given `task_id` does not exist. # Expected Input and Output formats: - Methods will be called with the following approximate sequence: ```python manager.add_task(1, \\"Design Phase\\", 2) manager.add_task(2, \\"Implementation\\", 1) manager.mark_task_completed(2) assert manager.get_task(1) == {\'task_id\': 1, \'task_name\': \'Design Phase\', \'priority\': 2, \'completed\': False} assert manager.get_completed_tasks() == [{\'task_id\': 2, \'task_name\': \'Implementation\', \'priority\': 1, \'completed\': True}] manager.remove_task(1) assert manager.get_all_tasks() == [{\'task_id\': 2, \'task_name\': \'Implementation\', \'priority\': 1, \'completed\': True}] ``` # Performance Requirements: - The structure should efficiently support multiple additions, removals, and retrievals, with reasonable time complexity. Implement the `ProjectManager` class as specified above: ```python class ProjectManager: def __init__(self): # Initialize your data structures here pass def add_task(self, task_id: int, task_name: str, priority: int) -> None: # Add the task with the given task_id, task_name, and priority pass def remove_task(self, task_id: int) -> None: # Remove the task with the given task_id from the project pass def mark_task_completed(self, task_id: int) -> None: # Mark the task with the given task_id as completed pass def get_task(self, task_id: int) -> dict: # Retrieve and return the task details as a dictionary pass def get_all_tasks(self) -> list: # Retrieve and return a list of all tasks sorted by priority pass def get_completed_tasks(self) -> list: # Retrieve and return a list of all completed tasks pass ```","solution":"class ProjectManager: def __init__(self): self.tasks = {} def add_task(self, task_id: int, task_name: str, priority: int) -> None: if task_id in self.tasks: raise ValueError(\\"Task with the same task_id already exists\\") self.tasks[task_id] = { \'task_id\': task_id, \'task_name\': task_name, \'priority\': priority, \'completed\': False } def remove_task(self, task_id: int) -> None: if task_id not in self.tasks: raise KeyError(\\"Task with the given task_id does not exist\\") del self.tasks[task_id] def mark_task_completed(self, task_id: int) -> None: if task_id not in self.tasks: raise KeyError(\\"Task with the given task_id does not exist\\") self.tasks[task_id][\'completed\'] = True def get_task(self, task_id: int) -> dict: if task_id not in self.tasks: raise KeyError(\\"Task with the given task_id does not exist\\") return self.tasks[task_id] def get_all_tasks(self) -> list: return sorted(self.tasks.values(), key=lambda x: x[\'priority\']) def get_completed_tasks(self) -> list: return [task for task in self.tasks.values() if task[\'completed\']]"},{"question":"Nested Tensors in PyTorch Objective The objective of this assessment is to evaluate your understanding of nested tensors in PyTorch, focusing on their construction, manipulation, operation, and conversion. Problem Statement You are given a dataset containing sequences of variable lengths. Your task is to implement a function that processes this dataset using nested tensors. Specifically, the function should: 1. Construct a nested tensor from a list of 1D tensors (each tensor representing a sequence). 2. Perform a specified operation on this nested tensor. 3. Convert the nested tensor to a padded tensor with a specific padding value. 4. Implement a custom operation for nested tensors that is not directly supported by the standard PyTorch operations. Function Signature ```python import torch def process_variable_length_sequences(sequences: list, operation: str, padding_value: float) -> torch.Tensor: Args: sequences (list of torch.Tensor): A list of 1D tensors, each of different lengths. operation (str): The operation to perform on the nested tensor. Supported operations: \'sum\', \'mean\'. padding_value (float): The padding value to use when converting the nested tensor to a padded tensor. Returns: torch.Tensor: A padded tensor after performing the specified operation. # Your implementation here ``` Explanation 1. **Input**: - `sequences` is a list of 1D tensors, where each tensor has a variable length. - `operation` specifies the operation to perform on the nested tensor. Currently, supported operations are: - `\'sum\'`: Sum all elements in each sequence. - `\'mean\'`: Compute the mean of each sequence. - `padding_value` is a float value used for padding when converting the nested tensor to a padded tensor. 2. **Output**: - The function should return a padded tensor after performing the specified operation on the nested tensor. Steps to Implement 1. **Construction**: Create a nested tensor from the list of sequences. 2. **Operation**: Apply the specified operation (\'sum\' or \'mean\') to the nested tensor. 3. **Conversion**: Convert the resulting nested tensor to a padded tensor using the provided padding value. Example ```python import torch sequences = [torch.tensor([3, 1, 2]), torch.tensor([0, 4]), torch.tensor([5, 6, 7, 8])] operation = \'mean\' padding_value = -1.0 result = process_variable_length_sequences(sequences, operation, padding_value) print(result) ``` Expected Output: ``` tensor([[ 2.0000, -1.0000, -1.0000, -1.0000], [ 2.0000, -1.0000, -1.0000, -1.0000], [ 6.5000, -1.0000, -1.0000, -1.0000]]) ``` Constraints - Each tensor in the `sequences` list must be a 1D tensor. - You must handle any errors that arise due to incompatible shapes or unsupported operations gracefully by raising an appropriate exception with a clear error message. Additional Task - Implement and test a new custom operation `scale` which multiplies each element by a given scaling factor for the nested tensor. Integrate this custom operation into the existing function `process_variable_length_sequences`. You are encouraged to write clean, efficient, and well-documented code to demonstrate your understanding of nested tensors in PyTorch.","solution":"import torch from torch.nn.utils.rnn import pad_sequence def process_variable_length_sequences(sequences: list, operation: str, padding_value: float) -> torch.Tensor: Args: sequences (list of torch.Tensor): A list of 1D tensors, each of different lengths. operation (str): The operation to perform on the nested tensor. Supported operations: \'sum\', \'mean\', \'scale\'. padding_value (float): The padding value to use when converting the nested tensor to a padded tensor. Returns: torch.Tensor: A padded tensor after performing the specified operation. # Validate sequences input if not all(isinstance(seq, torch.Tensor) and seq.ndim == 1 for seq in sequences): raise ValueError(\\"All elements in sequences must be 1D tensors.\\") # Create the nested tensor. Using a list for initial possibility of different sizes. nested_tensor = sequences # Perform the specified operation result = [] if operation == \'sum\': result = [torch.sum(seq) for seq in nested_tensor] elif operation == \'mean\': result = [torch.mean(seq.float()) for seq in nested_tensor] else: raise ValueError(\\"Unsupported operation. Supported operations: \'sum\', \'mean\'\\") result_tensor = torch.tensor(result) # Calculate lengths to pad to the maximum length max_length = max(map(len, sequences)) # Convert nested tensor to a padded tensor with padding_value padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=padding_value) return padded_sequences # Additional task: Implement and test a new custom operation `scale` which multiplies each element by a given scaling factor. def process_scaled_sequences(sequences: list, scale_factor: float, padding_value: float) -> torch.Tensor: Args: sequences (list of torch.Tensor): A list of 1D tensors, each of different lengths. scale_factor (float): The factor by which to scale each element of the sequences. padding_value (float): The padding value to use when converting the nested tensor to a padded tensor. Returns: torch.Tensor: A padded tensor after scaling each element of the nested tensor. # Validate sequences input if not all(isinstance(seq, torch.Tensor) and seq.ndim == 1 for seq in sequences): raise ValueError(\\"All elements in sequences must be 1D tensors.\\") # Scale each sequence scaled_sequences = [seq * scale_factor for seq in sequences] # Pad sequences to create a padded tensor with padding_value padded_scaled_sequences = pad_sequence(scaled_sequences, batch_first=True, padding_value=padding_value) return padded_scaled_sequences"},{"question":"**Objective:** The task requires you to demonstrate your understanding of asynchronous I/O operations and socket programming using Python. You will be required to implement a basic asynchronous TCP echo server using the `asyncio` and `socket` modules. # Task: Implement a Python program that runs an asynchronous TCP echo server. The server must accept multiple clients and echo back any message received from a client to the same client. # Requirements: - Use the `asyncio` module to handle asynchronous connections. - The server should handle multiple clients concurrently. - For each client, the server should read data sent by the client and send exactly the same data back to the client. # Constraints: - The server should run on localhost (`127.0.0.1`) and port `8888`. - The server should handle and respond to each client independently and concurrently. - Each incoming message might be up to 1024 bytes in length. # Input and Output format: - **Input**: Will be data sent by a client connected to the server. - **Output**: The server should echo back the exact same data received from the client. # Example: Assume three clients connect to the server and send the following messages: - Client 1 sends: \\"Hello, Server!\\" - Client 2 sends: \\"Python is awesome\\" - Client 3 sends: \\"Asyncio is powerful\\" The server should respond: - To Client 1 with: \\"Hello, Server!\\" - To Client 2 with: \\"Python is awesome\\" - To Client 3 with: \\"Asyncio is powerful\\" # Implementation Template: ```python import asyncio async def handle_client(reader, writer): # Implement the client handling logic here pass async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) ``` # Instructions: 1. Define the `handle_client` coroutine to handle incoming client connections. 2. The `handle_client` coroutine should read data from the client, and send back the same data to that client. 3. Ensure that the server can accept and handle multiple client connections simultaneously. Good luck!","solution":"import asyncio async def handle_client(reader, writer): while True: data = await reader.read(1024) if not data: break writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Question You are given an XML data string representing information about books in various categories. You need to process this XML data using the `xml.etree.ElementTree` module to fulfill the following requirements: 1. Parse the XML data string into an `ElementTree` object. 2. Find all the books in the category \\"Fiction\\". 3. For each book in the \\"Fiction\\" category, extract the `title`, `author`, and `year` published. 4. Modify the `year` of each \\"Fiction\\" book to be one year earlier (e.g., if a book was published in 2020, update it to 2019). 5. Write the modified XML data back to a string. **Input Format:** - You are provided with an XML string `books_data`. **Output Format:** - Return a modified XML string with the updated year for \\"Fiction\\" books. **Constraints:** - Do not use any libraries other than `xml.etree.ElementTree`. - Assume the XML data structure is well-formed. ```xml <catalog> <book category=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book category=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> </book> <book category=\\"Fiction\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> </book> </catalog> ``` Write a function `process_books(books_data: str) -> str` that implements the required functionality. # Function Signature ```python import xml.etree.ElementTree as ET def process_books(books_data: str) -> str: # Your code here pass ``` # Example **Input:** ```python books_data = \'\'\' <catalog> <book category=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book category=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> </book> <book category=\\"Fiction\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> </book> </catalog> \'\'\' ``` **Output:** ```xml <catalog> <book category=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1924</year> </book> <book category=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> </book> <book category=\\"Fiction\\"> <title>1984</title> <author>George Orwell</author> <year>1948</year> </book> </catalog> ```","solution":"import xml.etree.ElementTree as ET def process_books(books_data: str) -> str: # Parse the XML data string root = ET.fromstring(books_data) # Find all the books in the category \\"Fiction\\" for book in root.findall(\\"book[@category=\'Fiction\']\\"): year_element = book.find(\'year\') if year_element is not None: # Extract the current year and decrement it by one current_year = int(year_element.text) year_element.text = str(current_year - 1) # Write the modified XML data back to a string return ET.tostring(root, encoding=\'unicode\') # Example usage books_data = \'\'\'<catalog> <book category=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book category=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> </book> <book category=\\"Fiction\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> </book> </catalog>\'\'\' modified_data = process_books(books_data) print(modified_data)"},{"question":"You are tasked with implementing a custom module reloader function that utilizes the `imp` module. This function should locate, load, and reload a specified module. Given the deprecation of the `imp` module, you should also provide an equivalent implementation using the `importlib` functions. Function Signature ```python def custom_reload(module_name: str): pass ``` Requirements: 1. Implement the `custom_reload` function that performs the following steps: - Locate the module using `imp.find_module()`. - Load the module using `imp.load_module()`. - Reload the module using `imp.reload()`. All while ensuring that the deprecated `imp` functions are properly handled and exceptions (like `ImportError`) are caught and handled gracefully. 2. Implement equivalent functionality using `importlib`. The `importlib` part of your function should: - Find the module using `importlib.util.find_spec()`. - Load the module using `importlib.util.module_from_spec()`. - Reload the module using `importlib.reload()`. 3. The function should return a tuple: - The reloaded module object (both for `imp` and `importlib` approaches). - The string \\"imp\\" or \\"importlib\\" indicating which approach was used, based on the availability of the functions. Example Usage: ```python # Ensure the module with name \'module_name\' exists or create a dummy module for testing module = custom_reload(\'example_module\') print(module) # Should print the reloaded module object and approach used (\'imp\' or \'importlib\') ``` Constraints: - If a module cannot be found or reloaded, an appropriate error message should be returned. - Do not use any external libraries other than `imp` and `importlib`. - Handle deprecated warnings and ensure compatibility with Python 3.3 and above. Note: Due to the deprecation of the *imp* module, focus on providing a backward-compatible solution where possible, while promoting the use of the modern `importlib` approach. Good luck!","solution":"import warnings import importlib # Check if imp module is available, which is deprecated but may be present in older codebases. try: import imp imp_present = True except ImportError: imp_present = False def custom_reload(module_name: str): Reload the specified module using either the imp or importlib approaches. if not isinstance(module_name, str): return \\"Error: Module name must be a string.\\" if imp_present: try: # Locate module using imp file, pathname, description = imp.find_module(module_name) # Load module using imp module = imp.load_module(module_name, file, pathname, description) # Reload module using imp module = imp.reload(module) if file: file.close() return module, \'imp\' except ImportError as e: return f\\"ImportError: {e}\\" except Exception as e: return f\\"Error: {e}\\" else: try: # Locate module using importlib spec = importlib.util.find_spec(module_name) if spec is None: return f\\"Module \'{module_name}\' not found\\" # Load module using importlib module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) # Reload module using importlib module = importlib.reload(module) return module, \'importlib\' except ImportError as e: return f\\"ImportError: {e}\\" except Exception as e: return f\\"Error: {e}\\""},{"question":"# Objective: In this task, you will demonstrate your understanding of tracing and scripting models using the PyTorch `torch.utils.jit` module. # Problem Statement: You are required to implement a function that: 1. Creates a simple neural network model using `torch.nn.Module`. 2. Converts this model into TorchScript using both tracing and scripting. 3. Shows a comparison of the outputs of the original model, the traced model, and the scripted model to ensure they are identical. # Task: 1. **Define a Simple Neural Network Model**: Create a neural network model that consists of at least two layers (you can use layers such as Linear, ReLU, etc.). 2. **Trace the Model**: Use the `torch.jit.trace` function to convert the model into a TorchScript `ScriptModule`. 3. **Script the Model**: Use the `torch.jit.script` function to compile the original model into a TorchScript `ScriptModule`. 4. **Compare Outputs**: Create a random input tensor and pass it through the original model, the traced model, and the scripted model. Assert that all the outputs are the same. # Specifications: - **Input**: Your function should not take any input. - **Output**: Print out the following: 1. The original model output. 2. The traced model output. 3. The scripted model output. You should ensure all outputs are identical. # Example: ```python import torch import torch.nn as nn def model_tracing_and_scripting(): # Step 1: Define a simple neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.layer2 = nn.Linear(20, 1) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Create an instance of the model model = SimpleNN() # Step 2: Trace the model example_input = torch.randn(1, 10) traced_model = torch.jit.trace(model, example_input) # Step 3: Script the model scripted_model = torch.jit.script(model) # Step 4: Compare outputs original_output = model(example_input) traced_output = traced_model(example_input) scripted_output = scripted_model(example_input) # Ensure outputs are the same assert torch.allclose(original_output, traced_output), \\"Traced model output differs from the original model output\\" assert torch.allclose(original_output, scripted_output), \\"Scripted model output differs from the original model output\\" # Print the outputs print(\\"Original Model Output:\\", original_output) print(\\"Traced Model Output:\\", traced_output) print(\\"Scripted Model Output:\\", scripted_output) # Call the function model_tracing_and_scripting() ``` In this implementation, you need to create a simple neural network and convert it to TorchScript using both tracing and scripting. Then, compare the outputs of the original, traced, and scripted models.","solution":"import torch import torch.nn as nn def model_tracing_and_scripting(): # Step 1: Define a simple neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.layer2 = nn.Linear(20, 1) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Create an instance of the model model = SimpleNN() # Step 2: Trace the model example_input = torch.randn(1, 10) traced_model = torch.jit.trace(model, example_input) # Step 3: Script the model scripted_model = torch.jit.script(model) # Step 4: Compare outputs original_output = model(example_input) traced_output = traced_model(example_input) scripted_output = scripted_model(example_input) # Ensure outputs are the same assert torch.allclose(original_output, traced_output), \\"Traced model output differs from the original model output\\" assert torch.allclose(original_output, scripted_output), \\"Scripted model output differs from the original model output\\" # Print the outputs print(\\"Original Model Output:\\", original_output) print(\\"Traced Model Output:\\", traced_output) print(\\"Scripted Model Output:\\", scripted_output)"},{"question":"**Objective:** Write a Python function to load a specific dataset and create a customized plot using the `seaborn` library. The function should demonstrate your understanding of data normalization, constraint-based transformations, and plot labeling in seaborn. **Function Signature:** ```python def create_custom_plot(): pass ``` **Requirements:** 1. **Dataset Loading:** - Load the `healthexp` dataset provided by seaborn. 2. **Plot Creation:** - Use the `seaborn.objects` module to create the following two plots in a single figure: 1. A line plot showing health expenditure over the years for each country, normalized relative to the maximum value of each group. 2. Another line plot showing the percentage change in health expenditure from the value in the year 1970, constrained by a baseline. 3. **Customization:** - Normalize the spending to be relative to the maximum value within each country group and label the y-axis appropriately. - Apply a baseline constraint at the year 1970 for the percent change plot and label the y-axis to indicate the percentage change from the 1970 baseline. 4. **Labels:** - Add relevant labels for the y-axes to represent the normalized spending and percentage change respectively. 5. **Output:** - The function should save the created plot as `\\"custom_health_exp_plot.png\\"` in the current directory. **Constraints:** - Ensure the use of the `seaborn.objects` module for plotting. - Handle the dataset and transformations within the function. **Example:** This is an illustration of how the final plot might look like. The output should be a comprehensive visualization with two line plots demonstrating the insights mentioned in the requirements. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the first plot (Normalized relative to maximum value) plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Create the second plot (Percent change from 1970 baseline) plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Save the plots fig, axes = plt.subplots(2, 1, figsize=(10, 12)) plot1.on(axes[0]).plot() plot2.on(axes[1]).plot() plt.savefig(\\"custom_health_exp_plot.png\\") # Create the custom plot create_custom_plot() ``` **Note:** The examples provided in this question should help you understand the expected output and approach to solve this problem. Ensure your solution is logically structured, efficient, and well-documented.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt import pandas as pd def create_custom_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Normalize spending for each country relative to the maximum value healthexp[\'Normalized_Spending\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Calculate percent change from 1970 baseline_1970 = healthexp[healthexp[\'Year\'] == 1970].set_index(\'Country\')[\'Spending_USD\'] healthexp = healthexp.join(baseline_1970.rename(\'Baseline_1970\'), on=\'Country\') healthexp[\'Percent_Change\'] = (healthexp[\'Spending_USD\'] - healthexp[\'Baseline_1970\']) / healthexp[\'Baseline_1970\'] * 100 # Create the first plot (Normalized relative to maximum value) plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") ) # Create the second plot (Percent change from 1970 baseline) plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Percent_Change\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Save the plots fig, axes = plt.subplots(2, 1, figsize=(10, 12)) plot1.on(axes[0]).plot() plot2.on(axes[1]).plot() plt.savefig(\\"custom_health_exp_plot.png\\") # Create the custom plot create_custom_plot()"},{"question":"<|Analysis Begin|> The provided documentation gives a comprehensive overview of the ipaddress module in Python, including creation and manipulation of IP address, network, and interface objects, for both IPv4 and IPv6. The documentation includes examples of creating objects from strings and integers, describes the differences between IP versions, explains how to define networks, host interfaces, and provides mechanisms for object inspection and comparison. Important topics and functionalities covered: 1. Creation of IP addresses (both IPv4 and IPv6). 2. Defining and manipulating IP networks. 3. Host interface representation. 4. Inspecting attributes of addresses, networks, interfaces. 5. Iterating through networks. 6. Detailed error messages for invalid inputs. 7. Using IP addresses with other modules. 8. Treating network objects as lists. From this documentation, a challenging coding question can be constructed focusing on these capabilities and understanding of the module. <|Analysis End|> <|Question Begin|> # IP Network Management System You are tasked with designing a network management system using the Python `ipaddress` module. The system should be able to parse input data to create and manage IP networks and addresses. Specifically, your task is to create a function to analyze and manipulate provided IP addresses and networks. Function Signature ```python def analyze_ip_networks(data: List[Tuple[str, str, str]]) -> List[str]: pass ``` Input - A list of tuples, where each tuple contains: - A string representing an IP address (either IPv4 or IPv6). - A string representing a network in CIDR notation. - A string indicating an operation, which can be: - `NEEDS_HOST`: Check if the IP address can be considered a host within the provided network. - `MASK_INFO`: Provide the netmask and hostmask of the provided network. - `EXPLODE_COMPRESS`: Provide the exploded and compressed forms of the provided IP address (for IPv6, otherwise return the address as is). Output - A list of strings with results corresponding to each operation: - For `NEEDS_HOST`: Return \\"Yes\\" if the IP address is within the network, otherwise \\"No\\". - For `MASK_INFO`: Return a string with the netmask and hostmask separated by a comma (e.g., `\'255.255.255.0,0.0.0.255\'` for IPv4). - For `EXPLODE_COMPRESS`: Return the exploded and compressed forms separated by a comma if IPv6, otherwise return the IPv4 address (e.g., `\'2001:0db8:0000:0000:0000:0000:0000:0001,2001:db8::1\'`). Constraints - All input strings are valid representations of IP addresses and networks. - The operations will only be one of the three specified. Example ```python data = [ (\\"192.0.2.1\\", \\"192.0.2.0/24\\", \\"NEEDS_HOST\\"), (\\"2001:DB8::1\\", \\"2001:DB8::/96\\", \\"MASK_INFO\\"), (\\"2001:DB8::1\\", \\"\\", \\"EXPLODE_COMPRESS\\"), (\\"192.0.2.1\\", \\"192.0.3.0/24\\", \\"NEEDS_HOST\\") ] print(analyze_ip_networks(data)) # Output: # [\'Yes\', \'ffff:ffff:ffff:ffff:ffff:ffff::,::ffff:ffff\', \'2001:0db8:0000:0000:0000:0000:0000:0001,2001:db8::1\', \'No\'] ``` Your function must demonstrate understanding of IP address manipulation, network containment, netmask and hostmask calculation, and address representation conversions provided by Python\'s `ipaddress` module.","solution":"from typing import List, Tuple import ipaddress def analyze_ip_networks(data: List[Tuple[str, str, str]]) -> List[str]: results = [] for ip_str, network_str, operation in data: ip = ipaddress.ip_address(ip_str) if network_str: network = ipaddress.ip_network(network_str, strict=False) if operation == \\"NEEDS_HOST\\": results.append(\\"Yes\\" if ip in network else \\"No\\") elif operation == \\"MASK_INFO\\": results.append(f\\"{network.netmask},{network.hostmask}\\") elif operation == \\"EXPLODE_COMPRESS\\": if isinstance(ip, ipaddress.IPv6Address): results.append(f\\"{ip.exploded},{ip.compressed}\\") else: results.append(ip_str) return results"},{"question":"**Objective:** Demonstrate your understanding of the `termios` module by writing a function that sets the terminal to raw mode for user input, then restores it to its original state. Raw mode typically disables all input and output processing features, such as canonical mode, signal generation, echoing, and other special character effects. **Task:** Write a function `raw_input_mode(prompt: str = \\"Input: \\") -> str` that temporarily sets the terminal to raw mode to capture user input without any processing, and then restores the terminal to its original state after input is received. **Function Signature:** ```python def raw_input_mode(prompt: str = \\"Input: \\") -> str: ``` **Input:** - `prompt` (optional): A `str` representing the text to display when asking for user input. Default is `\\"Input: \\"`. **Output:** - A `str` representing the user input received while the terminal was in raw mode. **Requirements:** 1. Use `termios.tcgetattr(fd)` to get the current terminal attributes. 2. Modify the necessary flags to set the terminal to raw mode. 3. Ensure the original terminal attributes are restored after capturing user input, regardless of any exceptions that may occur. 4. Provide necessary imports and handle file descriptors correctly. **Constraints:** - The function must handle exceptions gracefully and ensure the terminal state is always restored to its initial settings. **Example:** ```python print(\\"Enter input in raw mode (type \'exit\' to stop):\\") while True: user_input = raw_input_mode() if user_input.strip().lower() == \'exit\': break print(f\\"You entered: {user_input}\\") ``` In this example, the user will be prompted to enter input in raw mode. Echoing should be disabled, and special processing should not be applied to the input. Typing \'exit\' will stop the loop. **Additional Information:** - Review your Unix system\'s documentation on terminal control interfaces if needed. - This question assumes the student has access to a Unix-like operating environment that supports the `termios` module.","solution":"import sys import termios import tty def raw_input_mode(prompt: str = \\"Input: \\") -> str: Temporarily sets the terminal to raw mode to capture user input without any processing, and then restores the terminal to its original state after input is received. Args: prompt (str): The text to display when asking for user input. Default is \\"Input: \\". Returns: str: The user input received while the terminal was in raw mode. fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: tty.setraw(fd) print(prompt, end=\'\', flush=True) user_input = sys.stdin.read(1) # capturing single character input in raw mode while user_input[-1] != \'n\': # continue until user presses enter user_input += sys.stdin.read(1) finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) return user_input if __name__ == \\"__main__\\": # This will run if executing the script directly, # allowing you to test the input mode manually in a terminal. print(\\"Enter input in raw mode (type \'exit\' to stop):\\") while True: user_input = raw_input_mode().strip() if user_input.lower() == \'exit\': break print(f\\"You entered: {user_input}\\")"},{"question":"# Question Design and implement a PyTorch model with quantization. The task is to perform dynamic quantization on a simple neural network model and compare the performance of the quantized model against the original floating-point model. Task Description 1. **Model Definition**: - Define a simple neural network model (use `nn.Sequential` or define your own class) with at least one linear layer. 2. **Dynamic Quantization**: - Apply dynamic quantization to the linear layer(s) of the model. 3. **Performance Comparison**: - Compare the inference time and model size between the original floating-point model and the quantized model. Requirements 1. **Model Structure**: - The simple neural network should have at least one linear layer. You can add other layers and activation functions as required. 2. **Quantization**: - Use dynamic quantization techniques as discussed in the provided documentation. 3. **Comparison**: - Measure the model size (using `torch.save` and then checking file size) for both original and quantized models. - Measure the inference time on random input tensors for both models. 4. **Report**: - Print the inference times and model sizes for both models. - Provide comments and explanations for each step in your code. Expected Input and Output - **Input**: The code should work without any specific input from the user other than running the script. - **Output**: The script should print the model sizes and inference times for comparison. Code Template ```python import torch import torch.nn as nn import time # Step 1: Define the Model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(128, 64) self.relu = nn.ReLU() self.out = nn.Linear(64, 10) def forward(self, x): x = self.fc(x) x = self.relu(x) x = self.out(x) return x # Step 2: Instantiate the Model model_fp32 = SimpleModel() # Step 3: Apply Dynamic Quantization model_int8 = torch.ao.quantization.quantize_dynamic( model_fp32, # the original model {nn.Linear}, # a set of layers to dynamically quantize dtype=torch.qint8) # the target dtype for quantized weights # Step 4: Measure Model Size # Save original model and quantized model torch.save(model_fp32.state_dict(), \'model_fp32.pth\') torch.save(model_int8.state_dict(), \'model_int8.pth\') # Get file sizes size_fp32 = os.path.getsize(\'model_fp32.pth\') size_int8 = os.path.getsize(\'model_int8.pth\') print(f\'Float32 Model Size: {size_fp32} bytes\') print(f\'INT8 Model Size: {size_int8} bytes\') # Step 5: Measure Inference Time input_data = torch.randn(1000, 128) # Example input data start_time = time.time() model_fp32(input_data) fp32_inference_time = time.time() - start_time start_time = time.time() model_int8(input_data) int8_inference_time = time.time() - start_time print(f\'Float32 Inference Time: {fp32_inference_time:.6f} seconds\') print(f\'INT8 Inference Time: {int8_inference_time:.6f} seconds\') ``` Please ensure that the provided code is complete and functional. The given code template outlines key steps but requires implementation details and verification.","solution":"import torch import torch.nn as nn import time import os # Step 1: Define the Model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: Instantiate the Model model_fp32 = SimpleModel() # Step 3: Apply Dynamic Quantization model_int8 = torch.ao.quantization.quantize_dynamic( model_fp32, # the original model {nn.Linear}, # a set of layers to dynamically quantize dtype=torch.qint8) # the target dtype for quantized weights # Step 4: Measure Model Size # Save original model and quantized model torch.save(model_fp32.state_dict(), \'model_fp32.pth\') torch.save(model_int8.state_dict(), \'model_int8.pth\') # Get file sizes size_fp32 = os.path.getsize(\'model_fp32.pth\') size_int8 = os.path.getsize(\'model_int8.pth\') print(f\'Float32 Model Size: {size_fp32} bytes\') print(f\'INT8 Model Size: {size_int8} bytes\') # Step 5: Measure Inference Time input_data = torch.randn(1000, 128) # Example input data # Measure inference time for FP32 model start_time = time.time() with torch.no_grad(): model_fp32(input_data) fp32_inference_time = time.time() - start_time # Measure inference time for INT8 model start_time = time.time() with torch.no_grad(): model_int8(input_data) int8_inference_time = time.time() - start_time print(f\'Float32 Inference Time: {fp32_inference_time:.6f} seconds\') print(f\'INT8 Inference Time: {int8_inference_time:.6f} seconds\')"},{"question":"**Data Visualization with Seaborn\'s `countplot`** You are given a dataset of Titanic passengers. Your task is to write a function `plot_titanic_data` that will create three different plots using seaborn\'s `countplot`. Each plot should be saved as a PNG file. # Input - The Titanic dataset is loaded for you using `sns.load_dataset(\\"titanic\\")`. - Your function will not take any parameters, but should create and save the following plots: 1. A count plot showing the number of passengers in each class. 2. A count plot showing the number of passengers in each class, grouped by whether they survived or not. 3. A normalized count plot showing the percentage of passengers in each class, grouped by whether they survived or not. # Output - Three files named: 1. `class_count.png` 2. `class_survived_count.png` 3. `class_survived_percentage.png` # Constraints - Use seaborn\'s `countplot` with sns.set_theme(style=\\"whitegrid\\"). - Ensure that plots render correctly and save without errors. # Function Signature ```python import seaborn as sns def plot_titanic_data(): pass ``` # Example Usage After calling `plot_titanic_data()`, the files `class_count.png`, `class_survived_count.png`, and `class_survived_percentage.png` should be created with the described plots. # Additional Notes - You can use Matplotlib\'s `savefig` function to save the plots (e.g., `plt.savefig(\'filename.png\')`). - Ensure to import necessary packages and handle any necessary configurations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # Plot 1: Count plot of passengers in each class plt.figure(figsize=(10, 6)) ax = sns.countplot(x=\'class\', data=titanic) ax.set_title(\'Count of Passengers in Each Class\') plt.savefig(\'class_count.png\') plt.close() # Plot 2: Count plot of passengers in each class, grouped by survival plt.figure(figsize=(10, 6)) ax = sns.countplot(x=\'class\', hue=\'survived\', data=titanic) ax.set_title(\'Count of Passengers in Each Class by Survival\') plt.savefig(\'class_survived_count.png\') plt.close() # Plot 3: Normalized count plot (percentage) of passengers in each class by survival plt.figure(figsize=(10, 6)) class_survived = titanic.groupby([\'class\', \'survived\']).size().reset_index(name=\'count\') total_per_class = class_survived.groupby(\'class\')[\'count\'].transform(\'sum\') class_survived[\'percentage\'] = class_survived[\'count\'] / total_per_class * 100 ax = sns.barplot(x=\'class\', y=\'percentage\', hue=\'survived\', data=class_survived) ax.set_title(\'Percentage of Passengers in Each Class by Survival\') ax.set_ylabel(\'Percentage\') plt.savefig(\'class_survived_percentage.png\') plt.close() # Call the function to generate and save the plots plot_titanic_data()"},{"question":"**Objective:** Demonstrate your understanding of the `torch.Size` class and its functionality in PyTorch. **Problem Statement:** You are given a tensor `t` whose dimensions represent different properties of a dataset. Implement a function `analyze_tensor(t: torch.Tensor) -> Tuple[int, List[int]]` that takes a tensor `t` and returns: - The number of dimensions in the tensor. - The size of each dimension as a list. **Input:** - A `torch.Tensor` `t`. **Output:** - A tuple where the first element is an integer representing the number of dimensions, and the second element is a list of integers representing the size of each dimension. **Constraints:** - Assume the tensor `t` has between 1 and 5 dimensions. **Example:** ```python import torch # Example tensor t = torch.ones(10, 20, 30) # Expected Output: # Dimensions number: 3 # Sizes of each dimension: [10, 20, 30] assert analyze_tensor(t) == (3, [10, 20, 30]) ``` **Performance Requirements:** - The implementation should efficiently determine the number of dimensions and their sizes using `torch.Size`. **Function Signature:** ```python from typing import List, Tuple import torch def analyze_tensor(t: torch.Tensor) -> Tuple[int, List[int]]: # Implement the function here pass ``` **Hint:** - Use the `size` method of the `torch.Tensor` to get the dimensions\' sizes. - Remember that `torch.Size` is a subclass of `tuple`, so you can use sequence operations on it.","solution":"from typing import List, Tuple import torch def analyze_tensor(t: torch.Tensor) -> Tuple[int, List[int]]: Analyzes the tensor to determine the number of dimensions and the size of each dimension. Parameters: t (torch.Tensor): The input tensor. Returns: Tuple[int, List[int]]: A tuple containing the number of dimensions and a list of sizes of each dimension. num_dims = t.dim() sizes = list(t.size()) return num_dims, sizes"},{"question":"Objective: To assess the understanding and application of the `unittest.mock` module in Python for unit testing. Problem Statement: You are given a class `FileProcessor` that processes file data in a certain way. We need to patch its dependencies and test the behavior of the method `process_files`. Here\'s the class definition: ```python import os class FileProcessor: def __init__(self, directory): self.directory = directory def get_file_list(self): return os.listdir(self.directory) def read_file(self, filename): with open(os.path.join(self.directory, filename), \'r\') as file: return file.read() def process_files(self): file_list = self.get_file_list() processed_data = {} for filename in file_list: file_data = self.read_file(filename) # Simulate processing processed_data[filename] = file_data.upper() return processed_data ``` Your task is to write unit tests for the `FileProcessor` class. Specifically, you need to test the following: 1. When `process_files` is called, it interacts correctly with the `os.listdir` and `open` functions. 2. It processes the files by converting their content to uppercase. Requirements: 1. Use the `unittest.mock` module to patch: - The `os.listdir` method to return a predefined list of filenames. - The `open` function to simulate reading file contents. 2. Verify: - `os.listdir` is called with the correct directory path. - The `open` function is called with the correct file paths. - The results of `process_files` return the expected processed data. Constraints: - You are not allowed to modify the `FileProcessor` class. Input and Output: - **Input**: Predefined class `FileProcessor`. - **Output**: Implement the unit tests using the `unittest` framework. Example Test Case: You should implement a unit test that mocks these dependencies and verifies the functionality as described. Implementation: ```python import unittest from unittest.mock import patch, mock_open class TestFileProcessor(unittest.TestCase): @patch(\'os.listdir\', return_value=[\'file1.txt\', \'file2.txt\']) @patch(\'builtins.open\', new_callable=mock_open, read_data=\'file content\') def test_process_files(self, mock_open, mock_listdir): directory = \'/some/directory\' processor = FileProcessor(directory) expected_result = { \'file1.txt\': \'FILE CONTENT\', \'file2.txt\': \'FILE CONTENT\' } result = processor.process_files() mock_listdir.assert_called_once_with(directory) mock_open.assert_any_call(\'/some/directory/file1.txt\', \'r\') mock_open.assert_any_call(\'/some/directory/file2.txt\', \'r\') self.assertEqual(result, expected_result) # This is the test runner for our unit test. Uncomment to run the tests # if __name__ == \'__main__\': # unittest.main() ``` Write your own set of tests using the structure provided, to fully verify the behavior of the `FileProcessor` class.","solution":"import os class FileProcessor: def __init__(self, directory): self.directory = directory def get_file_list(self): return os.listdir(self.directory) def read_file(self, filename): with open(os.path.join(self.directory, filename), \'r\') as file: return file.read() def process_files(self): file_list = self.get_file_list() processed_data = {} for filename in file_list: file_data = self.read_file(filename) # Simulate processing processed_data[filename] = file_data.upper() return processed_data"},{"question":"# Boolean Logic Simulator Objective: You are required to implement a Boolean Logic Simulator in Python that mimics the internal workings as described in the documentation. This simulator should be able to handle boolean operations and return native Python boolean values (`True` and `False`). Function Signature: ```python def boolean_logic_simulator(op1, op2, operation): Simulates boolean logic operations. Parameters: op1 (int): The first operand, which can be 0 or 1 representing False and True. op2 (int): The second operand, which can be 0 or 1 representing False and True. operation (str): The boolean operation to perform. Supported operations: \\"AND\\", \\"OR\\", \\"NOT_OP1\\", \\"NOT_OP2\\" Returns: bool: The result of the boolean operation. pass # Example Usage: # boolean_logic_simulator(1, 0, \\"AND\\") should return False # boolean_logic_simulator(1, 1, \\"OR\\") should return True # boolean_logic_simulator(0, 1, \\"NOT_OP1\\") should return True # boolean_logic_simulator(1, 1, \\"NOT_OP2\\") should return False ``` Constraints: - `op1` and `op2` can only be 0 or 1 representing `False` and `True`, respectively. If an invalid operand is passed, raise a `ValueError` with the message \\"Operands must be 0 or 1\\". - `operation` is a string and can be one of the following: `\\"AND\\"`, `\\"OR\\"`, `\\"NOT_OP1\\"`, `\\"NOT_OP2\\"`. If an invalid operation is passed, raise a `ValueError` with the message \\"Invalid operation\\". Examples and Explanations: 1. `boolean_logic_simulator(1, 0, \\"AND\\")` should return `False` because 1 AND 0 is `False`. 2. `boolean_logic_simulator(1, 1, \\"OR\\")` should return `True` because 1 OR 1 is `True`. 3. `boolean_logic_simulator(0, 1, \\"NOT_OP1\\")` should return `True` because NOT 0 is `True`. 4. `boolean_logic_simulator(1, 1, \\"NOT_OP2\\")` should return `False` because NOT 1 is `False`. Please implement the function `boolean_logic_simulator` as described above to pass all the given examples and adhere to the constraints.","solution":"def boolean_logic_simulator(op1, op2, operation): Simulates boolean logic operations. Parameters: op1 (int): The first operand, which can be 0 or 1 representing False and True. op2 (int): The second operand, which can be 0 or 1 representing False and True. operation (str): The boolean operation to perform. Supported operations: \\"AND\\", \\"OR\\", \\"NOT_OP1\\", \\"NOT_OP2\\" Returns: bool: The result of the boolean operation. Raises: ValueError: If the operands are not 0 or 1, or if the operation is invalid. if op1 not in {0, 1} or op2 not in {0, 1}: raise ValueError(\\"Operands must be 0 or 1\\") if operation == \\"AND\\": return bool(op1 & op2) elif operation == \\"OR\\": return bool(op1 | op2) elif operation == \\"NOT_OP1\\": return not bool(op1) elif operation == \\"NOT_OP2\\": return not bool(op2) else: raise ValueError(\\"Invalid operation\\")"},{"question":"# Advanced Coding Assessment: Custom Directory File Matcher Objective: Implement a function that takes a directory path and a pattern, and returns a list of all filenames in that directory (non-recursive) that match the given pattern using Unix shell-style wildcards. Function Signature: ```python def custom_file_matcher(directory: str, pattern: str) -> list: pass ``` Input: - `directory` (str): The path of the directory to search within. - `pattern` (str): The Unix shell-style wildcard pattern to match filenames against. Valid patterns include: - `*` matches everything - `?` matches any single character - `[seq]` matches any character in `seq` - `[!seq]` matches any character not in `seq` Output: - A list of filenames (list of str) in the given directory that match the specified pattern. The filenames should be returned in the order they are encountered. Constraints: - Do not use the `glob` module. - Utilize the `os`, `fnmatch`, and `os.path` modules only. - The solution must handle case normalization according to the operating system\'s standards. Example: Suppose the directory contains the following files: - `\'example.txt\'` - `\'sample.md\'` - `\'test.py\'` - `\'example.py\'` ```python >>> custom_file_matcher(\'/path/to/directory\', \'*.py\') [\'test.py\', \'example.py\'] >>> custom_file_matcher(\'/path/to/directory\', \'e*.txt\') [\'example.txt\'] >>> custom_file_matcher(\'/path/to/directory\', \'s*.md\') [\'sample.md\'] ``` Hints: - Remember to handle case normalization in your implementation. - Be careful with the directory path handling and ensure your solution works on different operating systems.","solution":"import os import fnmatch def custom_file_matcher(directory: str, pattern: str) -> list: Returns a list of filenames in the specified directory that match the given Unix shell-style wildcard pattern. Parameters: - directory (str): The path of the directory to search within. - pattern (str): The wildcard pattern to match filenames against. Returns: - list of str: List of filenames that match the pattern. # Get the list of files in the specified directory try: files = os.listdir(directory) except FileNotFoundError: return [] # Filter the files to match the given pattern matched_files = [filename for filename in files if fnmatch.fnmatch(filename, pattern)] return matched_files"},{"question":"**Title:** Implementing an Asynchronous Data Processing Pipeline with Comprehensions. **Objective:** To assess the understanding of asynchronous operations, comprehensions, and generator expressions in Python. **Task:** You are tasked with implementing a complex data processing pipeline that processes a stream of numerical data asynchronously. Your function needs to: 1. **Generate a Sequence:** Implement an asynchronous generator that yields squares of numbers from 1 to a given maximum (inclusive). 2. **Filter Even Numbers:** Use list comprehension to create a list from the generated sequence that only includes even numbers. 3. **Aggregation using Generators:** Use a generator expression to calculate the cumulative sum of the filtered list. 4. **Summarize:** Implement an asynchronous function that returns the sum of the sequence generated by the generator expression. **Function Specifications:** - **async_gen_squares(max_num: int) -> AsyncGenerator[int, None]** - Inputs: An integer `max_num` - Output: An asynchronous generator yielding squares of numbers from 1 to `max_num` inclusive. - **filter_even_numbers(sequence: List[int]) -> List[int]** - Inputs: A list of integers `sequence` - Output: A list of integers containing only the even numbers from the input sequence using list comprehension. - **gen_cumulative_sum(sequence: List[int]) -> Generator[int, None, None]** - Inputs: A list of integers `sequence` - Output: A generator yielding the cumulative sum of elements from the input sequence. - **async_sum_cumulative(sequence: List[int]) -> int** - Inputs: A list of integers `sequence` - Output: An integer representing the sum of all values yielded by the cumulative sum generator. **Constraints:** - The `max_num` will be between 1 and 1000. - The input sequence will always be a list of integers. - The functions need to handle asynchronous execution properly. **Example:** ```python import asyncio async def async_gen_squares(max_num: int): for i in range(1, max_num + 1): yield i ** 2 await asyncio.sleep(0.1) # Simulating asynchronous operation def filter_even_numbers(sequence): return [x for x in sequence if x % 2 == 0] def gen_cumulative_sum(sequence): total = 0 for num in sequence: total += num yield total async def async_sum_cumulative(sequence): return sum([x async for x in async_gen_squares(sequence)]) # Example Usage async def main(): # Step 1: Generate asynchronous sequence of square numbers async for num in async_gen_squares(10): print(num, end=\' \') # Output squares # Step 2: Filter even numbers from the generated sequence list evens = filter_even_numbers([1, 4, 9, 16, 25, 36, 49, 64, 81, 100]) print(evens) # Example output: [4, 16, 36, 64, 100] # Step 3: Cumulative sum using generator cumulative_sum = list(gen_cumulative_sum(evens)) print(cumulative_sum) # Example output: [4, 20, 56, 120, 220] # Step 4: Asynchronous function to calculate total sum total_sum = await async_sum_cumulative(cumulative_sum) print(total_sum) # Example output: 420 asyncio.run(main()) ``` **Explanation:** 1. **Asynchronous Generator:** `async_gen_squares` generates squares of numbers asynchronously up to `max_num`. 2. **Filtering with Comprehensions:** `filter_even_numbers` filters out only the even squares from the list using list comprehension. 3. **Generator for Cumulative Sum:** `gen_cumulative_sum` generates a cumulative sum from the filtered even numbers. 4. **Asynchronous Summation:** `async_sum_cumulative` returns the sum of all numbers generated by the cumulative sum generator asynchronously. Note: Students must ensure their functions handle asynchronous execution properly and utilize list comprehensions and generator expressions efficiently.","solution":"import asyncio from typing import AsyncGenerator, List, Generator async def async_gen_squares(max_num: int) -> AsyncGenerator[int, None]: for i in range(1, max_num + 1): await asyncio.sleep(0.1) # Simulating asynchronous operation yield i ** 2 def filter_even_numbers(sequence: List[int]) -> List[int]: return [x for x in sequence if x % 2 == 0] def gen_cumulative_sum(sequence: List[int]) -> Generator[int, None, None]: total = 0 for num in sequence: total += num yield total async def async_sum_cumulative(sequence: List[int]) -> int: return sum([x async for x in async_gen_squares(len(sequence))])"},{"question":"Objective: Implement an asynchronous task management system using Python\'s `asyncio.Future` objects. Your task is to create an asynchronous task that processes multiple tasks, manages future results, and handles exceptions appropriately. Problem Statement: You are given a list of URLs. Some URLs may fail to be processed. Implement an asynchronous function `process_urls(urls: List[str]) -> List[str]` that works as follows: 1. For each URL in the `urls` list, create an asyncio `Future` object to represent the result of fetching the URL. 2. Implement a coroutine function `fetch_url(fut: asyncio.Future, url: str)` that simulates the fetch operation by: - Sleeping asynchronously for a random duration between 0.1 to 0.5 seconds. - Randomly deciding to either set a successful result or raise an exception \\"FetchError: Failed to fetch\\" to simulate a failure. 3. Implement a callback function `on_fetch_complete(fut: asyncio.Future)` that prints the URL\'s result or the exception based on the state of the future. 4. Use `asyncio.create_task` to ensure these tasks are scheduled and executed concurrently. 5. The `process_urls` function should return a list where each element is either the successful fetch result or the string \\"FAILED\\" for failed URLs. Constraints: - Assume a moderate number of URLs (up to 100). - Must handle exceptions correctly. - Ensure proper synchronization and state handling of Futures. Input: - `urls: List[str]` - A list of URLs to be processed (strings). Output: - `List[str]` - Each element should be either the result of the URL fetch or \\"FAILED\\" for URLs that raise an exception. Example: ```python import asyncio import random from typing import List class FetchError(Exception): pass async def fetch_url(fut: asyncio.Future, url: str): try: # Simulate network delay await asyncio.sleep(random.uniform(0.1, 0.5)) # Randomly fail if random.choice([True, False]): raise FetchError(\\"Failed to fetch\\") fut.set_result(f\\"Content of {url}\\") except Exception as e: fut.set_exception(e) def on_fetch_complete(fut: asyncio.Future): try: result = fut.result() print(f\\"Fetch success: {result}\\") except FetchError as e: print(f\\"Fetch failed: {e}\\") async def process_urls(urls: List[str]) -> List[str]: loop = asyncio.get_running_loop() tasks = [] for url in urls: fut = loop.create_future() fut.add_done_callback(on_fetch_complete) task = asyncio.create_task(fetch_url(fut, url)) tasks.append(task) await asyncio.gather(*tasks) return [fut.result() if not fut.cancelled() else \\"FAILED\\" for fut in tasks] # Example usage: urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] result = asyncio.run(process_urls(urls)) print(result) ``` Provide a well-documented solution, ensuring that all edge cases are covered. Make sure to use the asyncio library logically and efficiently.","solution":"import asyncio import random from typing import List class FetchError(Exception): pass async def fetch_url(fut: asyncio.Future, url: str): try: # Simulate network delay await asyncio.sleep(random.uniform(0.1, 0.5)) # Randomly fail if random.choice([True, False]): raise FetchError(f\\"Failed to fetch {url}\\") fut.set_result(f\\"Content of {url}\\") except Exception as e: fut.set_exception(e) def on_fetch_complete(fut: asyncio.Future): try: result = fut.result() print(f\\"Fetch success: {result}\\") except FetchError as e: print(f\\"Fetch failed: {e}\\") async def process_urls(urls: List[str]) -> List[str]: loop = asyncio.get_running_loop() futures = [] for url in urls: fut = loop.create_future() fut.add_done_callback(on_fetch_complete) task = asyncio.create_task(fetch_url(fut, url)) futures.append(fut) await asyncio.gather(*futures, return_exceptions=True) return [fut.result() if fut.exception() is None else \\"FAILED\\" for fut in futures]"},{"question":"# Heapq Module Challenge You need to implement a task scheduling system using the `heapq` module. The task scheduling system should support adding tasks with different priorities, updating the priority of existing tasks, and fetching the next task to execute based on its priority. # Requirements 1. **Function Definitions:** - `add_task(task, priority)`: Adds a new task with the given priority. If the task already exists, update its priority. - `remove_task(task)`: Removes a task from the system. - `pop_task()`: Returns the task with the highest priority (the smallest priority value). If the heap is empty, raise a `KeyError`. 2. **Helper Structures:** - Use a heap to manage the priorities of tasks. - Use a dictionary to quickly locate tasks and their priorities. 3. **Example Use Case:** ```python add_task(\\"write code\\", 5) add_task(\\"release product\\", 7) add_task(\\"write spec\\", 1) add_task(\\"create tests\\", 3) # Update priority of an existing task add_task(\\"write code\\", 2) print(pop_task()) # Output: \\"write spec\\" print(pop_task()) # Output: \\"write code\\" print(pop_task()) # Output: \\"create tests\\" print(pop_task()) # Output: \\"release product\\" # Trying to pop from empty heap print(pop_task()) # Should raise KeyError ``` # Constraints - All tasks are unique strings. - Priorities are integers (negative, zero, or positive). # Input and Output Format - Inputs: The `add_task` and `remove_task` functions take a `task` as a string and a `priority` as an integer. - Outputs: The `pop_task` function returns the next task as a string. If the heap is empty, it raises a `KeyError`. # Performance Requirements - The solution should efficiently handle at least (10^5) tasks with their respective priorities. # Implementation Notes - Ensure that the task scheduling system maintains its efficiency over all operations. - Use the heapq module functions to manage the heap invariants. - Consider using additional helper functions or classes as needed to keep the solution organized and maintainable.","solution":"import heapq class TaskScheduler: def __init__(self): self._heap = [] self._entry_finder = {} # mapping of tasks to entries self._REMOVED = \'<removed-task>\' # placeholder for a removed task self._counter = 0 def add_task(self, task, priority=0): Add a new task or update the priority of an existing task. if task in self._entry_finder: self.remove_task(task) count = self._counter entry = [priority, count, task] self._entry_finder[task] = entry heapq.heappush(self._heap, entry) self._counter += 1 def remove_task(self, task): Mark an existing task as REMOVED. Raise KeyError if not found. entry = self._entry_finder.pop(task) entry[-1] = self._REMOVED def pop_task(self): Remove and return the lowest priority task. Raise KeyError if empty. while self._heap: priority, count, task = heapq.heappop(self._heap) if task is not self._REMOVED: del self._entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"**Objective:** Implement a custom PyTorch function and verify the correctness of its gradients using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. **Problem Statement:** 1. Implement a PyTorch function `my_function` that computes the following operation: [ f(x) = sin(x) + cos(x^2) ] Where `x` is a real-valued tensor. Ensure the function works with both forward and backward automatic differentiation. 2. Write a function `check_gradients` that tests the above implementation using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. **Function Specifications:** 1. **Function 1: `my_function`** - **Input:** A real-valued tensor `x` of arbitrary shape containing floating-point values. - **Output:** A tensor of the same shape as `x` containing the computed values of ( f(x) ). 2. **Function 2: `check_gradients`** - **Input:** No input parameters. - **Output:** Returns `True` if both gradient checks (`gradcheck` and `gradgradcheck`) pass, otherwise raises an appropriate error. ```python def check_gradients(): import torch from torch.autograd import gradcheck, gradgradcheck # Your implementation here return True ``` **Constraints:** - Use PyTorch\'s automatic differentiation library to implement the forward and backward passes. - Assume `x` will have no NaN or infinite values. **Performance Requirements:** - Your implementation should handle tensors of moderate size (e.g., 1000 elements) efficiently. - Ensure numerical stability in your gradient computations. **Example Usage:** ```python import torch # Define the input tensor x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Compute the function output output = my_function(x) print(output) # Check gradients assert check_gradients(), \\"Gradient check failed!\\" ``` # Submission Please submit the following: - `my_function` implementation. - `check_gradients` implementation. - A brief explanation of how you ensured numerical stability in your implementation.","solution":"import torch from torch.autograd import Function, gradcheck, gradgradcheck class MyFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.sin(x) + torch.cos(x**2) @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = grad_output * (torch.cos(x) - 2 * x * torch.sin(x**2)) return grad_x def my_function(x): return MyFunction.apply(x) def check_gradients(): x = torch.randn(10, dtype=torch.double, requires_grad=True) assert gradcheck(MyFunction.apply, (x,), eps=1e-6, atol=1e-4) assert gradgradcheck(MyFunction.apply, (x,), eps=1e-6, atol=1e-4) return True"},{"question":"You are tasked with creating a Python class that utilizes custom descriptors to manage its attributes. Descriptors should allow various types of control over class attributes, demonstrating your understanding of descriptors mentioned in the `python310` documentation. Objectives: 1. Create a class `CustomClass` that: - Has at least two attributes that use custom descriptors. - One attribute should use a read-only descriptor. - One attribute should use a descriptor that validates the data type upon setting. 2. Implement these descriptors using the methods from the `python310` package if possible. 3. Include a method in `CustomClass` that demonstrates the usage of these descriptors. Function Signatures: ```python class CustomClass: def __init__(self, read_only_value, validated_value): Initialize the class with provided values. :param read_only_value: Value for the read-only attribute. :param validated_value: Initial value for the attribute with validation. pass def show_attributes(self): Print out the attributes to show their current values. pass # Below are suggested descriptor classes to be used in CustomClass class ReadOnlyDescriptor: pass class ValidatedDescriptor: pass ``` Expected Input and Output Formats: 1. `__init__(self, read_only_value, validated_value)` - `read_only_value`: Any data type. - `validated_value`: Should only allow integer values. 2. Method `show_attributes()` should print the values of the attributes. Constraints and Limitations: - The `read_only_value` attribute should be set only during initialization and should not be modifiable thereafter. - The `validated_value` attribute should raise a `TypeError` if anything other than an integer is assigned to it. Example Usage: ```python obj = CustomClass(\\"initial_value\\", 42) obj.show_attributes() # Trying to modify the read-only attribute should raise an error. try: obj.read_only = \\"new_value\\" except AttributeError as e: print(e) # Setting the validated attribute to a non-integer should raise an error. try: obj.validated_value = \\"invalid_value\\" except TypeError as e: print(e) ``` **Challenge:** Ensure your solution is efficient and makes use of descriptor objects accurately as described in the documentation.","solution":"class ReadOnlyDescriptor: def __init__(self, value): self._value = value def __get__(self, instance, owner): return self._value def __set__(self, instance, value): raise AttributeError(\\"This attribute is read-only.\\") class ValidatedDescriptor: def __init__(self): self._values = {} def __get__(self, instance, owner): return self._values.get(instance) def __set__(self, instance, value): if not isinstance(value, int): raise TypeError(\\"Expected an integer value.\\") self._values[instance] = value class CustomClass: def __init__(self, read_only_value, validated_value): self._read_only_descriptor = ReadOnlyDescriptor(read_only_value) self.validated_value = validated_value @property def read_only(self): return self._read_only_descriptor.__get__(self, CustomClass) @read_only.setter def read_only(self, value): self._read_only_descriptor.__set__(self, value) validated_value = ValidatedDescriptor() def show_attributes(self): print(f\\"Read-only attribute: {self.read_only}\\") print(f\\"Validated attribute: {self.validated_value}\\")"},{"question":"You are required to implement a Python function that utilizes the cryptographic functionalities of the `hashlib` and `secrets` modules. Your task is to create a secure hash of a given message with the following specifications: Function Signature ```python def secure_hash(message: str, key: str, personalize: str, digest_size: int = 32) -> str: pass ``` # Specifications and Requirements: 1. **Inputs**: - `message` (str): The message string that needs to be hashed. - `key` (str): A secret key for using keyed hashing. - `personalize` (str): A string for personalizing the hash to avoid repetition attacks. - `digest_size` (int): Optional, default is `32`. Specifies the size of the output hash digest (between 1 and 64). 2. **Output**: - Returns a hexadecimal string representing the secure hash of the given `message`. 3. **Implementation Details**: - Use the `hashlib.blake2b` function to create the hash. - Incorporate keyed hashing using the provided `key`. - Use the `personalize` parameter to add personalization. - Ensure the digest output size is as specified by the `digest_size`. 4. **Constraints**: - `key` must be at least 16 characters long. - `personalize` must be at least 8 characters long. - `digest_size` must be between 1 and 64. 5. **Example**: ```python message = \\"Hello, secure world!\\" key = \\"supersecretkey123\\" personalize = \\"personalizeMe\\" hashed_message = secure_hash(message, key, personalize) print(hashed_message) # Output: a secure, personalized hash string ``` # Notes: - Familiarity with the `hashlib` and `secrets` modules is required. - Handle invalid inputs (e.g., key length less than 16) appropriately by raising a `ValueError`. - Ensure your function is efficient and follows best practices for cryptographic implementations.","solution":"import hashlib def secure_hash(message: str, key: str, personalize: str, digest_size: int = 32) -> str: Returns a secure hash of the given message using the blake2b algorithm with the given key, personalization string, and digest size. :param message: The message to be hashed. :param key: A secret key for keyed hashing (must be at least 16 characters long). :param personalize: A string for personalizing the hash (must be at least 8 characters long). :param digest_size: The size of the output hash digest (must be between 1 and 64, default is 32). :return: A hexadecimal string representing the secure hash of the message. # Validate inputs if len(key) < 16: raise ValueError(\\"Key must be at least 16 characters long.\\") if len(personalize) < 8: raise ValueError(\\"Personalize must be at least 8 characters long.\\") if not (1 <= digest_size <= 64): raise ValueError(\\"Digest size must be between 1 and 64.\\") # Create blake2b hash object with the specified parameters hasher = hashlib.blake2b( key=key.encode(), person=personalize.encode(), digest_size=digest_size ) # Update the hasher with the message hasher.update(message.encode()) # Return the hexadecimal digest of the hash return hasher.hexdigest()"},{"question":"Objective To demonstrate your understanding of seaborn, you are tasked with creating a comprehensive and insightful visualization using seaborn `objects`. This assessment will test your ability to create plots, customize them, and interpret data from a complex dataset involving time series data. Dataset You will use the `seaice` dataset from seaborn, which consists of the following columns: - `Date`: A datetime column representing specific dates. - `Extent`: A numerical column representing the extent of sea ice. Requirements Write a Python function `create_seaice_plots()` that: 1. Loads the `seaice` dataset using `seaborn.load_dataset`. 2. Creates a line plot showing the sea ice extent over time, ensuring that different years are represented in different colors. 3. Facets the plot by decades, with each subplot showing data for one decade. 4. Customizes the line plot with the following properties: - Use a linewidth of 0.5 for the main lines and 1 for any additional highlight lines. - Set the color using a color scale that emphasizes differences in years within each decade. 5. Sets a proper layout size and includes informative labels including a title and axis labels. Input - `None`: The function does not take any input. Output - The function should display the plot directly. Constraints - Use only seaborn and pandas for data manipulation and visualization. - Ensure that the function runs efficiently on the provided dataset. Performance Requirements - The visualization should be aesthetically pleasing and informative. - The function should be able to handle the `seaice` dataset efficiently without performance degradation. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def create_seaice_plots(): # Implementation here ``` Example ```python create_seaice_plots() ``` The above code should display a faceted line plot with sea ice extent over time, separated by decades with different colors for each year and customized linewidths.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_seaice_plots(): # Load the seaice dataset seaice = load_dataset(\'seaice\') # Create a new column for the decade seaice[\'Decade\'] = (seaice[\'Date\'].dt.year // 10) * 10 # Create the line plot using seaborn objects p = ( so.Plot(seaice, x=\'Date\', y=\'Extent\') .facet(\'Decade\') .add(so.Line(linewidth=0.5).alpha(0.7)) .layout(size=(10, 7)) .label( title=\\"Sea Ice Extent Over Time by Decade\\", xlabel=\\"Date\\", ylabel=\\"Extent\\" ) ) # Show plot p.show()"},{"question":"Parallel Merge Sort with Multiprocessing Objective: Implement a parallel version of the merge sort algorithm using the `multiprocessing` module. Your implementation should demonstrate the use of process pools to divide sorting tasks among multiple processes and correctly merge the sorted subarrays. Task: 1. Write a function `parallel_merge_sort(arr, pool_size)` that sorts an array `arr` using the merge sort algorithm with multiprocessing. 2. Use a process pool, with size specified by `pool_size`, to perform sorting concurrently. 3. Implement the merging process synchronously to combine the sorted subarrays. Function Signature: ```python def parallel_merge_sort(arr: list, pool_size: int) -> list: pass ``` Input: - `arr`: A list of integers to be sorted. - `pool_size`: An integer representing the number of worker processes in the pool. Output: - A sorted list of integers. Constraints: - The length of `arr` can be up to 1,000,000. - The value of `pool_size` will not exceed the number of CPUs available on the system. Example: ```python input_array = [38, 27, 43, 3, 9, 82, 10] pool_size = 4 sorted_array = parallel_merge_sort(input_array, pool_size) print(sorted_array) # Output: [3, 9, 10, 27, 38, 43, 82] ``` Guidelines: 1. **Dividing the Array**: Divide the array into subarrays that can be sorted independently. 2. **Using Pool for Parallel Execution**: Utilize `multiprocessing.Pool` to sort subarrays in parallel. 3. **Merging**: Implement a merge function to combine the sorted subarrays. Hints: - Use the `Pool.map` function to distribute the sorting of subarrays across the worker processes. - You may need a helper function to perform the merge sort on subarrays. - Consider utilizing the `Pool.close()` and `Pool.join()` methods to manage the lifecycle of the pool properly. Assessment Criteria: - Correctness: The function should return the correctly sorted array. - Efficiency: The implementation should leverage parallel processing effectively to handle large arrays. - Code Quality: The code should be well-structured, readable, and make appropriate use of multiprocessing features.","solution":"import multiprocessing as mp def merge(left, right): Merge two sorted lists into a single sorted list. result = [] i = j = 0 while i < len(left) and j < len(right): if left[i] < right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result.extend(left[i:]) result.extend(right[j:]) return result def merge_sort(arr): Sequential merge sort function. if len(arr) <= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) right = merge_sort(arr[mid:]) return merge(left, right) def parallel_merge_sort(arr, pool_size): Perform parallel merge sort using multiprocessing. if len(arr) <= 1: return arr # Create a pool of processes pool = mp.Pool(processes=pool_size) # Split the array into chunks size = len(arr) // pool_size sublists = [arr[i*size:(i+1)*size] for i in range(pool_size)] # For the remaining elements after division into chunks if len(arr) % pool_size != 0: sublists.append(arr[pool_size*size:]) # Perform merge sort on each chunk in parallel sorted_sublists = pool.map(merge_sort, sublists) # Close the pool and wait for the work to finish pool.close() pool.join() # Merging process while len(sorted_sublists) > 1: extra = sorted_sublists.pop() if len(sorted_sublists) % 2 == 1 else None sorted_sublists = [merge(sorted_sublists[i], sorted_sublists[i+1]) for i in range(0, len(sorted_sublists), 2)] if extra: sorted_sublists.append(extra) return sorted_sublists[0] if sorted_sublists else arr"},{"question":"# Anomaly Detection with scikit-learn Objective Given a dataset, your task is to implement both novelty detection and outlier detection using the `scikit-learn` package. Design functions for each detection method, apply them to the dataset, and analyze their results. Datasets 1. `X_train`: Training dataset contaminated with outliers. 2. `X_test`: New observations to be classified as either inliers or outliers. You can simulate these datasets using the following code: ```python import numpy as np # Simulate training data (100 2D points clustered around [0,0]) X_train = 0.3 * np.random.randn(100, 2) # Add inliers X_train = np.r_[X_train + 2, X_train - 2] # Add outliers X_train = np.r_[X_train, np.random.uniform(low=-6, high=6, size=(20, 2))] # Simulate test data (10 new observations) X_test = 0.3 * np.random.randn(10, 2) X_test = np.r_[X_test + 2, X_test - 2] # Add some outliers X_test = np.r_[X_test, np.random.uniform(low=-6, high=6, size=(2, 2))] ``` Tasks 1. **Isolation Forest for Outlier Detection**: - Implement a function `isolation_forest_outlier_detection(X_train, X_test, contamination)` that performs outlier detection using `IsolationForest`. - The function should return predictions for the `X_test` indicating which points are outliers and which are inliers. 2. **One-Class SVM for Novelty Detection**: - Implement a function `one_class_svm_novelty_detection(X_train, X_test, nu)` that performs novelty detection using `OneClassSVM`. - The function should return predictions for the `X_test`. 3. **Elliptic Envelope for Outlier Detection**: - Implement a function `elliptic_envelope_outlier_detection(X_train, X_test, contamination)` that performs outlier detection using `EllipticEnvelope`. - The function should return predictions for the `X_test`. 4. **Local Outlier Factor for Novelty Detection**: - Implement a function `lof_novelty_detection(X_train, X_test, n_neighbors)` that performs novelty detection using `LocalOutlierFactor` with the `novelty` parameter set to `True`. - The function should return predictions for the `X_test`. Constraints and Requirements - **Input formats**: - `X_train`: A 2D numpy array of shape `(n_samples, n_features)`. - `X_test`: A 2D numpy array of shape `(m_samples, n_features)`. - `contamination`: A float representing the proportion of outliers in the dataset. - `nu`: A float parameter for `OneClassSVM` representing an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. - `n_neighbors`: An integer representing the number of neighbors to use for `LocalOutlierFactor`. - **Output format**: Each function should return a 1D numpy array of shape `(m_samples,)` with predictions where `1` represents an inlier and `-1` represents an outlier. - **Performance requirements**: - The algorithms should be able to handle datasets with at least 1000 samples efficiently. - Proper utilization of `scikit-learn` functionalities is mandatory. Example Usage ```python # Example training data X_train = np.array([[...]]) # Your specific data array # Example test data X_test = np.array([[...]]) # Isolation Forest isolation_predictions = isolation_forest_outlier_detection(X_train, X_test, contamination=0.1) # One-Class SVM svm_predictions = one_class_svm_novelty_detection(X_train, X_test, nu=0.1) # Elliptic Envelope elliptic_predictions = elliptic_envelope_outlier_detection(X_train, X_test, contamination=0.1) # Local Outlier Factor lof_predictions = lof_novelty_detection(X_train, X_test, n_neighbors=20) ``` Good luck!","solution":"from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor def isolation_forest_outlier_detection(X_train, X_test, contamination): iforest = IsolationForest(contamination=contamination, random_state=42) iforest.fit(X_train) predictions = iforest.predict(X_test) return predictions def one_class_svm_novelty_detection(X_train, X_test, nu): oc_svm = OneClassSVM(nu=nu, gamma=\'auto\') oc_svm.fit(X_train) predictions = oc_svm.predict(X_test) return predictions def elliptic_envelope_outlier_detection(X_train, X_test, contamination): ee = EllipticEnvelope(contamination=contamination, random_state=42) ee.fit(X_train) predictions = ee.predict(X_test) return predictions def lof_novelty_detection(X_train, X_test, n_neighbors): lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination=\'auto\') lof.fit(X_train) predictions = lof.predict(X_test) return predictions"},{"question":"**Objective** Implement a Python class that demonstrates the use of the `NotImplemented` constant and appropriate handling of unsupported operations between different types. The class will especially focus on implementing arithmetic operations and will need to show a clear understanding of how `NotImplemented` is used. **Question** You are required to implement a class `MagicNumber` that represents magical numbers. This class should support addition, subtraction, and multiplication with both other `MagicNumber` instances and regular integers. If any operation is not supported, the method should return `NotImplemented`. Additionally, your implementation should also handle interactions with built-in Python constants `True`, `False`, and `None` in a meaningful way. # Requirements 1. Implement a class `MagicNumber`. 2. Initialize the class with a single integer value `num`. 3. Implement the following special methods for the class: - `__add__(self, other)` - `__radd__(self, other)` - `__sub__(self, other)` - `__rsub__(self, other)` - `__mul__(self, other)` - `__rmul__(self, other)` 4. For operations with integers: - Addition: Simply add the integer to the `MagicNumber`\'s value. - Subtraction: Subtract the integer from the `MagicNumber`\'s value. - Multiplication: Multiply the integer with the `MagicNumber`\'s value. 5. For operations with `MagicNumber` instances: - Addition: Add the values of the two `MagicNumber` instances. - Subtraction: Subtract the value of the other `MagicNumber` from the first one. - Multiplication: Multiply the values of the two `MagicNumber` instances. 6. If any operation is not between `MagicNumber` and either another `MagicNumber` or integer, the operation should return `NotImplemented`. 7. Ensure that the class handles `True`, `False`, and `None` appropriately: - Treat `True` as the integer `1`. - Treat `False` as the integer `0`. - If `None` is involved in any operation, it should raise a `TypeError`. # Input and Output - **Input:** - An integer for initializing `MagicNumber`. - Integer, `MagicNumber`, or Boolean for arithmetic operations. - **Output:** - An integer result for the arithmetic operations. - `NotImplemented` where the operation is not supported. - `TypeError` when encountering `None`. # Example ```python class MagicNumber: def __init__(self, num): self.num = num def __add__(self, other): if isinstance(other, (int, MagicNumber, bool)): if other is True: other = 1 elif other is False: other = 0 if isinstance(other, MagicNumber): return MagicNumber(self.num + other.num) return MagicNumber(self.num + other) return NotImplemented def __radd__(self, other): return self.__add__(other) def __sub__(self, other): if isinstance(other, (int, MagicNumber, bool)): if other is True: other = 1 elif other is False: other = 0 if isinstance(other, MagicNumber): return MagicNumber(self.num - other.num) return MagicNumber(self.num - other) return NotImplemented def __rsub__(self, other): return self.__sub__(other) def __mul__(self, other): if isinstance(other, (int, MagicNumber, bool)): if other is True: other = 1 elif other is False: other = 0 if isinstance(other, MagicNumber): return MagicNumber(self.num * other.num) return MagicNumber(self.num * other) return NotImplemented def __rmul__(self, other): return self.__mul__(other) def __repr__(self): return f\'MagicNumber({self.num})\' # Examples: mn1 = MagicNumber(10) mn2 = MagicNumber(5) print(mn1 + mn2) # MagicNumber(15) print(mn1 - mn2) # MagicNumber(5) print(mn1 * mn2) # MagicNumber(50) print(mn1 + 5) # MagicNumber(15) print(mn1 - 3) # MagicNumber(7) print(mn1 * 2) # MagicNumber(20) print(mn1 + True) # MagicNumber(11) print(mn1 - False) # MagicNumber(10) print(mn1 * True) # MagicNumber(10) # Example that raises TypeError try: print(mn1 + None) except TypeError as e: print(e) ``` **Constraints:** - The integer value for initializing `MagicNumber` is `0 <= num <= 10^6`. - Operations should be performed efficiently and should not exceed a time complexity of `O(1)` per operation.","solution":"class MagicNumber: def __init__(self, num): self.num = int(num) def __add__(self, other): if isinstance(other, MagicNumber): return MagicNumber(self.num + other.num) if isinstance(other, (int, bool)): other = int(other) return MagicNumber(self.num + other) if other is None: raise TypeError(\\"Cannot perform arithmetic with None.\\") return NotImplemented def __radd__(self, other): return self.__add__(other) def __sub__(self, other): if isinstance(other, MagicNumber): return MagicNumber(self.num - other.num) if isinstance(other, (int, bool)): other = int(other) return MagicNumber(self.num - other) if other is None: raise TypeError(\\"Cannot perform arithmetic with None.\\") return NotImplemented def __rsub__(self, other): if isinstance(other, (int, bool)): other = int(other) return MagicNumber(other - self.num) return NotImplemented def __mul__(self, other): if isinstance(other, MagicNumber): return MagicNumber(self.num * other.num) if isinstance(other, (int, bool)): other = int(other) return MagicNumber(self.num * other) if other is None: raise TypeError(\\"Cannot perform arithmetic with None.\\") return NotImplemented def __rmul__(self, other): return self.__mul__(other) def __repr__(self): return f\'MagicNumber({self.num})\'"},{"question":"# Question: Advanced Index Manipulation with Pandas Your task is to implement a function `transform_and_query_indexes` that performs a series of operations on a DataFrame with pandas Index objects. You will be given a DataFrame `df` and should perform the following steps: 1. **Reindex Data**: Reindex `df` using a given new index and fill any missing values with `0`. 2. **Modify Index**: Rename the index of the DataFrame using a specified new name. 3. **Filter Data**: Filter the rows where the index values are within a given range. 4. **Query Data**: Aggregate and return the count of unique values in a specific column for the filtered DataFrame. # Function Signature ```python import pandas as pd def transform_and_query_indexes(df: pd.DataFrame, new_index: pd.Index, index_name: str, start_val, end_val, column_name: str) -> int: pass ``` # Parameters - `df (pd.DataFrame)`: Input DataFrame. - `new_index (pd.Index)`: The new index to reindex `df` with. If any data points are missing in the new index, they should be filled with `0`. - `index_name (str)`: The new name for the DataFrame\'s index. - `start_val`: The start value of the index range to filter the data. - `end_val`: The end value of the index range to filter the data. - `column_name (str)`: The column name for which unique values should be counted. # Constraints - Ensure the DataFrame is reindexed correctly replacing missing data points with `0`. - The start and end value for filtering the index range are inclusive. - You may assume the index values are comparable (e.g., numeric or datetime). - The column specified for counting unique values exists in `df`. # Example Given the following DataFrame `df` and `new_index`: ```python data = { \'A\': [10, 20, 30, 40], \'B\': [\'x\', \'y\', \'z\', \'x\'] } index = pd.Index([1, 2, 3, 4]) df = pd.DataFrame(data, index=index) new_index = pd.Index([1, 2, 4, 5]) index_name = \'new_index\' start_val = 2 end_val = 4 column_name = \'B\' ``` Here\'s how the function should perform: 1. Reindexing `df` with `new_index` and filling missing values with `0` results in: ``` A B 1 10 x 2 20 y 4 40 x 5 0 0 ``` 2. Renaming the index to `new_index`: ``` A B new_index 1 10 x 2 20 y 4 40 x 5 0 0 ``` 3. Filtering rows with index values within `2` and `4`: ``` A B new_index 2 20 y 4 40 x ``` 4. Counting unique values in column `B` results in: `2`. Thus, the function should return `2`. # Implementation Write the implementation of the `transform_and_query_indexes` function below. ```python import pandas as pd def transform_and_query_indexes(df: pd.DataFrame, new_index: pd.Index, index_name: str, start_val, end_val, column_name: str) -> int: # Reindex the DataFrame and fill missing values with 0 df_reindexed = df.reindex(new_index).fillna(0) # Rename the index of the DataFrame df_reindexed.index.name = index_name # Filter rows where the index is within the given range df_filtered = df_reindexed.loc[start_val:end_val] # Count the number of unique values in the specified column unique_count = df_filtered[column_name].nunique() return unique_count ``` # Testing Use the provided example to verify that your function works correctly.","solution":"import pandas as pd def transform_and_query_indexes(df: pd.DataFrame, new_index: pd.Index, index_name: str, start_val, end_val, column_name: str) -> int: # Reindex the DataFrame and fill missing values with 0 df_reindexed = df.reindex(new_index).fillna(0) # Rename the index of the DataFrame df_reindexed.index.name = index_name # Filter rows where the index is within the given range df_filtered = df_reindexed.loc[start_val:end_val] # Count the number of unique values in the specified column unique_count = df_filtered[column_name].nunique() return unique_count"},{"question":"# Email Parsing and Analysis The `email.parser` module in Python provides several classes and functions to parse email messages from different kinds of data sources such as bytes-like objects, strings, and files. In this exercise, you will use this parsing capability to extract specific information from email messages. # Problem Statement Implement a function called `extract_email_info` that accepts a bytes-like object representing the raw data of an email message and returns a dictionary with the following information: - **Subject**: The subject of the email. - **From**: The sender\'s email address. - **To**: The recipient\'s email address. - **Date**: The date the email was sent. - **Body**: The main content of the email. - **IsMultipart**: A boolean indicating whether the email is a multipart message. # Function Signature ```python def extract_email_info(raw_email: bytes) -> dict: pass ``` # Input - `raw_email`: A bytes-like object containing the raw data of an email message. This data may include headers and body content. # Output - A dictionary with the following keys and their corresponding values extracted from the email: - \\"Subject\\": (string) The subject of the email. - \\"From\\": (string) The sender\'s email address. - \\"To\\": (string) The recipient\'s email address. - \\"Date\\": (string) The date the email was sent. - \\"Body\\": (string) The main content of the email. - \\"IsMultipart\\": (boolean) A boolean indicating whether the email is a multipart message. # Constraints - You should handle both MIME and non-MIME messages. - For multipart messages, the body should be the concatenation of all text/plain parts. # Example ```python raw_email = b\\"From: example@domain.comrnTo: recipient@domain.comrnSubject: Test EmailrnDate: Mon, 29 Nov 2021 12:34:56 -0000rnrnThis is the body of the email.\\" expected_output = { \\"Subject\\": \\"Test Email\\", \\"From\\": \\"example@domain.com\\", \\"To\\": \\"recipient@domain.com\\", \\"Date\\": \\"Mon, 29 Nov 2021 12:34:56 -0000\\", \\"Body\\": \\"This is the body of the email.\\", \\"IsMultipart\\": False } assert extract_email_info(raw_email) == expected_output raw_multipart_email = b\\"From: example@domain.comrnTo: recipient@domain.comrnSubject: Multipart EmailrnDate: Mon, 29 Nov 2021 12:34:56 -0000rnContent-Type: multipart/mixed; boundary=\\"simple boundary\\"rnrn--simple boundaryrnContent-Type: text/plainrnrnThis is the body of part 1.rn--simple boundaryrnContent-Type: text/plainrnrnThis is the body of part 2.rn--simple boundary--\\" expected_multipart_output = { \\"Subject\\": \\"Multipart Email\\", \\"From\\": \\"example@domain.com\\", \\"To\\": \\"recipient@domain.com\\", \\"Date\\": \\"Mon, 29 Nov 2021 12:34:56 -0000\\", \\"Body\\": \\"This is the body of part 1.nThis is the body of part 2.\\", \\"IsMultipart\\": True } assert extract_email_info(raw_multipart_email) == expected_multipart_output ``` # Notes 1. Use the `email.parser.BytesParser` and `email.policy` to parse the raw email data. 2. Ensure your implementation correctly handles multipart messages by concatenating the text/plain parts of the body.","solution":"import email from email import policy from email.parser import BytesParser def extract_email_info(raw_email: bytes) -> dict: msg = BytesParser(policy=policy.default).parsebytes(raw_email) def get_body(msg): if msg.is_multipart(): parts = [part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\', errors=\'replace\') for part in msg.iter_parts() if part.get_content_type() == \'text/plain\'] return \'n\'.join(parts) else: return msg.get_payload(decode=True).decode(msg.get_content_charset() or \'utf-8\', errors=\'replace\') email_info = { \\"Subject\\": msg[\'subject\'], \\"From\\": msg[\'from\'], \\"To\\": msg[\'to\'], \\"Date\\": msg[\'date\'], \\"Body\\": get_body(msg), \\"IsMultipart\\": msg.is_multipart() } return email_info"},{"question":"**Title:** URL Request and Parsing with `urllib` **Objective:** Demonstrate an understanding of the `urllib` module for handling and parsing URLs in Python. **Problem Statement:** You are tasked with implementing a function `fetch_url_data(url: str) -> dict` that performs the following operations: 1. Uses the `urllib.request` module to make a GET request to the given URL. 2. Parses the given URL into its components using `urllib.parse`. 3. Retrieves and parses the query parameters from the URL. 4. Returns a dictionary containing: - The HTTP response status code. - The final URL after all redirections. - Parsed components of the URL. - Query parameters as a dictionary. Your implementation should handle potential exceptions and ensure that the URL is quoted correctly. **Function Signature:** ```python def fetch_url_data(url: str) -> dict: ``` **Input:** - `url`: A string representing a URL (e.g., \\"http://example.com/path?param=value&another_param=another_value\\"). **Output:** - A dictionary containing the following keys: - `status_code`: Integer HTTP response status code. - `final_url`: String final URL after redirection (if any). - `parsed_url`: Dictionary containing the parsed components (`scheme`, `netloc`, `path`, `params`, `query`, `fragment`). - `query_params`: Dictionary of query parameters. **Constraints:** 1. Assume the URL provided is well-formed. 2. Handle possible exceptions such as `URLError` or `HTTPError`. 3. Ensure all components and query parameters are correctly parsed and returned. **Example:** ```python url = \\"http://example.com/path?param=value&another_param=another_value\\" result = fetch_url_data(url) # Expected output { \\"status_code\\": 200, \\"final_url\\": \\"http://example.com/path?param=value&another_param=another_value\\", \\"parsed_url\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/path\\", \\"params\\": \\"\\", \\"query\\": \\"param=value&another_param=another_value\\", \\"fragment\\": \\"\\" }, \\"query_params\\": { \\"param\\": \\"value\\", \\"another_param\\": \\"another_value\\" } } ``` **Note:** This question assesses the ability to understand and use the `urllib` module for making URL requests, handling responses, and parsing URLs into their components. It also tests the handling of query parameters and exceptions.","solution":"import urllib.request import urllib.parse def fetch_url_data(url: str) -> dict: Performs a GET request to the given URL, parses the URL, and retrieves query parameters. Returns a dictionary with the HTTP response status code, final URL, parsed URL components, and query parameters. try: # Perform URL request response = urllib.request.urlopen(url) status_code = response.getcode() # Get final URL after redirections final_url = response.geturl() # Parse the URL parsed_url = urllib.parse.urlparse(final_url) parsed_url_components = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment } # Parse query parameters query_params = urllib.parse.parse_qs(parsed_url.query) # Flatten single value lists to just strings for key in query_params: if len(query_params[key]) == 1: query_params[key] = query_params[key][0] return { \'status_code\': status_code, \'final_url\': final_url, \'parsed_url\': parsed_url_components, \'query_params\': query_params } except urllib.error.URLError as e: return { \'status_code\': e.code if hasattr(e, \'code\') else None, \'final_url\': None, \'parsed_url\': None, \'query_params\': None, \'error\': str(e) }"},{"question":"# Question: **Title: Python Interactive Mode and Error Handling** Problem Statement You have a set of tasks to perform in the interactive mode of Python as per the provided documentation. The goal is to demonstrate your understanding of executing Python scripts, handling errors, and working with interactive startup files. 1. **Executable Python Script:** Create a Python script that prints \\"Hello, World!\\" when executed. Make this script executable in Unix-based systems. 2. **Error Handling:** Write a Python function called `safe_divide` that takes two arguments, `a` and `b`, and returns their division result `a / b`. Ensure that the function properly handles the case where `b` is zero by catching the appropriate exception and returning the string \\"Division by zero error.\\" 3. **Interactive Startup File:** Create a Python script that will act as your interactive startup file. This script should: - Import the `math` module. - Set the primary prompt string (`sys.ps1`) to \\">>> MyPython: \\". - Print \\"Startup file loaded.\\" when the interactive Python shell starts. Input and Output Formats 1. **Executable Python Script:** - **Input:** None - **Output:** The script should print \\"Hello, World!\\" when executed. 2. **Error Handling:** - **Input:** Two integers, `a` and `b`. - **Output:** A float representing the result of `a / b`, or the string \\"Division by zero error\\" if `b` is zero. 3. **Interactive Startup File:** - **Input:** None (Executed via Python interactive mode). - **Output:** The interactive shell should display a custom prompt and print \\"Startup file loaded.\\" upon starting. Constraints 1. For the first task, ensure Unix-based system compatibility by adding the appropriate shebang line and making the file executable with the `chmod` command. 2. For the second task, use a `try` and `except` block to handle the division by zero error. 3. For the third task, ensure that the script is set to be used as the PYTHONSTARTUP file and includes the necessary imports and prompt customizations. Example: ```python # Example for Task 2: print(safe_divide(10, 2)) # Output: 5.0 print(safe_divide(10, 0)) # Output: Division by zero error ``` Good luck and prove your understanding of Python\'s interactive mode and error handling!","solution":"# Task 1: Executable Python Script # Save this script in a file named hello.py and use the terminal command `chmod +x hello.py` to make it executable. #!/usr/bin/env python3 print(\\"Hello, World!\\") # Task 2: Error Handling def safe_divide(a, b): Returns the result of dividing a by b. If division by zero is attempted, return the string \\"Division by zero error\\". try: return a / b except ZeroDivisionError: return \\"Division by zero error\\" # Task 3: Interactive Startup File # Save this script as startup.py and set the environment variable: # export PYTHONSTARTUP=/path/to/startup.py to use it as the interactive startup file. import sys import math sys.ps1 = \\">>> MyPython: \\" print(\\"Startup file loaded.\\")"},{"question":"You are required to implement a function that constructs and returns a valid MIME email message using the `email.mime` package. The email will consist of both text and an image attachment. You will be assessed on your ability to use the MIME classes effectively to structure and encode the email content appropriately. **Your task:** Implement the `create_mime_email` function that takes the following parameters: 1. `sender` (str): The email address of the sender. 2. `recipient` (str): The email address of the recipient. 3. `subject` (str): The subject of the email. 4. `body` (str): The plain text body of the email. 5. `image_data` (bytes): The binary data of the image to attach. 6. `image_type` (str): The MIME subtype of the image (e.g., \\"jpeg\\", \\"png\\"). The function should: - Construct a multipart email containing both text and image parts. - Properly encode the image attachment in base64 format. - Set the appropriate headers for each part. - Return the resulting email message as a string. **Constraints:** - The `sender`, `recipient`, and `subject` must be valid non-empty strings. - The `body` must be a valid non-empty string. - The `image_data` must be non-empty bytes. - The `image_type` must be a valid MIME subtype for images (e.g., \\"jpeg\\", \\"png\\"). **Example Usage:** ```python from email import policy from email.parser import BytesParser def create_mime_email(sender, recipient, subject, body, image_data, image_type): # Your code here # Example data sender = \\"example@example.com\\" recipient = \\"recipient@example.com\\" subject = \\"Subject of the Email\\" body = \\"This is the body of the email.\\" image_data = open(\\"example_image.png\\", \\"rb\\").read() image_type = \\"png\\" # Create MIME email email_message = create_mime_email(sender, recipient, subject, body, image_data, image_type) # Print the MIME email message print(email_message) ``` **Expected Output:** A correctly formatted MIME email string that includes both the text body and the image attachment. The string should be suitable for sending as an email using an SMTP server.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email import encoders def create_mime_email(sender, recipient, subject, body, image_data, image_type): # Validate input parameters if not all([sender, recipient, subject, body, image_data, image_type]): raise ValueError(\\"All input parameters must be provided and non-empty\\") # Create the root message with email headers msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Attach the plain text body text_part = MIMEText(body, \'plain\') msg.attach(text_part) # Attach the image image_part = MIMEImage(image_data, _subtype=image_type) encoders.encode_base64(image_part) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=f\'image.{image_type}\') msg.attach(image_part) return msg.as_string()"},{"question":"**Coding Assessment Question:** # Task You are required to implement a class `AsyncManager` that demonstrates the use of asynchronous iterators, callable objects, dynamic attributes, and mathematical operations. # Requirements: 1. Implement an `AsyncManager` class. 2. The class should have the following methods: - `__init__(self, start: int, end: int)`: Initializes the class with a range of numbers [start, end]. - `__aiter__(self)`: Should return an instance of an asynchronous iterator. - `__anext__(self)`: Should asynchronously return the next number in the range. - `set_multiplier(self, value: int)`: Sets the multiplier for the numbers. This should modify a dynamically set attribute. - `get_multiplier(self)`: Gets the current multiplier. - `calculate_power(self, base: int, exp: int, mod: int)`: Returns the result of `(base**exp) % mod` using the `pow()` function. # Expected Input and Output Format - `AsyncManager` initialization: `AsyncManager(start=1, end=10)` - `__aiter__()` and `__anext__()` should support asynchronous iteration. - `set_multiplier(value: int)` and `get_multiplier()` utilize dynamic attributes (`setattr` and `getattr`). - `calculate_power(base, exp, mod)` should return the calculated power modulo value. # Example Usage ```python import asyncio class AsyncManager: # Your implementation here # Initialize AsyncManager with range 1 to 10 async_manager = AsyncManager(1, 10) # Configure the manager async_manager.set_multiplier(5) # Print the multiplier print(async_manager.get_multiplier()) # Output should be 5 # Calculate power mod result = async_manager.calculate_power(3, 4, 5) print(result) # Output should be (3**4) % 5, i.e., 1 # Asynchronous iteration async def test_async_iteration(): async for number in async_manager: print(number) # Run the asynchronous code asyncio.run(test_async_iteration()) ``` # Constraints - Make sure to handle exceptions appropriately. - Ensure the asynchronous iterator works correctly with Python\'s `async for` syntax. # Performance Requirements - The class should be able to handle ranges up to (10^6). Please write the complete implementation of the `AsyncManager` class that meets the above requirements.","solution":"import asyncio class AsyncManager: def __init__(self, start: int, end: int): self.start = start self.end = end self.current = start self.multiplier = 1 def __aiter__(self): return self async def __anext__(self): if self.current > self.end: raise StopAsyncIteration value = self.current self.current += 1 await asyncio.sleep(0) # Yield control to the event loop return value def set_multiplier(self, value: int): setattr(self, \'multiplier\', value) def get_multiplier(self): return getattr(self, \'multiplier\') def calculate_power(self, base: int, exp: int, mod: int): return pow(base, exp, mod) # Example Usage async def test_async_iteration(manager): async for number in manager: print(number) # Initialize AsyncManager with range 1 to 10 async_manager = AsyncManager(1, 10) # Configure the manager async_manager.set_multiplier(5) # Print the multiplier print(async_manager.get_multiplier()) # Output should be 5 # Calculate power mod result = async_manager.calculate_power(3, 4, 5) print(result) # Output should be (3**4) % 5, i.e., 1 # Run the asynchronous code asyncio.run(test_async_iteration(async_manager))"},{"question":"**Title:** Implementing a Custom Time Logger with Format Conversion **Objective:** Create a custom time logging class that can log the current time, display it in various formats, and calculate the elapsed time between logs. # Problem Description You need to implement a class called `TimeLogger` that provides functionalities to log the current time, convert it to a human-readable string format, parse a time string back to a `struct_time` object, and calculate the elapsed time between two logs. # Instructions 1. **Class Definition:** Define a class `TimeLogger` that has the following methods: 2. **Methods to Implement:** - `__init__(self)`: Initialize an empty list to store log entries. - `log_time(self)`: Log the current time. Use `time.time()` to get the current time and store it as a float in the list of log entries. - `get_last_log(self) -> float`: Return the most recent log entry. If no entries are logged, return `None`. - `format_last_log(self, format_string: str) -> str`: Return the most recent log entry formatted as a string according to the provided format string. If no entries are logged, return `None`. Use `time.strftime()` for formatting. - `parse_time_string(self, time_string: str, format_string: str) -> time.struct_time`: Parse a given time string according to the format string and return a `struct_time` object. Use `time.strptime()` for parsing. - `get_elapsed_time(self) -> float`: Calculate the elapsed time in seconds between the two most recent log entries. If fewer than two logs are recorded, return `None`. # Constraints - Assume the format strings provided are valid format strings as per `time.strftime` and `time.strptime`. - You can assume the log entries are always in chronological order. - The system\'s clock will not be adjusted between logs. # Example Usage ```python import time class TimeLogger: def __init__(self): # Initialize an empty list to store log entries self.logs = [] def log_time(self): # Log the current time self.logs.append(time.time()) def get_last_log(self) -> float: # Return the most recent log entry or None if no entries return self.logs[-1] if self.logs else None def format_last_log(self, format_string: str) -> str: # Return the most recent log entry formatted as a string if not self.logs: return None last_log = time.localtime(self.logs[-1]) return time.strftime(format_string, last_log) def parse_time_string(self, time_string: str, format_string: str) -> time.struct_time: # Parse a given time string to a struct_time object return time.strptime(time_string, format_string) def get_elapsed_time(self) -> float: # Calculate the elapsed time between the two most recent log entries if len(self.logs) < 2: return None return self.logs[-1] - self.logs[-2] # Example usage logger = TimeLogger() logger.log_time() time.sleep(2) # Simulate some delay logger.log_time() print(logger.get_last_log()) # Prints the last logged time in seconds print(logger.format_last_log(\\"%Y-%m-%d %H:%M:%S\\")) # Prints the last logged time formatted as a string print(logger.get_elapsed_time()) # Prints the elapsed time in seconds between the last two logs ``` # Testing and Validation Ensure your implementation handles the following cases: 1. Logging time and retrieving logs correctly. 2. Formatting the logged time as a structured string. 3. Parsing a time string back to `struct_time`. 4. Calculating elapsed time between consecutive logs accurately. 5. Handling cases where there are insufficient logs for operations gracefully.","solution":"import time class TimeLogger: def __init__(self): # Initialize an empty list to store log entries self.logs = [] def log_time(self): # Log the current time self.logs.append(time.time()) def get_last_log(self) -> float: # Return the most recent log entry or None if no entries return self.logs[-1] if self.logs else None def format_last_log(self, format_string: str) -> str: # Return the most recent log entry formatted as a string if not self.logs: return None last_log = time.localtime(self.logs[-1]) return time.strftime(format_string, last_log) def parse_time_string(self, time_string: str, format_string: str) -> time.struct_time: # Parse a given time string to a struct_time object return time.strptime(time_string, format_string) def get_elapsed_time(self) -> float: # Calculate the elapsed time between the two most recent log entries if len(self.logs) < 2: return None return self.logs[-1] - self.logs[-2]"},{"question":"You are tasked with creating a Python script that fetches data from a secure API endpoint, handles potential redirects, and manages cookies and proxies. The task involves using the `urllib.request` library to achieve the following: 1. **Implement a Function `fetch_data`**: - Input: - `url` (string) - the endpoint to fetch data from. - `post_data` (dict, optional) - data to be sent in case of a POST request (default is None, implies a GET request). - `headers` (dict, optional) - custom headers to be added to the request (default is None). - `proxy` (dict, optional) - proxy settings mapped by protocol (default is None). - `auth` (tuple, optional) - tuple containing username and password for Basic HTTP Authentication (default is None). - `cookie` (dict, optional) - pre-existing cookies to be sent with the request (default is None). - Output: - Returns the response content (string) of the URL. - Raises `URLError` in case of a problem fetching the URL. 2. **Constraints**: - The function must handle potential HTTP redirects and automatically follow them. - If `post_data` is provided, the function should make a POST request; otherwise, it should make a GET request. - Ensure headers include a custom `User-Agent`. - If `auth` is provided, use HTTP Basic Authentication. - If `proxy` is provided, configure the OpenerDirector to use the specified proxy. - Manage and send cookies across multiple requests if `cookie` is provided. 3. **Performance Requirement**: - Ideally, the solution should handle responses and errors gracefully, ensuring resources are closed properly. Here is the starting skeleton for your function: ```python import urllib.request import urllib.parse import http.cookiejar def fetch_data(url, post_data=None, headers=None, proxy=None, auth=None, cookie=None): # Create an opener instance opener = urllib.request.build_opener() # Handle proxy settings if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) opener.add_handler(proxy_handler) # Handle authentication if auth: auth_handler = urllib.request.HTTPBasicAuthHandler() auth_handler.add_password(None, url, auth[0], auth[1]) opener.add_handler(auth_handler) # Handle cookies cookie_jar = http.cookiejar.CookieJar() if cookie: opener.add_handler(urllib.request.HTTPCookieProcessor(cookie_jar)) for name, value in cookie.items(): cookie_jar.set_cookie(http.cookiejar.Cookie( version=0, name=name, value=value, port=None, port_specified=False, domain=urllib.parse.urlparse(url).netloc, domain_specified=True, domain_initial_dot=False, path=\'/\', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest=None, rfc2109=False )) # Add headers if headers: opener.addheaders = [(k, v) for k, v in headers.items()] # Make the request try: if post_data: data = urllib.parse.urlencode(post_data).encode(\'utf-8\') req = urllib.request.Request(url, data=data) else: req = urllib.request.Request(url) # Ensure User-Agent is set req.add_header(\'User-Agent\', \'urllib-example/0.1\') with opener.open(req) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: raise e # Example usage if __name__ == \\"__main__\\": url = \\"https://example.com/api\\" post_data = {\\"key\\": \\"value\\"} headers = {\\"Referer\\": \\"https://example.com\\"} proxy = {\\"http\\": \\"http://proxy.example.com:8080\\"} auth = (\\"username\\", \\"password\\") cookie = {\\"session_id\\": \\"abc123\\"} try: content = fetch_data(url, post_data, headers, proxy, auth, cookie) print(\\"Response content:\\", content) except Exception as e: print(\\"Failed to fetch data:\\", str(e)) ``` This question tests: - Students\' ability to handle networking in Python. - Understanding of HTTP methods, headers, authentication, and proxies. - Management of cookies and custom URL handlers. - Error handling and resource management in network communications.","solution":"import urllib.request import urllib.parse import http.cookiejar def fetch_data(url, post_data=None, headers=None, proxy=None, auth=None, cookie=None): # Create an opener instance opener = urllib.request.build_opener() # Handle proxy settings if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) opener.add_handler(proxy_handler) # Handle authentication if auth: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, auth[0], auth[1]) auth_handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener.add_handler(auth_handler) # Handle cookies cookie_jar = http.cookiejar.CookieJar() if cookie: opener.add_handler(urllib.request.HTTPCookieProcessor(cookie_jar)) for name, value in cookie.items(): cookie = http.cookiejar.Cookie( version=0, name=name, value=value, port=None, port_specified=False, domain=urllib.parse.urlparse(url).netloc, domain_specified=True, domain_initial_dot=False, path=\'/\', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest=None, rfc2109=False ) cookie_jar.set_cookie(cookie) else: opener.add_handler(urllib.request.HTTPCookieProcessor(cookie_jar)) # Add headers req_headers = {\'User-Agent\': \'urllib-example/0.1\'} if headers: req_headers.update(headers) # Make the request try: if post_data: data = urllib.parse.urlencode(post_data).encode(\'utf-8\') req = urllib.request.Request(url, data=data, headers=req_headers) else: req = urllib.request.Request(url, headers=req_headers) response = opener.open(req) return response.read().decode(\'utf-8\') except urllib.error.URLError as e: raise e # Example usage if __name__ == \\"__main__\\": url = \\"https://example.com/api\\" post_data = {\\"key\\": \\"value\\"} headers = {\\"Referer\\": \\"https://example.com\\"} proxy = {\\"http\\": \\"http://proxy.example.com:8080\\"} auth = (\\"username\\", \\"password\\") cookie = {\\"session_id\\": \\"abc123\\"} try: content = fetch_data(url, post_data, headers, proxy, auth, cookie) print(\\"Response content:\\", content) except Exception as e: print(\\"Failed to fetch data:\\", str(e))"},{"question":"# PyTorch Benchmarking Task **Objective**: To assess your understanding of PyTorch\'s performance capabilities and your ability to work with performance benchmarking scripts. **Task**: Write a PyTorch script that performs the following: 1. **Configurations**: - Define a set of configurations for running performance benchmarks on the `Huggingface` suite. - The configurations should include combinations of `precision` (`fp32`, `amp`), `performance type` (`inference`, `training`), and `backend` (`eager`, `inductor`). 2. **Execution**: - Implement a function `run_performance_test` that executes the benchmarking script for each configuration using the provided command line template. - The function should accept parameters for `precision`, `performance type`, and `backend`. 3. **Parsing Results**: - Implement a function `parse_results` to extract and return the `geometric mean speedup`, `mean compilation time`, and `peak memory footprint compression ratio` from the benchmark output logs. 4. **Output**: - For each configuration, display the extracted performance metrics in a clear and concise format. **Command Line Template**: ``` python benchmarks/dynamo/huggingface.py --performance --inference --amp --backend inductor --device cuda ``` **Constraints**: - Assume the script is run in an environment with a GPU that supports PyTorch 2.0. - You may mock the output if necessary for testing purposes. # Function Specifications **Function 1**: `run_performance_test(precision: str, performance_type: str, backend: str) -> str` - **Input**: - `precision` (str): The precision type (`fp32`, `amp`). - `performance_type` (str): The performance type (`inference`, `training`). - `backend` (str): The backend type (`eager`, `inductor`). - **Output**: - `output_log` (str): Raw output log from the performance benchmarking script. **Function 2**: `parse_results(output_log: str) -> dict` - **Input**: - `output_log` (str): Raw output log from the performance benchmarking script. - **Output**: - `metrics` (dict): Dictionary containing `geometric mean speedup`, `mean compilation time`, and `peak memory footprint compression ratio`. # Example Usage ```python # Example configurations configurations = [ (\'amp\', \'inference\', \'inductor\'), (\'fp32\', \'training\', \'eager\'), # Add more configurations as needed ] for precision, performance_type, backend in configurations: output_log = run_performance_test(precision, performance_type, backend) metrics = parse_results(output_log) print(f\\"Configuration: {precision}, {performance_type}, {backend}\\") print(f\\"Geometric Mean Speedup: {metrics[\'geometric_mean_speedup\']}\\") print(f\\"Mean Compilation Time: {metrics[\'mean_compilation_time\']}\\") print(f\\"Peak Memory Footprint Compression Ratio: {metrics[\'peak_memory_footprint_compression_ratio\']}\\") print() ``` # Notes: - Ensure your script handles different configurations robustly. - Focus on clean and modular code. - You can assume the performance script and logs are in a format that makes parsing feasible as described.","solution":"import subprocess def run_performance_test(precision: str, performance_type: str, backend: str) -> str: Runs the PyTorch performance test with the specified configuration. Args: precision (str): The precision type (\'fp32\', \'amp\'). performance_type (str): The performance type (\'inference\', \'training\'). backend (str): The backend type (\'eager\', \'inductor\'). Returns: output_log (str): Raw output log from the performance benchmarking script. command = [ \\"python\\", \\"benchmarks/dynamo/huggingface.py\\", \\"--performance\\", f\\"--{performance_type}\\", f\\"--{precision}\\", f\\"--backend\\", backend, \\"--device\\", \\"cuda\\" ] process = subprocess.run(command, capture_output=True, text=True) return process.stdout def parse_results(output_log: str) -> dict: Parses the output log from the performance benchmarking script. Args: output_log (str): Raw output log from the performance benchmarking script. Returns: metrics (dict): Dictionary containing \'geometric_mean_speedup\', \'mean_compilation_time\', and \'peak_memory_footprint_compression_ratio\'. metrics = { \'geometric_mean_speedup\': None, \'mean_compilation_time\': None, \'peak_memory_footprint_compression_ratio\': None } for line in output_log.split(\'n\'): if \'Geometric Mean Speedup\' in line: metrics[\'geometric_mean_speedup\'] = float(line.split(\':\')[-1].strip()) elif \'Mean Compilation Time\' in line: metrics[\'mean_compilation_time\'] = float(line.split(\':\')[-1].strip()) elif \'Peak Memory Footprint Compression Ratio\' in line: metrics[\'peak_memory_footprint_compression_ratio\'] = float(line.split(\':\')[-1].strip()) return metrics"},{"question":"# Objective: Implement a Gaussian Mixture Model to cluster a given dataset and evaluate its performance using the Bayesian Information Criterion (BIC). Your solution should also demonstrate the effect of different initialization methods on model convergence. # Instructions: 1. **Function Signature**: ```python def gmm_clustering(data: np.ndarray, n_components: int, init_method: str) -> dict: pass ``` 2. **Input**: - `data` (np.ndarray): 2D array where each row represents a data point and each column represents a feature. - `n_components` (int): The number of mixture components to use. - `init_method` (str): Initialization method to use. One of `\\"kmeans\\"`, `\\"kmeans++\\"`, `\\"random_from_data\\"`, or `\\"random\\"`. 3. **Output**: - A dictionary with the following keys: - `\\"bic\\"`: The BIC value of the fitted model. - `\\"converged\\"`: Boolean indicating whether the model converged or not. - `\\"n_iter\\"`: Number of iterations the algorithm ran. - `\\"weights\\"`: The weights of each component (array). - `\\"means\\"`: The mean of each component (array). - `\\"covariances\\"`: The covariance of each component (array). 4. **Tasks**: - Implement and fit a `GaussianMixture` model using the input data and specified number of components. - Use the initialization method specified by `init_method`. - Assess and return the model\'s BIC value, convergence status, number of iterations, weights, means, and covariances. # Example Usage: ```python import numpy as np data = np.random.rand(100, 2) n_components = 3 init_method = \'kmeans\' result = gmm_clustering(data, n_components, init_method) print(result) # Output: # {\'bic\': ... # \'converged\': ... # \'n_iter\': ... # \'weights\': ... # \'means\': ... # \'covariances\': ... } ``` # Constraints: - Use `sklearn.mixture.GaussianMixture` for model implementation. - Handle potential exceptions, such as invalid input data or unsupported initialization methods. - Focus on code efficiency and clarity. # Notes: - The `sklearn.mixture.GaussianMixture` class provides methods and attributes to retrieve the required model parameters. - Evaluate the effect of different initialization methods by observing changes in the BIC value and convergence behavior.","solution":"import numpy as np from sklearn.mixture import GaussianMixture def gmm_clustering(data: np.ndarray, n_components: int, init_method: str) -> dict: Fit a Gaussian Mixture Model to the data and return the evaluation metrics. Parameters: data (np.ndarray): 2D array where each row represents a data point and each column represents a feature. n_components (int): The number of mixture components to use. init_method (str): Initialization method, one of \\"kmeans\\", \\"kmeans++\\", \\"random_from_data\\", or \\"random\\". Returns: dict: A dictionary containing the BIC, convergence status, number of iterations, weights, means, and covariances. # Mapping init_method to covariance_type argument in GaussianMixture init_params = { \'kmeans\': \'kmeans\', \'kmeans++\': \'kmeans\', \'random_from_data\': \'random\', \'random\': \'random\' } if init_method not in init_params: raise ValueError(\\"Unsupported initialization method\\") # Initialize the model gmm = GaussianMixture(n_components=n_components, init_params=init_params[init_method]) # Fit the model gmm.fit(data) # Collecting the required outputs result = { \\"bic\\": gmm.bic(data), \\"converged\\": gmm.converged_, \\"n_iter\\": gmm.n_iter_, \\"weights\\": gmm.weights_, \\"means\\": gmm.means_, \\"covariances\\": gmm.covariances_ } return result"},{"question":"**Objective:** You are required to write a function that initializes a simple two-layer neural network in PyTorch and initializes the weights and biases using random values. The network should be capable of basic forward propagation for a given input. **Function Name:** `init_simple_neural_network` **Function Signature:** `def init_simple_neural_network(input_dim: int, hidden_dim: int, output_dim: int) -> torch.nn.Module` **Inputs:** - `input_dim` (int): The size of the input layer. - `hidden_dim` (int): The size of the hidden layer. - `output_dim` (int): The size of the output layer. **Outputs:** - The function should return an instance of `torch.nn.Module` that represents the neural network model with initialized weights and biases. **Requirements:** 1. Your neural network should have one hidden layer and should use ReLU activation. 2. Initialize weights of the model using a normal distribution with mean=0.0 and std=0.01. 3. Initialize biases of the model using a constant value of 0.0. **Example Usage:** ```python import torch # Define the network dimensions input_dim = 10 hidden_dim = 5 output_dim = 2 # Initialize the neural network model = init_simple_neural_network(input_dim, hidden_dim, output_dim) # Create a random input tensor input_tensor = torch.rand((1, input_dim)) # Perform forward pass output = model(input_tensor) print(output) ``` **Constraints:** - Your code should correctly handle any positive integer values for `input_dim`, `hidden_dim`, and `output_dim`. - Make sure to use PyTorch\'s built-in random modules to initialize the weights and biases. **Notes:** - You may assume that the student is familiar with basic PyTorch modules and how to define simple neural network architectures. - The primary focus of this exercise is to ensure that the student can effectively utilize PyTorch\'s random and neural network initialization capabilities.","solution":"import torch import torch.nn as nn def init_simple_neural_network(input_dim: int, hidden_dim: int, output_dim: int) -> torch.nn.Module: Initializes a simple two-layer neural network with the given dimensions. Args: - input_dim (int): The size of the input layer. - hidden_dim (int): The size of the hidden layer. - output_dim (int): The size of the output layer. Returns: - torch.nn.Module: The initialized neural network model. class SimpleNeuralNetwork(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, output_dim) # Initialize weights and biases nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01) nn.init.constant_(self.fc1.bias, 0.0) nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01) nn.init.constant_(self.fc2.bias, 0.0) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x model = SimpleNeuralNetwork(input_dim, hidden_dim, output_dim) return model"},{"question":"# Question: Implementing a Custom Attention Mechanism using Mask Utilities Objective Your task is to implement a custom attention mechanism that uses provided mask utility functions from the PyTorch `torch.nn.attention.flex_attention` module. The custom attention mechanism should apply various masks to an input tensor and return the final masked output. Description You need to implement a function `custom_attention` that performs the following: 1. Creates a block mask for the input tensor. 2. Creates a general mask for the input tensor. 3. Combines these masks using both logical AND and OR operations to generate new masks. 4. Applies the appropriate mask to the input tensor using a designated attention mechanism. 5. Returns the masked output tensor. Ensure your implementation leverages the utility functions `create_block_mask`, `create_mask`, `and_masks`, `or_masks`, and `noop_mask` appropriately. Instructions 1. Function Name: `custom_attention` 2. Input: - `input_tensor` (torch.Tensor): A 2D input tensor of shape `[batch_size, seq_length]`. - `mask_type` (str): A string specifying the type of mask operation to apply. It can be one of `\\"and\\"`, `\\"or\\"`, or `\\"noop\\"`. 3. Output: - A masked tensor of the same shape as the input tensor. Constraints 1. The dimensions of `input_tensor` are such that `2 <= batch_size <= 128` and `2 <= seq_length <= 1024`. 2. You must use the provided mask utility functions: `create_block_mask`, `create_mask`, `and_masks`, `or_masks`, and `noop_mask`. ```python import torch import torch.nn.attention.flex_attention as fa def custom_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: # Step 1: Create a block mask block_mask = fa.create_block_mask(input_tensor.size(0), input_tensor.size(1)) # Step 2: Create a general mask general_mask = fa.create_mask(input_tensor.size(0), input_tensor.size(1)) # Step 3: Combine masks based on mask_type if mask_type == \\"and\\": combined_mask = fa.and_masks(block_mask, general_mask) elif mask_type == \\"or\\": combined_mask = fa.or_masks(block_mask, general_mask) elif mask_type == \\"noop\\": combined_mask = fa.noop_mask(input_tensor.size(0), input_tensor.size(1)) else: raise ValueError(\\"Invalid mask_type. Choose from \'and\', \'or\', or \'noop\'.\\") # Step 4: Apply the combined mask to the input tensor using an attention mechanism # For simplicity, assume the attention mechanism scales the input tensor by the mask masked_output = input_tensor * combined_mask return masked_output # Example usage input_tensor = torch.rand(4, 10) masked_output = custom_attention(input_tensor, \'and\') print(masked_output) ``` Notes - Ensure your function handles invalid `mask_type` values and raises appropriate errors. - The provided masks are binary masks that will multiply the input tensor element-wise. - The example usage demonstrates a basic application of your custom attention mechanism.","solution":"import torch # Placeholder utility functions to simulate the mask operations from \'torch.nn.attention.flex_attention\' def create_block_mask(batch_size, seq_length): return torch.ones((batch_size, seq_length)) def create_mask(batch_size, seq_length): return torch.ones((batch_size, seq_length)) def and_masks(mask1, mask2): return mask1 * mask2 def or_masks(mask1, mask2): return torch.clamp(mask1 + mask2, max=1) def noop_mask(batch_size, seq_length): return torch.ones((batch_size, seq_length)) def custom_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: batch_size, seq_length = input_tensor.size() # Step 1: Create a block mask block_mask = create_block_mask(batch_size, seq_length) # Step 2: Create a general mask general_mask = create_mask(batch_size, seq_length) # Step 3: Combine masks based on mask_type if mask_type == \\"and\\": combined_mask = and_masks(block_mask, general_mask) elif mask_type == \\"or\\": combined_mask = or_masks(block_mask, general_mask) elif mask_type == \\"noop\\": combined_mask = noop_mask(batch_size, seq_length) else: raise ValueError(\\"Invalid mask_type. Choose from \'and\', \'or\', or \'noop\'.\\") # Step 4: Apply the combined mask to the input tensor using an attention mechanism # For simplicity, assume the attention mechanism scales the input tensor by the mask masked_output = input_tensor * combined_mask return masked_output # Example usage input_tensor = torch.rand(4, 10) masked_output = custom_attention(input_tensor, \'and\') print(masked_output)"},{"question":"# Advanced Pandas Data Manipulation and Merging Problem Statement You are given several data sources in the form of pandas DataFrames. Your task is to combine these data sources using various merging techniques and produce a final report. Moreover, you will need to compare two resulting DataFrames and summarize their differences. # DataFrames 1. `sales`: ```python sales = pd.DataFrame({ \\"transaction_id\\": [1, 2, 3, 4, 5], \\"product_id\\": [101, 102, 103, 104, 105], \\"quantity\\": [1, 2, 3, 1, 2], \\"store_id\\": [1, 2, 1, 3, 2] }) ``` 2. `products`: ```python products = pd.DataFrame({ \\"product_id\\": [101, 102, 103, 104, 105], \\"product_name\\": [\\"Apple\\", \\"Banana\\", \\"Orange\\", \\"Peach\\", \\"Grape\\"], \\"price\\": [1.0, 0.5, 0.7, 1.2, 1.1] }) ``` 3. `stores`: ```python stores = pd.DataFrame({ \\"store_id\\": [1, 2, 3], \\"store_name\\": [\\"Store A\\", \\"Store B\\", \\"Store C\\"], \\"location\\": [\\"Downtown\\", \\"Uptown\\", \\"Suburb\\"] }) ``` # Tasks 1. Merge the `sales` and `products` DataFrames to include the product details for each transaction. 2. Merge the resulting DataFrame with the `stores` DataFrame to include store details for each transaction. 3. Create a summary DataFrame `sales_summary` that shows the total quantity sold and total revenue for each store. 4. Suppose you have another DataFrame `revised_sales` as below: ```python revised_sales = pd.DataFrame({ \\"transaction_id\\": [1, 2, 3, 4, 5], \\"product_id\\": [101, 105, 103, 104, 102], \\"quantity\\": [1, 3, 1, 3, 2], \\"store_id\\": [1, 2, 1, 3, 2] }) ``` Merge the `revised_sales` and `products` DataFrames, then merge with `stores` DataFrame. 5. Compare the two summary DataFrames (`sales_summary` and the resulting summary from revised sales) and highlight the differences. # Expected Outputs 1. **Merged DataFrame for Sales and Products**: * Columns: `transaction_id, product_id, quantity, store_id, product_name, price` 2. **Merged DataFrame with Stores**: * Columns: `transaction_id, product_id, quantity, store_id, product_name, price, store_name, location` 3. **Sales Summary DataFrame**: * Columns: `store_name, total_quantity, total_revenue` 4. **Comparison of Summary DataFrames**: * Display differences, if any, between `sales_summary` and the summary from `revised_sales` # Constraints 1. Assume all transactions, products, and stores are present in the respective DataFrames. 2. Handle possible missing or discrepant data gracefully. # Implementation Implement the following function to achieve the tasks: ```python import pandas as pd def analyze_sales_data(sales, products, stores, revised_sales): # Task 1: Merge sales and products to include product details merged_sales_products = pd.merge(sales, products, on=\\"product_id\\") # Task 2: Merge with stores to include store details merged_all = pd.merge(merged_sales_products, stores, on=\\"store_id\\") # Task 3: Create summary DataFrame sales_summary = merged_all.groupby(\\"store_name\\").agg( total_quantity=(\\"quantity\\", \\"sum\\"), total_revenue=(\\"price\\", lambda x: (x * merged_all[\'quantity\']).sum()) ).reset_index() # Task 4: Process revised_sales similarly revised_merged_sales_products = pd.merge(revised_sales, products, on=\\"product_id\\") revised_merged_all = pd.merge(revised_merged_sales_products, stores, on=\\"store_id\\") revised_sales_summary = revised_merged_all.groupby(\\"store_name\\").agg( total_quantity=(\\"quantity\\", \\"sum\\"), total_revenue=(\\"price\\", lambda x: (x * revised_merged_all[\'quantity\']).sum()) ).reset_index() # Task 5: Compare the two summary DataFrames comparison = sales_summary.compare(revised_sales_summary, keep_shape=True, keep_equal=True) return merged_sales_products, merged_all, sales_summary, revised_merged_all, revised_sales_summary, comparison # Test the function with given data output = analyze_sales_data(sales, products, stores, revised_sales) for df in output: print(df) ``` # Note: - Ensure the merge operations maintain necessary consistency and handle columns appropriately. - The final comparison should highlight any discrepancies between the two summary DataFrames.","solution":"import pandas as pd def analyze_sales_data(sales, products, stores, revised_sales): # Task 1: Merge sales and products to include product details merged_sales_products = pd.merge(sales, products, on=\\"product_id\\") # Task 2: Merge with stores to include store details merged_all = pd.merge(merged_sales_products, stores, on=\\"store_id\\") # Task 3: Create summary DataFrame sales_summary = merged_all.groupby(\\"store_name\\").agg( total_quantity=(\\"quantity\\", \\"sum\\"), total_revenue=(\\"price\\", lambda x: (x * merged_all.loc[x.index, \'quantity\']).sum()) ).reset_index() # Task 4: Process revised_sales similarly revised_merged_sales_products = pd.merge(revised_sales, products, on=\\"product_id\\") revised_merged_all = pd.merge(revised_merged_sales_products, stores, on=\\"store_id\\") revised_sales_summary = revised_merged_all.groupby(\\"store_name\\").agg( total_quantity=(\\"quantity\\", \\"sum\\"), total_revenue=(\\"price\\", lambda x: (x * revised_merged_all.loc[x.index, \'quantity\']).sum()) ).reset_index() # Task 5: Compare the two summary DataFrames comparison = sales_summary.compare(revised_sales_summary, keep_shape=True, keep_equal=True) return merged_sales_products, merged_all, sales_summary, revised_merged_all, revised_sales_summary, comparison"},{"question":"# Question: Using Seaborn Objects Interface to Create a Customized Plot **Objective:** Create a customized Seaborn plot using the `objects` interface that demonstrates your understanding of `Dash`, `Dots`, `Dodge`, and other key functionalities. **Task:** You are provided with the `penguins` dataset from Seaborn. Your task is to create a visualization that includes the following requirements: 1. A plot with `species` on the x-axis and `body_mass_g` on the y-axis. 2. Differentiate the data points using colors representing the `sex` of the penguins. 3. Add dash marks (`Dash`) to display individual data points, varying the line width based on `flipper_length_mm`. 4. Incorporate aggregate values using `Dash` and `Agg`. 5. Utilize `Dots` to show individual data points with `Dodge` and `Jitter` to avoid overlapping. 6. Ensure proper orientation if both coordinate variables are numeric. 7. Set the transparency (alpha value) of the dash marks to 0.5. **Input:** - None (the dataset is preloaded as `penguins` from seaborn). **Output:** - A visual plot that meets the above requirements. **Constraints:** - Do not change the dataset or modify values within it. - Use only the seaborn `objects` interface for creating the plot. # Expected Function Implementation ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load the penguin dataset penguins = load_dataset(\\"penguins\\") # Create the plot object p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add various elements to customize the plot p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot plt.show() # Example usage create_custom_plot() ``` Ensure your function implementation effectively demonstrates the required plot characteristics and uses the seaborn `objects` interface as shown in the provided documentation.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load the penguin dataset penguins = load_dataset(\\"penguins\\") # Create the plot object p = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") .add(so.Dash(), so.Agg()) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Display the plot p.show() # Example usage create_custom_plot()"},{"question":"Objective You are required to write functions that will allow insertion into, and searching in, a list, ensuring that the list remains sorted at all times. You must use the functions provided by the \\"bisect\\" module in Python. Problem Create a class called `SortedList` that manages a list of integers while keeping it sorted after each insertion. The class should have the following methods: 1. **`__init__(self)`**: Initializes an empty list. 2. **`insert(self, value: int) -> None`**: Inserts a value into the sorted list. 3. **`find_le(self, value: int) -> int`**: Returns the largest value in the list that is less than or equal to the given value. Raises a `ValueError` if no such value exists. 4. **`find_ge(self, value: int) -> int`**: Returns the smallest value in the list that is greater than or equal to the given value. Raises a `ValueError` if no such value exists. Input and Output - **Input for `insert(value: int)`**: An integer value to insert into the list. - **Output for `insert(value: int)`**: None - **Input for `find_le(value: int)`**: An integer value to find the largest value in the list that is less than or equal to this value. - **Output for `find_le(value: int)`**: The integer value found in the list, or raises `ValueError` if no such value exists. - **Input for `find_ge(value: int)`**: An integer value to find the smallest value in the list that is greater than or equal to this value. - **Output for `find_ge(value: int)`**: The integer value found in the list, or raises `ValueError` if no such value exists. Constraints - You must use the `bisect` functions for insertion and searching. - The list must remain sorted at all times without explicitly sorting the list. Example ```python # Example usage: sorted_list = SortedList() sorted_list.insert(3) sorted_list.insert(1) sorted_list.insert(4) sorted_list.insert(1) sorted_list.insert(5) print(sorted_list.find_le(4)) # Output: 4 print(sorted_list.find_ge(2)) # Output: 3 print(sorted_list.find_le(0)) # Raises ValueError print(sorted_list.find_ge(6)) # Raises ValueError ```","solution":"from bisect import bisect_left, bisect_right, insort class SortedList: def __init__(self): self.lst = [] def insert(self, value: int) -> None: insort(self.lst, value) def find_le(self, value: int) -> int: idx = bisect_right(self.lst, value) if idx == 0: raise ValueError(\\"No element less than or equal to the given value.\\") return self.lst[idx - 1] def find_ge(self, value: int) -> int: idx = bisect_left(self.lst, value) if idx == len(self.lst): raise ValueError(\\"No element greater than or equal to the given value.\\") return self.lst[idx]"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of PyTorch\'s FFT capabilities by applying several of the discrete Fourier transform and helper functions to signal processing. # Problem Statement You are given a 2-dimensional grayscale image (as a 2D tensor) representing some frequency domain information. Your task is to perform the following: 1. Apply a 2-dimensional Fast Fourier Transform (FFT) to convert the image from the frequency domain to the spatial domain. 2. Shift the zero-frequency component to the center of the spectrum using appropriate helper functions. 3. Apply an inverse FFT to recover the image in the spatial domain. 4. Normalize the recovered image tensor values to the range [0, 1]. # Input - A 2D tensor `freq_domain_tensor` of shape `(H, W)` representing the image in the frequency domain. # Output - A 2D tensor of the same shape `(H, W)` representing the image in the spatial domain, normalized to the range [0, 1]. # Constraints - You must use PyTorch library to solve this problem. - The input tensor will have at least one dimension size greater than 1 (i.e., H > 1 and W > 1). # Performance Requirements - The function should handle tensors representing images up to size 4096x4096 efficiently. # Function Signature ```python def process_frequency_domain_image(freq_domain_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Example ```python import torch # Sample input tensor freq_domain_tensor = torch.rand((4, 4)) # Expected output is a tensor of the same shape with values normalized to [0, 1] output_tensor = process_frequency_domain_image(freq_domain_tensor) print(output_tensor) # Output should be: # A 2D tensor of shape (4, 4) with values normalized to [0, 1] ``` # Notes - Remember to use `fftshift` and `ifftshift` wrapper functions to handle the frequency domain shifting. - Make sure to handle the normalization step correctly. # Implementation Tips - Use `torch.fft.fft2` and `torch.fft.ifft2` for 2D FFT and inverse FFT respectively. - `fftshift` and `ifftshift` functions help in shifting the frequency components. - For normalization, you can use min-max scaling with PyTorch tensor operations.","solution":"import torch def process_frequency_domain_image(freq_domain_tensor: torch.Tensor) -> torch.Tensor: Processes a frequency domain image tensor and returns it in the spatial domain, normalized to the range [0, 1]. # Apply a 2-dimensional Fast Fourier Transform (FFT) fft_image = torch.fft.ifft2(freq_domain_tensor) # Shift the zero-frequency component to the center of the spectrum fft_shifted = torch.fft.fftshift(fft_image) # Apply an inverse FFT to recover the image in the spatial domain recovered_image = torch.fft.ifft2(fft_shifted) # Take the real part as the final spatial domain image recovered_image_real = torch.real(recovered_image) # Normalize the recovered image to the range [0, 1] min_val = torch.min(recovered_image_real) max_val = torch.max(recovered_image_real) normalized_image = (recovered_image_real - min_val) / (max_val - min_val) return normalized_image"},{"question":"# Question: Text Classification with tf-idf and MultinomialNB You have been provided a collection of text documents. Your task is to write a Python function `train_text_classifier` using scikit-learn, which performs the following: 1. Tokenizes the text documents using `CountVectorizer`. 2. Transforms the token counts to a tf-idf representation using `TfidfTransformer`. 3. Trains a Multinomial Naive Bayes classifier (`MultinomialNB`). The input to the function is a Python dictionary with the following structure: ```python input_data = { \\"data\\": [\\"text document 1\\", \\"text document 2\\", ..., \\"text document N\\"], \\"target\\": [0, 1, ..., N-1], # Classification labels } ``` The function should output the trained classifier and the vectorizer used in the preprocessing pipeline. Function Signature ```python def train_text_classifier(input_data: dict) -> tuple: # Your code here ``` Constraints - The input dictionary `input_data` is guaranteed to have `data` and `target` as keys. - The length of `input_data[\\"data\\"]` and `input_data[\\"target\\"]` will be the same and at least 2. - The text documents can include arbitrary text content. Example ```python from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline def train_text_classifier(input_data: dict) -> tuple: vectorizer = TfidfVectorizer() classifier = MultinomialNB() pipeline = make_pipeline(vectorizer, classifier) pipeline.fit(input_data[\\"data\\"], input_data[\\"target\\"]) return pipeline.named_steps[\'multinomialnb\'], vectorizer # Example usage input_data = { \\"data\\": [\\"This is the first document.\\", \\"This is the second document.\\"], \\"target\\": [0, 1], } model, vectorizer = train_text_classifier(input_data) print(model) print(vectorizer.get_feature_names_out()) ``` **Expected Output:** Upon running the provided example usage code, the `model` will output the trained `MultinomialNB` classifier, and the `vectorizer` will output the names of the features extracted from the text documents.","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline def train_text_classifier(input_data: dict) -> tuple: Trains a text classifier using tf-idf and MultinomialNB. Args: input_data (dict): A dictionary with \'data\' (list of text documents) and \'target\' (list of labels). Returns: tuple: trained classifier, vectorizer used in the preprocessing pipeline. # Create a pipeline with count vectorizer, tf-idf transformer, and MultinomialNB classifier text_clf = Pipeline([ (\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'clf\', MultinomialNB()) ]) # Train the classifier text_clf.fit(input_data[\'data\'], input_data[\'target\']) # Extract the trained classifier and vectorizer trained_classifier = text_clf.named_steps[\'clf\'] vectorizer = text_clf.named_steps[\'vect\'] return trained_classifier, vectorizer"},{"question":"**Question:** Implement a Python function named `run_with_timeout` that uses the `subprocess` module to run a given command with a specified timeout. The function should capture both the `stdout` and `stderr` of the command, and if it times out, it should kill the subprocess and return an appropriate error message. # Function Signature: ```python def run_with_timeout(command: str, timeout: int) -> dict: Runs a command using the subprocess module with a specified timeout. :param command: The command to be executed as a string. :param timeout: The timeout period in seconds after which the command should be terminated if not completed. :return: A dictionary containing the following keys: - \'stdout\': Captured standard output of the command (str) - \'stderr\': Captured standard error of the command (str) - \'returncode\': The return code of the command (int) - \'timed_out\': A boolean indicating whether the command timed out (bool) ``` # Explanation: 1. The function takes a command as a string and a timeout value in seconds. 2. It runs the command using the `subprocess.run` function. 3. If the command completes within the timeout, it returns the captured `stdout`, `stderr`, and `returncode` in a dictionary. 4. If the command does not complete within the timeout, it kills the process, captures any available output, and returns an error message indicating a timeout. # Constraints: - The command provided will be a valid shell command. - Timeout will be a positive integer. # Example: ```python result = run_with_timeout(\\"ls -l /\\", 5) # Example Output: # { # \\"stdout\\": \\"<captured stdout>\\", # \\"stderr\\": \\"<captured stderr>\\", # \\"returncode\\": 0, # \\"timed_out\\": False # } result = run_with_timeout(\\"sleep 10\\", 5) # Example Output: # { # \\"stdout\\": \\"\\", # \\"stderr\\": \\"\\", # \\"returncode\\": -1, # \\"timed_out\\": True # } ``` # Notes: - Use `subprocess.PIPE` to capture `stdout` and `stderr`. - Handle the `subprocess.TimeoutExpired` exception to detect and handle timeouts. - Ensure that the subprocess is terminated in case of a timeout.","solution":"import subprocess def run_with_timeout(command: str, timeout: int) -> dict: try: result = subprocess.run( command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout, text=True ) return { \\"stdout\\": result.stdout, \\"stderr\\": result.stderr, \\"returncode\\": result.returncode, \\"timed_out\\": False } except subprocess.TimeoutExpired as e: return { \\"stdout\\": e.stdout or \\"\\", \\"stderr\\": e.stderr or \\"\\", \\"returncode\\": -1, \\"timed_out\\": True }"},{"question":"Objective: Implement a function that simulates a simple command-line processing system using the `shlex` module. This function will take a set of shell-like commands, split them into tokens, and perform operations based on predefined rules. Function Signature: ```python def process_commands(command: str) -> list: ``` Input: - `command`: A single string containing shell-like commands. For example, `\'echo \\"Hello, World\\"; ls -l; cat file.txt\'` Output: - A list of dictionaries, where each dictionary represents the result of a command. Each dictionary contains: - `command`: The command that was executed. - `tokens`: A list of tokens obtained from splitting the command using `shlex.split`. - `escaped_command`: The command rejoined using `shlex.join` (if possible). - `errors`: A list of any errors encountered during processing. Constraints: 1. The function should handle multiple commands separated by `;`. 2. Each command should be split into tokens using `shlex.split`. 3. Rejoin the tokens using `shlex.join` and ensure the rejoined command is safe from injection vulnerabilities. 4. Handle possible errors during tokenization and rejoining, and include them in the `errors` list. 5. The function should operate in POSIX mode. Example: ```python commands = \'echo \\"Hello; World\\"; ls -l; cat non_existent_file.txt\' result = process_commands(commands) # Example output: # [ # { # \'command\': \'echo \\"Hello; World\\"\', # \'tokens\': [\'echo\', \'Hello; World\'], # \'escaped_command\': \\"echo \'Hello; World\'\\", # \'errors\': [] # }, # { # \'command\': \'ls -l\', # \'tokens\': [\'ls\', \'-l\'], # \'escaped_command\': \'ls -l\', # \'errors\': [] # }, # { # \'command\': \'cat non_existent_file.txt\', # \'tokens\': [\'cat\', \'non_existent_file.txt\'], # \'escaped_command\': \'cat non_existent_file.txt\', # \'errors\': [\'File not found: non_existent_file.txt\'] # } # ] ``` Notes: 1. Assume `File not found` error for `cat` command on non-existent files only for simulation purposes. 2. The function need not execute the commands but should simulate tokenization and joining processes accurately.","solution":"import shlex def process_commands(command: str) -> list: result = [] commands = command.split(\';\') for cmd in commands: cmd = cmd.strip() tokens = [] escaped_command = \\"\\" errors = [] try: tokens = shlex.split(cmd, posix=True) except ValueError as e: errors.append(str(e)) try: if tokens: escaped_command = shlex.join(tokens) except ValueError as e: errors.append(str(e)) result.append({ \'command\': cmd, \'tokens\': tokens, \'escaped_command\': escaped_command, \'errors\': errors, }) return result"},{"question":"Pandas Plotting Assessment # Objective The objective of this coding assessment is to evaluate your ability to use the pandas plotting functionalities to visualize data effectively. You are required to implement a function that utilizes various plotting functions from the `pandas.plotting` module to create informative and insightful visualizations. # Task Description You are given a DataFrame containing demographic data of several cities. Implement a function `plot_city_demographics` that performs the following tasks: 1. **Andrews Curves Plot**: Create a plot using `andrews_curves` to visualize the clustering of cities based on their demographic attributes. 2. **Autocorrelation Plot**: Generate an autocorrelation plot using `autocorrelation_plot` for the population column. 3. **Scatter Matrix**: Generate a scatter matrix plot using `scatter_matrix` to visualize the relationships between all pairs of numeric columns. # Function Signature ```python import pandas as pd from pandas.plotting import andrews_curves, autocorrelation_plot, scatter_matrix import matplotlib.pyplot as plt def plot_city_demographics(df: pd.DataFrame) -> None: This function generates various plots to analyze city demographics. Parameters: df (pd.DataFrame): DataFrame containing city demographic data. The DataFrame will have the following columns: - \'City\': Name of the city (string) - \'Population\': Population of the city (integer) - \'Area\': Land area of the city in square kilometers (float) - \'GDP\': Gross Domestic Product of the city (float) - \'Unemployment\': Unemployment rate of the city (float) Returns: None pass ``` # Requirements - Utilize `andrews_curves` to create a plot that visualizes the clustering of cities. - Generate an `autocorrelation_plot` for the \'Population\' column. - Create a `scatter_matrix` plot for all numeric columns. - Display all plots within a single Jupyter notebook cell. # Constraints - The input DataFrame will have at least 10 rows. - Do not modify the input DataFrame in-place. - The function should not return any value; it should only display the plots. # Example Usage ```python data = { \'City\': [\'CityA\', \'CityB\', \'CityC\', \'CityD\', \'CityE\', \'CityF\', \'CityG\', \'CityH\', \'CityI\', \'CityJ\'], \'Population\': [1000000, 500000, 1500000, 200000, 1200000, 400000, 800000, 300000, 700000, 900000], \'Area\': [600, 350, 450, 300, 500, 400, 380, 250, 270, 390], \'GDP\': [12000, 6500, 15700, 7300, 9800, 5900, 7700, 3100, 5300, 8600], \'Unemployment\': [5.0, 7.2, 4.8, 6.1, 5.7, 8.3, 6.9, 6.3, 7.0, 5.4] } df = pd.DataFrame(data) plot_city_demographics(df) ``` # Additional Notes - Ensure that your plots are displayed with proper titles, and labels, and are clear and readable. - Use the pandas and matplotlib packages efficiently to avoid redundancy and improve performance.","solution":"import pandas as pd from pandas.plotting import andrews_curves, autocorrelation_plot, scatter_matrix import matplotlib.pyplot as plt def plot_city_demographics(df: pd.DataFrame) -> None: This function generates various plots to analyze city demographics. Parameters: df (pd.DataFrame): DataFrame containing city demographic data. The DataFrame will have the following columns: - \'City\': Name of the city (string) - \'Population\': Population of the city (integer) - \'Area\': Land area of the city in square kilometers (float) - \'GDP\': Gross Domestic Product of the city (float) - \'Unemployment\': Unemployment rate of the city (float) Returns: None plt.figure(figsize=(15, 8)) # Andrews Curves Plot plt.subplot(1, 3, 1) andrews_curves(df, \'City\') plt.title(\'Andrews Curves Plot\') # Autocorrelation Plot for Population plt.subplot(1, 3, 2) autocorrelation_plot(df[\'Population\']) plt.title(\'Autocorrelation Plot: Population\') # Scatter Matrix Plot scatter_matrix(df.select_dtypes(include=[int, float]), alpha=0.75, figsize=(15, 15), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix for Numeric Attributes\') plt.tight_layout() plt.show()"},{"question":"You are provided with a dataset representing logs for an e-commerce website. Each log entry contains the user ID, the product ID they interacted with, the type of interaction (click, view, purchase), and a timestamp. Write a function `filter_and_summarize_logs` that performs the following tasks using pandas: 1. **Load the dataset** into a pandas DataFrame. The dataset is provided as a CSV file path. 2. **Filter the logs** by a specific user ID and a specific interaction type. 3. **Select a subset of columns** from the filtered logs, specifically, the timestamp and the product ID. 4. **Sort the selected logs** by timestamp in descending order. 5. **Generate a summary** displaying the unique count of products interacted with and the total number of interactions. 6. **Handle any potential missing values** in the dataset gracefully. # Function Signature: ```python import pandas as pd def filter_and_summarize_logs(csv_file_path: str, user_id: int, interaction_type: str) -> dict: pass ``` # Input: - `csv_file_path`: A string representing the path to the CSV file containing the logs. - `user_id`: An integer representing the user ID to filter by. - `interaction_type`: A string representing the type of interaction to filter by (\'click\', \'view\', \'purchase\'). # Output: - A dictionary with: - `unique_products`: An integer representing the number of unique products the user interacted with. - `total_interactions`: An integer representing the total number of interactions of the given type by the user. - `filtered_logs`: A DataFrame containing the filtered logs with columns `timestamp` and `product_id`, sorted by timestamp in descending order. # Constraints: - Ensure the function handles large datasets efficiently. - Assume reasonable constraints on the dataset size, such as it fitting into memory. # Example: ```python # Given a CSV file path \\"logs.csv\\" with the following content: # user_id,product_id,interaction_type,timestamp # 1,101,click,2023-01-01 10:00:00 # 2,102,view,2023-01-01 11:00:00 # 1,103,purchase,2023-01-02 12:00:00 # 1,101,click,2023-01-03 13:00:00 result = filter_and_summarize_logs(\\"logs.csv\\", 1, \'click\') # Output would be: # { # \'unique_products\': 1, # \'total_interactions\': 2, # \'filtered_logs\': DataFrame with columns [timestamp, product_id] containing: # timestamp product_id # 2023-01-03 13:00:00 101 # 2023-01-01 10:00:00 101 # } ``` # Note: - Use appropriate pandas methods such as `loc`, `iloc`, `drop_duplicates`, `sort_values`, etc., to accomplish the tasks. - Ensure that the handling of missing values does not affect the integrity of the filtered logs and summary.","solution":"import pandas as pd def filter_and_summarize_logs(csv_file_path: str, user_id: int, interaction_type: str) -> dict: # Load the dataset into a pandas DataFrame df = pd.read_csv(csv_file_path) # Handle potential missing values df = df.dropna(subset=[\'user_id\', \'product_id\', \'interaction_type\', \'timestamp\']) # Filter the logs by specific user ID and interaction type filtered_logs = df[(df[\'user_id\'] == user_id) & (df[\'interaction_type\'] == interaction_type)] # Select subset of columns filtered_logs = filtered_logs[[\'timestamp\', \'product_id\']] # Sort logs by timestamp in descending order filtered_logs = filtered_logs.sort_values(by=\'timestamp\', ascending=False) # Generate summary unique_products = filtered_logs[\'product_id\'].nunique() total_interactions = len(filtered_logs) # Create the summary dictionary summary = { \'unique_products\': unique_products, \'total_interactions\': total_interactions, \'filtered_logs\': filtered_logs } return summary"},{"question":"**Title:** Implement a Customized Python Initialization **Question:** You are tasked with implementing a simplified version of Python\'s initialization configuration in Python. Using the documentation provided, simulate the behavior of configuring and initializing a Python interpreter with customized settings. # Requirements: 1. **Create Structure Classes**: - Implement classes for `PyPreConfig`, `PyConfig`, `PyWideStringList`, and `PyStatus`. 2. **Implement Configuration Functions**: - Create mock functions to simulate `PyPreConfig_InitPythonConfig`, `PyConfig_InitPythonConfig`, `Py_PreInitialize`, `Py_InitializeFromConfig`, and other relevant functions. - Ensure proper initialization based on the fields set in `PyPreConfig` and `PyConfig`. 3. **Simulate the Initialization Workflow**: - Initialize the Python configuration in the main function. - Customize the configurations (e.g., enable `utf8_mode`, set program name, etc.). - Preinitialize Python with `Py_PreInitialize`. - Continue to fully initialize Python with `Py_InitializeFromConfig`. 4. **Handle Status and Exceptions**: - Use the `PyStatus` structure to handle success, error, and exit states during the initialization process. - Implement proper exception handling and status management in your main initialization logic. # Input and Output Specifications: - **Input**: Configuration settings provided as fields in `PyPreConfig` and `PyConfig`. - **Output**: Simulated initialization output and status messages. # Constraints: - Do not use the actual `ctypes` library or any real interfacing with C code. Simulate the logic purely in Python. - Ensure that preinitialization happens correctly before the full initialization. - Implement precisely as per the methods and structure specifications in the documentation. # Example: ```python class PyPreConfig: def __init__(self): self.utf8_mode = -1 # Default value class PyConfig: def __init__(self): self.program_name = None self.utf8_mode = -1 # Default value def PyPreConfig_InitPythonConfig(preconfig): preconfig.utf8_mode = 1 def PyConfig_InitPythonConfig(config): config.utf8_mode = 1 def Py_PreInitialize(preconfig): # Simulated preinitialization logic status = PyStatus_Ok() if preconfig.utf8_mode != -1: print(\\"UTF-8 mode enabled in preinitialization\\") return status def Py_InitializeFromConfig(config): # Simulated initialization logic status = PyStatus_Ok() if config.program_name: print(f\\"Program name set to: {config.program_name}\\") return status class PyStatus: @staticmethod def Ok(): return PyStatus(\\"success\\") def __init__(self, status_type, err_msg=None): self.status_type = status_type self.err_msg = err_msg def main(): preconfig = PyPreConfig() PyPreConfig_InitPythonConfig(preconfig) status = Py_PreInitialize(preconfig) if status.status_type != \\"success\\": print(\\"Failed to preinitialize\\") return config = PyConfig() PyConfig_InitPythonConfig(config) config.program_name = \\"MyProgram\\" status = Py_InitializeFromConfig(config) if status.status_type != \\"success\\": print(\\"Failed to initialize\\") return print(\\"Initialization successful\\") if __name__ == \\"__main__\\": main() ``` In the example above, initialize the Python configuration using `PyPreConfig_InitPythonConfig`, enable `utf8_mode` during `Py_PreInitialize`, set the program name, and perform the full initialization with `Py_InitializeFromConfig`. Implement your solution considering the provided functionalities in the documentation. Test the initialization with varying configuration settings to ensure comprehensive coverage.","solution":"class PyPreConfig: def __init__(self): self.utf8_mode = -1 # Default value class PyConfig: def __init__(self): self.program_name = None self.utf8_mode = -1 # Default value class PyStatus: @staticmethod def Ok(): return PyStatus(\\"success\\") def __init__(self, status_type, err_msg=None): self.status_type = status_type self.err_msg = err_msg def PyPreConfig_InitPythonConfig(preconfig): preconfig.utf8_mode = 1 def PyConfig_InitPythonConfig(config): config.utf8_mode = 1 def Py_PreInitialize(preconfig): if preconfig.utf8_mode != -1: print(\\"UTF-8 mode enabled in preinitialization\\") return PyStatus.Ok() def Py_InitializeFromConfig(config): if config.program_name: print(f\\"Program name set to: {config.program_name}\\") return PyStatus.Ok() def main(): preconfig = PyPreConfig() PyPreConfig_InitPythonConfig(preconfig) status = Py_PreInitialize(preconfig) if status.status_type != \\"success\\": print(\\"Failed to preinitialize\\") return config = PyConfig() PyConfig_InitPythonConfig(config) config.program_name = \\"MyProgram\\" status = Py_InitializeFromConfig(config) if status.status_type != \\"success\\": print(\\"Failed to initialize\\") return print(\\"Initialization successful\\") if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question # Objective Create a custom event loop policy and a specific child watcher. Demonstrate their usage by creating an asyncio-based program that performs a task asynchronously. # Question You are required to: 1. Subclass the DefaultEventLoopPolicy to create a custom event loop policy that logs every time an event loop is retrieved or set. 2. Implement a custom child watcher that simply prints out when a child process is added, removed, or when its handler is called. 3. Create an asyncio-based program that: - Uses the custom event loop policy and the custom child watcher. - Spawns a child process, waits for it to complete, and prints its exit code. # Requirements 1. **Custom Event Loop Policy** - Subclass `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop()` and `set_event_loop()` methods to log messages indicating when these methods are called. 2. **Custom Child Watcher** - Implement all methods of `asyncio.AbstractChildWatcher`. - Print messages when `add_child_handler`, `remove_child_handler`, `attach_loop`, or `close` methods are called and when a child process terminates. 3. **Asyncio-based Program** - Write an asynchronous function to create a subprocess that runs a simple shell command (e.g., `sleep 1`, `echo Hello`). - Ensure your custom event loop policy and custom child watcher are in use. - Run the asynchronous function and print the subprocess\'s exit code upon completion. # Input and Output - **Input**: No user input required. - **Output**: Logs from the custom event loop policy and custom child watcher. Also, print the exit code of the subprocess. # Code Template ```python import asyncio # 1. Custom Event Loop Policy class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Event loop retrieved\\") return loop def set_event_loop(self, loop): super().set_event_loop(loop) print(\\"Event loop set\\") # 2. Custom Child Watcher class MyChildWatcher(asyncio.AbstractChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Handler added for child process {pid}\\") # Implementation details needed here def remove_child_handler(self, pid): print(f\\"Handler removed for child process {pid}\\") # Implementation details needed here def attach_loop(self, loop): print(f\\"Child watcher attached to loop\\") # Implementation details needed here def is_active(self): return True def close(self): print(\\"Child watcher closed\\") # Implementation details needed here # 3. Asyncio Program async def create_subprocess(): process = await asyncio.create_subprocess_shell(\\"echo Hello\\", stdout=asyncio.subprocess.PIPE) stdout, _ = await process.communicate() print(f\\"Subprocess exited with code {process.returncode}\\") print(f\\"Subprocess output: {stdout.decode().strip()}\\") def main(): asyncio.set_event_loop_policy(MyEventLoopPolicy()) asyncio.get_event_loop_policy().set_child_watcher(MyChildWatcher()) loop = asyncio.get_event_loop() loop.run_until_complete(create_subprocess()) if __name__ == \\"__main__\\": main() ``` Fill in the implementation details for custom methods in `MyChildWatcher` and ensure the `create_subprocess()` function operates asynchronously as required.","solution":"import asyncio # 1. Custom Event Loop Policy class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Event loop retrieved\\") return loop def set_event_loop(self, loop): super().set_event_loop(loop) print(\\"Event loop set\\") # 2. Custom Child Watcher class MyChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): super().__init__() self._loop = None def add_child_handler(self, pid, callback, *args): print(f\\"Handler added for child process {pid}\\") # Implementation to wait on child process termination def run_callback(): callback(pid, 0) # Assuming the child process returns 0 self._loop.call_soon(run_callback) def remove_child_handler(self, pid): print(f\\"Handler removed for child process {pid}\\") # No actual implementation needed for demo purposes def attach_loop(self, loop): self._loop = loop print(f\\"Child watcher attached to loop: {loop}\\") def is_active(self): return True def close(self): print(\\"Child watcher closed\\") # 3. Asyncio Program async def create_subprocess(): process = await asyncio.create_subprocess_shell(\\"echo Hello\\", stdout=asyncio.subprocess.PIPE) stdout, _ = await process.communicate() print(f\\"Subprocess exited with code {process.returncode}\\") print(f\\"Subprocess output: {stdout.decode().strip()}\\") def main(): asyncio.set_event_loop_policy(MyEventLoopPolicy()) asyncio.get_event_loop_policy().set_child_watcher(MyChildWatcher()) loop = asyncio.get_event_loop() loop.run_until_complete(create_subprocess()) if __name__ == \\"__main__\\": main()"},{"question":"**Question: Visualization and Data Transformation using Seaborn** You are provided with a dataset named `student_grades` containing information on students\' grades over different years across several courses. The dataset has the following columns: - `Year` (int): The academic year (e.g., 2019, 2020, etc.) - `Grade` (float): The grade scored by students. - `Course` (str): The name of the course (e.g., Math, Physics, etc.) - `StudentID` (str): Unique identifier for each student (e.g., \\"S001\\", \\"S002\\", etc.) Write a Python function using Seaborn that: 1. Loads the `student_grades` dataset. 2. Creates a line plot to show the relative performance (grade scaled relative to the maximum grade in the dataset) of students over the years per course. 3. Creates another line plot to show the percent change in grades from the baseline year (the minimum year in the dataset) for each course. 4. Includes appropriate labels for the y-axes indicating the transformations applied. **Function Signature:** ```python import seaborn.objects as so import pandas as pd def plot_student_performance(student_grades: pd.DataFrame): # Load the dataset # Step 1: Create the first line plot # Step 2: Create the second line plot pass ``` **Constraints:** - You should use `seaborn.objects` for creating the plots. - Plot titles and y-axis labels should reflect the transformation applied. - Handle missing data by excluding such entries from the plots. **Expected Inputs:** - `student_grades`: A pandas DataFrame of the structure described above. **Expected Outputs:** - Two plots displayed using the seaborn interface. Here is an example of the `student_grades` dataset: ```python import pandas as pd data = { \\"Year\\": [2019, 2020, 2019, 2020, 2021, 2019, 2020, 2021], \\"Grade\\": [85, 87, 90, 92, 88, 78, 80, 85], \\"Course\\": [\\"Math\\", \\"Math\\", \\"Physics\\", \\"Physics\\", \\"Physics\\", \\"Chemistry\\", \\"Chemistry\\", \\"Chemistry\\"], \\"StudentID\\": [\\"S001\\", \\"S001\\", \\"S002\\", \\"S002\\", \\"S002\\", \\"S003\\", \\"S003\\", \\"S003\\"] } student_grades = pd.DataFrame(data) ``` Implement the function `plot_student_performance(student_grades)` that follows the steps specified.","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def plot_student_performance(student_grades: pd.DataFrame): # Exclude missing data student_grades = student_grades.dropna() # Step 1: Create the first line plot showing relative performance max_grade = student_grades[\'Grade\'].max() student_grades[\'RelativePerformance\'] = student_grades[\'Grade\'] / max_grade plt.figure(figsize=(12, 6)) # Plotting relative performance relative_plot = so.Plot(student_grades, x=\'Year\', y=\'RelativePerformance\', color=\'Course\') relative_plot = relative_plot.add(so.Line()).add(so.Dot()) relative_plot.label(y=\\"Relative Performance (Scaled by Max Grade)\\") relative_plot.show() # Step 2: Create the second line plot showing percent change from baseline year baseline_year = student_grades[\'Year\'].min() baseline_grades = student_grades[student_grades[\'Year\'] == baseline_year].set_index(\'StudentID\')[\'Grade\'] student_grades = student_grades.set_index(\'StudentID\').join(baseline_grades, rsuffix=\'_baseline\') student_grades[\'PercentChange\'] = (student_grades[\'Grade\'] - student_grades[\'Grade_baseline\']) / student_grades[\'Grade_baseline\'] * 100 student_grades = student_grades.reset_index() plt.figure(figsize=(12, 6)) # Plotting percent change from baseline year percent_change_plot = so.Plot(student_grades, x=\'Year\', y=\'PercentChange\', color=\'Course\') percent_change_plot = percent_change_plot.add(so.Line()).add(so.Dot()) percent_change_plot.label(y=\\"Percent Change from Baseline Year\\") percent_change_plot.show()"},{"question":"<|Analysis Begin|> The provided documentation mentions two modules within the `torch.ao.ns` package, which deals with numerical suite functionalities for PyTorch using FX (a feature extraction and modification toolkit within PyTorch). The documentation includes a warning that the modules are early prototypes and subject to change. Specifically, the available functions in the `torch.ao.ns.fx.utils` module are: 1. `compute_sqnr(x, y)`: A function to compute the Signal-to-Quantization-Noise Ratio between two tensors x and y. 2. `compute_normalized_l2_error(x, y)`: A function to compute the normalized L2 error between two tensors x and y. 3. `compute_cosine_similarity(x, y)`: A function to compute the cosine similarity between two tensors x and y. All these functions seem to focus on comparing the properties of tensor pairs, which can be important when analyzing the impact of model quantization or other transformation on the data representations in neural networks. Given this information, it is possible to create a coding assessment that uses these utility functions to analyze differences in neural network transformations. <|Analysis End|> <|Question Begin|> **Question: Comparing Neural Network Outputs using PyTorch FX Utilities** You are provided with a pretrained neural network model and a quantized version of that model. To evaluate the effectiveness of quantization, you need to compare the outputs of the original and quantized models on a set of inputs. Specifically, you need to write code to compute the Signal-to-Quantization-Noise Ratio (SQNR), Normalized L2 Error, and Cosine Similarity between the outputs of the two models. Implement a function `evaluate_model_difference` with the following signature: ```python def evaluate_model_difference(original_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: Evaluate the difference between the outputs of the original and quantized models. Parameters: original_model (torch.nn.Module): The original pretrained model. quantized_model (torch.nn.Module): The quantized version of the original model. data_loader (torch.utils.data.DataLoader): DataLoader providing input data for evaluation. Returns: dict: A dictionary with the following keys and their corresponding values: - \'SQNR\': The average SQNR score across all inputs. - \'Normalized_L2_Error\': The average Normalized L2 Error across all inputs. - \'Cosine_Similarity\': The average Cosine Similarity score across all inputs. ``` **Inputs:** - `original_model` : A pretrained PyTorch neural network model. - `quantized_model` : A quantized version of the pretrained model. - `data_loader` : A PyTorch DataLoader providing mini-batches of input data. **Outputs:** - A dictionary with keys `\'SQNR\'`, `\'Normalized_L2_Error\'`, and `\'Cosine_Similarity\'`, where the values are the respective average scores computed over all samples in the DataLoader. **Constraints:** - You may assume both models are in evaluation mode and do not require gradients. - The input data and models are assumed to be on the same device (CPU or GPU). **Performance Requirements:** - Efficiently process the data without unnecessary computations. Aim for linear time complexity relative to the number of inputs. You may utilize the provided utility functions from `torch.ao.ns.fx.utils` for the metrics: ```python from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity ``` Write your implementation below: ```python def evaluate_model_difference(original_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: # Your code here return result ``` **Note:** Ensure that your implementation accurately measures and aggregates the required metrics for the whole dataset.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_model_difference(original_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: original_model.eval() quantized_model.eval() sqnr_values, l2_error_values, cosine_sim_values = [], [], [] with torch.no_grad(): for inputs, _ in data_loader: original_outputs = original_model(inputs) quantized_outputs = quantized_model(inputs) sqnr = compute_sqnr(original_outputs, quantized_outputs) l2_error = compute_normalized_l2_error(original_outputs, quantized_outputs) cosine_similarity = compute_cosine_similarity(original_outputs, quantized_outputs) sqnr_values.append(sqnr.item()) l2_error_values.append(l2_error.item()) cosine_sim_values.append(cosine_similarity.item()) result = { \'SQNR\': sum(sqnr_values) / len(sqnr_values), \'Normalized_L2_Error\': sum(l2_error_values) / len(l2_error_values), \'Cosine_Similarity\': sum(cosine_sim_values) / len(cosine_sim_values) } return result"},{"question":"Objective Demonstrate your understanding of the `curses` module by creating a simple text-based user interface application. Task Implement a Python function named `run_curses_interface` that uses the `curses` module to create a text-based user interface. The interface should have the following features: 1. **Initialization**: Initialize the curses application and set up the basics (`noecho`, `cbreak`, `keypad`). 2. **Main Window**: Create a main window that fills the screen and displays a simple menu with the following options: - `Press \'d\': Display a message in a new window` - `Press \'q\': Quit` 3. **Message Window**: When \'d\' is pressed, open a new window in the center of the screen that displays a message (e.g., \\"Hello, World!\\") in a specific color. 4. **Navigation**: Allow the user to close the message window and return to the main menu by pressing any key. 5. **Termination**: Properly terminate the curses application, restoring terminal settings even if an error occurs. Function Signature ```python def run_curses_interface(): ``` Constraints - The main window should use the entire terminal window size. - The message window should be centered and occupy a smaller portion of the screen. - The message should be displayed in a color that is different from the default terminal text color. - Ensure that terminal settings are restored even if an error occurs during the execution. Example When the user runs the function and presses \'d\', a new window appears with \\"Hello, World!\\" in color. When the user presses any key, the message window closes, and the main menu is shown again. Pressing \'q\' exits the application. ```plaintext Main Menu: Press \'d\': Display a message in a new window Press \'q\': Quit [User presses \'d\'] +------------------------------+ | | | Hello, World! | | | +------------------------------+ [User presses any key] Main Menu: Press \'d\': Display a message in a new window Press \'q\': Quit [User presses \'q\'] [Application exits, terminal restored] ``` Requirements - Use the `curses.wrapper` function to handle initialization and termination safely. - Use `curses.newwin` to create the message window. - Implement text color using curses color pairs. - Handle keyboard input using `getch()` or `getkey()` methods. Good luck!","solution":"import curses def run_curses_interface(stdscr): # Initialize the curses application curses.curs_set(0) # hide cursor curses.noecho() curses.cbreak() stdscr.keypad(True) # Start color functionality curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) # Main loop for the interface while True: stdscr.clear() height, width = stdscr.getmaxyx() main_menu_text = \\"Main Menu:nPress \'d\': Display a message in a new windownPress \'q\': Quit\\" stdscr.addstr(0, 0, main_menu_text) key = stdscr.getch() if key == ord(\'d\'): # Display the message window message = \\"Hello, World!\\" msg_height, msg_width = 5, len(message) + 4 win = curses.newwin(msg_height, msg_width, (height - msg_height) // 2, (width - msg_width) // 2) win.box() win.addstr(2, 2, message, curses.color_pair(1)) win.refresh() win.getch() # wait for any key to be pressed elif key == ord(\'q\'): break # End curses application curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() def run_curses_interface_wrapper(): curses.wrapper(run_curses_interface)"},{"question":"**Question: Implementing a Custom Sine Function Using PyTorch Function and Module** In this assignment, you will create a custom autograd function and integrate it into a custom PyTorch module for use in neural network models. Follow the steps below to complete your task. # Part 1: Creating the Custom Autograd Function 1. **Create a custom autograd function `MySineFunction`** by subclassing `torch.autograd.Function`. This function should implement: - `forward(ctx, input_tensor)`: Computes the sine of the input tensor. - `setup_context(ctx, inputs, output)`: Saves the input tensor to the context object for use in the backward pass. - `backward(ctx, grad_output)`: Computes the gradient of the sine function with respect to its input. (Recall the derivative of `sin(x)` is `cos(x)`). # Part 2: Implementing the Custom Module 2. **Create a custom module `SineModule`** by subclassing `torch.nn.Module`. This module should: - Implement an `__init__` method (no special parameters required). - Implement a `forward` method that uses `MySineFunction` to compute the sine of its input. # Part 3: Integrating and Testing 3. **Test the custom module**: - Create a random tensor with `requires_grad` set to `True`. - Pass this tensor through an instance of `SineModule`. - Compute the gradient with respect to the input using the `backward` method. - Verify the gradient is correctly calculated (it should approximate `cos(x)` for the input values). # Function Signature Ensure the function interfaces match the following specifications: ```python import torch import torch.nn as nn from torch.autograd import Function class MySineFunction(Function): @staticmethod def forward(ctx, input_tensor): # Implement forward computation (Hint: use torch.sin) pass @staticmethod def setup_context(ctx, inputs, output): # Save input tensor for backward computation pass @staticmethod def backward(ctx, grad_output): # Implement backward computation (Hint: use torch.cos on the saved tensor) pass class SineModule(nn.Module): def __init__(self): # Initialize the base class pass def forward(self, input_tensor): # Apply the custom sine function pass # Testing the module input_tensor = torch.randn(5, requires_grad=True) sine_module = SineModule() output = sine_module(input_tensor) output.sum().backward() print(\\"Input Tensor:\\", input_tensor) print(\\"Calculated Gradients:\\", input_tensor.grad) ``` # Evaluation Criteria - **Correctness**: The implementation should correctly compute the sine of the input tensor and compute the correct gradients. - **Efficiency**: The code should efficiently handle the forward and backward operations without unnecessary computations or excessive memory usage. - **Clarity**: The implementation should be well-documented, with clear comments explaining each part of the process.","solution":"import torch import torch.nn as nn from torch.autograd import Function class MySineFunction(Function): @staticmethod def forward(ctx, input_tensor): sine_output = torch.sin(input_tensor) ctx.save_for_backward(input_tensor) return sine_output @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * torch.cos(input_tensor) return grad_input class SineModule(nn.Module): def __init__(self): super(SineModule, self).__init__() def forward(self, input_tensor): return MySineFunction.apply(input_tensor) # Testing the module input_tensor = torch.randn(5, requires_grad=True) sine_module = SineModule() output = sine_module(input_tensor) output.sum().backward() print(\\"Input Tensor:\\", input_tensor) print(\\"Calculated Gradients:\\", input_tensor.grad)"},{"question":"# Python Coding Challenge Question: **Objective**: Your task is to implement a class `MathComprehensions` that leverages generator functionality, comprehensions, and asynchronous operations. This class will include several methods to perform computations on a list of integers. **Requirements**: 1. **Initialization**: - The class should be initialized with a list of integers. - Example: `MathComprehensions([1, 2, 3, 4, 5])`. 2. **Methods**: - `filter_evens`: - This method should return a generator that yields only even numbers from the list using a generator comprehension. - Example: For `[1, 2, 3, 4, 5]`, the generator should yield `2` and `4`. - `squared_numbers`: - This method should return a list of squared numbers computed using a list comprehension. - Example: For `[1, 2, 3, 4, 5]`, the output list should be `[1, 4, 9, 16, 25]`. - `calculate_sum`: - This method should return the sum of all numbers in the list using a generator. - Example: For `[1, 2, 3, 4, 5]`, the sum should be `15`. - `async_sum`: - This method should be an asynchronous method that computes the sum of numbers using an asynchronous generator. - Example: For `[1, 2, 3, 4, 5]`, the sum should be `15`. - `mangle_names`: - This method should demonstrate the usage of private name mangling. For this, you need to define a private variable and use it inside your class. Add a method `get_mangled_variable` to retrieve the value of this variable. **Constraints**: - All operations should handle up to `10^6` integers efficiently. - You are to demonstrate the concepts of generators, comprehensions, and asynchronous generators within the methods. **Input/Output Formats**: - Initialization: `__init__(self, numbers: List[int])` - Methods: - `filter_evens(self) -> Generator[int, None, None]` - `squared_numbers(self) -> List[int]` - `calculate_sum(self) -> int` - `async async_sum(self) -> int` - `get_mangled_variable(self) -> Any` **Example Usage**: ```python # Initialize the object math_comp = MathComprehensions([1, 2, 3, 4, 5]) # Filter evens evens = list(math_comp.filter_evens()) # Output: [2, 4] # Get squared numbers squares = math_comp.squared_numbers() # Output: [1, 4, 9, 16, 25] # Calculate sum total = math_comp.calculate_sum() # Output: 15 # Asynchronous sum calculation import asyncio async def get_async_sum(): return await math_comp.async_sum() # Example of getting private variable using name mangling mangled_value = math_comp.get_mangled_variable() ``` Implement the class `MathComprehensions` to meet the above requirements and constraints.","solution":"from typing import List, Generator, Any import asyncio class MathComprehensions: def __init__(self, numbers: List[int]): self.numbers = numbers self.__mangled_variable = \\"Private Variable\\" def filter_evens(self) -> Generator[int, None, None]: return (num for num in self.numbers if num % 2 == 0) def squared_numbers(self) -> List[int]: return [num ** 2 for num in self.numbers] def calculate_sum(self) -> int: return sum(num for num in self.numbers) async def async_sum(self) -> int: async def async_generator(nums): for num in nums: yield num total = 0 async for num in async_generator(self.numbers): total += num return total def get_mangled_variable(self) -> Any: return self.__mangled_variable"},{"question":"Objective Write a Python function that demonstrates the usage of `PLSCanonical` from `sklearn.cross_decomposition` for dimensionality reduction and regression. This exercise will test your comprehension of the fundamental concepts and implementation steps of the PLSCanonical algorithm. Problem Statement You are given two matrices, `X` and `Y`, where `X` is the predictor matrix and `Y` is the response matrix. Your task is to implement a function that performs the following steps: 1. Fit a `PLSCanonical` model to the given data. 2. Transform the original X and Y matrices using the trained model. 3. Perform regression on the transformed data `X` using the transformed data `Y` and predict the targets for the original `X` data. Function Signature ```python def pls_canonical_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Perform PLSCanonical regression on the provided data. Parameters: - X: numpy.ndarray, shape (n_samples, n_features) The predictor matrix. - Y: numpy.ndarray, shape (n_samples, n_targets) The response matrix. - n_components: int The number of components to keep. Returns: - X_transformed: numpy.ndarray, shape (n_samples, n_components) The transformed predictor matrix. - Y_transformed: numpy.ndarray, shape (n_samples, n_components) The transformed response matrix. - Y_pred: numpy.ndarray, shape (n_samples, n_targets) The predicted response values for the original predictor matrix. pass ``` Constraints - `X` and `Y` will be non-empty numpy arrays with a shape of at least (2, 2). - `n_components` will be a positive integer less than or equal to `min(n_samples, n_features, n_targets)`. Example ```python import numpy as np # Example data X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Y = np.array([[1, 2], [3, 4], [5, 6]]) n_components = 2 # Call the function X_transformed, Y_transformed, Y_pred = pls_canonical_regression(X, Y, n_components) print(\\"X Transformed:n\\", X_transformed) print(\\"Y Transformed:n\\", Y_transformed) print(\\"Y Predicted:n\\", Y_pred) ``` Expected Output ```plaintext X Transformed: [[ -Values- ]] Y Transformed: [[ -Values- ]] Y Predicted: [[ -Values- ]] ``` **Note**: Replace `-Values-` with the actual numeric output values. Additional Information You may need to import the following: ```python from sklearn.cross_decomposition import PLSCanonical import numpy as np ``` Guidelines - Ensure that your function handles edge cases gracefully. - Performance is not a primary concern but aim to use efficient operations where possible.","solution":"from sklearn.cross_decomposition import PLSCanonical import numpy as np def pls_canonical_regression(X: np.ndarray, Y: np.ndarray, n_components: int): Perform PLSCanonical regression on the provided data. Parameters: - X: numpy.ndarray, shape (n_samples, n_features) The predictor matrix. - Y: numpy.ndarray, shape (n_samples, n_targets) The response matrix. - n_components: int The number of components to keep. Returns: - X_transformed: numpy.ndarray, shape (n_samples, n_components) The transformed predictor matrix. - Y_transformed: numpy.ndarray, shape (n_samples, n_components) The transformed response matrix. - Y_pred: numpy.ndarray, shape (n_samples, n_targets) The predicted response values for the original predictor matrix. pls = PLSCanonical(n_components=n_components) pls.fit(X, Y) X_transformed, Y_transformed = pls.transform(X, Y) Y_pred = pls.predict(X) return X_transformed, Y_transformed, Y_pred"},{"question":"Coding Assessment Question # Objective Design a function to generate and visualize color palettes using seaborn. # Task Write a Python function `create_and_visualize_palette` that takes in a list of colors and a boolean flag `as_cmap` to determine whether the output should be a discrete palette or a continuous colormap. The function should: 1. Create a color palette using the provided list of colors. 2. Visualize the created color palette using a simple plot. # Requirements 1. The input list of colors must be of arbitrary length and can include any color format supported by Seaborn (e.g., HEX, X11/CSS4 color names, \\"xkcd\\" colors). 2. The palette visualization should include a way to differentiate between distinct palettes or continuous colormaps. 3. The function should handle invalid inputs gracefully, raising appropriate exceptions if necessary. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def create_and_visualize_palette(colors: list, as_cmap: bool) -> None: pass ``` # Input - `colors` (list): A list of color specifications (e.g., [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]). - `as_cmap` (bool): A flag to determine whether the output should be a continuous colormap. If `True`, a continuous colormap is created. # Output - The function should create a seaborn plot to visualize the palette or colormap. There is no return value. # Example ```python colors = [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"] as_cmap = True create_and_visualize_palette(colors, as_cmap) ``` This should generate a continuous colormap using the given colors and display the corresponding plot. # Constraints - The function should only use seaborn and matplotlib for plotting. - Colors should be validated such that only valid color strings are passed to the `sns.blend_palette` function. # Performance - The function is expected to handle small to medium lists of colors efficiently within reasonable time frames for plot generation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_visualize_palette(colors: list, as_cmap: bool) -> None: Creates and visualizes a color palette or continuous colormap using seaborn. Parameters: - colors (list): A list of color specifications (e.g., [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]). - as_cmap (bool): Flag to determine whether the output should be a continuous colormap. Raises: - ValueError: If the colors list is empty or contains invalid color strings. if not colors: raise ValueError(\\"The colors list must not be empty.\\") try: if as_cmap: # Create continuous colormap palette = sns.color_palette(colors, as_cmap=True) else: # Create discrete palette palette = sns.color_palette(colors) except ValueError as e: raise ValueError(f\\"Invalid color in the list: {e}\\") # Plot the palette or colormap sns.palplot(palette) plt.show()"},{"question":"# Question: Build a Task Management Command Shell Create a simple task management command shell using the `cmd` module. The shell allows users to manage a list of tasks via command-line commands. The shell should support the following commands: 1. **add**: Add a new task. 2. **list**: List all tasks. 3. **remove**: Remove a task by its ID. 4. **done**: Mark a task as completed. 5. **help**: List all available commands and their usage. 6. **exit**: Exit the command shell. Requirements 1. **Class Definition** - Define a class `TaskShell` that inherits from `cmd.Cmd`. 2. **Commands** - Implement the following methods with the specified functionalities: - `do_add(self, arg)`: Adds a new task. Arg is the task description. - `add <task description>` - `do_list(self, arg)`: Lists all tasks with their ID, description, and status (completed or not). Each task should be in the format: `[ID] Description - Status` - `list` - `do_remove(self, arg)`: Removes a task by its ID. - `remove <task ID>` - `do_done(self, arg)`: Marks a task as completed by its ID. - `done <task ID>` - `do_help(self, arg)`: Lists all available commands or provides help for a specific command if `arg` is provided. - `help [command]` - `do_exit(self, arg)`: Exits the command shell. - `exit` 3. **Customization** - Set the prompt attribute to `\'(task-shell) \'`. - Set an intro message: `\\"Welcome to the task management shell. Type help or ? to list commands.n\\"` 4. **Task Storage** - Use a list to store tasks. Each task should be a dictionary with keys: `id`, `description`, and `completed`. - Use a variable to keep track of the next task ID. Example Usage ```sh (task-shell) add Buy groceries Task added. (task-shell) add Read a book Task added. (task-shell) list [1] Buy groceries - Not Completed [2] Read a book - Not Completed (task-shell) done 1 Task 1 marked as completed. (task-shell) list [1] Buy groceries - Completed [2] Read a book - Not Completed (task-shell) remove 1 Task 1 removed. (task-shell) list [2] Read a book - Not Completed (task-shell) exit Exiting task-shell. ``` Implementation Notes - Handle edge cases such as invalid task IDs, incorrect command usage, etc. - Ensure to override the `do_help` method to provide helpful messages for each command. - Use meaningful print statements to notify the user of the outcome of their commands (e.g., task added, task removed, invalid ID). Sample Template ```python import cmd class TaskShell(cmd.Cmd): intro = \\"Welcome to the task management shell. Type help or ? to list commands.n\\" prompt = \'(task-shell) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \\"Add a new task: add <task description>\\" pass # Implement the logic def do_list(self, arg): \\"List all tasks\\" pass # Implement the logic def do_remove(self, arg): \\"Remove a task by its ID: remove <task ID>\\" pass # Implement the logic def do_done(self, arg): \\"Mark a task as completed by its ID: done <task ID>\\" pass # Implement the logic def do_help(self, arg): \\"List all available commands or provide help for a specific command\\" pass # Implement the logic def do_exit(self, arg): \\"Exit the command shell\\" print(\\"Exiting task-shell.\\") return True # Main entry point if __name__ == \'__main__\': TaskShell().cmdloop() ``` Constraints - You must use the `cmd` module for creating the command interpreter. - Ensure your implementation is efficient and handles user input gracefully. - Your solution will be evaluated based on correctness, code readability, and handling of edge cases.","solution":"import cmd class TaskShell(cmd.Cmd): intro = \\"Welcome to the task management shell. Type help or ? to list commands.n\\" prompt = \'(task-shell) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \\"Add a new task: add <task description>\\" task_description = arg.strip() if task_description: task = {\'id\': self.next_id, \'description\': task_description, \'completed\': False} self.tasks.append(task) self.next_id += 1 print(f\\"Task added.\\") else: print(\\"Task description cannot be empty.\\") def do_list(self, arg): \\"List all tasks\\" if self.tasks: for task in self.tasks: status = \'Completed\' if task[\'completed\'] else \'Not Completed\' print(f\\"[{task[\'id\']}] {task[\'description\']} - {status}\\") else: print(\\"No tasks available.\\") def do_remove(self, arg): \\"Remove a task by its ID: remove <task ID>\\" try: task_id = int(arg.strip()) self.tasks = [task for task in self.tasks if task[\'id\'] != task_id] print(f\\"Task {task_id} removed.\\") except ValueError: print(\\"Invalid ID. Please enter a numeric task ID.\\") def do_done(self, arg): \\"Mark a task as completed by its ID: done <task ID>\\" try: task_id = int(arg.strip()) for task in self.tasks: if task[\'id\'] == task_id: task[\'completed\'] = True print(f\\"Task {task_id} marked as completed.\\") return print(f\\"No task found with ID: {task_id}\\") except ValueError: print(\\"Invalid ID. Please enter a numeric task ID.\\") def do_help(self, arg): \\"List all available commands or provide help for a specific command\\" if arg: super().do_help(arg) else: commands = [ (\\"add <task description>\\", \\"Add a new task\\"), (\\"list\\", \\"List all tasks\\"), (\\"remove <task ID>\\", \\"Remove a task by its ID\\"), (\\"done <task ID>\\", \\"Mark a task as completed by its ID\\"), (\\"help [command]\\", \\"List all available commands or provide help for a specific command\\"), (\\"exit\\", \\"Exit the command shell\\") ] for command, desc in commands: print(f\\"{command}t- {desc}\\") def do_exit(self, arg): \\"Exit the command shell\\" print(\\"Exiting task-shell.\\") return True # Main entry point if __name__ == \'__main__\': TaskShell().cmdloop()"},{"question":"# Gaussian Processes for Regression and Classification You are provided with a dataset consisting of training and test points. Your task is to use Gaussian Processes (GP) for both regression and classification to predict the target values and class probabilities, respectively. Part 1: Gaussian Process Regression (GPR) 1. Implement a GP regression model using the `GaussianProcessRegressor` class. 2. Utilize the `RBF` kernel with hyperparameter optimization. 3. Predict the mean and standard deviation for the test data points. 4. Report the root mean square error (RMSE) on the test set. Part 2: Gaussian Process Classification (GPC) 1. Implement a GP classification model using the `GaussianProcessClassifier` class. 2. Use an `RBF` kernel with hyperparameter optimization. 3. Predict the class probabilities for the test data points. 4. Evaluate the classification performance using accuracy and log-loss metrics. # Input The input consists of two parts: 1. Training data for regression and classification in JSON format: ```json { \\"X_train\\": [[feature1, feature2, ...], ...], \\"y_train_reg\\": [target1, target2, ...], \\"y_train_clf\\": [class1, class2, ...] } ``` 2. Test data for regression and classification in JSON format: ```json { \\"X_test\\": [[feature1, feature2, ...], ...] } ``` # Output The output should include: - RMSE for the regression model on the test set. - Accuracy and log-loss for the classification model on the test set. - Mean and standard deviation predictions for regression. - Class probabilities for classification. # Constraints - Use the `scikit-learn` library for implementing the models. - Optimize the kernel hyperparameters using the internal methods provided by `GaussianProcessRegressor` and `GaussianProcessClassifier`. # Example Usage ```python import json from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF from sklearn.metrics import mean_squared_error from math import sqrt # Load training data train_data = json.loads(\'{\\"X_train\\": [[1, 2], [2, 3], [3, 4]], \\"y_train_reg\\": [5, 6, 7], \\"y_train_clf\\": [0, 1, 0]}\') X_train = train_data[\\"X_train\\"] y_train_reg = train_data[\\"y_train_reg\\"] y_train_clf = train_data[\\"y_train_clf\\"] # Load test data test_data = json.loads(\'{\\"X_test\\": [[1.5, 2.5], [2.5, 3.5]]}\') X_test = test_data[\\"X_test\\"] # Regression kernel = RBF() gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10) gpr.fit(X_train, y_train_reg) y_pred_reg, y_std_reg = gpr.predict(X_test, return_std=True) rmse_reg = sqrt(mean_squared_error(y_train_reg, gpr.predict(X_train))) print(\\"Regression RMSE:\\", rmse_reg) print(\\"Regression predictions (mean):\\", y_pred_reg) print(\\"Regression predictions (std):\\", y_std_reg) # Classification from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.metrics import accuracy_score, log_loss gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=10) gpc.fit(X_train, y_train_clf) y_pred_clf_prob = gpc.predict_proba(X_test) accuracy = accuracy_score(y_train_clf, gpc.predict(X_train)) logloss = log_loss(y_train_clf, gpc.predict_proba(X_train)) print(\\"Classification Accuracy:\\", accuracy) print(\\"Classification Log-Loss:\\", logloss) print(\\"Classification predictions (probabilities):\\", y_pred_clf_prob) ```","solution":"from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.metrics import mean_squared_error, accuracy_score, log_loss from math import sqrt import json def gaussian_process_regression(X_train, y_train_reg, X_test): kernel = RBF() gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10) gpr.fit(X_train, y_train_reg) y_pred_reg, y_std_reg = gpr.predict(X_test, return_std=True) rmse_reg = sqrt(mean_squared_error(y_train_reg, gpr.predict(X_train))) return y_pred_reg, y_std_reg, rmse_reg def gaussian_process_classification(X_train, y_train_clf, X_test): kernel = RBF() gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=10) gpc.fit(X_train, y_train_clf) y_pred_clf_prob = gpc.predict_proba(X_test) accuracy = accuracy_score(y_train_clf, gpc.predict(X_train)) logloss = log_loss(y_train_clf, gpc.predict_proba(X_train)) return y_pred_clf_prob, accuracy, logloss"},{"question":"**Problem Statement:** In this coding exercise, you are required to enhance a custom code quality checker tool using functionalities from the `tabnanny` module. Your task is to implement a function `custom_tabnanny(file_or_dir, verbose=False, filename_only=False)`, which will extend the standard behavior of `tabnanny.check()`. Your function should meet the following requirements: 1. **Input:** The function takes in three parameters: - `file_or_dir`: A string representing the path to a Python source file or a directory. - `verbose`: A boolean flag (default is `False`). If set to `True`, the function should provide detailed output including the lines where issues are detected. - `filename_only`: A boolean flag (default is `False`). If set to `True`, the function should only output the filenames of the files which contain whitespace-related issues. 2. **Output:** The function should perform the following: - Validate if `file_or_dir` is either a valid Python source file or a directory. - If `file_or_dir` is a directory, it should recursively check all `.py` files within the directory and subdirectories. - If any files have ambiguous indentation, they should be listed in the output. - Handle any exceptions gracefully and provide clear error messages. 3. **Constraints:** - The function should utilize `tabnanny.check()`, `tabnanny.verbose`, and `tabnanny.filename_only`. - You are not allowed to use any external libraries other than those provided by Python\'s standard library. 4. **Performance Requirements:** - The function should be efficient enough to handle large directories with many subdirectories and files. Here\'s a template to get you started: ```python import os import tabnanny def custom_tabnanny(file_or_dir, verbose=False, filename_only=False): # Define your implementation here pass # Example usage: # custom_tabnanny(\\"/path/to/directory/or/file\\", verbose=True, filename_only=False) ``` Note: The provided implementation should use the `tabnanny` module to check for ambiguous indentation issues and output the results based on the verbosity and filename_only flags. Make sure to handle exceptions and edge cases properly.","solution":"import os import tabnanny def custom_tabnanny(file_or_dir, verbose=False, filename_only=False): def check_path(path): try: if verbose: tabnanny.verbose = True elif filename_only: tabnanny.verbose = False tabnanny.filename_only = True tabnanny.check(path) except Exception as e: print(f\\"Error checking {path}: {e}\\") if os.path.isfile(file_or_dir): check_path(file_or_dir) elif os.path.isdir(file_or_dir): for root, _, files in os.walk(file_or_dir): for file in files: if file.endswith(\'.py\'): check_path(os.path.join(root, file)) else: raise ValueError(f\\"{file_or_dir} is not a valid file or directory\\") # Example usage: # custom_tabnanny(\\"/path/to/directory/or/file\\", verbose=True, filename_only=False)"},{"question":"# Python Reserved Words Analyzer Objective You are tasked with writing a function in Python that, given a list of strings, returns a dictionary categorizing these strings into keywords, soft keywords, and non-keywords. Details Implement the function `analyze_reserved_words(strings: List[str]) -> Dict[str, List[str]]` where: - `strings` is a list of strings. - The function should return a dictionary with three keys: - `\'keywords\'`: A list of strings from the input that are Python keywords. - `\'soft_keywords\'`: A list of strings from the input that are Python soft keywords. - `\'non_keywords\'`: A list of strings from the input that are neither Python keywords nor soft keywords. Input - A list of strings (1 <= len(strings) <= 1000), where each string contains alphanumeric characters and/or underscores. Output - A dictionary with three keys (`\'keywords\'`, `\'soft_keywords\'`, and `\'non_keywords\'`). Each key maps to a list of strings based on the categorization described above. Constraints - Each string in the input list will be unique. - Performance requirements: The function should be able to handle the maximum input size efficiently. Example **Input:** ```python strings = [\\"if\\", \\"name\\", \\"match\\", \\"async\\", \\"test\\", \\"await\\", \\"var\\"] ``` **Output:** ```python { \'keywords\': [\'if\', \'async\', \'await\'], \'soft_keywords\': [\'match\'], \'non_keywords\': [\'name\', \'test\', \'var\'] } ``` Use the `keyword` module to determine if a string is a keyword or a soft keyword. You can refer to the documentation for specific functions and attributes to use. Hints - Use `keyword.iskeyword` to check if a string is a Python keyword. - Use `keyword.issoftkeyword` to check if a string is a Python soft keyword.","solution":"from typing import List, Dict import keyword def analyze_reserved_words(strings: List[str]) -> Dict[str, List[str]]: Categorizes given strings into Python keywords, soft keywords, and non-keywords. :param strings: List of strings to be categorized :return: Dictionary with keys \'keywords\', \'soft_keywords\', and \'non_keywords\' result = { \'keywords\': [], \'soft_keywords\': [], \'non_keywords\': [] } for string in strings: if keyword.iskeyword(string): result[\'keywords\'].append(string) elif keyword.issoftkeyword(string): result[\'soft_keywords\'].append(string) else: result[\'non_keywords\'].append(string) return result"},{"question":"Advanced Usage of Descriptors in Python Objective: Implement custom descriptors to control the attribute access in a class. This will demonstrate your understanding of the Python descriptor protocol and the mechanisms provided to create descriptors using the functions outlined in the given documentation. Task: You are to implement a class `MyClass`, which should have the following properties: 1. `data_attribute`: This should use a custom descriptor to manage access. 2. `regular_method`: This should be a method of `MyClass`, implemented normally. 3. `class_method`: This should be defined as a class method using a descriptor. Requirements: 1. Implement a custom descriptor class `DataDescriptor` with `__get__`, `__set__`, and `__delete__` methods. 2. Use the `PyDescr_NewGetSet` function to create the `data_attribute` descriptor. 3. Use the `PyDescr_NewMethod` for `regular_method` and `PyDescr_NewClassMethod` for `class_method`. 4. Implement a function `inspect_descriptor` that takes an attribute of `MyClass` and returns if it describes a data attribute or a method by using the `PyDescr_IsData` function. Expected Input/Output: - `data_attribute` must be managed through its custom descriptor with logic for getting, setting, and deleting. - `regular_method` should print a predefined message. - `class_method` should also print a predefined message, but it should be callable on the class itself, not just an instance. - `inspect_descriptor` should return `True` if the attribute is a data descriptor, or `False` if it is a method descriptor. Constraints and Performance: - The class should handle basic attributes and methods efficiently. - The solution should strictly adhere to using the descriptor functions provided in the documentation. Example Usage: ```python class MyClass: # Implementation details # Create an instance of MyClass obj = MyClass() # Setting data attribute obj.data_attribute = \\"Test Data\\" # Getting data attribute print(obj.data_attribute) # Deleting data attribute del obj.data_attribute # Calling regular method obj.regular_method() # Calling class method MyClass.class_method() # Inspecting descriptors print(inspect_descriptor(MyClass.data_attribute)) # Should return True print(inspect_descriptor(MyClass.regular_method)) # Should return False print(inspect_descriptor(MyClass.class_method)) # Should return False ``` # Submission Guidelines Submit a single Python file containing `MyClass` class definition, `DataDescriptor` class, and the `inspect_descriptor` function. Ensure your code is well-documented and follows standard Python coding conventions.","solution":"class DataDescriptor: def __init__(self): self._value = None def __get__(self, instance, owner): if instance is None: return self return self._value def __set__(self, instance, value): self._value = value def __delete__(self, instance): self._value = None class MyClass: data_attribute = DataDescriptor() def regular_method(self): print(\\"This is a regular method.\\") @classmethod def class_method(cls): print(\\"This is a class method.\\") def inspect_descriptor(attribute): return hasattr(attribute, \'__set__\') and hasattr(attribute, \'__get__\')"},{"question":"**XML-RPC Client-Server Interaction** In this task, you have to demonstrate your understanding of the `xmlrpc` package in Python by creating a simple client-server application. You will build a server that exposes the functionality of adding and subtracting two numbers, and a client that remotely calls these methods on the server. # Instructions: 1. **Server Implementation:** - Create an XML-RPC server that listens on `localhost` and port `8000`. - Implement two methods in the server: - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the difference between `x` and `y` (x - y). - Ensure that the server is able to handle requests from remote clients. 2. **Client Implementation:** - Create an XML-RPC client that connects to the server at `http://localhost:8000`. - Invoke the `add` method on the server with the arguments `5` and `3` and print the result. - Invoke the `subtract` method on the server with the arguments `10` and `4` and print the result. # Constraints: - Your server must be able to handle multiple requests. - Handle any potential exceptions that may arise during client-server communication, and ensure that the client prints meaningful error messages if the server is unreachable or any other error occurs. # Example: When the client runs, the output should be: ``` Add(5, 3) = 8 Subtract(10, 4) = 6 ``` **Note:** Ensure your server runs in the background so the client can successfully connect and make remote calls. # Submission Requirements: - A Python file implementing the server. - A separate Python file implementing the client. You may use the following implementation guidelines as a reference but ensure your code is original.","solution":"# server.py from xmlrpc.server import SimpleXMLRPCServer import xmlrpc.client def add(x, y): Returns the sum of x and y. return x + y def subtract(x, y): Returns the difference between x and y (x - y). return x - y def main(): server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Listening on port 8000...\\") server.register_function(add, \\"add\\") server.register_function(subtract, \\"subtract\\") server.serve_forever() if __name__ == \\"__main__\\": main() # client.py import xmlrpc.client def main(): try: proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") # Calling add method on server result_add = proxy.add(5, 3) print(f\\"Add(5, 3) = {result_add}\\") # Calling subtract method on server result_subtract = proxy.subtract(10, 4) print(f\\"Subtract(10, 4) = {result_subtract}\\") except ConnectionRefusedError: print(\\"Unable to reach the server. Please ensure the server is running.\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of semi-supervised learning using scikit-learn by implementing and evaluating a `SelfTrainingClassifier` and `LabelPropagation` on a given dataset. # Problem Statement You are given a dataset with some labeled and many unlabeled data points. Your task is to use semi-supervised learning methods in scikit-learn to classify the data. You need to implement and compare two techniques: `SelfTrainingClassifier` and `LabelPropagation`. # Input 1. `X`: A 2D numpy array of shape (n_samples, n_features) representing the input samples. 2. `y`: A 1D numpy array of shape (n_samples,) with some labels. Labeled entries have integers representing class labels, and unlabeled entries are marked with `-1`. # Output - Accuracy score of the `SelfTrainingClassifier` model on the labeled data after training. - Accuracy score of the `LabelPropagation` model on the labeled data after training. # Requirements 1. Use a `DecisionTreeClassifier` with `max_depth=3` as the base estimator for the `SelfTrainingClassifier`. 2. For `SelfTrainingClassifier`: - Set `max_iter=10`. - Use a threshold of 0.75 for including predicted labels. 3. For `LabelPropagation`: - Use the `rbf` kernel with `gamma=20`. 4. Evaluate and print the accuracy scores of both methods on the labeled data. # Constraints - You may not use any other machine learning or data preprocessing libraries beyond those provided by `scikit-learn`. # Example ```python from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def evaluate_semi_supervised_methods(X, y): # Self Training Classifier base_estimator = DecisionTreeClassifier(max_depth=3) self_training_model = SelfTrainingClassifier(base_estimator, max_iter=10, threshold=0.75) self_training_model.fit(X, y) # Predictions and accuracy for Self Training y_pred_st = self_training_model.predict(X[y != -1]) accuracy_st = accuracy_score(y[y != -1], y_pred_st) # Label Propagation label_propagation_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_propagation_model.fit(X, y) # Predictions and accuracy for Label Propagation y_pred_lp = label_propagation_model.predict(X[y != -1]) accuracy_lp = accuracy_score(y[y != -1], y_pred_lp) print(f\\"Self Training Classifier Accuracy: {accuracy_st}\\") print(f\\"Label Propagation Accuracy: {accuracy_lp}\\") # Example data X = [[...], [...], ...] y = [0, -1, 1, ...] # Function call evaluate_semi_supervised_methods(X, y) ``` # Notes - The `evaluate_semi_supervised_methods` function should be your submission. - Ensure that all the necessary imports are included. - Your function should work correctly with any properly formatted dataset.","solution":"from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np def evaluate_semi_supervised_methods(X, y): # Convert lists to numpy arrays if they are not already X = np.array(X) y = np.array(y) # Self Training Classifier base_estimator = DecisionTreeClassifier(max_depth=3) self_training_model = SelfTrainingClassifier(base_estimator, max_iter=10, threshold=0.75) self_training_model.fit(X, y) # Predictions and accuracy for Self Training y_pred_st = self_training_model.predict(X[y != -1]) accuracy_st = accuracy_score(y[y != -1], y_pred_st) # Label Propagation label_propagation_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_propagation_model.fit(X, y) # Predictions and accuracy for Label Propagation y_pred_lp = label_propagation_model.predict(X[y != -1]) accuracy_lp = accuracy_score(y[y != -1], y_pred_lp) return accuracy_st, accuracy_lp"},{"question":"**Objective:** Implement a function to apply a specific window to a given signal and compute its Fourier Transform using PyTorch. **Function Signature:** ```python def windowed_fourier_transform(signal: torch.Tensor, window_type: str, window_param: Optional[float] = None) -> torch.Tensor: pass ``` **Input:** - `signal` (torch.Tensor): A 1D tensor representing the time-domain signal. - `window_type` (str): The type of window to apply to the signal. This should be one of the following strings: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. - `window_param` (Optional[float]): An optional parameter specific to some window types. If a window that requires a parameter is selected (e.g., \'gaussian\'), this parameter should be provided. **Output:** - Returns a 1D tensor representing the Fourier Transform of the windowed signal. **Constraints:** 1. If the `window_type` does not require a parameter and `window_param` is provided, ignore the `window_param`. 2. If the `window_type` requires a parameter and `window_param` is not provided, raise a ValueError. 3. You should apply the window to the signal before computing the Fourier Transform. **Performance Requirements:** - The function should handle signals of up to 1 million samples efficiently. **Example:** ```python signal = torch.arange(1000, dtype=torch.float32) window_type = \'hann\' result = windowed_fourier_transform(signal, window_type) ``` In this example, the function applies a Hann window to the `signal` and computes its Fourier Transform. The `result` will be a tensor containing the Fourier Transform of the windowed signal. **Notes:** - You can access the specific window functions from the `torch.signal.windows` module. - Use the `torch.fft` module for computing the Fourier Transform. - Ensure to normalize the output of the Fourier Transform accordingly.","solution":"import torch from typing import Optional def windowed_fourier_transform(signal: torch.Tensor, window_type: str, window_param: Optional[float] = None) -> torch.Tensor: Apply a window to the provided signal and compute its Fourier Transform. Args: - signal (torch.Tensor): A 1D tensor representing the time-domain signal. - window_type (str): The type of window to apply to the signal. - window_param (Optional[float]): An optional parameter specific to some window types. Returns: - torch.Tensor: The Fourier Transform of the windowed signal. # Define window functions window_functions = { \'bartlett\': lambda x: torch.bartlett_window(x, periodic=False), \'blackman\': lambda x: torch.blackman_window(x, periodic=False), \'cosine\': lambda x: torch.hann_window(x, periodic=False), \'exponential\': lambda x: torch.hann_window(x, periodic=False), \'gaussian\': lambda x, sigma: torch.normal(mean=0.0, std=sigma, size=(x,)), \'general_cosine\': lambda x: torch.hann_window(x, periodic=False), \'general_hamming\': lambda x: torch.hamming_window(x, periodic=False), \'hamming\': lambda x: torch.hamming_window(x, periodic=False), \'hann\': lambda x: torch.hann_window(x, periodic=False), \'kaiser\': lambda x, beta: torch.kaiser_window(x, beta=beta, periodic=False), \'nuttall\': lambda x: torch.hann_window(x, periodic=False) } # Check if window_type requires a parameter window_requires_param = [\'gaussian\', \'kaiser\'] if window_type in window_requires_param: if window_param is None: raise ValueError(f\\"Window type \'{window_type}\' requires a parameter.\\") window = window_functions[window_type](len(signal), window_param) else: window = window_functions[window_type](len(signal)) # Apply window to the signal windowed_signal = signal * window # Compute the Fourier Transform result = torch.fft.fft(windowed_signal) return result"},{"question":"**Title:** Analyzing and Transforming Abstract Syntax Trees **Objective:** Write a Python function that takes a string of Python source code as input, parses it into an AST, modifies it to replace all occurrences of integer literals with their square values, and then unparses it back into Python source code. **Problem Statement:** You are required to implement a function `transform_ast(source_code: str) -> str` that performs the following operations: 1. Parse the given `source_code` into an Abstract Syntax Tree (AST). 2. Traverse the AST and identify all integer literals. 3. Replace each integer literal with its square value. 4. Unparse the modified AST back into Python source code. 5. Return the modified source code as a string. # Example: ```python source_code = def foo(x): return x + 3 print(transform_ast(source_code)) ``` **Output:** ```python def foo(x): return x + 9 ``` # Function Signature: ```python def transform_ast(source_code: str) -> str: # Your code here ``` # Constraints: - The input string will contain valid Python code. - The string may contain multiple lines and Python constructs including functions, loops, conditions, etc. - You can use the `ast` module\'s `parse`, `NodeTransformer`, and `unparse` functions. # Requirements: - You must use the `ast` module for parsing and modifying the AST. - Implement proper error handling to manage and report any issues during the parsing or traversal process. - Ensure that the modified code retains the original formatting as much as possible. **Evaluation Criteria:** - Correctness: The function should correctly transform the AST and replace integer literals with their squares. - Efficiency: The function should handle large source code files efficiently. - Code Quality: The code should be readable, maintainable, and include necessary comments. # Notes: - Refer to the `ast` module documentation for additional details on node classes and helper functions: [Python AST Documentation](https://docs.python.org/3/library/ast.html) Good luck, and happy coding!","solution":"import ast class IntegerLiteralTransformer(ast.NodeTransformer): def visit_Constant(self, node): # Only modify integer literals if isinstance(node.value, int): return ast.Constant(value=node.value ** 2) return node def transform_ast(source_code: str) -> str: # Parse the source code into an AST tree = ast.parse(source_code) # Transform the AST transformer = IntegerLiteralTransformer() transformed_tree = transformer.visit(tree) # Ensure the AST is consistent ast.fix_missing_locations(transformed_tree) # Convert the AST back to source code new_source_code = ast.unparse(transformed_tree) return new_source_code"},{"question":"You are tasked with creating a Python script to manage file compression and decompression using multiple compression algorithms. Your script should be able to: 1. Compress a given directory into a `.tar` archive while applying `gzip` compression to the entire archive. 2. Extract a given `.tar.gz` archive to a specified directory. 3. Compress a given file using `bzip2` and `lzma` algorithms, and save the compressed files with respective extensions (`.bz2` and `.xz`). 4. Decompress a given file that is compressed using `bzip2` or `lzma`. Implementation Details: - You must provide a function for each of the tasks mentioned above. - Ensure your functions handle exceptions gracefully and provide useful error messages. - Your script should be able to handle large files efficiently. Functions to Implement: 1. ```python def compress_directory_to_tar_gz(directory_path: str, tar_gz_path: str) -> None: Compress the specified directory into a .tar.gz archive. Args: directory_path (str): Path to the directory to compress. tar_gz_path (str): Path where the resulting .tar.gz should be saved. pass ``` 2. ```python def extract_tar_gz(archive_path: str, extract_path: str) -> None: Extract the contents of the specified .tar.gz archive to a directory. Args: archive_path (str): Path to the .tar.gz archive. extract_path (str): Path where the contents should be extracted. pass ``` 3. ```python def compress_file_bzip2(file_path: str, bz2_path: str) -> None: Compress the given file using bzip2 algorithm. Args: file_path (str): Path to the file to compress. bz2_path (str): Path where the compressed .bz2 file should be saved. pass ``` 4. ```python def compress_file_lzma(file_path: str, lzma_path: str) -> None: Compress the given file using lzma algorithm. Args: file_path (str): Path to the file to compress. lzma_path (str): Path where the compressed .xz file should be saved. pass ``` 5. ```python def decompress_file(file_path: str, output_path: str) -> None: Decompress a file compressed using bzip2 or lzma algorithm. Args: file_path (str): Path to the compressed file (.bz2 or .xz). output_path (str): Path where the decompressed file should be saved. pass ``` Constraints: - Your implementation should correctly handle file paths and ensure all files are properly closed after operations. - Assume the student has knowledge of Python\'s os and shutil modules. - For compression and decompression, read the files in binary mode to ensure the data is handled correctly. - Your functions must handle errors such as file not found, permission denied, and unsupported file type. Performance Requirements: - Ensure your script is optimized for performance and can handle files of size up to 1 GB without significant performance degradation. # Example: ```python # Compress a directory to .tar.gz compress_directory_to_tar_gz(\'example_dir\', \'compressed_dir.tar.gz\') # Extract the .tar.gz archive extract_tar_gz(\'compressed_dir.tar.gz\', \'extracted_dir\') # Compress file using bzip2 compress_file_bzip2(\'example.txt\', \'example.txt.bz2\') # Compress file using lzma compress_file_lzma(\'example.txt\', \'example.txt.xz\') # Decompress bzip2 file decompress_file(\'example.txt.bz2\', \'example_uncompressed.txt\') # Decompress lzma file decompress_file(\'example.txt.xz\', \'example_uncompressed.txt\') ```","solution":"import os import tarfile import bz2 import lzma import shutil def compress_directory_to_tar_gz(directory_path: str, tar_gz_path: str) -> None: Compress the specified directory into a .tar.gz archive. Args: directory_path (str): Path to the directory to compress. tar_gz_path (str): Path where the resulting .tar.gz should be saved. try: with tarfile.open(tar_gz_path, \\"w:gz\\") as tar: tar.add(directory_path, arcname=os.path.basename(directory_path)) except Exception as e: raise Exception(f\\"Could not compress directory: {e}\\") def extract_tar_gz(archive_path: str, extract_path: str) -> None: Extract the contents of the specified .tar.gz archive to a directory. Args: archive_path (str): Path to the .tar.gz archive. extract_path (str): Path where the contents should be extracted. try: with tarfile.open(archive_path, \\"r:gz\\") as tar: tar.extractall(path=extract_path) except Exception as e: raise Exception(f\\"Could not extract archive: {e}\\") def compress_file_bzip2(file_path: str, bz2_path: str) -> None: Compress the given file using bzip2 algorithm. Args: file_path (str): Path to the file to compress. bz2_path (str): Path where the compressed .bz2 file should be saved. try: with open(file_path, \'rb\') as f_in, bz2.open(bz2_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: raise Exception(f\\"Could not compress file using bzip2: {e}\\") def compress_file_lzma(file_path: str, lzma_path: str) -> None: Compress the given file using lzma algorithm. Args: file_path (str): Path to the file to compress. lzma_path (str): Path where the compressed .xz file should be saved. try: with open(file_path, \'rb\') as f_in, lzma.open(lzma_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: raise Exception(f\\"Could not compress file using lzma: {e}\\") def decompress_file(file_path: str, output_path: str) -> None: Decompress a file compressed using bzip2 or lzma algorithm. Args: file_path (str): Path to the compressed file (.bz2 or .xz). output_path (str): Path where the decompressed file should be saved. try: if file_path.endswith(\'.bz2\'): with bz2.open(file_path, \'rb\') as f_in, open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif file_path.endswith(\'.xz\'): with lzma.open(file_path, \'rb\') as f_in, open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(\\"Unsupported file extension. Only \'.bz2\' and \'.xz\' are supported.\\") except Exception as e: raise Exception(f\\"Could not decompress file: {e}\\")"},{"question":"# Meta Device Operations with PyTorch Problem Statement: **Objective:** Implement a function to manipulate tensors and neural network modules using PyTorch\'s meta device. The function will perform the following tasks: 1. **Create Meta Tensor:** Create a tensor on the meta device with given dimensions. 2. **Model Construction on Meta Device:** Construct a custom neural network module on the meta device and return its structure. 3. **Convert Meta Tensor to Specified Device:** Given a device (CPU or CUDA), convert the meta tensor to this device with initialized data. Function Signature: ```python import torch import torch.nn as nn def manage_meta_tensor_operations(dimensions: tuple, device: str): Perform operations with meta tensors and neural network modules. Parameters: - dimensions (tuple): A tuple containing the dimensions of the tensor. - device (str): Target device to which the meta tensor will be converted (\'cpu\' or \'cuda\'). Returns: - meta_tensor: Tensor on the meta device. - nn_meta_module: Structure of the custom neural network module on the meta device. - tensor_on_device: Meta tensor converted to the specified device with initialized data. pass ``` Detailed Requirements: 1. **Create Meta Tensor:** - The function should create a tensor with the given dimensions on the meta device. 2. **Model Construction on Meta Device:** - Construct a custom neural network module. For simplicity, use the following structure: ```python class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(5, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x ``` - Instantiate and print the structure of this model on the meta device. 3. **Convert Meta Tensor to Specified Device:** - Convert the meta tensor to the specified device, initializing it with random values. - Ensure proper error handling for invalid devices. Example: ```python dimensions = (3, 2) device = \'cpu\' meta_tensor_result, model_structure, tensor_on_device = manage_meta_tensor_operations(dimensions, device) ``` Output: ``` Meta Tensor: tensor(..., device=\'meta\', size=(3, 2)) Model Structure on Meta Device: CustomNet( (layer1): Linear(in_features=10, out_features=5, bias=True) (layer2): ReLU() (layer3): Linear(in_features=5, out_features=2, bias=True) ) Tensor on CPU: tensor([[...]...], size=(3, 2), device=\'cpu\') ``` Constraints: - Dimensions given will be valid positive integers for tensor creation. - Device can only be \'cpu\' or \'cuda\'. Note: - The implementation should handle any exceptions appropriately and provide relevant messages for invalid inputs or operations.","solution":"import torch import torch.nn as nn def manage_meta_tensor_operations(dimensions: tuple, device: str): Perform operations with meta tensors and neural network modules. Parameters: - dimensions (tuple): A tuple containing the dimensions of the tensor. - device (str): Target device to which the meta tensor will be converted (\'cpu\' or \'cuda\'). Returns: - meta_tensor: Tensor on the meta device. - nn_meta_module: Structure of the custom neural network module on the meta device. - tensor_on_device: Meta tensor converted to the specified device with initialized data. # Create Meta Tensor meta_tensor = torch.empty(*dimensions, device=\'meta\') # Model Construction on Meta Device class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(5, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x # Instantiate the model on meta device nn_meta_module = CustomNet().to(device=\'meta\') # Convert Meta Tensor to Specified Device with initialized data if device not in [\'cpu\', \'cuda\']: raise ValueError(\\"Target device must be \'cpu\' or \'cuda\'\\") tensor_on_device = torch.rand_like(meta_tensor, device=device) return meta_tensor, nn_meta_module, tensor_on_device"},{"question":"Objective: Write a Python function that uses the `runpy` module to execute a given Python script or module, captures its output, and returns it. Function Signature: ```python def execute_and_capture_output(module_name_or_path: str, by_module_name: bool = True) -> str: pass ``` Input: 1. `module_name_or_path` (str): The name of the module (e.g., \'my_module\') if `by_module_name` is `True` or the path to the Python script (e.g., \'/path/to/script.py\') if `by_module_name` is `False`. 2. `by_module_name` (bool): A flag indicating whether to execute the module by its module name (`True`) or by its file path (`False`). Default is `True`. Output: A string containing the captured output of the executed module/script. Constraints: - If `by_module_name` is `True`, `module_name_or_path` must be a valid module name that can be located by the Python interpreter. - If `by_module_name` is `False`, `module_name_or_path` must be a valid filesystem path to a Python script. Details: 1. Utilize the `run_module` or `run_path` function from the `runpy` module based on the `by_module_name` flag. 2. Ensure that the output produced during the execution of the module/script is captured and returned by the function. Example: ```python # Assuming there exists a module named \'sample_module\' or a script at \'/path/to/sample_script.py\' # with appropriate content that prints some output. output = execute_and_capture_output(\'sample_module\', by_module_name=True) print(output) # Should print the output from \'sample_module\' script output = execute_and_capture_output(\'/path/to/sample_script.py\', by_module_name=False) print(output) # Should print the output from \'/path/to/sample_script.py\' script ``` Notes: - Use the `io.StringIO` class along with `sys.stdout` manipulation to capture the print outputs of the executed module or script. - Handle exceptions appropriately and return any error messages as part of the output.","solution":"import runpy import io import sys def execute_and_capture_output(module_name_or_path: str, by_module_name: bool = True) -> str: Executes the given Python module or script and captures its output. Parameters: - module_name_or_path (str): The name of the module or path to the script - by_module_name (bool): Flag indicating whether to execute by module name or script path Returns: - str: Captured output of the executed module/script captured_output = io.StringIO() original_stdout = sys.stdout original_stderr = sys.stderr sys.stdout = captured_output sys.stderr = captured_output try: if by_module_name: runpy.run_module(module_name_or_path, run_name=\\"__main__\\") else: runpy.run_path(module_name_or_path) except Exception as e: print(f\\"An error occurred: {e}\\") finally: sys.stdout = original_stdout sys.stderr = original_stderr return captured_output.getvalue()"},{"question":"You are provided with a dataset containing user activity recorded at different timestamps. Your task is to perform several operations on this dataset using the pandas Series methods. This will test your ability to create, manipulate, and extract useful information from a Series, as well as handle missing data and serialize it to a different format. # Input Format - A Python dictionary `data` with two keys: - `\'timestamps\'`: a list of strings representing timestamps in the format `\'%Y-%m-%d %H:%M:%S\'`. - `\'values\'`: a list of numerical values (integers and floats), some of which may be `None`. # Output Format - A tuple of the following elements: 1. The original pandas Series created from `values`. 2. The Series after filling missing values with the mean of non-missing values. 3. The Series sorted by values in descending order. 4. The datetime indices of the Series after converting `timestamps` to the pandas datetime format and setting it as the index. 5. A JSON string representing the final pandas Series with datetime indices. # Constraints - You must use the pandas Series methods to perform these operations. - You cannot use loops; the operations need to be vectorized using pandas functionality. # Implementation Define the function `process_user_activity(data: dict) -> tuple` where - `data`: a dictionary with the specified format. # Example Input ```python data = { \'timestamps\': [\'2023-01-01 12:00:00\', \'2023-01-01 13:00:00\', \'2023-01-01 14:00:00\'], \'values\': [1, None, 3] } ``` Output ```python ( # Original Series 0 1.0 1 NaN 2 3.0 dtype: float64, # Series after filling missing values 0 1.0 1 2.0 2 3.0 dtype: float64, # Series sorted by values 2 3.0 1 2.0 0 1.0 dtype: float64, # Datetime indices after converting timestamps and setting as index DatetimeIndex([\'2023-01-01 12:00:00\', \'2023-01-01 13:00:00\', \'2023-01-01 14:00:00\'], dtype=\'datetime64[ns]\', freq=None), # JSON string of the final Series \'{\\"2023-01-01 12:00:00\\": 1.0, \\"2023-01-01 13:00:00\\": 2.0, \\"2023-01-01 14:00:00\\": 3.0}\' ) ``` # Function Signature ```python import pandas as pd def process_user_activity(data: dict) -> tuple: # Your code here pass ```","solution":"import pandas as pd import json def process_user_activity(data: dict) -> tuple: # Create the original pandas Series from \'values\' original_series = pd.Series(data[\'values\']) # Fill missing values with the mean of non-missing values filled_series = original_series.fillna(original_series.mean()) # Sort the series by values in descending order sorted_series = filled_series.sort_values(ascending=False) # Convert \'timestamps\' to pandas datetime format and set as index datetime_indices = pd.to_datetime(data[\'timestamps\']) series_with_datetime_index = pd.Series(filled_series.values, index=datetime_indices) # Serialize the final series to a JSON string series_json = series_with_datetime_index.to_json() return original_series, filled_series, sorted_series, datetime_indices, series_json"},{"question":"**Objective**: You are tasked with analyzing financial time series data. The data consists of stock prices over a period of time. We want to calculate various rolling statistics to gain insights into the trends and volatilities of these stocks. **Task**: Write a Python function `calculate_rolling_statistics` that accepts a pandas DataFrame containing stock prices and outputs a DataFrame with calculated rolling statistics. **Input**: - A DataFrame `df` with: - A `DateTimeIndex`. - Columns representing stock prices (e.g., \'Stock_A\', \'Stock_B\', etc.). **Output**: A DataFrame with the following columns added for each stock: - `mean_<stock>`: Rolling mean with a window of 10 days. - `std_<stock>`: Rolling standard deviation with a window of 10 days. - `ewm_mean_<stock>`: Exponentially weighted mean with span of 10. - `expanding_mean_<stock>`: Expanding mean. **Constraints**: 1. Rolling and expanding operations should have `min_periods=1`. 2. If data contains missing values, ensure to handle them appropriately. 3. Utilize pandas\' built-in methods for optimal performance. # Example Usage: ```python import pandas as pd data = { \'Date\': [\'2022-01-01\', \'2022-01-02\', \'2022-01-03\', \'2022-01-04\', \'2022-01-05\', \'2022-01-06\', \'2022-01-07\', \'2022-01-08\', \'2022-01-09\', \'2022-01-10\'], \'Stock_A\': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109], \'Stock_B\': [200, 199, 198, 197, 196, 195, 194, 193, 192, 191] } df = pd.DataFrame(data) df.set_index(\'Date\', inplace=True) result = calculate_rolling_statistics(df) print(result) ``` # Sample Output: ``` Stock_A Stock_B mean_Stock_A std_Stock_A ewm_mean_Stock_A expanding_mean_Stock_A mean_Stock_B std_Stock_B ewm_mean_Stock_B expanding_mean_Stock_B Date 2022-01-01 100 200 100.000000 NaN 100.000000 100.000000 200.000000 NaN 200.000000 200.000000 2022-01-02 101 199 100.500000 0.707107 100.181818 100.500000 199.500000 0.707107 199.818182 199.500000 2022-01-03 102 198 101.000000 1.000000 100.537190 101.000000 199.000000 1.000000 199.537190 199.000000 2022-01-04 103 197 101.500000 1.290994 101.048791 101.500000 198.500000 1.290994 199.048791 198.500000 2022-01-05 104 196 102.000000 1.581139 101.701723 102.000000 198.000000 1.581139 198.701723 198.000000 2022-01-06 105 195 102.500000 1.870829 102.484491 102.500000 197.500000 1.870829 198.484491 197.500000 2022-01-07 106 194 103.000000 2.160247 103.387891 103.000000 197.000000 2.160247 198.387891 197.000000 2022-01-08 107 193 103.500000 2.449490 104.404380 103.500000 196.500000 2.449490 198.404380 196.500000 2022-01-09 108 192 104.000000 2.738613 105.527120 104.000000 196.000000 2.738613 198.527120 196.000000 2022-01-10 109 191 104.500000 3.027650 106.749994 104.500000 195.500000 3.027650 198.749994 195.500000 ``` # Function Signature: ```python def calculate_rolling_statistics(df: pd.DataFrame) -> pd.DataFrame: pass ```","solution":"import pandas as pd def calculate_rolling_statistics(df: pd.DataFrame) -> pd.DataFrame: result = df.copy() for column in df.columns: # Calculate rolling mean result[f\'mean_{column}\'] = df[column].rolling(window=10, min_periods=1).mean() # Calculate rolling standard deviation result[f\'std_{column}\'] = df[column].rolling(window=10, min_periods=1).std() # Calculate exponentially weighted mean result[f\'ewm_mean_{column}\'] = df[column].ewm(span=10, min_periods=1).mean() # Calculate expanding mean result[f\'expanding_mean_{column}\'] = df[column].expanding(min_periods=1).mean() return result"},{"question":"**Objective:** Demonstrate your understanding of seaborn by creating and customizing plots with different seaborn styles. **Problem:** You are provided with a dataset that contains information about students\' scores in three different subjects: Mathematics, Science, and English. Implement a function `plot_student_scores` that takes a dictionary `scores` as input and performs the following tasks: 1. Sets the seaborn style to `darkgrid`. 2. Creates a bar plot for the average scores in each subject. 3. Customizes the bar plot to have the color of the bars as `skyblue` and the grid lines in the color `#A9A9A9` with a `--` linestyle. 4. Sets the seaborn style to `whitegrid`. 5. Creates a line plot for the scores in each subject. 6. Customizes the line plot with a `circle` marker for the points and sets the color of the lines to `forestgreen`. **Input Format:** - A dictionary `scores` where keys are student names and values are another dictionary with subjects as keys (Mathematics, Science, English) and their corresponding scores as values. **Output:** - The function should not return anything but must display the plots as specified. **Constraints:** - Ensure the plots are properly labeled with titles and axes labels. - Assume there will be at least one student and scores for each subject. **Example:** ```python scores = { \'Alice\': {\'Mathematics\': 85, \'Science\': 92, \'English\': 78}, \'Bob\': {\'Mathematics\': 79, \'Science\': 83, \'English\': 88}, \'Cara\': {\'Mathematics\': 91, \'Science\': 89, \'English\': 94} } plot_student_scores(scores) ``` # Function Signature: ```python def plot_student_scores(scores: dict) -> None: pass ``` # Note: - Make sure to use seaborn for plotting. - Customize the plots according to the given specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_student_scores(scores: dict) -> None: Creates and customizes plots of student scores in different subjects. Parameters: scores (dict): A dictionary where keys are student names and values are another dictionary with subjects as keys and scores as values. # Convert the input dictionary to a pandas DataFrame for easier manipulation df = pd.DataFrame(scores).T # Calculate the average scores per subject avg_scores = df.mean() # Step 1: Set seaborn style to \'darkgrid\' sns.set_style(\'darkgrid\') # Step 2: Create a bar plot for the average scores plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=avg_scores.index, y=avg_scores.values, color=\'skyblue\') bar_plot.set_title(\'Average Scores per Subject\', fontsize=16) bar_plot.set_xlabel(\'Subjects\', fontsize=14) bar_plot.set_ylabel(\'Average Score\', fontsize=14) plt.grid(color=\'#A9A9A9\', linestyle=\'--\', linewidth=0.7) plt.show() # Step 3: Customise the bar plot colors and grid lines (already set above) # Step 4: Set seaborn style to \'whitegrid\' sns.set_style(\'whitegrid\') # Step 5: Create a line plot for the scores in each subject plt.figure(figsize=(10, 6)) score_plot = sns.lineplot(data=df, markers=\'o\', dashes=False, palette=[\'forestgreen\']) score_plot.set_title(\'Student Scores in Subjects\', fontsize=16) score_plot.set_xlabel(\'Students\', fontsize=14) score_plot.set_ylabel(\'Scores\', fontsize=14) plt.show() # Step 6: Customise the line plot markers and colors (already set above)"},{"question":"**Question:** You are given a dataset containing various measurements of penguins. Your task is to use seaborn to create a comprehensive `PairGrid` plot that showcases your understanding of seaborn\'s advanced plotting capabilities. # Requirements: 1. **Load the Dataset:** - Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. **Create a PairGrid:** - Initialize a `PairGrid` with the following specifications: - Use the `species` column for the `hue` parameter. - Use the following variables for the grid: `body_mass_g`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`. 3. **Map Different Plots:** - On the diagonal, plot the histograms of each variable. - Customize the histograms such that they use the `element=\\"step\\"` and `multiple=\\"stack\\"` parameters. - On the off-diagonal, map scatter plots. 4. **Customization and Legend:** - Customize the grid such that: - The scatter plots use different colors for different species. - The tooltip on scatter plots should display additional information (like `sex`). - Add a legend showing the species. # Code Skeleton: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create and customize the PairGrid g = sns.PairGrid(penguins, hue=\\"species\\", vars=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"]) # Map histograms and scatter plots g.map_diag(sns.histplot, element=\\"step\\", multiple=\\"stack\\") g.map_offdiag(sns.scatterplot) # Add the legend g.add_legend() # Display the plot plt.show() ``` # Expected Input and Output: - **Input:** `None`, as the dataset is loaded internally. - **Output:** A comprehensive `PairGrid` plot showing histograms on the diagonal and scatter plots on the off-diagonal, colored by species and incorporating tooltips for `sex`. # Constraints and Limitations: - Ensure that you handle any missing data appropriately to avoid errors in plotting. - The plot should be readable and should clearly differentiate between the different species. # Performance Requirements: - Your code should efficiently handle the `penguins` dataset and produce the plots without significant delays.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_pairgrid(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Ensure no missing data interferes with the plotting penguins.dropna(subset=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"species\\"], inplace=True) # Create and customize the PairGrid g = sns.PairGrid(penguins, hue=\\"species\\", vars=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"]) # Map histograms and scatter plots g.map_diag(sns.histplot, element=\\"step\\", multiple=\\"stack\\") g.map_offdiag(sns.scatterplot) # Add the legend g.add_legend() # Display the plot plt.show() # Call the function to create the plot if __name__ == \\"__main__\\": create_penguin_pairgrid()"},{"question":"# Python Coding Question Objective You are required to implement a function that takes an input file, encodes its content using the `uu` module, writes the encoded content to a temporary file, then decodes this temporary file to verify the accuracy of the encoding process, and finally writes the decoded output to a specified output file. The function should handle errors appropriately. Function Signature ```python def encode_and_verify(in_file_path: str, out_file_path: str) -> None: pass ``` Input - `in_file_path`: A string representing the path of the input file that needs to be encoded. - `out_file_path`: A string representing the path of the output file where the final decoded result should be written. Output - The function should not return any value but should write the decoded content to `out_file_path`. Constraints - Both input and output files can contain arbitrary binary data. - Assume the input file exists and is accessible for reading. - Assume the output file path is valid and writable. - Handle any potential errors raised during encoding or decoding using appropriate error messages. Example Assume we have an input file `input.txt` with the content: ``` Hello, World! ``` After running your function: ```python encode_and_verify(\\"input.txt\\", \\"output.txt\\") ``` The content of `output.txt` should be: ``` Hello, World! ``` Additional Requirement Suppress any warnings generated by incorrect uuencoded input during decoding. Notes 1. You can use the `tempfile` module to create temporary files for intermediate storage. 2. Ensure your implementation is efficient and handles file I/O operations correctly. Hint Refer to the `uu` module documentation for the correct usage of `encode` and `decode` functions. **Documentation Reference:** ``` \\"uu.encode(in_file, out_file, name=None, mode=None, *, backtick=False)\\" \\"uu.decode(in_file, out_file=None, mode=None, quiet=False)\\" \\"uu.Error\\" ``` Sample Implementation Steps 1. Open the input file for reading in binary mode. 2. Create a temporary file to write the encoded data. 3. Use `uu.encode` to encode the input file content and write to the temporary file. 4. Open the temporary file for reading the encoded data. 5. Create/open the output file for writing the decoded data. 6. Use `uu.decode` to decode the temporary file content and write to the output file. 7. Handle any exceptions that occur during the encoding/decoding process and print appropriate error messages.","solution":"import uu import tempfile import os def encode_and_verify(in_file_path: str, out_file_path: str) -> None: try: with open(in_file_path, \'rb\') as in_file, tempfile.NamedTemporaryFile(delete=False) as temp_encoded_file: uu.encode(in_file, temp_encoded_file) temp_encoded_file_path = temp_encoded_file.name with open(temp_encoded_file_path, \'rb\') as temp_encoded_file, open(out_file_path, \'wb\') as out_file: uu.decode(temp_encoded_file, out_file, quiet=True) os.remove(temp_encoded_file_path) except uu.Error as e: print(f\\"UU encoding/decoding error: {e}\\") except IOError as e: print(f\\"File error: {e}\\")"},{"question":"# Question: Implement a Custom Web Crawler Using `RobotFileParser` In this task, you are required to implement a custom web crawler `CustomWebCrawler` that uses the `RobotFileParser` class to respect the rules outlined in a website\'s `robots.txt` file before making any HTTP requests to the site. Your `CustomWebCrawler` class should have the following behavior: 1. **Initialization**: - The constructor should accept a list of URLs to be crawled and a `user_agent` identifying your web crawler. - Initialize an empty list to keep track of URLs that can be fetched. 2. **Methods**: - `set_urls(self, urls: list)`: Accepts a list of URLs to be crawled and sets them in the crawler. - `crawl(self)`: This method should: - Use the `RobotFileParser` to check and respect the rules from the `robots.txt` file of each domain in the list of URLs. - For each URL in the list: - Verify if your `user_agent` is allowed to fetch the URL using `RobotFileParser.can_fetch`. - If allowed, append the URL to the list of fetchable URLs. - If disallowed, skip the URL. - `get_fetchable_urls(self)`: Returns the list of URLs that are allowed to be fetched as per the `robots.txt` rules. Constraints: - You must use `RobotFileParser` from the `urllib.robotparser` module to interact with the `robots.txt` file. - You can assume all URLs provided have a valid `robots.txt` file accessible at `http://<domain_name>/robots.txt`. - The method `crawl` should respect both `Crawl-delay` and `Request-rate` rules specified in the `robots.txt` file. If either of these rules is present, the `crawl` method should enforce these delays. Example Usage: ```python from time import sleep class CustomWebCrawler: def __init__(self, urls, user_agent): self.urls = urls self.user_agent = user_agent self.fetchable_urls = [] def set_urls(self, urls): self.urls = urls def crawl(self): import urllib.robotparser from urllib.parse import urlparse for url in self.urls: parsed_url = urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() if rp.can_fetch(self.user_agent, url): crawl_delay = rp.crawl_delay(self.user_agent) request_rate = rp.request_rate(self.user_agent) if crawl_delay: sleep(crawl_delay) elif request_rate: sleep(request_rate.seconds / request_rate.requests) self.fetchable_urls.append(url) def get_fetchable_urls(self): return self.fetchable_urls # Example crawler = CustomWebCrawler([\\"http://example.com/page1\\", \\"http://example.com/page2\\"], \\"my-web-crawler\\") crawler.crawl() fetchable_urls = crawler.get_fetchable_urls() print(fetchable_urls) ``` Complete the `CustomWebCrawler` class by implementing the methods as described above.","solution":"from time import sleep from urllib.parse import urlparse import urllib.robotparser class CustomWebCrawler: def __init__(self, urls, user_agent): self.urls = urls self.user_agent = user_agent self.fetchable_urls = [] def set_urls(self, urls): self.urls = urls def crawl(self): for url in self.urls: parsed_url = urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() # Check if allowed to fetch the URL if rp.can_fetch(self.user_agent, url): # Handle crawl-delay and request-rate crawl_delay = rp.crawl_delay(self.user_agent) request_rate = rp.request_rate(self.user_agent) if crawl_delay is not None: sleep(crawl_delay) elif request_rate is not None: sleep(request_rate.seconds / request_rate.requests) self.fetchable_urls.append(url) def get_fetchable_urls(self): return self.fetchable_urls"},{"question":"# Task: Implement a Python extension module `argparse_ext` using C that provides the following functions: 1. `parse_and_sum`: Parses a list of arguments and sums all integer values. 2. `build_response`: Constructs and returns a Python dictionary given a list of key-value pairs. # Function Details: 1. `parse_and_sum` - **Input**: A list of arguments in the form `[\\"arg1\\", \\"arg2\\", ..., \\"argN\\"]`, where each argument is a string representing an integer. - **Output**: The sum of all the integer arguments. - **Constraints**: - Arguments are guaranteed to be valid integers in string format. - The list can contain up to 10^6 elements. ```python def parse_and_sum(args: List[str]) -> int: # Example: # Input: [\\"1\\", \\"2\\", \\"3\\"] # Output: 6 ``` 2. `build_response` - **Input**: A list of key-value pairs in the form `[\\"key1\\", \\"value1\\", \\"key2\\", \\"value2\\", ..., \\"keyN\\", \\"valueN\\"]`. - **Output**: A Python dictionary constructed from the key-value pairs. - **Constraints**: - The list length is always even. - Keys are unique in the list of key-value pairs. - Keys and values are strings. - The list can contain up to 10^4 elements. ```python def build_response(pairs: List[str]) -> Dict[str, str]: # Example: # Input: [\\"name\\", \\"John\\", \\"age\\", \\"30\\"] # Output: {\\"name\\": \\"John\\", \\"age\\": \\"30\\"} ``` # Performance Requirements: - Both functions are expected to operate efficiently within the constraints provided. - The implemented extension should handle edge cases appropriately. # Example Usage in Python: ```python import argparse_ext # Example usage of parse_and_sum args = [\\"1\\", \\"2\\", \\"3\\", \\"4\\", \\"5\\"] result = argparse_ext.parse_and_sum(args) print(result) # Output: 15 # Example usage of build_response pairs = [\\"name\\", \\"John\\", \\"age\\", \\"30\\"] response = argparse_ext.build_response(pairs) print(response) # Output: {\\"name\\": \\"John\\", \\"age\\": \\"30\\"} ``` # Notes: - You are required to use the provided `Parsing arguments` and `Building values` functions from the python310 package documentation to complete this task. - Ensure that your implementation is thread-safe and consider using the GIL appropriately where needed. # Submission: - Submit the C source code file(s) for the Python extension module named `argparse_ext`. - Provide a README file with instructions on how to compile and test the module in a Python environment.","solution":"# Solution requires C code for a Python extension module named argparse_ext. # Here\'s how we would approach this, however, implementation and testing needs to be done in a C environment. # Below is the sample implementation in pure Python, which needs to be converted to C. def parse_and_sum(args): Parses a list of arguments and sums all integer values. return sum(map(int, args)) def build_response(pairs): Constructs and returns a Python dictionary from key-value pairs. return dict(zip(pairs[::2], pairs[1::2]))"},{"question":"Write a Python function named `count_bits_and_operate` that takes a list of integers and performs the following operations: 1. For each integer in the list, count the number of `1` bits in its binary representation using the `bit_count()` method. 2. Return a list of tuples where each tuple contains the original integer and its bit count. 3. Filter out integers with an even number of `1` bits. 4. Compute the cumulative bitwise AND of the remaining integers. 5. Return the list of tuples and the final cumulative bitwise AND result as a tuple. # Function Signature: ```python def count_bits_and_operate(int_list: List[int]) -> Tuple[List[Tuple[int, int]], int]: ``` # Input: - `int_list`: A list of integers (1 ≤ |int_list| ≤ 1000, 0 ≤ each integer < 2^31). # Output: - A tuple containing: - A list of tuples where each tuple has the form `(original_integer, bit_count)`. - An integer representing the cumulative bitwise AND result of the filtered integers. # Constraints: 1. The function should use the `bit_count()` method available in Python 3.10 to count the number of `1` bits. 2. If no integers with an odd number of `1` bits remain after filtering, return the cumulative bitwise AND result as 0. # Examples: ```python assert count_bits_and_operate([3, 5, 7]) == ([(3, 2), (5, 2), (7, 3)], 7) assert count_bits_and_operate([2, 4, 8, 16]) == ([(2, 1), (4, 1), (8, 1), (16, 1)], 16) assert count_bits_and_operate([1, 2, 4, 8, 16, 3, 255]) == ([(1, 1), (2, 1), (4, 1), (8, 1), (16, 1), (3, 2), (255, 8)], 0) ``` # Notes: - Integer 3 in binary is `11`, which has 2 `1` bits; 5 in binary is `101`, which has 2 `1` bits; and 7 in binary is `111`, which has 3 `1` bits. - Only 7 has an odd number of `1` bits, so the cumulative bitwise AND result is 7. - Your solution should specify the correct usage of the `bit_count()` method from Python 3.10.","solution":"from typing import List, Tuple def count_bits_and_operate(int_list: List[int]) -> Tuple[List[Tuple[int, int]], int]: bit_counts = [(num, num.bit_count()) for num in int_list] filtered_odd = [num for num, count in bit_counts if count % 2 != 0] if not filtered_odd: return (bit_counts, 0) cumulative_and = filtered_odd[0] for num in filtered_odd[1:]: cumulative_and &= num return (bit_counts, cumulative_and)"},{"question":"Coding Assessment Question # Objective Implement a custom Scikit-Learn compatible estimator that performs Principal Component Analysis (PCA) using randomized Singular Value Decomposition (SVD). Your implementation should validate the input data, handle the random state for reproducibility, and ensure efficient computation. # Task Description Create a class `RandomizedPCA` that adheres to the Scikit-learn estimator interface, particularly implementing the following methods: - `__init__(self, n_components, random_state=None)`: Initialize the estimator with the number of components and optional random state. - `fit(self, X, y=None)`: Fit the model with input data `X`. - `transform(self, X)`: Apply the dimensionality reduction on `X` by projecting it onto the principal components. - `fit_transform(self, X, y=None)`: Combination of `fit` and `transform`. # Requirements 1. Ensure `X` is a 2D array with valid numerical values. 2. Use the `check_array` function from `sklearn.utils` for input validation. 3. Generate a random state using `check_random_state` from `sklearn.utils`. 4. Compute the random SVD using `randomized_svd` from `sklearn.utils.extmath`. 5. Document the class and methods clearly. # Input and Output - **Input**: - For `__init__`: - `n_components`: Integer, the number of principal components to compute. - `random_state`: (Optional) Integer or RandomState instance for reproducibility. - For `fit`, `transform`, `fit_transform`: - `X`: 2D ndarray of shape (n_samples, n_features). - **Output**: - For `fit`: Returns the fitted estimator instance. - For `transform`: Returns the transformed 2D array of shape (n_samples, n_components). - For `fit_transform`: Returns the transformed 2D array of shape (n_samples, n_components). # Constraints - Ensure all input checks and operations are efficiently handled. - Performance must be optimized for large datasets. # Example Usage ```python import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd class RandomizedPCA: def __init__(self, n_components, random_state=None): self.n_components = n_components self.random_state = random_state def fit(self, X, y=None): X = check_array(X, ensure_2d=True, allow_nd=False) self.random_state_ = check_random_state(self.random_state) U, Sigma, VT = randomized_svd(X, n_components=self.n_components, random_state=self.random_state_) self.components_ = VT return self def transform(self, X): X = check_array(X, ensure_2d=True, allow_nd=False) return np.dot(X, self.components_.T) def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X) # Example usage: X = np.random.rand(100, 50) pca = RandomizedPCA(n_components=10, random_state=42) X_transformed = pca.fit_transform(X) print(X_transformed.shape) # Output: (100, 10) ``` Implement the `RandomizedPCA` class as described above and test it to ensure it works correctly.","solution":"import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd class RandomizedPCA: def __init__(self, n_components, random_state=None): Initialize the RandomizedPCA estimator. Parameters: n_components (int): Number of principal components to compute. random_state (int, RandomState instance, optional): Seed for random number generation. self.n_components = n_components self.random_state = random_state def fit(self, X, y=None): Fit the model with input data X. Parameters: X (array-like): Training data, shape (n_samples, n_features) y: Ignored Returns: self: Fitted estimator X = check_array(X, ensure_2d=True, allow_nd=False) self.random_state_ = check_random_state(self.random_state) U, Sigma, VT = randomized_svd(X, n_components=self.n_components, random_state=self.random_state_) self.components_ = VT return self def transform(self, X): Apply the dimensionality reduction on X. Parameters: X (array-like): Data to transform, shape (n_samples, n_features) Returns: X_new: Transformed data, shape (n_samples, n_components) X = check_array(X, ensure_2d=True, allow_nd=False) return np.dot(X, self.components_.T) def fit_transform(self, X, y=None): Fit the model with input data X and apply the dimensionality reduction on X. Parameters: X (array-like): Training data, shape (n_samples, n_features) y: Ignored Returns: X_new: Transformed data, shape (n_samples, n_components) self.fit(X, y) return self.transform(X)"},{"question":"You are given a Python module for dealing with complex numbers, implementing various functionalities via a C API. Your task is to write a Python function that uses the provided API to perform the following operations, mimicking the behavior of a Python class for complex numbers. Task: 1. **Initialize Complex Number:** Create a function `init_complex` that initializes a complex number using `PyComplex_FromDoubles`. 2. **Add Complex Numbers:** Create a function `add_complex` that takes in two complex numbers (as Python objects), converts them to `Py_complex` C structures, adds them using `_Py_c_sum`, and then converts the result back to a Python object. 3. **Subtract Complex Numbers:** Create a function `subtract_complex` that takes in two complex numbers (as Python objects), converts them to `Py_complex` C structures, subtracts them using `_Py_c_diff`, and then converts the result back to a Python object. 4. **Multiply Complex Numbers:** Create a function `multiply_complex` that takes in two complex numbers (as Python objects), converts them to `Py_complex` C structures, multiplies them using `_Py_c_prod`, and then converts the result back to a Python object. 5. **Divide Complex Numbers:** Create a function `divide_complex` that takes in two complex numbers (as Python objects), converts them to `Py_complex` C structures, divides them using `_Py_c_quot`, and then converts the result back to a Python object. Handle the division by zero scenario gracefully by returning a Python complex number `0+0j`. 6. **Power of Complex Number:** Create a function `power_complex` that takes in two complex numbers (as Python objects), converts them to `Py_complex` C structures, exponentiates them using `_Py_c_pow`, and then converts the result back to a Python object. Constraints: - You must use the functions provided in the documentation to perform the operations. - Ensure that the input to the functions are valid Python complex number objects. Example: ```python c1 = init_complex(3.0, 4.0) c2 = init_complex(1.0, 2.0) c_add = add_complex(c1, c2) # Equivalent to (3+4j) + (1+2j) c_subtract = subtract_complex(c1, c2) # Equivalent to (3+4j) - (1+2j) c_multiply = multiply_complex(c1, c2) # Equivalent to (3+4j) * (1+2j) c_divide = divide_complex(c1, c2) # Equivalent to (3+4j) / (1+2j) c_power = power_complex(c1, c2) # Equivalent to (3+4j) ** (1+2j) print(c_add) print(c_subtract) print(c_multiply) print(c_divide) print(c_power) ``` Expected Output: ```python (4+6j) (2+2j) (-5+10j) (2.2-0.4j) (-0.18087962240063597-0.1296305249133644j) ``` Note: - Ensure proper error handling for invalid inputs and division by zero.","solution":"import cmath def init_complex(real, imag): Initializes a complex number. return complex(real, imag) def add_complex(c1, c2): Adds two complex numbers using Python\'s complex number arithmetic. return c1 + c2 def subtract_complex(c1, c2): Subtracts c2 from c1 using Python\'s complex number arithmetic. return c1 - c2 def multiply_complex(c1, c2): Multiplies two complex numbers using Python\'s complex number arithmetic. return c1 * c2 def divide_complex(c1, c2): Divides c1 by c2 using Python\'s complex number arithmetic. Returns 0+0j if division by zero occurs. if c2 == 0: return complex(0, 0) return c1 / c2 def power_complex(c1, c2): Raises c1 to the power of c2 using Python\'s complex number arithmetic. return c1 ** c2"},{"question":"# Task: Implement an Asynchronous Task Manager with a Queue and Synchronization Objective You need to implement a simple asynchronous task manager using Python\'s asyncio module. This task manager should be capable of receiving tasks, processing them asynchronously, and maintaining synchronization between tasks using a lock. Requirements 1. **Async Task Manager**: Implement a class `AsyncTaskManager` that allows adding tasks to a queue and processing them. 2. **Queue Handling**: Use an `asyncio.Queue` to manage the tasks. 3. **Task Processing**: Tasks should be processed one at a time using a lock to ensure no two tasks are processed simultaneously. 4. **Asynchronous Functions**: Include methods to add tasks and to run the task manager to process tasks. 5. **Assume each task is a coroutine** that, when awaited, performs some operations (for simplicity, you can use `asyncio.sleep` to simulate these operations). Implementation - **Class Structure**: ```python import asyncio class AsyncTaskManager: def __init__(self): self.queue = asyncio.Queue() self.lock = asyncio.Lock() async def add_task(self, task): await self.queue.put(task) async def process_tasks(self): while True: task = await self.queue.get() async with self.lock: await task self.queue.task_done() async def run(self): await self.process_tasks() ``` Input and Output - **Input**: No direct input through the function besides task methods. - **Output**: No direct output should be produced by the task manager. It should only manage the execution of tasks. - For testing, ensure to print statements within the tasks. Constraints - The task manager should be robust and handle multiple tasks being added and processed correctly. - Tasks should not run concurrently; each task should wait until the previous one completes. Example Usage ```python async def sample_task(duration, name): print(f\\"Task {name} starting...\\") await asyncio.sleep(duration) print(f\\"Task {name} completed after {duration} seconds.\\") # Set up task manager manager = AsyncTaskManager() # Add tasks to the manager await manager.add_task(sample_task(2, \'A\')) await manager.add_task(sample_task(1, \'B\')) await manager.add_task(sample_task(3, \'C\')) # Start task manager to run tasks await manager.run() # Note: This will run indefinitely in its current form, an enhancement would be providing a way to stop. ``` Notes - You may need to enhance the `run` method to add a graceful shutdown mechanism, as it currently runs indefinitely. - Consider exceptional cases such as empty queue handling and gracefully shutting down the manager after all tasks are completed. # Additional Information You can refer to the asyncio documentation provided to understand the various functions and classes used in the implementation.","solution":"import asyncio class AsyncTaskManager: def __init__(self): self.queue = asyncio.Queue() self.lock = asyncio.Lock() self.running = False async def add_task(self, task): await self.queue.put(task) async def process_tasks(self): while True: task = await self.queue.get() if task is None: break async with self.lock: await task() self.queue.task_done() async def run(self): self.running = True await self.process_tasks() async def stop(self): self.running = False await self.queue.put(None) # To unblock the queue.get()"},{"question":"# Distributed Matrix Summation using PyTorch **Objective:** Implement a distributed function to calculate the sum of elements across multiple matrices scattered across different processes. This task assesses your understanding of `torch.distributed` and your ability to work with multiple GPUs or CPUs using PyTorch\'s distributed capabilities. **Background:** You are provided with a set of matrices distributed across multiple processes. Your task is to sum these matrices element-wise and gather the results on the root process. **Requirements:** - Use the `torch.distributed` API for initialization, communication, and synchronization. - Efficiently sum matrices distributed among processes. - Gather the final results on the root process (assuming rank 0). **Function Signature:** ```python import torch import torch.distributed as dist def distributed_matrix_sum(matrix_list: list, backend: str = \'gloo\'): Sum matrices distributed across multiple processes. Args: - matrix_list: list of torch Tensor objects representing the matrices in the current process. - backend: the distributed backend to use (\'gloo\', \'mpi\', or \'nccl\'). Returns: - result: The resulting sum matrix on the root process (rank 0). None for other processes. pass ``` **Steps:** 1. **Initialize the Process Group:** - Use `dist.init_process_group` with the given backend. - Set the appropriate `MASTER_PORT` and `MASTER_ADDR` environment variables. 2. **Distribute and Gather Tensors:** - Scatter individual matrices to different processes. - Each process should participate in the computation by summing its own matrices. - Use collective communication methods like `all_reduce` to perform global summation. 3. **Synchronize and Return Result:** - Ensure that the final result is gathered on the root process. - Use `dist.destroy_process_group()` for cleanup. **Constraints:** - Assume that input matrices have the same shape. - Use only PyTorch\'s native distributed functions. - The function should be able to handle both GPU (with \'nccl\' backend) and CPU (with \'gloo\' backend) tensors. **Performance Requirements:** - The solution should efficiently utilize the distributed environment. - Proper synchronization and minimal communication overhead are expected. # Example Usage: ```python import torch.multiprocessing as mp def run(rank): matrix_list = [torch.randn(3, 3) for _ in range(2)] # Example matrices for each process result = distributed_matrix_sum(matrix_list) if rank == 0: print(f\\"Summed Matrix: {result}\\") if __name__ == \\"__main__\\": world_size = 4 # Number of processes backend = \'gloo\' # or \'nccl\' for GPU mp.spawn(run, args=(world_size, backend), nprocs=world_size, join=True) ``` **Note:** Ensure that your code is robust and handles different process ranks correctly. Thoroughly test the function on both CPU and GPU settings.","solution":"import torch import torch.distributed as dist def distributed_matrix_sum(matrix_list, backend=\'gloo\'): Sum matrices distributed across multiple processes. Args: - matrix_list: list of torch Tensor objects representing the matrices in the current process. - backend: the distributed backend to use (\'gloo\', \'mpi\', or \'nccl\'). Returns: - result: The resulting sum matrix on the root process (rank 0). None for other processes. # Initialize the process group dist.init_process_group(backend=backend) # Each process calculates the sum of its own matrices local_sum = sum(matrix_list) # Create a tensor to hold the global sum on each process global_sum = torch.zeros_like(local_sum) # Perform the all-reduce operation to sum globally across all processes dist.all_reduce(local_sum, op=dist.ReduceOp.SUM, group=dist.group.WORLD) global_sum = local_sum # Only process 0 will receive the final summed matrix if dist.get_rank() == 0: result = global_sum else: result = None # Clean up the distributed process group dist.destroy_process_group() return result"},{"question":"PyTorch Serialization and Deserialization Objective: Design and implement a class that saves and loads PyTorch model states efficiently while minimizing storage size and preserving tensor relationships. This assesses your understanding of PyTorch serialization, tensor views, and state_dict handling. Question: Your task is to implement a PyTorch-based utility class, `ModelSerializer`, with two primary methods: 1. **save_model**: Save the model\'s state_dict to a file but ensure that tensors with large storage objects are cloned to minimize file sizes. 2. **load_model**: Load the model\'s state_dict from a file and correctly restore the state to a given model. You should demonstrate the usage of these methods using a sample PyTorch model. # Class Structure: ```python import torch import os class ModelSerializer: @staticmethod def save_model(model, file_path): Saves the model\'s state_dict to `file_path` only after cloning tensors with large storages. Args: model (torch.nn.Module): The PyTorch model to be saved. file_path (str): Path to the file where the state_dict will be saved. pass @staticmethod def load_model(model, file_path): Loads a saved state_dict from `file_path` and restores it to the model. Args: model (torch.nn.Module): The model where the state_dict will be loaded. file_path (str): Path to the file from where the state_dict will be loaded. pass ``` # Example: Here’s an example of how you might use this class: ```python # Define a sample model class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(10, 1) def forward(self, x): return self.linear(x) # Initialize the model, serializer, and a sample tensor model = SimpleModel() serializer = ModelSerializer() # File path to store the state_dict save_path = \'simple_model_state.pt\' # Save the model state serializer.save_model(model, save_path) # Create a new instance of the model and load the state new_model = SimpleModel() serializer.load_model(new_model, save_path) ``` # Constraints: - Ensure that the saved file uses minimal storage by cloning tensors with excessively large storage objects before saving. - Maintain tensor storage sharing when appropriate to preserve relationships. - Assume that the environment is equipped with PyTorch installed. # Instructions: 1. Implement the `save_model` and `load_model` methods in the `ModelSerializer` class. 2. Demonstrate saving and loading with the `SimpleModel` example. 3. Ensure your implementation handles cloning when necessary and preserves tensor relationships. This question will test your ability to efficiently use PyTorch\'s serialization mechanisms and manage tensor states, ensuring smaller save file sizes and accurate state restoration.","solution":"import torch import os class ModelSerializer: @staticmethod def save_model(model, file_path): Saves the model\'s state_dict to `file_path` only after cloning tensors with large storages. Args: model (torch.nn.Module): The PyTorch model to be saved. file_path (str): Path to the file where the state_dict will be saved. state_dict = model.state_dict() # Clone tensors that have storages larger than their own size cloned_state_dict = {k: v.clone() if v.storage().size() > v.numel() else v for k, v in state_dict.items()} torch.save(cloned_state_dict, file_path) @staticmethod def load_model(model, file_path): Loads a saved state_dict from `file_path` and restores it to the model. Args: model (torch.nn.Module): The model where the state_dict will be loaded. file_path (str): Path to the file from where the state_dict will be loaded. state_dict = torch.load(file_path) model.load_state_dict(state_dict)"},{"question":"**Objective**: Demonstrate your understanding of sequence manipulation in Python. **Problem Statement**: You are to implement a function `manipulate_sequence` that takes a sequence of integers and performs a series of operations similar to those described in the documentation summary, but using native Python constructs and handling potential errors gracefully. # Function Signature ```python def manipulate_sequence(seq: list, operation: str, *args) -> list: ``` # Parameters: - `seq` (list): A list of integers. - `operation` (str): A string denoting the operation to be performed on the sequence. The operations are: - `\'concat\'`: Concatenate `seq` with another sequence provided in `args`. - `\'repeat\'`: Repeat `seq` a number of times specified in `args[0]`. - `\'getitem\'`: Retrieve an element at an index specified in `args[0]`. - `\'slice\'`: Retrieve a slice from `seq` between the start index `args[0]` and the end index `args[1]`. - `\'setitem\'`: Set the element at index `args[0]` to a value `args[1]`. - `\'delitem\'`: Delete the element at index `args[0]`. - `\'setslice\'`: Set a slice from `args[0]` to `args[1]` to a new list provided in `args[2]`. - `\'delslice\'`: Delete a slice from `args[0]` to `args[1]`. - `\'count\'`: Return the count of a value `args[0]` in `seq`. - `\'contains\'`: Check if a value `args[0]` is in `seq`. - `\'index\'`: Return the first index of a value `args[0]` in `seq`. - `\'tolist\'`: Convert the sequence to a list format. - `\'totuple\'`: Convert the sequence to a tuple format. # Returns: - A list representing the modified sequence (if applicable) or the result of the operation. # Constraints: - Use native Python constructs to simulate the described operations. - Raise appropriate exceptions if invalid operations or indices are provided. - Ensure that the sequence remains unchanged if an exception occurs. # Examples: ```python # Concatenation seq = [1, 2, 3] print(manipulate_sequence(seq, \'concat\', [4, 5])) # Output: [1, 2, 3, 4, 5] # Repetition seq = [1, 2, 3] print(manipulate_sequence(seq, \'repeat\', 2)) # Output: [1, 2, 3, 1, 2, 3] # Get item seq = [1, 2, 3] print(manipulate_sequence(seq, \'getitem\', 1)) # Output: 2 # Slice seq = [1, 2, 3, 4, 5] print(manipulate_sequence(seq, \'slice\', 1, 4)) # Output: [2, 3, 4] # Setting an item seq = [1, 2, 3] print(manipulate_sequence(seq, \'setitem\', 1, 10)) # Output: [1, 10, 3] # Deleting an item seq = [1, 2, 3] print(manipulate_sequence(seq, \'delitem\', 1)) # Output: [1, 3] # Setting a slice seq = [1, 2, 3, 4, 5] print(manipulate_sequence(seq, \'setslice\', 1, 4, [10, 11])) # Output: [1, 10, 11, 5] # Deleting a slice seq = [1, 2, 3, 4, 5] print(manipulate_sequence(seq, \'delslice\', 1, 4)) # Output: [1, 5] # Counting an item seq = [1, 2, 2, 3, 4] print(manipulate_sequence(seq, \'count\', 2)) # Output: 2 # Containment check seq = [1, 2, 3, 4, 5] print(manipulate_sequence(seq, \'contains\', 2)) # Output: True # Index of value seq = [1, 2, 3, 4, 2, 5] print(manipulate_sequence(seq, \'index\', 2)) # Output: 1 # Sequence to list seq = (1, 2, 3) print(manipulate_sequence(seq, \'tolist\')) # Output: [1, 2, 3] # Sequence to tuple seq = [1, 2, 3] print(manipulate_sequence(seq, \'totuple\')) # Output: (1, 2, 3) ``` # Note: Handle boundary cases and ensure the function behaves correctly for various forms of erroneous input.","solution":"def manipulate_sequence(seq, operation, *args): Manipulate the given sequence based on the specified operation and arguments. try: if operation == \'concat\': return seq + list(args[0]) elif operation == \'repeat\': return seq * args[0] elif operation == \'getitem\': return seq[args[0]] elif operation == \'slice\': return seq[args[0]:args[1]] elif operation == \'setitem\': seq[args[0]] = args[1] return seq elif operation == \'delitem\': del seq[args[0]] return seq elif operation == \'setslice\': seq[args[0]:args[1]] = args[2] return seq elif operation == \'delslice\': del seq[args[0]:args[1]] return seq elif operation == \'count\': return seq.count(args[0]) elif operation == \'contains\': return args[0] in seq elif operation == \'index\': return seq.index(args[0]) elif operation == \'tolist\': return list(seq) elif operation == \'totuple\': return tuple(seq) else: raise ValueError(\\"Invalid operation\\") except IndexError: raise IndexError(\\"Index out of range\\") except TypeError: raise TypeError(\\"Invalid type used in arguments\\")"},{"question":"**PyTorch Coding Assessment Question** **Objective:** Test your understanding of PyTorch\'s Core Aten Intermediate Representation (IR) by implementing a function that performs a tensor operation using Core Aten IR. # Problem Statement: You are provided with a custom tensor operation function from PyTorch\'s Core Aten IR. You need to implement a new function using this Core Aten IR that will: 1. Take two tensors as input. 2. Normalize the inputs to have zero mean and unit variance. 3. Compute the dot product of the normalized tensors. 4. Return the result as a scalar value. # Requirements: - Use `torch.ops.aten` to access Core Aten IR operators. - Perform normalization by subtracting the mean and dividing by the standard deviation of the tensor. # Function Signature: ```python import torch def custom_dot_product(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Args: - tensor_a (torch.Tensor): A 1-D tensor of floats. - tensor_b (torch.Tensor): A 1-D tensor of floats of the same shape as tensor_a. Returns: - torch.Tensor: A scalar tensor representing the dot product of the normalized input tensors. Constraints: - Both tensors are non-empty and have the same shape. - All elements in the tensors are non-negative floats. # Place your solution here pass # Example usage: # tensor_a = torch.tensor([1.0, 2.0, 3.0]) # tensor_b = torch.tensor([4.0, 5.0, 6.0]) # result = custom_dot_product(tensor_a, tensor_b) # print(result) # Should print a scalar value. ``` # Notes: - Input tensors will always be 1-dimensional and have the same length. - Handle any potential edge cases where the standard deviation might be zero. - Do not use any high-level PyTorch functions other than those from `torch.ops.aten`. # Constraints: - Time complexity should be linear with respect to the size of the input tensors. - Aim for clarity and efficiency in your implementation.","solution":"import torch def custom_dot_product(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Args: - tensor_a (torch.Tensor): A 1-D tensor of floats. - tensor_b (torch.Tensor): A 1-D tensor of floats of the same shape as tensor_a. Returns: - torch.Tensor: A scalar tensor representing the dot product of the normalized input tensors. Constraints: - Both tensors are non-empty and have the same shape. - All elements in the tensors are non-negative floats. # Calculate mean and standard deviation of tensor_a mean_a = torch.ops.aten.mean(tensor_a, tuple(range(tensor_a.ndim)), keepdim=False) std_a = torch.ops.aten.std(tensor_a, unbiased=False) # Calculate mean and standard deviation of tensor_b mean_b = torch.ops.aten.mean(tensor_b, tuple(range(tensor_b.ndim)), keepdim=False) std_b = torch.ops.aten.std(tensor_b, unbiased=False) # Normalize the tensors normalized_a = torch.ops.aten.div(torch.ops.aten.sub(tensor_a, mean_a), std_a) normalized_b = torch.ops.aten.div(torch.ops.aten.sub(tensor_b, mean_b), std_b) # Calculate the dot product of the normalized tensors dot_product_normalized = torch.ops.aten.dot(normalized_a, normalized_b) return dot_product_normalized # Example usage: # tensor_a = torch.tensor([1.0, 2.0, 3.0]) # tensor_b = torch.tensor([4.0, 5.0, 6.0]) # result = custom_dot_product(tensor_a, tensor_b) # print(result) # Should print a scalar value."},{"question":"In this assessment, you are required to demonstrate your understanding of the `email.header` module in Python by implementing a function that processes email headers. # Problem Statement Write a function `process_email_header(headers: dict) -> dict` that takes a dictionary `headers` with email header fields and their corresponding values, and returns a dictionary with processed values suitable for including international (non-ASCII) characters. The processing should use the functionality provided by the `email.header` module to ensure the headers comply with MIME standards. # Input Format - `headers`: A dictionary where keys are header names (strings, e.g., \\"Subject\\", \\"To\\") and values are the respective header values (strings). # Output Format - A dictionary with the same header names but values should be instances of `email.header.Header` that properly encode any non-ASCII characters. # Constraints - You should handle at least these common headers: \\"Subject\\", \\"To\\", \\"From\\". - Assume the values provided in `headers` may contain non-ASCII characters and should be properly encoded using the specified charset. # Example ```python from email.message import Message from email.header import Header input_headers = { \\"Subject\\": \\"Hello Wörld\\", \\"To\\": \\"example@example.com\\", \\"From\\": \\"user@domain.com\\", } # Your function def process_email_header(headers): # Implement the function # Assuming you call the function processed_headers = process_email_header(input_headers) # Convert to Message for demonstration msg = Message() for k, v in processed_headers.items(): msg[k] = v print(msg.as_string()) ``` Expected Output: ``` Subject: =?utf-8?q?Hello_W=C3=B6rld?= To: example@example.com From: user@domain.com ``` # Implementation Details 1. Use the `Header` class from the `email.header` module to handle each header. 2. Ensure non-ASCII characters are properly encoded using an appropriate charset, like UTF-8. 3. Do not change ASCII-only headers. Good Luck!","solution":"from email.header import Header def process_email_header(headers: dict) -> dict: processed_headers = {} for key, value in headers.items(): processed_headers[key] = Header(value, charset=\'utf-8\') return processed_headers"},{"question":"# Locale Formatter You are given a Python program that handles monetary transactions and needs to display amounts in different locale formats. Your task is to write a function `format_transaction_amounts` that accepts a list of transaction amounts and a locale string, then returns the amounts formatted as per the specified locale\'s monetary conventions. Implement the function `format_transaction_amounts(amounts: List[float], loc: str) -> List[str]` which formats the given list of transaction amounts according to the provided locale. **Function Signature:** ```python from typing import List def format_transaction_amounts(amounts: List[float], loc: str) -> List[str]: ``` # Input: - `amounts`: A list of float values representing transaction amounts. - `loc`: A string representing the locale, e.g., \'en_US.UTF-8\' for United States or \'de_DE.UTF-8\' for Germany. # Output: - A list of strings where each string is a formatted version of the corresponding amount in the input list according to the locale\'s conventions. # Exceptions: - If the specified locale is not recognized, your function should raise a `locale.Error`. # Example: ```python from typing import List import locale def format_transaction_amounts(amounts: List[float], loc: str) -> List[str]: try: locale.setlocale(locale.LC_ALL, loc) except locale.Error: raise locale.Error(f\\"Locale {loc} not recognized\\") formatted_amounts = [] for amt in amounts: formatted_amounts.append(locale.currency(amt, grouping=True)) # Reset to default locale after processing locale.setlocale(locale.LC_ALL, \'\') return formatted_amounts # Example Usage: amounts = [1234.56, 7890.12, 345.67] locale_code = \'en_US.UTF-8\' print(format_transaction_amounts(amounts, locale_code)) # Output should be similar to [\'1,234.56\', \'7,890.12\', \'345.67\'] locale_code = \'de_DE.UTF-8\' print(format_transaction_amounts(amounts, locale_code)) # Output should be similar to [\'1.234,56 €\', \'7.890,12 €\', \'345,67 €\'] ``` # Constraints: - Focus on locales that are widely supported and have clear monetary formatting rules (e.g., en_US, de_DE). - Ensure proper exception handling for unrecognized locales. - Reset the locale setting to the default user locale after processing to avoid side effects on other parts of the program. # Notes: - The function should handle list iteration and formatting robustly. - Utilize the `locale.currency()` function to format the amounts correctly. - Remember to handle locales that might not support certain currency symbols directly.","solution":"from typing import List import locale def format_transaction_amounts(amounts: List[float], loc: str) -> List[str]: try: locale.setlocale(locale.LC_ALL, loc) except locale.Error: raise locale.Error(f\\"Locale {loc} not recognized\\") formatted_amounts = [] for amt in amounts: formatted_amounts.append(locale.currency(amt, grouping=True)) # Reset to default locale after processing locale.setlocale(locale.LC_ALL, \'\') return formatted_amounts"},{"question":"Objective: Demonstrate understanding of the hashlib module\'s hashing methods, including creating and using hash objects, handling bytes-like data, and utilizing advanced features such as keyed hashing and salted hashing. Problem Statement: You are tasked with creating a secure file verification system using the hashlib module. Your solution will involve the following: 1. **Calculate File Hash**: - Write a function `calculate_file_hash(filepath, hash_type)` that takes a file path and a hash type (e.g., \'sha256\', \'blake2b\', etc.), reads the file in chunks, updates the hash object, and returns the hexadecimal digest of the file content. - **Input**: - `filepath` (str): Path to the file to be hashed. - `hash_type` (str): Type of the hash algorithm (e.g., \'sha256\', \'blake2b\'). - **Output**: - The hexadecimal digest (str) of the hashed file content. 2. **Verify File Hash**: - Write a function `verify_file_hash(filepath, expected_hash, hash_type)` that verifies if the hash of the file content matches the expected hash provided as input. - **Input**: - `filepath` (str): Path to the file to verify. - `expected_hash` (str): The expected hexadecimal hash value. - `hash_type` (str): Type of the hash algorithm. - **Output**: - Boolean value: True if the hashes match, otherwise False. 3. **Secure File Signing with Keyed Hashing**: - Write a function `sign_file(filepath, key, hash_type)` that takes a file path, a key (for keyed hashing), and a hash type, then returns the keyed-hash (HMAC equivalent) of the file. - **Input**: - `filepath` (str): Path to the file to be signed. - `key` (bytes): Byte string key to use for the keyed hash. - `hash_type` (str): Type of the hash algorithm. - **Output**: - The hexadecimal digest (str) of the signed file content. 4. **Example Use Case**: - Demonstrate the use of the above functions to: - Compute the hash of a file. - Verify the file hash. - Sign the file with a keyed hash. - Verify the signed file against the generated signature. Constraints: 1. Assume the files are not larger than 1 GB. 2. The key used for keyed hashing should be a bytes object up to 64 bytes. 3. Implement proper error handling for IO operations and unsupported hash types. Example: ```python # Example File Path file_path = \'example.txt\' hash_type = \'sha256\' key = b\'secure_key_example\' # Calculate Hash file_hash = calculate_file_hash(file_path, hash_type) print(\\"File Hash:\\", file_hash) # Verify Hash is_valid = verify_file_hash(file_path, file_hash, hash_type) print(\\"Hash Verification:\\", is_valid) # Sign File with Keyed Hash file_signature = sign_file(file_path, key, \'blake2b\') print(\\"File Signature:\\", file_signature) # Verify Signed File is_valid_signature = verify_file_hash(file_path, file_signature, \'blake2b\') print(\\"Signature Verification:\\", is_valid_signature) ``` Implement the following functions as specified: ```python import hashlib def calculate_file_hash(filepath, hash_type): # Implement this function to calculate and return the file hash pass def verify_file_hash(filepath, expected_hash, hash_type): # Implement this function to verify if the file hash matches the expected hash pass def sign_file(filepath, key, hash_type): # Implement this function to return the keyed hash of the file content pass ```","solution":"import hashlib def calculate_file_hash(filepath, hash_type): Calculate the hash of a file content. :param filepath: Path to the file to be hashed. :param hash_type: Type of the hash algorithm (e.g., \'sha256\', \'blake2b\'). :return: The hexadecimal digest of the hashed file content. try: hasher = hashlib.new(hash_type) with open(filepath, \'rb\') as f: # Read the file in chunks to handle large files for chunk in iter(lambda: f.read(4096), b\'\'): hasher.update(chunk) return hasher.hexdigest() except (IOError, ValueError) as e: print(f\\"Error: {e}\\") return None def verify_file_hash(filepath, expected_hash, hash_type): Verify if the hash of the file content matches the expected hash. :param filepath: Path to the file to verify. :param expected_hash: The expected hexadecimal hash value. :param hash_type: Type of the hash algorithm. :return: True if the hashes match, False otherwise. calculated_hash = calculate_file_hash(filepath, hash_type) return calculated_hash == expected_hash def sign_file(filepath, key, hash_type): Generate a keyed hash (HMAC equivalent) of the file content. :param filepath: Path to the file to be signed. :param key: Byte string key to use for the keyed hash. :param hash_type: Type of the hash algorithm. :return: The hexadecimal digest of the signed file content. try: hasher = hashlib.blake2b(key=key) if hash_type == \'blake2b\' else hashlib.new(hash_type) with open(filepath, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): hasher.update(chunk) return hasher.hexdigest() except (IOError, ValueError) as e: print(f\\"Error: {e}\\") return None"},{"question":"# Problem Description You are working on processing logs generated by a web server. Each log entry contains an IP address, a timestamp enclosed in square brackets, the request method (e.g., GET, POST), the requested path, the status code, and the size of the response in bytes (or a dash if unknown). Here are some sample log entries for reference: ```plaintext 123.45.67.89 - - [10/Jul/2023:22:14:56 +0000] \\"GET /index.html HTTP/1.1\\" 200 1024 98.76.54.32 - - [11/Jul/2023:12:45:21 +0000] \\"POST /submit-form HTTP/2.0\\" 404 - ``` # Task Write a Python function named `parse_log_entries` that processes these log entries using regular expressions. The function should: 1. Take a single string `log` as input, containing multiple lines, each line representing a single log entry. 2. Extract the following fields from each log entry: - IP address - Timestamp - Request method - Requested path - Status code - Response size 3. Return a list of dictionaries where each dictionary corresponds to one log entry with keys `\'ip\'`, `\'timestamp\'`, `\'method\'`, `\'path\'`, `\'status\'`, and `\'response_size\'`. # Input - A single string `log` containing multiple log entries separated by newline characters. # Output - A list of dictionaries. Each dictionary should contain the extracted fields for one log entry. # Example ```python log = \'\'\' 123.45.67.89 - - [10/Jul/2023:22:14:56 +0000] \\"GET /index.html HTTP/1.1\\" 200 1024 98.76.54.32 - - [11/Jul/2023:12:45:21 +0000] \\"POST /submit-form HTTP/2.0\\" 404 - \'\'\' expected_output = [ { \'ip\': \'123.45.67.89\', \'timestamp\': \'10/Jul/2023:22:14:56 +0000\', \'method\': \'GET\', \'path\': \'/index.html\', \'status\': \'200\', \'response_size\': \'1024\' }, { \'ip\': \'98.76.54.32\', \'timestamp\': \'11/Jul/2023:12:45:21 +0000\', \'method\': \'POST\', \'path\': \'/submit-form\', \'status\': \'404\', \'response_size\': \'-\' } ] ``` # Implementation Requirements - The regular expressions should efficiently capture the relevant parts of each log entry. - Ensure the function handles edge cases, such as missing response sizes represented by a dash (`-`). # Constraints 1. You should use the `re` module for implementing the solution. 2. The input log entries are well-formed and follow the pattern provided. # Additional Notes - Consider using non-capturing groups and named groups to handle and extract parts of the log entries effectively. - Think about cases where the log entry data might be inconsistent or partially missing, e.g., response size being a dash (`-`).","solution":"import re def parse_log_entries(log): Parse the log entries and extract fields. Args: log (str): A string containing multiple log entries. Returns: list of dict: A list of dictionaries where each dictionary contains the fields \'ip\', \'timestamp\', \'method\', \'path\', \'status\', \'response_size\'. log_pattern = re.compile( r\'(?P<ip>d+.d+.d+.d+) - - [(?P<timestamp>.*?)] \\"(?P<method>w+) (?P<path>.*?) HTTP/.*?\\" (?P<status>d+) (?P<response_size>d+|-)\' ) matches = log_pattern.finditer(log) result = [] for match in matches: entry = { \'ip\': match.group(\'ip\'), \'timestamp\': match.group(\'timestamp\'), \'method\': match.group(\'method\'), \'path\': match.group(\'path\'), \'status\': match.group(\'status\'), \'response_size\': match.group(\'response_size\') } result.append(entry) return result"},{"question":"Isotonic Regression Model Implementation # Objective Implement a function using the `IsotonicRegression` class from the `sklearn.isotonic` module to fit a model to given training data, predict new data points, and evaluate the performance of the model using mean squared error (MSE) on a test dataset. # Requirements Your implementation should include: 1. A function `fit_predict_isotonic(X_train, y_train, X_test, y_test, increasing=\'auto\')` that: - Fits an `IsotonicRegression` model to the training data `(X_train, y_train)`. - Predicts the target values for both the training set and the test set. - Calculates the mean squared error (MSE) for both the training set predictions and the test set predictions. 2. Expected input and output formats: - `X_train`: List of float values representing the training data features. - `y_train`: List of float values representing the training data targets. - `X_test`: List of float values representing the test data features. - `y_test`: List of float values representing the test data targets. - `increasing`: Parameter that determines the direction of the monotonicity constraint (\'auto\', True for non-decreasing, False for non-increasing). 3. The function should return a tuple `(mse_train, mse_test)`: - `mse_train`: Mean squared error for the predictions on the training data. - `mse_test`: Mean squared error for the predictions on the test data. # Constraints - `X_train`, `y_train`, `X_test`, and `y_test` are non-empty lists of real numbers with the same length for `X_train` and `y_train`, and `X_test` and `y_test`, respectively. - The length of each list should be at least 2. # Example ```python from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_predict_isotonic(X_train, y_train, X_test, y_test, increasing=\'auto\'): # Initialize the isotonic regression model with the specified increasing parameter model = IsotonicRegression(increasing=increasing) # Fit the model to the training data model.fit(X_train, y_train) # Predict the training and test data using the fitted model y_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test) # Calculate the mean squared error for training and test data mse_train = mean_squared_error(y_train, y_train_pred) mse_test = mean_squared_error(y_test, y_test_pred) return mse_train, mse_test # Example usage: X_train = [1, 2, 3, 4, 5] y_train = [2, 2, 3, 5, 5] X_test = [1.5, 2.5, 3.5] y_test = [2, 3, 4] mse_train, mse_test = fit_predict_isotonic(X_train, y_train, X_test, y_test) print(f\'MSE (Train): {mse_train}, MSE (Test): {mse_test}\') ``` In the example above: - `X_train` and `y_train` represent the training data. - `X_test` and `y_test` represent the test data. - The function fits an isotonic regression model to the training data, makes predictions, and calculates the mean squared error for both the training data and test data predictions. # Notes - You must use the `IsotonicRegression` class from the `sklearn.isotonic` module. - You may use `mean_squared_error` from `sklearn.metrics` to calculate the MSE.","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_predict_isotonic(X_train, y_train, X_test, y_test, increasing=\'auto\'): Fits an IsotonicRegression model to the training data and predicts on both training and test data. Calculates and returns the mean squared error for both datasets. Parameters: - X_train: List[float] - Training data features. - y_train: List[float] - Training data targets. - X_test: List[float] - Test data features. - y_test: List[float] - Test data targets. - increasing: \'auto\' or bool - Direction of monotonicity constraint (\'auto\', True, False). Returns: - mse_train: float - Mean squared error for training data. - mse_test: float - Mean squared error for test data. # Initialize the isotonic regression model with the specified increasing parameter model = IsotonicRegression(increasing=increasing) # Fit the model to the training data model.fit(X_train, y_train) # Predict the training and test data using the fitted model y_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test) # Calculate the mean squared error for training and test data mse_train = mean_squared_error(y_train, y_train_pred) mse_test = mean_squared_error(y_test, y_test_pred) return mse_train, mse_test"},{"question":"Advanced Data Visualization with Pandas Objective Demonstrate your ability to use pandas and matplotlib for creating complex visualizations with real-world datasets. Problem Statement You are given a dataset representing a small e-commerce company\'s sales data over a period of one year. The dataset is provided in a CSV file with the following columns: - `Date`: The date of the transaction (format: YYYY-MM-DD). - `CustomerID`: Unique identifier for each customer. - `ProductID`: Unique identifier for each product. - `Quantity`: Number of units sold in the transaction. - `Price`: Price per unit of the product. - `Revenue`: Total revenue for the transaction (Quantity * Price). Download the dataset: [sales_data.csv](LINK_TO_CSV_FILE) You are required to implement the following function: ```python import pandas as pd import matplotlib.pyplot as plt def analyze_and_plot_sales(file_path: str) -> None: Analyzes the sales data from the given CSV file and generates the following plots: 1. A time series plot showing the daily total revenue. 2. A box plot showing the distribution of transaction revenues. 3. A scatter plot showing the relationship between `Quantity` and `Revenue`. 4. An Andrews curves plot to visualize the clustering of customers based on their purchase history. The plots should be saved in the current working directory with the following filenames: - `total_revenue_time_series.png` - `transaction_revenue_boxplot.png` - `quantity_revenue_scatter.png` - `customer_andrews_curves.png` Parameters: file_path (str): The path to the CSV file. # Your code goes here # Example usage analyze_and_plot_sales(\'sales_data.csv\') ``` Input - `file_path` (str): The path to the sales data CSV file. Output - Four PNG files saved in the current working directory: - `total_revenue_time_series.png` - `transaction_revenue_boxplot.png` - `quantity_revenue_scatter.png` - `customer_andrews_curves.png` Constraints - Use the `pandas` and `matplotlib` libraries for data manipulation and plotting. - Ensure that the plots are clear and appropriately labeled. - Handle missing or malformed data gracefully. Hints 1. For the time series plot, consider setting the `Date` column as the index and using the `resample` method. 2. For the Andrews curves plot, you might want to aggregate customer purchase data. 3. Check the `pandas.plotting` module for functions like `andrews_curves` and others. By solving this problem, you will demonstrate your ability to manipulate data using pandas and create complex visualizations that can help drive business insights.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves def analyze_and_plot_sales(file_path: str) -> None: # Load the data df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Set Date as the index df.set_index(\'Date\', inplace=True) # Handle missing and malformed data (dropping rows with NaN values) df.dropna(inplace=True) # Time series plot showing daily total revenue daily_revenue = df.resample(\'D\')[\'Revenue\'].sum() plt.figure(figsize=(12, 6)) daily_revenue.plot() plt.title(\'Daily Total Revenue\') plt.xlabel(\'Date\') plt.ylabel(\'Revenue\') plt.savefig(\'total_revenue_time_series.png\') plt.close() # Box plot showing the distribution of transaction revenues plt.figure(figsize=(10, 6)) df[\'Revenue\'].plot.box() plt.title(\'Distribution of Transaction Revenues\') plt.ylabel(\'Revenue\') plt.savefig(\'transaction_revenue_boxplot.png\') plt.close() # Scatter plot showing the relationship between Quantity and Revenue plt.figure(figsize=(10, 6)) plt.scatter(df[\'Quantity\'], df[\'Revenue\']) plt.title(\'Quantity vs Revenue\') plt.xlabel(\'Quantity\') plt.ylabel(\'Revenue\') plt.savefig(\'quantity_revenue_scatter.png\') plt.close() # Aggregate purchase data by CustomerID for Andrews curves plot customer_data = df.groupby(\'CustomerID\').agg({ \'Quantity\': \'sum\', \'Revenue\': \'sum\' }).reset_index() plt.figure(figsize=(12, 6)) andrews_curves(customer_data, \'CustomerID\') plt.title(\'Andrews Curves of Customers based on Purchase History\') plt.savefig(\'customer_andrews_curves.png\') plt.close()"},{"question":"# Question: Working with Specialized Containers from the `collections` Module You are working on a project that requires the manipulation and analysis of a large set of data. You decide to utilize the specialized containers from the `collections` module to achieve this. You need to implement a function `analyze_products` that takes in a list of product entries and performs the following tasks: 1. **Store Product Data**: - Each product entry is represented as a tuple containing `product_id`, `category`, `subcategory`, `price`. - Group these product entries by category and subcategory using `defaultdict`. 2. **Calculate Statistics**: - For each category, calculate the total number of products and the average price using `Counter`. - Additionally, determine the most common subcategories within each category using `Counter`. 3. **Combine Data**: - Combine the statistics from different categories into a single `ChainMap`. 4. **Order Results**: - Return the combined results in an `OrderedDict` sorted by category and within each category by subcategory. Here\'s the function signature: ```python from collections import defaultdict, Counter, ChainMap, OrderedDict def analyze_products(product_entries): Analyzes a list of product entries to generate statistics by category and subcategory. Parameters: product_entries (list of tuples): A list where each tuple contains the following: - product_id (int): Unique identifier of the product - category (str): Category of the product - subcategory (str): Subcategory of the product - price (float): Price of the product Returns: OrderedDict: An ordered dictionary containing combined statistics by category and subcategory. pass # Example usage product_entries = [ (101, \'Electronics\', \'Smartphone\', 399.99), (102, \'Electronics\', \'Laptop\', 899.99), (103, \'Home\', \'Furniture\', 199.99), (104, \'Home\', \'Smartphone\', 299.99), (105, \'Electronics\', \'Smartphone\', 499.99), (106, \'Electronics\', \'Laptop\', 1099.99), (107, \'Home\', \'Furniture\', 249.99), ] result = analyze_products(product_entries) print(result) ``` # Expected Output: The function should return an `OrderedDict` that contains the combined statistics by category and subcategory. For example, the output for the provided `product_entries` could look like this: ```python OrderedDict([ (\'Electronics\', { \'total_products\': 4, \'average_price\': 699.99, \'most_common_subcategories\': {\'Smartphone\': 2, \'Laptop\': 2} }), (\'Home\', { \'total_products\': 3, \'average_price\': 249.99, \'most_common_subcategories\': {\'Furniture\': 2, \'Smartphone\': 1} }) ]) ``` # Constraints: - Each product has a unique `product_id`. - You can assume that `category` and `subcategory` are strings and `price` is a non-negative float. - Your solution should be efficient in terms of time complexity, given that the number of product entries could be large. # Notes: - Make use of the `collections` module to handle grouping, counting, and ordering efficiently. - Ensure your solution is clean and follows Pythonic conventions.","solution":"from collections import defaultdict, Counter, ChainMap, OrderedDict def analyze_products(product_entries): Analyzes a list of product entries to generate statistics by category and subcategory. Parameters: product_entries (list of tuples): A list where each tuple contains the following: - product_id (int): Unique identifier of the product - category (str): Category of the product - subcategory (str): Subcategory of the product - price (float): Price of the product Returns: OrderedDict: An ordered dictionary containing combined statistics by category and subcategory. categories = defaultdict(list) # Group by category and subcategory for product_id, category, subcategory, price in product_entries: categories[category].append((subcategory, price)) statistics = {} for category, items in categories.items(): total_products = len(items) total_price = sum(price for subcategory, price in items) average_price = total_price / total_products subcategory_counter = Counter(subcategory for subcategory, price in items) statistics[category] = { \'total_products\': total_products, \'average_price\': average_price, \'most_common_subcategories\': dict(subcategory_counter) } # Create ChainMap of statistics combined_statistics = ChainMap(statistics) # Convert to OrderedDict sorted by category sorted_combined_statistics = OrderedDict(sorted(combined_statistics.items())) return sorted_combined_statistics"},{"question":"**Context Manager Exercise with Python `contextvars`** The `contextvars` module in Python allows you to manage values local to a context and can be quite powerful for certain types of state management in asynchronous tasks. In this exercise, you are required to implement a class that uses `contextvars` to manage and manipulate user states within different contexts. # Task Implement a class `UserStateManager` that provides methods to start, update, and stop user state sessions using context variables. Specifically, you need to implement the following methods: 1. `__init__(self)`: Initialize the class. This should create a `ContextVar` named `\\"user_state\\"` that starts with an empty dictionary as its default value. 2. `start_session(self, user_id)`: Start a new session for a user. This method should set the initial state for a user (represented as a dictionary). 3. `update_session(self, user_id, key, value)`: Update the session state for a given user. This method should update the user\'s state dictionary with the provided key-value pair. 4. `stop_session(self, user_id)`: Stop the session for a user. This method should clear the state for the user. 5. `get_user_state(self, user_id)`: Get the current state for the user. # Specifications - **Input**: * `user_id`: An integer representing a user\'s ID. * `key`: A string representing the attribute of the user\'s state. * `value`: Any data type representing the value to be set for the given key. - **Output**: * `start_session`: None * `update_session`: None * `stop_session`: None * `get_user_state`: Dictionary representing the current state of the user. An empty dictionary if no state is found. # Example ```python manager = UserStateManager() manager.start_session(1) manager.update_session(1, \'name\', \'Alice\') manager.update_session(1, \'age\', 30) print(manager.get_user_state(1)) # Output: {\'name\': \'Alice\', \'age\': 30} manager.stop_session(1) print(manager.get_user_state(1)) # Output: {} ``` # Constraints 1. Use `contextvars` to manage the user state instead of class variables. 2. Ensure that operations on user states are performed within the appropriate context. # Code Template ```python import contextvars class UserStateManager: def __init__(self): self.user_state = contextvars.ContextVar(\'user_state\', default={}) def start_session(self, user_id): # Implement this method pass def update_session(self, user_id, key, value): # Implement this method pass def stop_session(self, user_id): # Implement this method pass def get_user_state(self, user_id): # Implement this method pass # Example usage: # manager = UserStateManager() # manager.start_session(1) # manager.update_session(1, \'name\', \'Alice\') # manager.update_session(1, \'age\', 30) # print(manager.get_user_state(1)) # Output: {\'name\': \'Alice\', \'age\': 30} # manager.stop_session(1) # print(manager.get_user_state(1)) # Output: {} ``` Ensure you test your implementation thoroughly with various scenarios to cover all edge cases.","solution":"import contextvars class UserStateManager: def __init__(self): self.user_state = contextvars.ContextVar(\'user_state\', default={}) def start_session(self, user_id): state = self.user_state.get().copy() state[user_id] = {} self.user_state.set(state) def update_session(self, user_id, key, value): state = self.user_state.get().copy() if user_id in state: state[user_id][key] = value self.user_state.set(state) def stop_session(self, user_id): state = self.user_state.get().copy() if user_id in state: del state[user_id] self.user_state.set(state) def get_user_state(self, user_id): state = self.user_state.get() return state.get(user_id, {})"},{"question":"Coding Assessment Question: Implement a Stacking Ensemble # Objective You are required to implement a stacking ensemble model using scikit-learn. The stacking ensemble should combine predictions from multiple base estimators to make a final prediction. # Description - Implement a function `stacking_ensemble` that takes as input a list of tuples `(estimator, X_train, y_train, X_test)` where `estimator` is an initialized scikit-learn estimator, `X_train`, and `y_train` are training features and labels, respectively, and `X_test` is the test set for which predictions are to be made. - Combine the predictions from each base estimator to form a metafeature set, which will be used to train the meta-model. - Use a specified meta-model to make the final predictions. # Inputs - `base_estimators`: A list of tuples of the form `(estimator, X_train, y_train, X_test)`. - `estimator`: An initialized scikit-learn estimator. - `X_train`: Training data features as a 2D numpy array. - `y_train`: Training data labels as a 1D numpy array. - `X_test`: Test data features as a 2D numpy array. - `meta_model`: An initialized scikit-learn estimator that will be used as the meta-model for combining predictions from the base estimators. # Outputs - A 1D numpy array containing the final predictions for `X_test`. # Constraints - You may assume that the input data is clean and properly preprocessed. - The base estimators and the meta-model are initialized and compatible with scikit-learn\'s standard API. - Performance is a consideration; ensure that your implementation is efficient. # Example Usage ```python from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC import numpy as np # Example data X_train_1 = np.array([[0, 0], [1, 1], [2, 2]]) y_train_1 = np.array([0, 1, 2]) X_test_1 = np.array([[1.5, 1.5]]) X_train_2 = np.array([[0, 1], [1, 2], [2, 3]]) y_train_2 = np.array([0, 1, 2]) X_test_2 = np.array([[1.5, 2.0]]) # Initialize base estimators base_estimators = [ (LogisticRegression(), X_train_1, y_train_1, X_test_1), (DecisionTreeClassifier(), X_train_2, y_train_2, X_test_2) ] # Initialize meta-model meta_model = SVC(probability=True) # Call the function final_predictions = stacking_ensemble(base_estimators, meta_model) print(final_predictions) ``` # Your Task Implement the `stacking_ensemble` function as described above.","solution":"import numpy as np from sklearn.base import clone def stacking_ensemble(base_estimators, meta_model): Combines predictions from multiple base estimators using a meta-model. Parameters: - base_estimators: List of tuples, each containing: - estimator: An initialized scikit-learn estimator. - X_train: Training data features as a 2D numpy array. - y_train: Training data labels as a 1D numpy array. - X_test: Test data features as a 2D numpy array. - meta_model: An initialized scikit-learn estimator that will be used as the meta-model. Returns: - A 1D numpy array containing the final predictions for X_test. # List to store predictions of base estimators for the training data meta_features_train = [] meta_features_test = [] # Create predictions for each base estimator for estimator, X_train, y_train, X_test in base_estimators: # Clone to avoid fitting the same estimator multiple times model = clone(estimator) model.fit(X_train, y_train) # Predict on training and test data train_pred = model.predict(X_train) test_pred = model.predict(X_test) # Store these predictions as metafeatures meta_features_train.append(train_pred) meta_features_test.append(test_pred) # Transpose and convert to numpy arrays meta_features_train = np.array(meta_features_train).T meta_features_test = np.array(meta_features_test).T # Train meta-model meta_model.fit(meta_features_train, y_train) # Final prediction with meta-model final_predictions = meta_model.predict(meta_features_test) return final_predictions"},{"question":"**Question:** Implement a Python function to handle and inspect netrc files, using the `netrc` class. The function should: 1. Instantiate a netrc object either from the default `.netrc` file or a specified file. 2. Validate the presence of login credentials for a list of hostnames. 3. Return a dictionary where keys are the hostnames, and the values are the tuples of `(login, account, password)`. Function Signature: ```python from typing import List, Dict, Tuple def get_netrc_credentials(file_path: str = None, hosts: List[str] = []) -> Dict[str, Tuple[str, str, str]]: pass ``` **Input:** - `file_path`: A string specifying the path to the netrc file (default is None, which will use the `.netrc` file in the user\'s home directory). - `hosts`: A list of strings, where each string is a hostname whose credentials need to be verified. **Output:** - A dictionary where keys are hostnames from the input list, and values are tuples of `(login, account, password)`. If a hostname from the list is not found in the netrc file, it should not appear in the output dictionary. **Constraints:** - Assume each password in the netrc file is a valid ASCII character subset without whitespaces or non-printable characters. - Handle both cases where the netrc file is either specified or default to `.netrc` in the user\'s home directory. - Handle the `FileNotFoundError` exception if the netrc file does not exist. - Handle `netrc.NetrcParseError` for syntactical errors in the netrc file and return an empty dictionary in such a case. Example: ```python # Assuming the content of .netrc is: # machine host1 # login user1 # password pass1 # machine host2 # login user2 # password pass2 print(get_netrc_credentials(hosts=[\'host1\', \'host3\'])) # Expected Output: {\'host1\': (\'user1\', None, \'pass1\')} ``` Additional Notes: - If none of the specified hosts are found in the netrc file, return an empty dictionary. - This task tests the student\'s understanding of file handling, exception handling, and usage of the `netrc` module in Python.","solution":"import os import netrc from typing import List, Dict, Tuple def get_netrc_credentials(file_path: str = None, hosts: List[str] = []) -> Dict[str, Tuple[str, str, str]]: Retrieves login credentials for specified hosts from a netrc file. :param file_path: Optional path to the netrc file. Defaults to None, which uses the .netrc file from the home directory. :param hosts: List of hostnames whose credentials need to be retrieved. :return: Dictionary with hostnames as keys and tuples of (login, account, password) as values. try: netrc_file = file_path or os.path.expanduser(\\"~/.netrc\\") netrc_data = netrc.netrc(netrc_file) credentials = {} for host in hosts: if host in netrc_data.hosts: login, account, password = netrc_data.authenticators(host) credentials[host] = (login, account, password) return credentials except FileNotFoundError: print(f\\"Netrc file not found at specified location: {file_path}\\") return {} except netrc.NetrcParseError as e: print(f\\"Error parsing netrc file: {e}\\") return {} # Example usage # print(get_netrc_credentials(hosts=[\'host1\', \'host3\']))"},{"question":"**Problem Statement: Implementing and Evaluating Stochastic Gradient Descent** # Objective You are tasked with building and evaluating machine learning models using Stochastic Gradient Descent (SGD) for both classification and regression. You will use the `SGDClassifier` and `SGDRegressor` classes from the scikit-learn package. The problem involves pre-processing the data, training the models, and evaluating their performance. # Dataset A toy dataset will be given for both classification and regression tasks. # Classification Dataset: The classification dataset consists of features and binary labels. - `X_class`: A 2D numpy array of shape `(n_samples, n_features)` containing the feature set. - `y_class`: A 1D numpy array of shape `(n_samples,)` containing binary labels (0 or 1). Task: 1. Perform feature scaling on `X_class`. 2. Train an `SGDClassifier` using the scaled feature set. - Use `loss=\'hinge\'` for a linear SVM. - Use `penalty=\'l2\'`. - Set `max_iter=1000` and `tol=1e-3`. 3. Evaluate the model\'s accuracy on the training set. 4. Output the model\'s coefficients and intercept. # Regression Dataset: The regression dataset consists of features and continuous target values. - `X_reg`: A 2D numpy array of shape `(n_samples, n_features)` containing the feature set. - `y_reg`: A 1D numpy array of shape `(n_samples,)` containing continuous target values. Task: 1. Perform feature scaling on `X_reg`. 2. Train an `SGDRegressor` using the scaled feature set. - Use `loss=\'squared_error\'` for ordinary least squares. - Use `penalty=\'l2\'`. - Set `max_iter=1000` and `tol=1e-3`. 3. Evaluate the model\'s mean squared error on the training set. 4. Output the model\'s coefficients and intercept. # Input Format You will receive four numpy arrays: - `X_class`: Features for classification. - `y_class`: Labels for classification. - `X_reg`: Features for regression. - `y_reg`: Target values for regression. # Output Format Your program should print the following: 1. Accuracy of the classification model on the training set. 2. Model coefficients and intercept for the classification model. 3. Mean squared error of the regression model on the training set. 4. Model coefficients and intercept for the regression model. # Example Assume the following inputs (this is just an illustration, actual inputs will vary): ```python X_class = np.array([[0.0, 0.0], [1.0, 1.0]]) y_class = np.array([0, 1]) X_reg = np.array([[0.0, 0.0], [1.0, 1.0]]) y_reg = np.array([0.0, 1.0]) ``` Expected output: ```plaintext Classification Accuracy: 1.0 Classification Coefficients: [[...]] Classification Intercept: [...] Regression Mean Squared Error: 0.0 Regression Coefficients: [[...]] Regression Intercept: [...] ``` # Constraints - Assume the datasets will have at least 2 samples and at most 10,000 samples. - Features will be floating point numbers. - Labels and target values will be integers or floating point numbers respectively. # Notes - Ensure you use appropriate data preprocessing methods such as scaling. - Utilize scikit-learn\'s `StandardScaler` for feature scaling. - Ensure reproducibility by setting `random_state` where applicable. # Solution Template ```python import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, mean_squared_error def sgd_class_and_reg(X_class, y_class, X_reg, y_reg): # Feature scaling scaler_class = StandardScaler() X_class_scaled = scaler_class.fit_transform(X_class) scaler_reg = StandardScaler() X_reg_scaled = scaler_reg.fit_transform(X_reg) # SGD Classifier clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_class_scaled, y_class) # Evaluate Classifier y_class_pred = clf.predict(X_class_scaled) class_accuracy = accuracy_score(y_class, y_class_pred) class_coefs = clf.coef_ class_intercept = clf.intercept_ print(f\\"Classification Accuracy: {class_accuracy}\\") print(f\\"Classification Coefficients: {class_coefs}\\") print(f\\"Classification Intercept: {class_intercept}\\") # SGD Regressor reg = SGDRegressor(loss=\'squared_error\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) reg.fit(X_reg_scaled, y_reg) # Evaluate Regressor y_reg_pred = reg.predict(X_reg_scaled) reg_mse = mean_squared_error(y_reg, y_reg_pred) reg_coefs = reg.coef_ reg_intercept = reg.intercept_ print(f\\"Regression Mean Squared Error: {reg_mse}\\") print(f\\"Regression Coefficients: {reg_coefs}\\") print(f\\"Regression Intercept: {reg_intercept}\\") # Example call (use actual test data instead) X_class_example = np.array([[0.0, 0.0], [1.0, 1.0]]) y_class_example = np.array([0, 1]) X_reg_example = np.array([[0.0, 0.0], [1.0, 1.0]]) y_reg_example = np.array([0.0, 1.0]) sgd_class_and_reg(X_class_example, y_class_example, X_reg_example, y_reg_example) ```","solution":"import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, mean_squared_error def sgd_class_and_reg(X_class, y_class, X_reg, y_reg): # Feature scaling scaler_class = StandardScaler() X_class_scaled = scaler_class.fit_transform(X_class) scaler_reg = StandardScaler() X_reg_scaled = scaler_reg.fit_transform(X_reg) # SGD Classifier clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_class_scaled, y_class) # Evaluate Classifier y_class_pred = clf.predict(X_class_scaled) class_accuracy = accuracy_score(y_class, y_class_pred) class_coefs = clf.coef_ class_intercept = clf.intercept_ print(f\\"Classification Accuracy: {class_accuracy}\\") print(f\\"Classification Coefficients: {class_coefs}\\") print(f\\"Classification Intercept: {class_intercept}\\") # SGD Regressor reg = SGDRegressor(loss=\'squared_error\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) reg.fit(X_reg_scaled, y_reg) # Evaluate Regressor y_reg_pred = reg.predict(X_reg_scaled) reg_mse = mean_squared_error(y_reg, y_reg_pred) reg_coefs = reg.coef_ reg_intercept = reg.intercept_ print(f\\"Regression Mean Squared Error: {reg_mse}\\") print(f\\"Regression Coefficients: {reg_coefs}\\") print(f\\"Regression Intercept: {reg_intercept}\\") # Example call (use actual test data instead) X_class_example = np.array([[0.0, 0.0], [1.0, 1.0]]) y_class_example = np.array([0, 1]) X_reg_example = np.array([[0.0, 0.0], [1.0, 1.0]]) y_reg_example = np.array([0.0, 1.0]) sgd_class_and_reg(X_class_example, y_class_example, X_reg_example, y_reg_example)"},{"question":"**Question: Implementing Semi-Supervised Learning with Scikit-Learn** You are provided with a dataset containing both labeled and unlabeled data. Your task is to implement a semi-supervised learning algorithm using scikit-learn to classify the data. Specifically, you will use the `LabelSpreading` method. This will help you understand the propagation of labels in a similarity graph constructed from the dataset. # Dataset: - The dataset will be provided as two numpy arrays `X` and `y`. - `X` is the feature matrix. - `y` is the target vector containing labels for some entries and `-1` for unlabeled entries. # Requirements: 1. Implement a function `semi_supervised_label_spreading` that: - Takes `X`, `y`, and parameters `gamma` for the RBF kernel and `max_iter` for the number of iterations. - Utilizes `LabelSpreading` from `sklearn.semi_supervised`. - Trains the model and returns the predicted labels for `X`. 2. Evaluate the performance: - After labeling the dataset, split it into a training set and a test set. - Train a simple supervised classifier (e.g., logistic regression) on the newly labeled training set. - Evaluate and return the classification accuracy on the test set. # Function Signature: ```python def semi_supervised_label_spreading(X: np.ndarray, y: np.ndarray, gamma: float, max_iter: int) -> float: Apply LabelSpreading to the dataset and evaluate its performance. Parameters: X (np.ndarray): The feature matrix. y (np.ndarray): The target vector, with `-1` for unlabeled entries. gamma (float): Parameter for RBF kernel. max_iter (int): Maximum number of iterations for LabelSpreading. Returns: float: Classification accuracy on the test set. pass ``` # Example Usage: ```python import numpy as np # Example data X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]) y = np.array([0, 1, -1, -1, 1]) # Set parameters gamma = 0.25 max_iter = 30 # Call the function accuracy = semi_supervised_label_spreading(X, y, gamma, max_iter) print(f\\"Classification accuracy: {accuracy}\\") ``` # Constraints: - You may assume that `X` and `y` have compatible shapes. - The dataset is small enough to fit into memory. - The dataset is balanced. # Additional Information: - Make sure to use an appropriate strategy for splitting the dataset into training and test sets after semi-supervised labeling. - You are allowed to use helper functions if needed. - The classification accuracy should be calculated using commonly used metrics from `sklearn.metrics`.","solution":"import numpy as np from sklearn.semi_supervised import LabelSpreading from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def semi_supervised_label_spreading(X: np.ndarray, y: np.ndarray, gamma: float, max_iter: int) -> float: Apply LabelSpreading to the dataset and evaluate its performance. Parameters: X (np.ndarray): The feature matrix. y (np.ndarray): The target vector, with `-1` for unlabeled entries. gamma (float): Parameter for RBF kernel. max_iter (int): Maximum number of iterations for LabelSpreading. Returns: float: Classification accuracy on the test set. # Apply the Label Spreading model label_spreading = LabelSpreading(kernel=\'rbf\', gamma=gamma, max_iter=max_iter) label_spreading.fit(X, y) y_full = label_spreading.transduction_ # Split the newly labeled data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y_full, test_size=0.3, random_state=42) # Train a simple classifier on the labeled data classifier = LogisticRegression() classifier.fit(X_train, y_train) # Predict on the test set and evaluate performance y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Seaborn Regression Analysis Task You are tasked with performing an exploratory data analysis using seaborn\'s regression plotting functionalities. Your goal is to reveal insights into the relationships between various features in a dataset. Here is what you need to do: 1. **Load the dataset:** You will use the `anscombe` dataset, which is built into seaborn. 2. **Polynomial Regression Plot:** Fit a second-order polynomial regression model to the \'II\' subset of the `anscombe` dataset and plot the regression line along with the scatterplot. 3. **Robust Regression Plot:** Fit a robust regression model to the \'III\' subset of the `anscombe` dataset, and plot the regression line with the scatterplot. 4. **Faceted Plots with Hue:** Use the `tips` dataset (also built into seaborn) to show how the relationship between `total_bill` and `tip` varies according to two categorical variables—`smoker` and `time`. Use faceting to separate the plots by `time` and color the points according to the `smoker` status. # Function Signature ```python def seaborn_regression_analysis(): pass ``` # Instructions - Use `sns.lmplot` for creating plots. - The plots should display the regression lines with the appropriate model fit. - Ensure to add titles and labels for clarity. - Use `seaborn` and `matplotlib.pyplot` libraries. - The `anscombe` and `tips` datasets should be loaded directly using `sns.load_dataset`. # Expected Output The function should produce and display the following plots: 1. Polynomial regression plot for the \'II\' subset of `anscombe`. 2. Robust regression plot for the \'III\' subset of `anscombe`. 3. Faceted plot of `total_bill` vs. `tip` with `smoker` as hue and `time` as column facet in the `tips` dataset. # Example Here is an example of what your plots might look like: **Polynomial Regression Plot (\'II\' subset of anscombe):** ![Polynomial Regression Plot](example_polynomial_plot.png) **Robust Regression Plot (\'III\' subset of anscombe):** ![Robust Regression Plot](example_robust_plot.png) **Faceted Plot of `total_bill` vs. `tip` (tips dataset):** ![Faceted Plot](example_faceted_plot.png) Ensure your function generates and shows these plots correctly when executed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def seaborn_regression_analysis(): # Load the anscombe dataset anscombe = sns.load_dataset(\'anscombe\') # Polynomial regression plot for the \'II\' subset of anscombe dataset_II = anscombe[anscombe[\'dataset\'] == \'II\'] plt.figure() sns.lmplot(x=\'x\', y=\'y\', data=dataset_II, order=2, ci=None) plt.title(\\"Second-Order Polynomial Regression (Dataset II)\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Y\\") plt.show() # Robust regression plot for the \'III\' subset of anscombe dataset_III = anscombe[anscombe[\'dataset\'] == \'III\'] plt.figure() sns.lmplot(x=\'x\', y=\'y\', data=dataset_III, robust=True, ci=None) plt.title(\\"Robust Regression (Dataset III)\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Y\\") plt.show() # Load the tips dataset tips = sns.load_dataset(\'tips\') # Faceted plot of total_bill vs. tip with smoker as hue and time as column facet plt.figure() g = sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", data=tips, ci=None) g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.add_legend() g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Relation between Total Bill and Tip by Time and Smoker Status\\") plt.show()"},{"question":"# Configuration Management with `configparser` You are tasked with managing configuration files for a software application. You need to create a Python script to handle the following requirements: 1. **Load Configuration**: - Read configuration data from a given string, file, or dictionary object. - The configuration will include sections such as `Database`, `Server`, and `User` with respective keys and values. 2. **Manipulate Configuration**: - Add a new section called `Logging` with keys `log_file` and `log_level`. - Update the `Server` section to include a key `timeout` with a value of `30`. 3. **Interpolation**: - Use both base and extended interpolation to create derived configuration values. - For example, derive `error_log` path in `Logging` section that references `log_file`. 4. **Save Configuration**: - Save the modified configuration to a new file. # Input 1. A string with initial configuration data. 2. A filename for saving the modified configuration. # Output - Save the resulting configuration to the specified file. # Constraints 1. Use `configparser` library functionalities. 2. Apply basic and extended interpolation where specified. 3. Handle any potential exceptions related to file handling and configuration parsing. # Example Given a string input representing the initial configuration: ```python initial_config = [DEFAULT] basedir = /etc/myapp datadir = %(basedir)s/data [Database] user = dbuser password = dbpassword [Server] host = localhost [User] username = admin ``` Your script should: - Add `Logging` section with `log_file = %(basedir)s/log/app.log` and `log_level = DEBUG`. - Update `Server` section to include `timeout = 30`. - Derive `error_log` in `Logging` as `{log_file}.error`. # Example Function Signature ```python import configparser def manage_config(initial_config: str, output_filename: str) -> None: # Your implementation here pass ``` **Note**: Your script should follow good programming practices, handle exceptions, and validate inputs appropriately.","solution":"import configparser from configparser import ExtendedInterpolation def manage_config(initial_config: str, output_filename: str) -> None: config = configparser.ConfigParser(interpolation=ExtendedInterpolation()) # Read configuration from the provided string config.read_string(initial_config) # Add \'Logging\' section with keys \'log_file\' and \'log_level\' if not config.has_section(\'Logging\'): config.add_section(\'Logging\') config[\'Logging\'][\'log_file\'] = \'{basedir}/log/app.log\' config[\'Logging\'][\'log_level\'] = \'DEBUG\' config[\'Logging\'][\'error_log\'] = \'{log_file}.error\' # Update \'Server\' section to include key \'timeout\' if not config.has_section(\'Server\'): config.add_section(\'Server\') config[\'Server\'][\'timeout\'] = \'30\' # Save the modified configuration to the specified file with open(output_filename, \'w\') as configfile: config.write(configfile)"},{"question":"You have been provided with a WAV file that contains audio data. Your task is to implement a function that reads the WAV file, extracts its parameters, and then creates a new WAV file by modifying these parameters. Specifically, you will change the frame rate of the original audio data and save it to a new file. # Function Signature ```python def modify_wav_file(input_file: str, output_file: str, new_frame_rate: int) -> None: pass ``` # Input - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path where the modified WAV file will be saved. - `new_frame_rate` (int): The new frame rate to be applied to the audio data. # Output - The function does not return anything. It will save the modified WAV file to the specified `output_file` location. # Constraints - `input_file` exists and is a valid WAV file using WAVE_FORMAT_PCM. - `new_frame_rate` is a positive integer. # Example ```python # Given an input WAV file \'input.wav\', modify its frame rate to 16000 and save it as \'output.wav\' modify_wav_file(\'input.wav\', \'output.wav\', 16000) ``` # Requirements 1. Open the input WAV file in read mode. 2. Extract the number of channels, sample width, and number of frames from the input file. 3. Read all the frames of audio data from the input file. 4. Open the output WAV file in write mode. 5. Set the number of channels, sample width (same as input), and update the frame rate (use `new_frame_rate`). 6. Write the audio frames to the output file. 7. Ensure the output file\'s header accurately reflects the new frame rate and other parameters. # Notes - You can use the methods `wave.open()`, `Wave_read.getnchannels()`, `Wave_read.getsampwidth()`, `Wave_read.getnframes()`, `Wave_read.readframes(n)`, `Wave_write.setnchannels(n)`, `Wave_write.setsampwidth(n)`, `Wave_write.setframerate(n)`, and `Wave_write.writeframes(data)` for this task. - Ensure to handle file closing properly to avoid any file corruption or data loss. Good luck with the implementation!","solution":"import wave def modify_wav_file(input_file: str, output_file: str, new_frame_rate: int) -> None: with wave.open(input_file, \'rb\') as input_wav: # Extract parameters from input file num_channels = input_wav.getnchannels() sample_width = input_wav.getsampwidth() num_frames = input_wav.getnframes() audio_frames = input_wav.readframes(num_frames) with wave.open(output_file, \'wb\') as output_wav: # Set new parameters for output file output_wav.setnchannels(num_channels) output_wav.setsampwidth(sample_width) output_wav.setframerate(new_frame_rate) # Write audio frames to the output file output_wav.writeframes(audio_frames)"},{"question":"# Command Line Interpreter with `cmd` Module You are tasked with building a custom interactive shell using the `cmd` module in Python. This shell should allow users to manage a list of tasks. Each task can have a description, a status (Pending or Completed), and a priority (Low, Medium, High). Requirements 1. Subclass the `Cmd` class to create your custom shell. 2. Implement the following commands with appropriate methods: - `add <description>`: Adds a new task with the given description, sets status to Pending, and priority to Medium. - `list`: Lists all tasks with their details. - `done <index>`: Marks the task at the given index as Completed. - `delete <index>`: Deletes the task at the given index. - `priority <index> <priority>`: Sets the priority of the task at the given index (valid priorities are Low, Medium, High). - `save <filename>`: Saves the current task list to a file. - `load <filename>`: Loads task list from a file. - `exit`: Exits the shell. 3. Ensure that the shell handles invalid commands gracefully by printing an appropriate error message. 4. Implement the `help` command to display descriptions for each command. 5. Use the provided structure to store tasks: ```python class Task: def __init__(self, description, status=\'Pending\', priority=\'Medium\'): self.description = description self.status = status self.priority = priority def __str__(self): return f\'{self.description} [{self.status}] (Priority: {self.priority})\' ``` ```python tasks = [] ``` Input and Output - **Input:** Commands entered by the user in the shell. - **Output:** Responses and task details printed to the console. Example Usage ``` python task_shell.py Welcome to the Task Manager Shell. Type help or ? to list commands. (task-manager) add Buy groceries Task added: Buy groceries [Pending] (Priority: Medium) (task-manager) list 1: Buy groceries [Pending] (Priority: Medium) (task-manager) priority 1 High Task updated: Buy groceries [Pending] (Priority: High) (task-manager) done 1 Task updated: Buy groceries [Completed] (Priority: High) (task-manager) exit Goodbye! ```","solution":"import cmd import pickle class Task: def __init__(self, description, status=\'Pending\', priority=\'Medium\'): self.description = description self.status = status self.priority = priority def __str__(self): return f\'{self.description} [{self.status}] (Priority: {self.priority})\' class TaskManagerShell(cmd.Cmd): intro = \'Welcome to the Task Manager Shell. Type help or ? to list commands.n\' prompt = \'(task-manager) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, description): Add a new task. Usage: add <description> if not description: print(\\"Error: No description provided.\\") return task = Task(description) self.tasks.append(task) print(f\'Task added: {task}\') def do_list(self, _): List all tasks. Usage: list if not self.tasks: print(\\"No tasks available.\\") for idx, task in enumerate(self.tasks, 1): print(f\'{idx}: {task}\') def do_done(self, index): Mark a task as completed. Usage: done <index> try: index = int(index) - 1 if 0 <= index < len(self.tasks): self.tasks[index].status = \'Completed\' print(f\'Task updated: {self.tasks[index]}\') else: print(\\"Error: Invalid task index.\\") except ValueError: print(\\"Error: Index should be an integer.\\") def do_delete(self, index): Delete a task. Usage: delete <index> try: index = int(index) - 1 if 0 <= index < len(self.tasks): deleted_task = self.tasks.pop(index) print(f\'Task deleted: {deleted_task}\') else: print(\\"Error: Invalid task index.\\") except ValueError: print(\\"Error: Index should be an integer.\\") def do_priority(self, args): Set task priority. Usage: priority <index> <priority> try: index, priority = args.split() index = int(index) - 1 if priority not in [\'Low\', \'Medium\', \'High\']: print(\\"Error: Invalid priority. Use Low, Medium, or High.\\") return if 0 <= index < len(self.tasks): self.tasks[index].priority = priority print(f\'Task updated: {self.tasks[index]}\') else: print(\\"Error: Invalid task index.\\") except ValueError: print(\\"Error: Usage is priority <index> <priority>.\\") def do_save(self, filename): Save tasks to a file. Usage: save <filename> try: with open(filename, \'wb\') as file: pickle.dump(self.tasks, file) print(f\'Tasks saved to {filename}\') except Exception as e: print(f\\"Error saving tasks: {e}\\") def do_load(self, filename): Load tasks from a file. Usage: load <filename> try: with open(filename, \'rb\') as file: self.tasks = pickle.load(file) print(f\'Tasks loaded from {filename}\') except Exception as e: print(f\\"Error loading tasks: {e}\\") def do_exit(self, _): Exit the task manager. Usage: exit print(\'Goodbye!\') return True def default(self, line): print(f\\"Error: Unrecognized command \'{line}\'\\") if __name__ == \'__main__\': TaskManagerShell().cmdloop()"},{"question":"# File Synchronizer and Logger You are required to implement a Python script that synchronizes the contents of a source directory to a destination directory. Additionally, the script should log all major steps of its execution. Specifically, your script should: 1. **Monitor**: Continuously monitor the source directory for any new files, modified files, or deleted files. 2. **Synchronize**: Reflect these changes in the destination directory. 3. **Log**: Log each detected change and the corresponding synchronization action (e.g., \\"File \'example.txt\' added\\", \\"File \'example.txt\' updated\\", \\"File \'example.txt\' deleted\\"). Input and Output Format - **Input**: The script should take three command-line arguments: 1. `source_dir` - The directory to monitor. 2. `dest_dir` - The directory to synchronize. 3. `log_file` - The file where logs will be written. - **Output**: Log entries written to the specified log file detailing the changes detected and actions taken. Constraints - The script should run indefinitely until manually stopped. - The log file must include timestamps for each logged entry. - The synchronization must handle nested directories within the source directory. - Only log changes that involve file creation, modification, and deletion. - Use appropriate Python standard libraries like `os`, `time`, `argparse`, and `logging`. Example Usage ```sh python file_sync.py /path/to/source /path/to/destination /path/to/logfile.log ``` Example Log Entries ```plaintext 2023-10-15 12:00:00 - INFO - File \'example.txt\' added. 2023-10-15 12:05:05 - INFO - File \'example.txt\' updated. 2023-10-15 12:10:10 - INFO - File \'example.txt\' deleted. ``` Performance Requirements - The script should efficiently handle monitoring and updating directories with up to 10^4 files. - Ensure that the script performs the synchronization in a timely manner without significant lag. # Implementation Guidelines - Use the `argparse` module to handle command-line arguments parsing. - Utilize the `os` module to interact with the file system. - Apply the `logging` module for logging with appropriate log levels and timestamps. - Use mechanisms to periodically check for changes and synchronize the directories. # Hints - Consider using `os.path` functions to help manage file paths. - Use the `time` module to manage periodic checks and timestamps. - You may find it helpful to use `filecmp` or similar utilities to detect file modifications. # Skeleton Code ```python import os import time import argparse import logging from shutil import copy2 def setup_logger(log_file): logging.basicConfig(filename=log_file, level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def monitor_directories(source_dir, dest_dir): # Implement the logic to continuously monitor and synchronize directories pass if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Directory Synchronization Tool\\") parser.add_argument(\\"source_dir\\", help=\\"Source directory to monitor\\") parser.add_argument(\\"dest_dir\\", help=\\"Destination directory to synchronize with\\") parser.add_argument(\\"log_file\\", help=\\"Log file to write changes and actions\\") args = parser.parse_args() setup_logger(args.log_file) monitor_directories(args.source_dir, args.dest_dir) ``` Implement the `monitor_directories` function to complete the synchronization logic as per the requirements.","solution":"import os import time import argparse import logging from shutil import copy2, rmtree from filecmp import cmpfiles def setup_logger(log_file): logging.basicConfig(filename=log_file, level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def get_all_files(directory): all_files = [] for root, dirs, files in os.walk(directory): for file in files: all_files.append(os.path.relpath(os.path.join(root, file), directory)) return all_files def synchronize_files(source_dir, dest_dir): source_files = set(get_all_files(source_dir)) dest_files = set(get_all_files(dest_dir)) # Files that are in source but not in destination added_files = source_files - dest_files for file in added_files: src_file_path = os.path.join(source_dir, file) dest_file_path = os.path.join(dest_dir, file) dest_file_folder = os.path.dirname(dest_file_path) if not os.path.exists(dest_file_folder): os.makedirs(dest_file_folder) copy2(src_file_path, dest_file_path) logging.info(f\\"File \'{file}\' added.\\") # Files that are both in source and destination but differ common_files = source_files & dest_files _, mismatches, _ = cmpfiles(source_dir, dest_dir, common_files, shallow=False) for file in mismatches: src_file_path = os.path.join(source_dir, file) dest_file_path = os.path.join(dest_dir, file) copy2(src_file_path, dest_file_path) logging.info(f\\"File \'{file}\' updated.\\") # Files that are in destination but not in source deleted_files = dest_files - source_files for file in deleted_files: dest_file_path = os.path.join(dest_dir, file) if os.path.exists(dest_file_path): os.remove(dest_file_path) dir_path = os.path.dirname(dest_file_path) # Clean up empty directories if not os.listdir(dir_path): os.removedirs(dir_path) logging.info(f\\"File \'{file}\' deleted.\\") def monitor_directories(source_dir, dest_dir): while True: synchronize_files(source_dir, dest_dir) time.sleep(1) # Check for changes every 1 second if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Directory Synchronization Tool\\") parser.add_argument(\\"source_dir\\", help=\\"Source directory to monitor\\") parser.add_argument(\\"dest_dir\\", help=\\"Destination directory to synchronize with\\") parser.add_argument(\\"log_file\\", help=\\"Log file to write changes and actions\\") args = parser.parse_args() setup_logger(args.log_file) monitor_directories(args.source_dir, args.dest_dir)"},{"question":"# Asynchronous File Processing with Platform Constraints Context: You are tasked with creating an asynchronous file processing system that reads from a file, performs some processing on its contents, and writes the transformed content to another file. Your implementation should be aware of platform-related constraints, particularly those related to the \\"asyncio\\" event loops on Windows. Objective: Write an asynchronous Python function using the `asyncio` module that reads data from an input file, converts all letters to uppercase, and writes the transformed data to an output file. Requirements: 1. Define the function `async def process_file(input_path: str, output_path: str) -> None:` - `input_path`: A string path to the input file. - `output_path`: A string path to the output file. 2. Your solution should use `asyncio` for reading and writing files. 3. Include error handling to manage files that might not exist or any read/write errors. Constraints: 1. On Windows: - You should account for the fact that `loop.add_reader()` and `loop.add_writer()` cannot be used and `SelectorEventLoop` has a limited socket count and does not support pipes. 2. The function should be compatible with macOS as well, keeping in mind the differences in event loop configurations for versions <= 10.8. Input and Output Formats: - Input: Paths to input and output text files. - Output: The output file should contain the uppercase version of the content from the input file. Example usage: ```python import asyncio async def main(): await process_file(\'input.txt\', \'output.txt\') asyncio.run(main()) ``` Notes: - Provide appropriate comments in your code explaining how you handle the platform-specific constraints. - Ensure your implementation is robust and can handle different sizes of input files without running into issues with event loop limitations on the supported platforms.","solution":"import asyncio import os async def process_file(input_path: str, output_path: str) -> None: Asynchronously reads from input file, transforms its content to uppercase, and writes it to the output file. :param input_path: A string path to the input file. :param output_path: A string path to the output file. if not os.path.exists(input_path): raise FileNotFoundError(f\\"The input file {input_path} does not exist.\\") loop = asyncio.get_event_loop() # Windows platform if os.name == \'nt\': policy = asyncio.WindowsSelectorEventLoopPolicy() asyncio.set_event_loop_policy(policy) try: # Read content from input file async with async_open(input_path, mode=\'r\') as infile: content = await infile.read() # Convert content to uppercase upper_content = content.upper() # Write content to output file async with async_open(output_path, mode=\'w\') as outfile: await outfile.write(upper_content) except Exception as e: print(f\\"An error occurred: {e}\\") class async_open: def __init__(self, filename, mode): self.filename = filename self.mode = mode async def __aenter__(self): self.file = await asyncio.to_thread(open, self.filename, self.mode) return self.file async def __aexit__(self, exc_type, exc, tb): await asyncio.to_thread(self.file.close)"},{"question":"**Objective:** You are tasked with creating a Python script that uses the `http.client` module to interact with an HTTP server. The script will demonstrate your understanding of creating an HTTP connection, sending various HTTP requests, and processing the responses. **Problem Statement:** Write a Python function, `check_website_status`, which takes a list of URLs as input and returns a dictionary where the keys are the URLs and the values are the status codes returned by the server for each URL. Additionally, if a URL returns a `404 Not Found` status, retrieve and return the response body as an error message in the dictionary. **Function Signature:** ```python def check_website_status(urls: list) -> dict: pass ``` **Input:** - `urls` (list): A list of URLs (strings) to check. Each URL must include the scheme (http or https). **Output:** - Dictionary where the keys are the URLs and the values are either the HTTP status code (int) if the response status is not `404`, or a tuple containing the status code and the response body (str) if the response status is `404`. **Constraints:** - Use the `http.client.HTTPConnection` or `http.client.HTTPSConnection` classes for making HTTP requests. - Properly close each connection after the request is made. - Handle possible exceptions such as `http.client.HTTPException`. **Examples:** ```python urls = [ \\"http://www.example.com\\", \\"https://www.github.com\\", \\"https://www.nonexistentwebsite.com/nonexistentpage\\" ] result = check_website_status(urls) print(result) # Expected output might look like: # { # \\"http://www.example.com\\": 200, # \\"https://www.github.com\\": 200, # \\"https://www.nonexistentwebsite.com/nonexistentpage\\": (404, \'<html>...</html>\') # } ``` **Hint:** - Use `urlparse` from the `urllib.parse` module to parse the URLs and extract the host and path. - Use the `request` method of `HTTPConnection` or `HTTPSConnection` to send a GET request. - Use the `getresponse` method to retrieve the response from the server. **Evaluation Criteria:** - Correct implementation of the function. - Proper use of `HTTPConnection` and `HTTPSConnection`. - Efficient handling of the connections and responses. - Handling different return statuses as specified.","solution":"import http.client from urllib.parse import urlparse def check_website_status(urls): status_dict = {} for url in urls: try: parsed_url = urlparse(url) connection = None if parsed_url.scheme == \'http\': connection = http.client.HTTPConnection(parsed_url.netloc) elif parsed_url.scheme == \'https\': connection = http.client.HTTPSConnection(parsed_url.netloc) if connection: connection.request(\\"GET\\", parsed_url.path or \\"/\\") response = connection.getresponse() status_code = response.status if status_code == 404: body = response.read().decode() status_dict[url] = (status_code, body) else: status_dict[url] = status_code connection.close() else: status_dict[url] = \'Unsupported scheme\' except http.client.HTTPException as e: status_dict[url] = f\'HTTPException: {e}\' except Exception as e: status_dict[url] = f\'Error: {e}\' return status_dict"},{"question":"Problem Statement You are given the task of visualizing and analyzing the relationship between variables in the \\"mpg\\" dataset using seaborn. This dataset contains various features of cars such as weight, acceleration, horsepower, etc. # Objective Write a Python function, `analyze_mpg()`, that performs the following tasks: 1. **Load the Dataset** - Load the \\"mpg\\" dataset using seaborn\'s `load_dataset()` method. 2. **Linear Regression Plot** - Create a linear regression plot to show the relationship between `weight` and `acceleration`. 3. **Polynomial Regression Plot** - Create a second-order polynomial regression plot to show the relationship between `weight` and `mpg`. 4. **Log-Linear Regression Plot** - Create a log-linear regression plot to show the relationship between `displacement` and `mpg`. 5. **Lowess Smoother Plot** - Create a plot using a locally-weighted scatterplot smoother (LOWESS) to show the relationship between `horsepower` and `mpg`. 6. **Logistic Regression Plot** - Create a logistic regression plot to show the probability that the origin of the car is from the USA based on its weight (`weight`). 7. **Robust Regression Plot** - Create a robust regression plot to show the relationship between `horsepower` and `weight`. 8. **Customized Linear Regression Plot** - Create a linear regression plot to show the relationship between `weight` and `horsepower` with the following customizations: - Confidence interval at 99% - Use \'x\' as the marker. - Set the marker color to \'.3\'. - Set the line color to \'r\'. # Constraints - You should use seaborn for all plots. - Ensure to set appropriate titles and labels for each plot. - The function should save each plot to a file (e.g., `linear_regression.png`, `polynomial_regression.png`, etc.). # Input The function does not take any inputs. # Output The function should save the respective plots with appropriate file names. # Function Signature ```python def analyze_mpg(): pass ``` # Example After running the function, the following files should be generated in the current directory: - `linear_regression.png` - `polynomial_regression.png` - `log_linear_regression.png` - `lowess_smoother.png` - `logistic_regression.png` - `robust_regression.png` - `customized_linear_regression.png` Each file should contain the corresponding plot as specified in the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def analyze_mpg(): # Load the dataset mpg = sns.load_dataset(\'mpg\') # 1. Linear Regression Plot plt.figure() sns.lmplot(x=\'weight\', y=\'acceleration\', data=mpg, ci=None) plt.title(\'Linear Regression: Weight vs Acceleration\') plt.savefig(\'linear_regression.png\') # 2. Polynomial Regression Plot plt.figure() sns.lmplot(x=\'weight\', y=\'mpg\', data=mpg, order=2, ci=None) plt.title(\'Polynomial Regression (Order 2): Weight vs MPG\') plt.savefig(\'polynomial_regression.png\') # 3. Log-Linear Regression Plot plt.figure() sns.lmplot(x=\'displacement\', y=\'mpg\', data=mpg, logx=True, ci=None) plt.title(\'Log-Linear Regression: Displacement vs MPG\') plt.savefig(\'log_linear_regression.png\') # 4. Lowess Smoother Plot plt.figure() sns.lmplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True, ci=None) plt.title(\'Lowess Smoother: Horsepower vs MPG\') plt.savefig(\'lowess_smoother.png\') # 5. Logistic Regression Plot # Classifier to use in the logistic regression plot - convert origin to binary (USA vs. others) mpg[\'origin_USA\'] = (mpg[\'origin\'] == \'usa\').astype(int) plt.figure() sns.lmplot(x=\'weight\', y=\'origin_USA\', data=mpg, logistic=True, ci=None) plt.title(\'Logistic Regression: Probability of USA origin based on Weight\') plt.savefig(\'logistic_regression.png\') # 6. Robust Regression Plot plt.figure() sns.lmplot(x=\'horsepower\', y=\'weight\', data=mpg, robust=True, ci=None) plt.title(\'Robust Regression: Horsepower vs Weight\') plt.savefig(\'robust_regression.png\') # 7. Customized Linear Regression Plot plt.figure() sns.lmplot(x=\'weight\', y=\'horsepower\', data=mpg, ci=99, markers=\'x\', scatter_kws={\'color\': \'.3\'}, line_kws={\'color\': \'r\'}) plt.title(\'Customized Linear Regression: Weight vs Horsepower\') plt.savefig(\'customized_linear_regression.png\') plt.close()"},{"question":"# Advanced Seaborn Coding Assessment Objective Design a Python function using the Seaborn library to create a series of plots that include different types of error bars for a given dataset. The function should take care of handling different types of error bars and plot these on a single visualization. This will test your understanding of both the fundamental and advanced concepts of the Seaborn library. Function Signature ```python def create_errorbar_plots(data: pd.DataFrame, x_col: str, y_col: str) -> None: Creates a series of plots with different types of error bars for a given dataset. Parameters: data (pd.DataFrame): DataFrame containing the data. x_col (str): The column name to be plotted on the x-axis. y_col (str): The column name to be plotted on the y-axis. Returns: None ``` Input - `data` (pd.DataFrame): A DataFrame containing at least two numerical columns. - `x_col` (str): The name of the column to be plotted on the x-axis. - `y_col` (str): The name of the column to be plotted on the y-axis. Output - The function should produce a plot with four subplots, each showing the data with a different type of error bar: 1. Standard deviation error bars 2. Standard error bars 3. Percentile interval error bars 4. Confidence interval error bars Constraints - You should use the `sns.pointplot` function from the Seaborn library for plotting points with error bars. - Each type of error bar should be appropriately labeled. - The function should display the final visualization using `plt.show()` from the Matplotlib library. - Handle any potential exceptions in data processing (e.g., if the specified columns do not exist in the DataFrame). Example Usage ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'x\': range(10), \'y\': [2, 3, 1, 5, 4, 6, 7, 8, 5, 6] }) # Create errorbar plots create_errorbar_plots(data, \'x\', \'y\') ``` Notes - Consult the Seaborn documentation for additional parameters and customization options for the `sns.pointplot` function. - Ensure that the subplots are well-organized and labeled for clear interpretation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_errorbar_plots(data: pd.DataFrame, x_col: str, y_col: str) -> None: Creates a series of plots with different types of error bars for a given dataset. Parameters: data (pd.DataFrame): DataFrame containing the data. x_col (str): The column name to be plotted on the x-axis. y_col (str): The column name to be plotted on the y-axis. Returns: None if x_col not in data.columns or y_col not in data.columns: raise ValueError(f\\"Columns {x_col} and/or {y_col} not found in the DataFrame\\") error_bars_types = [\'sd\', \'se\', \'ci\', \'pi\'] error_bars_labels = [\'Standard Deviation\', \'Standard Error\', \'Confidence Interval\', \'Percentile Interval\'] fig, axes = plt.subplots(2, 2, figsize=(15, 10)) axes_flat = axes.flatten() for ax, err_type, label in zip(axes_flat, error_bars_types, error_bars_labels): sns.pointplot(data=data, x=x_col, y=y_col, errorbar=err_type, ax=ax) ax.set_title(label) plt.tight_layout() plt.show()"},{"question":"**XML Parsing with Custom ContentHandler** As a seasoned Python developer, you are required to parse an XML file using the `xml.sax.handler` module and display key content from the document. You need to implement a custom `ContentHandler` class to handle specific XML elements and extract data. # Objective: Write a Python program that: 1. Implements a custom `ContentHandler` class to handle the start and end of specific XML elements and character data. 2. Uses this class to parse a provided XML file and extract specified data. # Requirements: 1. Define a class `CustomContentHandler` inheriting from `xml.sax.handler.ContentHandler`. 2. Override the following methods in `CustomContentHandler`: - `startElement(name, attrs)`: Capture the start of specific tags (`title`, `author`, `year`). - `endElement(name)`: Capture the end of the mentioned tags and store their content. - `characters(content)`: Collect character data from the XML elements. 3. Use the `xml.sax` module to parse an example XML file and print the content of the `title`, `author`, and `year` elements. # Input: You will be given an XML file with the following structure: ```xml <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> </book> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> </book> <!-- More book entries --> </library> ``` # Output: Print the collected content, formatted as follows: ``` Book Details: Title: Effective Python Author: Brett Slatkin Year: 2015 Title: Learning Python Author: Mark Lutz Year: 2013 ``` # Constraints: 1. All element tags and character data should be handled as described without exceptions. 2. Implement appropriate error handling using an `ErrorHandler`. # Implementation: ```python import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if self.current_data == \\"title\\": print(f\\"Title: {self.title}\\") elif self.current_data == \\"author\\": print(f\\"Author: {self.author}\\") elif self.current_data == \\"year\\": print(f\\"Year: {self.year}\\") print() self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title = content elif self.current_data == \\"author\\": self.author = content elif self.current_data == \\"year\\": self.year = content class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): raise exception def fatalError(self, exception): raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") if __name__ == \\"__main__\\": # Create an XMLReader parser = xml.sax.make_parser() # Turn off namespace processing parser.setFeature(xml.sax.handler.feature_namespaces, 0) # Override the default Context Handler handler = CustomContentHandler() parser.setContentHandler(handler) # Set custom error handler parser.setErrorHandler(CustomErrorHandler()) # Parse the XML file parser.parse(\\"books.xml\\") ``` Ensure you have an XML file named `books.xml` with the specified structure in your working directory before running the script.","solution":"import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.book_details = [] def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if name == \\"book\\": self.book_details.append({ \\"title\\": self.title, \\"author\\": self.author, \\"year\\": self.year }) self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content.strip() elif self.current_data == \\"author\\": self.author += content.strip() elif self.current_data == \\"year\\": self.year += content.strip() class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): raise exception def fatalError(self, exception): raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_books(xml_file): # Create an XMLReader parser = xml.sax.make_parser() # Turn off namespace processing parser.setFeature(xml.sax.handler.feature_namespaces, 0) # Override the default Context Handler handler = CustomContentHandler() parser.setContentHandler(handler) # Set custom error handler parser.setErrorHandler(CustomErrorHandler()) # Parse the XML file parser.parse(xml_file) return handler.book_details def print_book_details(book_details): print(\\"Book Details:\\") for book in book_details: print(f\\"Title: {book[\'title\']}\\") print(f\\"Author: {book[\'author\']}\\") print(f\\"Year: {book[\'year\']}\\") print() if __name__ == \\"__main__\\": book_details = parse_books(\\"books.xml\\") print_book_details(book_details)"},{"question":"Objective: To assess the student\'s ability to handle large datasets with pandas, focusing on efficient data loading, memory optimization, and chunk processing. Question: You are provided with a directory containing multiple Parquet files, each representing a month\'s worth of timeseries data. The columns in the dataset include: - `timestamp`: The timestamp of the record. - `id`: A unique identifier. - `name`: A low-cardinality text field (e.g., names of some entities). - `x`, `y`: Numeric measurements. Write functions to: 1. **Load and Reduce Memory Usage**: Load a specific subset of columns (`timestamp`, `id`, `name`) from all files into a single DataFrame, optimizing for memory by converting the `name` column to `pandas.Categorical` and downcasting the `id` column to the smallest possible numeric type. 2. **Monthly Aggregation**: Calculate the monthly mean for columns `x` and `y` and return the result as a DataFrame. 3. **Efficient Value Counting**: Return the count of each unique name across all files without loading all data into memory at once. Requirements: 1. Implement a function `load_and_optimize(directory: str, columns: list) -> pd.DataFrame`: - `directory`: The directory containing the Parquet files. - `columns`: The list of columns to load. - Return a DataFrame with the specified columns, optimized for memory usage. 2. Implement a function `calculate_monthly_means(df: pd.DataFrame) -> pd.DataFrame`: - `df`: Input DataFrame from the `load_and_optimize` function. - Return a DataFrame containing the monthly mean of columns `x` and `y`. 3. Implement a function `count_unique_names(directory: str) -> pd.Series`: - `directory`: The directory containing the Parquet files. - Return a Series with the counts of each unique name across all files. Constraints: - Assume each Parquet file fits into memory individually, but the entire dataset does not. - Optimize for memory usage and performance. - You should not load all data into memory simultaneously. Example: ```python load_and_optimize(\\"data/timeseries\\", [\\"timestamp\\", \\"id\\", \\"name\\"]) # Output: DataFrame with columns \'timestamp\', \'id\', \'name\' optimized for memory. calculate_monthly_means(df) # Output: DataFrame with the monthly mean of columns \'x\' and \'y\'. count_unique_names(\\"data/timeseries\\") # Output: Series with counts of each unique name. ``` Your task is to write the implementations for `load_and_optimize`, `calculate_monthly_means`, and `count_unique_names` according to the specified requirements.","solution":"import pandas as pd import os from pandas.api.types import CategoricalDtype def load_and_optimize(directory: str, columns: list) -> pd.DataFrame: Load specific subset of columns from Parquet files and optimize for memory usage. dfs = [] for filename in os.listdir(directory): if filename.endswith(\'.parquet\'): file_path = os.path.join(directory, filename) df = pd.read_parquet(file_path, columns=columns) df[\'name\'] = df[\'name\'].astype(CategoricalDtype()) df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'integer\') dfs.append(df) optimized_df = pd.concat(dfs, ignore_index=True) return optimized_df def calculate_monthly_means(df: pd.DataFrame) -> pd.DataFrame: Calculate the monthly mean of columns \'x\' and \'y\'. df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df.set_index(\'timestamp\', inplace=True) monthly_mean = df.resample(\'M\').mean().reset_index() return monthly_mean def count_unique_names(directory: str) -> pd.Series: Return the count of each unique name across all files. name_counts = pd.Series(dtype=\'int\') for filename in os.listdir(directory): if filename.endswith(\'.parquet\'): file_path = os.path.join(directory, filename) df = pd.read_parquet(file_path, columns=[\'name\']) df[\'name\'] = df[\'name\'].astype(CategoricalDtype()) name_counts = name_counts.add(df[\'name\'].value_counts(), fill_value=0) return name_counts"},{"question":"**Question: Configuring PyTorch Logging System** In this task, you are required to implement a function that configures the logging system for PyTorch based on a given input dictionary. The dictionary contains information about the log levels for various components and artifacts that should be enabled. You should use the `torch._logging.set_logs` API for configuring the logging. # Function Signature ```python def configure_pytorch_logging(log_settings: dict) -> None: ``` # Input - `log_settings` (dict): A dictionary where: - Keys are strings representing components or artifacts. - Values are either booleans or integers: - For components, values are integers representing the log level (`0` for `logging.NOTSET`, `10` for `logging.DEBUG`, and so on up to `50` for `logging.CRITICAL`). - For artifacts, values are booleans (`True` to enable, `False` to disable). # Output - The function should return `None`. It will configure the logging settings in PyTorch based on the provided dictionary. # Example ```python log_settings = { \\"dynamo\\": 10, # Set TorchDynamo log level to DEBUG \\"aot\\": 40, # Set AOTAutograd log level to ERROR \\"graph_code\\": True, # Enable the graph_code artifact \\"schedule\\": False # Disable the schedule artifact } configure_pytorch_logging(log_settings) ``` # Considerations 1. Ensure that you validate the input dictionary to handle invalid components or artifacts gracefully. 2. You may assume that the values provided in `log_settings` dictionary are valid (i.e., appropriate log levels or booleans). 3. Provide any necessary imports in your function, assuming `torch` is already installed. Implement the function `configure_pytorch_logging` that fulfills the above requirements.","solution":"import torch import logging def configure_pytorch_logging(log_settings: dict) -> None: Configures the logging system for PyTorch based on a given input dictionary. Parameters: log_settings (dict): A dictionary where keys are strings representing components or artifacts, and values are either booleans (for artifacts) or integers (for log levels). log_config = {} for key, value in log_settings.items(): if isinstance(value, bool): log_config[key] = value elif isinstance(value, int) and value in [logging.NOTSET, logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL]: log_config[key] = value else: raise ValueError(f\\"Invalid log level or artifact setting for key: {key}, value: {value}\\") torch._logging.set_logs(**log_config)"},{"question":"Objective Implement a Python class that simulates a reference counting mechanism for managing the lifecycle of resources. This will test your understanding of reference management and ensure that you can manage resource allocation and deallocation effectively. Problem Statement You are required to design a `Resource` class in Python that will manage a resource (e.g., a string value) and implement reference counting similar to what is described in the provided documentation. This class should handle incrementing and decrementing references safely and ensure proper cleanup when the resources are no longer needed. Requirements 1. **Class Definition**: Define a class `Resource` with the following methods: - `__init__(self, value: str)`: Constructor method to initialize the resource and set the initial reference count. - `inc_ref(self)`: Method to increment the reference count. - `dec_ref(self)`: Method to decrement the reference count and clean up the resource if the count reaches zero. - `get_value(self) -> str`: Method to get the value of the resource. 2. **Constraints**: - The class must manage the reference count correctly, ensuring no memory leaks. - The class should raise an exception if `dec_ref` is called on an already deallocated resource. 3. **Performance Considerations**: - The increment and decrement operations should be efficient and correctly manage state transitions. - You must ensure thread-safety (consider using the `threading` module) as this could be part of a larger system accessed from multiple threads. Input - Initialization of a `Resource` object with a string value. - Calls to `inc_ref()` and `dec_ref()` methods. - Retrieval of the resource value with `get_value()`. Output - String value when `get_value()` is called. - Exception raised if `dec_ref()` is called on an already released resource. Example Usage ```python from threading import Lock class Resource: def __init__(self, value: str): self.value = value self.ref_count = 1 self.lock = Lock() def inc_ref(self): with self.lock: if self.value is None: raise ValueError(\\"Cannot increment reference, resource is already deallocated.\\") self.ref_count += 1 def dec_ref(self): with self.lock: if self.ref_count == 0: raise ValueError(\\"Resource already deallocated.\\") self.ref_count -= 1 if self.ref_count == 0: self.cleanup() def cleanup(self): self.value = None def get_value(self) -> str: if self.value is None: raise ValueError(\\"Resource is deallocated.\\") return self.value # Example Usage resource = Resource(\\"Hello, World!\\") print(resource.get_value()) # Output: Hello, World! resource.inc_ref() resource.dec_ref() resource.dec_ref() # Should cleanup and deallocate the resource try: print(resource.get_value()) # Should raise ValueError: Resource is deallocated. except ValueError as e: print(e) try: resource.dec_ref() # Should raise ValueError: Resource already deallocated. except ValueError as e: print(e) ``` Notes - Ensure that the `Resource` class is thread-safe. - Validate all edge cases and handle exceptions appropriately.","solution":"from threading import Lock class Resource: def __init__(self, value: str): self.value = value self.ref_count = 1 self.lock = Lock() def inc_ref(self): with self.lock: if self.value is None: raise ValueError(\\"Cannot increment reference, resource is already deallocated.\\") self.ref_count += 1 def dec_ref(self): with self.lock: if self.ref_count == 0: raise ValueError(\\"Resource already deallocated.\\") self.ref_count -= 1 if self.ref_count == 0: self.cleanup() def cleanup(self): self.value = None def get_value(self) -> str: if self.value is None: raise ValueError(\\"Resource is deallocated.\\") return self.value"},{"question":"You are tasked with processing and verifying a set of encoded data files. These files can be encoded in various formats including base64, uuencoded, and quoted-printable formats. You need to write a Python function that decodes a given file, checks its integrity using a CRC32 checksum, and returns the decoded content if the integrity check passes. # Function Signature ```python def decode_and_verify(file_path: str, format_type: str, checksum: int) -> bytes: Decodes the file content based on the specified format and verifies its integrity using CRC32 checksum. :param file_path: Path to the encoded data file. :param format_type: The encoding format of the data (\'base64\', \'uu\', or \'quoted-printable\'). :param checksum: The expected CRC32 checksum of the decoded data. :return: Decoded file content if checksum matches, else raises a ValueError. :raises ValueError: If the checksum does not match. ``` # Input - `file_path` (str): The path to the encoded data file. - `format_type` (str): The encoding format of the data. It could be `\'base64\'`, `\'uu\'`, or `\'quoted-printable\'`. - `checksum` (int): The expected CRC32 checksum of the decoded data. # Output - `bytes`: The decoded binary content of the file if the checksum matches. # Exceptions - Raises a `ValueError` if the calculated checksum of the decoded data does not match the provided checksum. # Constraints 1. The file will contain data in one of the specified encoding formats and will end with a newline character. 2. Each file will be less than 10 MB in size. 3. The checksum provided is the unsigned CRC32 value. # Examples 1. Given an encoded file `data_base64.txt` with base64 content: ```txt aGVsbG8gd29ybGQ= ``` and checksum `0xfc3ff98e`, calling `decode_and_verify(\'data_base64.txt\', \'base64\', 0xfc3ff98e)` should return `b\'hello world\'`. 2. Given an encoded file `data_uu.txt` with uuencoded content: ```txt M9\'8+ ``` and checksum `0x1c291ca3`, calling `decode_and_verify(\'data_uu.txt\', \'uu\', 0x1c291ca3)` should return the corresponding decoded content if the checksum matches. Notes: - You can use the `binascii` module functions such as `a2b_uu`, `a2b_base64`, `a2b_qp`, and `crc32` to implement this function. - Ensure proper file handling and exception management for a robust solution. # Solution Skeleton ```python import binascii def decode_and_verify(file_path: str, format_type: str, checksum: int) -> bytes: # Step 1: Read the file content with open(file_path, \'r\') as file: encoded_data = file.read() # Step 2: Decode based on the format_type if format_type == \'base64\': decoded_data = binascii.a2b_base64(encoded_data) elif format_type == \'uu\': decoded_data = binascii.a2b_uu(encoded_data) elif format_type == \'quoted-printable\': decoded_data = binascii.a2b_qp(encoded_data) else: raise ValueError(\\"Unsupported format_type provided.\\") # Step 3: Verify the checksum calculated_checksum = binascii.crc32(decoded_data) if calculated_checksum != checksum: raise ValueError(\\"Checksum does not match.\\") return decoded_data ```","solution":"import binascii def decode_and_verify(file_path: str, format_type: str, checksum: int) -> bytes: Decodes the file content based on the specified format and verifies its integrity using CRC32 checksum. :param file_path: Path to the encoded data file. :param format_type: The encoding format of the data (\'base64\', \'uu\', or \'quoted-printable\'). :param checksum: The expected CRC32 checksum of the decoded data. :return: Decoded file content if checksum matches, else raises a ValueError. :raises ValueError: If the checksum does not match. # Step 1: Read the file content with open(file_path, \'r\') as file: encoded_data = file.read() # Step 2: Decode based on the format_type if format_type == \'base64\': decoded_data = binascii.a2b_base64(encoded_data) elif format_type == \'uu\': decoded_data = binascii.a2b_uu(encoded_data) elif format_type == \'quoted-printable\': decoded_data = binascii.a2b_qp(encoded_data) else: raise ValueError(\\"Unsupported format_type provided.\\") # Step 3: Verify the checksum calculated_checksum = binascii.crc32(decoded_data) & 0xffffffff # Get unsigned CRC32 if calculated_checksum != checksum: raise ValueError(\\"Checksum does not match.\\") return decoded_data"},{"question":"Objective Your task is to implement a utility function that leverages the `quopri` module to encode and decode text data using quoted-printable encoding. This function should be able to handle both file inputs/outputs and string inputs/outputs. This will demonstrate your understanding of both file handling and byte string manipulation in Python, along with the capabilities of the `quopri` module. Function Signature ```python def quoted_printable_utility(input_data, mode=\'decode\', file_mode=False, header=False, quotetabs=False): Encodes or decodes the given input data using quoted-printable encoding. Parameters: input_data (str or bytes): The input data to be encoded or decoded. If file_mode is True, this should be the file path. mode (str): The operation mode, either \'encode\' or \'decode\'. Defaults to \'decode\'. file_mode (bool): If True, input_data is treated as a file path, and the function reads from and writes to files. Defaults to False. header (bool): If True, and the mode is \'decode\', underscores will be decoded as spaces. For \'encode\', spaces are encoded as underscores. Defaults to False. quotetabs (bool): If True, and the mode is \'encode\', embedded spaces and tabs are encoded. Defaults to False. Returns: str or bytes: The encoded or decoded data. If file_mode is True, returns None. ``` Requirements - If `file_mode` is False, `input_data` should be treated as either a string or bytes. - When `mode` is `decode`, the provided byte string should be decoded and the resulting string returned. - When `mode` is `encode`, the provided string should be encoded and the resulting byte string returned. - If `file_mode` is True, `input_data` should be treated as a file path. - When `mode` is `decode`, the function should read from the input file, decode the data, and write the output to a file named `decoded_output.txt`. - When `mode` is `encode`, the function should read from the input file, encode the data, and write the output to a file named `encoded_output.txt`. Example Tests ```python # Test cases for string input (non-file mode) input_str = \\"This is a test string.\\" encoded_bytes = quoted_printable_utility(input_data=input_str, mode=\'encode\') decoded_str = quoted_printable_utility(input_data=encoded_bytes, mode=\'decode\') assert decoded_str == input_str # Test cases for file input (file mode) with open(\'test_input.txt\', \'wb\') as f: f.write(b\'This is another test string.\') quoted_printable_utility(input_data=\'test_input.txt\', mode=\'encode\', file_mode=True) with open(\'encoded_output.txt\', \'rb\') as f: encoded_content = f.read() quoted_printable_utility(input_data=\'encoded_output.txt\', mode=\'decode\', file_mode=True) with open(\'decoded_output.txt\', \'rb\') as f: decoded_content = f.read() assert decoded_content == b\'This is another test string.\' ``` Constraints and Considerations - Ensure proper error handling for file operations. - Input validation to ensure `input_data` is appropriate for the given `file_mode`. - Performance is not a key concern, but the solution should be efficient for reasonably large text files.","solution":"import quopri def quoted_printable_utility(input_data, mode=\'decode\', file_mode=False, header=False, quotetabs=False): Encodes or decodes the given input data using quoted-printable encoding. Parameters: input_data (str or bytes): The input data to be encoded or decoded. If file_mode is True, this should be the file path. mode (str): The operation mode, either \'encode\' or \'decode\'. Defaults to \'decode\'. file_mode (bool): If True, input_data is treated as a file path, and the function reads from and writes to files. Defaults to False. header (bool): If True, and the mode is \'decode\', underscores will be decoded as spaces. For \'encode\', spaces are encoded as underscores. Defaults to False. quotetabs (bool): If True, and the mode is \'encode\', embedded spaces and tabs are encoded. Defaults to False. Returns: str or bytes: The encoded or decoded data. If file_mode is True, returns None. if file_mode: if mode == \'decode\': with open(input_data, \'rb\') as file: encoded_data = file.read() decoded_data = quopri.decodestring(encoded_data, header=header) with open(\'decoded_output.txt\', \'wb\') as file: file.write(decoded_data) elif mode == \'encode\': with open(input_data, \'rb\') as file: raw_data = file.read() encoded_data = quopri.encodestring(raw_data, quotetabs=quotetabs) with open(\'encoded_output.txt\', \'wb\') as file: file.write(encoded_data) else: if mode == \'decode\': decoded_data = quopri.decodestring(input_data, header=header) return decoded_data.decode(\'utf-8\') elif mode == \'encode\': encoded_data = quopri.encodestring(input_data.encode(\'utf-8\'), quotetabs=quotetabs) return encoded_data"},{"question":"# Question: Evaluate String Patterns with Regular Expressions **Objective:** Create a Python function that processes a list of strings and outputs the strings that match specific patterns using regular expressions. **Task:** You need to write a function `filter_strings_based_on_patterns(strings: List[str], patterns: List[str]) -> List[str]` that takes in a list of strings and a list of regex patterns. The function should return a list containing only those strings that match **any** of the provided patterns. **Function Signature:** ```python def filter_strings_based_on_patterns(strings: List[str], patterns: List[str]) -> List[str]: ``` **Input:** - `strings`: A list of strings where each string is a sequence of ASCII characters. - `patterns`: A list of regular expression patterns (as strings) that will be used to filter the list of strings. **Output:** - A list of strings from the input list `strings` that match any of the provided regex `patterns`. **Constraints:** - The list `strings` can have up to 10,000 strings. - Each string inside `strings` has a length between 1 and 100 characters. - The list `patterns` can have up to 100 regex patterns. - Each pattern inside `patterns` has a length between 1 and 50 characters. **Example:** ```python strings = [\\"apple\\", \\"banana\\", \\"cherry\\", \\"date\\", \\"elderberry\\"] patterns = [r\'^a\', r\'rr\', r\'e\'] output = filter_strings_based_on_patterns(strings, patterns) print(output) # Output: [\\"apple\\", \\"cherry\\", \\"elderberry\\"] ``` **Explanation:** - The string \\"apple\\" matches the pattern `r\'^a\'` (starts with \'a\'). - The string \\"cherry\\" matches the pattern `r\'rr\'` (contains \'rr\'). - The string \\"elderberry\\" matches the pattern `r\'e\'` (ends with \'e\'). **Notes:** - You are required to use the `re` module for regular expression operations. - Ensure that your function is efficient and can handle the upper limits of the input constraints.","solution":"import re from typing import List def filter_strings_based_on_patterns(strings: List[str], patterns: List[str]) -> List[str]: Filters a list of strings based on a list of regular expression patterns. :param strings: List of strings to be filtered. :param patterns: List of regex patterns to use for filtering. :return: List of strings that match any of the provided patterns. matching_strings = [] for string in strings: for pattern in patterns: if re.search(pattern, string): matching_strings.append(string) break return matching_strings"},{"question":"**Question: Implementing Concurrent Data Fetching with Threading and Multiprocessing** You are tasked to design a system that concurrently fetches data from multiple URLs and processes the data fetched. You have to implement two versions of this system: one using thread-based parallelism (using the `threading` module) and another using process-based parallelism (using the `multiprocessing` module). Your solution should demonstrate clear understanding and appropriate use of the threading and multiprocessing features provided by Python. # Specifications Input: - A list of URLs: `List[str]` Output: - A dictionary where the keys are the URLs and the values are the processed data fetched from those URLs: `Dict[str, Any]` Constraints: - For the sake of this exercise, you can assume `data` is a simple JSON object fetched from the URL and the processing involves parsing the JSON and extracting a specific field called \'required_field\'. Thread-based Version: 1. Use the `threading` module to create multiple threads. 2. Implement thread-safe data fetching and processing. 3. Use appropriate synchronization mechanisms to prevent data race conditions. 4. Ensure the program handles exceptions and errors gracefully. Process-based Version: 1. Use the `multiprocessing` module to create multiple processes. 2. Implement inter-process communication to collect processed results. 3. Ensure synchronization between processes where necessary. 4. Handle exceptions and errors appropriately. # Example: ```python # Sample URL list urls = [ \\"http://example.com/data1\\", \\"http://example.com/data2\\", \\"http://example.com/data3\\" ] # Expected output format (the actual data would depend on the fetched JSON\'s \'required_field\') { \\"http://example.com/data1\\": \\"value1\\", \\"http://example.com/data2\\": \\"value2\\", \\"http://example.com/data3\\": \\"value3\\" } ``` # Requirements: - Implement two functions: - `fetch_data_using_threads(urls: List[str]) -> Dict[str, Any]` - `fetch_data_using_multiprocessing(urls: List[str]) -> Dict[str, Any]` - Each function must return a dictionary mapping URLs to their processed data. - Demonstrate the correct usage of synchronization primitives in the `threading` and `multiprocessing` modules. - Write inline comments to explain key sections of your code. # Notes: - Use the `requests` library to fetch data from URLs. If not available, mock the URL fetching part. - Ensure your implementation is efficient and handles a large number of URLs gracefully.","solution":"import threading import multiprocessing import requests from typing import List, Dict, Any import json def fetch_data(url: str) -> Any: Fetch data from a URL and return the required field from the JSON response. try: response = requests.get(url) data = response.json() return data.get(\'required_field\', None) except Exception as e: return str(e) def fetch_data_using_threads(urls: List[str]) -> Dict[str, Any]: Fetch data from URLs using threads and return a dictionary mapping URLs to their processed data. result_lock = threading.Lock() results = {} def thread_worker(url: str): data = fetch_data(url) with result_lock: results[url] = data threads = [] for url in urls: thread = threading.Thread(target=thread_worker, args=(url,)) threads.append(thread) thread.start() for thread in threads: thread.join() return results def fetch_data_using_multiprocessing(urls: List[str]) -> Dict[str, Any]: Fetch data from URLs using multiprocessing and return a dictionary mapping URLs to their processed data. manager = multiprocessing.Manager() results = manager.dict() def process_worker(url: str, results): data = fetch_data(url) results[url] = data processes = [] for url in urls: process = multiprocessing.Process(target=process_worker, args=(url, results)) processes.append(process) process.start() for process in processes: process.join() return dict(results)"},{"question":"Objective To test your understanding of PyTorch\'s CUDA interface, especially device management and memory management, by creating a function that utilizes CUDA devices, populates a tensor with random values, and monitors memory usage. Problem Statement You are required to implement a function `explore_cuda_memory` that does the following: 1. **Initial Setup**: Checks if a CUDA device is available. If not, raise an appropriate exception. 2. **Device Information**: Prints the name and total memory of the current CUDA device. 3. **Memory Pre-Allocation**: - Query the initial memory usage and print it. - Query the initial maximum allocated memory and print it. 4. **Random Tensor Generation**: Creates a random tensor of a given size on the CUDA device. 5. **Memory Post-Allocation**: - Query the memory usage after the tensor allocation and print it. - Query the maximum memory allocated so far and print it. 6. **Memory Cleanup**: Frees up GPU memory allocated for the tensor and resets peak memory statistics. Function Signature ```python def explore_cuda_memory(tensor_size: int): pass ``` Input - `tensor_size (int)`: The size (number of elements) of the tensor to be created. The function should create a 1D tensor with these many elements. Constraints - You can assume the size will be a positive integer. - The function should handle the CUDA device selection and memory cleanup properly. - The function should not use more memory than required. Efficient memory management is key. Expected Output The function should return no value, but print the following: 1. The name of the current CUDA device. 2. Total memory of the current CUDA device. 3. Initial memory usage. 4. Initial maximum memory allocated. 5. Memory usage after tensor allocation. 6. Maximum memory allocated after tensor allocation. Performance Requirements - The solution should be efficient both in terms of time and memory. - Properly free up GPU memory to avoid memory leaks. Example ```python explore_cuda_memory(1000000) ``` Expected printed output (values will vary based on your GPU and the time of execution): ``` Device Name: Tesla T4 Total Memory: 15.75 GB Initial Memory Usage: 82 MB Initial Max Memory Allocated: 82 MB Memory Usage After Tensor Allocation: 92 MB Max Memory Allocated After Tensor Allocation: 92 MB Memory freed and peak memory stats reset. ``` Use the `torch.cuda` module documentation to help with your implementation.","solution":"import torch def explore_cuda_memory(tensor_size: int): Explore CUDA memory usage by creating a random tensor on the GPU and monitoring memory usage. Args: tensor_size (int): The size (number of elements) of the tensor to be created. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA device is not available.\\") device = torch.device(\\"cuda:0\\") device_name = torch.cuda.get_device_name(device) total_memory = torch.cuda.get_device_properties(device).total_memory print(f\\"Device Name: {device_name}\\") print(f\\"Total Memory: {total_memory // (1024 ** 2)} MB\\") # Initial memory usage information initial_mem_alloc = torch.cuda.memory_allocated(device) initial_max_mem_alloc = torch.cuda.max_memory_allocated(device) print(f\\"Initial Memory Usage: {initial_mem_alloc // (1024 ** 2)} MB\\") print(f\\"Initial Max Memory Allocated: {initial_max_mem_alloc // (1024 ** 2)} MB\\") # Create a random tensor tensor = torch.randn(tensor_size, device=device) # Memory usage after tensor allocation post_alloc_mem_usage = torch.cuda.memory_allocated(device) max_mem_allocated = torch.cuda.max_memory_allocated(device) print(f\\"Memory Usage After Tensor Allocation: {post_alloc_mem_usage // (1024 ** 2)} MB\\") print(f\\"Max Memory Allocated After Tensor Allocation: {max_mem_allocated // (1024 ** 2)} MB\\") # Free the allocated memory and reset peak memory stats del tensor torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats(device) print(\\"Memory freed and peak memory stats reset.\\")"},{"question":"Floating-point arithmetic can be tricky due to the way floating-point numbers are represented in computer hardware. In this exercise, you will implement a function to accurately sum a list of floating-point numbers and verify the precision of the result. # Task Implement a function `accurate_sum(numbers: List[float]) -> Tuple[float, str, float]` that performs the following tasks: 1. Compute the sum of a list of floating-point numbers using Python\'s built-in `sum()` function. 2. Compute the sum of the same list of floating-point numbers using the `math.fsum()` function, which is designed to mitigate precision loss during summation. 3. Return a tuple containing: - The sum calculated using the built-in `sum()` function. - The hexadecimal representation of the exact float value of this sum. - The sum calculated using `math.fsum()`. # Input - An integer `n` representing the number of elements in the list, where `1 <= n <= 10^5`. - A list of `n` floating-point numbers. # Output - A tuple containing: - A floating-point number representing the sum calculated using the built-in `sum()` function. - A string representing the hexadecimal representation of the float value of this sum. - A floating-point number representing the sum calculated using `math.fsum()`. # Example ```python from typing import List, Tuple import math def accurate_sum(numbers: List[float]) -> Tuple[float, str, float]: # Calculate sum using built-in sum() built_in_sum = sum(numbers) # Get hexadecimal representation hex_repr = float.hex(built_in_sum) # Calculate sum using math.fsum() fsum_result = math.fsum(numbers) return (built_in_sum, hex_repr, fsum_result) # Example usage numbers = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] print(accurate_sum(numbers)) # Output: (0.9999999999999999, \'0x1.fffffffffffffp-1\', 1.0) ``` # Constraints - You should handle the input size efficiently to avoid exceeding time limits. - Use built-in Python libraries without additional packages. # Notes - Carefully compare the sums obtained from the built-in `sum()` function and `math.fsum()` function. - Pay attention to the hexadecimal representation of the floating-point sum. # Hints - Remember that due to representation errors, the sums obtained by the two different methods might slightly differ, even though they represent the same mathematical sum. - Use `float.hex()` for obtaining the exact representation of the floating-point number.","solution":"from typing import List, Tuple import math def accurate_sum(numbers: List[float]) -> Tuple[float, str, float]: Returns a tuple containing: - The sum of the list using built-in sum() function. - The hexadecimal representation of the float value of this sum. - The sum of the list using math.fsum() function. # Calculate sum using built-in sum() built_in_sum = sum(numbers) # Get hexadecimal representation hex_repr = float.hex(built_in_sum) # Calculate sum using math.fsum() fsum_result = math.fsum(numbers) return (built_in_sum, hex_repr, fsum_result)"},{"question":"# Pandas Coding Challenge Objective Your task is to manipulate and analyze a DataFrame using pandas to address both memory usage concerns and proper DataFrame mutation practices. Problem Statement You are given a DataFrame containing various datatypes, including `object`, `int64`, and `float64`. You need to perform the following tasks: 1. **Memory Analysis**: - Create a function `memory_analysis(df: pd.DataFrame) -> pd.DataFrame` that takes a DataFrame and returns a summary DataFrame indicating the memory usage of each column in human-readable form. - The summary DataFrame should have: - Two columns: `Column` and `Memory Usage`. - The `Column` column includes the column names of the input DataFrame. - The `Memory Usage` column includes the memory usage of each column in bytes. 2. **DataFrame Mutation**: - Create a function `safe_mutation(df: pd.DataFrame) -> pd.DataFrame` that mutates the DataFrame in a safe way. This function should: - Add a new column named \'new_column\' where each value is double the corresponding value in the \'int_col\' column. - Ensure that the original DataFrame is not modified. - Return the new mutated DataFrame. Input - A DataFrame `df` following this schema: ```plaintext int_col : int64 float_col : float64 obj_col : object ``` Output - For `memory_analysis(df)`, a DataFrame with the following structure: ```plaintext Column Memory Usage int_col xxx bytes float_col xxx bytes obj_col xxx bytes ``` - For `safe_mutation(df)`, a DataFrame with the following structure: ```plaintext int_col : int64 float_col : float64 obj_col : object new_column : int64 ``` Ensure your implementations handle data efficiently and avoid unnecessary memory consumption. Example ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'int_col\': [1, 2, 3], \'float_col\': [1.0, 2.0, 3.0], \'obj_col\': [\'a\', \'b\', \'c\'] } df = pd.DataFrame(data) # Example usage of memory_analysis print(memory_analysis(df)) # Example usage of safe_mutation print(safe_mutation(df)) ``` Your implementation should adhere to these guidelines and provide correct and efficient outputs.","solution":"import pandas as pd def memory_analysis(df: pd.DataFrame) -> pd.DataFrame: Returns a summary DataFrame indicating the memory usage of each column in human-readable form. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: DataFrame with columns \'Column\' and \'Memory Usage\'. memory_usage = df.memory_usage(deep=True) summary_df = pd.DataFrame({ \'Column\': memory_usage.index, \'Memory Usage\': memory_usage.values }) summary_df[\'Memory Usage\'] = summary_df[\'Memory Usage\'].apply(lambda x: f\\"{x} bytes\\") return summary_df.reset_index(drop=True) def safe_mutation(df: pd.DataFrame) -> pd.DataFrame: Mutates the DataFrame in a safe way. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The mutated DataFrame with an additional \'new_column\'. new_df = df.copy() new_df[\'new_column\'] = new_df[\'int_col\'] * 2 return new_df"},{"question":"Objective: Create a function that takes a list of mixed data types (integers, floats, and strings) and converts it to a formatted string. The function should: 1. Convert numbers to strings with a specified number of decimal places for floats. 2. Handle and format each element appropriately. 3. Gracefully handle errors in conversion and formatting. Function Signature: ```python def format_mixed_list(data_list: list, float_precision: int) -> str: pass ``` Input: 1. `data_list` (List): A list containing elements that are either integers, floats, or strings. 2. `float_precision` (int): The number of decimal places to format floating-point numbers. Output: * A single string where each element from the input list is converted to a string and concatenated with a space separator. Constraints: 1. Floats must be formatted according to the given precision. 2. If an element cannot be converted properly, it should be replaced with the string `\\"<error>\\"`. 3. String elements should be included as-is. 4. The function should ensure that the resulting string doesn\'t have trailing or leading whitespace. Example: ```python data_list = [23, 4.5678, \'example\', 1002, 3.14159, \'test\'] precision = 2 result = format_mixed_list(data_list, precision) print(result) # Expected Output: \\"23 4.57 example 1002 3.14 test\\" ``` Additional Notes: - You cannot use global variables. - Implement error handling for invalid inputs within the function. # Your Task: Implement the `format_mixed_list` function in Python.","solution":"def format_mixed_list(data_list: list, float_precision: int) -> str: formatted_list = [] for item in data_list: if isinstance(item, int): formatted_list.append(str(item)) elif isinstance(item, float): formatted_list.append(f\\"{item:.{float_precision}f}\\") elif isinstance(item, str): formatted_list.append(item) else: formatted_list.append(\\"<error>\\") return \' \'.join(formatted_list)"},{"question":"Problem Statement You are tasked with implementing functionality for a simple file-based counter system that uses file locking to ensure safe concurrent access. Your system should allow initializing the counter, incrementing and decrementing the counter, and reading the current counter value. The counter\'s value must be stored in a file, and you must ensure that operations on the counter are atomic and thread-safe using file locking mechanisms. Requirements 1. Implement a class `FileCounter`. 2. The class should have the following methods: - `__init__(self, filepath: str)`: Initializes the counter file. If the file does not exist, it should be created and initialized to zero. - `increment(self)`: Atomically increments the counter by 1. - `decrement(self)`: Atomically decrements the counter by 1. - `get_value(self)`: Returns the current value of the counter. 3. Use the `fcntl.lockf` method to ensure that operations on the counter are thread-safe. 4. Ensure that the file is properly locked during read and write operations to prevent race conditions. Input and Output Formats - You do not need to handle input and output from the main function. You should only implement the class and its methods. - The `increment` and `decrement` methods do not return any value. - The `get_value` method returns an integer representing the current counter value. Example ```python counter = FileCounter(\'/path/to/counter.txt\') counter.increment() counter.increment() print(counter.get_value()) # Output should be 2 counter.decrement() print(counter.get_value()) # Output should be 1 ``` Constraints - You can assume that the file path provided will be valid and writable. - Focus on ensuring thread safety using file locks. - Handle potential exceptions that might arise due to file I/O issues. Performance Requirements - The operations should be atomic and lock the file only for the duration necessary to perform read/write operations to minimize contention. ```python import fcntl import os class FileCounter: def __init__(self, filepath: str): self.filepath = filepath # Initialize the file and counter if not existent if not os.path.exists(filepath): with open(filepath, \'w\') as f: f.write(\'0\') def increment(self): Atomically increment the counter value by 1 with open(self.filepath, \'r+\') as f: fcntl.lockf(f, fcntl.LOCK_EX) value = int(f.read().strip()) f.seek(0) f.write(str(value + 1) + \'n\') f.flush() fcntl.lockf(f, fcntl.LOCK_UN) def decrement(self): Atomically decrement the counter value by 1 with open(self.filepath, \'r+\') as f: fcntl.lockf(f, fcntl.LOCK_EX) value = int(f.read().strip()) f.seek(0) f.write(str(value - 1) + \'n\') f.flush() fcntl.lockf(f, fcntl.LOCK_UN) def get_value(self): Get the current counter value with open(self.filepath, \'r\') as f: fcntl.lockf(f, fcntl.LOCK_SH) value = int(f.read().strip()) fcntl.lockf(f, fcntl.LOCK_UN) return value ```","solution":"import fcntl import os class FileCounter: def __init__(self, filepath: str): self.filepath = filepath # Initialize the file and counter if not existent if not os.path.exists(filepath): with open(filepath, \'w\') as f: f.write(\'0\') def increment(self): Atomically increment the counter value by 1 with open(self.filepath, \'r+\') as f: fcntl.lockf(f, fcntl.LOCK_EX) value = int(f.read().strip()) f.seek(0) f.write(str(value + 1) + \'n\') f.flush() fcntl.lockf(f, fcntl.LOCK_UN) def decrement(self): Atomically decrement the counter value by 1 with open(self.filepath, \'r+\') as f: fcntl.lockf(f, fcntl.LOCK_EX) value = int(f.read().strip()) f.seek(0) f.write(str(value - 1) + \'n\') f.flush() fcntl.lockf(f, fcntl.LOCK_UN) def get_value(self): Get the current counter value with open(self.filepath, \'r\') as f: fcntl.lockf(f, fcntl.LOCK_SH) value = int(f.read().strip()) fcntl.lockf(f, fcntl.LOCK_UN) return value"},{"question":"# SAX XML Parsing Challenge In this task, you are required to implement a custom SAX ContentHandler to parse an XML document. Your handler should be able to extract and print specific data from the XML document in a specified format. Problem Statement You are given an XML document containing information about various books. Each book element has details such as title, author, genre, price, and publish date. Your task is to write a custom SAX ContentHandler subclass that extracts and prints the title and author of each book. The XML structure is as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <genre>Genre 1</genre> <price>29.99</price> <publish_date>2021-01-01</publish_date> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <genre>Genre 2</genre> <price>39.99</price> <publish_date>2020-05-15</publish_date> </book> <!-- More book elements --> </library> ``` Requirements 1. **ContentHandler Implementation**: - Subclass `xml.sax.handler.ContentHandler`. - Implement the method `startElement` to detect when a book element starts and track when the title and author tags start. - Implement the method `endElement` to detect when the title and author tags end and store their values. - Implement the method `characters` to collect the content between the start and end tags. 2. **Extracting and Printing Information**: - Upon detecting the end of a book element, print the title and author in the following format: ```plaintext Title: Book Title X, Author: Author X ``` - Ensure that each book\'s title and author are printed on a new line. 3. **Performance**: - The XML parser should handle large XML documents efficiently. Input and Output - **Input**: An XML file with the structure mentioned above. - **Output**: Printed book titles and authors in the specified format. Constraints - You must use the `xml.sax` module for parsing. - Implement the `startElement`, `endElement`, and `characters` methods to handle the relevant XML content. Example ```python import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.isTitle = False self.isAuthor = False self.currentTitle = \\"\\" self.currentAuthor = \\"\\" def startElement(self, name, attrs): if name == \\"title\\": self.isTitle = True elif name == \\"author\\": self.isAuthor = True def endElement(self, name): if name == \\"title\\": self.isTitle = False elif name == \\"author\\": self.isAuthor = False elif name == \\"book\\": print(f\\"Title: {self.currentTitle.strip()}, Author: {self.currentAuthor.strip()}\\") self.currentTitle = \\"\\" self.currentAuthor = \\"\\" def characters(self, content): if self.isTitle: self.currentTitle += content elif self.isAuthor: self.currentAuthor += content def parseBooks(xml_file): parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) parser.parse(xml_file) if __name__ == \\"__main__\\": parseBooks(\\"books.xml\\") ``` This example sets up the ContentHandler to track when book elements, titles, and authors are being processed and then prints the relevant information in the specified format.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.currentElement = \\"\\" self.currentTitle = \\"\\" self.currentAuthor = \\"\\" def startElement(self, name, attrs): self.currentElement = name def endElement(self, name): if name == \\"book\\": print(f\\"Title: {self.currentTitle.strip()}, Author: {self.currentAuthor.strip()}\\") self.currentTitle = \\"\\" self.currentAuthor = \\"\\" def characters(self, content): if self.currentElement == \\"title\\": self.currentTitle += content elif self.currentElement == \\"author\\": self.currentAuthor += content def parseBooks(xmlFile): parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) parser.parse(xmlFile) # Usage example: # if __name__ == \\"__main__\\": # parseBooks(\\"books.xml\\")"},{"question":"# Advanced Python C-Extension Creation Your task is to create a custom Python extension type that simulates a simple counter object using the Python C-API. The counter will have the following functionalities: 1. **Initialization:** Initialize the counter with a specific integer value. 2. **Increment:** Increment the counter value by a given number. 3. **Decrement:** Decrement the counter value by a given number. 4. **Representation:** Provide a string representation (`str`) and a detailed representation (`repr`) of the counter. 5. **Comparison:** Allow comparison of counter objects based on their values. 6. **Attributes:** Manage attributes to get and set the counter value. Required Steps: 1. **Define the `PyCounterObject` Structure:** - Include `PyObject_HEAD`. - Include an integer to store the counter\'s value. 2. **Create the Type Object:** - Fill appropriate fields of `PyTypeObject` for methods and operations including initialization, deallocation, string representations, attribute handling, and comparison. 3. **Implement Key Methods:** - Implement the following methods in C: `tp_init`, `tp_dealloc`, `tp_repr`, `tp_str`, `tp_richcompare`, `tp_getattro`, and `tp_setattro`. 4. **Create Module Definition:** - Define a module that initializes the `PyCounterType`. # Detailed Instructions 1. **Counter Object Structure:** ```c typedef struct { PyObject_HEAD int counter_value; } PyCounterObject; ``` 2. **Type Object Definition:** Define the `PyTypeObject` for `PyCounterType`. ```c static PyTypeObject PyCounterType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"counter.Counter\\", .tp_basicsize = sizeof(PyCounterObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_doc = \\"Custom counter object\\", .tp_init = (initproc) PyCounter_init, .tp_dealloc = (destructor) PyCounter_dealloc, .tp_repr = (reprfunc) PyCounter_repr, .tp_str = (reprfunc) PyCounter_str, .tp_richcompare = (richcmpfunc) PyCounter_richcompare, .tp_getattro = (getattrofunc) PyObject_GenericGetAttr, .tp_setattro = (setattrofunc) PyObject_GenericSetAttr }; ``` 3. **Implementation of Methods:** - **Initialization (`tp_init`):** Initialize the counter. - **Deallocation (`tp_dealloc`):** Free the counter resources. - **Representation (`tp_repr` & `tp_str`):** Provide human-readable and detailed representations. - **Comparison (`tp_richcompare`):** Compare counter objects. - **Attribute Handling (`tp_getattro` & `tp_setattro`):** Manage getting and setting the counter value. ```c static int PyCounter_init(PyCounterObject *self, PyObject *args, PyObject *kwds) { int initial_value = 0; if (!PyArg_ParseTuple(args, \\"i\\", &initial_value)) { return -1; } self->counter_value = initial_value; return 0; } static void PyCounter_dealloc(PyCounterObject *self) { Py_TYPE(self)->tp_free((PyObject*)self); } static PyObject * PyCounter_repr(PyCounterObject *self) { return PyUnicode_FromFormat(\\"Counter(value=%d)\\", self->counter_value); } static PyObject * PyCounter_str(PyCounterObject *self) { return PyUnicode_FromFormat(\\"%d\\", self->counter_value); } static PyObject * PyCounter_richcompare(PyObject *obj1, PyObject *obj2, int op) { if (!PyObject_TypeCheck(obj1, &PyCounterType) || !PyObject_TypeCheck(obj2, &PyCounterType)) { Py_RETURN_NOTIMPLEMENTED; } int v1 = ((PyCounterObject *)obj1)->counter_value; int v2 = ((PyCounterObject *)obj2)->counter_value; int result; switch (op) { case Py_LT: result = v1 < v2; break; case Py_LE: result = v1 <= v2; break; case Py_EQ: result = v1 == v2; break; case Py_NE: result = v1 != v2; break; case Py_GT: result = v1 > v2; break; case Py_GE: result = v1 >= v2; break; default: Py_RETURN_NOTIMPLEMENTED; } return result ? Py_True : Py_False; } ``` 4. **Module Definition:** Define the module and initialize the `PyCounterType`. ```c static PyModuleDef counter_module = { PyModuleDef_HEAD_INIT, .m_name = \\"counter\\", .m_doc = \\"Module that creates a counter object\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_counter(void) { PyObject *m; if (PyType_Ready(&PyCounterType) < 0) return NULL; m = PyModule_Create(&counter_module); if (m == NULL) return NULL; Py_INCREF(&PyCounterType); if (PyModule_AddObject(m, \\"Counter\\", (PyObject *) &PyCounterType) < 0) { Py_DECREF(&PyCounterType); Py_DECREF(m); return NULL; } return m; } ``` Once you have the above setup correctly, compile and test your module to ensure that the custom counter object behaves as expected. **Constraints:** - Use Python 3.10 or above. - Handle memory carefully to prevent leaks. - Ensure that comparisons and string representations are accurate. Performance Requirements: - Operations should be efficient and handle typical use cases without significant delay.","solution":"# This is Python code representation that will correspond to the functionality # described in the C-API custom Python extension type. class Counter: def __init__(self, initial_value=0): self.counter_value = initial_value def increment(self, value=1): self.counter_value += value def decrement(self, value=1): self.counter_value -= value def __str__(self): return str(self.counter_value) def __repr__(self): return f\\"Counter(value={self.counter_value})\\" def __eq__(self, other): if isinstance(other, Counter): return self.counter_value == other.counter_value return False def __lt__(self, other): if isinstance(other, Counter): return self.counter_value < other.counter_value return False def __le__(self, other): if isinstance(other, Counter): return self.counter_value <= other.counter_value return False def __gt__(self, other): if isinstance(other, Counter): return self.counter_value > other.counter_value return False def __ge__(self, other): if isinstance(other, Counter): return self.counter_value >= other.counter_value return False @property def value(self): return self.counter_value @value.setter def value(self, new_value): self.counter_value = new_value"},{"question":"# Understanding and Using Seaborn Color Palettes **Problem Statement:** You are provided with a set of visual data and are required to generate a comparative visualization using Seaborn. The task will assess your understanding of Seaborn’s color palettes and your ability to customize and apply these palettes effectively. **Task:** 1. **Data Preparation:** - Generate synthetic data for two different categories. - Each category should contain 100 data points, drawn from a normal distribution with different means and standard deviations. 2. **Color Palette Creation:** - Create three different color palettes using Seaborn: - A default categorical color palette. - A perceptually-uniform color palette. - A custom gradient palette. 3. **Visualization:** - Create three comparative scatter plots using the synthetic data created in Step 1. Each plot should use one of the three created palettes. - Your plots should include appropriate titles, and legends and should use different colors to represent each category. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def generate_comparative_visualizations(): # Your implementation here # Call the function to execute the comparison generate_comparative_visualizations() ``` **Requirements:** - The synthetic data should be generated using `numpy`. - Use appropriate Seaborn functions to create and use different color palettes. - Ensure that the scatter plots are clearly labeled, and each point\'s category is distinguishable based on color. - Each scatter plot should use a different color palette, and this should be reflected in the plot titles. **Grading Criteria:** - Proper generation and visualization of synthetic data. - Correct use of Seaborn functions to create and apply different color palettes. - Clarity and readability of the scatter plots. - Proper labeling and titling of the plots. **Example Output:** Three scatter plots should be generated and displayed, each utilizing a distinct color palette for visual comparison.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def generate_comparative_visualizations(): # Step 1: Data Preparation category_1 = np.random.normal(loc=0, scale=1, size=100) category_2 = np.random.normal(loc=5, scale=1.5, size=100) data = pd.DataFrame({ \'value\': np.concatenate([category_1, category_2]), \'category\': [\'Category 1\'] * 100 + [\'Category 2\'] * 100 }) # Step 2: Color Palette Creation default_palette = sns.color_palette() perceptual_palette = sns.color_palette(\\"viridis\\", as_cmap=False) custom_palette = sns.light_palette(\\"navy\\", reverse=True, n_colors=2) # Step 3: Visualization fig, axs = plt.subplots(1, 3, figsize=(18, 6)) sns.scatterplot( data=data, x=data.index, y=\'value\', hue=\'category\', palette=default_palette, ax=axs[0] ) axs[0].set_title(\'Default Color Palette\') sns.scatterplot( data=data, x=data.index, y=\'value\', hue=\'category\', palette=perceptual_palette, ax=axs[1] ) axs[1].set_title(\'Perceptually-Uniform Color Palette\') sns.scatterplot( data=data, x=data.index, y=\'value\', hue=\'category\', palette=custom_palette, ax=axs[2] ) axs[2].set_title(\'Custom Gradient Color Palette\') for ax in axs: ax.legend(loc=\'upper right\') plt.show() # Call the function to execute the comparison generate_comparative_visualizations()"},{"question":"# Advanced Coding Assessment Question Objective Write a Python program that uses the `asyncio` module to simulate downloading files from a list of URLs and processing some local file I/O operations asynchronously. The program should be able to handle platform-specific limitations and configurations as described in the provided documentation. Requirements 1. **Function Implementation**: - Implement a function `async download_and_process(urls: List[str], local_files: List[str]) -> None`. This function should asynchronously download files from the given URLs and read content from local files simultaneously. 2. **Platform-Specific Handling**: - On Windows, ensure that the ProactorEventLoop is used, which supports network operations and subprocess but has limitations for file descriptors. - On macOS, ensure the compatibility with versions 10.6 and above by configuring selectors manually if needed. Expected Function Signature ```python import asyncio from typing import List async def download_and_process(urls: List[str], local_files: List[str]) -> None: pass ``` Input - `urls`: A list of URLs as strings from which data should be downloaded. - `local_files`: A list of file paths as strings to be read concurrently. Output - The function does not return any value but processes the URLs and local files asynchronously, printing a message for each completed download and file read. Constraints 1. Implement proper error handling for network and file I/O operations. 2. Ensure that the program correctly handles different platform-specific limitations and configurations for asyncio loops as described in the documentation. 3. Assume no more than 100 URLs and 100 file paths will be provided as input lists. Performance Requirements - The function should run efficiently and utilize asynchronous features to handle I/O-bound tasks concurrently. # Example ```python import asyncio from typing import List async def download_and_process(urls: List[str], local_files: List[str]) -> None: async def download(url): print(f\\"Downloading from {url}\\") # Simulate network delay await asyncio.sleep(1) print(f\\"Completed download from {url}\\") async def read_file(file_path): print(f\\"Reading from {file_path}\\") # Simulate file read delay await asyncio.sleep(0.5) print(f\\"Completed reading from {file_path}\\") tasks = [download(url) for url in urls] + [read_file(file) for file in local_files] await asyncio.gather(*tasks) # Example usage urls = [\\"http://example.com/file1\\", \\"http://example.com/file2\\"] local_files = [\\"localfile1.txt\\", \\"localfile2.txt\\"] asyncio.run(download_and_process(urls, local_files)) ``` Implement the `download_and_process` function ensuring adherence to the platform-specific requirements as described in the documentation provided.","solution":"import asyncio import platform import aiohttp from typing import List import os async def download_and_process(urls: List[str], local_files: List[str]) -> None: if platform.system() == \\"Windows\\": asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) elif platform.system() == \\"Darwin\\": # macOS if platform.mac_ver()[0] < \'10.6\': raise Exception(\\"Unsupported macOS version.\\") async def download(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: content = await response.read() print(f\\"Downloaded {len(content)} bytes from {url}\\") async def read_file(file_path): with open(file_path, \'r\') as f: content = f.read() print(f\\"Read {len(content)} bytes from {file_path}\\") tasks = [download(url) for url in urls] + [read_file(file) for file in local_files] await asyncio.gather(*tasks) # Example usage urls = [\\"http://example.com/file1\\", \\"http://example.com/file2\\"] local_files = [\\"localfile1.txt\\", \\"localfile2.txt\\"] if __name__ == \\"__main__\\": asyncio.run(download_and_process(urls, local_files))"},{"question":"# Create a MIME Mail with Multiple Attachments Design a Python function called `create_mime_email` that builds a MIME email with a combination of text, an image, and an audio file. This email should be able to be realistically sent via a mail server. **Function Signature:** ```python def create_mime_email(text, image_data, audio_data, sender, recipient, subject): pass ``` **Parameters:** - `text` (str): The body text of the email. - `image_data` (bytes): The binary data for an image attachment. - `audio_data` (bytes): The binary data for an audio attachment. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `subject` (str): The subject of the email. **Output:** - Returns a `MIMEMultipart` email object that can be used to send the email. **Constraints:** - Assume the image is in JPEG format and the audio is in WAV format. - The image and audio should be encoded using base64. **Example:** ```python text = \\"This is a sample email with text, image, and audio attachments.\\" image_data = open(\'sample.jpg\', \'rb\').read() audio_data = open(\'sample.wav\', \'rb\').read() sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" subject = \\"Sample Email\\" email = create_mime_email(text, image_data, audio_data, sender, recipient, subject) ``` **Instructions:** 1. You will need to import the appropriate classes (`MIMEMultipart`, `MIMEText`, `MIMEImage`, `MIMEAudio`). 2. Create a MIMEMultipart email object. 3. Attach the text as a `MIMEText` object. 4. Attach the image as a `MIMEImage` object. 5. Attach the audio as a `MIMEAudio` object. 6. Set the appropriate headers for sender, recipient, and subject. 7. Ensure proper encoding and content-type headers are set for image and audio data. The resulting MIME email object should contain all parts correctly formatted and ready to be serialized and sent.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email import encoders def create_mime_email(text, image_data, audio_data, sender, recipient, subject): # Create the root message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Attach the text (plain text) text_part = MIMEText(text, \'plain\') msg.attach(text_part) # Attach the image (JPEG) image_part = MIMEImage(image_data, _subtype=\\"jpeg\\") encoders.encode_base64(image_part) image_part.add_header(\'Content-Disposition\', \'attachment; filename=\\"image.jpg\\"\') msg.attach(image_part) # Attach the audio (WAV) audio_part = MIMEAudio(audio_data, _subtype=\\"wav\\") encoders.encode_base64(audio_part) audio_part.add_header(\'Content-Disposition\', \'attachment; filename=\\"audio.wav\\"\') msg.attach(audio_part) return msg"},{"question":"**Objective:** Write a Python function using `scikit-learn`\'s `pairwise` module to compute various pairwise matrices (distance and kernel) for a given set of input vectors. Your implementation should demonstrate an understanding of both distance metrics and kernels. **Problem Statement:** Implement a function `compute_pairwise_matrices` that takes as input two sets of vectors `X` and `Y`, an array of metrics, and an array of kernels. For each metric and kernel provided, the function should compute and return the respective pairwise distance and kernel matrices. **Function Signature:** ```python def compute_pairwise_matrices(X: np.ndarray, Y: np.ndarray, metrics: list, kernels: list) -> dict: pass ``` **Inputs:** 1. `X`: A 2D NumPy array of shape `(n_samples_X, n_features)` representing the first set of vectors. 2. `Y`: A 2D NumPy array of shape `(n_samples_Y, n_features)` representing the second set of vectors. 3. `metrics`: A list of strings, where each string is a valid distance metric (e.g., `[\'euclidean\', \'manhattan\']`). 4. `kernels`: A list of strings, where each string is a valid kernel function (e.g., `[\'linear\', \'polynomial\']`). **Outputs:** - A dictionary with the following keys: - `\'metrics\'`: A nested dictionary where each key is a metric name, and its value is the pairwise distance matrix computed for that metric. - `\'kernels\'`: A nested dictionary where each key is a kernel name, and its value is the pairwise kernel matrix computed for that kernel. **Constraints:** - The function should support the following distance metrics: `euclidean`, `manhattan`, `cosine`. - The function should support the following kernels: `linear`, `polynomial`, `sigmoid`, `rbf`. **Performance Requirements:** - The function should handle input arrays `X` and `Y` with up to 10,000 vectors efficiently. **Example:** ```python import numpy as np from sklearn.metrics.pairwise import pairwise_distances, pairwise_kernels def compute_pairwise_matrices(X: np.ndarray, Y: np.ndarray, metrics: list, kernels: list) -> dict: result = {\'metrics\': {}, \'kernels\': {}} for metric in metrics: result[\'metrics\'][metric] = pairwise_distances(X, Y, metric=metric) for kernel in kernels: result[\'kernels\'][kernel] = pairwise_kernels(X, Y, metric=kernel) return result # Example usage X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) metrics = [\'euclidean\', \'manhattan\'] kernels = [\'linear\', \'rbf\'] output = compute_pairwise_matrices(X, Y, metrics, kernels) for key, value in output.items(): print(key, value) ``` Expected output should include dictionaries containing the computed matrices for each metric and kernel provided.","solution":"import numpy as np from sklearn.metrics.pairwise import pairwise_distances, pairwise_kernels def compute_pairwise_matrices(X: np.ndarray, Y: np.ndarray, metrics: list, kernels: list) -> dict: Compute pairwise distance and kernel matrices for given input vectors, metrics, and kernels. Parameters: X (np.ndarray): First set of vectors of shape (n_samples_X, n_features). Y (np.ndarray): Second set of vectors of shape (n_samples_Y, n_features). metrics (list): List of distance metrics to compute. kernels (list): List of kernel functions to compute. Returns: dict: A dictionary with keys \'metrics\' and \'kernels\' containing respective pairwise matrices. result = {\'metrics\': {}, \'kernels\': {}} for metric in metrics: result[\'metrics\'][metric] = pairwise_distances(X, Y, metric=metric) for kernel in kernels: result[\'kernels\'][kernel] = pairwise_kernels(X, Y, metric=kernel) return result"},{"question":"# Question: Asynchronous Download Manager You are tasked with implementing a basic download manager using the asyncio library. Your download manager should fetch data concurrently from multiple URLs and process this data asynchronously. Requirements: 1. Implement a function `fetch_data(url: str) -> str` that simulates an asynchronous download of data from a URL. In this simulation, use `asyncio.sleep` to mimic network latency. 2. Implement a function `process_data(data: str) -> str` that processes the downloaded data asynchronously. 3. Implement a function `download_manager(urls: List[str]) -> List[str]` that takes a list of URLs, fetches the data from each URL concurrently, processes the data, and returns a list of processed data results. Constraints: - A maximum of 5 concurrent download tasks should run at any time. - The content fetched from each URL should be a simple string containing the URL itself after a simulated delay. Input: - A list of URLs: `urls: List[str]` Output: - A list of processed data results: `List[str]` Use the following function signatures: ```python import asyncio from typing import List async def fetch_data(url: str) -> str: # Simulate downloading data pass async def process_data(data: str) -> str: # Simulate processing data pass async def download_manager(urls: List[str]) -> List[str]: # Manage the download and processing of data concurrently pass ``` Example: ```python urls = [ \\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\", \\"http://example.com/6\\", ] # Expected output (order may vary due to concurrency): # [\'Processed http://example.com/1\', \'Processed http://example.com/2\', ..., \'Processed http://example.com/6\'] results = asyncio.run(download_manager(urls)) print(results) ``` # Note: - Ensure to handle exceptions that may occur during fetching and processing of data. - You should use `asyncio.gather` or `asyncio.create_task` to manage concurrency. - You may use additional functions/helpers if necessary.","solution":"import asyncio from typing import List # Simulate downloading data with a given URL async def fetch_data(url: str) -> str: await asyncio.sleep(1) # Simulate network delay return url # Simulate processing fetched data async def process_data(data: str) -> str: await asyncio.sleep(0.5) # Simulate processing time return f\\"Processed {data}\\" # Download manager that fetches and processes data from multiple URLs concurrently async def download_manager(urls: List[str]) -> List[str]: semaphore = asyncio.Semaphore(5) async def limited_fetch_and_process(url): async with semaphore: data = await fetch_data(url) return await process_data(data) tasks = [limited_fetch_and_process(url) for url in urls] results = await asyncio.gather(*tasks) return results"},{"question":"Coding Assessment Question # Question Title: Memory Management and Boolean Handling in pandas # Objective: Demonstrate your understanding of pandas\' DataFrame memory usage reporting and Boolean operation handling. # Problem Statement: You are given a dataset simulating information about various items in a store, including item IDs, prices, stock quantities, restocking dates, and categories. Your task is to perform specific operations on this dataset to show detailed memory usage, properly handle missing values, and correctly apply Boolean logic to extract certain data subsets. # Instructions: 1. **DataFrame Memory Usage** - Load the given dataset and inspect its memory usage with and without detailed (`deep`) memory accounting. - Print out both memory usage reports. 2. **Handling Missing Values** - Identify missing values in the dataset and provide a cleaned version by filling in or properly handling any `NaN` or `None` values using appropriate pandas methods. - Convert any integer columns that may contain missing values to nullable integer types. 3. **Boolean Logic** - Extract and print all rows where the stock quantity is above average and restocking date is not null. - Ensure your solution appropriately handles the common pitfalls with Boolean operations using the methods `.any()`, `.all()`, or `.empty()` where required. # Input: A CSV file `store_data.csv` containing the dataset with the following columns: - `ItemID` (integer): Unique identifier for each item. - `Price` (float): Price of the item. - `StockQuantity` (integer): Number of items currently in stock. - `RestockDate` (datetime): Date when the item was restocked. - `Category` (string): Category to which the item belongs. # Expected Output: 1. Print statements for memory usage: - Memory usage report. - Detailed memory usage report (`memory_usage=\'deep\'`). 2. Cleaned DataFrame removing/handling missing values. 3. Filtered DataFrame where `StockQuantity` is above average and `RestockDate` is not null, displayed using proper Boolean logic handling. # Constraints: - Use pandas for DataFrame operations. - Ensure proper performance by avoiding unnecessary calculations or redundant data operations. # Example: ```python import pandas as pd # Step 1: Load the dataset df = pd.read_csv(\'store_data.csv\') # Step 2: Display memory usage reports print(\\"Memory usage report:\\") print(df.info()) print(\\"Detailed memory usage report:\\") print(df.info(memory_usage=\'deep\')) # Step 3: Handle missing values # (Example implementation might include df.fillna() or df.dropna()) df[\'StockQuantity\'] = df[\'StockQuantity\'].astype(pd.Int64Dtype()) df[\'RestockDate\'] = pd.to_datetime(df[\'RestockDate\']) df = df.dropna() # Step 4: Boolean handling - Filter rows based on conditions # Calculate the average stock quantity average_stock = df[\'StockQuantity\'].mean() # Filter the DataFrame using Boolean logic filtered_df = df[(df[\'StockQuantity\'] > average_stock) & (df[\'RestockDate\'].notna())] # Display the filtered DataFrame print(filtered_df) ``` # Notes: - Make sure to handle missing values appropriately without altering the original logical integrity of the dataset. - Use `.copy()` when necessary to avoid mutating the original data during UDF operations. - Ensure the filtered dataset output aligns with the specified Boolean conditions correctly.","solution":"import pandas as pd def manage_store_data(file_path): Load the dataset, display memory usage, handle missing values, and filter rows based on specific conditions. Parameters: file_path (str): Path to the CSV file containing the dataset. Returns: pd.DataFrame: Filtered DataFrame based on conditions. # Step 1: Load the dataset df = pd.read_csv(file_path) # Step 2: Display memory usage reports memory_usage_report = df.memory_usage() detailed_memory_usage_report = df.memory_usage(deep=True) print(\\"Memory usage report (normal):\\") print(memory_usage_report) print(\\"Detailed memory usage report (deep):\\") print(detailed_memory_usage_report) # Step 3: Handle missing values df[\'StockQuantity\'] = df[\'StockQuantity\'].astype(pd.Int64Dtype()) df[\'RestockDate\'] = pd.to_datetime(df[\'RestockDate\'], errors=\'coerce\') df = df.fillna({ \'Price\': df[\'Price\'].mean(), \'StockQuantity\': 0, \'RestockDate\': pd.Timestamp(\'1900-01-01\'), \'Category\': \'Unknown\' }) # Step 4: Boolean handling - Filter rows based on conditions average_stock = df[\'StockQuantity\'].mean() filtered_df = df[(df[\'StockQuantity\'] > average_stock) & (df[\'RestockDate\'] != pd.Timestamp(\'1900-01-01\'))] return filtered_df"},{"question":"# Question: Resource Limit Management and Reporting In this exercise, you are required to write a Python function that can manipulate and report system resource limits and usage information for a given process. We will test your function with various resource limits, and your function should handle these changes gracefully. **Function Signature:** ```python def manage_resource_limits(pid: int, resource_type: int, new_limits: tuple = None) -> dict: pass ``` # Parameters: 1. `pid` (int): The process ID for which resource limits are being managed. If `pid` is 0, the current process is targeted. 2. `resource_type` (int): The specific resource type to manage. This could be any of the constants defined in the \\"resource\\" module, such as `resource.RLIMIT_CPU`, `resource.RLIMIT_FSIZE`, etc. 3. `new_limits` (tuple, optional): A tuple (soft_limit, hard_limit) representing the new limits to be set for the given resource. If this parameter is `None`, the function should not change the limits but only report the current limits. # Return: - Returns a dictionary with keys \\"current_limits\\" and \\"usage_info\\". The value of \\"current_limits\\" is a tuple representing the current (soft_limit, hard_limit) of the specified resource. The value of \\"usage_info\\" is a dictionary detailing current resource usage based on `resource.getrusage()`. # Usage Information Dictionary Keys: - \\"user_time\\": Time spent in user mode (in seconds) - \\"system_time\\": Time spent in system mode (in seconds) - Any other relevant fields from the `getrusage()` output as you see fit. # Constraints: - Properly handle exceptions like `ValueError`, `ProcessLookupError`, and `PermissionError`. - Utilize the `resource` module functions `prlimit()` and `getrusage()` to achieve the function objectives. - Ensure that `new_limits` when provided do not violate system constraints and raise an appropriate error message if they do. # Example: ```python import resource def manage_resource_limits(pid: int, resource_type: int, new_limits: tuple = None) -> dict: # Your implementation here # Example usage: # Change limits for current process and report: try: result = manage_resource_limits(0, resource.RLIMIT_CPU, (10, 20)) print(result) except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Explanation:** 1. For the current process (`pid = 0`), manage the CPU time limit (`resource.RLIMIT_CPU`) setting the soft limit to 10 seconds and the hard limit to 20 seconds. 2. The output should include the current limits and resource usage information for user and system time along with other relevant metrics. Your task is to implement the `manage_resource_limits` function correctly, ensuring it adheres to the above specifications and handles all edge cases appropriately.","solution":"import resource def manage_resource_limits(pid: int, resource_type: int, new_limits: tuple = None) -> dict: try: if new_limits: # Set new resource limits resource.prlimit(pid, resource_type, new_limits) # Get current resource limits current_limits = resource.prlimit(pid, resource_type) # Get resource usage information usage = resource.getrusage(resource.RUSAGE_SELF) usage_info = { \\"user_time\\": usage.ru_utime, \\"system_time\\": usage.ru_stime, \\"max_rss\\": usage.ru_maxrss, \\"shared_memory\\": usage.ru_ixrss, \\"unshared_data\\": usage.ru_idrss, \\"unshared_stack\\": usage.ru_isrss, } return { \\"current_limits\\": current_limits, \\"usage_info\\": usage_info } except ValueError as ve: raise ve except ProcessLookupError as ple: raise ple except PermissionError as pe: raise pe"},{"question":"You are tasked with visualizing a dataset using Seaborn, ensuring that the plots are visually appealing and appropriately styled for different contexts. Write a function `styled_plots` that accomplishes the following: 1. Generates and plots a set of subplots demonstrating four distinct Seaborn figure styles. 2. Adjusts the context of one of the subplots to suit a presentation setting. 3. Applies custom styling to one of the subplots to override the default parameters. 4. For one of the subplots, removes the top and right spines to enhance the visual clarity. 5. Uses temporary styling changes within a `with` statement for one subplot to demonstrate nested configurations. # Function Signature ```python def styled_plots(): pass ``` # Requirements 1. **Input:** None. 2. **Output:** The function should display a figure containing multiple subplots with the described styling. 3. **Plot Styles:** - Subplot 1: Uses `darkgrid` style. - Subplot 2: Uses `whitegrid` style. - Subplot 3: Uses `dark` style, and the context is set to `talk`. - Subplot 4: Uses `ticks` style with custom parameters (`axes.facecolor`: `.85`). 4. **Customization and Spine Adjustments:** - For Subplot 2, remove the top and right spines. - For Subplot 3, ensure the context is set using `sns.set_context(\\"talk\\")`. 5. **Temporary Styling:** Use a `with` statement to temporarily set a different style for one subplot. 6. **Demonstrate the Seaborn functions:** `set_style`, `set_context`, `despine`, and temporary styling with `with`. # Constraints - Use the `sinplot` function from the documentation to generate the sample plots. - Ensure that all plots are part of a single figure for better comparison. # Example ```python def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) import numpy as np import seaborn as sns import matplotlib.pyplot as plt def styled_plots(): sns.set_theme() f = plt.figure(figsize=(10, 10)) gs = f.add_gridspec(2, 2) with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) sinplot(6) with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[0, 1]) sinplot(6) sns.despine() sns.set_style(\\"dark\\") sns.set_context(\\"talk\\") ax = f.add_subplot(gs[1, 0]) sinplot(6) sns.set_style(\\"ticks\\", {\\"axes.facecolor\\": \\".85\\"}) ax = f.add_subplot(gs[1, 1]) sinplot(6) f.tight_layout() plt.show() styled_plots() ``` # Performance - The function should efficiently switch between different styles and contexts, demonstrating the flexibility and power of Seaborn in enhancing plot aesthetics.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def styled_plots(): sns.set_theme() f = plt.figure(figsize=(12, 8)) gs = f.add_gridspec(2, 2) with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) sinplot(6) ax.set_title(\'Darkgrid Style\') with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[0, 1]) sinplot(6) sns.despine() ax.set_title(\'Whitegrid Style with Despine\') sns.set_style(\\"dark\\") sns.set_context(\\"talk\\") ax = f.add_subplot(gs[1, 0]) sinplot(6) ax.set_title(\'Dark Style with Talk Context\') sns.set_style(\\"ticks\\", {\\"axes.facecolor\\": \\".85\\"}) ax = f.add_subplot(gs[1, 1]) sinplot(6) ax.set_title(\'Ticks Style with Custom Background\') f.tight_layout() plt.show()"},{"question":"You are provided with a set of student records, where each record comprises attributes: `student_id`, `name`, `courses`, and `grades`. Your task is to implement several functions that manipulate these records using tuples and struct sequences. Specifications: 1. **Add Record**: ```python def add_record(records, student_id, name, courses, grades): Add a new student record to the records list. Parameters: - records (list of tuples): The list of student records. - student_id (int): The student\'s ID. - name (str): The student\'s name. - courses (tuple): The courses enrolled by the student. - grades (tuple): The grades received by the student in the respective courses. Returns: - list of tuples: Updated list with the new student record. ``` 2. **Get Student Info**: ```python def get_student_info(records, student_id): Retrieve the information of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The ID of the student to find. Returns: - tuple: The student record if found, else None. ``` 3. **Update Record**: ```python def update_record(records, student_id, new_name=None, new_courses=None, new_grades=None): Update the information of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The student ID corresponding to the record to update. - new_name (str): The new name of the student. (optional) - new_courses (tuple): The new set of courses for the student. (optional) - new_grades (tuple): The new set of grades for the student. (optional) Returns: - list of tuples: Updated list of records. ``` 4. **Delete Record**: ```python def delete_record(records, student_id): Delete the record of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The ID of the student whose record needs to be deleted. Returns: - list of tuples: Updated list after removing the student record. ``` 5. **List Top Students**: ```python def list_top_students(records, n=3): List the top `n` students based on their average grade. Parameters: - records (list of tuples): The list of student records. - n (int): The number of top students to list. Returns: - list of tuples: The records of the top `n` students sorted by average grade. ``` Tuple and Struct Sequence Handling: - Create and manipulate tuples for handling student records and courses. - Utilize struct sequences to define a robust data structure for student information. - Ensure the functions can handle cases where inputs may not be correctly formatted or if there are missing records. **Constraints:** - You must use tuples and struct sequences as provided in the documentation. - The functions should handle possible edge cases and errors gracefully. **Performance:** - Your implementation should be efficient in terms of both time and space complexity. - Aim to minimize the usage of expensive operations within the functions. # Example Usage: ```python students = [ (101, \\"Alice\\", (\\"Math\\", \\"Science\\"), (90, 95)), (102, \\"Bob\\", (\\"History\\", \\"Math\\"), (85, 80)), (103, \\"Charlie\\", (\\"Science\\", ), (70, )), ] # Adding a new student record students = add_record(students, 104, \\"David\\", (\\"English\\", \\"Math\\"), (88, 92)) # Fetching student info print(get_student_info(students, 102)) # Output: (102, \'Bob\', (\'History\', \'Math\'), (85, 80)) # Updating a student record students = update_record(students, 101, new_name=\\"Alicia\\") # Deleting a student record students = delete_record(students, 103) # Listing top 2 students top_students = list_top_students(students, 2) for student in top_students: print(student) ```","solution":"def add_record(records, student_id, name, courses, grades): Add a new student record to the records list. Parameters: - records (list of tuples): The list of student records. - student_id (int): The student\'s ID. - name (str): The student\'s name. - courses (tuple): The courses enrolled by the student. - grades (tuple): The grades received by the student in the respective courses. Returns: - list of tuples: Updated list with the new student record. new_record = (student_id, name, courses, grades) return records + [new_record] def get_student_info(records, student_id): Retrieve the information of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The ID of the student to find. Returns: - tuple: The student record if found, else None. for record in records: if record[0] == student_id: return record return None def update_record(records, student_id, new_name=None, new_courses=None, new_grades=None): Update the information of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The student ID corresponding to the record to update. - new_name (str): The new name of the student. (optional) - new_courses (tuple): The new set of courses for the student. (optional) - new_grades (tuple): The new set of grades for the student. (optional) Returns: - list of tuples: Updated list of records. updated_records = [] for record in records: if record[0] == student_id: new_record = ( student_id, new_name if new_name is not None else record[1], new_courses if new_courses is not None else record[2], new_grades if new_grades is not None else record[3], ) updated_records.append(new_record) else: updated_records.append(record) return updated_records def delete_record(records, student_id): Delete the record of a student given their ID. Parameters: - records (list of tuples): The list of student records. - student_id (int): The ID of the student whose record needs to be deleted. Returns: - list of tuples: Updated list after removing the student record. return [record for record in records if record[0] != student_id] def list_top_students(records, n=3): List the top `n` students based on their average grade. Parameters: - records (list of tuples): The list of student records. - n (int): The number of top students to list. Returns: - list of tuples: The records of the top `n` students sorted by average grade. def average_grade(grades): return sum(grades) / len(grades) sorted_records = sorted(records, key=lambda record: average_grade(record[3]), reverse=True) return sorted_records[:n]"},{"question":"Objective Your task is to demonstrate your understanding of the `pickletools` module by implementing a function that performs several operations on pickles, including disassembly, opcode iteration, and optimization. Problem Statement Implement a function `analyze_and_optimize_pickle(pickled_data: bytes) -> dict` that takes a pickled byte string and returns a dictionary with the following keys and values: 1. **original_disassembly**: A string containing the disassembly of the original pickled data using `pickletools.dis`. 2. **opcodes_count**: An integer indicating the total number of opcodes in the original pickled data using `pickletools.genops`. 3. **optimized_disassembly**: A string containing the disassembly of the optimized pickled data using `pickletools.optimize`. Input - `pickled_data` (bytes): A byte string containing the pickled data. Output - dict: A dictionary with the keys `original_disassembly`, `opcodes_count`, and `optimized_disassembly`, according to the description above. Constraints - You may assume that the `pickled_data` is a valid pickle. - Use a default indentation level of 4 spaces and annotate disassembly output. Example ```python import pickle import pickletools def analyze_and_optimize_pickle(pickled_data: bytes) -> dict: import io from pickletools import dis, genops, optimize # Disassemble the original pickle original_disassembly_io = io.StringIO() dis(pickled_data, out=original_disassembly_io, annotate=True) original_disassembly = original_disassembly_io.getvalue() # Count opcodes in the original pickle opcodes_count = sum(1 for _ in genops(pickled_data)) # Optimize the pickle optimized_pickle = optimize(pickled_data) # Disassemble the optimized pickle optimized_disassembly_io = io.StringIO() dis(optimized_pickle, out=optimized_disassembly_io, annotate=True) optimized_disassembly = optimized_disassembly_io.getvalue() return { \\"original_disassembly\\": original_disassembly, \\"opcodes_count\\": opcodes_count, \\"optimized_disassembly\\": optimized_disassembly, } # Example usage: data = pickle.dumps((1, 2, 3)) result = analyze_and_optimize_pickle(data) print(result) ``` **Note**: The example usage demonstrates how to use the function with some pickled data. The output will provide the disassembled original and optimized pickle data and the count of opcodes in the original pickle.","solution":"import io import pickletools def analyze_and_optimize_pickle(pickled_data: bytes) -> dict: Performs operations on pickled data including disassembly, opcode counting, and optimization. Parameters: pickled_data (bytes): A byte string containing the pickled data. Returns: dict: A dictionary with \'original_disassembly\', \'opcodes_count\', and \'optimized_disassembly\' as keys. # Disassemble the original pickle original_disassembly_io = io.StringIO() pickletools.dis(pickled_data, out=original_disassembly_io, annotate=True) original_disassembly = original_disassembly_io.getvalue() # Count opcodes in the original pickle opcodes_count = sum(1 for _ in pickletools.genops(pickled_data)) # Optimize the pickle optimized_pickle = pickletools.optimize(pickled_data) # Disassemble the optimized pickle optimized_disassembly_io = io.StringIO() pickletools.dis(optimized_pickle, out=optimized_disassembly_io, annotate=True) optimized_disassembly = optimized_disassembly_io.getvalue() return { \\"original_disassembly\\": original_disassembly, \\"opcodes_count\\": opcodes_count, \\"optimized_disassembly\\": optimized_disassembly, }"},{"question":"In this coding assessment, you are required to demonstrate your comprehension of Python\'s `resource` module by performing the following tasks: 1. Write a function `current_resource_limits` that takes a resource constant (e.g., `resource.RLIMIT_NOFILE`) as an argument and returns a tuple of the current soft and hard limits for that resource. 2. Write a function `set_resource_limit` that takes a resource constant and a tuple `(soft, hard)` to set new resource limits. Handle all potential exceptions and ensure the function can correctly set the limits without violating any constraints (e.g., the soft limit should not exceed the hard limit). 3. Write a function `detailed_resource_usage` to retrieve and return a dictionary object that describes the resources consumed by the calling process. The dictionary should include keys for each of the fields described in the result of `resource.getrusage(resource.RUSAGE_SELF)`: - \\"ru_utime\\" - \\"ru_stime\\" - \\"ru_maxrss\\" - \\"ru_ixrss\\" - \\"ru_idrss\\" - \\"ru_isrss\\" - \\"ru_minflt\\" - \\"ru_majflt\\" - \\"ru_nswap\\" - \\"ru_inblock\\" - \\"ru_oublock\\" - \\"ru_msgsnd\\" - \\"ru_msgrcv\\" - \\"ru_nsignals\\" - \\"ru_nvcsw\\" - \\"ru_nivcsw\\" # Function Signatures ```python import resource def current_resource_limits(resource_type: int) -> tuple: Given a resource constant, returns a tuple (soft, hard) with the current soft and hard limits for that resource. Args: resource_type (int): The resource constant (e.g., resource.RLIMIT_CPU). Returns: tuple: The current soft and hard limits of the resource. pass def set_resource_limit(resource_type: int, limits: tuple) -> None: Given a resource constant and a tuple (soft, hard), sets new resource limits. Args: resource_type (int): The resource constant (e.g., resource.RLIMIT_CPU). limits (tuple): A tuple (soft, hard) representing the new limits. Returns: None pass def detailed_resource_usage() -> dict: Retrieves detailed resource usage information for the calling process. Returns: dict: A dictionary with fields describing resource usage. pass ``` # Example Usage ```python import resource # Getting current limits for number of open files print(current_resource_limits(resource.RLIMIT_NOFILE)) # Output: (1024, 4096) # Setting a new limit for number of open files try: set_resource_limit(resource.RLIMIT_NOFILE, (2048, 4096)) print(\\"Resource limit updated successfully.\\") except ValueError as e: print(f\\"Failed to update resource limit: {e}\\") # Retrieving detailed resource usage information usage = detailed_resource_usage() print(usage[\\"ru_utime\\"]) print(usage[\\"ru_stime\\"]) ``` # Constraints - Ensure that the `set_resource_limit` function correctly handles invalid limits and raises appropriate exceptions. - Only set the soft limit if it is less than or equal to the hard limit. - You are not allowed to increase the hard limit unless you are running the script as a super-user. Good luck, and make sure to test your functions thoroughly!","solution":"import resource def current_resource_limits(resource_type: int) -> tuple: Given a resource constant, returns a tuple (soft, hard) with the current soft and hard limits for that resource. Args: resource_type (int): The resource constant (e.g., resource.RLIMIT_CPU). Returns: tuple: The current soft and hard limits of the resource. return resource.getrlimit(resource_type) def set_resource_limit(resource_type: int, limits: tuple) -> None: Given a resource constant and a tuple (soft, hard), sets new resource limits. Args: resource_type (int): The resource constant (e.g., resource.RLIMIT_CPU). limits (tuple): A tuple (soft, hard) representing the new limits. Returns: None soft, hard = limits current_soft, current_hard = resource.getrlimit(resource_type) if soft > hard: raise ValueError(\\"Soft limit cannot be greater than hard limit.\\") if hard > current_hard: raise PermissionError(\\"Cannot increase hard limit unless running as super-user.\\") resource.setrlimit(resource_type, (soft, hard)) def detailed_resource_usage() -> dict: Retrieves detailed resource usage information for the calling process. Returns: dict: A dictionary with fields describing resource usage. usage = resource.getrusage(resource.RUSAGE_SELF) return { \\"ru_utime\\": usage.ru_utime, \\"ru_stime\\": usage.ru_stime, \\"ru_maxrss\\": usage.ru_maxrss, \\"ru_ixrss\\": usage.ru_ixrss, \\"ru_idrss\\": usage.ru_idrss, \\"ru_isrss\\": usage.ru_isrss, \\"ru_minflt\\": usage.ru_minflt, \\"ru_majflt\\": usage.ru_majflt, \\"ru_nswap\\": usage.ru_nswap, \\"ru_inblock\\": usage.ru_inblock, \\"ru_oublock\\": usage.ru_oublock, \\"ru_msgsnd\\": usage.ru_msgsnd, \\"ru_msgrcv\\": usage.ru_msgrcv, \\"ru_nsignals\\": usage.ru_nsignals, \\"ru_nvcsw\\": usage.ru_nvcsw, \\"ru_nivcsw\\": usage.ru_nivcsw }"},{"question":"**Coding Assessment Question** # Objective The goal of this assessment is to test your understanding of importing modules and managing module imports using Python\'s `importlib` library. # Problem Statement Given the deprecated usage of the `imp` module in the function below, your task is to: 1. Identify and replace the deprecated `imp` functions with their equivalents from the `importlib` library. # Deprecated Implementation Here\'s a function that loads a module and returns it, using the deprecated `imp` module: ```python import imp import sys def load_module_deprecated(module_name): try: file, pathname, description = imp.find_module(module_name) try: return imp.load_module(module_name, file, pathname, description) finally: if file: file.close() except ImportError: return None ``` # Your Task Write a function `load_module(module_name)` that performs the same functionality as `load_module_deprecated` but uses the `importlib` library instead of the deprecated `imp` module. # Input - `module_name` (string): A string representing the name of the module to be imported. # Output - The module object, if the module can be successfully imported. - `None` if the module cannot be imported. # Constraints 1. You should not use any functions from the `imp` module. 2. Assume `module_name` will be a valid string for a module name. # Example ```python # Using the deprecated function mod = load_module_deprecated(\\"math\\") # <module \'math\' (built-in)> # Using your implemented function mod = load_module(\\"math\\") # <module \'math\' (built-in)> ``` **Notes**: - Your function should manage any exceptions that occur during the import process and return `None` if the module cannot be imported. - Make sure to close any files as necessary. # Solution Template ```python import importlib.util import sys def load_module(module_name): try: spec = importlib.util.find_spec(module_name) if spec is None: return None module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module except ImportError: return None # Example usage: mod = load_module(\\"math\\") print(mod) # <module \'math\' (built-in)> ```","solution":"import importlib.util import sys def load_module(module_name): Load a module by name using the importlib library. Args: module_name (str): The name of the module to load. Returns: module: The loaded module object, or None if the module cannot be loaded. try: spec = importlib.util.find_spec(module_name) if spec is None: return None module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module except Exception: return None"},{"question":"# HMAC Implementation and Validation You are tasked with implementing a function that generates an HMAC digest of a given message using a specified key and hashing algorithm. You will then validate this digest against a provided expected digest to ensure the integrity and authenticity of the message. Requirements: - **Function Name:** `generate_and_validate_hmac` - **Input:** - `key` (str): The secret key used for HMAC generation. - `message` (str): The message to be hashed. - `algorithm` (str): The hashing algorithm to be used (e.g., \'sha256\'). - `expected_digest` (str): The expected HMAC digest in hexadecimal format. - **Output:** - `bool`: `True` if the generated HMAC digest matches the expected digest, `False` otherwise. Constraints: - You must use the `hmac` module for HMAC generation and validation. - The key must be converted to bytes before usage. - The message must be updated in the HMAC object. - The comparison of the calculated and expected digests should use a secure comparison method to prevent timing attacks. Example: ```python def generate_and_validate_hmac(key, message, algorithm, expected_digest): pass # Example usage: key = \\"my_secret_key\\" message = \\"This is a confidential message.\\" algorithm = \\"sha256\\" expected_digest = \\"7d8acd... (some hex digest string)\\" result = generate_and_validate_hmac(key, message, algorithm, expected_digest) print(result) # Should output True or False based on the validation ``` # Notes: - Properly handle encoding of the key and message as required by the `hmac` module. - Ensure to use `hmac.compare_digest` for secure comparison of digests.","solution":"import hmac import hashlib def generate_and_validate_hmac(key, message, algorithm, expected_digest): Generates an HMAC digest of the given message using the specified key and algorithm, and validates it against the expected digest. Parameters: - key: (str) The secret key used for HMAC generation. - message: (str) The message to be hashed. - algorithm: (str) The hashing algorithm to be used (e.g., \'sha256\'). - expected_digest: (str) The expected HMAC digest in hexadecimal format. Returns: - bool: True if the generated HMAC digest matches the expected digest, False otherwise. # Convert key and message to bytes key_bytes = key.encode(\'utf-8\') message_bytes = message.encode(\'utf-8\') # Create the HMAC object h = hmac.new(key_bytes, message_bytes, getattr(hashlib, algorithm)) # Generate the HMAC digest generated_digest = h.hexdigest() # Validate the generated digest against the expected digest return hmac.compare_digest(generated_digest, expected_digest)"},{"question":"**Coding Assessment Question: Evaluating Regression Models with Custom Scoring** You are given a dataset containing features and a target variable that you need to predict using a regression model. Your task is to: 1. Implement and train a regression model using scikit-learn. 2. Create a custom scoring function that calculates the Mean Squared Logarithmic Error (MSLE). 3. Evaluate the performance of your regression model using this custom scoring function during cross-validation. **Instructions:** 1. **Load the dataset:** Use `sklearn.datasets.make_regression` to generate a regression dataset with the following parameters: - `n_samples=200`, `n_features=5`, `noise=0.1`, `random_state=42`. 2. **Implement a Regression Model:** - Use the `sklearn.linear_model.LinearRegression` class to create a regression model and fit it to the dataset. 3. **Custom Scoring Function:** - Create a custom scoring function to calculate the Mean Squared Logarithmic Error (MSLE). - Use `sklearn.metrics.make_scorer` to integrate this function with scikit-learn’s cross-validation tools. 4. **Evaluate the Model:** - Use `sklearn.model_selection.cross_val_score` to evaluate your model with the custom MSLE scoring function. - Perform a 5-fold cross-validation and report the mean and standard deviation of the cross-validation scores. 5. **Output:** - Print the mean and standard deviation of the MSLE scores from the cross-validation. **Constraints:** - Use a random state of 42 wherever applicable for reproducibility. - Ensure that the custom scoring function computes the MSLE in a vectorized form for efficiency. **Example:** ```python import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_log_error # Step 1: Load the dataset X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42) y = np.abs(y) # Ensure targets are non-negative as required for MSLE # Step 2: Implement a Regression Model model = LinearRegression() # Step 3: Custom Scoring Function def custom_msle(y_true, y_pred): return mean_squared_log_error(y_true, y_pred) msle_scorer = make_scorer(custom_msle, greater_is_better=False) # Step 4: Evaluate the Model cv_scores = cross_val_score(model, X, y, cv=5, scoring=msle_scorer) # Step 5: Output print(f\'Mean MSLE: {np.mean(cv_scores)}, Std Dev MSLE: {np.std(cv_scores)}\') ```","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_log_error # Step 1: Load the dataset X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42) y = np.abs(y) # Ensure targets are non-negative as required for MSLE # Step 2: Implement a Regression Model model = LinearRegression() # Step 3: Custom Scoring Function def custom_msle(y_true, y_pred): return mean_squared_log_error(y_true, y_pred) msle_scorer = make_scorer(custom_msle, greater_is_better=False) # Step 4: Evaluate the Model cv_scores = cross_val_score(model, X, y, cv=5, scoring=msle_scorer) # Step 5: Output print(f\'Mean MSLE: {np.mean(cv_scores)}, Std Dev MSLE: {np.std(cv_scores)}\') # Storing the result to use in unit tests mean_msle = np.mean(cv_scores) std_dev_msle = np.std(cv_scores)"},{"question":"You are tasked with creating a Python extension module that can be initialized using the multi-phase initialization strategy explained in the provided documentation. Your module should include the following features: 1. **Module Name**: `mymodule` 2. **A Single Integer Attribute**: `version`, initialized to 310. 3. **A Single String Attribute**: `description`, initialized to `\\"Python 3.10 Custom Module\\"`. 4. **A Custom Function**: `greet(name: str) -> str`: Takes a name as input and returns a greeting message as `\\"Hello, {name}!\\"`. Requirements: 1. **Create Module**: - Define the module using the `PyModuleDef` structure. - Implement multi-phase initialization with a create function and an exec function. 2. **Add Module Attributes**: - Use the appropriate functions to attach `version` and `description` to the module. 3. **Define `greet` Function**: - Implement the `greet` function and add it to the module\'s method definitions. 4. **Initialization**: - Ensure that the module is properly initialized in both creation and execution phases. Expected Input and Output: - Expected module structure after initialization: ```python import mymodule assert mymodule.version == 310 assert mymodule.description == \\"Python 3.10 Custom Module\\" assert mymodule.greet(\\"Alice\\") == \\"Hello, Alice!\\" ``` Constraints: - Follow the provided specifications and use multi-phase initialization as outlined. - Ensure that all module state and definitions align with best practices for memory management and error handling as described. Performance: - The module should be robust and handle multiple imports and deletions without leaks or errors. Implementation: ```c #include <Python.h> // Function to greet a user static PyObject* greet(PyObject* self, PyObject* args) { const char* name; if (!PyArg_ParseTuple(args, \\"s\\", &name)) { return NULL; } return PyUnicode_FromFormat(\\"Hello, %s!\\", name); } // Method definitions static PyMethodDef mymodule_methods[] = { {\\"greet\\", greet, METH_VARARGS, \\"Greet someone by name\\"}, {NULL, NULL, 0, NULL} // Sentinel }; // Module create function static PyObject* mymodule_create(PyObject* spec, PyModuleDef* def) { PyObject* module = PyModule_Create(def); if (!module) return NULL; // Add integer version attribute if (PyModule_AddIntConstant(module, \\"version\\", 310) < 0) return NULL; // Add string description attribute if (PyModule_AddStringConstant(module, \\"description\\", \\"Python 3.10 Custom Module\\") < 0) return NULL; return module; } // Module exec function static int mymodule_exec(PyObject* module) { // Initialize module state if needed return 0; // Success } // Module definition static PyModuleDef_Slot mymodule_slots[] = { {Py_mod_create, mymodule_create}, {Py_mod_exec, mymodule_exec}, {0, NULL} // Sentinel }; static struct PyModuleDef mymodule_module = { PyModuleDef_HEAD_INIT, \\"mymodule\\", // Module name \\"A custom module for Python 3.10\\", // Module docstring 0, // m_size mymodule_methods, // Module methods mymodule_slots, // m_slots NULL, // m_traverse NULL, // m_clear NULL // m_free }; // Module initialization PyMODINIT_FUNC PyInit_mymodule(void) { return PyModuleDef_Init(&mymodule_module); } ```","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"# Advanced PyTorch Serialization and Deserialization Objective: To assess the student\'s understanding of PyTorch\'s serialization and deserialization functionalities, including saving/loading tensors, handling tensor views, saving/loading model states, and using TorchScript to save `torch.nn.Module`. Task: You are required to perform the following tasks: 1. **Saving and Loading Tensors with Views:** - Create two tensors where one is a view of the other. - Save these tensors to a file. - Load the tensors back and verify that the view relationship is preserved. - Alter the loaded view tensor and check if it changes the original tensor due to the shared storage. 2. **Managing Storage Size:** - Create a large tensor and then create a small tensor as a slice of the large tensor. - Save the small tensor using two methods: a) directly saving it, b) saving a cloned version of it. - Load both and verify the storage size of the loaded tensors to illustrate the difference. 3. **State Dict Management:** - Define a custom neural network module that consists of at least two submodules. - Save the state dict of the module. - Load the state dict into a newly created instance of the module and verify state dict equality. 4. **Using TorchScript:** - Convert your custom neural network module to a TorchScript module using `torch.jit.script`. - Save the scripted module. - Load the module back and verify it retains its functionality. Requirements: - You should provide concise comments to explain your code. - Ensure to include verification steps that confirm the correctness of each step. Constraints: - Use only PyTorch functionalities as described. - Handle potential errors that might occur during serialization and deserialization. Input and Output: **Input Format:** No specific input is needed from the user as the operations are internal. **Output Format:** Prints/verifications that demonstrate the successful completion of each task. Provide a structured solution in a single Python file.","solution":"import torch import torch.nn as nn # Task 1: Saving and Loading Tensors with Views def save_load_tensors_with_views(): # Create a tensor original_tensor = torch.randn(4, 4) # Create a view of the original tensor view_tensor = original_tensor[:, 1] # Save tensors torch.save({\'original_tensor\': original_tensor, \'view_tensor\': view_tensor}, \'tensors_with_views.pth\') # Load tensors loaded = torch.load(\'tensors_with_views.pth\') loaded_original_tensor = loaded[\'original_tensor\'] loaded_view_tensor = loaded[\'view_tensor\'] # Verify the view relationship is preserved loaded_view_tensor[0] = 10 assert loaded_original_tensor[0, 1] == 10, \\"The view relationship was not preserved.\\" return loaded_original_tensor, loaded_view_tensor # Task 2: Managing Storage Size def manage_storage_size(): large_tensor = torch.randn(100, 100) small_tensor = large_tensor[:, 0] cloned_small_tensor = small_tensor.clone() # Save small tensor directly and the cloned version torch.save(small_tensor, \'small_tensor.pth\') torch.save(cloned_small_tensor, \'cloned_small_tensor.pth\') # Load tensors loaded_small_tensor = torch.load(\'small_tensor.pth\') loaded_cloned_small_tensor = torch.load(\'cloned_small_tensor.pth\') # Verify the storage size original_storage_size = small_tensor.storage().size() cloned_storage_size = cloned_small_tensor.storage().size() loaded_storage_size = loaded_small_tensor.storage().size() loaded_cloned_storage_size = loaded_cloned_small_tensor.storage().size() assert loaded_storage_size == original_storage_size, \\"Storage size of the directly saved tensor is incorrect.\\" assert loaded_cloned_storage_size == cloned_storage_size, \\"Storage size of the cloned tensor is incorrect.\\" return loaded_storage_size, loaded_cloned_storage_size # Task 3: State Dict Management class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = nn.Linear(10, 10) self.layer2 = nn.Linear(10, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) return x def state_dict_management(): model = SimpleNet() # Save the state dict of the model torch.save(model.state_dict(), \'simple_net_state_dict.pth\') # Load the state dict into a new model instance new_model = SimpleNet() new_model.load_state_dict(torch.load(\'simple_net_state_dict.pth\')) # Verify state dict equality for key_item in model.state_dict().items(): assert torch.equal(new_model.state_dict()[key_item[0]], key_item[1]), f\\"Mismatch in state_dict for {key_item[0]}\\" return True # Task 4: Using TorchScript def torchscript_usage(): model = SimpleNet() scripted_model = torch.jit.script(model) # Save the scripted model scripted_model.save(\'scripted_model.pth\') # Load the scripted model loaded_scripted_model = torch.jit.load(\'scripted_model.pth\') # Verify functionality input_tensor = torch.randn(1, 10) original_output = model(input_tensor) loaded_output = loaded_scripted_model(input_tensor) assert torch.equal(original_output, loaded_output), \\"TorchScript model output does not match original model output.\\" return True"},{"question":"# Custom Data Structure Implementation with Special Methods Problem: In this exercise, you are required to implement a custom data structure called `Vector`. This should encapsulate a list of numbers and offer arithmetic operations similar to what you would find in mathematical vectors. Specifically, you need to ensure that this class correctly handles addition, subtraction, and scalar multiplication operations, both in regular and augmented forms. Additionally, the class should be equipped to handle vector norm calculation, string representation, and indexing operations. Direct attribute access should be controlled to prevent external modifications to the internal list. Requirements: 1. **Vector Class**: - Initialize with a list of numbers. - Implement the following special methods: - `__add__(self, other)` - `__sub__(self, other)` - `__mul__(self, scalar)` - `__iadd__(self, other)` - `__isub__(self, other)` - `__imul__(self, scalar)` - `__repr__(self)` - `__str__(self)` - `__getitem__(self, index)` - `__setitem__(self, index, value)` - Implement custom methods: - `norm(self)` – Calculate and return the Euclidean norm of the vector. - Use `__getattr__()` and `__setattr__()` to manage direct access to internal attributes. Expected Input and Output: - **Initialization**: ```python v = Vector([1, 2, 3]) ``` - **Addition Operation**: ```python w = Vector([4, 5, 6]) z = v + w # should return Vector([5, 7, 9]) ``` - **Subtraction Operation**: ```python z = v - w # should return Vector([-3, -3, -3]) ``` - **Scalar Multiplication Operation**: ```python z = v * 2 # should return Vector([2, 4, 6]) ``` - **Augmented Assignment Operations**: ```python v += w # should alter `v` to be Vector([5, 7, 9]) ``` - **Norm Calculation**: ```python v.norm() # should return value 9.1104335791443 (sqrt(5^2 + 7^2 + 9^2)) ``` - **Indexing Operations**: ```python v[1] # should return 7 v[1] = 10 # should set the second element to 10 ``` - **String Representation**: ```python print(v) # should print Vector([5, 10, 9]) ``` Constraints: - Ensure that the operations handle edge cases such as incompatible vector sizes or invalid index access. - Ensure that vector operations return a new `Vector` instance when necessary. - In-place operations should modify the instance itself. Implementation: ```python import math class Vector: def __init__(self, elements): self._elements = elements def __add__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") return Vector([a + b for a, b in zip(self._elements, other._elements)]) def __sub__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") return Vector([a - b for a, b in zip(self._elements, other._elements)]) def __mul__(self, scalar): return Vector([a * scalar for a in self._elements]) def __iadd__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") self._elements = [a + b for a, b in zip(self._elements, other._elements)] return self def __isub__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") self._elements = [a - b for a, b in zip(self._elements, other._elements)] return self def __imul__(self, scalar): self._elements = [a * scalar for a in self._elements] return self def norm(self): return math.sqrt(sum(x * x for x in self._elements)) def __repr__(self): return f\\"Vector({self._elements})\\" def __str__(self): return f\\"Vector({self._elements})\\" def __getitem__(self, index): return self._elements[index] def __setitem__(self, index, value): self._elements[index] = value def __getattr__(self, name): if name.startswith(\'_\'): raise AttributeError(f\\"Attribute \'{name}\' is private and cannot be accessed directly\\") return object.__getattribute__(self, name) def __setattr__(self, name, value): if name.startswith(\'_\'): object.__setattr__(self, name, value) else: raise AttributeError(f\\"Attribute \'{name}\' cannot be set directly\\") ``` Ensure that your implementation handles all the specifications.","solution":"import math class Vector: def __init__(self, elements): self._elements = elements def __add__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") return Vector([a + b for a, b in zip(self._elements, other._elements)]) def __sub__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") return Vector([a - b for a, b in zip(self._elements, other._elements)]) def __mul__(self, scalar): return Vector([a * scalar for a in self._elements]) def __iadd__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") self._elements = [a + b for a, b in zip(self._elements, other._elements)] return self def __isub__(self, other): if len(self._elements) != len(other._elements): raise ValueError(\\"Vectors must be of same length\\") self._elements = [a - b for a, b in zip(self._elements, other._elements)] return self def __imul__(self, scalar): self._elements = [a * scalar for a in self._elements] return self def norm(self): return math.sqrt(sum(x * x for x in self._elements)) def __repr__(self): return f\\"Vector({self._elements})\\" def __str__(self): return f\\"Vector({self._elements})\\" def __getitem__(self, index): return self._elements[index] def __setitem__(self, index, value): self._elements[index] = value def __getattr__(self, name): if name.startswith(\'_\'): raise AttributeError(f\\"Attribute \'{name}\' is private and cannot be accessed directly\\") return object.__getattribute__(self, name) def __setattr__(self, name, value): if name.startswith(\'_\'): object.__setattr__(self, name, value) else: raise AttributeError(f\\"Attribute \'{name}\' cannot be set directly\\")"},{"question":"**Coding Challenge: International Event Scheduler** **Objective:** You are required to design a function that schedules an international event for a company. The event must be scheduled such that it accommodates attendees from three different time zones and ensures that the event time does not clash with any timezone’s night hours (defined as 10 PM to 6 AM in each participant’s local time). **Requirements:** 1. Write a function `schedule_event` that accepts the following parameters: - `date`: A string in the format \'YYYY-MM-DD\' representing the event\'s date. - `start_time`: A string in the format \'HH:MM\' representing the proposed start time in the company\'s local time. - `company_timezone`: A string representing the timezone of the company (e.g., \'UTC\', \'America/New_York\'). - `attendee_timezones`: A list of strings representing the timezones of the attendees (e.g., [\'Europe/London\', \'Asia/Tokyo\', \'America/Los_Angeles\']). 2. The function should return a dictionary where the keys are the attendees\' timezones and the values are the scheduled event times in their local time as strings in the format `YYYY-MM-DD HH:MM`. 3. The function should ensure: - The event time, when converted to each attendee\'s local time, does not fall between 10 PM and 6 AM. - If the proposed `start_time` falls within this period for any timezone, adjust the start time by advancing it in 30-minute increments until a valid time is found for all timezones. **Constraints:** - Use the `datetime`, `timezone`, and `timedelta` classes from the `datetime` module. - The function should handle daylight saving time transitions correctly. - Assume that valid time zone strings are provided. **Example:** ```python def schedule_event(date, start_time, company_timezone, attendee_timezones): pass # Example Usage event_date = \'2023-11-15\' proposed_start = \'21:00\' company_tz = \'America/New_York\' attendees_tz = [\'Europe/London\', \'Asia/Tokyo\', \'America/Los_Angeles\'] scheduled_times = schedule_event(event_date, proposed_start, company_tz, attendees_tz) print(scheduled_times) ``` **Expected Output:** The output should be a dictionary containing event times adjusted to the local times of attendees without violating the night hours constraint. For example: ```python { \'Europe/London\': \'2023-11-15 03:00\', \'Asia/Tokyo\': \'2023-11-15 12:00\', \'America/Los_Angeles\': \'2023-11-14 18:00\' } ``` The provided example assumes the event was adjusted such that it does not clash with any attendee’s night hours. **Notes:** - Consider edge cases such as daylight saving transitions. - Ensure that the date adjustments maintain the original event date for the company\'s time zone. - Use the latest IANA time zone database available in the `zoneinfo` module.","solution":"from datetime import datetime, timedelta from zoneinfo import ZoneInfo def is_night_time(local_time): if 22 <= local_time.hour or local_time.hour < 6: return True return False def schedule_event(date, start_time, company_timezone, attendee_timezones): company_datetime = datetime.strptime(f\\"{date} {start_time}\\", \\"%Y-%m-%d %H:%M\\").replace(tzinfo=ZoneInfo(company_timezone)) valid_time = False while not valid_time: valid_time = True for timezone in attendee_timezones: attendee_time = company_datetime.astimezone(ZoneInfo(timezone)) if is_night_time(attendee_time): company_datetime += timedelta(minutes=30) valid_time = False break scheduled_event_times = {tz: company_datetime.astimezone(ZoneInfo(tz)).strftime(\\"%Y-%m-%d %H:%M\\") for tz in attendee_timezones} return scheduled_event_times"},{"question":"You are provided with a dataset containing health expenditure data from various countries over the years. Your task is to preprocess this data and create a series of visualizations using the `seaborn.objects` module. Your final output should include visualizations that make effective use of facets, colors, stacking, and other customizations. Dataset The dataset is already included in the seaborn package and can be loaded as follows: ```python from seaborn import load_dataset healthexp = load_dataset(\\"healthexp\\") ``` Preprocessing Requirements: 1. **Pivot the data** so that the index is \'Year\', the columns are \'Country\', and the values are \'Spending_USD\'. 2. **Interpolate missing values** in the dataset. 3. **Stack** the pivoted data, rename the resulting series to \'Spending_USD\', reset the index, and sort by \'Country\'. Visualization Requirements: 1. **Facet Plot**, generating one plot for each country: - Use the preprocessed data. - Create a facet plot with `Year` on the x-axis and `Spending_USD` on the y-axis. - Wrap the facets to display three plots per row. 2. **Color Customization**: - Add an area plot to the facet plot colored by \'Country\'. 3. **Edge Customization**: - Create another facet plot similar to the first one but with the area plot having a fixed fill color and an edge color based on \'Country\'. 4. **Shaded Region with Line**: - Draw a shaded region with `so.Area()` combined with a line using `so.Line()`, with the area having `edgewidth=0`. 5. **Orientation and Stacking**: - Create a plot where `Spending_USD` is on the x-axis and `Year` is on the y-axis. - Add a stacked area plot showing part-whole relationships with transparency set to 0.7. Expected Code Implementation: Implement the above steps in Python using the seaborn objects as described. Ensure that each plot is clear and correctly implemented as per the requirements. You can use the following preamble: ```python import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Facet Plot - Standard p1 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p1.add(so.Area()) p1.show() # Facet Plot - Color Customization p2 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p2.add(so.Area(), color=\\"Country\\") p2.show() # Facet Plot - Edge Customization p3 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p3.add(so.Area(color=\\".5\\", edgewidth=2), edgecolor=\\"Country\\") p3.show() # Shaded Region with Line p4 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\") p4.add(so.Area(edgewidth=0)).add(so.Line()) p4.show() # Orientation and Stacking p5 = so.Plot(healthexp, \\"Spending_USD\\", \\"Year\\", color=\\"Country\\") p5.add(so.Area(alpha=0.7), so.Stack()) p5.show() ``` Ensure that the plots are properly rendered and show the required information as specified in the requirements. Constraints: - You must use the seaborn library and the seaborn.objects module for all visualizations. - Ensure that the code is well-commented and organized.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) def create_standard_facet_plot(data): p1 = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p1.add(so.Area()) return p1 def create_color_customization_facet_plot(data): p2 = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p2.add(so.Area(), color=\\"Country\\") return p2 def create_edge_customization_facet_plot(data): p3 = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p3.add(so.Area(color=\\".5\\", edgewidth=2), edgecolor=\\"Country\\") return p3 def create_shaded_region_with_line_plot(data): p4 = so.Plot(data, \\"Year\\", \\"Spending_USD\\") p4.add(so.Area(edgewidth=0)).add(so.Line()) return p4 def create_orientation_and_stacking_plot(data): p5 = so.Plot(data, \\"Spending_USD\\", \\"Year\\", color=\\"Country\\") p5.add(so.Area(alpha=0.7), so.Stack()) return p5"},{"question":"# Python Coding Assessment **Objective:** Assess the candidate\'s ability to work with date and time handling functions from the Python datetime module. **Problem Statement:** Implement a Python function `process_date_objects(dates)`. This function will receive a list of tuples, where each tuple contains 3 integers representing a year, month, and day. The function should: 1. Validate each tuple to ensure that it represents a valid date. 2. Convert each tuple into a `datetime.date` object. 3. Sort these date objects in ascending order. 4. Return a list of dictionaries, each containing the year, month, and day components of the sorted date objects. # Function Signature ```python def process_date_objects(dates: list) -> list: Processes a list of date tuples into sorted date dictionaries. Parameters: dates (list of tuples): List where each tuple contains three integers: year, month, day. Returns: List of dictionaries: Sorted date objects in dictionaries with year, month, and day keys. pass ``` # Input - `dates` : A list of tuples. Each tuple contains three elements. - `year` : Integer (1 <= year <= 9999) - `month` : Integer (1 <= month <= 12) - `day` : Integer (1 <= day <= 31) # Output - A list of dictionaries, each containing three key-value pairs: `year`, `month`, `day` in ascending order of the date they represent. # Constraints - If any tuple is invalid and cannot represent a date, it should be skipped. - The function should handle edge cases such as leap years, month boundaries, etc. - The timezone information (if any) should be ignored. # Example ```python assert process_date_objects([(2023, 10, 15), (2020, 2, 29), (2021, 12, 31), (2019, 11, 1)]) == [ {\'year\': 2019, \'month\': 11, \'day\': 1}, {\'year\': 2020, \'month\': 2, \'day\': 29}, {\'year\': 2021, \'month\': 12, \'day\': 31}, {\'year\': 2023, \'month\': 10, \'day\': 15} ] assert process_date_objects([(2023, 10, 32), (2020, 2, 30), (2021, 12, 32), (2019, 11, 1)]) == [ {\'year\': 2019, \'month\': 11, \'day\': 1} ] ``` # Notes - Remember to test your function with various edge cases, including leap years, invalid dates, and minimum/maximum value boundaries. - Ensure proper exception handling to skip invalid dates and keep the function robust. **Performance Requirements:** The solution should efficiently handle a reasonably large list of date tuples, say up to 10,000 tuples, and the implementation should adhere to good coding practices.","solution":"from datetime import date def process_date_objects(dates): Processes a list of date tuples into sorted date dictionaries. Parameters: dates (list of tuples): List where each tuple contains three integers: year, month, day. Returns: List of dictionaries: Sorted date objects in dictionaries with year, month, and day keys. valid_dates = [] # Validate dates and convert to date objects for year, month, day in dates: try: d = date(year, month, day) valid_dates.append(d) except ValueError: continue # Sort the valid date objects valid_dates.sort() # Convert date objects to list of dictionaries sorted_dates = [{\'year\': d.year, \'month\': d.month, \'day\': d.day} for d in valid_dates] return sorted_dates"},{"question":"Objective You are required to implement a Python function to dynamically import a module, execute a function from that module with given arguments, and handle any exceptions that may arise during this process. This task will test your understanding of dynamic importing, argument parsing, and exception handling in Python. Problem Statement Write a function `execute_dynamic_function(module_name: str, func_name: str, *args) -> any` that performs the following: 1. Dynamically imports the module specified by `module_name`. 2. Executes the function specified by `func_name` in the imported module, passing in the `args` as arguments to that function. 3. Returns the result of the function call. 4. Catches and handles any exceptions that occur during the import or execution process, returning a string message describing the error. Input - `module_name` (str): The name of the module to be imported. - `func_name` (str): The name of the function to be executed from the imported module. - `*args`: The arguments to be passed to the function. Output - Returns the result of the function execution, or a string describing any error encountered. Constraints - Assume that the function name provided always exists in the specified module. - The arguments are valid for the specified function. Example ```python result = execute_dynamic_function(\'math\', \'sqrt\', 16) print(result) # Output: 4.0 error_result = execute_dynamic_function(\'nonexistent_module\', \'sqrt\', 16) print(error_result) # Output: \'Error: No module named \'nonexistent_module\'\' ``` Requirements 1. Your function should use Python\'s dynamic import capabilities. 2. It should correctly parse and pass arguments to the specified function. 3. Proper exception handling must be in place for the import and function execution steps. Performance The function should handle imports and function calls efficiently, but no specific performance constraints are enforced beyond typical execution speeds for dynamic imports and function calls in Python. ```python def execute_dynamic_function(module_name: str, func_name: str, *args) -> any: try: # Dynamically import the module module = __import__(module_name) # Retrieve the function from the module func = getattr(module, func_name) # Execute the function with provided arguments return func(*args) except Exception as e: # Handle exceptions and return error message return f\\"Error: {str(e)}\\" # Example usage result = execute_dynamic_function(\'math\', \'sqrt\', 16) print(result) # Expected Output: 4.0 error_result = execute_dynamic_function(\'nonexistent_module\', \'sqrt\', 16) print(error_result) # Expected Output: \'Error: No module named \'nonexistent_module\'\' ```","solution":"def execute_dynamic_function(module_name: str, func_name: str, *args) -> any: try: # Dynamically import the module module = __import__(module_name) # Retrieve the function from the module func = getattr(module, func_name) # Execute the function with provided arguments return func(*args) except Exception as e: # Handle exceptions and return error message return f\\"Error: {str(e)}\\""},{"question":"# Question: PyCapsule Manipulation and Validation Background: You have been given a task to implement a C extension module using the `PyCapsule` mechanism provided in Python. Your goal is to encapsulate a C pointer and manipulate its attributes using Python code. You will also validate and retrieve information from the capsule. Task: 1. **Create a PyCapsule:** - Implement a Python function `create_capsule(pointer: int, name: str)` that takes an integer representing a C pointer (`void *`) and a string `name`, and returns a `PyCapsule` object encapsulating the pointer. 2. **Set Attributes:** - Implement a Python function `set_capsule_attributes(capsule, context: int, destructor: callable)` that sets the context and destructor of an existing `PyCapsule`. 3. **Validate and Retrieve Information:** - Implement a Python function `validate_and_retrieve(capsule, name: str) -> (bool, dict)` that verifies the validity of the capsule using `PyCapsule_IsValid` and retrieves its internal attributes (pointer, name, context, and destructor). The function should return a tuple where the first element is a boolean indicating validity and the second element is a dictionary containing the capsule\'s attributes if valid. Constraints: - The `pointer` and `context` should be non-null values. - You may assume that the name provided during capsule creation will always be a non-empty string. - The destructor, if provided, will appropriately handle the capsule without causing segmentation faults. Performance Requirement: The function calls should handle capsules efficiently, ensuring minimal overhead in validation and attribute retrieval. ```python # Example skeleton for reference def create_capsule(pointer: int, name: str): # Your implementation here pass def set_capsule_attributes(capsule, context: int, destructor: callable): # Your implementation here pass def validate_and_retrieve(capsule, name: str) -> (bool, dict): # Your implementation here pass # Example usage: if __name__ == \\"__main__\\": capsule = create_capsule(123456, \\"example.attribute\\") set_capsule_attributes(capsule, 789012, None) valid, attributes = validate_and_retrieve(capsule, \\"example.attribute\\") print(f\\"Capsule Valid: {valid}\\") print(f\\"Attributes: {attributes}\\") ``` In your implementation, ensure you make use of the `PyCapsule` API methods provided in the documentation to create, manipulate, and validate capsules. Good luck!","solution":"import ctypes import builtins # Simulated `PyCapsule` API for the example class PyCapsule: def __init__(self, pointer, name): self.pointer = pointer self.name = name self.context = None self.destructor = None def create_capsule(pointer: int, name: str): Create a PyCapsule object encapsulating the pointer and name. if not isinstance(pointer, int) or not pointer: raise ValueError(\\"Invalid pointer provided\\") if not isinstance(name, str) or not name: raise ValueError(\\"Invalid name provided\\") return PyCapsule(pointer, name) def set_capsule_attributes(capsule, context: int, destructor: callable): Set context and destructor of an existing PyCapsule. if not isinstance(capsule, PyCapsule): raise ValueError(\\"Invalid capsule provided\\") if not isinstance(context, int) or not context: raise ValueError(\\"Invalid context provided\\") capsule.context = context capsule.destructor = destructor def validate_and_retrieve(capsule, name: str) -> (bool, dict): Validate the capsule and retrieve its attributes. if not isinstance(capsule, PyCapsule): return False, {} valid = capsule.name == name attributes = { \\"pointer\\": capsule.pointer, \\"name\\": capsule.name, \\"context\\": capsule.context, \\"destructor\\": capsule.destructor, } return valid, attributes"},{"question":"# NIS Module Exercise Background You are working as a system administrator in a UNIX environment and you have been tasked with creating a Python script to interact with the NIS (Network Information Service) to retrieve and display specific information. Task Write a Python function called `retrieve_nis_info` that performs the following steps: 1. Fetch all available NIS maps in the system\'s default NIS domain using the `nis.maps()` function. 2. For each map, retrieve all the key-value pairs using the `nis.cat(mapname)` function. 3. Combine all the key-value pairs from all maps into a single dictionary named `combined_nis_data`. 4. Print out `combined_nis_data` where each key-value pair is printed on a new line in the format `\\"key: value\\"`. Function Signature ```python def retrieve_nis_info() -> None: pass ``` Example Output If the NIS maps contain the following data: - Map `passwd.byname` has entries `(\'alice\': \'x:1000:1000:Alice:/home/alice:/bin/bash\')`. - Map `group.byname` has entries `(\'admins\': \'x:1000:\')`. Your `combined_nis_data` dictionary would be: ```python { \'alice\': \'x:1000:1000:Alice:/home/alice:/bin/bash\', \'admins\': \'x:1000:\' } ``` Printed output: ``` alice: x:1000:1000:Alice:/home/alice:/bin/bash admins: x:1000: ``` Constraints - The function must handle any `nis.error` exceptions that occur during the execution and print a meaningful error message. Additional Requirements - Ensure your solution is robust and handles potential edge cases, such as no maps available or empty data in maps.","solution":"import nis def retrieve_nis_info() -> None: Fetch all available NIS maps, retrieve their key-value pairs, and print each key-value pair in the format \'key: value\'. try: # Get all available NIS maps maps = nis.maps() # Initialize an empty dictionary to hold combined NIS data combined_nis_data = {} # Iterate over each map and retrieve key-value pairs for mapname in maps: # Retrieve the key-value pairs for the current map map_data = nis.cat(mapname) # Combine the retrieved data into the combined_nis_data dictionary combined_nis_data.update(map_data) # Print out the combined NIS data for key, value in combined_nis_data.items(): print(f\\"{key}: {value}\\") except nis.error as e: print(f\\"An error occurred while retrieving NIS information: {e}\\")"},{"question":"# Question: Implement and Compare Clustering Algorithms Objective You are required to implement and compare three different clustering algorithms from scikit-learn: K-Means, DBSCAN, and Agglomerative Clustering. The comparison should be based on different metrics such as the Adjusted Rand Index, Silhouette Score, and Calinski-Harabasz Index for a given dataset. Instructions 1. **Data Loading and Preprocessing** - Use the `load_iris` dataset from `sklearn.datasets`. - Standardize the dataset using `StandardScaler` from `sklearn.preprocessing`. 2. **Clustering Implementation** - Implement K-Means clustering with `n_clusters=3`. - Implement DBSCAN with `eps=0.5` and `min_samples=5`. - Implement Agglomerative Clustering with `n_clusters=3`. 3. **Evaluation Metrics** - Evaluate the clustering performance for each algorithm using: - Adjusted Rand Index. - Silhouette Score. - Calinski-Harabasz Index. 4. **Comparison** - Print out the evaluation metrics for each algorithm. - Compare the results and discuss which algorithm performs best based on the metrics. Code Template ```python import numpy as np import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score # Load and standardize the dataset data = load_iris() X = StandardScaler().fit_transform(data.data) # Implement K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(X) # Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(X) # Implement Agglomerative Clustering agglo = AgglomerativeClustering(n_clusters=3) agglo_labels = agglo.fit_predict(X) # Evaluate the clustering performance metrics = {} metrics[\'KMeans\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, kmeans_labels), \'Silhouette Score\': silhouette_score(X, kmeans_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, kmeans_labels) } metrics[\'DBSCAN\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, dbscan_labels), \'Silhouette Score\': silhouette_score(X, dbscan_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, dbscan_labels) } metrics[\'Agglomerative\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, agglo_labels), \'Silhouette Score\': silhouette_score(X, agglo_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, agglo_labels) } # Print the evaluation metrics for each algorithm for algo, scores in metrics.items(): print(f\\"--- {algo} ---\\") for metric, score in scores.items(): print(f\\"{metric}: {score}\\") print() # Analysis and Discussion print(\\"Based on the provided metrics, discuss which algorithm performs best and explain why.\\") ``` Expected Output The solution should output the scores for each metric for all the three clustering algorithms and provide a brief discussion on which algorithm performs best based on these metrics. Constraints - Use only the provided dataset and specified clustering algorithms. - Ensure the code is optimized and follows best practices for readability and performance.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score def clustering_evaluation(): # Load and standardize the dataset data = load_iris() X = StandardScaler().fit_transform(data.data) # Implement K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(X) # Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(X) # Implement Agglomerative Clustering agglo = AgglomerativeClustering(n_clusters=3) agglo_labels = agglo.fit_predict(X) # Evaluate the clustering performance metrics = {} metrics[\'KMeans\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, kmeans_labels), \'Silhouette Score\': silhouette_score(X, kmeans_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, kmeans_labels) } metrics[\'DBSCAN\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, dbscan_labels), \'Silhouette Score\': silhouette_score(X, dbscan_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, dbscan_labels) } metrics[\'Agglomerative\'] = { \'Adjusted Rand Index\': adjusted_rand_score(data.target, agglo_labels), \'Silhouette Score\': silhouette_score(X, agglo_labels), \'Calinski-Harabasz Index\': calinski_harabasz_score(X, agglo_labels) } return metrics"},{"question":"You are tasked with ensuring the presence of the latest version of the `pip` installer in a given environment using the `ensurepip` package. Implement a Python function `ensure_latest_pip` that ensures `pip` is installed and up-to-date in either the current environment, system-wide, or a specified root directory. Function Signature: ```python def ensure_latest_pip(root: str = None, upgrade: bool = True, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0) -> str: pass ``` Input: - `root` (optional, str): An alternative root directory to install `pip` relative to. Default is `None`, meaning the default install location for the current environment. - `upgrade` (optional, bool): Indicates whether or not to upgrade an existing installation of `pip` to the available version. Default is `True`. - `user` (optional, bool): Indicates whether to use the user scheme rather than installing globally. Default is `False`. - `altinstall` (optional, bool): If set to `True`, the script `pipX` will *not* be installed. Default is `False`. - `default_pip` (optional, bool): If set to `True`, the script `pip` will also be installed. Default is `False`. - `verbosity` (optional, int): Controls the level of output from the bootstrapping process. Default is `0`. Output: - Returns a string that indicates the status of the operation, such as \\"pip installed\\", \\"pip updated\\", or any relevant error messages. Constraints: - If both `altinstall` and `default_pip` are `True`, your function should return a string that indicates an error, rather than raising an exception. Example Usage: ```python print(ensure_latest_pip()) # Output: \\"pip installed\\" print(ensure_latest_pip(upgrade=False, verbosity=1)) # Output: \\"pip already up-to-date\\" (assuming pip was already installed) print(ensure_latest_pip(root=\\"/path/to/root\\", default_pip=True)) # Output: \\"pip and pipX.Y installed at /path/to/root\\" ``` Notes: - Utilize the `ensurepip.bootstrap` function along with the other details specified in the documentation to complete this task. - Ensure that the function handles and returns appropriate messages for different scenarios described in the constraints and variations of the function usage.","solution":"import ensurepip import subprocess def ensure_latest_pip(root: str = None, upgrade: bool = True, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0) -> str: if altinstall and default_pip: return \\"Error: Cannot have both altinstall and default_pip set to True\\" try: # Check existing pip version existing_pip_version = subprocess.run([\'pip\', \'--version\'], capture_output=True, text=True) existing_pip_installed = existing_pip_version.returncode == 0 # Bootstrap pip if necessary args = [\'--upgrade\'] if upgrade else [] if root: args.extend([\'--root\', root]) if user: args.append(\'--user\') if altinstall: args.append(\'--altinstall\') if default_pip: args.append(\'--default-pip\') if verbosity > 0: args.append(f\'-{\\"v\\" * verbosity}\') ensurepip.bootstrap(args) # Check final pip version to assess the update/installation status final_pip_version = subprocess.run([\'pip\', \'--version\'], capture_output=True, text=True) if existing_pip_installed: if upgrade and existing_pip_version.stdout != final_pip_version.stdout: return \\"pip updated\\" else: return \\"pip already up-to-date\\" else: return \\"pip installed\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"**Question: Implement a Bi-directional Conversion Utility using `quopri` Module** **Objective:** You are required to implement a utility that facilitates the bi-directional conversion—both encoding and decoding—of given quoted-printable data using Python\'s `quopri` module. **Function Specifications:** 1. **Function Name:** `bi_directional_conversion` 2. **Input:** - `operation` (string): Specifies the operation to perform. It can be either \\"encode\\" or \\"decode\\". - `data` (bytes): The data to be encoded or decoded. - `quotetabs` (bool, optional): Applicable only when `operation` is \\"encode\\". This flag is to control encoding of spaces and tabs within the content (default value is `False`). - `header` (bool, optional): A flag for both encoding and decoding. For encoding, it controls if spaces are encoded as underscores. For decoding, it controls if underscores are decoded as spaces (default value is `False`). 3. **Output:** - Returns the result as bytes which is either encoded or decoded data based on the operation specified. **Constraints:** - You must use the appropriate functions from the `quopri` module to accomplish the encoding and decoding. - Ensure that input data is always in bytes format. - Proper handling of optional flags `quotetabs` and `header`. **Example:** ```python def bi_directional_conversion(operation, data, quotetabs=False, header=False): # Your implementation here # Encode example input_data = b\'Hello World!\' result = bi_directional_conversion(\'encode\', input_data, quotetabs=True, header=True) print(result) # Expected output: (encoded bytes) # Decode example encoded_data = b\'SGVsbG8gV29ybGQh\' result = bi_directional_conversion(\'decode\', encoded_data, header=False) print(result) # Expected output: b\'Hello World!\' ``` **Explanation:** - For encoding, it processes the input bytes into the quoted-printable format using the `quopri.encodestring` function. - For decoding, it converts quoted-printable encoded bytes back into their original form using the `quopri.decodestring` function. - The `quotetabs` and `header` flags modify the encoding and decoding behavior according to the user\'s inputs. Write the function `bi_directional_conversion()` implementing the above specifications.","solution":"import quopri def bi_directional_conversion(operation, data, quotetabs=False, header=False): Perform bidirectional conversion using the quopri module. Parameters: - operation: \'encode\' or \'decode\' - data: bytes to be encoded or decoded - quotetabs: controls encoding of spaces and tabs (only for encoding) - header: controls if spaces are encoded/decoded as underscores Returns: - Encoded or decoded bytes based on the operation if operation == \'encode\': return quopri.encodestring(data, quotetabs=quotetabs, header=header) elif operation == \'decode\': return quopri.decodestring(data, header=header) else: raise ValueError(\\"Operation must be \'encode\' or \'decode\'\\")"},{"question":"# PyTorch Optimization Question Objective: Write a PyTorch function that takes a batch of input data and performs a series of operations, ensuring that the operations are optimized based on the conditions specified. Your function should check if the conditions for the persistent algorithm are satisfied and, if they are, apply the appropriate optimizations. Input: - A PyTorch tensor `input_data` of shape `(batch_size, num_features)` where `batch_size` and `num_features` are arbitrary. - A boolean `use_gpu` which indicates whether to use GPU for computations. - A boolean `use_cudnn` which indicates whether cuDNN is enabled. Output: - A PyTorch tensor of shape `(batch_size, output_features)` after performing the operations. Constraints: - The function should ensure the operations are performed using `torch.float16` if necessary. - The function should move the tensor to a V100 GPU if available and if `use_gpu` is True. - The input tensor should not be in `PackedSequence` format. - If all the specified conditions are met, the function should make use of any available optimizations. Instructions: 1. Verify if the input conditions for optimizations are satisfied. 2. If the conditions are met, convert the tensor to `torch.float16` and move it to GPU if specified. 3. Perform a sample operation: Matrix multiplication of the input tensor with a random weight tensor `W` of compatible dimensions. 4. Return the result of the operation. ```python import torch def optimized_tensor_operations(input_data, use_gpu=False, use_cudnn=False): Optimize tensor operations based on specified conditions. Arguments: input_data (torch.Tensor): The input tensor of shape (batch_size, num_features). use_gpu (bool): Whether to use GPU for computations. use_cudnn (bool): Whether cuDNN is enabled. Returns: torch.Tensor: The resulting tensor of shape (batch_size, output_features). # Check if input_data is in PackedSequence format if isinstance(input_data, torch.nn.utils.rnn.PackedSequence): raise ValueError(\\"input_data should not be in PackedSequence format\\") # Check if cudnn is enabled cudnn_enabled = use_cudnn # Check if the input data is on the GPU if use_gpu and torch.cuda.is_available(): # Check if a V100 GPU is being used device_name = torch.cuda.get_device_name(0) if \\"V100\\" in device_name: v100_gpu = True else: v100_gpu = False else: cudnn_enabled = False v100_gpu = False # Move tensor to GPU and convert to float16 if all conditions are satisfied if cudnn_enabled and v100_gpu: input_data = input_data.cuda().to(torch.float16) elif use_gpu and torch.cuda.is_available(): input_data = input_data.cuda() # Perform an example matrix multiplication operation output_features = input_data.size(1) + 10 # Output features for the example operation W = torch.randn(input_data.size(1), output_features, dtype=input_data.dtype, device=input_data.device) result = torch.matmul(input_data, W) return result # Example Usage input_data = torch.randn(32, 128) result = optimized_tensor_operations(input_data, use_gpu=True, use_cudnn=True) print(result) ``` Note: - Ensure the input tensor is not in PackedSequence format. - Test the function with both GPU and non-GPU environments, and with cuDNN enabled and disabled.","solution":"import torch def optimized_tensor_operations(input_data, use_gpu=False, use_cudnn=False): Optimize tensor operations based on specified conditions. Arguments: input_data (torch.Tensor): The input tensor of shape (batch_size, num_features). use_gpu (bool): Whether to use GPU for computations. use_cudnn (bool): Whether cuDNN is enabled. Returns: torch.Tensor: The resulting tensor of shape (batch_size, output_features). # Check if input_data is in PackedSequence format if isinstance(input_data, torch.nn.utils.rnn.PackedSequence): raise ValueError(\\"input_data should not be in PackedSequence format\\") # Check if cudnn is enabled cudnn_enabled = use_cudnn # Check if the input data is on the GPU if use_gpu and torch.cuda.is_available(): # Check if a V100 GPU is being used device_name = torch.cuda.get_device_name(0) if \\"V100\\" in device_name: v100_gpu = True else: v100_gpu = False else: cudnn_enabled = False v100_gpu = False # Move tensor to GPU and convert to float16 if all conditions are satisfied if cudnn_enabled and v100_gpu: input_data = input_data.cuda().to(torch.float16) elif use_gpu and torch.cuda.is_available(): input_data = input_data.cuda() # Perform an example matrix multiplication operation output_features = input_data.size(1) + 10 # Output features for the example operation W = torch.randn(input_data.size(1), output_features, dtype=input_data.dtype, device=input_data.device) result = torch.matmul(input_data, W) return result"},{"question":"**Coding Assessment Question:** # Objective: You are tasked with creating a utility function using the `resource` module in Python. This utility function will help in monitoring and managing system resources for a given process. # Problem Statement: Implement a function `resource_manager` that accepts a PID (process ID) and performs the following tasks: 1. Retrieves and prints the current CPU time limit for the given process. 2. Sets a new CPU time limit if the current soft limit is less than a given threshold. 3. Retrieves and prints the resource usage information for the given process. # Function Signature: ```python def resource_manager(pid: int, cpu_limit_threshold: int) -> None: pass ``` # Input: - `pid (int)`: The process ID of the target process. If `pid` is 0, the function should operate on the current process. - `cpu_limit_threshold (int)`: The new soft limit for CPU time in seconds if the current CPU time soft limit is below this threshold. # Output: - The function should print the current CPU time limits (soft and hard). - Print the updated CPU time limits (soft and hard) if changes were made. - Print the resource usage information for the specified process. # Constraints: - Ensure that the new soft limit does not exceed the hard limit. - Handle `ValueError` and `OSError` exceptions appropriately. - The function must work on Linux systems with Python 3.4+. # Example: ```python def example_usage(): import os current_pid = os.getpid() resource_manager(current_pid, 10) # The function call should: # 1. Print the current CPU time limits. # 2. Set new CPU time limit if current soft limit is less than 10 seconds and print the updated limits. # 3. Print resource usage information for the current process. ``` # Notes: 1. You may refer to the resource module documentation to utilize `getrlimit`, `setrlimit`, and `getrusage` methods. 2. Always ensure proper exception handling for robust code. Good luck!","solution":"import resource import os def resource_manager(pid: int, cpu_limit_threshold: int) -> None: try: # If pid is 0, operate on the current process if pid == 0: pid = os.getpid() # Get the current CPU time limits (soft and hard) current_limits = resource.getrlimit(resource.RLIMIT_CPU) soft_limit, hard_limit = current_limits print(f\\"Current CPU time limits: Soft={soft_limit}, Hard={hard_limit}\\") # If the current soft limit is less than the given threshold, update it if soft_limit < cpu_limit_threshold: new_soft_limit = min(cpu_limit_threshold, hard_limit) resource.setrlimit(resource.RLIMIT_CPU, (new_soft_limit, hard_limit)) print(f\\"Updated CPU time limits: Soft={new_soft_limit}, Hard={hard_limit}\\") # Get the resource usage information for the specified process usage = resource.getrusage(resource.RUSAGE_SELF if pid == os.getpid() else pid) print(f\\"Resource usage for PID {pid}:\\") print(f\\"User CPU time used: {usage.ru_utime} seconds\\") print(f\\"System CPU time used: {usage.ru_stime} seconds\\") print(f\\"Maximum resident set size: {usage.ru_maxrss}\\") except ValueError as ve: print(f\\"ValueError encountered: {ve}\\") except OSError as oe: print(f\\"OSError encountered: {oe}\\")"},{"question":"**Objective**: Implement a scikit-learn pipeline to calibrate classifier probabilities, and evaluate the improvement in probability predictions. **Problem Statement**: You are provided with a classification dataset. Your task is to: 1. Train a `RandomForestClassifier` on the dataset. 2. Evaluate and plot the calibration curve of the uncalibrated classifier. 3. Use `CalibratedClassifierCV` to calibrate the classifier probabilities using both `sigmoid` and `isotonic` methods. 4. Evaluate and plot the calibration curves of the calibrated classifiers. 5. Compare the Brier score and log-loss for the original and calibrated classifiers. **Dataset**: Assume that the dataset has been loaded into a DataFrame `df` with feature columns `X1, X2, ... Xn` and a target column `y` for binary classification (with classes 0 and 1). Split the data into a training set (80%) and a testing set (20%). **Requirements**: 1. Implement the function `evaluate_calibration` ```python def evaluate_calibration(df): Evaluates and plots the calibration curves for original and calibrated classifiers. Parameters: df (DataFrame): DataFrame containing feature columns `X1, X2, ... Xn` and a target column `y`. Returns: None ``` **Implementation details**: - Train a `RandomForestClassifier` on the training set. - Evaluate and plot the calibration curve of the uncalibrated classifier. - Calibrate the classifier using `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods. - Evaluate and plot the calibration curves for the calibrated classifiers. - Compare the Brier score and log-loss for the original and calibrated classifiers and print the results. 2. Make use of `train_test_split` from `sklearn.model_selection` for splitting the data. 3. Use `calibration_curve` from `sklearn.calibration` to get calibration curve values. 4. Utilize `sklearn.metrics` for calculating the Brier score and log-loss. **Plot details**: - Plot a graph with calibration curves for the uncalibrated classifier and both calibrated classifiers. - Ensure the x-axis represents the average predicted probability in each bin, and the y-axis represents the fraction of positive samples in that bin. - Additionally, include a histogram to show the number of samples in each predicted probability bin. **Example usage**: ```python # Assuming df is the DataFrame with necessary data evaluate_calibration(df) ``` **Expected Output**: - Display plots showing the calibration curves for the original, sigmoid-calibrated, and isotonic-calibrated classifiers. - Print Brier scores and log-loss for each classifier. Constraints: - The dataset should have at least 1000 samples to avoid overfitting in isotonic regression. - Ensure that all classes are present in each fold during cross-validation.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.metrics import brier_score_loss, log_loss def evaluate_calibration(df): Evaluates and plots the calibration curves for original and calibrated classifiers. Parameters: df (DataFrame): DataFrame containing feature columns `X1, X2, ... Xn` and a target column `y`. Returns: None # Split the DataFrame into features and target variable X = df.drop(columns=[\'y\']) y = df[\'y\'] # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train the RandomForestClassifier rf = RandomForestClassifier(random_state=42) rf.fit(X_train, y_train) # Uncalibrated predictions y_prob_rf = rf.predict_proba(X_test)[:, 1] # Calibrate with isotonic and sigmoid methods isotonic = CalibratedClassifierCV(rf, method=\'isotonic\', cv=\'prefit\') sigmoid = CalibratedClassifierCV(rf, method=\'sigmoid\', cv=\'prefit\') isotonic.fit(X_train, y_train) sigmoid.fit(X_train, y_train) y_prob_isotonic = isotonic.predict_proba(X_test)[:, 1] y_prob_sigmoid = sigmoid.predict_proba(X_test)[:, 1] # Calculate calibration curves prob_true_rf, prob_pred_rf = calibration_curve(y_test, y_prob_rf, n_bins=10) prob_true_isotonic, prob_pred_isotonic = calibration_curve(y_test, y_prob_isotonic, n_bins=10) prob_true_sigmoid, prob_pred_sigmoid = calibration_curve(y_test, y_prob_sigmoid, n_bins=10) # Plot calibration curves plt.figure(figsize=(10, 10)) plt.plot(prob_pred_rf, prob_true_rf, \'s-\', label=\'RandomForestClassifier\') plt.plot(prob_pred_isotonic, prob_true_isotonic, \'s-\', label=\'Isotonic calibration\') plt.plot(prob_pred_sigmoid, prob_true_sigmoid, \'s-\', label=\'Sigmoid calibration\') plt.plot([0, 1], [0, 1], \'k:\', label=\'Perfectly calibrated\') plt.xlabel(\'Mean predicted probability\') plt.ylabel(\'Fraction of positives\') plt.legend() plt.title(\'Calibration plots (reliability curve)\') plt.show() # Print Brier score and Log-loss for each classifier print(\'Brier score (No calibration):\', brier_score_loss(y_test, y_prob_rf)) print(\'Brier score (Isotonic calibration):\', brier_score_loss(y_test, y_prob_isotonic)) print(\'Brier score (Sigmoid calibration):\', brier_score_loss(y_test, y_prob_sigmoid)) print(\'Log-loss (No calibration):\', log_loss(y_test, y_prob_rf)) print(\'Log-loss (Isotonic calibration):\', log_loss(y_test, y_prob_isotonic)) print(\'Log-loss (Sigmoid calibration):\', log_loss(y_test, y_prob_sigmoid))"},{"question":"**Objective:** To assess students\' understanding of file handling, pattern matching, and script automation in Python. **Problem Statement:** You are tasked with creating a simplified version of the `sdist` command used in the Python packaging ecosystem. Your tool should be able to generate a manifest of files to include in a source distribution based on specified patterns, similar to how `MANIFEST.in` works. Your application will work in two stages: 1. **Parse Instructions:** Read inclusion and exclusion patterns from a given manifest template file. 2. **Generate Manifest:** Apply the rules to generate a manifest file listing all files to be included in the distribution. **Function Signature:** ```python def generate_manifest(manifest_template: str, source_dir: str) -> List[str]: Generates a manifest list of files to include in a source distribution. :param manifest_template: The path to the file containing inclusion/exclusion patterns. :param source_dir: The root directory of the source tree to be inspected. :return: A list of file paths to be included in the distribution, sorted alphabetically. pass ``` # Input: 1. `manifest_template` - A string representing the file path to the manifest template (similar to MANIFEST.in) containing inclusion and exclusion patterns. 2. `source_dir` - A string representing the root directory of the source tree where files are to be included/excluded. # Output: - Return a list of strings, each representing a file path. The file paths should be relative to `source_dir` and sorted alphabetically. # Constraints: 1. The manifest template file contains one command per line. 2. Commands include: - `include pattern`: Includes files matching the pattern. - `recursive-include dir pattern`: Recursively includes files matching the pattern in the specified directory. - `prune pattern`: Excludes directories matching the pattern. 3. Patterns can use shell-style wildcards (`*` for any number of characters, `?` for a single character). 4. Assume basic file system operations and wildcard patterns are available and work similarly to Unix. 5. Handle edge cases such as overlapping inclusion/exclusion patterns intelligently. # Examples: **Example 1:** Manifest file (`manifest_template`): ``` include *.txt recursive-include src *.py prune test ``` Source directory structure: ``` src/ main.py utils.py README.txt test/ test_main.py test_utils.py ``` **Expected Output:** ```python [ \\"README.txt\\", \\"src/main.py\\", \\"src/utils.py\\", ] ``` **Example 2:** Manifest file (`manifest_template`): ``` include *.md recursive-include doc *.rst ``` Source directory structure: ``` doc/ introduction.rst setup.rst README.md CHANGELOG.md LICENSE ``` **Expected Output:** ```python [ \\"CHANGELOG.md\\", \\"README.md\\", \\"doc/introduction.rst\\", \\"doc/setup.rst\\", ] ``` # Notes: - You do not need to implement the actual archiving of files; focus solely on generating the manifest list. - Assume the implemented function will be used in a controlled environment with proper file system structure matching the examples.","solution":"import os import glob def generate_manifest(manifest_template: str, source_dir: str) -> list: Generates a manifest list of files to include in a source distribution. :param manifest_template: The path to the file containing inclusion/exclusion patterns. :param source_dir: The root directory of the source tree to be inspected. :return: A list of file paths to be included in the distribution, sorted alphabetically. includes = [] excludes = [] with open(manifest_template, \'r\') as file: lines = file.readlines() for line in lines: line = line.strip() if line.startswith(\'include \'): pattern = line[len(\'include \'):].strip() includes += glob.glob(os.path.join(source_dir, pattern), recursive=False) elif line.startswith(\'recursive-include \'): parts = line[len(\'recursive-include \'):].strip().split() if len(parts) == 2: directory, pattern = parts search_path = os.path.join(source_dir, directory, \'**\', pattern) includes += glob.glob(search_path, recursive=True) elif line.startswith(\'prune \'): directory = line[len(\'prune \'):].strip() search_path = os.path.join(source_dir, directory, \'**\') excludes += glob.glob(search_path, recursive=True) # Remove excluded items from includes includes = set(includes) includes -= set(excludes) # Convert paths to be relative to source_dir and sort manifest = [os.path.relpath(path, source_dir) for path in includes] manifest.sort() return manifest"},{"question":"Objective: Demonstrate your understanding of the PyTorch `torch.fft` module by implementing a function that performs a series of FFT operations, including normalization and shifting, on a given 2D input signal. Problem Description: Write a Python function using PyTorch to perform the following operations on a 2D input tensor: 1. Compute the 2D FFT of the input tensor. 2. Shift the zero-frequency component to the center of the spectrum. 3. Normalize the transformed tensor so that its values lie between 0 and 1. 4. Shift the zero-frequency component back to its original position. 5. Compute the inverse 2D FFT to return the tensor to the spatial domain. Function Signature: ```python def process_2d_fft(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` Input: - `input_tensor`: A 2D PyTorch tensor of shape `(M, N)` representing the input signal. Output: - Returns a 2D PyTorch tensor of shape `(M, N)` after performing the specified FFT operations. Constraints: - The input tensor will always have a shape of `(M, N)` with both dimensions being powers of 2. - You must use the PyTorch `torch.fft` module for FFT operations. - The normalization step should ensure that all values in the transformed tensor lie within the inclusive range [0, 1]. Example: ```python import torch input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) output_tensor = process_2d_fft(input_tensor) print(output_tensor) ``` Expected output should be a 2D tensor, returned to the spatial domain, with its values transformed through the sequence of FFT operations specified. Additional Notes: - For the normalization step, you can use the real part of the complex numbers after the FFT but before the inverse FFT step. - Consider the performance implications of each operation, as operations on large tensors can be computationally intensive.","solution":"import torch def process_2d_fft(input_tensor: torch.Tensor) -> torch.Tensor: # Compute the 2D FFT of the input tensor fft_result = torch.fft.fft2(input_tensor) # Shift the zero-frequency component to the center of the spectrum fft_shifted = torch.fft.fftshift(fft_result) # Normalize the transformed tensor so that its values lie between 0 and 1 magnitude = torch.abs(fft_shifted) normalized = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min()) # Shift the zero-frequency component back to its original position fft_unshifted = torch.fft.ifftshift(fft_shifted) # Compute the inverse 2D FFT to return the tensor to the spatial domain ifft_result = torch.fft.ifft2(fft_unshifted) # Return the real part of the inverse FFT result to get the spatial domain tensor return ifft_result.real"},{"question":"Coding Assessment: Logging Configuration with `logging.config` In this assessment, you are required to configure a logging system using the `dictConfig` function from the `logging.config` module. You will need to demonstrate your understanding of setting up handlers, formatters, and loggers through a well-defined configuration dictionary. Additionally, you should create a custom logging handler to showcase using user-defined objects in the logging configuration. # Task 1. **Create a Custom Logging Handler:** - Implement a custom logging handler `MyCustomHandler` that extends `logging.Handler`. - The handler should format log messages to include a custom prefix and print the messages to the console. 2. **Define a Logging Configuration Dictionary:** - Set up a configuration dictionary to include: - A custom formatter that formats log messages with timestamps and log levels. - The custom handler `MyCustomHandler`. - A root logger that uses the custom handler. 3. **Apply the Configuration and Log Messages:** - Use the `dictConfig` function to apply the logging configuration. - Log messages at various levels (e.g., DEBUG, INFO, WARNING, ERROR) to demonstrate the logging setup. # Implementation Requirements Custom Handler Implementation Create a custom handler `MyCustomHandler` in Python: ```python import logging class MyCustomHandler(logging.Handler): def __init__(self): super().__init__() def emit(self, record): log_entry = self.format(record) print(f\\"CustomLog: {log_entry}\\") ``` Logging Configuration Dictionary Define a configuration dictionary with the following structure: - **Formatter**: A custom formatter with a specified format string. - **Handler**: The custom `MyCustomHandler`. - **Root Logger**: Configured to use the custom handler. Sample snippet for the configuration dictionary: ```python LOGGING_CONFIG = { \'version\': 1, \'formatters\': { \'custom_formatter\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'my_custom_handler\': { \'class\': \'MyCustomHandler\', \'formatter\': \'custom_formatter\' }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'my_custom_handler\'] }, } ``` Applying the Configuration and Logging Use the `logging.config.dictConfig` to apply the configuration and perform logging: ```python import logging.config def main(): logging.config.dictConfig(LOGGING_CONFIG) logger = logging.getLogger() logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") if __name__ == \\"__main__\\": main() ``` # Evaluation Your solution will be evaluated based on: - Correct implementation of the custom handler. - Accuracy of the logging configuration dictionary. - Successful application of the configuration and logging output. Ensure your code is clean, well-commented, and follows Python best practices.","solution":"import logging import logging.config class MyCustomHandler(logging.Handler): def __init__(self): super().__init__() def emit(self, record): log_entry = self.format(record) print(f\\"CustomLog: {log_entry}\\") LOGGING_CONFIG = { \'version\': 1, \'formatters\': { \'custom_formatter\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'my_custom_handler\': { \'()\': MyCustomHandler, # Note the use of \'()\' to specify the class \'formatter\': \'custom_formatter\' }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'my_custom_handler\'] }, } def initialize_logging(): logging.config.dictConfig(LOGGING_CONFIG) def log_messages(): logger = logging.getLogger() logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") if __name__ == \\"__main__\\": initialize_logging() log_messages()"},{"question":"**Question Title: Principal Component Analysis on Large Datasets using IncrementalPCA** # Problem Statement You are provided with a large dataset that cannot be loaded into memory all at once. Your task is to implement a solution using `IncrementalPCA` from the `sklearn.decomposition` module to perform Principal Component Analysis (PCA) on this dataset. # Requirements 1. Load the dataset in chunks. 2. Perform incremental fitting of the `IncrementalPCA` model on these chunks. 3. Reduce the dataset to a specified number of principal components. 4. Transform the reduced dataset into the desired lower-dimensional space. 5. Output the cumulative explained variance ratio of the selected components. # Input - `file_path` (string): The path to a CSV file containing the dataset. - `chunk_size` (int): The number of rows per chunk to load. - `n_components` (int): The number of principal components to reduce the dataset to. # Output - A 2D numpy array of shape `(n_samples, n_components)` representing the reduced dataset. - A float representing the cumulative explained variance ratio of the selected components. # Constraints - Assume the CSV file does not contain missing values. - You may assume the dataset has at least `n_components` features. # Performance Requirements - The solution should be memory efficient, processing only a small chunk of the dataset at a time. # Example ```python import numpy as np import pandas as pd from sklearn.decomposition import IncrementalPCA def increment_pca(file_path, chunk_size, n_components): # Your code here # Example Usage file_path = \'large_dataset.csv\' chunk_size = 1000 n_components = 5 reduced_data, explained_variance = increment_pca(file_path, chunk_size, n_components) print(reduced_data) print(f\'Cumulative explained variance ratio: {explained_variance:.2f}\') ``` # Notes - Use pandas to load the data in chunks. - Use the `partial_fit` method of `IncrementalPCA` to incrementally fit the model on each chunk. - After fitting the model, use the `transform` method to reduce the dataset. - Calculate the cumulative explained variance ratio using the `explained_variance_ratio_` attribute. Good Luck!","solution":"import pandas as pd import numpy as np from sklearn.decomposition import IncrementalPCA def increment_pca(file_path, chunk_size, n_components): ipca = IncrementalPCA(n_components=n_components) for chunk in pd.read_csv(file_path, chunksize=chunk_size): ipca.partial_fit(chunk) transformed_data = [] for chunk in pd.read_csv(file_path, chunksize=chunk_size): transformed_chunk = ipca.transform(chunk) transformed_data.append(transformed_chunk) reduced_data = np.vstack(transformed_data) explained_variance = np.sum(ipca.explained_variance_ratio_) return reduced_data, explained_variance"},{"question":"# Coding Assessment Task: Implementing a Custom Special Function Computation Objective: You are required to write a PyTorch function that calculates a custom special function over a given tensor of values. This custom function will combine multiple special functions provided by the `torch.special` module to achieve the final result. Problem Description: Implement a function `custom_special_function` that takes a single 1-D tensor of floating-point numbers as input and returns a 1-D tensor of the same size where each element is computed using the following formula: [ text{result}[i] = text{logit}left(text{softmax}(text{xlogy}(x[i], text{erf}(x[i]) + text{exp2}(x[i]))) right) ] Where: - ( text{erf}(x) ) is the error function. - ( text{exp2}(x) ) computes (2^x). - ( text{xlogy}(x, y) ) computes (x cdot log(y)). - ( text{softmax}(z) ) applies the softmax function to the tensor (z). - ( text{logit}(p) ) returns the log-odds of (p). Constraints: - The input tensor will contain at least one and at most 10^6 floating-point numbers. - All values in the input tensor are within ([-10, 10]). Implementation: Provide a Python function `custom_special_function(x: torch.Tensor) -> torch.Tensor` within the following code template: ```python import torch def custom_special_function(x: torch.Tensor) -> torch.Tensor: Calculates a custom special function over the input tensor x. Args: - x (torch.Tensor): A 1-D tensor of floating-point numbers. Returns: - torch.Tensor: A 1-D tensor of the same size with computed values. # Ensure input is a 1-D tensor assert x.dim() == 1, \\"Input must be a 1-D tensor\\" # Implement the formula given in the problem description erf_x = torch.special.erf(x) exp2_x = torch.special.exp2(x) xlogy_x = torch.special.xlogy(x, erf_x + exp2_x) softmax_x = torch.special.softmax(xlogy_x, dim=0) result = torch.special.logit(softmax_x) return result # Example usage: # x = torch.tensor([0.5, 1.0, 1.5]) # result = custom_special_function(x) # print(result) ``` Evaluation Criteria: Your implementation will be evaluated based on: 1. **Correctness** - The function should correctly implement the formula provided. 2. **Efficiency** - The function should efficiently handle input tensors up to 10^6 elements. 3. **Code Quality** - The code should be well-organized, readable, and follow good coding practices. Notes: - The softmax function typically applies across dimensions; ensure it is correctly applied within your implementation. - Leverage PyTorch operations efficiently to minimize unnecessary computations.","solution":"import torch def custom_special_function(x: torch.Tensor) -> torch.Tensor: Calculates a custom special function over the input tensor x. Args: - x (torch.Tensor): A 1-D tensor of floating-point numbers. Returns: - torch.Tensor: A 1-D tensor of the same size with computed values. # Ensure input is a 1-D tensor assert x.dim() == 1, \\"Input must be a 1-D tensor\\" # Calculate each part of the formula step-by-step as given erf_x = torch.special.erf(x) exp2_x = torch.special.exp2(x) xlogy_x = torch.special.xlogy(x, erf_x + exp2_x) # Apply the softmax function on the calculated xlogy values softmax_x = torch.softmax(xlogy_x, dim=0) # Apply logit function on the softmax results result = torch.special.logit(softmax_x) return result # Example code to run the function (testing purposes) # x = torch.tensor([0.5, 1.0, 1.5]) # result = custom_special_function(x) # print(result)"},{"question":"**Sound File Batch Processing and Analysis** **Objective**: Create and implement a Python function that processes a batch of sound files. The function should determine the type and properties of each sound file using the `sndhdr` module, and then return a summary report containing the analyzed data in a specific format. **Task**: 1. Write a function named `analyze_sound_files(file_list)` that takes a list of filenames (strings) as input. 2. For each file in `file_list`, determine its type and properties using the `sndhdr.what(filename)` or `sndhdr.whathdr(filename)` function. 3. Collect the results in a structured format and return a summary report. **Details**: - If the type of a file cannot be determined, skip that file and do not include it in the report. - The summary report should be a list of dictionaries, where each dictionary contains the following keys and their corresponding values: `\'filename\'`, `\'filetype\'`, `\'framerate\'`, `\'nchannels\'`, `\'nframes\'`, and `\'sampwidth\'`. - Ensure that all entries in the summary report are sorted by `\'filename\'` in ascending order. **Function Signature**: ```python def analyze_sound_files(file_list: list) -> list: pass ``` **Input**: - `file_list`: A list of strings, where each string is a filename of a sound file. **Output**: - Returns a list of dictionaries, each dictionary representing the summary of one sound file\'s properties. **Example**: ```python file_list = [\\"sound1.wav\\", \\"sound2.au\\", \\"sound3.aiff\\"] result = analyze_sound_files(file_list) # Example of the expected output format # The actual values will depend on the sound files\' properties [ {\'filename\': \'sound1.wav\', \'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 100000, \'sampwidth\': 16}, {\'filename\': \'sound2.au\', \'filetype\': \'au\', \'framerate\': 8000, \'nchannels\': 1, \'nframes\': 50000, \'sampwidth\': \'U\'}, {\'filename\': \'sound3.aiff\', \'filetype\': \'aiff\', \'framerate\': 48000, \'nchannels\': 2, \'nframes\': 120000, \'sampwidth\': 24}, ] ``` **Constraints**: - `file_list` contains a maximum of 100 filenames. - Handle any exceptions or errors gracefully, such as missing files or invalid formats, by skipping the problematic file. **Notes**: - Make sure to include appropriate error handling and input validation. - The function must be efficient and handle large files within reasonable time limits. - Utilize Python\'s built-in sorting functionality to sort the summary report by `\'filename\'`.","solution":"import sndhdr def analyze_sound_files(file_list): Analyzes a list of sound files for their properties. Parameters: - file_list: list of filenames (strings) Returns: - List of dictionaries containing sound file properties summary_report = [] for filename in file_list: try: result = sndhdr.what(filename) if result: file_info = { \'filename\': filename, \'filetype\': result.filetype, \'framerate\': result.framerate, \'nchannels\': result.nchannels, \'nframes\': result.nframes, \'sampwidth\': result.sampwidth } summary_report.append(file_info) except Exception as e: # Ignore the file if there\'s an error and print the error message print(f\\"Error processing file {filename}: {e}\\") # Sort the summary report by filename summary_report.sort(key=lambda x: x[\'filename\']) return summary_report"},{"question":"Write a Python function `convert_html_entities(text: str) -> str` that takes a string containing HTML entities and returns a string with those entities replaced by their corresponding Unicode characters. Your function should handle both named character references (e.g., `&gt;`, `&amp;`, `&copy;`, etc.) and numeric character references (e.g., `&#62;`, `&#038;`, `&#169;`, etc.). # Input - `text`: A string containing HTML entities and/or numeric character references. Example: `\\"Tom &amp; Jerry &#169; Warner Bros.\\"` # Output - A string with all HTML entities replaced by their corresponding Unicode characters. Example: `\\"Tom & Jerry © Warner Bros.\\"` # Constraints 1. You may assume that the input text contains well-formed HTML entities and numeric character references. 2. You should use the `html.entities` module to perform these conversions. 3. The resulting string should have all entities/numeric references correctly replaced. # Function Signature ```python def convert_html_entities(text: str) -> str: pass ``` # Examples ```python assert convert_html_entities(\\"Tom &amp; Jerry &#169; Warner Bros.\\") == \\"Tom & Jerry © Warner Bros.\\" assert convert_html_entities(\\"I love &lt;coding&gt; and &copy;2023\\") == \\"I love <coding> and ©2023\\" ``` # Notes - You may find the `html5` dictionary in the `html.entities` module especially useful for handling named character references. - To handle numeric character references, note that these references typically use the format `&#NNN;` where `NNN` is a decimal number representing the Unicode code point. You can use `chr` to convert these code points to the corresponding character.","solution":"import html def convert_html_entities(text: str) -> str: This function takes a string with HTML entities and numeric character references, replacing them with their corresponding Unicode characters. return html.unescape(text)"},{"question":"# Problem: Constructing MIME Email from Scratch You are required to build a function that constructs a multipart MIME email with mixed content. The email should have the following components: 1. A plain text part. 2. An HTML part. 3. An attachment (a text file). The function should be named `create_mime_email` and adhere to the following specifications: Function Signature ```python def create_mime_email(subject: str, sender: str, recipient: str, plain_text: str, html_text: str, attachment_filename: str, attachment_data: bytes) -> email.mime.multipart.MIMEMultipart: # Your code here ``` Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `plain_text` (str): The plain text content of the email. - `html_text` (str): The HTML content of the email. - `attachment_filename` (str): The filename for the attachment. - `attachment_data` (bytes): The data of the attachment as bytes. Output - Returns an instance of `email.mime.multipart.MIMEMultipart` representing the complete email with all components attached. Detailed Requirements 1. **MIME Components**: - The plain text and HTML parts should be represented using `email.mime.text.MIMEText`. - The attachment should be represented using `email.mime.application.MIMEApplication`. 2. **Email Headers**: - The subject, sender, and recipient fields should be set as headers on the root `MIMEMultipart` object. 3. **Handling Attachments**: - Filename and data should be included in the MIME application part. - Use base64 encoding for the attachment. 4. **MIME Structure**: - The root MIME message should be of type `multipart/mixed`. - The email should contain text in both plain text and HTML formats. Example Usage ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" plain_text = \\"This is the plain text content.\\" html_text = \\"<html><body><h1>This is HTML content</h1></body></html>\\" attachment_filename = \\"example.txt\\" attachment_data = b\\"This is the content of the attached file.\\" email_message = create_mime_email(subject, sender, recipient, plain_text, html_text, attachment_filename, attachment_data) print(email_message.as_string()) ``` In this task, you must demonstrate proficiency in assembling and structuring MIME email messages using the provided classes in the `email.mime` module.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.application import MIMEApplication from email import encoders def create_mime_email(subject: str, sender: str, recipient: str, plain_text: str, html_text: str, attachment_filename: str, attachment_data: bytes) -> MIMEMultipart: # Create the MIME Multipart container msg = MIMEMultipart(\\"mixed\\") msg[\\"Subject\\"] = subject msg[\\"From\\"] = sender msg[\\"To\\"] = recipient # Create the plain text part part1 = MIMEText(plain_text, \\"plain\\") msg.attach(part1) # Create the HTML part part2 = MIMEText(html_text, \\"html\\") msg.attach(part2) # Create the attachment part part3 = MIMEApplication(attachment_data, Name=attachment_filename) part3[\'Content-Disposition\'] = f\'attachment; filename=\\"{attachment_filename}\\"\' encoders.encode_base64(part3) msg.attach(part3) return msg"},{"question":"**Question: Resampling and Analyzing Stock Price Data** You are given a CSV file containing stock price data with the following columns: - `Date`: The date and time of the record in the format `YYYY-MM-DD HH:MM:SS`. - `Open`: The opening price of the stock. - `High`: The highest price of the stock during the timeframe. - `Low`: The lowest price of the stock during the timeframe. - `Close`: The closing price of the stock. - `Volume`: The trading volume of the stock. The goal is to resample this data to a weekly frequency and then perform some descriptive analysis. **Steps:** 1. Load the CSV file into a DataFrame and ensure that the `Date` column is treated as a datetime object. 2. Set the `Date` column as the index of the DataFrame. 3. Resample the data to a weekly frequency, using the week\'s ending as the resampling anchor point. 4. Use resampling methods to calculate the following for each week: - Opening price (`Open`) of the week: The opening price of the first trading day of the week. - Highest price (`High`) during the week. - Lowest price (`Low`) during the week. - Closing price (`Close`) of the last trading day of the week. - Total trading volume (`Volume`) during the week. 5. Return a new DataFrame containing the resampled data with these calculations. **Function Signature:** ```python def resample_stock_data(file_path: str) -> pd.DataFrame: Resample the stock data to a weekly frequency and return the calculated DataFrame. Parameters: - file_path (str): The file path to the CSV file containing the stock price data. Returns: - pd.DataFrame: The DataFrame containing the resampled weekly stock price data. pass ``` **Example:** Given the initial data: ``` Date, Open, High, Low, Close, Volume 2023-01-01 09:00:00, 100, 110, 90, 105, 1000 2023-01-02 09:00:00, 106, 115, 100, 110, 1500 2023-01-03 09:00:00, 111, 120, 105, 115, 2000 ... ``` The resulting DataFrame after resampling might look like: ``` Date | Open | High | Low | Close | Volume 2023-01-08 | 100 | 120 | 90 | 115 | 4500 ... ``` **Constraints:** - You may assume the date format in the CSV file is consistent and does not contain any missing values. - Ensure the resampling anchor point is set to \'W-FRI\', meaning the week ends on Friday. **Note:** You might need the `pandas` library for this question, make sure it is imported in your solution.","solution":"import pandas as pd def resample_stock_data(file_path: str) -> pd.DataFrame: Resample the stock data to a weekly frequency and return the calculated DataFrame. Parameters: - file_path (str): The file path to the CSV file containing the stock price data. Returns: - pd.DataFrame: The DataFrame containing the resampled weekly stock price data. # Load the CSV file into a DataFrame df = pd.read_csv(file_path) # Convert the \'Date\' column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Set the \'Date\' column as the index df.set_index(\'Date\', inplace=True) # Resample the data to weekly frequency, anchored to end on Friday (\'W-FRI\') resampled_df = df.resample(\'W-FRI\').agg({ \'Open\': \'first\', \'High\': \'max\', \'Low\': \'min\', \'Close\': \'last\', \'Volume\': \'sum\' }) # Reset the index to convert the Date index back into a column resampled_df.reset_index(inplace=True) return resampled_df"},{"question":"**Question: Implement a Reusable Logging Context Manager** In this exercise, you will implement a reusable context manager that logs the time taken by the block of code it manages. This will help you understand how to define both reusable and reentrant context managers, which do not rely on a fresh instance each time. # Requirements: 1. **LogTimeContextManager class** - This class should be a context manager that logs the entry and exit times of the block of code it manages. - The class should be reusable across multiple `with` statements. - It should print a log message with the time difference between entry and exit. - The class should also support reentrancy, meaning it should work correctly if used within a block of code that is itself managed by another instance of the same context manager. # Implementation Details: 1. **Constructor** - The constructor should accept a `name` parameter and store it. 2. **`__enter__` method** - Record the current time as the entry time. - Print a log message showing the entry time. 3. **`__exit__` method** - Record the current time as the exit time. - Calculate the time difference between entry and exit. - Print a log message showing the exit time and the time difference. 4. **Support reentrancy and reusability** # Example Usage: ```python import time from contextlib import ContextDecorator import logging logging.basicConfig(level=logging.INFO) class LogTimeContextManager(ContextDecorator): def __init__(self, name): self.name = name def __enter__(self): self.start_time = time.monotonic() logging.info(f\'Entering: {self.name} at {self.start_time}\') return self def __exit__(self, exc_type, exc_value, traceback): self.end_time = time.monotonic() duration = self.end_time - self.start_time logging.info(f\'Exiting: {self.name} at {self.end_time}, Duration: {duration:.4f}s\') return False # Using the context manager with LogTimeContextManager(\'performance test\'): time.sleep(1) # Using the context manager as a decorator @LogTimeContextManager(\'decorated performance test\') def sleep_and_log(): time.sleep(2) sleep_and_log() ``` # Constraints: - You may not use any imports other than `time`, `contextlib`, and `logging`. - Your context manager should handle any exceptions that occur within the block and still log the exit time and duration. # Performance Requirements: - The context manager must work correctly even if the managed block of code runs for a very short or very long time. - Ensure minimal overhead is added by the context manager itself. Complete the implementation of the `LogTimeContextManager` class based on these requirements and examples.","solution":"import time import logging # Configure logging logging.basicConfig(level=logging.INFO) class LogTimeContextManager: def __init__(self, name): self.name = name def __enter__(self): self.start_time = time.monotonic() logging.info(f\'Entering: {self.name} at {self.start_time}\') return self def __exit__(self, exc_type, exc_value, traceback): self.end_time = time.monotonic() duration = self.end_time - self.start_time logging.info(f\'Exiting: {self.name} at {self.end_time}, Duration: {duration:.4f}s\') if exc_type: logging.error(f\'Error: {exc_type}, {exc_value}\') return False"},{"question":"**Coding Assessment Question: Seaborn Advanced Plotting** You are tasked with analyzing the flight data for the year 1960 using the seaborn library to generate insightful visualizations. The dataset can be loaded using `seaborn.load_dataset(\\"flights\\")` and filtering for the year 1960. **Objective:** Write a function `create_flight_plots` that creates a series of bar plots using the seaborn.objects library to visualize the number of passengers for each month in the year 1960. Follow these steps: 1. Load the flights dataset. 2. Filter the data for the year 1960. 3. Create a basic bar plot to show the number of passengers each month. 4. Customize the bar plot by changing the orientation to have months on the y-axis. 5. Create a second plot that shows the average number of passengers, separated by months, with error bars representing the standard deviation. 6. Ensure the plots include appropriate titles and axis labels for clarity. **Function Signature:** ```python def create_flight_plots(): # Your code here pass ``` **Expected Output:** - The function should not return any value. - It should display two plots: 1. A basic bar plot with months on the x-axis and passengers on the y-axis. 2. An advanced bar plot with error bars, showing the standard deviation of passengers per month. **Detailed Steps:** 1. **Load and Filter Data:** - Use `seaborn.load_dataset(\\"flights\\")` to load the dataset. - Filter it to include only the data for the year 1960. 2. **Basic Bar Plot:** - Create a bar plot using seaborn.objects to show the number of passengers for each month. 3. **Change Orientation:** - Modify the bar plot so that the months are on the y-axis, and passengers are on the x-axis. 4. **Advanced Bar Plot with Error Bars:** - Create another bar plot. - Add error bars to this plot showing the standard deviation of passengers for each month. - Use appropriate seaborn objects to implement the error bars. 5. **Customization and Labels:** - Ensure both plots have titles and axis labels. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset def create_flight_plots(): # Load the dataset flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Basic bar plot p1 = so.Plot(flights[\\"month\\"], flights[\\"passengers\\"]).add(so.Bar()).label(title=\\"Monthly Passengers (1960)\\", x=\\"Month\\", y=\\"Number of Passengers\\") p1.show() # Change orientation p2 = so.Plot(flights[\\"passengers\\"], flights[\\"month\\"]).add(so.Bar()).label(title=\\"Monthly Passengers (1960)\\", x=\\"Number of Passengers\\", y=\\"Month\\") p2.show() # Advanced bar plot with error bars agg = flights.groupby(\\"month\\")[\\"passengers\\"].agg([\\"mean\\", \\"std\\"]).reset_index() p3 = ( so.Plot(agg, \\"mean\\", \\"month\\") .add(so.Bar(alpha=0.5)) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .label(title=\\"Average Monthly Passengers with Error Bars (1960)\\", x=\\"Average Number of Passengers\\", y=\\"Month\\") ) p3.show() ``` Submit your implementation of the `create_flight_plots` function which should display the described plots when executed.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_flight_plots(): # Load the dataset flights = load_dataset(\\"flights\\") flights_1960 = flights.query(\\"year == 1960\\") # Basic bar plot p1 = so.Plot(flights_1960, x=\\"month\\", y=\\"passengers\\").add(so.Bar()).label(title=\\"Monthly Passengers (1960)\\", x=\\"Month\\", y=\\"Number of Passengers\\") p1.show() # Change orientation p2 = so.Plot(flights_1960, y=\\"month\\", x=\\"passengers\\").add(so.Bar()).label(title=\\"Monthly Passengers (1960)\\", x=\\"Number of Passengers\\", y=\\"Month\\") p2.show() # Advanced bar plot with error bars agg = flights_1960.groupby(\\"month\\")[\\"passengers\\"].agg([\\"mean\\", \\"std\\"]).reset_index() p3 = ( so.Plot(agg, x=\\"mean\\", y=\\"month\\") .add(so.Bar(alpha=0.5)) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .label(title=\\"Average Monthly Passengers with Error Bars (1960)\\", x=\\"Average Number of Passengers\\", y=\\"Month\\") ) p3.show()"},{"question":"**Question: Scatter Plot Customization and Interpretation** You are given a dataset of car features called `car_data` which contains the following columns: - `mpg` (Miles per Gallon): Numeric. - `horsepower`: Numeric. - `weight`: Numeric. - `acceleration`: Numeric. - `origin`: Categorical (with values \'USA\', \'Japan\', \'Europe\'). Your task is to write a function `custom_scatter_plot` to generate a complex scatter plot using Seaborn that meets the following criteria: 1. Plot `mpg` on the x-axis and `horsepower` on the y-axis. 2. Use the `origin` column to color the points. 3. Vary the marker style based on the `origin` column. 4. Encode the size of the points using the `weight` column. 5. Use larger size markers for improved visualization, setting the sizes between 50 and 300. 6. Normalize the `weight` column values before using them to control the marker size. 7. Ensure that all unique values in the `origin` column are displayed in the legend. Below is the signature of the function you should implement: ```python def custom_scatter_plot(car_data): import seaborn as sns import matplotlib.pyplot as plt # Write your code here plt.show() ``` # Input - `car_data`: A Pandas DataFrame containing the columns `mpg`, `horsepower`, `weight`, `acceleration`, `origin`. # Output - The function should not return anything, but should display a scatter plot meeting the specified criteria. # Constraints - None of the columns will contain missing values. - The DataFrame will have at least 100 rows. # Example Suppose you have the following data in `car_data` DataFrame: ``` mpg horsepower weight acceleration origin 0 25 100 2000 14 USA 1 30 130 2800 18 Europe 2 22 110 2200 15 Japan ... ``` After calling `custom_scatter_plot(car_data)`, a scatter plot should be displayed with: - `mpg` on the x-axis. - `horsepower` on the y-axis. - Points colored and styled by `origin`. - Point sizes varying by `weight` (normalized). # Notes - Make sure your plot is easily readable and the legends are clear. - Utilize Seaborn\'s `scatterplot` function and its parameters effectively.","solution":"def custom_scatter_plot(car_data): import seaborn as sns import matplotlib.pyplot as plt import pandas as pd from sklearn.preprocessing import MinMaxScaler # Normalize the weight column to control the marker size scaler = MinMaxScaler(feature_range=(50, 300)) car_data[\'weight\'] = scaler.fit_transform(car_data[[\'weight\']]) # Create a scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot( data=car_data, x=\'mpg\', y=\'horsepower\', hue=\'origin\', style=\'origin\', size=\'weight\', sizes=(50, 300) ) # Improve visualization by setting larger sizes for markers plt.legend(title=\'Origin\') plt.xlabel(\'Miles per Gallon (mpg)\') plt.ylabel(\'Horsepower\') plt.title(\'Scatter plot of MPG vs Horsepower with weight and origin encoding\') # Display the plot plt.show()"},{"question":"**Unix Password Database Manipulation Task** You have been provided the documentation for the `pwd` module in Python, which allows you to access the Unix user account and password databases. Your task is to implement a function called `find_users_with_shell(shell)`, which: 1. Accepts a single string argument, `shell`, that represents a user command interpreter (e.g., `/bin/bash`, `/usr/sbin/nologin`). 2. Returns a list of user names (`pw_name`) who have their `pw_shell` attribute set to the provided `shell`. **Function Signature:** ```python def find_users_with_shell(shell: str) -> List[str]: ``` **Input:** - `shell` (str): The command interpreter to search for. **Output:** - List of strings: Each string is a user name (`pw_name`) using the specified `shell`. **Example:** ```python # Assuming the /etc/passwd contains: # root:x:0:0:root:/root:/bin/bash # daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin # user1:x:1000:1000:User One:/home/user1:/bin/bash print(find_users_with_shell(\'/bin/bash\')) # Output: [\'root\', \'user1\'] print(find_users_with_shell(\'/usr/sbin/nologin\')) # Output: [\'daemon\'] ``` **Constraints:** - You may assume the `pwd` module is available. - The function must handle the absence of any matches gracefully by returning an empty list. - Optimal performance is encouraged but not strictly necessary given the typical size of Unix password databases. **Guidelines:** - You should use the `pwd.getpwall()` function to fetch all password entries. - Iterate through the fetched entries and filter based on the `pw_shell` attribute. - Collect and return the `pw_name` attributes of matching entries. **Note:** This task evaluates your ability to: - Understand and manipulate structures and functions from Python\'s standard library. - Apply basic search and filtering operations on data structures. Good luck!","solution":"import pwd from typing import List def find_users_with_shell(shell: str) -> List[str]: Returns a list of user names who have their `pw_shell` attribute set to the provided `shell`. Parameters: shell (str): The command interpreter to search for. Returns: List[str]: List of user names using the specified shell. users = pwd.getpwall() return [user.pw_name for user in users if user.pw_shell == shell]"},{"question":"# Problem: Predicting Housing Prices Using Scikit-Learn Linear Models In this task, you will develop a set of machine learning models to predict housing prices using different linear regression techniques available in scikit-learn. We\'ll provide you with a housing dataset containing various features about each house such as the number of rooms, the age of the house, etc. The dataset consists of: - A feature matrix `X` of shape `(n_samples, n_features)` - A target vector `y` of shape `(n_samples,)` You\'ll implement the following functions: 1. **linear_regression(X, y):** - Train a simple linear regression model on the data. - Return the coefficients and intercept of the trained model. 2. **ridge_regression(X, y, alpha):** - Train a ridge regression model with a given regularization parameter `alpha`. - Return the coefficients, intercept, and the best `alpha` if cross-validation is involved. 3. **lasso_regression(X, y, alpha):** - Train a lasso regression model with a given regularization parameter `alpha`. - Return the coefficients, intercept, and the best `alpha` if cross-validation is involved. 4. **elastic_net_regression(X, y, alpha, l1_ratio):** - Train an elastic net regression model with given parameters `alpha` and `l1_ratio`. - Return the coefficients, intercept, and the best parameters if cross-validation is involved. # Constraints - Use scikit-learn version 0.24 or newer. - You should not modify the input data `X` and `y` during training. - Properly handle missing values if any. # Expected Performance Your models should: - Show a good fit on the training data (you may use metrics such as R² score). - Demonstrate the impact of regularization parameters (especially for Ridge and Lasso regression). # Example Usage ```python # Assume X and y are provided X = ... y = ... # Linear Regression coef, intercept = linear_regression(X, y) print(f\\"Coefficients: {coef}, Intercept: {intercept}\\") # Ridge Regression alpha = 1.0 coef, intercept, best_alpha = ridge_regression(X, y, alpha) print(f\\"Coefficients: {coef}, Intercept: {intercept}, Best Alpha: {best_alpha}\\") # Lasso Regression alpha = 0.1 coef, intercept, best_alpha = lasso_regression(X, y, alpha) print(f\\"Coefficients: {coef}, Intercept: {intercept}, Best Alpha: {best_alpha}\\") # Elastic Net Regression alpha = 0.1 l1_ratio = 0.5 coef, intercept, best_params = elastic_net_regression(X, y, alpha, l1_ratio) print(f\\"Coefficients: {coef}, Intercept: {intercept}, Best Params: {best_params}\\") ``` # Implementation ```python from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import GridSearchCV def linear_regression(X, y): reg = LinearRegression() reg.fit(X, y) return reg.coef_, reg.intercept_ def ridge_regression(X, y, alpha): reg = Ridge(alpha=alpha) reg.fit(X, y) return reg.coef_, reg.intercept_, alpha def lasso_regression(X, y, alpha): reg = Lasso(alpha=alpha) reg.fit(X, y) return reg.coef_, reg.intercept_, alpha def elastic_net_regression(X, y, alpha, l1_ratio): reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio) reg.fit(X, y) return reg.coef_, reg.intercept_, (alpha, l1_ratio) ``` Please ensure to validate your functions thoroughly and demonstrate with suitable examples.","solution":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import GridSearchCV def linear_regression(X, y): reg = LinearRegression() reg.fit(X, y) return reg.coef_, reg.intercept_ def ridge_regression(X, y, alpha): reg = Ridge(alpha=alpha) reg.fit(X, y) return reg.coef_, reg.intercept_, alpha def lasso_regression(X, y, alpha): reg = Lasso(alpha=alpha) reg.fit(X, y) return reg.coef_, reg.intercept_, alpha def elastic_net_regression(X, y, alpha, l1_ratio): reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio) reg.fit(X, y) return reg.coef_, reg.intercept_, (alpha, l1_ratio)"},{"question":"**Objective:** Implement a function that uses Python\'s native capabilities to emulate the behavior of `PyOS_FSPath`, covering string and byte handling, and raising appropriate exceptions for invalid inputs. **Problem Statement:** You are required to implement a function `custom_fs_path(path)` in Python that emulates the behavior of the `PyOS_FSPath` function described in the C documentation. The function should take a single argument and return the file system representation for that input. **Function Signature:** ```python def custom_fs_path(path): # Your implementation here ``` **Input:** - `path`: A single argument that could be: 1. A string representing a file path. 2. A bytes object representing a file path. 3. An object that implements the `os.PathLike` interface (meaning it has a `__fspath__` method returning either a string or bytes object). **Output:** - The function should return: 1. A new string reference if `path` is a string. 2. A new bytes reference if `path` is a bytes object. 3. The result of `path.__fspath__()` if `path` implements the `os.PathLike` interface and `__fspath__()` returns either a string or bytes object. 4. Raise a `TypeError` if the input does not meet any of the above conditions or if `path.__fspath__()` does not return a string or bytes object. **Constraints:** - The function should handle various edge cases including but not limited to: - `None` or invalid data types being passed as `path`. - Properly returning or raising exceptions as specified. - Ensure that it works efficiently with large strings and bytes objects. **Examples:** 1. **Input:** `\'example/path\'` (as string) **Output:** `\'example/path\'` (new string reference) 2. **Input:** `b\'example/path\'` (as bytes) **Output:** `b\'example/path\'` (new bytes reference) 3. **Input:** ```python import os class CustomPath: def __fspath__(self): return \\"custom/path\\" custom_path = CustomPath() ``` **Output:** `\'custom/path\'` (string from `__fspath__()`) 4. **Input:** ```python class InvalidPath: pass invalid_path = InvalidPath() ``` **Output:** Raise `TypeError` **Notes:** - You may assume that the function will only be tested with objects that the Python `os` module can handle. - Focus on writing clean, concise, and efficient code. - Include proper error handling to ensure robustness. - Add appropriate comments to explain your logic. **Bonus:** - Extend the function to print a message before raising `TypeError`, indicating that an invalid type was encountered, specifying the type of the invalid input.","solution":"import os def custom_fs_path(path): Emulate the behavior of PyOS_FSPath in Python. Parameters: - path: a string, bytes, or an object implementing os.PathLike Returns: - A file system path (str or bytes) Raises: - TypeError: if the path is neither a string, bytes, or an os.PathLike object returning str/bytes if isinstance(path, (str, bytes)): return path if hasattr(path, \'__fspath__\'): fspath = path.__fspath__() if isinstance(fspath, (str, bytes)): return fspath else: raise TypeError(f\\"The __fspath__ method must return str or bytes, not {type(fspath).__name__}\\") raise TypeError(f\\"Invalid type: {type(path).__name__}. Expected str, bytes, or an object implementing os.PathLike\\") # Examples print(custom_fs_path(\'example/path\')) # Expected Output: \'example/path\' print(custom_fs_path(b\'example/path\')) # Expected Output: b\'example/path\'"},{"question":"You are required to demonstrate your understanding of Python iterator objects by implementing a series of functions. Your task is to implement the following functions that will work with both sequence iterator and callable iterator. 1. **Sequence Iterator Function** - Implement a function `sequence_iterator_sum` that takes a sequence (e.g., list, tuple) and returns the sum of the elements in the sequence using the sequence iterator functionality. - **Input**: A sequence (list or tuple) of integers (e.g., `[1, 2, 3, 4]` or `(1, 2, 3, 4)`). - **Output**: An integer that is the sum of the elements in the sequence. - **Constraints**: - The sequence may contain up to 10^6 elements. - Elements in the sequence are between -10^6 and 10^6. ```python def sequence_iterator_sum(sequence): # Your code here pass ``` 2. **Callable Iterator Function** - Implement a function `callable_iterator_sum` that takes a callable (a function with no parameters) and a sentinel value. The function should call the callable repeatedly, summing the results until the sentinel value is returned. - **Input**: - A callable object (function with no parameters), - A sentinel value (integer). - **Output**: An integer that is the sum of the values returned by the callable until the sentinel value is encountered. - **Constraints**: - The callable will return a finite sequence of integers between -10^6 and 10^6 before returning the sentinel. - The sequence generated by the callable may not exceed 10^6 elements. ```python def callable_iterator_sum(callable_obj, sentinel): # Your code here pass ``` # Example ```python # Example for sequence_iterator_sum print(sequence_iterator_sum([1, 2, 3, 4])) # Output: 10 # Example for callable_iterator_sum def my_callable(): values = [1, 2, 3, 4, 0] # 0 is the sentinel for value in values: yield value callable_gen = my_callable() print(callable_iterator_sum(lambda: next(callable_gen), 0)) # Output: 10 ``` Ensure your functions handle the constraints efficiently. The solutions should be optimal in terms of time and space complexity.","solution":"def sequence_iterator_sum(sequence): Returns the sum of the elements in the given sequence using an iterator. return sum(sequence) def callable_iterator_sum(callable_obj, sentinel): Returns the sum of the values returned by the callable until the sentinel value is encountered. total = 0 for value in iter(callable_obj, sentinel): total += value return total"},{"question":"# Question: Advanced PyTorch Backend Configuration You are tasked with developing a Python script using PyTorch that demonstrates an understanding of various backend configuration settings. You should configure two different backends and perform tensor operations emphasizing performance optimization. Goals: 1. **Configure CUDA and cuDNN Backends**: Write a script that checks and configures the CUDA and cuDNN backends. 2. **Tensor Operation**: Create a large matrix multiplication operation and evaluate the performance with different backend configurations. 3. **Compare Performance**: Run the operation with different backend settings and compare their performance. Details: 1. **Backend Configuration**: - Check if CUDA and cuDNN backends are available. - Enable TensorFloat-32 support for CUDA backend if the GPU architecture is Ampere or newer. - Set cuDNN to use deterministic algorithms and benchmark multiple algorithms to select the fastest. 2. **Tensor Operation**: - Create two large random tensors and perform matrix multiplication. - Measure the time taken for the operation using different settings (with and without TensorFloat-32, deterministic algorithms, and benchmarking). 3. **Performance Comparison**: - Compare the operation times with different configurations and print the results. Input: - No user input is required for this script. Output: - Print the availability of CUDA and cuDNN backends. - Print the time taken for matrix multiplications with different configurations. Constraints: - Use a GPU-enabled environment with PyTorch installed. - Ensure different configurations are applied correctly. Example: ```python import torch import time # Check if CUDA is available is_cuda_available = torch.backends.cuda.is_built() print(f\\"CUDA available: {is_cuda_available}\\") # Check if cuDNN is available is_cudnn_available = torch.backends.cudnn.is_available() print(f\\"cuDNN available: {is_cudnn_available}\\") # Function to measure time for matrix multiplication def measure_time(tensor_size, use_tf32, use_deterministic): A = torch.randn(tensor_size, tensor_size, device=\'cuda\') B = torch.randn(tensor_size, tensor_size, device=\'cuda\') torch.backends.cuda.matmul.allow_tf32 = use_tf32 torch.backends.cudnn.deterministic = use_deterministic torch.backends.cudnn.benchmark = not use_deterministic torch.cuda.synchronize() start_time = time.time() C = torch.matmul(A, B) torch.cuda.synchronize() end_time = time.time() return end_time - start_time tensor_size = 8192 # Measure and print times time_tf32_deterministic = measure_time(tensor_size, True, True) print(f\\"Time with TF32 and deterministic: {time_tf32_deterministic:.6f} seconds\\") time_tf32_benchmark = measure_time(tensor_size, True, False) print(f\\"Time with TF32 and benchmark: {time_tf32_benchmark:.6f} seconds\\") time_fp32_deterministic = measure_time(tensor_size, False, True) print(f\\"Time with FP32 and deterministic: {time_fp32_deterministic:.6f} seconds\\") time_fp32_benchmark = measure_time(tensor_size, False, False) print(f\\"Time with FP32 and benchmark: {time_fp32_benchmark:.6f} seconds\\") ``` Note: You should run this script in an environment with access to a GPU supporting CUDA and cuDNN to see meaningful results.","solution":"import torch import time def check_backend_availability(): Checks and prints the availability of CUDA and cuDNN backends. is_cuda_available = torch.cuda.is_available() print(f\\"CUDA available: {is_cuda_available}\\") is_cudnn_available = torch.backends.cudnn.is_available() print(f\\"cuDNN available: {is_cudnn_available}\\") return is_cuda_available, is_cudnn_available def measure_time(tensor_size, use_tf32, use_deterministic): Measures and returns the time taken for a matrix multiplication operation on the GPU. Args: tensor_size (int): Size of the matrix. use_tf32 (bool): Whether to use TensorFloat-32 for CUDA. use_deterministic (bool): Whether to use deterministic algorithms for cuDNN. Returns: float: The time taken for the matrix multiplication operation. A = torch.randn(tensor_size, tensor_size, device=\'cuda\') B = torch.randn(tensor_size, tensor_size, device=\'cuda\') torch.backends.cuda.matmul.allow_tf32 = use_tf32 torch.backends.cudnn.deterministic = use_deterministic torch.backends.cudnn.benchmark = not use_deterministic torch.cuda.synchronize() start_time = time.time() C = torch.matmul(A, B) torch.cuda.synchronize() end_time = time.time() return end_time - start_time def main(): is_cuda_available, is_cudnn_available = check_backend_availability() if not is_cuda_available or not is_cudnn_available: print(\\"CUDA and cuDNN must be available to run the performance tests.\\") return tensor_size = 8192 # Measure and print times time_tf32_deterministic = measure_time(tensor_size, True, True) print(f\\"Time with TF32 and deterministic: {time_tf32_deterministic:.6f} seconds\\") time_tf32_benchmark = measure_time(tensor_size, True, False) print(f\\"Time with TF32 and benchmark: {time_tf32_benchmark:.6f} seconds\\") time_fp32_deterministic = measure_time(tensor_size, False, True) print(f\\"Time with FP32 and deterministic: {time_fp32_deterministic:.6f} seconds\\") time_fp32_benchmark = measure_time(tensor_size, False, False) print(f\\"Time with FP32 and benchmark: {time_fp32_benchmark:.6f} seconds\\") if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> From the provided documentation, it seems that the focus is on the module `torch.nn.attention.bias` which includes classes and functions related to \\"CausalBias\\". This concept is typically used in attention mechanisms, which is a fundamental part of Transformer models and other neural network architectures that require handling sequential data with the ability to look at dependencies in sequences. The main elements mentioned are: - `CausalBias` - Functions `causal_lower_right` and `causal_upper_left` - A variant class named `CausalVariant`. Given this information, I will design a question that involves implementing a custom attention mechanism using a causal bias. This will require students to understand and apply PyTorch\'s tensor operations and the concept of causal masking. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are asked to implement a custom attention mechanism using PyTorch. The attention mechanism should incorporate a causal bias as defined in PyTorch\'s `torch.nn.attention.bias` module, ensuring that the attention mechanism only attends to the previous tokens (causal masking). Requirements: 1. **Input**: - `queries` (Tensor of shape `(batch_size, seq_length, embedding_dim)`): The query matrix. - `keys` (Tensor of shape `(batch_size, seq_length, embedding_dim)`): The key matrix. - `values` (Tensor of shape `(batch_size, seq_length, embedding_dim)`): The value matrix. 2. **Output**: - `output` (Tensor of shape `(batch_size, seq_length, embedding_dim)`): The result of the attention mechanism with causal bias applied. Constraints: - The implementation should ensure that each position in the sequence can only attend to positions before it or to itself. - You are required to use functions from the `torch.nn.attention.bias` module to implement the causal bias. Instructions: 1. Implement the function `causal_attention` with the specified input and output. 2. Your function should perform the following steps: - Calculate the dot-product attention scores between `queries` and `keys`. - Apply the causal bias to the attention scores. - Normalize the scores using the softmax function. - Calculate the weighted sum of the `values` using the normalized scores. Here is the skeleton code: ```python import torch import torch.nn.functional as F from torch.nn.attention.bias import causal_lower_right def causal_attention(queries, keys, values): Args: queries (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) keys (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) values (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) Returns: torch.Tensor: Tensor of shape (batch_size, seq_length, embedding_dim) # Step 1: Compute attention scores attention_scores = torch.bmm(queries, keys.transpose(1, 2)) # Step 2: Apply causal mask causal_mask = causal_lower_right(seq_length=queries.size(1)) masked_attention_scores = attention_scores + causal_mask # Step 3: Apply softmax to get normalized attention weights attention_weights = F.softmax(masked_attention_scores, dim=-1) # Step 4: Compute the output as the weighted sum of values output = torch.bmm(attention_weights, values) return output ``` **Note**: Ensure that the `causal_lower_right` function properly applies the causal masking as per the causal attention mechanism requirements. Test your implementation with sample tensors to verify its correctness. Example Usage: ```python batch_size = 2 seq_length = 5 embedding_dim = 4 queries = torch.randn(batch_size, seq_length, embedding_dim) keys = torch.randn(batch_size, seq_length, embedding_dim) values = torch.randn(batch_size, seq_length, embedding_dim) output = causal_attention(queries, keys, values) print(output) # Expected shape: (batch_size, seq_length, embedding_dim) ``` # Explanation 1. **Input**: - `queries`, `keys`, `values` tensors with the same shape `(batch_size, seq_length, embedding_dim)`. 2. **Output**: - `output` tensor with the shape `(batch_size, seq_length, embedding_dim)`. Ensure that your implementation uses the causal bias correctly, taking advantage of the available functions in the `torch.nn.attention.bias` module to handle the masking.","solution":"import torch import torch.nn.functional as F def causal_attention(queries, keys, values): Args: queries (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) keys (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) values (torch.Tensor): Tensor of shape (batch_size, seq_length, embedding_dim) Returns: torch.Tensor: Tensor of shape (batch_size, seq_length, embedding_dim) # Step 1: Compute attention scores attention_scores = torch.bmm(queries, keys.transpose(1, 2)) # Step 2: Apply causal mask batch_size, seq_length, _ = queries.size() causal_mask = torch.triu(torch.ones(seq_length, seq_length) * float(\'-inf\'), diagonal=1) causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1) masked_attention_scores = attention_scores + causal_mask # Step 3: Apply softmax to get normalized attention weights attention_weights = F.softmax(masked_attention_scores, dim=-1) # Step 4: Compute the output as the weighted sum of values output = torch.bmm(attention_weights, values) return output"},{"question":"**Objective:** Implement a Ridge Regression model with cross-validation in Python using the scikit-learn library. The goal is to find the optimal regularization parameter (alpha) that minimizes the prediction error. You will use a given dataset for training and testing your model. **Input:** You\'ll be given two inputs: 1. **training_data.csv**: A CSV file with the training data, where the last column is the target variable `y` and the rest are the feature variables `X`. 2. **testing_data.csv**: A CSV file with the testing data, which has the same columns as the training data. **Output:** The script should output the following: 1. The optimal regularization parameter (alpha) found through cross-validation. 2. The coefficients of the trained Ridge Regression model. 3. The mean squared error (MSE) on the test data using the trained model. **Constraints:** - Use scikit-learn\'s `RidgeCV` for cross-validation. - Use scikit-learn\'s `mean_squared_error` for evaluation. - Assume the dataset fits into memory. **Example:** Assume `training_data.csv` and `testing_data.csv` are given with the structure mentioned above: ```python import numpy as np import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error # Load the data training_data = pd.read_csv(\'training_data.csv\') testing_data = pd.read_csv(\'testing_data.csv\') # Prepare the data X_train = training_data.iloc[:, :-1].values y_train = training_data.iloc[:, -1].values X_test = testing_data.iloc[:, :-1].values y_test = testing_data.iloc[:, -1].values # Define a set of alphas to search over alphas = np.logspace(-6, 6, 13) # Implement Ridge Regression with cross-validation ridge_cv = RidgeCV(alphas=alphas) ridge_cv.fit(X_train, y_train) # Get the best alpha from cross-validation optimal_alpha = ridge_cv.alpha_ # Get the coefficients coefficients = ridge_cv.coef_ # Predict on the test set y_pred = ridge_cv.predict(X_test) # Calculate the MSE on the test data mse = mean_squared_error(y_test, y_pred) # Print the results print(f\\"Optimal Alpha: {optimal_alpha}\\") print(f\\"Coefficients: {coefficients}\\") print(f\\"Mean Squared Error: {mse}\\") ``` **Instructions:** 1. Read the training and testing data. 2. Prepare the feature matrix X and the target vector y for both datasets. 3. Use `RidgeCV` from `sklearn.linear_model` to find the optimal alpha. 4. Fit the model on the training data and evaluate it on the test data. 5. Print the optimal alpha, the coefficients of the trained model, and the mean squared error on the test data.","solution":"import numpy as np import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error def ridge_regression_with_cv(train_csv, test_csv): # Load the data training_data = pd.read_csv(train_csv) testing_data = pd.read_csv(test_csv) # Prepare the data X_train = training_data.iloc[:, :-1].values y_train = training_data.iloc[:, -1].values X_test = testing_data.iloc[:, :-1].values y_test = testing_data.iloc[:, -1].values # Define a set of alphas to search over alphas = np.logspace(-6, 6, 13) # Implement Ridge Regression with cross-validation ridge_cv = RidgeCV(alphas=alphas) ridge_cv.fit(X_train, y_train) # Get the best alpha from cross-validation optimal_alpha = ridge_cv.alpha_ # Get the coefficients coefficients = ridge_cv.coef_ # Predict on the test set y_pred = ridge_cv.predict(X_test) # Calculate the MSE on the test data mse = mean_squared_error(y_test, y_pred) return optimal_alpha, coefficients, mse"},{"question":"# Objective You are required to write a function that visualizes the relationship between different networks in a brain dataset using seaborn and demonstrates various customization options. # Dataset You will be using the `brain_networks` dataset. It can be loaded using: ```python import seaborn as sns from seaborn import load_dataset networks = load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) ``` # Task Requirements 1. Load the dataset and preprocess it as follows: - Rename the axis to \\"timepoint\\". - Stack the multi-index columns. - Group by `timepoint`, `network`, and `hemi` and compute the mean. - Unstack the `network` level. - Reset the index. - Filter the data to include only `timepoint` < 100. 2. Create a pair plot comparing network activity focusing on the following axes combinations: - `x`-axis values: [\\"5\\", \\"8\\", \\"12\\", \\"15\\"] - `y`-axis values: [\\"6\\", \\"13\\", \\"16\\"] 3. Customize the plot to have a size of (8, 5), and share axes (both `x` and `y`). 4. Add paths to the plot: - Set `color` based on the `hemi` column. - Customize the properties with `linewidth` 1 and `alpha` 0.8. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd def visualize_brain_networks(): # Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display the plot p.show() ``` # Constraints - You must only use seaborn and pandas for data manipulation and visualization. - Ensure your solution is efficient and leverages the seaborn object\'s interface correctly. # Expected Output The function should display a pair plot with the specified customizations, showing the trajectory relationships between the specified networks and differentiation between hemispheres.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def visualize_brain_networks(): # Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display the plot p.show()"},{"question":"**Question:** Using the `spwd` module in Python, you are required to implement a function that performs two tasks: 1. Retrieves the shadow password database entry for a given user name. 2. Calculates and returns the average time in days between password changes for all the users in the shadow password database. Function Signature: ```python def shadow_password_analysis(username: str) -> (dict, float): Given a user name, this function performs two tasks: 1. Retrieves the shadow password database entry for the given username. 2. Calculates and returns the average time in days between password changes for all users. Args: username (str): The login name of the user whose shadow password database entry is to be retrieved. Returns: tuple: A tuple containing: - dict: A dictionary with the following keys corresponding to the user\'s shadow password data: - \'sp_namp\': Login name - \'sp_pwdp\': Encrypted password - \'sp_lstchg\': Date of last change - \'sp_min\': Minimal number of days between changes - \'sp_max\': Maximum number of days between changes - \'sp_warn\': Number of days before password expires to warn user about it - \'sp_inact\': Number of days after password expires until account is disabled - \'sp_expire\': Number of days since 1970-01-01 when account expires - \'sp_flag\': Reserved - float: The average time in days between password changes for all users in the shadow password database. Raises: PermissionError: If the user doesn\'t have privileges to access the shadow password database. KeyError: If the entry for the given user name cannot be found. pass ``` Constraints: - You must use the `spwd` module to access the shadow password database. - Handle `PermissionError` and `KeyError` exceptions appropriately. - The function should return data in the format specified above. **Example:** ```python # Example usage try: user_data, average_days = shadow_password_analysis(\\"testuser\\") print(user_data) print(f\\"Average days between password changes: {average_days:.2f}\\") except PermissionError: print(\\"Insufficient privileges to access the shadow password database.\\") except KeyError: print(\\"The user \'testuser\' cannot be found.\\") ``` **Notes:** - You need root privileges to execute functions from the `spwd` module. - Assume that there is at least one user in the shadow password database and at least one entry for the provided `username`.","solution":"import spwd import os def shadow_password_analysis(username: str) -> (dict, float): try: # Retrieve the shadow password entry for the given username user_info = spwd.getspnam(username) # Extract the relevant information into a dictionary user_data = { \'sp_namp\': user_info.sp_namp, \'sp_pwdp\': user_info.sp_pwdp, \'sp_lstchg\': user_info.sp_lstchg, \'sp_min\': user_info.sp_min, \'sp_max\': user_info.sp_max, \'sp_warn\': user_info.sp_warn, \'sp_inact\': user_info.sp_inact, \'sp_expire\': user_info.sp_expire, \'sp_flag\': user_info.sp_flag } # Retrieve all entries in the shadow password database all_users_info = spwd.getspall() # Calculate the average maximum number of days between password changes total_days = sum(user.sp_max for user in all_users_info) average_days_between_changes = total_days / len(all_users_info) return user_data, average_days_between_changes except PermissionError as e: raise PermissionError(\\"Insufficient privileges to access the shadow password database.\\") from e except KeyError as e: raise KeyError(f\\"The user \'{username}\' cannot be found.\\") from e"},{"question":"# Objective: Implement a custom Python logging setup as described below. The aim is to assess your understanding of advanced logging concepts, including custom handlers, filters, contextual information, and logging to multiple destinations. # Task: You need to create a logging system with the following requirements: 1. **Log Levels and Message Formatting**: - Log messages must include the time, logger name, log level, and message. - Time should be formatted in UTC. - Log messages for the console should be simple, while those for the file should be detailed. 2. **Custom Handlers and Filters**: - Implement two handlers: one for logging to a file and another to console. - The file handler should log all messages with log level `DEBUG` and above. - The console handler should log messages with log level `INFO` and above. - Filter out messages containing the word \\"sensitive\\" to avoid logging them. 3. **Contextual Information**: - Add contextual information such as the username and IP address of the user performing the action, which should be included in the log messages. # Implementation Details: 1. **Logging Configuration**: - Implement the logging configuration via a dictionary-based configuration using `logging.config.dictConfig()`. - The format for console log messages should be: `[%(asctime)s] %(name)s - %(levelname)s - %(message)s` - The format for file log messages should be: `[%(asctime)s][%(username)s][%(ip)s] %(name)s - %(levelname)s - %(message)s` - Use UTC for time in log messages. 2. **Custom Filter**: - Implement a custom filter to exclude messages containing the word \\"sensitive\\". 3. **Contextual Information**: - Implement a mechanism to pass and use contextual information (e.g., username and IP address) in the log messages. # Example Usage: ```python import logging import logging.config # Define the custom logging configuration. LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'[%(asctime)s] %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'detailed\': { \'format\': \'[%(asctime)s][%(username)s][%(ip)s] %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'filters\': { \'no_sensitive\': { \'()\': \'your_module.NoSensitiveFilter\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'simple\', \'filters\': [\'no_sensitive\'], }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'filters\': [\'no_sensitive\'], }, }, \'loggers\': { \'\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, }, } # Apply the logging configuration. logging.config.dictConfig(LOGGING_CONFIG) # Obtain a logger instance. logger = logging.getLogger(\'exampleLogger\') # Add contextual information. extra = {\'username\': \'User1\', \'ip\': \'127.0.0.1\'} # Example log messages. logger.debug(\'This is a debug message\', extra=extra) logger.info(\'This is a sensitive information\', extra=extra) logger.warning(\'This is a warning message\', extra=extra) # Define the NoSensitiveFilter class here. ``` # Expected Output: The file \\"app.log\\" should contain: ``` [2023-03-23 23:47:11,000][User1][127.0.0.1] exampleLogger - DEBUG - This is a debug message [2023-03-23 23:47:11,000][User1][127.0.0.1] exampleLogger - WARNING - This is a warning message ``` The console should show: ``` [2023-03-23 23:47:11] exampleLogger - WARNING - This is a warning message ``` Notes: - You must implement the `NoSensitiveFilter` class to exclude messages containing the word \\"sensitive\\". - Ensure that your implementation handles the passage of contextual information properly. - You should submit a Python script (`your_module.py`) that defines the logging configuration, the custom filter, and demonstrates the usage.","solution":"import logging import logging.config # Define the custom filter to exclude \'sensitive\' messages. class NoSensitiveFilter(logging.Filter): def filter(self, record): return \'sensitive\' not in record.getMessage() # Define the custom logging configuration. LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'[%(asctime)s] %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'detailed\': { \'format\': \'[%(asctime)s][%(username)s][%(ip)s] %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'filters\': { \'no_sensitive\': { \'()\': NoSensitiveFilter, }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'simple\', \'filters\': [\'no_sensitive\'], }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'filters\': [\'no_sensitive\'], }, }, \'loggers\': { \'\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, }, } # Apply the logging configuration. logging.config.dictConfig(LOGGING_CONFIG) # Obtain a logger instance. logger = logging.getLogger(\'exampleLogger\') # Add contextual information. extra = {\'username\': \'User1\', \'ip\': \'127.0.0.1\'} # Example log messages. logger.debug(\'This is a debug message\', extra=extra) logger.info(\'This is a sensitive information\', extra=extra) logger.warning(\'This is a warning message\', extra=extra)"},{"question":"# Audio Processing: Implement a Volume Control and Format Conversion Utility You are required to implement a function that processes an audio fragment by adjusting its volume, converting its sample width, and optionally converting it to a different encoding format. Function Signature ```python def process_audio_fragment(fragment, sample_width, new_sample_width, volume_factor, to_encoding=None): Processes the given audio fragment by adjusting its volume and converting its sample width. :param fragment: A bytes-like object containing raw audio data. :param sample_width: An integer representing the width of each sample in bytes (1, 2, 3, or 4). :param new_sample_width: An integer representing the desired width of each sample in bytes (1, 2, 3, or 4). :param volume_factor: A float representing the factor by which to adjust the volume (e.g., 0.5 to halve the volume, 2.0 to double the volume). :param to_encoding: An optional string specifying the target encoding format (\'alaw\' or \'ulaw\'). If None, no encoding conversion is performed. :return: A bytes object representing the processed audio fragment. pass ``` Requirements 1. **Input and Output**: - The function takes a bytes-like object `fragment` containing the raw audio data. - The `sample_width` parameter specifies the width of the samples in the input fragment. - The `new_sample_width` specifies the desired width of the samples in the output. - The `volume_factor` is a floating-point number representing the factor by which the volume should be adjusted. - The `to_encoding` parameter is an optional string that can be \'alaw\' or \'ulaw\' to specify the desired output encoding format. - The function should return a processed bytes object. 2. **Constraints and Limitations**: - The input `fragment` should be non-empty. - The `sample_width` and `new_sample_width` should be one of {1, 2, 3, 4}. - The `volume_factor` must be a positive floating-point number. 3. **Performance Requirements**: - The function should handle fragments of reasonable length (e.g., up to several megabytes) efficiently. Example ```python sample_data = b\'x01x02x03x04\' # Example audio fragment sample_width = 2 # 2-byte samples new_sample_width = 1 # Convert to 1-byte samples volume_factor = 1.5 # Increase volume by 50% to_encoding = \'alaw\' # Convert to a-LAW encoding result = process_audio_fragment(sample_data, sample_width, new_sample_width, volume_factor, to_encoding) print(result) # Should print the processed audio fragment ``` Use the `audioop` module functions to implement this utility. Ensure to handle the conversions and volume adjustment accurately according to the provided parameters.","solution":"import audioop def process_audio_fragment(fragment, sample_width, new_sample_width, volume_factor, to_encoding=None): Processes the given audio fragment by adjusting its volume and converting its sample width. :param fragment: A bytes-like object containing raw audio data. :param sample_width: An integer representing the width of each sample in bytes (1, 2, 3, or 4). :param new_sample_width: An integer representing the desired width of each sample in bytes (1, 2, 3, or 4). :param volume_factor: A float representing the factor by which to adjust the volume (e.g., 0.5 to halve the volume, 2.0 to double the volume). :param to_encoding: An optional string specifying the target encoding format (\'alaw\' or \'ulaw\'). If None, no encoding conversion is performed. :return: A bytes object representing the processed audio fragment. # Adjust the volume adjusted_volume_fragment = audioop.mul(fragment, sample_width, volume_factor) # Convert the sample width if needed if sample_width != new_sample_width: converted_fragment = audioop.lin2lin(adjusted_volume_fragment, sample_width, new_sample_width) else: converted_fragment = adjusted_volume_fragment # Convert the encoding if needed if to_encoding == \'alaw\': encoded_fragment = audioop.lin2alaw(converted_fragment, new_sample_width) elif to_encoding == \'ulaw\': encoded_fragment = audioop.lin2ulaw(converted_fragment, new_sample_width) else: encoded_fragment = converted_fragment return encoded_fragment"},{"question":"You are given a list of student records, where each record is a dictionary containing the student\'s \'name\', \'age\', and a list of \'grades\'. Your task is to implement a function `process_student_records` that processes this list and returns a report with the following functionalities: 1. Calculate the average grade for each student and create a new dictionary with the student\'s name as the key and their average grade as the value. 2. Identify and return the name(s) of the student(s) with the highest average grade. 3. Create a list of tuples where each tuple contains a student\'s name and their number of grades. 4. Remove any students younger than a given age and return the updated list of records. # Function Signature: ```python def process_student_records(students: list, min_age: int) -> tuple: pass ``` # Input: - `students` (list): A list of dictionaries where each dictionary represents a student with the following keys: - `\'name\'` (str): The name of the student. - `\'age\'` (int): The age of the student. - `\'grades\'` (list): A list of integers representing the grades of the student. - `min_age` (int): The minimum age requirement; students younger than this age should be removed from the list. # Output: - A tuple containing: 1. A dictionary with student names as keys and their average grades as values. 2. A list of student names with the highest average grade. 3. A list of tuples where each tuple contains a student\'s name and their number of grades. 4. An updated list of student records, removing any students who are younger than `min_age`. # Example: ```python students = [ {\'name\': \'Alice\', \'age\': 22, \'grades\': [88, 75, 92]}, {\'name\': \'Bob\', \'age\': 19, \'grades\': [95, 100, 90]}, {\'name\': \'Charlie\', \'age\': 20, \'grades\': [70, 85]}, {\'name\': \'Dave\', \'age\': 18, \'grades\': [60, 65, 70]} ] min_age = 20 result = process_student_records(students, min_age) # Result should be: # ( # {\'Alice\': 85.0, \'Charlie\': 77.5}, # [\'Alice\'], # [(\'Alice\', 3), (\'Charlie\', 2)], # [{\'name\': \'Alice\', \'age\': 22, \'grades\': [88, 75, 92]}, {\'name\': \'Charlie\', \'age\': 20, \'grades\': [70, 85]}] # ) ``` # Constraints: - Each student\'s name is unique. - The list of students is not empty. - Grades are integers between 0 and 100. Implement the function `process_student_records`.","solution":"def process_student_records(students, min_age): # Step 1: Calculate the average grade for each student average_grades = {} for student in students: if student[\'grades\']: avg_grade = sum(student[\'grades\']) / len(student[\'grades\']) else: avg_grade = 0 average_grades[student[\'name\']] = avg_grade # Step 2: Identify the student(s) with the highest average grade max_avg = max(average_grades.values()) top_students = [name for name, avg in average_grades.items() if avg == max_avg] # Step 3: Create list of tuples (name, number of grades) num_grades_list = [(student[\'name\'], len(student[\'grades\'])) for student in students] # Step 4: Remove students younger than min_age filtered_students = [student for student in students if student[\'age\'] >= min_age] return average_grades, top_students, num_grades_list, filtered_students"},{"question":"**Coding Assessment Question:** # Objective: Implement a Python class that mimics the behavior of a C-based PyObject type and manages reference counting explicitly. # Description: You are required to create a Python class named `PyObject` which will contain basic attributes and methods similar to those described in the provided documentation. This class should simulate the reference counting mechanism used in Python\'s C API. # Requirements: 1. **Class Definition:** - Create a class `PyObject` with the following attributes: - `ob_refcnt`: An integer indicating the object\'s reference count. - `ob_type`: A string representing the type of the object. 2. **Methods:** - `__init__(self, ob_type: str)`: Constructor to initialize the type and set the reference count to 1. - `get_refcnt(self) -> int`: Method to return the current reference count. - `inc_ref(self)`: Method to increment the reference count by 1. - `dec_ref(self)`: Method to decrement the reference count by 1. If the reference count reaches 0, print a message indicating that the object is being destroyed. - `get_type(self) -> str`: Method to return the type of the object. 3. **Functionality Testing:** - Create instances of `PyObject` and manipulate their reference counts using the `inc_ref()` and `dec_ref()` methods. Validate that the reference count mechanisms work as expected. # Example: ```python # Define the PyObject class based on the requirements class PyObject: def __init__(self, ob_type: str): self.ob_refcnt = 1 self.ob_type = ob_type def get_refcnt(self) -> int: return self.ob_refcnt def inc_ref(self): self.ob_refcnt += 1 def dec_ref(self): self.ob_refcnt -= 1 if self.ob_refcnt == 0: print(f\\"The {self.ob_type} object is being destroyed.\\") def get_type(self) -> str: return self.ob_type # Testing the PyObject class obj = PyObject(\'ExampleType\') print(obj.get_refcnt()) # Output: 1 obj.inc_ref() print(obj.get_refcnt()) # Output: 2 obj.dec_ref() print(obj.get_refcnt()) # Output: 1 obj.dec_ref() # Output: The ExampleType object is being destroyed ``` # Notes: - Ensure your class handles the reference count correctly. - The implementation should be robust and handle edge cases, such as decrementing the reference count below 0. Good luck and happy coding!","solution":"class PyObject: def __init__(self, ob_type: str): self.ob_refcnt = 1 self.ob_type = ob_type def get_refcnt(self) -> int: return self.ob_refcnt def inc_ref(self): self.ob_refcnt += 1 def dec_ref(self): self.ob_refcnt -= 1 if self.ob_refcnt == 0: print(f\\"The {self.ob_type} object is being destroyed.\\") def get_type(self) -> str: return self.ob_type"},{"question":"**Objective:** To assess comprehension in file handling, exception handling, and integration of file descriptors within Python, students are required to use the Python C API concepts detailed in the documentation. **Problem Statement:** Implement a Python function `custom_file_operations(fd, mode, input_str)` that performs the following operations: 1. Creates a Python file object from an existing file descriptor (`fd`) using the specified `mode`. 2. Writes `input_str` to the file object. 3. Reads back the content from the file object. 4. Returns the read content as a string. Ensure to handle any exceptions appropriately and close the file descriptor if it was originally opened within the function. **Function Signature:** ```python def custom_file_operations(fd: int, mode: str, input_str: str) -> str: pass ``` **Parameters:** - `fd` (int): The file descriptor of an already opened file. - `mode` (str): The mode in which the file should be opened (e.g., \'w+\' for writing and reading). - `input_str` (str): The input string to be written to the file. **Returns:** - `str`: The content read back from the file. **Constraints:** - You may assume the file descriptor `fd` is valid and the file can be opened with the given `mode`. - Ensure to use appropriate error handling to manage any potential issues during file operations. **Example:** ```python import os import tempfile # Create a temporary file and get its file descriptor fd, path = tempfile.mkstemp() try: # Example usage of custom_file_operations function result = custom_file_operations(fd, \'w+\', \'Hello, World!\') print(result) # Expected output: \'Hello, World!\' finally: os.close(fd) os.remove(path) ``` **Notes:** - Ensure your implementation leverages Python\'s built-in `io` functionalities and adheres to the concepts shown in the provided documentation. - Properly manage resources (i.e., file descriptors) to avoid resource leaks.","solution":"import os import io def custom_file_operations(fd: int, mode: str, input_str: str) -> str: try: # create a Python file object from the file descriptor with io.open(fd, mode) as file_obj: # Write to the file object file_obj.write(input_str) # Move the cursor to the beginning to read the content back file_obj.seek(0) # Read the content back from the file object content = file_obj.read() return content except Exception as e: raise e finally: try: # close the file descriptor os.close(fd) except OSError: pass"},{"question":"# Custom Type Creation using Python\'s C API **Objective:** Create a custom Python type from a module specification using Python\'s C API through `python310`. Implement this custom type as a heap-allocated type with specific behaviors defined using slots. # **Instructions:** 1. **Type Specification:** - Define a type named `CustomType`. - Inherit from the built-in `object` class. - Include a few custom slots: - `tp_init`: Custom constructor that initializes with a single integer attribute `value`. - `tp_dealloc`: Custom destructor. - `tp_repr`: Custom string representation (`__repr__`), which should return the string `CustomType(value=<value>)`. 2. **Module Definition:** - Create a module named `custommodule` and register `CustomType` within this module. 3. **Implementation Details:** - Utilize `PyType_FromModuleAndSpec` to define `CustomType` ensuring it’s created as a heap type. - Ensure proper memory management in the constructor and destructor. - Specifically, handle any reference counting issues and ensure the `value` attribute is correctly managed. 4. **Verify Type Behavior:** - Write a Python function `create_custom_type()` that imports the module, creates an instance of `CustomType`, sets the value, and prints its representation. For example: ```python import custommodule def create_custom_type(): obj = custommodule.CustomType(10) print(obj) # Expected output: CustomType(value=10) ``` # **Constraints:** - You must define the type and its behavior using only the provided Python C API functions from `python310`. - Ensure your solution handles resource management appropriately, avoiding memory leaks. - Thoroughly test your type’s behavior and ensure it aligns with expected Python object semantics. # **Expected Output:** Your resulting solution should correctly implement the `CustomType` with correct initialization, string representation, and memory management as per the guidelines above. # **Evaluation Criteria:** - Correctness of the type creation process. - Proper slot handling (init, dealloc, repr). - Handling of inheritance and module association correctly. - Proper memory and reference management.","solution":"def CustomType_init(self, value): self.value = value def CustomType_repr(self): return f\\"CustomType(value={self.value})\\" class CustomType: def __init__(self, value=0): CustomType_init(self, value) def __repr__(self): return CustomType_repr(self)"},{"question":"**Title: Context Variable Management in Asynchronous Tasks** Problem Statement You are tasked to implement a system that simulates processing multiple user requests in an asynchronous server environment, while ensuring each request\'s context remains isolated. You will use the `contextvars` module to achieve this. You need to simulate a scenario where each request increments a counter specific to the user, stores some session variables, and at the end of the request, resets the context to maintain isolation. Requirements 1. Implement a function `process_request(session_id, counter, session_var)` that uses `contextvars` to: - Set the `session_id`. - Increment the `counter` for that session by one. - Store the `session_var` in the context. - Return a dictionary containing the updated `counter` and `session_var`. 2. Implement an asynchronous `main` function to simulate handling multiple requests concurrently using `asyncio`. - Each request will call the `process_request` function. - Ensure that the context is reset properly after each request is processed. Function Signatures ```python import asyncio import contextvars # function to process a request def process_request(session_id: str, counter: int, session_var: str) -> dict: pass # async function to simulate handling requests async def main() -> None: pass ``` Example ```python # Sample simulation: # Simulate processing requests asyncio.run(main()) # Expected output: # [{\'session_id\': \'user1\', \'counter\': 1, \'session_var\': \'data1\'}, # {\'session_id\': \'user2\', \'counter\': 1, \'session_var\': \'data2\'}, # {\'session_id\': \'user1\', \'counter\': 2, \'session_var\': \'data3\'}] ``` Constraints 1. Use the `contextvars` module as described to manage context-local state. 2. Assume `session_id` is always a string, `counter` is an integer, and `session_var` is a string. 3. Ensure the solution is free from race conditions and context bleeding across asynchronous tasks. Notes - Utilize the `ContextVar` class to declare context variables for session ID, counter, and session variables. - Use the `copy_context()` function and `context.run()` method to manage and isolate context for each request. - Carefully reset the context after processing each request to maintain proper state management and isolation. **Good luck!**","solution":"import asyncio import contextvars session_id_var = contextvars.ContextVar(\\"session_id\\") counter_var = contextvars.ContextVar(\\"counter\\") session_var_entry = contextvars.ContextVar(\\"session_var\\") def process_request(session_id: str, counter: int, session_var: str) -> dict: session_id_var.set(session_id) counter_var.set(counter + 1) session_var_entry.set(session_var) return { \\"session_id\\": session_id_var.get(), \\"counter\\": counter_var.get(), \\"session_var\\": session_var_entry.get() } async def handle_request(session_id: str, counter: int, session_var: str) -> dict: ctx = contextvars.copy_context() return await asyncio.wrap_future(asyncio.get_event_loop().run_in_executor( None, ctx.run, process_request, session_id, counter, session_var )) async def main(): requests = [ (\\"user1\\", 0, \\"data1\\"), (\\"user2\\", 0, \\"data2\\"), (\\"user1\\", 1, \\"data3\\"), ] results = await asyncio.gather( *[handle_request(session_id, counter, session_var) for session_id, counter, session_var in requests] ) for result in results: print(result)"},{"question":"# Question: Implementing a Simple NIS-based User Information Retrieval System **Objective:** Implement a Python program using the deprecated \\"nis\\" module to retrieve and display user information based on a username lookup. **Task:** You are required to write a function `user_info(username: str) -> dict` that retrieves information about a user from an NIS map called `\\"passwd.byname\\"`. This map contains user information where each key is a username, and each value is a colon-separated string of user details typically found in the `/etc/passwd` file. The function should: 1. Use `nis.match()` to get the user information string for the given username. 2. Parse the retrieved string to extract specific user details. 3. Return a dictionary containing the following user details: - `username`: the username - `password`: the user\'s password (note that this might be shadowed and not directly available) - `user_id`: the user\'s ID - `group_id`: the user\'s group ID - `user_info`: additional user information (this is usually the comment field) - `home_directory`: the user\'s home directory - `shell`: the user\'s login shell **Function Signature:** ```python def user_info(username: str) -> dict: pass ``` **Example:** ```python # Example user data in NIS map \\"passwd.byname\\": # Key: \\"jdoe\\" # Value: \\"jdoe:password:1001:1001:John Doe:/home/jdoe:/bin/bash\\" user_data = user_info(\\"jdoe\\") print(user_data) ``` **Expected Output:** ```python { \\"username\\": \\"jdoe\\", \\"password\\": \\"password\\", \\"user_id\\": \\"1001\\", \\"group_id\\": \\"1001\\", \\"user_info\\": \\"John Doe\\", \\"home_directory\\": \\"/home/jdoe\\", \\"shell\\": \\"/bin/bash\\" } ``` **Constraints:** - Ensure that the function raises a custom Exception with an appropriate message if the NIS query fails or if the user is not found. - You may assume that the NIS system is properly configured and that the `\\"passwd.byname\\"` map exists. **Notes:** - Since the \\"nis\\" module is deprecated in Python 3.11, this exercise assumes the use of an environment where it is still functional. - Do not use external libraries to parse the NIS map; rely solely on the provided \\"nis\\" module and standard Python functions.","solution":"import nis def user_info(username: str) -> dict: Retrieves user information from the NIS map \\"passwd.byname\\" based on the provided username. Args: username (str): The username to lookup in the NIS map. Returns: dict: A dictionary containing user details such as username, password, user_id, group_id, user_info, home_directory, and shell. Raises: Exception: If there is an error retrieving data from NIS or the user is not found. try: user_data_str = nis.match(username, \\"passwd.byname\\") user_data_str = user_data_str.decode(\'utf-8\') except nis.error: raise Exception(f\\"Could not find user \'{username}\' in NIS map \'passwd.byname\'\\") user_data_list = user_data_str.split(\':\') if len(user_data_list) != 7: raise Exception(f\\"Unexpected user data format for user \'{username}\'\\") user_info_dict = { \\"username\\": user_data_list[0], \\"password\\": user_data_list[1], \\"user_id\\": user_data_list[2], \\"group_id\\": user_data_list[3], \\"user_info\\": user_data_list[4], \\"home_directory\\": user_data_list[5], \\"shell\\": user_data_list[6] } return user_info_dict"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},D={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],M={key:0},L={key:1};function N(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",D,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+u(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",M,"See more"))],8,F)):d("",!0)])}const O=p(z,[["render",N],["__scopeId","data-v-a7ce6604"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/56.md","filePath":"chatai/56.md"}'),j={name:"chatai/56.md"},X=Object.assign(j,{setup(n){return(e,l)=>(a(),s("div",null,[x(O)]))}});export{Y as __pageData,X as default};
