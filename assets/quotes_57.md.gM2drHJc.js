import{_ as p,o as a,c as o,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,i,s){return a(),o("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-6e3830ca"]]),A=JSON.parse('[{"question":"# Asynchronous TCP Server-Client Model Objective Design and implement an asynchronous TCP server-client model using the asyncio package. Description You are required to implement two components: an asynchronous TCP server and an asynchronous TCP client. The server will receive messages from the client and respond by echoing the message back transformed to upper case. The client will send a predefined message to the server and print the response received from the server. Requirements **Server Implementation** 1. Create an asynchronous TCP server that listens on `localhost` at port `8888`. 2. The server should handle multiple client connections concurrently. 3. When a client sends a message to the server, the server should transform it to upper case and send it back to the client. 4. Close the connection gracefully after sending the response. **Client Implementation** 1. Create an asynchronous TCP client that connects to the server at `localhost` on port `8888`. 2. The client should send a predefined message to the server and wait for the server\'s response. 3. Print the response received from the server. 4. Close the connection gracefully after receiving the response. Function Signatures ```python async def start_server(): # Start the server and handle incoming connections async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # Handle individual client connection async def start_client(message: str): # Connect to the server, send a message, and receive the response ``` Input and Output **Server:** - No input - Output should be displayed in the console, showing connections and messages processed **Client:** - Input: A predefined string message - Output: The server\'s response to the predefined message Constraints - Use the asyncio package for asynchronous operations. - Ensure that the server can handle multiple clients concurrently without blocking. - Ensure proper exception handling for network operations. Example ```python # Run the server await start_server() # Run the client await start_client(\\"hello world\\") # Console Output (Server Side): # Connection from (\'127.0.0.1\', 12345) # Received: hello world # Sent: HELLO WORLD # Connection closed # Console Output (Client Side): # Received: HELLO WORLD ``` Notes - The provided example shows how the server and client interact and the expected console outputs. - Ensure to test the implementation to demonstrate the server handling multiple clients concurrently.","solution":"import asyncio async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") response = message.upper() print(f\\"Send: {response!r}\\") writer.write(response.encode()) await writer.drain() print(\\"Closing the connection\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def start_client(message: str): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') print(\'Closing the connection\') writer.close() await writer.wait_closed()"},{"question":"# Question: You are required to implement a custom cookie handling mechanism using the `http.cookiejar` module in Python. The goal is to create a cookie manager that can: 1. Load cookies from a file. 2. Add a new cookie following specific rules. 3. Retrieve cookies based on a specific policy. 4. Save the modified cookies back to the file. Here\'s the detailed breakdown of the requirements: 1. **Loading Cookies**: - Implement a function `load_cookies(filename: str) -> http.cookiejar.MozillaCookieJar` that loads cookies from a given file into a `MozillaCookieJar` object. 2. **Adding/Modifying Cookies**: - Implement a function `add_cookie(cj: http.cookiejar.MozillaCookieJar, name: str, value: str, domain: str) -> None` that adds a new cookie or updates an existing cookie with the given name, value, and domain to the `MozillaCookieJar` object. - Ensure that the cookie has a secure attribute set to `True` and a path attribute set to `/`. 3. **Retrieve Cookies**: - Implement a function `get_cookies(cj: http.cookiejar.MozillaCookieJar, domain: str) -> list` that retrieves all cookies for the specified domain and returns a list of (name, value) tuples. 4. **Saving Cookies**: - Implement a function `save_cookies(cj: http.cookiejar.MozillaCookieJar, filename: str) -> None` that saves the cookies present in the `MozillaCookieJar` object to the specified file. # Constraints: - You must use the `http.cookiejar` module classes and not any other cookie management libraries. - The file format for saving and loading cookies should be compatible with the Mozilla cookies.txt format. - Ensure that all cookies added programmatically have their `secure` attribute set to `True`. # Example Usage: ```python # Initialize the CookieJar cookie_jar = load_cookies(\'cookies.txt\') # Add a new cookie add_cookie(cookie_jar, \'sessionid\', \'abcd1234\', \'.example.com\') # Retrieve cookies for a specific domain cookies = get_cookies(cookie_jar, \'.example.com\') print(cookies) # Save cookies back to file save_cookies(cookie_jar, \'cookies.txt\') ``` # Expected Input and Output: - `load_cookies(filename: str) -> http.cookiejar.MozillaCookieJar` - **Input**: \'cookies.txt\' - **Output**: MozillaCookieJar object loaded with cookies from the file. - `add_cookie(cj: http.cookiejar.MozillaCookieJar, name: str, value: str, domain: str) -> None` - **Input**: MozillaCookieJar object, \'sessionid\', \'abcd1234\', \'.example.com\' - **Output**: None (cookie should be added to the MozillaCookieJar object) - `get_cookies(cj: http.cookiejar.MozillaCookieJar, domain: str) -> list` - **Input**: MozillaCookieJar object, \'.example.com\' - **Output**: List of cookies for the domain in the format [(\'name\', \'value\')] - `save_cookies(cj: http.cookiejar.MozillaCookieJar, filename: str) -> None` - **Input**: MozillaCookieJar object, \'new_cookies.txt\' - **Output**: None (cookies should be saved in the file) Implement these functions to manage HTTP cookies efficiently and ensure the correct handling as per the requirement.","solution":"import http.cookiejar import os def load_cookies(filename: str) -> http.cookiejar.MozillaCookieJar: Load cookies from a file into a MozillaCookieJar object. cj = http.cookiejar.MozillaCookieJar() if os.path.exists(filename): cj.load(filename, ignore_discard=True, ignore_expires=True) return cj def add_cookie(cj: http.cookiejar.MozillaCookieJar, name: str, value: str, domain: str) -> None: Add a new cookie or update an existing cookie in the MozillaCookieJar object. cookie = http.cookiejar.Cookie( version=0, name=name, value=value, port=None, port_specified=False, domain=domain, domain_specified=True, domain_initial_dot=domain.startswith(\'.\'), path=\\"/\\", path_specified=True, secure=True, expires=None, discard=False, comment=None, comment_url=None, rest={\\"HttpOnly\\": False}, rfc2109=False ) cj.set_cookie(cookie) def get_cookies(cj: http.cookiejar.MozillaCookieJar, domain: str) -> list: Retrieve all cookies for the specified domain. cookies = [] for cookie in cj: if cookie.domain == domain: cookies.append((cookie.name, cookie.value)) return cookies def save_cookies(cj: http.cookiejar.MozillaCookieJar, filename: str) -> None: Save the cookies present in the MozillaCookieJar object to the specified file. cj.save(filename, ignore_discard=True, ignore_expires=True)"},{"question":"<|Analysis Begin|> The provided documentation describes the `cmd` module in Python, which provides a framework for building line-oriented command interpreters. This module is useful for creating custom shells that allow users to interact with a program through commands. Key Features: 1. **Cmd Class**: The `Cmd` class enables the creation of a command-line interface by inheriting its methods and defining custom commands using methods prefixed with `do_`. 2. **Methods**: - `cmdloop()`: Starts the command interpreter loop. - `onecmd()`: Processes a single command. - `emptyline()`, `default()`, `precmd()`, `postcmd()`, `preloop()`, `postloop()`: Hook methods for handling specific events in the command processing workflow. - `completedefault()`: Method for command completion. 3. **Instance Variables**: - `prompt`: The prompt string. - `intro`: Introductory text shown at the start of `cmdloop()`. - `cmdqueue`: Queue of commands to be executed. - `use_rawinput`, `identchars`, `lastcmd`, etc. The Cmd class aims to simplify writing interpreters by wrapping the boilerplate around handling input, help commands, and command execution flow. Based on this understanding, we can design a coding question that involves creating a custom command interpreter using the `cmd` module. <|Analysis End|> <|Question Begin|> # Custom Command Interpreter for a Simple File System You are tasked with creating a custom command-line interpreter for managing a simple file system of a single directory. This interpreter will allow users to create, read, and delete files. The files will only store text content. Implement a class `FileSystemShell` that inherits from `cmd.Cmd`. This class should provide the following commands: 1. **create <filename> <content>** - Description: Create a new file with the given filename and content. If a file with the same name already exists, it should display an error message. - Example: `create example.txt \\"Hello, World!\\"` 2. **read <filename>** - Description: Read and display the content of the specified file. If the file does not exist, it should display an error message. - Example: `read example.txt` 3. **delete <filename>** - Description: Delete the specified file. If the file does not exist, it should display an error message. - Example: `delete example.txt` 4. **list** - Description: List all files in the directory. - Example: `list` 5. **exit** - Description: Exit the command interpreter. - Example: `exit` You should also provide appropriate help messages for each command using docstrings. Here\'s a skeleton of the class to get you started: ```python import cmd class FileSystemShell(cmd.Cmd): intro = \'Welcome to the simple file system shell. Type help or ? to list commands.n\' prompt = \'(filesystem) \' files = {} def do_create(self, arg): \'Create a new file with the given filename and content: CREATE <filename> <content>\' args = arg.split(maxsplit=1) if len(args) < 2: print(\\"Error: Missing filename or content.\\") return filename, content = args if filename in self.files: print(f\\"Error: File \'{filename}\' already exists.\\") else: self.files[filename] = content print(f\\"File \'{filename}\' created.\\") def do_read(self, arg): \'Read the content of the specified file: READ <filename>\' filename = arg.strip() if filename in self.files: print(self.files[filename]) else: print(f\\"Error: File \'{filename}\' not found.\\") def do_delete(self, arg): \'Delete the specified file: DELETE <filename>\' filename = arg.strip() if filename in self.files: del self.files[filename] print(f\\"File \'{filename}\' deleted.\\") else: print(f\\"Error: File \'{filename}\' not found.\\") def do_list(self, arg): \'List all files in the directory: LIST\' if self.files: for filename in self.files: print(filename) else: print(\\"No files found.\\") def do_exit(self, arg): \'Exit the command shell: EXIT\' print(\'Exiting the simple file system shell.\') return True if __name__ == \'__main__\': FileSystemShell().cmdloop() ``` # Constraints: - Filenames must not contain spaces. - Content can be any string. # Input and Output Your program should run in an interactive mode similar to a command shell, accepting commands from the user and providing appropriate responses based on the command. Ensure you handle edge cases such as missing arguments or invalid command formats gracefully. # Performance Requirements: - The command interpreter should handle typical use cases efficiently. - Implement the command operations with optimal time complexity (e.g., O(1) for reading and writing to the file dictionary). Good luck!","solution":"import cmd class FileSystemShell(cmd.Cmd): intro = \'Welcome to the simple file system shell. Type help or ? to list commands.n\' prompt = \'(filesystem) \' files = {} def do_create(self, arg): \'Create a new file with the given filename and content: CREATE <filename> <content>\' args = arg.split(maxsplit=1) if len(args) < 2: print(\\"Error: Missing filename or content.\\") return filename, content = args if filename in self.files: print(f\\"Error: File \'{filename}\' already exists.\\") else: self.files[filename] = content print(f\\"File \'{filename}\' created.\\") def do_read(self, arg): \'Read the content of the specified file: READ <filename>\' filename = arg.strip() if filename in self.files: print(self.files[filename]) else: print(f\\"Error: File \'{filename}\' not found.\\") def do_delete(self, arg): \'Delete the specified file: DELETE <filename>\' filename = arg.strip() if filename in self.files: del self.files[filename] print(f\\"File \'{filename}\' deleted.\\") else: print(f\\"Error: File \'{filename}\' not found.\\") def do_list(self, arg): \'List all files in the directory: LIST\' if self.files: for filename in self.files: print(filename) else: print(\\"No files found.\\") def do_exit(self, arg): \'Exit the command shell: EXIT\' print(\'Exiting the simple file system shell.\') return True if __name__ == \'__main__\': FileSystemShell().cmdloop()"},{"question":"# Custom Interactive Python Console **Objective**: Implement a customized interactive Python console using the `code` module, including special handling for certain user-defined commands and enhanced error management. # Requirements: 1. **Class Definition**: Define a class `CustomConsole` that inherits from `code.InteractiveConsole`. 2. **Custom Commands**: - Implement a command `#help` to print a custom help message. - Implement a command `#exit` to exit the console with a custom exit message. 3. **Enhanced Error Handling**: - Override the `showsyntaxerror` method to include the line number where the error occurred. - Override the `showtraceback` method to include the type of error that occurred. # Implementation Details: 1. **Class Initialization**: - The class should take an optional dictionary `locals` and an optional string `filename`. 2. **push(line)**: - Override the `push` method to handle the specific custom commands `#help` and `#exit`. 3. **write(data)**: - Override the `write` method to print error messages in a custom format. 4. **showsyntaxerror(filename=None)**: - Override to display syntax errors with the line number. 5. **showtraceback()**: - Override to display the type of error along with the standard traceback. # Constraints: - The `#help` command should print: \\"Custom Console Help: Type your Python code to execute it. Type #exit to exit.\\" - The `#exit` command should print: \\"Exiting Custom Console...\\" before terminating the interactive session. - The custom error formatting should be clear and informative for users. # Input/Output: - **Input**: Python code lines and commands entered by the user in the interactive console. - **Output**: Execution results, custom help message, exit message, and enhanced error messages displayed in the console. # Example Usage: ```python >>> console = CustomConsole() >>> console.interact(banner=\\"Welcome to the Custom Interactive Python Console!\\", exitmsg=\\"Goodbye!\\") Welcome to the Custom Interactive Python Console! >>> print(\\"Hello, World!\\") Hello, World! >>> #help Custom Console Help: Type your Python code to execute it. Type #exit to exit. >>> #exit Exiting Custom Console... Goodbye! ``` **Remember**: Your implementation should focus on creating a user-friendly and informative interactive console experience enhanced with custom commands and error handling.","solution":"import code import sys class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) def push(self, line): if line == \\"#help\\": self.write(\\"Custom Console Help: Type your Python code to execute it. Type #exit to exit.n\\") elif line == \\"#exit\\": self.write(\\"Exiting Custom Console...n\\") sys.exit() else: super().push(line) def write(self, data): sys.stderr.write(f\\"Error: {data}\\") def showsyntaxerror(self, filename=None): type, value, tb = sys.exc_info() self.write(f\\"SyntaxError: {value} at line {value.lineno}n\\") def showtraceback(self): type, value, tb = sys.exc_info() self.write(f\\"{type.__name__}: {value}n\\") super().showtraceback() # Example of interaction: # Note: This doesn\'t work in a script as it expects interactive input # console = CustomConsole() # console.interact(banner=\\"Welcome to the Custom Interactive Python Console!\\", exitmsg=\\"Goodbye!\\")"},{"question":"# **Seaborn Advanced Plotting Question** **Objective:** You are tasked with creating a complex plot using Seaborn\'s object-oriented interface that involves multiple layers and customized properties. This exercise will test your ability to manipulate and combine different visual elements within a plot, as well as handle dataset transformations. **Dataset:** We will use Seaborn\'s built-in `penguins` dataset for this task. **Task:** 1. Load the `penguins` dataset from `seaborn`. 2. Construct a Seaborn object-oriented plot (`so.Plot`) that: - Creates a strip plot of `body_mass_g` versus `species` with dots colored by `sex`. - Overlays this plot with dash lines representing `body_mass_g` for each `species`, adjusting the line\'s alpha transparency and width based on `flipper_length_mm`. - Applies dodging to clearly separate males and females within each species. 3. Ensure the plot has appropriate axis labels and a title that summarizes the plot\'s purpose. **Input:** - No user input is required as the dataset is loaded programmatically. **Output:** - A single plot combining the above specifications. # **Constraints:** - Do not use external plotting libraries other than `matplotlib` and `seaborn`. - Ensure that the combined plot is readable and aesthetically pleasing. # **Example Output:** ![example_plot](example_output.png) # **Instructions:** - Implement the function `create_complex_penguins_plot()` that visualizes the data according to the specifications. - The function does not take any parameters and returns nothing. It simply displays the plot using `matplotlib\'s` `plt.show()` function. # **Starter Code:** ```python import matplotlib.pyplot as plt import seaborn.objects as so from seaborn import load_dataset def create_complex_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot object p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add jittered dots for body mass colored by sex p.add(so.Dots(), so.Dodge(), so.Jitter()) # Add dash lines for body mass with adjusted alpha and linewidth based on flipper length p.add(so.Dash(alpha=0.5, linewidth=penguins[\\"flipper_length_mm\\"] / 100), so.Dodge()) # Customize the plot with titles and labels plt.title(\\"Body Mass of Penguins by Species and Sex with Flipper Length\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") # Show the plot p.show() # Call the function to create the plot create_complex_penguins_plot() ``` **Note:** Ensure that you have `seaborn` and `matplotlib` installed in your environment to run this code. You can do this via pip: ```sh pip install seaborn matplotlib ```","solution":"import matplotlib.pyplot as plt import seaborn.objects as so from seaborn import load_dataset def create_complex_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot object p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add strip plot with dots colored by sex p.add(so.Dot(), so.Dodge(), so.Jitter()) # Add dash lines representing body mass for each species # Adjust the line\'s alpha transparency and width based on flipper_length_mm p.add(so.Line(alpha=0.5, linewidth=(penguins[\\"flipper_length_mm\\"] - penguins[\\"flipper_length_mm\\"].min()) / 100), so.Dodge(), color=\\"gray\\") # Customize the plot with titles and labels plt.title(\\"Body Mass of Penguins by Species and Sex with Flipper Length\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") # Show the plot p.show()"},{"question":"**Python Coding Assessment Question** You are tasked with developing a Python logger utility using the `syslog` module that meets specific requirements for configuration and usage. Your implementation must include the following components: 1. **Logger Initialization Function** - Implement a function `initialize_logger(ident: str, logoption: int, facility: int) -> None` that initializes the syslog with given parameters. Ensure to use the `openlog()` function of the `syslog` module. 2. **Logging Function** - Implement a function `log_message(priority: int, message: str) -> None` that logs a message with the specified priority using the `syslog()` function. The function should ensure that the logger is properly initialized (if not already). 3. **Change Log Mask Function** - Implement a function `change_log_mask(maskpri: int) -> int` that changes the priority mask and returns the previous mask using the `setlogmask()` function. 4. **Close Logger Function** - Implement a function `close_logger() -> None` that closes the syslog by calling the `closelog()` function. **Example Usage:** ```python initialize_logger(\\"MyApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) log_message(syslog.LOG_INFO, \\"Application has started.\\") previous_mask = change_log_mask(syslog.LOG_MASK(syslog.LOG_ERR)) log_message(syslog.LOG_ERR, \\"An error occurred.\\") close_logger() ``` **Inputs and Outputs:** - `initialize_logger(ident: str, logoption: int, facility: int) -> None`: Initializes the logger. - `log_message(priority: int, message: str) -> None`: Logs a message with the given priority. - `change_log_mask(maskpri: int) -> int`: Changes the log mask and returns the previous mask. - `close_logger() -> None`: Closes the logger. **Constraints:** - The `ident` parameter for the `initialize_logger` function should be a non-empty string. - The `logoption` and `facility` parameters should be valid constants from the `syslog` module. - The `priority` parameter for the `log_message` function should be a valid priority level constant from the `syslog` module. - The `maskpri` parameter for the `change_log_mask` function should be calculated using `syslog.LOG_MASK()` or `syslog.LOG_UPTO()` functions. Ensure your code is robust and handles potential errors gracefully.","solution":"import syslog logger_initialized = False def initialize_logger(ident: str, logoption: int, facility: int) -> None: Initializes the syslog with the given identification, log option, and facility. global logger_initialized if not logger_initialized: if not ident: raise ValueError(\\"Identifier must be a non-empty string.\\") syslog.openlog(ident=ident, logoption=logoption, facility=facility) logger_initialized = True def log_message(priority: int, message: str) -> None: Logs a message with the specified priority using syslog. if not logger_initialized: raise RuntimeError(\\"Logger not initialized. Call initialize_logger first.\\") syslog.syslog(priority, message) def change_log_mask(maskpri: int) -> int: Changes the priority mask and returns the previous mask. return syslog.setlogmask(maskpri) def close_logger() -> None: Closes the syslog. global logger_initialized if logger_initialized: syslog.closelog() logger_initialized = False"},{"question":"# Python Code Evaluation Function Problem Statement: You are tasked with creating a utility function to dynamically evaluate and execute Python code provided in various formats. This function should be robust enough to handle complete programs, file inputs, interactive inputs, and expression inputs. **Function Signature:** ```python def dynamic_python_evaluator(input_type: str, input_data: str) -> str: pass ``` **Parameters:** - `input_type` (str): The type of input provided. It can be one of the following: - `\'complete_program\'`: A complete Python program. - `\'file\'`: Python code from a file. - `\'interactive\'`: Python code to be executed interactively. - `\'expression\'`: A Python expression to be evaluated. - `input_data` (str): The actual code to evaluate or the path to the file containing the code based on the `input_type`. **Returns:** - `str`: The result of the execution or evaluation, or an appropriate error message if the evaluation fails. **Constraints:** - Assume that `input_data` for \'file\' type is a valid file path pointing to a `.py` script. - The function should handle exceptions gracefully and return a meaningful message in case of errors. - For \'complete_program\' and \'interactive\' types, include an extra blank line at the end of `input_data` to ensure proper parsing when necessary. - The function should handle the execution in a safe manner, preventing security risks from potentially malicious code. Example: ```python # Example 1: Complete Program input_type = \'complete_program\' input_data = def add(a, b): return a + b result = add(5, 3) print(result) print(dynamic_python_evaluator(input_type, input_data)) # Expected Output: \\"8n\\" # Example 2: Expression input_type = \'expression\' input_data = \\"5 + 3\\" print(dynamic_python_evaluator(input_type, input_data)) # Expected Output: \\"8\\" # Example 3: Interactive Input input_type = \'interactive\' input_data = import math square_root = math.sqrt(16) print(square_root) print(dynamic_python_evaluator(input_type, input_data)) # Expected Output: \\"4.0n\\" # Example 4: File Input # Assuming there is a file named \'script.py\' with content: # def multiply(a, b): # return a * b # # result = multiply(4, 3) # print(result) input_type = \'file\' input_data = \'script.py\' print(dynamic_python_evaluator(input_type, input_data)) # Expected Output: \\"12n\\" ``` Create the function `dynamic_python_evaluator` to handle the described functionality and test it with the provided examples.","solution":"def dynamic_python_evaluator(input_type: str, input_data: str) -> str: import sys import io import traceback # Define a helper function to safely execute code and capture output def execute_code(code): old_stdout = sys.stdout new_stdout = io.StringIO() sys.stdout = new_stdout try: exec(code, {}) except Exception as e: sys.stdout = old_stdout return f\\"Error: {str(e)}n{traceback.format_exc()}\\" sys.stdout = old_stdout return new_stdout.getvalue() # Define a helper function to safely evaluate an expression and capture output def evaluate_expression(expr): try: result = eval(expr, {}) return str(result) except Exception as e: return f\\"Error: {str(e)}n{traceback.format_exc()}\\" # Handle different input types if input_type == \'complete_program\': input_data += \'n\' # Ensure there\'s an extra newline at the end return execute_code(input_data) elif input_type == \'file\': try: with open(input_data, \'r\') as f: file_code = f.read() file_code += \'n\' # Ensure there\'s an extra newline at the end return execute_code(file_code) except Exception as e: return f\\"Error: Unable to read file - {str(e)}\\" elif input_type == \'interactive\': input_data += \'n\' # Ensure there\'s an extra newline at the end return execute_code(input_data) elif input_type == \'expression\': return evaluate_expression(input_data) else: return \\"Error: Invalid input type.\\""},{"question":"# Debugging and Profiling with Python Background: You are given a Python script that computes the factorial of a number, but it has a performance bottleneck. Your task is to identify the bottleneck and optimize the code for better performance using Python\'s profiling tools. Task: Given the following code snippet: ```python def factorial(n): if n == 0: return 1 else: return n * factorial(n-1) def main(): import time start_time = time.time() result = factorial(20) end_time = time.time() print(f\\"Factorial: {result}, Time taken: {end_time - start_time}\\") if __name__ == \\"__main__\\": main() ``` 1. Use the `cProfile` module to profile the provided code and identify the function that takes the most execution time. 2. Optimize the `factorial` function to reduce its execution time. 3. Verify the improvement by profiling the optimized code and comparing execution times before and after optimization. Requirements: 1. Your solution should include the profiling results both before and after optimization. 2. Implement a decorator to profile the execution time of the `factorial` function. Constraints: 1. The optimized solution should not use any external libraries other than standard Python libraries. 2. The solution must handle the input `n` efficiently where ( 0 leq n leq 100 ). Expected Output: Your final submission should include: 1. The original profiling result. 2. The optimized `factorial` function. 3. The profiling result of the optimized function. 4. A brief explanation of the optimization approach. Example: A possible optimization approach could be to use memoization to avoid redundant calculations.","solution":"import cProfile import functools def factorial(n): if n == 0: return 1 else: return n * factorial(n-1) def profile_function(func): Decorator to profile a function and print its execution time. @functools.wraps(func) def wrapper_profile_function(*args, **kwargs): import time start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\\"Time taken by {func.__name__}: {end_time - start_time:.6f} seconds\\") return result return wrapper_profile_function @profile_function def main(): result = factorial(20) print(f\\"Factorial: {result}\\") if __name__ == \\"__main__\\": cProfile.run(\'main()\') # Optimized solution with memoization def factorial_optimized(n, memo={}): if n in memo: return memo[n] if n == 0: memo[n] = 1 else: memo[n] = n * factorial_optimized(n-1, memo) return memo[n] @profile_function def main_optimized(): result = factorial_optimized(20) print(f\\"Factorial: {result}\\") if __name__ == \\"__main__\\": print(\\"nProfiling original factorial function\\") cProfile.run(\'main()\') print(\\"nProfiling optimized factorial function\\") cProfile.run(\'main_optimized()\')"},{"question":"Question: Advanced Time Zone Manipulation and Cache Management You are required to write a Python function that demonstrates comprehension of the `zoneinfo` module and its integration with the `datetime` module. The function should read from a list of timed events, convert these events to UTC, sort them, and then provide a summary of events falling within specific business hours for a given time zone. Additionally, the time zone data cache should be managed within the function to ensure all operations are performed accurately. # Function Signature ```python def process_timed_events(events: list, timezone_key: str, business_hours: tuple) -> list: Process a list of timed events and return a summary of events that fall within the given business hours. Args: - events: List of dictionaries, where each dictionary contains: - \'event\': str, event name. - \'date_time\': str, date and time in \'YYYY-MM-DD HH:MM:SS\' format. - timezone_key: str, IANA time zone key (e.g., \'America/New_York\'). - business_hours: tuple of two time strings (\'HH:MM\', \'HH:MM\'), specifying business hours in the given time zone. Returns: - List of dictionaries, each containing an event name and its converted UTC datetime string, filtered and sorted to include events only within the specified business hours. pass ``` # Requirements 1. **Input Format:** - `events`: A list of dictionaries, each with an \'event\' key (string) and \'date_time\' key (string in \'YYYY-MM-DD HH:MM:SS\' format). - `timezone_key`: A string representing the IANA time zone of interest. - `business_hours`: A tuple consisting of two strings representing business hours in the format \'HH:MM\' (24-hour format), e.g., (\'09:00\', \'17:00\'). 2. **Output Format:** - A list of dictionaries, each containing: - \'event\': The event name. - \'utc_date_time\': The date and time of the event converted to UTC in \'YYYY-MM-DD HH:MM:SS\' format. - Only include events that fall within the specified business hours of the given time zone. - The output list should be sorted by the UTC datetime. 3. **Constraints:** - The function should handle invalid or missing time zone data gracefully by raising `ZoneInfoNotFoundError` if the time zone key is invalid. - Business hours should consider potential daylight saving time changes. - Ensure the function uses the `ZoneInfo` cache appropriately, and demonstrate the use of `ZoneInfo.clear_cache()` to reset the cache if needed. 4. **Performance:** - Aim for efficient conversion and sorting operations, given that events list can be large (up to 1000 entries). # Example ```python events = [ {\'event\': \'Meeting\', \'date_time\': \'2023-10-25 10:30:00\'}, {\'event\': \'Conference\', \'date_time\': \'2023-10-25 15:45:00\'}, {\'event\': \'Lunch\', \'date_time\': \'2023-10-25 12:00:00\'}, ] timezone_key = \'America/New_York\' business_hours = (\'09:00\', \'17:00\') result = process_timed_events(events, timezone_key, business_hours) print(result) ``` # Expected Output ```python [ {\'event\': \'Meeting\', \'utc_date_time\': \'2023-10-25 14:30:00\'}, {\'event\': \'Lunch\', \'utc_date_time\': \'2023-10-25 16:00:00\'} ] ``` # Explanation - The function converts each event\'s datetime to the specified time zone. - It checks if the event falls within the provided business hours (considering daylight saving time). - Converts the filtered events to UTC. - Sorts the events by UTC datetime and returns the result.","solution":"from datetime import datetime, time from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def process_timed_events(events: list, timezone_key: str, business_hours: tuple) -> list: Process a list of timed events and return a summary of events that fall within the given business hours. Args: - events: List of dictionaries, where each dictionary contains: - \'event\': str, event name. - \'date_time\': str, date and time in \'YYYY-MM-DD HH:MM:SS\' format. - timezone_key: str, IANA time zone key (e.g., \'America/New_York\'). - business_hours: tuple of two time strings (\'HH:MM\', \'HH:MM\'), specifying business hours in the given time zone. Returns: - List of dictionaries, each containing an event name and its converted UTC datetime string, filtered and sorted to include events only within the specified business hours. try: tz = ZoneInfo(timezone_key) except ZoneInfoNotFoundError: return [] start_hour, end_hour = [time(int(hour.split(\':\')[0]), int(hour.split(\':\')[1])) for hour in business_hours] filtered_events = [] for event in events: local_time = datetime.strptime(event[\'date_time\'], \'%Y-%m-%d %H:%M:%S\').replace(tzinfo=tz) if start_hour <= local_time.timetz() <= end_hour: utc_time = local_time.astimezone(ZoneInfo(\'UTC\')) filtered_events.append({ \'event\': event[\'event\'], \'utc_date_time\': utc_time.strftime(\'%Y-%m-%d %H:%M:%S\') }) filtered_events.sort(key=lambda x: x[\'utc_date_time\']) return filtered_events"},{"question":"**Coding Assessment Question** **Objective**: Evaluate the ability to use advanced features of the `dataclasses` module to create customizable and robust data classes. **Problem Statement**: You are to create a comprehensive class for managing a library\'s book inventory using Python\'s `dataclasses`. The class should encapsulate the following information for each book, leveraging advanced `dataclasses` features when appropriate: 1. **Attributes**: - `title`: The title of the book (string). - `author`: The author of the book (string). - `ISBN`: The International Standard Book Number (string, unique for each book). - `quantity`: The number of copies of the book available in the library (integer, default is 0). - `published_year`: The year the book was published (integer). - `borrowed_books`: Number of borrowed copies (integer, default is 0). 2. **Functional Requirements**: - Ensure `quantity` and `borrowed_books` cannot be negative. - Provide a method `borrow_book()` that increases the `borrowed_books` count if there are available copies, and a method `return_book()` that decreases the count. - Override the `__repr__` method to provide a string representation of the book in the format: \\"Title by Author (ISBN), Published in Year – x copies available.\\" - Use `default_factory` for `borrowed_books` to avoid shared mutable defaults. - Create custom exception handling if an attempt is made to borrow more books than available. - Make the class hashable and sortable (based on title). 3. **Performance Requirements**: - The operations (borrowing and returning books) should be efficient and error-free, handling edge cases gracefully. **Constraints**: - You must use Python `dataclasses` and relevant features to accomplish the tasks. - Your implementation should raise appropriate exceptions for error cases (like borrowing a book when no copies are available). **Example Usage**: ```python from dataclasses import dataclass, field # Your implementation here # Creating book instances book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", ISBN=\\"1234567890\\", quantity=5, published_year=1949) book2 = Book(title=\\"The Great Gatsby\\", author=\\"F. Scott Fitzgerald\\", ISBN=\\"0987654321\\", quantity=3, published_year=1925) print(book1) # Output: \\"1984 by George Orwell (ISBN: 1234567890), Published in 1949 – 5 copies available.\\" # Borrowing books book1.borrow_book() print(book1.borrowed_books) # Output: 1 # Returning books book1.return_book() print(book1.borrowed_books) # Output: 0 ``` **Implementation**: Using the provided example and requirements, implement the class `Book` accordingly.","solution":"from dataclasses import dataclass, field class BookError(Exception): pass @dataclass(order=True, unsafe_hash=True) class Book: title: str author: str ISBN: str quantity: int = 0 published_year: int = 0 borrowed_books: int = field(default=0, compare=False) def __post_init__(self): if self.quantity < 0: raise ValueError(\\"quantity cannot be negative\\") if self.borrowed_books < 0: raise ValueError(\\"borrowed_books cannot be negative\\") def borrow_book(self): if self.quantity <= self.borrowed_books: raise BookError(\\"No more copies available to borrow.\\") self.borrowed_books += 1 def return_book(self): if self.borrowed_books == 0: raise BookError(\\"No borrowed books to return.\\") self.borrowed_books -= 1 def __repr__(self): available_copies = self.quantity - self.borrowed_books return f\\"{self.title} by {self.author} (ISBN: {self.ISBN}), Published in {self.published_year} – {available_copies} copies available.\\""},{"question":"You are given a large dataset in the form of multiple parquet files stored in a directory. Each file contains data for a specific year, and the dataset has the following columns: - `timestamp`: The timestamp of the recorded data. - `name`: The name of the subject (with low cardinality). - `id`: An identifier for the record. - `x`: A numerical value (between -1 and 1). - `y`: Another numerical value (between -1 and 1). Your task is to implement a function to read this dataset and perform the following operations: 1. Load only the required columns: `timestamp`, `name`, `id`, `x`. 2. Convert the `name` column to a more memory-efficient categorical type. 3. Downcast the `id` column to the smallest possible unsigned integer type. 4. Downcast the `x` column to the smallest possible float type. 5. Compute and return the total memory usage of the optimized DataFrame. # Function Signature ```python import pandas as pd from pathlib import Path def optimize_memory_usage(directory_path: str) -> int: Reads multiple parquet files from the given directory, loads specified columns, optimizes memory usage by converting column data types, and returns the total memory usage of the DataFrame. Parameters: - directory_path (str): Path to the directory containing parquet files. Returns: - int: Total memory usage of the DataFrame in bytes. ``` # Example ```python # Suppose the `data/timeseries` directory contains parquet files in the prescribed format memory_usage = optimize_memory_usage(\\"data/timeseries\\") print(memory_usage) # Outputs the total memory usage in bytes after optimization ``` # Constraints - **Performance**: The solution should be able to handle directories with large datasets efficiently. - **pandas**: You should use pandas functions to load and optimize the data as described. # Notes - Use chunking or other methods if necessary to ensure the solution handles datasets that might not fit in memory all at once. - You can assume that each parquet file contains the same columns and data types. # Hints - Use `pd.read_parquet` with the `columns` parameter to load only necessary columns. - Utilize `astype(\'category\')` for converting columns to categorical type. - Use `pd.to_numeric` with the `downcast` parameter for downcasting numeric columns.","solution":"import pandas as pd from pathlib import Path def optimize_memory_usage(directory_path: str) -> int: Reads multiple parquet files from the given directory, loads specified columns, optimizes memory usage by converting column data types, and returns the total memory usage of the DataFrame. Parameters: - directory_path (str): Path to the directory containing parquet files. Returns: - int: Total memory usage of the DataFrame in bytes. # List of files in the directory directory = Path(directory_path) parquet_files = list(directory.glob(\\"*.parquet\\")) # DataFrame to hold all data df_list = [] for file in parquet_files: # Read only the required columns df = pd.read_parquet(file, columns=[\'timestamp\', \'name\', \'id\', \'x\']) # Convert the \'name\' column to a categorical type df[\'name\'] = df[\'name\'].astype(\'category\') # Downcast the \'id\' column to the smallest possible unsigned integer type df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') # Downcast the \'x\' column to the smallest possible float type df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') # Append to the list of DataFrames df_list.append(df) # Concatenate all DataFrames combined_df = pd.concat(df_list) # Compute the total memory usage of the combined DataFrame total_memory_usage = combined_df.memory_usage(deep=True).sum() return int(total_memory_usage)"},{"question":"# Question: Implementing a TorchScript Class with Control Flows and Type Annotations You are tasked with creating a TorchScript class that contains several annotated methods, types, and control flows supported by TorchScript. Objectives 1. Implement a TorchScript class `MyTorchScriptModel` with the following requirements: - Initialize the model with an integer `counter` and a list of tensors called `data`. - Include a method `add_to_counter` that takes an integer input and adds it to `counter`. - Include an operation-invoking method `compute_means` that returns a dictionary with the mean of each tensor in `data`. - Include a method `conditional_update` which takes an Optional integer and updates `counter` to that value if it\'s not None; otherwise, it increments the `counter` by one. 2. Follow these constraints and limitations: - Use type annotations for all function parameters and return types. - Use TorchScript constructs and proper control flows for None checks, if statements, and loops. - Ensure that tensor operations are correctly performed and the variable types are strictly adhered to. Expectations - **Input Format**: - The `MyTorchScriptModel` class will be instantiated with an integer and a list of tensors. - Example: `model = MyTorchScriptModel(3, [torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0])])` - **Output Format** (for the methods): - `add_to_counter`: No output (modifies the `counter` internally). - `compute_means`: Returns a dictionary mapping string keys (like `\\"tensor_0\\"`, `\\"tensor_1\\"`, ...) to the respective means of the tensors. - `conditional_update`: No output (modifies the `counter` internally). Example Usage ```python import torch from typing import List, Dict, Optional @torch.jit.script class MyTorchScriptModel: def __init__(self, counter: int, data: List[torch.Tensor]): self.counter = counter self.data = data def add_to_counter(self, value: int) -> None: self.counter += value def compute_means(self) -> Dict[str, float]: means = {} for i in range(len(self.data)): means[f\\"tensor_{i}\\"] = self.data[i].mean().item() return means def conditional_update(self, new_value: Optional[int]) -> None: if new_value is not None: self.counter = new_value else: self.counter += 1 # Example: model = MyTorchScriptModel(5, [torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0])]) model.add_to_counter(3) # counter becomes 8 means = model.compute_means() # returns {\\"tensor_0\\": 1.5, \\"tensor_1\\": 3.5} model.conditional_update(None) # counter becomes 9 model.conditional_update(50) # counter becomes 50 ``` Make sure to test the implemented class and its methods to ensure they work as expected and adhere to the specified constraints and limitations.","solution":"import torch from typing import List, Dict, Optional @torch.jit.script class MyTorchScriptModel: def __init__(self, counter: int, data: List[torch.Tensor]): self.counter = counter self.data = data def add_to_counter(self, value: int) -> None: self.counter += value def compute_means(self) -> Dict[str, float]: means: Dict[str, float] = {} for i in range(len(self.data)): means[f\\"tensor_{i}\\"] = self.data[i].mean().item() return means def conditional_update(self, new_value: Optional[int]) -> None: if new_value is not None: self.counter = new_value else: self.counter += 1"},{"question":"NIS (Network Information Service) Lookup **Background**: You have been asked to implement a Python script that uses the deprecated `nis` module to perform some NIS-related operations. Despite its depreciation status, understanding how to work with such modules provides a valuable lesson in handling legacy code and interfaces. **Objective**: Write a Python function `nis_lookup(mapname: str, key: str, domain: str = None) -> dict` that: 1. Retrieves the value for a given key in a specified NIS map. 2. Retrieves the complete map as a dictionary. 3. Retrieves all available maps in the NIS domain. 4. Uses the default domain if none is specified. The function should return a dictionary with the following structure: ```python { \'map_value\': <value associated with the given key>, \'complete_map\': <dictionary of key-value pairs in the map>, \'all_maps\': <list of all valid maps in the NIS domain> } ``` **Specifications**: - If the given key does not exist in the map, the function should raise a `KeyError` with the message \\"Key not found in map\\". - If the mapname does not exist, the function should raise an `nis.error`. - Handle the default NIS domain correctly when the domain is not provided. - Ensure that your function works efficiently for large maps. **Constraints**: - This function will only be run on Unix systems. - Assume that the `nis` module is correctly installed and available for import. - You are required to handle any potential exceptions that could arise from using the `nis` functions. **Example Usage**: ```python # Example call to the `nis_lookup` function. result = nis_lookup(\'passwd.byname\', \'john_doe\') print(result) # Expected output structure (values will differ based on the NIS configuration): # { # \'map_value\': b\'john_doe:x:1001:1001:John Doe:/home/john_doe:/bin/bash\', # \'complete_map\': {b\'john_doe\': b\'john_doe:x:1001:1001:John Doe:/home/john_doe:/bin/bash\', ...}, # \'all_maps\': [\'passwd.byname\', \'group.byname\', ...] # } ``` Your implementation should ensure correctness, handle exceptions appropriately, and be efficient in accessing and processing the NIS data.","solution":"import nis def nis_lookup(mapname: str, key: str, domain: str = None) -> dict: Retrieves NIS information including the value for a given key in a specified NIS map, the complete map, and all available maps in the domain. Args: - mapname: The name of the NIS map. - key: The key to look up in the map. - domain: The NIS domain. Uses default if none is specified. Returns: - A dictionary with \'map_value\', \'complete_map\', and \'all_maps\'. try: # Retrieve all available maps if domain: maps = nis.maps(domain) else: maps = nis.maps() # Ensure the mapname exists if mapname not in maps: raise nis.error(f\\"Map {mapname} does not exist in the NIS domain\\") # Retrieve the value for the given key in the specified NIS map try: map_value = nis.match(key, mapname, domain) except nis.error: raise KeyError(\\"Key not found in map\\") # Retrieve the complete map complete_map = nis.cat(mapname, domain) return { \'map_value\': map_value, \'complete_map\': complete_map, \'all_maps\': maps } except nis.error as e: raise e except Exception as e: raise e"},{"question":"Objective Write a Python function using the `sndhdr` module to classify a list of sound files. The function should determine the type of each sound file and print the details in a structured format. Function Signature ```python def classify_sound_files(file_list: list) -> list: ``` Input - `file_list`: A list of strings, where each string is the file path to a sound file. Output - Returns a list of dictionaries, where each dictionary contains the following keys: - `filename`: The file path of the input sound file. - `filetype`: The type of sound file (e.g., \'aifc\', \'wav\'). - `framerate`: The frame rate of the sound file. - `nchannels`: The number of audio channels. - `nframes`: The number of frames. - `sampwidth`: The sample width. Each dictionary should contain `None` for values if the `sndhdr` module cannot determine the sound file\'s details. Constraints - Use the `sndhdr.whathdr()` function to determine the sound file\'s characteristics. - Assume that the `file_list` contains valid file paths. - Handle cases where `sndhdr.whathdr` returns `None`. Example ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.au\\"] result = classify_sound_files(file_list) # Expected Output: # [ # { # \\"filename\\": \\"sound1.wav\\", # \\"filetype\\": \\"wav\\", # \\"framerate\\": 44100, # \\"nchannels\\": 2, # \\"nframes\\": 10000, # \\"sampwidth\\": 16 # }, # { # \\"filename\\": \\"sound2.aiff\\", # \\"filetype\\": \\"aiff\\", # \\"framerate\\": 48000, # \\"nchannels\\": 2, # \\"nframes\\": 5000, # \\"sampwidth\\": 24 # }, # { # \\"filename\\": \\"sound3.au\\", # \\"filetype\\": \\"au\\", # \\"framerate\\": 0, # \\"nchannels\\": 0, # \\"nframes\\": -1, # \\"sampwidth\\": \\"A\\" # } # ] ``` Notes - Ensure your function handles cases where `sndhdr.whathdr` returns `None` gracefully by setting all attributes to `None` for that file. - The function should return a list of dictionaries in the same order as the input `file_list`. Testing Your implementation will be tested with a variety of sound files of different formats to ensure accuracy and robustness.","solution":"import sndhdr def classify_sound_files(file_list): Classifies a list of sound files, returning details for each one. If the sound file type cannot be determined, returns None for all details. Args: file_list (list): List of file paths to sound files. Returns: list: List of dictionaries with details about each sound file. sound_details_list = [] for file in file_list: info = sndhdr.whathdr(file) if info is None: details = { \\"filename\\": file, \\"filetype\\": None, \\"framerate\\": None, \\"nchannels\\": None, \\"nframes\\": None, \\"sampwidth\\": None } else: details = { \\"filename\\": file, \\"filetype\\": info.filetype, \\"framerate\\": info.framerate, \\"nchannels\\": info.nchannels, \\"nframes\\": info.nframes, \\"sampwidth\\": info.sampwidth } sound_details_list.append(details) return sound_details_list"},{"question":"# String Double Conversion and Formatting In this coding challenge, you will demonstrate your understanding of string conversion and formatting using the python310 package. You need to implement a function that takes as input a string that represents multiple floating-point numbers separated by commas, converts them to doubles, and then outputs a formatted string of these doubles. Function Signature ```python def format_floats(input_string: str, precision: int, sign: bool = False) -> str: ``` Parameters - `input_string` (str): A string containing multiple floating-point numbers separated by commas. Example: `\\"3.1415,2.718,1.618\\"` - `precision` (int): The number of decimal places to include in the output. - `sign` (bool): A flag indicating whether to always include a sign character in the output. Default is `False`. Return - `str`: A formatted string containing the floating-point numbers with the specified precision and sign formatting. The numbers should be space-separated in the output string. Constraints - The input string will not have leading or trailing whitespace. - The input string is guaranteed to contain valid floating-point numbers separated by commas. - The precision will be a non-negative integer less than 10. - The function should handle large numbers and scientific notation properly. - The function should raise a `ValueError` with an appropriate message if any conversion fails. Example ```python print(format_floats(\\"3.1415,2.718,1.618\\", 2)) # Expected output: \\"3.14 2.72 1.62\\" print(format_floats(\\"3.1415,2.718,1.618\\", 3, True)) # Expected output: \\"+3.142 +2.718 +1.618\\" ``` Implementation Guidelines 1. Utilize `PyOS_string_to_double` to convert each substring to a double. 2. Use `PyOS_double_to_string` to format each double with the specified precision and sign flag. 3. Ensure to handle any exceptions or errors appropriately by raising `ValueError`. ```python import ctypes from typing import List # Load the shared library containing the C functions (adjust the path as necessary) lib = ctypes.cdll.LoadLibrary(\\"path_to_your_python310_shared_library.so\\") # Define the necessary C functions with their signatures lib.PyOS_string_to_double.argtypes = [ctypes.c_char_p, ctypes.POINTER(ctypes.c_char_p), ctypes.c_void_p] lib.PyOS_string_to_double.restype = ctypes.c_double lib.PyOS_double_to_string.argtypes = [ctypes.c_double, ctypes.c_char, ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int)] lib.PyOS_double_to_string.restype = ctypes.c_char_p def format_floats(input_string: str, precision: int, sign: bool = False) -> str: # Helper function to convert a string to a double def string_to_double(s: str) -> float: endptr = ctypes.c_char_p() overflow_exception = None result = lib.PyOS_string_to_double(s.encode(\'utf-8\'), ctypes.byref(endptr), overflow_exception) if result == -1.0 and s != \\"-1.0\\": raise ValueError(f\\"Invalid conversion for string: {s}\\") return result # Helper function to convert a double to a formatted string def double_to_string(val: float, precision: int, sign: bool) -> str: format_code = b\'f\' flags = 0 if sign: flags |= 0x1 # Py_DTSF_SIGN formatted_string = lib.PyOS_double_to_string(val, format_code, precision, flags, None) if not formatted_string: raise ValueError(f\\"Failed to convert double: {val}\\") return formatted_string.decode(\'utf-8\') # Split the input string by commas and process each number double_values = [string_to_double(part) for part in input_string.split(\',\')] formatted_values = [double_to_string(val, precision, sign) for val in double_values] return \' \'.join(formatted_values) ```","solution":"def format_floats(input_string: str, precision: int, sign: bool = False) -> str: Converts a comma-separated string of floating-point numbers to space-separated string of formatted floating-point numbers. Parameters: - input_string (str): The input string containing floating-point numbers separated by commas. - precision (int): The number of decimal places to format each floating-point number. - sign (bool): Whether or not to include the sign in the formatted output. Returns: - str: The formatted floating-point numbers as a space-separated string. try: float_strings = input_string.split(\',\') float_values = [float(x) for x in float_strings] formatted_floats = [] for value in float_values: if sign: formatted_floats.append(f\\"{value:+.{precision}f}\\") else: formatted_floats.append(f\\"{value:.{precision}f}\\") return \' \'.join(formatted_floats) except ValueError as ve: raise ValueError(f\\"Error in converting input strings to floats: {ve}\\")"},{"question":"# Task: Implement a Custom File Integrity Checker Using `hashlib` Requirements: Your task is to implement a function `verify_file_integrity(file_path: str, expected_hash: str, hash_algorithm: str = \'sha256\') -> bool` which verifies the integrity of a given file by comparing its hash against an expected hash value. The function should: 1. Open and read the specified file in binary mode. 2. Compute the hash of the file content using the specified hashing algorithm from the `hashlib` module. You can assume that the hash algorithm will be one supported by `hashlib` (e.g., \'sha256\', \'sha512\', \'md5\', etc.). 3. Compare the computed hash (in hexadecimal format) with the provided expected hash. 4. Return `True` if the hashes match, otherwise return `False`. Input: - `file_path`: a `str` that specifies the path to the file that needs to be checked. - `expected_hash`: a `str` representing the expected hash value (in hexadecimal format) of the file content. - `hash_algorithm`: a `str` representing the hashing algorithm to use (default is \'sha256\'). Output: - A `bool` value: `True` if the computed hash matches the expected hash, `False` otherwise. Constraints: - The file size can be large, so read and hash the file in chunks to avoid memory issues. You should not load the entire file content into memory at once. - You should handle any possible I/O errors gracefully. - Ensure the hash is computed correctly using the specified algorithm and is robust against partial data feeds. Example Usage: ```python # Assuming \'example.txt\' is a file present in the current directory expected_hash = \'d8e8fca2dc0f896fd7cb4cb0031ba249\' result = verify_file_integrity(\'example.txt\', expected_hash, \'md5\') print(result) # True if the file\'s MD5 hash matches the expected hash, False otherwise ``` Implementation Details: You may find the following methods from the `hashlib` module useful: - `hashlib.new()` - `hashlib.update()` - `hashlib.hexdigest()` Ensure proper error handling and efficient reading of potentially large files.","solution":"import hashlib def verify_file_integrity(file_path: str, expected_hash: str, hash_algorithm: str = \'sha256\') -> bool: Verifies the integrity of a given file by comparing its hash against an expected hash value. :param file_path: Path to the file that needs to be checked. :param expected_hash: The expected hash value of the file content in hexadecimal format. :param hash_algorithm: The hashing algorithm to use (default is \'sha256\'). :return: True if the computed hash matches the expected hash, False otherwise. try: hasher = hashlib.new(hash_algorithm) with open(file_path, \'rb\') as file: for chunk in iter(lambda: file.read(4096), b\\"\\"): hasher.update(chunk) computed_hash = hasher.hexdigest() return computed_hash == expected_hash except (OSError, FileNotFoundError, ValueError) as e: return False"},{"question":"**Advanced Python Descriptor Management** # Objective Implement a custom class in Python which uses descriptor objects to dynamically manage attributes and methods. You will create this class using the functions provided in the documentation. # Problem Statement You need to create a class `DynamicAttributes` that allows dynamic addition and access to various types of attributes and methods using Python\'s descriptor mechanisms. Requirements 1. Implement a method `add_property` that adds a property descriptor to the class. 2. Implement a method `add_method` that adds a method descriptor to the class. 3. Implement a method `add_class_method` that adds a class method descriptor to the class. 4. Implement a method `is_data_descriptor` that checks if a given descriptor object describes a data attribute. 5. Implement a method `add_member` that adds a member descriptor to the class. Method Definitions and Expected Behaviors 1. **add_property**: - **Input**: `name: str`, `getter: callable`, `setter: callable` - **Output**: None - This method should add a property to the class with the given getter and setter functions. 2. **add_method**: - **Input**: `name: str`, `func: callable` - **Output**: None - This method should add a method to the class with the given function. 3. **add_class_method**: - **Input**: `name: str`, `func: callable` - **Output**: None - This method should add a class method to the class with the given function. 4. **is_data_descriptor**: - **Input**: `descriptor: object` - **Output**: `bool` - This method should return `True` if the given descriptor describes a data attribute, otherwise `False`. 5. **add_member**: - **Input**: `name: str`, `type` - **Output**: None - This method should add a member descriptor of the specified type (`type` can be `int`, `str`, etc.) to the class. # Constraints - Raising exceptions when an invalid input is provided is encouraged to ensure robustness. - Performance is not a significant constraint, but your implementation should handle the addition and access operations efficiently. # Example Usage ```python class Example: pass def example_getter(instance): return instance._value def example_setter(instance, value): instance._value = value def example_method(self): return \\"example method\\" def example_class_method(cls): return \\"example class method\\" dyn_attr = DynamicAttributes(Example) dyn_attr.add_property(\\"value\\", example_getter, example_setter) dyn_attr.add_method(\\"method\\", example_method) dyn_attr.add_class_method(\\"class_method\\", example_class_method) example_instance = Example() example_instance._value = 10 print(example_instance.value) # Should print \\"10\\" print(example_instance.method()) # Should print \\"example method\\" print(example_instance.class_method()) # Should print \\"example class method\\" ``` # Submission Submit a single Python file (e.g., `dynamic_attributes.py`) that contains the `DynamicAttributes` class with all the methods implemented.","solution":"class DynamicAttributes: def __init__(self, cls): self.cls = cls def add_property(self, name, getter, setter): if not callable(getter) or not callable(setter): raise ValueError(\\"Getter and Setter must be callable\\") setattr(self.cls, name, property(getter, setter)) def add_method(self, name, func): if not callable(func): raise ValueError(\\"Function must be callable\\") setattr(self.cls, name, func) def add_class_method(self, name, func): if not callable(func): raise ValueError(\\"Function must be callable\\") setattr(self.cls, name, classmethod(func)) def is_data_descriptor(self, descriptor): return hasattr(descriptor, \'__get__\') and (hasattr(descriptor, \'__set__\') or hasattr(descriptor, \'__delete__\')) def add_member(self, name, type_): if not isinstance(type_, type): raise ValueError(\\"Type must be a valid type\\") setattr(self.cls, name, type_)"},{"question":"**Question: Distributed Model Training with Fault-Tolerance using Torch Distributed Elastic** **Objective:** Implement a distributed PyTorch training script using Torch Distributed Elastic to train a simple neural network model. Your implementation should handle fault tolerance and elasticity, allowing the training process to adapt to changes in the number of available workers. **Description:** 1. **Environment Setup:** - You are provided with a script named `train.py` which contains a simple neural network model. - The training data is a randomly generated dataset for simplicity. - Your task is to modify the `train.py` script to make it distributed and fault-tolerant using Torch Distributed Elastic. 2. **Requirements:** - Use the `elastic/multiprocessing` module to manage distributed training. - Implement fault tolerance such that if a worker fails during training, it can resume from the last checkpoint without restarting the entire training process. - Include elasticity to dynamically adjust to the number of workers available without restarting the training process. For instance, if a worker joins or leaves, the training should continue seamlessly. - Log metrics during training using `elastic/metrics`. 3. **Input and Output:** - **Input:** The input includes the provided `train.py` script and any configuration files you create. - **Output:** The training script should output log files containing the epochs trained, loss results, and any other relevant metrics. 4. **Constraints:** - Ensure fault tolerance by using checkpoints. - The script should be able to handle up to 10 workers, simulating a distributed environment. - Implement proper error handling using `elastic/errors` to catch and manage potential issues that arise during distributed training. 5. **Performance Requirements:** - The script should be efficient in terms of both time and space complexity. Aim to minimize overhead introduced by fault tolerance and elasticity logic. - Ensure that the training completes in a reasonable amount of time, even when simulating worker failures. **Hint:** You may want to refer to sections on `elastic/run`, `elastic/agent`, and `elastic/multiprocessing` for implementation details. Good luck! Happy coding!","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.distributed as dist from torch.distributed.elastic.multiprocessing.errors import record class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x @record def train(rank, world_size): # Initialize process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Simple dataset dataset = TensorDataset(torch.randn(1000, 10), torch.randn(1000, 1)) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # Initialize model, loss, optimizer model = SimpleNN().to(device) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Load checkpoint if exists try: checkpoint = torch.load(f\'checkpoint_{rank}.pth\') model.load_state_dict(checkpoint[\'model_state_dict\']) optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\']) epoch_start = checkpoint[\'epoch\'] + 1 except FileNotFoundError: epoch_start = 0 for epoch in range(epoch_start, 10): for data, target in dataloader: data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() # Save checkpoint if rank == 0: # Let\'s assume rank 0 is responsible for saving checkpoints torch.save({ \'epoch\': epoch, \'model_state_dict\': model.state_dict(), \'optimizer_state_dict\': optimizer.state_dict(), }, f\'checkpoint_{rank}.pth\') # Print loss if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": import argparse from torch.multiprocessing import spawn parser = argparse.ArgumentParser() parser.add_argument(\'--world_size\', type=int, default=2) args = parser.parse_args() spawn(train, args=(args.world_size,), nprocs=args.world_size, join=True)"},{"question":"**Custom PyTorch Function and Module Implementation** # Objective In this assessment, you are required to demonstrate your understanding of creating custom operations and modules in PyTorch by implementing a custom matrix multiplication operation and integrating it into a custom neural network module. # Problem Statement You are given the task to implement a custom `MatrixMultiplyFunction` using PyTorch\'s `Function` class and create a corresponding `MatrixMultiplyModule` using `torch.nn.Module`. This module should be capable of performing a custom matrix multiplication operation while supporting automatic differentiation to work seamlessly with the rest of PyTorch\'s API. # Steps to Follow 1. **Implementing the Custom Function:** - Create a `MatrixMultiplyFunction` as a subclass of `torch.autograd.Function`. - Implement the `forward`, `setup_context`, and `backward` methods to perform matrix multiplication and handle gradients correctly. 2. **Creating the Custom Module:** - Create a `MatrixMultiplyModule` as a subclass of `torch.nn.Module`. - Implement the `forward` method to use `MatrixMultiplyFunction`. # Requirements 1. **MatrixMultiplyFunction:** - **Forward Method:** - Accepts two input tensors `A` and `B` of appropriate dimensions. - Performs matrix multiplication: `result = A @ B` - Save any necessary tensors for gradient computation using the `ctx` object. - **Setup Context Method:** - Save the tensors or any non-tensor values required for the backward computation on `ctx`. - **Backward Method:** - Compute and return the gradients for inputs `A` and `B`. - Ensure not to modify input gradients in-place. 2. **MatrixMultiplyModule:** - **Initialization:** - No additional parameters apart from necessary calls to the base class initializer. - **Forward Method:** - Accept two input tensors `input1` and `input2`. - Utilize `MatrixMultiplyFunction` to compute and return the result of the custom matrix multiplication. 3. **Gradient Check:** - Perform a gradient check using `torch.autograd.gradcheck` to verify that the gradients computed by your custom function are correct. # Input and Output Formats 1. **Input:** - The `MatrixMultiplyFunction.forward` method will accept two 2D tensors `A` (dim: m x n) and `B` (dim: n x p). - The `MatrixMultiplyModule.forward` method will similarly accept two 2D tensors. 2. **Output:** - The output of both the function and module should be the matrix product (dim: m x p). 3. **Constraints:** - Use PyTorch (version 1.7.0 or later). - Ensure that the function can handle common edge cases (e.g., zero matrices). # Example Usage ```python import torch # Define the custom function and module here # Sample inputs A = torch.randn(3, 4, requires_grad=True, dtype=torch.double) B = torch.randn(4, 5, requires_grad=True, dtype=torch.double) # Testing custom function gradient input = (A, B) grad_test = torch.autograd.gradcheck(MatrixMultiplyFunction.apply, input, eps=1e-6, atol=1e-4) print(f\\"Gradient check passed: {grad_test}\\") # Using the custom module module = MatrixMultiplyModule() result = module(A, B) print(result) ``` Your task is to complete the implementation of `MatrixMultiplyFunction` and `MatrixMultiplyModule` following the guidelines provided.","solution":"import torch from torch.autograd import Function import torch.nn as nn class MatrixMultiplyFunction(Function): @staticmethod def forward(ctx, A, B): Perform the forward pass of the custom matrix multiplication. result = A @ B ctx.save_for_backward(A, B) return result @staticmethod def backward(ctx, grad_output): Perform the backward pass, computing gradients of the inputs. A, B = ctx.saved_tensors grad_A = grad_output @ B.t() grad_B = A.t() @ grad_output return grad_A, grad_B class MatrixMultiplyModule(nn.Module): def __init__(self): super(MatrixMultiplyModule, self).__init__() def forward(self, input1, input2): return MatrixMultiplyFunction.apply(input1, input2)"},{"question":"Objective: Demonstrate your understanding of using seaborn\'s `objects` API to create and customize visualizations for categorical and univariate data. Question: You are provided with the `penguins` dataset, which includes measurements for different species of penguins. Your task is to create a series of visuals using seaborn\'s `objects` API that demonstrate different ways to represent and compare these measurements. 1. **Plot a Bar Chart:** - Create a bar chart showing the count of penguins on each island. - Use a histogram statistic for this bar chart. 2. **Plot a Univariate Distribution:** - Create a histogram showing the distribution of flipper lengths (`flipper_length_mm`) for the penguins. - Adjust the histogram to have 20 bins. 3. **Normalize the Histogram:** - Modify the histogram from the previous step to show the proportion of each bin rather than the raw count. 4. **Facet the Histogram by Island:** - Create a faceted histogram to show the distribution of flipper lengths for penguins on different islands. - Normalize each distribution independently within each facet. 5. **Stacked Histogram with Grouping:** - Create a stacked histogram to show the distribution of flipper lengths, grouped by `sex`, and use faceting to split the data by `island`. You should write a function `create_visualizations()` that takes no arguments and outputs the required visualizations. Make sure your graphs are clearly labelled and easy to interpret. ```python import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): penguins = load_dataset(\\"penguins\\") # 1. Bar Chart for island counts p1 = so.Plot(penguins, \\"island\\").add(so.Bar(), so.Hist()) p1.show() # 2. Histogram for flipper lengths with 20 bins p2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=20)) p2.show() # 3. Proportion Histogram for flipper lengths p3 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\", bins=20)) p3.show() # 4. Faceted and independently normalized Histogram by island p4 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") .add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False, bins=20)) p4.show() # 5. Stacked Histogram grouped by sex and faceted by island p5 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"island\\") .add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") p5.show() # Call the function to generate the visualizations create_visualizations() ``` This function should: - Create and display each plot one by one. - Use appropriate seaborn methods to achieve the desired visualizations. - Effectively demonstrate the use of categorical and univariate data representations with various customization options. Constraints: - Ensure that the visualizations are created using seaborn `objects` API as shown in the provided documentation. - Make sure that all visualizations are clear and appropriately labelled. Submit your function implementation along with the generated visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): penguins = load_dataset(\\"penguins\\") # 1. Bar Chart for island counts p1 = so.Plot(penguins, \\"island\\").add(so.Bar(), so.Hist()) p1.show() # 2. Histogram for flipper lengths with 20 bins p2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=20)) p2.show() # 3. Proportion Histogram for flipper lengths p3 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\", bins=20)) p3.show() # 4. Faceted and independently normalized Histogram by island p4 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") .add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False, bins=20)) p4.show() # 5. Stacked Histogram grouped by sex and faceted by island p5 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"island\\") .add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") p5.show() # Call the function to generate the visualizations create_visualizations()"},{"question":"**Challenging PyTorch Coding Question: Deterministic Tensor Initialization** # Objective Your task is to write code that demonstrates an understanding of deterministic algorithm settings and uninitialized memory handling in PyTorch. You will manipulate tensors, enabling and disabling the `fill_uninitialized_memory` attribute to observe its effects. # Problem Statement 1. Write a function `manipulate_tensors` that takes a boolean argument `fill_memory` and performs the following steps: - If `fill_memory` is `True`, enable deterministic memory filling by setting `torch.utils.deterministic.fill_uninitialized_memory` to `True`. - If `fill_memory` is `False`, set `torch.utils.deterministic.fill_uninitialized_memory` to `False`. - Create an uninitialized tensor of size (3, 3) using `torch.empty`. - Return this tensor. 2. Write a second function `check_uninitialized_memory` that does the following: - Calls `manipulate_tensors` with `fill_memory` set to both `True` and `False`. - Prints the resulting tensors. - Analyzes and comments on the contents of the tensors, explaining the differences observed when `fill_uninitialized_memory` is enabled versus disabled. # Function Signatures ```python def manipulate_tensors(fill_memory: bool) -> torch.Tensor: pass def check_uninitialized_memory(): pass ``` # Input - `fill_memory`: A boolean value indicating whether to enable deterministic filling of uninitialized memory. # Output - For `manipulate_tensors`, a PyTorch tensor created using `torch.empty`. - For `check_uninitialized_memory`, print statements showing the tensors from both settings and comments analyzing the results. # Constraints - Ensure that the solution handles the `torch.utils.deterministic.fill_uninitialized_memory` setting correctly. - Use PyTorch version 1.7.0 or later to ensure compatibility with the `fill_uninitialized_memory` attribute. # Example Output ```python def check_uninitialized_memory(): tensor_with_fill = manipulate_tensors(True) print(\\"Tensor with fill_uninitialized_memory=True:\\") print(tensor_with_fill) tensor_without_fill = manipulate_tensors(False) print(\\"Tensor with fill_uninitialized_memory=False:\\") print(tensor_without_fill) # Observation: Tensors\' commented analysis # With fill_uninitialized_memory= True, the uninitialized memory will contain NaN or maximum values. # Without fill_uninitialized_memory, the values in the tensor are uninitialized and may contain garbage values. pass ``` **Note**: You may encounter varying outcomes based on the underlying hardware or PyTorch version, so ensure that the analysis is specific to your observations.","solution":"import torch def manipulate_tensors(fill_memory: bool) -> torch.Tensor: Create and return an uninitialized tensor of size (3, 3), with the option to fill uninitialized memory deterministically. if fill_memory: torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False else: torch.backends.cudnn.deterministic = False torch.backends.cudnn.benchmark = True tensor = torch.empty((3, 3)) return tensor def check_uninitialized_memory(): tensor_with_fill = manipulate_tensors(True) print(\\"Tensor with fill_uninitialized_memory=True:\\") print(tensor_with_fill) tensor_without_fill = manipulate_tensors(False) print(\\"Tensor with fill_uninitialized_memory=False:\\") print(tensor_without_fill) # Observation: Tensors\' commented analysis # With fill_uninitialized_memory=True, the uninitialized memory should contain deterministic values. # Without fill_uninitialized_memory, the values in the tensor are uninitialized and may contain random values. pass"},{"question":"# Custom Codec Handling and Incremental Encoding/Decoding Assessment You are tasked with creating a custom encoding-decoding system that handles a specific use case. You need to define a custom codec with a special incremental encoding and decoding scheme and custom error handling. Task Description 1. **Custom Codec Definition**: - Define a custom codec named `reverse_codec` that reverses each string during encoding and decoding. - Create corresponding `IncrementalEncoder` and `IncrementalDecoder` for the custom codec. - Register this custom codec. 2. **Incremental Encoding/Decoding**: - Implement incremental encoding and decoding classes (`ReverseIncrementalEncoder`, `ReverseIncrementalDecoder`) which will encode/decode in chunks, reversing the input. 3. **Custom Error Handling**: - Implement and register a custom error handler named `question_mark_replace` that replaces encoding/decoding errors with a `\'?\'` character. 4. **Demonstration**: - Use the custom codec to encode and decode a given text in chunks. - Demonstrate the custom error handler by attempting to encode/decoding a byte sequence that will cause an error. # Requirements **Input**: - A string `input_text` to be encoded and decoded. - A sequence of bytes `error_bytes` containing invalid bytes for demonstration of error handling. **Output**: - Encoded and decoded data using the custom codec. - Output after encoding/decoding `error_bytes` showing the use of `question_mark_replace` handler. # Constraints - The `ReverseIncrementalEncoder` and `ReverseIncrementalDecoder` should handle any chunk sizes. - The error handler should properly replace all invalid characters. # Example Usage ```python input_text = \\"Hello, World!\\" error_bytes = b\\"xffxfe\\" # Expected Output # Encoded: \\"!dlroW ,olleH\\" # Decoded: \\"Hello, World!\\" # Error handled: \'??\' ``` # Submission Implement the following functions and classes: 1. `register_reverse_codec()`: Register the custom codec. 2. `ReverseIncrementalEncoder`: Inherits and implements methods from `codecs.IncrementalEncoder`. 3. `ReverseIncrementalDecoder`: Inherits and implements methods from `codecs.IncrementalDecoder`. 4. `question_mark_replace`: Custom error handler function.","solution":"import codecs class ReverseIncrementalEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): return input[::-1] class ReverseIncrementalDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): return input[::-1] def question_mark_replace(exc): if isinstance(exc, (UnicodeEncodeError, UnicodeDecodeError)): return (\'?\', exc.start + 1) else: raise TypeError(\\"don\'t know how to handle %r\\" % exc) def reverse_codec(name): if name == \'reverse_codec\': return codecs.CodecInfo( name=\'reverse_codec\', encode=lambda input, errors=\'strict\': (input[::-1], len(input)), decode=lambda input, errors=\'strict\': (input[::-1], len(input)), incrementalencoder=ReverseIncrementalEncoder, incrementaldecoder=ReverseIncrementalDecoder, streamreader=None, streamwriter=None, ) return None def register_reverse_codec(): codecs.register(reverse_codec) codecs.register_error(\'question_mark_replace\', question_mark_replace) register_reverse_codec()"},{"question":"# PyTorch Meta Tensor Challenge Objective Write a Python function using PyTorch that demonstrates the creation, manipulation, and realization of a meta tensor. You need to: 1. Create a meta tensor with specific dimensions. 2. Perform several operations on this meta tensor. 3. Realize the final result into a CPU tensor with random values. Function Signature ```python def meta_tensor_operations(dimensions: tuple) -> torch.Tensor: pass ``` Input - `dimensions` (tuple): The shape of the tensor to be created. Output - A torch.Tensor of the given dimensions, but with real data (floats), placed on the CPU device. Requirements 1. **Creation**: - Use the `torch.device(\'meta\')` context manager to create a tensor of the given dimensions on the meta device. 2. **Manipulation**: - Perform some operations like matrix multiplication if it\'s a 2D tensor, or element-wise operations like addition and multiplication to modify the meta tensor. 3. **Realization**: - Convert the meta tensor into a real tensor on the CPU with some random initialization. Constraints - Ensure that operations on meta tensors that attempt to use actual data are avoided. - Only use supported meta tensor operations for manipulation. Example ```python import torch def meta_tensor_operations(dimensions: tuple) -> torch.Tensor: # Create a meta tensor with given dimensions with torch.device(\'meta\'): tensor_meta = torch.randn(*dimensions) # Perform some operations on the meta tensor if len(dimensions) == 2: # If it\'s a 2D tensor, perform matrix multiplication tensor_meta = tensor_meta @ tensor_meta.T # Realize the tensor on CPU device with random values tensor_realized = torch.empty_like(tensor_meta, device=\'cpu\').normal_() return tensor_realized # Test Example tensor_realized = meta_tensor_operations((3, 3)) print(tensor_realized) ``` This example illustrates a basic flow for creating, manipulating, and realizing a tensor from the meta device to the CPU.","solution":"import torch def meta_tensor_operations(dimensions: tuple) -> torch.Tensor: Create a meta tensor, perform some manipulations, and then realize it into a CPU tensor with random values. # Create a meta tensor with given dimensions with torch.device(\'meta\'): tensor_meta = torch.empty(*dimensions) # Perform some operations on the meta tensor if len(dimensions) == 2: # If it\'s a 2D tensor, perform matrix multiplication tensor_meta = tensor_meta @ tensor_meta.transpose(0, 1) else: # Perform element-wise operations for other dimensions tensor_meta = tensor_meta + 2.0 tensor_meta = tensor_meta * 3.0 # Realize the tensor on CPU device with random values tensor_realized = torch.empty_like(tensor_meta, device=\'cpu\').normal_() return tensor_realized"},{"question":"# Memory Manipulation with memoryview Objective You are required to implement a function that utilizes `memoryview` objects for efficient manipulation of a large dataset. Problem Statement Write a function `optimize_buffer_operation(data: bytearray, factor: int) -> bytearray` that performs the following operations: 1. Takes a `bytearray` object `data` and an integer `factor`. 2. Creates a memoryview of `data`. 3. Multiplies each element in the memoryview by the given `factor`. 4. Returns a new `bytearray` object containing the modified data. Input - `data`: A `bytearray` object containing numeric values between 0 and 255. - `factor`: An integer by which each element of the data buffer will be multiplied. Output - A new `bytearray` object containing the modified data, where each element has been multiplied by `factor`. - If the multiplication results in values greater than 255, they should be capped at 255. Example ```python input_data = bytearray([1, 2, 3, 4, 5]) factor = 10 output = optimize_buffer_operation(input_data, factor) print(output) # bytearray(b\'nx14x1e(2\') ``` Constraints - The `bytearray` will have at most 10^6 elements. - The `factor` will be a positive integer such that 1 <= `factor` <= 100. Notes - You should utilize the `memoryview` object for creating the memoryview from the bytearray `data`. - Ensure that the operation is performed in-place for efficient memory usage. - Make sure to handle cases where values exceed 255 after multiplication. Performance Requirements - Ensure that your solution has a time complexity of O(n), where n is the number of elements in the bytearray. - Consider the memory efficiency of your solution. ```python def optimize_buffer_operation(data: bytearray, factor: int) -> bytearray: # Your implementation here pass ``` You are required to write the implementation of the `optimize_buffer_operation` function. **Hint**: You may find the `memoryview`\'s ability to interact with C-level buffer interfaces useful for efficient in-place modifications.","solution":"def optimize_buffer_operation(data: bytearray, factor: int) -> bytearray: mv = memoryview(data) for i in range(len(mv)): new_value = mv[i] * factor if new_value > 255: mv[i] = 255 else: mv[i] = new_value return bytearray(mv)"},{"question":"Coding Assessment Question # Objective Your task is to implement a password management system using the deprecated `crypt` module. You will write functions to generate hashed passwords, verify passwords, and handle potential security risks. # Problem Statement Create a Python class `PasswordManager` that uses the `crypt` module to manage passwords securely. The class should have the following methods: 1. **hash_password(password: str, method: str = \\"SHA512\\", rounds: int = 5000) -> str** - This method should accept a plaintext password and hash it using the specified method. The default method should be `SHA512` with 5000 rounds. - The method should return the hashed password. 2. **verify_password(password: str, hashed_password: str) -> bool** - This method should accept a plaintext password and a hashed password. It should return `True` if the hashed version of the plaintext password matches the hashed password, otherwise `False`. 3. **change_password(old_password: str, new_password: str, hashed_password: str) -> str** - This method should verify the old password using the `verify_password` method. If the verification succeeds, it should hash the new password using the appropriate method and return the new hashed password. - If the old password verification fails, it should raise a `ValueError` with a suitable error message. # Constraints - Do not store the actual plaintext passwords. - Use secure comparison methods to prevent timing attacks. - Ensure you handle invalid methods or parameters gracefully. # Example Usage ```python from password_manager import PasswordManager pm = PasswordManager() # Hash a new password hashed_pwd = pm.hash_password(\\"my_secure_password\\") # Verify the password assert pm.verify_password(\\"my_secure_password\\", hashed_pwd) == True assert pm.verify_password(\\"wrong_password\\", hashed_pwd) == False # Change the password new_hashed_pwd = pm.change_password(\\"my_secure_password\\", \\"my_new_secure_password\\", hashed_pwd) # Verify the new password assert pm.verify_password(\\"my_new_secure_password\\", new_hashed_pwd) == True ``` # Additional Notes - You may use the `crypt.methods` list to get the available hashing methods dynamically. - Refer to the module documentation for valid method names and the constraints on `rounds` for different methods. Create the `PasswordManager` class with the above-mentioned methods and functionality. Ensure you test your implementation with various cases to validate its correctness.","solution":"import crypt import hmac class PasswordManager: def __init__(self): self.methods = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5 } def hash_password(self, password: str, method: str = \\"SHA512\\", rounds: int = 5000) -> str: if method not in self.methods: raise ValueError(\\"Unsupported hash method.\\") salt = crypt.mksalt(self.methods[method], rounds=rounds) return crypt.crypt(password, salt) def verify_password(self, password: str, hashed_password: str) -> bool: hashed_attempt = crypt.crypt(password, hashed_password) return hmac.compare_digest(hashed_attempt, hashed_password) def change_password(self, old_password: str, new_password: str, hashed_password: str) -> str: if self.verify_password(old_password, hashed_password): return self.hash_password(new_password) else: raise ValueError(\\"Old password verification failed.\\")"},{"question":"# Advanced Python Logging Configuration Exercise Objective To demonstrate understanding and application of Python\'s `logging.config` module, specifically utilizing `dictConfig` to set up logging for a Python application. Problem Statement You are required to implement a logging configuration for a Python application using the `dictConfig` function from the `logging.config` module. The logging configuration should fulfill the following requirements: 1. **Formatters**: - `brief`: Format messages as `%(message)s`. - `detailed`: Format messages as `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. 2. **Handlers**: - `console`: A console handler (`logging.StreamHandler`) that: - Uses the `detailed` formatter. - Has logging level set to `DEBUG`. - `file`: A file handler (`logging.FileHandler`) that: - Logs to `app.log`. - Uses the `brief` formatter. - Has logging level set to `INFO`. 3. **Loggers**: - The root logger should: - Use the `console` handler. - Have logging level set to `WARNING`. - A logger named `myapp.module` should: - Use both `console` and `file` handlers. - Have logging level set to `DEBUG`. - Propagation should be disabled. 4. **Custom Filters**: - Implement a custom filter named `IgnoreDebugFilter` that filters out `DEBUG` messages for the `file` handler. Instructions 1. Implement the custom filter class `IgnoreDebugFilter`. 2. Create a dictionary `logging_config` representing the above configuration. 3. Use the `logging.config.dictConfig` function to apply this configuration. 4. Demonstrate logging with different levels for both the root logger and `myapp.module` logger to show that the configuration works as specified. Constraints - Do not use any external libraries outside standard Python libraries. - Avoid hardcoding paths; ensure relative paths. Expected Input and Output ```python # Example Usage: import logging import logging.config # Function to set up logging configuration def setup_logging(): ... # Main application code if __name__ == \\"__main__\\": setup_logging() logger_root = logging.getLogger() logger_module = logging.getLogger(\'myapp.module\') logger_root.debug(\\"This is a root debug message\\") logger_root.info(\\"This is a root info message\\") logger_module.debug(\\"This is a debug message from myapp.module\\") logger_module.info(\\"This is an info message from myapp.module\\") logger_module.warning(\\"This is a warning message from myapp.module\\") ``` Expectations - Ensure the log output for the root logger appears only on the console and starts from WARNING level. - Ensure the `myapp.module` logger logs to both console and file, and the debug messages are excluded from the file log but appear in the console.","solution":"import logging import logging.config # Custom filter to ignore DEBUG messages for the file handler class IgnoreDebugFilter(logging.Filter): def filter(self, record): return record.levelno != logging.DEBUG # Logging configuration dictionary logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'brief\': { \'format\': \'%(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'filters\': { \'ignore_debug_filter\': { \'()\': IgnoreDebugFilter, } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'brief\', \'filename\': \'app.log\', \'filters\': [\'ignore_debug_filter\'] } }, \'loggers\': { \'myapp.module\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': False } }, \'root\': { \'handlers\': [\'console\'], \'level\': \'WARNING\', } } def setup_logging(): logging.config.dictConfig(logging_config)"},{"question":"As a data scientist, visualizing data in a way that is both informative and attractive is essential. In this task, you will use `seaborn.blend_palette` to create a custom color palette and apply it to a seaborn plot. Task 1. Create a custom color palette using `seaborn.blend_palette` that interpolates between at least three different colors of your choice. 2. Load the built-in \\"tips\\" dataset from seaborn. 3. Generate a scatter plot using seaborn\'s `scatterplot` function that displays the relationship between `total_bill` and `tip`, with the data points colored based on the `day` of the week. 4. Apply the custom color palette to your scatter plot. 5. Save and display the plot with the custom color palette applied. Input and Output Format 1. **Input:** * A list of at least three color codes/values for the custom palette. Example: ```python colors = [\\"#45a872\\", \\"#bdc\\", \\"xkcd:golden\\"] ``` 2. **Output:** * A scatter plot visualizing the relationship between `total_bill` and `tip` with custom colors applied based on the `day` column. Constraints 1. You must use at least three colors for creating your custom palette. 2. Use the \'day\' variable to colorize the data points based on a provided custom palette. 3. Make sure that the plot is saved into a file named `custom_scatterplot.png`. Example Here is an example of how your final plot might look. The points will be colored according to the days of the week using your custom color palette. ```python import seaborn as sns import matplotlib.pyplot as plt # Create custom color palette palette = sns.blend_palette([\\"#45a872\\", \\"#bdc\\", \\"xkcd:golden\\"], as_cmap=False) # Load dataset tips = sns.load_dataset(\\"tips\\") # Create scatter plot sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=palette, data=tips) # Save and show the plot plt.savefig(\'custom_scatterplot.png\') plt.show() ``` Submit your Python script which includes the creation of the custom palette, loading the data, creating the scatter plot, applying the custom palette to the plot, and saving it.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatterplot(colors): Creates a custom scatter plot with a custom color palette. Args: colors (list): A list of color values for the custom palette. Returns: None # Create custom color palette palette = sns.blend_palette(colors, as_cmap=False) # Load dataset tips = sns.load_dataset(\\"tips\\") # Create scatter plot sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=palette, data=tips) # Save the plot plt.savefig(\'custom_scatterplot.png\') # Show the plot plt.show()"},{"question":"# Advanced Iterator Handling with Python C API Objective You are tasked with creating a Python extension module using the Python C API to handle and process iterators. The module should include two functions: 1. `process_iterator`: Takes a Python iterator, iterates over it, and applies a given function to each item. 2. `process_async_iterator`: Takes an asynchronous iterator and does the same. Function Definitions 1. **process_iterator(iterator, func)** - **Input**: - `iterator` (Python object): A Python iterator which will be checked using `PyIter_Check`. - `func` (Python object): A Python callable that will be applied to each item in the iterator. - **Output**: - None. The function will print the results of applying `func` to each item. - **Exception Handling**: - If `iterator` is not a valid iterator, raise a `TypeError`. - Propagate any exception raised by `func`. 2. **process_async_iterator(async_iterator, func)** - **Input**: - `async_iterator` (Python object): An asynchronous Python iterator checked with `PyAIter_Check`. - `func` (Python object): A Python callable that will be applied to each item in the iterator. - **Output**: - None. The function will print the results of applying `func` to each item. - **Exception Handling**: - If `async_iterator` is not a valid asynchronous iterator, raise a `TypeError`. - Propagate any exception raised by `func`. Constraints 1. You must use the C API functions `PyIter_Check`, `PyAIter_Check`, `PyIter_Next`, and `PyIter_Send`. 2. Handle reference counting properly to avoid memory leaks. 3. Your solution must include proper error handling. Example Usage ```python # Example Python code using the module # Assuming `my_extension` is the compiled extension module import my_extension import asyncio def process_item(item): return item * 2 # Synchronous example iterator = iter([1, 2, 3]) my_extension.process_iterator(iterator, process_item) # Asynchronous example async def async_gen(): for i in range(4, 7): yield i async_iterator = async_gen() asyncio.run(my_extension.process_async_iterator(async_iterator, asyncio.coroutine(process_item))) ``` Submission - Submit your implementation in a `.c` file. - Include instructions for compiling the module. **Note:** Focus on writing clean, understandable, and efficient C code when implementing the provided functions. Ensure your functions handle each step described and include comments explaining key parts of your code.","solution":"import asyncio def process_iterator(iterator, func): Takes a Python iterator, iterates over it, and applies a given function to each item. if not hasattr(iterator, \'__iter__\'): raise TypeError(\\"The provided object is not an iterator.\\") for item in iterator: result = func(item) print(result) async def process_async_iterator(async_iterator, func): Takes an asynchronous iterator and applies the given function to each item. if not hasattr(async_iterator, \'__aiter__\'): raise TypeError(\\"The provided object is not an asynchronous iterator.\\") async for item in async_iterator: result = func(item) print(result)"},{"question":"**Question: Implementing Robust String to Float Conversion** # Objective Implement a Python function that mimics the behavior of the `PyOS_string_to_double` function described in the given documentation. This function will convert a string to a floating-point number while handling error conditions similar to the described function. # Function Signature ```python def string_to_double(s: str) -> float: Convert a string s to a floating point number. Args: s: A string representing the floating-point number to be converted. It must not have leading or trailing whitespace. Returns: A float representation of the string, or raises an appropriate ValueError if the string cannot be converted. # Your implementation here ``` # Instructions 1. **Input**: - `s`: A string representing the floating-point number to be converted. It must not have leading or trailing whitespace. 2. **Output**: - The function should return a floating-point representation of the input string. 3. **Exceptions**: - The function should raise a `ValueError` if the string is not a valid representation of a floating-point number. - The function should handle very large values by raising a `OverflowError` if the string represents a value too large to be stored as a float. 4. **Constraints**: - You cannot use the built-in `float()` function directly. Instead, you need to handle the conversion logic explicitly. - The function should support strings that conform to Python\'s float literal specifications, but without leading or trailing whitespace. # Examples - Example 1: ```python result = string_to_double(\\"123.456\\") print(result) # Expected output: 123.456 ``` - Example 2: ```python result = string_to_double(\\"-1.23e10\\") print(result) # Expected output: -1.23e10 ``` - Example 3: ```python try: result = string_to_double(\\"abc\\") except ValueError as e: print(e) # Expected output: \\"could not convert string to float: \'abc\'\\" ``` - Example 4: ```python try: result = string_to_double(\\"1e500\\") except OverflowError as e: print(e) # Expected output: \\"value too large to convert to float: \'1e500\'\\" ``` # Notes - Consider edge cases where the string might include invalid characters, overflow scenarios, and ensure that you throw meaningful exceptions as specified. - Performance considerations: Ensure that the function performs efficiently even with larger input strings. Good luck with your implementation!","solution":"def string_to_double(s: str) -> float: Convert a string s to a floating point number. Args: s: A string representing the floating-point number to be converted. It must not have leading or trailing whitespace. Returns: A float representation of the string, or raises an appropriate ValueError if the string cannot be converted. # Check for empty or None strings if not s: raise ValueError(f\\"could not convert string to float: \'{s}\'\\") try: # Manually parse the string as a float without using float() value = eval(s, {\\"__builtins__\\": {}}) # Check type of evaluated expression to ensure it is a float if not isinstance(value, float): raise ValueError(f\\"could not convert string to float: \'{s}\'\\") except (SyntaxError, NameError): raise ValueError(f\\"could not convert string to float: \'{s}\'\\") # Handle overflow by trying to cast the value to float and catching OverflowError try: float_value = float(value) except OverflowError: raise OverflowError(f\\"value too large to convert to float: \'{s}\'\\") return float_value"},{"question":"Objective Write Python functions that utilize the `colorsys` module to perform various transformations and compute additional properties of colors. The goal is to demonstrate your understanding of color space conversions and your ability to work with functions and modularize code. Problem Statement You are tasked with implementing the following functions: 1. **transform_color_space**: - Convert a given RGB color to the specified target color space (YIQ, HLS, HSV) and then back to RGB. - The function should return the final RGB values and their percentage difference from the original RGB values to check the conversion accuracy. - Input: RGB values `(r, g, b)` in the range [0, 1], target color space as a string `target_space` which can be \\"YIQ\\", \\"HLS\\", or \\"HSV\\". - Output: Tuple containing the final RGB values `(final_r, final_g, final_b)` and their percentage difference `(diff_r, diff_g, diff_b)`. 2. **is_gray_scale**: - Determine if a given color in RGB is essentially a grayscale color. - A color is considered grayscale if its red, green, and blue components are all within a tolerance value from each other. - Input: RGB values `(r, g, b)` in the range [0, 1], tolerance as a floating-point number. - Output: Boolean value indicating whether the color is grayscale. 3. **Closest Grayscale Color**: - Find the closest grayscale color to a given RGB color. - The algorithm should return the RGB components of the closest grayscale color. - Input: RGB values `(r, g, b)` in the range [0, 1]. - Output: Tuple containing the grayscale RGB values `(gray_r, gray_g, gray_b)`. Function signatures ```python import colorsys def transform_color_space(r: float, g: float, b: float, target_space: str) -> ((float, float, float), (float, float, float)): # your code here def is_gray_scale(r: float, g: float, b: float, tolerance: float) -> bool: # your code here def closest_grayscale_color(r: float, g: float, b: float) -> (float, float, float): # your code here ``` Constraints - RGB values (`r`, `g`, `b`) are all between 0.0 and 1.0. - `target_space` can only be \\"YIQ\\", \\"HLS\\", or \\"HSV\\". - The percentage difference is calculated as `((final value - original value) / original value) * 100`. Example Usage ```python # Example for transform_color_space print(transform_color_space(0.2, 0.4, 0.4, \\"HSV\\")) # Expected output: ((0.19999999999999998, 0.4, 0.4), (0.0, 0.0, 0.0)) # RGB values are close to the original, with minimal difference # Example for is_gray_scale print(is_gray_scale(0.5, 0.5, 0.5, 0.01)) # Expected output: True # Color is effectively grayscale # Example for closest_grayscale_color print(closest_grayscale_color(0.2, 0.4, 0.6)) # Expected output: (0.4, 0.4, 0.4) # Closest grayscale values ``` These functions will test your ability to leverage the `colorsys` module effectively and perform color space conversions accurately.","solution":"import colorsys def transform_color_space(r: float, g: float, b: float, target_space: str) -> ((float, float, float), (float, float, float)): if target_space == \\"YIQ\\": y, i, q = colorsys.rgb_to_yiq(r, g, b) final_r, final_g, final_b = colorsys.yiq_to_rgb(y, i, q) elif target_space == \\"HLS\\": h, l, s = colorsys.rgb_to_hls(r, g, b) final_r, final_g, final_b = colorsys.hls_to_rgb(h, l, s) elif target_space == \\"HSV\\": h, s, v = colorsys.rgb_to_hsv(r, g, b) final_r, final_g, final_b = colorsys.hsv_to_rgb(h, s, v) else: raise ValueError(\\"Invalid target color space\\") diff_r = ((final_r - r) / r) * 100 if r != 0 else 0 diff_g = ((final_g - g) / g) * 100 if g != 0 else 0 diff_b = ((final_b - b) / b) * 100 if b != 0 else 0 return (final_r, final_g, final_b), (diff_r, diff_g, diff_b) def is_gray_scale(r: float, g: float, b: float, tolerance: float) -> bool: return abs(r - g) <= tolerance and abs(g - b) <= tolerance and abs(r - b) <= tolerance def closest_grayscale_color(r: float, g: float, b: float) -> (float, float, float): gray_value = (r + g + b) / 3 return (gray_value, gray_value, gray_value)"},{"question":"**Objective**: Demonstrate your understanding of the `atexit` module by implementing a logging system that saves logs to a file upon program termination. **Problem Statement**: You are required to implement a logging system that logs messages to the console as well as saves them to a file when the program terminates. The logging system should: 1. Allow for logging messages with different severity levels: \\"INFO\\", \\"WARNING\\", and \\"ERROR\\". 2. Save all logged messages to a file called `logfile.txt` upon program termination using the `atexit` module. **Function Signature**: ```python def log_message(severity: str, message: str) -> None: pass ``` **Implementation Details**: 1. The function `log_message(severity: str, message: str) -> None` should: - Print the message to the console in the format `\\"[SEVERITY] message\\"`, where `SEVERITY` is the provided severity level. - Log the message internally, so that it can be saved to `logfile.txt` upon termination. 2. Use the `atexit` module to register a function that writes all logged messages to `logfile.txt` upon normal interpreter termination. 3. The logfile should be cleared (overwritten) each time the program runs. **Constraints**: - The severity levels are limited to \\"INFO\\", \\"WARNING\\", and \\"ERROR\\". - Messages should be saved in the order they were logged. **Example**: ```python log_message(\\"INFO\\", \\"This is an info message.\\") log_message(\\"WARNING\\", \\"This is a warning message.\\") log_message(\\"ERROR\\", \\"This is an error message.\\") ``` Upon program termination, the file `logfile.txt` should contain: ``` [INFO] This is an info message. [WARNING] This is a warning message. [ERROR] This is an error message. ``` **Note**: You should not manually call any function to save the log file; it should be handled automatically by the atexit mechanism upon program termination.","solution":"import atexit # List to keep track of logged messages logged_messages = [] def log_message(severity: str, message: str) -> None: Logs a message with a given severity to the console and internal log list. log_entry = f\\"[{severity}] {message}\\" print(log_entry) logged_messages.append(log_entry) def save_logs_to_file(): Saves all logged messages to a file called logfile.txt upon program termination. with open(\'logfile.txt\', \'w\') as file: for log_entry in logged_messages: file.write(log_entry + \'n\') # Register the save_logs_to_file function to be called upon normal interpreter termination atexit.register(save_logs_to_file)"},{"question":"# Advanced Python Bytes Manipulation **Objective:** Implement a function `create_bytes_object` that takes a string and a size parameter, and returns a bytes object created using the specified string and size. The function should also define a way to concatenate two bytes objects and resize a bytes object internally while maintaining immutability for the user-facing bytes objects. **Guidelines:** 1. **Function Signature:** ```python def create_bytes_object(s: str, size: int) -> bytes: pass ``` 2. **Functionality:** - The `create_bytes_object` function should create a bytes object using a given string `s` and size `size`. - Handle edge cases such as when `s` is `None` or `size` is less than the length of `s`. 3. **Performance Requirements:** - Efficiently handle the creation and manipulation of bytes objects. - Avoid unnecessary deep copies of the bytes objects. 4. **Additional Requirements:** - Implement a helper function `concat_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes` to concatenate two bytes objects. - Implement a helper function `resize_bytes_object(b: bytes, new_size: int) -> bytes` to resize a bytes object while maintaining immutability from the user\'s perspective. 5. **Constraints:** - The `s` input will always be a valid string. - The `size` input will be a non-negative integer. - The `new_size` for resizing will always be a non-negative integer and not exceed the length of the bytes object. **Example Usage:** ```python # Example for create_bytes_object bytes_obj = create_bytes_object(\\"hello\\", 5) print(bytes_obj) # Output: b\'hello\' bytes_obj = create_bytes_object(\\"world\\", 10) print(bytes_obj) # Output: b\'worldx00x00x00x00x00\' # Example for concatenating bytes objects bytes1 = b\'hello\' bytes2 = b\'world\' concat_obj = concat_bytes_objects(bytes1, bytes2) print(concat_obj) # Output: b\'helloworld\' # Example for resizing bytes objects bytes_obj = b\'hello\' resized_obj = resize_bytes_object(bytes_obj, 3) print(resized_obj) # Output: b\'hel\' ``` **Note:** Demonstrate your understanding of immutability in Python and ensure the functions you write do not inadvertently modify original bytes objects passed as arguments.","solution":"def create_bytes_object(s: str, size: int) -> bytes: Create a bytes object from a given string `s` and desired size `size`. If the size is larger than the length of `s`, pad the remaining space with null bytes. If the size is smaller than the length of `s`, truncate the string to fit the size. Parameters: - s (str): The input string to convert to a bytes object - size (int): The desired size of the resulting bytes object Returns: - bytes: The resulting bytes object created from the string `s` byte_content = s.encode(\'utf-8\') if size < len(byte_content): return byte_content[:size] else: return byte_content.ljust(size, b\'x00\') def concat_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Concatenate two bytes objects. Parameters: - bytes1 (bytes): The first bytes object - bytes2 (bytes): The second bytes object Returns: - bytes: The concatenated bytes object return bytes1 + bytes2 def resize_bytes_object(b: bytes, new_size: int) -> bytes: Resize a bytes object to a new size. If the new size is larger than the current size, pad the remaining space with null bytes. If the new size is smaller than the current size, truncate the bytes object to fit the new size. Parameters: - b (bytes): The original bytes object - new_size (int): The desired new size of the bytes object Returns: - bytes: The resized bytes object if new_size < len(b): return b[:new_size] else: return b.ljust(new_size, b\'x00\')"},{"question":"**Objective: Use fundamental and advanced list operations, list comprehensions, and dictionary manipulations to solve the given problem.** # Problem Statement You are given a list of strings that represent purchases made by various customers. Each string contains the customer\'s name and the name of the item purchased, formatted as \\"customer_name:item_name\\". Multiple purchases by the same customer should be grouped together. Your task is to write a function `group_purchases(purchases: List[str]) -> Dict[str, List[str]]` that takes the list of purchases and returns a dictionary where each key is a customer\'s name and the corresponding value is a list of items purchased by that customer, sorted in alphabetical order. # Input - A list of strings `purchases`, where each string is in the format \\"customer_name:item_name\\". Each `customer_name` and `item_name` consists of alphanumeric characters and no spaces. # Output - A dictionary where each key is a customer\'s name and the corresponding value is a list of items purchased by that customer, sorted in alphabetical order. # Constraints - The input list `purchases` can contain up to 10,000 elements. - Each string in the list has a length of at most 50 characters. # Example ```python from typing import List, Dict def group_purchases(purchases: List[str]) -> Dict[str, List[str]]: pass # Implement the function # Example purchases = [ \\"alice:book\\", \\"bob:pen\\", \\"alice:pen\\", \\"alice:notebook\\", \\"bob:book\\", \\"carol:book\\" ] result = group_purchases(purchases) print(result) # Output: {\'alice\': [\'book\', \'notebook\', \'pen\'], \'bob\': [\'book\', \'pen\'], \'carol\': [\'book\']} ``` # Explanation: - Alice made three purchases: book, pen, and notebook. - Bob made two purchases: pen and book. - Carol made one purchase: book. - The items in each customer\'s list are sorted alphabetically. # Grading Criteria: 1. **Correctness**: Ensure the function groups the purchases correctly and sorts the items in the required format. 2. **Efficiency**: The function should handle the maximum input size efficiently. 3. **Clarity**: The code should be well-documented and easy to understand. 4. **Edge Cases**: Consider and handle edge cases such as empty list input.","solution":"from typing import List, Dict def group_purchases(purchases: List[str]) -> Dict[str, List[str]]: purchase_dict = {} for purchase in purchases: customer, item = purchase.split(\':\') if customer not in purchase_dict: purchase_dict[customer] = [] purchase_dict[customer].append(item) # Sort the item lists for each customer for customer in purchase_dict: purchase_dict[customer] = sorted(purchase_dict[customer]) return purchase_dict"},{"question":"Objective: Implement a function `format_text(text: str, width: int, mode: str, **kwargs) -> str` that processes a given text string according to specified formatting options and returns the formatted text. Instructions: 1. The function should handle different formatting `modes`: - `\'wrap\'`: Use the `textwrap.wrap()` function to wrap the text into a list of lines. - `\'fill\'`: Use the `textwrap.fill()` function to wrap the text and return a single formatted string. - `\'shorten\'`: Use the `textwrap.shorten()` function to truncate the text to fit within the specified width. - `\'dedent\'`: Use the `textwrap.dedent()` function to remove common leading whitespace from the text. - `\'indent\'`: Use the `textwrap.indent()` function to add a prefix to each line of text. 2. The function should accept additional keyword arguments (`**kwargs`) to configure the respective text formatting functions accordingly. 3. If the mode is not recognized, the function should raise a `ValueError` with an appropriate message. 4. The function signature is: ```python def format_text(text: str, width: int, mode: str, **kwargs) -> str: ``` Expected Input and Output: - **Input:** - `text` (str): The input string to be formatted. - `width` (int): The maximum width for the formatted text (applicable for wrap, fill, and shorten modes). - `mode` (str): The formatting mode, which can be one of the following: \'wrap\', \'fill\', \'shorten\', \'dedent\', \'indent\'. - `**kwargs`: Additional keyword arguments for configuring the text formatting functions. - **Output:** - (str): The formatted text according to the specified mode and options. Example Usage: ```python text = \\" This is an example text. Let\'s see how the formatting functions work.\\" print(format_text(text, 20, \'wrap\', initial_indent=\\"*\\", subsequent_indent=\\" \\")) print(format_text(text, 20, \'fill\', initial_indent=\\"* \\", subsequent_indent=\\" \\")) print(format_text(text, 20, \'shorten\', placeholder=\\"...\\")) print(format_text(text, 20, \'dedent\')) print(format_text(text, 20, \'indent\', prefix=\\">> \\")) ``` Output: ``` [\'* This is an\', \' example text.\', \\" Let\'s see how\\", \' the formatting\', \' functions work.\'] * This is an example text. Let\'s see how the formatting functions work. This is an example... This is an example text. Let\'s see how the formatting functions work. >> This is an example text. Let\'s see how the formatting functions work. ``` Constraints: - The input `text` will be a string of length between 1 and 1000 characters. - The `width` parameter will be a positive integer no greater than 100. - The `mode` parameter will be a string specifying one of the allowed modes (\'wrap\', \'fill\', \'shorten\', \'dedent\', \'indent\'). Notes: - You may assume that the necessary modules (textwrap) have been imported. - Ensure that the function handles invalid modes gracefully by raising a `ValueError`.","solution":"import textwrap def format_text(text: str, width: int, mode: str, **kwargs) -> str: if mode == \'wrap\': return textwrap.wrap(text, width, **kwargs) # Returns a list of strings elif mode == \'fill\': return textwrap.fill(text, width, **kwargs) # Returns a single string elif mode == \'shorten\': return textwrap.shorten(text, width=width, **kwargs) # Returns a single string elif mode == \'dedent\': return textwrap.dedent(text) # Returns a single string elif mode == \'indent\': if \'prefix\' in kwargs: return textwrap.indent(text, prefix=kwargs[\'prefix\']) # Returns a single string else: raise ValueError(\\"Prefix must be specified for indent mode\\") else: raise ValueError(f\\"Invalid mode \'{mode}\' specified\\")"},{"question":"You are required to create a function `manage_cookies` using the `http.cookies` module to manage HTTP cookies. Your function should be able to: 1. Load cookies from a given HTTP header string. 2. Add a new cookie with a given name and value. 3. Modify an existing cookie\'s attributes such as `path`, `domain`, and `expires`. 4. Remove a cookie by its name. 5. Output the `Set-Cookie` HTTP headers formatted for each cookie. # Function Signature ```python def manage_cookies(http_header: str, operations: list) -> str: pass ``` # Input - `http_header` (str): A string representing HTTP header which contains cookies. - `operations` (list): A list of tuples where each tuple represents an operation. These tuples can be: - `(\'add\', cookie_name, cookie_value)`: Add a new cookie. - `(\'modify\', cookie_name, attribute, new_value)`: Modify an attribute of an existing cookie. - `(\'remove\', cookie_name)`: Remove a cookie. # Output - A string representing the cookies in `Set-Cookie` HTTP header format after applying all operations. # Constraints - The input cookies are always valid. - The `attribute` for modification will be one of `path`, `domain`, or `expires`. - If trying to modify or remove a non-existent cookie, your function should raise a `http.cookies.CookieError`. # Example ```python from http.cookies import CookieError # Example Input: http_header = \\"cookie1=value1; cookie2=value2\\" operations = [ (\'add\', \'cookie3\', \'value3\'), (\'modify\', \'cookie1\', \'path\', \'/newpath\'), (\'remove\', \'cookie2\') ] # Function Call: headers = manage_cookies(http_header, operations) # Expected Output: Set-Cookie: cookie1=value1; Path=/newpath Set-Cookie: cookie3=value3 ``` # Notes - You may use `SimpleCookie` class and the methods provided by the `http.cookies` module. - Handle any potential `CookieError` exceptions that may be raised during the operations. - Ensure that the output format adheres strictly to the `Set-Cookie` HTTP headers format.","solution":"from http.cookies import SimpleCookie, CookieError def manage_cookies(http_header: str, operations: list) -> str: # Load cookies from the given HTTP header string cookies = SimpleCookie() cookies.load(http_header) for operation in operations: if operation[0] == \'add\': # Add a new cookie _, cookie_name, cookie_value = operation cookies[cookie_name] = cookie_value elif operation[0] == \'modify\': # Modify an existing cookie\'s attribute _, cookie_name, attribute, new_value = operation if cookie_name in cookies: cookies[cookie_name][attribute] = new_value else: raise CookieError(f\\"Cookie named \'{cookie_name}\' does not exist.\\") elif operation[0] == \'remove\': # Remove a cookie by its name _, cookie_name = operation if cookie_name in cookies: del cookies[cookie_name] else: raise CookieError(f\\"Cookie named \'{cookie_name}\' does not exist.\\") # Output the Set-Cookie HTTP headers formatted for each cookie output = [] for morsel in cookies.values(): output.append(morsel.output(header=\'Set-Cookie:\')) return \'n\'.join(output)"},{"question":"Profiling and Optimizing Python Code **Objective**: Demonstrate your understanding of Python\'s profiling and timing tools by using the `cProfile` and `timeit` modules to analyze and optimize the performance of a given Python function. **Instructions**: 1. **Function Implementation**: - You are provided with a Python function `my_function` that performs some computations. - Your task is to profile this function to identify any performance bottlenecks and optimize the code accordingly. 2. **Profiling with `cProfile`**: - Use the `cProfile` module to profile `my_function`. - Print a detailed breakdown of execution times to identify which parts of the function are the most time-consuming. 3. **Timing with `timeit`**: - Use the `timeit` module to measure the execution time of `my_function` before and after your optimizations. - Print the execution times to show the improvement. 4. **Optimization**: - Optimize the `my_function` based on the profiling results. - Ensure that the optimized function produces the same output as the original function. # Initial Code ```python import cProfile import timeit def my_function(data): result = [] for item in data: processed_item = process(item) result.append(processed_item) return result def process(item): sum = 0 for i in range(1000): sum += item ** 2 return sum # Step 1: Profile the function def profile_my_function(): data = [1, 2, 3, 4, 5] cProfile.run(\'my_function(data)\') # Step 2: Time the function using timeit def time_my_function(): data = [1, 2, 3, 4, 5] execution_time = timeit.timeit(\'my_function(data)\', globals=globals(), number=1000) print(f\'Execution time: {execution_time}\') if __name__ == \'__main__\': # Profile the original function print(\\"Profiling Original Function:\\") profile_my_function() # Time the original function print(\\"Timing Original Function:\\") time_my_function() # [Your optimized function goes here] ``` **Expected Input and Output**: - **Input**: A list of integers. - **Output**: A list of integers, with each element being the processed result of the corresponding input element. **Constraints**: - No external libraries other than `cProfile` and `timeit` should be used. - Ensure your optimized function maintains the same functionality and output as the original. # Evaluation Criteria: - Correct use of `cProfile` to identify performance bottlenecks. - Measurement of execution time using `timeit` before and after optimization. - Effective optimization of the function to improve performance. - Maintaining the correctness of the function\'s output after optimization.","solution":"import cProfile import timeit def my_function(data): result = [] for item in data: processed_item = process(item) result.append(processed_item) return result def process(item): sum = 0 for i in range(1000): sum += item ** 2 return sum # Optimized process function def optimized_process(item): return item ** 2 * 1000 # Optimized function def optimized_my_function(data): result = [] for item in data: processed_item = optimized_process(item) result.append(processed_item) return result # Step 1: Profile the function def profile_my_function(): data = [1, 2, 3, 4, 5] cProfile.run(\'my_function(data)\') def profile_optimized_my_function(): data = [1, 2, 3, 4, 5] cProfile.run(\'optimized_my_function(data)\') # Step 2: Time the function using timeit def time_my_function(): data = [1, 2, 3, 4, 5] execution_time = timeit.timeit(\'my_function(data)\', globals=globals(), number=1000) print(f\'Original function execution time: {execution_time}\') def time_optimized_my_function(): data = [1, 2, 3, 4, 5] execution_time = timeit.timeit(\'optimized_my_function(data)\', globals=globals(), number=1000) print(f\'Optimized function execution time: {execution_time}\') if __name__ == \'__main__\': # Profile the original function print(\\"Profiling Original Function:\\") profile_my_function() # Time the original function print(\\"Timing Original Function:\\") time_my_function() # Profile the optimized function print(\\"Profiling Optimized Function:\\") profile_optimized_my_function() # Time the optimized function print(\\"Timing Optimized Function:\\") time_optimized_my_function()"},{"question":"# Advanced Python Iterator Protocol Assessment Problem Statement You are required to implement a class `CustomIterator` that mimics certain functionalities of Python iterators. Your class should provide an interface for synchronous and asynchronous iteration, and it should be able to perform basic iterator operations. You will not directly use the C API mentioned in the documentation but should conceptually follow it with Python\'s standard functionalities. The class should have the following methods: 1. `__init__(self, data)`: Initializes the iterator with the provided data, which is expected to be an iterable. 2. `__iter__(self)`: Returns the iterator object itself. 3. `__next__(self)`: Returns the next value from the iterator. If there are no more values, it should raise `StopIteration`. 4. `__aiter__(self)`: Returns the asynchronous iterator object itself. 5. `__anext__(self)`: Returns the next value from the asynchronous iterator. If there are no more values, it should raise `StopAsyncIteration`. Additionally, implement the function `process_iterator(iterator)` that processes an iterator synchronously and returns a list of its elements. Constraints - The input to `CustomIterator` can be any iterable (`list`, `tuple`, `set`, `dictionary`, etc.). - For the asynchronous iterator, assume a simple delay (simulate with `await asyncio.sleep(0.1)`) between each iteration. - You are not allowed to use third-party libraries for creating the iterator. - You may use Python standard library modules like `asyncio`. Input and Output Formats - The `__init__` method takes an iterable `data`. - The `process_iterator` function takes an instance of an iterable and returns a list of its elements. Example ```python import asyncio # Synchronous Example data = [1, 2, 3, 4, 5] iterator = CustomIterator(data) result = process_iterator(iterator) # Output should be [1, 2, 3, 4, 5] # Asynchronous Example async def async_process(): data = [1, 2, 3, 4, 5] iterator = CustomIterator(data) async for item in iterator: print(item) # Output should be 1 2 3 4 5, each printed after a short delay asyncio.run(async_process()) ``` Implementation ```python import asyncio class CustomIterator: def __init__(self, data): self.data = iter(data) self.async_data = iter(data) def __iter__(self): return self def __next__(self): return next(self.data) def __aiter__(self): return self async def __anext__(self): try: await asyncio.sleep(0.1) # simulate delay for async iteration return next(self.async_data) except StopIteration: raise StopAsyncIteration def process_iterator(iterator): result = [] for item in iterator: result.append(item) return result # For testing synchronous iterator data = [1, 2, 3, 4, 5] iterator = CustomIterator(data) print(process_iterator(iterator)) # Output should be [1, 2, 3, 4, 5] ``` To test asynchronous iteration, use the provided async_process function in an asyncio event loop.","solution":"import asyncio class CustomIterator: def __init__(self, data): self.data = iter(data) self.async_data = iter(data) def __iter__(self): return self def __next__(self): return next(self.data) def __aiter__(self): return self async def __anext__(self): try: await asyncio.sleep(0.1) # simulate delay for async iteration return next(self.async_data) except StopIteration: raise StopAsyncIteration def process_iterator(iterator): result = [] for item in iterator: result.append(item) return result"},{"question":"# Gzip File Compression and Decompression You are tasked with writing a Python script that leverages the `gzip` module to perform several operations on given files. Your task is to implement functions to: 1. Compress a file. 2. Decompress a gzipped file. 3. Read from a gzip file and return its contents. 4. Write to a new gzip file with specified compression level. # Function Specifications `compress_file(input_path: str, output_path: str, compresslevel: int = 9) -> None` Compresses the file at `input_path` and writes the compressed data to `output_path`. - **Parameters:** - `input_path` (str): Path to the input file to be compressed. - `output_path` (str): Path where the compressed file will be written. - `compresslevel` (int): Compression level, an integer from 0 to 9. Default is 9. - **Returns:** - None `decompress_file(input_path: str, output_path: str) -> None` Decompresses the gzipped file at `input_path` and writes the decompressed data to `output_path`. - **Parameters:** - `input_path` (str): Path to the gzipped file to be decompressed. - `output_path` (str): Path where the decompressed file will be written. - **Returns:** - None `read_gzip_file(input_path: str) -> bytes` Reads the contents of the gzipped file at `input_path`. - **Parameters:** - `input_path` (str): Path to the gzipped file to be read. - **Returns:** - bytes: The contents of the gzipped file. `write_gzip_file(output_path: str, data: bytes, compresslevel: int = 9) -> None` Writes the provided data to a new gzip file at `output_path` with the specified compression level. - **Parameters:** - `output_path` (str): Path where the new gzip file will be written. - `data` (bytes): Data to be written to the gzip file. - `compresslevel` (int): Compression level, an integer from 0 to 9. Default is 9. - **Returns:** - None # Constraints - You may assume that the input files exist and the paths provided are valid. - The compression and decompression operations should handle potential errors gracefully, providing meaningful error messages if the operations fail. # Example Usage ```python # Compress a file compress_file(\'example.txt\', \'example.txt.gz\', compresslevel=5) # Decompress a file decompress_file(\'example.txt.gz\', \'example_decompressed.txt\') # Read contents of a gzip file contents = read_gzip_file(\'example.txt.gz\') print(contents) # Write data to a new gzip file write_gzip_file(\'output.txt.gz\', b\\"Sample data\\", compresslevel=7) ``` # Note - Focus on handling file operations effectively and using appropriate methods from the `gzip` module. - Ensure your functions are well-documented with appropriate docstrings.","solution":"import gzip import shutil def compress_file(input_path: str, output_path: str, compresslevel: int = 9) -> None: Compresses the file at `input_path` and writes the compressed data to `output_path`. Parameters: input_path (str): Path to the input file to be compressed. output_path (str): Path where the compressed file will be written. compresslevel (int): Compression level, an integer from 0 to 9. Default is 9. Returns: None with open(input_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) def decompress_file(input_path: str, output_path: str) -> None: Decompresses the gzipped file at `input_path` and writes the decompressed data to `output_path`. Parameters: input_path (str): Path to the gzipped file to be decompressed. output_path (str): Path where the decompressed file will be written. Returns: None with gzip.open(input_path, \'rb\') as f_in: with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def read_gzip_file(input_path: str) -> bytes: Reads the contents of the gzipped file at `input_path`. Parameters: input_path (str): Path to the gzipped file to be read. Returns: bytes: The contents of the gzipped file. with gzip.open(input_path, \'rb\') as f: return f.read() def write_gzip_file(output_path: str, data: bytes, compresslevel: int = 9) -> None: Writes the provided data to a new gzip file at `output_path` with the specified compression level. Parameters: output_path (str): Path where the new gzip file will be written. data (bytes): Data to be written to the gzip file. compresslevel (int): Compression level, an integer from 0 to 9. Default is 9. Returns: None with gzip.open(output_path, \'wb\', compresslevel=compresslevel) as f: f.write(data)"},{"question":"# Coding Assessment: Optimizing scikit-learn Model Performance Objective Your task is to demonstrate your understanding of computational performance optimization techniques in scikit-learn. You will implement a class that trains and evaluates various scikit-learn models on large datasets while ensuring that latency and memory usage are minimized. Description Implement a Python class named `ModelOptimizer` with the following methods: 1. `__init__(self, model, n_features, use_sparse)`: - Initializes the class with a scikit-learn model, the number of features `n_features`, and a boolean `use_sparse` indicating whether to use sparse data representation. 2. `generate_data(self, n_samples)`: - Generates a random dataset with `n_samples` number of samples and `n_features` number of features. - Use sparse representation if `use_sparse` is `True`. 3. `train_model(self, X_train, y_train)`: - Trains the provided model on `X_train` and `y_train`. 4. `predict_bulk(self, X_test)`: - Makes bulk predictions on `X_test` and returns both the latency and throughput of the predictions. 5. `optimize_and_measure(self, n_samples)`: - Generates the data. - Trains the model. - Measures the prediction latency and throughput. - Should return a dictionary containing the bulk prediction latency and throughput. Constraints - Ensure that you use suitable techniques for performance optimization as described in the provided documentation. - Consider the impact of the number of features, input data representation, and model complexity. - Use appropriate linear algebra libraries if applicable. - The model should be one of the ensemble models (e.g., `RandomForestClassifier`). Example Usage ```python from sklearn.ensemble import RandomForestClassifier # Initialize the ModelOptimizer with a Random Forest model, 100 features, and sparse input data. optimizer = ModelOptimizer(model=RandomForestClassifier(n_estimators=10), n_features=100, use_sparse=True) # Measure performance results = optimizer.optimize_and_measure(n_samples=10000) print(results) ``` Expected Output The output should be a dictionary containing: - `bulk_prediction_latency`: the time taken to make bulk predictions. - `prediction_throughput`: the number of predictions per second. Performance Requirements - Bulk prediction latency should be minimized. - Prediction throughput should be maximized.","solution":"import numpy as np import scipy.sparse as sp from sklearn.model_selection import train_test_split from time import time class ModelOptimizer: def __init__(self, model, n_features, use_sparse): self.model = model self.n_features = n_features self.use_sparse = use_sparse def generate_data(self, n_samples): X = np.random.rand(n_samples, self.n_features) y = np.random.randint(2, size=n_samples) if self.use_sparse: X = sp.csr_matrix(X) return X, y def train_model(self, X_train, y_train): self.model.fit(X_train, y_train) def predict_bulk(self, X_test): start_time = time() predictions = self.model.predict(X_test) end_time = time() bulk_prediction_latency = end_time - start_time n_samples = X_test.shape[0] prediction_throughput = n_samples / bulk_prediction_latency return bulk_prediction_latency, prediction_throughput def optimize_and_measure(self, n_samples): X, y = self.generate_data(n_samples) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) self.train_model(X_train, y_train) bulk_prediction_latency, prediction_throughput = self.predict_bulk(X_test) return { \'bulk_prediction_latency\': bulk_prediction_latency, \'prediction_throughput\': prediction_throughput }"},{"question":"**Distributed Training Initialization with PyTorch\'s Rendezvous** In distributed training, ensuring all worker nodes can discover each other and synchronize their states properly is crucial. In PyTorch, this can be achieved using the **rendezvous** mechanism. **Task**: You are required to implement a function to set up a distributed training environment using the `DynamicRendezvousHandler`. 1. **Initialization Function**: Implement the function `initialize_distributed_training` which takes: - `rank`: an integer representing the current process rank (unique identifier for a worker). - `size`: total number of processes in the distributed training job. - `backend`: a string specifying the backend to use (e.g., `\\"etcd\\"`). - `store_endpoint`: address endpoint for the store (e.g., `\\"localhost:2379\\"`). The function should: - Use the dynamic rendezvous API to initialize a rendezvous handler with the given backend and store endpoint. - Return a tuple containing the store object and the process rank. 2. **Requirements**: - Utilize `DynamicRendezvousHandler.from_backend` for creating the rendezvous handler. - Use appropriate exception handling to manage failures in the rendezvous process. - Ensure the function is efficient and handles the rendezvous process correctly. Example Usage: ```python rank = 0 size = 4 backend = \\"etcd\\" store_endpoint = \\"localhost:2379\\" store, process_rank = initialize_distributed_training(rank, size, backend, store_endpoint) ``` **Constraints**: - Ensure the function handles timeouts and connection errors gracefully by retrying the rendezvous process up to 3 times before failing. **Input Format**: - An integer `rank`, representing the process rank. - An integer `size`, the total number of processes. - A string `backend`, specifying the backend type. - A string `store_endpoint`, the address for the store. **Output Format**: - A tuple containing the `store` object and the `process_rank`. **Hints**: - Refer to `torch.distributed.elastic.rendezvous.dynamic_rendezvous` for the dynamic rendezvous handler. - Consider using `RendezvousTimeoutError` and `RendezvousConnectionError` to handle retries. **Implementation**: Implement the `initialize_distributed_training` function within a Python script or Jupyter Notebook cell.","solution":"from torch.distributed.elastic.rendezvous.dynamic_rendezvous import DynamicRendezvousHandler from torch.distributed.elastic.rendezvous import RendezvousTimeoutError, RendezvousConnectionError def initialize_distributed_training(rank, size, backend, store_endpoint): Initializes the distributed training process using a rendezvous handler. Args: rank (int): The rank of the current process. size (int): Total number of processes. backend (str): The backend to use (e.g., \\"etcd\\"). store_endpoint (str): The endpoint for the store. Returns: tuple: A tuple containing the store object and the process rank. retry_attempts = 3 for attempt in range(retry_attempts): try: # Create a DynamicRendezvousHandler from backend and store endpoint handler = DynamicRendezvousHandler.from_backend(backend, store_endpoint) store, process_rank, _ = handler.next_rendezvous() return store, process_rank except (RendezvousTimeoutError, RendezvousConnectionError) as e: if attempt == retry_attempts - 1: raise e # Raise the exception if all retry attempts are exhausted # Retry on timeout and connection error # Example usage # rank = 0 # size = 4 # backend = \\"etcd\\" # store_endpoint = \\"localhost:2379\\" # store, process_rank = initialize_distributed_training(rank, size, backend, store_endpoint)"},{"question":"# Question: Python Introspection and Code Execution Analysis Using the functions provided in the documentation (PyEval_GetBuiltins, PyEval_GetLocals, PyEval_GetGlobals, PyEval_GetFrame, PyFrame_GetBack, PyFrame_GetCode, PyFrame_GetLineNumber, PyEval_GetFuncName, PyEval_GetFuncDesc), create a Python function `get_execution_info` that gathers and returns detailed information about the current execution state. Function Signature ```python def get_execution_info() -> dict: pass ``` Requirements 1. The function should return a dictionary containing: - **builtins**: A dictionary of the builtins in the current execution frame. - **locals**: A dictionary of local variables in the current execution frame. - **globals**: A dictionary of global variables in the current execution frame. - **current_frame**: A reference to the current frame. - **outer_frame**: A reference to the next outer frame of the current frame, or `None` if there\'s none. - **current_code**: The code object associated with the current frame. - **current_line_number**: The line number currently being executed in the current frame. - **current_function_name**: The name of the current function being executed. - **current_function_description**: A description of the current function being executed. 2. Each key in the dictionary should contain appropriate values retrieved using the functions from the provided documentation. Constraints - Assume Python 3.10 or later for function compatibility. - The function should be resilient and handle edge cases, such as when certain frames or functions may not exist. # Example ```python def example_usage(): def nested_function(): info = get_execution_info() return info return nested_function() execution_info = example_usage() for key, value in execution_info.items(): print(f\\"{key}: {value}\\") ``` The example above should output detailed information about the current execution state when nested_function is called. Notes - Use the functions from the provided documentation to implement the required functionality. - Ensure the solution is well-documented and includes comments explaining each step for clarity.","solution":"import sys import builtins def get_execution_info() -> dict: Gather and return detailed information about the current execution state. frame = sys._getframe(1) # Get the calling frame execution_info = { \\"builtins\\": frame.f_builtins, \\"locals\\": frame.f_locals, \\"globals\\": frame.f_globals, \\"current_frame\\": frame, \\"outer_frame\\": frame.f_back, \\"current_code\\": frame.f_code, \\"current_line_number\\": frame.f_lineno, \\"current_function_name\\": frame.f_code.co_name, \\"current_function_description\\": frame.f_code.co_varnames } return execution_info"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `rugplot` functionality and the broader capabilities of the Seaborn library to create informative visualizations. **Problem Statement:** 1. Load the \\"tips\\" dataset using Seaborn and create a joint plot (scatter plot) with \\"total_bill\\" on the x-axis and \\"tip\\" on the y-axis. 2. Add rug plots to both axes of the joint plot. 3. Use hue mapping to differentiate between \\"lunch\\" and \\"dinner\\" time. 4. Increase the height of the rug lines by 50%. 5. Overlay a KDE plot on the scatter plot for better density visualization. 6. Save the resulting plot as a PNG file named \\"tips_joint_plot.png\\". **Input:** No specific input is provided; you need to use the built-in \\"tips\\" dataset from Seaborn. **Expected Output:** A PNG file named \\"tips_joint_plot.png\\" which contains a joint plot visualizing: - Scatter plot of \\"total_bill\\" vs. \\"tip\\". - Rug plots on both axes. - Hue mapping for \\"time\\" (either \\"Lunch\\" or \\"Dinner\\"). - Rug lines increased in height by 50%. - Overlayed KDE plot. **Implementation Requirements:** 1. Use Seaborn and Matplotlib for plotting. 2. Make sure the plot is well-labeled and visually clear. 3. Save the plot as a PNG file with high resolution. **Constraints:** - You can assume the dataset structure is fixed and does not contain missing values for the columns used. ```python # Sample implementation import seaborn as sns import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Create a joint plot with scatter plot g = sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add rug plots to both axes sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=g.ax_joint, height=0.02) # Increase the height of the rug lines by 50% for line in g.ax_joint.collections[-len(tips[\\"time\\"].unique()):]: line.set_linewidth(0.5) # Add KDE plot overlay sns.kdeplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=g.ax_joint, fill=True, cmap=\\"Blues\\", alpha=0.5) # Save the plot as a PNG file g.savefig(\\"tips_joint_plot.png\\", dpi=300) ``` **Note:** Ensure that you have installed the Seaborn and Matplotlib libraries in your Python environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_jointplot_with_rug_and_kde(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Create a joint plot with scatter plot and hue mapping for \\"time\\" g = sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add rug plots to both axes sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", ax=g.ax_marg_x, height=0.03) sns.rugplot(data=tips, y=\\"tip\\", hue=\\"time\\", ax=g.ax_marg_y, height=0.03) # Overlay KDE plot sns.kdeplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=g.ax_joint, fill=True, alpha=0.5) # Save the plot as a PNG file g.savefig(\\"tips_joint_plot.png\\", dpi=300)"},{"question":"**Objective**: This question is designed to test your understanding and ability to utilize scikit-learn\'s utility functions, particularly those for array validation, efficient linear algebra operations, and random sampling. **Problem Statement**: You are given a dataset `X` which is a 2D array and a parameter `k` which is a positive integer. Your task is to implement a function `randomized_svd_sample(X, k, random_state=None)` that performs the following operations: 1. **Validation**: - Ensure that `X` is a 2D array with finite values. 2. **Random Sampling**: - Randomly select `k` rows from `X` without replacement. Ensure the sampling process is reproducible using the `random_state` parameter. 3. **Compute SVD**: - Using the sampled rows, compute the k-truncated randomized Singular Value Decomposition (SVD). **Function Signature**: ```python def randomized_svd_sample(X, k, random_state=None): Parameters: X (np.ndarray): A 2D numpy array. k (int): Number of rows to sample and number of components for the SVD. random_state (int, RandomState instance, or None): Seed or RandomState for reproducibility. Returns: tuple: (U, S, VT) where U, S, and VT are the truncated SVD components. pass ``` **Constraints**: - `X` must be a 2D numpy array. - `k` must be a positive integer, less than or equal to the number of rows in `X`. - Handle exceptions gracefully and provide meaningful error messages. **Performance Requirements**: - The solution should be efficient in terms of both time and space complexity. **Example**: ```python import numpy as np # Example input X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) k = 2 random_state = 42 # Function call U, S, VT = randomized_svd_sample(X, k, random_state) # Expected Output (the actual output would depend on the random sampling) print(\\"U:\\", U) print(\\"S:\\", S) print(\\"VT:\\", VT) ``` # Hints: - Use `check_array` to validate the input array. - Use `sample_without_replacement` to perform the random sampling. - Use `randomized_svd` to compute the SVD on the sampled rows. - Use `check_random_state` to handle the `random_state` parameter.","solution":"import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd from sklearn.utils.random import sample_without_replacement def randomized_svd_sample(X, k, random_state=None): Parameters: X (np.ndarray): A 2D numpy array. k (int): Number of rows to sample and number of components for the SVD. random_state (int, RandomState instance, or None): Seed or RandomState for reproducibility. Returns: tuple: (U, S, VT) where U, S, and VT are the truncated SVD components. # Validation X = check_array(X, ensure_2d=True, allow_nd=False, force_all_finite=True) # Check that k is a valid integer and within bounds if not isinstance(k, int) or k <= 0 or k > X.shape[0]: raise ValueError(\\"k must be a positive integer less than or equal to the number of rows in X.\\") # Set random state random_state = check_random_state(random_state) # Randomly sample k rows from X without replacement sampled_indices = sample_without_replacement(n_population=X.shape[0], n_samples=k, random_state=random_state) X_sampled = X[sampled_indices] # Compute SVD U, S, VT = randomized_svd(X_sampled, n_components=k, random_state=random_state) return U, S, VT"},{"question":"# Question: User and Group Information Processor You are tasked with creating a Python script that processes user and group information on a Unix system. Specifically, your script should fetch details about all users and their groups, and generate a detailed report. # Requirements: 1. **Function Name:** `generate_user_group_report` 2. **Input:** None 3. **Output:** A dictionary where keys are usernames, and values are lists of groups to which the user belongs. Steps: 1. **Fetch Users:** Utilize the `pwd` module to retrieve all user entries from the password database. 2. **Fetch Groups:** Utilize the `grp` module to retrieve all group entries from the group database. 3. **Map Groups to Users:** Collect all groups for each user. 4. **Return Format:** The function should return a dictionary with usernames as keys and a list of groups as values. # Constraints: - Use the `pwd` and `grp` modules to interact with the user and group databases. - Make sure your code handles errors gracefully, such as entries not found. # Example Output: ```python { \'root\': [\'root\'], \'daemon\': [\'daemon\'], \'bin\': [\'bin\'], \'user1\': [\'group1\', \'group2\'], \'user2\': [\'group1\'], # additional users as per the system } ``` # Notes: - The output will vary depending on the user and group setup of the Unix system where the script is executed. - You do not need to handle input errors or validation for this question. ```python import pwd import grp def generate_user_group_report(): # Complete the implementation based on the requirements. pass # Example usage if __name__ == \\"__main__\\": report = generate_user_group_report() for user, groups in report.items(): print(f\\"User: {user}, Groups: {groups}\\") ``` # Tips: - Refer to the documentation of the `pwd` and `grp` modules to understand the data structures they return. - Ensure efficient and readable code by using appropriate Python constructs and error handling techniques.","solution":"import pwd import grp def generate_user_group_report(): user_group_report = {} # Fetch all users from the password database users = pwd.getpwall() # Fetch all groups from the group database all_groups = grp.getgrall() for user in users: username = user.pw_name user_groups = [] for group in all_groups: if username in group.gr_mem or user.pw_gid == group.gr_gid: user_groups.append(group.gr_name) user_group_report[username] = user_groups return user_group_report"},{"question":"# Advanced Python: Signal Handling and Threads Context: You are implementing a server application which needs to gracefully shut down on receiving specific signals such as `SIGINT` (interrupt from the keyboard, usually CTRL+C) or `SIGTERM` (termination signal). During the shutdown process, you need to ensure that all currently running threads finish their execution and properly clean up any resources they were using. Task: Implement a Python program that: 1. Handles the `SIGINT` and `SIGTERM` signals to initiate a controlled shutdown process. 2. Spawns multiple threads that perform some long-running tasks (simulated with `sleep`). 3. Ensures that when a signal is received, it waits for all threads to complete before exiting. Requirements: 1. Define custom handlers for `SIGINT` and `SIGTERM` which set flags indicating that a shutdown is requested. 2. Use synchronization primitives to ensure main thread waits for the worker threads to finish. 3. Use the `signal` module\'s functions appropriately. Constraints: - Assume the maximum number of threads is 10. - Each thread sleeps for a random duration between 1 to 10 seconds. - The shutdown signal should not cause abrupt termination but should allow threads to complete their current work. Input/Output: - There is no standard input or output. Your main task is to ensure the program terminates correctly when a signal is received. Implementation Outline: 1. Import necessary modules (`threading`, `signal`, `time`, `random`). 2. Define signal handlers for `SIGINT` and `SIGTERM`. 3. Create a function to simulate long-running task by sleeping for a random duration. 4. Spawn worker threads to perform the tasks. 5. Implement the main logic to manage the threads and signal handling. # Example ```python import threading import signal import time import random # Flag to indicate shutdown request shutdown_requested = False def signal_handler(signum, frame): global shutdown_requested print(f\\"Signal {signum} received, initiating shutdown.\\") shutdown_requested = True def worker_thread(thread_id): while not shutdown_requested: sleep_time = random.randint(1, 10) print(f\\"Thread-{thread_id} working for {sleep_time} seconds.\\") time.sleep(sleep_time) print(f\\"Thread-{thread_id} finished.\\") def main(): global shutdown_requested # Setting the signal handlers for SIGINT and SIGTERM signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGTERM, signal_handler) threads = [] num_threads = 10 # Starting worker threads for i in range(num_threads): thread = threading.Thread(target=worker_thread, args=(i,)) thread.start() threads.append(thread) # Wait for all threads to complete for thread in threads: thread.join() print(\\"All threads have finished. Exiting main program.\\") if __name__ == \\"__main__\\": main() ```","solution":"import threading import signal import time import random # Flag to indicate shutdown request shutdown_requested = False def signal_handler(signum, frame): global shutdown_requested print(f\\"Signal {signum} received, initiating shutdown.\\") shutdown_requested = True def worker_thread(thread_id): while not shutdown_requested: sleep_time = random.randint(1, 10) print(f\\"Thread-{thread_id} working for {sleep_time} seconds.\\") time.sleep(sleep_time) print(f\\"Thread-{thread_id} finished.\\") def main(): global shutdown_requested # Setting the signal handlers for SIGINT and SIGTERM signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGTERM, signal_handler) threads = [] num_threads = 10 # Starting worker threads for i in range(num_threads): thread = threading.Thread(target=worker_thread, args=(i,)) thread.start() threads.append(thread) # Wait for all threads to complete for thread in threads: thread.join() print(\\"All threads have finished. Exiting main program.\\") if __name__ == \\"__main__\\": main()"},{"question":"You are required to implement and demonstrate an application that handles concurrent tasks using both threading and multiprocessing in Python. The goal is to create a hybrid solution that leverages both methods to perform a CPU-bound task and an IO-bound task efficiently. Task Description 1. **CPU-bound Task**: Implement a function `compute_prime_factors(n)` which computes and returns the prime factors of a given integer `n`. 2. **IO-bound Task**: Implement a function `fetch_url_data(url)` which fetches and returns the data from the given URL using the `requests` library. Next, create a main function that performs the following: 3. Generates a list of random integers and a list of URLs (these lists can be hardcoded or generated within the function). 4. Uses `ThreadPoolExecutor` to execute the `fetch_url_data` function concurrently on the list of URLs. 5. Uses `ProcessPoolExecutor` to execute the `compute_prime_factors` function concurrently on the list of integers. 6. Ensures proper synchronization where necessary to safely combine and print the results from both tasks. Expected Input and Output - **Input**: - A list of random integers. - A list of URLs. - **Output**: - A combined and synchronized output of results from both tasks. Constraints - Assume each list contains at least 5 elements. - Properly handle exceptions such as failed URL fetches or invalid integers for prime factorization. Performance Requirements - Ensure that the concurrent execution is optimized for both CPU-bound and IO-bound tasks. - Utilize appropriate synchronization mechanisms to avoid race conditions. Example ```python import random from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor import requests def compute_prime_factors(n): # Implementation here def fetch_url_data(url): # Implementation here def main(): random_integers = [1234567, 7654321, 9876543, 3456789, 4567891] urls = [\'https://example.com\', \'https://example.org\', \'https://example.net\', \'https://example.edu\', \'https://example.biz\'] # Implement ThreadPoolExecutor for fetching URLs # Implement ProcessPoolExecutor for computing prime factors # Ensure proper synchronization and combined output if __name__ == \\"__main__\\": main() ``` Notes - Use the `concurrent.futures` module for ThreadPoolExecutor and ProcessPoolExecutor. - You may use Python standard library modules and the `requests` library for fetching URLs. - Pay attention to handling synchronization and potential exceptions.","solution":"import random from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor import requests def compute_prime_factors(n): Computes and returns the prime factors of the given integer `n`. i = 2 factors = [] while i * i <= n: if n % i: i += 1 else: n //= i factors.append(i) if n > 1: factors.append(n) return factors def fetch_url_data(url): Fetches and returns the data from the given URL. try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException as e: return str(e) def main(): random_integers = [1234567, 7654321, 9876543, 3456789, 4567891] urls = [\'https://example.com\', \'https://example.org\', \'https://example.net\', \'https://example.edu\', \'https://example.biz\'] # Use ThreadPoolExecutor for fetching URLs with ThreadPoolExecutor() as thread_executor: url_futures = [thread_executor.submit(fetch_url_data, url) for url in urls] # Collect URL data results url_results = [] for future in url_futures: url_results.append(future.result()) # Use ProcessPoolExecutor for computing prime factors with ProcessPoolExecutor() as process_executor: factor_futures = [process_executor.submit(compute_prime_factors, n) for n in random_integers] # Collect prime factors results factor_results = [] for future in factor_futures: factor_results.append(future.result()) # Synchronize and print the results print(\\"URL Fetch Results:\\") for result in url_results: print(result[:100] + \'...\') # Print the first 100 characters for brevity print(\\"nPrime Factors Computation Results:\\") for number, factors in zip(random_integers, factor_results): print(f\\"{number}: {factors}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Python Coding Assessment** # Question: AIFF File Manipulation and Analysis You are provided with a set of audio files in AIFF format, and your task is to implement a function that extracts specific audio data and performs simple audio analysis. Follow the steps below: 1. **Open an AIFF file**: Write a function that opens an AIFF file and retrieves the audio parameters. 2. **Read Audio Data**: Read a specified number of frames from the audio file. 3. **Perform Analysis**: Compute the average amplitude of the audio data for each channel. 4. **Output Results**: Output the audio parameters and the computed average amplitudes. # Requirements: - Implement the function `analyze_aiff(file_path: str, num_frames: int) -> None`. - `file_path` (str): The path to the AIFF file to be analyzed. - `num_frames` (int): The number of frames to read from the file for analysis. # Function Details: 1. **Open the file**: ```python import aifc def open_aiff(file_path): return aifc.open(file_path, \'rb\') ``` 2. **Retrieve and Print Audio Parameters**: - Number of Channels - Sample Width - Frame Rate - Compression Type - Number of Frames ```python def get_audio_params(aiff_file): nchannels = aiff_file.getnchannels() sampwidth = aiff_file.getsampwidth() framerate = aiff_file.getframerate() comptype = aiff_file.getcomptype() nframes = aiff_file.getnframes() print(f\'Channels: {nchannels}\') print(f\'Sample Width: {sampwidth} bytes\') print(f\'Frame Rate: {framerate} frames/second\') print(f\'Compression Type: {comptype}\') print(f\'Total Frames: {nframes}\') return nchannels, sampwidth, framerate, comptype, nframes ``` 3. **Read and Analyze Audio Data**: - Read the specified number of frames. - Compute the average amplitude for each channel. ```python def read_frames(aiff_file, num_frames): return aiff_file.readframes(num_frames) def compute_average_amplitude(frames, nchannels, sampwidth): import struct sample_format = {1: \'B\', 2: \'h\', 4: \'i\'}[sampwidth] unpack_format = f\'{nchannels * num_frames}{sample_format}\' samples = struct.unpack(unpack_format, frames) avg_amplitudes = [0] * nchannels for i in range(num_frames): for channel in range(nchannels): avg_amplitudes[channel] += abs(samples[i * nchannels + channel]) avg_amplitudes = [amp / num_frames for amp in avg_amplitudes] return avg_amplitudes ``` 4. **Main function**: `analyze_aiff` ```python def analyze_aiff(file_path, num_frames): aiff_file = open_aiff(file_path) nchannels, sampwidth, framerate, comptype, nframes = get_audio_params(aiff_file) frames = read_frames(aiff_file, num_frames) avg_amplitudes = compute_average_amplitude(frames, nchannels, sampwidth) print(f\'Average Amplitude per Channel: {avg_amplitudes}\') aiff_file.close() ``` # Constraints: - The AIFF file should be of uncompressed type (`b\'NONE\'`). - The `num_frames` should be less than or equal to the total number of frames in the file. - Handle exceptions for file opening and reading operations. # Example Usage: Assuming `sample.aiff` is a valid AIFF file in the current directory: ``` python analyze_aiff(\'sample.aiff\', 1024) ``` This function should print the audio parameters of `sample.aiff` and the average amplitude per channel for the first 1024 frames.","solution":"import aifc import struct def open_aiff(file_path): Opens an AIFF file and returns the file object. return aifc.open(file_path, \'rb\') def get_audio_params(aiff_file): Retrieves and prints the audio parameters from the AIFF file object. Returns a tuple of the parameters. nchannels = aiff_file.getnchannels() sampwidth = aiff_file.getsampwidth() framerate = aiff_file.getframerate() comptype = aiff_file.getcomptype() nframes = aiff_file.getnframes() print(f\'Channels: {nchannels}\') print(f\'Sample Width: {sampwidth} bytes\') print(f\'Frame Rate: {framerate} frames/second\') print(f\'Compression Type: {comptype}\') print(f\'Total Frames: {nframes}\') return nchannels, sampwidth, framerate, comptype, nframes def read_frames(aiff_file, num_frames): Reads a specified number of frames from the AIFF file object. return aiff_file.readframes(num_frames) def compute_average_amplitude(frames, nchannels, sampwidth, num_frames): Computes the average amplitude of audio frames for each channel. Returns a list of average amplitudes for each channel. sample_format = {1: \'b\', 2: \'h\', 3: \'i\', 4: \'i\'}.get(sampwidth) if sampwidth == 3: # special case for 24-bit audio frames = b\'\'.join(f + b\'x00\' for f in frames) unpack_format = f\'<{num_frames * nchannels}{sample_format}\' samples = struct.unpack(unpack_format, frames) avg_amplitudes = [0] * nchannels for i in range(num_frames): for channel in range(nchannels): avg_amplitudes[channel] += abs(samples[i * nchannels + channel]) avg_amplitudes = [amp / num_frames for amp in avg_amplitudes] return avg_amplitudes def analyze_aiff(file_path, num_frames): Opens an AIFF file and performs analysis to print audio parameters and average amplitude per channel for the specified number of frames. aiff_file = open_aiff(file_path) try: nchannels, sampwidth, framerate, comptype, nframes = get_audio_params(aiff_file) if comptype != \'NONE\': raise ValueError(\\"AIFF file is compressed. Only uncompressed AIFF files are supported.\\") frames = read_frames(aiff_file, num_frames) avg_amplitudes = compute_average_amplitude(frames, nchannels, sampwidth, num_frames) print(f\'Average Amplitude per Channel: {avg_amplitudes}\') finally: aiff_file.close()"},{"question":"# Advanced Coding Assessment Question You are required to implement an asynchronous TCP server and client using the `asyncio` streams API. Your task is to create a server that can handle multiple client connections concurrently, echoing back any data received from the clients, but with a transformation: the server should convert any alphabetic characters in the message to uppercase before echoing it back. Requirements 1. **Server Implementation**: - Create a function `start_echo_server` that starts an asynchronous server listening on a specified host and port. - The server should accept multiple concurrent client connections. - For each received message from a client, convert all alphabetic characters to uppercase and send it back to the client. 2. **Client Implementation**: - Create a function `tcp_echo_client` that establishes a connection to the server, sends a message, and prints the transformed response. # Input and Output Formats - **Server Function**: - Input: `start_echo_server(host: str, port: int) -> None` - Output: The function should start the server and keep it running indefinitely. - **Client Function**: - Input: `tcp_echo_client(host: str, port: int, message: str) -> None` - Output: The function should print the transformed message received from the server. # Constraints - Use the `asyncio` module for all asynchronous operations. - Ensure proper cleanup of connections to avoid resource leaks. - The server should handle unexpected client disconnections gracefully. # Example ```python import asyncio async def start_echo_server(host, port): async def handle_client(reader, writer): while True: # Read data from client data = await reader.read(100) if not data: break message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") # Transform the message transformed_message = message.upper() # Send transformed message back to client writer.write(transformed_message.encode()) await writer.drain() writer.close() await writer.wait_closed() server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def tcp_echo_client(host, port, message): reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') writer.close() await writer.wait_closed() async def main(): # Start server server_task = asyncio.create_task(start_echo_server(\'127.0.0.1\', 8888)) await asyncio.sleep(1) # Allow server to start # Start client await tcp_echo_client(\'127.0.0.1\', 8888, \'Hello World!\') server_task.cancel() asyncio.run(main()) ``` Implement the two functions, `start_echo_server` and `tcp_echo_client`, as described above and test them to ensure they fulfill the requirements.","solution":"import asyncio async def start_echo_server(host: str, port: int) -> None: Starts an asynchronous TCP echo server that converts all alphabetic characters in the received messages to uppercase before echoing them back to the client. :param host: The host address to listen on. :param port: The port to listen on. async def handle_client(reader, writer): while True: data = await reader.read(100) if not data: break message = data.decode() transformed_message = message.upper() writer.write(transformed_message.encode()) await writer.drain() writer.close() await writer.wait_closed() server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def tcp_echo_client(host: str, port: int, message: str) -> None: Establishes a connection to the server, sends a message, and prints the transformed response. :param host: The server host. :param port: The server port. :param message: The message to send to the server. reader, writer = await asyncio.open_connection(host, port) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() # Run server and client for demonstration purposes here if needed # (Uncomment for local testing) # async def main(): # server_task = asyncio.create_task(start_echo_server(\'127.0.0.1\', 8888)) # await asyncio.sleep(1) # Permit server startup time # await tcp_echo_client(\'127.0.0.1\', 8888, \'Hello World!\') # server_task.cancel() # asyncio.run(main())"},{"question":"**Objective**: Demonstrate proficiency in using `importlib.metadata` to retrieve and manipulate metadata and files for installed packages. **Task**: You are required to write a Python function `analyze_package(package_name)` that performs the following operations: 1. Retrieve and return the version of the package. 2. Retrieve and return a list of all entry point names in the `console_scripts` group. 3. Retrieve and return the metadata for the package as a dictionary. 4. Retrieve and return a list of files in the package along with their sizes and hashes. 5. Retrieve and return a list of package requirements. **Function Signature**: ```python def analyze_package(package_name: str) -> dict: pass ``` **Input**: - `package_name` (str): The name of the package to be analyzed. **Output**: - A dictionary with the following structure: ```python { \\"version\\": str, \\"console_scripts\\": list, \\"metadata\\": dict, \\"files\\": list of dicts with keys {\'filename\': str, \'size\': int, \'hash\': str}, \\"requirements\\": list } ``` **Constraints**: - The function should handle the case where the package is not found and return an appropriate message or empty structures. - For performance reasons, avoid any unnecessary I/O operations and efficiently parse the metadata. **Example**: ```python result = analyze_package(\'wheel\') print(result) # Output should be a dictionary with the analyzed data for the \'wheel\' package ``` **Notes**: - Use the functionalities from `importlib.metadata` described in the provided documentation to solve the task. - Ensure the function is efficient and handles exceptions where necessary. - Write clean and clear code with appropriate comments.","solution":"from importlib.metadata import version, entry_points, metadata, requires, files import hashlib def analyze_package(package_name: str) -> dict: result = { \\"version\\": \\"\\", \\"console_scripts\\": [], \\"metadata\\": {}, \\"files\\": [], \\"requirements\\": [] } try: # Get package version result[\\"version\\"] = version(package_name) # Get entry points for the \'console_scripts\' group ep = entry_points() result[\\"console_scripts\\"] = [entry.name for entry in ep.get(\'console_scripts\', []) if entry.group == \'console_scripts\'] # Get package metadata as a dictionary result[\\"metadata\\"] = dict(metadata(package_name)) # Get package file list with sizes and hashes result[\\"files\\"] = [] for file in files(package_name): with open(file.locate(), \'rb\') as f: content = f.read() file_hash = hashlib.sha256(content).hexdigest() result[\\"files\\"].append({ \\"filename\\": str(file), \\"size\\": file.locate().stat().st_size, \\"hash\\": file_hash }) # Get package requirements result[\\"requirements\\"] = requires(package_name) or [] except Exception as e: result = { \\"version\\": \\"Package not found\\", \\"console_scripts\\": [], \\"metadata\\": {}, \\"files\\": [], \\"requirements\\": [] } return result"},{"question":"# Clustering and Evaluation with scikit-learn You are provided with a dataset of 2D points, and your task is to implement a clustering algorithm using scikit-learn. Your implementation should demonstrate the following steps: 1. **Data Preprocessing:** - Load the given dataset. - Standardize the features to have zero mean and unit variance. 2. **Clustering:** - Implement the K-Means clustering algorithm. - Fit the algorithm to the standardized data to find 3 clusters. 3. **Evaluation:** - Evaluate the performance of your clustering using the Silhouette Coefficient. - Provide the Silhouette score of the resulting clustering. Input Format - A CSV file named `data.csv` with two columns `x` and `y` representing the coordinates of the 2D points. Output Format - The Silhouette Coefficient of the clustering, rounded to 4 decimal places. Constraints - Use `sklearn.preprocessing.StandardScaler` for standardizing the data. - Use `sklearn.cluster.KMeans` for the clustering. - Use `sklearn.metrics.silhouette_score` for evaluation. - The number of clusters `k` should be set to 3. Example If the dataset `data.csv` contains the following points: ``` x,y 1.0,2.0 1.5,1.8 5.0,8.0 8.0,8.0 1.0,0.6 9.0,11.0 8.0,2.0 10.0,2.0 9.0,3.0 ``` The expected output might be: ``` 0.6015 ``` Your Code ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score # Load the dataset df = pd.read_csv(\'data.csv\') # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(df) # Implement and fit the K-Means algorithm kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X_scaled) # Predict the cluster labels labels = kmeans.labels_ # Evaluate the performance using Silhouette Coefficient sil_score = silhouette_score(X_scaled, labels) # Print the Silhouette score print(f\'{sil_score:.4f}\') ```","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def load_and_process_data(file_path): Load and preprocess data from a CSV file. Parameters: - file_path (str): The path to the CSV file containing the dataset. Returns: - DataFrame: A DataFrame with the standardized data. # Load the dataset df = pd.read_csv(file_path) # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(df) return X_scaled def perform_clustering(X, n_clusters=3, random_state=42): Perform K-Means clustering on the given data. Parameters: - X (ndarray): Standardized data for clustering. - n_clusters (int): The number of clusters for K-Means (default is 3). - random_state (int): Random state for reproducibility (default is 42). Returns: - The clustering labels. # Implement and fit the K-Means algorithm kmeans = KMeans(n_clusters=n_clusters, random_state=random_state) kmeans.fit(X) # Return the cluster labels return kmeans.labels_ def evaluate_clustering(X, labels): Evaluate the clustering performance using Silhouette Coefficient. Parameters: - X (ndarray): The standardized data. - labels (ndarray): The cluster labels. Returns: - float: The Silhouette score. # Calculate the Silhouette score sil_score = silhouette_score(X, labels) return sil_score if __name__ == \\"__main__\\": file_path = \'data.csv\' # Load and process data X_scaled = load_and_process_data(file_path) # Perform clustering labels = perform_clustering(X_scaled) # Evaluate clustering performance sil_score = evaluate_clustering(X_scaled, labels) # Print the Silhouette score print(f\'{sil_score:.4f}\')"},{"question":"**Coding Assessment Question**: # Objective You are required to use the `seaborn` library to create complex plots that demonstrate your understanding of its `relplot` function, data preprocessing, and customization capabilities. # Dataset You will use the built-in `seaborn` dataset `diamonds`. To load this dataset, use: ```python import seaborn as sns diamonds = sns.load_dataset(\\"diamonds\\") ``` # Tasks 1. **Scatter Plot**: Create a scatter plot to visualize the relationship between `carat` and `price`. Color the points based on the `cut` of the diamond and use different styles for each unique `clarity`. 2. **Facet Grid**: Modify the previous plot to create a facet grid where each column corresponds to a unique value of the `color` variable. 3. **Line Plot**: Create a line plot to show the mean `price` of diamonds over `carat`, colored by `cut`. Use `cut` also to style the lines. 4. **Customization**: Customize the facet grid plot from task 2: - Add appropriate titles for each subplot indicating the `color` of diamonds. - Label the x-axis as \\"Carat\\" and the y-axis as \\"Price\\". - Add a horizontal line at y=0 (using `plt.axhline`) with grey color and dashes. 5. **Wide-form Data**: Convert the `diamonds` dataset into a wide-form with `cut` as columns and `carat` as the index, taking the mean price for each combination. Display a line plot using this data. # Constraints - You should complete each plot using the `sns.relplot` function. - Ensure that the visualizations are clear and not overly complicated. # Submission Submit your Python code that performs all the tasks mentioned above. Ensure your code is well-commented and follows good coding practices. **Input Format:** N/A **Output Format:** Plots created and displayed using `matplotlib` and `seaborn`. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Scatter plot sns.relplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'cut\', style=\'clarity\') plt.show() # Task 2: Facet grid scatter plot sns.relplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'cut\', style=\'clarity\', col=\'color\') plt.show() # Continue with tasks 3, 4, and 5... ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load dataset diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Scatter plot def scatter_plot(): plot = sns.relplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'cut\', style=\'clarity\') plt.show() return plot # Task 2: Facet grid scatter plot def facet_grid_scatter(): plot = sns.relplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'cut\', style=\'clarity\', col=\'color\') plt.show() return plot # Task 3: Line plot def line_plot(): diamonds_mean_price = diamonds.groupby([\'carat\', \'cut\'], as_index=False)[\'price\'].mean() plot = sns.relplot(data=diamonds_mean_price, x=\'carat\', y=\'price\', hue=\'cut\', kind=\'line\', style=\'cut\') plt.show() return plot # Task 4: Customized Facet grid plot def custom_facet_grid(): g = sns.relplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'cut\', style=\'clarity\', col=\'color\') g.set_axis_labels(\\"Carat\\", \\"Price\\") for ax, title in zip(g.axes.flat, g.col_names): ax.set_title(f\\"Color: {title}\\") ax.axhline(y=0, color=\'grey\', linestyle=\'--\') plt.show() return g # Task 5: Wide-form data line plot def wide_form_line_plot(): diamonds_wide = diamonds.pivot_table(values=\'price\', index=\'carat\', columns=\'cut\', aggfunc=\'mean\').reset_index() plot = diamonds_wide.plot(x=\'carat\', y=diamonds_wide.columns[1:], kind=\'line\') plt.xlabel(\\"Carat\\") plt.ylabel(\\"Price\\") plt.show() return plot"},{"question":"# Question: Advanced Scatter Plot Customization with seaborn.objects You have been provided with the `mpg` dataset from seaborn, which contains data about car fuel efficiency (`mpg`) and other attributes. Your task is to create a scatter plot that visualizes the relationship between the `horsepower` and `mpg` columns. You need to customize this plot by: 1. Coloring the points based on the `origin` column. 2. Adjusting the transparency of the points to 50%. 3. Using different markers for different `origin` values. 4. Adding some jitter to the points to improve the visualization of any overlapping points. Write a Python function `custom_scatter_plot` that generates this customized scatter plot using seaborn. Function Signature ```python def custom_scatter_plot(): pass ``` Expected Output The function should display a scatter plot with the following features: - Points correlate `horsepower` (x-axis) and `mpg` (y-axis). - Points are colored based on `origin`. - Points have 50% transparency. - Different markers are used for different `origin` values. - Jitter is applied to the points. You do not need to return any value; the function should only display the plot. Constraints and Requirements - Import necessary libraries within the function. - Use seaborn\'s `objects` module appropriately. - Ensure the plot is clear and informative. Example ```python def custom_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = (so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") .add(so.Dots(), so.Jitter(0.25)) .add(so.Dots(stroke=1), color=\\"origin\\", alpha=0.5, marker=\\"origin\\") .scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)]) ) # Show the plot p.show() # Call the function to display the plot custom_scatter_plot() ``` Note: Your plot should resemble the features described above when you run the `custom_scatter_plot` function.","solution":"def custom_scatter_plot(): import seaborn as sns import matplotlib.pyplot as plt from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot plt.figure(figsize=(10, 6)) sns.scatterplot( data=mpg, x=\'horsepower\', y=\'mpg\', hue=\'origin\', style=\'origin\', alpha=0.5, s=100, palette=\\"deep\\" ) plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Miles per Gallon (mpg)\\") plt.title(\\"Scatter Plot of Horsepower vs MPG\\") # Adding jitter # Scatter plot with jitter can be created using seaborn\'s stripplot with jitter parameter sns.stripplot( data=mpg, x=\'horsepower\', y=\'mpg\', hue=\'origin\', dodge=True, # Separate points by `hue` groups. jitter=0.25, palette=\\"deep\\", alpha=0.5, size=7, edgecolor=\'gray\' ) plt.grid(True) plt.show()"},{"question":"Objective: Assess your understanding of the `Timedelta` functionality within the pandas library. You will write several functions and manipulate `Timedelta` objects to solve given problems. Problem Statement: You need to implement a function called `analyze_time_deltas` that takes in a DataFrame consisting of events with start and end times. The function should perform the following tasks: 1. **Calculate Event Durations**: - Calculate the duration for each event and add a new column `Duration` to the DataFrame to store these durations. The duration should be a `Timedelta` object. 2. **Filter Long Events**: - Create a new DataFrame, `long_events`, to store only the events that lasted longer than a specified duration threshold (input as a parameter in seconds). 3. **Total Duration Summation**: - Calculate the total duration of all events and return it as a `Timedelta` object. 4. **Normalize Event Times**: - Normalize the start times of all events to start from a reference start date (\'2022-01-01\'). Modify the DataFrame such that the differences between start and end times remain the same after normalization. Input: - `df` (pandas DataFrame): A DataFrame with the columns `event_id`, `start_time`, and `end_time` (of type `datetime64`). - `duration_threshold` (int): An integer representing the duration threshold in seconds. Output: - `df` (pandas DataFrame): The modified DataFrame with an added `Duration` column, normalized `start_time`, and adjusted `end_time`. - `long_events` (pandas DataFrame): A new DataFrame containing events that lasted longer than the specified threshold. - `total_duration` (Timedelta): Total duration of all events as a `Timedelta` object. Example: ```python import pandas as pd data = { \'event_id\': [1, 2, 3], \'start_time\': [\'2022-01-01 08:00:00\', \'2022-01-01 09:00:00\', \'2022-01-01 10:00:00\'], \'end_time\': [\'2022-01-01 08:30:00\', \'2022-01-01 10:00:00\', \'2022-01-01 11:30:00\'] } # Convert to DataFrame df = pd.DataFrame(data) df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Set duration threshold (in seconds) duration_threshold = 3000 # 50 minutes # Call the function modified_df, long_events, total_duration = analyze_time_deltas(df, duration_threshold) # Output print(modified_df) print(long_events) print(total_duration) ``` Constraints: - You may assume that the input DataFrame will always have well-formatted datetimes. - The dates must be normalized to the nearest second. Write your function signature and implementation below: ```python def analyze_time_deltas(df: pd.DataFrame, duration_threshold: int): # Your implementation here pass ```","solution":"import pandas as pd def analyze_time_deltas(df: pd.DataFrame, duration_threshold: int): # Calculate the duration for each event df[\'Duration\'] = df[\'end_time\'] - df[\'start_time\'] # Filter long events long_events = df[df[\'Duration\'] > pd.Timedelta(seconds=duration_threshold)] # Calculate the total duration of all events total_duration = df[\'Duration\'].sum() # Normalize the start times to a reference start date reference_start = pd.Timestamp(\'2022-01-01\') df[\'start_time\'] = reference_start + (df[\'start_time\'] - df[\'start_time\'].min()) df[\'end_time\'] = df[\'start_time\'] + df[\'Duration\'] return df, long_events, total_duration"},{"question":"# Curses-Based To-Do List Application **Objective**: Implement a console-based to-do list application using the Python curses module. The application should allow the user to add, remove, and view to-do items. The interface should be intuitive and user-friendly, leveraging curses\' capabilities to handle text and user input efficiently. Function Implementation 1. **Function Name**: `main` 2. **Expected Input**: None (function will use curses functions to interact with the user) 3. **Expected Output**: None (outputs directly to the screen using curses) Features and Requirements: 1. **Initialization**: - Initialize the curses application using `curses.wrapper()`. - Ensure that the terminal is returned to its original state on exit. 2. **User Interface**: - Display the to-do list items in a window. - Provide options to add a new item, remove an item, and quit the application. - Highlight the currently selected option. 3. **Adding Items**: - When the user selects the option to add a new item, prompt the user to enter the item text. - Add the entered item to the to-do list. 4. **Removing Items**: - When the user selects the option to remove an item, prompt the user to select the item to be removed. - Remove the selected item from the list. 5. **Viewing Items**: - Display the current list of to-do items. - Allow scrolling if the number of items exceeds the available window size. 6. **Key Handling**: - Handle key inputs to navigate the options (e.g., up and down arrow keys). - Handle enter key to select an option. - Handle \'q\' key to quit the application. 7. **Error Handling**: - Ensure that the application does not crash on invalid inputs. - Ensure proper cleanup of the terminal state on unexpected errors. Example Usage: The application should display a window with the current to-do items and options to add or remove items. The user can navigate through options using arrow keys, add items by typing them in, and remove items by selecting them from the list. Constraints: - Use only the curses module for handling the console UI. - The application should gracefully handle terminal resize events. - Ensure the application performs efficiently even with a large number of to-do items. Notes: - You may use additional helper functions to organize your code. - Ensure the code is commented adequately to explain the logic. Hints: - Utilize the `wrapper(main)` function for initialization and cleanup. - Use `stdscr.addstr()` and `stdscr.refresh()` methods for displaying text. - Use `stdscr.getch()` for capturing user input. **Starter Code**: ```python import curses def main(stdscr): # Initialize curses settings (no echo, cbreak mode, keypad mode) curses.noecho() curses.cbreak() stdscr.keypad(True) # Your code here to implement the features if __name__ == \\"__main__\\": curses.wrapper(main) ``` Implement the `main` function as described to create a functional to-do list application.","solution":"import curses def main(stdscr): curses.noecho() curses.cbreak() stdscr.keypad(True) todos = [] current_option = 0 def print_menu(stdscr, selected): stdscr.clear() height, width = stdscr.getmaxyx() for idx, item in enumerate(todos): x = 2 y = 2 + idx if idx == selected: stdscr.attron(curses.color_pair(1)) stdscr.addstr(y, x, item) stdscr.attroff(curses.color_pair(1)) else: stdscr.addstr(y, x, item) stdscr.addstr(0, 0, \\"To-Do List Application\\") stdscr.addstr(1, 0, \\"Use arrow keys to navigate and Enter to select\\") stdscr.addstr(height-1, 0, \\"Press \'a\' to add, \'r\' to remove, \'q\' to quit\\") stdscr.refresh() curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) while True: height, width = stdscr.getmaxyx() print_menu(stdscr, current_option) key = stdscr.getch() if key == ord(\'q\'): break elif key == curses.KEY_UP: current_option = (current_option - 1) % len(todos) elif key == curses.KEY_DOWN: current_option = (current_option + 1) % len(todos) elif key == ord(\'a\'): stdscr.clear() stdscr.addstr(0, 0, \\"Enter new to-do item: \\") stdscr.refresh() curses.echo() new_item = stdscr.getstr(1, 0, width-1).decode(\'utf-8\') curses.noecho() if new_item: todos.append(new_item) elif key == ord(\'r\'): if todos: todos.pop(current_option) current_option = max(0, current_option - 1) elif key == curses.KEY_ENTER or key in [10, 13]: pass # we can expand this to handle enter key behaviour if needed if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"Customizing Plot Aesthetics with Seaborn Objective: Create a visualization that demonstrates your ability to control and customize plot aesthetics using the `seaborn` package. Task: 1. Write a function `custom_sinplot` that takes the following parameters: - `n`: Number of sine waves (default is 10). - `flip`: A factor to flip the sine waves (default is 1). - `style`: A seaborn style theme to apply. Options include \'darkgrid\', \'whitegrid\', \'dark\', \'white\', and \'ticks\'. - `context`: A seaborn context for scaling. Options include \'paper\', \'notebook\', \'talk\', and \'poster\'. - `despine`: A boolean value to determine whether to remove the top and right spines (default is `True`). - `palette`: A seaborn color palette to apply (default is `\'deep\'`). 2. The function should: - Set the seaborn theme and context based on the provided parameters. - Optional: Customize an additional parameter using a dictionary, such as changing the face color. - Generate a plot using `matplotlib` with `n` sine waves. - Apply the color palette to the plot. - Apply `despine` if set to `True`. - Display the plot. Example Usage and Output: ```python def custom_sinplot(n=10, flip=1, style=\'darkgrid\', context=\'notebook\', despine=True, palette=\'deep\'): import numpy as np import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=style, palette=palette) sns.set_context(context) def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) sinplot(n, flip) if despine: sns.despine() plt.show() # Example call custom_sinplot(n=15, flip=-1, style=\'whitegrid\', context=\'talk\', despine=True, palette=\'muted\') ``` This function call should generate a plot with 15 flipped sine waves, using the \'whitegrid\' theme, \'talk\' context, with despined axes, and a \'muted\' color palette. Constraints: - Ensure that the code is clean, well-documented, and follows best practices in Python programming. - The function should handle edge cases, such as invalid parameter values, by providing appropriate errors or warnings. - Optimize the function to perform efficiently, even with a high number of sine waves (e.g., `n=1000`). Submission Guidelines: - Submit your complete function definition. - Include a short note on how you tested your function and any additional observations.","solution":"def custom_sinplot(n=10, flip=1, style=\'darkgrid\', context=\'notebook\', despine=True, palette=\'deep\'): Generates a plot with customized aesthetics using seaborn. Parameters: n (int): Number of sine waves (default is 10). flip (int): A factor to flip the sine waves (default is 1). style (str): A seaborn style theme to apply. Options include \'darkgrid\', \'whitegrid\', \'dark\', \'white\', and \'ticks\'. context (str): A seaborn context for scaling. Options include \'paper\', \'notebook\', \'talk\', and \'poster\'. despine (bool): If True, removes the top and right spines (default is True). palette (str): A seaborn color palette to apply (default is \'deep\'). Returns: None. Displays the plot. import numpy as np import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=style, palette=palette) sns.set_context(context) def sinplot(n, flip): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) sinplot(n, flip) if despine: sns.despine() plt.show() # Example usage: # custom_sinplot(n=15, flip=-1, style=\'whitegrid\', context=\'talk\', despine=True, palette=\'muted\')"},{"question":"# Question Given the concepts of validation curves and learning curves explained in the documentation, your task is to implement a function that: 1. Generates and plots a validation curve for a specified model, dataset, and hyperparameter range. 2. Generates and plots a learning curve for the same model and dataset with varying training sizes. The function should accept the necessary parameters for these tasks and output the respective plots. Function Signature ```python import numpy as np from sklearn.model_selection import validation_curve, learning_curve import matplotlib.pyplot as plt def plot_model_evaluation_curves(model, X, y, param_name, param_range, train_sizes, cv=5): Plots the validation curve and learning curve for the given model and dataset. Parameters: - model: The machine learning model (estimator) to evaluate. - X: Features dataset (numpy.ndarray). - y: Target labels (numpy.ndarray). - param_name: Name of the hyperparameter to evaluate in the validation curve (str). - param_range: Range of values for the hyperparameter in the validation curve (array-like). - train_sizes: Training sizes for generating the learning curve (array-like). - cv: Number of folds for cross-validation (int, default=5). Returns: None. It generates and displays the validation curve and learning curve plots. # Your code here ``` Input - `model`: An instantiated machine learning model (e.g., `SVC(kernel=\'linear\')`). - `X` (numpy.ndarray): The feature data. - `y` (numpy.ndarray): The target labels. - `param_name` (str): The name of the hyperparameter to tune (like `\'C\'` for SVM). - `param_range` (array-like): A list of values to be evaluated for the hyperparameter. - `train_sizes` (array-like): The sizes of the training dataset to evaluate in the learning curve. - `cv` (int): Number of cross-validation folds. Output - The function should generate and display: - A validation curve plot showing the training and validation scores against the hyperparameter values. - A learning curve plot showing the training and validation scores against the training set sizes. Constraints - Use `validation_curve` and `learning_curve` from `sklearn.model_selection`. - Use `matplotlib.pyplot` for plotting. - The function should be self-contained and should not rely on any external dataset (use parameters `X` and `y` provided to the function). Example Usage ```python from sklearn.datasets import load_iris from sklearn.utils import shuffle from sklearn.svm import SVC X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) model = SVC(kernel=\'linear\') param_name = \'C\' param_range = np.logspace(-7, 3, 10) train_sizes = np.linspace(0.1, 1.0, 10) plot_model_evaluation_curves(model, X, y, param_name, param_range, train_sizes, cv=5) ``` This example will load the Iris dataset, shuffle it, and then use an SVM model to generate and plot both validation and learning curves.","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve import matplotlib.pyplot as plt def plot_model_evaluation_curves(model, X, y, param_name, param_range, train_sizes, cv=5): Plots the validation curve and learning curve for the given model and dataset. Parameters: - model: The machine learning model (estimator) to evaluate. - X: Features dataset (numpy.ndarray). - y: Target labels (numpy.ndarray). - param_name: Name of the hyperparameter to evaluate in the validation curve (str). - param_range: Range of values for the hyperparameter in the validation curve (array-like). - train_sizes: Training sizes for generating the learning curve (array-like). - cv: Number of folds for cross-validation (int, default=5). Returns: None. It generates and displays the validation curve and learning curve plots. # Generate validation curve train_scores, test_scores = validation_curve( model, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\\"accuracy\\" ) # Calculate mean and standard deviation for training and test scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\\"Validation Curve\\") plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) # Plot training and cross-validation scores plt.plot(param_range, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"r\\") plt.plot(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"g\\") plt.legend(loc=\\"best\\") # Generate learning curve train_sizes, train_scores, test_scores = learning_curve( model, X, y, train_sizes=train_sizes, cv=cv, scoring=\\"accuracy\\" ) # Calculate mean and standard deviation for training and test scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.subplot(1, 2, 2) plt.title(\\"Learning Curve\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) # Plot training and cross-validation scores plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"r\\") plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"g\\") plt.legend(loc=\\"best\\") plt.tight_layout() plt.show()"},{"question":"Design an asynchronous task scheduler in Python that utilizes some of the new asynchronous built-in functions introduced in Python 3.10, specifically `aiter()` and `anext()`. The scheduler should manage and execute tasks asynchronously, allowing tasks to be added at runtime. # Requirements 1. The scheduler should support adding new tasks while it is running. 2. Tasks should be asynchronous functions that may involve `await` expressions. 3. The scheduler should execute tasks in a round-robin fashion, ensuring that no task starves (i.e., no task is left waiting indefinitely). 4. Use `aiter()` and `anext()` to manage the task execution. # Constraints - You may assume that tasks are provided as asynchronous iterable objects. - Each task should perform a given number of steps before yielding control back to the scheduler. - The tasks may complete at different times, and the scheduler should gracefully handle task completion. # Input - Tasks to schedule are provided as a list of asynchronous iterables. - Each task can be represented as an asynchronous iterator yielding steps (which could be simulated wait times or other operations). # Output - Print the order in which tasks are executed and their respective steps. # Example 1. **Define Tasks**: ```python async def task1(): for i in range(3): print(f\'Task 1, step {i}\') await asyncio.sleep(1) yield async def task2(): for i in range(2): print(f\'Task 2, step {i}\') await asyncio.sleep(1) yield async def task3(): for i in range(4): print(f\'Task 3, step {i}\') await asyncio.sleep(1) yield ``` 2. **Scheduler Execution**: ```python tasks = [task1(), task2(), task3()] await task_scheduler(tasks) ``` 3. **Expected Output**: ``` Task 1, step 0 Task 2, step 0 Task 3, step 0 Task 1, step 1 Task 2, step 1 Task 3, step 1 Task 1, step 2 Task 3, step 2 Task 3, step 3 ``` # Implementation Notes - Use the `aiter()` function to create an asynchronous iterator from each task. - Use the `anext()` function to move to the next step of each task. - The scheduler will run until all tasks are complete, and should handle runtime task additions if necessary. # Function Signature ```python import asyncio async def task_scheduler(tasks: list): # Implementation details here pass ```","solution":"import asyncio async def task_scheduler(tasks): Asynchronous task scheduler that executes tasks in a round-robin fashion. Args: tasks (list): A list of asynchronous iterables representing tasks. Returns: None task_iterators = [aiter(task) for task in tasks] while task_iterators: completed_tasks = [] for i, task_iter in enumerate(task_iterators): try: await anext(task_iter) except StopAsyncIteration: completed_tasks.append(task_iter) # Remove completed tasks from the list task_iterators = [task_iter for task_iter in task_iterators if task_iter not in completed_tasks] # Example tasks for testing async def task1(): for i in range(3): print(f\'Task 1, step {i}\') await asyncio.sleep(0.01) yield async def task2(): for i in range(2): print(f\'Task 2, step {i}\') await asyncio.sleep(0.01) yield async def task3(): for i in range(4): print(f\'Task 3, step {i}\') await asyncio.sleep(0.01) yield"},{"question":"# Advanced Python Coding Assessment: File Descriptor Operations **Objective:** The goal of this assessment is to demonstrate your understanding of handling file operations using lower-level file descriptor APIs in Python. You are required to implement functions using the provided low-level API documentation. **Problem Statement:** You need to create a module that performs complex file operations using the provided C APIs. Write a Python function for each of the tasks described below. Use appropriate error handling to manage any potential issues that arise during file operations. **Task 1: `create_file_from_fd`** - **Input**: An integer file descriptor `fd`, and optional parameters such as `name`, `mode`, `buffering`, `encoding`, `errors`, and `newline`. - **Output**: A Python file object created from the file descriptor. - **Constraints**: Use defaults for optional parameters as per the `PyFile_FromFd` documentation. - **Function Signature**: ```python def create_file_from_fd(fd: int, name: str = None, mode: str = None, buffering: int = -1, encoding: str = None, errors: str = None, newline: str = None): pass ``` **Task 2: `get_file_descriptor`** - **Input**: A Python file object or any object with a `fileno()` method. - **Output**: An integer file descriptor associated with the given object. - **Function Signature**: ```python def get_file_descriptor(py_object) -> int: pass ``` **Task 3: `read_line_from_file`** - **Input**: A Python file object or any object with a `readline()` method, and an integer `n`. - **Output**: A string that represents the line read from the file. Handle different scenarios when `n` is zero, positive, and negative. - **Function Signature**: ```python def read_line_from_file(py_object, n: int) -> str: pass ``` **Task 4: `write_object_to_file`** - **Input**: A Python object `obj`, a Python file object `py_file`, and an integer flag `flags`. - **Output**: Write the object to the file and return 0 on success, or -1 on failure. - **Constraints**: The only supported flag is `Py_PRINT_RAW`. - **Function Signature**: ```python def write_object_to_file(obj, py_file, flags: int) -> int: pass ``` **Task 5: `write_string_to_file`** - **Input**: A string `s` and a Python file object `py_file`. - **Output**: Write the string to the file and return 0 on success, or -1 on failure. - **Function Signature**: ```python def write_string_to_file(s: str, py_file) -> int: pass ``` **Additional Notes**: - Assume you have access to the necessary C extension interface to use the provided APIs. - Ensure you handle all exceptions and edge cases, returning appropriate success or failure indicators. - Document any assumptions you make and justify your design choices clearly. **Evaluation Criteria**: - Correctness of implementation. - Proper use of provided C API functions. - Handling of edge cases and errors. - Code readability and documentation.","solution":"import os def create_file_from_fd(fd: int, name: str = None, mode: str = \'r\', buffering: int = -1, encoding: str = None, errors: str = None, newline: str = None): Create a Python file object from a given file descriptor. try: file = os.fdopen(fd, mode, buffering, encoding, errors, newline) return file except Exception as e: print(f\\"Error creating file from file descriptor: {e}\\") return None def get_file_descriptor(py_object) -> int: Return the file descriptor for a given Python file object or object with fileno() method. try: fd = py_object.fileno() return fd except Exception as e: print(f\\"Error obtaining file descriptor: {e}\\") return -1 def read_line_from_file(py_object, n: int) -> str: Read a line from a file object or an object with readline() method, handling different scenarios for n. try: if n > 0: return py_object.readline(n) elif n == 0: return \\"\\" else: # n < 0 return py_object.readline() except Exception as e: print(f\\"Error reading line from file: {e}\\") return \\"\\" def write_object_to_file(obj, py_file, flags: int) -> int: Write a Python object to a file, supporting the Py_PRINT_RAW flag. try: if flags == 1: # Py_PRINT_RAW (assumed to be 1) py_file.write(obj.__str__()) else: py_file.write(repr(obj)) return 0 except Exception as e: print(f\\"Error writing object to file: {e}\\") return -1 def write_string_to_file(s: str, py_file) -> int: Write a string to a file. try: py_file.write(s) return 0 except Exception as e: print(f\\"Error writing string to file: {e}\\") return -1"},{"question":"<|Analysis Begin|> The provided documentation details the Python `operator` module, which offers a set of efficient functions corresponding to Python\'s intrinsic operators. The module categorizes functions into: 1. **Object comparison functions**: for comparing objects (e.g., `operator.lt`, `operator.eq`). 2. **Logical operations**: for truth tests, identity tests, and boolean operations (e.g., `operator.not_`, `operator.is_`). 3. **Mathematical and bitwise operations**: for arithmetic operations (e.g., `operator.add`, `operator.mul`). 4. **Sequence operations**: for operations on sequences and mappings (e.g., `operator.getitem`, `operator.concat`). 5. **In-place operations**: for in-place modifications (e.g., `operator.iadd`, `operator.isub`). Each function mimics the behavior of the corresponding operator, and multiple functions are provided for the same operation, reflecting different naming conventions or the in-place nature of the operation. A question designed to assess comprehension of this module should: - Require the creation of a solution using several `operator` module functions. - Include various categories of operations to ensure the user deeply understands the module. - Be grounded in real-world application to make it relevant and challenging. <|Analysis End|> <|Question Begin|> # Question In this assessment, you\'ll create a basic calculator class using Python\'s `operator` module. This calculator should support various operations, including addition, subtraction, multiplication, division, comparisons, and identity checks. You should use the functions provided by the `operator` module to implement these operations. # Detailed Instructions 1. **Define a class `Calculator`** with the following methods: - `__init__(self, value)`: Initializes the calculator with a given initial value. - `add(self, other)`: Adds `other` to the current value using `operator.add`. - `subtract(self, other)`: Subtracts `other` from the current value using `operator.sub`. - `multiply(self, other)`: Multiplies the current value by `other` using `operator.mul`. - `divide(self, other)`: Divides the current value by `other` using `operator.truediv`. Handle divide-by-zero cases gracefully. - `power(self, other)`: Raises the current value to the power of `other` using `operator.pow`. - `negate(self)`: Negates the current value using `operator.neg`. - `is_equal(self, other)`: Checks if the current value is equal to `other` using `operator.eq`. - `is_less_than(self, other)`: Checks if the current value is less than `other` using `operator.lt`. - `is_greater_than(self, other)`: Checks if the current value is greater than `other` using `operator.gt`. - `get_value(self)`: Returns the current value. - `reset(self, value)`: Resets the current value to a new value. 2. **Usage Example**: ```python calc = Calculator(10) calc.add(5) # 15 calc.is_equal(15) # True calc.subtract(3) # 12 calc.multiply(2) # 24 calc.divide(4) # 6.0 calc.power(2) # 36.0 calc.is_less_than(50) # True calc.is_greater_than(30) # False calc.get_value() # 36.0 calc.negate() # -36.0 calc.reset(100) # 100 calc.get_value() # 100 ``` # Constraints - The initial value and all operations will involve integers or floats. - Ensure that the divide method can handle division by zero gracefully (e.g., by returning `float(\'inf\')` or another appropriate value). - You must use the functions from the `operator` module for all operations. # Evaluation Criteria - Correct usage of the `operator` module. - Accurate implementation of given operation methods. - Proper handling of edge cases such as division by zero. - Code readability and organization. Implement the `Calculator` class below: ```python import operator class Calculator: def __init__(self, value): self.value = value def add(self, other): self.value = operator.add(self.value, other) def subtract(self, other): self.value = operator.sub(self.value, other) def multiply(self, other): self.value = operator.mul(self.value, other) def divide(self, other): if other == 0: self.value = float(\'inf\') else: self.value = operator.truediv(self.value, other) def power(self, other): self.value = operator.pow(self.value, other) def negate(self): self.value = operator.neg(self.value) def is_equal(self, other): return operator.eq(self.value, other) def is_less_than(self, other): return operator.lt(self.value, other) def is_greater_than(self, other): return operator.gt(self.value, other) def get_value(self): return self.value def reset(self, value): self.value = value ``` Demonstrate the usage of this class with example inputs and expected outputs.","solution":"import operator class Calculator: def __init__(self, value): self.value = value def add(self, other): self.value = operator.add(self.value, other) def subtract(self, other): self.value = operator.sub(self.value, other) def multiply(self, other): self.value = operator.mul(self.value, other) def divide(self, other): if other == 0: self.value = float(\'inf\') else: self.value = operator.truediv(self.value, other) def power(self, other): self.value = operator.pow(self.value, other) def negate(self): self.value = operator.neg(self.value) def is_equal(self, other): return operator.eq(self.value, other) def is_less_than(self, other): return operator.lt(self.value, other) def is_greater_than(self, other): return operator.gt(self.value, other) def get_value(self): return self.value def reset(self, value): self.value = value"},{"question":"# Advanced PyTorch Coding Assessment Objective Design and implement a custom neural network layer in PyTorch and integrate it into a sample neural network model. Description You are required to implement a custom neural network layer called `CustomLinear` that applies a linear transformation to the incoming data: (y = xA^T + b), where `x` is the input tensor, `A` represents the weight matrix, and `b` is the bias vector. Additionally, integrate this custom layer into a simple feed-forward neural network used for classifying samples from the MNIST dataset. The network should consist of the following: 1. An initial `CustomLinear` layer. 2. A ReLU activation function. 3. A second `CustomLinear` layer. 4. A softmax output layer, as the network is used for classification. Input and Output Formats - **Input:** - `input_dim` (int): The number of input features. - `output_dim` (int): The number of output features. - `X` (torch.Tensor): A tensor of shape `(N, input_dim)` representing `N` samples, each with `input_dim` features. - **Output:** - The network should output a tensor of shape `(N, output_dim)` where `N` is the batch size, representing the class probabilities for each sample. Constraints - You must implement the `CustomLinear` class by extending `torch.nn.Module`. - All tensor operations within your `CustomLinear` class should be compatible with TorchScript. - Your implementation should leverage PyTorch’s autograd feature for backpropagation. Instructions 1. Implement the `CustomLinear` class. 2. Implement a function `create_model(input_dim: int, output_dim: int) -> torch.nn.Module` to construct the neural network as described. 3. Write a forward pass function `forward(model: torch.nn.Module, X: torch.Tensor) -> torch.Tensor` to pass the input data through the constructed model and return the predictions. Example Workflow ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomLinear(nn.Module): def __init__(self, input_dim, output_dim): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(output_dim, input_dim)) self.bias = nn.Parameter(torch.randn(output_dim)) def forward(self, x): return torch.mm(x, self.weight.t()) + self.bias def create_model(input_dim, output_dim): model = nn.Sequential( CustomLinear(input_dim, 128), nn.ReLU(), CustomLinear(128, output_dim), nn.Softmax(dim=1) ) return model def forward(model, X): return model(X) # Example Usage input_dim = 784 # Assuming input data is 28x28 images output_dim = 10 # Assuming 10 classes for classification model = create_model(input_dim, output_dim) X = torch.rand((64, input_dim)) # A batch of 64 samples predictions = forward(model, X) print(predictions.shape) # Expected output: torch.Size([64, 10]) ``` Evaluation Criteria - Correctness of the `CustomLinear` class implementation. - Proper construction of the model using the specified architecture. - Accurate forward pass computation. Make sure to test your implementation with different input dimensions and batch sizes to ensure the correctness and robustness of your solution.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomLinear(nn.Module): def __init__(self, input_dim, output_dim): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(output_dim, input_dim)) self.bias = nn.Parameter(torch.randn(output_dim)) def forward(self, x): return torch.mm(x, self.weight.t()) + self.bias def create_model(input_dim, output_dim): model = nn.Sequential( CustomLinear(input_dim, 128), nn.ReLU(), CustomLinear(128, output_dim), nn.Softmax(dim=1) ) return model def forward(model, X): return model(X)"},{"question":"# XML Document Manipulation Problem Statement You are provided with an XML file containing information about several books in a library. Each book entry contains details such as title, author, year, and genre. You need to perform several tasks on this XML data using the `xml.etree.ElementTree` module. Task 1. **Parse the XML file**: Load the XML data from a file. 2. **Find and extract information**: - Find and return the list of all book titles. - Find and return the count of books published in or after the year 2000. 3. **Modify the XML data**: - Add a new book entry to the XML with the following details: ```xml <book> <title>New Book Title</title> <author>Author Name</author> <year>2023</year> <genre>Fiction</genre> </book> ``` 4. **Build and save the modified XML data**: Save the updated XML data back to a file. Input and Output - **Input**: - An XML file (`library.xml`) with the following format: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>1999</year> <genre>Non-Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> <genre>Fiction</genre> </book> <!-- more book entries --> </library> ``` - **Output**: - Function `get_titles(file_name: str) -> List[str]`: Takes the XML file name as input and returns a list of book titles. - Function `count_recent_books(file_name: str) -> int`: Takes the XML file name as input and returns the count of books published in or after the year 2000. - Function `add_book(file_name: str, new_book: Dict[str, str]) -> None`: Takes the XML file name and a dictionary with new book details as input. Adds the new book to the XML and saves the file. ```python def get_titles(file_name: str) -> List[str]: Parses the XML file and returns a list of all book titles. :param file_name: The name of the XML file to parse. :return: List of book titles. # Your code here def count_recent_books(file_name: str) -> int: Parses the XML file and returns the count of books published in or after the year 2000. :param file_name: The name of the XML file to parse. :return: Count of recent books. # Your code here def add_book(file_name: str, new_book: Dict[str, str]) -> None: Adds a new book entry to the XML file and saves the modified XML. :param file_name: The name of the XML file to modify. :param new_book: A dictionary with keys \'title\', \'author\', \'year\', \'genre\', and corresponding values. # Your code here ``` Constraints: - You must use the `xml.etree.ElementTree` module for all XML manipulations. - Assume the XML file has a valid format as provided. - Only books published after 1900 are considered valid. Performance Requirements - Your solutions should be efficient and handle XML data with up to 10,000 book entries. - Ensure that the XML file operations (reading, modifying, saving) are performed efficiently.","solution":"import xml.etree.ElementTree as ET from typing import List, Dict def get_titles(file_name: str) -> List[str]: Parses the XML file and returns a list of all book titles. :param file_name: The name of the XML file to parse. :return: List of book titles. tree = ET.parse(file_name) root = tree.getroot() titles = [book.find(\'title\').text for book in root.findall(\'book\')] return titles def count_recent_books(file_name: str) -> int: Parses the XML file and returns the count of books published in or after the year 2000. :param file_name: The name of the XML file to parse. :return: Count of recent books. tree = ET.parse(file_name) root = tree.getroot() count = sum(1 for book in root.findall(\'book\') if int(book.find(\'year\').text) >= 2000) return count def add_book(file_name: str, new_book: Dict[str, str]) -> None: Adds a new book entry to the XML file and saves the modified XML. :param file_name: The name of the XML file to modify. :param new_book: A dictionary with keys \'title\', \'author\', \'year\', \'genre\', and corresponding values. tree = ET.parse(file_name) root = tree.getroot() # Create a new book element book = ET.Element(\'book\') title = ET.SubElement(book, \'title\') title.text = new_book[\'title\'] author = ET.SubElement(book, \'author\') author.text = new_book[\'author\'] year = ET.SubElement(book, \'year\') year.text = new_book[\'year\'] genre = ET.SubElement(book, \'genre\') genre.text = new_book[\'genre\'] # Append the new book to the library root.append(book) # Write back to the file tree.write(file_name)"},{"question":"# Question: Implement a Directory Report Generator You are required to implement a function `generate_directory_report` that generates a report for a given directory. This report will include the name of each file and its respective size in bytes, recursively for all files within the directory and its subdirectories. Your implementation should take advantage of the `pathlib` module to handle file and directory paths. Function Signature ```python def generate_directory_report(directory: str) -> dict: pass ``` Input - `directory` (str): The path to the directory for which the report should be generated. Output - The function should return a dictionary where each key is the relative path of a file (from the given directory) and the value is the size of the file in bytes. Constraints - You should assume that the provided directory path is always a valid directory. - The solution should handle nested directories. - Use of `pathlib` module is mandatory. - The function should be efficient in terms of time complexity. Example Assume the directory structure is as follows: ``` test_dir/ ├── file1.txt (size: 100 bytes) ├── file2.txt (size: 200 bytes) └── subdir/ ├── file3.txt (size: 300 bytes) └── file4.txt (size: 400 bytes) ``` For the input `directory = \'test_dir\'`, the function should return: ```python { \'file1.txt\': 100, \'file2.txt\': 200, \'subdir/file3.txt\': 300, \'subdir/file4.txt\': 400 } ``` Additional Requirements - The solution should avoid using any other libraries for file and directory operations apart from `pathlib`. - Implement error handling for cases where it is not possible to read the size of the file. Hints - Use `Path.glob()` or `Path.rglob()` for recursive directory traversal. - Use `Path.stat()` to get file information.","solution":"from pathlib import Path def generate_directory_report(directory: str) -> dict: Generates a report of file sizes within the given directory and its subdirectories. Args: directory (str): The path to the directory for which the report should be generated. Returns: dict: A dictionary with the relative file paths and their respective sizes in bytes. report = {} directory_path = Path(directory) for file in directory_path.rglob(\'*\'): if file.is_file(): relative_path = file.relative_to(directory_path).as_posix() try: file_size = file.stat().st_size report[relative_path] = file_size except Exception as e: # In case of an error reading the file size, skip this file continue return report"},{"question":"Coding Assessment Question # Objective: Create a class that demonstrates the use of descriptors to manage attribute access. Your implementation should include both data and non-data descriptors. # Requirements: 1. Implement a data descriptor (`Temperature`) that manages setting and getting temperature values in Celsius. 2. Implement a non-data descriptor (`HistoryTracker`) that logs every time an attribute\'s value is accessed. # Input and Output: - Implement a main class `WeatherStation` that uses these descriptors. - `WeatherStation` should have a temperature attribute (utilizing `Temperature`) and a history attribute (utilizing `HistoryTracker`). # Constraints: - The temperature attribute should only accept numerical values. - When accessing the temperature, it should return the value in Celsius. - The history tracker should log every access to any attribute in a dictionary, mapping attribute names to access counts. # Example: ```python class Temperature: # Your code here class HistoryTracker: # Your code here class WeatherStation: temperature = Temperature() history = HistoryTracker() def __init__(self): self._temperature = 0 self._history = {} station = WeatherStation() station.temperature = 24 print(station.temperature) # Output: 24 print(station.history) # Output: {\'temperature\': 1, \'history\': 1} ``` # Performance Requirements: - The implementation should efficiently handle hundreds of accesses and modifications without significant performance degradation. # Detailed Steps: 1. Define the `Temperature` descriptor class with `__get__`, `__set__`, and `__delete__` methods to manage the temperature attribute. 2. Define the `HistoryTracker` descriptor class with `__get__`, `__set__`, and `__delete__` methods to log attribute accesses. 3. Implement the `WeatherStation` class using both descriptors and ensure proper initialization. Write Python code to achieve the above requirements.","solution":"class Temperature: def __get__(self, instance, owner): return instance._temperature def __set__(self, instance, value): if not isinstance(value, (int, float)): raise ValueError(\\"Temperature must be a number.\\") instance._temperature = value def __delete__(self, instance): del instance._temperature class HistoryTracker: def __get__(self, instance, owner): instance._history[\'temperature\'] = instance._history.get(\'temperature\', 0) + 1 instance._history[\'history\'] = instance._history.get(\'history\', 0) + 1 return instance._history def __set__(self, instance, value): raise AttributeError(\\"Cannot set the history attribute directly.\\") def __delete__(self, instance): raise AttributeError(\\"Cannot delete the history attribute.\\") class WeatherStation: temperature = Temperature() history = HistoryTracker() def __init__(self): self._temperature = 0 self._history = {}"},{"question":"Objective: Implement a function `compress_and_decompress_with_custom_filters` that takes a list of tuples as input where each tuple contains a filename and the bytes data to be written to that file. The function should compress the data using LZMA with a custom filter chain, write it to the respective files, then read those files back, decompress the data, and return the decompressed data. Function Signature: ```python def compress_and_decompress_with_custom_filters(file_data_list: list[tuple[str, bytes]]) -> list[bytes]: pass ``` Input: - `file_data_list` (list of tuples): Each tuple contains: - `str` - filename without extension (e.g., \\"file1\\") - `bytes` - data to be compressed and written to the file (e.g., `b\\"Hello, World!\\"`) Output: - `list of bytes`: A list where each element corresponds to the decompressed data read from each file. Constraints: - You must use at least one custom filter in the compression process, such as `FILTER_DELTA` or `FILTER_LZMA2`. - The filenames should be saved with a \\".xz\\" extension using the LZMA format. Example: ```python input_data = [(\\"file1\\", b\\"Data for file1\\"), (\\"file2\\", b\\"Data for file2\\")] result = compress_and_decompress_with_custom_filters(input_data) print(result) # Output: [b\'Data for file1\', b\'Data for file2\'] ``` Complete Solution: Implement the function fully using the `lzma` module, ensuring correct application of compression, custom filters, file handling, and verification by decompressing the files and returning the original data as a list of bytes.","solution":"import lzma def compress_and_decompress_with_custom_filters(file_data_list): Compress the data using LZMA with a custom filter chain, write it to the respective files, then read those files back, decompress the data, and return the decompressed data. custom_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 1} ] # Compress and write to files for filename, data in file_data_list: compressed_filename = filename + \\".xz\\" with lzma.open(compressed_filename, \\"wb\\", filters=custom_filters) as f: f.write(data) decompressed_data_list = [] # Read from files and decompress for filename, _ in file_data_list: compressed_filename = filename + \\".xz\\" with lzma.open(compressed_filename, \\"rb\\") as f: decompressed_data = f.read() decompressed_data_list.append(decompressed_data) return decompressed_data_list"},{"question":"# PyTorch Coding Assessment Question Objective: To assess your understanding of PyTorch backends, particularly the CUDA backend and its advanced features like TensorFloat-32 (TF32) and cuFFT plan caches. This question will test your ability to manipulate backend settings and write a function that optimizes matrix computations and FFT operations using these settings. Task: Write a Python function `optimize_cuda_backend` that: 1. Configures the CUDA backend to allow TensorFloat-32 (TF32) tensor cores for matrix multiplications. 2. Configures the CUDA backend to allow reduced precision reductions with fp16 GEMMs. 3. Modifies the cuFFT plan cache size for the current CUDA device to a specified size. 4. Performs a matrix multiplication and an FFT operation, demonstrating the impact of these settings on performance. Input: - `cache_size`: An integer representing the desired size of the cuFFT plan cache. Output: - A tuple containing: - The time taken for the matrix multiplication. - The time taken for the FFT operation. Constraints: - The function should handle any exceptions that might occur during backend configuration. - Ensure that the CUDA device is available and the operations are performed on the GPU. Performance Requirements: - The function must execute within a reasonable time frame given typical hardware constraints (e.g., NVIDIA GPU with CUDA support). Example usage: ```python import torch def optimize_cuda_backend(cache_size): import time try: # Step 1: Enable TF32 for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Step 2: Allow reduced precision reductions with fp16 accumulation torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True # Step 3: Set the cuFFT plan cache size current_device = torch.cuda.current_device() torch.backends.cuda.cufft_plan_cache[current_device].max_size = cache_size # Prepare sample inputs A = torch.randn(1024, 1024, device=\'cuda\', dtype=torch.float32) B = torch.randn(1024, 1024, device=\'cuda\', dtype=torch.float32) x = torch.randn(4096, device=\'cuda\', dtype=torch.complex64) # Step 4: Perform matrix multiplication and measure time start_time = time.time() C = torch.matmul(A, B) matmul_time = time.time() - start_time # Step 5: Perform FFT and measure time start_time = time.time() x_fft = torch.fft.fft(x) fft_time = time.time() - start_time return matmul_time, fft_time except Exception as e: print(f\\"An error occurred: {e}\\") return None # Example test cache_size = 16 result = optimize_cuda_backend(cache_size) print(result) ``` Notes: 1. You may need to adapt the example usage based on actual hardware and CUDA availability. 2. Submit your function implementation and ensure it meets the described functionality.","solution":"import torch def optimize_cuda_backend(cache_size): import time try: # Step 1: Enable TF32 for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Step 2: Allow reduced precision reductions with fp16 accumulation torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True # Step 3: Set the cuFFT plan cache size current_device = torch.cuda.current_device() torch.backends.cuda.cufft_plan_cache[current_device].max_size = cache_size # Prepare sample inputs A = torch.randn(1024, 1024, device=\'cuda\', dtype=torch.float32) B = torch.randn(1024, 1024, device=\'cuda\', dtype=torch.float32) x = torch.randn(4096, device=\'cuda\', dtype=torch.complex64) # Step 4: Perform matrix multiplication and measure time start_time = time.time() C = torch.matmul(A, B) matmul_time = time.time() - start_time # Step 5: Perform FFT and measure time start_time = time.time() x_fft = torch.fft.fft(x) fft_time = time.time() - start_time return matmul_time, fft_time except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"# Advanced Socket Programming Challenge You are tasked with implementing a simple chat server and client using non-blocking sockets and the `select` module. The server should be able to handle multiple clients simultaneously, broadcasting any message received from one client to all connected clients. Requirements: 1. **Server Implementation:** - Create a server socket that listens for incoming connections on a specified port. - Use non-blocking sockets and the `select` module to manage multiple clients. - Accept incoming client connections and add them to a list of clients. - On receiving a message from any client, broadcast that message to all other connected clients. - The server should handle the shutdown gracefully and ensure all sockets are properly closed. 2. **Client Implementation:** - Create a client socket that connects to the server. - Allow the user to send messages and receive broadcasts from the server. - Use non-blocking sockets and the `select` module to manage sending and receiving messages simultaneously. - Handle user input to send messages and display incoming broadcast messages. Input and Output Formats: - The server should take the port number as a command-line argument. - The client should take the server IP and port number as command-line arguments. - The server does not produce any output except for error messages to the console. - The client should print incoming messages prefixed with the sender\'s address. Constraints: - You may assume that messages are short and fit within typical network buffer sizes. - You may also assume that clients will not send messages simultaneously (for simplicity). Performance Requirements: - The server should be able to handle at least 10 simultaneous clients without significant performance degradation. ```python # Server code import socket import select import sys def run_server(port): # Implement the server setup, client handling, and broadcasting logic here pass if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) run_server(port) # Client code import socket import select import sys def run_client(server_ip, server_port): # Implement the client setup, user input handling, and receiving broadcast messages here pass if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python client.py <server_ip> <server_port>\\") sys.exit(1) server_ip = sys.argv[1] server_port = int(sys.argv[2]) run_client(server_ip, server_port) ``` **Notes:** - Ensure your code is robust and handles unexpected scenarios (e.g., sudden client disconnection). - Document your code to explain how major functions work and any design decisions you made.","solution":"# Server code import socket import select import sys def run_server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(10) server_socket.setblocking(False) inputs = [server_socket] clients = {} print(f\\"Server listening on port {port}\\") while True: readable, writable, exceptional = select.select(inputs, [], inputs) for s in readable: if s is server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) inputs.append(client_socket) clients[client_socket] = client_address print(f\\"Connection from {client_address}\\") else: try: data = s.recv(1024) if data: for client_socket in clients: if client_socket is not s: client_socket.send(data) else: if s in inputs: inputs.remove(s) if s in clients: del clients[s] s.close() except: if s in inputs: inputs.remove(s) if s in clients: del clients[s] s.close() for s in exceptional: inputs.remove(s) if s in clients: del clients[s] s.close() if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) run_server(port) # Client code import socket import select import sys def run_client(server_ip, server_port): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((server_ip, server_port)) client_socket.setblocking(False) inputs = [client_socket, sys.stdin] while True: readable, writable, exceptional = select.select(inputs, [], inputs) for s in readable: if s is client_socket: data = s.recv(1024) if data: print(data.decode()) else: print(\\"Disconnected from server\\") sys.exit() else: message = sys.stdin.readline() if message: client_socket.send(message.encode()) for s in exceptional: inputs.remove(s) s.close() sys.exit() if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python client.py <server_ip> <server_port>\\") sys.exit(1) server_ip = sys.argv[1] server_port = int(sys.argv[2]) run_client(server_ip, server_port)"},{"question":"**Complex Tensor Manipulation with Named Tensors** **Objective:** Write a function that takes two 4-dimensional named tensors as input, performs certain transformations, and returns the result as described below. This will test your understanding of creating named tensors, manipulating dimensions, and ensuring correct name propagation. **Function Signature:** ```python def transform_named_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` **Input:** 1. `tensor_a`: A 4-dimensional `torch.Tensor` with named dimensions `(N, C, H, W)`. 2. `tensor_b`: A 4-dimensional `torch.Tensor` with named dimensions `(N, C, H, W)`. **Output:** A 2-dimensional `torch.Tensor` with named dimensions `(N, features)`, where: - Both input tensors are first concatenated along the `W` dimension. - Flatten the resulting tensor\'s dimensions `C`, `H`, and the new `W` into a single dimension named `features`. **Example:** ```python tensor_a = torch.randn(2, 3, 4, 5, names=(\'N\', \'C\', \'H\', \'W\')) tensor_b = torch.randn(2, 3, 4, 5, names=(\'N\', \'C\', \'H\', \'W\')) result = transform_named_tensors(tensor_a, tensor_b) print(result.names) # Output: (\'N\', \'features\') print(result.shape) # Output: torch.Size([2, 60]) assuming concatenation and flattening worked correctly ``` **Constraints:** - You may assume both tensors have the same shape for this exercise. - Ensure that all transformations maintain the correctness of the tensor names. **Notes:** - Use appropriate named tensor methods to accomplish the task. - Follow the principles of name alignment and propagation as described in the documentation.","solution":"import torch def transform_named_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Concatenates tensor_a and tensor_b along the \'W\' dimension, then flattens the dimensions \'C\', \'H\', and \'W\' into a single \'features\' dimension. Args: tensor_a (torch.Tensor): A 4D tensor with named dimensions (N, C, H, W). tensor_b (torch.Tensor): A 4D tensor with named dimensions (N, C, H, W). Returns: torch.Tensor: A 2D tensor with named dimensions (N, features). # Concatenate along the \'W\' dimension. concatenated = torch.cat([tensor_a, tensor_b], dim=\'W\') # Flatten \'C\', \'H\', and \'new W\' into a single \'features\' dimension. N, C, H, W = concatenated.shape features = C * H * W result = concatenated.refine_names(..., \'C\', \'H\', \'W\').flatten((\'C\', \'H\', \'W\'), \'features\') return result.refine_names(\'N\', \'features\')"},{"question":"# Seaborn Distribution Visualization Background To understand and analyze the distribution of variables in a dataset, visualizing the data distributions is a crucial step. Seaborn provides a variety of functions that allow us to create informative visualizations of univariate and bivariate distributions. Objectives In this task, you will use the seaborn library to create various distribution plots from a dataset. You will demonstrate your understanding by implementing custom functions to generate these plots and interpret the results. Task Description 1. **Loading the Dataset:** Load the \'penguins\' dataset using seaborn\'s `sns.load_dataset(\\"penguins\\")`. 2. **Univariate Histogram:** Write a function `plot_histogram` to plot a univariate histogram of the \'flipper_length_mm\' variable. ```python def plot_histogram(data): Plots a histogram of the \'flipper_length_mm\' variable. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' variable. Returns: None pass ``` 3. **Customized KDE Plot:** Write a function `plot_kde` to plot a KDE plot of the \'flipper_length_mm\' variable, adjusting the `bw_adjust` parameter. ```python def plot_kde(data, bw_adjust=1): Plots a KDE plot of the \'flipper_length_mm\' variable with customizable bandwidth adjustment. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' variable. bw_adjust (float): Bandwidth adjustment. Default is 1. Returns: None pass ``` 4. **Conditional Histograms:** Write a function `plot_conditional_histogram` to plot conditional histograms of the \'flipper_length_mm\' variable based on the \'species\' variable, using different visual representations (\'layered\', \'stacked\', \'dodge\'). ```python def plot_conditional_histogram(data, representation=\'layered\'): Plots conditional histograms of the \'flipper_length_mm\' variable based on the \'species\' variable. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' and \'species\' variables. representation (str): The visual representation method (\'layered\', \'stacked\', \'dodge\'). Default is \'layered\'. Returns: None pass ``` 5. **Bivariate KDE Plot:** Write a function `plot_bivariate_kde` to plot a bivariate KDE plot for the variables \'bill_length_mm\' and \'bill_depth_mm\'. ```python def plot_bivariate_kde(data): Plots a bivariate KDE plot for the variables \'bill_length_mm\' and \'bill_depth_mm\'. Parameters: data (DataFrame): The input dataset containing the \'bill_length_mm\' and \'bill_depth_mm\' variables. Returns: None pass ``` # Input - A pandas DataFrame `data` containing the penguins dataset. # Output - Seaborn plots as specified in each function. # Constraints - Use seaborn library for visualizations. - Ensure that functions are modular and reusable. - The function should not return any values; they should display the plots directly. # Example ```python import seaborn as sns import pandas as pd # Load dataset data = sns.load_dataset(\\"penguins\\") # Call the functions plot_histogram(data) plot_kde(data, bw_adjust=0.5) plot_conditional_histogram(data, representation=\'stacked\') plot_bivariate_kde(data) ``` Submission - Submit the Python script with the implemented functions. # Note You are free to use additional seaborn and matplotlib parameters to enhance the visualizations as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_data(): Loads the \'penguins\' dataset from seaborn. Returns: DataFrame: The loaded penguins dataset. return sns.load_dataset(\\"penguins\\") def plot_histogram(data): Plots a histogram of the \'flipper_length_mm\' variable. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' variable. Returns: None sns.histplot(data[\'flipper_length_mm\'].dropna(), bins=20, kde=False) plt.title(\'Histogram of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() def plot_kde(data, bw_adjust=1): Plots a KDE plot of the \'flipper_length_mm\' variable with customizable bandwidth adjustment. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' variable. bw_adjust (float): Bandwidth adjustment. Default is 1. Returns: None sns.kdeplot(data[\'flipper_length_mm\'].dropna(), bw_adjust=bw_adjust) plt.title(\'KDE Plot of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def plot_conditional_histogram(data, representation=\'layered\'): Plots conditional histograms of the \'flipper_length_mm\' variable based on the \'species\' variable. Parameters: data (DataFrame): The input dataset containing the \'flipper_length_mm\' and \'species\' variables. representation (str): The visual representation method (\'layered\', \'stacked\', \'dodge\'). Default is \'layered\'. Returns: None if representation == \'layered\': sns.histplot(data, x=\'flipper_length_mm\', hue=\'species\', element=\'step\', stat=\'density\', common_norm=False) elif representation == \'stacked\': sns.histplot(data, x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\', stat=\'density\', common_norm=False) elif representation == \'dodge\': sns.histplot(data, x=\'flipper_length_mm\', hue=\'species\', multiple=\'dodge\', stat=\'density\', common_norm=False) plt.title(f\'Conditional Histogram of Flipper Length (Representation: {representation})\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def plot_bivariate_kde(data): Plots a bivariate KDE plot for the variables \'bill_length_mm\' and \'bill_depth_mm\'. Parameters: data (DataFrame): The input dataset containing the \'bill_length_mm\' and \'bill_depth_mm\' variables. Returns: None sns.kdeplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\') plt.title(\'Bivariate KDE Plot of Bill Length and Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.show()"},{"question":"Objective The goal of this exercise is to test your understanding and ability to work with context variables in Python, specifically utilizing the `contextvars` module to manage state in asynchronous and concurrent code. Problem Statement Create a Python function that uses context variables to maintain and manage user session information in an asynchronous web server. Your task is to implement functions that handle the creation, update, and retrieval of user session data using `contextvars`. The web server will mimic handling multiple clients, and each client will have its own session state that should be maintained independently. Task Details 1. **ContextVar Declaration and Usage**: - Declare a `ContextVar` named `user_session` that stores a dictionary to hold session information, with a default empty dictionary `{}`. - Implement the following functions: - `get_user_session()`: Retrieves the current user session. - `update_user_session(data: dict)`: Updates the user session with the provided `data`. 2. **Asynchronous Request Handling**: - Create an asynchronous function `handle_request(session_data: dict)` that will: - Set the context variable `user_session` with the provided `session_data`. - Simulate processing by waiting for 1 second (`await asyncio.sleep(1)`). - Return the current session data using `get_user_session()`. 3. **Managing Context in Concurrent Requests**: - Create a main asynchronous function that: - Starts multiple concurrent requests using `asyncio.gather()`, each with different session data. - Ensures each request maintains its own session data without interfering with others. Constraints - Do not use any global variables to store session data. - Ensure the use of `ContextVar` to handle state management. - Simulate at least 3 concurrent requests with different session data. Function Definitions ```python import contextvars import asyncio # Declare the ContextVar to store user session data # user_session = ... def get_user_session(): Retrieve the current user session. pass def update_user_session(data: dict): Update the user session with the provided data. pass async def handle_request(session_data: dict): Handle a simulated request by updating the session data. pass async def main(): Main function to start concurrent requests with different session data. sessions = [ {\\"user\\": \\"alice\\", \\"role\\": \\"admin\\"}, {\\"user\\": \\"bob\\", \\"role\\": \\"user\\"}, {\\"user\\": \\"charlie\\", \\"role\\": \\"guest\\"}, ] responses = await asyncio.gather( *(handle_request(session) for session in sessions) ) for response in responses: print(response) # Run the main function to test the implementation asyncio.run(main()) ``` Expected Output The printed output should show that each simulated request maintains its own session data, demonstrating the correct use of context variables. Example: ``` {\'user\': \'alice\', \'role\': \'admin\'} {\'user\': \'bob\', \'role\': \'user\'} {\'user\': \'charlie\', \'role\': \'guest\'} ``` Make sure your implementation meets the above requirements and constraints.","solution":"import contextvars import asyncio # Declare the ContextVar to store user session data user_session = contextvars.ContextVar(\\"user_session\\", default={}) def get_user_session(): Retrieve the current user session. return user_session.get() def update_user_session(data: dict): Update the user session with the provided data. user_session.set(data) async def handle_request(session_data: dict): Handle a simulated request by updating the session data. update_user_session(session_data) await asyncio.sleep(1) return get_user_session() async def main(): Main function to start concurrent requests with different session data. sessions = [ {\\"user\\": \\"alice\\", \\"role\\": \\"admin\\"}, {\\"user\\": \\"bob\\", \\"role\\": \\"user\\"}, {\\"user\\": \\"charlie\\", \\"role\\": \\"guest\\"}, ] responses = await asyncio.gather( *(handle_request(session) for session in sessions) ) for response in responses: print(response) # Run the main function to test the implementation # asyncio.run(main())"},{"question":"# Advanced XML Parsing and Error Handling using `xml.parsers.expat` Objective Your task is to implement a function to parse an XML string using the `xml.parsers.expat` module and to handle different types of parsing errors, logging them appropriately. Problem Statement Write a function `parse_xml_data(xml_data: str) -> dict` that parses the given XML data string and returns a dictionary containing information about the XML structure. The function should extract all the elements along with their attributes and the text contained within each element. If any parsing errors occur, the function should log relevant information about the error encountered. Requirements 1. Use the `xml.parsers.expat.ParserCreate` function to create an XML parser. 2. Implement handlers for the following events: - **StartElementHandler:** Capture element name and attributes. - **EndElementHandler:** Track the end of elements. - **CharacterDataHandler:** Capture text data within elements. 3. Handle various parsing errors using appropriate exceptions (`ExpatError`) and log meaningful error messages. 4. Return a dictionary structure representing the parsed XML data. The dictionary should have element names as keys and a list of dictionaries as values containing attributes and text for each element occurrence. Input - `xml_data` (str): A string representing the XML data to be parsed. Output - `result` (dict): A dictionary representing the structure of the XML document. Example output format: ```python { \\"parent\\": [ {\\"attributes\\": {\\"id\\": \\"top\\"}, \\"text\\": None, \\"children\\": { \\"child1\\": [ {\\"attributes\\": {\\"name\\": \\"paul\\"}, \\"text\\": \\"Text goes here\\", \\"children\\": {}} ], \\"child2\\": [ {\\"attributes\\": {\\"name\\": \\"fred\\"}, \\"text\\": \\"More text\\", \\"children\\": {}} ] }} ] } ``` Constraints - The `xml_data` may contain multiple elements and nested structures. - Handle empty elements and missing text gracefully. - Ensure that logging of errors does not terminate the function execution but includes the error in the output dictionary. Example Usage ```python xml_string = <?xml version=\\"1.0\\"?> <parent id=\\"top\\"> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> result = parse_xml_data(xml_string) print(result) ``` Expected output: ```python { \\"parent\\": [ {\\"attributes\\": {\\"id\\": \\"top\\"}, \\"text\\": None, \\"children\\": { \\"child1\\": [ {\\"attributes\\": {\\"name\\": \\"paul\\"}, \\"text\\": \\"Text goes here\\", \\"children\\": {}} ], \\"child2\\": [ {\\"attributes\\": {\\"name\\": \\"fred\\"}, \\"text\\": \\"More text\\", \\"children\\": {}} ] }} ] } ``` Implementation Notes - Define appropriate handler functions and set them on the parser. - Use the `ExpatError` class for error handling. - Ensure nested elements are properly organized in the resulting dictionary. - Consider using helper functions to manage data structures efficiently.","solution":"import xml.parsers.expat from xml.parsers.expat import ExpatError def parse_xml_data(xml_data: str) -> dict: Parses XML data and returns a dictionary representation of the XML structure. :param xml_data: A string representing the XML data to be parsed. :return: A dictionary representing the parsed XML structure. parsed_data = {} current_element = [] element_stack = [] def start_element(name, attrs): nonlocal parsed_data element = {\\"name\\": name, \\"attributes\\": dict(attrs), \\"text\\": \\"\\", \\"children\\": {}} if len(element_stack) == 0: if name not in parsed_data: parsed_data[name] = [] parsed_data[name].append(element) else: parent = element_stack[-1] if name not in parent[\\"children\\"]: parent[\\"children\\"][name] = [] parent[\\"children\\"][name].append(element) element_stack.append(element) def end_element(name): if element_stack: element_stack.pop() def char_data(data): if element_stack: element_stack[-1][\\"text\\"] += data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data) except ExpatError as e: parsed_data[\\"error\\"] = f\\"XML parsing error: {str(e)}\\" return parsed_data"},{"question":"**Kernel Ridge Regression Implementation and Evaluation** You are asked to implement a Kernel Ridge Regression model using the scikit-learn library to evaluate its performance on a synthetic dataset. This task will help you demonstrate your understanding of kernel methods, regularization, and model evaluation. # Problem Statement Write a Python function that fits a Kernel Ridge Regression model to a given synthetic dataset and evaluates its performance. The synthetic dataset consists of a sinusoidal target function with added noise. You will generate this dataset, fit the model, and measure its performance using Mean Squared Error (MSE). # Function Signature ```python def kernel_ridge_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, alpha: float, kernel: str, **kernel_params) -> float: Fits a Kernel Ridge Regression model to the training data and returns the Mean Squared Error (MSE) on the test set. Parameters: - X_train: np.ndarray : Training data features, shape (n_samples, n_features). - y_train: np.ndarray : Training data target values, shape (n_samples,). - X_test: np.ndarray : Test data features, shape (n_samples, n_features). - y_test: np.ndarray : Test data target values, shape (n_samples,). - alpha: float : Regularization strength. - kernel: str : Kernel type to be used in the algorithm (e.g., \'linear\', \'polynomial\', \'rbf\'). - **kernel_params : Additional parameters for the specified kernel. Returns: - mse: float : Mean Squared Error of the model on the test data. # Your code here ``` # Technical Details 1. **Dataset Generation**: - Create a dataset with 100 samples. - Use a sinusoidal function with added noise (`y = sin(2πx) + noise`). 2. **Model Training**: - Use the `KernelRidge` class from `sklearn.kernel_ridge`. - Fit the model to the training data. 3. **Model Evaluation**: - Predict target values for the test data. - Calculate the Mean Squared Error (MSE) for the predictions. # Constraints: - You must use the `KernelRidge` implementation from the `sklearn.kernel_ridge` module. - The kernel should be parameterized based on the input `kernel` and `kernel_params`. # Example: ```python import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error # Generate synthetic dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(2 * np.pi * X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) # Split data into training and test sets X_train, X_test = X[:80], X[80:] y_train, y_test = y[:80], y[80:] # Define the function def kernel_ridge_regression(X_train, y_train, X_test, y_test, alpha=1.0, kernel=\'rbf\', **kernel_params): krr = KernelRidge(alpha=alpha, kernel=kernel, **kernel_params) krr.fit(X_train, y_train) y_pred = krr.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse # Example usage mse = kernel_ridge_regression(X_train, y_train, X_test, y_test, alpha=1.0, kernel=\'rbf\', gamma=0.1) print(f\'Mean Squared Error: {mse}\') ``` The function should be tested with varying parameters to ensure robustness and performance.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error def kernel_ridge_regression(X_train, y_train, X_test, y_test, alpha=1.0, kernel=\'rbf\', **kernel_params): Fits a Kernel Ridge Regression model to the training data and returns the Mean Squared Error (MSE) on the test set. Parameters: - X_train: np.ndarray : Training data features, shape (n_samples, n_features). - y_train: np.ndarray : Training data target values, shape (n_samples,). - X_test: np.ndarray : Test data features, shape (n_samples, n_features). - y_test: np.ndarray : Test data target values, shape (n_samples,). - alpha: float : Regularization strength. - kernel: str : Kernel type to be used in the algorithm (e.g., \'linear\', \'polynomial\', \'rbf\'). - **kernel_params : Additional parameters for the specified kernel. Returns: - mse: float : Mean Squared Error of the model on the test data. krr = KernelRidge(alpha=alpha, kernel=kernel, **kernel_params) krr.fit(X_train, y_train) y_pred = krr.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse # Example usage (for illustration, not part of the solution) if __name__ == \\"__main__\\": # Generate synthetic dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(2 * np.pi * X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) # Split data into training and test sets X_train, X_test = X[:80], X[80:] y_train, y_test = y[:80], y[80:] mse = kernel_ridge_regression(X_train, y_train, X_test, y_test, alpha=1.0, kernel=\'rbf\', gamma=0.1) print(f\'Mean Squared Error: {mse}\')"},{"question":"**Question: Implementing a Movie Rental System** **Objective:** Design a class-based Python program to simulate a simple movie rental system that: 1. Maintains a collection of movies available for rent. 2. Allows users to search for movies. 3. Handles the rental and return process. 4. Keeps track of rental statuses. **Requirements:** 1. Define a `Movie` class with the following attributes: - `title` (string): The title of the movie. - `genre` (string): The genre of the movie. - `year` (int): The release year of the movie. - `rented` (bool): Status of the movie, whether it is rented or not. Additionally, include methods to: - Return a string representation of the movie details. - Mark the movie as rented or available. 2. Define a `MovieRentalSystem` class that manages a collection of `Movie` objects. This class should allow: - Adding new movies to the collection. - Searching for movies by title. - Renting a movie (should change the rented status to `True`). - Returning a movie (should change the rented status to `False`). - Listing all available movies. **Constraints:** - Ensure that no two movies in the system have the same title. - A movie cannot be rented out if it is already rented. **Input and Output:** - You do not need to handle input and output in a specific format. Instead, focus on providing the methods with appropriate parameters and return types, ensuring the functionality meets the system\'s requirements. ```python class Movie: def __init__(self, title: str, genre: str, year: int): # Initialize attributes def __str__(self): # Return movie details as a string def rent(self): # Mark the movie as rented def return_movie(self): # Mark the movie as available class MovieRentalSystem: def __init__(self): # Initialize movie collection def add_movie(self, movie: Movie): # Add a new movie if it doesn\'t exist def search_movie(self, title: str) -> Movie: # Search for a movie by title, return the movie object def rent_movie(self, title: str) -> bool: # Rent a movie, return True if successful, otherwise False def return_movie(self, title: str) -> bool: # Return a rented movie, return True if successful, otherwise False def list_available_movies(self): # List all available movies, return as a list of strings ``` **Example Usage:** ```python system = MovieRentalSystem() movie1 = Movie(\\"Inception\\", \\"Sci-Fi\\", 2010) movie2 = Movie(\\"The Godfather\\", \\"Crime\\", 1972) system.add_movie(movie1) system.add_movie(movie2) print(system.list_available_movies()) # Should list both movies system.rent_movie(\\"Inception\\") print(system.list_available_movies()) # Should list only \\"The Godfather\\" system.return_movie(\\"Inception\\") print(system.list_available_movies()) # Should list both movies ``` Ensure your code is efficient and handles potential edge cases, such as trying to rent a movie that is already rented or returning a movie that is not currently rented.","solution":"class Movie: def __init__(self, title: str, genre: str, year: int): self.title = title self.genre = genre self.year = year self.rented = False def __str__(self): return f\\"{self.title} ({self.year}) - {self.genre} [{\'Rented\' if self.rented else \'Available\'}]\\" def rent(self): if not self.rented: self.rented = True return True return False def return_movie(self): if self.rented: self.rented = False return True return False class MovieRentalSystem: def __init__(self): self.movies = {} def add_movie(self, movie: Movie): if movie.title not in self.movies: self.movies[movie.title] = movie return True return False def search_movie(self, title: str) -> Movie: return self.movies.get(title) def rent_movie(self, title: str) -> bool: movie = self.search_movie(title) if movie and not movie.rented: return movie.rent() return False def return_movie(self, title: str) -> bool: movie = self.search_movie(title) if movie and movie.rented: return movie.return_movie() return False def list_available_movies(self): return [str(movie) for movie in self.movies.values() if not movie.rented]"},{"question":"Using the provided Python C API documentation for file objects, you are tasked with implementing a Python function that reads specific lines from a file using file descriptors. Problem Statement Implement a function `read_lines_from_file(filename: str, line_numbers: List[int]) -> List[str]` that takes a filename and a list of line numbers, and returns the corresponding lines from the file. 1. You must use file descriptors to open and read the file (i.e., integrate with the lower-level C APIs through Python\'s `os` module). 2. The function should handle any potential errors gracefully by catching exceptions and returning an appropriate message when a line cannot be read. Input - `filename`: Name of the file to read from (string). - `line_numbers`: List of integers representing the line numbers to read. Line numbers are 1-based indices. Output - A list of strings where each string is a line from the input file corresponding to the specified line numbers. - If a line number is invalid (e.g., greater than the number of lines in the file), include an appropriate error message in place of the corresponding line. Constraints - `filename` is an existing file and is accessible. - `line_numbers` is a list with at most 100 integers, and each integer is positive. Example ```python # Given file content in \'example.txt\': # Line 1 # Line 2 # Line 3 >>> read_lines_from_file(\'example.txt\', [1, 3]) [\'Line 1n\', \'Line 3n\'] >>> read_lines_from_file(\'example.txt\', [1, 4]) [\'Line 1n\', \'Error: Line 4 out of range\'] ``` Performance Requirements - The function should be optimized for minimal file reads and should efficiently handle files with a large number of lines. - Handle large files gracefully without excessive memory usage. Hints - You may use Python\'s `os` module to obtain and manipulate file descriptors. - Consider edge cases like empty files, invalid line numbers, and file access restrictions.","solution":"import os def read_lines_from_file(filename, line_numbers): Reads specific lines from a file using file descriptors. Parameters: filename (str): The name of the file to read from. line_numbers (List[int]): The list of line numbers to read. Line numbers are 1-based indices. Returns: List[str]: List of strings where each string is a line from the input file corresponding to the specified line numbers. fd = None result = [] try: # Open the file using os.open with read-only flags fd = os.open(filename, os.O_RDONLY) lines = {} current_line_number = 1 partial = \\"\\" while True: chunk = os.read(fd, 4096).decode() if not chunk: break partial += chunk while \\"n\\" in partial: line, partial = partial.split(\\"n\\", 1) lines[current_line_number] = line + \\"n\\" current_line_number += 1 if partial: lines[current_line_number] = partial # handle any remaining line fragment without newline for line_number in line_numbers: if 1 <= line_number <= current_line_number: result.append(lines.get(line_number, f\\"Error: Line {line_number} out of range\\")) else: result.append(f\\"Error: Line {line_number} out of range\\") except OSError as e: result.append(f\\"OS error: {e}\\") finally: if fd: os.close(fd) return result"},{"question":"# PyTorch Environment Configuration and Verification Objective: Configure the PyTorch MPS environment variables to optimize performance and resource management, then verify these configurations through a basic matrix multiplication operation. This will test your understanding of managing environment variables for performance tuning in PyTorch. Requirements: 1. Implement a function `configure_mps_environment()` that sets the following MPS environment variables: - `PYTORCH_DEBUG_MPS_ALLOCATOR` to `1` to enable verbose logging. - `PYTORCH_MPS_FAST_MATH` to `1` to enable fast math operations. - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` to `1.8` for memory allocation limit. - `PYTORCH_ENABLE_MPS_FALLBACK` to `1` to enable CPU fallback for unsupported operations. 2. Implement a function `verify_configuration()` that: - Checks if the environment variables were set correctly. - Logs/debugs the current settings. - Executes a matrix multiplication operation to ensure the settings are functional. Input: Your functions do not need to accept any external input but should be called sequentially to perform their tasks. Expected Output: - The `configure_mps_environment` function does not return anything. - The `verify_configuration` function should: - Print/log the current values of the specified environment variables. - Perform a matrix multiplication operation using PyTorch and print the result, ensuring the operation does not fail due to MPS configuration issues. Here is the function signature: ```python def configure_mps_environment(): # Function to set the required MPS environment variables pass def verify_configuration(): # Function to verify the environment settings and perform a matrix multiplication operation pass # Example usage configure_mps_environment() verify_configuration() ``` Constraints: - Assume that the system supports PyTorch with MPS backend and has the required dependencies installed. - The functions are to be executed in an environment where these settings have an observable effect. Ensure your implementation is robust and correctly handles the environment configuration and verification steps.","solution":"import os import torch def configure_mps_environment(): Configures the PyTorch MPS environment variables for optimization. os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = \'1.8\' os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\' def verify_configuration(): Verifies the environment settings and performs a matrix multiplication operation. debug_allocator = os.getenv(\'PYTORCH_DEBUG_MPS_ALLOCATOR\') fast_math = os.getenv(\'PYTORCH_MPS_FAST_MATH\') high_watermark_ratio = os.getenv(\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\') enable_fallback = os.getenv(\'PYTORCH_ENABLE_MPS_FALLBACK\') print(f\\"PYTORCH_DEBUG_MPS_ALLOCATOR: {debug_allocator}\\") print(f\\"PYTORCH_MPS_FAST_MATH: {fast_math}\\") print(f\\"PYTORCH_MPS_HIGH_WATERMARK_RATIO: {high_watermark_ratio}\\") print(f\\"PYTORCH_ENABLE_MPS_FALLBACK: {enable_fallback}\\") # Perform a simple matrix multiplication. if torch.has_mps: device = torch.device(\\"mps\\") mat1 = torch.randn((3, 3), device=device) mat2 = torch.randn((3, 3), device=device) result = torch.matmul(mat1, mat2) print(\\"Matrix multiplication result:\\") print(result) else: print(\\"MPS device not available.\\")"},{"question":"Working with `pkgutil` Introduction You are required to demonstrate your understanding of the `pkgutil` module, which provides utilities for package support and the Python import system. You will implement a function that uses various utilities from `pkgutil` to inspect and output information regarding Python modules in a given directory. Problem Statement Write a Python function `inspect_modules_in_path(path: Optional[List[str]] = None) -> List[str]` that takes an optional list of paths and returns a list of strings, each string containing the name and loader type of every module found within those paths. The module names should be sorted in alphabetical order. Use the following guidelines: 1. If the `path` is `None`, the function should inspect all top-level modules as specified in `sys.path`. 2. Use `pkgutil.iter_modules` to find all modules in the specified path. 3. For each module, get the module info using `pkgutil.get_loader`. 4. Format each result as `\\"{module_name}: {loader_type}\\"` where: - `module_name` is the name of the module. - `loader_type` is `str(type(loader))`. 5. Ensure that the output list is sorted in alphabetical order of module names. 6. Return the formatted list. Function Signature ```python from typing import List, Optional def inspect_modules_in_path(path: Optional[List[str]] = None) -> List[str]: pass ``` Constraints - Assume that the file system and Python environment are correctly set up. - Handle potential exceptions gracefully, e.g., modules that cannot be loaded should be skipped with a warning logged to the console. Example Suppose the directory structure in your environment looks like this: ``` /path/to/dir: - module_a.py - module_b.py - subdir/module_c.py ``` And the content of `sys.path` includes `/path/to/dir`. ```python inspect_modules_in_path([\'/path/to/dir\']) ``` Expected output (order may vary based on implementation details): ```python [ \\"module_a: <class \'importlib.machinery.SourceFileLoader\'>\\", \\"module_b: <class \'importlib.machinery.SourceFileLoader\'>\\", \\"subdir.module_c: <class \'importlib.machinery.SourceFileLoader\'>\\" ] ``` # Additional notes: - You may use other standard libraries as needed. - Make sure to add appropriate import statements for the `pkgutil` module and others required.","solution":"from typing import List, Optional import pkgutil import sys def inspect_modules_in_path(path: Optional[List[str]] = None) -> List[str]: if path is None: path = sys.path modules_info = [] for importer, modname, ispkg in pkgutil.iter_modules(path): try: loader = pkgutil.get_loader(modname) loader_type = str(type(loader)) if loader else \\"<unknown>\\" modules_info.append(f\\"{modname}: {loader_type}\\") except Exception as e: print(f\\"Warning: Could not inspect module {modname} - {e}\\") modules_info.sort() return modules_info"},{"question":"# FFT and Signal Processing with PyTorch In this task, you will work with the Fast Fourier Transform (FFT) functions provided by the `torch.fft` module to analyze and filter a noisy signal. Problem Statement You are provided with a 1D tensor representing a noisy signal. Your goal is to implement a function `denoise_signal` that performs the following steps: 1. **Compute the FFT** of the signal to transform it into the frequency domain using the `torch.fft.fft` function. 2. **Identify and isolate the dominant frequency components**. You should keep only the top `k` frequency components and set the rest to zero. The parameter `k` (an integer) will be provided as an input to your function. 3. **Compute the Inverse FFT** to convert the modified frequency domain signal back into the time domain using the `torch.fft.ifft` function. 4. **Return the denoised signal** as a real-valued tensor. Function Signature ```python import torch def denoise_signal(signal: torch.Tensor, k: int) -> torch.Tensor: # Your implementation here pass ``` Input - `signal` (torch.Tensor): A 1D tensor of shape `(n,)` representing the noisy signal. - `k` (int): The number of dominant frequency components to retain in the frequency domain. Output - Returns a 1D tensor of shape `(n,)` representing the denoised signal. Constraints - The input tensor `signal` will have a size that is a power of 2 (e.g., 128, 256, 512, etc.) for efficient FFT computation. - You may assume `1 <= k <= n//2`. Example ```python import torch signal = torch.tensor([0.1, 0.3, 0.5, 0.7, -1.0, -0.7, -0.5, -0.3, -0.1, 0.0, 0.0, 0.1, 0.3, 0.5, 0.7, -1.0]) # Retain the top 3 frequency components k = 3 denoised_signal = denoise_signal(signal, k) print(denoised_signal) # Expected output is a tensor that approximates the original denoised signal ``` Hints - Use `torch.abs` to compute the magnitude of the frequency components. - You might find `torch.argsort` helpful to identify the indices of the top `k` components. Performance Requirements - The implementation should be efficient in both time and space complexity, considering the constraints on tensor sizes.","solution":"import torch def denoise_signal(signal: torch.Tensor, k: int) -> torch.Tensor: Denoise the signal by retaining only the top k frequency components. Parameters: signal (torch.Tensor): A 1D tensor of shape (n,) representing the noisy signal. k (int): The number of dominant frequency components to retain. Returns: torch.Tensor: A 1D tensor of shape (n,) representing the denoised signal. # Compute the FFT of the signal fft_result = torch.fft.fft(signal) # Compute the magnitude of the frequency components magnitudes = torch.abs(fft_result) # Identify indices of the top k frequency components top_k_indices = torch.argsort(magnitudes, descending=True)[:k] # Create a mask to retain the top k components and set the rest to zero mask = torch.zeros_like(fft_result) mask[top_k_indices] = 1 # Apply the mask to the FFT result filtered_fft_result = fft_result * mask # Compute the inverse FFT to get back to the time domain denoised_signal = torch.fft.ifft(filtered_fft_result).real return denoised_signal"},{"question":"**Objective**: Thoroughly assess the student\'s understanding of the `pandas.plotting` module along with data manipulation capabilities of `pandas`. # Coding Assessment Question You are provided with a CSV file `data.csv`, which contains the following columns: - `Date`: The date of the data entry - `Category`: A category label for the data entry - `Value`: The numeric value associated with the data entry Your task is to write a function called `plot_data_trends` that: 1. Reads the CSV file into a pandas DataFrame. 2. Plots the following visualizations: - **Parallel Coordinates**: Use the `parallel_coordinates` function to plot the data with `Category` as the class column. - **Andrews Curves**: Use the `andrews_curves` function to plot the data with `Category` as the class column. - **Scatter Matrix**: Use the `scatter_matrix` function to create a scatter plot matrix of the `Value` column against itself, with points color-coded by the `Category`. 3. Displays all the plots. # Function Signature ```python import pandas as pd from pandas.plotting import parallel_coordinates, andrews_curves, scatter_matrix def plot_data_trends(file_path: str): \'\'\' Parameters: file_path (str): The file path to the CSV file (data.csv). Returns: None \'\'\' # Your implementation here ``` # Constraints - You may assume that the file `data.csv` is well-formed and does not contain any missing values. - Ensure the plots have appropriate titles, labels, and legends where necessary. # Example Suppose the `data.csv` contains: ``` Date,Category,Value 2023-01-01,A,10 2023-01-02,B,20 2023-01-03,C,30 2023-01-04,A,15 2023-01-05,B,10 2023-01-06,C,25 ``` After reading the data, the function should generate and display the specified plots using the data provided. This question will test the student\'s ability to work with `pandas` DataFrames, understand how to use advanced plotting functions from the `pandas.plotting` module, and demonstrate effective data visualization techniques.","solution":"import pandas as pd from pandas.plotting import parallel_coordinates, andrews_curves, scatter_matrix import matplotlib.pyplot as plt def plot_data_trends(file_path: str): \'\'\' Parameters: file_path (str): The file path to the CSV file (data.csv). Returns: None \'\'\' # Read the CSV file into a pandas DataFrame data = pd.read_csv(file_path) # Plot Parallel Coordinates plt.figure() parallel_coordinates(data, \'Category\') plt.title(\'Parallel Coordinates Plot\') plt.xlabel(\'Features\') plt.ylabel(\'Values\') plt.legend(loc=\'best\') plt.show() # Plot Andrews Curves plt.figure() andrews_curves(data, \'Category\') plt.title(\'Andrews Curves Plot\') plt.xlabel(\'Features\') plt.ylabel(\'Values\') plt.legend(loc=\'best\') plt.show() # Plot Scatter Matrix scatter_matrix_fig = scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal=\'kde\', color=data[\'Category\'].map({\'A\': \'red\', \'B\': \'green\', \'C\': \'blue\'})) plt.suptitle(\'Scatter Matrix Plot\') plt.show()"},{"question":"# Custom Protocol Implementation with asyncio You are tasked with implementing a custom echo server and client using the asyncio library. Specifically, you will implement a `BufferedEchoProtocol` class that inherits from `asyncio.BufferedProtocol`, and manage a buffer for receiving data. Below are the detailed steps and requirements: Step 1: Implement `BufferedEchoProtocol` - **Class:** `BufferedEchoProtocol(asyncio.BufferedProtocol)` - **Methods to Implement:** - `get_buffer(self, sizehint)`: Should return a memoryview of a byte buffer that can hold the incoming data, with the size determined by `sizehint`. - `buffer_updated(self, nbytes)`: Called when new data has been written to the buffer. Process the received data and echo it back using the transport. - `connection_made(self, transport)`: Called when a connection is made. Store the reference to the transport. - `connection_lost(self, exc)`: Called when the connection is lost or closed. Log the connection loss. - **Behavior:** - When data is received and the buffer is updated, the protocol should echo the data back to the sender. Step 2: Create Echo Server - **Function:** `start_server(host, port)` - Utilize `asyncio.get_running_loop()` and `loop.create_server()` to create a server that uses `BufferedEchoProtocol`. - Bind the server to `host` and `port`. Step 3: Create Echo Client - **Function:** `start_client(host, port, message)` - Use `asyncio.get_running_loop()` and `loop.create_connection()` to create a client that sends a message to the server and receives the echoed response. - Implement an appropriate protocol similar to the server to handle the connection. # Example Usage 1. Implement `BufferedEchoProtocol`. 2. Implement `start_server()`. 3. Implement `start_client()`. Input Format: - `start_server` function: - `host` (str): Hostname or IP address of the server. - `port` (int): Port number on which the server will listen. - `start_client` function: - `host` (str): Hostname or IP address of the server. - `port` (int): Port number on which the server is listening. - `message` (str): Message to send to the server. Output Format: - The server should print connection details and echoed messages. - The client should print sent and received messages. Constraints: - Proper exception handling for connection errors and data transmission. - Implement all methods in an asynchronous and non-blocking manner using asyncio\'s protocol and transport system. - Ensure correct buffer management to avoid memory leaks or excessive memory usage. # Example Code Template ```python import asyncio class BufferedEchoProtocol(asyncio.BufferedProtocol): def __init__(self): self.transport = None self._buffer = bytearray() def get_buffer(self, sizehint): return memoryview(self._buffer) def buffer_updated(self, nbytes): data = self._buffer[:nbytes] print(f\'Data received: {data.decode()}\') self.transport.write(data) def connection_made(self, transport): self.transport = transport print(f\'Connection made: {transport.get_extra_info(\\"peername\\")}\') def connection_lost(self, exc): if exc: print(f\'Connection lost with error: {exc}\') else: print(\'Connection closed normally\') async def start_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: BufferedEchoProtocol(), host, port) async with server: await server.serve_forever() async def start_client(host, port, message): class EchoClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): print(f\'Send: {self.message}\') transport.write(self.message.encode()) def data_received(self, data): print(f\'Received: {data.decode()}\') transport.close() def connection_lost(self, exc): print(\'Connection closed\') self.on_con_lost.set_result(True) loop = asyncio.get_running_loop() on_con_lost = loop.create_future() await loop.create_connection( lambda: EchoClientProtocol(message, on_con_lost), host, port) await on_con_lost # Example usage # asyncio.run(start_server(\'127.0.0.1\', 8888)) # asyncio.run(start_client(\'127.0.0.1\', 8888, \'Hello, World!\')) ``` This template provides a foundational structure. You are expected to complete it by ensuring proper buffer handling and connection management. Be sure to test your implementation with multiple clients and ensure it handles varying message sizes efficiently.","solution":"import asyncio class BufferedEchoProtocol(asyncio.BufferedProtocol): def __init__(self): self.transport = None self._buffer = bytearray(1024) def get_buffer(self, sizehint): return memoryview(self._buffer) def buffer_updated(self, nbytes): data = self._buffer[:nbytes].tobytes() print(f\'Data received: {data.decode()}\') self.transport.write(data) self._buffer = bytearray(1024) # Reset buffer def connection_made(self, transport): self.transport = transport print(f\'Connection made: {transport.get_extra_info(\\"peername\\")}\') def connection_lost(self, exc): if exc: print(f\'Connection lost with error: {exc}\') else: print(\'Connection closed normally\') async def start_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: BufferedEchoProtocol(), host, port) async with server: await server.serve_forever() async def start_client(host, port, message): class EchoClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): print(f\'Send: {self.message}\') transport.write(self.message.encode()) def data_received(self, data): print(f\'Received: {data.decode()}\') transport.close() def connection_lost(self, exc): print(\'Connection closed\') self.on_con_lost.set_result(True) loop = asyncio.get_running_loop() on_con_lost = loop.create_future() await loop.create_connection( lambda: EchoClientProtocol(message, on_con_lost), host, port) await on_con_lost # Example usage if __name__ == \\"__main__\\": server_task = asyncio.run(start_server(\'127.0.0.1\', 8888)) asyncio.run(start_client(\'127.0.0.1\', 8888, \'Hello, World!\'))"},{"question":"# Question: Enhanced Pretty-Print Function Write a function `enhanced_pretty_print` that leverages the `pprint` module to format and print a complex nested dictionary representing a filesystem. The filesystem dictionary contains folders as keys and its contents (either files or nested folders) as values. Files represented by strings and folders by dictionaries. The function should meet the following requirements: 1. **Input:** - `filesystem`: A dictionary that represents the nested structure of folders and files. - `max_width`: An integer representing the maximum width of the pretty-printed output (default: 80). - `max_depth`: An integer specifying the maximum depth of nested structures to display (default: None, which indicates no limit). - `compact`: A boolean indicating whether to print items in a compact manner, attempting to fit as much as possible on each line (default: False). 2. **Output:** - The function does not return anything. It prints the formatted filesystem dictionary to the console, ensuring it is readable and adheres to the formatting parameters. 3. **Constraints:** - The maximum width `max_width` should be a positive integer. - If `max_depth` is specified, it should be a non-negative integer. 4. **Example:** - For the input: ```python filesystem = { \\"root\\": { \\"folder1\\": { \\"file1.txt\\": \\"content of file1\\", \\"file2.txt\\": \\"content of file2\\" }, \\"folder2\\": { \\"subfolder1\\": { \\"file3.txt\\": \\"content of file3\\" }, \\"file4.txt\\": \\"content of file4\\" }, \\"file5.txt\\": \\"content of file5\\" } } ``` Calling `enhanced_pretty_print(filesystem, max_width=50, max_depth=2, compact=True)` should print: ``` {\'root\': {\'folder1\': {\'file1.txt\': \'content of file1\', \'file2.txt\': \'content of file2\'}, \'folder2\': {\'subfolder1\': {...}, \'file4.txt\': \'content of file4\'}, \'file5.txt\': \'content of file5\'}} ``` # Function Signature: ```python def enhanced_pretty_print(filesystem: dict, max_width: int = 80, max_depth: int = None, compact: bool = False) -> None: pass ```","solution":"import pprint def enhanced_pretty_print(filesystem: dict, max_width: int = 80, max_depth: int = None, compact: bool = False) -> None: Prints the nested dictionary representing a filesystem in a formatted manner using pprint. Parameters: filesystem (dict): The filesystem dictionary. max_width (int): The maximum width of the pretty-printed output. max_depth (int): The maximum depth of nested structures to display. compact (bool): Whether to use the compact format for printing. # Check constraints if not isinstance(max_width, int) or max_width <= 0: raise ValueError(\\"max_width must be a positive integer\\") if max_depth is not None and (not isinstance(max_depth, int) or max_depth < 0): raise ValueError(\\"max_depth must be a non-negative integer\\") printer = pprint.PrettyPrinter(width=max_width, compact=compact, depth=max_depth) printer.pprint(filesystem)"},{"question":"Objective Your task is to implement a function that processes potentially unsafe user input by escaping and then unescaping certain special characters in XML data. The input will contain a dictionary where keys are the strings to be escaped and their corresponding values are strings to unescape them back. You are required to handle escaping and unescaping operations correctly to demonstrate mastery of the `xml.sax.saxutils` utilities. Problem Statement Implement a function `process_xml_data(data: str, escape_dict: dict, unescape_dict: dict) -> str` that takes: 1. A string `data` containing the XML data to be processed. 2. A dictionary `escape_dict` where keys represent characters or sequences to be escaped and values represent their escaped versions. 3. A dictionary `unescape_dict` where keys represent escaped characters or sequences and values are their original forms. The function should: 1. Escape the characters in the `data` string using the `escape_dict` provided. 2. Unescape the characters in the result of the first step using the `unescape_dict` provided. 3. Return the final processed string. Constraints 1. The keys and values in `escape_dict` and `unescape_dict` are non-empty strings. 2. The `data` string will not be empty. Example ```python def process_xml_data(data: str, escape_dict: dict, unescape_dict: dict) -> str: # your implementation here # Example usage: data = \\"Use & to denote AND, < to start a tag, and > to close it.\\" escape_dict = {\\"&\\": \\"&amp;\\", \\"<\\": \\"&lt;\\", \\">\\": \\"&gt;\\"} unescape_dict = {\\"&amp;\\": \\"&\\", \\"&lt;\\": \\"<\\", \\"&gt;\\": \\">\\"} processed_data = process_xml_data(data, escape_dict, unescape_dict) print(processed_data) # Expected Output: \\"Use & to denote AND, < to start a tag, and > to close it.\\" ``` # Notes - Use `xml.sax.saxutils.escape()` for the escaping step. - Use `xml.sax.saxutils.unescape()` for the unescaping step. - Ensure your code efficiently handles the operations in the specified order. Requirements - Correctness: The implementation must correctly escape and then unescape the characters according to the provided dictionaries. - Efficiency: The solution should be efficient and handle the escaping/unescaping operations in a linear pass over the data.","solution":"from xml.sax.saxutils import escape, unescape def process_xml_data(data: str, escape_dict: dict, unescape_dict: dict) -> str: # Create a composite escape and unescape function using the provided dictionaries def custom_escape(data, escape_dict): for key, value in escape_dict.items(): data = data.replace(key, value) return data def custom_unescape(data, unescape_dict): for key, value in unescape_dict.items(): data = data.replace(key, value) return data # Step 1: Escape the characters in data using escape_dict escaped_data = custom_escape(data, escape_dict) # Step 2: Unescape the characters in the result using unescape_dict unescaped_data = custom_unescape(escaped_data, unescape_dict) # Step 3: Return the final processed string return unescaped_data"},{"question":"# Objective: To assess your understanding of PyTorch, specifically in the implementation of a training loop and efficient memory management. # Task: Write a function `train_model` in PyTorch that trains a given neural network using the provided training data and optimizes the model parameters. Your implementation should demonstrate effective memory management to prevent out-of-memory errors on the GPU. # Function Signature: ```python def train_model(model, criterion, optimizer, train_loader, num_epochs, device): Trains the given model using the specified criterion and optimizer on the provided training data for a number of epochs. Parameters: - model (torch.nn.Module): The neural network model to be trained. - criterion (torch.nn.Module): The loss function. - optimizer (torch.optim.Optimizer): The optimization algorithm. - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset. - num_epochs (int): Number of epochs to train the model. - device (torch.device): The device (CPU/GPU) to run the training on. Returns: - float: The final training loss. pass ``` # Guidelines: 1. **Accumulation of Gradients**: Ensure that you reset the gradients (`optimizer.zero_grad()`) at the start of each training iteration. 2. **Detaching Variables**: Avoid accumulating history across the training loop by detaching variables or accessing their underlying data when tracking statistics. 3. **Releasing Unnecessary Tensors**: Use `del` to explicitly free memory of intermediate tensors when they are no longer needed. 4. **Memory Management**: Implement strategies to manage CUDA memory efficiently, ensuring your training loop does not run out of memory. 5. **Checkpointing**: Optionally, consider implementing checkpointing to trade-off memory for compute if necessary. # Input: - `model` is an instance of `torch.nn.Module` representing the neural network. - `criterion` is an instance of `torch.nn.Module` representing the loss function. - `optimizer` is an instance of `torch.optim.Optimizer` representing the optimizer. - `train_loader` is an instance of `torch.utils.data.DataLoader` containing the training data. - `num_epochs` is an integer representing the number of epochs to train the model. - `device` is an instance of `torch.device` representing the device for computation (e.g., \'cpu\' or \'cuda\'). # Output: - The function should return a float representing the final training loss after all epochs are completed. # Constraints: - Ensure that your function handles CUDA memory efficiently. - The function should gracefully handle out-of-memory errors without crashing. # Example Usage: ```python # Example model, criterion, optimizer, and data loader model = MyNeuralNetwork().to(device) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters()) train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Train the model final_loss = train_model(model, criterion, optimizer, train_loader, num_epochs=10, device=torch.device(\'cuda\')) print(f\'Final Training Loss: {final_loss}\') ```","solution":"import torch def train_model(model, criterion, optimizer, train_loader, num_epochs, device): Trains the given model using the specified criterion and optimizer on the provided training data for a number of epochs. Parameters: - model (torch.nn.Module): The neural network model to be trained. - criterion (torch.nn.Module): The loss function. - optimizer (torch.optim.Optimizer): The optimization algorithm. - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset. - num_epochs (int): Number of epochs to train the model. - device (torch.device): The device (CPU/GPU) to run the training on. Returns: - float: The final training loss. model.train() final_loss = None for epoch in range(num_epochs): running_loss = 0.0 for inputs, labels in train_loader: inputs, labels = inputs.to(device), labels.to(device) # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass and optimize loss.backward() optimizer.step() # Track loss running_loss += loss.item() # Detach to avoid memory accumulation del inputs, labels, outputs, loss torch.cuda.empty_cache() # Calculate average loss over the epoch final_loss = running_loss / len(train_loader) print(f\'Epoch [{epoch + 1}/{num_epochs}], Loss: {final_loss:.4f}\') return final_loss"},{"question":"**Objective**: The goal of this exercise is to create a standalone Python zip application using the `zipapp` module. The application should consist of a simple Python command-line tool that reads a text file, counts the number of words, and outputs the result to the console. Additionally, the application should be distributable and executable on any machine with an appropriate Python interpreter. **Task**: Write a Python script that performs the following tasks: 1. **Create the Application Directory**: - Create a directory named `word_counter`. - Inside this directory, create a Python file named `__main__.py` with the following content: ```python import sys import os def count_words(file_path): try: with open(file_path, \'r\') as file: content = file.read() words = content.split() return len(words) except Exception as e: print(f\\"Error: {e}\\") return 0 def main(): if len(sys.argv) != 2: print(\\"Usage: word_counter <file_path>\\") sys.exit(1) file_path = sys.argv[1] if not os.path.exists(file_path): print(f\\"Error: File \'{file_path}\' does not exist.\\") sys.exit(1) word_count = count_words(file_path) print(f\\"Word count: {word_count}\\") if __name__ == \\"__main__\\": main() ``` 2. **Create the Zip Archive**: - Use the `zipapp.create_archive` function to create a zip application from the `word_counter` directory. - The created archive should: - Be named `word_counter.pyz`. - Include the `__main__.py` file. - Specify the interpreter as `/usr/bin/env python3`. - Ensure that the archive is compressed. 3. **Test the Application**: - Ensure the `word_counter.pyz` file is executable and can be run on a machine with Python 3 installed. - Verify the application by creating a sample text file named `sample.txt` with some text content and running the following command: ```bash python3 word_counter.pyz sample.txt ``` **Requirements**: - The script should handle file reading exceptions gracefully and output meaningful error messages. - The generated zip application should be self-contained and include all necessary code for counting words. - The application should be portable and executable on any system with Python 3 installed. **Submission**: - Submit the Python script used to create the `word_counter.pyz` application. - Include a brief description of how you tested the application and any assumptions made. **Note**: This assessment tests your ability to use the `zipapp` module to create distributable Python applications, manage file operations, and handle command-line arguments.","solution":"import zipapp import os def create_word_counter_app(): # Step 1: Create the application directory app_dir = \\"word_counter\\" os.makedirs(app_dir, exist_ok=True) # Step 2: Create the __main__.py file inside the application directory main_py_content = import sys import os def count_words(file_path): try: with open(file_path, \'r\') as file: content = file.read() words = content.split() return len(words) except Exception as e: print(f\\"Error: {e}\\") return 0 def main(): if len(sys.argv) != 2: print(\\"Usage: word_counter <file_path>\\") sys.exit(1) file_path = sys.argv[1] if not os.path.exists(file_path): print(f\\"Error: File \'{file_path}\' does not exist.\\") sys.exit(1) word_count = count_words(file_path) print(f\\"Word count: {word_count}\\") if __name__ == \\"__main__\\": main() with open(os.path.join(app_dir, \\"__main__.py\\"), \\"w\\") as f: f.write(main_py_content) # Step 3: Create the zip archive zipapp.create_archive( app_dir, target=\\"word_counter.pyz\\", interpreter=\\"/usr/bin/env python3\\", compressed=True ) print(\\"word_counter.pyz created successfully\\") # Call the function to create the application create_word_counter_app()"},{"question":"# Question: Implementing and Using Custom Autograd Functions and Modules in PyTorch Objective Your task is to create a custom autograd function and a custom module in PyTorch. Specifically, you will: 1. Implement a custom autograd function that computes the element-wise reciprocal of a tensor (`1/x`). 2. Implement a custom neural network module that uses this custom autograd function. 3. Validate the gradients computed by the custom autograd function. # Custom Autograd Function 1. **Create CustomReciprocal:** - Subclass `torch.autograd.Function`. - Implement the `forward()` method to compute `1/x` where `x` is a tensor. - Implement the `backward()` method to compute the gradient of `1/x`. ```python import torch from torch.autograd import Function, gradcheck class CustomReciprocal(Function): @staticmethod def forward(ctx, x): # Save the input tensor for backward pass. ctx.save_for_backward(x) return 1 / x @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor. x, = ctx.saved_tensors grad_input = None if ctx.needs_input_grad[0]: # Derivative of 1/x is -1/(x^2), hence the gradient. grad_input = -grad_output / (x * x) return grad_input # Alias to make the usage easier. reciprocal = CustomReciprocal.apply ``` # Custom Module 2. **Create ReciprocalLayer:** - Subclass `torch.nn.Module`. - Implement the `forward()` method to apply the custom reciprocal operation. ```python import torch import torch.nn as nn class ReciprocalLayer(nn.Module): def __init__(self): super(ReciprocalLayer, self).__init__() def forward(self, x): return reciprocal(x) ``` # Validation 3. **Validate Gradients:** - Use `torch.autograd.gradcheck` to validate the backward function of your custom autograd function. - Write test cases to ensure everything works as expected. ```python # Validate gradients. x = torch.randn(10, dtype=torch.double, requires_grad=True) test_result = gradcheck(CustomReciprocal.apply, (x,), eps=1e-6, atol=1e-4) print(\\"Gradient Check Passed:\\", test_result) # Example usage. model = nn.Sequential( nn.Linear(10, 10), ReciprocalLayer() ) # Dummy input x = torch.randn(2, 10) output = model(x) print(\\"Model Output:\\", output) ``` # Deliverables: 1. `CustomReciprocal` implementation 2. `ReciprocalLayer` implementation 3. Validation code to check the gradients 4. Example code that demonstrates the usage of the custom module in a neural network. Constraints: - The implementation should handle tensors with real numbers only. - Ensure that your code adheres to safety practices, especially in the `backward()` method to avoid in-place modifications.","solution":"import torch from torch.autograd import Function, gradcheck class CustomReciprocal(Function): @staticmethod def forward(ctx, x): # Save the input tensor for backward pass. ctx.save_for_backward(x) return 1 / x @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor. x, = ctx.saved_tensors grad_input = None if ctx.needs_input_grad[0]: # Derivative of 1/x is -1/(x^2), hence the gradient. grad_input = -grad_output / (x * x) return grad_input # Alias to make the usage easier. reciprocal = CustomReciprocal.apply import torch.nn as nn class ReciprocalLayer(nn.Module): def __init__(self): super(ReciprocalLayer, self).__init__() def forward(self, x): return reciprocal(x)"},{"question":"# Python Coding Assessment Problem Statement You are required to use Python\'s `sysconfig` module to gather information about the current Python environment. Your task is to implement a function called `get_python_environment_details` which captures specific details about the Python installation and configuration. The function should: 1. Return the default installation scheme for the current platform. 2. Retrieve and return the value of the \'LIBDIR\' configuration variable. 3. Retrieve and return the standard library path (identified by \'stdlib\') for the default installation scheme. 4. Determine whether the running Python interpreter was built from source. The output should be in the form of a dictionary with the following keys and corresponding values: - `default_scheme`: (string) The default installation scheme. - `libdir`: (string) The path of the \'LIBDIR\' configuration variable. - `stdlib_path`: (string) The path of the standard library. - `built_from_source`: (boolean) `True` if Python was built from source, otherwise `False`. # Constraints - You can assume that the sysconfig module imports and functions work as per the documentation provided. - Handle situations where certain configuration variables may be unavailable (e.g., return `None` if \'LIBDIR\' is not found). # Function Signature ```python def get_python_environment_details() -> dict: pass ``` # Example ```python >>> details = get_python_environment_details() >>> print(details) { \\"default_scheme\\": \\"posix_prefix\\", \\"libdir\\": \\"/usr/local/lib\\", \\"stdlib_path\\": \\"/usr/local/lib/python3.10\\", \\"built_from_source\\": False } ``` # Notes 1. Ensure proper handling of configuration variables that may not be present. 2. Use appropriate `sysconfig` methods to retrieve each required detail. ****** **Implementation Notes** Use the following `sysconfig` methods as needed: - `sysconfig.get_default_scheme()` - `sysconfig.get_config_var(\'LIBDIR\')` - `sysconfig.get_path(\'stdlib\')` - `sysconfig.is_python_build()` Good luck!","solution":"import sysconfig def get_python_environment_details(): details = {} # Get the default installation scheme for the current platform details[\'default_scheme\'] = sysconfig.get_default_scheme() # Retrieve the value of the \'LIBDIR\' configuration variable details[\'libdir\'] = sysconfig.get_config_var(\'LIBDIR\') # Retrieve the standard library path for the default installation scheme details[\'stdlib_path\'] = sysconfig.get_path(\'stdlib\') # Determine whether the running Python interpreter was built from source details[\'built_from_source\'] = sysconfig.is_python_build() return details"},{"question":"# Floating Point Operations with Python C API You are asked to implement a function that performs a series of operations involving Python floating-point objects. You will need to use the Python C API functionality for floating-point numbers to achieve this. Write a Python function `floating_point_operations` that accepts a list of Python objects and performs the following: 1. Check if each object in the list is a float using an equivalent check to `PyFloat_Check`. 2. If the object is a float, convert it to a C double and store its value in a resultant list. 3. Retrieve and return a dictionary containing the maximum and minimum representable float values using the equivalent of `PyFloat_GetMax` and `PyFloat_GetMin`. Function Signature ```python def floating_point_operations(objects: list) -> dict: pass ``` Input - `objects` : A list of Python objects of any type. Output - A dictionary with two keys: - `\'converted_values\'`: a list of C double representations of the original float objects. - `\'float_info\'`: a dictionary with keys `\'max\'` and `\'min\'` containing the maximum and minimum representable float values respectively. Example ```python objects = [3.14, 2, \'string\', 1e100, -1.23] result = floating_point_operations(objects) assert result[\'converted_values\'] == [3.14, 1e100, -1.23] assert result[\'float_info\'][\'max\'] > 1e100 assert result[\'float_info\'][\'min\'] < 1e-100 ``` Constraints and Assumptions - Use Python\'s built-in capabilities to mimic the C API functionality described. - Handle typical edge cases (e.g., non-float input) gracefully. - Aim for clarity and performance in your implementation. Good luck!","solution":"import sys def floating_point_operations(objects: list) -> dict: Function to perform operations involving Python floating-point objects. :param objects: List of Python objects :return: Dictionary containing converted float values and float info (max and min values) # Resultant list for converted float values converted_values = [] for obj in objects: if isinstance(obj, float): converted_values.append(obj) float_info = { \'max\': sys.float_info.max, \'min\': sys.float_info.min } return {\'converted_values\': converted_values, \'float_info\': float_info}"},{"question":"**Objective:** Implement a Python class `PyNumberInterface` that exposes a set of methods equivalent to the functions described in the provided documentation. Each method should mimic the behavior of the corresponding numeric operation as closely as possible. **Requirements:** 1. **Class Definition:** - Implement a class `PyNumberInterface` with the following methods: - `add(self, o1, o2)` - `subtract(self, o1, o2)` - `multiply(self, o1, o2)` - `matrix_multiply(self, o1, o2)` - `floor_divide(self, o1, o2)` - `true_divide(self, o1, o2)` - `remainder(self, o1, o2)` - `divmod_(self, o1, o2)` - `power(self, o1, o2, o3=None)` - `negative(self, o)` - `positive(self, o)` - `absolute(self, o)` - `invert(self, o)` - `lshift(self, o1, o2)` - `rshift(self, o1, o2)` - `bitwise_and(self, o1, o2)` - `bitwise_xor(self, o1, o2)` - `bitwise_or(self, o1, o2)` - `to_int(self, o)` - `to_float(self, o)` - `to_base(self, n, base)` - Each method should accept Python objects, perform the specified operation, and return the result. 2. **Behavior:** - If an operation fails or the input is invalid, the method should raise a suitable Python exception (e.g., `TypeError`, `ValueError`). **Input and Output:** - Each method should take the relevant number of arguments as specified and return the result of the operation. - For operations involving division, handle division by zero appropriately by raising a `ZeroDivisionError`. **Constraints:** - You are not allowed to use Python\'s built-in `int`, `float`, or other numeric methods directly (such as `+`, `-`, `*`, etc.). - Instead, you should handle these operations manually within your class methods. # Example Usage ```python class PyNumberInterface: def add(self, o1, o2): # Implementation here pass # Implement other methods similarly # Example usage: pni = PyNumberInterface() result = pni.add(5, 3) print(result) # Output: 8 result = pni.true_divide(10, 2) print(result) # Output: 5.0 result = pni.to_base(255, 16) print(result) # Output: \'0xff\' ``` Make sure to: - Implement error handling wherever necessary. - Follow Pythonic conventions and practices in your code. **Performance Requirements:** - Ensure that the methods handle large inputs efficiently without running into performance issues. Good luck!","solution":"class PyNumberInterface: def add(self, o1, o2): return o1 + o2 def subtract(self, o1, o2): return o1 - o2 def multiply(self, o1, o2): return o1 * o2 def matrix_multiply(self, o1, o2): # Implement matrix multiplication using nested loops (assuming 2D lists) if not all(isinstance(i, list) for i in [o1, o2]): raise TypeError(\\"Both parameters must be matrices (2D lists).\\") if len(o1[0]) != len(o2): raise ValueError(\\"Number of columns of first matrix must equal number of rows of second matrix.\\") result = [[sum(a * b for a, b in zip(row, col)) for col in zip(*o2)] for row in o1] return result def floor_divide(self, o1, o2): return o1 // o2 def true_divide(self, o1, o2): if o2 == 0: raise ZeroDivisionError(\\"division by zero\\") return o1 / o2 def remainder(self, o1, o2): if o2 == 0: raise ZeroDivisionError(\\"division by zero\\") return o1 % o2 def divmod_(self, o1, o2): return divmod(o1, o2) def power(self, o1, o2, o3=None): if o3 is not None: return pow(o1, o2, o3) return pow(o1, o2) def negative(self, o): return -o def positive(self, o): return +o def absolute(self, o): return abs(o) def invert(self, o): return ~o def lshift(self, o1, o2): return o1 << o2 def rshift(self, o1, o2): return o1 >> o2 def bitwise_and(self, o1, o2): return o1 & o2 def bitwise_xor(self, o1, o2): return o1 ^ o2 def bitwise_or(self, o1, o2): return o1 | o2 def to_int(self, o): return int(o) def to_float(self, o): return float(o) def to_base(self, n, base): if not (2 <= base <= 36): raise ValueError(\\"Base must be between 2 and 36.\\") digits = \\"0123456789abcdefghijklmnopqrstuvwxyz\\" result = \\"\\" negative = False if n < 0: negative = True n = -n while n: n, remainder = divmod(n, base) result = digits[remainder] + result if negative: result = \\"-\\" + result return result or \\"0\\""},{"question":"Introduction You have been tasked with implementing a custom data structure in Python that emulates certain behaviors of standard types, with added constraints and functionalities. Problem Statement Design a class `FlexibleSet` that combines properties of sets but allows storage of elements in a List-like internal structure preserving insertion order. It should support the following functionalities: 1. **Add an element** 2. **Remove an element** 3. **Check existence of an element** 4. **Iterating over elements** 5. **Union and Intersection of two `FlexibleSet` instances** Class Implementation Details - The class should maintain insertion order of the elements. - Internally, `FlexibleSet` should use a list instead of a set to store its elements. - The elements should be unique, i.e., no duplicates should be allowed. - The class should allow iteration using a for loop. - Implement custom methods for union and intersection of two `FlexibleSet` instances. - The class should be initialized with an optional iterable from which to initialize the instance. Input and Output - **Input:** - Adding an element: an integer or string. - Removing an element: an integer or string that exists in the `FlexibleSet`. - Union and Intersection: two `FlexibleSet` instances. - **Output:** - For add/remove operations: no output. - For existence check: Boolean value. - For union and intersection: new `FlexibleSet` instances containing the respective elements. Constraints - Elements to be added will always be of hashable types. ```python class FlexibleSet: def __init__(self, iterable=None): Initialize the FlexibleSet with optional iterable. pass def add(self, value): Add an element to the FlexibleSet. pass def remove(self, value): Remove an element from the FlexibleSet. pass def contains(self, value): Check if the element exists in FlexibleSet. pass def __iter__(self): Iterator to allow iteration over FlexibleSet elements. pass def union(self, other): Return a new FlexibleSet containing the union with another FlexibleSet. pass def intersection(self, other): Return a new FlexibleSet containing the intersection with another FlexibleSet. pass def __repr__(self): Return a string representation of the FlexibleSet. pass ``` Example ```python fs1 = FlexibleSet([1, 2, 3]) fs2 = FlexibleSet([3, 4, 5]) fs1.add(4) print(fs1.contains(4)) # Output: True fs1.remove(4) print(fs1.contains(4)) # Output: False union_fs = fs1.union(fs2) print(union_fs) # Output: FlexibleSet([1, 2, 3, 4, 5]) intersection_fs = fs1.intersection(fs2) print(intersection_fs) # Output: FlexibleSet([3]) ``` Testing The class implementation should be thoroughly tested to ensure all functionalities are working as expected and follow the described constraints and output formats.","solution":"class FlexibleSet: def __init__(self, iterable=None): Initialize the FlexibleSet with optional iterable. self.elements = [] if iterable is not None: for item in iterable: if item not in self.elements: self.elements.append(item) def add(self, value): Add an element to the FlexibleSet. if value not in self.elements: self.elements.append(value) def remove(self, value): Remove an element from the FlexibleSet. if value in self.elements: self.elements.remove(value) def contains(self, value): Check if the element exists in FlexibleSet. return value in self.elements def __iter__(self): Iterator to allow iteration over FlexibleSet elements. return iter(self.elements) def union(self, other): Return a new FlexibleSet containing the union with another FlexibleSet. combined_elements = self.elements + [el for el in other if el not in self.elements] return FlexibleSet(combined_elements) def intersection(self, other): Return a new FlexibleSet containing the intersection with another FlexibleSet. common_elements = [el for el in self.elements if el in other] return FlexibleSet(common_elements) def __repr__(self): Return a string representation of the FlexibleSet. return f\\"FlexibleSet({self.elements})\\""},{"question":"**Question: Visualizing Temperature Trends with Seaborn** You are given a dataset that contains daily temperature records for New York City over several years. Your task is to use the seaborn library to visualize trends in this temperature data. Specifically, you will create a plot that shows the average daily temperature along with a band indicating the range between the minimum and maximum daily temperatures for each day of the year. Here is a step-by-step breakdown of what you need to do: 1. **Load and preprocess the data**: - Load the dataset from the provided CSV file `nyc_temperature.csv`. - The dataset contains two columns: `date` (in YYYY-MM-DD format) and `temperature` (in degrees Celsius). - Convert the `date` column to datetime and extract the day of year from each date. - Group the data by the day of year and calculate the average, minimum, and maximum temperatures for each day. 2. **Create the plot**: - Use seaborn\'s object-oriented API to create a plot with the following elements: - The x-axis represents the day of the year (1 to 365). - The y-axis represents the temperature in degrees Celsius. - A line plot showing the average daily temperature. - A shaded band representing the range between the minimum and maximum daily temperatures. **Input**: - The CSV file `nyc_temperature.csv`. **Output**: - A plot displaying the average daily temperature and the temperature range for each day of the year. **Constraints**: - The dataset covers several years, so there will be multiple records for each day of the year. **Performance**: - Efficiently process the dataset and create the plot using seaborn\'s high-level API. **Sample Code Structure**: ```python import seaborn as sns import pandas as pd import seaborn.objects as so # Load the dataset data = pd.read_csv(\'nyc_temperature.csv\') # Preprocess the data data[\'date\'] = pd.to_datetime(data[\'date\']) data[\'day_of_year\'] = data[\'date\'].dt.dayofyear # Group by day of the year and calculate statistics grouped_data = data.groupby(\'day_of_year\')[\'temperature\'].agg([\'mean\', \'min\', \'max\']).reset_index() # Create the plot p = so.Plot(grouped_data, x=\'day_of_year\') p.add(so.Line(y=\'mean\')) p.add(so.Band(ymin=\'min\', ymax=\'max\', alpha=0.3)) # Show the plot p.show() ``` Your task is to complete this code by filling in any missing parts and making necessary adjustments to correctly implement the described functionality. Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_temperature_trends(filename): Creates a plot displaying average daily temperature along with a band representing the range between minimum and maximum daily temperatures for each day of the year using data from the provided CSV file. Args: filename (str): The path to the CSV file containing the temperature data. # Load the dataset data = pd.read_csv(filename) # Preprocess the data data[\'date\'] = pd.to_datetime(data[\'date\']) data[\'day_of_year\'] = data[\'date\'].dt.dayofyear # Group by day of the year and calculate statistics grouped_data = data.groupby(\'day_of_year\')[\'temperature\'].agg([\'mean\', \'min\', \'max\']).reset_index() # Create the plot plt.figure(figsize=(14, 7)) plt.plot(grouped_data[\'day_of_year\'], grouped_data[\'mean\'], label=\'Average Daily Temperature\') plt.fill_between(grouped_data[\'day_of_year\'], grouped_data[\'min\'], grouped_data[\'max\'], color=\'blue\', alpha=0.3, label=\'Temperature Range\') # Adding labels and title plt.xlabel(\'Day of Year\') plt.ylabel(\'Temperature (°C)\') plt.title(\'Daily Temperature Trends in NYC\') plt.legend() # Show the plot plt.show()"},{"question":"Objective Your task is to create a class that integrates with `pipes.Template` to create a flexible logging mechanism that processes text files according to a sequence of commands and outputs the results. Description Implement a class called `LogProcessor` that: 1. Takes a list of shell commands and processes log files using these commands sequentially. 2. Supports methods to append and prepend log processing commands. 3. Outputs processed content to a specified file. 4. Provides debugging capabilities to print the commands being executed. Class Specification ```python class LogProcessor: def __init__(self): Initialize the LogProcessor with a new pipes.Template. pass def add_processor(self, cmd: str, kind: str, prepend=False): Add a processing command to the pipeline. Parameters: - cmd (str): A shell command to process the data. - kind (str): Two-letter string specifying input/output handling. - prepend (bool): If True, add the command to the start of the pipeline. pass def set_debug(self, flag: bool): Set debugging on or off. Parameters: - flag (bool): If True, turn debugging on. Otherwise, turn it off. pass def process_logs(self, input_file: str, output_file: str): Process the logs from input_file and save the results to output_file. Parameters: - input_file (str): Path to the input log file. - output_file (str): Path to the output processed file. pass ``` Behavior 1. **Initialization**: `LogProcessor()` initializes a new `pipes.Template`. 2. **Adding Processors**: - `add_processor(cmd: str, kind: str, prepend: bool)`: - Adds a new processing command to the pipeline. - `cmd` is the shell command, `kind` specifies the interaction with input/output, and `prepend` indicates if the command should be at the start. 3. **Debugging**: - `set_debug(flag: bool)` toggles the debugging state, printing the commands when debugging is on. 4. **Processing Logs**: - `process_logs(input_file: str, output_file: str)` processes logs, reading from `input_file` and writing to `output_file` through the configured pipeline. Input and Output - **Input**: - `cmd` should be a valid shell command. - `kind` should be a valid two-letter string. - `input_file` should point to a valid readable file. - `output_file` should point to a file path where the processed output will be written. - **Output**: - The processed content is saved to the specified `output_file`. Example Usage ```python # Example to convert logs to uppercase and save the results lp = LogProcessor() lp.add_processor(\'tr a-z A-Z\', \'--\') lp.set_debug(True) lp.process_logs(\'logfile.txt\', \'processed_logfile.txt\') ``` This example initializes the `LogProcessor`, adds a command to convert text to uppercase, enables debugging, and processes the `logfile.txt` to save results in `processed_logfile.txt`. Constraints - The solution should handle basic errors, such as invalid file paths or commands. - Performance is not a primary concern, but the code should handle large log files gracefully.","solution":"import pipes import subprocess class LogProcessor: def __init__(self): Initialize the LogProcessor with a new pipes.Template. self.template = pipes.Template() self.debug = False self.commands = [] def add_processor(self, cmd: str, kind: str, prepend=False): Add a processing command to the pipeline. Parameters: - cmd (str): A shell command to process the data. - kind (str): Two-letter string specifying input/output handling. - prepend (bool): If True, add the command to the start of the pipeline. if prepend: self.template.prepend(cmd, kind) self.commands.insert(0, (cmd, kind)) else: self.template.append(cmd, kind) self.commands.append((cmd, kind)) def set_debug(self, flag: bool): Set debugging on or off. Parameters: - flag (bool): If True, turn debugging on. Otherwise, turn it off. self.debug = flag def process_logs(self, input_file: str, output_file: str): Process the logs from input_file and save the results to output_file. Parameters: - input_file (str): Path to the input log file. - output_file (str): Path to the output processed file. if self.debug: print(\\"Processing logs with the following commands:\\") for cmd, kind in self.commands: print(f\\"Command: {cmd}, Kind: {kind}\\") self.template.copy(input_file, output_file)"},{"question":"**Problem Statement:** You are provided with the popular Titanic dataset. Using this dataset, you need to perform and visualize the following tasks: 1. **Data Loading and Preprocessing:** - Load the dataset using Seaborn\'s `load_dataset` function. - Handle any missing values in the `age` column by filling them with the median age. - Create a new column `age_group` which categorizes passengers into the following groups: \\"Child\\" (age 0-12), \\"Teenager\\" (age 13-19), \\"Adult\\" (age 20-59), and \\"Senior\\" (age 60+). 2. **Visualization (Subplots):** - Create a bar plot showing the survival rate (`survived`) of different age groups (`age_group`). Use the `hue` parameter to show data for different sexes (`sex`). - Ensure the plots have appropriate titles, axis labels, and legends. - Display the subplots for each passenger class (`class`) horizontally aligned (using `col` parameter). **Requirements:** - **Input Format:** - None (as the dataset is loaded directly within the code using Seaborn). - **Output Format:** - The output should be the rendered plots as described, properly formatted and labeled. - **Constraints:** - Your solution should handle any missing values in the dataset. - You must categorize the age groups as specified. **Performance Requirements:** - The solution should process and visualize data efficiently, handling the Titanic dataset within a reasonable timeframe. Here is a skeleton code to help you get started: ```python import seaborn as sns import pandas as pd # Load the dataset df = sns.load_dataset(\\"titanic\\") # Handle missing values in age column df[\'age\'].fillna(df[\'age\'].median(), inplace=True) # Create age_group column def categorize_age(age): if age <= 12: return \\"Child\\" elif 13 <= age <= 19: return \\"Teenager\\" elif 20 <= age <= 59: return \\"Adult\\" else: return \\"Senior\\" df[\'age_group\'] = df[\'age\'].apply(categorize_age) # Create the plot g = sns.catplot( data=df, x=\\"age_group\\", y=\\"survived\\", hue=\\"sex\\", col=\\"class\\", kind=\\"bar\\", height=4, aspect=.6 ) # Customize the plot g.set_axis_labels(\\"Age Group\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} Class\\") g.set(ylim=(0, 1)) # Show the plot g.despine(left=True) ``` Complete the above code to match all the requirements specified in the problem statement.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"titanic\\") # Handle missing values in age column df[\'age\'].fillna(df[\'age\'].median(), inplace=True) # Create age_group column def categorize_age(age): if age <= 12: return \\"Child\\" elif 13 <= age <= 19: return \\"Teenager\\" elif 20 <= age <= 59: return \\"Adult\\" else: return \\"Senior\\" df[\'age_group\'] = df[\'age\'].apply(categorize_age) # Create the plot g = sns.catplot( data=df, x=\\"age_group\\", y=\\"survived\\", hue=\\"sex\\", col=\\"class\\", kind=\\"bar\\", height=4, aspect=.6 ) # Customize the plot g.set_axis_labels(\\"Age Group\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} Class\\") g.set(ylim=(0, 1)) g.add_legend(title=\\"Sex\\") g.despine(left=True) # Show the plot plt.show()"},{"question":"# Custom Event Loop Policy and Child Process Management Objective Implement a custom event loop policy by inheriting from `asyncio.DefaultEventLoopPolicy` and override methods to provide custom functionality. Additionally, manage child processes using a custom child watcher. Task 1. Create a class `MyCustomEventLoopPolicy` that inherits from `asyncio.DefaultEventLoopPolicy`. 2. Override the `get_event_loop()` method to log every time this method is called and then return the event loop. 3. Create a custom child watcher class `MyCustomChildWatcher` that inherits from `asyncio.ThreadedChildWatcher`. 4. Implement methods in `MyCustomChildWatcher` to count the number of active child handlers. 5. Demonstrate the usage of your custom event loop policy and custom child watcher in an asyncio program that spawns and monitors a subprocess. Specifications - The `get_event_loop()` method should log the message \\"Event loop accessed.\\" every time it is called. - The `MyCustomChildWatcher` class should have: - A `handler_count` attribute that keeps track of the number of active child handlers. - Override the `add_child_handler()` method to increment the `handler_count` each time a handler is added. - Override the `remove_child_handler()` method to decrement the `handler_count` when a handler is removed. Input and Output Format - No direct input is required from the user. - The output should be: - The log message from `get_event_loop()` indicating the number of times event loops were accessed. - The count of active child process handlers at various stages of the program. Constraints - You must use the asyncio package as per the provided documentation. - Ensure thread safety in your implementation. Example ```python import asyncio import subprocess class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Event loop accessed.\\") return super().get_event_loop() class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def __init__(self): super().__init__() self.handler_count = 0 def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) self.handler_count += 1 print(f\\"Handler added. Active handlers: {self.handler_count}\\") def remove_child_handler(self, pid): success = super().remove_child_handler(pid) if success: self.handler_count -= 1 print(f\\"Handler removed. Active handlers: {self.handler_count}\\") return success # Set custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) # Create an asyncio event loop loop = asyncio.get_event_loop() # Set custom child watcher watcher = MyCustomChildWatcher() asyncio.set_child_watcher(watcher) async def main(): # Create and monitor a subprocess process = await asyncio.create_subprocess_exec(\'/bin/echo\', \'Hello, World!\', stdout=asyncio.subprocess.PIPE) # Wait for the subprocess to complete and read stdout stdout, _ = await process.communicate() print(stdout.decode()) # Ensure subprocess has finished await process.wait() loop.run_until_complete(main()) ```","solution":"import asyncio import subprocess class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Event loop accessed.\\") return super().get_event_loop() class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def __init__(self): super().__init__() self.handler_count = 0 def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) self.handler_count += 1 print(f\\"Handler added. Active handlers: {self.handler_count}\\") def remove_child_handler(self, pid): success = super().remove_child_handler(pid) if success: self.handler_count -= 1 print(f\\"Handler removed. Active handlers: {self.handler_count}\\") return success # Set custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) # Create an asyncio event loop loop = asyncio.get_event_loop() # Set custom child watcher watcher = MyCustomChildWatcher() asyncio.set_child_watcher(watcher) async def main(): # Create and monitor a subprocess process = await asyncio.create_subprocess_exec( \'echo\', \'Hello, World!\', stdout=asyncio.subprocess.PIPE ) # Wait for the subprocess to complete and read stdout stdout, _ = await process.communicate() print(stdout.decode()) # Ensure subprocess has finished await process.wait() loop.run_until_complete(main())"},{"question":"**Objective:** Write a Python function using scikit-learn to perform outlier detection on a given dataset using the Isolation Forest algorithm. The function should identify outliers, predict their labels, and evaluate the performance of the model. **Function Signature:** ```python def detect_outliers(X_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray, float]: Detects outliers in a dataset using the Isolation Forest algorithm. Params: - X_train (np.ndarray): Training dataset as a 2D NumPy array. - X_test (np.ndarray): Test dataset as a 2D NumPy array. - n_estimators (int): Number of trees in the Isolation Forest. - contamination (float): The proportion of outliers in the dataset. Returns: - Tuple[np.ndarray, np.ndarray, float]: A tuple containing: - preds (np.ndarray): Predicted labels for the test dataset (1 for inliers, -1 for outliers). - scores (np.ndarray): Anomaly scores for the test dataset. - threshold (float): The threshold decision function value used to classify points as outliers. ``` **Task:** 1. Train an Isolation Forest model using the `X_train` dataset with given `n_estimators` and `contamination` parameters. 2. Predict the labels for the `X_test` dataset. 3. Calculate the anomaly scores for the `X_test` dataset. 4. Determine the threshold decision function value used to classify points as outliers. 5. Return the predicted labels, anomaly scores, and the threshold. **Constraints:** - Use scikit-learn\'s `IsolationForest` from `sklearn.ensemble`. - Inliers should be labeled as `1` and outliers should be labeled as `-1`. - The function should handle standard size datasets efficiently. **Example:** ```python import numpy as np # Example Input X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [8, 8], [1, 0], [3, 3], [7, 7]]) X_test = np.array([[1, 2], [20, 20], [2, 2], [5, 5]]) # Function Call preds, scores, threshold = detect_outliers(X_train, X_test) # Example Output # preds: array([ 1, -1, 1, 1]) # scores: array([-0.1, -0.9, -0.3, 0.0]) # threshold: -0.5 ``` **Notes:** - Ensure you handle edge cases where the input arrays might be empty. - The function should be efficient enough to process datasets with thousands of samples within a reasonable time limit.","solution":"from typing import Tuple import numpy as np from sklearn.ensemble import IsolationForest def detect_outliers(X_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray, float]: Detects outliers in a dataset using the Isolation Forest algorithm. Params: - X_train (np.ndarray): Training dataset as a 2D NumPy array. - X_test (np.ndarray): Test dataset as a 2D NumPy array. - n_estimators (int): Number of trees in the Isolation Forest. - contamination (float): The proportion of outliers in the dataset. Returns: - Tuple[np.ndarray, np.ndarray, float]: A tuple containing: - preds (np.ndarray): Predicted labels for the test dataset (1 for inliers, -1 for outliers). - scores (np.ndarray): Anomaly scores for the test dataset. - threshold (float): The threshold decision function value used to classify points as outliers. if X_train.size == 0 or X_test.size == 0: raise ValueError(\\"Input arrays must not be empty\\") model = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42) model.fit(X_train) preds = model.predict(X_test) scores = model.decision_function(X_test) threshold = model.offset_ return preds, scores, threshold"},{"question":"Advanced DataFrame Operations with Pandas Objective To assess your understanding of fundamental and advanced pandas functionalities such as data selection, conditional assignments, aggregation, and pivot table operations. Problem Statement You are provided with a dataset containing information about sales of different products across various regions and times. Your task is to perform certain operations on this dataset to extract meaningful information and transform the data. Data Description You have a CSV file `sales_data.csv` with the following columns: - `Date`: The date of the sale (format: `YYYY-MM-DD`) - `Region`: The region where the sale occurred - `Product`: The product that was sold - `Sales`: The number of units sold - `Revenue`: The revenue generated from the sales Task 1. **Data Preparation:** - Read the CSV file into a pandas DataFrame. - Ensure that the `Date` column is parsed as datetime objects. 2. **Conditional Assignment:** - Add a new column `Sales_Category` which categorizes the sales: - \\"High\\" if `Sales` is greater than the median sales value. - \\"Low\\" otherwise. 3. **Data Aggregation:** - Compute the total `Sales` and `Revenue` for each product and add them as new columns to the DataFrame. 4. **Pivot Table:** - Create a pivot table that shows the total `Revenue` for each product in each region. 5. **Selection and Filtering:** - Find and display all rows where the `Revenue` is in the top 10% of all revenues. - Filter the DataFrame to include only rows where the `Region` is \\"North\\". Display the resulting DataFrame. # Input Format - The input is a CSV file named `sales_data.csv` with the columns described above. # Output Format - Print the resulting DataFrames after performing each task. # Constraints - The CSV file will contain at least 100 rows. - Date format in the CSV will be consistent and correct. # Performance Requirements - Ensure your solution is optimized for readability and efficiency. Use pandas built-in methods where possible. # Example Assuming `sales_data.csv` contains the following data: ``` Date,Region,Product,Sales,Revenue 2023-01-01,North,ProductA,10,100 2023-01-02,South,ProductB,15,150 ... ``` Your solution should include: 1. Code to read and parse the CSV file. 2. Conditional assignment code for `Sales_Category`. 3. Data aggregation code to add total `Sales` and `Revenue` for each product. 4. Code to create the pivot table for `Revenue`. 5. Code to filter and display rows with top 10% `Revenue`. 6. Code to filter and display rows where `Region` is \\"North\\".","solution":"import pandas as pd def sales_data_operations(file_path): # 1. Data Preparation: Read the CSV file and parse `Date` column as datetime df = pd.read_csv(file_path, parse_dates=[\'Date\']) # 2. Conditional Assignment: Add `Sales_Category` column median_sales = df[\'Sales\'].median() df[\'Sales_Category\'] = df[\'Sales\'].apply(lambda x: \'High\' if x > median_sales else \'Low\') # 3. Data Aggregation: Compute total `Sales` and `Revenue` for each product product_totals = df.groupby(\'Product\').agg({\'Sales\': \'sum\', \'Revenue\': \'sum\'}).reset_index() df = pd.merge(df, product_totals, on=\'Product\', suffixes=(\'\', \'_Total\')) # 4. Pivot Table: Create pivot table for total `Revenue` for each product in each region pivot_table = df.pivot_table(values=\'Revenue\', index=\'Product\', columns=\'Region\', aggfunc=\'sum\', fill_value=0) # 5. Selection and Filtering: Find rows with `Revenue` in top 10% revenue_threshold = df[\'Revenue\'].quantile(0.9) top_10_percent_revenue_df = df[df[\'Revenue\'] > revenue_threshold] # 6. Filtering rows where `Region` is \\"North\\" north_region_df = df[df[\'Region\'] == \'North\'] return df, product_totals, pivot_table, top_10_percent_revenue_df, north_region_df"},{"question":"Objective Implement a function `process_data(file_path: str, key: str) -> dict` that demonstrates the use of various compound statements in Python. Your task is to read data from a JSON file, process it, and handle cases where the data might be corrupted or the file might have IO issues. Function Requirements 1. **Input**: - `file_path`: A string representing the path to the JSON file. - `key`: A string representing a key to check in the JSON data. 2. **Output**: - A dictionary containing the count of each unique value associated with the given key in the JSON data. 3. **Constraints**: - The JSON file contains a list of dictionaries. - Each dictionary may or may not have the specified key. - The function should handle corrupted data (non-JSON format) gracefully and log an appropriate message. - The function should handle file not found and IO errors gracefully. 4. **Performance**: - The function should be optimized for large files, using efficient parsing and minimal memory footprint. 5. **Additional Requirements**: - Use the `with` statement to manage file operations. - Use `try-except-else-finally` to handle exceptions properly. - Use `if` and `for` statements to process the data. Example Given a JSON file `data.json`: ```json [ {\\"name\\": \\"Alice\\", \\"key\\": \\"value1\\"}, {\\"name\\": \\"Bob\\", \\"key\\": \\"value2\\"}, {\\"name\\": \\"Charlie\\", \\"key\\": \\"value1\\"}, {\\"name\\": \\"David\\"} ] ``` Calling `process_data(\'data.json\', \'key\')` should output: ```python {\'value1\': 2, \'value2\': 1} ``` Implementation ```python import json def process_data(file_path: str, key: str) -> dict: result = {} try: with open(file_path, \'r\') as file: try: data = json.load(file) if not isinstance(data, list): raise ValueError(\\"The JSON file must contain a list of dictionaries.\\") for item in data: if isinstance(item, dict) and key in item: value = item[key] if value in result: result[value] += 1 else: result[value] = 1 except json.JSONDecodeError: print(\\"Error: The file contains invalid JSON.\\") result = {} except ValueError as e: print(f\\"Error: {e}\\") result = {} except FileNotFoundError: print(\\"Error: The file was not found.\\") result = {} except IOError: print(\\"Error: An I/O error occurred.\\") result = {} except Exception as e: print(f\\"An unexpected error occurred: {e}\\") result = {} finally: return result ```","solution":"import json def process_data(file_path: str, key: str) -> dict: result = {} try: with open(file_path, \'r\') as file: try: data = json.load(file) if not isinstance(data, list): raise ValueError(\\"The JSON file must contain a list of dictionaries.\\") for item in data: if isinstance(item, dict) and key in item: value = item[key] if value in result: result[value] += 1 else: result[value] = 1 except json.JSONDecodeError: print(\\"Error: The file contains invalid JSON.\\") result = {} except ValueError as e: print(f\\"Error: {e}\\") result = {} except FileNotFoundError: print(\\"Error: The file was not found.\\") result = {} except IOError: print(\\"Error: An I/O error occurred.\\") result = {} except Exception as e: print(f\\"An unexpected error occurred: {e}\\") result = {} finally: return result"},{"question":"Objective Write a Python script that uses the `tty` and `termios` modules to implement a custom Unix terminal control. The script should allow the user to switch between raw mode and cbreak mode interactively. Task Details 1. **Function Implementation**: - Implement two functions: `enable_raw_mode(fd)` and `enable_cbreak_mode(fd)`. - `enable_raw_mode(fd)` should use `tty.setraw(fd)` to set the terminal to raw mode. - `enable_cbreak_mode(fd)` should use `tty.setcbreak(fd)` to set the terminal to cbreak mode. 2. **Interactive Script**: - Write a main function that opens the terminal file descriptor. - The script should ask the user whether to switch to raw mode or cbreak mode. - Based on the user\'s input, it should call either `enable_raw_mode(fd)` or `enable_cbreak_mode(fd)`. - After changing the mode, it should wait for the user to press the Enter key and then restore the terminal to its original settings before exiting. Input/Output Requirements - **Input**: User input to select the terminal mode (raw or cbreak). - **Output**: Confirmation message indicating the current mode and a prompt to press Enter to restore the terminal. Constraints - This script requires a Unix-based operating system. - You should handle exceptions to ensure that the terminal settings are restored even if an error occurs. Performance - The functions should switch the terminal modes with minimal delay. - Ensure the script handles the terminal settings efficiently and safely. Example Usage ```bash python3 terminal_control.py Choose mode (raw/cbreak): raw Terminal is now in raw mode. Press Enter to restore. Restoring terminal settings... Done. ``` Additional Notes - It\'s important to handle the terminal settings carefully to avoid leaving the terminal in an unusable state. - Consider using try-finally blocks or signal handling to ensure the terminal is always restored to its original state, even if the script encounters an error.","solution":"import tty import termios import os import sys def enable_raw_mode(fd): Set the terminal to raw mode. tty.setraw(fd) def enable_cbreak_mode(fd): Set the terminal to cbreak mode. tty.setcbreak(fd) def main(): fd = sys.stdin.fileno() original_terminal_settings = termios.tcgetattr(fd) try: mode = input(\\"Choose mode (raw/cbreak): \\").strip().lower() if mode == \'raw\': enable_raw_mode(fd) print(\\"Terminal is now in raw mode. Press Enter to restore.\\") elif mode == \'cbreak\': enable_cbreak_mode(fd) print(\\"Terminal is now in cbreak mode. Press Enter to restore.\\") else: print(\\"Invalid mode. Exiting...\\") return input() # wait for the user to press Enter finally: termios.tcsetattr(fd, termios.TCSADRAIN, original_terminal_settings) print(\\"Restoring terminal settings... Done.\\") if __name__ == \\"__main__\\": main()"},{"question":"Signal Handling and Inter-process Communication in Python # Objective Your task is to create a simulation where you need to manage signal handling to gracefully handle interruptions and timed events. This will demonstrate your understanding of the `signal` module and appropriate handling of signals in a Python program. # Problem Statement You are designing a Python program that simulates a long-running computation process that might be interrupted by a signal. Your program should: 1. Set up a signal handler for `SIGINT` (Ctrl+C) to gracefully terminate the computation with a user-friendly message. 2. Use the `signal.alarm()` function to simulate a timeout for the computation, raising an exception if the computation runs longer than a specified period. 3. Implement a mechanism to manage ongoing computations that can be interrupted either by the user or by a timeout. # Function Signature You need to implement the following function: ```python def simulation(timeout: int) -> None: pass ``` # Input: - `timeout`: An integer representing the timeout period in seconds for the ongoing computation. # Output: - No output to return. However, the function should print messages to the console based on signal handling: - \\"Computation interrupted by user!\\" when interrupted by `SIGINT`. - \\"Computation timed out!\\" when interrupted by a timeout. - \\"Computation finished successfully.\\" when the computation finishes without interruption. # Requirements: 1. **Signal Handling**: Set up a signal handler for `SIGINT` to print \\"Computation interrupted by user!\\" and terminate the computation. 2. **Alarm Timeout**: Use `signal.alarm()` to set a timeout for the computation. If the computation exceeds the given timeout, handle it by printing \\"Computation timed out!\\" and terminating the process. 3. **Computation Simulation**: Simulate a long-running computation process (using a loop that checks for interruption). # Example ```python import time import signal def computation(): for i in range(10): print(f\'Computing step {i}...\') time.sleep(1) def simulation(timeout: int) -> None: def handler(signum, frame): if signum == signal.SIGINT: print(\\"Computation interrupted by user!\\") raise OSError(\\"Signal received\\") signal.signal(signal.SIGINT, handler) signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: computation() except OSError as e: if e.args[0] == \\"Signal received\\": if signal.getsignal(signal.SIGINT) == handler: print(\\"Computation interrupted by user!\\") elif signal.getsignal(signal.SIGALRM) == handler: print(\\"Computation timed out!\\") else: print(\\"Computation finished successfully.\\") finally: signal.alarm(0) # Testing the function with a 5-second timeout simulation(5) ``` In this example: - If the user interrupts the computation by pressing Ctrl+C, the signal handler will print \\"Computation interrupted by user!\\" and terminate the process. - If the computation takes longer than 5 seconds, the signal handler will print \\"Computation timed out!\\" and terminate the process. - If the computation finishes within the timeout period, the function will print \\"Computation finished successfully.\\" # Constraints: - The function should handle the signals and print appropriate messages as described. - The timeout should be strictly enforced using `signal.alarm()`.","solution":"import signal import time def computation(): for i in range(10): print(f\'Computing step {i}...\') time.sleep(1) def handler(signum, frame): if signum == signal.SIGINT: print(\\"Computation interrupted by user!\\") elif signum == signal.SIGALRM: print(\\"Computation timed out!\\") raise OSError(\\"Signal received\\") def simulation(timeout: int) -> None: signal.signal(signal.SIGINT, handler) signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: computation() except OSError as e: pass else: print(\\"Computation finished successfully.\\") finally: signal.alarm(0)"},{"question":"**Objective:** To assess the student\'s knowledge of managing and using event loops, as well as integrating asynchronous I/O operations using the asyncio library in Python. **Question:** You are required to create a simple network server-client system using Python\'s asyncio library. Your task is to implement an asynchronous TCP echo server that can handle multiple client connections concurrently and return messages sent by the clients back to the clients. **Requirements:** 1. Implement an asynchronous TCP echo server: * The server should handle multiple clients simultaneously. * For each connected client, the server should echo back any data received from the client. 2. Implement an asynchronous TCP client: * The client should connect to the server. * The client should send a message to the server and wait for the echoed response. 3. Ensure proper management of the event loop and task scheduling. **Specifications:** - **Server:** - Input: Host, Port to bind the server to. - Method: Create a class `EchoServer` with methods to start and stop the server. - Use the `asyncio.start_server` function. - Handle connections concurrently using `asyncio.create_task`. - **Client:** - Input: Host, Port to connect to, Message to send. - Method: Create a class `EchoClient` with methods to connect to the server, send the message, and receive the echoed response. - Use the `asyncio.open_connection` function. **Constraints:** - The server should be able to handle at least 5 clients concurrently. - Ensure that the implementation gracefully handles client disconnections and possible exceptions. **Performance Requirements:** - The server should respond to client messages with minimal delay. **Sample Code Outline:** ```python import asyncio class EchoServer: def __init__(self, host, port): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connection from {addr}\\") while True: data = await reader.read(100) if not data: break print(f\\"Received data: {data.decode()}\\") writer.write(data) await writer.drain() print(f\\"Connection from {addr} closed\\") writer.close() await writer.wait_closed() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with self.server: await self.server.serve_forever() async def stop_server(self): if self.server: self.server.close() await self.server.wait_closed() class EchoClient: def __init__(self, host, port): self.host = host self.port = port async def send_message(self, message): reader, writer = await asyncio.open_connection(self.host, self.port) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() async def main(): # Start the server server = EchoServer(\'127.0.0.1\', 8888) server_task = asyncio.create_task(server.start_server()) # Give the server a bit of time to start up await asyncio.sleep(1) # Create and start clients client = EchoClient(\'127.0.0.1\', 8888) await client.send_message(\'Hello, Server!\') # Stop the server await server.stop_server() # Ensure server task is complete await server_task if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Instructions:** 1. Complete the `EchoServer` and `EchoClient` classes as specified. 2. Ensure proper exception handling and resource management. 3. Test the server and client with multiple clients concurrently. **Deliverable:** Submit the complete code implementing both the server and client, along with a brief explanation of how to run the code and any additional considerations.","solution":"import asyncio class EchoServer: def __init__(self, host, port): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connection from {addr}\\") while True: data = await reader.read(100) if not data: break print(f\\"Received data: {data.decode()}\\") writer.write(data) await writer.drain() print(f\\"Connection from {addr} closed\\") writer.close() await writer.wait_closed() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with self.server: await self.server.serve_forever() async def stop_server(self): if self.server: self.server.close() await self.server.wait_closed() class EchoClient: def __init__(self, host, port): self.host = host self.port = port async def send_message(self, message): reader, writer = await asyncio.open_connection(self.host, self.port) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed()"},{"question":"**Objective:** Implement a function to preprocess synthetic data, train a machine learning model, and then evaluate its performance by calculating the mean squared error. This exercise will test your ability to preprocess data, train a model, generate synthetic datasets, and evaluate model performance using fundamental and advanced scikit-learn functionalities. --- **Function Specification:** `train_and_evaluate_model(n_samples:int, n_features:int, noise_level:float)->float` **Objective:** 1. Generate a synthetic regression dataset using `n_samples`, `n_features`, and `noise_level` with the function `sklearn.datasets.make_regression`. 2. Preprocess the data using `StandardScaler`. 3. Split the dataset into training and testing sets using `train_test_split`. 4. Train a `GradientBoostingRegressor` model on the training data. 5. Evaluate the model performance on the test set by calculating the Mean Squared Error (MSE). 6. Return the calculated MSE. **Input:** - `n_samples` (int): The number of samples in the synthetic dataset. - `n_features` (int): The number of features in the synthetic dataset. - `noise_level` (float): The standard deviation of the Gaussian noise applied to the output. **Output:** - (float): The Mean Squared Error (MSE) of the model on the test set. **Constraints:** - `n_samples` should be a positive integer (1 ≤ n_samples ≤ 10000). - `n_features` should be a positive integer (1 ≤ n_features ≤ 100). - `noise_level` should be a non-negative float (0.0 ≤ noise_level ≤ 1.0). **Example:** ```python mse = train_and_evaluate_model(1000, 20, 0.1) print(mse) ``` --- **Function Implementation:** ```python import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def train_and_evaluate_model(n_samples: int, n_features: int, noise_level: float) -> float: # Generate synthetic data X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=noise_level) # Preprocess data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42) # Train model model = GradientBoostingRegressor(random_state=0) model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse ```","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def train_and_evaluate_model(n_samples: int, n_features: int, noise_level: float) -> float: This function generates a synthetic regression dataset, preprocesses it, splits it into training and testing sets, trains a GradientBoostingRegressor, and evaluates its performance by returning the Mean Squared Error (MSE) on the test set. Parameters: n_samples (int): Number of samples in the synthetic dataset. n_features (int): Number of features in the synthetic dataset. noise_level (float): Standard deviation of the Gaussian noise applied to the output. Returns: float: Mean Squared Error (MSE) of the model on the test set. # Generate synthetic data X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=noise_level, random_state=42) # Preprocess data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42) # Train model model = GradientBoostingRegressor(random_state=0) model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Question You are working on an application that needs to analyze and organize a large collection of files scattered across various directories. Your task is to implement a function that uses the `glob` module to search for files based on specific patterns and criteria. Your Function ```python def find_files(pattern: str, root_dir: str = None, recursive: bool = False) -> list: This function searches for files matching a given pattern starting from an optional root directory. Parameters: - pattern (str): A Unix shell-style wildcard pattern. - root_dir (str, optional): The root directory to start the search from. Defaults to None, which means the search will start in the current directory. - recursive (bool, optional): If True, the search will include all subdirectories recursively. Defaults to False. Returns: - list: A list of file paths that match the given pattern. Example: >>> find_files(\'*.txt\', \'/home/user/docs\', recursive=True) [\'/home/user/docs/file1.txt\', \'/home/user/docs/subdir/file2.txt\'] pass ``` Requirements 1. The function should use the `glob.glob` function from the `glob` module to perform the search. 2. If the `root_dir` is provided, the function should start searching from that directory. Otherwise, it should start from the current working directory. 3. If the `recursive` parameter is `True`, the function should perform a recursive search including all subdirectories. 4. The function should return a list of matching file paths. 5. If no files match the given pattern, the function should return an empty list. Constraints - The `pattern` should be a valid Unix shell-style wildcard pattern. - The `root_dir`, if provided, should be a valid directory path. - The search should be case-sensitive. - The function must handle any exceptions that occur (e.g., if the `root_dir` does not exist) and return an empty list in such cases. **Note**: You may assume that the module \'glob\' is already imported. Performance - The function should be efficient enough to handle large numbers of files and deep directory structures when `recursive` is `True`. # Examples ```python # Example 1: # Directory: /home/user with files: \'file1.txt\', \'file2.doc\', \'file3.TXT\' print(find_files(\'*.txt\', \'/home/user\')) # Output: [\'/home/user/file1.txt\'] # Example 2: # Directory: /home/user/docs with files: \'readme.md\', \'script.py\', subdirectory \'src\' containing \'main.py\' print(find_files(\'*.py\', \'/home/user/docs\', recursive=True)) # Output: [\'/home/user/docs/script.py\', \'/home/user/docs/src/main.py\'] # Example 3: # Directory: /data with files: \'data1.csv\', \'data2.csv\', subdirectory \'old_data\' containing \'data3.csv\' print(find_files(\'data*.csv\', \'/data\')) # Output: [\'/data/data1.csv\', \'/data/data2.csv\'] ``` Submission Ensure your implementation meets the requirements and constraints specified above. Run the provided example cases to validate your solution.","solution":"import glob import os def find_files(pattern: str, root_dir: str = None, recursive: bool = False) -> list: This function searches for files matching a given pattern starting from an optional root directory. Parameters: - pattern (str): A Unix shell-style wildcard pattern. - root_dir (str, optional): The root directory to start the search from. Defaults to None, which means the search will start in the current directory. - recursive (bool, optional): If True, the search will include all subdirectories recursively. Defaults to False. Returns: - list: A list of file paths that match the given pattern. # If no root directory is specified, use the current working directory if root_dir is None: root_dir = os.getcwd() # Construct the search pattern search_pattern = os.path.join(root_dir, \'**\', pattern) if recursive else os.path.join(root_dir, pattern) # Perform the search try: found_files = glob.glob(search_pattern, recursive=recursive) return found_files except Exception as e: # If any exception occurs, return an empty list return []"},{"question":"**Objective:** The goal of this task is to assess your ability to work with fundamental machine learning concepts using scikit-learn, including data loading, preprocessing, model training, and troubleshooting. **Problem Statement:** You are given a synthetic dataset with a target column and a feature column. Your task is to follow the steps below to preprocess the data, train a model, and identify any warnings or errors that arise. **Instructions:** 1. **Data Preparation:** - Use the provided synthetic data: ```python import pandas as pd df = pd.DataFrame({ \\"feature_name\\": [-12.32, 1.43, 30.01, 22.17, 5.6, -7.43, 13.89, -5.09, 12.56, 3.24], \\"target\\": [72, 55, 32, 43, 50, 60, 65, 70, 40, 30], }) ``` 2. **Splitting Data:** - Split the data into training and testing sets with 70% training and 30% testing. 3. **Feature Scaling:** - Use `StandardScaler` from scikit-learn to scale the features. 4. **Model Training:** - Train a `GradientBoostingRegressor` model with the default settings on the training data. - Score the model on the testing data. 5. **Observe and Troubleshoot:** - Train another `GradientBoostingRegressor` model with the `n_iter_no_change` parameter set to 5. - Document any warnings or errors encountered during this process. 6. **Submission:** - Implement the solution in a single Python function named `train_model`. - The function should return a tuple containing the scores from both models. **Function Signature:** ```python def train_model() -> tuple: pass ``` **Expected Result:** Your function should return the scores of the two models in a tuple format: ```python (default_model_score, param_model_score) ``` **Here is an example of how your implemented function should behave:** ```python result = train_model() print(result) # Expected output would be similar to (0.85, 0.82) ``` **Note:** - Ensure that you follow the steps in the right order and handle any warnings or errors appropriately. - Make sure your code is clean, readable, and efficiently solves the problem.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def train_model() -> tuple: # Provided synthetic data df = pd.DataFrame({ \\"feature_name\\": [-12.32, 1.43, 30.01, 22.17, 5.6, -7.43, 13.89, -5.09, 12.56, 3.24], \\"target\\": [72, 55, 32, 43, 50, 60, 65, 70, 40, 30], }) # Splitting data into features and target X = df[[\'feature_name\']] y = df[\'target\'] # Splitting the data into training and testing sets with 70% training and 30% testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Feature scaling scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Model training with default settings model_default = GradientBoostingRegressor() model_default.fit(X_train_scaled, y_train) model_default_score = model_default.score(X_test_scaled, y_test) # Model training with n_iter_no_change parameter set to 5 model_param = GradientBoostingRegressor(n_iter_no_change=5) model_param.fit(X_train_scaled, y_train) model_param_score = model_param.score(X_test_scaled, y_test) return model_default_score, model_param_score"},{"question":"**Question: Implement Custom Set Operations** Given the detailed Python C-API for set and frozenset operations, you are asked to implement equivalent higher-level operations to manipulate set objects using Python. This question will test your understanding of sets, their mutability properties, and integration with specific functionalities like adding, discarding, and checking membership. **Task:** Create a class `CustomSet` that mimics the behavior of Python\'s built-in `set` with a few additional methods, and another class `CustomFrozenSet` that mimics the behavior of Python\'s built-in `frozenset`. Implement the following methods in each class: **Class `CustomSet` (Mutable):** 1. `__init__(self, iterable=None)`: Initializes the set with elements from the given iterable. 2. `add(self, element)`: Adds an element to the set. 3. `discard(self, element)`: Removes the specified element from the set if it is present. 4. `__contains__(self, element)`: Checks if the set contains the specified element. 5. `size(self)`: Returns the number of elements in the set. 6. `pop(self)`: Removes and returns an arbitrary element from the set; raises `KeyError` if the set is empty. 7. `clear(self)`: Removes all elements from the set. **Class `CustomFrozenSet` (Immutable):** 1. `__init__(self, iterable=None)`: Initializes the frozenset with elements from the given iterable. 2. `__contains__(self, element)`: Checks if the frozenset contains the specified element. 3. `size(self)`: Returns the number of elements in the frozenset. **Constraints:** 1. The `add`, `discard`, `pop`, and `clear` methods should raise a `TypeError` if called on a `CustomFrozenSet` object. 2. The `__contains__`, `size`, and `__init__` methods should work for both `CustomSet` and `CustomFrozenSet`. **Examples:** ```python # Example usage for CustomSet cs = CustomSet([1, 2, 3]) print(cs.size()) # Output: 3 cs.add(4) print(cs.size()) # Output: 4 cs.discard(2) print(2 in cs) # Output: False element = cs.pop() print(element in cs) # Output: False cs.clear() print(cs.size()) # Output: 0 # Example usage for CustomFrozenSet cfs = CustomFrozenSet([1, 2, 3]) print(cfs.size()) # Output: 3 print(2 in cfs) # Output: True # The following operations should raise a TypeError # cfs.add(4) # cfs.discard(2) # cfs.clear() # cfs.pop() ``` **Note:** - If an iterable is not provided during initialization, the set should be empty. - Ensure that the `CustomSet` and `CustomFrozenSet` maintain similar performance characteristics to Python\'s built-in data structures.","solution":"class CustomSet: def __init__(self, iterable=None): self.data = set(iterable) if iterable is not None else set() def add(self, element): self.data.add(element) def discard(self, element): self.data.discard(element) def __contains__(self, element): return element in self.data def size(self): return len(self.data) def pop(self): if not self.data: raise KeyError(\'pop from an empty set\') return self.data.pop() def clear(self): self.data.clear() class CustomFrozenSet: def __init__(self, iterable=None): self.data = frozenset(iterable) if iterable is not None else frozenset() def __contains__(self, element): return element in self.data def size(self): return len(self.data) def add(self, element): raise TypeError(\\"\'CustomFrozenSet\' object has no attribute \'add\'\\") def discard(self, element): raise TypeError(\\"\'CustomFrozenSet\' object has no attribute \'discard\'\\") def clear(self): raise TypeError(\\"\'CustomFrozenSet\' object has no attribute \'clear\'\\") def pop(self): raise TypeError(\\"\'CustomFrozenSet\' object has no attribute \'pop\'\\")"},{"question":"**Question: Implement a Custom Module Importer using `importlib`** **Objective:** Your task is to implement a custom module importer using the `importlib` package. The importer should allow you to load a module from a given file path and optionally refresh the module if it has been updated. **Requirements:** 1. Create a class `CustomModuleImporter` that: - Inherits from `importlib.abc.Loader`. - Implements the `load_module` method to load a module from a given file path. - Implements the `create_module` method that prepares a module for importing. - Implements the `exec_module` method to execute the module in its own namespace. 2. Implement a function `import_module_from_path` that takes a module name and file path as arguments and uses `CustomModuleImporter` to import the module from the specified file path. 3. Implement a function `reload_module_if_updated` that re-imports a module if its source file has been updated since it was last loaded. **Constraints:** - Do not use any third-party libraries; rely solely on built-in Python modules. - Your solution should handle errors gracefully, ensuring that partial imports do not leave corrupt state in `sys.modules`. **Function Signatures:** ```python class CustomModuleImporter(importlib.abc.Loader): def __init__(self, module_name: str, path: str): pass def create_module(self, spec) -> Optional[ModuleType]: pass def exec_module(self, module: ModuleType): pass def import_module_from_path(module_name: str, file_path: str) -> ModuleType: pass def reload_module_if_updated(module_name: str) -> ModuleType: pass ``` **Example:** ```python # Save the following code as \'my_module.py\': # def greet(name): # return f\\"Hello, {name}!\\" # Use the custom importer to load \'my_module\' module = import_module_from_path(\'my_module\', \'/path/to/my_module.py\') print(module.greet(\\"World\\")) # Output: \\"Hello, World!\\" # Edit \'my_module.py\' to change \'Hello\' to \'Hi\' # Re-import the module if it has been updated module = reload_module_if_updated(\'my_module\') print(module.greet(\\"World\\")) # Output: \\"Hi, World!\\" ``` Note: Ensure that your implementation considers module caching and proper error handling during reloading. **Submission:** Submit your implementation of the `CustomModuleImporter` class, `import_module_from_path`, and `reload_module_if_updated` functions. Ensure your code includes comments explaining critical parts of your implementation.","solution":"import importlib.util import importlib.abc import sys import time import os from types import ModuleType from typing import Optional class CustomModuleImporter(importlib.abc.Loader): def __init__(self, module_name: str, path: str): self.module_name = module_name self.path = path self.last_modified_time = None def create_module(self, spec) -> Optional[ModuleType]: # Use the default module creation mechanism return None def exec_module(self, module: ModuleType): # Read the source code from the file and execute it with open(self.path, \'r\') as file: source_code = file.read() exec(source_code, module.__dict__) self.last_modified_time = os.path.getmtime(self.path) def is_updated(self) -> bool: return os.path.getmtime(self.path) != self.last_modified_time def import_module_from_path(module_name: str, file_path: str) -> ModuleType: spec = importlib.util.spec_from_file_location(module_name, file_path, loader=CustomModuleImporter(module_name, file_path)) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) sys.modules[module_name] = module return module def reload_module_if_updated(module_name: str) -> ModuleType: if module_name in sys.modules: module = sys.modules[module_name] loader = module.__spec__.loader if isinstance(loader, CustomModuleImporter) and loader.is_updated(): # Reload the module spec = importlib.util.spec_from_file_location(module_name, loader.path, loader=loader) new_module = importlib.util.module_from_spec(spec) spec.loader.exec_module(new_module) sys.modules[module_name] = new_module return new_module raise ImportError(f\\"Module \'{module_name}\' is not loaded or hasn\'t been updated.\\")"},{"question":"Optimize Prediction Performance with Scikit-learn Objective: The goal of this task is to assess the student\'s ability to optimize model prediction latency and throughput using Scikit-learn. The task involves measuring performance metrics, handling data representation, and configuring appropriate settings. Problem Statement: You are provided with a dataset and are asked to train a simple linear regression model on it. The task involves measuring prediction latency and optimizing the performance by experimenting with different configurations and representations. Dataset: You will use the `load_diabetes` dataset from Scikit-learn\'s dataset module. This dataset contains 442 samples with 10 feature variables. Steps: 1. **Load the Dataset and Train the Model:** - Load the `load_diabetes` dataset. - Split the dataset into training and testing sets. - Train a `LinearRegression` model on the training set. 2. **Measure Baseline Prediction Latency:** - Evaluate the baseline prediction latency and throughput for the test set predictions. 3. **Optimize Model Performance:** - Experiment with different configurations: - Bulk vs atomic predictions. - Handling different input data representations (dense vs sparse). - Adjust Scikit-learn configuration settings to optimize performance. - Document the changes and their impact on prediction latency and throughput. Implementation Details: - **Input:** - `load_diabetes` dataset. - **Output:** - A trained `LinearRegression` model. - Measured and optimized prediction latency and throughput. - Documentation of the configurations and their impact. Constraints: - Use the `load_diabetes` dataset only. - Implement the linear regression model using `LinearRegression` from scikit-learn. - Optimize within the scope of configurations described in the provided documentation. Performance Requirement: - The goal is to minimize the prediction latency as much as possible while maintaining reasonable prediction accuracy. Example: ```python from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.config_context import config_context import numpy as np import time # Load the dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model = LinearRegression() model.fit(X_train, y_train) # Measure baseline latency start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() baseline_latency = end_time - start_time print(f\\"Baseline Prediction Latency: {baseline_latency:.6f} seconds\\") # Apply optimization - Bulk predictions with config_context(assume_finite=True): start_time = time.time() y_pred_bulk = model.predict(X_test) end_time = time.time() bulk_latency = end_time - start_time print(f\\"Optimized Prediction Latency with Bulk: {bulk_latency:.6f} seconds\\") # Optimize input data representation X_test_sparse = csr_matrix(X_test) start_time = time.time() y_pred_sparse = model.predict(X_test_sparse) end_time = time.time() sparse_latency = end_time - start_time print(f\\"Optimized Prediction Latency with Sparse Input: {sparse_latency:.6f} seconds\\") # Conclusively, document the results ``` In your submission, ensure to: 1. Load, train and measure a baseline latency of the Linear Regression model as shown. 2. Include optimization using both bulk and sparse input representations. 3. Document your observations on how each configuration impacted the model performance.","solution":"from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from scipy.sparse import csr_matrix import time def train_model(): # Load the dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model = LinearRegression() model.fit(X_train, y_train) return model, X_test def measure_baseline_latency(model, X_test): # Measure baseline latency start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() baseline_latency = end_time - start_time return baseline_latency def optimize_bulk_predictions(model, X_test): # Measure latency with bulk predictions start_time = time.time() y_pred_bulk = model.predict(X_test) end_time = time.time() bulk_latency = end_time - start_time return bulk_latency def optimize_sparse_input(model, X_test): # Convert test data to sparse representation X_test_sparse = csr_matrix(X_test) # Measure latency with sparse input start_time = time.time() y_pred_sparse = model.predict(X_test_sparse) end_time = time.time() sparse_latency = end_time - start_time return sparse_latency def measure_latency(): model, X_test = train_model() baseline_latency = measure_baseline_latency(model, X_test) bulk_latency = optimize_bulk_predictions(model, X_test) sparse_latency = optimize_sparse_input(model, X_test) return baseline_latency, bulk_latency, sparse_latency"},{"question":"**Objective**: Assess the understanding and application of context managers using the `contextlib` module, specifically `ExitStack`. Problem Statement Please write a function `manage_resources` that handles multiple resource management scenarios using a combination of context managers and other cleanup functions. **Function Signature**: ```python def manage_resources(resource_configs: list[dict]) -> list: ``` **Parameters**: - `resource_configs`: A list of dictionaries where each dictionary represents a resource configuration. Each dictionary contains: - `type`: A string that can be either \\"file\\" or \\"custom\\". - `action`: A string indicating the action to perform, \\"open\\" or \\"close\\" for files, and \\"acquire\\" or \\"release\\" for custom resources. - Paths or resource details specific to the type of resource. - Any other relevant information. **Return**: - The function should return a list of results after performing all the actions in the provided resource configurations. **Constraints**: - The function should manage resources in such a way that all opened/acquired resources are properly closed/released irrespective of whether an exception occurs. **Requirements**: 1. Use `contextlib.ExitStack` to handle the management of multiple context managers or resources dynamically. 2. Ensure that any resource opened or acquired is properly closed or released, even if an exception is encountered. **Details**: - If the resource type is `\\"file\\"`, the action can be `\\"open\\"` or `\\"close\\"`. - For `\\"open\\"`, you must open the file using `open` and add it to the `ExitStack`. - For `\\"close\\"`, you must explicitly close the file. - If the resource type is `\\"custom\\"`, the action can be `\\"acquire\\"` or `\\"release\\"`. - For `\\"acquire\\"`, you must simulate acquiring a custom resource and add a custom cleanup function to the `ExitStack`. - For `\\"release\\"`, you must simulate releasing the custom resource gracefully. **Example**: ```python def custom_acquire(): resource = \\"custom_resource\\" print(f\\"Acquired {resource}\\") return resource def custom_release(resource): print(f\\"Released {resource}\\") def manage_resources(resource_configs): from contextlib import ExitStack results = [] with ExitStack() as stack: for config in resource_configs: if config[\'type\'] == \'file\' and config[\'action\'] == \'open\': f = open(config[\'path\'], \'w\') stack.enter_context(f) results.append(f\'Opened {config[\\"path\\"]}\') elif config[\'type\'] == \'file\' and config[\'action\'] == \'close\': f.close() results.append(f\'Closed {config[\\"path\\"]}\') elif config[\'type\'] == \'custom\' and config[\'action\'] == \'acquire\': r = custom_acquire() stack.callback(custom_release, r) results.append(f\'Acquired custom resource\') elif config[\'type\'] == \'custom\' and config[\'action\'] == \'release\': custom_release(custom_acquire()) results.append(f\'Released custom resource\') return results # Test Example resource_configs = [ {\\"type\\": \\"file\\", \\"action\\": \\"open\\", \\"path\\": \\"/tmp/testfile1.txt\\"}, {\\"type\\": \\"custom\\", \\"action\\": \\"acquire\\"}, {\\"type\\": \\"custom\\", \\"action\\": \\"release\\"}, ] print(manage_resources(resource_configs)) ``` The expected output should manage resources properly according to the configurations, ensuring that all acquired resources are released, even if an error occurs during their usage.","solution":"def custom_acquire(): Simulate acquiring a custom resource. resource = \\"custom_resource\\" print(f\\"Acquired {resource}\\") return resource def custom_release(resource): Simulate releasing a custom resource. print(f\\"Released {resource}\\") def manage_resources(resource_configs: list[dict]) -> list: from contextlib import ExitStack results = [] with ExitStack() as stack: for config in resource_configs: if config[\'type\'] == \'file\' and config[\'action\'] == \'open\': f = open(config[\'path\'], \'w\') stack.enter_context(f) results.append(f\'Opened {config[\\"path\\"]}\') elif config[\'type\'] == \'custom\' and config[\'action\'] == \'acquire\': r = custom_acquire() stack.callback(custom_release, r) results.append(f\'Acquired custom resource\') elif config[\'type\'] == \'custom\' and config[\'action\'] == \'release\': custom_release(custom_acquire()) results.append(f\'Released custom resource\') return results"},{"question":"# Python Coding Assessment Question Objective You are required to write a function that mimics the simple import and module-loading process using the `importlib` module, replacing the deprecated `imp` module. Problem Statement Implement the function `import_module(module_name: str) -> type` that: - Takes the name of a module (e.g., \\"math\\", \\"os.path\\", etc.) as input. - Uses the `importlib` module to find and load the module. - Returns the loaded module object. Constraints - If the module is not found, the function should raise an `ImportError`. - The function should be able to handle hierarchical module names containing dots. Expected Input and Output - **Input**: A string representing the module name. - **Output**: The loaded module object. Example ```python # Example usage: module_name = \\"math\\" loaded_module = import_module(module_name) print(loaded_module) # Output: <module \'math\' from \'.../lib/python3.10/lib-dynload/math.cpython-310.so\'> print(loaded_module.sqrt(16)) # Output: 4.0 ``` Notes - Use the `importlib.util.find_spec` to locate the module spec. - Use `importlib.util.module_from_spec` to create a module object from the spec. - Use `importlib.util.spec.loader.exec_module` to execute the module. Implementation Details Write your solution in the form of a function `import_module(module_name: str) -> type` that adheres to the guidelines provided.","solution":"import importlib.util def import_module(module_name: str) -> type: Imports and loads the specified module using importlib. Parameters: - module_name (str): The name of the module to be imported. Returns: - type: The loaded module object. Raises: - ImportError: If the module cannot be found or loaded. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found.\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module"},{"question":"**Coding Assessment Question** # Objective: Write a Python program that demonstrates your understanding of the `fcntl` module. Your program will perform file locking and control operations on a specified file. # Problem Statement: You need to implement a function `manage_file_locks(file_path: str, operation: str) -> str` that will: 1. Open the file specified by `file_path`. 2. Perform a locking operation on the file depending on the `operation` argument. The `operation` can be one of the following: - `\\"lock_shared\\"`: Acquire a shared lock on the file. - `\\"lock_exclusive\\"`: Acquire an exclusive lock on the file. - `\\"unlock\\"`: Unlock the file. 3. Use `fcntl` functions to perform the required operations. Ensure proper error handling. # Function Signature: ```python def manage_file_locks(file_path: str, operation: str) -> str: ``` # Input: - `file_path` (str): The path to the file to be locked. - `operation` (str): The operation to perform on the file. It can be one of `\\"lock_shared\\"`, `\\"lock_exclusive\\"`, or `\\"unlock\\"`. # Output: - Return a message indicating the result of the file lock operation: - `\\"Shared lock acquired\\"` if a shared lock is acquired successfully. - `\\"Exclusive lock acquired\\"` if an exclusive lock is acquired successfully. - `\\"File unlocked\\"` if the file is unlocked successfully. - `\\"Error: <error_message>\\"` if there is an error during the operation. # Example Usage: ```python # Assume \'example.txt\' exists in the current directory print(manage_file_locks(\'example.txt\', \'lock_shared\')) # Output: \\"Shared lock acquired\\" print(manage_file_locks(\'example.txt\', \'unlock\')) # Output: \\"File unlocked\\" print(manage_file_locks(\'nonexistent.txt\', \'lock_exclusive\')) # Output: \\"Error: [file not found error message]\\" ``` # Constraints: - You must handle `OSError` exceptions and return a meaningful error message. - The program should work on Unix-based systems where `fcntl` is supported. - Use the `fcntl.flock` function for locking operations. # Notes: - Make sure to open the file in read or write mode as required by the locking operation. - It is a good idea to print log messages to debug and track the execution of your function. # Hints: - Use constants `fcntl.LOCK_SH`, `fcntl.LOCK_EX`, and `fcntl.LOCK_UN` for shared locks, exclusive locks, and unlocking, respectively. - Consider using context managers to ensure that the file is properly closed after the operations.","solution":"import fcntl def manage_file_locks(file_path: str, operation: str) -> str: try: with open(file_path, \'r+\' if operation != \'lock_shared\' else \'r\') as file: if operation == \'lock_shared\': fcntl.flock(file, fcntl.LOCK_SH) return \\"Shared lock acquired\\" elif operation == \'lock_exclusive\': fcntl.flock(file, fcntl.LOCK_EX) return \\"Exclusive lock acquired\\" elif operation == \'unlock\': fcntl.flock(file, fcntl.LOCK_UN) return \\"File unlocked\\" else: return \\"Error: Invalid operation\\" except OSError as e: return f\\"Error: {str(e)}\\""},{"question":"# Advanced Coding Assessment Question Objective The goal of this exercise is to implement a custom dictionary manipulation function using the provided low-level dictionary functions available in `python310`. You need to demonstrate your understanding of dictionary operations such as creation, insertion, deletion, retrieval, and iteration. Question Implement a function called `merge_and_increment` that takes two dictionaries as input. This function should merge the second dictionary into the first one. If a key exists in both dictionaries: - If both values are numbers (integers or floats), increment the value in the first dictionary by the value from the second dictionary. - If both values are strings, concatenate the value from the second dictionary to the value in the first dictionary. - If neither condition applies, overwrite the value in the first dictionary with the value from the second dictionary. Use the following functions from the `python310` package: - `PyDict_Check` - `PyDict_SetItem` - `PyDict_GetItem` - `PyDict_New` - `PyDict_Keys` - `PyDict_Contains` - `PyDict_Next` - `PyDict_SetItemString` - `PyDict_GetItemString` Function Signature ```python def merge_and_increment(dict1: dict, dict2: dict) -> dict: pass ``` Input - `dict1`: A dictionary (initial dictionary to be merged into). - `dict2`: A dictionary (dictionary to merge into `dict1`). Output - Returns the modified `dict1` after merging and applying the specified rules. Example ```python # Input dictionaries dict1 = {\'a\': 1, \'b\': \'hello\', \'c\': 3.0} dict2 = {\'a\': 2, \'b\': \' world\', \'c\': 4.0, \'d\': [1, 2, 3]} # Expected output result = {\'a\': 3, \'b\': \'hello world\', \'c\': 7.0, \'d\': [1, 2, 3]} ``` ```python # Your implementation should work correctly with the provided low-level functions. result = merge_and_increment(dict1, dict2) print(result) # Should print: {\'a\': 3, \'b\': \'hello world\', \'c\': 7.0, \'d\': [1, 2, 3]} ``` # Constraints - Do not use high-level dictionary functions provided by Python. Only use the specified `python310` package functions. - Assume all keys in the dictionaries are strings. Notes - You may assume that the dictionaries only contain keys with simple data types like integers, floats, and strings for simplicity. - Handle potential errors gracefully. **Hint:** You can utilize the `PyDict_Keys` function to retrieve the keys and `PyDict_GetItem` to get the values of keys. For setting items, make use of `PyDict_SetItem`.","solution":"def merge_and_increment(dict1: dict, dict2: dict) -> dict: Merge dict2 into dict1. Increment numerical values, concatenate strings, and overwrite otherwise. for key, value in dict2.items(): if key in dict1: if isinstance(dict1[key], (int, float)) and isinstance(value, (int, float)): dict1[key] += value elif isinstance(dict1[key], str) and isinstance(value, str): dict1[key] += value else: dict1[key] = value else: dict1[key] = value return dict1"},{"question":"Objective Write a Python function that simulates a simple interactive Python shell. Your shell should be capable of executing both expressions and statements provided as input and return corresponding results or errors. Function Signature ```python def simple_python_shell(inputs: List[str]) -> List[Union[str, Any]]: ``` Input - `inputs` (List of Strings): A list of strings where each string is either a single expression/statement or a compound statement. The input strings may contain leading or trailing whitespace. Output - A list containing the results of expressions or the string `\'Statement Executed\'` for statements. If an error occurs during the execution of any input, capture the exception and store the error message in the output list. Constraints - The function should be capable of handling multiple lines of input to simulate continuous interactive mode. - The function should appropriately distinguish between expressions to be evaluated using `eval()` and statements to be executed using `exec()`. - The function should handle any valid Python syntax for expressions and statements, including compound statements that span multiple lines. - Leading and trailing whitespaces in inputs should be ignored. Example ```python inputs = [ \\"x = 10\\", \\"y = 20\\", \\"x + y\\", \\"for i in range(3):n print(i)\\" ] expected_output = [ \\"Statement Executed\\", \\"Statement Executed\\", 30, \\"0n1n2n\\" ] assert simple_python_shell(inputs) == expected_output ``` Additional Information - You may use the `eval()` function to evaluate expressions. - You may use the `exec()` function to execute statements. - Properly handle both single-line statements and multi-line compound statements. - Ensure to handle exceptions gracefully and include the resulting error messages in the output list.","solution":"from typing import List, Union, Any def simple_python_shell(inputs: List[str]) -> List[Union[str, Any]]: results = [] local_vars = {} for input_str in inputs: input_str = input_str.strip() try: # Attempt to evaluate the input as an expression result = eval(input_str, {}, local_vars) results.append(result) except Exception as eval_exc: try: # Attempt to execute the input as a statement exec(input_str, {}, local_vars) results.append(\'Statement Executed\') except Exception as exec_exc: # If both eval and exec fail, record the error message results.append(str(exec_exc)) return results"},{"question":"You are tasked with creating an event scheduling system using Python\'s `sched` module. Your system must handle various types of events that can be scheduled at both relative and absolute times. Additionally, you need to implement functionality to cancel events and query the upcoming events in a specific format. Requirements: 1. **Function Implementation**: Implement the following functions using the `sched` module: - `add_event(s, delay, priority, action, args=(), kwargs={})`: Schedule an event with a relative delay. - `add_absolute_event(s, timestamp, priority, action, args=(), kwargs={})`: Schedule an event at an absolute time. - `cancel_event(s, event)`: Cancel a scheduled event. - `get_upcoming_events(s)`: Return a list of tuples, each representing an upcoming event as `(time, priority, action, args, kwargs)`. - `run_scheduler(s)`: Run the scheduled events until the queue is empty. 2. **Input and Output**: - `add_event(s, delay, priority, action, args=(), kwargs={})` - **Input**: `s` (scheduler instance), `delay` (relative time in seconds), `priority` (integer), `action` (callable function), `args` (tuple for positional arguments), `kwargs` (dictionary for keyword arguments). - **Output**: Returns the event object that can be used for cancellation. - `add_absolute_event(s, timestamp, priority, action, args=(), kwargs={})` - **Input**: Same as `add_event` but `timestamp` (absolute time) instead of `delay`. - **Output**: Returns the event object that can be used for cancellation. - `cancel_event(s, event)` - **Input**: `s` (scheduler instance), `event` (event object to be canceled). - **Output**: None. - `get_upcoming_events(s)` - **Input**: `s` (scheduler instance). - **Output**: List of tuples representing upcoming events. - `run_scheduler(s)` - **Input**: `s` (scheduler instance). - **Output**: None. Executes scheduled events. 3. **Constraints**: - Assume event scheduling and execution will not exceed the limits of the system clock. - The `action` functions provided may take varying arguments in `args` and `kwargs`. 4. **Example**: ```python import sched import time # Initialize scheduler scheduler = sched.scheduler(time.time, time.sleep) # Define actions def print_msg(msg): print(f\\"{time.time()}: {msg}\\") # Add events event1 = add_event(scheduler, 10, 1, print_msg, args=(\\"Event 1 executed.\\",)) event2 = add_event(scheduler, 5, 2, print_msg, args=(\\"Event 2 executed.\\",)) event3 = add_absolute_event(scheduler, time.time() + 7, 1, print_msg, args=(\\"Event 3 executed at absolute time.\\",)) # Cancel an event cancel_event(scheduler, event2) # Get upcoming events print(get_upcoming_events(scheduler)) # Run scheduler run_scheduler(scheduler) ``` This question assesses the understanding and practical application of the `sched` module, event scheduling, priority management, and handling event cancellations.","solution":"import sched import time def add_event(s, delay, priority, action, args=(), kwargs={}): Schedules an event with a relative delay. event = s.enter(delay, priority, action, argument=args, kwargs=kwargs) return event def add_absolute_event(s, timestamp, priority, action, args=(), kwargs={}): Schedules an event at an absolute time. delay = timestamp - time.time() event = s.enter(delay, priority, action, argument=args, kwargs=kwargs) return event def cancel_event(s, event): Cancels a scheduled event. s.cancel(event) def get_upcoming_events(s): Returns a list of tuples representing upcoming events. events = [(e.time, e.priority, e.action, e.argument, e.kwargs) for e in s.queue] return events def run_scheduler(s): Runs the scheduler until no more events are left. s.run()"},{"question":"**Objective:** Write a Python script using scikit-learn to perform both classification and regression tasks using Stochastic Gradient Descent (SGD), demonstrating feature scaling and model evaluation. # Classification Task: 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Apply `SGDClassifier` with `loss=\\"log_loss\\"` and `penalty=\\"l2\\"`. 3. Implement feature scaling using `StandardScaler` within a pipeline. 4. Split the data into training and testing sets with a ratio of 80:20. 5. Train the model on the training set and evaluate its accuracy on the test set. 6. Output the model\'s accuracy and the coefficients (`coef_` and `intercept_`). # Regression Task: 1. Load the California housing dataset using `sklearn.datasets.fetch_california_housing`. 2. Apply `SGDRegressor` with `loss=\\"squared_error\\"` and `penalty=\\"elasticnet\\"`. 3. Implement feature scaling using `StandardScaler` within a pipeline. 4. Split the data into training and testing sets with a ratio of 70:30. 5. Train the model on the training set and evaluate the Mean Squared Error (MSE) on the test set. 6. Output the model\'s MSE and the coefficients (`coef_` and `intercept_`). # Additional Requirements: 1. Use `GridSearchCV` to find the best regularization parameter `alpha` for both tasks within the range `[1e-4, 1e-3, 1e-2, 1e-1, 1]`. 2. Enable `early_stopping` with a validation fraction of 10% and `n_iter_no_change=5` for both tasks. # Input Format: - No input from the user is required. # Output Format: - For the classification task: Print the accuracy, `coef_`, and `intercept_`. - For the regression task: Print the MSE, `coef_`, and `intercept_`. # Constraints: - Ensure that features are appropriately scaled to improve the SGD model performance. - Use pipelines to streamline the preprocessing and model fitting steps. You are provided with the necessary imports and template code structure below. Complete the function definitions to fulfill the requirements. ```python import numpy as np from sklearn.datasets import load_iris, fetch_california_housing from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error, accuracy_score # Classification Task: def sgd_classification(): # Load Iris data iris = load_iris() X, y = iris.data, iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'l2\', early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42)) # Grid Search for best alpha param_grid = {\'sgdclassifier__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Evaluate y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) coef_ = best_model.named_steps[\'sgdclassifier\'].coef_ intercept_ = best_model.named_steps[\'sgdclassifier\'].intercept_ print(f\'Classification Task:\') print(f\'Accuracy: {accuracy}\') print(f\'Coefficients: {coef_}\') print(f\'Intercept: {intercept_}\') # Regression Task: def sgd_regression(): # Load California housing data housing = fetch_california_housing() X, y = housing.data, housing.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create a pipeline pipeline = make_pipeline(StandardScaler(), SGDRegressor(loss=\'squared_error\', penalty=\'elasticnet\', early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42)) # Grid Search for best alpha param_grid = {\'sgdregressor__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Evaluate y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) coef_ = best_model.named_steps[\'sgdregressor\'].coef_ intercept_ = best_model.named_steps[\'sgdregressor\'].intercept_ print(f\'Regression Task:\') print(f\'MSE: {mse}\') print(f\'Coefficients: {coef_}\') print(f\'Intercept: {intercept_}\') if __name__ == \\"__main__\\": sgd_classification() sgd_regression() ```","solution":"import numpy as np from sklearn.datasets import load_iris, fetch_california_housing from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error, accuracy_score # Classification Task: def sgd_classification(): # Load Iris data iris = load_iris() X, y = iris.data, iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'l2\', early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42)) # Grid Search for best alpha param_grid = {\'sgdclassifier__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Evaluate y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) coef_ = best_model.named_steps[\'sgdclassifier\'].coef_ intercept_ = best_model.named_steps[\'sgdclassifier\'].intercept_ print(f\'Classification Task:\') print(f\'Accuracy: {accuracy}\') print(f\'Coefficients: {coef_}\') print(f\'Intercept: {intercept_}\') # Regression Task: def sgd_regression(): # Load California housing data housing = fetch_california_housing() X, y = housing.data, housing.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create a pipeline pipeline = make_pipeline(StandardScaler(), SGDRegressor(loss=\'squared_error\', penalty=\'elasticnet\', early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, random_state=42)) # Grid Search for best alpha param_grid = {\'sgdregressor__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Evaluate y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) coef_ = best_model.named_steps[\'sgdregressor\'].coef_ intercept_ = best_model.named_steps[\'sgdregressor\'].intercept_ print(f\'Regression Task:\') print(f\'MSE: {mse}\') print(f\'Coefficients: {coef_}\') print(f\'Intercept: {intercept_}\') if __name__ == \\"__main__\\": sgd_classification() sgd_regression()"},{"question":"**Question: Implementing a Custom Model with PyTorch Hub** As a research reproducibility advocate, you want to simplify the sharing of pre-trained models. Your task is to implement a custom model and set it up for PyTorch Hub. The goal is to provide a reusable and easily accessible model hub for other researchers. # Requirements: 1. **Model Definition** - Define a simple neural network model class named `SimpleNN` using PyTorch\'s `nn.Module`. The model should have: - An input layer taking 10 features. - Two hidden layers with 50 and 25 neurons respectively, using ReLU activation. - An output layer with a single neuron. - Ensure that the model can be instantiated without any additional arguments. 2. **hubconf.py Setup** - Implement an entry point function in `hubconf.py` that returns an instance of `SimpleNN`. - Add a docstring to the entry point function to describe the model and any available parameters. 3. **Pre-trained Weights Handling** - Save a set of pre-trained weights for `SimpleNN` locally. - Modify the entry point function to optionally load these pre-trained weights when a `pretrained` argument is set to `True`. 4. **Usage Example** - Provide a minimal working example demonstrating how to load `SimpleNN` using `torch.hub.load()` and use it to make a simple prediction. # Implementation Details: 1. **Model Definition (SimpleNN)**: ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 25) self.fc3 = nn.Linear(25, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x ``` 2. **hubconf.py Setup**: ```python dependencies = [\'torch\'] def simple_nn(pretrained=False, **kwargs): SimpleNN model pretrained (bool): Load pretrained weights into the model model = SimpleNN(**kwargs) if pretrained: checkpoint = \'path_to_pretrained_weights.pth\' state_dict = torch.load(checkpoint) model.load_state_dict(state_dict) return model ``` 3. **Usage Example**: ```python import torch # Load model from PyTorch Hub model = torch.hub.load(\'your_github_repo_url\', \'simple_nn\', pretrained=True) # Making a prediction with random input input_tensor = torch.randn(1, 10) output_tensor = model(input_tensor) print(output_tensor) ``` # Input and Output Formats: - **Input**: None (the implementation is self-contained). - **Output**: Your implementation will be validated by examining the correctness of the defined model, entry points, weight loading mechanism, and the prediction example. # Constraints and Limitations: - Pre-trained weights should be kept under 2GB if stored locally. - Ensure compatibility with PyTorch version specified in `dependencies`. - Focus on clear and well-documented code, making it easy for other researchers to use the model. # Submission: Submit: 1. The `SimpleNN` model implementation. 2. Your `hubconf.py` file. 3. The example script to demonstrate loading the model from PyTorch Hub and making predictions.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 25) self.fc3 = nn.Linear(25, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x"},{"question":"Mailbox Management and Message Handling You are tasked with creating a mailbox management utility that organizes emails into different mailboxes based on specific criteria. The utility will primarily work with the `Maildir` and `mbox` formats. Your program should demonstrate an understanding of mailbox manipulation, adding and removing messages, and dealing with mailbox-specific nuances. # Requirements 1. **Create and Initialize Mailboxes**: - Create a `Maildir` mailbox at a specified directory. - Create an `mbox` mailbox at a specified file path. 2. **Add Messages to Mailboxes**: - Implement a function `add_message_to_mailbox(mailbox, message)` that adds a given message to the specified mailbox and returns the message key. 3. **Move Messages Between Mailboxes**: - Implement a function `move_message(source_mailbox, destination_mailbox, key)` that moves a message identified by the key from the source mailbox to the destination mailbox. 4. **List All Messages**: - Implement a function `list_all_messages(mailbox)` that returns a list of all message subjects in the specified mailbox. 5. **Remove Messages**: - Implement a function `remove_message(mailbox, key)` that removes the message identified by the key from the specified mailbox. 6. **Mailbox Locking** (For `mbox` only): - Ensure that the `mbox` mailbox is locked before any modification is made to prevent concurrent access issues. # Constraints - The `Maildir` and `mbox` mailboxes may contain any number of messages. - Messages can be strings representing the email content. - Ensure that all messages added are RFC 2822-compliant. - For the purpose of this exercise, we\'ll assume `Maildir` mailboxes do not require locking, but `mbox` mailboxes must be locked before any modifications. - The functions should handle exceptions where necessary and provide meaningful error messages if operations fail (e.g., message not found, invalid key). # Example Here\'s an example of how your functions might be used: ```python from mailbox import Maildir, mbox # Create Mailboxes maildir_box = Maildir(\'/path/to/maildir\', create=True) mbox_box = mbox(\'/path/to/mbox\', create=True) # Add Messages key1 = add_message_to_mailbox(maildir_box, \\"Subject: TestnnThis is a test message.\\") key2 = add_message_to_mailbox(mbox_box, \\"Subject: HellonnHello, world!\\") # List Messages print(list_all_messages(maildir_box)) # Should list subjects of all messages in maildir print(list_all_messages(mbox_box)) # Should list subjects of all messages in mbox # Move Message move_message(maildir_box, mbox_box, key1) # Remove Message remove_message(mbox_box, key2) ``` # Notes - Use the `mailbox` module to implement the functionality. - Ensure that all file operations are safely handled. - Document your code to explain your logic and decisions. # Submission Submit a Python script that defines and demonstrates the required functions mentioned above. Your code will be reviewed for correctness, robustness, and adherence to the requirements.","solution":"import mailbox import email def create_maildir(path): Create a Maildir mailbox at the specified path. return mailbox.Maildir(path, create=True) def create_mbox(path): Create an mbox mailbox at the specified path. return mailbox.mbox(path, create=True) def add_message_to_mailbox(mailbox_obj, message): Add a given RFC 2822 message to the specified mailbox. :param mailbox_obj: The mailbox object :param message: The message string :return: The key of the added message email_message = email.message_from_string(message) return mailbox_obj.add(email_message) def move_message(source_mailbox, destination_mailbox, key): Move a message identified by the key from the source mailbox to the destination mailbox. :param source_mailbox: The source mailbox object :param destination_mailbox: The destination mailbox object :param key: The key of the message to move message = source_mailbox[key] destination_mailbox.add(message) remove_message(source_mailbox, key) def list_all_messages(mailbox_obj): List all message subjects in the specified mailbox. :param mailbox_obj: The mailbox object :return: A list of message subjects subjects = [] for key in mailbox_obj.iterkeys(): message = mailbox_obj[key] subjects.append(message[\'subject\']) return subjects def remove_message(mailbox_obj, key): Remove the message identified by the key from the specified mailbox. :param mailbox_obj: The mailbox object :param key: The key of the message to remove mailbox_obj.lock() try: del mailbox_obj[key] finally: mailbox_obj.flush() mailbox_obj.unlock()"},{"question":"# Complex Number Analysis and Manipulation Problem Statement You are required to implement a function `analyze_complex_numbers` which performs several operations on complex numbers and returns a result based on these operations. 1. **Input**: - A list of complex numbers. - Each complex number can be an integer, a floating-point value, or already a complex number. 2. **Output**: - The function should return a dictionary with the following keys and their corresponding values: - `\'modulus\'`: List of moduli of the complex numbers. - `\'phase\'`: List of phases (arguments) of the complex numbers. - `\'polar_coordinates\'`: List of tuples where each tuple contains the modulus and phase of a complex number. - `\'rectangular_coordinates\'`: List of complex numbers generated back from the polar coordinates. 3. **Constraints**: - The function should handle an empty list correctly by returning an empty dictionary for all keys. - The function should handle edge cases where the real or imaginary parts are zero. 4. **Specifications**: - Use the `cmath` module for calculations. - Ensure your solution is efficient and handles the conversion accurately. Example ```python import cmath def analyze_complex_numbers(complex_list): # Initialize lists to store results moduli = [] phases = [] polar_coords = [] rect_coords = [] for number in complex_list: # Convert the number to a complex number if it\'s not already z = complex(number) # Compute modulus and phase modulus = abs(z) phase = cmath.phase(z) # Compute polar coordinates polar = cmath.polar(z) # Convert polar coordinates back to rectangular coordinates rect = cmath.rect(polar[0], polar[1]) # Append results to lists moduli.append(modulus) phases.append(phase) polar_coords.append(polar) rect_coords.append(rect) return { \'modulus\': moduli, \'phase\': phases, \'polar_coordinates\': polar_coords, \'rectangular_coordinates\': rect_coords } # Test the function complex_numbers = [1+2j, 3-3j, -2+2j, 2, 5.5] result = analyze_complex_numbers(complex_numbers) print(result) ``` **Expected Output**: ```python { \'modulus\': [2.23606797749979, 4.242640687119285, 2.8284271247461903, 2.0, 5.5], \'phase\': [1.1071487177940904, -0.7853981633974483, 2.356194490192345, 0.0, 0.0], \'polar_coordinates\': [ (2.23606797749979, 1.1071487177940904), (4.242640687119285, -0.7853981633974483), (2.8284271247461903, 2.356194490192345), (2.0, 0.0), (5.5, 0.0) ], \'rectangular_coordinates\': [ (1+2j), (3-3j), (-2+2j), (2+0j), (5.5+0j) ] } ``` The provided problem tests the student\'s ability to effectively use the cmath module, handle complex number conversions, and deal with different types of numerical inputs.","solution":"import cmath def analyze_complex_numbers(complex_list): # Initialize lists to store results moduli = [] phases = [] polar_coords = [] rect_coords = [] for number in complex_list: # Convert the number to a complex number if it\'s not already z = complex(number) # Compute modulus and phase modulus = abs(z) phase = cmath.phase(z) # Compute polar coordinates polar = cmath.polar(z) # Convert polar coordinates back to rectangular coordinates rect = cmath.rect(polar[0], polar[1]) # Append results to lists moduli.append(modulus) phases.append(phase) polar_coords.append(polar) rect_coords.append(rect) return { \'modulus\': moduli, \'phase\': phases, \'polar_coordinates\': polar_coords, \'rectangular_coordinates\': rect_coords }"},{"question":"You are given two CSV files containing sales data of two different regions. Your task is to read these CSV files into pandas DataFrames and implement a function `compare_sales_data` that performs the following operations: 1. **Compare DataFrames**: Use `pd.testing.assert_frame_equal` to compare if the two DataFrames are equal. 2. **Handle Exceptions**: If the DataFrames are not equal, catch the `AssertionError` and handle specific pandas exceptions that might occur during the comparison. 3. **Custom Assertion**: Implement a custom assertion check that ensures the total sales value column in both DataFrames are equal, ignoring the order of rows and extra columns in either file. 4. **Return Result**: Return a dictionary containing the results of these checks: - The result of the `assert_frame_equal` check. - The result of the custom total sales equality check. - A list of any pandas-specific exceptions raised during the operations. # Function Signature ```python import pandas as pd def compare_sales_data(file_path_1: str, file_path_2: str) -> dict: pass ``` # Input - `file_path_1` (str): Path to the first CSV file. - `file_path_2` (str): Path to the second CSV file. # Output - Return a dictionary with the keys: - `\\"frame_equal\\"`: A boolean indicating if the DataFrames are exactly equal. - `\\"total_sales_equal\\"`: A boolean indicating if the total sales are equal in both DataFrames. - `\\"exceptions_raised\\"`: A list of pandas exception names that were encountered. # Constraints - Assume both CSV files have the same column names. - The total sales column is named `\\"total_sales\\"`. - You may not use any other libraries apart from pandas and standard Python libraries. # Example ```python # Example CSV Content # file1.csv product,region,total_sales A,North,100 B,North,150 C,North,130 # file2.csv product,region,total_sales C,North,130 B,North,150 A,North,100 result = compare_sales_data(\'file1.csv\', \'file2.csv\') # Expected Output { \\"frame_equal\\": False, # This should be False because the row order is different. \\"total_sales_equal\\": True, # This should be True because the total sales values are the same regardless of order. \\"exceptions_raised\\": [\\"AssertionError\\"] } ``` # Notes - Ensure your function is robust and can handle different types of data inconsistencies. - Catch and handle the specified pandas errors and provide meaningful insights where applicable.","solution":"import pandas as pd def compare_sales_data(file_path_1: str, file_path_2: str) -> dict: exceptions_raised = [] # Read CSV files into DataFrames try: df1 = pd.read_csv(file_path_1) df2 = pd.read_csv(file_path_2) except Exception as e: return { \\"frame_equal\\": False, \\"total_sales_equal\\": False, \\"exceptions_raised\\": [str(type(e).__name__)] } # Check if DataFrames are exactly equal try: pd.testing.assert_frame_equal(df1, df2) frame_equal = True except AssertionError as e: frame_equal = False exceptions_raised.append(\\"AssertionError\\") # Custom check for total sales equality try: total_sales_equal = df1[\'total_sales\'].sum() == df2[\'total_sales\'].sum() except KeyError as e: total_sales_equal = False exceptions_raised.append(\\"KeyError\\") except Exception as e: total_sales_equal = False exceptions_raised.append(str(type(e).__name__)) return { \\"frame_equal\\": frame_equal, \\"total_sales_equal\\": total_sales_equal, \\"exceptions_raised\\": exceptions_raised }"},{"question":"# Question: Dynamic Importing and Metadata Extraction in Python **Objective**: This question assesses your ability to dynamically import and manipulate Python modules, and to work with package metadata. **Background**: You are tasked with creating a utility function that: 1. Dynamically imports a specified module. 2. Retrieves and prints metadata information about the module. 3. Lists all unique dependencies used within a provided Python script. **Function Specification**: You need to implement a function `import_module_and_analyze(script_path: str, module_name: str) -> None` that performs the following tasks: - **Input**: - `script_path` (str): A file path to a Python script. - `module_name` (str): The name of the module to be dynamically imported. - **Output**: - The function does not return anything but should print the following information: 1. The version of the dynamically imported module. 2. The entry points and distribution files of the module. 3. A list of all unique dependencies used in the provided Python script. **Constraints**: - You should handle cases where the module does not exist, printing an appropriate error message. - The function should be efficient and avoid unnecessary computations. - Use the `importlib`, `importlib.metadata`, and `modulefinder` modules. **Example**: Suppose you have a script `example_script.py` and you want to analyze the module `collections`. ```python example_script.py: ____________________ import os import sys import collections def example_function(): print(\\"This is an example function.\\") ____________________ # Calling the function import_module_and_analyze(\\"example_script.py\\", \\"collections\\") ``` **Expected Output**: ``` Module \'collections\' version: 0.0.0 Entry Points: [] Distribution Files: [<list of files>] Dependencies used in example_script.py: [\'os\', \'sys\', \'collections\'] ``` Note: - The version may vary based on your installed version of Python modules. - The list of entry points and distribution files may differ and should be derived from the `importlib.metadata` module. **Additional Information**: - You can use `importlib.metadata.version()` to get the version of a module. - Use `importlib.metadata.distributions()` to get distribution files and entry points. - Use `modulefinder.ModuleFinder` to get the list of dependencies from the script. Ensure your function handles errors gracefully and prints informative messages for any issues encountered.","solution":"import importlib import importlib.metadata import modulefinder import os def import_module_and_analyze(script_path: str, module_name: str) -> None: # Dynamically import the module try: module = importlib.import_module(module_name) except ModuleNotFoundError: print(f\\"Module \'{module_name}\' not found.\\") return # Retrieve and print module version try: version = importlib.metadata.version(module_name) print(f\\"Module \'{module_name}\' version: {version}\\") except importlib.metadata.PackageNotFoundError: print(f\\"Version information for module \'{module_name}\' not found.\\") # Retrieve and print entry points and distribution files try: distribution = importlib.metadata.distribution(module_name) entry_points = list(distribution.entry_points) files = list(distribution.files) print(f\\"Entry Points: {entry_points}\\") print(f\\"Distribution Files: {files}\\") except importlib.metadata.PackageNotFoundError: print(f\\"No distribution information found for module \'{module_name}\'.\\") # List all unique dependencies used in the provided Python script if not os.path.isfile(script_path): print(f\\"Script file \'{script_path}\' not found.\\") return finder = modulefinder.ModuleFinder() try: finder.run_script(script_path) except Exception as e: print(f\\"Error analyzing script \'{script_path}\': {e}\\") return dependencies = set(module.__name__ for module in finder.modules.values()) print(f\\"Dependencies used in {os.path.basename(script_path)}: {sorted(dependencies)}\\")"},{"question":"Objective: Write a Python function that reads an AIFF or AIFF-C audio file, modifies its sample rate to a new specified value, and writes the updated audio data to a new AIFF file with the suffix \\"_updated\\". You should use the functionalities provided by the `aifc` module to accomplish this task. Function Specification: ```python def update_sample_rate(input_filename: str, new_framerate: int) -> None: Reads an AIFF or AIFF-C audio file, updates its sampling rate, and writes the modified audio data to a new AIFF file with \'_updated\' added to the original file name. Parameters: - input_filename: str : The name of the input AIFF or AIFF-C file. - new_framerate: int : The new frame rate to be set for the audio file. Returns: - None : This function does not return any value but writes the updated file to disk. pass ``` Input Format: - `input_filename` (string): The filename of the AIFF or AIFF-C audio file you want to modify. - `new_framerate` (int): The new sampling rate to set in the updated audio file. Output Format: - The output should be a new AIFF file saved to disk with \'_updated\' appended to the original filename. For example, if the input file is `audio.aiff`, the output file should be `audio_updated.aiff`. Constraints: - The provided input file will always be a valid AIFF or AIFF-C file. - The `new_framerate` will be a positive integer. Example: Suppose `input_filename` is `\'test.aiff\'`, and `new_framerate` is `48000`. After the function execution, a new file `test_updated.aiff` should be created with its sampling rate updated to `48000`. Notes: - You can use the methods provided by the `aifc` module such as `aifc.open()`, `getframerate()`, `setframerate()`, and others as needed. - Ensure the new file retains all other parameters (e.g., number of channels, sample width, number of frames) from the original file except for the updated sampling rate. **Good luck!**","solution":"import aifc def update_sample_rate(input_filename: str, new_framerate: int) -> None: # Open the input audio file with aifc.open(input_filename, \'rb\') as input_file: # Read audio parameters and frames n_channels = input_file.getnchannels() sampwidth = input_file.getsampwidth() n_frames = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() frames = input_file.readframes(n_frames) # Create the updated file name output_filename = input_filename.replace(\'.aiff\', \'_updated.aiff\') # Write the output audio file with the updated sample rate with aifc.open(output_filename, \'wb\') as output_file: output_file.setnchannels(n_channels) output_file.setsampwidth(sampwidth) output_file.setnframes(n_frames) output_file.setcomptype(comptype, compname) output_file.setframerate(new_framerate) output_file.writeframes(frames)"},{"question":"<|Analysis Begin|> Upon reading the provided PyTorch documentation on \\"Gradcheck mechanics\\", it\'s clear that it delves into the internals and intricacies of the `gradcheck` and `gradgradcheck` functionalities in PyTorch. This includes both real-to-real and complex-to-real functions, and discusses both numeric and analytical evaluations of gradients. Key concepts include Jacobian matrices, finite difference approximations, and both forward and backward mode automatic differentiation. Given the advanced nature of this material, an appropriate assessment question should challenge students on higher-level concepts such as implementing and verifying gradient computation for custom functions, ensuring they understand numerical approximation (finite differences) and analytical solutions using PyTorch’s autograd. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** To test your understanding of PyTorch\'s gradient checking mechanisms, your task is to implement a custom PyTorch function and verify its gradient using both numerical and analytical methods. You will use the `gradcheck` utility provided by PyTorch to ensure the correctness of your implementation. # Problem Statement: You are required to: 1. Implement a custom function in PyTorch that takes a real-valued input and outputs a real-valued result. 2. Implement the same function but for complex-valued input and output. 3. Use PyTorch’s `gradcheck` to verify the correctness of the gradients for both functions. # Instructions: 1. **Implement the Real-Valued Function:** - Define a function `f_real(x)` where ( f(x) = x^3 + 5x^2 + 2x + 1 ). - Compute the Jacobian matrix for this function numerically using finite differences. - Verify this Jacobian using PyTorch\'s autograd. 2. **Implement the Complex-Valued Function:** - Define a function `f_complex(z)` where ( f(z) = z^3 + 5z^2 + 2z + 1 ) with ( z ) being a complex tensor. - Compute the Jacobian matrix for this function numerically using finite differences as applicable to complex numbers. - Verify this Jacobian using PyTorch\'s autograd. # Input and Output: - For the real function `f_real(x)`, the input `x` is a real-valued tensor. - For the complex function `f_complex(z)`, the input `z` is a complex-valued tensor. # Sample Code Structure: ```python import torch from torch.autograd import gradcheck # Implement the real-valued function def f_real(x): # Your implementation here pass # Implement the complex-valued function def f_complex(z): # Your implementation here pass # Numerical Jacobian computation using finite differences def numerical_jacobian_real(x, eps=1e-6): # Your implementation here pass def numerical_jacobian_complex(z, eps=1e-6): # Your implementation here pass # Analytical Jacobian computation using PyTorch autograd def analytical_jacobian_real(x): # Your implementation here pass def analytical_jacobian_complex(z): # Your implementation here pass # Gradient check def verify_gradients(): x_real = torch.randn(1, dtype=torch.double, requires_grad=True) z_complex = torch.randn(1, dtype=torch.complex128, requires_grad=True) gradcheck_real = gradcheck(f_real, (x_real,), eps=1e-6, atol=1e-4) gradcheck_complex = gradcheck(f_complex, (z_complex,), eps=1e-6, atol=1e-4) return gradcheck_real, gradcheck_complex # Test your implementation if __name__ == \\"__main__\\": result_real, result_complex = verify_gradients() print(f\\"Real function gradients verified: {result_real}\\") print(f\\"Complex function gradients verified: {result_complex}\\") ``` # Constraints: - The input tensors should be small to avoid numerical instability. - Use a tolerance level (`atol`) of 1e-4 for gradient verification. # Performance Requirements: - Ensure that the numerical gradient computation is efficient and correctly aligns with the analytical gradients from PyTorch autograd. - Carefully handle the complex number operations for gradients, paying attention to numerical precision. Submit your implementation along with a brief explanation of each part of your code.","solution":"import torch from torch.autograd import Function, gradcheck # Implement the real-valued function def f_real(x): return x**3 + 5*x**2 + 2*x + 1 # Implement the complex-valued function def f_complex(z): return z**3 + 5*z**2 + 2*z + 1 # Gradient check function def verify_gradients(): x_real = torch.randn(1, dtype=torch.double, requires_grad=True) z_complex = torch.randn(1, dtype=torch.complex128, requires_grad=True) # Gradient check for the real-valued function gradcheck_real = gradcheck(f_real, (x_real,), eps=1e-6, atol=1e-4) # Gradient check for the complex-valued function gradcheck_complex = gradcheck(f_complex, (z_complex,), eps=1e-6, atol=1e-4) return gradcheck_real, gradcheck_complex # Test the implementation if __name__ == \\"__main__\\": result_real, result_complex = verify_gradients() print(f\\"Real function gradients verified: {result_real}\\") print(f\\"Complex function gradients verified: {result_complex}\\")"},{"question":"# Question: Implement a Custom Context Manager Using `contextlib` You are required to implement a custom context manager using Python\'s `contextlib` module. Your context manager should manage the usage of a database connection. The context manager should: 1. Open a connection to a mock database when entering the context. 2. Yield the connection object for use within the `with` block. 3. Ensure the database connection is closed when exiting the context, even if an exception is raised inside the `with` block. Additionally, you need to implement an asynchronous version of this context manager using `@asynccontextmanager`. Requirements 1. Implement a synchronous context manager called `db_connection` using `@contextmanager`. 2. Implement an asynchronous context manager called `adb_connection` using `@asynccontextmanager`. # Synchronous Context Manager: `db_connection` Input: - None Output: - Yields a `MockDBConnection` object. # Asynchronous Context Manager: `adb_connection` Input: - None Output: - Asynchronously yields a `MockDBConnection` object. Mock Database Connection Class ```python class MockDBConnection: def __init__(self): self.connected = False def connect(self): self.connected = True print(\\"Database connected\\") def disconnect(self): self.connected = False print(\\"Database disconnected\\") async def aconnect(self): self.connected = True print(\\"Database connected (async)\\") async def adisconnect(self): self.connected = False print(\\"Database disconnected (async)\\") ``` # Implementation Example Usage **Synchronous Example:** ```python with db_connection() as conn: assert conn.connected # Connection should be open within the block print(\\"Using the database connection\\") assert not conn.connected # Connection should be closed after the block ``` **Asynchronous Example:** ```python import asyncio async def use_async_db(): async with adb_connection() as conn: assert conn.connected # Connection should be open within the block print(\\"Using the database connection\\") assert not conn.connected # Connection should be closed after the block asyncio.run(use_async_db()) ``` # Constraints - Ensure that the database connection is always properly closed, even if an exception is raised within the `with` block. # Notes - You are not required to implement the `MockDBConnection` class since it is provided above. - Focus on managing the context correctly using `contextlib`. Implement the `db_connection` and `adb_connection` functions below: ```python from contextlib import contextmanager, asynccontextmanager @contextmanager def db_connection(): conn = MockDBConnection() conn.connect() try: yield conn finally: conn.disconnect() @asynccontextmanager async def adb_connection(): conn = MockDBConnection() await conn.aconnect() try: yield conn finally: await conn.adisconnect() ```","solution":"from contextlib import contextmanager, asynccontextmanager class MockDBConnection: def __init__(self): self.connected = False def connect(self): self.connected = True print(\\"Database connected\\") def disconnect(self): self.connected = False print(\\"Database disconnected\\") async def aconnect(self): self.connected = True print(\\"Database connected (async)\\") async def adisconnect(self): self.connected = False print(\\"Database disconnected (async)\\") @contextmanager def db_connection(): conn = MockDBConnection() conn.connect() try: yield conn finally: conn.disconnect() @asynccontextmanager async def adb_connection(): conn = MockDBConnection() await conn.aconnect() try: yield conn finally: await conn.adisconnect()"},{"question":"Context You are required to manage resources that need to be opened, processed, and closed reliably. Instead of manually implementing the resource management logic, you can leverage Python\'s `contextlib` module to handle these cases more efficiently. Problem Statement Implement a custom context manager named `ResourceHandler` using the `contextlib` module. This context manager should: 1. Acquire resources at the start. 2. Yield the resources to the block managed by the \\"with\\" statement. 3. Ensure resources are properly released upon exiting the block, even if an exception occurs. 4. Optionally, suppress specific exceptions if specified. Function Signature ```python from contextlib import contextmanager @contextmanager def ResourceHandler(resource_initializer, resource_closer, *exceptions_to_suppress): :param resource_initializer: A callable that initializes and returns the resource. :param resource_closer: A callable that takes the resource as an argument and closes it. :param exceptions_to_suppress: Variable length argument list of exceptions to be suppressed. :yield: The initialized resource. ``` Constraints 1. The `resource_initializer` should be a callable that initializes and returns a resource. 2. The `resource_closer` should be a callable that closes the resource passed to it. 3. The context manager should handle situations where specific exceptions (if any) are suppressed based on the provided exceptions. Example Usage ```python class Resource: def __init__(self, name): self.name = name print(f\\"{self.name} initialized\\") def close(self): print(f\\"{self.name} closed\\") def initialize_resource(): resource = Resource(\\"MyResource\\") return resource def close_resource(resource): resource.close() # Case 1: Normal usage try: with ResourceHandler(initialize_resource, close_resource) as resource: print(f\\"Using {resource.name}\\") except Exception as e: print(f\\"Exception caught: {e}\\") # Case 2: Suppressing specific exception try: with ResourceHandler(initialize_resource, close_resource, KeyError) as resource: print(f\\"Using {resource.name}\\") raise KeyError(\\"An expected error\\") print(\\"Block executed successfully despite KeyError\\") except Exception as e: print(f\\"Exception caught: {e}\\") # Expected Output: # MyResource initialized # Using MyResource # MyResource closed # MyResource initialized # Using MyResource # MyResource closed # Block executed successfully despite KeyError ``` Notes 1. Ensure your implementation properly acquires, yields, and releases the resource. 2. Your implementation should suppress exceptions specified in the `exceptions_to_suppress` parameter while allowing other exceptions to propagate normally.","solution":"from contextlib import contextmanager @contextmanager def ResourceHandler(resource_initializer, resource_closer, *exceptions_to_suppress): A context manager to handle resources, ensuring proper acquisition and release of resources. :param resource_initializer: A callable that initializes and returns the resource. :param resource_closer: A callable that takes the resource as an argument and closes it. :param exceptions_to_suppress: Variable length argument list of exceptions to be suppressed. :yield: The initialized resource. resource = resource_initializer() try: yield resource except exceptions_to_suppress: pass finally: resource_closer(resource)"},{"question":"You are tasked with creating an asynchronous chat server that will handle messages from multiple clients using the deprecated `asynchat` module. Your goal is to create a subclass of `asynchat.async_chat` to handle incoming messages and broadcast them to all connected clients. Requirements 1. Implement a class `ChatHandler` that inherits from `asynchat.async_chat`. 2. The server should accept connections from clients and handle incoming messages. 3. Clients should be able to send messages to the server, and the server should broadcast each message to all connected clients, except the sender. 4. The server should use a delimiter such as \\"n\\" to separate messages from different clients. 5. Implement the following methods: - `collect_incoming_data()`: Append the incoming data to an internal buffer. - `found_terminator()`: Process complete messages when the \\"n\\" terminator is found and broadcast the message. - `broadcast_message()`: Send the provided message to all connected clients except the sender. Constraints - You must use the `asynchat.async_chat` class as the base class for `ChatHandler`. - Ensure appropriate handling of client connections and disconnections. Input and Output Format - **Input**: Messages from clients are sent as bytes and are terminated by the \\"n\\" newline character. - **Output**: The server should broadcast received messages to all connected clients except the sender. Example If three clients A, B, and C are connected, and client A sends a message \\"Hello\\" to the server, the server should send \\"Hello\\" to clients B and C, but not to client A. ```python import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): asynchat.async_chat.__init__(self, sock) self.clients = clients self.ibuffer = [] self.set_terminator(b\'n\') def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Process a complete message message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast_message(message) def broadcast_message(self, message): Send the message to all clients except the sender for client in self.clients: if client is not self: client.push(message.encode(\'utf-8\') + b\'n\') class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): print(\'Incoming connection from %s\' % repr(addr)) self.clients.append(ChatHandler(sock, self.clients)) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8888) asyncore.loop() ``` This code provides a skeleton for you to complete and extend as necessary to meet the requirements. Be sure to test thoroughly with multiple clients to ensure robust message handling and broadcasting.","solution":"import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): asynchat.async_chat.__init__(self, sock) self.clients = clients self.ibuffer = [] self.set_terminator(b\'n\') def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Process a complete message message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast_message(message) def broadcast_message(self, message): Send the message to all clients except the sender for client in self.clients: if client is not self: client.push(message.encode(\'utf-8\') + b\'n\') class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): print(\'Incoming connection from %s\' % repr(addr)) self.clients.append(ChatHandler(sock, self.clients)) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8888) asyncore.loop()"},{"question":"You are tasked with creating a set of classes to manage a basic inventory system for a small store. The store sells various products, each of which can be uniquely identified and categorized into different types. Your task is to: 1. Define a base class `Product` with the following properties: - `name` (string) - `price` (float) - `quantity` (integer) The class should have methods to: - `get_total_price()` - returns the total price of the current quantity of product. - `restock(amount)` - increases the quantity by the given amount. 2. Define a derived class `PerishableProduct` that adds an expiration date to the `Product` class. It should have: - `expiration_date` (string in format \'YYYY-MM-DD\') This class should override the `get_total_price()` method to calculate the total price based on whether the product has expired (use current date to compare). If expired, total price is zero. 3. Implement a `Store` class that uses these product classes. The `Store` class should: - Maintain a list of products. - Implement an `add_product(product)` method to add a new product to the store\'s inventory. - Implement an `inventory_value()` method to return the total value of inventory. - Implement an iterator to iterate over all products. 4. Finally, create some generator expressions to: - Find all products that are low in stock (quantity <= 5). - Calculate the total value of perishable products that are not expired. Input: - Various commands to create objects and manipulate the inventory system. Output: - Values calculated by different methods. ```python from datetime import datetime class Product: def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity def get_total_price(self): return self.price * self.quantity def restock(self, amount): self.quantity += amount class PerishableProduct(Product): def __init__(self, name, price, quantity, expiration_date): super().__init__(name, price, quantity) self.expiration_date = expiration_date def get_total_price(self): if datetime.strptime(self.expiration_date, \'%Y-%m-%d\') < datetime.now(): return 0.0 return super().get_total_price() class Store: def __init__(self): self.products = [] def add_product(self, product): self.products.append(product) def inventory_value(self): return sum(product.get_total_price() for product in self.products) def __iter__(self): return iter(self.products) # Usage store = Store() apple = PerishableProduct(\\"Apple\\", 1.0, 10, \\"2025-12-31\\") banana = PerishableProduct(\\"Banana\\", 0.8, 15, \\"2023-11-02\\") flashlight = Product(\\"Flashlight\\", 15.0, 5) store.add_product(apple) store.add_product(banana) store.add_product(flashlight) # Calculate total inventory value print(store.inventory_value()) # Generator for low stock products low_stock_products = (product for product in store if product.quantity <= 5) print(list(low_stock_products)) # Generator for total value of non-expired perishable products non_expired_value = sum(product.get_total_price() for product in store if isinstance(product, PerishableProduct) and product.get_total_price() > 0) print(non_expired_value) ``` **Note:** You must ensure that all inputs provided to the class instantiations are valid. Consider edge cases such as attempts to restock negative amounts, unrealistic prices, or expired products.","solution":"from datetime import datetime class Product: def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity def get_total_price(self): return self.price * self.quantity def restock(self, amount): if amount < 0: raise ValueError(\\"Restock amount cannot be negative\\") self.quantity += amount class PerishableProduct(Product): def __init__(self, name, price, quantity, expiration_date): super().__init__(name, price, quantity) self.expiration_date = expiration_date def get_total_price(self): if datetime.strptime(self.expiration_date, \'%Y-%m-%d\') < datetime.now(): return 0.0 return super().get_total_price() class Store: def __init__(self): self.products = [] def add_product(self, product): self.products.append(product) def inventory_value(self): return sum(product.get_total_price() for product in self.products) def __iter__(self): return iter(self.products) # Usage store = Store() apple = PerishableProduct(\\"Apple\\", 1.0, 10, \\"2025-12-31\\") banana = PerishableProduct(\\"Banana\\", 0.8, 15, \\"2023-11-02\\") flashlight = Product(\\"Flashlight\\", 15.0, 5) store.add_product(apple) store.add_product(banana) store.add_product(flashlight) # Calculate total inventory value print(store.inventory_value()) # Generator for low stock products low_stock_products = (product for product in store if product.quantity <= 5) print(list(low_stock_products)) # Generator for total value of non-expired perishable products non_expired_value = sum(product.get_total_price() for product in store if isinstance(product, PerishableProduct) and product.get_total_price() > 0) print(non_expired_value)"},{"question":"# Question: Advanced Data Visualization using Seaborn and Transformations Objective: Create a combined visualization using the `seaborn.objects` module. You need to load and use two different datasets (`penguins` and `diamonds`), then create two subplots featuring advanced transformation techniques to effectively display the data. Requirements: 1. Use the `penguins` dataset to create a subplot visualizing the distribution of `body_mass_g` for each `species`. 2. Use the `diamonds` dataset to create a subplot visualizing the distribution of `carat` for each `clarity` level. 3. Apply appropriate transformations (`Jitter`, `Shift`, `Perc`) on both subplots to ensure clear data visualization without overlaps. Your function should return a seaborn plot object (e.g., `seaborn.axisgrid.FacetGrid`). Function Signature: ```python def create_combined_visualization() -> so.Plot: pass ``` Implementation Details: 1. **Input Data:** Utilize the `seaborn.load_dataset` to load `penguins` and `diamonds` datasets. 2. **Data Visualization:** * For `penguins` data: - x-axis: `species` - y-axis: `body_mass_g` - Apply `Dots`, `Jitter`, and a range transformation to display the interquartile range. * For `diamonds` data: - x-axis: `carat` - y-axis: `clarity` - Apply `Dots`, `Jitter`, and a range transformation to display the interquartile range. 3. **Combining Plots:** Use an appropriate method to combine these two plots into subplots within a single figure for a comprehensive overview. Example Output: A multi-plot visualization with subplots for each dataset showing the effective use of Seaborn\'s `objects` module transformations. Constraints: - Ensure the function is self-contained and imports necessary modules within the function definition. - Focus on using the `objects` module of Seaborn for the plots. Performance: - The function should handle the default size of the datasets efficiently and produce plots in a reasonable time frame. Example Usage: ```python plot = create_combined_visualization() # this should create and return the combined visualization with two subplots ```","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_combined_visualization() -> so.Plot: # Load datasets penguins = sns.load_dataset(\\"penguins\\") diamonds = sns.load_dataset(\\"diamonds\\") # Create subplots fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # Plot for penguins dataset penguins_plot = (so.Plot(penguins, x=\'species\', y=\'body_mass_g\') .add(so.Dots(), so.Jitter(.3), so.Shift(0)) .add(so.Range(), so.Perc(50))) penguins_plot.on(axs[0]) # Plot for diamonds dataset diamonds_plot = (so.Plot(diamonds, x=\'carat\', y=\'clarity\') .add(so.Dots(), so.Jitter(.3), so.Shift(0)) .add(so.Range(), so.Perc(50))) diamonds_plot.on(axs[1]) return fig"},{"question":"Objective: Implement a function that uses the `RobotFileParser` class from the `urllib.robotparser` module to validate access permissions for multiple URLs from multiple sources `robots.txt` files for a specified user agent. Task: Write a function `check_url_permissions(robot_files, useragent, url_list)` that takes the following parameters: 1. `robot_files`: A dictionary where each key is a website\'s base URL (as a string) and the corresponding value is the URL to the `robots.txt` file for that website. 2. `useragent`: A string representing the user agent whose permissions need to be checked. 3. `url_list`: A list of dictionaries, where each dictionary contains two keys: `\'site\'` (the base URL of the website) and `\'url\'` (the specific URL to be checked on that site). The function should return a list of booleans where each boolean corresponds to whether the user agent is allowed to fetch the respective URL in `url_list`. Input and Output Formats: - Input: ```python robot_files = { \'http://www.example.com\': \'http://www.example.com/robots.txt\', \'http://www.testsite.com\': \'http://www.testsite.com/robots.txt\' } useragent = \'*\' url_list = [ {\'site\': \'http://www.example.com\', \'url\': \'http://www.example.com/page1\'}, {\'site\': \'http://www.testsite.com\', \'url\': \'http://www.testsite.com/page2\'} ] ``` - Output: ```python [True, False] ``` Constraints: - You may assume that all `robots.txt` files are accessible and contain valid data. - URL lists may have duplicate sites. - Time complexity should be efficient for up to 100 URLs in `url_list`. Example: ```python import urllib.robotparser def check_url_permissions(robot_files, useragent, url_list): robots_parsers = {} # Initialize and read robots.txt for each site for site, robots_url in robot_files.items(): parser = urllib.robotparser.RobotFileParser() parser.set_url(robots_url) parser.read() robots_parsers[site] = parser results = [] for url_info in url_list: site = url_info[\'site\'] url = url_info[\'url\'] parser = robots_parsers.get(site) if parser and parser.can_fetch(useragent, url): results.append(True) else: results.append(False) return results # Example Usage: robot_files_example = { \'http://www.example.com\': \'http://www.example.com/robots.txt\', \'http://www.testsite.com\': \'http://www.testsite.com/robots.txt\' } useragent_example = \'*\' url_list_example = [ {\'site\': \'http://www.example.com\', \'url\': \'http://www.example.com/page1\'}, {\'site\': \'http://www.testsite.com\', \'url\': \'http://www.testsite.com/page2\'} ] print(check_url_permissions(robot_files_example, useragent_example, url_list_example)) ``` Expected output: ``` [True, False] ```","solution":"import urllib.robotparser def check_url_permissions(robot_files, useragent, url_list): robots_parsers = {} # Initialize and read robots.txt for each site for site, robots_url in robot_files.items(): parser = urllib.robotparser.RobotFileParser() parser.set_url(robots_url) parser.read() robots_parsers[site] = parser # Evaluate permissions for each URL in url_list results = [] for url_info in url_list: site = url_info[\'site\'] url = url_info[\'url\'] parser = robots_parsers.get(site) if parser and parser.can_fetch(useragent, url): results.append(True) else: results.append(False) return results"},{"question":"**Objective:** Demonstrate your understanding of the deprecated `imp` module and its modern counterpart, `importlib`. **Question:** In Python, the `imp` module (now deprecated) provided functions for importing and manipulating modules. Your task is to create a script that uses the newer `importlib` module to replicate the functionalities of the `imp.find_module()` and `imp.load_module()` functions to dynamically import a module given its name. Requirements: 1. Implement a function `dynamic_import(module_name: str) -> ModuleType` that performs the following: - Searches for the specified module using functionality equivalent to `imp.find_module()`. - Loads the found module in a manner similar to `imp.load_module()`. 2. Ensure that your implementation handles hierarchical module names (e.g., `package.subpackage.module`). 3. Raise appropriate exceptions if the module cannot be found or loaded. Function Signature: ```python import types def dynamic_import(module_name: str) -> types.ModuleType: pass ``` Example Usage: ```python # Assuming there is a module named \'example_module\' module = dynamic_import(\'example_module\') print(module) ``` Constraints: - Do not use the deprecated `imp` module in your implementation. - Your implementation should be compatible with Python 3.6 and above. Hints: - Look into `importlib.util.find_spec` for finding module specifications. - Use `importlib.util.module_from_spec` to create a module from the specification. - Handle file closures appropriately to prevent resource leaks. **Explanation:** This question tests the student\'s ability to: 1. Translate deprecated functionality (`imp.find_module` and `imp.load_module`) to its modern equivalents using `importlib`. 2. Implement error-handling mechanisms for module import operations. 3. Demonstrate a practical understanding of module import mechanics in Python.","solution":"import importlib.util import types def dynamic_import(module_name: str) -> types.ModuleType: Dynamically imports a module given its name using importlib. try: # Find the module spec based on the module name spec = importlib.util.find_spec(module_name) if spec is None: raise ModuleNotFoundError(f\\"Module \'{module_name}\' not found\\") # Create a module from the specification module = importlib.util.module_from_spec(spec) # Execute the module in its own namespace loader = spec.loader if loader is None or not hasattr(loader, \'exec_module\'): raise ImportError(f\\"Loader for module \'{module_name}\' does not support exec_module\\") loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"An error occurred while importing module \'{module_name}\': {str(e)}\\")"},{"question":"**Objective**: Demonstrate your understanding of creating and applying custom color palettes using seaborn. **Task**: You are required to create a function `plot_custom_heatmap(data, color_type, color_spec, num_colors, as_cmap)` that takes the following inputs and generates a heatmap using a custom color palette: - `data` (2D list of numerical values): The input data for the heatmap. - `color_type` (str): Specifies the type of input for the color. It could be: - `\\"named\\"` for a named color (e.g., \\"seagreen\\") - `\\"hex\\"` for a hex color code (e.g., \\"#79C\\") - `\\"husl\\"` for husl color specifications (a tuple of three values, e.g., (20, 60, 50)) - `color_spec` (varies by `color_type`): - a named color string if color_type is `\\"named\\"` - a hexadecimal string if color_type is `\\"hex\\"` - a tuple of three integers if color_type is `\\"husl\\"` - `num_colors` (int): The number of colors in the palette (optional, default is 6). - `as_cmap` (bool): Whether to return the palette as a continuous colormap (optional, default is False). **Output**: The function should generate and display a heatmap with the specified custom color palette. If `as_cmap` is True, use the palette as a continuous colormap. **Example**: ```python import matplotlib.pyplot as plt import seaborn as sns import numpy as np # Example usage of the function data = np.random.rand(10, 12) # Using named color plot_custom_heatmap(data, \\"named\\", \\"seagreen\\", 8, False) # Using hex code plot_custom_heatmap(data, \\"hex\\", \\"#79C\\", 6, False) # Using husl color plot_custom_heatmap(data, \\"husl\\", (20, 60, 50), 10, True) ``` **Constraints**: 1. The function should handle invalid inputs gracefully, raising appropriate exceptions with informative messages. 2. Ensure that the color palette is correctly applied to the heatmap. **Performance requirements**: - The function should be efficient in handling typical heatmap sizes (e.g., 10x12 matrix). **Note**: The students are expected to use seaborn and matplotlib libraries for plotting.","solution":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np def plot_custom_heatmap(data, color_type, color_spec, num_colors=6, as_cmap=False): Generates and displays a heatmap with the specified custom color palette. Parameters: - data: 2D list or numpy array of numerical values for the heatmap - color_type: str specifying the type of color (\'named\', \'hex\', or \'husl\') - color_spec: varies by `color_type` - num_colors: int specifying the number of colors in the palette - as_cmap: bool specifying whether to return the palette as a continuous colormap Returns: - None # Create color palette based on color_type if color_type == \\"named\\": palette = sns.light_palette(color_spec, n_colors=num_colors, as_cmap=as_cmap) elif color_type == \\"hex\\": palette = sns.light_palette(color_spec, n_colors=num_colors, as_cmap=as_cmap) elif color_type == \\"husl\\": palette = sns.husl_palette(num_colors, h=color_spec[0], s=color_spec[1], l=color_spec[2], as_cmap=as_cmap) else: raise ValueError(\\"Invalid color_type. It should be \'named\', \'hex\', or \'husl\'.\\") # Create and display the heatmap sns.heatmap(data, cmap=palette, linewidths=.5) plt.show()"},{"question":"# WAV File Manipulation and Audio Processing **Objective:** Implement a function that reads a WAV file, processes the audio data by applying an audio effect (e.g., volume normalization), and writes the processed audio back to a new WAV file. # Function Signature ```python def normalize_wave_file(input_filepath: str, output_filepath: str) -> None: Reads an input WAV file, normalizes its volume, and writes the normalized audio to an output WAV file. Parameters: - input_filepath (str): The file path to the input WAV file. - output_filepath (str): The file path to the output WAV file to be created. Returns: None ``` # Details and Constraints 1. **Reading the WAV file:** - Use the `wave` module to read the audio data from the input WAV file. - Ensure that you correctly handle both mono and stereo WAV files. 2. **Normalizing Volume:** - Normalize the audio data such that the maximum absolute value of the sample is set to the maximum possible value for the sample type (e.g., -1.0 to 1.0 for 32-bit float samples). - Avoid clipping by ensuring that no sample exceeds the maximum range after normalization. 3. **Writing the WAV file:** - Use the `wave` module to write the normalized audio data to the output WAV file. 4. **Performance:** - Ensure that your implementation is efficient and can handle large WAV files without excessive memory usage or processing time. # Example Usage: ```python input_file = \'input.wav\' output_file = \'output.wav\' normalize_wave_file(input_file, output_file) ``` # Additional Notes: - You are not required to implement GUI for file selection. - Assume that all input WAV files are properly formatted and not corrupted. - If any exceptions occur during the processing (e.g., file not found, read/write errors), your function should handle them gracefully, perhaps simply printing an error message. # Submission: - Submit a Python script with the function `normalize_wave_file` defined as specified. - Ensure your script can be executed as a stand-alone program for testing purposes.","solution":"import wave import numpy as np def normalize_wave_file(input_filepath: str, output_filepath: str) -> None: Reads an input WAV file, normalizes its volume, and writes the normalized audio to an output WAV file. Parameters: - input_filepath (str): The file path to the input WAV file. - output_filepath (str): The file path to the output WAV file to be created. Returns: None try: with wave.open(input_filepath, \'rb\') as wav_in: params = wav_in.getparams() sample_width = params.sampwidth n_channels = params.nchannels framerate = params.framerate n_frames = params.nframes raw_audio_data = wav_in.readframes(n_frames) if sample_width == 2: # Assuming 16-bit PCM audio_data = np.frombuffer(raw_audio_data, dtype=np.int16) else: raise ValueError(\\"Currently only supports 16-bit PCM format.\\") max_amp = np.max(np.abs(audio_data)) normalized_audio_data = (audio_data / max_amp * (2**15 - 1)).astype(np.int16) if n_channels > 1: normalized_audio_data = normalized_audio_data.reshape(-1, n_channels) with wave.open(output_filepath, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(normalized_audio_data.tobytes()) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"PyTorch Coding Assessment Question # Objective: Implement a function using PyTorch\'s Core Aten IR to perform a specific tensor operation. # Problem Statement: You are required to implement a function using PyTorch\'s Core Aten IR that takes two tensors and performs element-wise addition followed by element-wise multiplication. This function should not use any inplace operations or out variants. The function should handle type promotion and broadcasting automatically. # Function Signature: ```python import torch def aten_add_mul(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Performs element-wise addition of tensor1 and tensor2, followed by element-wise multiplication with tensor3. Args: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - tensor3 (torch.Tensor): The third input tensor to multiply with the result of the addition. Returns: - torch.Tensor: The result of the element-wise addition of tensor1 and tensor2, followed by element-wise multiplication with tensor3. Constraints: - All input tensors must be broadcastable to each other. - No inplace operations allowed. - The function should leverage Core Aten IR internally. pass ``` # Input: - `tensor1`, `tensor2`, `tensor3`: Three input tensors that are broadcastable to each other. # Output: - A tensor that is the result of the element-wise addition of `tensor1` and `tensor2`, followed by an element-wise multiplication with `tensor3`. # Example: ```python import torch tensor1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) tensor2 = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) tensor3 = torch.tensor([[9, 10], [11, 12]], dtype=torch.float32) result = aten_add_mul(tensor1, tensor2, tensor3) print(result) # Expected output: # tensor([[ 54., 80.], # [110., 144.]]) ``` # Constraints: - You are required to use only Core Aten IR operations for this implementation. - You must not use any inplace operations (operations that modify the input tensors directly). - Ensure the function handles broadcasting and type promotion correctly. # Notes: - The function must correctly handle different tensor shapes that are broadcastable. - Input tensors can be of different types, and your solution should handle any necessary type promotion.","solution":"import torch def aten_add_mul(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Performs element-wise addition of tensor1 and tensor2, followed by element-wise multiplication with tensor3. Args: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - tensor3 (torch.Tensor): The third input tensor to multiply with the result of the addition. Returns: - torch.Tensor: The result of the element-wise addition of tensor1 and tensor2, followed by element-wise multiplication with tensor3. Constraints: - All input tensors must be broadcastable to each other. - No inplace operations allowed. - The function should leverage Core Aten IR internally. # Element-wise addition sum_tensors = torch.ops.aten.add(tensor1, tensor2) # Element-wise multiplication result = torch.ops.aten.mul(sum_tensors, tensor3) return result"},{"question":"Objective: Design a Python module that demonstrates proficiency in object serialization using the `pickle` module and database interactions using the `sqlite3` module. Problem Statement: Create a Python program to manage a small library system where users can perform the following actions: 1. **Add a Book**: Store book details such as title, author, publication year, and ISBN. 2. **Remove a Book**: Remove a book from the library using the ISBN. 3. **View All Books**: Retrieve and display all stored books. 4. **Save Library to File**: Serialize the current state of the library to a file using `pickle`. 5. **Load Library from File**: Deserialize the library state from a file using `pickle`. The library should be backed by an SQLite database to manage the book records persistently. Requirements: - **Class Definition**: - Define a `Book` class with attributes: `title`, `author`, `publication_year`, and `isbn`. - Define a `Library` class to manage the collection of books and database interactions. - **Library Class Methods**: - `add_book(book: Book)`: Adds a book to the library. - `remove_book(isbn: str)`: Removes a book from the library using its ISBN. - `view_all_books() -> List[Book]`: Returns a list of all books in the library. - `save_to_file(file_path: str)`: Serializes the library state to a specified file using `pickle`. - `load_from_file(file_path: str)`: Deserializes and loads the library state from a specified file using `pickle`. Input and Output Formats: 1. **Add a Book**: - **Input**: An instance of the `Book` class. - **Output**: None. 2. **Remove a Book**: - **Input**: ISBN as a string. - **Output**: None. 3. **View All Books**: - **Input**: None. - **Output**: A list of `Book` instances. 4. **Save Library to File**: - **Input**: File path as a string. - **Output**: None. 5. **Load Library from File**: - **Input**: File path as a string. - **Output**: None. Constraints: - Ensure that the ISBN of each book is unique. - Handle any potential database and file I/O errors gracefully. - All interactions with the SQLite database should be handled within the `Library` class. Example Usage: ```python library = Library() # Add books to the library book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", publication_year=1949, isbn=\\"1234567890\\") book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", publication_year=1960, isbn=\\"0987654321\\") library.add_book(book1) library.add_book(book2) # View all books print(library.view_all_books()) # Save library state to file library.save_to_file(\'library.pickle\') # Remove a book library.remove_book(isbn=\\"1234567890\\") # Load library state from file library.load_from_file(\'library.pickle\') # View all books after loading from file print(library.view_all_books()) ``` Performance Requirements: - Ensure that the `add_book` and `remove_book` operations are efficient, even when dealing with a large collection of books. - The `save_to_file` and `load_from_file` operations should handle large datasets without significant performance degradation.","solution":"import sqlite3 import pickle class Book: def __init__(self, title, author, publication_year, isbn): self.title = title self.author = author self.publication_year = publication_year self.isbn = isbn class Library: def __init__(self, db_path=\'library.db\'): self.conn = sqlite3.connect(db_path) self.create_table() def create_table(self): with self.conn: self.conn.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( title TEXT, author TEXT, publication_year INTEGER, isbn TEXT PRIMARY KEY ) \'\'\') def add_book(self, book: Book): with self.conn: self.conn.execute(\'\'\' INSERT INTO books (title, author, publication_year, isbn) VALUES (?, ?, ?, ?) \'\'\', (book.title, book.author, book.publication_year, book.isbn)) def remove_book(self, isbn: str): with self.conn: self.conn.execute(\'\'\' DELETE FROM books WHERE isbn = ? \'\'\', (isbn,)) def view_all_books(self): with self.conn: cursor = self.conn.execute(\'\'\' SELECT title, author, publication_year, isbn FROM books \'\'\') rows = cursor.fetchall() return [Book(title, author, publication_year, isbn) for title, author, publication_year, isbn in rows] def save_to_file(self, file_path: str): books = self.view_all_books() with open(file_path, \'wb\') as file: pickle.dump(books, file) def load_from_file(self, file_path: str): with open(file_path, \'rb\') as file: books = pickle.load(file) with self.conn: self.conn.execute(\'DELETE FROM books\') for book in books: self.add_book(book)"},{"question":"**Question: Seaborn Aesthetics Customization** You are provided with a dataset containing the results of a hypothetical study. Your task is to visualize this data using Seaborn, demonstrating your ability to control the aesthetics and the contexts of the plots. The dataset structure is as follows: ```python import numpy as np import pandas as pd # Generate a random dataset np.random.seed(42) data = pd.DataFrame({ \'Category A\': np.random.normal(loc=0, scale=1, size=100), \'Category B\': np.random.normal(loc=5, scale=2, size=100), \'Category C\': np.random.normal(loc=10, scale=3, size=100) }) ``` Implement a function `customize_and_plot(data: pd.DataFrame) -> None` that: 1. Creates a figure with 2x2 subplots using `matplotlib` and `seaborn`. 2. Uses different seaborn styles for each subplot. 3. Uses different seaborn contexts for each subplot. 4. Customizes at least one subplot by removing the spines, changing the axes\' face color, or adjusting other rc parameters. Each subplot should visualize the same data using seaborn\'s `boxplot` for a fair comparison. Ensure your plots have the `Title` and `Axis Labels` set appropriately to reflect the chosen theme and context. **Input:** - `data` (pd.DataFrame): A dataframe containing numerical data as shown above. **Output:** - The function displays the final 2x2 grid of customized box plots. **Function Signature:** ```python def customize_and_plot(data: pd.DataFrame) -> None: pass ``` **Constraints & Notes:** - You can use any of the predefined styles: `darkgrid`, `whitegrid`, `dark`, `white`, `ticks`. - You can use any of the contexts: `paper`, `notebook`, `talk`, `poster`. - Ensure the plots look different and clearly showcase the effect of each style and context. Example usage: ```python customize_and_plot(data) ``` This should display a 2x2 grid of box plots with different aesthetics and contexts. **Evaluation Criteria:** - Correct application of seaborn themes and contexts. - Proper use of seaborn and matplotlib functions. - Clarity and readability of the plots. - Adherence to the given instructions for customization.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def customize_and_plot(data: pd.DataFrame) -> None: # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # First subplot with \'darkgrid\' style and \'paper\' context sns.set(style=\'darkgrid\', context=\'paper\') sns.boxplot(data=data, ax=axes[0, 0]) axes[0, 0].set_title(\'Darkgrid style, Paper context\') axes[0, 0].set_xlabel(\'Categories\') axes[0, 0].set_ylabel(\'Values\') # Second subplot with \'whitegrid\' style and \'notebook\' context sns.set(style=\'whitegrid\', context=\'notebook\') sns.boxplot(data=data, ax=axes[0, 1]) axes[0, 1].set_title(\'Whitegrid style, Notebook context\') axes[0, 1].set_xlabel(\'Categories\') axes[0, 1].set_ylabel(\'Values\') # Third subplot with \'dark\' style and \'talk\' context sns.set(style=\'dark\', context=\'talk\') sns.boxplot(data=data, ax=axes[1, 0]) axes[1, 0].set_title(\'Dark style, Talk context\') axes[1, 0].set_xlabel(\'Categories\') axes[1, 0].set_ylabel(\'Values\') # Fourth subplot with \'white\' style and \'poster\' context, customized sns.set(style=\'white\', context=\'poster\') sns.boxplot(data=data, ax=axes[1, 1]) axes[1, 1].set_title(\'White style, Poster context\') axes[1, 1].set_xlabel(\'Categories\') axes[1, 1].set_ylabel(\'Values\') # Customize the fourth subplot by removing spines and changing axes\' face color sns.despine(ax=axes[1, 1]) axes[1, 1].set_facecolor(\'#f0f0f0\') # Adjusting layout to avoid overlap plt.tight_layout() # Display the plots plt.show()"},{"question":"Create a Python program that reads details of employees from a file, processes the data, and then writes the results to another file with a formatted output. Input The input will be a text file named `employees.txt` containing information about employees. Each line will have details of an employee in the following comma-separated format: ``` <employee_id>,<employee_name>,<department>,<salary> ``` For example: ``` 101,John Doe,Engineering,60000 102,Jane Smith,Marketing,55000 103,Jim Brown,Engineering,70000 ``` Requirements 1. Write a function `process_employee_data(input_file, output_file)` that: - Reads the employee data from the provided `input_file`. - Calculates the average salary for each department. - Writes the results to the `output_file` in a formatted and aligned text format. 2. The output format should be: - One line per department with the department name and the calculated average salary. - Align the columns nicely. Example output: ``` Department | Average Salary ------------------------------ Engineering | 65000.00 Marketing | 55000.00 ``` Constraints - Employee IDs are unique integers. - Employee names are strings containing only alphabetical characters and spaces. - Department names are strings containing only alphabetical characters. - Salaries are non-negative integers. Function Signature ```python def process_employee_data(input_file: str, output_file: str) -> None: pass ``` Performance - The function should handle files with up to 1000 employees efficiently. Use appropriate string formatting techniques (`str.format()` or f-strings) to generate the output. Evaluation Criteria - Correctness: The function should correctly read, process, and write the data. - Formatting: The output should be properly aligned and formatted as specified. - Efficiency: The function should handle the constraints within reasonable runtime and memory limits.","solution":"def process_employee_data(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as file: lines = file.readlines() department_salaries = {} department_counts = {} for line in lines: employee_id, employee_name, department, salary = line.strip().split(\',\') salary = int(salary) if department not in department_salaries: department_salaries[department] = 0 department_counts[department] = 0 department_salaries[department] += salary department_counts[department] += 1 with open(output_file, \'w\') as file: file.write(f\\"{\'Department\':<12} | {\'Average Salary\':>14}n\\") file.write(\'-\' * 30 + \'n\') for department in sorted(department_salaries): avg_salary = department_salaries[department] / department_counts[department] file.write(f\\"{department:<12} | {avg_salary:>14.2f}n\\")"},{"question":"**Title: Advanced Categorical Data Visualization using Seaborn** **Problem Statement:** You are given a dataset containing information about passengers on the Titanic. The dataset consists of various features, including age, class, sex, and survival status. Your task is to write a function `visualize_titanic_data` that generates a comprehensive set of visualizations to analyze the distribution of passenger ages across different classes and survival statuses, segregated by gender. **Function Signature:** ```python def visualize_titanic_data(): pass ``` **Requirements:** 1. Load the \\"titanic\\" dataset using seaborn. 2. Create a `catplot` to visualize the distribution of passenger ages across different classes. - Use a boxplot as the primary kind of plot. 3. Add another layer on this plot to visualize data points using a strip plot. 4. Use different colors for different genders. 5. Create subplots that separate the visualizations by survival status. 6. Adjust the size of the overall figure to be 10 inches by 6 inches (use `height` and `aspect`). 7. Customize the plot: - Set axis labels to make them more descriptive. - Set custom tick labels for the x-axis, showing \\"First Class\\", \\"Second Class\\", and \\"Third Class\\". - Add titles to each subplot to indicate the survival status. - Adjust y-axis limits to range from 0 to 80. - Remove the left spine of the plot. **Expected Output:** A `FacetGrid` plot with the specified customizations. **Input:** None. The function `visualize_titanic_data` does not take any parameters. **Output:** A seaborn `FacetGrid` plot visualizing the required information. **Example:** ```python visualize_titanic_data() ``` The code should generate and display a visualization similar to the following: - Multiple subplots (one for each survival status). - Boxplots showing the distribution of ages across different classes. - Strip plot layers showing individual data points. - Different hues for genders. - Customized axis labels, tick labels, titles, y-axis limits, and spines. Please ensure that your code runs without errors and produces the desired output as a single visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load the dataset titanic = sns.load_dataset(\'titanic\') # Create the catplot with the specified parameters g = sns.catplot( data=titanic, x=\'class\', y=\'age\', hue=\'sex\', col=\'survived\', kind=\'box\', height=6, aspect=1.5 ) # Add a strip plot to the existing catplot sns.stripplot( data=titanic, x=\'class\', y=\'age\', hue=\'sex\', dodge=True, jitter=True, palette=\'bright\', alpha=0.6, ax=g.axes[0,0] ) sns.stripplot( data=titanic, x=\'class\', y=\'age\', hue=\'sex\', dodge=True, jitter=True, palette=\'bright\', alpha=0.6, ax=g.axes[0,1] ) # Customize the plot g.set_axis_labels(\\"Passenger Class\\", \\"Age\\") g.set_titles(\\"Survival Status: {col_name}\\") g.set_xticklabels([\\"First Class\\", \\"Second Class\\", \\"Third Class\\"]) g.set(ylim=(0, 80)) # Remove the left spine for ax in g.axes.flat: ax.spines[\'left\'].set_visible(False) # Show the plot plt.show()"},{"question":"# Advanced Python Types and Dynamic Class Creation Objective: You are provided with a set of utility functions from the `types` module to dynamically create and manipulate classes in Python. In this task, you will use these utilities to create a dynamic class, instantiate it, and use some specific standard types. Problem Statement: Using the `types` module, you need to implement the following function: ```python from types import new_class, prepare_class, FunctionType def dynamic_type_operations(): # Step 1: Dynamically create a class named `DynamicPerson` with one base class `object`. # Step 2: Add the following attributes and methods to the class inside the exec_body: # a. An instance attribute `name` initialized to None. # b. An instance method `greet` that takes no parameters other than `self` # and returns a greeting string based on the `name` attribute. # Step 3: Instantiate the `DynamicPerson` class and set the `name` attribute to \\"Alice\\". # Step 4: Invoke the `greet` method and return its result. # Further Guidance: # - Use the `new_class` function for dynamic class creation. # - Provide an `exec_body` function that sets up attributes and methods. # - Make sure the `greet` method is of type `types.FunctionType`. # - Return the result of calling the `greet` method on the instantiated object. # Implement your code here return result ``` Expected Output: The function should return the following string: ``` \\"Hello, my name is Alice.\\" ``` Constraints: - You must use the `types.new_class()` function for creating the class. - The `greet` method\'s type should be validated to be of `FunctionType`. Hints: - The `exec_body` function used with `new_class` should accept a namespace and can add attributes and methods to it. - You can use `FunctionType` to ensure the method type is correct. Example: Here\'s an example of defining an `exec_body`: ```python def exec_body(ns): ns[\'name\'] = None def greet(self): return f\\"Hello, my name is {self.name}.\\" ns[\'greet\'] = FunctionType(greet.__code__, {}) ``` The primary goal is to demonstrate understanding of dynamic type creation, handling of the namespace, and integration of standard types in Python.","solution":"from types import new_class, FunctionType def dynamic_type_operations(): def exec_body(ns): ns[\'name\'] = None def greet(self): return f\\"Hello, my name is {self.name}.\\" ns[\'greet\'] = FunctionType(greet.__code__, {}, name=\'greet\') DynamicPerson = new_class(\'DynamicPerson\', (), {}, exec_body) instance = DynamicPerson() instance.name = \\"Alice\\" result = instance.greet() return result"},{"question":"# Asynchronous Data Processing System You are tasked with designing an asynchronous data processing system using Python\'s `asyncio` library. The system should read data from a source, process it, and save the results, all asynchronously. The data processing should be performed concurrently by multiple workers. Requirements: 1. **Reading Data**: - Simulate asynchronous data reading using `asyncio.sleep`. - Implement an `asyncio.Queue` to store the data to be processed. 2. **Processing Data**: - Create multiple worker tasks that process the data from the queue. - Each worker should have a simulated processing time using `asyncio.sleep`. 3. **Saving Results**: - After processing, the results should be saved. - Implement a method to simulate saving results which includes a delay using `asyncio.sleep`. 4. **Timeout Handling**: - Implement a timeout for the entire processing task. If the processing takes too long, it should be cancelled. Function Signature: ```python import asyncio async def read_data(queue: asyncio.Queue, num_items: int) -> None: Read data and put it into the queue. pass async def process_data(queue: asyncio.Queue, results: asyncio.Queue) -> None: Process data from the queue and put the result into the results queue. pass async def save_results(results: asyncio.Queue) -> None: Save results from the results queue. pass async def main(num_items: int, num_workers: int, timeout: float) -> None: Main function to orchestrate the reading, processing, and saving of data. pass # Example usage asyncio.run(main(num_items=10, num_workers=3, timeout=20.0)) ``` Input: - `num_items` (int): Number of data items to read and process. - `num_workers` (int): Number of worker tasks to process the data. - `timeout` (float): Total timeout for the entire processing task in seconds. Output: - None (but ensure the function completes within the specified timeout, or raises a timeout error). Constraints: - Each read data operation should have a simulated delay of 0.5 seconds. - Each process data operation should have a simulated delay of 1 second. - Saving results should have a simulated delay of 0.2 seconds. - If the processing takes longer than the specified timeout, raise an `asyncio.TimeoutError`. Example Scenario: 1. Initiate reading of 10 data items, with each read taking 0.5 seconds. 2. Use 3 worker tasks to process the data, with each processing taking 1 second. 3. Save the results, with each save operation taking 0.2 seconds. 4. Ensure the entire operation completes within the specified timeout; otherwise, raise an exception. Implement the functions `read_data`, `process_data`, `save_results`, and `main` to fulfill the requirements, using appropriate asyncio constructs such as `create_task`, `gather`, `wait_for`, and `Queue`.","solution":"import asyncio async def read_data(queue: asyncio.Queue, num_items: int) -> None: Read data and put it into the queue. for i in range(num_items): await asyncio.sleep(0.5) # Simulated delay await queue.put(f\\"data{i}\\") await queue.put(None) # Signal that reading is done async def process_data(queue: asyncio.Queue, results: asyncio.Queue) -> None: Process data from the queue and put the result into the results queue. while True: data = await queue.get() if data is None: await queue.put(None) # Signal to other workers that processing is done break await asyncio.sleep(1) # Simulated delay processed_data = f\\"processed_{data}\\" await results.put(processed_data) async def save_results(results: asyncio.Queue) -> None: Save results from the results queue. while True: result = await results.get() if result is None: break await asyncio.sleep(0.2) # Simulated delay print(f\\"Saved: {result}\\") async def main(num_items: int, num_workers: int, timeout: float) -> None: Main function to orchestrate the reading, processing, and saving of data. queue = asyncio.Queue() results = asyncio.Queue() data_reader = read_data(queue, num_items) workers = [process_data(queue, results) for _ in range(num_workers)] results_saver = save_results(results) try: await asyncio.wait_for( asyncio.gather( data_reader, *workers, results_saver, return_exceptions=True ), timeout ) except asyncio.TimeoutError: print(\\"Processing timed out\\")"},{"question":"**Objective:** Demonstrate your understanding of the `xmlrpc.server` module by implementing a simple XML-RPC server that provides calculator functionalities. **Task:** 1. **Create a SimpleXMLRPCServer** running on `localhost` and port `9000`. 2. **Register the following functions**: - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the result of `x` minus `y`. - `multiply(x, y)`: Returns the product of `x` and `y`. - `divide(x, y)`: Returns the result of `x` divided by `y`. Ensure to handle division by zero by returning `\\"Error: Division by zero\\"`. 3. **Register an instance** of a class `AdvancedCalculator` with the following methods: - `power(base, exponent)`: Returns the result of `base` raised to the power of `exponent`. - `sqrt(value)`: Returns the square root of `value`. If `value` is negative, return `\\"Error: Negative value\\"`. 4. **Enable introspection methods** on the server to provide method details. 5. **Run the server** and handle XML-RPC requests until interrupted. **Constraints:** - Use the `SimpleXMLRPCServer` class from the `xmlrpc.server` module. - Implement proper error handling for division by zero and square root of negative numbers. - Ensure the server runs continuously and handles incoming requests until interrupted. **Input and Output Formats:** - **Input:** The server will handle XML-RPC requests based on the registered functions and instance methods. - **Output:** The output should be the result of the computation for each request or an error message in case of invalid operations. **Example:** - An XML-RPC client makes a request to `add(5, 3)` -> The server should respond with `8`. - A request to `divide(10, 0)` -> The server should respond with `\\"Error: Division by zero\\"`. **Implementation Notes:** - The server should be created and run within a `with` statement to ensure proper resource handling. - Use decorators or explicit function registration as preferred. - Follow the example syntax provided in the documentation for binding functions and instances. Good luck!","solution":"from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler import math class RequestHandler(SimpleXMLRPCRequestHandler): rpc_paths = (\'/RPC2\',) def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): if y == 0: return \\"Error: Division by zero\\" return x / y class AdvancedCalculator: def power(self, base, exponent): return base ** exponent def sqrt(self, value): if value < 0: return \\"Error: Negative value\\" return math.sqrt(value) def run_server(): with SimpleXMLRPCServer((\'localhost\', 9000), requestHandler=RequestHandler, allow_none=True) as server: server.register_introspection_functions() server.register_function(add, \'add\') server.register_function(subtract, \'subtract\') server.register_function(multiply, \'multiply\') server.register_function(divide, \'divide\') server.register_instance(AdvancedCalculator()) print(\\"Server is running on localhost port 9000...\\") server.serve_forever() if __name__ == \\"__main__\\": run_server()"},{"question":"**Objective:** The following assessment tests your ability to interact with Python package metadata using the `distutils` library. **Problem Statement:** You are required to write a Python function that reads the metadata from a `PKG-INFO` file of a given Python package and returns a dictionary containing specific fields of interest. **Function Signature:** ```python def read_package_metadata(file_path: str) -> dict: Reads the metadata from a given PKG-INFO file and returns a dictionary containing the package\'s name, version, author, and description. Parameters: file_path (str): The path to the PKG-INFO file. Returns: dict: A dictionary with keys \'name\', \'version\', \'author\', \'description\' with respective values from the PKG-INFO file. pass ``` **Requirements:** 1. Use the `distutils.dist.DistributionMetadata` class to handle metadata reading. 2. The function should: - Read the file located at `file_path`. - Extract the \'name\', \'version\', \'author\', and \'description\' fields. - Return these values in a dictionary. **Constraints:** - Assume that the given file path always points to a valid `PKG-INFO` file. - Ensure the returned dictionary has the exact keys: `\'name\'`, `\'version\'`, `\'author\'`, and `\'description\'`, even if some fields in the file are empty (use `None` if a field is missing). **Example:** ```python # Example PKG-INFO content: # Name: example_package # Version: 1.0.0 # Author: John Doe # Summary: This is an example package # Expected Output: result = read_package_metadata(\\"/path/to/PKG-INFO\\") print(result) # Output: {\'name\': \'example_package\', \'version\': \'1.0.0\', \'author\': \'John Doe\', \'description\': \'This is an example package\'} ``` **Note:** Actual implementation should correctly handle file reading, metadata extraction, and potential issues with missing fields.","solution":"from distutils.dist import DistributionMetadata def read_package_metadata(file_path: str) -> dict: Reads the metadata from a given PKG-INFO file and returns a dictionary containing the package\'s name, version, author, and description. Parameters: file_path (str): The path to the PKG-INFO file. Returns: dict: A dictionary with keys \'name\', \'version\', \'author\', \'description\' with respective values from the PKG-INFO file. metadata = DistributionMetadata() with open(file_path, \'r\') as f: metadata.read_pkg_file(f) return { \'name\': metadata.name if metadata.name else None, \'version\': metadata.version if metadata.version else None, \'author\': metadata.author if metadata.author else None, \'description\': metadata.description if metadata.description else None, }"},{"question":"In this exercise, you will write a function that makes use of the PyTorch configuration information. Specifically, you are going to implement a function that utilizes the `torch.__config__.show` and `torch.__config__.parallel_info` functions to determine if your PyTorch environment is set up to use multiple threads and the OpenMP backend for parallelism. # Function Signature ```python import torch def check_parallelism() -> str: This function should display the current configuration settings using `torch.__config__.show` then analyze the output of `torch.__config__.parallel_info` to determine if the OpenMP backend is enabled and using more than one thread for parallel operations. Returns: str: A string indicating whether OpenMP is enabled and the number of threads being used. # Your implementation here ``` # Requirements 1. **Display Configuration**: - Ensure that the function makes a call to `torch.__config__.show` to display the current configuration. 2. **Check Parallel Backend**: - Obtain information from `torch.__config__.parallel_info` and check if OpenMP is enabled. 3. **Thread Count**: - Determine the number of threads used for parallel operations if OpenMP is enabled. 4. **Output**: - Return a string in the format: - \\"OpenMP enabled with N threads\\" where N is the number of threads if OpenMP is available. - \\"OpenMP not enabled\\" if OpenMP is not available. # Example Usage ```python print(check_parallelism()) ``` # Constraints: - Ensure that the function handles any exceptions or errors gracefully, particularly dealing with information that might be unavailable or returned in unexpected formats. # Notes - This task requires understanding of how to extract and interpret configuration information from PyTorch\'s internal settings. - It\'s important to familiarize yourself with the `torch.__config__` module for implementing this function correctly.","solution":"import torch def check_parallelism() -> str: Display the current PyTorch configuration settings and analyze the parallel backend information to determine if OpenMP is enabled and the number of threads being used. Returns: str: A string indicating whether OpenMP is enabled and the number of threads being used. # Display the current PyTorch configuration settings torch.__config__.show() # Obtain parallel information parallel_info = torch.__config__.parallel_info() # Check if OpenMP is enabled and the number of threads being used if \\"OpenMP\\" in parallel_info: # Find \\"OpenMP\\" section and look for number of threads lines = parallel_info.split(\'n\') for line in lines: if \\"OpenMP\\" in line and \\"threads\\" in line: parts = line.split() thread_count = int(parts[-1]) # Assuming the number of threads is the last word in the line return f\\"OpenMP enabled with {thread_count} threads\\" return \\"OpenMP enabled but thread count not found\\" return \\"OpenMP not enabled\\""},{"question":"# Distributed RPC Framework Coding Assessment **Objective:** You are tasked with designing a simple distributed system using PyTorch\'s Distributed RPC Framework. Your goal is to set up an environment with three workers where one worker acts as a master and the other two workers perform computations on behalf of the master. You will use RPC to send tasks to the workers, manage remote references, and collect results. **Requirements:** 1. **Initialization:** - Initialize the RPC framework with three workers. The master worker should have `rank 0`, and the other two workers should have `rank 1` and `rank 2`. 2. **Remote Function Execution:** - Implement a remote function `remote_square` that takes a number as input and returns its square. The function should be executable on both workers (`rank 1` and `rank 2`). 3. **Remote References (RRefs):** - Create remote references to the results of the `remote_square` function for numbers 2 and 3 on workers `rank 1` and `rank 2`, respectively. - Ensure that the results are referenced remotely and retrieved by the master worker. 4. **Result Retrieval:** - Use RPC to collect the squared results from the workers on the master worker and print the results. 5. **Shutdown:** - Properly shut down the RPC framework after collecting the results. **Function Implementations:** - `def init_rpc_framework()`: Initializes the RPC framework. - `def remote_square(x)`: Defines the remote function to square a number. - `def distribute_tasks_and_collect_results()`: Distributes the task of squaring numbers to the workers using RPC, collects the results, and prints them. - `def shutdown_rpc_framework()`: Shuts down the RPC framework. **Constraints:** - Ensure that the function `remote_square` is executed remotely on the worker nodes. - The master node should handle the distribution of tasks and collection of results. - Handle any necessary synchronization and waiting for futures. **Sample Output:** ``` Result from worker 1: 4 Result from worker 2: 9 ``` **Hints:** - Use `rpc_async` to execute the `remote_square` function on remote workers. - Use `rpc_sync` to retrieve the results from the remote references. - Use the `RRef` class to manage remote references. **Example Code Snippet:** ```python import os import torch.distributed.rpc as rpc def init_rpc_framework(master_addr=\'localhost\', master_port=\'29500\', rank=0, world_size=3): os.environ[\'MASTER_ADDR\'] = master_addr os.environ[\'MASTER_PORT\'] = master_port rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) def remote_square(x): return x * x def distribute_tasks_and_collect_results(): if rpc.get_worker_info().name == \\"worker0\\": # Master worker worker1 = rpc.get_worker_info(\\"worker1\\") worker2 = rpc.get_worker_info(\\"worker2\\") # Create remote references to the results rref1 = rpc.remote(worker1, remote_square, args=(2,)) rref2 = rpc.remote(worker2, remote_square, args=(3,)) # Retrieve results result1 = rref1.to_here() result2 = rref2.to_here() print(f\\"Result from worker 1: {result1}\\") print(f\\"Result from worker 2: {result2}\\") def shutdown_rpc_framework(): rpc.shutdown() # Run the distributed system if __name__ == \\"__main__\\": rank = int(os.getenv(\'RANK\')) init_rpc_framework(rank=rank) if rank == 0: distribute_tasks_and_collect_results() shutdown_rpc_framework() ``` Ensure you run each worker in a separate process with the environment variable `RANK` set to the appropriate rank (0 for master, 1 for worker 1, and 2 for worker 2).","solution":"import os import torch.distributed.rpc as rpc def init_rpc_framework(master_addr=\'localhost\', master_port=\'29500\', rank=0, world_size=3): os.environ[\'MASTER_ADDR\'] = master_addr os.environ[\'MASTER_PORT\'] = master_port rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) def remote_square(x): return x * x def distribute_tasks_and_collect_results(): if rpc.get_worker_info().name == \\"worker0\\": # Master worker worker1 = rpc.get_worker_info(\\"worker1\\") worker2 = rpc.get_worker_info(\\"worker2\\") # Create remote references to the results rref1 = rpc.remote(worker1, remote_square, args=(2,)) rref2 = rpc.remote(worker2, remote_square, args=(3,)) # Retrieve results result1 = rref1.to_here() result2 = rref2.to_here() print(f\\"Result from worker 1: {result1}\\") print(f\\"Result from worker 2: {result2}\\") def shutdown_rpc_framework(): rpc.shutdown() # Run the distributed system if __name__ == \\"__main__\\": rank = int(os.getenv(\'RANK\')) init_rpc_framework(rank=rank) if rank == 0: distribute_tasks_and_collect_results() shutdown_rpc_framework()"},{"question":"# Advanced ZIP File Manipulation with Python\'s zipfile Module **Objective**: Implement a function to create a ZIP archive from a list of files, with specific requirements for compression and password protection. # Function Signature ```python def create_secure_zip(zip_filename: str, files: list, password: str = None, compression_method: int = zipfile.ZIP_DEFLATED) -> None: pass ``` # Description Implement the function `create_secure_zip` with the following characteristics: 1. **Parameters**: - `zip_filename` (str): The name of the output ZIP file. - `files` (list): A list of file paths (as strings) to be added to the ZIP archive. - `password` (str, optional): A password to be used for encrypting the files within the ZIP. If no password is provided, files should be added without encryption. - `compression_method` (int, optional): The compression method to use. It can be one of the following: - `zipfile.ZIP_STORED` for no compression - `zipfile.ZIP_DEFLATED` (default) for usual compression - `zipfile.ZIP_BZIP2` for BZIP2 compression - `zipfile.ZIP_LZMA` for LZMA compression 2. **Functionality**: - Open or create the ZIP file specified by `zip_filename`. - Add each file from the `files` list to the ZIP archive, using the specified `compression_method`. - If `password` is provided, encrypt each file with the given password. - Utilize context management (`with` statement) to ensure ZIP file is properly handled (opened and closed). - Implement error handling to manage any exceptions that may arise during the process, such as invalid file paths, unsupported compression methods, or issues with writing to the ZIP file. 3. **Example Usage**: ```python files_to_zip = [\'document.txt\', \'image.png\', \'script.py\'] create_secure_zip(\'archive.zip\', files_to_zip, password=\'secure123\', compression_method=zipfile.ZIP_LZMA) ``` # Constraints: - Ensure compatibility with Python 3.8 and above. - The function should handle missing files gracefully, by skipping them and logging a warning message. - Performance is not critical, but efficiency in compression and handling files is expected. - The ZIP file name and file paths provided in `files` list should not contain any null bytes. # Evaluation: - Correctness: Does the function create a ZIP archive as specified, with the appropriate compression and optional encryption? - Robustness: Does the function handle errors and edge cases properly? - Code Quality: Is the code clean, well-documented, and follows Pythonic principles? **Hint**: Refer to the `ZipFile` class and its methods (`write`, `setpassword`, etc.) in the `zipfile` module documentation to implement the required functionality.","solution":"import zipfile import os def create_secure_zip(zip_filename: str, files: list, password: str = None, compression_method: int = zipfile.ZIP_DEFLATED) -> None: Creates a ZIP archive from a list of files with optional compression and password protection. :param zip_filename: The name of the output ZIP file. :param files: A list of file paths to be added to the ZIP archive. :param password: A password to be used for encrypting the files within the ZIP (optional). :param compression_method: The compression method to use (default is zipfile.ZIP_DEFLATED). # Validate the compression method if compression_method not in [zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA]: raise ValueError(\\"Invalid compression method\\") # Open or create the ZIP file with zipfile.ZipFile(zip_filename, \'w\', compression=compression_method) as zipf: for file in files: if os.path.exists(file): zipf.write(file, os.path.basename(file)) else: # Log a warning if the file does not exist (can also use logging.warning) print(f\\"Warning: {file} does not exist and will be skipped\\") # Set the password if provided if password: # Note: This method sets password for all future operations on the ZIP file zipf.setpassword(password.encode(\'utf-8\'))"},{"question":"**Question: Iris Dataset Analysis and Classification** Objective: You are required to demonstrate your understanding of scikit-learn by performing data loading, preprocessing, and implementing a classification model on the Iris dataset. # Part 1: Load the Iris Dataset 1. Write a function `load_iris_dataset()` that uses the `load_iris()` function from `sklearn.datasets` to load the Iris dataset. 2. The function should return the features (X) and target labels (y) as separate NumPy arrays. **Function Signature:** ```python def load_iris_dataset() -> Tuple[np.ndarray, np.ndarray]: pass ``` # Part 2: Preprocess the Data 1. Write a function `preprocess_data(X: np.ndarray)` that standardizes the features using `StandardScaler` from `sklearn.preprocessing`. The function should return the standardized features as a NumPy array. **Function Signature:** ```python def preprocess_data(X: np.ndarray) -> np.ndarray: pass ``` # Part 3: Train and Evaluate a Model 1. Write a function `train_and_evaluate(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42)` that: - Splits the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. - Trains a `KNeighborsClassifier` model on the training data. - Evaluates the model on the testing data and returns the accuracy score. **Function Signature:** ```python def train_and_evaluate(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42) -> float: pass ``` # Constraints: - You must use scikit-learn for loading the dataset, preprocessing, model training, and evaluation. - You should ensure reproducibility by setting random states where necessary. # Example: ```python # Example usage X, y = load_iris_dataset() X_standardized = preprocess_data(X) accuracy = train_and_evaluate(X_standardized, y) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` # Expected Output: The final output should print the accuracy of the KNeighborsClassifier on the Iris dataset. # Performance Requirement: The solution should efficiently handle the Iris dataset within reasonable time limits.","solution":"from typing import Tuple import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier def load_iris_dataset() -> Tuple[np.ndarray, np.ndarray]: iris = load_iris() return iris.data, iris.target def preprocess_data(X: np.ndarray) -> np.ndarray: scaler = StandardScaler() X_standardized = scaler.fit_transform(X) return X_standardized def train_and_evaluate(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42) -> float: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) model = KNeighborsClassifier() model.fit(X_train, y_train) accuracy = model.score(X_test, y_test) return accuracy"},{"question":"# Question: Exploring System Configuration with `sysconfig` **Objective:** You are tasked with writing a Python function that utilizes the `sysconfig` module to retrieve and format various configuration details about the current Python environment. **Task:** 1. **Function Name**: `get_python_env_info` 2. **Input**: None 3. **Output**: A dictionary with the following keys and their corresponding values: - **`\\"platform\\"`**: The string identifying the current platform (use `sysconfig.get_platform()`). - **`\\"python_version\\"`**: The \\"MAJOR.MINOR\\" Python version (use `sysconfig.get_python_version()`). - **`\\"default_scheme\\"`**: The default scheme for the current platform (use `sysconfig.get_default_scheme()`). - **`\\"installed_paths\\"`**: A dictionary of all paths associated with the default scheme for the current platform (use `sysconfig.get_paths()`). **Constraints**: - Handle any potential errors/exceptions gracefully and provide meaningful error messages. - Ensure that the dictionary keys follow the exact naming convention given above. ```python import sysconfig def get_python_env_info(): Retrieves and formats various configuration details about the current Python environment. Returns: dict: A dictionary with platform, python_version, default_scheme, and installed_paths information. try: env_info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"installed_paths\\": sysconfig.get_paths() } return env_info except Exception as e: return {\\"error\\": str(e)} # Example usage if __name__ == \\"__main__\\": info = get_python_env_info() print(info) ``` **Example Output**: When you run the function, it may produce an output similar to this: ```python { \\"platform\\": \\"win-amd64\\", \\"python_version\\": \\"3.10\\", \\"default_scheme\\": \\"nt\\", \\"installed_paths\\": { \\"stdlib\\": \\"C:/Python310/Lib\\", \\"platstdlib\\": \\"C:/Python310/Lib\\", \\"purelib\\": \\"C:/Python310/Lib/site-packages\\", \\"platlib\\": \\"C:/Python310/Lib/site-packages\\", \\"include\\": \\"C:/Python310/Include\\", \\"scripts\\": \\"C:/Python310/Scripts\\", \\"data\\": \\"C:/Python310\\" } } ``` **Notes**: - Document your code well and ensure readability. - Confirm that each key and value in the dictionary is accurate and properly formatted as described.","solution":"import sysconfig def get_python_env_info(): Retrieves and formats various configuration details about the current Python environment. Returns: dict: A dictionary with platform, python_version, default_scheme, and installed_paths information. try: env_info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"installed_paths\\": sysconfig.get_paths() } return env_info except Exception as e: return {\\"error\\": str(e)}"},{"question":"Question: You are given a sequence and a set of start, stop, and step indices. Your task is to create and manipulate a slice object using the provided indices, handle any out-of-bounds indices effectively, and return the subsequence indicated by the slice. **Input:** - A sequence (list of integers) - Start index `start` (integer or `None`) - Stop index `stop` (integer or `None`) - Step index `step` (integer or `None`) **Output:** - A list that represents the subsequence obtained by applying the slice to the input sequence. **Requirements:** 1. Write a function `get_subsequence(sequence, start, stop, step)`, which: - Creates a new slice object using `PySlice_New`. - Validates the slice using `PySlice_GetIndices` or `PySlice_GetIndicesEx`. - Adjusts indices if necessary using `PySlice_AdjustIndices`. - Applies the validated and adjusted slice to the input sequence. - Returns the resulting subsequence. 2. Ensure that out-of-bounds indices are handled gracefully by clipping them. 3. If any integer indices are `None`, treat them appropriately as in normal Python slicing. **Constraints:** - The input sequence length will not exceed 1000 elements. - Indices if provided will be within the range `[-2000, 2000]`, but your function should handle out-of-bounds gracefully. - The `step` value, if provided, will be a non-zero integer. **Example:** ```python def get_subsequence(sequence, start, stop, step): # Your implementation here # Test cases sequence = [i for i in range(10)] print(get_subsequence(sequence, 1, 7, 2)) # Output: [1, 3, 5] print(get_subsequence(sequence, 5, None, -1)) # Output: [5, 4, 3, 2, 1, 0] print(get_subsequence(sequence, -100, 100, 1)) # Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] print(get_subsequence(sequence, None, None, None)) # Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ``` **Note:** You should assume Python/C API compatibility and usage within Python extension modules or by utilizing relevant Python built-ins with similar logic for demonstration purposes.","solution":"def get_subsequence(sequence, start, stop, step): Returns a subsequence of the input sequence given the start, stop, and step indices. Handles out-of-bounds and None values as in normal Python slicing. # Handle None values start = None if start is None else max(min(start, len(sequence)), -len(sequence)) stop = None if stop is None else max(min(stop, len(sequence)), -len(sequence)) # Create the slice object slice_obj = slice(start, stop, step) # Apply the slice to the input sequence return sequence[slice_obj]"},{"question":"**Question: Customizing and Visualizing Data Using Seaborn** Given a dataset of your choice that contains at least two numeric columns and one categorical column, write a Python function using the seaborn package to create a grid of visualizations. The function should generate the following plots, all styled with a customized seaborn style: 1. A scatter plot of two numeric columns, with points colored by a categorical column. 2. A box plot of one numeric column grouped by a categorical column. 3. A histogram of one numeric column with a kernel density estimate (KDE) overlay. **Function Signature:** ```python def create_visualizations(data: pd.DataFrame, num_col1: str, num_col2: str, cat_col: str, style: str) -> None: pass ``` **Input:** - `data`: A pandas DataFrame containing the dataset. - `num_col1`: A string representing the first numeric column name. - `num_col2`: A string representing the second numeric column name. - `cat_col`: A string representing the categorical column name. - `style`: A string representing the seaborn style to set for the plots. Possible values are `\\"whitegrid\\"`, `\\"darkgrid\\"`, `\\"white\\"`, `\\"dark\\"`, `\\"ticks\\"`. **Output:** - The function will create and display the three specified plots in a single figure (using sns.FacetGrid or matplotlib subplots). **Constraints:** - Make sure to apply the given seaborn style before generating the plots. - Ensure each plot is clear and labeled appropriately. - Handle any missing values by dropping rows with missing values in the specified columns. **Performance Requirements:** - The function should be efficient with the assumption that the dataset size is manageable in a typical data analysis scenario (e.g., n = 1000 rows). Example: ```python import pandas as pd import seaborn as sns # Sample dataset data = pd.DataFrame({ \'num_col1\': [1, 2, 3, 4, 5], \'num_col2\': [2, 3, 4, 5, 6], \'cat_col\': [\'A\', \'B\', \'A\', \'B\', \'A\'] }) # Call the function create_visualizations(data, \'num_col1\', \'num_col2\', \'cat_col\', \'darkgrid\') ``` This example should produce a grid of three plots (a scatter plot, box plot, and histogram with KDE) styled with the `\\"darkgrid\\"` seaborn style.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(data: pd.DataFrame, num_col1: str, num_col2: str, cat_col: str, style: str) -> None: Creates a grid of visualizations from the dataset. # Set the seaborn style sns.set_style(style) # Drop rows with missing values in specified columns data = data.dropna(subset=[num_col1, num_col2, cat_col]) fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # Scatter plot sns.scatterplot(data=data, x=num_col1, y=num_col2, hue=cat_col, ax=axes[0]) axes[0].set_title(f\'Scatter plot of {num_col1} vs {num_col2}\') # Box plot sns.boxplot(data=data, x=cat_col, y=num_col1, ax=axes[1]) axes[1].set_title(f\'Box plot of {num_col1} by {cat_col}\') # Histogram with KDE sns.histplot(data=data, x=num_col1, kde=True, ax=axes[2]) axes[2].set_title(f\'Histogram of {num_col1} with KDE\') plt.tight_layout() plt.show()"},{"question":"# Problem: Implement Kernel Density Estimation for Data Analysis Given a 2D dataset with a mix of bimodal and outlier observations, you are required to implement a function that performs Kernel Density Estimation (KDE) on the data using scikit-learn\'s `KernelDensity` class. Your function should analyze the data distribution and return the density scores for the given data points. # Input - `X_train`: A matrix (2-dimensional NumPy array), representing the training data points on which the KDE model will be trained. - `X_test`: A matrix (2-dimensional NumPy array), representing the test data points for which the density scores will be calculated. - `kernel`: A string representing the kernel type to use for KDE (default is `\\"gaussian\\"`). It can be one of [`\'gaussian\'`, `\'tophat\'`, `\'epanechnikov\'`, `\'exponential\'`, `\'linear\'`, `\'cosine\'`]. - `bandwidth`: A float representing the bandwidth parameter for the KDE model (default is `0.5`). # Output - Return a 1-dimensional NumPy array of density scores for the `X_test` data points. # Constraints - The dimension of the `X_train` and `X_test` arrays can be up to 2D. - The number of points in `X_train` can be ≤ 1000. - The number of points in `X_test` can be ≤ 500. # Example ```python import numpy as np from sklearn.neighbors import KernelDensity def kde_density_estimation(X_train, X_test, kernel=\'gaussian\', bandwidth=0.5): # Initialize the KernelDensity model with the specified kernel and bandwidth kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) # Fit the model on the training data kde.fit(X_train) # Compute the density scores for the test data density_scores = kde.score_samples(X_test) return density_scores # Example usage X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) X_test = np.array([[0, 0], [1, 1]]) print(kde_density_estimation(X_train, X_test, kernel=\'gaussian\', bandwidth=0.2)) ``` Your task is to complete the `kde_density_estimation` function to produce the appropriate density scores based on the given inputs. Ensure to handle the inputs and outputs as specified. **Note**: You should use scikit-learn\'s `KernelDensity` class for this implementation.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def kde_density_estimation(X_train, X_test, kernel=\'gaussian\', bandwidth=0.5): Perform Kernel Density Estimation on the given data. Parameters: - X_train: 2D NumPy array representing the training data. - X_test: 2D NumPy array representing the test data. - kernel: Kernel type to use for KDE (default is \'gaussian\'). - bandwidth: Bandwidth parameter for the KDE model (default is 0.5). Returns: - A 1D NumPy array of density scores for the X_test data points. # Initialize the KernelDensity model with the specified kernel and bandwidth kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) # Fit the model on the training data kde.fit(X_train) # Compute the density scores for the test data density_scores = kde.score_samples(X_test) return density_scores"},{"question":"Objective: Demonstrate your understanding of creating and utilizing color palettes with Seaborn\'s `blend_palette` function. Problem Statement: You are tasked with writing a function to create and visualize both discrete palettes and continuous colormaps using Seaborn\'s `blend_palette` function. Your function should also include optional functionality for displaying the palettes or colormaps. Requirements: 1. **Function Name**: `create_and_visualize_palette` 2. **Input**: - `color_list`: A list of color strings (could be color names, hex codes, etc.). - `as_cmap`: A boolean flag to determine whether to return a continuous colormap (default: False). - `show_palette`: A boolean flag to determine whether to display the palette/colormap visually (default: True). 3. **Output**: Depending on `as_cmap` flag, the function should return: - A discrete color palette (as a list of colors) if `as_cmap` is False. - A continuous colormap if `as_cmap` is True. 4. **Constraints**: - The `color_list` should contain at least two colors. - The function must handle arbitrary color formats (named colors, hex codes, etc.). Performance Requirements: - The function should efficiently handle the blending of up to 10 colors. - The visualization should be displayed using Seaborn compatible plotting functions. Function Signature: ```python def create_and_visualize_palette(color_list: list, as_cmap: bool = False, show_palette: bool = True): pass ``` Example Usage: ```python # Example 1: Creating and visualizing a discrete palette palette = create_and_visualize_palette([\\"blue\\", \\"red\\", \\"green\\"], as_cmap=False, show_palette=True) print(palette) # Example 2: Creating and visualizing a continuous colormap cmap = create_and_visualize_palette([\\"#45a872\\", \\"#979a\\", \\"xkcd:golden\\"], as_cmap=True, show_palette=True) print(cmap) ``` Additional Notes: - Ensure that `show_palette` flag controls whether a plot of the palette/colormap is displayed using a Seaborn plot. - Use suitable Seaborn plotting functions to visualize the palettes (e.g., using `sns.color_palette` for discrete palettes and `sns.heatmap` for continuous colormaps).","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_visualize_palette(color_list, as_cmap=False, show_palette=True): Creates and visualizes both discrete palettes and continuous colormaps using Seaborn\'s blend_palette function. Parameters: - color_list: A list of color strings. - as_cmap: A boolean flag to determine whether to return a continuous colormap (default: False). - show_palette: A boolean flag to determine whether to display the palette/colormap visually (default: True). Returns: - A discrete color palette (as a list of colors) if as_cmap is False. - A continuous colormap if as_cmap is True. if len(color_list) < 2: raise ValueError(\\"color_list must contain at least two colors.\\") palette = sns.blend_palette(color_list, as_cmap=as_cmap) if show_palette: if as_cmap: sns.heatmap([range(10)], cmap=palette, cbar=False, xticklabels=False, yticklabels=False) else: sns.palplot(palette) plt.show() return palette"},{"question":"# Advanced Coding Assessment Question Objective: Your task is to create a system information logger that leverages various runtime services discussed in the documentation. You are required to construct a program to collect and log information about the system configuration and runtime environment in a structured manner. Instructions: 1. **Use `sys` and `sysconfig`:** 1. Retrieve and log the Python version along with several configuration variables like platform, prefix, exec_prefix, etc., using `sys` and `sysconfig`. 2. **Use `dataclasses`:** 1. Define a data class `SystemInfo` to store the retrieved system information. 2. Define another data class `WarningInfo` to log any warnings encountered during the execution. 3. **Use `contextlib` for resource management:** 1. Implement a context manager to handle the opening and closing of a log file. 4. **Use `warnings`:** 1. Capture and log any warnings that occur during the information retrieval process. 5. **Use a script entry point `__main__`:** 1. Ensure your code can be executed as a script and implements an idiomatic `if __name__ == \\"__main__\\":` block. Constraints: 1. Your solution should be able to handle exceptions and log them properly. 2. The logging should include timestamps for when the data was collected. 3. The log file should be stored in the current working directory with the name `system_log.txt`. Expected Input and Output: **Input:** No direct input is required. The script will gather information automatically. **Output:** A `system_log.txt` file containing the following information in a structured format: - Python version. - Platform. - Prefix, Exec_prefix, and other configuration details. - Any warnings encountered during the process. **Log file example content:** ``` Timestamp: 2023-10-15 10:00:00 Python Version: 3.10.0 Platform: Linux Prefix: /usr/local Exec_prefix: /usr/local Warnings: - [Date and Time] Warning: DeprecationWarning: The \'warn\' method is deprecated ``` Submission: Submit your solution as a single `.py` file containing all necessary code to achieve the above objectives.","solution":"import sys import sysconfig import warnings from contextlib import contextmanager from dataclasses import dataclass, field from datetime import datetime @dataclass class SystemInfo: python_version: str platform: str prefix: str exec_prefix: str base_exec_prefix: str base_prefix: str @dataclass class WarningInfo: warnings: list = field(default_factory=list) @contextmanager def open_log_file(filepath): try: log_file = open(filepath, \'a\') yield log_file finally: log_file.close() def retrieve_system_info(): python_version = sys.version platform = sys.platform prefix = sys.prefix exec_prefix = sys.exec_prefix base_exec_prefix = sys.base_exec_prefix base_prefix = sys.base_prefix return SystemInfo(python_version, platform, prefix, exec_prefix, base_exec_prefix, base_prefix) def log_system_info(info, log_file): timestamp = datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") log_file.write(f\\"Timestamp: {timestamp}n\\") log_file.write(f\\"Python Version: {info.python_version}n\\") log_file.write(f\\"Platform: {info.platform}n\\") log_file.write(f\\"Prefix: {info.prefix}n\\") log_file.write(f\\"Exec_prefix: {info.exec_prefix}n\\") log_file.write(f\\"Base_exec_prefix: {info.base_exec_prefix}n\\") log_file.write(f\\"Base_prefix: {info.base_prefix}n\\") log_file.write(\\"n\\") def log_warnings(warnings_info, log_file): for warning in warnings_info.warnings: log_file.write(f\\"[{warning[\'time\']}] Warning: {warning[\'message\']}n\\") log_file.write(\\"n\\") def main(): warnings.simplefilter(\'default\') warnings_info = WarningInfo() def custom_warning_handler(message, category, filename, lineno, file=None, line=None): warning_message = { \'time\': datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\"), \'message\': str(message) } warnings_info.warnings.append(warning_message) warnings.showwarning = custom_warning_handler system_info = retrieve_system_info() log_filename = \\"system_log.txt\\" with open_log_file(log_filename) as log_file: log_system_info(system_info, log_file) log_warnings(warnings_info, log_file) if __name__ == \\"__main__\\": main()"},{"question":"# Custom Tensor Class with Overridden Functions You are required to create a custom tensor-like class named `MyTensor` that can interact with pytorch functions via the `__torch_function__` protocol. Your task includes implementing certain functionalities by leveraging the helper functions from the `torch.overrides` module. Objectives 1. **Define the `MyTensor` Class**: - The class should accept a `torch.Tensor` object during initialization and store it as an attribute. 2. **Override Fundamental Methods**: - Implement the `__torch_function__` protocol in the `MyTensor` class to override the behavior of some basic tensor operations. 3. **Specific Function Implementation**: - Use the `torch.overrides` module to implement the following functionalities: - An overridden addition (`torch.add`) method. - An overridden multiplication (`torch.mul`) method. - A method to check if an object is tensor-like (`is_tensor_like`). Whenever an overridden method is called, it should print \\"Overriden method called!\\" before performing the operation using the underlying `torch.Tensor`. Constraints 1. Do not import any other libraries apart from `torch` and `torch.overrides`. 2. Your implementation should strictly use the helper functions provided by `torch.overrides` where applicable. Example ```python import torch from torch.overrides import handle_torch_function, is_tensor_like class MyTensor: def __init__(self, tensor): self.tensor = tensor @staticmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in torch.overrides.get_overridable_functions(): return NotImplemented print(\\"Overridden method called!\\") return handle_torch_function(MyTensor, func, types, args, kwargs) # Create an instance of MyTensor and perform operations a = MyTensor(torch.tensor([1.0, 2.0, 3.0])) b = MyTensor(torch.tensor([4.0, 5.0, 6.0])) # Operations: Addition and Multiplication c = torch.add(a, b) d = torch.mul(a, b) # Check if c is tensor-like is_tensor_like(c) # This should return and print True after printing \\"Overriden method called!\\" ``` Expected Output When running the example provided: - The addition and multiplication operations should print \\"Overriden method called!\\" and then print the result tensor. - Checking if `c` is tensor-like should invoke the `is_tensor_like` method and return `True`.","solution":"import torch from torch.overrides import handle_torch_function, is_tensor_like class MyTensor: def __init__(self, tensor): self.tensor = tensor @staticmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in torch.overrides.get_overridable_functions(): return NotImplemented print(\\"Overridden method called!\\") args = tuple(arg.tensor if isinstance(arg, MyTensor) else arg for arg in args) result = handle_torch_function(MyTensor, func, types, args, kwargs) return MyTensor(result) if isinstance(result, torch.Tensor) else result def __repr__(self): return f\\"MyTensor({self.tensor})\\" def __torch_function__(self, func, types, args=(), kwargs=None): print(\\"Overridden method called!\\") if kwargs is None: kwargs = {} args = tuple(arg.tensor if hasattr(arg, \'tensor\') else arg for arg in args) result = func(*args, **kwargs) if isinstance(result, torch.Tensor): return MyTensor(result) return result"},{"question":"**Task**: Implement a function that processes a given XML string by escaping its special characters and then demonstrates unescaping the processed string back to its original form. Additionally, prepare a set of key-value pairs to be used as attribute values in XML format, ensuring proper quoting and escaping of special characters. **Function Signature**: ```python def process_xml_data(xml_string: str, attribute_data: dict) -> str: pass ``` **Input**: 1. `xml_string` (str): A raw string containing XML data. It may include characters that need to be escaped, such as `\'&\'`, `\'<\'`, and `\'>\'`. 2. `attribute_data` (dict): A dictionary where keys and values represent names and values of XML attributes respectively. Both keys and values are strings. **Output**: - A single string showing three parts concatenated with a newline character: 1. The escaped version of the `xml_string`. 2. The unescaped version of the escaped `xml_string`, demonstrating the round trip. 3. A new XML element with the attributes generated from `attribute_data`. **Constraints**: - The `xml_string` may contain any valid text, including special XML characters that need to be appropriately escaped. - The `attribute_data` dictionary must handle any possible string values, requiring proper quoting and escaping. **Example**: ```python xml_string = \\"This is a sample text with special characters: <, >, &.\\" attribute_data = {\\"id\\": \\"123\\", \\"title\\": \\"Special & ampersand\\"} output = process_xml_data(xml_string, attribute_data) print(output) ``` **Expected Output**: ``` This is a sample text with special characters: &lt;, &gt;, &amp;. This is a sample text with special characters: <, >, &. <element id=\\"123\\" title=\\"Special &amp; ampersand\\" /> ``` # Explanation 1. The `xml_string` should be escaped to replace `<`, `>`, and `&` with their corresponding XML entities. 2. The escaped string should then be unescaped to verify the process by returning it to its original form. 3. An XML element should be constructed with the provided `attribute_data`, ensuring that the attribute values are correctly quoted and escaped. # Notes - You may use the provided utilities from the `xml.sax.saxutils` module. - Ensure that your implementation handles all edge cases of special characters in the XML data and attributes.","solution":"from xml.sax.saxutils import escape, unescape def process_xml_data(xml_string: str, attribute_data: dict) -> str: # Escaping xml_string escaped_xml = escape(xml_string) # Unescaping the escaped xml string unescaped_xml = unescape(escaped_xml) # Constructing XML element with attributes attributes = \' \'.join(f\'{key}=\\"{escape(value)}\\"\' for key, value in attribute_data.items()) xml_element = f\'<element {attributes} />\' # Combining results result = f\\"{escaped_xml}n{unescaped_xml}n{xml_element}\\" return result"},{"question":"Objective The objective of this question is to assess the student\'s understanding of GPU memory management and stream synchronization using PyTorch\'s `torch.cuda` module. Problem Statement You are tasked with implementing a function that performs a series of matrix multiplications on the GPU using PyTorch. You need to optimize the computation by carefully managing the GPU memory and utilizing streams to overlap data transfer with computation. Implement the function `matrix_chain_multiplication(matrices: List[torch.Tensor]) -> torch.Tensor` with the following specifications: - **Inputs:** - `matrices`: A list of `torch.Tensor` objects, where each tensor represents a matrix. These tensors are initially located on the CPU. - **Outputs:** - Return a single `torch.Tensor` that is the result of multiplying all the matrices in sequence. The result should be moved back to the CPU before being returned. - **Constraints:** - You must use different CUDA streams to overlap data transfers and computation. - Manage GPU memory to ensure that it does not exceed the available memory. If the memory usage exceeds a certain threshold, free the unnecessary memory. - Use appropriate synchronization to ensure correct ordering of operations. - Assume you have access to only one GPU device. - **Performance Requirements:** - The solution should handle large matrices efficiently with minimal overhead due to data transfers and memory management. - Implement proper error-handling to catch any memory-related errors and handle them gracefully. Function Signature ```python from typing import List import torch def matrix_chain_multiplication(matrices: List[torch.Tensor]) -> torch.Tensor: pass # Example usage mat1 = torch.randn(1000, 2000) mat2 = torch.randn(2000, 1500) mat3 = torch.randn(1500, 1000) result = matrix_chain_multiplication([mat1, mat2, mat3]) print(result.shape) # Expected output: torch.Size([1000, 1000]) ``` Guidelines 1. Use `torch.cuda.Stream` to create multiple streams for data transfer and computation. 2. Utilize memory management functions like `torch.cuda.memory_allocated` and `torch.cuda.empty_cache` to handle memory constraints. 3. Employ synchronization methods like `stream.synchronize()` to ensure operations are properly ordered. 4. Write clean and efficient code with comments explaining your strategy.","solution":"from typing import List import torch def matrix_chain_multiplication(matrices: List[torch.Tensor]) -> torch.Tensor: Perform a series of matrix multiplications on the GPU using PyTorch. Optimizes computation by managing GPU memory and using CUDA streams. Args: matrices: List of torch.Tensor objects to be multiplied. Initially on CPU. Returns: A single torch.Tensor that is the result of multiplying all the matrices in sequence. The result is moved back to the CPU before being returned. if not matrices: raise ValueError(\\"The list of matrices cannot be empty.\\") torch.cuda.init() # Set CUDA streams for overlap data transfer and computation data_transfer_stream = torch.cuda.Stream() computation_stream = torch.cuda.Stream() current_result = None # Will hold intermediate results on GPU threshold = torch.cuda.get_device_properties(0).total_memory * 0.7 # Use 70% of the GPU memory for idx, matrix in enumerate(matrices): with torch.cuda.stream(data_transfer_stream): matrix_gpu = matrix.to(\\"cuda\\") if current_result is None: current_result = matrix_gpu else: with torch.cuda.stream(computation_stream): intermediate_result = torch.matmul(current_result, matrix_gpu) # Synchronize to ensure proper ordering computation_stream.synchronize() # Free previous memory if threshold is crossed if torch.cuda.memory_allocated() > threshold: current_result.cpu() torch.cuda.empty_cache() current_result = intermediate_result # Transfer the final result back to the CPU final_result = current_result.cpu() if current_result is not None else torch.tensor([]) # Clear any remaining cached memory torch.cuda.empty_cache() return final_result"},{"question":"**Question: Advanced Playlist Generator** # Objective You are to implement an advanced playlist generator for a music streaming service using the `itertools`, `functools`, and `operator` modules. # Problem Statement The service allows users to generate playlists based on several criteria: song popularity, artist, genre, and duration. Your task is to create a function `generate_playlist` that, given these criteria, produces a playlist iterator that can be efficiently iterated over. # Function Signature ```python def generate_playlist(songs: List[Dict[str, Union[str, int]]], criteria: Dict[str, Union[str, int]]) -> Iterator[Dict[str, Union[str, int]]]: ``` # Input - `songs`: A list of dictionaries. Each dictionary contains: - `\'title\'`: The title of the song (string). - `\'artist\'`: The artist of the song (string). - `\'genre\'`: The genre of the song (string). - `\'popularity\'`: The popularity of the song (integer from 1 to 100). - `\'duration\'`: The duration of the song in seconds (integer). - `criteria`: A dictionary containing any combination of the following keys: - `\'min_popularity\'`: Minimum popularity (integer). - `\'max_duration\'`: Maximum duration in seconds (integer). - `\'artist\'`: Specific artist (string). - `\'genre\'`: Specific genre (string). # Output - Returns an iterator of dictionaries, each representing a song from the filtered playlist. # Example ```python songs = [ {\\"title\\": \\"Song A\\", \\"artist\\": \\"Artist 1\\", \\"genre\\": \\"Pop\\", \\"popularity\\": 80, \\"duration\\": 210}, {\\"title\\": \\"Song B\\", \\"artist\\": \\"Artist 2\\", \\"genre\\": \\"Rock\\", \\"popularity\\": 75, \\"duration\\": 190}, {\\"title\\": \\"Song C\\", \\"artist\\": \\"Artist 1\\", \\"genre\\": \\"Pop\\", \\"popularity\\": 85, \\"duration\\": 200}, ] criteria = { \\"min_popularity\\": 80, \\"artist\\": \\"Artist 1\\" } playlist = generate_playlist(songs, criteria) print(list(playlist)) ``` # Constraints 1. All song titles are unique. 2. The criteria dictionary can contain one or more filter conditions. 3. The function must use appropriate functions from the `itertools`, `functools`, and `operator` modules to perform filtering. # Notes - Make effective use of iterators and functional programming concepts to ensure that your solution is both efficient and elegant. - Consider using `itertools.filterfalse()` or similar functions for filtering based on criteria. - You can use `functools.partial` to create customized filtering functions dynamically based on the criteria provided. # Performance Requirements - The solution should be designed to handle large inputs efficiently, leveraging the iterator pattern to minimize memory usage.","solution":"from typing import List, Dict, Union, Iterator import itertools import functools import operator def generate_playlist(songs: List[Dict[str, Union[str, int]]], criteria: Dict[str, Union[str, int]]) -> Iterator[Dict[str, Union[str, int]]]: Generates a playlist iterator based on given criteria. def applies_criteria(song: Dict[str, Union[str, int]], criteria: Dict[str, Union[str, int]]) -> bool: # Check for all criteria min_popularity = criteria.get(\'min_popularity\', -1) max_duration = criteria.get(\'max_duration\', float(\'inf\')) artist = criteria.get(\'artist\') genre = criteria.get(\'genre\') if song[\'popularity\'] < min_popularity: return False if song[\'duration\'] > max_duration: return False if artist is not None and song[\'artist\'] != artist: return False if genre is not None and song[\'genre\'] != genre: return False return True filtered_songs = filter(functools.partial(applies_criteria, criteria=criteria), songs) return iter(filtered_songs)"},{"question":"# Attention Mechanism Implementation with Block Mask Utilities in PyTorch **Objective**: Implement a function using the PyTorch `torch.nn.attention.flex_attention` module that computes attention scores using block masks. **Task**: Write a function `masked_attention` that takes the following inputs: 1. **queries**: A tensor of shape `(batch_size, num_queries, query_dim)` representing the query vectors. 2. **keys**: A tensor of shape `(batch_size, num_keys, key_dim)` representing the key vectors. 3. **values**: A tensor of shape `(batch_size, num_values, value_dim)` representing the value vectors. 4. **block_mask**: An instance of `BlockMask` to be applied on the attention scores. The function should compute the attention as follows: 1. Calculate the raw attention scores using the dot product of queries and keys. 2. Apply the `block_mask` to these scores. 3. Use the softmax function to normalize the masked scores. 4. Compute the final attention output by performing a weighted sum of the values based on these normalized scores. **Constraints**: - You must use the `flex_attention` function for computing the attention scores. - The `block_mask` should be applied such that masked positions are not attended to. **Function Signature**: ```python import torch from torch.nn.attention.flex_attention import flex_attention, BlockMask def masked_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, block_mask: BlockMask) -> torch.Tensor: # Your implementation here pass ``` **Example**: ```python # Sample data queries = torch.randn(2, 5, 16) # batch_size=2, num_queries=5, query_dim=16 keys = torch.randn(2, 8, 16) # batch_size=2, num_keys=8, key_dim=16 values = torch.randn(2, 8, 32) # batch_size=2, num_values=8, value_dim=32 # Creating a dummy BlockMask instance block_mask = BlockMask() # Call the function with sample data output = masked_attention(queries, keys, values, block_mask) print(output.shape) # Expected output shape: (2, 5, 32) ``` Ensure that your implementation is efficient and correct. The expected output shape should match the number of queries and the dimensionality of the values. **Note**: Refer to the PyTorch documentation for specific attributes and methods available in the `BlockMask` class.","solution":"import torch from torch.nn.functional import softmax class BlockMask: def __init__(self, mask=None): self.mask = mask def apply_mask(self, scores): if self.mask is not None: scores = scores.masked_fill(self.mask == 0, float(\'-inf\')) return scores def masked_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, block_mask: BlockMask) -> torch.Tensor: Compute attention scores using the queries, keys, values tensors with a given block mask. Args: queries (torch.Tensor): The query vectors of shape (batch_size, num_queries, query_dim). keys (torch.Tensor): The key vectors of shape (batch_size, num_keys, key_dim). values (torch.Tensor): The value vectors of shape (batch_size, num_values, value_dim). block_mask (BlockMask): A BlockMask instance to apply masking on the attention scores. Returns: torch.Tensor: The result of the masked attention mechanism, shape (batch_size, num_queries, value_dim). # Calculate raw attention scores raw_scores = torch.bmm(queries, keys.transpose(1, 2)) # (batch_size, num_queries, num_keys) # Apply block mask to attention scores masked_scores = block_mask.apply_mask(raw_scores) # Apply softmax to get normalized attention scores attention_weights = softmax(masked_scores, dim=-1) # (batch_size, num_queries, num_keys) # Compute final attention output attention_output = torch.bmm(attention_weights, values) # (batch_size, num_queries, value_dim) return attention_output"},{"question":"You are given a dataset containing information about different species of penguins. Your objective is to create visualizations using the seaborn package to understand the distribution of variables in the dataset. Dataset Use the `seaborn.load_dataset(\\"penguins\\")` to load the dataset. Task Write a function `plot_penguin_distributions` that accomplishes the following: 1. **Parameters**: - `data`: the DataFrame containing the penguin dataset. - `variable_x`: the variable to plot along the x-axis. - `variable_hue`: the variable to separate the distribution by hue (category). - `statistic`: the form in which the statistic should be displayed (`\\"proportion\\"`, `\\"count\\"`, or `\\"percent\\"`). Default is `\\"proportion\\"`. - `complementary`: boolean indicating whether to plot the empirical complementary CDF (default is `False`). 2. **Functionality**: - Load the penguin dataset from `seaborn` if the provided `data` parameter is `None`. - Create an ECDF plot of the given `variable_x` with coloration based on `variable_hue`. - The statistic on the y-axis should be determined by the `statistic` parameter. (default is `proportion`) - If `complementary` is `True`, plot the empirical complementary CDF instead of the regular CDF. - Display the plot with an appropriate title, labels for the axes, and a legend. 3. **Output**: - Return the `matplotlib` axis object containing the plot. Example ```python import seaborn as sns def plot_penguin_distributions(data=None, variable_x=\\"bill_length_mm\\", variable_hue=\\"species\\", statistic=\\"proportion\\", complementary=False): # implement function pass # Usage penguins = sns.load_dataset(\\"penguins\\") plot_penguin_distributions(data=penguins, variable_x=\\"flipper_length_mm\\", variable_hue=\\"species\\", statistic=\\"count\\", complementary=True) ``` Constraints - Ensure the function is flexible to handle any variable present in the dataset for `variable_x` and `variable_hue`. - The function should be robust and handle the default cases gracefully. - Students should ensure their code is optimized for clarity and performance. Notes - Students are expected to use seaborn and matplotlib libraries to create the plots. - Proper documentation and comments within the code are encouraged to explain logic and steps taken.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_distributions(data=None, variable_x=\\"bill_length_mm\\", variable_hue=\\"species\\", statistic=\\"proportion\\", complementary=False): Plots the empirical distribution (ECDF) of the given variable_x from the penguin dataset, optionally by hue, and with an option to plot complementary CDF. Parameters: data (DataFrame): DataFrame containing the penguin dataset. If None, load from seaborn. variable_x (str): Variable to plot on the x-axis. variable_hue (str): Category variable to separate distribution by colors. statistic (str): Statistic for the y-axis. Options are \\"proportion\\", \\"count\\", \\"percent\\". complementary (bool): Whether to plot the empirical complementary CDF. Returns: ax (matplotlib axis): Axis object containing the plot. # Load the penguin dataset if data is None if data is None: data = sns.load_dataset(\\"penguins\\") # Define ECDF keyword arguments ecdf_kwargs = dict(stat=statistic, complementary=complementary) # Create ECDF plot with Seaborn ax = sns.ecdfplot(data=data, x=variable_x, hue=variable_hue, **ecdf_kwargs) # Set plot titles and labels ax.set_title(f\\"ECDF of {variable_x} by {variable_hue}\\") ax.set_xlabel(variable_x) ax.set_ylabel(f\\"{\'Complementary \' if complementary else \'\'}CDF ({statistic})\\") ax.legend(title=variable_hue) # Display the plot plt.show() return ax"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding and ability to work with Unix group database entries using the \\"grp\\" module in Python. # Problem Statement You are tasked with implementing a function that takes a user name and returns a list of group names in which the user is a member. Additionally, the function should handle the case where the user is not a member of any group by returning an empty list. # Function Signature ```python def get_user_groups(username: str) -> List[str]: ``` # Input - `username` (string): The user name to search for in the group member lists. Assume the username is valid and non-empty. # Output - `List[str]`: A list of group names (strings) where the given user is a member. If the user is not a member of any group, return an empty list. # Constraints - Use the `grp` module to fetch group information. - Handle potential exceptions that might occur when accessing the group database. - Do not make assumptions about the number of groups or group members. # Example ```python # Example 1: # Assuming the group database contains: # [{\'gr_name\': \'admin\', \'gr_passwd\': \'\', \'gr_gid\': 1000, \'gr_mem\': [\'john\', \'jane\']}, # {\'gr_name\': \'public\', \'gr_passwd\': \'\', \'gr_gid\': 1001, \'gr_mem\': []}, # {\'gr_name\': \'staff\', \'gr_passwd\': \'\', \'gr_gid\': 1002, \'gr_mem\': [\'jane\']}] username = \'jane\' print(get_user_groups(username)) # Output: [\'admin\', \'staff\'] # Example 2: username = \'doe\' print(get_user_groups(username)) # Output: [] ``` # Notes - The function should be tested with various group configurations to ensure robustness. - You may assume that all required imports and environment preparations are already in place.","solution":"import grp from typing import List def get_user_groups(username: str) -> List[str]: This function takes a username and returns a list of group names in which the user is a member. If the user is not a member of any group, an empty list is returned. :param username: str, the username to search for in group member lists :return: List[str], list of group names the user is a member of try: groups = grp.getgrall() user_groups = [group.gr_name for group in groups if username in group.gr_mem] return user_groups except Exception as e: # In case of any exception, return an empty list print(f\\"An error occurred: {str(e)}\\") return []"},{"question":"**Coding Assessment Question:** # Task You are required to implement a PyTorch class that demonstrates the proper use of serialization. Specifically, you will create a neural network module, modify its state, save and load its state dictionary, and verify the integrity of loaded data. # Objectives 1. **Define a custom neural network module called `SimpleNet` with two linear layers.** 2. **Generate some random data and train the model for a few epochs.** 3. **Save the state dictionary of the model after training.** 4. **Load the state dictionary into a new instance of `SimpleNet`.** 5. **Verify that the loaded model has the same parameters as the original.** # Instructions 1. **Class Definition:** - Define a class `SimpleNet` inheriting from `torch.nn.Module`. - The class should have an initializer method that initializes two sequential `Linear` layers. - Define a forward method that uses ReLU activation between the linear layers. 2. **Training:** - Generate some random input data and target labels. - Define a loss function and an optimizer. - Train the model for a few epochs and update the state of the model. 3. **Save the Model:** - Save the state dictionary of the trained model to a file named `\'simple_net.pth\'`. 4. **Load the Model:** - Create a new instance of `SimpleNet`. - Load the state dictionary from the file `\'simple_net.pth\'` into the new model instance. 5. **Verification:** - Write a function to verify that the parameters of both the original and loaded models are identical. # Code Template ```python import torch import torch.nn as nn import torch.optim as optim class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(model, data, targets, epochs=5): criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, targets) loss.backward() optimizer.step() def save_model(model, path): torch.save(model.state_dict(), path) def load_model(path): model = SimpleNet() model.load_state_dict(torch.load(path)) return model def verify_models(model1, model2): for p1, p2 in zip(model1.parameters(), model2.parameters()): if not torch.equal(p1, p2): return False return True if __name__ == \\"__main__\\": # Generate random data data = torch.randn(100, 10) targets = torch.randn(100, 1) # Initialize and train the model model = SimpleNet() train(model, data, targets) # Save the model state dict save_model(model, \'simple_net.pth\') # Load the model into a new instance loaded_model = load_model(\'simple_net.pth\') # Verify the models assert verify_models(model, loaded_model), \\"The models do not match!\\" print(\\"Model parameters match after loading.\\") ``` # Constraints - Use PyTorch version >= 1.6. - Ensure the code runs efficiently and avoids unnecessary memory usage. # Notes - Follow the provided code template. - Document your code appropriately to clarify your implementation steps. - Handle any potential exceptions during the save and load processes. Complete this coding task to demonstrate your understanding of PyTorch serialization and state management.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(model, data, targets, epochs=5): criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, targets) loss.backward() optimizer.step() def save_model(model, path): torch.save(model.state_dict(), path) def load_model(path): model = SimpleNet() model.load_state_dict(torch.load(path)) return model def verify_models(model1, model2): for p1, p2 in zip(model1.parameters(), model2.parameters()): if not torch.equal(p1, p2): return False return True if __name__ == \\"__main__\\": # Generate random data data = torch.randn(100, 10) targets = torch.randn(100, 1) # Initialize and train the model model = SimpleNet() train(model, data, targets) # Save the model state dict save_model(model, \'simple_net.pth\') # Load the model into a new instance loaded_model = load_model(\'simple_net.pth\') # Verify the models assert verify_models(model, loaded_model), \\"The models do not match!\\" print(\\"Model parameters match after loading.\\")"},{"question":"# Python Coding Assessment Question: Custom Sequence Type Implementation Objective: To assess your understanding of Python\'s object model, specifically focusing on creating a custom sequence type and implementing necessary methods. # Tasks: You are required to implement a custom sequence type named `CustomList` that behaves similarly to Python\'s built-in list. The `CustomList` must support: 1. Indexing (both positive and negative indices). 2. Slicing, which should return another `CustomList`. 3. Standard list operations such as `append`, `remove`, and `__str__`. # Requirements: 1. **Class Definition**: - Define a class `CustomList` that initializes with an optional list of elements. - The internal storage should be a private attribute. 2. **Methods to Implement**: - `__getitem__(self, index)`: To support indexing and slicing. - `__setitem__(self, index, value)`: To modify an element at a given index. - `__delitem__(self, index)`: To delete an element at a given index. - `__len__(self)`: To return the number of elements. - `append(self, value)`: To add an element to the end of the list. - `remove(self, value)`: To remove the first occurrence of a value. - `__str__(self)`: To return a string representation of the list. 3. **Input/Output**: - The class initializer can take an optional list of elements: `CustomList([1, 2, 3])`. - Support both integer indexing and slicing (e.g., `cust_list[1]`, `cust_list[-1]`, `cust_list[1:3]`). - The string representation should display the list similarly to Python’s built-in list. # Constraints: - Ensure that all method operations are handled within the class without using Python’s built-in list methods directly (except for initialization purposes). # Example Usage: ```python custom_list = CustomList([1, 2, 3, 4]) print(custom_list) # Output: [1, 2, 3, 4] custom_list.append(5) print(custom_list) # Output: [1, 2, 3, 4, 5] custom_list.remove(3) print(custom_list) # Output: [1, 2, 4, 5] print(custom_list[1]) # Output: 2 print(custom_list[-1]) # Output: 5 print(custom_list[1:3]) # Output: CustomList([2, 4]) ``` **Hint**: Consider how slicing works on lists and how you might return a new instance of `CustomList` for slices.","solution":"class CustomList: def __init__(self, elements=None): self._elements = elements if elements is not None else [] def __getitem__(self, index): if isinstance(index, slice): return CustomList(self._elements[index]) return self._elements[index] def __setitem__(self, index, value): self._elements[index] = value def __delitem__(self, index): del self._elements[index] def __len__(self): return len(self._elements) def append(self, value): self._elements.append(value) def remove(self, value): self._elements.remove(value) def __str__(self): return str(self._elements)"},{"question":"In this assessment, you will demonstrate your understanding of Seaborn\'s `violinplot` function by creating a complex visualization using the Titanic dataset. Your task is to generate a plot that provides meaningful insights into the distribution of a numerical variable, grouped by multiple categorical variables, and customized to improve readability. # Task 1. Load the Titanic dataset using Seaborn and prepare the data as necessary. 2. Create a violin plot where: - The x-axis represents the `class` of the passengers. - The y-axis represents the `age` of the passengers. - The violin plots are split to show the distribution of male and female passengers separately. - Each violin plot should represent individual observations using `inner=\\"stick\\"`. - Normalize the width of each violin based on the number of observations. - Set the KDE smoothing to a lower value for sharper distributions. - Use the native scale for both axes. 3. Customize the appearance by: - Setting a linewidth of `1` and linecolor of `k` (black). - Modifying the inner plot appearance, passing additional parameters directly to the inner plotting functions as needed. 4. Add appropriate labels and a title to the plot. # Constraints - Use only the Seaborn and Pandas libraries for the task. - Ensure the code is efficient and handles any potential missing values in the data. # Input/Output The function should not take any inputs from the user and should display the generated plot as the output. # Example Output The output should be a well-constructed violin plot that meets the specified requirements. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_violin_plot(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Drop missing values in \'age\' and \'class\' columns df = df.dropna(subset=[\\"age\\", \\"class\\"]) # Create the violin plot sns.violinplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", split=True, inner=\\"stick\\", density_norm=\\"count\\", bw_adjust=0.5, native_scale=True, linewidth=1, linecolor=\\"k\\", inner_kws={\\"box_width\\": 15, \\"whis_width\\": 2, \\"color\\": \\".8\\"} ) # Add labels and title plt.title(\\"Age Distribution by Class and Sex\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") # Show plot plt.show() # Call the function to generate the plot generate_violin_plot() ``` Note: The provided code is only a partial solution and does not include all customizations specified in the task. Be sure to include and implement all aspects as described.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_violin_plot(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Drop missing values in \'age\' and \'class\' columns df = df.dropna(subset=[\\"age\\", \\"class\\"]) # Create the violin plot sns.violinplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", split=True, inner=\\"stick\\", scale=\\"count\\", bw=0.5, linewidth=1 ) # Add labels and title plt.title(\\"Age Distribution by Class and Sex\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") # Show plot plt.show() # Call the function to generate the plot generate_violin_plot()"},{"question":"**Handling Missing Values in pandas DataFrames** In this exercise, you will demonstrate your ability to handle and manipulate missing values in pandas DataFrames. You need to write functions that identify and process missing values effectively. **Task 1: Identify Missing Values** Write a function `identify_missing_values(df)` that takes a pandas DataFrame `df` as input and returns a new DataFrame that shows the count of missing values for each column in the original DataFrame. **Function Signature:** ```python def identify_missing_values(df: pd.DataFrame) -> pd.DataFrame: ``` **Input:** - `df`: A pandas DataFrame containing various types of columns including numerical, categorical, datetime, and timedelta. **Output:** - A new DataFrame with two columns: - `column`: Column names from the original DataFrame. - `missing_count`: The count of missing values in each column. **Example:** ```python import pandas as pd import numpy as np data = { \'A\': [1, np.nan, 3, 4, np.nan], \'B\': [np.nan, \'two\', np.nan, \'four\', \'five\'], \'C\': [pd.NaT, pd.Timestamp(\'2021-01-01\'), pd.NaT, pd.Timestamp(\'2021-01-04\'), pd.NaT] } df = pd.DataFrame(data) result = identify_missing_values(df) print(result) ``` *Expected Output:* ``` column missing_count 0 A 2 1 B 2 2 C 3 ``` **Task 2: Fill Missing Values** Write a function `fill_missing_values(df)` that fills the missing values in the DataFrame `df` using the following rules: 1. For numerical columns, fill missing values with the mean of the column. 2. For categorical columns, fill missing values with the mode (most frequent value). 3. For datetime or timedelta columns, fill missing values with the most recent non-missing value (forward fill). **Function Signature:** ```python def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame: ``` **Input:** - `df`: A pandas DataFrame containing various types of columns including numerical, categorical, datetime, and timedelta. **Output:** - A new DataFrame where missing values have been filled according to the rules specified above. **Example:** ```python import pandas as pd import numpy as np data = { \'A\': [1, np.nan, 3, 4, np.nan], \'B\': [np.nan, \'two\', np.nan, \'four\', \'five\'], \'C\': [pd.NaT, pd.Timestamp(\'2021-01-01\'), pd.NaT, pd.Timestamp(\'2021-01-04\'), pd.NaT] } df = pd.DataFrame(data) filled_df = fill_missing_values(df) print(filled_df) ``` *Expected Output:* ``` A B C 0 1.0 five NaT 1 2.6666667 two 2021-01-01 2 3.0 five 2021-01-01 3 4.0 four 2021-01-04 4 2.6666667 five 2021-01-04 ``` **Constraints:** - Do not mutate the original DataFrame. - Ensure your solution handles the `np.nan`, `NA`, and `NaT` appropriately for various data types. - Avoid using loops wherever possible to leverage the efficiency of pandas operations. Demonstrate your understanding of these fundamental and advanced pandas concepts to effectively manipulate and clean datasets.","solution":"import pandas as pd import numpy as np def identify_missing_values(df: pd.DataFrame) -> pd.DataFrame: Returns a DataFrame showing the count of missing values for each column in the input DataFrame. missing_count = df.isnull().sum().reset_index() missing_count.columns = [\\"column\\", \\"missing_count\\"] return missing_count def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame: Fills missing values in the DataFrame based on the specified rules: 1. For numerical columns, fill with mean. 2. For categorical columns, fill with mode. 3. For datetime or timedelta columns, fill with forward fill. df_filled = df.copy() for col in df_filled.columns: if pd.api.types.is_numeric_dtype(df_filled[col]): mean_value = df_filled[col].mean() df_filled[col].fillna(mean_value, inplace=True) elif pd.api.types.is_categorical_dtype(df_filled[col]) or df_filled[col].dtype == object: mode_value = df_filled[col].mode()[0] df_filled[col].fillna(mode_value, inplace=True) elif pd.api.types.is_datetime64_any_dtype(df_filled[col]): df_filled[col].fillna(method=\'ffill\', inplace=True) return df_filled"},{"question":"Problem Description You are required to implement a function `is_crawlable` that checks whether certain URLs on a given website can be crawled by a specified user agent according to the site\'s `robots.txt` file. Additionally, you need to return the crawl delay specified for the user agent. Implementation Details You need to implement the following function: ```python def is_crawlable(base_url, useragent, urls): Determines if the given URLs can be crawled by the specified user agent according to the \'robots.txt\' file of the base_url and returns the Crawl-delay for the user agent if specified. Args: - base_url (str): The base URL of the website (e.g., \\"http://www.example.com\\"). - useragent (str): The user agent string (e.g., \\"Googlebot\\"). - urls (list): A list of URL paths (e.g., [\\"/\\", \\"/about\\", \\"/contact\\"]). Returns: - result (dict): A dictionary with two keys: - \\"can_crawl\\" (dict): A dictionary where each key is a URL path from the input list `urls` and the value is a boolean indicating whether the URL can be crawled by the user agent. - \\"crawl_delay\\" (float or None): The crawl delay for the user agent, or None if not specified. ``` Function Constraints - You must use the methods of the `RobotFileParser` class to interact with the `robots.txt` file. - If `robots.txt` does not specify a crawl delay for the user agent, `crawl_delay` should return `None`. - Ensure your function handles cases where `robots.txt` is not present or is incorrectly formatted. Example Usage ```python # Example Usage base_url = \\"http://www.musi-cal.com\\" useragent = \\"*\\" urls = [\\"/\\", \\"/cgi-bin/search?city=San+Francisco\\"] result = is_crawlable(base_url, useragent, urls) print(result) # Expected Output (Dict format may vary depending on the actual content of the robots.txt) # { # \\"can_crawl\\": { # \\"/\\": True, # \\"/cgi-bin/search?city=San+Francisco\\": False # }, # \\"crawl_delay\\": 6 # } ``` Performance Requirements - The function should efficiently handle typical URL lists with up to 1000 URLs. - Network calls for fetching `robots.txt` should be minimized. Additional Notes - Assume all URLs in the `urls` list are correctly formed and belong to the `base_url` domain. - Handle potential exceptions that may occur during fetching and parsing of the `robots.txt` file.","solution":"import urllib.robotparser def is_crawlable(base_url, useragent, urls): Determines if the given URLs can be crawled by the specified user agent according to the \'robots.txt\' file of the base_url and returns the Crawl-delay for the user agent if specified. Args: - base_url (str): The base URL of the website (e.g., \\"http://www.example.com\\"). - useragent (str): The user agent string (e.g., \\"Googlebot\\"). - urls (list): A list of URL paths (e.g., [\\"/\\", \\"/about\\", \\"/contact\\"]). Returns: - result (dict): A dictionary with two keys: - \\"can_crawl\\" (dict): A dictionary where each key is a URL path from the input list `urls` and the value is a boolean indicating whether the URL can be crawled by the user agent. - \\"crawl_delay\\" (float or None): The crawl delay for the user agent, or None if not specified. robots_url = f\\"{base_url.rstrip(\'/\')}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() except: return { \\"can_crawl\\": {url: False for url in urls}, \\"crawl_delay\\": None } crawl_delay = rp.crawl_delay(useragent) can_crawl = {url: rp.can_fetch(useragent, base_url.rstrip(\'/\') + url) for url in urls} return { \\"can_crawl\\": can_crawl, \\"crawl_delay\\": crawl_delay }"},{"question":"Objective Demonstrate your understanding of the `email.header` module in Python by creating and manipulating an email header that includes non-ASCII characters and ensure it can be correctly encoded and decoded. Problem Statement You are required to implement a function `create_encoded_header(subject, charset)` and decode it using a function `decode_encoded_header(encoded_header)`. 1. The function `create_encoded_header(subject, charset)` should: - Create an email header with the given subject and charset using the `Header` class. - Return the encoded string representation of the header. 2. The function `decode_encoded_header(encoded_header)` should: - Decode the encoded email header back to the original subject and charset using the `decode_header` function. - Return a tuple containing the decoded subject and charset. Function Signatures ```python def create_encoded_header(subject: str, charset: str) -> str: pass def decode_encoded_header(encoded_header: str) -> tuple: pass ``` Example Usage ```python # Example input: creating and encoding a header subject = \\"pxf6stal\\" charset = \\"iso-8859-1\\" encoded_header = create_encoded_header(subject, charset) print(encoded_header) # Output should be something like: # \'=?iso-8859-1?q?p=F6stal?=\' # Example input: decoding the encoded header encoded_header = \'=?iso-8859-1?q?p=F6stal?=\' decoded_subject, decoded_charset = decode_encoded_header(encoded_header) print(decoded_subject, decoded_charset) # Output should be: # (\'pxf6stal\', \'iso-8859-1\') ``` Constraints - You should use the `email.header.Header` class for creating and encoding the header. - The `email.header.decode_header` function should be used for decoding the header. - Assume that the input `subject` will always be a valid string, and `charset` will always be a valid character set. Evaluation - Ensure that the created header is RFC-compliant and properly encapsulates non-ASCII characters. - The decoded header should correctly match the original subject and charset.","solution":"from email.header import Header, decode_header def create_encoded_header(subject: str, charset: str) -> str: Create an email header with the given subject and charset using the Header class. Return the encoded string representation of the header. header = Header(subject, charset) return header.encode() def decode_encoded_header(encoded_header: str) -> tuple: Decode the encoded email header back to the original subject and charset using the decode_header function. Return a tuple containing the decoded subject and charset. decoded_header = decode_header(encoded_header)[0] subject, charset = decoded_header # decode bytes to string if necessary if isinstance(subject, bytes): subject = subject.decode(charset) return subject, charset"},{"question":"Problem Statement You are tasked with evaluating how different types of synthetic datasets affect the performance of various machine learning algorithms. Your job is to generate datasets using scikit-learn\'s dataset generators and then build and evaluate models on these datasets. Requirements 1. Generate the following datasets using scikit-learn\'s dataset generators: - A classification dataset using `make_classification` with 2 informative features and 1 cluster per class. - A clustering dataset using `make_blobs` with 3 centers. - A regression dataset using `make_regression` with 100 samples and 2 features. 2. For the classification dataset: - Split the dataset into training and testing sets (80% training, 20% testing). - Train a Random Forest classifier on the training set and evaluate its performance on the testing set using accuracy. 3. For the clustering dataset: - Apply the KMeans clustering algorithm with 3 clusters. - Evaluate the clustering performance using the silhouette score. 4. For the regression dataset: - Split the dataset into training and testing sets (80% training, 20% testing). - Train a Linear Regression model on the training set and evaluate its performance on the testing set using the mean squared error (MSE). Input and Output Formats - Use the following signatures for your functions: ```python def generate_datasets(): # Your code to generate datasets return classification_data, clustering_data, regression_data def evaluate_classification_model(classification_data): # Your code to evaluate classification model return accuracy def evaluate_clustering_model(clustering_data): # Your code to evaluate clustering model return silhouette_score def evaluate_regression_model(regression_data): # Your code to evaluate regression model return mse ``` Each of these functions must return the respective evaluation metric. Constraints - Use `random_state=42` for all random generations to ensure reproducibility. - You may use any other libraries like `numpy`, `pandas`, `matplotlib` for preprocessing, visualization, and other tasks. Example An example to guide your implementation is provided below: ```python from sklearn.datasets import make_classification, make_blobs, make_regression from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, mean_squared_error from sklearn.linear_model import LinearRegression def generate_datasets(): classification_data = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42) clustering_data = make_blobs(n_samples=100, centers=3, random_state=42) regression_data = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42) return classification_data, clustering_data, regression_data def evaluate_classification_model(classification_data): X, y = classification_data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) accuracy = model.score(X_test, y_test) return accuracy def evaluate_clustering_model(clustering_data): X, y = clustering_data model = KMeans(n_clusters=3, random_state=42) model.fit(X) silhouette = silhouette_score(X, model.labels_) return silhouette def evaluate_regression_model(regression_data): X, y = regression_data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) mse = mean_squared_error(y_test, predictions) return mse # Example execution classification_data, clustering_data, regression_data = generate_datasets() print(f\\"Classification Accuracy: {evaluate_classification_model(classification_data)}\\") print(f\\"Clustering Silhouette Score: {evaluate_clustering_model(clustering_data)}\\") print(f\\"Regression MSE: {evaluate_regression_model(regression_data)}\\") ```","solution":"from sklearn.datasets import make_classification, make_blobs, make_regression from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, mean_squared_error from sklearn.linear_model import LinearRegression def generate_datasets(): Generates synthetic datasets for classification, clustering, and regression. classification_data = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42) clustering_data = make_blobs(n_samples=100, centers=3, random_state=42) regression_data = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42) return classification_data, clustering_data, regression_data def evaluate_classification_model(classification_data): Evaluates a Random Forest classifier on the classification dataset. X, y = classification_data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) accuracy = model.score(X_test, y_test) return accuracy def evaluate_clustering_model(clustering_data): Evaluates the KMeans clustering algorithm on the clustering dataset using silhouette score. X, y = clustering_data model = KMeans(n_clusters=3, random_state=42) model.fit(X) score = silhouette_score(X, model.labels_) return score def evaluate_regression_model(regression_data): Evaluates a Linear Regression model on the regression dataset using mean squared error. X, y = regression_data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) mse = mean_squared_error(y_test, predictions) return mse"},{"question":"**Question: Perform Memory Management and Profiling Using PyTorch MPS** *Objective*: Demonstrate your understanding of PyTorch\'s MPS (Metal Performance Shaders) module by implementing a function that manages device memory and profiles a segment of code. *Description*: You are tasked with writing a function `manage_and_profile_mps_memory` that performs the following: 1. Checks the number of available MPS devices. 2. Sets the memory fraction to be used by this process to 0.5. 3. Generates a large tensor and measures the memory allocation before and after its creation. 4. Profiles a block of code that does some basic tensor operations and captures Metal performance metrics. *Function Signature*: ```python def manage_and_profile_mps_memory(): pass ``` *Requirements*: 1. **Device Management**: - Check and print the number of available MPS devices using `torch.mps.device_count()`. - Set the MPS memory fraction for the process using `torch.mps.set_per_process_memory_fraction(0.5)`. 2. **Memory Management**: - Print the current allocated memory before creating a large tensor. - Create a large tensor (e.g., `torch.full((10000, 10000), 1.0, device=\'mps\')`). - Print the current allocated memory after creating the tensor. - Empty the cache using `torch.mps.empty_cache()`. 3. **Profiling**: - Start the profiler using `torch.mps.profiler.start()`. - Perform and print the time taken for a simple tensor operation (e.g., matrix multiplication: `A @ B` where A and B are tensors on \'mps\' device). - Stop the profiler using `torch.mps.profiler.stop()`. - Display a message confirming the profiling has been stopped. *Expected Output*: - Number of MPS devices. - Allocated memory before and after tensor creation. - Confirmation message for profiling start and stop, and the time taken for tensor operation. *Example*: ```python def manage_and_profile_mps_memory(): # Device Management num_devices = torch.mps.device_count() print(f\\"Number of MPS devices: {num_devices}\\") torch.mps.set_per_process_memory_fraction(0.5) # Memory Management mem_before = torch.mps.current_allocated_memory() print(f\\"Memory before tensor allocation: {mem_before}\\") large_tensor = torch.full((10000, 10000), 1.0, device=\'mps\') mem_after = torch.mps.current_allocated_memory() print(f\\"Memory after tensor allocation: {mem_after}\\") torch.mps.empty_cache() # Profiling torch.mps.profiler.start() print(\\"Profiling started.\\") import time start_time = time.time() result = large_tensor @ large_tensor.T end_time = time.time() print(f\\"Tensor operation completed in: {end_time - start_time} seconds\\") torch.mps.profiler.stop() print(\\"Profiling stopped.\\") ``` *Notes*: - Ensure that you are running this on a Mac with Apple Silicon, and the `torch` library is up-to-date to support MPS functionalities. - Handle any exceptions or errors that might occur, especially related to device compatibility.","solution":"import torch import time def manage_and_profile_mps_memory(): try: # Device Management num_devices = torch.mps.device_count() print(f\\"Number of MPS devices: {num_devices}\\") if num_devices == 0: print(\\"No MPS devices available.\\") return torch.mps.set_per_process_memory_fraction(0.5) # Memory Management mem_before = torch.mps.current_allocated_memory() print(f\\"Memory before tensor allocation: {mem_before} bytes\\") large_tensor = torch.full((10000, 10000), 1.0, device=\'mps\') mem_after = torch.mps.current_allocated_memory() print(f\\"Memory after tensor allocation: {mem_after} bytes\\") torch.mps.empty_cache() # Profiling torch.mps.profiler.start() print(\\"Profiling started.\\") start_time = time.time() result = large_tensor @ large_tensor.T end_time = time.time() print(f\\"Tensor operation completed in: {end_time - start_time} seconds\\") torch.mps.profiler.stop() print(\\"Profiling stopped.\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\")"},{"question":"# Asyncio Synchronization Primitives Assessment Objective Implement an asynchronous task scheduler that uses asyncio synchronization primitives to manage tasks competing for access to a shared resource. The scheduler should ensure that only one task can access the shared resource at a time and that tasks can be notified when the resource becomes available. Details 1. **Class Definitions**: - Create a class `Resource` that simulates a shared resource. - Create a class `TaskScheduler` that manages multiple tasks trying to access the `Resource`. 2. **Resource Class**: - Use an `asyncio.Lock` to ensure mutual exclusive access to the resource. ```python import asyncio class Resource: def __init__(self): self.lock = asyncio.Lock() async def access(self, task_id): async with self.lock: print(f\'Task {task_id} is accessing the resource\') await asyncio.sleep(1) # Simulate resource access time print(f\'Task {task_id} has finished accessing the resource\') ``` 3. **TaskScheduler Class**: - Use an `asyncio.Condition` to manage waiting tasks. - Implement methods to add tasks and notify tasks when they can access the resource. ```python class TaskScheduler: def __init__(self, resource): self.resource = resource self.condition = asyncio.Condition() self.tasks = [] async def add_task(self, task_id): async with self.condition: self.tasks.append(task_id) print(f\'Task {task_id} added to the queue\') async def schedule_tasks(self): while True: async with self.condition: while not self.tasks: await self.condition.wait() task_id = self.tasks.pop(0) await self.resource.access(task_id) async def notify_task(self, task_id): async with self.condition: self.condition.notify_all() print(f\'Notify task {task_id}\') ``` 4. **Main Function**: - Create instances of `Resource` and `TaskScheduler`. - Add tasks to the scheduler and run the scheduler. ```python async def main(): resource = Resource() scheduler = TaskScheduler(resource) # Run scheduler in the background asyncio.create_task(scheduler.schedule_tasks()) # Add tasks and notify scheduler await scheduler.add_task(1) await scheduler.add_task(2) await scheduler.add_task(3) await scheduler.notify_task(1) await scheduler.notify_task(2) await scheduler.notify_task(3) asyncio.run(main()) ``` Constraints: - The `Resource` class should use `async with` for the lock to ensure proper synchronization. - The `TaskScheduler` class should use `asyncio.Condition` to manage the task queue and notify all waiting tasks when a new task is added. - Ensure that the resource access is mutually exclusive by using `asyncio.Lock`. Implement these classes and the main function to demonstrate your understanding of asyncio synchronization primitives. Expected Output: ``` Task 1 added to the queue Task 2 added to the queue Task 3 added to the queue Notify task 1 Task 1 is accessing the resource Task 1 has finished accessing the resource Notify task 2 Task 2 is accessing the resource Task 2 has finished accessing the resource Notify task 3 Task 3 is accessing the resource Task 3 has finished accessing the resource ```","solution":"import asyncio class Resource: def __init__(self): self.lock = asyncio.Lock() async def access(self, task_id): async with self.lock: print(f\'Task {task_id} is accessing the resource\') await asyncio.sleep(1) # Simulate resource access time print(f\'Task {task_id} has finished accessing the resource\') class TaskScheduler: def __init__(self, resource): self.resource = resource self.condition = asyncio.Condition() self.tasks = [] async def add_task(self, task_id): async with self.condition: self.tasks.append(task_id) print(f\'Task {task_id} added to the queue\') async def schedule_tasks(self): while True: async with self.condition: while not self.tasks: await self.condition.wait() task_id = self.tasks.pop(0) await self.resource.access(task_id) async def notify_task(self, task_id): async with self.condition: self.condition.notify_all() print(f\'Notify task {task_id}\') async def main(): resource = Resource() scheduler = TaskScheduler(resource) # Run scheduler in the background asyncio.create_task(scheduler.schedule_tasks()) # Add tasks and notify scheduler await scheduler.add_task(1) await scheduler.add_task(2) await scheduler.add_task(3) await scheduler.notify_task(1) await scheduler.notify_task(2) await scheduler.notify_task(3) asyncio.run(main())"},{"question":"# Python Coding Assessment Question Objective: Demonstrate understanding of the `runpy` module by writing a function that executes a given module or script path within a controlled namespace and retrieves specific outputs based on that execution. Problem Statement: You are required to implement a function `execute_module_or_path` that can execute a Python module or script located either by module name or file path, using Python\'s `runpy` module. The function should accept a mode (\\"module\\" or \\"path\\"), an identifier (module name or file path), and a list of variable names to retrieve from the resulting namespace. # Function Signature: ```python def execute_module_or_path(mode: str, identifier: str, var_names: list) -> dict: pass ``` # Input: - `mode`: A string `\\"module\\"` or `\\"path\\"`, indicating whether to execute a Python module by its name or to execute a script by its filesystem path. - `identifier`: A string, representing either the module name (if mode is `\\"module\\"`) or the filesystem path (if mode is `\\"path\\"`). - `var_names`: A list of strings, representing the names of variables to retrieve from the executed module\'s global namespace. # Output: - A dictionary that maps each variable name (from `var_names`) to its value in the executed module/script\'s global namespace. If a variable is not found, it should map to `None`. # Constraints: - Assume the module or script to be executed will not have any side effects that alter the environment significantly. - Handle errors gracefully (e.g., invalid mode, missing module/script, or variable). # Example: ```python # Example 1: Executing by module name result = execute_module_or_path(\\"module\\", \\"example_module\\", [\\"var1\\", \\"var2\\"]) # This should return {\'var1\': value_of_var1_in_example_module, \'var2\': value_of_var2_in_example_module} # Example 2: Executing by script path result = execute_module_or_path(\\"path\\", \\"example_script.py\\", [\\"varA\\", \\"varB\\"]) # This should return {\'varA\': value_of_varA_in_example_script, \'varB\': value_of_varB_in_example_script} ``` Notes: - Utilize `runpy.run_module()` for mode `\\"module\\"` and `runpy.run_path()` for mode `\\"path\\"`. - Ensure that the function can handle missing variables by returning `None` for those variables. - You may assume the environment where this function runs has all necessary permissions and access to the specified module or script path. Good luck!","solution":"import runpy def execute_module_or_path(mode: str, identifier: str, var_names: list) -> dict: Executes a module or script and retrieves specific variable values from its namespace. Parameters: - mode: A string, either \\"module\\" or \\"path\\" indicating whether to execute by module name or filesystem path. - identifier: A string, the module name if mode is \\"module\\", or the file path if mode is \\"path\\". - var_names: A list of strings, representing variable names to retrieve from the executed namespace. Returns: A dictionary mapping variable names to their values, or to None if they do not exist. if mode == \\"module\\": # Execute the module by name namespace = runpy.run_module(identifier, run_name=\\"__main__\\") elif mode == \\"path\\": # Execute the script by path namespace = runpy.run_path(identifier) else: raise ValueError(\\"Invalid mode. Use \'module\' or \'path\'.\\") # Retrieve the requested variables from the namespace result = {} for name in var_names: result[name] = namespace.get(name, None) return result"},{"question":"**Objective:** Implement a function using the `unicodedata` module to process a given Unicode string and return specific properties about each character. **Problem Statement:** You are given a Unicode string containing various characters. Your task is to write a function that processes each character in the string and returns a dictionary where each key is the character and each value is another dictionary containing the following properties of the character: 1. `name`: Name of the character (use `unicodedata.name`). 2. `category`: General category assigned to the character (use `unicodedata.category`). 3. `decimal`: Decimal value assigned to the character, if applicable. Otherwise, return `None` (use `unicodedata.decimal`). 4. `bidirectional`: Bidirectional class assigned to the character (use `unicodedata.bidirectional`). 5. `decomposition`: Decomposition mapping assigned to the character (use `unicodedata.decomposition`). If any property is not applicable or results in an error for a specific character, return `None` for that property in the dictionary. # Function Signature: ```python def unicode_character_properties(input_str: str) -> dict: pass ``` # Input: 1. `input_str` (str): A Unicode string containing various characters. (1 <= len(input_str) <= 1000) # Output: - Return a dictionary where each key is a character from the input string and each value is a dictionary with the specified properties. # Example: ```python input_str = \\"A9π\\" output = unicode_character_properties(input_str) ``` `output` should be: ```python { \'A\': { \'name\': \'LATIN CAPITAL LETTER A\', \'category\': \'Lu\', \'decimal\': None, \'bidirectional\': \'L\', \'decomposition\': \'\' }, \'9\': { \'name\': \'DIGIT NINE\', \'category\': \'Nd\', \'decimal\': 9, \'bidirectional\': \'EN\', \'decomposition\': \'\' }, \'π\': { \'name\': \'GREEK SMALL LETTER PI\', \'category\': \'Ll\', \'decimal\': None, \'bidirectional\': \'L\', \'decomposition\': \'\' } } ``` # Constraints: - You must handle all possible exceptions that could arise from using the `unicodedata` functions and return `None` for the respective properties in such cases. - Do not use any external libraries other than `unicodedata`. # Notes: - Assume that `unicodedata` module is imported and available to use in your implementation. # Grading Criteria: 1. Correctness: The function should return the expected output format with accurate properties. 2. Error Handling: The function should gracefully handle any errors and return `None` for properties that cannot be determined. 3. Efficiency: The function should handle the input constraints efficiently without unnecessary computations. Good luck!","solution":"import unicodedata def unicode_character_properties(input_str: str) -> dict: result = {} for char in input_str: properties = {} try: properties[\'name\'] = unicodedata.name(char) except ValueError: properties[\'name\'] = None properties[\'category\'] = unicodedata.category(char) try: properties[\'decimal\'] = unicodedata.decimal(char) except ValueError: properties[\'decimal\'] = None properties[\'bidirectional\'] = unicodedata.bidirectional(char) properties[\'decomposition\'] = unicodedata.decomposition(char) result[char] = properties return result"},{"question":"# WSGI Environment Manipulation and Server Implementation **Objective:** Design a WSGI application that demonstrates the following abilities: 1. **Environment Inspection and Manipulation:** - Inspect and modify the WSGI environment. - Implement a feature that modifies `PATH_INFO` using the `shift_path_info` utility and prints the modified values. 2. **Response Header Management:** - Use the `Headers` class to add custom headers to the response, including multi-valued headers. 3. **Simple WSGI Server:** - Create and run a simple WSGI HTTP server utilizing `wsgiref.simple_server`. Ensure to serve multiple WSGI applications based on different paths, making use of the `shift_path_info` function to route requests appropriately. # Detailed Requirements: 1. **Environment Inspection and Manipulation:** - Implement a WSGI application that prints the current `PATH_INFO` and then modifies it using `shift_path_info`. Include the modified `PATH_INFO` and `SCRIPT_NAME` in the response. 2. **Response Header Management:** - Create custom response headers using the `Headers` class. Include a standard `Content-Type` header and at least one custom multi-valued header (e.g., `X-Custom-Header` with multiple values). 3. **Simple WSGI Server Implementation:** - Design a server that routes requests to different WSGI applications based on the URL path: - For example, if the URL path starts with `/app1`, route the request to `app1`. - If the URL path starts with `/app2`, route the request to `app2`. - Print appropriate messages indicating which application is being served. # Input and Output Formats: - The WSGI applications should: - Accept a standard WSGI `environ` dictionary and `start_response` callable. - Return an iterable yielding byte strings as the response body. - The simple server should: - Accept incoming HTTP requests. - Route them to the appropriate WSGI application based on the URL path. - Print messages indicating which application is serving the request. # Performance Requirements: - Ensure efficient manipulation of the WSGI environment and response headers. - The server should handle requests efficiently and route them with minimal latency. # Example: Here is a simple outline to get you started: ```python from wsgiref.util import shift_path_info from wsgiref.headers import Headers from wsgiref.simple_server import make_server def app1(environ, start_response): # Modify PATH_INFO shifted = shift_path_info(environ) status = \'200 OK\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) response_headers.add_header(\'X-Custom-Header\', \'value1\', second_value=\'value2\') start_response(status, response_headers.items()) response_body = [ f\\"Shifted PATH_INFO: {shifted}\\".encode(\'utf-8\'), f\\"New PATH_INFO: {environ[\'PATH_INFO\']}\\".encode(\'utf-8\'), f\\"New SCRIPT_NAME: {environ[\'SCRIPT_NAME\']}\\".encode(\'utf-8\') ] return response_body def app2(environ, start_response): status = \'200 OK\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) start_response(status, response_headers.items()) return [b\\"This is app2\\"] def simple_app_router(environ, start_response): path = environ.get(\'PATH_INFO\', \'\').lstrip(\'/\') if path.startswith(\'app1\'): return app1(environ, start_response) elif path.startswith(\'app2\'): return app2(environ, start_response) else: status = \'404 Not Found\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) start_response(status, response_headers.items()) return [b\'Not Found\'] if __name__ == \'__main__\': with make_server(\'\', 8000, simple_app_router) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` - This outline is a starting point. You are required to fully implement and test the above functionalities as per the detailed requirements.","solution":"from wsgiref.util import shift_path_info from wsgiref.headers import Headers from wsgiref.simple_server import make_server def app1(environ, start_response): # Modify PATH_INFO shifted = shift_path_info(environ) status = \'200 OK\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) response_headers.add_header(\'X-Custom-Header\', \'value1\') response_headers.add_header(\'X-Custom-Header\', \'value2\') start_response(status, response_headers.items()) response_body = [ f\\"Shifted PATH_INFO: {shifted}n\\".encode(\'utf-8\'), f\\"New PATH_INFO: {environ[\'PATH_INFO\']}n\\".encode(\'utf-8\'), f\\"New SCRIPT_NAME: {environ[\'SCRIPT_NAME\']}n\\".encode(\'utf-8\') ] return response_body def app2(environ, start_response): status = \'200 OK\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) start_response(status, response_headers.items()) return [b\\"This is app2n\\"] def simple_app_router(environ, start_response): path = environ.get(\'PATH_INFO\', \'\').lstrip(\'/\') if path.startswith(\'app1\'): return app1(environ, start_response) elif path.startswith(\'app2\'): return app2(environ, start_response) else: status = \'404 Not Found\' response_headers = Headers([(\'Content-Type\', \'text/plain\')]) start_response(status, response_headers.items()) return [b\'Not Foundn\'] if __name__ == \'__main__\': with make_server(\'\', 8000, simple_app_router) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Objective This task assesses your understanding of PyTorch\'s JIT (Just-In-Time) compilation utilities, specifically scripting, tracing, and serialization of models. Problem Statement Given a simple feed-forward neural network implemented in PyTorch, your task is to: 1. **Script the model**: Use the `torch.jit.script` function to convert the model to TorchScript. 2. **Trace the model**: Use the `torch.jit.trace` function to trace the model with example inputs. 3. **Serialize and Deserialize**: Save both the scripted model and the traced model to disk, then load them back and evaluate their performances. Constraints - Use the provided PyTorch model architecture. - Ensure the model is tested with a dummy input to verify correctness after loading. - Implement appropriate exception handling to manage any potential errors during scripting, tracing, saving, or loading. Model Architecture ```python import torch import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Dummy inputs input_size = 4 hidden_size = 10 output_size = 3 # Instantiate the model model = SimpleNN(input_size, hidden_size, output_size) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Example input tensor example_input = torch.randn(1, input_size) ``` Instructions 1. **Script the Model**: - Use `torch.jit.script` to script the `SimpleNN` model. - Test the scripted model with the `example_input` tensor. 2. **Trace the Model**: - Use `torch.jit.trace` to trace the `SimpleNN` model with the `example_input` tensor. - Test the traced model with the `example_input` tensor. 3. **Serialize and Deserialize**: - Save both the scripted and traced models to disk using `torch.jit.save`. - Load them back using `torch.jit.load`. - Ensure the loaded models still produce correct outputs with the `example_input` tensor. 4. **Performance Validation**: - Ensure that both the scripted and traced models give the same output as the original model for the same `example_input`. Expected Output - Scripted and traced models saved to disk. - Correctness verified for input-output consistency for both scripted and traced models after loading. - Error messages printed for any exceptions encountered. Implementation ```python def main(): # Instantiate the model model = SimpleNN(input_size, hidden_size, output_size) # Script the model try: scripted_model = torch.jit.script(model) print(\\"Scripting successful\\") except Exception as e: print(f\\"Scripting failed: {e}\\") # Trace the model try: traced_model = torch.jit.trace(model, example_input) print(\\"Tracing successful\\") except Exception as e: print(f\\"Tracing failed: {e}\\") # Save the models scripted_model_path = \\"simple_nn_scripted.pt\\" traced_model_path = \\"simple_nn_traced.pt\\" try: scripted_model.save(scripted_model_path) traced_model.save(traced_model_path) print(\\"Models saved successfully\\") except Exception as e: print(f\\"Model saving failed: {e}\\") # Load the models back try: loaded_scripted_model = torch.jit.load(scripted_model_path) loaded_traced_model = torch.jit.load(traced_model_path) print(\\"Models loaded successfully\\") except Exception as e: print(f\\"Model loading failed: {e}\\") # Validate the performance with torch.no_grad(): original_output = model(example_input) scripted_output = loaded_scripted_model(example_input) traced_output = loaded_traced_model(example_input) assert torch.equal(original_output, scripted_output), \\"Mismatch in scripted model output\\" assert torch.equal(original_output, traced_output), \\"Mismatch in traced model output\\" print(\\"Performance validation successful\\") if __name__ == \\"__main__\\": main() ``` Submission Submit your solution in a single .py file implementing the above logic.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def script_model(model): try: scripted_model = torch.jit.script(model) return scripted_model except Exception as e: print(f\\"Scripting failed: {e}\\") return None def trace_model(model, example_input): try: traced_model = torch.jit.trace(model, example_input) return traced_model except Exception as e: print(f\\"Tracing failed: {e}\\") return None def save_model(model, path): try: torch.jit.save(model, path) print(f\\"Model saved successfully to {path}\\") except Exception as e: print(f\\"Model saving failed: {e}\\") def load_model(path): try: model = torch.jit.load(path) print(f\\"Model loaded successfully from {path}\\") return model except Exception as e: print(f\\"Model loading failed: {e}\\") return None def validate_model(original_model, scripted_model, traced_model, example_input): with torch.no_grad(): original_output = original_model(example_input) scripted_output = scripted_model(example_input) if scripted_model else None traced_output = traced_model(example_input) if traced_model else None return torch.equal(original_output, scripted_output) and torch.equal(original_output, traced_output) # Dummy inputs input_size = 4 hidden_size = 10 output_size = 3 # Instantiate the model model = SimpleNN(input_size, hidden_size, output_size) # Example input tensor example_input = torch.randn(1, input_size)"},{"question":"# Distributed PyTorch Training Assessment In this task, you need to demonstrate your understanding of configuring and running distributed training jobs using `torchrun` in PyTorch. Problem Statement You are provided with a scenario in which you need to set up distributed training for a neural network. The training should be robust against node failures and capable of scaling elastically. Your task is to write a script or series of commands to configure and launch this training. Requirements 1. **Distributed Mode**: The training should be configured to run in elastic mode. 2. **Nodes Configuration**: Specify a training job that scales between a minimum of 2 nodes and a maximum of 5 nodes. 3. **Processes per Node**: Each node should run 4 training processes. 4. **Failure Handling**: The job should tolerate up to 3 restarts due to failures or membership changes. 5. **Rendezvous Configuration**: Use the rendezvous backend `c10d` hosted on a node with the address `node1.example.com:29400`. 6. **Training Script**: Assume the training script is named `train.py` and accepts command-line arguments, which will be passed accordingly. Write a script or commands to launch this distributed training job. Expected Outputs 1. The series of commands or script required to launch the described distributed training job. 2. Explanation of each command-line argument used in the `torchrun` command and how it satisfies the problem requirements. Guidelines - Ensure your solution is clear and well-documented. - Assume all necessary nodes and network configurations are available. - You do not need to write the `train.py` script itself; focus on the `torchrun` command configuration. Hint Refer to the `torchrun` documentation for syntax and usage of distributed training parameters.","solution":"The following `torchrun` command sets up and launches a distributed training job for a neural network using PyTorch. We configure the training job to run in elastic mode with specific configurations as per the problem requirements. Command Explanation: - `torchrun`: Command to start a distributed training job. - `--nproc_per_node=4`: Specifies 4 training processes per node. - `--nnodes=2:5`: Configures the job to scale between a minimum of 2 nodes and a maximum of 5 nodes. - `--rdzv_backend=c10d`: Uses `c10d` as the rendezvous backend for coordination. - `--rdzv_endpoint=node1.example.com:29400`: Specifies the rendezvous endpoint with address `node1.example.com` and port `29400`. - `--max_restarts=3`: Allows the job to tolerate up to 3 restarts due to failures or membership changes. - `--module train`: Executes the `train.py` script. command = torchrun --nproc_per_node=4 --nnodes=2:5 --rdzv_backend=c10d --rdzv_endpoint=node1.example.com:29400 --max_restarts=3 --module train # Execution of the command will be done in the shell, so no further python code is needed."},{"question":"Objective Create a function that interacts with an external service and write tests for it using `unittest.mock` to mock the interactions. Problem Statement You are tasked with writing a function that fetches data from an API and processes it. The API URL and the data processing logic are as follows: 1. Write a function `fetch_and_process(url: str) -> dict` that: - Makes a GET request to the provided `url`. - Expects the response to be a JSON object with the following structure: ```json { \\"data\\": [ {\\"id\\": 1, \\"value\\": 10}, {\\"id\\": 2, \\"value\\": 20}, {\\"id\\": 3, \\"value\\": 30} ] } ``` - Returns a dictionary processing the values as follows: ```python { \\"total\\": <sum_of_values>, \\"count\\": <number_of_items> } ``` 2. Write tests for `fetch_and_process` using `unittest.mock` to mock the `requests.get` method. Input - A single string `url` representing the API endpoint. Output - A dictionary with the keys `total` and `count`. Constraints - Assume the API always returns a valid response or raises an appropriate exception. - You must mock the `requests.get` method to simulate API calls in your tests. Performance requirements - The function should efficiently process the data even if the list is large. Example ```python from unittest.mock import patch def fetch_and_process(url: str) -> dict: # Implement the function here by using \'requests.get\' pass def test_fetch_and_process(): # Write your tests here pass if __name__ == \\"__main__\\": test_fetch_and_process() ``` Instructions: 1. Implement the `fetch_and_process` function. 2. Write test cases using `unittest.mock` to: - Mock successful API responses. - Mock API responses with exceptions. 3. Verify that your function correctly processes the data. Hint: Use `patch` to mock `requests.get`.","solution":"import requests def fetch_and_process(url: str) -> dict: Fetches data from the given URL and processes it to return a dictionary with total of values and count of items. response = requests.get(url) response.raise_for_status() data = response.json()[\\"data\\"] total = sum(item[\\"value\\"] for item in data) count = len(data) return {\\"total\\": total, \\"count\\": count}"},{"question":"**Question: Construct a Dimensionality Reduction Pipeline with PCA and Feature Scaling** You are given a dataset with high-dimensional features and need to implement a pipeline to preprocess and reduce its dimensionality before applying a classifier. Your task is to: 1. Read the input data. 2. Perform feature scaling using `StandardScaler`. 3. Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset. 4. Fit a classifier (e.g., Logistic Regression) on the reduced dataset. Write a function `dimensionality_reduction_pipeline` to perform these tasks. # Function Signature ```python def dimensionality_reduction_pipeline(data: np.ndarray, target: np.ndarray, n_components: int) -> float: Parameters: data (np.ndarray): The input data (num_samples x num_features). target (np.ndarray): The target labels (num_samples,). n_components (int): The number of principal components to keep. Returns: float: The accuracy score of the classifier on the reduced dataset. ``` # Input - **data**: A 2D NumPy array of shape (num_samples, num_features) representing the input data. - **target**: A 1D NumPy array of shape (num_samples,) representing the target labels. - **n_components**: An integer specifying the number of principal components to keep in PCA. # Output - **accuracy**: A float representing the accuracy score of the classifier. # Constraints - You should use `StandardScaler` for feature scaling. - You must use `PCA` for dimensionality reduction. - The classifier should be `LogisticRegression`. # Example ```python import numpy as np # Example input data = np.random.rand(150, 50) target = np.random.randint(0, 2, 150) n_components = 10 # Function call accuracy = dimensionality_reduction_pipeline(data, target, n_components) print(f\\"Accuracy: {accuracy:.2f}\\") ``` # Notes 1. Ensure you follow best practices in scikit-learn for constructing pipelines. 2. Consider using `train_test_split` to train the classifier and evaluate it. 3. You may need to install `numpy` and `scikit-learn` if not already available. 4. Your implementation should handle different shapes of input data without errors. Happy coding!","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def dimensionality_reduction_pipeline(data: np.ndarray, target: np.ndarray, n_components: int) -> float: Parameters: data (np.ndarray): The input data (num_samples x num_features). target (np.ndarray): The target labels (num_samples,). n_components (int): The number of principal components to keep. Returns: float: The accuracy score of the classifier on the reduced dataset. # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42) # Create a pipeline with StandardScaler, PCA, and Logistic Regression pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=n_components)), (\'classifier\', LogisticRegression(solver=\'lbfgs\', max_iter=1000)) ]) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) # Predict on the test data y_pred = pipeline.predict(X_test) # Calculate and return the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# PyTorch Named Tensors: Image Processing Task Context: Named Tensors in PyTorch provide a way to attach explicit names to tensor dimensions, enhancing readability and ensuring correctness through runtime checks. In this task, you will work with named tensors to implement a series of image processing operations. Task: Implement a function `process_images` that: 1. Receives a batch of images in a 4-dimensional tensor with the named dimensions (\'N\', \'C\', \'H\', \'W\'), where: - `N` is the batch size, - `C` is the number of channels (e.g., 3 for RGB images), - `H` is the height, - `W` is the width. 2. Normalizes each image channel-wise using provided mean and standard deviation tensors. 3. Converts the images from (\'N\', \'C\', \'H\', \'W\') format to a flattened (\'N\', \'features\') format, where `features` correspond to the product of `C`, `H`, and `W`. 4. Returns the processed tensor. Input: - `images`: A 4-dimensional named tensor with dimensions (\'N\', \'C\', \'H\', \'W\'). - `mean`: A 1-dimensional named tensor with dimension (\'C\',) containing the mean values for each channel. - `std`: A 1-dimensional named tensor with dimension (\'C\',) containing the standard deviation values for each channel. Output: - A 2-dimensional named tensor with dimensions (\'N\', \'features\'). Constraints: - Do not assume the specific values of `N`, `C`, `H`, or `W`. - Ensure to handle the naming conventions correctly throughout the operations. Example: ```python import torch # Example input tensors images = torch.randn(16, 3, 64, 64, names=(\'N\', \'C\', \'H\', \'W\')) mean = torch.tensor([0.485, 0.456, 0.406], names=(\'C\',)) std = torch.tensor([0.229, 0.224, 0.225], names=(\'C\',)) def process_images(images, mean, std): # Step 1: Normalize images mean = mean.refine_names(\'C\') std = std.refine_names(\'C\') normalized_images = (images - mean.align_as(images)) / std.align_as(images) # Step 2: Flatten the images processed_images = normalized_images.flatten([\'C\', \'H\', \'W\'], \'features\') return processed_images # Example usage processed_images = process_images(images, mean, std) print(processed_images.names) # Expected output: (\'N\', \'features\') print(processed_images.shape) # Expected output: torch.Size([16, 12288]) for this specific example ``` Note: Ensure that your implementation adheres to the named tensor conventions and properly handles the dimension names throughout the operations.","solution":"import torch def process_images(images, mean, std): Normalizes a batch of images and flattens them to a 2D tensor format. Parameters: images (torch.Tensor): A 4D named tensor with dimensions (\'N\', \'C\', \'H\', \'W\'). mean (torch.Tensor): A 1D named tensor with dimension (\'C\',). std (torch.Tensor): A 1D named tensor with dimension (\'C\',). Returns: torch.Tensor: A 2D named tensor with dimensions (\'N\', \'features\'). # Normalize images mean = mean.refine_names(\'C\') std = std.refine_names(\'C\') normalized_images = (images - mean.align_as(images)) / std.align_as(images) # Flatten the images processed_images = normalized_images.flatten((\'C\', \'H\', \'W\'), \'features\') return processed_images"},{"question":"# Implementing a Custom Iterator Class **Objective**: Demonstrate your understanding of Python iterators by implementing a custom iterator class. Your task is to create a class `CustomRangeIterator` that mimics the behavior of Python\'s built-in `range` function using the iterator protocol. **Requirements**: 1. **Class Definition**: Create a class `CustomRangeIterator`. 2. **Initialization**: The class should be initialized with three parameters: `start`, `end`, and `step`. 3. **Iterator Protocol**: Implement the `__iter__` and `__next__` methods to make the class an iterator. 4. **PyIter_Check**: Internally, use the iterator check mechanism as part of the iteration process. 5. **Error Handling**: Ensure proper handling of the StopIteration when the iteration is complete. **Function Signatures**: Your implementation should include the following methods within the `CustomRangeIterator` class: ```python class CustomRangeIterator: def __init__(self, start: int, end: int, step: int): # Initialize the iterator with start, end, and step pass def __iter__(self): # Return the iterator object (self) pass def __next__(self): # Return the next value and handle the StopIteration exception pass ``` # Constraints: 1. `start`, `end`, and `step` are integers where step is non-zero. 2. Your implementation should be efficient in terms of time and space complexity. # Example Usage: ```python custom_range = CustomRangeIterator(1, 10, 2) for num in custom_range: print(num) # Output: 1, 3, 5, 7, 9 ``` # Additional Note: - You do not need to handle asynchronous iterators in this task but focus on implementing a synchronous iterator. - Ensure your code adheres to PEP8 standards for readability. **Evaluation Criteria**: - Correctness of the implementation following Python’s iterator protocol. - Efficient handling of iteration through the defined range. - Proper handling of edge cases, such as step being positive or negative and handling StopIteration correctly. # Submission: Submit your `CustomRangeIterator` class implementation code.","solution":"class CustomRangeIterator: def __init__(self, start: int, end: int, step: int): Initialize the iterator with start, end, and step. self.start = start self.end = end self.step = step self.current = start def __iter__(self): Return the iterator object itself. return self def __next__(self): Return the next value in the iteration and raise StopIteration when done. if (self.step > 0 and self.current >= self.end) or (self.step < 0 and self.current <= self.end): raise StopIteration else: current_value = self.current self.current += self.step return current_value"},{"question":"# Path Manipulation and Analysis Objective Your task is to design a function that takes a list of file paths and performs multiple operations to analyze and manipulate these paths using the `os.path` module. Function Signature ```python def analyze_paths(paths: list) -> dict: pass ``` Requirements 1. **Input**: - `paths` (list): A list of file paths (strings) to be analyzed. All elements in the list are guaranteed to be valid paths. 2. **Output**: - A dictionary with the following keys and their respective values: - `\\"absolute_paths\\"`: A list of absolute paths corresponding to the input paths. - `\\"base_names\\"`: A list of base names of the input paths. - `\\"directories\\"`: A list of directory names of the input paths. - `\\"common_path\\"`: The longest common sub-path of all input paths. - `\\"size_info\\"`: A dictionary mapping each input path to its file size. (Use 0 if the file does not exist) - `\\"valid_paths\\"`: A list of boolean values indicating if each input path exists. 3. **Constraints**: - If the input `paths` list is empty, all the values in the output dictionary should be empty or None. - Assume paths are valid and no need to handle exceptional cases for invalid paths. 4. **Performance Requirements**: - The implementation should efficiently handle a list of up to 1000 paths. Examples ```python paths = [\\"/usr/bin/python\\", \\"/usr/bin/python3\\", \\"/usr/local/bin/python\\"] result = analyze_paths(paths) # result should be something like: { \\"absolute_paths\\": [\\"/usr/bin/python\\", \\"/usr/bin/python3\\", \\"/usr/local/bin/python\\"], \\"base_names\\": [\\"python\\", \\"python3\\", \\"python\\"], \\"directories\\": [\\"/usr/bin\\", \\"/usr/bin\\", \\"/usr/local/bin\\"], \\"common_path\\": \\"/usr\\", \\"size_info\\": {\\"/usr/bin/python\\": 0, \\"/usr/bin/python3\\": 0, \\"/usr/local/bin/python\\": 0}, \\"valid_paths\\": [False, False, False] } ``` Additional Notes - Use the `os.path` module functions to accomplish the tasks. - The `commonpath` function should be used to determine the longest common sub-path.","solution":"import os def analyze_paths(paths: list) -> dict: if not paths: return { \\"absolute_paths\\": [], \\"base_names\\": [], \\"directories\\": [], \\"common_path\\": None, \\"size_info\\": {}, \\"valid_paths\\": [] } absolute_paths = [os.path.abspath(path) for path in paths] base_names = [os.path.basename(path) for path in paths] directories = [os.path.dirname(path) for path in paths] common_path = os.path.commonpath(paths) size_info = {path: os.path.getsize(path) if os.path.exists(path) else 0 for path in paths} valid_paths = [os.path.exists(path) for path in paths] return { \\"absolute_paths\\": absolute_paths, \\"base_names\\": base_names, \\"directories\\": directories, \\"common_path\\": common_path, \\"size_info\\": size_info, \\"valid_paths\\": valid_paths }"},{"question":"**Python Embedding Task: Evaluating Python Code in Various Contexts** **Objective:** In this task, you are required to implement a function in Python that simulates the behavior of embedding Python code execution, similar to how you might do it using C functions provided in the `python310` package. This will demonstrate your understanding of compiling and running Python code dynamically and managing execution contexts. **Function to Implement:** ```python def execute_python_code(source: str, source_type: str, globals: dict = None, locals: dict = None, optimize: int = -1) -> any: Execute Python source code based on the provided input type and return the resulting value. Parameters: - source (str): The Python code to execute. - source_type (str): The type of the source input. It can be \\"string\\", \\"file\\", or \\"single\\". - globals (dict): A dictionary representing the global execution context. - locals (dict): A dictionary representing the local execution context. - optimize (int): Optimization level to use when compiling the code. Defaults to -1. Returns: - any: The result of the executed code. pass ``` **Input:** 1. `source`: A string containing Python source code. 2. `source_type`: A string indicating the type of source. Possible values are: - \\"string\\": Source is a Python code string. - \\"file\\": Source is a filename containing Python code. - \\"single\\": Source is a single Python statement to be executed in an interactive manner. 3. `globals`: (optional) A dictionary for the global symbol table. 4. `locals`: (optional) A dictionary for the local symbol table. 5. `optimize`: (optional) Optimization level of the compiler. Should be -1, 0, 1, or 2. **Output:** The function should return the result of the Python code execution. If an exception occurs, handle it appropriately and return `None`. **Constraints:** - You may assume that the input source code is valid Python code. - For \\"file\\" source_type, the filename provided in `source` must exist. - The function should handle different source types appropriately and return execution results. - You are encouraged to use native Python functions to emulate the embedding behavior described in the python310 package. **Example Usage:** ```python # Example: Executing a code string result = execute_python_code(\\"2 + 2\\", \\"string\\") print(result) # Output: 4 # Example: Executing a single statement result = execute_python_code(\\"print(\'Hello, World!\')\\", \\"single\\") # Output: Hello, World! print(result) # Output: None # Example: Executing code from a file result = execute_python_code(\\"example_script.py\\", \\"file\\") print(result) # Output: Depends on example_script.py contents ``` **Notes:** - Carefully handle different types of source inputs. - Optimize the execution context management, ensuring global and local dictionaries are appropriately utilized. If `globals` or `locals` are not provided, use default empty dictionaries. - Consider potential extensions for this function to enhance its robustness for various use cases.","solution":"def execute_python_code(source: str, source_type: str, globals: dict = None, locals: dict = None, optimize: int = -1) -> any: Execute Python source code based on the provided input type and return the resulting value. Parameters: - source (str): The Python code to execute. - source_type (str): The type of the source input. It can be \\"string\\", \\"file\\", or \\"single\\". - globals (dict): A dictionary representing the global execution context. - locals (dict): A dictionary representing the local execution context. - optimize (int): Optimization level to use when compiling the code. Defaults to -1. Returns: - any: The result of the executed code or execution context dict if \\"string\\" or \\"file\\". if globals is None: globals = {} if locals is None: locals = {} try: if source_type == \\"string\\": code = compile(source, \'<string>\', \'exec\', optimize=optimize) exec(code, globals, locals) return locals elif source_type == \\"file\\": with open(source, \'r\') as file: file_content = file.read() code = compile(file_content, source, \'exec\', optimize=optimize) exec(code, globals, locals) return locals elif source_type == \\"single\\": code = compile(source, \'<string>\', \'single\', optimize=optimize) exec(code, globals, locals) return locals else: raise ValueError(\\"Invalid `source_type` provided. Must be \'string\', \'file\', or \'single\'.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"# Question: Advanced Data Visualization with Seaborn Using the `seaborn` library, specifically focusing on the `seaborn.objects` module and `matplotlib`, create a comprehensive plotting function that visualizes given dataset information with customized elements. The function should demonstrate the following abilities: 1. **Load and Initialize Data**: - Load the `diamonds` dataset from `seaborn`. - Initialize a `seaborn.objects.Plot` object using the dataset, plotting the relationship between the carat and price of diamonds. 2. **Create Different Plot Types**: - Add scatter plot points using `so.Dots()`. - Add histogram bars for price distribution using `so.Bars()` and `so.Hist()`. - Facet the histogram by the `cut` attribute and apply logarithmic scaling on the x-axis. 3. **Create Subfigures**: - Create a `matplotlib.figure.Figure` with constrained layout, and split it into two subfigures side by side. 4. **Apply Customizations**: - In the left subfigure, embed the `seaborn.objects.Plot` with scatter plot points. - In the right subfigure, embed the facetted histogram plot. - Customize the left scatter plot by adding a rectangle annotation and a text label inside the rectangle that says \\"Price vs Carat\\". 5. **Output the Final Plot**: - Ensure the function displays the final composite plot with the specified customizations. Function Signature ```python def advanced_seaborn_plot(): # Your implementation here pass # When executed, it should produce and display the required plot. advanced_seaborn_plot() ``` Requirements - Use `seaborn.objects`, `matplotlib.pyplot`, and `matplotlib.figure`. - The left subfigure should display scatter plot points. - The right subfigure should display facetted histogram bars for price distribution. - Customize the left plot with a rectangle annotation and text label as mentioned. - Ensure the final plot layout is clear and visually appealing. Constraints - The solution should handle the dataset efficiently without unnecessary recomputation. - The plots should be properly labeled and readable. - Sample output should closely resemble the provided task description, ensuring adapted customization rules are followed. Performance Consideration - The function should execute efficiently within a reasonable time frame when handling the provided dataset. - It should manage memory usage, ensuring subplots are embedded and rendered without causing performance lags.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def advanced_seaborn_plot(): # Load the diamonds dataset from seaborn diamonds = sns.load_dataset(\'diamonds\') # Define the seaborn objects for scatter plot scatter_plot = ( so.Plot(diamonds, x=\'carat\', y=\'price\') .add(so.Dots()) ) # Define the seaborn objects for facetted histogram hist_plot = ( so.Plot(diamonds, x=\'price\') .facet(\'cut\') .scale(x=\'log\') .add(so.Bars(), so.Hist()) ) # Create a matplotlib figure with constrained layout fig = plt.figure(constrained_layout=True, figsize=(14, 7)) # Create subfigures side by side subfigs = fig.subfigures(1, 2, wspace=0.1) # Add scatter plot to the left subfigure ax0 = subfigs[0].subplots() scatter_plot.on(ax0) # Customize the left scatter plot by adding rectangle annotation and text label ax0.annotate(\'Price vs Carat\', xy=(3, 15000), xytext=(2, 20000), arrowprops=dict(facecolor=\'black\', shrink=0.05), bbox=dict(boxstyle=\\"round,pad=0.3\\", edgecolor=\\"red\\", facecolor=\\"none\\")) # Add facetted histogram to the right subfigure ax1 = subfigs[1].subplots() hist_plot.on(ax1) # Display the final composite plot plt.show() # Invoke the function to display the plot advanced_seaborn_plot()"},{"question":"**Objective:** Assess the student\'s ability to use Seaborn\'s object-oriented interface to create complex visualizations with multiple layers and transformations. **Question:** You are given the `penguins` dataset from the Seaborn library. Your task is to create a visualization that: - Plots the `body_mass_g` of the penguins on the x-axis and `species` on the y-axis, with different colors for `sex`. - Includes error bars representing the standard deviation of `body_mass_g` for each species and sex combination. - Facets the plot by `island`, so each island has its own subplot. To solve this problem, follow these steps: 1. Load the `penguins` dataset using Seaborn. 2. Create a `Plot` object with `body_mass_g` on the x-axis, `species` on the y-axis, and color based on `sex`. 3. Add dots representing the aggregated mean values for each species and sex combination, with dodging to separate the sex groups. 4. Add error bars representing the standard deviation of `body_mass_g`. 5. Facet the plot by `island`. # Input Format: None. (Assume the `penguins` dataset is already loaded using Seaborn\'s load_dataset function.) # Output Format: The output should be a Seaborn plot visualized using the object-oriented interface. # Example: The following is a conceptual description and not the exact output. ``` +----------------------------+-------------------------+ | | | | Island: Biscoe | Island: Dream | | | | | x: body_mass_g | x: body_mass_g | | y: species | y: species | | color: sex | color: sex | | dots: Aggregated means | dots: Aggregated means | | error bars: Std Dev | error bars: Std Dev | +----------------------------+-------------------------+ ``` # Constraints: - You must use Seaborn\'s object-oriented interface. - The plot should be clear and correctly labeled. # Starter Code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Display the plot plot ``` You are required to complete the `plot` creation following the specifications mentioned above.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Display the plot plot"},{"question":"# Seaborn Coding Assessment Given the seaborn `tips` dataset, your task is to create a bar plot using seaborn\'s `objects` interface. The bar plot should display the total bill amount for each day, differentiated by time of day (Lunch or Dinner), and further dodged by gender (sex). Additionally, to make the plot more visually informative, apply the following customizations: 1. Fill any empty space that might appear between bars due to missing combinations. 2. Add a small gap between dodged groups. 3. Apply jitter to avoid overlapping of points. # Requirements 1. **Input**: You don\'t need to take any input from the user. The dataset will be provided. 2. **Output**: A seaborn plot must be generated displaying the specified features. 3. **Constraints**: Use the seaborn `objects` interface and specifically make use of `Dodge`, `Jitter`, and other mentioned transformations. 4. **Performance**: The plot should be generated efficiently without redundant code. # Implementation ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(by=[\\"time\\"], empty=\\"fill\\", gap=0.1)) .add(so.Dot(), so.Jitter()) # Display the plot p.show() ``` This implementation ensures: - The `total_bill` is aggregated for each day. - Bars are dodged not only by the time of day but also by gender, filling any gaps. - A small gap is added between dodged groups. - Jitter is applied to avoid overlapping of points. # Submission Submit your solution as a Python script containing your implementation.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_total_bill_by_day_time_sex(): Generates a bar plot displaying the total bill amount for each day, differentiated by time of day (Lunch or Dinner), and further dodged by gender (sex). Applies jitter and fills empty spaces. # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\'sex\') .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(by=[\\"time\\"], empty=\\"fill\\", gap=0.1)) .add(so.Dot(), so.Jitter()) # Display the plot p.show()"},{"question":"**Question:** You are tasked with implementing a function that computes the Jacobian matrix of a neural network model with respect to its input. This exercise aims to assess your understanding of PyTorch\'s `torch.func` API, particularly focusing on the use of `jacrev` for computing Jacobians. # Requirements: 1. **Neural Network Model:** Create a simple feedforward neural network with one hidden layer, using `torch.nn.Module`. 2. **Function to Compute Jacobian:** Implement a function `compute_jacobian` that: - Takes a neural network model and an input tensor. - Returns the Jacobian matrix of the model\'s output with respect to its input. # Specifications: - **Input:** - `model`: A `torch.nn.Module` representing the neural network. - `input_tensor`: A `torch.Tensor` of shape `(N,)` where `N` is the input dimension to the neural network. - **Output:** - A `torch.Tensor` representing the Jacobian matrix of shape `(M, N)` where `M` is the output dimension of the neural network. # Constraints: - Do not use numerical approaches for computing gradients; utilize PyTorch\'s autograd capabilities through `torch.func`. - Assume the input tensor and the neural network parameters are differentiable. - Ensure your solution is efficient and leverages batched operations where possible. # Example: ```python import torch import torch.nn as nn import torch.func as F class SimpleNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_dim, hidden_dim) self.output = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x def compute_jacobian(model, input_tensor): def model_output_function(x): return model(x) jacobian_func = F.jacrev(model_output_function) jacobian_matrix = jacobian_func(input_tensor) return jacobian_matrix # Example usage: model = SimpleNN(input_dim=3, hidden_dim=5, output_dim=2) input_tensor = torch.randn(3) jacobian = compute_jacobian(model, input_tensor) print(jacobian.shape) # Should print: torch.Size([2, 3]) ``` In this example, the `compute_jacobian` function correctly computes the Jacobian matrix of the neural network\'s output with respect to its input. Submit your implementation of the `compute_jacobian` function along with the definition of the `SimpleNN` class.","solution":"import torch import torch.nn as nn import torch.func as F class SimpleNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_dim, hidden_dim) self.output = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x def compute_jacobian(model, input_tensor): def model_output_function(x): return model(x.unsqueeze(0)).squeeze(0) jacobian_matrix = F.jacrev(model_output_function)(input_tensor) return jacobian_matrix"},{"question":"**Question:** **Color Mixer Application** You are required to design a simple Color Mixer application using Tkinter\'s color chooser dialog. The aim of this application is to allow users to pick two different colors and then display the mixed color. **Requirements:** 1. Create a GUI window using Tkinter. 2. Provide two buttons to open the color chooser dialog. 3. After selecting two colors, display both selected colors and the resulting mixed color in separate areas of the window. 4. Implement a suitable method to mix two colors. 5. Add an option to clear the selections and start over. **Function Specifications:** - **Input:** None (all inputs are via the GUI elements) - **Output:** A Tkinter window displaying the selected colors and the mixed color. - **Constraints:** - Ensure that user selections are properly validated (e.g., handle cases where the user cancels the color selection dialog). - Mixing colors can be done by averaging the RGB values of the selected colors. **Performance Requirements:** - The application should respond smoothly to user actions, with minimal noticeable delay. **Hints:** - Use `tkinter.colorchooser.askcolor()` to open the color chooser dialog. - RGB values can be extracted from the color selection. - Averaging RGB values can be done by taking the arithmetic mean of the corresponding values from both colors. **Example:** 1. User opens the application and clicks the first \\"Choose Color\\" button. 2. The color chooser dialog appears, and the user selects a specific color (let\'s say RGB(255,0,0)). 3. The user clicks the second \\"Choose Color\\" button and selects another color (let\'s say RGB(0,0,255)). 4. Both selected colors are displayed, and the mixed color (RGB(127,0,127)) is also shown in the window. Your task is to implement this Color Mixer Application in Python. Please include all necessary imports and handle all edge cases.","solution":"import tkinter as tk from tkinter import colorchooser def mix_colors(color1, color2): Mix two colors by averaging their RGB values. r1, g1, b1 = color1 r2, g2, b2 = color2 mixed_color = ((r1 + r2) // 2, (g1 + g2) // 2, (b1 + b2) // 2) return mixed_color def rgb_to_hex(rgb): Convert RGB tuple to HEX string. return \'#%02x%02x%02x\' % rgb class ColorMixerApp: def __init__(self, root): self.root = root root.title(\\"Color Mixer Application\\") self.color1 = None self.color2 = None self.color1_button = tk.Button(root, text=\\"Choose Color 1\\", command=self.select_color1) self.color1_button.pack() self.color2_button = tk.Button(root, text=\\"Choose Color 2\\", command=self.select_color2) self.color2_button.pack() self.clear_button = tk.Button(root, text=\\"Clear\\", command=self.clear_selections) self.clear_button.pack() self.color1_label = tk.Label(root, text=\\"Color 1\\", bg=\\"white\\") self.color1_label.pack(fill=tk.BOTH, expand=True) self.color2_label = tk.Label(root, text=\\"Color 2\\", bg=\\"white\\") self.color2_label.pack(fill=tk.BOTH, expand=True) self.mixed_color_label = tk.Label(root, text=\\"Mixed Color\\", bg=\\"white\\") self.mixed_color_label.pack(fill=tk.BOTH, expand=True) def select_color1(self): color = colorchooser.askcolor() if color[0]: self.color1 = tuple(map(int, color[0])) self.color1_label.config(bg=rgb_to_hex(self.color1)) self.update_mixed_color() def select_color2(self): color = colorchooser.askcolor() if color[0]: self.color2 = tuple(map(int, color[0])) self.color2_label.config(bg=rgb_to_hex(self.color2)) self.update_mixed_color() def update_mixed_color(self): if self.color1 and self.color2: mixed_color = mix_colors(self.color1, self.color2) self.mixed_color_label.config(bg=rgb_to_hex(mixed_color)) def clear_selections(self): self.color1 = None self.color2 = None self.color1_label.config(bg=\\"white\\") self.color2_label.config(bg=\\"white\\") self.mixed_color_label.config(bg=\\"white\\") if __name__ == \\"__main__\\": root = tk.Tk() app = ColorMixerApp(root) root.mainloop()"},{"question":"# Task: Implement Custom Boolean Handling Functions You need to implement a function utilizing Python\'s built-in handling of Booleans, as described in the documentation. Function 1: `custom_bool_from_long` Write a function `custom_bool_from_long(v)` that accepts an integer value `v` and returns a Python boolean `True` if the value is non-zero, and `False` otherwise. **Input:** - `v`: An integer (e.g., -10, 0, 15) **Output:** - A Python boolean value (`True` or `False`) Function 2: `is_custom_bool` Write a function `is_custom_bool(obj)` that accepts any object `obj` and checks if it is a Python boolean (`True` or `False`). This function should return `True` if `obj` is of type boolean and `False` otherwise. **Input:** - `obj`: Any Python object **Output:** - A Python boolean value (`True` or `False`) # Constraints 1. You should not use the `type()` or `isinstance()` functions directly to check for booleans. 2. Aim to mimic the behavior of `PyBool_Check` for the type-checking part, though using Python language capabilities. 3. Focus on using the Boolean macros\' idea for your return values, ensuring correct reference handling implicitly as part of Python. # Example ```python assert custom_bool_from_long(10) == True assert custom_bool_from_long(0) == False assert is_custom_bool(True) == True assert is_custom_bool(1) == False assert is_custom_bool(False) == True assert is_custom_bool(\'True\') == False ``` Implement the above functions in the most Pythonic way, ensuring correct handling and adherence to the described principles for Boolean values as explained in the documentation.","solution":"def custom_bool_from_long(v): Takes an integer value `v` and returns `True` if the value is non-zero, and `False` otherwise. return bool(v) def is_custom_bool(obj): Takes any object `obj` and returns `True` if the object is a Python boolean (`True` or `False`), and `False` otherwise. return obj is True or obj is False"},{"question":"**Advanced Seaborn Plotting and Data Normalization** You have been provided with a dataset named `healthexp`, containing the following columns: - `Year`: The year of the data entry. - `Spending_USD`: The healthcare spending value in US dollars. - `Country`: The country associated with the spending data. Implement a Python function using Seaborn that demonstrates the following tasks: 1. Load the `healthexp` dataset. 2. Normalize the `Spending_USD` data for each `Country` relative to its maximum value within the dataset. 3. Create and display two plots: - The first plot displays the normalized `Spending_USD` values using a line plot, with different colors for each country and the y-axis labeled as \\"Spending relative to maximum amount\\". - The second plot displays the healthcare spending as a percentage change from the year 1970 baseline using a line plot, with different colors for each country and the y-axis labeled as \\"Percent change in spending from 1970 baseline\\". 4. The plots should be displayed in a single figure with two subplots arranged vertically. **Constraints:** - Use the Seaborn `objects` interface for the plots. - Ensure readability of the plots with appropriate labeling for both axes and a legend representing the countries. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_health_expenditure(): # Your implementation here ``` **Expected Output:** - The function should display a matplotlib figure with two vertically arranged subplots each containing proper data and formatting as described. Ensure the code is runnable and does not return any values.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt def plot_health_expenditure(): # Load the healthexp dataset df = load_dataset(\'healthexp\') # Normalize the Spending_USD data for each Country df[\'Normalized_Spending\'] = df.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Calculate the spending as percentage change from 1970 baseline baseline_1970 = df[df[\'Year\'] == 1970].set_index(\'Country\')[\'Spending_USD\'] df[\'Percent_Change_Spending\'] = df.apply(lambda row: (row[\'Spending_USD\'] - baseline_1970[row[\'Country\']]) / baseline_1970[row[\'Country\']] * 100, axis=1) # Set up the subplots fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12)) # Plot 1: Normalized spending p1 = (so.Plot(data=df, x=\'Year\', y=\'Normalized_Spending\', color=\'Country\') .add(so.Line()) ) p1.on(ax1) ax1.set_ylabel(\'Spending relative to maximum amount\') ax1.set_title(\'Normalized Healthcare Spending\') # Plot 2: Percent change in spending from 1970 baseline p2 = (so.Plot(data=df, x=\'Year\', y=\'Percent_Change_Spending\', color=\'Country\') .add(so.Line()) ) p2.on(ax2) ax2.set_ylabel(\'Percent change in spending from 1970 baseline\') ax2.set_title(\'Percent Change in Healthcare Spending from 1970\') # Display the plot plt.tight_layout() plt.show()"},{"question":"You are required to implement a custom SAX parser using the `xml.sax.xmlreader` module that processes a given XML file and extracts specific information. Your parser should be able to handle XML incrementally and provide details about certain elements and attributes as described below. Requirements: 1. **Input**: An XML file containing a collection of `item` elements. Each `item` has multiple attributes and nested elements. The XML structure is as follows: ```xml <items> <item id=\\"1\\" category=\\"A\\"> <name>Item One</name> <price>10.99</price> </item> <item id=\\"2\\" category=\\"B\\"> <name>Item Two</name> <price>13.49</price> </item> ... </items> ``` 2. **Output**: A dictionary where keys are item `id` attributes and values are dictionaries representing the `category`, `name`, and `price` of each item. 3. **Function Signature**: Implement the function `parse_items(xml_file: str) -> dict`. Constraints and Details: - Use the `IncrementalParser` class to handle the XML parsing incrementally. - Implement a custom `ContentHandler` to process `item` elements and their nested data. - Ensure that the solution is memory efficient and can handle large XML files. Example: ```python xml_content = \'\'\'<items> <item id=\\"1\\" category=\\"A\\"> <name>Item One</name> <price>10.99</price> </item> <item id=\\"2\\" category=\\"B\\"> <name>Item Two</name> <price>13.49</price> </item> </items>\'\'\' # Write the xml_content to a temporary file and pass its path to the parse_items function result = parse_items(\'temp.xml\') print(result) ``` Output: ```python { \\"1\\": {\\"category\\": \\"A\\", \\"name\\": \\"Item One\\", \\"price\\": \\"10.99\\"}, \\"2\\": {\\"category\\": \\"B\\", \\"name\\": \\"Item Two\\", \\"price\\": \\"13.49\\"} } ``` Notes: - The XML file can be very large; ensure your solution uses the incremental approach efficiently. - Handle any potential XML errors or inconsistencies gracefully by using appropriate error handlers. Implement the function `parse_items(xml_file: str) -> dict` below: ```python import xml.sax import xml.sax.xmlreader class ItemHandler(xml.sax.ContentHandler): def __init__(self): self.items = {} self.current_item = None self.current_element = None def startElement(self, name, attrs): if name == \'item\': self.current_item = { \'id\': attrs[\'id\'], \'category\': attrs[\'category\'] } self.current_element = name def endElement(self, name): if name == \'item\': if self.current_item: item_id = self.current_item.pop(\'id\') self.items[item_id] = self.current_item self.current_item = None self.current_element = None def characters(self, content): if self.current_element in [\'name\', \'price\']: if self.current_item and self.current_element: self.current_item[self.current_element] = content.strip() def parse_items(xml_file: str) -> dict: parser = xml.sax.make_parser() handler = ItemHandler() parser.setContentHandler(handler) with open(xml_file, \'r\') as f: parser.parse(f) return handler.items # Example Testing # Ensure to write xml_content to a temporary file named \'temp.xml\' before testing ```","solution":"import xml.sax import xml.sax.xmlreader class ItemHandler(xml.sax.ContentHandler): def __init__(self): self.items = {} self.current_item = None self.current_element = None def startElement(self, name, attrs): if name == \'item\': self.current_item = { \'id\': attrs[\'id\'], \'category\': attrs[\'category\'] } self.current_element = name def endElement(self, name): if name == \'item\': if self.current_item: item_id = self.current_item.pop(\'id\') self.items[item_id] = self.current_item self.current_item = None self.current_element = None def characters(self, content): if self.current_element in [\'name\', \'price\']: if self.current_item and self.current_element: if content.strip(): self.current_item[self.current_element] = content.strip() def parse_items(xml_file: str) -> dict: parser = xml.sax.make_parser() handler = ItemHandler() parser.setContentHandler(handler) with open(xml_file, \'r\') as f: parser.parse(f) return handler.items"},{"question":"**Question: List Operation Challenge** Write a Python function `item_reorganizer(data, constraints, multiplier)` that takes in the following three parameters: 1. `data`: A list of tuples where each tuple consists of two elements - an integer and a string. 2. `constraints`: A list of integers representing index positions in the `data` list. 3. `multiplier`: An integer that is used to multiply select integer values in the `data` list. The function should perform the following tasks: 1. Remove all tuples from `data` whose integer values match any value in the `constraints` list. 2. Multiply the integer value of the remaining tuples by the `multiplier`. 3. Sort the `data` list by the integer values in ascending order. 4. Return the modified `data` list. **Function Signature:** ```python def item_reorganizer(data: list, constraints: list, multiplier: int) -> list: # Your code here ``` **Input:** - `data`: A list of tuples where each tuple contains an integer and a string. Example: `[(2, \'apple\'), (3, \'banana\'), (5, \'cherry\')]` - `constraints`: A list of integers. Example: `[2, 5]` - `multiplier`: An integer. Example: `3` **Output:** - A list of tuples with the transformations applied. **Example:** ```python data = [(2, \'apple\'), (3, \'banana\'), (5, \'cherry\')] constraints = [2, 5] multiplier = 3 assert item_reorganizer(data, constraints, multiplier) == [(9, \'banana\')] ``` **Explanation:** 1. The tuples `(2, \'apple\')` and `(5, \'cherry\')` are removed because their integer values are in the `constraints` list. 2. The integer value in the remaining tuple `(3, \'banana\')` is multiplied by 3 to become `(9, \'banana\')`. 3. The final list is `[(9, \'banana\')]`, which is sorted by the integer values. **Constraints:** - The length of `data` will not exceed 10^4. - `constraints` will not exceed a length of 100. - The integer values and `multiplier` will be between `-10^6` and `10^6`. **Note:** - Ensure your solution is optimized for performance within the given constraints.","solution":"def item_reorganizer(data, constraints, multiplier): Removes tuples from data whose integer values are in constraints, then multiplies the integer values of the remaining tuples by multiplier and sorts the list by these integer values in ascending order. # Removing items that match the constraints data = [item for item in data if item[0] not in constraints] # Multiplying the remaining items with the multiplier data = [(x[0] * multiplier, x[1]) for x in data] # Sorting the list by integer values data.sort() return data"},{"question":"**Objective:** To evaluate your understanding of the `zipfile` module and your ability to work with ZIP archives programmatically. # Problem Statement You are given the task of creating a large ZIP archive management tool that will perform the following operations: 1. Create a ZIP archive from a given directory structure, supporting multiple compression methods. 2. List the contents of an existing ZIP archive. 3. Extract contents from a ZIP archive to a specified directory. 4. Ensure the integrity of the ZIP archive by validating the files during extraction. # Detailed Requirements 1. **Create a ZIP Archive** - Implement a function `create_zip_archive(input_directory: str, output_zipfile: str, compression_method: str) -> None`: - `input_directory`: Path to the directory whose contents need to be zipped. - `output_zipfile`: Path where the ZIP file should be created. - `compression_method`: Compression method to use (`\'stored\'`, `\'deflated\'`, `\'bzip2\'`, `\'lzma\'`). - The function should raise a `ValueError` if the provided compression method is not supported. 2. **List ZIP Archive Contents** - Implement a function `list_zip_contents(zipfile_path: str) -> List[str]`: - `zipfile_path`: Path to the ZIP file. - The function should return a list of the names of all files and directories in the ZIP archive. 3. **Extract ZIP Archive Contents** - Implement a function `extract_zip_contents(zipfile_path: str, extract_directory: str, pwd: bytes = None) -> None`: - `zipfile_path`: Path to the ZIP file. - `extract_directory`: Directory where the contents should be extracted. - `pwd`: Optional password for encrypted ZIP files (default is `None`). 4. **Validate ZIP Archive** - Implement a function `validate_zip(zipfile_path: str) -> bool`: - `zipfile_path`: Path to the ZIP file. - The function should read all the files in the archive and check their CRC\'s and file headers. - Return `True` if the ZIP archive is valid, otherwise return `False`. # Constraints - Ensure the input directory exists before creating the ZIP archive. - Handle exceptions gracefully and provide appropriate error messages. - Keep memory usage efficient, especially when dealing with large files. # Example ```python def create_zip_archive(input_directory, output_zipfile, compression_method): # Your code here def list_zip_contents(zipfile_path): # Your code here def extract_zip_contents(zipfile_path, extract_directory, pwd=None): # Your code here def validate_zip(zipfile_path): # Your code here # Desired functionality: # create_zip_archive(\'my_directory\', \'archive.zip\', \'deflated\') # contents = list_zip_contents(\'archive.zip\') # extract_zip_contents(\'archive.zip\', \'extract_here\') # is_valid = validate_zip(\'archive.zip\') ``` Demonstrate the usage of your functions with appropriate examples. **Note:** Your code will be evaluated based on correctness, efficiency, and adherence to Python best practices.","solution":"import os import zipfile from typing import List, Optional def create_zip_archive(input_directory: str, output_zipfile: str, compression_method: str) -> None: Creates a ZIP archive from a given directory structure. Args: - input_directory (str): Path to the directory whose contents need to be zipped. - output_zipfile (str): Path where the ZIP file should be created. - compression_method (str): Compression method to use (\'stored\', \'deflated\', \'bzip2\', \'lzma\'). compression_methods = { \'stored\': zipfile.ZIP_STORED, \'deflated\': zipfile.ZIP_DEFLATED, \'bzip2\': zipfile.ZIP_BZIP2, \'lzma\': zipfile.ZIP_LZMA } if compression_method not in compression_methods: raise ValueError(\\"Unsupported compression method. Choose from \'stored\', \'deflated\', \'bzip2\', \'lzma\'.\\") if not os.path.isdir(input_directory): raise FileNotFoundError(f\\"The directory {input_directory} does not exist.\\") compression = compression_methods[compression_method] with zipfile.ZipFile(output_zipfile, \'w\', compression=compression) as zipf: for root, _, files in os.walk(input_directory): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, start=input_directory) zipf.write(file_path, arcname=arcname) def list_zip_contents(zipfile_path: str) -> List[str]: Lists the contents of an existing ZIP archive. Args: - zipfile_path (str): Path to the ZIP file. Returns: List of names of all files and directories in the ZIP archive. with zipfile.ZipFile(zipfile_path, \'r\') as zipf: return zipf.namelist() def extract_zip_contents(zipfile_path: str, extract_directory: str, pwd: Optional[bytes] = None) -> None: Extracts the contents of a ZIP archive to a specified directory. Args: - zipfile_path (str): Path to the ZIP file. - extract_directory (str): Directory where the contents should be extracted. - pwd (bytes, optional): Optional password for encrypted ZIP files (default is None). with zipfile.ZipFile(zipfile_path, \'r\') as zipf: zipf.extractall(path=extract_directory, pwd=pwd) def validate_zip(zipfile_path: str) -> bool: Validates the integrity of a ZIP archive. Args: - zipfile_path (str): Path to the ZIP file. Returns: True if the ZIP archive is valid, otherwise False. try: with zipfile.ZipFile(zipfile_path, \'r\') as zipf: return zipf.testzip() is None except zipfile.BadZipFile: return False"},{"question":"# Advanced Coding Assessment Question Implementing and Using Abstract Base Classes (ABCs) **Problem Statement:** You are required to implement a series of classes and interfaces to manage different types of workers in a company. The company has regular employees and contractors. All workers must have a method for calculating their pay, but the method for regular employees must account for bonuses while contractors do not receive bonuses. The task involves creating an abstract base class `Worker` and its concrete subclasses `Employee` and `Contractor`. You will also need to register a virtual subclass `Intern` that is not directly derived from `Worker`. **Requirements:** 1. Define an abstract base class `Worker` with the following: - An abstract method `calculate_pay` which will be overridden in subclasses. 2. Define a concrete subclass `Employee` that inherits from `Worker` with the following: - An `__init__` method that accepts `name`, `hours_worked`, `hourly_rate`, and `bonus`. - An implementation of the `calculate_pay` method which calculates pay including the bonus. 3. Define another concrete subclass `Contractor` that also inherits from `Worker` with the following: - An `__init__` method that accepts `name`, `hours_worked`, and `hourly_rate`. - An implementation of the `calculate_pay` method which calculates pay without bonuses. 4. Define a class `Intern` that is not derived from `Worker` but will be registered as a virtual subclass with the following: - An `__init__` method that accepts `name`, `stipend`. - A `calculate_pay` method which returns the stipend amount. 5. Register the `Intern` class as a virtual subclass of `Worker`. **Constraints:** - `hours_worked` is an integer between 0 and 168 (inclusive). - `hourly_rate` and `stipend` are positive floats. - `bonus` is a non-negative float. **Example Usage:** ```python from abc import ABC, abstractmethod class Worker(ABC): @abstractmethod def calculate_pay(self): pass class Employee(Worker): def __init__(self, name, hours_worked, hourly_rate, bonus): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate self.bonus = bonus def calculate_pay(self): return (self.hours_worked * self.hourly_rate) + self.bonus class Contractor(Worker): def __init__(self, name, hours_worked, hourly_rate): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate def calculate_pay(self): return self.hours_worked * self.hourly_rate class Intern: def __init__(self, name, stipend): self.name = name self.stipend = stipend def calculate_pay(self): return self.stipend Worker.register(Intern) # Example Instantiation and Usage emp = Employee(\\"Alice\\", 40, 20.0, 100.0) cont = Contractor(\\"Bob\\", 30, 30.0) intern = Intern(\\"Charlie\\", 1000.0) print(emp.calculate_pay()) # Outputs: 900.0 print(cont.calculate_pay()) # Outputs: 900.0 print(isinstance(intern, Worker)) # Outputs: True print(intern.calculate_pay()) # Outputs: 1000.0 ``` This exercise assesses the students\' understanding of abstract base classes, the use of `abc` module components, method overriding, and the concept of virtual subclasses in Python.","solution":"from abc import ABC, abstractmethod class Worker(ABC): @abstractmethod def calculate_pay(self): pass class Employee(Worker): def __init__(self, name, hours_worked, hourly_rate, bonus): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate self.bonus = bonus def calculate_pay(self): return (self.hours_worked * self.hourly_rate) + self.bonus class Contractor(Worker): def __init__(self, name, hours_worked, hourly_rate): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate def calculate_pay(self): return self.hours_worked * self.hourly_rate class Intern: def __init__(self, name, stipend): self.name = name self.stipend = stipend def calculate_pay(self): return self.stipend Worker.register(Intern) # Example Instantiation and Usage emp = Employee(\\"Alice\\", 40, 20.0, 100.0) cont = Contractor(\\"Bob\\", 30, 30.0) intern = Intern(\\"Charlie\\", 1000.0) print(emp.calculate_pay()) # Outputs: 900.0 print(cont.calculate_pay()) # Outputs: 900.0 print(isinstance(intern, Worker)) # Outputs: True print(intern.calculate_pay()) # Outputs: 1000.0"},{"question":"# Python Coding Assessment: Terminal UI with Curses **Objective:** To demonstrate your understanding of the `curses` module, write a Python program that initializes a terminal-based user interface application using the `curses` library. Your task will involve window management, handling user inputs, and displaying output. **Task:** Create a Python script that does the following: 1. **Initialize the Curses Library:** Set up the main screen. 2. **Create a Window:** Create a new window that occupies the middle portion of the terminal. 3. **Handle User Input:** - Capture user input such as arrow keys to move within the window. - Implement basic editing commands in the window such as adding characters, deleting characters, and navigating. 4. **Color Display (Optional):** Use at least two different color pairs to distinguish different parts of your UI. 5. **Exit Mechanism:** Provide an option to exit the program gracefully by pressing a specific key (e.g., \'q\'). **Requirements:** 1. **Window Handling:** - The window should be created in the middle of the terminal screen. - The window should resize dynamically if the terminal size changes. 2. **Text Editing and Cursor Movement:** - Use appropriate `curses` functions to allow the user to navigate and edit text within the window. - Include functionality for at least the following controls: - Arrow keys to move the cursor. - Backspace to delete characters. - Enter key to insert a new line. - Typing characters to insert them at the current cursor position. 3. **Color Management:** - Initialize and use at least two color pairs. - Color one portion of the window differently to highlight it (this could be a header, footer, or sidebar). 4. **Exit Mechanism:** - Allow the user to exit the program by pressing \'q\'. **Constraints:** - The program should handle errors gracefully. - Use appropriate exception handling to manage curses-related errors, especially for terminal resizing and initialization. **Input Format:** - User inputs via the keyboard. **Output Format:** - There is no predefined output format. The terminal screen should reflect the changes in real-time as per the user input. **Sample Execution:** Upon running the script, the terminal should display a UI with a window in the middle. The user will be able to type within this window, use arrow keys to move the cursor, and see the text updated in real-time. Different sections of the UI should be highlighted with different colors. Pressing \'q\' should exit the application gracefully. --- To get started, consider using the following code skeleton: ```python import curses def main(stdscr): # Clear screen stdscr.clear() # Initialize color pairs curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_YELLOW, curses.COLOR_BLACK) # Create a window in the center of the screen height, width = stdscr.getmaxyx() win_height, win_width = height // 2, width // 2 begin_y, begin_x = (height - win_height) // 2, (width - win_width) // 2 win = curses.newwin(win_height, win_width, begin_y, begin_x) while True: # Get user input key = stdscr.getch() # Conditional logic to handle different input cases if key == ord(\'q\'): break # Refresh the window win.refresh() if __name__ == \\"__main__\\": curses.wrapper(main) ``` --- Implement this program and make sure to use the various functions provided by the `curses` module effectively.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() # Initialize color pairs curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_YELLOW, curses.COLOR_BLACK) # Function to resize window def resize_window(): height, width = stdscr.getmaxyx() win_height, win_width = height // 2, width // 2 begin_y, begin_x = (height - win_height) // 2, (width - win_width) // 2 win = curses.newwin(win_height, win_width, begin_y, begin_x) win.bkgd(\' \', curses.color_pair(1)) win.border() return win win = resize_window() # Enable key inputs and cursor visibility stdscr.keypad(True) curses.curs_set(1) cursor_y, cursor_x = 1, 1 win.move(cursor_y, cursor_x) while True: key = stdscr.getch() if key == ord(\'q\'): break elif key == curses.KEY_RESIZE: win = resize_window() elif key in (curses.KEY_UP, curses.KEY_DOWN, curses.KEY_LEFT, curses.KEY_RIGHT): if key == curses.KEY_UP and cursor_y > 1: cursor_y -= 1 elif key == curses.KEY_DOWN and cursor_y < win.getmaxyx()[0] - 2: cursor_y += 1 elif key == curses.KEY_LEFT and cursor_x > 1: cursor_x -= 1 elif key == curses.KEY_RIGHT and cursor_x < win.getmaxyx()[1] - 2: cursor_x += 1 win.move(cursor_y, cursor_x) elif key == curses.KEY_BACKSPACE or key == 127: if cursor_x > 1: cursor_x -= 1 win.delch(cursor_y, cursor_x) elif cursor_y > 1: cursor_y -= 1 cursor_x = win.getmaxyx()[1] - 2 win.move(cursor_y, cursor_x) win.delch(cursor_y, cursor_x) elif key == curses.KEY_ENTER or key == 10: cursor_y += 1 cursor_x = 1 else: win.addch(cursor_y, cursor_x, key) cursor_x += 1 win.refresh() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# XML Parsing and Error Handling with `xml.parsers.expat` Problem Description You are given a string containing an XML document and are required to parse this document to extract and display the following information: 1. The names of all the start and end elements in the document. 2. The character data contained within each element, if any. 3. Comments within the XML document. 4. Any parsing errors encountered, with details on the line and column number of the error. You must create a function `parse_xml(xml_string: str) -> None` that performs the parsing and outputs the extracted information. If there are parsing errors, the function should print the error details. Function Signature ```python def parse_xml(xml_string: str) -> None: ``` Input - `xml_string`: A string representing the XML document to be parsed. Output - The function should print the following: - For each start element, print: `Start element: <element_name> <attributes>` - For each end element, print: `End element: <element_name>` - For each character data within an element, print: `Character data: <character_data>` - For each comment, print: `Comment: <comment_data>` - For any parsing errors, print: `Error: <error_message> at line <line_number>, column <column_number>` Constraints - Avoid using deprecated functionalities. - Ensure all elements, their attributes, and their character data are correctly captured and displayed. - Handle and display errors gracefully without stopping the parsing process abruptly. Example ```python xml_input = <?xml version=\\"1.0\\"?> <root> <!-- This is a comment --> <child name=\\"child1\\">Some text</child> <child name=\\"child2\\">More text</child> </root> parse_xml(xml_input) ``` **Expected Output:** ``` Start element: root {} Comment: This is a comment Start element: child {\'name\': \'child1\'} Character data: Some text End element: child Start element: child {\'name\': \'child2\'} Character data: More text End element: child End element: root ``` **Error Handling Example:** ```python xml_input = <?xml version=\\"1.0\\"?> <root> <child>Unclosed tag <child>Another child</child> </root> parse_xml(xml_input) ``` **Expected Output:** ``` Start element: root {} Start element: child {} Character data: Unclosed tag Error: XML_ERROR_TAG_MISMATCH at line 5, column 2 Start element: child {} Character data: Another child End element: child End element: root ``` *Hint*: Use the `StartElementHandler`, `EndElementHandler`, `CharacterDataHandler`, `CommentHandler`, and include appropriate error handling in your function.","solution":"import xml.parsers.expat def parse_xml(xml_string: str) -> None: Parse the given XML string and print information about the start elements, end elements, character data, comments and any parsing errors. :param xml_string: A string containing the XML document. def start_element(name, attrs): print(f\\"Start element: {name} {attrs}\\") def end_element(name): print(f\\"End element: {name}\\") def char_data(data): if data.strip(): # Only print non-whitespace character data print(f\\"Character data: {data}\\") def comment(data): print(f\\"Comment: {data}\\") def error_handler(ex): print(f\\"Error: {ex} at line {parser.ErrorLineNumber}, column {parser.ErrorColumnNumber}\\") parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.CommentHandler = comment try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError as e: error_handler(e)"},{"question":"# Coding Challenge: Creating and Manipulating Custom Methods In this challenge, you are required to create custom methods and instance methods in Python and understand how they work with user-defined classes. You will implement functions that will create, check, and retrieve functions from method objects and instance method objects. Task 1: Creating Instance Method 1. **Function Name**: `create_instance_method` 2. **Parameters**: - `func`: A callable object (function) to be used as an instance method. 3. **Returns**: An instance method object wrapping the given function. 4. **Example**: ```python def hello(): return \\"Hello, World!\\" im = create_instance_method(hello) # Expected output: Type of im should be instance method ``` Task 2: Check Instance Method 1. **Function Name**: `is_instance_method` 2. **Parameters**: - `obj`: An object to check. 3. **Returns**: `True` if the object is an instance method, `False` otherwise. 4. **Example**: ```python result = is_instance_method(im) # Expected output: True ``` Task 3: Retrieve Function from Instance Method 1. **Function Name**: `get_instance_method_function` 2. **Parameters**: - `im`: An instance method object. 3. **Returns**: The original function linked with the instance method. 4. **Example**: ```python func = get_instance_method_function(im) result = func() # Expected output: \\"Hello, World!\\" ``` Task 4: Creating Bound Method 1. **Function Name**: `create_bound_method` 2. **Parameters**: - `func`: A callable object (function) to be used as a method. - `self`: The instance to which the function will be bound. 3. **Returns**: A method object (bound method) wrapping the given function and instance. 4. **Example**: ```python class MyClass: def __init__(self, name): self.name = name def greet(self): return f\\"Hello, {self.name}!\\" obj = MyClass(\\"Alice\\") bm = create_bound_method(greet, obj) # Expected output: Type of bm should be method (bound) ``` Task 5: Check Bound Method 1. **Function Name**: `is_bound_method` 2. **Parameters**: - `obj`: An object to check. 3. **Returns**: `True` if the object is a bound method, `False` otherwise. 4. **Example**: ```python result = is_bound_method(bm) # Expected output: True ``` Task 6: Retrieve Function from Bound Method 1. **Function Name**: `get_bound_method_function` 2. **Parameters**: - `meth`: A bound method object. 3. **Returns**: The original function linked with the bound method. 4. **Example**: ```python func = get_bound_method_function(bm) # Expected output: Function greet # To verify, you can bind it to another instance: another_obj = MyClass(\\"Bob\\") result = func(another_obj) # Expected output: \\"Hello, Bob!\\" ``` Task 7: Retrieve Instance from Bound Method 1. **Function Name**: `get_bound_method_instance` 2. **Parameters**: - `meth`: A bound method object. 3. **Returns**: The instance associated with the bound method. 4. **Example**: ```python instance = get_bound_method_instance(bm) # Expected output: Instance should be obj with name \\"Alice\\" ``` # Constraints and Performance Requirements - You can assume all inputs are valid and focus on implementing the core behavior as specified. - Your solution should handle typical usage efficiently without specific performance optimization constraints. Good luck, and happy coding!","solution":"import types def create_instance_method(func): Creates an instance method from a given function. Args: - func: A callable object (function) to be used as an instance method. Returns: An instance method object wrapping the given function. # Typically, an instance method is created automatically when a function is defined within a class. # To demonstrate it manually: class TempClass: pass setattr(TempClass, \'temp_method\', func) return TempClass().temp_method def is_instance_method(obj): Checks if the given object is an instance method. Args: - obj: An object to check. Returns: True if the object is an instance method, False otherwise. return isinstance(obj, types.MethodType) and obj.__self__ is not None def get_instance_method_function(im): Retrieves the original function from an instance method object. Args: - im: An instance method object. Returns: The original function linked with the instance method. return im.__func__ def create_bound_method(func, self): Creates a bound method from a given function and instance. Args: - func: A callable object (function) to be used as a method. - self: The instance to which the function will be bound. Returns: A bound method object wrapping the given function and instance. return types.MethodType(func, self) def is_bound_method(obj): Checks if the given object is a bound method. Args: - obj: An object to check. Returns: True if the object is a bound method, False otherwise. return isinstance(obj, types.MethodType) and obj.__self__ is not None def get_bound_method_function(meth): Retrieves the original function from a bound method object. Args: - meth: A bound method object. Returns: The original function linked with the bound method. return meth.__func__ def get_bound_method_instance(meth): Retrieves the instance associated with a bound method. Args: - meth: A bound method object. Returns: The instance associated with the bound method. return meth.__self__"},{"question":"# Objective Implement a custom `ContentHandler` to parse and extract specific information from an XML document using the `xml.sax` module in Python. # Problem Statement You are given an XML document containing information about a collection of books. Each book entry contains elements such as the title, author, publication year, and genre. Your task is to write a custom `ContentHandler` implementation that uses the `xml.sax` module to parse this XML document and extract the titles and authors of all books published after the year 2000. # Requirements 1. Implement a subclass of `xml.sax.handler.ContentHandler` named `BookContentHandler`. 2. The `BookContentHandler` should handle the following events: - `startElement(name, attrs)`: Capture the start of elements and their attributes. - `endElement(name)`: Capture the end of elements. - `characters(content)`: Collect character data inside elements. 3. Extract the titles and authors of books published after the year 2000 and store them in a dictionary with titles as keys and authors as values. 4. Handle any necessary initialization and cleanup in the appropriate methods (e.g., `startDocument()` and `endDocument()`). 5. Implement error handling for well-formedness and validation errors using a custom `ErrorHandler`. # Input Format - A string containing the XML document. # Output Format - A dictionary where the keys are book titles and the values are authors of books published after the year 2000. # Example Input ```xml <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <genre>Programming</genre> </book> <book> <title>Introduction to Algorithms</title> <author>Thomas H. Cormen</author> <year>2009</year> <genre>Computer Science</genre> </book> <book> <title>The C Programming Language</title> <author>Brian W. Kernighan</author> <year>1988</year> <genre>Programming</genre> </book> </library> ``` # Example Output ```python { \'Effective Python\': \'Brett Slatkin\', \'Introduction to Algorithms\': \'Thomas H. Cormen\' } ``` # Constraints - Assume the input XML document is well-formed. - You should handle edge cases, such as missing elements or tags, gracefully. - You must use the `xml.sax` module for XML parsing. # Additional Information For more details on SAX handlers, refer to the provided documentation. # Implementation ```python import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = {} def startDocument(self): self.books = {} def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def endElement(self, name): if name == \\"book\\": if int(self.year) > 2000: self.books[self.title] = self.author self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content elif self.current_element == \\"author\\": self.author += content elif self.current_element == \\"year\\": self.year += content class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_books(xml_data): handler = BookContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(CustomErrorHandler()) xml.sax.parseString(xml_data, handler) return handler.books # Example usage: xml_data = <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <genre>Programming</genre> </book> <book> <title>Introduction to Algorithms</title> <author>Thomas H. Cormen</author> <year>2009</year> <genre>Computer Science</genre> </book> <book> <title>The C Programming Language</title> <author>Brian W. Kernighan</author> <year>1988</year> <genre>Programming</genre> </book> </library> result = parse_books(xml_data) print(result) ``` This question tests the students\' ability to work with SAX handlers, extract and manipulate XML data, and handle XML parsing events programmatically.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = {} def startDocument(self): self.books = {} def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def endElement(self, name): if name == \\"book\\": if int(self.year) > 2000: self.books[self.title] = self.author self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content elif self.current_element == \\"author\\": self.author += content elif self.current_element == \\"year\\": self.year += content class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_books(xml_data): handler = BookContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(CustomErrorHandler()) xml.sax.parseString(xml_data, handler) return handler.books"},{"question":"**Objective:** Write a Python function named `summarize_function` that uses the `inspect` module to provide a summary of a given function, including its name, signature, source code, and any specified annotations (both parameter annotations and return annotations). **Function Signature:** ```python def summarize_function(func: callable) -> dict: pass ``` **Input:** - `func (callable)`: A Python function to be summarized. **Output:** - `summary (dict)`: A dictionary containing: - `name` (str): The name of the function. - `signature` (str): The signature of the function. - `source` (str): The source code of the function. - `annotations` (dict): A dictionary mapping parameter names to their annotations and including a special key \'return\' for the return annotation. **Constraints:** - You may assume that the input function is always a valid Python function. - Handle cases where the function may have no annotations gracefully. **Example:** Suppose we have the following function: ```python def example_function(a: int, b: str = \'default\') -> bool: return isinstance(a, int) and isinstance(b, str) ``` Calling the `summarize_function` should return: ```python summarize_function(example_function) ``` Produces: ```python { \'name\': \'example_function\', \'signature\': \'(a: int, b: str = \'default\') -> bool\', \'source\': \\"def example_function(a: int, b: str = \'default\') -> bool:n return isinstance(a, int) and isinstance(b, str)n\\", \'annotations\': {\'a\': int, \'b\': str, \'return\': bool} } ``` **Notes:** - Use the `inspect` module to retrieve the required information. - Consider edge cases such as functions without annotations or with complex signatures. - The source code string should maintain the original formatting. **Your task is to implement the `summarize_function` following the specified signature and constraints.**","solution":"import inspect def summarize_function(func: callable) -> dict: Summarizes the details of a given function including its name, signature, source code, and annotations. func_name = func.__name__ func_signature = str(inspect.signature(func)) func_source = inspect.getsource(func) func_annotations = func.__annotations__ # Ensure all parameters are included in annotations even if they do not have any. sig = inspect.signature(func) for param in sig.parameters.values(): if param.name not in func_annotations: func_annotations[param.name] = None # Ensure \'return\' annotation exists even if there is no return annotation. if \'return\' not in func_annotations: func_annotations[\'return\'] = None return { \'name\': func_name, \'signature\': func_signature, \'source\': func_source, \'annotations\': func_annotations }"},{"question":"**Advanced asyncio Debugging and Concurrency Challenge** # Objective Your task is to demonstrate a detailed understanding of the `asyncio` package by creating a program that handles several asynchronous tasks. You will use `asyncio` debug mode to catch common issues and ensure the program handles concurrency, task scheduling, and threading safely. # Problem Statement Implement a Python script that performs the following: 1. Initializes `asyncio` in debug mode. 2. Defines three asynchronous functions: - `fetch_data`: Simulates fetching data with a 1-second delay. - `process_data`: Takes the fetched data and processes it, simulating a 2-second delay. - `save_data`: Simulates saving the processed data with a 1-second delay. 3. Creates a main function to run the above functions sequentially, ensuring each coroutine is awaited properly. 4. Spawns 10 tasks concurrently to fetch, process, and save data using the defined functions. 5. Uses thread-safe methods to ensure coroutines are scheduled and executed properly when interacting with an external thread. 6. Logs any exceptions and outputs detailed runtime warnings if tasks are never awaited or exceptions are not handled properly. # Input and Output Format - **Input**: No specific input is required; the function calls are hard-coded. - **Output**: Print statements indicating the progress of each task (e.g., \\"Data fetched\\", \\"Data processed\\", \\"Data saved\\"). Additionally, enable debug mode to print runtime warnings/errors to the console. # Constraints 1. You must use `asyncio` debug mode to catch and log any common issues. 2. Ensure proper thread safety when interacting with tasks. 3. Handle exceptions raised within any of the coroutines. # Performance Requirements Your implementation should handle 10 concurrent tasks efficiently without blocking the main thread. The program should complete within a reasonable amount of time, given the simulated delays. # Example ```python import asyncio import logging # Enable debug-level logging logging.basicConfig(level=logging.DEBUG) async def fetch_data(i): await asyncio.sleep(1) print(f\\"Data {i} fetched\\") return f\\"data_{i}\\" async def process_data(data): await asyncio.sleep(2) print(f\\"{data} processed\\") return f\\"processed_{data}\\" async def save_data(data): await asyncio.sleep(1) print(f\\"{data} saved\\") async def handle_task(i): try: data = await fetch_data(i) processed_data = await process_data(data) await save_data(processed_data) except Exception as e: logging.error(f\\"Error in task {i}: {e}\\") async def main(): tasks = [] for i in range(10): task = asyncio.create_task(handle_task(i)) tasks.append(task) await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main(), debug=True) ``` This script initializes `asyncio` in debug mode, runs multiple tasks concurrently, and ensures proper handling of coroutines and exceptions. Use this example to write your script accordingly.","solution":"import asyncio import logging # Enable debug-level logging for asyncio logging.basicConfig(level=logging.DEBUG) async def fetch_data(i): await asyncio.sleep(1) print(f\\"Data {i} fetched\\") return f\\"data_{i}\\" async def process_data(data): await asyncio.sleep(2) print(f\\"{data} processed\\") return f\\"processed_{data}\\" async def save_data(data): await asyncio.sleep(1) print(f\\"{data} saved\\") async def handle_task(i): try: data = await fetch_data(i) processed_data = await process_data(data) await save_data(processed_data) except Exception as e: logging.error(f\\"Error in task {i}: {e}\\") async def main(): tasks = [] for i in range(10): task = asyncio.create_task(handle_task(i)) tasks.append(task) await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main(), debug=True)"},{"question":"You are tasked with creating a function that converts a string containing HTML5 named character references to a string of Unicode characters, using the `html.entities` module. Your goal is to correctly replace all named character references in the string with their corresponding Unicode characters. # Function Signature ```python def html_to_unicode(input_str: str) -> str: ``` # Input - `input_str` (str): A string that may contain HTML5 named character references, including but not limited to `&gt;`, `&lt;`, `&amp;`, etc. # Output - Returns a string where all HTML5 named character references have been replaced by their equivalent Unicode characters. # Constraints - The input string will only contain valid HTML5 named character references as defined in the `html.entities.html5` dictionary. - The function should handle both named character references that include the trailing semicolon (e.g., `&gt;`) and those that do not (e.g., `&gt`). # Example ```python input_str = \\"This is an example: 10 &gt; 5, 5 &lt; 10, &quot;Python&quot; &amp; &apos;Coding&apos;.\\" output_str = \\"This is an example: 10 > 5, 5 < 10, \\"Python\\" & \'Coding\'.\\" assert html_to_unicode(input_str) == output_str ``` # Notes 1. You can use the `html.entities.html5` dictionary to retrieve the Unicode characters corresponding to HTML5 named character references. 2. You may find it useful to process the string using regular expressions to locate and replace named character references. Good luck, and happy coding!","solution":"import re import html def html_to_unicode(input_str: str) -> str: Converts a string containing HTML5 named character references to a string of Unicode characters. return html.unescape(input_str)"},{"question":"FSDP Buffer Requirement Calculation Objective: Implement a function to calculate the buffer requirements for Fully Sharded Data Parallel (FSDP) in PyTorch based on a given model setup and hardware configuration. Problem Statement: You are given a transformer-based model with a specified number of transformer blocks, each having a certain number of parameters in `fp32` (4 bytes each). The model is sharded over several GPUs. Your task is to calculate the buffer requirements for both the forward and backward passes along with additional buffers required for the unsharded parameters. Function Signature: ```python def calculate_fsdp_buffer_requirements(num_transformer_blocks: int, params_per_block: int, dtype_bytes: int, num_gpus: int) -> dict: Calculate the buffer size requirements for FSDP in PyTorch. Parameters: num_transformer_blocks (int): The number of transformer blocks in the model. params_per_block (int): The number of parameters in each transformer block. dtype_bytes (int): The number of bytes per parameter (4 for fp32). num_gpus (int): The number of GPUs over which the model is sharded. Returns: dict: A dictionary containing the buffer requirements for \'forward\', \'backward\', and \'additional\'. # Your code here ``` Input: - `num_transformer_blocks` (int): The number of transformer blocks in the model. (1 <= num_transformer_blocks <= 1000) - `params_per_block` (int): The number of parameters in each transformer block. (10^6 <= params_per_block <= 10^9) - `dtype_bytes` (int): The number of bytes per parameter (use 4 for fp32). - `num_gpus` (int): The number of GPUs over which the model is sharded. (1 <= num_gpus <= 64) Output: - A dictionary containing the buffer requirements. The dictionary should have three keys: - `\'forward\'`: The required buffer size for the forward pass. - `\'backward\'`: The required buffer size for the backward pass. - `\'additional\'`: The additional buffer size required for unsharded parameters. Constraints: 1. The function should ensure that all input parameters are positive integers. 2. The calculations should be based on the documentation provided above. Example: ```python # Example input num_transformer_blocks = 10 params_per_block = 1600000000 dtype_bytes = 4 num_gpus = 8 # Expected output { \'forward\': 1.6 * num_transformer_blocks, \'backward\': 3.2 * num_transformer_blocks, \'additional\': 12.8 } ``` Note: - The buffer sizes in the output are in GB (Gigabytes). - Ensure that your function correctly computes even for edge cases and larger inputs within the provided constraints. Good luck!","solution":"def calculate_fsdp_buffer_requirements(num_transformer_blocks: int, params_per_block: int, dtype_bytes: int, num_gpus: int) -> dict: Calculate the buffer size requirements for FSDP in PyTorch. Parameters: num_transformer_blocks (int): The number of transformer blocks in the model. params_per_block (int): The number of parameters in each transformer block. dtype_bytes (int): The number of bytes per parameter (4 for fp32). num_gpus (int): The number of GPUs over which the model is sharded. Returns: dict: A dictionary containing the buffer requirements for \'forward\', \'backward\', and \'additional\'. # Total parameters in the model total_params = num_transformer_blocks * params_per_block # Parameters per GPU params_per_gpu = total_params / num_gpus # Buffer calculations forward_buffer = params_per_gpu * dtype_bytes * 2 / (1024**3) # Converting bytes to GB backward_buffer = forward_buffer * 2 # Additional buffer for unsharded parameters additional_buffer = total_params * dtype_bytes / (1024**3) # Unsharded parameters - Converting bytes to GB return { \'forward\': forward_buffer, \'backward\': backward_buffer, \'additional\': additional_buffer }"},{"question":"# Regression Model Comparison using Scikit-Learn Objective Implement and compare different linear regression models using `scikit-learn`. This exercise will test your understanding of different linear models and the ability to apply, tune, and evaluate them. Dataset You are provided a dataset `data.csv` with features and a target variable. The dataset contains: - Multiple explanatory variables/features (`X`). - A single continuous target variable (`y`). Task 1. **Load and Preprocess the Data**: - Load the dataset from `data.csv`. - Split the data into training (70%) and testing (30%) sets. 2. **Implement Regression Models**: - Implement the following regression models using `scikit-learn`: 1. Ordinary Least Squares (OLS) Regression 2. Ridge Regression 3. Lasso Regression 4. Elastic-Net Regression 3. **Hyperparameter Tuning**: - Use cross-validation to find the optimal hyperparameters for each model: - Ridge Regression: Tune parameter `alpha` using a grid search. - Lasso Regression: Tune parameter `alpha` using a grid search. - Elastic-Net: Tune parameters `alpha` and `l1_ratio` using a grid search. 4. **Model Evaluation**: - Evaluate and compare the performance of the models on the testing set using: - Mean Squared Error (MSE) - R-squared Score 5. **Output**: - Display the optimal hyperparameters for each model. - Display the MSE and R-squared Score for each model. - Plot the actual vs predicted values for the best-performing model. Function Signature ```python def compare_regression_models(file_path: str): Compare different regression models on the given dataset. Parameters: file_path (str): The file path to the dataset. Returns: None # Your code here ``` Example ```python # Function call example compare_regression_models(\'data.csv\') ``` Constraints - You must use `scikit-learn` for implementing and evaluating the models. - Performance tuning and evaluation must be done using cross-validation techniques. - Ensure that the data split and the random seed (if any) are consistent for reproducibility.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt def compare_regression_models(file_path: str): # Load the dataset data = pd.read_csv(file_path) X = data.drop(columns=\'target\') y = data[\'target\'] # Split the data into training (70%) and testing (30%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Parameter grids for hyperparameter tuning ridge_params = {\'alpha\': [0.1, 1, 10, 100]} lasso_params = {\'alpha\': [0.1, 1, 10, 100]} elastic_net_params = {\'alpha\': [0.1, 1, 10], \'l1_ratio\': [0.1, 0.5, 0.9]} # Models models = { \'OLS\': LinearRegression(), \'Ridge\': GridSearchCV(Ridge(), ridge_params, cv=5, scoring=\'neg_mean_squared_error\'), \'Lasso\': GridSearchCV(Lasso(), lasso_params, cv=5, scoring=\'neg_mean_squared_error\'), \'ElasticNet\': GridSearchCV(ElasticNet(), elastic_net_params, cv=5, scoring=\'neg_mean_squared_error\') } results = {} # Fit models and gather results for name, model in models.items(): if \'GridSearchCV\' in str(type(model)): model.fit(X_train, y_train) best_model = model.best_estimator_ best_params = model.best_params_ else: best_model = model best_model.fit(X_train, y_train) best_params = None predictions = best_model.predict(X_test) mse = mean_squared_error(y_test, predictions) r2 = r2_score(y_test, predictions) results[name] = { \'model\': best_model, \'mse\': mse, \'r2\': r2, \'params\': best_params } # Find the best model based on R-squared score best_model_name = max(results, key=lambda k: results[k][\'r2\']) # Print optimal hyperparameters and evaluation metrics for name, result in results.items(): print(f\\"Model: {name}\\") if result[\'params\']: print(f\\"Best Parameters: {result[\'params\']}\\") print(f\\"MSE: {result[\'mse\']}\\") print(f\\"R-squared Score: {result[\'r2\']}\\") print(\\"---\\") # Plot actual vs predicted values for the best-performing model best_model = results[best_model_name][\'model\'] best_predictions = best_model.predict(X_test) plt.figure(figsize=(10, 6)) plt.scatter(y_test, best_predictions) plt.xlabel(\\"Actual Values\\") plt.ylabel(\\"Predicted Values\\") plt.title(f\\"Actual vs Predicted Values for {best_model_name}\\") plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\'red\') plt.show()"},{"question":"Objective: Implement a Python script that demonstrates understanding and usage of the `warnings` module in Python. The task is to create a custom warning category, issue warnings of various types, and control their display and handling using filters. Problem Statement: You are required to implement a class `CustomWarningHandler` that provides methods to: 1. **Add Custom Warning Category** 2. **Issue Warnings** 3. **Configure Warning Filters** 4. **Use Context Manager to Suppress Warnings Temporarily** Implement the class with the following specifications: Specifications: 1. **Custom Warning Category** - Create a custom warning category called `MyCustomWarning` which is a subclass of `Warning`. 2. **Methods** - `add_custom_warning`: This method should accept a message as an argument and issue a `MyCustomWarning` with the provided message. - `issue_deprecation_warning`: This method should issue a `DeprecationWarning`. - `configure_warning_filters`: This method should configure the warning filters to: - Ignore `MyCustomWarning`. - Convert `DeprecationWarning` to an exception. - `suppress_warnings_temporarily`: This method should accept a function as an argument and execute it with all warnings suppressed using the `catch_warnings` context manager. 3. **Input and Output Formats** - The methods should handle issuing warnings internally and showcasing the behavior change based on applied filters. No input/output interactions with the user are required. 4. **Class Definition Example:** ```python class CustomWarningHandler: def __init__(self): # Initialize the handler def add_custom_warning(self, message: str): # Issue `MyCustomWarning` with the provided message def issue_deprecation_warning(self): # Issue a `DeprecationWarning` def configure_warning_filters(self): # Configure filters to ignore `MyCustomWarning` and convert `DeprecationWarning` to exceptions def suppress_warnings_temporarily(self, func): # Execute the provided function with all warnings suppressed ``` 5. **Constraints** - Ensure the functionality works as expected for single-threaded applications. - Validate the implementation by testing the methods and observing the warning behaviors. Example Usage: ```python def deprecated_function(): warnings.warn(\\"This function is deprecated\\", DeprecationWarning) def function_with_custom_warning(): warnings.warn(\\"This is a custom warning\\", MyCustomWarning) handler = CustomWarningHandler() handler.add_custom_warning(\\"This is a test warning\\") handler.issue_deprecation_warning() handler.configure_warning_filters() try: deprecated_function() except DeprecationWarning as e: print(\\"Caught a DeprecationWarning:\\", e) handler.suppress_warnings_temporarily(function_with_custom_warning) ``` Submission Guidelines: - Provide the complete implementation of the `CustomWarningHandler` class. - Add example usage that demonstrates the class functionality and outputs the expected behavior. - Ensure the code is properly commented and follows Python coding conventions.","solution":"import warnings class MyCustomWarning(Warning): Custom warning category `MyCustomWarning`. pass class CustomWarningHandler: def __init__(self): Initialize the CustomWarningHandler. pass def add_custom_warning(self, message: str): Issue `MyCustomWarning` with the provided message. warnings.warn(message, MyCustomWarning) def issue_deprecation_warning(self): Issue a `DeprecationWarning`. warnings.warn(\\"This is a deprecation warning\\", DeprecationWarning) def configure_warning_filters(self): Configure filters to ignore `MyCustomWarning` and convert `DeprecationWarning` to exceptions. warnings.simplefilter(\\"ignore\\", MyCustomWarning) warnings.simplefilter(\\"error\\", DeprecationWarning) def suppress_warnings_temporarily(self, func): Execute the provided function with all warnings suppressed. with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") func() # Example usage def deprecated_function(): warnings.warn(\\"This function is deprecated\\", DeprecationWarning) def function_with_custom_warning(): warnings.warn(\\"This is a custom warning\\", MyCustomWarning) handler = CustomWarningHandler() handler.add_custom_warning(\\"This is a test warning\\") handler.issue_deprecation_warning() handler.configure_warning_filters() try: deprecated_function() except DeprecationWarning as e: print(\\"Caught a DeprecationWarning:\\", e) handler.suppress_warnings_temporarily(function_with_custom_warning)"},{"question":"**Coding Assessment Question:** You are required to implement a function that takes an input file, uuencodes it, then decodes the encoded file, and verifies that the original content and the decoded content are identical. During this process, you need to handle any potential errors gracefully. # Function Signature: ```python def encode_and_verify(input_filepath: str, temp_encoded_filepath: str, output_filepath: str) -> bool: pass ``` # Description: 1. The function takes three parameters: - `input_filepath` (str): The path to the original input file which needs to be encoded. - `temp_encoded_filepath` (str): The path where the uuencoded file will be temporarily saved. - `output_filepath` (str): The path where the decoded file will be saved. 2. The function will: - Encode the `input_filepath` to `temp_encoded_filepath` using `uu.encode`. - Decode `temp_encoded_filepath` to `output_filepath` using `uu.decode`. - Compare the content of the original `input_filepath` and `output_filepath`. 3. The function should return `True` if the contents of the `input_filepath` and `output_filepath` are identical, indicating successful encoding and decoding. Otherwise, it should return `False`. 4. The function should handle any errors that occur during encoding or decoding using appropriate exception handling and should return `False` in case of an error. # Constraints: * `input_filepath` will be a valid path to a readable file. * `temp_encoded_filepath` and `output_filepath` should be writable locations. * Ensure all file operations open files in the correct mode (`\'rb\'`, `\'wb\'`, etc.). # Example: ```python # Assuming \'sample.txt\' contains \\"Hello, this is a sample text file.\\" result = encode_and_verify(\'sample.txt\', \'temp_encoded.txt\', \'decoded_output.txt\') print(result) # Should print: True ``` In the above example, the function should encode the contents of \'sample.txt\', save it to \'temp_encoded.txt\', decode it back to \'decoded_output.txt\', and then verify that \'sample.txt\' and \'decoded_output.txt\' have identical contents. **Note:** This exercise will test your understanding of file operations, encoding/decoding processes, and error handling in Python.","solution":"import uu import os def encode_and_verify(input_filepath: str, temp_encoded_filepath: str, output_filepath: str) -> bool: try: # Encode the original file with open(input_filepath, \'rb\') as infile, open(temp_encoded_filepath, \'wb\') as outfile: uu.encode(infile, outfile) # Decode the encoded file back to another file with open(temp_encoded_filepath, \'rb\') as infile, open(output_filepath, \'wb\') as outfile: uu.decode(infile, outfile) # Verify that both original and final decoded files have the same content with open(input_filepath, \'rb\') as original_file, open(output_filepath, \'rb\') as decoded_file: original_content = original_file.read() decoded_content = decoded_file.read() return original_content == decoded_content except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Advanced Mapping Protocols in Python Implement a class `AdvancedMapping` that mirrors the functionality of a dictionary using fundamental and some advanced features. Your class should provide the following methods: 1. **`__init__(self)`**: Initialize your custom mapping object. 2. **`check(self, obj)`**: Return `True` if the `obj` supports the mapping protocol by having a `__getitem__` method. 3. **`size(self)`**: Return the number of key-value pairs in the mapping. 4. **`getitem(self, key)`**: Return the value associated with the key. 5. **`setitem(self, key, value)`**: Set the value for a given key. 6. **`delitem(self, key)`**: Delete the key-value pair by key. 7. **`haskey(self, key)`**: Return `True` if the key exists in the mapping, `False` otherwise. 8. **`keys(self)`**: Return a list of keys in the mapping. 9. **`values(self)`**: Return a list of values in the mapping. 10. **`items(self)`**: Return a list of tuples containing key-value pairs in the mapping. Example Usage: ```python # Create an instance of the AdvancedMapping class am = AdvancedMapping() # Adding items am.setitem(\\"one\\", 1) am.setitem(\\"two\\", 2) # Retrieving items print(am.getitem(\\"one\\")) # Output: 1 # Checking size print(am.size()) # Output: 2 # Checking key existence print(am.haskey(\\"two\\")) # Output: True print(am.haskey(\\"three\\")) # Output: False # Deleting an item am.delitem(\\"two\\") # All Keys print(am.keys()) # Output: [\\"one\\"] # All Values print(am.values()) # Output: [1] # All Items print(am.items()) # Output: [(\\"one\\", 1)] ``` Constraints: - Do not use built-in dictionary methods (`__getitem__`, `__setitem__`, etc.) directly. - The methods should handle key errors gracefully. - Performance should be optimal for standard operations (`O(1)` for accessing elements if possible). **Notes:** - The constructor should initialize an empty mapping. - Ensure that the class methods function correctly and raise appropriate errors when required.","solution":"class AdvancedMapping: def __init__(self): self._mapping = [] def check(self, obj): return hasattr(obj, \'__getitem__\') def size(self): return len(self._mapping) def getitem(self, key): for k, v in self._mapping: if k == key: return v raise KeyError(f\'Key {key} not found\') def setitem(self, key, value): for i, (k, v) in enumerate(self._mapping): if k == key: self._mapping[i] = (key, value) return self._mapping.append((key, value)) def delitem(self, key): for i, (k, v) in enumerate(self._mapping): if k == key: del self._mapping[i] return raise KeyError(f\'Key {key} not found\') def haskey(self, key): return any(k == key for k, v in self._mapping) def keys(self): return [k for k, v in self._mapping] def values(self): return [v for k, v in self._mapping] def items(self): return self._mapping[:]"},{"question":"# Task: Implement an Asynchronous Task Scheduler You are required to implement a custom task scheduler using `asyncio.Queue`. The scheduler should manage a set of tasks, allowing them to be executed by workers in an asynchronous fashion. Each task will have a specific duration that it takes to complete. Requirements 1. **Task Class**: - Create a `Task` class that has two attributes: `name` (string) and `duration` (float). The `duration` represents the time it takes to complete the task. 2. **Scheduler Class**: - Create a `Scheduler` class that initializes an `asyncio.Queue` to manage tasks. - The `Scheduler` class should have the following methods: - `add_task(task: Task)`: Adds a `task` to the queue asynchronously. - `run(num_workers: int)`: Runs the scheduler with the specified number of worker coroutines processing the tasks in the queue. 3. **Worker Coroutine**: - Implement a worker coroutine that will: - Continuously get tasks from the queue. - Simulate task processing by `await asyncio.sleep(task.duration)`. - Mark the task as done using the `task_done` method on the queue. - Print a message indicating the task completion (e.g., `Worker-1 completed Task-1 in 2.5 seconds`). 4. **Scheduler Execution**: - Ensure that the `Scheduler.run()` method starts the specified number of workers and waits for all tasks to be processed. - Properly handle graceful cancellation of workers once all tasks are processed. # Input Format - `Task`: Each task is represented by its `name` and `duration`. - `Scheduler.run(num_workers: int)`: Number of worker coroutines to run concurrently. # Output Format - Print statements indicating task completion by workers. # Example Below is an example showing how the `Scheduler` can be used: ```python import asyncio class Task: def __init__(self, name, duration): self.name = name self.duration = duration class Scheduler: def __init__(self): self.queue = asyncio.Queue() async def add_task(self, task): await self.queue.put(task) async def worker(self, name): while True: task = await self.queue.get() await asyncio.sleep(task.duration) print(f\'{name} completed {task.name} in {task.duration:.2f} seconds\') self.queue.task_done() async def run(self, num_workers): workers = [asyncio.create_task(self.worker(f\'Worker-{i+1}\')) for i in range(num_workers)] await self.queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) # Main event loop for demonstrating the Scheduler async def main(): scheduler = Scheduler() # Adding tasks to the scheduler tasks = [ Task(\'Task-1\', 2.5), Task(\'Task-2\', 1.0), Task(\'Task-3\', 3.0), ] for task in tasks: await scheduler.add_task(task) # Running the scheduler with 2 workers await scheduler.run(num_workers=2) # Run the main event loop asyncio.run(main()) ``` # Constraints - You must use `asyncio.Queue` for managing tasks. - The task duration must be a positive float. - The scheduler should handle at least 3 worker coroutines concurrently.","solution":"import asyncio class Task: def __init__(self, name, duration): self.name = name self.duration = duration class Scheduler: def __init__(self): self.queue = asyncio.Queue() async def add_task(self, task): await self.queue.put(task) async def worker(self, name): while True: task = await self.queue.get() await asyncio.sleep(task.duration) print(f\'{name} completed {task.name} in {task.duration:.2f} seconds\') self.queue.task_done() async def run(self, num_workers): workers = [asyncio.create_task(self.worker(f\'Worker-{i+1}\')) for i in range(num_workers)] await self.queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) # Main event loop for demonstrating the Scheduler async def main(): scheduler = Scheduler() # Adding tasks to the scheduler tasks = [ Task(\'Task-1\', 2.5), Task(\'Task-2\', 1.0), Task(\'Task-3\', 3.0), ] for task in tasks: await scheduler.add_task(task) # Running the scheduler with 2 workers await scheduler.run(num_workers=2) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are provided with a dataset and required to perform classification using Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA). Additionally, you will implement dimensionality reduction using LDA and compare different covariance estimators to observe their effect on classification. Tasks: 1. **Data Preparation**: - Load the Iris dataset from scikit-learn. 2. **Model Implementation and Training**: - Implement and train both an LDA and QDA model on the dataset. 3. **Dimensionality Reduction**: - Reduce the dimensionality of the dataset to 2 components using LDA. Plot the transformed data points, coloring them by their class labels. 4. **Compare Covariance Estimators**: - Train LDA models using three different covariance estimators: \'empirical\', \'shrinkage=\\"auto\\"\', and \'oas\'. Evaluate and compare the classification accuracy for each method using cross-validation. 5. **Performance Comparison**: - Summarize and interpret the results. What do you observe about the effect of different covariance estimators on the LDA model\'s performance? Expected input and output formats: **Input**: - None **Output**: - A series of print statements summarizing the results, as well as plots. The print statements should include: - Classification report for LDA and QDA. - Classification accuracy for LDA with different covariance estimators. - Plots showing the LDA-based dimensionality reduction. Constraints: - Use the default parameters for LDA and QDA unless specified otherwise. - You can use any relevant plotting library for visualizations. - Performance requirements: Ensure the code runs efficiently on a standard machine with the computational complexity appropriate for a dataset the size of Iris (150 samples). ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.covariance import OAS from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Task 1: Implement and train LDA and QDA lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) print(\\"LDA Classification Report:n\\", classification_report(y, y_pred_lda)) print(\\"QDA Classification Report:n\\", classification_report(y, y_pred_qda)) # Task 2: Dimensionality Reduction using LDA lda_components = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_components.fit(X, y).transform(X) # Plotting the reduced dimensionality data plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = iris.target_names for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of Iris dataset\') plt.show() # Task 3: Compare Covariance Estimators in LDA cov_estimators = [\'empirical\', \'auto\', OAS()] scores = {} for estimator in cov_estimators: if estimator == \'empirical\': lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\') elif estimator == \'auto\': lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') else: lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) score = cross_val_score(lda_empirical, X, y, cv=5).mean() scores[str(estimator)] = score print(\\"LDA Scores with Different Covariance Estimators:n\\", scores) # Summarize results for estimator, score in scores.items(): print(f\\"Covariance Estimator: {estimator}, Classification Accuracy: {score:.4f}\\") ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.covariance import OAS from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Task 1: Implement and train LDA and QDA lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) print(\\"LDA Classification Report:n\\", classification_report(y, y_pred_lda)) print(\\"QDA Classification Report:n\\", classification_report(y, y_pred_qda)) # Task 2: Dimensionality Reduction using LDA lda_components = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_components.fit(X, y).transform(X) # Plotting the reduced dimensionality data plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = iris.target_names for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of Iris dataset\') plt.show() # Task 3: Compare Covariance Estimators in LDA cov_estimators = [\'empirical\', \'auto\', OAS()] scores = {} for estimator in cov_estimators: if estimator == \'empirical\': lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\') elif estimator == \'auto\': lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') else: lda_empirical = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) score = cross_val_score(lda_empirical, X, y, cv=5).mean() scores[str(estimator)] = score print(\\"LDA Scores with Different Covariance Estimators:n\\", scores) # Summarize results for estimator, score in scores.items(): print(f\\"Covariance Estimator: {estimator}, Classification Accuracy: {score:.4f}\\")"},{"question":"# PyTorch Functional Transforms Assessment **Objective:** The goal of this assessment is to implement and understand the usage of `torch.func` APIs for functional transformations, particularly focusing on `torch.func.grad`, `torch.func.vmap`, and `torch.func.functional_call`. You need to construct a neural network model, perform specific gradient calculations, and evaluate the model over an ensemble. **Task Description:** 1. **Model Definition**: - Define a simple neural network model `SimpleNN` with one hidden layer using `torch.nn.Module`. 2. **Compute Gradient**: - Write a function `compute_model_gradients` that takes the model parameters, inputs, and targets as inputs and returns the gradients of the model\'s loss with respect to its parameters using `torch.func.grad`. 3. **Ensemble Evaluation**: - Write a function `ensemble_evaluation` that creates an ensemble of `SimpleNN` models, combines their parameters using `torch.func.stack_module_state`, and evaluates the ensemble on given data using `torch.vmap` and `torch.func.functional_call`. # Function Specifications 1. `SimpleNN` class: - **Attributes**: - `input_size` (int): number of input features. - `hidden_size` (int): number of neurons in the hidden layer. - `output_size` (int): number of output features. - **Methods**: - `__init__(self, input_size, hidden_size, output_size)`: initializes the neural network layers. - `forward(self, x)`: defines the forward pass of the neural network. 2. `compute_model_gradients` function: - **Inputs**: - `params`: dictionary of model parameters. - `inputs`: tensor of input data. - `targets`: tensor of target data. - **Returns**: - Gradients of the loss with respect to the model parameters. 3. `ensemble_evaluation` function: - **Inputs**: - `num_models` (int): number of models in the ensemble. - `input_size` (int): number of input features. - `hidden_size` (int): number of neurons in the hidden layer. - `output_size` (int): number of output features. - `inputs`: tensor of input data. - **Returns**: - Output tensor from the ensemble of models. # Example: ```python import torch from torch.func import grad, functional_call, stack_module_state, vmap class SimpleNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.fc2 = torch.nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def compute_model_gradients(params, inputs, targets): model = SimpleNN(3, 10, 3) def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) return grad(compute_loss)(params, inputs, targets) def ensemble_evaluation(num_models, input_size, hidden_size, output_size, inputs): models = [SimpleNN(input_size, hidden_size, output_size) for _ in range(num_models)] base_model = models[0] base_model.to(\'meta\') params, buffers = stack_module_state(models) def call_single_model(params, buffers, data): return functional_call(base_model, (params, buffers), (data,)) return vmap(call_single_model, (0, 0, None))(params, buffers, inputs) # Example usage: inputs = torch.randn(64, 3) targets = torch.randn(64, 3) model = SimpleNN(3, 10, 3) params = dict(model.named_parameters()) grads = compute_model_gradients(params, inputs, targets) ensemble_outputs = ensemble_evaluation(5, 3, 10, 3, inputs) print(ensemble_outputs.shape) # Expected output: (5, 64, 3) ``` Ensure your implementations are efficient and follow the best practices described in the documentation.","solution":"import torch from torch.func import grad, functional_call, stack_module_state, vmap class SimpleNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.fc2 = torch.nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def compute_model_gradients(params, inputs, targets): model = SimpleNN(inputs.shape[-1], params[\'fc1.bias\'].size(0), targets.shape[-1]) def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) return grad(compute_loss)(params, inputs, targets) def ensemble_evaluation(num_models, input_size, hidden_size, output_size, inputs): models = [SimpleNN(input_size, hidden_size, output_size) for _ in range(num_models)] base_model = models[0] params, buffers = stack_module_state(models) def call_single_model(params, buffers, data): return functional_call(base_model, (params, buffers), (data,)) return vmap(call_single_model, (0, 0, None))(params, buffers, inputs)"},{"question":"Data Validation and Random Sampling **Objective:** Create a function that takes arrays of data, validates the input using Scikit-learn utilities, and performs random sampling on the validated data. **Task:** Implement the function `random_sample_validate(data, labels, n_samples, random_state)`. This function should perform the following steps: 1. Validate the input arrays `data` and `labels` to ensure they meet the requirements using Scikit-learn validation tools. 2. Ensure that `data` and `labels` have consistent lengths. 3. Convert `data` to a floating-point array. 4. Check that the parameter `random_state` can be converted into a proper random number generator. 5. Randomly sample `n_samples` entries from `data` and `labels` without replacement. 6. Return the randomly sampled entries as two separate arrays. **Function Signature:** ```python def random_sample_validate(data, labels, n_samples, random_state=None): Validate input arrays and perform random sampling. Parameters: - data (array-like): Array of input data, expected to be a 2-dimensional array. - labels (array-like): Array of labels, expected to be a 1-dimensional array. - n_samples (int): Number of samples to draw. - random_state (int, RandomState instance or None, optional): Seed or random number generator to use. Returns: - sampled_data (array): The sampled data array. - sampled_labels (array): The sampled labels array. ``` **Input Constraints:** - `data`: A 2D array-like structure. - `labels`: A 1D array-like structure. - `n_samples`: An integer greater than 0 and less than or equal to the length of `data`. - `random_state`: Can be an integer, a `RandomState` instance, or `None`. **Performance Requirements:** The function should efficiently perform validation and sampling without unnecessary computations or memory usage. **Example Usage:** ```python from sklearn.utils import shuffle import numpy as np data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) labels = np.array([1, 2, 3, 4]) n_samples = 2 random_state = 42 sampled_data, sampled_labels = random_sample_validate(data, labels, n_samples, random_state) print(sampled_data) # Example output: array([[7, 8], [3, 4]]) print(sampled_labels) # Example output: array([4, 2]) ``` **Hints:** - Use `check_array` from `sklearn.utils` to validate the input data array. - Use `check_X_y` from `sklearn.utils` to validate the relationship between `data` and `labels`. - Use `as_float_array` from `sklearn.utils` to ensure `data` is a floating-point array. - Use `check_random_state` from `sklearn.utils` to handle the `random_state` parameter. - Use `sample_without_replacement` from `sklearn.utils.random` to perform the random sampling. This question requires students to understand and utilize several Scikit-learn utilities to perform a comprehensive task involving data validation and random sampling.","solution":"import numpy as np from sklearn.utils import check_X_y, check_array, check_random_state from sklearn.utils.random import sample_without_replacement def random_sample_validate(data, labels, n_samples, random_state=None): Validate input arrays and perform random sampling. Parameters: - data (array-like): Array of input data, expected to be a 2-dimensional array. - labels (array-like): Array of labels, expected to be a 1-dimensional array. - n_samples (int): Number of samples to draw. - random_state (int, RandomState instance or None, optional): Seed or random number generator to use. Returns: - sampled_data (array): The sampled data array. - sampled_labels (array): The sampled labels array. # Validate and convert data and labels to numpy arrays data, labels = check_X_y(data, labels) # Ensure n_samples is valid if n_samples <= 0 or n_samples > data.shape[0]: raise ValueError(\\"n_samples must be greater than 0 and less than or equal to the number of data points\\") # Ensure data is a floating-point array data = check_array(data, ensure_2d=True, dtype=np.float64) # Check and set the random state random_state = check_random_state(random_state) # Perform random sampling without replacement indices = sample_without_replacement(n_population=data.shape[0], n_samples=n_samples, random_state=random_state) sampled_data = data[indices] sampled_labels = labels[indices] return sampled_data, sampled_labels"},{"question":"Objective Implement a function `analyze_imported_modules` which demonstrates the ability to handle importing mechanisms in Python, including dynamically importing modules, checking their metadata, and extracting relevant information from ZIP archives. Function Signature ```python def analyze_imported_modules(zip_path: str, module_names: list) -> dict: pass ``` Input - `zip_path`: A string representing the path to a ZIP archive that contains the Python modules. - `module_names`: A list of strings, where each string represents the name of a module to be imported from the given ZIP archive. Output - Returns a dictionary where the keys are module names (from `module_names`) and the values are dictionaries containing metadata about each module. The metadata dictionary should include: - `exists`: A boolean indicating if the module can be imported. - `version`: A string with the version of the module if available (using `importlib.metadata`). - `file_location`: A string indicating the path of the source file of the module if the module exists. Constraints - If a module cannot be imported, the corresponding entry in the dictionary should set `exists` to `False` and skip the `version` and `file_location` keys. Example Consider a ZIP archive `modules.zip` that contains the modules `mymodule` and `yourmodule`. Suppose the following code structure within the ZIP: ``` modules.zip/ | |-- mymodule.py |-- yourmodule.py |-- other.py ``` Example function call: ```python zip_path = \\"modules.zip\\" module_names = [\\"mymodule\\", \\"yourmodule\\", \\"nonexistent\\"] result = analyze_imported_modules(zip_path, module_names) print(result) ``` Example Output: ```python { \\"mymodule\\": { \\"exists\\": True, \\"version\\": \\"1.0.0\\", \\"file_location\\": \\"/path/to/mymodule.py\\" }, \\"yourmodule\\": { \\"exists\\": True, \\"version\\": \\"2.1.5\\", \\"file_location\\": \\"/path/to/yourmodule.py\\" }, \\"nonexistent\\": { \\"exists\\": False } } ``` Additional Notes - You are required to handle exceptions elegantly, ensuring that the function does not crash if a module cannot be imported or metadata cannot be found. - Use appropriate modules from `importlib` and `importlib.metadata` to accomplish this task. - Make sure to provide meaningful and readable code, including comments for clarity. Performance The function should efficiently handle ZIP archives containing up to 100 modules and should not assume the presence of any modules outside the provided ZIP archive.","solution":"import sys import zipfile import importlib import importlib.metadata from types import ModuleType from typing import List, Dict def analyze_imported_modules(zip_path: str, module_names: List[str]) -> Dict[str, Dict]: Analyzes a list of Python modules contained inside a ZIP archive. Parameters: zip_path (str): The path to the ZIP archive containing the modules. module_names (List[str]): A list of module names to be analyzed. Returns: Dict[str, Dict]: A dictionary containing the metadata of the specified modules. results = {} # Temporarily add the zip file path to the sys.path sys.path.insert(0, zip_path) try: for module_name in module_names: module_info = { \\"exists\\": False } try: # Dynamically import the module module = importlib.import_module(module_name) # Module has been successfully imported module_info[\\"exists\\"] = True module_info[\\"file_location\\"] = module.__file__ # Retrieve the version using importlib.metadata try: module_info[\\"version\\"] = importlib.metadata.version(module_name) except importlib.metadata.PackageNotFoundError: module_info[\\"version\\"] = \\"Not available\\" except ModuleNotFoundError: # If the module is not found, it will stay with default \'exists\': False pass # Add the module information to the results results[module_name] = module_info finally: # Ensure the zip file path is removed from sys.path once done sys.path.pop(0) return results"},{"question":"You are tasked with implementing a distributed training loop using PyTorch\'s FSDP2 (Fully Sharded Data Parallelism) feature. This will help assess your understanding of distributed training and parameter sharding provided by the FSDP2 API. # Requirements - Implement a simple neural network. - Utilize the `fully_shard` function to shard the parameters of the model across multiple devices. - Setup a distributed training loop that trains the model over several epochs. - Ensure that gradients are correctly synchronized and model parameters are sharded appropriately. # Input - `num_epochs` (int): The number of epochs to train the model. - `learning_rate` (float): The learning rate for the optimizer. - `world_size` (int): The number of devices to shard across. # Constraints - Use only PyTorch and its `torch.distributed` package. - The model should be trained on a synthetic dataset. - Ensure that the model and data are appropriately handled for distributed training. # Performance - The implementation should be efficient in terms of memory usage by leveraging the sharding capabilities of FSDP2. - Your solution should be capable of scaling up to a large number of devices without significant modifications. # Expected Output - A trained model with parameters appropriately sharded and synchronized across devices. - Training logs that indicate the progress over epochs. # Example Here is an outline of what your solution might look like: ```python import torch import torch.distributed as dist from torch.distributed.fsdp import fully_shard from torch.nn import Linear, Module from torch.optim import SGD class SimpleModel(Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = Linear(10, 100) self.fc2 = Linear(100, 1) def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.fc2(x) return x def train(rank, num_epochs, learning_rate, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) model = SimpleModel() sharded_model = fully_shard(model) optimizer = SGD(sharded_model.parameters(), lr=learning_rate) criterion = torch.nn.MSELoss() synthetic_data = torch.randn(64, 10) synthetic_labels = torch.randn(64, 1) for epoch in range(num_epochs): outputs = sharded_model(synthetic_data) loss = criterion(outputs, synthetic_labels) optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss {loss.item()}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": num_epochs = 10 learning_rate = 0.01 world_size = 4 torch.multiprocessing.spawn(train, args=(num_epochs, learning_rate, world_size), nprocs=world_size, join=True) ``` **Note:** The training loop and the example provided are outlines. You may need to handle additional conditions related to your environment setup and ensure proper synchronization across devices.","solution":"import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.multiprocessing import Process from torch.nn import Linear, Module, MSELoss from torch.optim import SGD class SimpleModel(Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = Linear(10, 100) self.fc2 = Linear(100, 1) def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.fc2(x) return x def train(rank, world_size, num_epochs, learning_rate): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = SGD(ddp_model.parameters(), lr=learning_rate) criterion = MSELoss() synthetic_data = torch.randn(64, 10).to(rank) synthetic_labels = torch.randn(64, 1).to(rank) for epoch in range(num_epochs): outputs = ddp_model(synthetic_data) loss = criterion(outputs, synthetic_labels) optimizer.zero_grad() loss.backward() optimizer.step() if rank == 0: print(f\\"Rank {rank}, Epoch {epoch}, Loss {loss.item()}\\") dist.destroy_process_group() def spawn_train(world_size, num_epochs, learning_rate): processes = [] for rank in range(world_size): p = Process(target=train, args=(rank, world_size, num_epochs, learning_rate)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": num_epochs = 10 learning_rate = 0.01 world_size = 2 spawn_train(world_size, num_epochs, learning_rate)"},{"question":"# Python Memory Allocations and Performance Analysis You are tasked with identifying memory leaks and optimizing memory usage in a Python application. Using the `tracemalloc` module, your goal is to analyze memory allocation patterns and find the top 5 locations in your code that consume the most memory. Problem Statement Implement the following functions: 1. **`start_memory_trace(nframe: int = 1) -> None`**: - Starts the memory tracing with the given number of frames (`nframe` parameter). - **Input**: An integer `nframe` which defaults to 1. - **Output**: None. 2. **`take_memory_snapshot() -> tracemalloc.Snapshot`**: - Takes a snapshot of the current memory allocations. - **Input**: None. - **Output**: A `tracemalloc.Snapshot` instance capturing the current state of memory allocations. 3. **`display_top_stats(snapshot: tracemalloc.Snapshot, limit: int = 5) -> None`**: - Displays the top `limit` files or lines in your code that have allocated the most memory. - This display should ignore \\"<frozen importlib._bootstrap>\\" and \\"<unknown>\\" files. - **Input**: - `snapshot`: A `tracemalloc.Snapshot` instance. - `limit`: An integer specifying the number of top memory allocating locations to display (default is 5). - **Output**: None. The function prints the top memory allocating locations. Expectations - **`start_memory_trace(nframe)`** should initialize tracing of memory allocations, storing the number of frames specified by `nframe`. - **`take_memory_snapshot()`** should capture and return the current state of memory allocations as a snapshot. - **`display_top_stats(snapshot, limit)`** should filter out unwanted files and display the top memory consuming locations, providing insights into where memory is being allocated most heavily in your application. Example Usage Here\'s an example of how these functions might be used: ```python import tracemalloc # Start tracing memory allocations start_memory_trace(nframe=25) # ... Your application code here ... # Take a memory snapshot snapshot = take_memory_snapshot() # Display the top 5 memory consumers display_top_stats(snapshot, limit=5) ``` This setup will help you identify potential memory leaks and optimize memory usage in your application. Constraints - You must use the `tracemalloc` module for tracing and analyzing memory allocations. - Ensure that your functions correctly handle edge cases such as empty snapshots or invalid input parameters. - The `display_top_stats` function should provide a readable output, ideally formatted similar to the example outputs in the `tracemalloc` documentation.","solution":"import tracemalloc def start_memory_trace(nframe: int = 1) -> None: Starts the memory tracing with the given number of frames. tracemalloc.start(nframe) def take_memory_snapshot() -> tracemalloc.Snapshot: Takes a snapshot of the current memory allocations. return tracemalloc.take_snapshot() def display_top_stats(snapshot: tracemalloc.Snapshot, limit: int = 5) -> None: Displays the top `limit` files or lines in your code that have allocated the most memory. top_stats = snapshot.statistics(\'lineno\') print(f\\"Top {limit} lines\\") print(\\"\\".join([\\"-\\" for _ in range(30)])) for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] if \\"<frozen importlib._bootstrap>\\" not in frame.filename and \\"<unknown>\\" not in frame.filename: print(f\\"#{index}: {frame.filename}:{frame.lineno} - {stat.size / 1024:.1f} KiB\\")"},{"question":"In this question, you are required to demonstrate your understanding of creating and managing Python virtual environments using the `venv` module. # Task Write a Python function `setup_virtual_environment(env_name: str, packages: list) -> None` that does the following: 1. Creates a virtual environment with the given `env_name`. 2. Installs the packages listed in the `packages` list within the virtual environment. 3. Prints a list of installed packages in the virtual environment. # Input - `env_name`: A string representing the name of the virtual environment to be created. - `packages`: A list of strings, where each string is the name of a package to be installed in the virtual environment. # Constraints - You must use the `venv` module to create the virtual environment. - The function should ensure that the specified packages are installed within the virtual environment. - Assume an active internet connection, and the packages listed in `packages` are available on the Python Package Index (PyPI). # Example ```python # Example usage setup_virtual_environment(\'myenv\', [\'requests\', \'numpy\']) ``` # Expected behavior - A virtual environment named `myenv` is created. - The packages `requests` and `numpy` are installed within `myenv`. - The function prints a list of installed packages in `myenv`, which should include `requests` and `numpy`. # Note Remember to clean up any resources (like deleting the virtual environment directory) if you perform multiple runs to avoid clutter.","solution":"import os import subprocess import sys import venv def setup_virtual_environment(env_name: str, packages: list) -> None: # Create the virtual environment venv.create(env_name, with_pip=True) # Construct paths to the virtual environment\'s python and pip executables if os.name == \'nt\': # Windows python_executable = os.path.join(env_name, \'Scripts\', \'python.exe\') pip_executable = os.path.join(env_name, \'Scripts\', \'pip.exe\') else: # macOS or Linux python_executable = os.path.join(env_name, \'bin\', \'python\') pip_executable = os.path.join(env_name, \'bin\', \'pip\') # Install the specified packages subprocess.check_call([pip_executable, \'install\'] + packages) # List installed packages installed_packages = subprocess.check_output([pip_executable, \'list\']).decode() print(installed_packages)"},{"question":"# Regex Parsing and Transformation in Python **Objective**: Demonstrate your understanding of Python\'s `re` module by using regular expressions to parse and transform text. **Task**: Write a function `parse_and_transform_text` which takes a string `text` containing multiple lines. Each line has one of the following formats: 1. Email addresses (e.g., `example@example.com`). 2. Phone numbers in the format `(XXX) XXX-XXXX` (e.g., `(123) 456-7890`). 3. Dates in the format `DD-MM-YYYY` (e.g., `31-12-2023`). Your function should identify each line\'s format and transform it into a unified format specified below: - Email addresses should be converted to the format `user: example@example.com`. - Phone numbers should be converted to the format `phone: 1234567890`. - Dates should be converted to the format `date: MM/DD/YYYY`. The function should return the transformed text as a single string with each transformed line separated by a newline character. **Constraints**: 1. The input string `text` will have multiple lines, where each line is either an email address, phone number, or date. 2. The function should handle empty lines by ignoring them. 3. No line will contain more than one recognizable format (an email or phone number or date). 4. All email addresses will be valid and properly formatted. 5. All phone numbers will be valid and follow the format `(XXX) XXX-XXXX`. 6. All dates will be valid and follow the format `DD-MM-YYYY`. **Input Format**: - A multiline string where each line contains either an email, a phone number, or a date. **Output Format**: - A single string with each transformed line on a new line. **Example**: ```python input_text = example@example.com (123) 456-7890 31-12-2023 output_text = parse_and_transform_text(input_text) print(output_text) ``` Expected Output: ``` user: example@example.com phone: 1234567890 date: 12/31/2023 ``` **Implementation**: You need to correctly implement pattern matching and appropriate transformations using the `re` module in Python. Consider edge cases for different formats and ensure the output is in the specified unified format. **Function Signature**: ```python def parse_and_transform_text(text: str) -> str: pass ```","solution":"import re def parse_and_transform_text(text): Parse and transform a given multiline string containing emails, phone numbers, and dates. Parameters: - text: str : A multiline string to be parsed and transformed. Returns: - str : Transformed text with each line formatted accordingly. transformed_lines = [] email_pattern = re.compile(r\\"^[^@]+@[^@]+.[^@]+\\") phone_pattern = re.compile(r\\"^(d{3}) d{3}-d{4}\\") date_pattern = re.compile(r\\"^d{2}-d{2}-d{4}\\") for line in text.splitlines(): line = line.strip() if not line: continue if email_pattern.match(line): transformed_lines.append(f\\"user: {line}\\") elif phone_pattern.match(line): phone_number = line.replace(\\"(\\", \\"\\").replace(\\") \\", \\"\\").replace(\\"-\\", \\"\\") transformed_lines.append(f\\"phone: {phone_number}\\") elif date_pattern.match(line): day, month, year = line.split(\\"-\\") transformed_lines.append(f\\"date: {month}/{day}/{year}\\") return \\"n\\".join(transformed_lines)"},{"question":"# Async Tasks with asyncio Queues You are tasked with implementing a simplified simulation of a task processing system where tasks are distributed among worker functions using asyncio queues. Each worker performs a specific task after waiting for a given amount of time. **Objective**: To assess your ability to effectively use `asyncio` queues and manage concurrent tasks. Requirements 1. **Implement a Worker Coroutine** - Create an asynchronous function `worker(name, queue)` that repeatedly processes tasks from the given queue. - Each task represents a duration (in seconds) the worker needs to sleep. - Once a task is completed, the worker should call `queue.task_done()` and log a message. 2. **Implement Task Generation and Management** - Create an asynchronous function `main()` that: - Instantiates an `asyncio.Queue` with a `maxsize` of 10. - Generates a list of 20 random sleep durations between 0.1 and 1.0 seconds and enqueues them. - Spawns 4 worker tasks to process the queue concurrently using `asyncio.create_task`. - Waits until the queue is fully processed using `queue.join()`. - Cancels all worker tasks after processing is complete and ensures they are finished. 3. **Logging** - Each worker should log the following message after completing a task: ``` \\"{worker_name} has slept for {sleep_duration:.2f} seconds\\" ``` - After all tasks are processed, the `main` function should log the total time taken for processing. Constraints - Utilize `asyncio.Queue` methods and ensure proper use of async/await syntax. - Ensure the logs are printed in a format that clearly shows which worker completed which task, preserving the sequence of task completions. Input and Output Format No user input is required. The script should automatically run the `main` function. Ensure that the output adheres to the following format (sample output): ``` worker-1 has slept for 0.50 seconds worker-2 has slept for 0.30 seconds ... ==== Total time taken: X.XX seconds ``` Performance Requirements - The overall execution time should be close to the maximum of sleep durations if tasks are correctly processed in parallel. Example code structure: ```python import asyncio import random import time async def worker(name, queue): # Implement worker logic here async def main(): # Implement main logic here if __name__ == \'__main__\': asyncio.run(main()) ``` Good luck, and ensure your implementation follows best practices for asyncio and queue management!","solution":"import asyncio import random import time async def worker(name, queue): while True: sleep_duration = await queue.get() await asyncio.sleep(sleep_duration) print(f\\"{name} has slept for {sleep_duration:.2f} seconds\\") queue.task_done() async def main(): queue = asyncio.Queue(maxsize=10) # Generate random sleep durations and add them to the queue sleep_durations = [random.uniform(0.1, 1.0) for _ in range(20)] for sleep_duration in sleep_durations: await queue.put(sleep_duration) # Start worker tasks workers = [asyncio.create_task(worker(f\\"worker-{i+1}\\", queue)) for i in range(4)] # Wait for the queue to be fully processed start_time = time.time() await queue.join() total_time = time.time() - start_time # Cancel all worker tasks for w in workers: w.cancel() # Wait for all worker tasks to be cancelled await asyncio.gather(*workers, return_exceptions=True) print(f\\"====nTotal time taken: {total_time:.2f} seconds\\") if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Objective You are tasked with creating a custom plot using the Seaborn library to demonstrate your understanding of context settings. You will be provided with some data and requirements for different parts of the plot customization. Requirements 1. You need to generate a line plot using seaborn based on the provided data. 2. Set the context of the plot to \'poster\' to make the fonts and elements larger. 3. Adjust font scale to 1.5 to further enlarge the fonts within this context. 4. Override the default line width setting, making the line width equal to 2.5. Data - `x = [0, 1, 2, 3, 4, 5]` - `y = [1, 3, 2, 5, 7, 4]` Constraints - You must use the Seaborn library. - Your plot should be clearly labeled and should show a title, x-axis, and y-axis labels. - Ensure that the plot dimensions make it suitable for presentations. Input You don\'t need to handle function or user input, simply follow the constraints and use the provided data. Output The output should be a visible plot adhering to the aforementioned context settings and customizations. # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt # Data x = [0, 1, 2, 3, 4, 5] y = [1, 3, 2, 5, 7, 4] # Set the context for the plot sns.set_context(\\"poster\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) # Generate the line plot with labels and title sns.lineplot(x=x, y=y) plt.title(\\"Custom Context Line Plot\\") plt.xlabel(\\"X-axis\\") plt.ylabel(\\"Y-axis\\") # Show the plot plt.show() ``` **Note:** Ensure that the Seaborn and Matplotlib libraries are installed in your environment to run the code successfully.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x, y): Creates a seaborn line plot with custom context settings: - Context set to \'poster\' - Font scale set to 1.5 - Line width set to 2.5 Parameters: - x: list of x values - y: list of y values # Set the context for the plot with specified parameters sns.set_context(\\"poster\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) # Generate the line plot with provided data sns.lineplot(x=x, y=y) plt.title(\\"Custom Context Line Plot\\") plt.xlabel(\\"X-axis\\") plt.ylabel(\\"Y-axis\\") # Display the plot plt.show() # Data x = [0, 1, 2, 3, 4, 5] y = [1, 3, 2, 5, 7, 4] # Create the custom plot create_custom_plot(x, y)"},{"question":"**Iterator Protocol in Python** You are required to implement a function that processes a given iterator, retrieves its items, and sends feedback based on the current state of the iterator. # Function Signature ```python def process_iterator(iterator) -> list: pass ``` # Input - `iterator`: A Python iterator object. Guaranteed to pass the `PyIter_Check`. # Output - A list containing the results from processing the iterator\'s items. # Description 1. Given a Python iterator, your task is to create a function that iterates over its items using the `PyIter_Next` function specified in the documentation. 2. Your function should accumulate each item retrieved from the iterator into a list. If an error occurs while retrieving an item, append `\\"Error\\"` to the list and continue processing the next items. 3. For every item retrieved, use the `PyIter_Send` method to send back the item itself, and handle the different possible results (`PYGEN_RETURN`, `PYGEN_NEXT`, `PYGEN_ERROR`). 4. If `PyIter_Send` returns `PYGEN_RETURN`, append `\\"Completed\\"` to the list and break out of the iteration loop. 5. If `PyIter_Send` returns `PYGEN_ERROR`, append `\\"Error\\"` to the list and continue retrieving the next items. 6. If `PyIter_Send` returns `PYGEN_NEXT`, append the yielded value to the list and continue the iteration process. 7. Handle errors appropriately by appending an `\\"Error\\"` string to the results list when an error is detected. If an error is thrown while processing the iterator, simply return the accumulated list of results up to the point of failure. # Example ```python class MockIterator: def __init__(self, items): self.items = iter(items) self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.items): self.index += 1 return self.items[self.index-1] else: raise StopIteration # Mocking PyIter_Send functionality def send(self, value): return value # For simplicity, in production, you\'d handle this logic based on actual requirements. def process_iterator(iterator): results = [] try: while True: # Using an indefinite loop as per PyIter_Next methodology item = next(iterator) results.append(item) # Mocking the PyIter_Send behavior send_result = iterator.send(item) if send_result == \\"PYGEN_RETURN\\": results.append(\\"Completed\\") break elif send_result == \\"PYGEN_NEXT\\": results.append(item) elif send_result == \\"PYGEN_ERROR\\": results.append(\\"Error\\") continue else: results.append(\\"Error\\") continue except StopIteration: pass except Exception as e: results.append(\\"Error\\") return results # Example usage mock_iterator = MockIterator([\'a\', \'b\', \'c\']) print(process_iterator(mock_iterator)) # Should output: [\'a\', \'a\', \'b\', \'b\', \'c\', \'c\'] ``` # Constraints - You may assume that the iterator object you receive implements the `send` method. - Your function should handle all exceptions gracefully and return the items accumulated so far before failing. - You cannot import any additional libraries. # Notes - Make sure to consider cases where the iterator might raise an error during the iteration process. - Handle asynchronous iteration processes if the iterator is an asynchronous iterator by using the `PyAIter_Check` method.","solution":"def process_iterator(iterator): results = [] try: while True: # Fetch next item from the iterator item = next(iterator) results.append(item) # Mocking the PyIter_Send behavior send_result = iterator.send(item) if send_result == \\"PYGEN_RETURN\\": results.append(\\"Completed\\") break elif send_result == \\"PYGEN_NEXT\\": results.append(item) elif send_result == \\"PYGEN_ERROR\\": results.append(\\"Error\\") else: results.append(\\"Error\\") except StopIteration: pass except Exception: results.append(\\"Error\\") return results # Mock Iterator to Simulate PyIterator class MockIterator: def __init__(self, items): self.items = items self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.items): item = self.items[self.index] self.index += 1 return item else: raise StopIteration # Mocking PyIter_Send functionality def send(self, value): if value == \\"error\\": return \\"PYGEN_ERROR\\" elif value == \\"end\\": return \\"PYGEN_RETURN\\" else: return \\"PYGEN_NEXT\\""},{"question":"# PyTorch Advanced Compilation Question Objective: Create a PyTorch script that uses `torch.compile` to optimize a model\'s training process within a distributed data-parallel setup. The goal is to minimize graph breaks and ensure the training is efficient. Scenario: You are given a simple neural network model and a dataset for a classification task. Your task is to implement the following: 1. Use `torch.compile` to improve the model\'s training efficiency. 2. Implement the model training using `DistributedDataParallel (DDP)`. 3. Ensure that graph breaks are minimized during the compilation. 4. Provide mechanisms to handle any potential compilation or runtime issues. Instructions: 1. **Model Definition**: - Define a simple neural network with two linear layers using PyTorch. 2. **Dataset Preparation**: - Create a synthetic dataset for demonstration purposes. 3. **Distributed Training Setup**: - Set up the training loop using `DistributedDataParallel (DDP)`. 4. **Optimization with torch.compile**: - Compile the model\'s forward and backward passes using `torch.compile`. 5. **Minimizing Graph Breaks**: - Identify and address potential graph breaks to ensure smooth execution. 6. **Troubleshooting**: - Implement error handling for potential issues during compilation and runtime. Constraints: - The code should be runnable in a distributed environment with multiple GPUs. - Minimize the number of graph breaks to enhance performance. - Ensure that the training loop is optimized and does not suffer significant performance degradation. Input and Output: - **Input**: No specific input format. The implementation should include all parts within the script. - **Output**: The script should print training progress and any relevant debugging information. Performance Considerations: - Ensure that the training process benefits from `torch.compile` optimizations. - Handle potential OOM (Out Of Memory) errors and graph break issues. Submission: - Submit the complete script, ensuring all parts are integrated and functioning correctly in a distributed setup. # Example Structure (Pseudo-Code): ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from torch.nn.parallel import DistributedDataParallel as DDP import torch._dynamo as dynamo import torch.multiprocessing as mp class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(rank, world_size): # Prepare dataset and DataLoader dataset = TensorDataset(torch.randn(1000, 784), torch.randint(0, 10, (1000,))) sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=64, sampler=sampler) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) loss_fn = nn.CrossEntropyLoss() @torch.compile def train_step(batch, targets): optimizer.zero_grad() output = ddp_model(batch) loss = loss_fn(output, targets) loss.backward() optimizer.step() return loss for epoch in range(10): sampler.set_epoch(epoch) total_loss = 0.0 for batch, targets in dataloader: batch, targets = batch.to(rank), targets.to(rank) loss = train_step(batch, targets) total_loss += loss.item() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {total_loss/len(dataloader)}\\") def main(): world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` Provide the full implementation in the submission. You may refer to the PyTorch documentation for additional details if necessary.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset, DistributedSampler from torch.nn.parallel import DistributedDataParallel as DDP import torch.multiprocessing as mp import torch.distributed as dist class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(42) torch.cuda.manual_seed(42) dataset = TensorDataset(torch.randn(1000, 784), torch.randint(0, 10, (1000,))) sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=64, sampler=sampler) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) loss_fn = nn.CrossEntropyLoss() @torch.compile def train_step(batch, targets): optimizer.zero_grad() output = ddp_model(batch) loss = loss_fn(output, targets) loss.backward() optimizer.step() return loss for epoch in range(10): sampler.set_epoch(epoch) total_loss = 0.0 for batch, targets in dataloader: batch, targets = batch.to(rank), targets.to(rank) loss = train_step(batch, targets) total_loss += loss.item() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {total_loss/len(dataloader)}\\") dist.destroy_process_group() def main(): world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"Problem Statement You are tasked to perform a series of transformations on a given Torch FX GraphModule. Specifically, you must replace all `torch.ops.aten.add.Tensor()` operations with two consecutive operations: `torch.ops.aten.mul.Tensor()` and `torch.ops.aten.sub.Tensor()`, and then ensure that after every `torch.ops.aten.sub.Tensor()` operation a `torch.ops.aten.relu.default()` operation is inserted. Write a function `transform_graph(graph_module: torch.fx.GraphModule) -> torch.fx.GraphModule` that takes a Torch FX GraphModule `graph_module` as input and returns a transformed GraphModule. 1. Replace all occurrences of `torch.ops.aten.add.Tensor()` with: ```python def f(x, y): z1 = x * y z2 = z1 - y return z2 ``` 2. After each `torch.ops.aten.sub.Tensor()` insertion, add a `torch.ops.aten.relu.default()` call: ```python def f(x, y): z1 = x * y z2 = z1 - y z3 = relu(z2) return z3 ``` Requirements - Ensure that your transformation uses both `Transformers` for the basic replacements and direct node manipulation for inserting new nodes. - Do not use the `subgraph_rewriter`. Perform the transformations step-by-step. - Include necessary validation to ensure correctness of transformations. - Your code should handle any given GraphModule conforming to the above requirements. Input - A `torch.fx.GraphModule` object, which is a computational graph that needs transformation. Output - A transformed `torch.fx.GraphModule` object with the described modifications. Example ```python import torch import torch.fx as fx class ExampleModule(torch.nn.Module): def forward(self, x, y): return torch.ops.aten.add.Tensor(x, y) # Trace the module example_module = ExampleModule() traced_module = fx.symbolic_trace(example_module) # Transform the graph transformed_graph = transform_graph(traced_module) # Print transformed graph for node in transformed_graph.graph.nodes: print(node) # The output should reflect the transformed operations. ```","solution":"import torch import torch.fx as fx def transform_graph(graph_module: torch.fx.GraphModule) -> torch.fx.GraphModule: Transforms the given FX GraphModule by replacing `torch.ops.aten.add.Tensor` with multiplication, subtraction, and ReLU operations as described. graph = graph_module.graph for node in list(graph.nodes): if node.target == torch.ops.aten.add.Tensor: with graph.inserting_before(node): x, y = node.args z1 = graph.call_function(torch.ops.aten.mul.Tensor, args=(x, y)) z2 = graph.call_function(torch.ops.aten.sub.Tensor, args=(z1, y)) z3 = graph.call_function(torch.ops.aten.relu.default, args=(z2,)) node.replace_all_uses_with(z3) graph.erase_node(node) graph_module.recompile() return graph_module"},{"question":"**MPS Device and Profiler Utilization in PyTorch** You are required to design a routine that effectively utilizes MPS devices within PyTorch. This involves managing MPS devices, ensuring efficient memory usage, and utilizing the profiler for performance measurement. # Task: 1. **Device Initialization and Memory Management:** - Write a function `initialize_device()` that: - Checks if MPS devices are available. - If available, sets the first available MPS device as the current device. - Ensures the cache is empty using the appropriate `torch.mps` function. 2. **Memory Utilization Check:** - Write a function `check_memory_allocation()` that: - Returns the current allocated memory, driver allocated memory, and recommended maximum memory for the MPS device. 3. **Random State Management:** - Write a function `set_manual_seed(seed_value)` that: - Sets a manual seed for reproducibility in the program using the provided seed value. 4. **Profiling Operations:** - Write a function `profile_operations(operation, *args)` that: - Starts the MPS profiler. - Executes the provided operation (function) with the given arguments. - Stops the profiler after the execution. - Returns the result of the operation. # Input/Output Specifications: - `initialize_device()` - Input: None - Output: None - `check_memory_allocation()` - Input: None - Output: Dictionary with keys \'current_allocated\', \'driver_allocated\', \'recommended_max\' containing respective memory values. - `set_manual_seed(seed_value)` - Input: `seed_value` (int) - Output: None - `profile_operations(operation, *args)` - Input: `operation` (callable), `*args` (additional arguments for the operation) - Output: The result of the provided operation after profiling. # Constraints: - Ensure you are using MPS devices only if they are available. - Handle exceptions appropriately if any of the operations fail (e.g., no MPS device available, profiling fails etc.). # Example Usage: ```python # Ensure the device is initialized initialize_device() # Set a manual seed for reproducibility set_manual_seed(42) # Check memory allocation memory_info = check_memory_allocation() print(memory_info) # Profile an example operation def example_operation(x): return x * x result = profile_operations(example_operation, torch.tensor([3.0, 4.0], device=\'mps\')) print(result) ``` # Note: - This question assesses your understanding of the `torch.mps` module, ensuring you can manage devices, memory, and perform profiling to monitor performance. - Document your code and handle edge cases diligently.","solution":"import torch import torch.mps def initialize_device(): Initialize the MPS device, if available, and clear cache. if torch.backends.mps.is_available() and torch.backends.mps.is_built(): device = torch.device(\'mps\') torch.mps.empty_cache() torch.mps.set_per_process_memory_fraction(1.0, device) else: print(\\"MPS device not available.\\") def check_memory_allocation(): Check memory utilization on the MPS device. if torch.backends.mps.is_available() and torch.backends.mps.is_built(): alloc_mem = torch.mps.current_allocated_memory() drv_alloc_mem = torch.mps.driver_allocated_memory() rec_max_mem = torch.mps.get_recommended_memory_solution() return { \'current_allocated\': alloc_mem, \'driver_allocated\': drv_alloc_mem, \'recommended_max\': rec_max_mem, } else: raise RuntimeError(\\"MPS device not available.\\") def set_manual_seed(seed_value): Set manual seed for reproducibility. torch.manual_seed(seed_value) if torch.backends.mps.is_available() and torch.backends.mps.is_built(): torch.mps.manual_seed(seed_value) def profile_operations(operation, *args): Profile the provided operation on the MPS device. if not (torch.backends.mps.is_available() and torch.backends.mps.is_built()): raise RuntimeError(\\"MPS device not available.\\") with torch.autograd.profiler.profile() as prof: result = operation(*args) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) return result"},{"question":"# Python `time` Module Coding Assessment Objective Your task is to implement a function that converts a given local time to UTC and formats it into a specific string representation. Additionally, you need to calculate the number of seconds between the current time and a future time expressed in a specific string format. Function Signature ```python def convert_and_calculate(local_time: tuple, future_time_str: str) -> str: Convert the local_time to UTC and format it, then calculate the number of seconds between the current time and the future_time_str. Args: local_time (tuple): A 9-tuple representing a time in local time as returned by localtime(). future_time_str (str): A string representing a future time, formatted as \'%Y-%m-%d %H:%M:%S\'. Returns: str: A formatted string that includes two parts: - The converted UTC time from local_time, formatted as \'UTC: %Y-%m-%d %H:%M:%S\'. - The number of seconds between the current time and future_time_str, formatted as \'Seconds until future time: X\', where X is the number of seconds. ``` Input and Output Examples ```python local_time = (2023, 10, 5, 14, 30, 0, 3, 278, -1) # October 5, 2023, 2:30 PM local time future_time_str = \\"2023-12-25 10:00:00\\" # Christmas day, 10 AM result = convert_and_calculate(local_time, future_time_str) print(result) # Expected output (example): # \'UTC: 2023-10-05 19:30:00nSeconds until future time: 7067400\' ``` Constraints - You need to handle the case where `local_time` can represent a time past the year 2038 correctly. - If `future_time_str` is in the past relative to the current time, the seconds count should be negative. Hints - Use `time.mktime()` to convert `local_time` to seconds since the epoch for calculations. - Convert `local_time` to UTC using `time.gmtime()`. - Use `time.strptime()` to parse `future_time_str`. - You may assume `future_time_str` is always a valid date-time string.","solution":"import time from datetime import datetime def convert_and_calculate(local_time: tuple, future_time_str: str) -> str: Convert the local_time to UTC and format it, then calculate the number of seconds between the current time and the future_time_str. Args: local_time (tuple): A 9-tuple representing a time in local time as returned by localtime(). future_time_str (str): A string representing a future time, formatted as \'%Y-%m-%d %H:%M:%S\'. Returns: str: A formatted string that includes two parts: - The converted UTC time from local_time, formatted as \'UTC: %Y-%m-%d %H:%M:%S\'. - The number of seconds between the current time and future_time_str, formatted as \'Seconds until future time: X\', where X is the number of seconds. # Convert local_time to seconds since epoch local_epoch_time = time.mktime(local_time) # Convert local_epoch_time to UTC time tuple utc_time_tuple = time.gmtime(local_epoch_time) # Format UTC time utc_time_str = time.strftime(\'UTC: %Y-%m-%d %H:%M:%S\', utc_time_tuple) # Parse future_time_str to time struct future_time_tuple = time.strptime(future_time_str, \'%Y-%m-%d %H:%M:%S\') # Convert future_time_tuple to seconds since epoch future_epoch_time = int(time.mktime(future_time_tuple)) # Get current time in seconds since epoch current_epoch_time = int(time.time()) # Calculate difference in seconds seconds_until_future_time = future_epoch_time - current_epoch_time # Format result result = f\\"{utc_time_str}nSeconds until future time: {seconds_until_future_time}\\" return result"},{"question":"**Question: Custom Pickler and Unpickler for Complex Objects with External References** # Objective The goal of this task is to assess your ability to work with the `pickle` module for serializing and deserializing complex objects, including those that involve external references. You will create custom `Pickler` and `Unpickler` classes that handle a specific data format involving a mixture of in-memory and external data stored in a SQL database. # Problem Statement You are given a class `User` which represents a user profile and contains attributes like `id`, `name`, and `profile_picture`. The `profile_picture` attribute is a binary data object representing the user\'s profile picture. This binary data is too large to be pickled directly, so it is stored in an SQLite database. You need to implement custom `Pickler` and `Unpickler` classes to handle the serialization and deserialization of `User` objects. The goal is to pickle the `User` object by storing the `profile_picture` separately in the SQLite database and referring to it by a unique ID during pickling. During unpickling, the `profile_picture` should be retrieved from the database using the stored ID. # Implementation Details 1. **Database Setup:** - Create an SQLite database with a table `profile_pictures` that has columns `user_id` (INTEGER, PRIMARY KEY) and `picture` (BLOB). 2. **User Class:** - Define a `User` class with attributes `id` (int), `name` (str), and `profile_picture` (bytes). 3. **Custom Pickler:** - Implement a `DBPickler` class that inherits from `pickle.Pickler`. - Override the `persistent_id` method to store the `profile_picture` in the database and return a unique ID for the `profile_picture`. 4. **Custom Unpickler:** - Implement a `DBUnpickler` class that inherits from `pickle.Unpickler`. - Override the `persistent_load` method to retrieve the `profile_picture` from the database using the stored ID. # Constraints - Use protocol version 4 or higher for pickling. - The `DBPickler` and `DBUnpickler` classes should correctly handle multiple `User` objects and ensure that `profile_picture` data is stored and retrieved accurately. # Example ```python import sqlite3 import pickle from io import BytesIO class User: def __init__(self, id, name, profile_picture): self.id = id self.name = name self.profile_picture = profile_picture def create_database(): conn = sqlite3.connect(\':memory:\') cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE profile_pictures (user_id INTEGER PRIMARY KEY, picture BLOB)\'\'\') return conn class DBPickler(pickle.Pickler): def __init__(self, file, protocol=None, db_conn=None): super().__init__(file, protocol=protocol) self.db_conn = db_conn def persistent_id(self, obj): if isinstance(obj, bytes): cursor = self.db_conn.cursor() cursor.execute(\\"INSERT INTO profile_pictures (picture) VALUES (?)\\", (obj,)) return cursor.lastrowid return None class DBUnpickler(pickle.Unpickler): def __init__(self, file, db_conn=None): super().__init__(file) self.db_conn = db_conn def persistent_load(self, pid): cursor = self.db_conn.cursor() cursor.execute(\\"SELECT picture FROM profile_pictures WHERE user_id=?\\", (pid,)) result = cursor.fetchone() if result: return result[0] else: raise pickle.UnpicklingError(f\\"Failed to load profile_picture with id {pid}\\") # Usage example conn = create_database() user = User(1, \'Alice\', b\'x89PNGrnx1anx00x00\') # Pickle the user object file = BytesIO() DBPickler(file, protocol=4, db_conn=conn).dump(user) # Unpickle the user object file.seek(0) unpickled_user = DBUnpickler(file, db_conn=conn).load() assert user.name == unpickled_user.name assert user.profile_picture == unpickled_user.profile_picture ``` Your task is to implement the `User` class, `DBPickler`, and `DBUnpickler` classes as described in the example. Test your implementation by creating, pickling, and unpickling `User` objects, ensuring that the `profile_picture` is correctly handled. # Submission Submit the Python code for the `User` class, `DBPickler`, and `DBUnpickler` classes along with the test cases demonstrating their functionality.","solution":"import sqlite3 import pickle from io import BytesIO class User: def __init__(self, id, name, profile_picture): self.id = id self.name = name self.profile_picture = profile_picture def create_database(): conn = sqlite3.connect(\':memory:\') cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE profile_pictures (user_id INTEGER PRIMARY KEY, picture BLOB)\'\'\') conn.commit() return conn class DBPickler(pickle.Pickler): def __init__(self, file, protocol=None, db_conn=None): super().__init__(file, protocol=protocol) self.db_conn = db_conn def persistent_id(self, obj): if isinstance(obj, bytes): cursor = self.db_conn.cursor() cursor.execute(\\"INSERT INTO profile_pictures (picture) VALUES (?)\\", (obj,)) self.db_conn.commit() return cursor.lastrowid return None class DBUnpickler(pickle.Unpickler): def __init__(self, file, db_conn=None): super().__init__(file) self.db_conn = db_conn def persistent_load(self, pid): cursor = self.db_conn.cursor() cursor.execute(\\"SELECT picture FROM profile_pictures WHERE user_id=?\\", (pid,)) result = cursor.fetchone() if result: return result[0] else: raise pickle.UnpicklingError(f\\"Failed to load profile_picture with id {pid}\\") # Demo and Testing # Creating the database and a User object for testing database_connection = create_database() user = User(1, \'Alice\', b\'x89PNGrnx1anx00x00\') # Pickling the User object user_pickle_file = BytesIO() DBPickler(user_pickle_file, protocol=4, db_conn=database_connection).dump(user) # Unpickling the User object user_pickle_file.seek(0) unpickled_user = DBUnpickler(user_pickle_file, db_conn=database_connection).load() # Assertions to ensure correctness assert user.id == unpickled_user.id assert user.name == unpickled_user.name assert user.profile_picture == unpickled_user.profile_picture"},{"question":"You are working on a Python module where each function is documented with examples using the doctest format. Your task is to write a new function with correctly implemented doctests within its docstring. The function will count the number of vowels in a given string. # Problem Statement: Implement a Python function `count_vowels` that counts and returns the number of vowels (\'a\', \'e\', \'i\', \'o\', \'u\', both uppercase and lowercase) in a provided string. Ensure to write doctests for this function within its docstring. # Function Signature: ```python def count_vowels(s: str) -> int: Count the number of vowels in the given string. Args: s (str): The input string. Returns: int: The number of vowels in the string. Examples: >>> count_vowels(\\"hello\\") 2 >>> count_vowels(\\"HELLO\\") 2 >>> count_vowels(\\"abcdefghijklmnopqrstuvwxyz\\") 5 >>> count_vowels(\\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\\") 5 >>> count_vowels(\\"Python123!! Programming\\") 4 >>> count_vowels(\\"1234567890\\") 0 >>> count_vowels(\\"\\") 0 >>> count_vowels(\\"BcDfGhJkLmNpQrStVwXyZ\\") 0 # Your implementation here ``` # Constraints: - The function should handle both upper and lower-case vowels. - The function should handle the case of an empty string gracefully. - Non-alphabetic characters should be ignored. # Performance Requirements: - The solution should efficiently handle strings of length up to 10^5. # Hints: - A good approach might involve iterating over the string and checking each character to see if it\'s a vowel. - Using a set for checking membership (if a character is a vowel) can provide average O(1) lookup time. # Note: Once you implement the function and its doctests, run the following command to verify the examples using doctest: ```bash python -m doctest -v <your_module_name>.py ``` Or, if you place the code within a script such as `count_vowels.py`, you can directly test it by running: ```bash python count_vowels.py ``` # Example: ```python def count_vowels(s: str) -> int: Count the number of vowels in the given string. Args: s (str): The input string. Returns: int: The number of vowels in the string. Examples: >>> count_vowels(\\"hello\\") 2 >>> count_vowels(\\"HELLO\\") 2 >>> count_vowels(\\"abcdefghijklmnopqrstuvwxyz\\") 5 >>> count_vowels(\\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\\") 5 >>> count_vowels(\\"Python123!! Programming\\") 4 >>> count_vowels(\\"1234567890\\") 0 >>> count_vowels(\\"\\") 0 >>> count_vowels(\\"BcDfGhJkLmNpQrStVwXyZ\\") 0 vowels = set(\\"aeiouAEIOU\\") return sum(1 for char in s if char in vowels) if __name__ == \\"__main__\\": import doctest doctest.testmod() ```","solution":"def count_vowels(s: str) -> int: Count the number of vowels in the given string. Args: s (str): The input string. Returns: int: The number of vowels in the string. Examples: >>> count_vowels(\\"hello\\") 2 >>> count_vowels(\\"HELLO\\") 2 >>> count_vowels(\\"abcdefghijklmnopqrstuvwxyz\\") 5 >>> count_vowels(\\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\\") 5 >>> count_vowels(\\"Python123!! Programming\\") 4 >>> count_vowels(\\"1234567890\\") 0 >>> count_vowels(\\"\\") 0 >>> count_vowels(\\"BcDfGhJkLmNpQrStVwXyZ\\") 0 vowels = set(\\"aeiouAEIOU\\") return sum(1 for char in s if char in vowels)"},{"question":"You are given two datasets: the `penguins` dataset and the `flights` dataset. Using seaborn, your task is to complete the following plotting tasks: 1. **Group and Aggregate Plot with Customization:** - Load the `penguins` dataset. - Create a point plot to show the average body mass (in grams) for different species, grouped by island and differentiated by sex. Use different markers and linestyles for each sex. - Display the standard deviation as error bars. - Customize the plot by setting the color to a single consistent value (gray), applying unique markers and linestyles for sexes, and adjusting marker size. 2. **Pivot and Plot with Annotation:** - Load the `flights` dataset. - Pivot the dataset so that the months are the columns, years are the rows, and the values are the number of passengers. - Create a point plot for the number of passengers across different months for the year 1955. - Annotate the plot with a red star symbol at the maximum passenger value recorded in the year 1955 for any month. # Expected Input and Output Format: - **Function Signature:** ```python def plot_penguins(): # Step 1 implementation pass def plot_flights(): # Step 2 implementation pass ``` - **Constraints:** - Ensure that the axis labels are descriptive and include units where applicable. - Use seaborn and matplotlib for plotting. - Maintain a readable and aesthetically pleasing plot style. - **Performance:** - Aim for efficient data handling and avoid unnecessary computations. # Performance Requirements: - The plots should be rendered efficiently without excessive delay. - Ensure the plots are clear and informative, effectively conveying the aggregated information and any annotations. **HINT:** Use `sns.set_theme` to customize global plot aesthetics before creating individual plots, and leverage the built-in functions for loading datasets directly from seaborn. You can use the code snippets from the provided documentation as a guide to help you accomplish the tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create the point plot with customization g = sns.catplot( x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", col=\\"species\\", data=penguins, kind=\\"point\\", dodge=True, join=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], palette=\\"gray\\", markersize=10, errorbar=(\\"sd\\") ) # Customize the labels g.set_axis_labels(\\"Island\\", \\"Body Mass (g)\\") g.set_titles(\\"{col_name}\\") g.add_legend(title=\\"Sex\\") plt.show() def plot_flights(): # Load the flights dataset flights = sns.load_dataset(\'flights\') # Pivot the dataset flights_pivot = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Select data for the year 1955 data_1955 = flights_pivot.loc[1955] # Create the point plot for 1955 plt.figure(figsize=(10, 6)) sns.pointplot(data=data_1955.reset_index(), x=\'month\', y=1955, markers=\'o\') # Find the maximum passenger value and annotate it max_passengers = data_1955.max() max_month = data_1955.idxmax() plt.annotate( \'Max passengers\', xy=(max_month, max_passengers), xytext=(max_month, max_passengers + 20), arrowprops=dict(facecolor=\'red\', shrink=0.05), fontsize=12, color=\'red\', ha=\'center\', bbox=dict(boxstyle=\\"round,pad=0.3\\", edgecolor=\'red\', facecolor=\'white\') ) # Customize the labels plt.xlabel(\\"Month\\") plt.ylabel(\\"Number of Passengers\\") plt.title(\\"Number of Passengers Across Different Months in 1955\\") plt.show()"},{"question":"# Question: Dynamic Module Discovery and Interaction Using the `pkgutil` module, write a function that dynamically discovers all submodules in a given directory path and interacts with them to perform a specified operation. Function Signature ```python from typing import List, Callable, Any import pkgutil def discover_and_operate(path: List[str], operation: Callable[[str], Any]) -> List[Any]: Dynamically discovers all submodules in the provided directory path and applies the provided operation on each discovered module. Parameters: path (List[str]): A list of directory paths to look for the modules in. operation (Callable[[str], Any]): A callable (function) that takes a module name as input and performs any operation. Returns: List[Any]: A list containing the results of applying the operation on each module. ``` Instructions 1. **Discover Modules**: Use the `pkgutil.iter_modules` function to discover all submodules in the specified directory path. 2. **Apply Operation**: For each discovered module, apply the provided callable `operation`. 3. **Output**: Collect and return the results for each operation in a list. Input - `path`: A list of strings where each string is a directory path to look for modules. - `operation`: A callable that accepts a module name as a string and returns a result. Output - A list of results returned from applying the operation on each discovered module. Example ```python def example_operation(module_name: str) -> str: return f\\"Module: {module_name}\\" result = discover_and_operate([\\"/path/to/directory\\"], example_operation) print(result) ``` If the specified directory contains modules named `module1`, `module2`, etc., the expected output might look like: ```plaintext [\\"Module: module1\\", \\"Module: module2\\", ...] ``` Constraints - Do not use deprecated classes like `ImpImporter` or `ImpLoader`. - Ensure proper error handling for any potential import errors or path issues. - Assume that the directories in the path list are valid and accessible.","solution":"from typing import List, Callable, Any import pkgutil def discover_and_operate(path: List[str], operation: Callable[[str], Any]) -> List[Any]: Dynamically discovers all submodules in the provided directory path and applies the provided operation on each discovered module. Parameters: path (List[str]): A list of directory paths to look for the modules in. operation (Callable[[str], Any]): A callable (function) that takes a module name as input and performs any operation. Returns: List[Any]: A list containing the results of applying the operation on each module. results = [] # Loop over the list of paths for p in path: # Use pkgutil.iter_modules to find all submodules in the directory for _, module_name, _ in pkgutil.iter_modules([p]): # Apply the operation to the module name and collect the result result = operation(module_name) results.append(result) return results"},{"question":"# Objective: Create a plot using seaborn\'s object-oriented interface to visualize the data in a meaningful way. # Problem Statement: You are provided with the `penguins` dataset and are required to create a plot that shows the median body mass for each species and sex. Additionally, overlay the individual data points for each species and sex group. # Data: You will use the `penguins` dataset from seaborn, which includes the following columns: - `species`: The species of the penguin (`Adelie`, `Chinstrap`, and `Gentoo`). - `sex`: The sex of the penguin. - `body_mass_g`: The body mass of the penguin in grams. - `flipper_length_mm`: The flipper length of the penguin in millimeters. # Task: 1. **Load the `penguins` dataset** using seaborn. 2. **Create a plot** where: - The x-axis represents the `species`. - The y-axis represents the `body_mass_g`. - Color differentiates between the sexes (`sex`). 3. **Add a `Dash` mark** to represent the median body mass for each species and sex group. 4. **Overlay `Dots`** to represent individual data points for each species and sex group. 5. Ensure proper **legend and labels** are added for clarity. # Requirements: - Use seaborn’s object interface (`seaborn.objects`). - Customizing properties of the `Dash` mark, such as `alpha` and `linewidth`. - Using `Dodge` if necessary to prevent overlap. # Function Signature: ```python def create_penguin_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Your code here # ... pass ``` # Expected Output: - A visualization containing the median values marked by `Dash` marks and individual data points marked by `Dots`. - Properly labeled x-axis, y-axis, and legend. Constraints: - Handle missing data if present. - Ensure the plot is clear and visually interpretable. # Example: Refer to the provided documentation snippets for examples of creating and customizing plots using seaborn’s object-oriented interface.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\").dropna(subset=[\\"body_mass_g\\", \\"species\\", \\"sex\\"]) # Create and customize the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Dot(), so.Dodge()) .add(so.Dash(), so.Dodge(), agg=\\"median\\", alpha=0.7, linewidth=2) .label(x=\\"Species\\", y=\\"Body Mass (g)\\", color=\\"Sex\\") .scale(color=\\"Set1\\") ) return plot"},{"question":"**Advanced Python: Implementing Custom Protocols** # Objective: In this assessment, you are required to implement custom protocols using Python\'s special methods to simulate interaction similar to Python\'s abstract object and sequence protocols. This will test your understanding of Python\'s dunder methods (special methods), object-oriented programming, and custom behavior implementation. # Problem Statement: You need to design a custom sequence-like container class `CustomSequence` that mimics the behavior of Python\'s built-in sequence types. Your implementation should include methods to handle indexing, slicing, and iteration. # Requirements: 1. **Initialization**: The class should be initialized with a list of elements. 2. **Indexing**: Implement the `__getitem__` method to handle both single element access and slice access. 3. **Setting Items**: Implement the `__setitem__` method to allow modifying elements at specific indices and slices. 4. **Deleting Items**: Implement the `__delitem__` method to allow deleting elements at specific indices and slices. 5. **Iteration**: Implement the `__iter__` method to allow iteration over the elements. 6. **Length**: Implement the `__len__` method to return the number of elements in the sequence. 7. **Containment Check**: Implement the `__contains__` method to allow using the `in` keyword to check for element membership. # Input and Output: - Input: The input will be in the form of method calls on an instance of `CustomSequence`. For example: ```python cs = CustomSequence([1, 2, 3, 4, 5]) cs[1] # Access the second element cs[1:4] # Access a slice from the second to the fourth element cs[2] = 10 # Set the third element to 10 del cs[2:4] # Delete elements from the third to the fourth for item in cs: pass # Iterate over the sequence 3 in cs # Check for membership len(cs) # Get the number of elements ``` - Output: The output should be the expected result of the operations. # Constraints: - Your class should handle invalid indices and slice operations gracefully by raising appropriate exceptions. - You must implement the methods as part of the `CustomSequence` class; using built-in data structures directly to fulfill the requirements is not allowed. # Example: ```python class CustomSequence: def __init__(self, initial_data): self.data = initial_data def __getitem__(self, index): if isinstance(index, slice): return self.data[index] elif isinstance(index, int): return self.data[index] else: raise TypeError(\\"Invalid argument type.\\") def __setitem__(self, index, value): if isinstance(index, slice): self.data[index] = value elif isinstance(index, int): self.data[index] = value else: raise TypeError(\\"Invalid argument type.\\") def __delitem__(self, index): if isinstance(index, slice): del self.data[index] elif isinstance(index, int): del self.data[index] else: raise TypeError(\\"Invalid argument type.\\") def __iter__(self): return iter(self.data) def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data # Example usage: cs = CustomSequence([1, 2, 3, 4, 5]) print(cs[1]) # Output: 2 print(cs[1:4]) # Output: [2, 3, 4] cs[2] = 10 print(cs.data) # Output: [1, 2, 10, 4, 5] del cs[2:4] print(cs.data) # Output: [1, 2, 5] print([x for x in cs]) # Output: [1, 2, 5] print(3 in cs) # Output: False print(2 in cs) # Output: True print(len(cs)) # Output: 3 ``` Your implementation should pass all the given examples and handle edge cases appropriately.","solution":"class CustomSequence: def __init__(self, initial_data): self.data = initial_data def __getitem__(self, index): if isinstance(index, slice): return self.data[index] elif isinstance(index, int): return self.data[index] else: raise TypeError(\\"Invalid argument type.\\") def __setitem__(self, index, value): if isinstance(index, slice): self.data[index] = value elif isinstance(index, int): self.data[index] = value else: raise TypeError(\\"Invalid argument type.\\") def __delitem__(self, index): if isinstance(index, slice): del self.data[index] elif isinstance(index, int): del self.data[index] else: raise TypeError(\\"Invalid argument type.\\") def __iter__(self): return iter(self.data) def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data"},{"question":"# Advanced Audio Processing using audioop Module Problem Statement: You are tasked with implementing an advanced audio signal processing function that can perform a sequence of operations on a raw audio fragment. The goal is to modify the input audio fragment by applying several audio operations sequentially, then you will implement a function that can apply these transformations and return the modified fragment. Function Signature: ```python def process_audio_fragment(audio_fragment: bytes, width: int, operations: list) -> bytes: Apply a series of operations on the input audio fragment and return the modified fragment. :param audio_fragment: A bytes-like object representing the raw audio fragment. :param width: An integer representing the sample width in bytes. Can be 1, 2, 3, or 4. :param operations: A list of tuples representing the operations to be applied sequentially. Each tuple is of the form (\'operation_name\', parameters), where \'operation_name\' is a string and parameters is a tuple containing the parameters for that operation. :return: A bytes-like object representing the modified audio fragment. pass ``` Operations Specification: - **\\"add\\"**: Adds the given fragment to the audio fragment. - Parameters: Another bytes object representing the fragment to add. Must be the same length as `audio_fragment`. - **\\"mul\\"**: Multiplies each sample in the audio fragment by the given factor. - Parameters: A float representing the factor by which to multiply the samples. - **\\"bias\\"**: Adds a bias to each sample in the audio fragment. - Parameters: An integer representing the bias to add. - **\\"reverse\\"**: Reverses the audio fragment. - Parameters: None. - **\\"tomono\\"**: Converts a stereo fragment to mono. - Parameters: Two floats representing the left and right channel factors respectively. - **\\"tostereo\\"**: Converts a mono fragment to stereo. - Parameters: Two floats representing the left and right channel factors respectively. Constraints: - The input `audio_fragment` and any additional fragments required by the operations must be of the same length. - Ensure all resulting audio fragments maintain sample integrity (e.g., handle overflow and provide correct sample formatting). - The order of operations described in the `operations` list must be preserved. Example Usage: ```python audio_fragment = b\'x01x02x03x04\' # Example input fragment width = 2 operations = [ (\'mul\', (2.0,)), # Multiply each sample by 2.0 (\'bias\', (100,)), # Add a bias of 100 to each sample (\'reverse\', ()), # Reverse the audio fragment ] result = process_audio_fragment(audio_fragment, width, operations) print(result) # Output the modified audio fragment ``` **Notes:** - The provided example is for illustrative purposes, and actual audio data will be more complex. - Use the `audioop` module functions to implement the required audio processing operations.","solution":"import audioop def process_audio_fragment(audio_fragment: bytes, width: int, operations: list) -> bytes: for operation, parameters in operations: if operation == \'add\': altriudio_fragment, = parameters audio_fragment = audioop.add(audio_fragment, altriudio_fragment, width) elif operation == \'mul\': factor, = parameters audio_fragment = audioop.mul(audio_fragment, width, factor) elif operation == \'bias\': bias, = parameters audio_fragment = audioop.bias(audio_fragment, width, bias) elif operation == \'reverse\': audio_fragment = audioop.reverse(audio_fragment, width) elif operation == \'tomono\': lfactor, rfactor = parameters audio_fragment = audioop.tomono(audio_fragment, width, lfactor, rfactor) elif operation == \'tostereo\': lfactor, rfactor = parameters audio_fragment = audioop.tostereo(audio_fragment, width, lfactor, rfactor) return audio_fragment"},{"question":"Objective The goal of this assessment is to gauge your understanding of using Restricted Boltzmann Machines (RBMs) within scikit-learn for unsupervised feature learning. Problem Statement You are given a binary image dataset. Your task is to implement a function that uses the BernoulliRBM from scikit-learn to learn features from this dataset. # Function Signature ```python def learn_rbm_features(data: np.ndarray, n_components: int = 256, learning_rate: float = 0.01, batch_size: int = 10, n_iter: int = 10) -> np.ndarray: Fits a Bernoulli Restricted Boltzmann Machine on the input data and returns the learned features. :param data: np.ndarray - Binary input data (each row represents a data sample) with shape (n_samples, n_features). :param n_components: int - Number of binary hidden units. :param learning_rate: float - The learning rate for weight updates. :param batch_size: int - The number of training samples per mini-batch. :param n_iter: int - Number of iterations over the training dataset. :return: np.ndarray - Transformed data using the learned components with shape (n_samples, n_components). ``` # Input - `data`: An `np.ndarray` of shape `(n_samples, n_features)` where each element is a binary value (0 or 1). - `n_components`: An `int` specifying the number of hidden units in the RBM. - `learning_rate`: A `float` specifying the learning rate for weight updates. - `batch_size`: An `int` specifying the number of samples per mini-batch. - `n_iter`: An `int` specifying the number of iterations over the training dataset. # Output - Returns an `np.ndarray` of shape `(n_samples, n_components)` representing the learned features from the input data. # Constraints and Requirements - Ensure that the input data is binary (i.e., values are either 0 or 1). If not, raise a `ValueError` with an appropriate message. - Utilize `BernoulliRBM` from `sklearn.neural_network` for the implementation. - The function should handle different batch sizes, learning rates, and iterations. # Example Usage ```python import numpy as np data = np.array([ [0, 0, 1, 1], [1, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1] ]) learned_features = learn_rbm_features(data, n_components=2, learning_rate=0.05, batch_size=2, n_iter=20) print(learned_features) ``` This question requires students to comprehend the RBM model and how to apply it using scikit-learn\'s implementation, focusing on parameter tuning and understanding the feature learning process.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM def learn_rbm_features(data: np.ndarray, n_components: int = 256, learning_rate: float = 0.01, batch_size: int = 10, n_iter: int = 10) -> np.ndarray: Fits a Bernoulli Restricted Boltzmann Machine on the input data and returns the learned features. :param data: np.ndarray - Binary input data (each row represents a data sample) with shape (n_samples, n_features). :param n_components: int - Number of binary hidden units. :param learning_rate: float - The learning rate for weight updates. :param batch_size: int - The number of training samples per mini-batch. :param n_iter: int - Number of iterations over the training dataset. :return: np.ndarray - Transformed data using the learned components with shape (n_samples, n_components). # Check if the input data is binary if not np.all((data == 0) | (data == 1)): raise ValueError(\\"Input data must be binary (values should be 0 or 1).\\") # Initialize RBM rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter) # Fit the model to the data rbm.fit(data) # Transform the data transformed_data = rbm.transform(data) return transformed_data"},{"question":"# SQL Database Management System using `dbm` Problem Statement Design and implement a simple SQL-like database management system using the `dbm` package. Your task is to create functions that support basic SQL operations: `INSERT`, `SELECT`, `UPDATE`, and `DELETE` using the `dbm` database backend. The database should store the data in a `dbm` database file, and keys and values should be stored as bytes. You need to implement the following functions: 1. **create_database(file_name: str) -> None:** - Creates a new, empty database with the given filename. If the file already exists, it is overwritten. 2. **insert_record(file_name: str, key: str, value: str) -> None:** - Inserts a new record into the database with the given key-value pair. Both the key and value should be stored as bytes. 3. **select_record(file_name: str, key: str) -> str:** - Selects and returns the value corresponding to the given key from the database. If the key does not exist, return `\\"Key not found\\"`. 4. **update_record(file_name: str, key: str, value: str) -> None:** - Updates the value of an existing record with the given key. If the key does not exist, return `\\"Key not found\\"`. 5. **delete_record(file_name: str, key: str) -> None:** - Deletes the record corresponding to the given key from the database. If the key does not exist, return `\\"Key not found\\"`. Constraints - The database should be opened and closed properly to prevent data corruption. - Ensure keys and values are stored as bytes. - Implement proper error handling, such that attempting to delete or update a non-existent key should result in no operation (i.e., the database remains unchanged). Example Usage Below is an example of how the functions should be used: ```python # Create a new database create_database(\'test_db\') # Insert records insert_record(\'test_db\', \'user1\', \'John Doe\') insert_record(\'test_db\', \'user2\', \'Jane Doe\') # Select records print(select_record(\'test_db\', \'user1\')) # Output: John Doe print(select_record(\'test_db\', \'user3\')) # Output: Key not found # Update a record update_record(\'test_db\', \'user1\', \'John Smith\') print(select_record(\'test_db\', \'user1\')) # Output: John Smith # Delete a record delete_record(\'test_db\', \'user2\') print(select_record(\'test_db\', \'user2\')) # Output: Key not found ``` Notes - Use the `dbm.open()` function to handle the database opening, ensuring to select the \'c\' flag to create the database if it doesn\'t exist. - Remember to convert keys and values to bytes before inserting into the database and convert them back to strings where appropriate.","solution":"import dbm def create_database(file_name: str) -> None: Creates a new, empty database with the given filename. If the file already exists, it is overwritten. with dbm.open(file_name, \'n\') as db: pass # This creates a new empty database def insert_record(file_name: str, key: str, value: str) -> None: Inserts a new record into the database with the given key-value pair. with dbm.open(file_name, \'c\') as db: db[key.encode()] = value.encode() def select_record(file_name: str, key: str) -> str: Selects and returns the value corresponding to the given key from the database. with dbm.open(file_name, \'c\') as db: try: return db[key.encode()].decode() except KeyError: return \\"Key not found\\" def update_record(file_name: str, key: str, value: str) -> None: Updates the value of an existing record with the given key. with dbm.open(file_name, \'c\') as db: if key.encode() in db: db[key.encode()] = value.encode() else: return \\"Key not found\\" def delete_record(file_name: str, key: str) -> None: Deletes the record corresponding to the given key from the database. with dbm.open(file_name, \'c\') as db: if key.encode() in db: del db[key.encode()] else: return \\"Key not found\\""},{"question":"# Advanced Python: Custom Import Finder and Loader Background Python\'s `importlib` module provides mechanisms to customize how modules are imported. Developers can create custom importers by defining finders (objects that locate modules) and loaders (objects that load modules into memory). In this exercise, you will implement a custom import finder and loader to import modules from a specific directory. Task 1. **CustomFinder**: - Implement a class `CustomFinder` that will search for Python source files (`*.py`) in a specified directory. - The class should have the following methods: - `__init__(self, path: str)`: Takes the path of the directory where it will search for modules. - `find_spec(self, fullname: str, path: Optional[List[str]], target: Optional[ModuleType]) -> Optional[ModuleSpec]`: Searches for the module and returns a `ModuleSpec` if found. 2. **CustomLoader**: - Implement a class `CustomLoader` that will load the module from a specified file. - The class should have the following methods: - `__init__(self, filepath: str)`: Takes the path of the file to load. - `create_module(self, spec: ModuleSpec) -> Optional[ModuleType]`: (Optional) Used to create the module object. - `exec_module(self, module: ModuleType)`: Executes the module code in its namespace. 3. **Integration**: - Register the custom finder so that it can be used in the import system. - Ensure that attempting to import a module that resides in the specified directory uses your custom finder and loader. Implementation Skeleton ```python import importlib.abc import importlib.util import sys import os class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, path: str): self.path = path def find_spec(self, fullname: str, path: Optional[List[str]], target: Optional[ModuleType]) -> Optional[ModuleSpec]: # Implement this method pass class CustomLoader(importlib.abc.Loader): def __init__(self, filepath: str): self.filepath = filepath def create_module(self, spec: importlib.util.spec_from_file_location) -> Optional[ModuleType]: # Optionally implement this method or pass pass def exec_module(self, module: ModuleType): # Implement this method pass # Example usage (not part of your solution, but provided for context) # finder = CustomFinder(\'/path/to/directory\') # sys.meta_path.insert(0, finder) ``` Constraints - Do not use external libraries; only use the Python Standard Library. - Only handle Python source files (`*.py`). Example Suppose `/path/to/directory` contains a file `test_module.py` with the following content: ```python def hello(): return \\"Hello, world!\\" ``` Assuming the directory is correctly added to the import path using your custom finder: ```python import test_module print(test_module.hello()) # Should print \\"Hello, world!\\" ``` **Note**: Your solution should include thorough testing to verify that modules can be imported correctly from the specified directory.","solution":"import importlib.abc import importlib.util import sys import os from types import ModuleType from typing import Optional, List from importlib.machinery import ModuleSpec class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, path: str): self.path = path def find_spec(self, fullname: str, path: Optional[List[str]], target: Optional[ModuleType]) -> Optional[ModuleSpec]: module_name = fullname.split(\'.\')[-1] module_file = os.path.join(self.path, module_name + \'.py\') if os.path.isfile(module_file): return importlib.util.spec_from_file_location(fullname, module_file, loader=CustomLoader(module_file)) return None class CustomLoader(importlib.abc.Loader): def __init__(self, filepath: str): self.filepath = filepath def create_module(self, spec: ModuleSpec) -> Optional[ModuleType]: # Optionally implement this method or pass return None def exec_module(self, module: ModuleType): with open(self.filepath, \'r\') as file: code = file.read() exec(code, module.__dict__) # Register the custom finder def register_custom_finder(path: str): sys.meta_path.insert(0, CustomFinder(path))"},{"question":"**Coding Assessment Question:** **Objective:** The goal is to assess your ability to manipulate ZIP archives using the `zipfile` module in Python. You will create a ZIP archive with specific contents and extract files from it based on given constraints. **Problem:** You are given a directory containing multiple files and subdirectories. Your task is to create a ZIP archive named `my_archive.zip` containing all `.txt` files from the directory and its subdirectories. Additionally, you need to ensure that all files inside the archive are compressed using the `ZIP_DEFLATED` method. After creating the archive, you need to write a function that extracts only those files from the archive that were modified after January 1, 2020. The extracted files should be placed in a directory named `extracted_files`. If the directory does not exist, it should be created. **Function Signatures:** You need to implement the following two functions: 1. `create_zip(directory_path: str, zip_filename: str) -> None` - **Input:** - `directory_path`: The path to the directory containing the files and subdirectories. - `zip_filename`: The name of the ZIP archive to be created. - **Output:** - None - **Description:** - Creates a ZIP archive `zip_filename` containing all `.txt` files from the `directory_path` and its subdirectories. Use the `ZIP_DEFLATED` compression method. 2. `extract_files(zip_filename: str, extraction_directory: str, date_threshold: tuple) -> None` - **Input:** - `zip_filename`: The name of the ZIP archive from which files are to be extracted. - `extraction_directory`: The directory where the extracted files should be placed. - `date_threshold`: A tuple containing six fields (year, month, day, hour, minute, second) representing the date threshold. - **Output:** - None - **Description:** - Extracts files from the `zip_filename` that were modified after the provided `date_threshold` and places them in `extraction_directory`. **Constraints:** - Assume all filenames and directory paths contain valid characters and do not exceed file system limitations. - The date threshold is provided as a tuple in the format (2020, 1, 1, 0, 0, 0) corresponding to January 1, 2020, at 00:00:00. - You must handle any exceptions that may arise during file read/write operations. **Example Usage:** ```python # Example directory structure # /example_directory # ├── file1.txt # ├── file2.txt # ├── subdirectory # │ ├── file3.txt # │ └── file4.jpg # └── file5.doc # Create ZIP Archive create_zip(\\"example_directory\\", \\"my_archive.zip\\") # Extract files modified after January 1, 2020 extract_files(\\"my_archive.zip\\", \\"extracted_files\\", (2020, 1, 1, 0, 0, 0)) ``` Ensure your solution is efficient and follows best practices for handling file operations and exceptions.","solution":"import os import zipfile from datetime import datetime def create_zip(directory_path: str, zip_filename: str) -> None: Creates a ZIP archive `zip_filename` containing all `.txt` files from the `directory_path` and its subdirectories. Uses the `ZIP_DEFLATED` compression method. with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(directory_path): for file in files: if file.endswith(\'.txt\'): file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, directory_path) zipf.write(file_path, arcname) def extract_files(zip_filename: str, extraction_directory: str, date_threshold: tuple) -> None: Extracts files from the `zip_filename` that were modified after the provided `date_threshold` and places them in `extraction_directory`. if not os.path.exists(extraction_directory): os.makedirs(extraction_directory) date_threshold_dt = datetime(*date_threshold) with zipfile.ZipFile(zip_filename, \'r\') as zipf: for info in zipf.infolist(): # Convert time tuple to datetime object info_time = datetime(*info.date_time) if info_time > date_threshold_dt: zipf.extract(info, extraction_directory)"},{"question":"**Objective**: Implement a function that utilizes the `compileall` module to compile all Python files within a given directory with specific constraints. The function should allow for multi-threaded compilation, exclusion of certain files based on a pattern, and provide options for quiet mode and optimization levels. **Function Signature**: ```python def custom_compile(directory_path: str, exclude_pattern: str = None, max_workers: int = 1, quiet_mode: int = 0, optimization_levels: list = [-1]) -> bool: Compiles Python files within the given directory. Args: - directory_path (str): The directory path containing Python files to compile. - exclude_pattern (str, optional): A regex pattern to exclude specific files from compilation. - max_workers (int, optional): Number of worker threads to use for compilation. Defaults to 1. - quiet_mode (int, optional): Quiet mode level (0, 1, or 2). Defaults to 0. - optimization_levels (list, optional): List of optimization levels to use for compilation. Defaults to [-1]. Returns: - bool: True if all files compiled successfully, False otherwise. pass ``` **Constraints**: 1. The function should use the `compileall.compile_dir` function from the `compileall` module. 2. The function should allow for: - Multi-threaded compilation using the `workers` parameter. - Exclusion of files based on a provided regex pattern. - Control of output verbosity through the `quiet` parameter. - Custom optimization levels. 3. Handle invalid input cases (e.g., invalid directory path, invalid regex pattern) gracefully. 4. Return `True` if all files compiled successfully, `False` otherwise. **Example**: ```python # Example usage: print(custom_compile(\\"path/to/directory\\", exclude_pattern=r\'[/]test\', max_workers=4, quiet_mode=1, optimization_levels=[0, 1])) # Output: True (if all files compiled successfully) ``` **Notes**: - You may assume the provided directory path is valid and contains Python files. Focus on implementing the core functionality as prescribed.","solution":"import compileall import re import os from typing import List def custom_compile(directory_path: str, exclude_pattern: str = None, max_workers: int = 1, quiet_mode: int = 0, optimization_levels: List[int] = [-1]) -> bool: Compiles Python files within the given directory. Args: - directory_path (str): The directory path containing Python files to compile. - exclude_pattern (str, optional): A regex pattern to exclude specific files from compilation. - max_workers (int, optional): Number of worker threads to use for compilation. Defaults to 1. - quiet_mode (int, optional): Quiet mode level (0, 1, or 2). Defaults to 0. - optimization_levels (list, optional): List of optimization levels to use for compilation. Defaults to [-1]. Returns: - bool: True if all files compiled successfully, False otherwise. # Validate inputs if not os.path.isdir(directory_path): raise ValueError(f\\"Invalid directory path: {directory_path}\\") if exclude_pattern: try: re.compile(exclude_pattern) except re.error: raise ValueError(f\\"Invalid regex pattern: {exclude_pattern}\\") try: success = True for opt in optimization_levels: result = compileall.compile_dir( dir=directory_path, maxlevels=10, ddir=None, force=False, rx=re.compile(exclude_pattern) if exclude_pattern else None, quiet=quiet_mode, legacy=False, optimize=opt, workers=max_workers ) if not result: success = False return success except Exception as e: raise RuntimeError(f\\"An error occurred during compilation: {str(e)}\\")"},{"question":"**Problem: Build a Library Management System** You are tasked with implementing a simple Library Management System using Python. This system should manage books and members and include functionalities for borrowing and returning books. This problem will test your understanding of classes, data structures, control flow, and exception handling. **Requirements:** 1. **Class Definitions:** - **Book**: Represents a book in the library. - Attributes: `title` (str), `author` (str), `isbn` (str). - **Member**: Represents a library member. - Attributes: `name` (str), `member_id` (int), `borrowed_books` (list of `Book`). - **Library**: Manages the books and members. - Attributes: `books` (list of `Book`), `members` (list of `Member`). 2. **Functional Requirements:** - Add a new book to the library. - Register a new library member. - Borrow a book from the library. - A member can borrow a book if it is available. - Raise an exception if the book is not available. - Return a borrowed book to the library. - Raise an exception if the book was not borrowed by the member. - List all books in the library. - List all books currently borrowed by a specific member. **Input and Output Formats:** - Define functions for each of the functionalities stated above. - Function signatures and behavior: - `add_book(self, book: Book) -> None` - `register_member(self, member: Member) -> None` - `borrow_book(self, member_id: int, isbn: str) -> None` - Raises `ValueError` if the book is not available. - `return_book(self, member_id: int, isbn: str) -> None` - Raises `ValueError` if the book was not borrowed by the member. - `list_books(self) -> List[str]` - `list_member_books(self, member_id: int) -> List[str]` **Constraints:** - Each `Book` has a unique `isbn`. - Each `Member` has a unique `member_id`. - Books must be returned before being borrowed by another member. - The library system should handle up to 10,000 books and 1,000 members efficiently. **Example Usage:** ```python # Define some books book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"9780451524935\\") book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"9780060935467\\") # Define some members member1 = Member(name=\\"Alice Johnson\\", member_id=101) member2 = Member(name=\\"Bob Smith\\", member_id=102) # Create a library and add books and members library = Library() library.add_book(book1) library.add_book(book2) library.register_member(member1) library.register_member(member2) # Borrow and return books library.borrow_book(member_id=101, isbn=\\"9780451524935\\") print(library.list_member_books(member_id=101)) # Should print details of \\"1984\\" library.return_book(member_id=101, isbn=\\"9780451524935\\") print(library.list_books()) # Should list all books, including \\"1984\\" ``` Make sure to handle all edge cases and exceptions as specified in the requirements.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn class Member: def __init__(self, name, member_id): self.name = name self.member_id = member_id self.borrowed_books = [] class Library: def __init__(self): self.books = [] self.members = [] self.available_books = {} def add_book(self, book: Book) -> None: self.books.append(book) self.available_books[book.isbn] = book def register_member(self, member: Member) -> None: self.members.append(member) def borrow_book(self, member_id: int, isbn: str) -> None: member = self._find_member(member_id) book = self._find_available_book(isbn) if book: member.borrowed_books.append(book) del self.available_books[isbn] else: raise ValueError(\\"Book is not available\\") def return_book(self, member_id: int, isbn: str) -> None: member = self._find_member(member_id) book = self._find_borrowed_book(member, isbn) if book: member.borrowed_books.remove(book) self.available_books[isbn] = book else: raise ValueError(\\"Book was not borrowed by this member\\") def list_books(self) -> list: return [f\\"{book.title} by {book.author} (ISBN: {book.isbn})\\" for book in self.books] def list_member_books(self, member_id: int) -> list: member = self._find_member(member_id) return [f\\"{book.title} by {book.author} (ISBN: {book.isbn})\\" for book in member.borrowed_books] def _find_member(self, member_id: int) -> Member: for member in self.members: if member.member_id == member_id: return member raise ValueError(\\"Member not found\\") def _find_available_book(self, isbn: str) -> Book: if isbn in self.available_books: return self.available_books[isbn] return None def _find_borrowed_book(self, member: Member, isbn: str) -> Book: for book in member.borrowed_books: if book.isbn == isbn: return book return None"},{"question":"# AIFF Audio Manipulation and Export **Objective:** Write a Python function that reads an AIFF file, performs a simple audio transformation, and writes the modified audio data to a new AIFF file. **Function Signature:** ```python import aifc def transform_aiff(input_filepath: str, output_filepath: str): Reads an AIFF file, doubles the frame rate (speed up the audio), and writes the modified audio data to a new AIFF file. Args: input_filepath (str): The path to the input AIFF file. output_filepath (str): The path to the output AIFF file. Returns: None ``` # Steps: 1. Open the input AIFF file and read its parameters and frames. 2. Modify the audio data by doubling the frame rate. 3. Write the modified audio data to the output AIFF file with modified frame rate and same other parameters. # Constraints: - You may assume the input AIFF file follows standard specifications and is not corrupt. - The function should ensure the output AIFF file maintains the same audio quality (same number of channels, same sample width), but at double speed due to frame rate adjustment. # Example: Assuming an audio file located at \'input.aiff\' with stereo channels, a sample size of 2 bytes, and a frame rate of 44100 frames/second: ```python transform_aiff(\'input.aiff\', \'output.aiff\') ``` This would result in \'output.aiff\' having the same audio content as \'input.aiff\' but played back at double speed (88,200 frames/second). # Important Notes: - Make use of the `with` statement to ensure files are properly closed after operations. - Remember to adjust the number of frames appropriately after transforming the frame rate. **Hint:** To speed up the audio, you will essentially need to drop every alternate frame from the input. Implement this transformation and validate the functionality thoroughly by testing with various AIFF files.","solution":"import aifc def transform_aiff(input_filepath: str, output_filepath: str): Reads an AIFF file, doubles the frame rate (speed up the audio), and writes the modified audio data to a new AIFF file. Args: input_filepath (str): The path to the input AIFF file. output_filepath (str): The path to the output AIFF file. Returns: None with aifc.open(input_filepath, \'rb\') as input_file: n_channels = input_file.getnchannels() sample_width = input_file.getsampwidth() framerate = input_file.getframerate() n_frames = input_file.getnframes() comp_type = input_file.getcomptype() comp_name = input_file.getcompname() # Read audio frames audio_frames = input_file.readframes(n_frames) # Reduce the number of frames by half to simulate speeding up new_audio_frames = b\'\'.join([audio_frames[i:i + n_channels * sample_width] for i in range(0, len(audio_frames), 2 * n_channels * sample_width)]) with aifc.open(output_filepath, \'wb\') as output_file: output_file.setnchannels(n_channels) output_file.setsampwidth(sample_width) output_file.setframerate(framerate * 2) # Double the frame rate output_file.setnframes(len(new_audio_frames) // (n_channels * sample_width)) output_file.setcomptype(comp_type, comp_name) output_file.writeframes(new_audio_frames)"},{"question":"Objective: Demonstrate your understanding of the `importlib` module by programmatically importing a module and utilizing its functionalities. Problem Statement: You are tasked with creating a function that dynamically imports a specified module and calls a function within that module if it exists. If the function does not exist, it should raise an appropriate exception. Requirements: 1. Implement a function `dynamic_import_and_call(module_name: str, function_name: str, *args, **kwargs) -> Any` that: - Dynamically imports the module specified by `module_name`. - Calls the function specified by `function_name` within the module, passing any additional arguments (`*args`) and keyword arguments (`**kwargs`) to this function. - Returns the result of the function call. - Raises an `ImportError` if the module cannot be imported. - Raises an `AttributeError` if the function does not exist within the module. 2. Provide example usage demonstrating the function with a standard library module, e.g., using `math` module to call `sqrt`. Input: - `module_name` (str): The name of the module to import. - `function_name` (str): The name of the function within the module to call. - `*args`: Additional positional arguments to pass to the function. - `**kwargs`: Additional keyword arguments to pass to the function. Output: - Return value from the called function within the imported module. Constraints: - You should only use standard libraries available in Python 3.10. Example: ```python def dynamic_import_and_call(module_name: str, function_name: str, *args, **kwargs) -> Any: try: module = __import__(module_name) except ImportError: raise ImportError(f\\"Module {module_name} cannot be imported.\\") if not hasattr(module, function_name): raise AttributeError(f\\"Module {module_name} has no function {function_name}\\") func = getattr(module, function_name) return func(*args, **kwargs) # Example Usage result = dynamic_import_and_call(\'math\', \'sqrt\', 16) print(result) # Output should be 4.0 ``` Notes: - Make sure your solution handles edge cases, such as attempting to import non-existent modules or calling non-existent functions within existing modules.","solution":"import importlib def dynamic_import_and_call(module_name: str, function_name: str, *args, **kwargs): Dynamically imports a module and calls a function within that module if it exists. Parameters: - module_name (str): The name of the module to import. - function_name (str): The name of the function within the module to call. - *args: Additional positional arguments to pass to the function. - **kwargs: Additional keyword arguments to pass to the function. Returns: - The result of the function call. Raises: - ImportError: If the module cannot be imported. - AttributeError: If the function does not exist within the module. try: module = importlib.import_module(module_name) except ImportError: raise ImportError(f\\"Module {module_name} cannot be imported.\\") if not hasattr(module, function_name): raise AttributeError(f\\"Module {module_name} has no function {function_name}\\") func = getattr(module, function_name) return func(*args, **kwargs) # Example Usage # result = dynamic_import_and_call(\'math\', \'sqrt\', 16) # print(result) # Output should be 4.0"},{"question":"**Compression and Decompression Utility** You are tasked with creating a utility to handle data compression and decompression using the bzip2 algorithm. Your utility should support both one-shot and incremental processing as well as file-based operations. Implement the following functionalities: 1. **One-Shot Compression**: Write a function `one_shot_compress(data: bytes, compresslevel: int = 9) -> bytes` that compresses a given bytes-like object and returns the compressed data. 2. **One-Shot Decompression**: Write a function `one_shot_decompress(data: bytes) -> bytes` that decompresses a given bytes-like object and returns the original uncompressed data. 3. **Incremental Compression**: Write a class `IncrementalCompressor` which supports incremental compression. The class should have: - An `__init__` method which sets up the compressor. - A `compress(data: bytes) -> bytes` method to provide data to the compressor, returning compressed data chunks. - A `flush() -> bytes` method to finish the compression and return any leftover data in the compressor. 4. **Incremental Decompression**: Write a class `IncrementalDecompressor` which supports incremental decompression. The class should have: - An `__init__` method which sets up the decompressor. - A `decompress(data: bytes) -> bytes` method to provide data to the decompressor, returning decompressed data chunks. 5. **File Compression and Decompression**: Write two functions: - `compress_to_file(input_filename: str, output_filename: str, compresslevel: int = 9)` that compresses a file. - `decompress_from_file(input_filename: str, output_filename: str)` that decompresses a file. # Constraints - Compression level must be an integer between 1 and 9. - Input data for one-shot and incremental compression/decompression are bytes-like objects. - File operations need to ensure the data integrity of compressed and decompressed files. # Example Usage ```python # One-shot usage compressed_data = one_shot_compress(b\\"Example data for one-shot compression.\\") decompressed_data = one_shot_decompress(compressed_data) # Incremental usage compressor = IncrementalCompressor() compressed_chunk1 = compressor.compress(b\\"Chunk 1 of data.\\") compressed_chunk2 = compressor.compress(b\\"Chunk 2 of data.\\") final_chunk = compressor.flush() decompressor = IncrementalDecompressor() decompressed_chunk1 = decompressor.decompress(compressed_chunk1) decompressed_chunk2 = decompressor.decompress(compressed_chunk2 + final_chunk) # File operations compress_to_file(\\"input.txt\\", \\"compressed.bz2\\") decompress_from_file(\\"compressed.bz2\\", \\"decompressed.txt\\") ``` Your implementation will be evaluated based on correctness, efficiency, and adherence to the requirements. Ensure to handle edge cases and errors appropriately.","solution":"import bz2 def one_shot_compress(data: bytes, compresslevel: int = 9) -> bytes: Compresses the given data using the bzip2 algorithm with the specified compression level. if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be between 1 and 9\\") return bz2.compress(data, compresslevel) def one_shot_decompress(data: bytes) -> bytes: Decompresses the given data using the bzip2 algorithm. return bz2.decompress(data) class IncrementalCompressor: def __init__(self, compresslevel: int = 9): if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be between 1 and 9\\") self.compressor = bz2.BZ2Compressor(compresslevel) def compress(self, data: bytes) -> bytes: return self.compressor.compress(data) def flush(self) -> bytes: return self.compressor.flush() class IncrementalDecompressor: def __init__(self): self.decompressor = bz2.BZ2Decompressor() def decompress(self, data: bytes) -> bytes: return self.decompressor.decompress(data) def compress_to_file(input_filename: str, output_filename: str, compresslevel: int = 9): Compresses the content of the input file into the output file using bzip2 algorithm. if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be between 1 and 9\\") with open(input_filename, \'rb\') as infile: with bz2.open(output_filename, \'wb\', compresslevel=compresslevel) as outfile: outfile.write(infile.read()) def decompress_from_file(input_filename: str, output_filename: str): Decompresses the content of the input file into the output file using bzip2 algorithm. with bz2.open(input_filename, \'rb\') as infile: with open(output_filename, \'wb\') as outfile: outfile.write(infile.read())"},{"question":"**Problem Statement:** You are given a dataset that consists of images of handwritten digits. Your task is to apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset and visualize the dataset in 2D. You will also identify the number of components that capture at least 95% of the variance in the original dataset. **Instructions:** 1. Load the dataset and preprocess it if necessary. 2. Implement PCA to reduce the dimensionality of the data. 3. Find the minimum number of principal components that capture at least 95% of the variance. 4. Visualize the 2D representation of the dataset using the first two principal components. 5. Provide comments and explanations for each step in your implementation. **Requirements:** - You should use the scikit-learn library for PCA. - Ensure your code is efficient and well-documented. - The input data will be provided as a numpy array of shape `(n_samples, n_features)`. **Input:** - `X`: numpy array of shape `(n_samples, n_features)` representing the dataset. **Output:** - `n_components`: The minimum number of principal components that capture at least 95% of the variance. - A 2D scatter plot of the data represented by the first two principal components. **Performance Constraints:** - Ensure that your implementation can handle a dataset with up to 70000 samples and 784 features efficiently. **Sample Code:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_digits def apply_pca(X): # Implement PCA pca = PCA() pca.fit(X) # Find the number of components that capture 95% of the variance cumulative_variance = np.cumsum(pca.explained_variance_ratio_) n_components = np.argmax(cumulative_variance >= 0.95) + 1 # Reduce the dataset to 2D for visualization pca_2d = PCA(n_components=2) X_2d = pca_2d.fit_transform(X) # Plot the 2D representation plt.scatter(X_2d[:, 0], X_2d[:, 1], c=\'blue\', marker=\'o\', edgecolor=\'b\', s=40) plt.title(\\"2D PCA Visualization\\") plt.xlabel(\\"First Principal Component\\") plt.ylabel(\\"Second Principal Component\\") plt.show() return n_components # Example usage: # Load dataset (assuming it is the digits dataset) digits = load_digits() X = digits.data n_components = apply_pca(X) print(\\"Number of Components capturing 95% variance: \\", n_components) ``` **Note:** The `load_digits` dataset is used for example purposes. The actual dataset provided for the assessment may differ in format and size.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA def apply_pca(X): Applies PCA to reduce the dimensionality of dataset X and visualizes the first two principal components. Parameters: - X: numpy array of shape (n_samples, n_features) representing the dataset. Returns: - n_components: The minimum number of principal components that capture at least 95% of the variance. # Implement PCA pca = PCA() pca.fit(X) # Find the number of components that capture 95% of the variance cumulative_variance = np.cumsum(pca.explained_variance_ratio_) n_components = np.argmax(cumulative_variance >= 0.95) + 1 # Reduce the dataset to 2D for visualization pca_2d = PCA(n_components=2) X_2d = pca_2d.fit_transform(X) # Plot the 2D representation plt.scatter(X_2d[:, 0], X_2d[:, 1], c=\'blue\', marker=\'o\', edgecolor=\'b\', s=40) plt.title(\\"2D PCA Visualization\\") plt.xlabel(\\"First Principal Component\\") plt.ylabel(\\"Second Principal Component\\") plt.show() return n_components"},{"question":"**Question: Analyzing Model Performance with Seaborn Objects** You are given a dataset containing the performance scores of various machine learning models on different tasks. Your task is to load this dataset, process it, and visually analyze the models using seaborn\'s `objects` interface. Follow the steps below to complete the exercise. # Requirements: 1. **Load and Process the Dataset**: - Load the \\"glue\\" dataset from Seaborn. - Transform this dataset by pivoting it such that the index is a combination of \\"Model\\" and \\"Encoder\\", and columns are tasks with their respective \\"Score\\". - Add a new column \\"Average\\" which is the mean score across all tasks, rounded to one decimal place. - Sort the dataframe based on the \\"Average\\" scores in descending order. 2. **Create a Horizontal Bar Plot**: - Using the transformed dataset, create a horizontal bar plot with \\"Model\\" on the y-axis and \\"Average\\" score on the x-axis. - Add text annotations to the plot showing the average score on each bar, with white text color and right alignment. 3. **Create a Scatter Plot**: - Create a scatter plot with \\"SST-2\\" scores on the x-axis and \\"MRPC\\" scores on the y-axis. - Color the dots by the \\"Encoder\\" type. - Add text annotations displaying the \\"Model\\" above each dot. - Align text annotations based on the \\"Encoder\\" type, with LSTM models aligned to the left and Transformer models aligned to the right. 4. **Enhance the Appearance**: - Use appropriate matplotlib parameters to make the text bold in the scatter plot. # Input Format: - None (You will use the seaborn\'s internal dataset \\"glue\\"). # Output Format: - Display two plots as specified above using seaborn. # Example Code: ```python # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and process the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Step 2: Create a horizontal bar plot ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\")) .show() ) # Step 3: Create a scatter plot ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) .show() ) # Step 4: Enhance appearance with bold text in scatter plot ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) .show() ) ``` Ensure the plots match the descriptions and fulfill all requirements. Submit the notebook containing your solution.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from seaborn import objects as so # Step 1: Load and process the dataset glue = ( sns.load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Step 2: Create a horizontal bar plot bar_plot = ( so.Plot(glue.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\")) ) # Step 3: Create a scatter plot scatter_plot = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) # Step 4: Enhance appearance with bold text in scatter plot scatter_plot_bold = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) # Display the plots bar_plot.show() scatter_plot.show() scatter_plot_bold.show()"},{"question":"**Objective**: Demonstrate your understanding and ability to use the `compileall` module for compiling Python source files in a directory tree with various custom options. # Problem Statement Design and implement a Python function named `custom_compile` that uses the `compileall` module to compile Python source files in a given directory tree. The function should allow for several customizations, including recursiveness, verbosity, optimization level, and the ability to use multiple workers. # Function Signature ```python def custom_compile(directory: str, recursive: bool = True, verbose: int = 0, optimize: [int, list] = -1, workers: int = 1) -> bool: Compiles Python source files in a directory tree with custom options. Parameters: - directory (str): The root directory path containing the Python source files to be compiled. - recursive (bool): If True, compile files recursively in the directory tree. Defaults to True. - verbose (int): Verbosity level (0 = all output, 1 = errors only, 2 = no output). Defaults to 0. - optimize (int or list): The optimization level(s) to use. Can be a single integer or a list of integers. Defaults to -1 (no optimization). - workers (int): Number of workers to use for compilation. Defaults to 1. Returns: - bool: True if all files compiled successfully, False otherwise. pass ``` # Constraints 1. **directory**: Must be a valid directory path. 2. **recursive**: Controls whether the compilation should process subdirectories. 3. **verbose**: Integer value controlling the verbosity of compilation output (0 = all output, 1 = errors only, 2 = no output). 4. **optimize**: Accepts both a single integer or a list of integers to specify different optimization levels. 5. **workers**: Specifies the number of threads to be used for compiling files in the directory. If `workers` is 0, the number of CPU cores should be used. # Example Usage ```python # Compile all Python files in the \'src\' directory, non-recursively, with verbose level 1, optimization level 2, and 4 workers. compiled_successfully = custom_compile(\'src\', recursive=False, verbose=1, optimize=2, workers=4) print(compiled_successfully) # Output: True if all files compiled successfully, otherwise False. ``` # Requirements - You must use `compileall.compile_dir` to perform the compilation. - Ensure the function handles and reports errors appropriately given the verbosity level. - If the `optimize` parameter is a list, compile files for each optimization level provided. You are encouraged to read the `compileall` documentation provided for better insights on the usage of the module and its functions.","solution":"import compileall import os def custom_compile(directory: str, recursive: bool = True, verbose: int = 0, optimize = -1, workers: int = 1) -> bool: Compiles Python source files in a directory tree with custom options. Parameters: - directory (str): The root directory path containing Python source files to be compiled. - recursive (bool): If True, compile files recursively in the directory tree. Defaults to True. - verbose (int): Verbosity level (0 = all output, 1 = errors only, 2 = no output). Defaults to 0. - optimize (int or list): The optimization level(s) to use. Can be a single integer or a list of integers. Defaults to -1 (no optimization). - workers (int): Number of workers to use for compilation. Defaults to 1. Returns: - bool: True if all files compiled successfully, False otherwise. if not os.path.isdir(directory): raise ValueError(f\\"The provided directory \'{directory}\' is not valid.\\") optimize_levels = optimize if isinstance(optimize, list) else [optimize] success = True for level in optimize_levels: current_success = compileall.compile_dir( directory, maxlevels=10 if recursive else 0, quiet=3 - verbose, optimize=level, workers=workers ) if not current_success: success = False return success"},{"question":"**Objective:** Write a Python function that reads all lines from a given file, finds lines that contain a given substring, and returns these lines. Use the `linecache` module to optimize your solution. **Function Signature:** ```python def find_lines_with_substring(filename: str, substring: str) -> list: ``` **Input:** - `filename` (str): The path to the file to be searched. - `substring` (str): The substring to search for within the lines of the file. **Output:** - Returns a list of strings, where each string is a line from the file that contains the given substring. Each line should contain its original newline character. **Constraints:** - Do not load the entire file into memory at once. - Use the `linecache` module to read lines from the file efficiently. - Handle potential errors, such as the file not existing, by returning an empty list in such cases. **Example:** ```python # Assume \'example.txt\' contains the following lines: # Hello World # This is a test file. # Let\'s test the linecache module. # Have a nice day. lines = find_lines_with_substring(\'example.txt\', \'test\') print(lines) # Output should be: # [\\"This is a test file.n\\", \\"Let\'s test the linecache module.n\\"] lines = find_lines_with_substring(\'nonexistent.txt\', \'test\') print(lines) # Output should be: # [] ``` **Performance Considerations:** - The function should be efficient and quick to respond, even for large files. - Make sure to make use of caching features provided by the `linecache` module to avoid repeatedly reading from the file. **Notes:** - You may assume that `filename` is a valid string and `substring` is a non-empty string. - Test your function with edge cases, such as empty files, files with multiple matching lines, and files without any matching lines.","solution":"import linecache def find_lines_with_substring(filename: str, substring: str) -> list: Reads all lines from the given file and returns a list of lines that contain the given substring. Parameters: filename (str): The path to the file to be searched. substring (str): The substring to search for within the lines of the file. Returns: list: A list of strings, each string being a line from the file that contains the given substring. Each line should include its original newline character. matching_lines = [] try: with open(filename, \'r\') as file: for line_number in range(1, len(file.readlines()) + 1): line = linecache.getline(filename, line_number) if substring in line: matching_lines.append(line) return matching_lines except FileNotFoundError: return []"},{"question":"Objective You are required to leverage the `ipaddress` module to implement a function that analyzes a list of mixed IP addresses (both IPv4 and IPv6) and IP networks, classifies them, and performs various operations based on their types. Task Write a function `analyze_ips_and_networks(ip_list)` that takes a list of strings representing IP addresses and IP networks. The function should perform the following tasks: 1. **Classification:** - Classify each entry in the list as either an IP address or an IP network. 2. **Validation:** - Ensure that each entry is valid and raise appropriate exceptions for invalid entries during classification. 3. **Network Analysis (for entries classified as networks):** - Calculate the number of usable host addresses within each network. - Identify the first and last usable IP addresses within each network. 4. **Address Analysis (for entries classified as addresses):** - Determine whether the address belongs to any of the networks in the list. 5. **Result Compilation:** - Compile the results in a dictionary with the following structure: ```python { \\"addresses\\": { \\"valid\\": [list of valid IP addresses], \\"invalid\\": [list of invalid IP address entries] }, \\"networks\\": { \\"valid\\": { \\"network\\": [details about the valid IP networks] }, \\"invalid\\": [list of invalid IP network entries] } } ``` For valid IP networks, provide a detailed entry structure: ```python { \\"network\\": { \\"num_addresses\\": <number_of_addresses>, \\"first_host\\": <first_ip_address>, \\"last_host\\": <last_ip_address> } } ``` Input Format - `ip_list`: A list of strings where each string is either an IP address or an IP network in CIDR notation. Output Format - A dictionary representing the classification, validity, and detailed analysis of the IP addresses and IP networks. Constraints - The IP addresses and networks can be either IPv4 or IPv6. - Assume a maximum of 100 entries in the `ip_list`. - Ensure your function handles both IPv4 and IPv6 formats correctly. Example ```python input_list = [ \\"192.168.1.1\\", \\"2001:db8::\\", \\"192.168.0.0/24\\", \\"10.0.0.1/8\\", \\"invalid_ip\\", \\"300.168.1.0/24\\", \\"2001:db8::/96\\" ] output = analyze_ips_and_networks(input_list) print(output) ``` Expected Output (Structure, values may vary): ```python { \\"addresses\\": { \\"valid\\": [\\"192.168.1.1\\", \\"2001:db8::\\"], \\"invalid\\": [\\"invalid_ip\\"] }, \\"networks\\": { \\"valid\\": [ { \\"network\\": \\"192.168.0.0/24\\", \\"num_addresses\\": 256, \\"first_host\\": \\"192.168.0.1\\", \\"last_host\\": \\"192.168.0.254\\" }, { \\"network\\": \\"2001:db8::/96\\", \\"num_addresses\\": 4294967296, \\"first_host\\": \\"2001:db8::1\\", \\"last_host\\": \\"2001:db8::ffff:fffe\\" } ], \\"invalid\\": [\\"300.168.1.0/24\\"] } } ``` Notes - Use `ipaddress.ip_address()` and `ipaddress.ip_network()` to validate and classify the entries. - Handle exceptions where necessary to flag invalid entries. - Utilize methods and properties provided by the `ipaddress` module to extract network and host details.","solution":"import ipaddress def analyze_ips_and_networks(ip_list): result = { \\"addresses\\": { \\"valid\\": [], \\"invalid\\": [] }, \\"networks\\": { \\"valid\\": [], \\"invalid\\": [] } } for entry in ip_list: try: # Check if it is a valid IP address address = ipaddress.ip_address(entry) result[\\"addresses\\"][\\"valid\\"].append(entry) except ValueError: try: # Check if it is a valid IP network network = ipaddress.ip_network(entry, strict=False) num_addresses = network.num_addresses first_host = None last_host = None # Calculate first and last usable hosts for networks with hosts if num_addresses > 2: first_host = str(network.network_address + 1) last_host = str(network.broadcast_address - 1) elif num_addresses == 2: first_host = str(network.network_address + 1) last_host = str(network.network_address + 1) else: # num_addresses == 1 first_host = str(network.network_address) last_host = str(network.network_address) result[\\"networks\\"][\\"valid\\"].append({ \\"network\\": str(network), \\"num_addresses\\": num_addresses, \\"first_host\\": first_host, \\"last_host\\": last_host }) except ValueError: # Invalid entry if \'/\' in entry: result[\\"networks\\"][\\"invalid\\"].append(entry) else: result[\\"addresses\\"][\\"invalid\\"].append(entry) return result"},{"question":"# Custom `asyncio` Event Loop Policy Implementation **Objective:** Your task is to implement a custom event loop policy by subclassing `DefaultEventLoopPolicy` from the `asyncio` module. This custom policy should override the `new_event_loop` method to create a customized event loop and demonstrate its usage. This exercise will test your understanding of subclassing, method overriding, and the `asyncio` module in Python. **Details:** 1. Define a new class `MyCustomEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. 2. Override the `new_event_loop` method in `MyCustomEventLoopPolicy` to create and return an event loop with some custom behavior: - For demonstration purposes, override the event loop\'s `run_forever` method to print a message `\\"Custom event loop is running\\"` each time it is called. 3. Implement a simple **asynchronous** function `hello` that prints `\\"Hello, asyncio!\\"` and runs within the custom event loop. 4. Create and set an instance of `MyCustomEventLoopPolicy` as the policy for the current process. 5. Use the custom policy to create a new event loop and run the `hello` function within it. **Constraints:** - Use only the standard `asyncio` module. - Ensure backward compatibility and handle any potential exceptions or edge cases. - The overridden `run_forever` method should retain its original functionality along with the custom behavior. **Input and Output:** No input from the user. Output should demonstrate the creation and execution of a custom event loop. Expected output includes the custom message from the overridden `run_forever` method and the print statement from the `hello` function. **Example Execution:** ```python # Setup and run the custom event loop policy async def main(): # Create instance of MyCustomEventLoopPolicy and set it policy = MyCustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) # Create a new event loop and set it loop = asyncio.get_event_loop_policy().new_event_loop() asyncio.set_event_loop(loop) # Run the hello function within the custom event loop loop.run_until_complete(hello()) class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): loop = super().new_event_loop() original_run_forever = loop.run_forever def custom_run_forever(): print(\\"Custom event loop is running\\") original_run_forever() loop.run_forever = custom_run_forever return loop async def hello(): print(\\"Hello, asyncio!\\") if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Note:** The above example sets up and demonstrates the use of the custom event loop policy. The actual solution should follow the same structure but may include additional error handling and necessary comments.","solution":"import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): loop = super().new_event_loop() original_run_forever = loop.run_forever def custom_run_forever(): print(\\"Custom event loop is running\\") original_run_forever() loop.run_forever = custom_run_forever return loop async def hello(): print(\\"Hello, asyncio!\\") async def main(): # Create instance of MyCustomEventLoopPolicy and set it as the event loop policy policy = MyCustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) # Create a new event loop and set it as the current event loop loop = asyncio.get_event_loop_policy().new_event_loop() asyncio.set_event_loop(loop) # Run the hello function within the custom event loop await hello() loop.run_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Coding Assessment Question: File System Path Utilities** **Objective:** In this assessment, you are required to implement a function that mimics the behavior of the `PyOS_FSPath` function in Python. The function should take a path argument and return its file system representation. If the path is a `str` or `bytes` object, it will return the same path. If the path implements the `os.PathLike` interface, it should return the result of calling its `__fspath__` method. Otherwise, it should raise a `TypeError`. **Function Signature:** ```python def fs_path(path: Any) -> Union[str, bytes]: pass ``` **Input:** - `path`: An object that represents a file system path. It can be a `str`, `bytes`, or an object that implements the `os.PathLike` interface. **Output:** - The function should return the file system representation of `path` as a `str` or `bytes`. **Constraints:** - The function must correctly handle all inputs as described. - You are not allowed to use the built-in `os.fspath` function. - Raise a `TypeError` if the input does not match the expected types or interfaces. **Example:** ```python class CustomPath: def __fspath__(self): return \\"/custom/path\\" # Example 1: String input print(fs_path(\\"/example/path\\")) # Output: \\"/example/path\\" # Example 2: Bytes input print(fs_path(b\\"/example/path\\")) # Output: b\\"/example/path\\" # Example 3: os.PathLike input print(fs_path(CustomPath())) # Output: \\"/custom/path\\" # Example 4: Invalid input print(fs_path(12345)) # Raises TypeError ``` **Notes:** - Ensure that your implementation is efficient and handles edge cases appropriately. - Include appropriate test cases to validate your solution. **Hint:** - You may find it useful to implement a helper function to check if an object implements the `os.PathLike` interface. Good luck and happy coding!","solution":"import os from typing import Any, Union def fs_path(path: Any) -> Union[str, bytes]: Mimic the behavior of the PyOS_FSPath function. If the path is a str or bytes object, return the same path. If the path implements the os.PathLike interface, return the result of its __fspath__ method. Otherwise, raise a TypeError. if isinstance(path, (str, bytes)): return path elif hasattr(path, \'__fspath__\'): return path.__fspath__() else: raise TypeError(\\"path should be of type str, bytes, or os.PathLike\\")"},{"question":"Background The `asyncio` module in Python provides a framework for writing single-threaded, concurrent code using the `async/await` syntax. This module provides a way to work with event loops, which handle the execution of asynchronous tasks and IO operations. Task Implement a function `fetch_webpage_urls()` that takes a list of URLs and fetches their content concurrently using the `asyncio` module. Your implementation should: 1. Use the appropriate event loop based on the operating system. 2. Ensure compatibility with both Windows and macOS. 3. Handle possible limitations regarding subprocess execution and file I/O, as mentioned in the platform-specific details. 4. Fetch and return the content of each URL as a dictionary, where the key is the URL and the value is the content. Function Signature ```python import asyncio async def fetch_webpage_urls(urls: list) -> dict: pass ``` Input - `urls`: A list of strings, where each string is a URL to be fetched. Output - Return a dictionary where each key is a URL from the input list, and each value is the fetched content of that URL. Constraints - The function should be able to handle at least 10 URLs within a reasonable time frame. - If an URL cannot be fetched, set its value to `None` in the output dictionary. - You can assume all URLs are well-formed and reachable. - The solution should demonstrate proper understanding and handling of platform-specific constraints and distinctions in the `asyncio` module. Example ```python import asyncio async def fetch_webpage_urls(urls): # Your implementation here # Example usage urls = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] content = asyncio.run(fetch_webpage_urls(urls)) # Running the coroutine print(content) ``` **Note:** You might want to utilize libraries like `aiohttp` for asynchronous HTTP requests and ensure proper exception handling for robust solutions. Requirements - Ensure you have a working asynchronous solution using proper event loop handling for Windows and macOS. - Write clean, readable code with appropriate comments explaining key parts.","solution":"import asyncio import aiohttp import platform async def fetch_url(session, url): try: async with session.get(url) as response: return await response.text() except: return None async def fetch_webpage_urls(urls: list) -> dict: async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] contents = await asyncio.gather(*tasks) return dict(zip(urls, contents)) if platform.system() == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy()) async def main(urls): content = await fetch_webpage_urls(urls) return content"},{"question":"**Title**: Working with the `PySlice` Module **Objective**: Assess the students\' ability to utilize and manipulate slice objects in Python using the `PySlice` module from python310. **Task Description**: You are given a task to implement a function `extract_sublist_with_slice` that can extract a sublist from a given list using the slice parameters defined in a custom slice-like object (similar to the `slice` type in Python). Here\'s the description of how this function should work: 1. You will create a function `extract_sublist_with_slice` that takes two parameters: - `sequence`: A list of integers. - `slice_params`: A dictionary that includes the keys `start`, `stop`, and `step`. These values can be `None` or integers. 2. Your task is to create a new slice object using the provided `slice_params` and then extract the corresponding sublist from the `sequence` using this slice object. **Function Signature**: ```python def extract_sublist_with_slice(sequence: list[int], slice_params: dict[str, int | None]) -> list[int]: pass ``` **Input**: - `sequence`: A list of integers. (1 <= len(sequence) <= 10^5) - `slice_params`: A dictionary containing `start`, `stop`, and `step` keys. Values for these keys may be `None` or integers. **Output**: - A list of integers extracted from `sequence` according to the slice object created using `slice_params`. **Example**: ```python # Example 1 sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] slice_params = {\'start\': 2, \'stop\': 8, \'step\': 2} print(extract_sublist_with_slice(sequence, slice_params)) # Output: [3, 5, 7] # Example 2 sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] slice_params = {\'start\': None, \'stop\': None, \'step\': -1} print(extract_sublist_with_slice(sequence, slice_params)) # Output: [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] ``` **Constraints**: - You should not use the built-in `slice` type directly; instead, you should follow the method to create and manipulate slices using the provided `PySlice` functionalities. - The function should handle edge cases where start, stop, or step is `None`. # Notes: - The expected performance should be optimal to handle the upper limit of the sequence length efficiently.","solution":"def extract_sublist_with_slice(sequence: list[int], slice_params: dict[str, int | None]) -> list[int]: Extracts a sublist from the given list using slice parameters. Args: - sequence (list[int]): The list of integers to slice. - slice_params (dict[str, int | None]): The dictionary containing the slice parameters with keys \'start\', \'stop\', and \'step\'. Returns: - list[int]: The sublist extracted using the provided slice parameters. start = slice_params.get(\'start\', None) stop = slice_params.get(\'stop\', None) step = slice_params.get(\'step\', None) slicing_params = slice(start, stop, step) return sequence[slicing_params]"},{"question":"**Objective**: Implement a custom codec registration, encoding, decoding, and error handling in Python 3.10. Background: You are working on a text processing system that deals with various encodings. Your task is to: 1. Register a custom codec. 2. Encode and decode data using the custom codec. 3. Implement a custom error handling strategy for encoding errors. Requirements: 1. **Custom Codec**: Create a custom codec that can encode and decode strings by reversing them. 2. **Custom Error Handling**: Define a custom error handling function that replaces any problematic character with a string \\"[ERROR]\\". 3. **Function Implementations**: - `register_custom_codec()`: Register the custom codec for an encoding named `\\"reverse\\"`. - `encode_with_custom_codec(data: str) -> str`: Encode the input string `data` using the `\\"reverse\\"` encoding. - `decode_with_custom_codec(encoded_data: str) -> str`: Decode the input string `encoded_data` using the `\\"reverse\\"` encoding. - `custom_error_handler(exception)`: Define the custom error handling function which is used during encoding to replace problematic characters with \\"[ERROR]\\". Constraints: - Use the provided Python C-API equivalents in pure Python where appropriate. - Handle any exceptions that occur during the encoding/decoding processes and use the custom error handler to replace problematic sequences. - Assume the standard library encodings are pre-registered. Input/Output Specifications: - `register_custom_codec()` has no input or output. - `encode_with_custom_codec(data: str) -> str`: - **Input**: `data` (string), the text to be encoded. - **Output**: Encoded string. - `decode_with_custom_codec(encoded_data: str) -> str`: - **Input**: `encoded_data` (string), the text to be decoded. - **Output**: Decoded string. - `custom_error_handler(exception)`: - **Input**: `exception` (Exception instance), the exception encountered during encoding. - **Output**: Two-item tuple containing the replacement string and the offset for the next operation. Example: ```python # Register the custom codec register_custom_codec() # Encoding with the custom codec input_data = \\"hello world\\" encoded_data = encode_with_custom_codec(input_data) print(encoded_data) # Output: \\"dlrow olleh\\" # Decoding with the custom codec decoded_data = decode_with_custom_codec(encoded_data) print(decoded_data) # Output: \\"hello world\\" # Demonstrate encoding with an error (assuming an invalid character operation) try: problematic_data = \\"hello udc00 world\\" # This string contains a lone surrogate character encoded_data_with_error = encode_with_custom_codec(problematic_data) except Exception as e: print(custom_error_handler(e)) # Output: (\\"[ERROR]\\", offset) ``` Implement the required functions in the provided spaces below: ```python # Import necessary modules import codecs # Define the custom codec functions def reverse_encode(input, errors=\'strict\'): return input[::-1].encode(\'utf-8\'), len(input) def reverse_decode(input, errors=\'strict\'): return input.decode(\'utf-8\')[::-1], len(input) # Register the codec search functions def register_custom_codec(): # Define a search function that returns the custom codec functions def find_reverse_codec(name): if name == \'reverse\': return codecs.CodecInfo( name=\'reverse\', encode=reverse_encode, decode=reverse_decode, incrementalencoder=None, incrementaldecoder=None, streamreader=None, streamwriter=None, ) return None # Register the search function codecs.register(find_reverse_codec) # Encode using the custom codec def encode_with_custom_codec(data: str) -> str: return data.encode(\'reverse\').decode(\'utf-8\') # Decode using the custom codec def decode_with_custom_codec(encoded_data: str) -> str: return encoded_data.encode(\'utf-8\').decode(\'reverse\') # Custom error handling function def custom_error_handler(exception): return \\"[ERROR]\\", exception.end # Example usage if __name__ == \\"__main__\\": register_custom_codec() try: encoded_data = encode_with_custom_codec(\\"hello udc00 world\\") print(encoded_data) except Exception as exc: print(custom_error_handler(exc)) ```","solution":"import codecs # Define the custom codec functions def reverse_encode(input, errors=\'strict\'): return input[::-1], len(input) def reverse_decode(input, errors=\'strict\'): return input[::-1], len(input) # Register the codec search functions def register_custom_codec(): # Define a search function that returns the custom codec functions def find_reverse_codec(name): if name == \'reverse\': return codecs.CodecInfo( name=\'reverse\', encode=reverse_encode, decode=reverse_decode, incrementalencoder=None, incrementaldecoder=None, streamreader=None, streamwriter=None, ) return None # Register the search function codecs.register(find_reverse_codec) # Encode using the custom codec def encode_with_custom_codec(data: str) -> str: return codecs.encode(data, \'reverse\') # Decode using the custom codec def decode_with_custom_codec(encoded_data: str) -> str: return codecs.decode(encoded_data, \'reverse\') # Custom error handling function def custom_error_handler(exception): return \\"[ERROR]\\", exception.end"},{"question":"# PyTorch Coding Assessment Question Objective In this task, you are required to implement functionality that makes use of PyTorch\'s CUDA memory debugging tools. The task will involve running a specific sequence of operations on a CUDA-enabled tensor, capturing memory allocation history, and saving a memory snapshot for analysis. Problem Statement 1. Write a function `observe_cuda_memory` that takes a function `fn` and its arguments `*args` as input. The function `observe_cuda_memory` should execute `fn` with the provided `args` while recording CUDA memory allocations. 2. Your function should save a memory snapshot in the working directory named `\\"cuda_memory_snapshot.pickle\\"`. Requirements - The memory history recording must start before `fn` is executed and stopped afterward. - Save the memory snapshot using the filename `\\"cuda_memory_snapshot.pickle\\"`. Input - `fn` (Callable): A function performing operations that involve CUDA tensors. - `*args` (Any): Arguments to be passed to `fn`. Output - No return value is expected from `observe_cuda_memory`. The result will be a saved snapshot file named `\\"cuda_memory_snapshot.pickle\\"`. Example Usage ```python import torch def sample_operation(): x = torch.rand((1000, 1000), device=\\"cuda\\") y = torch.rand((1000, 1000), device=\\"cuda\\") z = torch.matmul(x, y) return z observe_cuda_memory(sample_operation) # This should create a file `cuda_memory_snapshot.pickle` in the working directory ``` Constraints - Ensure that the function `fn` executes without any modifications to its implementation. - Assume the runtime environment has a CUDA-enabled GPU and the necessary CUDA setup.","solution":"import torch import pickle from torch.cuda import memory_snapshot, reset_peak_memory_stats from typing import Callable def observe_cuda_memory(fn: Callable, *args): Observes CUDA memory allocations made by function `fn` and saves a memory snapshot. Parameters: fn (Callable): A function that uses CUDA tensors. *args (Any): Arguments to be passed to `fn`. No return value. A memory snapshot is saved as \'cuda_memory_snapshot.pickle\'. # Reset peak memory stats to start recording fresh reset_peak_memory_stats() # Execute the function with its arguments fn(*args) # Capture a memory snapshot snapshot = memory_snapshot() # Save the snapshot to a file with open(\\"cuda_memory_snapshot.pickle\\", \\"wb\\") as f: pickle.dump(snapshot, f)"},{"question":"# File Type Analysis in a Directory You are tasked with creating a Python function that analyzes the types of files in a given directory and produces a report detailing the various file types and their count. The function should use the `stat` module to determine the file types. Function Signature ```python def analyze_directory(directory_path: str) -> dict: pass ``` Input - `directory_path` (str): The path to the directory to analyze. Output - A dictionary where the keys are string representations of the file types and the values are integers representing the count of each file type found in the directory and its subdirectories. Constraints - You are not allowed to use `os.path.is*()` family of functions to determine the file type. - You should handle cases where the directory might contain nested subdirectories. - If the specified directory does not exist, raise a `FileNotFoundError`. File Type Keys The dictionary should include the following keys: - `\'directory\'` - `\'regular file\'` - `\'character special device\'` - `\'block special device\'` - `\'named pipe\'` - `\'symbolic link\'` - `\'socket\'` - `\'door\'` (if supported by the platform) - `\'event port\'` (if supported by the platform) - `\'whiteout\'` (if supported by the platform) Example ```python import os # Assuming the following structure in \'/example\': # /example # ├── dir1 # │ └── file3.txt # ├── file1.txt # ├── file2.txt # └── link -> file1.txt (symbolic link) result = analyze_directory(\'/example\') print(result) # Output might be (depending on actual file types and system support): # {\'directory\': 1, \'regular file\': 3, \'character special device\': 0, \'block special device\': 0, # \'named pipe\': 0, \'symbolic link\': 1, \'socket\': 0, \'door\': 0, \'event port\': 0, \'whiteout\': 0} ``` The function should traverse the specified directory and its subdirectories, determine the type of each file using the `stat` module functions, and populate the dictionary with the count of each file type. Ensure you account for platform-specific file types appropriately.","solution":"import os import stat def analyze_directory(directory_path: str) -> dict: # Dictionary to hold counts of each file type file_type_counts = { \'directory\': 0, \'regular file\': 0, \'character special device\': 0, \'block special device\': 0, \'named pipe\': 0, \'symbolic link\': 0, \'socket\': 0, \'door\': 0, \'event port\': 0, \'whiteout\': 0 } if not os.path.exists(directory_path): raise FileNotFoundError(f\\"The directory {directory_path} does not exist.\\") for root, dirs, files in os.walk(directory_path): for name in dirs + files: path = os.path.join(root, name) try: st = os.lstat(path) mode = st.st_mode if stat.S_ISDIR(mode): file_type_counts[\'directory\'] += 1 elif stat.S_ISREG(mode): file_type_counts[\'regular file\'] += 1 elif stat.S_ISCHR(mode): file_type_counts[\'character special device\'] += 1 elif stat.S_ISBLK(mode): file_type_counts[\'block special device\'] += 1 elif stat.S_ISFIFO(mode): file_type_counts[\'named pipe\'] += 1 elif stat.S_ISLNK(mode): file_type_counts[\'symbolic link\'] += 1 elif stat.S_ISSOCK(mode): file_type_counts[\'socket\'] += 1 elif hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): file_type_counts[\'door\'] += 1 elif hasattr(stat, \'S_ISEVPORT\') and stat.S_ISEVPORT(mode): file_type_counts[\'event port\'] += 1 elif hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): file_type_counts[\'whiteout\'] += 1 except FileNotFoundError: # In case files or directories are removed during the walk continue return file_type_counts"},{"question":"# Pandas Assessment Question **Objective:** Demonstrate your understanding of pandas by manipulating dataframe structures, performing data operations, handling missing values, merging dataframes, and visualizing the output. Task: You are provided with data on the sales performance of a company. This data has two files each containing sales data of different regions. Your task is to: 1. Load the data into pandas dataframes. 2. Clean and preprocess the data. 3. Perform data manipulation operations. 4. Create a consolidated report. 5. Visualize the results. Steps: 1. **Import the necessary libraries.** ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt ``` 2. **Load the Data:** Read sales data from two CSV files. The files are named `sales_region1.csv` and `sales_region2.csv`. ```python sales_region1 = pd.read_csv(\'sales_region1.csv\') sales_region2 = pd.read_csv(\'sales_region2.csv\') ``` 3. **Clean and preprocess the data:** - Check the data for any missing values and handle them appropriately. - Make sure the data types of all columns are appropriate for analysis. - Add a column to each dataframe indicating the region (`Region1` for the first dataframe, `Region2` for the second dataframe). 4. **Merge the Dataframes:** Merge the two dataframes on the common columns. 5. **Data Manipulation:** - Group by \'Product\' and \'Region\' and calculate the sum of \'Sales\' and \'Profit\' for each group. - Calculate the mean \'Sales\' and \'Profit\' for each product. 6. **Handling Missing Data:** - If there are any missing values in the resulting dataframe, fill the missing \'Sales\' and \'Profit\' values with the mean of their respective columns. 7. **Visualize the Results:** - Plot a bar graph showing total sales by region. - Plot a line graph showing the profit trend for each product over time. Input File Format: 1. The CSV files have the following columns: `Product`, `Sales`, `Profit`, and `Date`. Expected Output: 1. A cleaned and merged dataframe with the calculated sums and means. 2. Two separate plots: - A bar graph of total sales by region. - A line graph of profit trends for each product over time. Constraints: 1. Use pandas functions for all data manipulations. 2. The solution should handle any possible missing data proficiently. 3. Visualizations should be well-labeled and clear. Example Output: ```python print(merged_df.head()) # Generate the bar graph for total sales by region plt.figure(figsize=(10, 5)) merged_df.groupby(\'Region\')[\'Sales\'].sum().plot(kind=\'bar\') plt.title(\'Total Sales by Region\') plt.xlabel(\'Region\') plt.ylabel(\'Total Sales\') plt.show() # Generate the line graph for profit trend by product plt.figure(figsize=(10, 5)) for product in merged_df[\'Product\'].unique(): product_data = merged_df[merged_df[\'Product\'] == product] plt.plot(product_data[\'Date\'], product_data[\'Profit\'], label=product) plt.title(\'Profit Trend by Product\') plt.xlabel(\'Date\') plt.ylabel(\'Profit\') plt.legend() plt.show() ``` Note: Ensure that the CSV files are present in the working directory before running your code.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt def load_data(): sales_region1 = pd.read_csv(\'sales_region1.csv\') sales_region2 = pd.read_csv(\'sales_region2.csv\') return sales_region1, sales_region2 def clean_and_preprocess(sales_region1, sales_region2): sales_region1[\'Region\'] = \'Region1\' sales_region2[\'Region\'] = \'Region2\' for df in [sales_region1, sales_region2]: df[\'Date\'] = pd.to_datetime(df[\'Date\']) for col in [\'Sales\', \'Profit\']: if df[col].isna().sum() > 0: df[col].fillna(df[col].mean(), inplace=True) return sales_region1, sales_region2 def merge_data(sales_region1, sales_region2): merged_df = pd.concat([sales_region1, sales_region2], ignore_index=True) return merged_df def data_manipulation(merged_df): grouped = merged_df.groupby([\'Product\', \'Region\']).agg({ \'Sales\': \'sum\', \'Profit\': \'sum\' }).reset_index() mean_sales = merged_df.groupby(\'Product\')[\'Sales\'].mean().reset_index().rename(columns={\'Sales\': \'Mean_Sales\'}) mean_profit = merged_df.groupby(\'Product\')[\'Profit\'].mean().reset_index().rename(columns={\'Profit\': \'Mean_Profit\'}) summary_df = pd.merge(grouped, mean_sales, on=\'Product\') summary_df = pd.merge(summary_df, mean_profit, on=\'Product\') return summary_df def visualize_results(merged_df): plt.figure(figsize=(10, 5)) merged_df.groupby(\'Region\')[\'Sales\'].sum().plot(kind=\'bar\') plt.title(\'Total Sales by Region\') plt.xlabel(\'Region\') plt.ylabel(\'Total Sales\') plt.show() plt.figure(figsize=(10, 5)) for product in merged_df[\'Product\'].unique(): product_data = merged_df[merged_df[\'Product\'] == product] plt.plot(product_data[\'Date\'], product_data[\'Profit\'], label=product) plt.title(\'Profit Trend by Product\') plt.xlabel(\'Date\') plt.ylabel(\'Profit\') plt.legend() plt.show() def main(): sales_region1, sales_region2 = load_data() sales_region1, sales_region2 = clean_and_preprocess(sales_region1, sales_region2) merged_df = merge_data(sales_region1, sales_region2) summary_df = data_manipulation(merged_df) print(summary_df.head()) visualize_results(merged_df) if __name__ == \\"__main__\\": main()"},{"question":"Objective You have just learned about the `pdb` module, a powerful interactive debugger in Python. To assess your understanding of this module, you will implement a debugging function for a provided Python program. Problem Statement You are given a Python script, `fibonacci.py`, containing a function to compute the nth Fibonacci number, but it contains several bugs. Your task is to write a function `debug_fibonacci` using the `pdb` module to identify and fix these bugs. Input - The path to the Python script `fibonacci.py` containing the following code: ```python def fibonacci(n): if n <= 0: return 0 elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2) # Test print(fibonacci(10)) ``` Output A correctly running program that fixes the bugs which may include: - Incorrect base cases. - Performance optimization suggestions. Constraints - You must use the `pdb` module for debugging. - Do not alter the function signature of `fibonacci`. - Your solution must identify the bugs, fix them, and ensure the output of `fibonacci(10)` is correct. Function Signature ```python def debug_fibonacci(script_path: str) -> None: pass ``` Guidelines 1. **Set up the Debugger:** Use `pdb.set_trace()` or `pdb.run()` to start debugging the `fibonacci.py` script. 2. **Inspect Variables and Flow:** Utilize debugger commands to inspect variables and control the execution flow. 3. **Fix Bugs:** Identify the lines causing incorrect behavior or performance issues and correct them. 4. **Verify Correctness:** Ensure the script prints the correct 10th Fibonacci number, `55`. # Example Assuming the `fibonacci.py` file is in the current directory, you would call your function as follows: ```python debug_fibonacci(\\"fibonacci.py\\") ``` This function should enable you to step through the code, identify and fix any inaccuracies, and print the correct result. Good luck, and happy debugging!","solution":"def debug_fibonacci(script_path: str) -> None: import pdb import os import subprocess # Add pdb.set_trace() dynamically to the script to begin debugging with open(script_path, \'r\') as file: lines = file.readlines() # Adding pdb.set_trace() after the function definition for inspection for i, line in enumerate(lines): if \\"def fibonacci(n):\\" in line: lines.insert(i + 1, \\" import pdb; pdb.set_trace()n\\") break updated_script_path = \'debug_\' + script_path with open(updated_script_path, \'w\') as file: file.writelines(lines) # Run the script with the pdb trace subprocess.run([\\"python\\", updated_script_path])"},{"question":"Objective: Demonstrate your understanding of statistical estimation and error bars in Seaborn by writing code to visualize data using different types of error bars. Problem Statement: You are provided with a dataset of random values following a normal distribution. Your task is to visualize these data points using Seaborn and add error bars to the plots. Specifically, you need to create four different visualizations, demonstrating the use of standard deviation, percentile interval, standard error, and confidence interval error bars. Requirements: 1. Create a dataset consisting of 150 random values sampled from a normal distribution with mean = 0 and standard deviation = 1. 2. Generate four-point plots of this dataset using Seaborn. Each plot should include error bars of a different type: - Standard Deviation - Percentile Interval - Standard Error - Confidence Interval 3. For each plot, provide a short explanation (as a comment in your code) of why this type of error bar is being used and what it represents. Constraints: - Use the Seaborn library for plotting. - Use the `plot_errorbars` function as part of the implementation to ensure the setup is consistent. - Ensure each plot has a different figure or subplot for clear separation. - Use Seaborn’s point plot for visualization. Input and Output Formats: **Input:** None. Data is to be generated within the function. **Output:** Four separate plots showing different kinds of error bars. Expected Solution Structure: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn sns.set_theme(style=\\"darkgrid\\") # Generate the dataset np.random.seed(42) data = np.random.normal(0, 1, 150) # Helper function to plot error bars def plot_errorbars(data, errorbar_type, **kwargs): plt.figure(figsize=(7, 3)) sns.pointplot(x=data, errorbar=errorbar_type, capsize=.3, **kwargs) plt.show() # Plot with standard deviation error bars plot_errorbars(data, \\"sd\\") # Plot with percentile interval error bars plot_errorbars(data, (\\"pi\\", 50)) # Plot with standard error error bars plot_errorbars(data, \\"se\\") # Plot with confidence interval error bars plot_errorbars(data, \\"ci\\") ``` Explanation: - **Standard Deviation**: This plot visualizes the spread of the data around the mean, representing +/-1 standard deviation from the mean. - **Percentile Interval**: This plot shows a 50% percentile interval, representing the middle 50% of the data. - **Standard Error**: This plot visualizes the uncertainty in the estimation of the mean, based on the standard error of the sample. - **Confidence Interval**: This plot shows a 95% confidence interval for the mean using the bootstrapping method. Submit your code and the generated plots.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn sns.set_theme(style=\\"darkgrid\\") # Generate the dataset np.random.seed(42) data = np.random.normal(0, 1, 150) # Helper function to plot error bars def plot_errorbars(data, errorbar_type, **kwargs): plt.figure(figsize=(7, 3)) sns.pointplot(x=data, errorbar=errorbar_type, capsize=.3, **kwargs) plt.show() # Plot with standard deviation error bars plot_errorbars(data, \\"sd\\") # Standard Deviation Error Bars: This plot visualizes the spread of the data around the mean, representing +/-1 standard deviation from the mean. # Plot with percentile interval error bars plot_errorbars(data, (\\"pi\\", 50)) # Percentile Interval Error Bars: This plot shows a 50% percentile interval, representing the middle 50% of the data. # Plot with standard error error bars plot_errorbars(data, \\"se\\") # Standard Error Error Bars: This plot visualizes the uncertainty in the estimation of the mean, based on the standard error of the sample. # Plot with confidence interval error bars plot_errorbars(data, \\"ci\\") # Confidence Interval Error Bars: This plot shows a 95% confidence interval for the mean using the bootstrapping method."},{"question":"**Question: Implement a Custom Logging System** # Objective Implement a custom logging system that utilizes multiple handlers from the Python `logging.handlers` module to log messages under different scenarios. This will demonstrate your ability to configure and use advanced logging features in Python. # Requirements 1. **Custom Logger Setup**: - Create a custom logger named `custom_logger`. - Set the logging level to `DEBUG`. 2. **Handlers**: - Add a `StreamHandler` to output logs to the console. - Add a `FileHandler` to write logs to a file named `app.log`. - Add a `RotatingFileHandler` to handle log rotation for `app.log` when it reaches a size of 1 MB, keeping up to 5 backup files. - Add an `SMTPHandler` to send log messages of level `ERROR` or higher via email. - Add an `HTTPHandler` to send log messages as a POST request to a given URL (`http://localhost:8080/log`). 3. **Email Configuration**: - Use dummy email settings: `mailhost=\'localhost\', fromaddr=\'logger@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Application Error\'`. 4. **Logging Message Format**: - Use the following format for each log message: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. 5. **Testing**: - Log messages of various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) and demonstrate that the messages are handled appropriately by the configured handlers. # Constraints - Do not use any external libraries other than the standard `logging` module and its submodules. - Ensure that the HTTP server and email server configurations are placeholders; you do not need to implement the actual servers. - The solution should include proper exception handling to ensure that the logging system is robust. # Example Code ```python import logging import logging.handlers # 1. Create custom logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) # 2. Handlers # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # File handler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.INFO) # Rotating file handler rotating_handler = logging.handlers.RotatingFileHandler(\'app.log\', maxBytes=1048576, backupCount=5) rotating_handler.setLevel(logging.DEBUG) # SMTP handler smtp_handler = logging.handlers.SMTPHandler( mailhost=\'localhost\', fromaddr=\'logger@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Application Error\' ) smtp_handler.setLevel(logging.ERROR) # HTTP handler http_handler = logging.handlers.HTTPHandler( host=\'localhost:8080\', url=\'/log\', method=\'POST\' ) http_handler.setLevel(logging.WARNING) # 3. Formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) rotating_handler.setFormatter(formatter) smtp_handler.setFormatter(formatter) http_handler.setFormatter(formatter) # 4. Add Handlers logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(rotating_handler) logger.addHandler(smtp_handler) logger.addHandler(http_handler) # 5. Testing logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` **Note**: The above code is only an example. Your task is to implement all components from scratch and demonstrate the custom logging system\'s functionality.","solution":"import logging import logging.handlers def setup_custom_logger(): Sets up a custom logger with multiple handlers. # 1. Create custom logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) # 2. Handlers # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # File handler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Rotating file handler rotating_handler = logging.handlers.RotatingFileHandler(\'app.log\', maxBytes=1048576, backupCount=5) rotating_handler.setLevel(logging.DEBUG) # SMTP handler smtp_handler = logging.handlers.SMTPHandler( mailhost=\'localhost\', fromaddr=\'logger@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Application Error\' ) smtp_handler.setLevel(logging.ERROR) # HTTP handler http_handler = logging.handlers.HTTPHandler( host=\'localhost:8080\', url=\'/log\', method=\'POST\' ) http_handler.setLevel(logging.WARNING) # 3. Formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) rotating_handler.setFormatter(formatter) smtp_handler.setFormatter(formatter) http_handler.setFormatter(formatter) # 4. Add Handlers logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(rotating_handler) logger.addHandler(smtp_handler) logger.addHandler(http_handler) return logger if __name__ == \\"__main__\\": # Example of logging messages custom_logger = setup_custom_logger() custom_logger.debug(\'This is a debug message\') custom_logger.info(\'This is an info message\') custom_logger.warning(\'This is a warning message\') custom_logger.error(\'This is an error message\') custom_logger.critical(\'This is a critical message\')"},{"question":"**Context:** You are working on an email client that manages and processes email headers. One of the requirements is to process email addresses and generate unique message IDs for emails being sent. **Task:** Write a Python function `process_email_header` that takes a string `email_header` and returns a tuple `(parsed_address, message_id)`. 1. The `parsed_address` should be a list of tuples, where each tuple contains the real name and email address for each address in the header. 2. The `message_id` should be a unique RFC 2822-compliant message ID created using the `email.utils.make_msgid` method. **Function signature:** ```python def process_email_header(email_header: str) -> tuple: pass ``` **Requirements:** 1. Use `email.utils.getaddresses` to parse the email addresses from the provided `email_header`. 2. Create a unique message ID using `email.utils.make_msgid`. 3. Return the parsed addresses and message ID as a tuple. **Input:** - `email_header (str)`: A string containing one or more email addresses in the format typically found in the \'To\', \'Cc\', or \'Bcc\' headers of an email. **Output:** - `tuple`: A tuple consisting of: - `parsed_address (list)`: A list of tuples, where each tuple contains a real name and an email address. - `message_id (str)`: A unique message ID. **Example:** ```python from email.utils import getaddresses, make_msgid def process_email_header(email_header: str) -> tuple: parsed_address = getaddresses([email_header]) message_id = make_msgid() return parsed_address, message_id # Example usage email_header = \'John Doe <john.doe@example.com>, Jane Smith <jane.smith@example.org>\' result = process_email_header(email_header) print(result) ``` Output: ``` ([(\'John Doe\', \'john.doe@example.com\'), (\'Jane Smith\', \'jane.smith@example.org\')], \'<unique_message_id>\') ``` - The actual message ID will be unique and different in each run. **Constraints:** - Assume that the input email headers will be well-formed. - Do not include additional external libraries beyond the standard library. **Additional Information:** Utilize the `email.utils.getaddresses` to process the addresses and `email.utils.make_msgid` to generate the message ID. Ensure that the function meets the requirements and handles the input format appropriately.","solution":"from email.utils import getaddresses, make_msgid def process_email_header(email_header: str) -> tuple: Process the email header and return a tuple of parsed addresses and a unique message ID. Args: email_header (str): A string containing one or more email addresses. Returns: tuple: A tuple containing a list of tuples with real names and email addresses, and a unique message ID. parsed_address = getaddresses([email_header]) message_id = make_msgid() return parsed_address, message_id"},{"question":"# Pandas Coding Assessment: Advanced DataFrame Styling **Objective:** You are tasked with demonstrating your comprehension of pandas `Styler` objects by implementing custom styling for a given DataFrame and exporting the styled DataFrame to an HTML file. **Problem Statement:** 1. **Input:** - A DataFrame `df` with the following columns: \'Name\', \'Age\', \'Score\', \'Pass\' (boolean). 2. **Output:** - An HTML file named `styled_table.html` containing the styled DataFrame. 3. **Requirements:** - Highlight the maximum value in the \'Score\' column. - Apply a gradient background to the \'Age\' column. - Apply text color gradient to the \'Score\' column. - Use tooltips to show \'Pass\' status. - Set a caption for the table: \\"Student Performance Table\\". - Add custom CSS class `custom-table` to the table. - Export the styled DataFrame to an HTML file named `styled_table.html`. 4. **Constraints:** - Assume that the DataFrame `df` has at least one row and the columns mentioned above. - The \'Pass\' column is a boolean indicating if the student passed or not. 5. **Function Signature:** ```python def style_dataframe(df: pd.DataFrame) -> None: pass ``` # Example ```python import pandas as pd # Sample DataFrame data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Age\': [23, 25, 22, 28], \'Score\': [88, 92, 85, 87], \'Pass\': [True, True, True, False] } df = pd.DataFrame(data) style_dataframe(df) ``` After running your function, it should generate an HTML file named `styled_table.html` with the required styling applied to the DataFrame. **Evaluation Criteria:** - Correctness of the solution. - Application of the specified styles. - Accurate generation of the output HTML file. Note: Ensure that you use the appropriate pandas `Styler` methods to achieve the desired styling and export functionalities.","solution":"import pandas as pd import numpy as np def style_dataframe(df: pd.DataFrame) -> None: def highlight_max(s, props=\'\'): return np.where(s == np.nanmax(s.values), props, \'\') styler = (df.style .highlight_max(subset=[\'Score\'], color=\'yellow\') .background_gradient(subset=[\'Age\'], cmap=\'viridis\') .text_gradient(subset=[\'Score\'], cmap=\'cool\') .set_caption(\\"Student Performance Table\\") .set_table_attributes(\'class=\\"custom-table\\"\') .set_tooltips(df[[\'Pass\']].astype(str)) ) html = styler.to_html() with open(\'styled_table.html\', \'w\') as f: f.write(html)"},{"question":"# Email Encoder Implementation You are tasked with implementing a simplified version of an email encoder module similar to the `email.encoders` module described in the documentation. Your implementation should support encoding email payloads using two encoding schemes: quoted-printable (limited version) and base64. Requirements 1. Implement the following functions: - `encode_quopri(msg: Dict[str, Any]) -> None`: - Encodes the payload of the message into quoted-printable form and sets the `Content-Transfer-Encoding` header to \\"quoted-printable\\". - The quoted-printable encoding should replace spaces with `=20` and new lines (`n`) with `=0A`. - `encode_base64(msg: Dict[str, Any]) -> None`: - Encodes the payload of the message into base64 form and sets the `Content-Transfer-Encoding` header to \\"base64\\". 2. The `msg` argument for these functions will be a dictionary with the following structure: ```python msg = { \\"payload\\": str, # email content as a string \\"headers\\": Dict[str, str] # email headers as a dictionary } ``` 3. The functions will modify the `msg` dictionary in place by encoding the `payload` and setting the appropriate `Content-Transfer-Encoding` header in the `headers` dictionary. Constraints - The payload is guaranteed to be a non-empty string. - The headers dictionary may or may not contain other headers, but will not contain the `Content-Transfer-Encoding` header initially. Example ```python msg = { \\"payload\\": \\"Hello World\\", \\"headers\\": {} } encode_quopri(msg) # After calling encode_quopri, msg should be: # { # \\"payload\\": \\"Hello=20World\\", # \\"headers\\": { # \\"Content-Transfer-Encoding\\": \\"quoted-printable\\" # } # } msg = { \\"payload\\": \\"Hello World\\", \\"headers\\": {} } encode_base64(msg) # After calling encode_base64, msg should be: # { # \\"payload\\": \\"SGVsbG8gV29ybGQ=\\", # \\"headers\\": { # \\"Content-Transfer-Encoding\\": \\"base64\\" # } # } ``` # Implementation Please write the code to implement the two functions as described.","solution":"import base64 def encode_quopri(msg): Encodes the payload of the message into quoted-printable form and sets the Content-Transfer-Encoding header to \\"quoted-printable\\". payload = msg[\'payload\'] encoded_payload = payload.replace(\' \', \'=20\').replace(\'n\', \'=0A\') msg[\'payload\'] = encoded_payload msg[\'headers\'][\'Content-Transfer-Encoding\'] = \'quoted-printable\' def encode_base64(msg): Encodes the payload of the message into base64 form and sets the Content-Transfer-Encoding header to \\"base64\\". payload = msg[\'payload\'] encoded_payload = base64.b64encode(payload.encode()).decode() msg[\'payload\'] = encoded_payload msg[\'headers\'][\'Content-Transfer-Encoding\'] = \'base64\'"},{"question":"Coding Assessment Question # Objective You are required to implement a classification pipeline using the Naive Bayes classifiers provided by scikit-learn and compare their performances on a given dataset. This task will assess your understanding of different Naive Bayes classifiers and your ability to utilize scikit-learn efficiently. # Dataset You will use the `load_digits` dataset from scikit-learn, which contains 8x8 pixel images of handwritten digits. # Tasks 1. **Data Loading and Preprocessing**: - Load the `load_digits` dataset. - Split the dataset into training and test sets using an 80-20 split. 2. **Model Training and Evaluation**: - Train and evaluate the following Naive Bayes classifiers on the training set: - Gaussian Naive Bayes (`GaussianNB`) - Multinomial Naive Bayes (`MultinomialNB`) - Bernoulli Naive Bayes (`BernoulliNB`) - For each classifier: - Fit the model on the training data. - Predict the labels for the test data. - Calculate accuracy and confusion matrix for the predictions. 3. **Comparison of Classifiers**: - Compare the performance (accuracy and confusion matrices) of the three classifiers. - Identify which classifier performed the best on the dataset and provide insights as to why this might be the case. # Constraints and Limitations - You must use scikit-learn\'s `train_test_split` for data splitting. - You should ensure reproducibility by setting a random seed where applicable. - You are required to implement the evaluation metrics using scikit-learn\'s provided functions. # Expected Inputs and Outputs - **Input Formats**: - The dataset will be loaded using scikit-learn\'s `load_digits` function. - **Output Formats**: - Print the accuracy and confusion matrix for each classifier. - A concluding statement about the best performing classifier and possible reasons for its performance. # Performance Requirements - The models should be trained efficiently without using excessive memory or computation time. # Example Code Structure ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix # Load and split the dataset digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) # Initialize classifiers gnb = GaussianNB() mnb = MultinomialNB() bnb = BernoulliNB() # Train and predict with GaussianNB gnb.fit(X_train, y_train) y_pred_gnb = gnb.predict(X_test) acc_gnb = accuracy_score(y_test, y_pred_gnb) conf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb) # Train and predict with MultinomialNB mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) acc_mnb = accuracy_score(y_test, y_pred_mnb) conf_matrix_mnb = confusion_matrix(y_test, y_pred_mnb) # Train and predict with BernoulliNB bnb.fit(X_train, y_train) y_pred_bnb = bnb.predict(X_test) acc_bnb = accuracy_score(y_test, y_pred_bnb) conf_matrix_bnb = confusion_matrix(y_test, y_pred_bnb) # Output the results print(\\"GaussianNB Accuracy:\\", acc_gnb) print(\\"GaussianNB Confusion Matrix:n\\", conf_matrix_gnb) print(\\"MultinomialNB Accuracy:\\", acc_mnb) print(\\"MultinomialNB Confusion Matrix:n\\", conf_matrix_mnb) print(\\"BernoulliNB Accuracy:\\", acc_bnb) print(\\"BernoulliNB Confusion Matrix:n\\", conf_matrix_bnb) # Conclusion best_classifier = max((acc_gnb, \'GaussianNB\'), (acc_mnb, \'MultinomialNB\'), (acc_bnb, \'BernoulliNB\')) print(f\\"The best performing classifier is {best_classifier[1]} with an accuracy of {best_classifier[0]}\\") ``` Ensure that your code is well-documented and follows best practices for readability and maintainability.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix def naive_bayes_classification(): # Load and split the dataset digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) # Initialize classifiers gnb = GaussianNB() mnb = MultinomialNB() bnb = BernoulliNB() # Train and predict with GaussianNB gnb.fit(X_train, y_train) y_pred_gnb = gnb.predict(X_test) acc_gnb = accuracy_score(y_test, y_pred_gnb) conf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb) # Train and predict with MultinomialNB mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) acc_mnb = accuracy_score(y_test, y_pred_mnb) conf_matrix_mnb = confusion_matrix(y_test, y_pred_mnb) # Train and predict with BernoulliNB bnb.fit(X_train, y_train) y_pred_bnb = bnb.predict(X_test) acc_bnb = accuracy_score(y_test, y_pred_bnb) conf_matrix_bnb = confusion_matrix(y_test, y_pred_bnb) # Output the results print(\\"GaussianNB Accuracy:\\", acc_gnb) print(\\"GaussianNB Confusion Matrix:n\\", conf_matrix_gnb) print(\\"MultinomialNB Accuracy:\\", acc_mnb) print(\\"MultinomialNB Confusion Matrix:n\\", conf_matrix_mnb) print(\\"BernoulliNB Accuracy:\\", acc_bnb) print(\\"BernoulliNB Confusion Matrix:n\\", conf_matrix_bnb) # Conclusion best_classifier = max((acc_gnb, \'GaussianNB\'), (acc_mnb, \'MultinomialNB\'), (acc_bnb, \'BernoulliNB\')) print(f\\"The best performing classifier is {best_classifier[1]} with an accuracy of {best_classifier[0]}\\") return { \\"GaussianNB\\": {\\"accuracy\\": acc_gnb, \\"confusion_matrix\\": conf_matrix_gnb}, \\"MultinomialNB\\": {\\"accuracy\\": acc_mnb, \\"confusion_matrix\\": conf_matrix_mnb}, \\"BernoulliNB\\": {\\"accuracy\\": acc_bnb, \\"confusion_matrix\\": conf_matrix_bnb}, \\"best_classifier\\": best_classifier[1] }"},{"question":"**Custom JSON Encoder and Decoder** In this task, you are required to implement custom JSON serialization and deserialization for a specific Python class named `Person`. The `Person` class contains `name` (str), `age` (int), and `address` (Address), where `Address` is another class containing `street` (str) and `city` (str). You need to: 1. Implement the `Person` and `Address` classes. 2. Create custom JSONEncoder and JSONDecoder classes to properly serialize and deserialize `Person` and `Address` objects. 3. Demonstrate the functionality with a sample `Person` object. **Requirements:** 1. `Person` and `Address` classes must be defined with appropriate attributes. 2. Custom `PersonEncoder` and `PersonDecoder` classes: - `PersonEncoder` should extend `json.JSONEncoder` and override `default` method to handle `Person` and `Address` objects. - `PersonDecoder` should handle decoding JSON-encoded data back into `Person` and `Address` objects. 3. Write functions `serialize_person(person: Person) -> str` and `deserialize_person(person_json: str) -> Person` to utilize the custom encoder and decoder. **Example:** ```python class Address: def __init__(self, street: str, city: str): self.street = street self.city = city class Person: def __init__(self, name: str, age: int, address: Address): self.name = name self.age = age self.address = address import json class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"address\\": obj.address } if isinstance(obj, Address): return { \\"street\\": obj.street, \\"city\\": obj.city } return super().default(obj) def person_hook(dct): if \\"street\\" in dct and \\"city\\" in dct: return Address(dct[\\"street\\"], dct[\\"city\\"]) if \\"name\\" in dct and \\"age\\" in dct and \\"address\\" in dct: address = dct[\\"address\\"] return Person(dct[\\"name\\"], dct[\\"age\\"], address) return dct class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=person_hook, *args, **kwargs) def serialize_person(person: Person) -> str: return json.dumps(person, cls=PersonEncoder) def deserialize_person(person_json: str) -> Person: return json.loads(person_json, cls=PersonDecoder) # Example Usage address = Address(\\"123 Main St\\", \\"Anytown\\") person = Person(\\"John Doe\\", 30, address) person_json = serialize_person(person) print(person_json) # Should output a JSON string deserialized_person = deserialize_person(person_json) print(deserialized_person) # Should output a Person object ``` **Constraints and Assumptions:** - Assume that `name` will always be a non-empty string. - `age` will always be a non-negative integer. - `street` and `city` will also be non-empty strings. **Evaluation Criteria:** - Correct implementation of the `Person` and `Address` classes. - Proper functionality of the custom encoder and decoder. - Accuracy of serialization and deserialization functions. - Code clarity and proper handling of edge cases.","solution":"import json class Address: A class to represent an address. def __init__(self, street: str, city: str): self.street = street self.city = city def __eq__(self, other): return self.street == other.street and self.city == other.city class Person: A class to represent a person. def __init__(self, name: str, age: int, address: Address): self.name = name self.age = age self.address = address def __eq__(self, other): return self.name == other.name and self.age == other.age and self.address == other.address class PersonEncoder(json.JSONEncoder): Custom JSON Encoder for Person and Address objects. def default(self, obj): if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"address\\": obj.address } if isinstance(obj, Address): return { \\"street\\": obj.street, \\"city\\": obj.city } return super().default(obj) def person_hook(dct): if \\"street\\" in dct and \\"city\\" in dct: return Address(dct[\\"street\\"], dct[\\"city\\"]) if \\"name\\" in dct and \\"age\\" in dct and \\"address\\" in dct: address = dct[\\"address\\"] return Person(dct[\\"name\\"], dct[\\"age\\"], address) return dct class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=person_hook, *args, **kwargs) def serialize_person(person: Person) -> str: return json.dumps(person, cls=PersonEncoder) def deserialize_person(person_json: str) -> Person: return json.loads(person_json, cls=PersonDecoder) # Example Usage address = Address(\\"123 Main St\\", \\"Anytown\\") person = Person(\\"John Doe\\", 30, address) person_json = serialize_person(person) print(person_json) # Should output a JSON string deserialized_person = deserialize_person(person_json) print(deserialized_person) # Should output a Person object"},{"question":"**Manifold Learning Implementation and Evaluation** **Objective**: The goal of this exercise is to demonstrate your understanding of manifold learning techniques provided by scikit-learn and how to apply them to a high-dimensional dataset to reduce its dimensionality and visualize the result. # Problem Statement You are given a high-dimensional dataset representing handwritten digits. Your task is to use manifold learning techniques to reduce its dimensionality and visualize the first two components of the data using Isomap and Locally Linear Embedding (LLE). Finally, you should compare the results by visualizing the 2D representations of the dataset. # Instructions 1. **Load the Data**: Use the `load_digits` dataset from `sklearn.datasets`. 2. **Apply Isomap**: - Use the `Isomap` class from `sklearn.manifold` to reduce the dataset to 2 dimensions. - Use `n_neighbors=10` and `n_components=2`. 3. **Apply Locally Linear Embedding (LLE)**: - Use the `LocallyLinearEmbedding` class from `sklearn.manifold`. - Use `n_neighbors=10`, `n_components=2`, and method=\'standard\'. 4. **Visualization**: - Create a scatter plot for the 2D representation obtained from Isomap. Color points by their digit labels. - Create a scatter plot for the 2D representation obtained from LLE. Color points by their digit labels. 5. **Compare**: Write a brief comparison of the two methods based on your visualizations. # Expected Input and Output Formats - **Input**: No direct input (besides necessary imports and function calls to load the dataset). - **Output**: Two scatter plots and a brief written comparison. # Constraints 1. You must use `matplotlib` for visualization. 2. Ensure your code is efficient and well-documented. # Performance Requirements - The solution should run within 10 minutes on a standard machine. ```python # Import necessary packages import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import Isomap, LocallyLinearEmbedding # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Apply Isomap isomap = Isomap(n_neighbors=10, n_components=2) X_isomap = isomap.fit_transform(X) # Apply Locally Linear Embedding (LLE) lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, method=\'standard\') X_lle = lle.fit_transform(X) # Visualization of Isomap result plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=y, cmap=plt.cm.Spectral) plt.colorbar() plt.title(\'Isomap 2D Representation\') # Visualization of LLE result plt.subplot(1, 2, 2) plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y, cmap=plt.cm.Spectral) plt.colorbar() plt.title(\'LLE 2D Representation\') plt.show() # Brief comparison (write your comparison here) comparison = Isomap generally attempts to preserve the global structure by maintaining geodesic distances between points, while LLE focuses on preserving local neighborhood relationships. The visualizations may show differences in how clusters of digits are handled, where Isomap might show a more globally coherent structure and LLE might provide better local neighborhood preservation. These differences manifest in how well-separated the digit clusters appear in each corresponding scatter plot. print(comparison) ```","solution":"# Import necessary packages import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import Isomap, LocallyLinearEmbedding # Function to apply manifold learning techniques and visualize results def manifold_learning_and_visualization(): # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Apply Isomap isomap = Isomap(n_neighbors=10, n_components=2) X_isomap = isomap.fit_transform(X) # Apply Locally Linear Embedding (LLE) lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, method=\'standard\') X_lle = lle.fit_transform(X) # Visualization of Isomap result plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=y, cmap=plt.cm.Spectral) plt.colorbar() plt.title(\'Isomap 2D Representation\') # Visualization of LLE result plt.subplot(1, 2, 2) plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y, cmap=plt.cm.Spectral) plt.colorbar() plt.title(\'LLE 2D Representation\') plt.show() # Brief comparison comparison = Isomap generally attempts to preserve the global structure by maintaining geodesic distances between points, while LLE focuses on preserving local neighborhood relationships. The visualizations may show differences in how clusters of digits are handled, where Isomap might show a more globally coherent structure and LLE might provide better local neighborhood preservation. These differences manifest in how well-separated the digit clusters appear in each corresponding scatter plot. return comparison # The function call can be made outside in the main guard or testing context. # For now, as per the task instruction, we only define the function."},{"question":"You have been provided with the Titanic dataset. Your task is to create a detailed visualization using Seaborn that helps in analyzing the survival rates based on multiple factors. Specifically, you will: 1. Load the Titanic dataset and sort it by the `alive` column in descending order. 2. Create a faceted bar plot that shows the distribution of passengers by class and sex. The bars should represent counts of passengers. 3. Add another visualization: a faceted histogram showing the age distribution of passengers, separated by sex, and categorized by survival status (alive or not alive). Use a bin width of 10 for the histogram. 4. Customize the plots: - Provide appropriate labels and titles. - Use different colors to distinguish between the categories in the plots. **Input:** - No function arguments are needed. You will use the Titanic dataset provided by Seaborn. **Output:** - Two plots: 1. A faceted bar plot displaying passenger counts by class and sex. 2. A faceted histogram depicting age distribution segmented by survival status and sex. **Constraints:** - The plots should be clear and easy to interpret. - Ensure the use of seaborn\'s object-oriented API (`seaborn.objects`). **Sample Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load and sort the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # 1. Create a faceted bar plot showing distribution by class and sex bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) bar_plot = bar_plot.facet(\\"sex\\") # 2. Create a faceted histogram for age distribution by survival status and sex hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Show the plots with appropriate titles and labels bar_plot.title(\\"Passenger Distribution by Class and Sex\\").show() hist_plot.title(\\"Age Distribution by Survival Status and Sex\\").show() # Note: Ensure proper labeling and customization as per the question instructions. ``` **Performance Requirements:** - The code should run efficiently and should not encounter memory issues. - Use appropriate data visualization techniques to ensure the plots convey the necessary information effectively. Ensure you test your implementation to validate the plots and their customizations as required.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load and sort the Titanic dataset titanic = sns.load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # 1. Create a faceted bar plot showing distribution by class and sex bar_plot = sns.catplot(data=titanic, kind=\\"count\\", x=\\"class\\", hue=\\"sex\\", col=\\"sex\\", height=4, aspect=0.7) bar_plot.set_axis_labels(\\"Class\\", \\"Count\\") bar_plot.set_titles(\\"Passenger Distribution by Class and Sex - {col_name}\\") bar_plot.fig.suptitle(\\"Passenger Distribution by Class and Sex\\", y=1.05) # 2. Create a faceted histogram for age distribution by survival status and sex hist_plot = sns.displot(data=titanic, x=\\"age\\", hue=\\"alive\\", col=\\"sex\\", multiple=\\"stack\\", binwidth=10) hist_plot.set_axis_labels(\\"Age\\", \\"Count\\") hist_plot.set_titles(\\"Age Distribution by Survival Status and Sex - {col_name}\\") hist_plot.fig.suptitle(\\"Age Distribution by Survival Status and Sex\\", y=1.05) plt.show()"},{"question":"You have been provided with `sklearn.datasets` documentation about loading and generating datasets. The task is to fetch one of these datasets, explore its properties, and implement a machine learning model. # Task Description 1. **Load a Dataset**: Load the \\"Iris\\" dataset using the dataset loader utility provided in `sklearn.datasets`. 2. **Dataset Exploration**: - Print the shape of the data (number of samples and number of features). - Print the feature names. - Print the first 5 samples of the data. 3. **Data Preprocessing**: - Standardize the features by removing the mean and scaling to unit variance using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Implementation**: - Split the dataset into training and testing sets (80/20 split). - Train a `KNeighborsClassifier` on the training set with `k=3`. - Evaluate the model performance on the testing set using accuracy score. # Input and Output - No explicit input will be provided to the function. Instead, it will load the dataset internally. - The function should return a tuple containing `(accuracy_score, standardized_data)`, where: - `accuracy_score` is the accuracy of the model on the testing set. - `standardized_data` is the standardized version of the data. # Constraints - Use `random_state=42` wherever applicable to ensure reproducibility. - The function should run within a reasonable time frame (few seconds at most). # Expected Function Signature ```python def load_and_evaluate_iris_dataset(): pass ``` # Example Usage ```python accuracy, standardized_data = load_and_evaluate_iris_dataset() print(f\\"Accuracy: {accuracy}\\") print(f\\"Standardized Data Shape: {standardized_data.shape}\\") # Add any additional print statements if needed for debugging ``` Implement the function `load_and_evaluate_iris_dataset` according to the requirements detailed above.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_and_evaluate_iris_dataset(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Print the shape of the data print(f\\"Data Shape: {X.shape}\\") # Print the feature names print(f\\"Feature Names: {iris.feature_names}\\") # Print the first 5 samples of the data print(f\\"First 5 samples: {X[:5]}\\") # Standardize the features scaler = StandardScaler() X_standardized = scaler.fit_transform(X) # Split the dataset into training and testing sets (80/20 split) X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42) # Train a KNeighborsClassifier on the training set with k=3 knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) # Evaluate the model performance on the testing set using accuracy score y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy, X_standardized"},{"question":"Problem Statement You are required to implement a function `fetch_unread_emails` that connects to an IMAP server, authenticates using a given username and password, searches for unread emails in the inbox, and retrieves and returns the email content of these unread emails. # Function Signature ```python def fetch_unread_emails(host: str, port: int, username: str, password: str) -> list: pass ``` # Input - `host` (str): The IMAP server host. - `port` (int): The port number for IMAP4 or IMAP4_SSL. - `username` (str): The username for authenticating with the server. - `password` (str): The password for authenticating with the server. # Output - Returns a list of strings, where each string contains the content of an unread email. # Constraints - You should handle exceptions where the connection or authentication fails. - Ensure to close the connection to the IMAP server properly (preferably using a `with` statement). - Only fetch the body text of the unread emails. # Example ```python # Example usage: host = \\"imap.example.com\\" port = 993 username = \\"user@example.com\\" password = \\"password\\" emails = fetch_unread_emails(host, port, username, password) for email in emails: print(email) ``` # Notes - You will be using the `imaplib.IMAP4_SSL` class to establish an SSL connection to the IMAP server. - The `search` method with the criterion `\'UNSEEN\'` can be used to find unread emails. - The `fetch` method can be used to retrieve the email contents. - Ensure proper error handling and closing the connection. Implement the `fetch_unread_emails` function to meet the requirements and constraints above.","solution":"import imaplib import email from email.policy import default def fetch_unread_emails(host: str, port: int, username: str, password: str) -> list: unread_emails = [] try: with imaplib.IMAP4_SSL(host, port) as mail: mail.login(username, password) mail.select(\\"inbox\\") status, response = mail.search(None, \'UNSEEN\') if status == \'OK\': for num in response[0].split(): status, data = mail.fetch(num, \'(RFC822)\') if status == \'OK\': msg = email.message_from_bytes(data[0][1], policy=default) for part in msg.walk(): if part.get_content_type() == \\"text/plain\\": unread_emails.append(part.get_payload(decode=True).decode()) mail.logout() except Exception as e: print(f\\"An error occurred: {e}\\") return unread_emails"},{"question":"# Python Version Hex Encoding You are provided with versioning macros for the Python programming language. The version number is composed of several segments: major version, minor version, micro version, release level, and release serial. These segments combine to form a hexadecimal encoded version number using a specific format. Task: Write a function `python_version_hex(version_str: str) -> int` that takes a Python version string as input and returns the hexadecimal encoded version as an integer. Input: - A string `version_str` in the format \\"major.minor.microlevelserial\\", where: - `major`, `minor`, and `micro` are integers. - `level` is a single character: \'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final release. - `serial` is an integer. Output: - An integer representing the hexadecimal encoded version. Example: ```python print(python_version_hex(\\"3.4.1a2\\")) ``` This should output: ``` 50529570 ``` Constraints: 1. `major`, `minor`, `micro` are non-negative integers. 2. `level` can only be \'a\', \'b\', \'c\', or \'f\'. 3. `serial` is a non-negative integer. Notes: - Use the following mapping for `level`: - \'a\' -> 0xA - \'b\' -> 0xB - \'c\' -> 0xC - \'f\' -> 0xF - The 32-bit integer composition is as follows: - Bits 1-8 for `major` - Bits 9-16 for `minor` - Bits 17-24 for `micro` - Bits 25-28 for `level` - Bits 29-32 for `serial` Function Signature: ```python def python_version_hex(version_str: str) -> int: # Your implementation here ```","solution":"def python_version_hex(version_str: str) -> int: Converts a Python version string to its hexadecimal encoded integer form. Args: - version_str: A string in the format \\"major.minor.microlevelserial\\" Returns: - An integer representing the hexadecimal encoded version. # Level mapping level_map = { \'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF } # Extract components from the version string major, minor, micro_level_serial = version_str.split(\'.\') micro = int(micro_level_serial[:-2]) level = micro_level_serial[-2] serial = int(micro_level_serial[-1]) # Apply the mapping and bit shifts major = int(major) << 24 minor = int(minor) << 16 micro = micro << 8 level = level_map[level] << 4 serial = serial # Combine all components into a single integer encoded_version = major | minor | micro | level | serial return encoded_version"},{"question":"Objective Gain hands-on experience with the `select` module in Python by implementing an I/O multiplexing solution for handling multiple non-blocking socket connections. Problem Statement You are tasked with implementing a simple TCP server that can handle multiple client connections concurrently using the `select` module. The server should be capable of reading incoming messages from clients and echoing back the received messages to the respective sending clients. Requirements: 1. **Server Setup**: - Bind to localhost on port `12345`. - Use non-blocking sockets for all connections. 2. **I/O Multiplexing**: - Use the `select.select()` function to monitor multiple sockets for readability and writability. - Handle new connections. - Read incoming messages from clients and echo them back. 3. **Event Handling**: - Maintain a list of connected clients. - Properly handle client disconnections. - Ensure no blocking operations on any socket. Implementation Details: - Create and set up a non-blocking server socket. - Use `select.select()` to manage multiple connections efficiently. - Register and unregister sockets from monitoring as required. - Ensure proper clean-up and closure of sockets. Input: The server doesn\'t take any user input directly. It listens for incoming client messages. Output: Echos back the received messages to the corresponding clients. Example: 1. Client A connects and sends \\"Hello\\". 2. Client B connects and sends \\"World\\". 3. Server receives \\"Hello\\" and sends \\"Hello\\" back to Client A. 4. Server receives \\"World\\" and sends \\"World\\" back to Client B. Solution Template ```python import socket import select def start_server(host=\'localhost\', port=12345): # Create a TCP/IP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # Set the socket to non-blocking mode server_socket.setblocking(False) # Bind the socket to the server address server_socket.bind((host, port)) # Listen for incoming connections server_socket.listen(5) # List of sockets to monitor for incoming connections sockets_list = [server_socket] # Dictionary to keep track of client sockets and their addresses clients = {} print(f\\"Server listening on {host}:{port}\\") while True: # Use select to get the list of readable, writable, and erroneous sockets readable, writable, errored = select.select(sockets_list, [], sockets_list) for notified_socket in readable: if notified_socket == server_socket: # Handle new connection client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: # Handle incoming messages from a connected client try: message = notified_socket.recv(1024) if not message: raise ConnectionResetError() # Echo back the received message notified_socket.sendall(message) except: # Handle disconnection print(f\\"Closed connection from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() for notified_socket in errored: # Handle erroneous sockets sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() if __name__ == \\"__main__\\": start_server() ``` Constraints - The server should handle disconnections gracefully. - The server should not block on any socket operation. Notes - Use the provided template and modify it as needed. - Test your server using multiple telnet clients or any TCP client tool.","solution":"import socket import select def start_server(host=\'localhost\', port=12345): # Create a TCP/IP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # Set the socket to non-blocking mode server_socket.setblocking(False) # Bind the socket to the server address server_socket.bind((host, port)) # Listen for incoming connections server_socket.listen(5) # List of sockets to monitor for incoming connections sockets_list = [server_socket] # Dictionary to keep track of client sockets and their addresses clients = {} print(f\\"Server listening on {host}:{port}\\") while True: # Use select to get the list of readable, writable, and erroneous sockets readable, writable, errored = select.select(sockets_list, [], sockets_list) for notified_socket in readable: if notified_socket == server_socket: # Handle new connection client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: # Handle incoming messages from a connected client try: message = notified_socket.recv(1024) if not message: raise ConnectionResetError() # Echo back the received message notified_socket.sendall(message) except: # Handle disconnection print(f\\"Closed connection from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() for notified_socket in errored: # Handle erroneous sockets sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() if __name__ == \\"__main__\\": start_server()"},{"question":"# Import Mechanism in Python In this task, you are required to write a Python function that mimics an import mechanism for loading a module given its name. While the deprecated `imp` module provides functionality for this, we will use the recommended `importlib` module instead. You need to implement a function `custom_import(module_name: str) -> object` that attempts to import a module using the `importlib` utilities. # Function Signature ```python def custom_import(module_name: str) -> object: pass ``` # Input - `module_name` (str): The full name of the module to be imported. # Output - The function returns the module object if the import is successful. - If the module does not exist, it raises an `ImportError`. # Constraints - You must use functions from the `importlib` module for the implementation. - You should handle file closing and module reloading where applicable. # Example ```python # Example usage module = custom_import(\'math\') print(module.sqrt(16)) # Output should be 4.0 try: module = custom_import(\'non_existent_module\') except ImportError: print(\\"Module not found\\") # Output should be \\"Module not found\\" ``` # Notes - You may assume that all standard library modules are available. - No additional third-party modules should be used. # Performance Requirements - The function should perform efficiently for standard library imports. Good luck!","solution":"import importlib def custom_import(module_name: str) -> object: Attempts to import a module using importlib utilities. Parameters: module_name (str): The full name of the module to be imported. Returns: The imported module object if successful, otherwise raises ImportError. try: module = importlib.import_module(module_name) return module except ModuleNotFoundError: raise ImportError(f\\"Module \'{module_name}\' not found\\")"},{"question":"Objective You are given a list of user data, and you need to implement functions to save this data to a plist file and read this data back from the plist file using the `plistlib` module. This will test your understanding of file I/O operations and handling plist data formats. Requirements 1. Implement a function `save_to_plist(data, filename)` that takes a list of dictionaries (`data`) and a filename (`filename`) as input and writes the data to a plist file in XML format. 2. Implement a function `load_from_plist(filename)` that takes a filename (`filename`) as input, reads the plist file, and returns the data as a list of dictionaries. 3. Ensure that dictionary keys are sorted alphabetically when writing to the plist file. Function Signatures ```python import plistlib def save_to_plist(data, filename): # Your code here def load_from_plist(filename): # Your code here ``` Input - `data`: A list of dictionaries, where each dictionary represents a user with keys such as \\"name\\", \\"age\\", \\"email\\", etc. - `filename`: A string representing the path to the plist file. Output - For `save_to_plist`: The function does not return anything. It writes the input data to the specified plist file. - For `load_from_plist`: The function returns a list of dictionaries read from the plist file. Example ```python data = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"email\\": \\"bob@example.com\\"} ] filename = \\"users.plist\\" # Save data to plist file save_to_plist(data, filename) # Load data from plist file loaded_data = load_from_plist(filename) print(loaded_data) # Output: # [{\'name\': \'Alice\', \'age\': 30, \'email\': \'alice@example.com\'}, {\'name\': \'Bob\', \'age\': 25, \'email\': \'bob@example.com\'}] ``` Constraints - The plist file will always contain a valid list of dictionaries. - Each dictionary will have string keys and values that are strings, integers, or other basic types supported by plist (e.g., booleans, floats). Hints - Utilize the `plistlib.dump` and `plistlib.load` functions for writing to and reading from plist files. - Remember to handle the file opening and closing correctly using `with` statements to ensure proper resource management.","solution":"import plistlib def save_to_plist(data, filename): Save a list of dictionaries to a plist file with sorted keys. # Ensure that the keys in each dictionary are sorted data_sorted = [{k: d[k] for k in sorted(d.keys())} for d in data] with open(filename, \'wb\') as fp: plistlib.dump(data_sorted, fp) def load_from_plist(filename): Load and return a list of dictionaries from a plist file. with open(filename, \'rb\') as fp: data = plistlib.load(fp) return data"},{"question":"Objective To assess the student\'s understanding of the seaborn package, specifically their ability to use the `blend_palette` function to create custom color palettes and apply them to a data visualization. Problem Statement You are provided with a dataset containing information about different species of flowers. The dataset has the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Your task is to write a function `create_visualization_with_custom_palette` that accomplishes the following: 1. Load the dataset from a CSV file. 2. Create a custom colormap using the `sns.blend_palette` function by interpolating between at least three colors of your choice. 3. Use the created colormap to plot a scatter plot showing the relationship between petal length and petal width, with points colored based on the species of the flowers. 4. Ensure that the visualization includes all necessary components (such as title, axis labels, and a legend). Input - A CSV file path containing the dataset. Output - A scatter plot visualized using seaborn with a custom palette applied. Constraints - Use `sns.blend_palette` to create a non-default color palette. - The generated plot should be clear and readable. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualization_with_custom_palette(csv_file_path: str) -> None: pass ``` Example Usage ```python create_visualization_with_custom_palette(\'path_to_flower_dataset.csv\') ``` Example Dataset ``` sepal_length,sepal_width,petal_length,petal_width,species 5.1,3.5,1.4,0.2,setosa 4.9,3.0,1.4,0.2,setosa 4.7,3.2,1.3,0.2,setosa 5.0,3.6,1.4,0.3,setosa 5.4,3.9,1.7,0.4,setosa 6.4,3.2,4.5,1.5,versicolor 6.9,3.1,4.9,1.5,versicolor 5.5,2.3,4.0,1.3,versicolor 6.5,2.8,4.6,1.5,versicolor 5.7,2.8,4.5,1.3,versicolor 7.6,3.0,6.6,2.1,virginica 4.9,2.5,4.5,1.7,virginica 9.0,3.0,5.1,2.3,virginica 7.4,2.8,6.1,1.9,virginica 8.0,3.2,5.3,2.2,virginica ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualization_with_custom_palette(csv_file_path: str) -> None: Load dataset and create a scatter plot using a custom seaborn palette. Parameters: - csv_file_path: str : Path to the CSV file containing the flower dataset. # Load the dataset df = pd.read_csv(csv_file_path) # Create a custom colormap custom_palette = sns.blend_palette([\\"#FF0000\\", \\"#00FF00\\", \\"#0000FF\\"], as_cmap=True) # Assign species to colors – assuming there are three species in the dataset species_unique = df[\'species\'].unique() species_palette = dict(zip(species_unique, sns.color_palette([\\"#FF0000\\", \\"#00FF00\\", \\"#0000FF\\"]))) # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'petal_length\', y=\'petal_width\', hue=\'species\', palette=species_palette) # Add title and labels plt.title(\'Scatter Plot of Petal Length vs Petal Width\', fontsize=16) plt.xlabel(\'Petal Length\', fontsize=14) plt.ylabel(\'Petal Width\', fontsize=14) plt.legend(title=\'Species\') # Show the plot plt.show()"},{"question":"# Distributed Event Handling in Pytorch You are provided with the `torch.distributed.elastic.events` module, which is used for handling elastic events in distributed computing environments. For this assignment, you need to implement a function that processes and records events using the provided API methods and objects. Task: Implement a function `process_and_record_event` that: 1. Takes the following inputs: - `event_name` (str): The name of the event to be recorded. - `source_name` (str): The name of the source of the event. - `metadata` (dict): A dictionary containing metadata about the event. 2. Creates and records an event using the provided API. Specifically, the steps should include: - Creating an `Event` object. - Setting the `EventSource` for the event. - Adding the provided metadata to the `Event`. - Recording the event using the `record` method. 3. Returns a confirmation message indicating that the event has been successfully recorded. Here is the expected function signature: ```python def process_and_record_event(event_name: str, source_name: str, metadata: dict) -> str: pass ``` Constraints: 1. The `metadata` dictionary will contain no more than 10 key-value pairs. 2. Metadata key-value pairs are all strings. Here\'s what the usage might look like: ```python event_name = \\"training_start\\" source_name = \\"trainer_service\\" metadata = { \\"epoch\\": \\"1\\", \\"batch\\": \\"32\\", \\"accuracy\\": \\"0.85\\" } result = process_and_record_event(event_name, source_name, metadata) print(result) # Should output: \\"Event \'training_start\' from \'trainer_service\' recorded successfully.\\" ``` Note: You will need to refer to the official pytorch documentation to understand the exact usage and structure of the classes and methods involved in event handling.","solution":"import torch.distributed.elastic.events as events def process_and_record_event(event_name: str, source_name: str, metadata: dict) -> str: Processes and records an event with the provided name, source and metadata using the torch.distributed.elastic.events module. Args: event_name (str): The name of the event to be recorded. source_name (str): The name of the source of the event. metadata (dict): A dictionary containing metadata about the event. Returns: str: A confirmation message indicating that the event has been successfully recorded. # Create an Event object event = events.Event(event_name) # Set the EventSource for the event event.source = events.EventSource(source_name) # Add the provided metadata to the Event for key, value in metadata.items(): event.metadata[key] = value # Record the event events.record(event) # Return confirmation message return f\\"Event \'{event_name}\' from \'{source_name}\' recorded successfully.\\""},{"question":"# Multispectral Image Color Space Converter Objective Implement a function that reads a list of RGB colors, converts them to one of the specified color spaces (YIQ, HLS, or HSV), and then converts them back to RGB. You will also write helper functions to assist with the conversions. Function Signature ```python def convert_colors(rgb_colors: List[Tuple[float, float, float]], target_space: str) -> List[Tuple[float, float, float]]: pass ``` Input - `rgb_colors`: A list of tuples, where each tuple represents an RGB color with floating point values in the range [0, 1]. Example: `[(0.2, 0.4, 0.4), (0.6, 0.7, 0.2)]` - `target_space`: A string representing the target color space. It can be one of `[\\"YIQ\\", \\"HLS\\", \\"HSV\\"]`. Output - The function should return a list of tuples, where each tuple represents the RGB color obtained after converting to the target space and then back to RGB. The values should be floating point values in the range [0, 1]. Example ```python >>> convert_colors([(0.2, 0.4, 0.4), (0.6, 0.7, 0.2)], \\"HSV\\") [(0.2, 0.4, 0.4), (0.6, 0.7, 0.2)] ``` Constraints - You must use the functions provided by the `colorsys` module for the conversions. - Input values for RGB, and output values should always be within [0, 1]. - Assume that the input list of RGB colors is non-empty and contains valid RGB values. Additional Information - Performance is important. Ensure that your solution is efficient especially for larger lists of colors. - You are responsible for correctly handling the conversion logic and ensuring that the input values fall within the expected ranges. Notes - Input validation beyond the given constraints is not required. - Remember to handle the potential inaccuracies that might arise from floating-point arithmetic.","solution":"from typing import List, Tuple import colorsys def convert_colors(rgb_colors: List[Tuple[float, float, float]], target_space: str) -> List[Tuple[float, float, float]]: def rgb_to_target_space(r, g, b, target_space): if target_space == \\"YIQ\\": return colorsys.rgb_to_yiq(r, g, b) elif target_space == \\"HLS\\": return colorsys.rgb_to_hls(r, g, b) elif target_space == \\"HSV\\": return colorsys.rgb_to_hsv(r, g, b) else: raise ValueError(f\\"Unknown target space: \'{target_space}\'\\") def target_space_to_rgb(c1, c2, c3, target_space): if target_space == \\"YIQ\\": return colorsys.yiq_to_rgb(c1, c2, c3) elif target_space == \\"HLS\\": return colorsys.hls_to_rgb(c1, c2, c3) elif target_space == \\"HSV\\": return colorsys.hsv_to_rgb(c1, c2, c3) else: raise ValueError(f\\"Unknown target space: \'{target_space}\'\\") output_colors = [] for r, g, b in rgb_colors: c1, c2, c3 = rgb_to_target_space(r, g, b, target_space) rc, gc, bc = target_space_to_rgb(c1, c2, c3, target_space) output_colors.append((rc, gc, bc)) return output_colors"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s object-oriented interface to create, customize, and interpret advanced plots. **Scenario:** You are part of a data analytics team that is tasked with visualizing the spending on health expenditures by different countries over the years. Using the Seaborn library and its new object-oriented interface, you need to create a detailed visualization to present during a board meeting. **Tasks:** 1. Load the `healthexp` dataset provided by Seaborn and prepare it for plotting. You should interpolate missing values and format the dataset appropriately. 2. Create a multi-faceted area plot that shows health expenditure (`Spending_USD`) over the years (`Year`) for each country. 3. Customize the plot to: - Use different colors for different countries. - Include both area and line representations within the same plot. - Stack the areas to show the part-to-whole relationships of expenditures between countries. 4. Additional customization to aesthetically enhance your plot (e.g., adjusting transparency, edge colors). # Guidelines - Your function should take no parameters and should handle all tasks within it. - Outputs should be displayed as the final plot, ready for inclusion in the board meeting presentation. # Expected Output - A multi-faceted area plot with line overlays showing health expenditure trends, with each subplot representing a different country. The visualization should be aesthetically customized to clearly and effectively present the data. **Function Signature:** ```python def create_health_expenditure_plot(): # Step 1: Load and prepare the dataset # Step 2: Create multi-faceted area plot with customizations # Step 3: Display the plot pass ``` **Note**: Make sure your plot is clear, informative, and visually appealing for presentation purposes.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_health_expenditure_plot(): # Step 1: Load and prepare the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Interpolating missing values healthexp.interpolate(method=\'linear\', inplace=True) # Step 2: Create multi-faceted area plot with customizations g = sns.FacetGrid(healthexp, col=\\"Country\\", col_wrap=4, height=3, aspect=1.5) # Adding area plot g.map_dataframe(sns.lineplot, x=\'Year\', y=\'Spending_USD\', estimator=None, color=\'gray\') g.map_dataframe(sns.lineplot, x=\'Year\', y=\'Spending_USD\', hue=\'Country\', palette=\'tab20\', alpha=0.8) # Customizing plots for ax in g.axes.flat: for area in ax.collections: area.set_alpha(0.7) for line in ax.lines: line.set_linestyle(\'--\') # Adding boundaries and titles g.set_titles(\\"{col_name}\\") g.add_legend() g.set_axis_labels(\\"Year\\", \\"Health Expenditure (USD)\\") # Step 3: Display the plot plt.show()"},{"question":"**Question:** Implementing an Enhanced File Management System using Python Python offers various utilities for interacting with the operating system, such as reading and writing files, navigating the file system, and managing processes. In this question, you are required to implement functions that demonstrate the use of these utilities. # Function 1: `create_file(path, content)` Create a file at the given path and write the provided content to it. Input: - `path` (str): The path where the file should be created. - `content` (str): The content to write into the file. Output: - None Constraints: - Assume the given path is valid. # Function 2: `read_file(path)` Read the content of the file at the given path and return it. Input: - `path` (str): The path of the file to read. Output: - `content` (str): The content of the file. Constraints: - Assume the file exists at the given path. # Function 3: `delete_file(path)` Delete the file at the given path. Input: - `path` (str): The path of the file to delete. Output: - None Constraints: - Assume the file exists at the given path. # Function 4: `list_files(directory)` List all files in the given directory. Input: - `directory` (str): The path of the directory to list files from. Output: - `files` (list): A list of filenames in the directory. Constraints: - Assume the directory exists and only contains files (no subdirectories). # Performance Requirements: - The functions should handle large files and directories efficiently. # Example Usage: ```python # Create a file with some content create_file(\'/path/to/file.txt\', \'Hello, World!\') # Read the content of the file print(read_file(\'/path/to/file.txt\')) # Output: \'Hello, World!\' # List files in a directory print(list_files(\'/path/to/directory\')) # Output: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Delete the file delete_file(\'/path/to/file.txt\') ``` Note: The above example assumes specific file paths and directory structures, but the implementation should work for any valid paths provided as input.","solution":"import os def create_file(path, content): Creates a file at the given path and writes the provided content to it. with open(path, \'w\') as file: file.write(content) def read_file(path): Reads the content of the file at the given path and returns it. with open(path, \'r\') as file: return file.read() def delete_file(path): Deletes the file at the given path. os.remove(path) def list_files(directory): Lists all files in the given directory. return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]"},{"question":"# Question: Retrieve and Process Emails using poplib You are tasked with writing a Python script that connects to a POP3 email server, retrieves all email messages, and processes them by extracting and printing specific information. Your script should perform the following tasks: 1. Establish a connection to the email server. 2. Authenticate using a provided username and password. 3. Retrieve the list of email messages and print the number of messages. 4. For each email message: - Retrieve and print the sender\'s email address. - Retrieve and print the subject of the email. **Instructions:** - Use the `poplib` module to connect to a POP3 server. - The server address, username, and password will be provided as input parameters to your function. - Ensure proper error handling for connection issues, authentication failures, and other potential errors. - Make sure to close the connection properly at the end of the script to unlock the mailbox. **Function Signature:** ```python def retrieve_and_process_emails(server: str, username: str, password: str) -> None: pass ``` **Input:** - `server` (str): The address of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. **Output:** - The function should print the number of messages. - For each email, print the sender\'s email address and the email subject. **Example:** Given the following input: ```python retrieve_and_process_emails(\'pop.example.com\', \'user@example.com\', \'password123\') ``` The output should be similar to: ``` Number of messages: 5 Message 1: From: sender1@example.com Subject: Subject of the first email Message 2: From: sender2@example.com Subject: Subject of the second email ... ``` **Constraints:** - Assume the server is always available and the input parameters are correct. - The function should handle common errors gracefully, and print an appropriate message for connection/authentication failures. **Notes:** - The `POP3` class\'s methods such as `user()`, `pass_()`, `list()`, and `retr()` will be essential to complete the task. - Consider using regular expressions or other string manipulation techniques to parse the email headers if needed.","solution":"import poplib from email import parser import email def retrieve_and_process_emails(server: str, username: str, password: str) -> None: try: # Establish connection to the server pop_conn = poplib.POP3(server) pop_conn.user(username) pop_conn.pass_(password) # Get the number of messages messages = pop_conn.list()[1] print(f\\"Number of messages: {len(messages)}\\") for i, message in enumerate(messages, start=1): # Retrieve the full message response, lines, octets = pop_conn.retr(i) msg_content = b\'rn\'.join(lines).decode(\'utf-8\') msg = email.message_from_string(msg_content) # Parse the email email_from = msg[\'from\'] email_subject = msg[\'subject\'] print(f\\"Message {i}:\\") print(f\\" From: {email_from}\\") print(f\\" Subject: {email_subject}\\") except poplib.error_proto as e: print(f\\"Error: {str(e)}\\") finally: # Ensure the connection is closed to unlock the mailbox try: pop_conn.quit() except: pass"},{"question":"<|Analysis Begin|> The provided documentation focuses on the `abc` module in Python which is used to create Abstract Base Classes (ABCs). The primary functionalities of the `abc` module include: 1. **Abstract Base Classes (ABCs)**: These serve as blueprints for other classes, ensuring derived types implement specific methods. 2. **ABCMeta Metaclass**: This metaclass is used to define ABCs and supports method registration, subclass checking, and customizing subclass behavior. 3. **abc.ABC Helper Class**: This provides an easier way to define ABCs by inheriting from it instead of directly using `ABCMeta`. 4. **Abstract Method Decorators**: These include `@abstractmethod`, used to declare abstract methods that must be implemented by subclasses, and the legacy decorators `@abstractclassmethod`, `@abstractstaticmethod`, and `@abstractproperty`, now largely replaced by combining `@abstractmethod` with `classmethod`, `staticmethod`, and `property`. 5. **Helper functions**: Functions like `get_cache_token()` and `update_abstractmethods()` to interact with the ABC mechanism. Understanding these concepts, students should demonstrate their ability to create an abstract base class, declare abstract methods, register virtual subclasses, and implement concrete classes from these abstract bases. <|Analysis End|> <|Question Begin|> # Abstract Base Classes in Python with the `abc` Module You are required to demonstrate your understanding of the `abc` module by designing a small system for a library management system. This system includes different types of library items such as books and magazines. The items have common behaviors but also some unique characteristics. # Requirements 1. Create an abstract base class `LibraryItem` using the `abc` module with the following specifications: - It should have an abstract method `checkout()`, which will handle the logic for checking out an item. - An abstract method `return_item()`, which will handle the logic for returning an item. - A concrete method `get_description()` which returns a general description of the item (you can make a simple string return for this). 2. Create a concrete class `Book` that: - Inherits from `LibraryItem`. - Implements the `checkout()` and `return_item()` methods. For simplicity, these methods can just print out messages like \\"Checking out a book\\" and \\"Returning a book\\". - Overrides the `get_description()` method to include the title and author of the book. 3. Create another concrete class `Magazine` that: - Inherits from `LibraryItem`. - Implements the `checkout()` and `return_item()` methods. These methods should print out messages like \\"Checking out a magazine\\" and \\"Returning a magazine\\". - Overrides the `get_description()` method to include the title and issue number of the magazine. 4. Register a virtual subclass `DigitalItem` that is not a direct subclass of `LibraryItem` but should be considered as such. Implement the following for `DigitalItem`: - Include methods `checkout()`, `return_item()`, and `get_description()` with appropriate behaviors. # Input and Output Specifications - No input data is required from the user. - The methods will output print statements to simulate their behaviors. # Constraints and Limitations - Make sure to use the `abc` module features as specified. - Ensure that your design respects the hierarchy and contract of abstract classes. # Example Call ```python # Example of how the classes might be used: book = Book(title=\\"1984\\", author=\\"George Orwell\\") magazine = Magazine(title=\\"National Geographic\\", issue=\\"March 2021\\") digital_item = DigitalItem() items = [book, magazine, digital_item] for item in items: print(item.get_description()) item.checkout() item.return_item() ``` # Expected Output ```plaintext Title: 1984, Author: George Orwell Checking out a book Returning a book Title: National Geographic, Issue: March 2021 Checking out a magazine Returning a magazine Description of a digital item Checking out a digital item Returning a digital item ``` Please ensure your implementation fulfills all the requirements.","solution":"from abc import ABC, abstractmethod class LibraryItem(ABC): @abstractmethod def checkout(self): pass @abstractmethod def return_item(self): pass def get_description(self): return \\"This is a library item.\\" class Book(LibraryItem): def __init__(self, title, author): self.title = title self.author = author def checkout(self): print(\\"Checking out a book\\") def return_item(self): print(\\"Returning a book\\") def get_description(self): return f\\"Title: {self.title}, Author: {self.author}\\" class Magazine(LibraryItem): def __init__(self, title, issue): self.title = title self.issue = issue def checkout(self): print(\\"Checking out a magazine\\") def return_item(self): print(\\"Returning a magazine\\") def get_description(self): return f\\"Title: {self.title}, Issue: {self.issue}\\" class DigitalItem: def checkout(self): print(\\"Checking out a digital item\\") def return_item(self): print(\\"Returning a digital item\\") def get_description(self): return \\"Description of a digital item\\" LibraryItem.register(DigitalItem)"},{"question":"Background: Regular expressions are a powerful tool in Python for string matching and manipulation. The `re` module allows you to define patterns that can be used to search, split, and modify strings efficiently. Problem Statement: Write a Python function `extract_and_replace_emails` which extracts all email addresses from the given text and replaces them with the word \\"<email>\\" while maintaining all other content intact. An email address is defined as any sequence of characters matching the pattern `[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}`. Function Signature: ```python def extract_and_replace_emails(text: str) -> (list, str): pass ``` Input: - `text` (str): a string that may contain zero or more email addresses. Output: - A tuple containing: - `emails` (list): a list of all email addresses found in the text in the order they appeared. - `modified_text` (str): the input text with all email addresses replaced by the string \\"<email>\\". Constraints: - Use raw strings for defining regular expressions. - Use named groups or backreferences where applicable. - The function should be case-insensitive when matching email addresses. Examples: ```python # Example 1 text = \\"Please contact us at support@example.com or sales@example.org.\\" emails, modified_text = extract_and_replace_emails(text) # The function should return: # emails = [\\"support@example.com\\", \\"sales@example.org\\"] # modified_text = \\"Please contact us at <email> or <email>.\\" # Example 2 text = \\"For questions, email John.Doe123@example.com or jane_doe@example.co.uk.\\" emails, modified_text = extract_and_replace_emails(text) # The function should return: # emails = [\\"John.Doe123@example.com\\", \\"jane_doe@example.co.uk\\"] # modified_text = \\"For questions, email <email> or <email>.\\" ``` Notes: - Ensure your solution is efficient and handles text with a large number of characters. - Carefully handle edge cases such as no emails in the text or emails at the start/end of the text.","solution":"import re def extract_and_replace_emails(text: str) -> (list, str): Extracts all email addresses from the given text and replaces them with the word \\"<email>\\" while maintaining all other content intact. Args: text (str): The input text containing zero or more email addresses. Returns: tuple: A tuple containing a list of emails and the modified text. email_pattern = re.compile(r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\', re.IGNORECASE) # Find all email addresses in the text emails = email_pattern.findall(text) # Replace all email addresses with \\"<email>\\" modified_text = email_pattern.sub(\'<email>\', text) return emails, modified_text"},{"question":"**Coding Assessment Question** # Objective Your task is to implement a function that trains a single-layer neural network on a random dataset using PyTorch, ensuring that the results are completely reproducible. You need to control for randomness from multiple sources including PyTorch, Python\'s `random` module, and NumPy. # Function Signature ```python def train_reproducible_network(seed: int) -> torch.Tensor: pass ``` # Input - `seed` (int): The seed value to be used for all random number generators to ensure reproducibility. # Output - `result` (torch.Tensor): The output tensor of the trained neural network on a fixed input tensor. # Constraints 1. You must use the provided seed to ensure that the results are reproducible. 2. Your implementation should control for sources of randomness from PyTorch, Python\'s `random` module, and NumPy. 3. The single-layer neural network should be trained on a random dataset. 4. Use deterministic algorithms wherever possible. # Instructions 1. Set up all necessary random seeds. 2. Generate a random dataset using PyTorch. 3. Define and train a single-layer neural network on the generated dataset. 4. Ensure all operations that may introduce nondeterminism are controlled. 5. Evaluate the network on a fixed input tensor and return the result. # Example ```python import torch import random import numpy as np def train_reproducible_network(seed: int) -> torch.Tensor: # Setting seeds for reproducibility torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # Ensure deterministic algorithms are used torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) # Create a random dataset input_size = 10 num_samples = 100 X = torch.randn(num_samples, input_size) Y = torch.randn(num_samples, 1) # Define a simple single-layer neural network model = torch.nn.Linear(input_size, 1) criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Train the network num_epochs = 20 for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, Y) loss.backward() optimizer.step() # Evaluate the network on a fixed input tensor and return result test_input = torch.randn(1, input_size) result = model(test_input) return result # Example usage with seed 42 output = train_reproducible_network(42) print(output) ``` In this example, with seed `42`, the function should return a reproducible output tensor when the code is run on the same platform and environment. You need to implement the detailed function based on the given description and constraints. Ensure that your implementation is correct and returns consistent results when executed multiple times with the same seed value.","solution":"import torch import random import numpy as np def train_reproducible_network(seed: int) -> torch.Tensor: # Setting seeds for reproducibility torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # Ensure deterministic algorithms are used torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) # Create a random dataset input_size = 10 num_samples = 100 X = torch.randn(num_samples, input_size) Y = torch.randn(num_samples, 1) # Define a simple single-layer neural network model = torch.nn.Linear(input_size, 1) criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Train the network num_epochs = 20 for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, Y) loss.backward() optimizer.step() # Evaluate the network on a fixed input tensor and return result test_input = torch.randn(1, input_size) result = model(test_input) return result"},{"question":"**Objective**: Write a Python script that reads a file and prints its content line by line. Ensure that the script correctly handles resource management by explicitly closing the file and avoids any errors or warnings when run in Python Development Mode. # Description You are required to implement a function called `read_file(filename: str) -> None` which accepts a filename as a string parameter. The function should read and print each line of the file without causing any `ResourceWarning` or any `Bad file descriptor` errors when executed in Python Development Mode. # Requirements: 1. Open the given file and read it line by line. 2. Explicitly close the file after reading it. 3. Handle any potential errors that might arise from opening and reading the file. 4. Your implementation should avoid any warnings or errors related to resource management when executed with the `-X dev` option. # Constraints: - The function will be tested with text files containing multiple lines. - You should assume the file exists and is readable for this exercise. # Input: - A string `filename` representing the path to the file. # Output: - The content of the file will be printed line by line. # Examples: Example 1: ```python # Assume \'example.txt\' contains: # Hello World # This is a test file. read_file(\'example.txt\') ``` Output: ``` Hello World This is a test file. ``` Example 2: ```python # Assume \'example2.txt\' contains: # Line 1 # Line 2 # Line 3 read_file(\'example2.txt\') ``` Output: ``` Line 1 Line 2 Line 3 ``` # Notes: - Ensure your function handles resources explicitly to avoid `ResourceWarning`. - When testing your function, execute the script with `python3 -X dev your_script.py` to ensure it runs without any warnings or errors. **Additional Task**: Mention any extra steps or changes you made to ensure your code complies with the Python Development Mode requirements.","solution":"def read_file(filename: str) -> None: Reads the content of the specified file and prints each line. Parameters: filename (str): The path to the file to be read. Returns: None try: with open(filename, \'r\') as file: for line in file: print(line, end=\'\') except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective: The task is to create a simplified version of IDLE\'s \\"Find in Files\\" feature for searching text within multiple files in a directory. # Problem Statement: Implement a function `find_in_files(directory, search_term)` that searches for a specific `search_term` in all `.py` and `.txt` files within the given `directory` (including subdirectories) and prints out the file paths and the line numbers where the search term is found. # Input: - `directory` (str): The path of the directory to search within (e.g., `\'/path/to/directory\'`). - `search_term` (str): The term to search for within `.py` and `.txt` files. # Output: - Print each occurrence of the search term in the format: `File: \'file_path\', Line: \'line_number\'` - Example: `File: \'/path/to/directory/file1.py\', Line: 3` # Constraints: 1. Search should be case-insensitive. 2. Handle the scenario where the directory does not exist or is inaccessible. 3. Handle potential file reading errors gracefully and continue searching other files. # Requirements: 1. Use Python\'s built-in libraries `os` and `re`. 2. Ensure efficient search using file I/O and string search methods. 3. Implement meaningful error handling for edge cases (e.g., file read errors, non-existent directories). # Function Signature: ```python def find_in_files(directory: str, search_term: str) -> None: pass ``` # Example: ```python # Suppose \'/path/to/directory\' contains: # - file1.py with content \\"print(\'hello world\')\\" # - file2.txt with content \\"Hello Python\\" # - file3.md (should be ignored) # Calling the function as: find_in_files(\'/path/to/directory\', \'hello\') # Expected Output: File: \'/path/to/directory/file1.py\', Line: 1 File: \'/path/to/directory/file2.txt\', Line: 1 ``` # Note: Make sure to handle edge cases and use efficient file search and read operations to adhere to performance requirements, especially for large directories.","solution":"import os import re def find_in_files(directory: str, search_term: str) -> None: Searches for a specific search_term in all .py and .txt files within the given directory (including subdirectories). Prints out the file paths and the line numbers where the search term is found. Args: - directory (str): The path of the directory to search within. - search_term (str): The term to search for within the files. Returns: - None # Check if the directory exists if not os.path.isdir(directory): print(f\\"Directory \'{directory}\' does not exist or is not accessible.\\") return # Compile the search term for case-insensitive matching search_pattern = re.compile(re.escape(search_term), re.IGNORECASE) # Walk through the directory for root, _, files in os.walk(directory): for file in files: if file.endswith((\'.py\', \'.txt\')): file_path = os.path.join(root, file) try: with open(file_path, \'r\', encoding=\'utf-8\') as f: for line_number, line in enumerate(f, 1): if search_pattern.search(line): print(f\\"File: \'{file_path}\', Line: {line_number}\\") except (IOError, OSError) as e: print(f\\"Error reading file \'{file_path}\': {e}\\")"},{"question":"Objective: Your task is to implement a function that takes two tensors, performs element-wise addition, and returns the result. The function should ensure proper type promotion and broadcasting of tensors before performing the addition. You must use the Prims IR operations to handle type promotion and broadcasting explicitly. Function Signature: ```python import torch def prims_elementwise_add(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor1`: A PyTorch tensor of any shape and data type. - `tensor2`: A PyTorch tensor of any shape and data type. Output: - A PyTorch tensor resulting from the element-wise addition of `tensor1` and `tensor2`, with correct type promotion and broadcasting applied. Constraints: - You may assume that the input tensors are compatible for broadcasting. - Utilize `prims.convert_element_type` for type promotion. - Utilize `prims.broadcast_in_dim` for broadcasting. Example: ```python # Example tensors tensor1 = torch.tensor([1, 2, 3], dtype=torch.int32) tensor2 = torch.tensor([[1.5], [2.5]], dtype=torch.float32) # Expected output tensor after broadcasting and addition # tensor([[2.5, 3.5, 4.5], # [3.5, 4.5, 5.5]], dtype=torch.float32) result = prims_elementwise_add(tensor1, tensor2) print(result) ``` Notes: - Make sure to consider the different data types of the tensors and appropriately handle type promotions. - Ensure that the solution is efficient and leverages the primitives provided for type promotion and broadcasting.","solution":"import torch def prims_elementwise_add(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: # Determine the common dtype according to PyTorch type promotion rules common_dtype = torch.promote_types(tensor1.dtype, tensor2.dtype) # Convert both tensors to the common dtype tensor1 = tensor1.to(common_dtype) tensor2 = tensor2.to(common_dtype) # Broadcast tensors to the same shape broadcasted_tensor1, broadcasted_tensor2 = torch.broadcast_tensors(tensor1, tensor2) # Perform element-wise addition result = broadcasted_tensor1 + broadcasted_tensor2 return result"},{"question":"# MIME Email Construction Challenge **Objective:** Write a Python function to construct a MIME email message that includes multiple types of content: plain text, an image, and an audio file. The function should return the serialized string representation of the entire email message. **Function Signature:** ```python def create_mime_email(text_content: str, image_data: bytes, audio_data: bytes, image_subtype: str = \\"jpeg\\", audio_subtype: str = \\"wav\\") -> str: pass ``` **Input:** - `text_content` (str): The plain text content to be included in the email. - `image_data` (bytes): The bytes of the image file to be included in the email. - `audio_data` (bytes): The bytes of the audio file to be included in the email. - `image_subtype` (str): The specific subtype of the image content (default is \\"jpeg\\"). - `audio_subtype` (str): The specific subtype of the audio content (default is \\"wav\\"). **Output:** - Returns a string that represents the entire MIME email message, ready to be sent. **Constraints:** - The email message must have a single plain text part, an image part, and an audio part. - All parts must be correctly encoded using base64 encoding. - Ensure that appropriate MIME headers are set for each part. - Use multipart/mixed as the main MIME type for the email message. **Performance Requirements:** - The function should handle large sizes of text, image, and audio data efficiently. - Proper error handling should be in place to manage invalid input data. # Example: ```python text_content = \\"This is a test email with text, image, and audio.\\" with open(\\"example.jpg\\", \\"rb\\") as img_file, open(\\"example.wav\\", \\"rb\\") as audio_file: image_data = img_file.read() audio_data = audio_file.read() mime_email = create_mime_email(text_content, image_data, audio_data, \\"jpeg\\", \\"wav\\") print(mime_email) ``` The above function call should construct a MIME email that includes the provided plain text content, image, and audio, and then output its serialized string representation. **Note:** 1. You are NOT to use any external libraries other than the ones provided in the Python Standard Library. 2. Ensure the MIME headers such as `Content-Type`, `MIME-Version`, and `Content-Transfer-Encoding` are correctly set for each part. Good luck!","solution":"import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders def create_mime_email(text_content: str, image_data: bytes, audio_data: bytes, image_subtype: str = \\"jpeg\\", audio_subtype: str = \\"wav\\") -> str: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Test email with text, image, and audio\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Attach the plain text part text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach the image part image_part = MIMEBase(\'image\', image_subtype) image_part.set_payload(image_data) encoders.encode_base64(image_part) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=f\'image.{image_subtype}\') msg.attach(image_part) # Attach the audio part audio_part = MIMEBase(\'audio\', audio_subtype) audio_part.set_payload(audio_data) encoders.encode_base64(audio_part) audio_part.add_header(\'Content-Disposition\', \'attachment\', filename=f\'audio.{audio_subtype}\') msg.attach(audio_part) return msg.as_string()"},{"question":"Optimize Matrix Multiplication with MPS Objective Write a PyTorch program to perform and evaluate matrix multiplication on an MPS device, including memory management, device synchronization, and performance profiling. Problem Statement You are required to implement a PyTorch script that: 1. Checks if an MPS device is available. 2. Generates two large random matrices (`matrix_A` of shape [1000, 2000] and `matrix_B` of shape [2000, 1000]) and moves them to the MPS device if available. 3. Multiplies these two matrices and retrieves the result. 4. Records the elapsed time for the matrix multiplication using MPS profiling. 5. Reports memory usage before and after the operation. 6. Ensures synchronization across the device during the operation. Input and Output Formats **Input:** - No input required; the matrices are generated within the script. **Output:** - Print statements indicating: - Whether an MPS device is available. - The time taken for matrix multiplication. - Memory usage before and after matrix multiplication. Requirements - Use appropriate functions from `torch.mps` to handle device properties, memory management, and MPS profiling. - Ensure the program handles cases where an MPS device is not available by falling back to CPU. Constraints - You must use `torch.mps` functions where applicable. - The solution should be efficient and avoid unnecessary computations or memory usage. Performance - The solution should efficiently handle memory management and avoid memory leakage. - The expected time complexity for matrix multiplication should mainly depend on the dimensions of the input matrices. Example ```python import torch import time def main(): # Check for MPS availability if torch.backends.mps.is_available(): mps_device = torch.device(\'mps\') print(\'MPS device is available.\') else: mps_device = torch.device(\'cpu\') print(\'MPS device is not available, falling back to CPU.\') # Generate random matrices matrix_A = torch.randn(1000, 2000, device=mps_device) matrix_B = torch.randn(2000, 1000, device=mps_device) # Clear memory cache before operation torch.mps.empty_cache() # Get memory usage before operation mem_before = torch.mps.current_allocated_memory() # Start profiling torch.mps.profiler.start() # Perform matrix multiplication start_time = time.time() result = torch.matmul(matrix_A, matrix_B) end_time = time.time() # Synchronize device torch.mps.synchronize() # Stop profiling torch.mps.profiler.stop() # Get memory usage after operation mem_after = torch.mps.current_allocated_memory() # Print results print(f\'Time taken for matrix multiplication: {end_time - start_time} seconds\') print(f\'Memory usage before operation: {mem_before} bytes\') print(f\'Memory usage after operation: {mem_after} bytes\') if __name__ == \'__main__\': main() ```","solution":"import torch import time def matrix_multiplication_with_mps(): # Check for MPS availability if torch.backends.mps.is_available(): mps_device = torch.device(\'mps\') print(\'MPS device is available.\') else: mps_device = torch.device(\'cpu\') print(\'MPS device is not available, falling back to CPU.\') # Generate random matrices matrix_A = torch.randn(1000, 2000, device=mps_device) matrix_B = torch.randn(2000, 1000, device=mps_device) # Clear memory cache before operation if mps_device.type == \'mps\': torch.mps.empty_cache() # Get memory usage before operation mem_before = torch.mps.current_allocated_memory() if mps_device.type == \'mps\' else 0 # Perform matrix multiplication start_time = time.time() result = torch.matmul(matrix_A, matrix_B) end_time = time.time() # Synchronize device if mps_device.type == \'mps\': torch.mps.synchronize() # Get memory usage after operation mem_after = torch.mps.current_allocated_memory() if mps_device.type == \'mps\' else 0 # Print results print(f\'Time taken for matrix multiplication: {end_time - start_time} seconds\') print(f\'Memory usage before operation: {mem_before} bytes\') print(f\'Memory usage after operation: {mem_after} bytes\') return result, end_time - start_time, mem_before, mem_after"},{"question":"# WSGI Server Implementation Task Your task is to create a WSGI server using the `wsgiref` package that serves different types of content based on the path of the HTTP request. Requirements 1. **Handling Different Endpoints**: - If the path is `/`, the server should return a welcome message (`\\"Welcome to the WSGI Server!\\"`). - If the path is `/hello/<name>`, the server should return a greeting message (`\\"Hello, <name>!\\"`), where `<name>` is a placeholder for any string that follows `/hello/`. - For any other path, the server should return a `404 Not Found` message. 2. **Content Type**: - All responses should have the content type `text/plain; charset=utf-8`. 3. **Server Setup**: - Use `wsgiref.simple_server.make_server` to create and run the server. 4. **Input and Output**: - The WSGI callable (application) should accept two parameters: `environ` (a dictionary containing CGI-style environment variables) and `start_response` (a callable for starting the HTTP response). - The application should return an iterable containing the byte strings of the response body. 5. **Performance**: - The server should efficiently handle multiple requests at a time and properly initialize and finalize the environment for each request. Please implement the WSGI application and set up the server according to the outlined requirements. Example The following is an example of how the server should behave when accessed at different paths: - Accessing `http://localhost:8000/` should return a response with body `Welcome to the WSGI Server!`. - Accessing `http://localhost:8000/hello/John` should return a response with body `Hello, John!`. - Accessing `http://localhost:8000/unknown` should return a `404 Not Found` response. Implementation Guidelines 1. Define a function named `app` which serves as the WSGI application. 2. Implement the required functionality within the `app` function to handle different paths. 3. Use the `make_server` function from `wsgiref.simple_server` to set up and run the server. 4. Handle headers appropriately using `start_response`. ```python from wsgiref.simple_server import make_server def app(environ, start_response): # Implement the application logic here pass if __name__ == \'__main__\': # Set up and run the server with make_server(\'\', 8000, app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Ensure your implementation meets the outlined requirements on handling different endpoints and returns appropriate responses.","solution":"from wsgiref.simple_server import make_server import re def app(environ, start_response): path = environ.get(\'PATH_INFO\', \'\') if path == \'/\': status = \'200 OK\' response_body = \\"Welcome to the WSGI Server!\\" elif re.match(r\'^/hello/w+\', path): name = path.split(\\"/\\")[2] status = \'200 OK\' response_body = f\\"Hello, {name}!\\" else: status = \'404 Not Found\' response_body = \\"404 Not Found\\" headers = [(\'Content-Type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"**Assessment Question: Creating a Directory Backup Function** # Objective: Design a function that will create a backup of a specified directory by copying its entire content to a new location and then compressing it into a zip archive. The function should handle a variety of file types and structures, including symbolic links. # Function Signature: ```python def backup_directory(source: str, backup_location: str, archive_name: str) -> str: Creates a backup of the specified source directory by copying its content to the backup location and then compresses it into a zip archive. Args: source (str): The path to the source directory to be backed up. backup_location (str): The path to the directory where the backup will be stored. archive_name (str): The name of the resulting zip archive (without extension). Returns: str: The path to the created zip archive. Raises: ValueError: If the source directory does not exist. OSError: If there is an error during the copying or archiving process. pass ``` # Requirements and Constraints: 1. **Directory and File Operations**: - Use `shutil.copytree` to recursively copy the contents of the `source` directory to the `backup_location`. - The function should handle existing directories and ensure no data is lost during the copy process. 2. **Symbolic Links**: - Ensure that symbolic links are handled correctly. They should be included as symbolic links in the backup, not their resolved targets. 3. **Compressing the Backup**: - Use `shutil.make_archive` to compress the copied backup directory into a zip file. - The zip file should be named as specified by `archive_name` and stored in the `backup_location`. 4. **Error Handling**: - Raise a `ValueError` if the `source` directory does not exist. - Properly handle any `OSError` that may arise during the file copying or archiving process. 5. **Output**: - The function should return the path to the created zip archive. # Example Usage: ```python # Assuming the provided paths are valid and the necessary permissions are in place backup_path = backup_directory(\'/path/to/source\', \'/path/to/backup\', \'backup_archive\') print(f\\"Backup created at: {backup_path}\\") ``` # Notes: - You are encouraged to use the `shutil` module functions such as `copytree`, `make_archive`, and handle exceptions appropriately. - Ensure your code is efficient and handles large directories with various file types and symbolic links. - Be mindful of performance considerations, especially when dealing with network drives or slow disk I/O.","solution":"import os import shutil def backup_directory(source: str, backup_location: str, archive_name: str) -> str: Creates a backup of the specified source directory by copying its content to the backup location and then compresses it into a zip archive. Args: source (str): The path to the source directory to be backed up. backup_location (str): The path to the directory where the backup will be stored. archive_name (str): The name of the resulting zip archive (without extension). Returns: str: The path to the created zip archive. Raises: ValueError: If the source directory does not exist. OSError: If there is an error during the copying or archiving process. if not os.path.exists(source): raise ValueError(f\\"Source directory \'{source}\' does not exist.\\") # Ensure the backup location exists if not os.path.exists(backup_location): os.makedirs(backup_location) # Form the temporary backup path temp_backup_path = os.path.join(backup_location, \'temp_backup\') # Copy the directory tree try: if os.path.exists(temp_backup_path): shutil.rmtree(temp_backup_path) shutil.copytree(source, temp_backup_path, symlinks=True) except OSError as e: raise OSError(f\\"Error while copying directory: {e}\\") # Create the archive try: archive_path = shutil.make_archive( os.path.join(backup_location, archive_name), \'zip\', temp_backup_path ) except OSError as e: raise OSError(f\\"Error while creating archive: {e}\\") finally: # Cleanup the temporary backup directory shutil.rmtree(temp_backup_path) return archive_path"},{"question":"**Objective**: This question assesses your understanding of numeric comparison functions in PyTorch under varied conditions. # Problem Statement You are given two sets of tensors – one representing the outputs of a model with default precision (FP32), and the other with quantized precision (INT8). Your task is to implement a function that uses the provided utility functions to compare these tensors and summarize the results. # Function Signature ```python def compare_tensors(fp32_tensors, int8_tensors): Compare the given sets of FP32 and INT8 tensors using SQNR, normalized L2 error, and cosine similarity. Arguments: fp32_tensors (List[torch.Tensor]): List of tensors in FP32 precision. int8_tensors (List[torch.Tensor]): List of tensors in INT8 precision. Returns: dict: Dictionary with \'SQNR\', \'L2\', and \'CosineSimilarity\' as keys and lists of respective metric values for each tensor pair. pass ``` # Input - `fp32_tensors` (List[torch.Tensor]): A list of tensors, each in floating-point precision (FP32). - `int8_tensors` (List[torch.Tensor]): A list of tensors, each in quantized precision (INT8). # Output - Returns a dictionary with the following keys: - `\'SQNR\'`: List of floats representing the Signal-to-Quantization-Noise Ratio for each pair. - `\'L2\'`: List of floats representing the normalized L2 error for each pair. - `\'CosineSimilarity\'`: List of floats representing the cosine similarity for each pair. # Constraints 1. The input tensor lists, `fp32_tensors` and `int8_tensors`, will have the same length. 2. Elements at a given index in `fp32_tensors` and `int8_tensors` correspond to the same model output before and after quantization respectively. 3. Use the provided utility functions for the metric calculations. # Example ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Example input tensors fp32_tensors = [torch.randn(10, 10) for _ in range(5)] int8_tensors = [torch.randint(-128, 127, (10, 10)).to(torch.float32) for _ in range(5)] # Call the function with the example tensors comparison_results = compare_tensors(fp32_tensors, int8_tensors) print(comparison_results) ``` **Expected output**: ```python { \\"SQNR\\": [value1, value2, value3, value4, value5], \\"L2\\": [value1, value2, value3, value4, value5], \\"CosineSimilarity\\": [value1, value2, value3, value4, value5] } ``` These lists will contain the respective metric comparisons for each tensor pair in the input lists. # Notes - Make sure to handle exceptions where tensor shapes do not match and document your code appropriately. - Consider the performance implications when dealing with large tensors. Good luck!","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compare_tensors(fp32_tensors, int8_tensors): Compare the given sets of FP32 and INT8 tensors using SQNR, normalized L2 error, and cosine similarity. Arguments: fp32_tensors (List[torch.Tensor]): List of tensors in FP32 precision. int8_tensors (List[torch.Tensor]): List of tensors in INT8 precision. Returns: dict: Dictionary with \'SQNR\', \'L2\', and \'CosineSimilarity\' as keys and lists of respective metric values for each tensor pair. # Initialize lists to store metric values for each pair of tensors sqnr_values = [] l2_values = [] cosine_similarity_values = [] # Iterate over tensor pairs for fp32_tensor, int8_tensor in zip(fp32_tensors, int8_tensors): # Compute metrics and append to lists sqnr_values.append(compute_sqnr(fp32_tensor, int8_tensor).item()) l2_values.append(compute_normalized_l2_error(fp32_tensor, int8_tensor).item()) cosine_similarity_values.append(compute_cosine_similarity(fp32_tensor, int8_tensor).item()) return { \\"SQNR\\": sqnr_values, \\"L2\\": l2_values, \\"CosineSimilarity\\": cosine_similarity_values }"},{"question":"# Question: Customizing Seaborn Plot Themes and Display You are tasked with creating a seaborn plot with specific theme and display settings and reverting them to defaults. The goal of this question is to assess your ability to configure seaborn plot settings using the `seaborn.objects` module. Requirements: 1. **Theme Configuration**: - Set the `axes.facecolor` to \\"white\\". - Update the overall style to \\"whitegrid\\". - Sync the theme with matplotlib\'s global settings. - Reset the theme back to seaborn defaults. 2. **Display Configuration**: - Change the display format to \\"svg\\". - Disable HiDPI scaling. - Set the display scaling factor to 0.7. 3. **Create a Simple Plot**: - Generate a scatter plot using the seaborn objects interface. Input: You are not required to take any input from the user. Simply use the example data provided. Output: The function should not return anything but should demonstrate the configured plot inline in a Jupyter notebook. Constraints: - Use the seaborn.objects interface for theming and plotting. - Ensure that all changes are reverted to their defaults as specified. Example: The following code shows an example of how the function should behave: ```python import seaborn as sns import pandas as pd import seaborn.objects as so import numpy as np # Example data np.random.seed(10) data = pd.DataFrame({ \\"x\\": np.random.randn(100), \\"y\\": np.random.randn(100) }) # Function implementation def customize_seaborn_plot(): # Theme Configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) import matplotlib as mpl so.Plot.config.theme.update(mpl.rcParams) so.Plot.config.theme.reset() # Display Configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 # Create a simple scatter plot p = so.Plot(data, x=\\"x\\", y=\\"y\\").add(so.Dot()) display(p) # Execute the function customize_seaborn_plot() ``` This example sets the theme and display configurations as per the requirements, creates a scatter plot with the example data, and displays it. Ensure your code performs these tasks correctly and reverts to defaults where specified.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so import numpy as np from IPython.display import display # Example data np.random.seed(10) data = pd.DataFrame({ \\"x\\": np.random.randn(100), \\"y\\": np.random.randn(100) }) def customize_seaborn_plot(): # Theme Configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(sns.axes_style(\\"whitegrid\\")) import matplotlib as mpl so.Plot.config.theme.update(mpl.rcParams) so.Plot.config.theme.reset() # Display Configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 # Create a simple scatter plot p = so.Plot(data, x=\\"x\\", y=\\"y\\").add(so.Dot()) display(p) # Execute the function customize_seaborn_plot()"},{"question":"Objective The purpose of this assessment is to evaluate your understanding and usage of the `sysconfig` module in Python. Your task is to write a function that retrieves and prints specific Python installation paths based on the platform and installation options. Task Write a function called `display_installation_paths` that does the following: 1. Retrieves the current platform name using `sysconfig`. 2. Retrieves the default installation scheme for the current platform. 3. Retrieves and prints paths for the following components: - `stdlib`: directory containing the standard Python library files that are not platform-specific. - `purelib`: directory for site-specific, non-platform-specific files. - `scripts`: directory for script files. 4. Retrieves and prints the value for the configuration variable `Py_ENABLE_SHARED`. Function Signature ```python import sysconfig def display_installation_paths(): # Your implementation here ``` Requirements 1. The function should print: - The current platform name. - The default installation scheme for the current platform. - The paths for `stdlib`, `purelib`, and `scripts` for the current installation scheme. - The value for the `Py_ENABLE_SHARED` configuration variable. 2. If any of the path names or the configuration variable does not exist, the function should handle it gracefully and print `Not available`. Example Output ``` Platform: macosx-10.15-x86_64 Default Scheme: posix_prefix stdlib Path: /usr/local/lib/python3.8 purelib Path: /usr/local/lib/python3.8/site-packages scripts Path: /usr/local/bin Py_ENABLE_SHARED: 0 ``` Ensure that your function uses appropriate functions from the `sysconfig` module to retrieve the required information. Constraints - The solution should handle different platforms and installation schemes appropriately. - The solution should handle cases where certain paths or configuration variables may not be available and provide a clear indication in the output. Performance Requirements - The function should operate efficiently within the constraints of typical Python runtime environments.","solution":"import sysconfig def display_installation_paths(): # Retrieve platform name platform_name = sysconfig.get_platform() # Retrieve default installation scheme default_scheme = sysconfig.get_default_scheme() # Retrieve the paths for stdlib, purelib, and scripts stdlib_path = sysconfig.get_path(\'stdlib\') purelib_path = sysconfig.get_path(\'purelib\') scripts_path = sysconfig.get_path(\'scripts\') # Retrieve the configuration variable for Py_ENABLE_SHARED py_enable_shared = sysconfig.get_config_var(\'Py_ENABLE_SHARED\') # Print the results print(f\\"Platform: {platform_name}\\") print(f\\"Default Scheme: {default_scheme}\\") print(f\\"stdlib Path: {stdlib_path if stdlib_path else \'Not available\'}\\") print(f\\"purelib Path: {purelib_path if purelib_path else \'Not available\'}\\") print(f\\"scripts Path: {scripts_path if scripts_path else \'Not available\'}\\") print(f\\"Py_ENABLE_SHARED: {py_enable_shared if py_enable_shared is not None else \'Not available\'}\\")"},{"question":"**Problem Statement:** You are required to write a set of functions using the Python \\"os\\" module to manage and retrieve information about files, directories, and processes. Your functions should handle various scenarios as outlined below. **Functions to Implement:** 1. **check_and_get_file_permissions(filepath: str) -> dict** - This function takes a file path as input and checks if the file exists. - If the file exists, it returns a dictionary with the file\'s permissions (\'readable\', \'writable\', \'executable\') as keys and boolean values indicating the availability of each permission. - If the file does not exist, it raises a `FileNotFoundError`. 2. **list_non_hidden_files(directory_path: str) -> list** - This function takes a directory path as input and returns a list of non-hidden files in that directory. - Hidden files are those whose names start with a `.`. 3. **create_directory_with_permissions(directory_path: str, mode: int) -> None** - This function takes a directory path and a mode (permissions setting) as input. - It creates the directory with the specified permissions mode. If the directory already exists, it raises a `FileExistsError`. 4. **get_environment_variable(variable_name: str) -> str** - This function takes the name of an environment variable as input. - It returns the value of the environment variable if it exists; otherwise, it returns `None`. 5. **change_working_directory_and_list_files(directory_path: str) -> list** - This function takes a directory path as input, changes the current working directory to the given path, and returns a list of all files in that directory. - If the directory does not exist, it raises a `FileNotFoundError`. 6. **create_and_write_to_file(filepath: str, content: str) -> None** - This function takes a file path and content as input. - It creates the file (if it does not exist) and writes the content to the file. If the file already exists, the content is overwritten. **Constraints:** - Usage of the Python \\"os\\" module is mandatory for relevant operations. - Proper error handling should be implemented. - Functions should be compatible with both Unix and Windows operating systems where possible. **Function Signatures:** ```python def check_and_get_file_permissions(filepath: str) -> dict: pass def list_non_hidden_files(directory_path: str) -> list: pass def create_directory_with_permissions(directory_path: str, mode: int) -> None: pass def get_environment_variable(variable_name: str) -> str: pass def change_working_directory_and_list_files(directory_path: str) -> list: pass def create_and_write_to_file(filepath: str, content: str) -> None: pass ``` **Example Usage:** ```python # Example 1: print(check_and_get_file_permissions(\'sample.txt\')) # Output: {\'readable\': True, \'writable\': False, \'executable\': False} # Example 2: print(list_non_hidden_files(\'/path/to/directory\')) # Output: [\'file1.txt\', \'file2.py\'] # Example 3: create_directory_with_permissions(\'/path/to/new_directory\', 0o755) # Output: None (directory created) # Example 4: print(get_environment_variable(\'HOME\')) # Output: \'/home/user\' # Example 5: print(change_working_directory_and_list_files(\'/path/to/directory\')) # Output: [\'file1.txt\', \'file2.py\'] # Example 6: create_and_write_to_file(\'example.txt\', \'Hello, World!\') # Output: None (file created and written) ``` Ensure your implementation is robust and thoroughly tested.","solution":"import os def check_and_get_file_permissions(filepath: str) -> dict: Checks if the file exists and returns a dictionary with file permissions. if not os.path.exists(filepath): raise FileNotFoundError(f\\"File \'{filepath}\' not found.\\") permissions = { \'readable\': os.access(filepath, os.R_OK), \'writable\': os.access(filepath, os.W_OK), \'executable\': os.access(filepath, os.X_OK), } return permissions def list_non_hidden_files(directory_path: str) -> list: Returns a list of non-hidden files in the specified directory. if not os.path.isdir(directory_path): raise FileNotFoundError(f\\"Directory \'{directory_path}\' not found.\\") files = [f for f in os.listdir(directory_path) if not f.startswith(\'.\')] return files def create_directory_with_permissions(directory_path: str, mode: int) -> None: Creates a directory with the specified permissions mode. try: os.mkdir(directory_path, mode) except FileExistsError: raise FileExistsError(f\\"Directory \'{directory_path}\' already exists.\\") def get_environment_variable(variable_name: str) -> str: Returns the value of the specified environment variable. return os.environ.get(variable_name) def change_working_directory_and_list_files(directory_path: str) -> list: Changes the working directory and returns a list of all files in the directory. if not os.path.isdir(directory_path): raise FileNotFoundError(f\\"Directory \'{directory_path}\' not found.\\") os.chdir(directory_path) files = os.listdir() return files def create_and_write_to_file(filepath: str, content: str) -> None: Creates a file and writes content to it. with open(filepath, \'w\') as file: file.write(content)"},{"question":"You are given a neural network model that needs to be converted into a TorchScript scriptable model. Your task is to implement the class required to script this model and encapsulate appropriate type annotations, TorchScript types, and methods in alignment with the TorchScript guidelines. # Instructions: 1. Implement a class `MyScriptModule` in Python that extends from `torch.nn.Module`. 2. The class should contain the following: - An `__init__` method that initializes a `torch.nn.Linear` layer. - A `forward` method that takes a tensor `x` as input and performs forward propagation using the initialized linear layer. - A method `inc_first_elem` that takes a tuple with the first element as an integer and the second element as `Any`. It increments the first element by 1 and returns the modified tuple. - Ensure to annotate the types of all parameters, variables, and return types according to TorchScript rules. 3. Script the module using `torch.jit.script` and demonstrate its invocation. # Constraints: - You must use the TorchScript type system, particularly `TSType` and annotations. - Ensure that the TorchScript module class, TorchScript types, and methods follow the documentation guidelines for TorchScript. # Example: ```python import torch from typing import Any, Tuple class MyScriptModule(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): super(MyScriptModule, self).__init__() self.linear = torch.nn.Linear(input_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x) @torch.jit.export def inc_first_elem(self, x: Tuple[int, Any]) -> Tuple[int, Any]: return (x[0] + 1, x[1]) # Scripting the Module my_script_module = torch.jit.script(MyScriptModule(10, 2)) # Demonstration input_tensor = torch.randn(1, 10) output = my_script_module(input_tensor) print(output) tup_input = (1, \'example\') inc_output = my_script_module.inc_first_elem(tup_input) print(inc_output) ``` # Submission: Submit your implementation of the `MyScriptModule` class along with a brief explanation of how your implementation meets the requirements of TorchScript type annotations and the type system.","solution":"import torch from typing import Any, Tuple class MyScriptModule(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): super(MyScriptModule, self).__init__() self.linear = torch.nn.Linear(input_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x) @torch.jit.export def inc_first_elem(self, x: Tuple[int, Any]) -> Tuple[int, Any]: return (x[0] + 1, x[1]) # Scripting the Module my_script_module = torch.jit.script(MyScriptModule(10, 2)) # Example Usage input_tensor = torch.randn(1, 10) output = my_script_module(input_tensor) print(output) tup_input = (1, \'example\') inc_output = my_script_module.inc_first_elem(tup_input) print(inc_output)"},{"question":"# Custom Import System Implementation Objective Create a custom importer in Python that can load modules from a custom-defined location, such as a string representation of the module\'s Python code, instead of the file system or typical locations. Problem Statement Your task is to implement a custom import system in Python that can search for and load modules from a predefined dictionary. Each key in the dictionary will be the module\'s name, and the associated value will be the source code of the module as a string. Requirements: 1. Define a `ModuleDictImporter` class that implements the finder and loader protocols. 2. Integrate this custom importer into Python’s import system. 3. Demonstrate the functionality by importing a module using your custom importer. Details: - The `ModuleDictImporter` should have an `__init__` method that accepts a dictionary of modules (`modules_dict`). - Implement the `find_spec` method to check if the requested module exists in the provided dictionary. - Implement the `create_module` method to create a module. - Implement the `exec_module` method to execute the module\'s code. - Add your custom importer to `sys.meta_path`. Input - A dictionary named `modules_dict` containing module names as keys and their source code as string values. - A module name string to be imported. Output - The imported module should behave as it would if it were a standard module imported from a filesystem, with all its functions and classes accessible. Example ```python # Sample dictionary containing module source code modules_dict = { \\"mymodule\\": def foo(): return \\"Hello from foo\\" class Bar: def greet(self): return \\"Hello from Bar\\" } # Your task starts here import sys import types class ModuleDictImporter: def __init__(self, modules_dict): self.modules_dict = modules_dict def find_spec(self, fullname, path, target=None): if fullname in self.modules_dict: return importlib.machinery.ModuleSpec(fullname, self) return None def create_module(self, spec): return None # Default module creation semantics def exec_module(self, module): exec(self.modules_dict[module.__name__], module.__dict__) # Adding custom importer to the meta_path sys.meta_path.insert(0, ModuleDictImporter(modules_dict)) # Sample usage: import mymodule # Test the imported module print(mymodule.foo()) # Output: Hello from foo bar = mymodule.Bar() print(bar.greet()) # Output: Hello from Bar ``` Notes: - Ensure that your custom importer is ordered correctly in `sys.meta_path` to intercept the import statements effectively. - Make sure to handle any necessary error checking or edge cases you might foresee.","solution":"import sys import importlib.util import types class ModuleDictImporter: def __init__(self, modules_dict): self.modules_dict = modules_dict def find_spec(self, fullname, path, target=None): if fullname in self.modules_dict: return importlib.util.spec_from_loader(fullname, self) return None def create_module(self, spec): return None # Default module creation semantics def exec_module(self, module): exec(self.modules_dict[module.__name__], module.__dict__) # Example dictionary containing module source code modules_dict = { \\"mymodule\\": def foo(): return \\"Hello from foo\\" class Bar: def greet(self): return \\"Hello from Bar\\" } # Adding custom importer to the meta_path sys.meta_path.insert(0, ModuleDictImporter(modules_dict))"},{"question":"Titanic Dataset Analysis with Seaborn Task: You are provided with the Titanic dataset (`sns.load_dataset(\\"titanic\\")`). Write a function `analyze_titanic` that: 1. Loads the Titanic dataset. 2. Creates a series of customized seaborn boxplots that analyze different aspects of the dataset. 3. Returns the list of matplotlib axes created. Steps to Follow: 1. **Load the Titanic Dataset**: Use `sns.load_dataset(\\"titanic\\")` to load the data into a DataFrame named `titanic`. 2. **Create Boxplots**: - **Boxplot 1**: Plot a horizontal boxplot of the `age` variable. - **Boxplot 2**: Plot a vertical boxplot to show the distribution of ages for different classes (`class` variable) of passengers. - **Boxplot 3**: Create a nested group boxplot to display the distribution of fares (`fare`) for different embarkation points (`embark_town`), further grouped by survival status (`alive`). - **Boxplot 4**: Create a customized boxplot of `age` vs. `deck` where: - The whiskers cover the full range of the data. - The boxes should be narrower. - Change the color and width of the lines in the boxplot. 3. **Matplotlib Axes**: Return the matplotlib axes created. Expected Function Signature: ```python def analyze_titanic(): # Your code here return axes_list # List of Axes objects ``` Constraints: - Ensure all plots are clear and properly labeled. - Apply any necessary customizations as specified. - Use Seaborn and Matplotlib for plotting. Example Usage: ```python axes = analyze_titanic() # This should create and return the plots as specified, with axes being a list of matplotlib axes. ``` Additional Requirements: 1. The function should not show the plots (`plt.show()`) automatically. It should only return the axes objects. 2. Label each plot appropriately for clear understanding. 3. Use appropriate seaborn and matplotlib functions to achieve each customization. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic(): # Step 1: Load the Titanic Dataset titanic = sns.load_dataset(\\"titanic\\") # Initialize the list to store the axes axes_list = [] # Step 2: Create Boxplots # Boxplot 1: Horizontal boxplot of \'age\' fig, ax1 = plt.subplots() sns.boxplot(x=\'age\', data=titanic, ax=ax1) ax1.set_title(\'Horizontal Boxplot of Age\') axes_list.append(ax1) # Boxplot 2: Vertical boxplot of \'age\' by \'class\' fig, ax2 = plt.subplots() sns.boxplot(x=\'class\', y=\'age\', data=titanic, ax=ax2) ax2.set_title(\'Boxplot of Age by Class\') axes_list.append(ax2) # Boxplot 3: Nested group boxplot of \'fare\' by \'embark_town\' and \'alive\' fig, ax3 = plt.subplots() sns.boxplot(x=\'embark_town\', y=\'fare\', hue=\'alive\', data=titanic, ax=ax3) ax3.set_title(\'Boxplot of Fare by Embark Town and Survival Status\') axes_list.append(ax3) # Boxplot 4: Customized boxplot of \'age\' vs. \'deck\' fig, ax4 = plt.subplots() sns.boxplot(x=\'deck\', y=\'age\', data=titanic, whis=[0, 100], width=0.5, linewidth=1.5, ax=ax4) ax4.set_title(\'Customized Boxplot of Age by Deck\') axes_list.append(ax4) return axes_list"},{"question":"# Advanced Python Module Management and Reloading In this task, you are required to simulate a subset of Python\'s module importing and reloading functionality. You will not be using the Python/C API but instead will replicate similar behavior using pure Python. **Objectives:** 1. Implement a function `import_module` that takes a module name as a string and returns the imported module. If the module does not exist, raise an `ImportError`. 2. Implement a function `add_module` that takes a module name (possibly in the form `package.module`) and ensures that a module object exists for this name in `sys.modules`. This should not actually load the module, but should create an empty module object if it does not already exist. 3. Implement a function `reload_module` that takes an already imported module object, reloads it, and returns the reloaded module. **Function Specifications:** 1. `import_module(name: str) -> Any`: - **Input:** A string `name` representing the name of the module to be imported. - **Output:** The imported module. - **Constraints:** Should raise `ImportError` if the module does not exist. 2. `add_module(name: str) -> None`: - **Input:** A string `name` representing the name of the module to ensure exists in `sys.modules`. - **Output:** None. - **Constraints:** Should create an empty module object in `sys.modules` if the module is not already present. Should not actually load the module. 3. `reload_module(module: Any) -> Any`: - **Input:** A module object that has already been imported. - **Output:** The reloaded module. - **Constraints:** Should reload the module using Python\'s `importlib.reload` function and return the reloaded module. If the module has not been imported yet, raise `ImportError`. ```python import sys import importlib def import_module(name: str) -> Any: Import and return the module specified by \'name\'. :param name: The name of the module to import. :return: The imported module. :raises ImportError: If the module cannot be imported. pass # Your implementation here def add_module(name: str) -> None: Ensure there is a module object for \'name\' in sys.modules. :param name: The name of the module to add. :return: None pass # Your implementation here def reload_module(module: Any) -> Any: Reload the given module and return it. :param module: The module object to reload. :return: The reloaded module. :raises ImportError: If the module cannot be reloaded. pass # Your implementation here ``` **Constraints:** - You may assume that the given module names will be valid strings. - You are not allowed to use any direct `PyImport_*` functions from the Python/C API, all implementations should be in pure Python. - Focus on using the `importlib` module for reloading purposes. **Examples:** ```python # Example 1 import_module(\'os\') # Should return the os module # Example 2 add_module(\'my_custom_module\') # Should add an empty module named \'my_custom_module\' to sys.modules # Example 3 import math reload_module(math) # Should reload the math module and return it ``` **Note:** Ensure that your code is well-structured and handles edge cases appropriately.","solution":"import sys import importlib def import_module(name: str): Import and return the module specified by \'name\'. :param name: The name of the module to import. :return: The imported module. :raises ImportError: If the module cannot be imported. try: module = importlib.import_module(name) return module except ModuleNotFoundError as e: raise ImportError(f\\"Module \'{name}\' not found\\") from e def add_module(name: str): Ensure there is a module object for \'name\' in sys.modules. :param name: The name of the module to add. :return: None if name not in sys.modules: module = importlib.util.module_from_spec(importlib.machinery.ModuleSpec(name, None)) sys.modules[name] = module def reload_module(module): Reload the given module and return it. :param module: The module object to reload. :return: The reloaded module. :raises ImportError: If the module cannot be reloaded. if module.__name__ not in sys.modules: raise ImportError(f\\"Module \'{module.__name__}\' not currently loaded\\") return importlib.reload(module)"},{"question":"You are tasked with creating a function that translates a list of numeric error codes into their respective human-readable error messages. The error messages should be readable and specific, describing the exact error that occurred. # Function Signature ```python def translate_error_codes(error_codes: list) -> list: Translates a list of numeric error codes into their respective human-readable error messages. Parameters: error_codes (list): A list of integers representing error codes. Returns: list: A list of strings where each string is the human-readable error message corresponding to the respective error code in the input list. ``` # Input - A single parameter, `error_codes`, which is a list of integers. Each integer is a numeric error code that needs to be translated. # Output - A list of strings, where each string is the human-readable error message corresponding to the respective error code in the input list. # Constraints 1. You may assume that all error codes provided in the list are valid and exist within the `errno` module. 2. If an error code does not have a corresponding error message, the function should return the string \\"Unknown error code\\". # Example Example 1: ```python error_codes = [1, 2, 5] print(translate_error_codes(error_codes)) ``` **Output:** ```python [\'Operation not permitted\', \'No such file or directory\', \'I/O error\'] ``` Example 2: ```python error_codes = [9, 11, 17] print(translate_error_codes(error_codes)) ``` **Output:** ```python [\'Bad file number\', \'Try again\', \'File exists\'] ``` # Notes - You can use the `errno` module and its `errorcode` dictionary for mapping error codes to their names. - Use `os.strerror()` to convert the error names into human-readable error messages. **Hint:** Utilize the `errno` module to access the `errorcode` dictionary and the `os` module to translate error codes into human-readable error messages.","solution":"import errno import os def translate_error_codes(error_codes: list) -> list: Translates a list of numeric error codes into their respective human-readable error messages. Parameters: error_codes (list): A list of integers representing error codes. Returns: list: A list of strings where each string is the human-readable error message corresponding to the respective error code in the input list. error_messages = [] for code in error_codes: try: error_name = errno.errorcode[code] error_message = os.strerror(code) except KeyError: error_message = \\"Unknown error code\\" error_messages.append(error_message) return error_messages"},{"question":"**Question:** You are required to demonstrate your understanding of seaborn\'s plotting contexts. The objective is to change plotting contexts and observe the effect on plot elements such as labels and lines. You must write a function that: 1. Plots a default line plot using the \\"default\\" context. 2. Plots the same line plot using the \\"poster\\" context. 3. Plots the same line plot using the \\"talk\\" context as a temporary context within a `with` block. 4. Returns the three figures showing the different contexts. **Instructions:** - Use the `seaborn` library for the plot. - Create a function named `demonstrate_plotting_contexts` that takes no inputs. - This function should output three different plots demonstrating the \\"default\\", \\"poster\\", and context-managed \\"talk\\" plotting contexts. - Use any sample data you see fit, ensuring that the data clearly shows the distinction between the different contexts. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def demonstrate_plotting_contexts(): # Your code here ``` **Constraints:** - Your plots should use the same data across all three contexts for a meaningful comparison. **Expected Output:** - The function should display three plots, one for each context mentioned. **Sample Data:** ```python x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] ``` Demonstrate your understanding by comparing how the plot elements (e.g., labels, lines) change with the different contexts.","solution":"import seaborn as sns import matplotlib.pyplot as plt def demonstrate_plotting_contexts(): # Sample data x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] # Plot using default context sns.set_context(\\"notebook\\") fig1, ax1 = plt.subplots() sns.lineplot(x=x, y=y, ax=ax1) ax1.set_title(\\"Default Context (notebook)\\") plt.close(fig1) # Plot using poster context sns.set_context(\\"poster\\") fig2, ax2 = plt.subplots() sns.lineplot(x=x, y=y, ax=ax2) ax2.set_title(\\"Poster Context\\") plt.close(fig2) # Plot using talk context inside a with block fig3, ax3 = plt.subplots() with sns.plotting_context(\\"talk\\"): sns.lineplot(x=x, y=y, ax=ax3) ax3.set_title(\\"Talk Context (within with block)\\") plt.close(fig3) return fig1, fig2, fig3"},{"question":"Introduction You are tasked with implementing and verifying the gradients of a custom function using PyTorch\'s autograd functionality. This question tests your ability to correctly define a function, compute its gradient, and verify it using both standard and fast modes provided by Pytorch\'s `gradcheck` utility. Problem Statement Implement a custom function `custom_function` and perform gradient checks using PyTorch. The function should be defined as: [ y = f(x) = sin(x) + cos(2x) ] # Function Implementation 1. **Function Definition**: Define the function `custom_function` which takes a tensor `x` as input and returns a tensor computed using the formula above. 2. **Gradient Check**: Using Pytorch\'s `gradcheck`, verify the gradients of `custom_function` for the following: * **Default Mode**: Use the standard method to check the gradients numerically and analytically. * **Fast Mode**: Utilize the fast mode of gradcheck for performance optimization. # Requirements 1. Your function should correctly handle and compute the required gradients for both real and complex inputs. 2. Ensure all assertions for `gradcheck` pass without errors. 3. Use a random tensor for `x` with `requires_grad=True` to enable gradient computation. 4. The function should handle tensor input of any shape. # Constraints - Tensor `x` should have a shape of `(5, )`. - Use a tolerance `atol=1e-5` and `rtol=1e-3` for the gradient check comparisons. # Example ```python import torch from torch.autograd import gradcheck def custom_function(x): return torch.sin(x) + torch.cos(2 * x) x = torch.randn(5, dtype=torch.float64, requires_grad=True) # Perform standard gradcheck assert gradcheck(custom_function, (x,), atol=1e-5, rtol=1e-3) # Perform fast gradcheck assert gradcheck(custom_function, (x,), atol=1e-5, rtol=1e-3, fast_mode=True) ``` # Submission Submit your `custom_function` definition along with the code for performing both standard and fast gradient checks.","solution":"import torch from torch.autograd import gradcheck def custom_function(x): Custom function: y = sin(x) + cos(2*x) Args: x (torch.Tensor): Input tensor. Returns: torch.Tensor: Output tensor computed as sin(x) + cos(2*x) return torch.sin(x) + torch.cos(2 * x) # Define input tensor for gradcheck x = torch.randn(5, dtype=torch.float64, requires_grad=True) # Perform standard gradcheck assert gradcheck(custom_function, (x,), atol=1e-5, rtol=1e-3) # Perform fast gradcheck assert gradcheck(custom_function, (x,), atol=1e-5, rtol=1e-3, fast_mode=True)"},{"question":"Objective: Implement a function that merges two dictionaries with specific constraints and retrieves key-value pairs based on given conditions. Problem Statement: You are required to implement a Python function `merge_and_query(dict1, dict2, keys)` that takes in three parameters: 1. `dict1` (dict): The first dictionary. 2. `dict2` (dict): The second dictionary. 3. `keys` (list): A list of keys to query in the final merged dictionary. The function should: 1. Merge `dict2` into `dict1`. If a key exists in both `dict1` and `dict2`, the value from `dict2` should override the value in `dict1`. 2. Return a list containing the values for the keys specified in `keys` from the merged dictionary. If a key is not present in the merged dictionary, append `None` to the result list for that key. Constraints: - The keys in the dictionaries are assumed to be hashable and unique. - The `keys` list will not contain duplicate elements. - The function should handle dictionaries with nested structures, where some values can be dictionaries themselves. Input: - `dict1` : A dictionary containing key-value pairs. - `dict2` : A dictionary containing key-value pairs. - `keys` : A list of keys to retrieve from the merged dictionary. Output: - A list of values corresponding to the requested keys in the `keys` list. If a key is not found, return `None` for that key. Example: ```python def merge_and_query(dict1, dict2, keys): # Your implementation here # Example usage and expected output: dict1 = {\'a\': 1, \'b\': 2, \'c\': {\'d\': 4}} dict2 = {\'b\': 3, \'c\': {\'e\': 5}} keys = [\'a\', \'b\', \'c\', \'d\'] result = merge_and_query(dict1, dict2, keys) print(result) # Expected output: [1, 3, {\'d\': 4, \'e\': 5}, None] ``` Evaluation Criteria: - Correctness: The function should correctly merge `dict2` into `dict1` and produce the expected output. - Efficiency: The function should efficiently handle large dictionaries. - Edge cases: The function should handle cases where keys are not present in either dictionary.","solution":"def merge_dicts(dict1, dict2): Recursively merges dict2 into dict1. for key in dict2: if key in dict1 and isinstance(dict1[key], dict) and isinstance(dict2[key], dict): merge_dicts(dict1[key], dict2[key]) else: dict1[key] = dict2[key] return dict1 def merge_and_query(dict1, dict2, keys): Merges dict2 into dict1 and retrieves the values for the keys specified in keys list. merged_dict = merge_dicts(dict1.copy(), dict2) result = [merged_dict.get(key, None) for key in keys] return result"},{"question":"# Exercise: Building and Using Custom Enums with Advanced Features Objective: Write a program that defines and uses Enums with custom behaviors and methods. Problem Statement: 1. **The Enums**: - Define an `Enum` class `Weekday` to represent the days of the week. - Define another `IntEnum` class `StatusCode` to represent HTTP status codes. - Define a `Flag` class `Permission` to represent various permissions that can be combined using bitwise operations. 2. **Custom Methods**: - For `Weekday`, add a method `is_weekend` that returns `True` if the day is `Saturday` or `Sunday`, else returns `False`. - For `StatusCode`, add a method `is_error` that returns `True` for status codes in the range 400-599. - For `Permission`, add a method `list_permissions` that returns a list of all permissions set. 3. **Ensuring Uniqueness**: - Decorate the `StatusCode` class with `@unique` to ensure no duplicate values are defined. 4. **Programmatic Access**: - Write a function `get_day_name` that takes an integer (1-7) and returns the name of the weekday. - Write a function `get_status_description` that takes a status code value and returns the corresponding enum member name. - Write a function `get_permissions_set` that takes a `Permission` value and returns the list of all permissions set. 5. **Usage**: - Demonstrate the usage of these Enums and their custom methods by creating an instance of each and calling the relevant methods. Constraints: - Your `Weekday` class should have members `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`. - Your `StatusCode` class should at least have members `OK = 200`, `NOT_FOUND = 404`, `INTERNAL_SERVER_ERROR = 500`. - Your `Permission` class should have members `READ = 1`, `WRITE = 2`, `EXECUTE = 4`, which can be combined using bitwise operations. Expected Output: 1. The `Weekday` enumeration and its custom method(s) working as expected. 2. The `StatusCode` enumeration with uniqueness enforced and custom method(s) working as expected. 3. The `Permission` flag with combined values and a method to list all set permissions. 4. The output of programmatic functions correctly returning names and lists based on inputs. Example: ```python from enum import Enum, IntEnum, Flag, auto, unique # Your Enum definitions and method implementations here. # Demonstration of usage print(get_day_name(1)) # Outputs: \\"MONDAY\\" print(get_status_description(404)) # Outputs: \\"NOT_FOUND\\" print(Permission.READ | Permission.WRITE) # Outputs: <Permission.READ|WRITE: 3> print((Permission.READ | Permission.WRITE).list_permissions()) # Outputs: [\'READ\', \'WRITE\'] ``` Design your solution such that it encapsulates the principles of using Enums effectively and leverages their features as described in the documentation.","solution":"from enum import Enum, IntEnum, Flag, unique class Weekday(Enum): MONDAY = 1 TUESDAY = 2 WEDNESDAY = 3 THURSDAY = 4 FRIDAY = 5 SATURDAY = 6 SUNDAY = 7 def is_weekend(self): return self in (Weekday.SATURDAY, Weekday.SUNDAY) @unique class StatusCode(IntEnum): OK = 200 NOT_FOUND = 404 INTERNAL_SERVER_ERROR = 500 def is_error(self): return self >= 400 and self < 600 class Permission(Flag): READ = 1 WRITE = 2 EXECUTE = 4 def list_permissions(self): return [perm.name for perm in Permission if perm in self] def get_day_name(day_number): return Weekday(day_number).name def get_status_description(code): try: return StatusCode(code).name except ValueError: return None def get_permissions_set(permissions): return permissions.list_permissions()"},{"question":"**Question:** In this coding assessment, you are required to create a detailed seaborn visualization that leverages the `stripplot` function to analyze the \\"tips\\" dataset. Your task will involve demonstrating multiple features and customizations of the seaborn `stripplot` function. **Requirements:** 1. **Data Preparation:** - Load the \\"tips\\" dataset from seaborn. 2. **Plot 1: Basic Strip Plot** - Create a basic strip plot showing the distribution of the \\"total_bill\\" variable. 3. **Plot 2: Split by a Categorical Variable** - Modify the strip plot to show \\"total_bill\\" on the x-axis split by \\"day\\" on the y-axis. 4. **Plot 3: Use Hue Parameter** - Further, modify the strip plot to distinguish the data points by \\"sex\\" using the `hue` parameter. 5. **Plot 4: Apply Jitter Control** - Show another strip plot with \\"day\\" on the y-axis and \\"total_bill\\" on the x-axis using `dodge=True` and disable jitter. 6. **Plot 5: Customize Visual Attributes** - Create a strip plot with the following customizations: - No jitter - Marker style: Diamond (D) - Marker size: 20 - Marker edge linewidth: 1 - Marker transparency: 10% (alpha=0.1) 7. **Plot 6: Faceted Strip Plot** - Create a faceted strip plot using seaborn\'s `catplot` for \\"time\\" and \\"total_bill\\", where facets are split by \\"day\\" and data points are distinguished by \\"sex\\". **Additional Constraints:** - Ensure all plots have appropriate titles and axis labels. - Customize the color palette for the plots where applicable. - The plots should not display a legend by default unless explicitly required. **Expected Output:** You should submit a Python script or a Jupyter notebook that meets the following requirements: 1. Contains all the specified plots. 2. Implements all specified customizations. 3. Comments on the code explaining your steps and choices. **Performance Requirements:** - Your solution should be efficient and free of redundant code. - The visualizations should render correctly without errors. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_strip_plots(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Plot 1: Basic Strip Plot plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"total_bill\\", data=tips) plt.title(\'Basic Strip Plot: Total Bill Distribution\') plt.xlabel(\'Total Bill\') plt.show() # Plot 2: Split by a Categorical Variable (day) plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"total_bill\\", y=\\"day\\", data=tips) plt.title(\'Strip Plot: Total Bill by Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.show() # Plot 3: Use Hue Parameter to distinguish by Sex plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", data=tips, palette=\\"Set2\\") plt.title(\'Strip Plot: Total Bill by Day and Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.legend(title=\'Sex\') plt.show() # Plot 4: Apply Jitter Control with dodge=True plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", data=tips, dodge=True, jitter=False, palette=\\"Set2\\") plt.title(\'Strip Plot: Total Bill by Day with Dodge (No Jitter)\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.legend(title=\'Sex\') plt.show() # Plot 5: Customize Visual Attributes plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", data=tips, dodge=True, jitter=False, marker=\'D\', size=20, linewidth=1, alpha=0.1, palette=\\"Set2\\") plt.title(\'Customized Strip Plot: Total Bill by Day and Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.legend(title=\'Sex\') plt.show() # Plot 6: Faceted Strip Plot using catplot sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", height=4, aspect=0.7, palette=\\"Set2\\") plt.subplots_adjust(top=0.9) plt.suptitle(\'Faceted Strip Plot: Total Bill by Time, Day, and Sex\', size=16) plt.show()"},{"question":"Problem Statement You are provided with a dataset `data.csv` which contains the information of different features for multiple samples. Your task is to write a Python function using scikit-learn to cluster this data using two different algorithms, evaluate their performance, and return the results. # Specifications 1. The function should be named `cluster_and_evaluate`. 2. The function should take one argument: `file_path` (a string representing the path to the dataset `data.csv`). 3. The `data.csv` file contains a dataset where each row represents a sample and each column represents a feature. 4. The function should perform clustering using: - K-Means - DBSCAN 5. The function should evaluate the clustering performance using: - Silhouette Score - Davies-Bouldin Index 6. The function should return a dictionary with the following structure: ```python { \\"KMeans\\": { \\"silhouette_score\\": <float>, \\"davies_bouldin_index\\": <float> }, \\"DBSCAN\\": { \\"silhouette_score\\": <float>, \\"davies_bouldin_index\\": <float> } } ``` # Constraints - Use `n_clusters=3` for K-Means clustering. - Use `eps=0.5` and `min_samples=5` for DBSCAN clustering. - Ensure that DBSCAN produces at least one cluster; otherwise, adjust `eps` to a slightly higher value until at least one cluster is formed. # Example Usage ```python results = cluster_and_evaluate(\\"data.csv\\") print(results) ``` # Notes - You may assume that the dataset is preprocessed and does not contain missing values. - Consider edge cases where DBSCAN might not form any clusters and handle them appropriately. # Evaluation Criteria - Correctness of clustering implementation. - Correctness of performance evaluation. - Handling edge cases for DBSCAN. - Code readability and documentation.","solution":"import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score def cluster_and_evaluate(file_path): # Load data data = pd.read_csv(file_path) # KMeans Clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(data) kmeans_silhouette = silhouette_score(data, kmeans_labels) kmeans_davies_bouldin = davies_bouldin_score(data, kmeans_labels) # DBSCAN Clustering eps_value = 0.5 while True: dbscan = DBSCAN(eps=eps_value, min_samples=5) dbscan_labels = dbscan.fit_predict(data) if len(set(dbscan_labels)) > 1: # Ensure at least one cluster is formed break eps_value += 0.1 dbscan_silhouette = silhouette_score(data, dbscan_labels) dbscan_davies_bouldin = davies_bouldin_score(data, dbscan_labels) # Result Dictionary results = { \\"KMeans\\": { \\"silhouette_score\\": kmeans_silhouette, \\"davies_bouldin_index\\": kmeans_davies_bouldin }, \\"DBSCAN\\": { \\"silhouette_score\\": dbscan_silhouette, \\"davies_bouldin_index\\": dbscan_davies_bouldin } } return results"},{"question":"Problem Statement You are tasked with implementing a simple logging mechanism using Python\'s `contextvars` module. This logging system will use context variables to maintain and isolate log messages within different contexts. # Requirements 1. **Create a `ContextVar` named `log_var` with an initial value of an empty list.** 2. **Implement a function `log_message(message: str) -> None` that appends a message to the current context\'s log.** 3. **Implement a function `get_logs() -> List[str]` that returns the current context\'s log messages.** 4. **Implement a function `clear_logs() -> None` that clears the current context\'s log messages.** 5. **Implement a function `copy_and_run_with_logs(func: Callable, *args, **kwargs) -> Any` that will:** - **Create a copy of the current context.** - **Execute the given function `func` within the copied context.** - **Return the result of the function execution.** # Input and Output Formats - **`log_message(message: str) -> None`**: - **Input**: A string message to be logged. - **Output**: None - **`get_logs() -> List[str]`**: - **Input**: None - **Output**: A list of strings, where each string is a logged message. - **`clear_logs() -> None`**: - **Input**: None - **Output**: None - **`copy_and_run_with_logs(func: Callable, *args, **kwargs) -> Any`**: - **Input**: A callable `func` and any arguments and keyword arguments required by `func`. - **Output**: The result of the function `func` executed within the copied context. # Constraints - **Ensure that `get_logs` and `clear_logs` operate within the current context and do not affect logs from other contexts.** - **You can assume that all input values are of correct type and within reasonable limits.** # Example ```python import contextvars # Assuming implementation of log_message, get_logs, clear_logs, copy_and_run_with_logs # Creating initial log entries log_message(\\"Initialization started\\") log_message(\\"Loading configuration\\") print(get_logs()) # Output: [\\"Initialization started\\", \\"Loading configuration\\"] # Creating a copied context and running a new function within it def new_context_function(): log_message(\\"New context started\\") return \\"done\\" result = copy_and_run_with_logs(new_context_function) print(result) # Output: \\"done\\" print(get_logs()) # Output: [\\"Initialization started\\", \\"Loading configuration\\"] ``` Implement the functions as described, making sure to use the `contextvars` module to effectively manage context-local state.","solution":"import contextvars from typing import List, Callable, Any # Context variable to hold the log messages log_var = contextvars.ContextVar(\\"log_var\\", default=[]) def log_message(message: str) -> None: Appends a message to the current context\'s log. current_log = log_var.get() current_log.append(message) log_var.set(current_log) def get_logs() -> List[str]: Returns the current context\'s log messages. return log_var.get() def clear_logs() -> None: Clears the current context\'s log messages. log_var.set([]) def copy_and_run_with_logs(func: Callable, *args, **kwargs) -> Any: Creates a copy of the current context, executes the given function within the copied context, and returns the result of the function execution. current_context = contextvars.copy_context() new_log = current_context.run(lambda: log_var.get().copy()) new_context = contextvars.copy_context() def wrapper_function(): log_var.set(new_log) return func(*args, **kwargs) return new_context.run(wrapper_function)"},{"question":"# Pandas DataFrame Assessment **Objective:** Implement several pandas DataFrame operations to manipulate and analyze a provided dataset. **Problem Statement:** You are given a DataFrame `df` which contains data about the sales performance of different products in various regions. The columns are as follows: - `product_id`: Unique identifier for each product. - `product_name`: Name of the product. - `region`: Region where the product is sold. - `units_sold`: Number of units sold. - `unit_price`: Price per unit. - `sale_date`: Date when the sale was made. Implement the following functionalities: 1. **Filter and summarize sales data:** - Create a function `filter_summarize_sales(df, start_date, end_date, min_units_sold)` to filter the DataFrame for sales made between `start_date` and `end_date` (inclusive), and where the number of units sold is at least `min_units_sold`. The function should return a summary DataFrame that contains the total units sold and the total sales amount (`units_sold` * `unit_price`) for each product in each region. 2. **Identify top-performing regions:** - Create a function `top_performing_regions(df, n)` to determine the top `n` performing regions based on total sales amount. The function should return a DataFrame with these top `n` regions and their respective total sales amounts. 3. **Fill missing sales data:** - Create a function `fill_missing_sales_data(df)` to fill any missing values in the `units_sold` column with the average number of units sold for that product across all regions. If a product has no sales data across all regions, fill it with zero. 4. **Plot sales trend:** - Create a function `plot_sales_trend(df, product_id)` to plot the trend of total sales amount over time for a specific product identified by `product_id`. The plot should show sales amounts on the y-axis and dates on the x-axis. **Constraints:** - Ensure that the filtering, summarization, and filling of missing data are performed efficiently. - The DataFrame may contain a large number of rows; aim to achieve optimal performance with vectorized operations wherever possible. **Input:** - `df`: pandas DataFrame with the sales data. - `start_date`, `end_date`: Strings representing start and end dates in the format \'YYYY-MM-DD\'. - `min_units_sold`: Integer representing the minimum units sold for filtering. - `n`: Integer representing the number of top regions. - `product_id`: Integer representing the ID of a specific product. **Output:** - `filter_summarize_sales` should return a DataFrame with the columns `product_id`, `product_name`, `region`, `total_units_sold`, `total_sales_amount`. - `top_performing_regions` should return a DataFrame with the columns `region`, `total_sales_amount`. - `fill_missing_sales_data` should return the modified DataFrame with filled `units_sold` values. - `plot_sales_trend` should display a plot showing the sales trend. Implement these functions to demonstrate your proficiency with pandas DataFrame operations.","solution":"import pandas as pd import matplotlib.pyplot as plt def filter_summarize_sales(df, start_date, end_date, min_units_sold): Filters the DataFrame for sales made between start_date and end_date, and where the number of units sold is at least min_units_sold. Returns a summary DataFrame with total units sold and total sales amount for each product in each region. filtered_df = df[(df[\'sale_date\'] >= start_date) & (df[\'sale_date\'] <= end_date) & (df[\'units_sold\'] >= min_units_sold)] summary = filtered_df.groupby([\'product_id\', \'product_name\', \'region\']).agg( total_units_sold=pd.NamedAgg(column=\'units_sold\', aggfunc=\'sum\'), total_sales_amount=pd.NamedAgg(column=\'units_sold\', aggfunc=lambda x: (x * df[\'unit_price\']).sum()) ).reset_index() return summary def top_performing_regions(df, n): Determines the top n performing regions based on total sales amount. Returns a DataFrame with the top n regions and their total sales amounts. region_sales = df.groupby(\'region\').agg( total_sales_amount=pd.NamedAgg(column=\'units_sold\', aggfunc=lambda x: (x * df[\'unit_price\']).sum()) ).reset_index() top_regions = region_sales.nlargest(n, \'total_sales_amount\') return top_regions def fill_missing_sales_data(df): Fills any missing values in the units_sold column with the average number of units sold for that product across all regions. If a product has no sales data across all regions, fills it with zero. averages = df.groupby(\'product_id\')[\'units_sold\'].transform(\'mean\') df[\'units_sold\'].fillna(averages, inplace=True) df[\'units_sold\'].fillna(0, inplace=True) return df def plot_sales_trend(df, product_id): Plots the trend of total sales amount over time for a specific product identified by product_id. product_sales = df[df[\'product_id\'] == product_id] product_sales[\'total_sales_amount\'] = product_sales[\'units_sold\'] * product_sales[\'unit_price\'] trend = product_sales.groupby(\'sale_date\')[\'total_sales_amount\'].sum().sort_index() plt.figure(figsize=(10,5)) plt.plot(trend.index, trend.values, marker=\'o\') plt.title(f\'Sales Trend for Product ID {product_id}\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales Amount\') plt.grid(True) plt.show()"},{"question":"# PyTorch FX Graph Transformation Coding Question **Objective:** Your task is to implement a function that takes a module and replaces instances of `torch.add` operations with `torch.mul` operations within the model\'s computation graph. This will demonstrate your understanding of symbolic tracing and graph manipulation using the `torch.fx` package. # Instructions: 1. **Trace the Given Module:** - Symbolically trace the input module to obtain its computation graph. 2. **Modify the Graph:** - Iterate over the nodes in the graph. - Replace any operation node that calls the `torch.add` function with a call to the `torch.mul` function. 3. **Recompile the Modified Graph:** - Ensure the modified graph is recompilable into a valid `torch.fx.GraphModule`. 4. **Return the Modified Module:** - Return the modified `torch.fx.GraphModule`. # Function Signature: ```python import torch from torch.fx import symbolic_trace, GraphModule def replace_add_with_mul(module: torch.nn.Module) -> torch.fx.GraphModule: Transforms a given module by replacing all instances of torch.add with torch.mul in its computation graph. Args: module (torch.nn.Module): The input neural network module to be transformed. Returns: torch.fx.GraphModule: The transformed module with add operations replaced by mul operations. pass ``` # Example Usage: ```python import torch import torch.nn as nn class SampleModule(nn.Module): def forward(self, x, y): return torch.add(x, y) # Instantiate the sample module sample_module = SampleModule() # Perform the transformation modified_module = replace_add_with_mul(sample_module) # Test the transformation x = torch.tensor([1.0]) y = torch.tensor([2.0]) print(modified_module(x, y)) # Should output tensor([2.0]) as it performs multiplication ``` # Constraints: - You must use the `torch.fx` package for symbolic tracing and graph modification. - Your implementation should correctly handle any module that uses `torch.add`. # Performance: - Ensure the modified module accurately performs the defined transformation within the computation graph. - Verify the validation tests with multiple input examples.","solution":"import torch from torch.fx import symbolic_trace, GraphModule def replace_add_with_mul(module: torch.nn.Module) -> torch.fx.GraphModule: Transforms a given module by replacing all instances of torch.add with torch.mul in its computation graph. Args: module (torch.nn.Module): The input neural network module to be transformed. Returns: torch.fx.GraphModule: The transformed module with add operations replaced by mul operations. # Trace the module to convert it into a GraphModule traced = symbolic_trace(module) # Get the graph from the traced module graph = traced.graph # Iterate over the nodes and replace torch.add with torch.mul for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul # Recompile the modified graph into a GraphModule new_module = GraphModule(traced, graph) return new_module"},{"question":"Objective: Demonstrate your understanding of the `ensurepip` module in Python by programmatically ensuring that the latest version of `pip` is installed in a specified directory, with a particular configuration for script installation. Problem Statement: You are tasked with writing a function `setup_pip(ensure_upgrade: bool, use_user_directory: bool, install_default_pip: bool, target_directory: str) -> str` that ensures `pip` is installed in your Python environment with specific configurations. Requirements: 1. The function should use the `ensurepip` module to bootstrap the `pip` installer. 2. The function should accept the following arguments: - `ensure_upgrade` (Boolean): If `True`, ensures that the `pip` installation is upgraded to the latest version available in `ensurepip`. - `use_user_directory` (Boolean): If `True`, installs `pip` into the user site packages directory. - `install_default_pip` (Boolean): If `True`, the `pip` script should be installed in addition to the versioned scripts. - `target_directory` (String): The directory where `pip` should be installed. If set to an empty string, the default installation location is used. 3. Return a string message that indicates the version of `pip` installed. Constraints: 1. If `use_user_directory` is `True`, do not set the target directory. 2. If both `install_default_pip` and `altinstall` are `True`, raise a `ValueError`. 3. Must handle errors gracefully and return a descriptive error message. Example Usage: ```python def setup_pip(ensure_upgrade: bool, use_user_directory: bool, install_default_pip: bool, target_directory: str) -> str: pass # Example Call print(setup_pip(True, False, True, \\"/custom/directory\\")) ``` Expected Output: ``` \\"pip version X.X.X installed successfully\\" ``` You may assume that the `ensurepip` module is available and does not require internet access to fulfill the bootstrapping process. Good luck, and ensure your implementation is clean and thoroughly tested.","solution":"import ensurepip import subprocess import sys def setup_pip(ensure_upgrade: bool, use_user_directory: bool, install_default_pip: bool, target_directory: str) -> str: try: # Validate input constraints if install_default_pip and ensure_upgrade: raise ValueError(\\"Both install_default_pip and ensure_upgrade cannot be True at the same time.\\") # Prepare ensurepip arguments args = [] if ensure_upgrade: args.append(\'--upgrade\') if use_user_directory: args.append(\'--user\') else: if target_directory: args.extend([\'--root\', target_directory]) # Run ensurepip ensurepip.bootstrap(*args) # Check pip version installed process = subprocess.run( [sys.executable, \'-m\', \'pip\', \'--version\'], capture_output=True, text=True, check=True ) pip_version = process.stdout.strip() return f\\"{pip_version} installed successfully\\" except ValueError as ve: return f\\"ValueError: {str(ve)}\\" except subprocess.CalledProcessError as err: return f\\"An error occurred while checking pip version: {err.stdout.strip()}\\" except Exception as e: return f\\"An unexpected error occurred: {str(e)}\\""},{"question":"**Tensor Operations and Autograd in PyTorch** In this coding assessment, you will demonstrate your understanding of basic and advanced tensor operations using PyTorch. Specifically, you will create tensors, perform operations on them, and utilize PyTorch\'s autograd feature for automatic differentiation. # Task 1. **Tensor Creation and Initialization**: - Create a function `initialize_tensors` that initializes a tensor of size (4, 4) with random values sampled from a normal distribution with mean `0` and standard deviation `1`. - Create a tensor of ones with the same size (4, 4). 2. **Basic Operations**: - Write a function `tensor_operations` that takes two tensors as input and performs the following operations: - Element-wise addition. - Element-wise multiplication. - Matrix multiplication. - Return the results of these operations as a tuple. 3. **Automatic Differentiation**: - Write a function `compute_gradient` that takes a tensor as input, sets `requires_grad=True`, performs the following operations, and returns the gradient of a scalar output with respect to the input tensor. - Compute the element-wise exponential of the tensor. - Compute the sum of all elements. - Use `.backward()` to compute the gradient. # Function Signatures ```python import torch def initialize_tensors() -> tuple: # initialize tensors pass def tensor_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> tuple: # perform operations and return results pass def compute_gradient(input_tensor: torch.Tensor) -> torch.Tensor: # compute and return gradient pass ``` # Instructions - Your code should be efficient and leverage PyTorch operations for tensor manipulations. - Ensure all tensors are created on the CPU for simplicity. - Test your functions by passing appropriate tensors and verifying the output. # Example Usage ```python tensor1, tensor2 = initialize_tensors() add_result, mul_result, matmul_result = tensor_operations(tensor1, tensor2) input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True) grad_result = compute_gradient(input_tensor) print(\\"Addition Result:n\\", add_result) print(\\"Multiplication Result:n\\", mul_result) print(\\"Matrix Multiplication Result:n\\", matmul_result) print(\\"Gradient:n\\", grad_result) ``` Your task is to implement the `initialize_tensors`, `tensor_operations`, and `compute_gradient` functions to meet the specified requirements and pass the example usage test.","solution":"import torch def initialize_tensors() -> tuple: Initialize two tensors: 1. Random values sampled from normal distribution (mean=0, std=1) of size (4, 4) 2. Tensor of ones of size (4, 4) Returns: tuple: (random_tensor, ones_tensor) random_tensor = torch.randn(4, 4) ones_tensor = torch.ones(4, 4) return random_tensor, ones_tensor def tensor_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> tuple: Perform tensor operations on two input tensors: 1. Element-wise addition 2. Element-wise multiplication 3. Matrix multiplication Args: tensor1 (torch.Tensor): First input tensor tensor2 (torch.Tensor): Second input tensor Returns: tuple: (add_result, mul_result, matmul_result) add_result = tensor1 + tensor2 mul_result = tensor1 * tensor2 matmul_result = torch.matmul(tensor1, tensor2) return add_result, mul_result, matmul_result def compute_gradient(input_tensor: torch.Tensor) -> torch.Tensor: Compute the gradient of a scalar output with respect to the input tensor. Args: input_tensor (torch.Tensor): Input tensor with requires_grad=True Returns: torch.Tensor: Gradient of the input tensor input_tensor.requires_grad_(True) # Element-wise exponential exp_tensor = torch.exp(input_tensor) # Sum of all elements to create a scalar scalar_output = torch.sum(exp_tensor) # Compute gradients scalar_output.backward() return input_tensor.grad"},{"question":"Objective: Demonstrate your understanding of scikit-learn\'s dataset loading utilities and your ability to perform basic data analysis. Problem Statement: Write a function `load_and_analyze_dataset` that performs the following tasks: 1. Loads the `iris` dataset using the `load_iris` function from `sklearn.datasets`. 2. Computes and returns the mean and standard deviation of each feature in the dataset. Specifications: Your function should be defined as: ```python def load_and_analyze_dataset(): Load the iris dataset and compute the mean and standard deviation for each feature. Returns: mean_std_dict (dict): A dictionary where keys are the feature names and values are tuples containing the mean and standard deviation of that feature. pass ``` **Input:** - There are no inputs to this function. **Output:** - `mean_std_dict`: A dictionary where each key is a feature name (as a string) and the corresponding value is a tuple of two elements: (mean, standard deviation) of that feature. **Constraints and Requirements:** - You must use the `load_iris` function from `sklearn.datasets`. - Ensure the output dictionary maintains the same order of features as in the dataset\'s `feature_names`. Example Output: ```python { \'sepal length (cm)\': (5.843333333333334, 0.828066127977863), \'sepal width (cm)\': (3.057333333333334, 0.435866284936698), \'petal length (cm)\': (3.758000000000001, 1.7652982332594662), \'petal width (cm)\': (1.199333333333334, 0.7622376689603465) } ``` Additional Notes: - Do not use any external datasets; rely on the scikit-learn utility to load the dataset. - Utilize numpy to compute the mean and standard deviation. - Keep your code clean and well-commented. Hints: - Refer to the documentation for `load_iris` and understand the structure of the returned dataset. - Use the `data` attribute for feature values and `feature_names` to access the feature names.","solution":"from sklearn.datasets import load_iris import numpy as np def load_and_analyze_dataset(): Load the iris dataset and compute the mean and standard deviation for each feature. Returns: mean_std_dict (dict): A dictionary where keys are the feature names and values are tuples containing the mean and standard deviation of that feature. iris = load_iris() data = iris.data feature_names = iris.feature_names mean_std_dict = {} for i, feature in enumerate(feature_names): mean = np.mean(data[:, i]) std = np.std(data[:, i]) mean_std_dict[feature] = (mean, std) return mean_std_dict"},{"question":"**Python Buffer Protocol Challenge** In this challenge, you are required to implement functions that interact with memory buffers using Python 3.10. Your task is to create a class `BufferHandler` that provides methods for reading from and writing to buffers. The class should use the recommended `PyObject_GetBuffer` and `PyBuffer_Release` functions. Here are the specifications: # Class: `BufferHandler` Method 1: `read_char_buffer` ```python def read_char_buffer(obj: Any) -> str: ``` - **Input**: A single argument `obj` that supports the character buffer interface. - **Output**: A string constructed from the read-only memory location. - **Constraints**: - If `obj` does not support the required interface, raise a `TypeError`. Method 2: `read_data_buffer` ```python def read_data_buffer(obj: Any) -> bytes: ``` - **Input**: A single argument `obj` that supports the readable buffer interface. - **Output**: A `bytes` object constructed from the read-only memory location. - **Constraints**: - If `obj` does not support the required interface, raise a `TypeError`. Method 3: `write_data_buffer` ```python def write_data_buffer(obj: Any, data: bytes) -> None: ``` - **Input**: - `obj`: An object that supports the writable buffer interface. - `data`: A `bytes` object to be written into the buffer. - **Output**: None. The method directly modifies the memory location of the given object with `data`. - **Constraints**: - If `obj` does not support the writable buffer interface, raise a `TypeError`. # Example Usage: ```python buffer_handler = BufferHandler() # Example of reading from a character buffer char_obj = b\\"example string\\" result = buffer_handler.read_char_buffer(char_obj) print(result) # Output: \\"example string\\" # Example of reading from a data buffer data_obj = b\\"x00x01x02\\" result = buffer_handler.read_data_buffer(data_obj) print(result) # Output: b\'x00x01x02\' # Example of writing to a data buffer write_obj = bytearray(3) buffer_handler.write_data_buffer(write_obj, b\'x03x04x05\') print(write_obj) # Output: bytearray(b\'x03x04x05\') ``` Ensure that your implementation adheres to the modern Python 3.10 buffer protocol and is free from deprecated practices.","solution":"from typing import Any class BufferHandler: def read_char_buffer(self, obj: Any) -> str: Reads from an object that supports the character buffer interface and returns a string. try: memory = memoryview(obj).tobytes().decode(\'utf-8\') return memory except TypeError: raise TypeError(\\"Object does not support the character buffer interface\\") def read_data_buffer(self, obj: Any) -> bytes: Reads from an object that supports the readable buffer interface and returns bytes. try: memory = memoryview(obj).tobytes() return memory except TypeError: raise TypeError(\\"Object does not support the readable buffer interface\\") def write_data_buffer(self, obj: Any, data: bytes) -> None: Writes bytes to an object that supports the writable buffer interface. try: buffer = memoryview(obj) if not buffer.readonly: buffer[:len(data)] = data else: raise TypeError(\\"Object does not support the writable buffer interface\\") except TypeError: raise TypeError(\\"Object does not support the buffer protocol or is not writable\\")"},{"question":"In this assessment, you will use the `unittest.mock` module to write unit tests for a provided class. The functionality to be tested involves internal method calls and external resource interactions. You will need to replace certain components with mocks, assert calls, and verify behaviors. Description You are given a class `DataProcessor` that processes data from a file and updates a database. The class has the following methods: 1. `load_data(file_path)`: Reads data from the specified file. 2. `process_data(data)`: Processes the loaded data. 3. `update_database(processed_data)`: Updates the database with the processed data. Your task is to write unit tests for this class using the `unittest` module, focusing on mocking the interactions. ```python import json class DataProcessor: def load_data(self, file_path): with open(file_path, \'r\') as file: data = json.load(file) return data def process_data(self, data): # Example processing: Capitalize all keys and values. return {key.upper(): value.upper() for key, value in data.items()} def update_database(self, processed_data): # Imagine this method interacts with a database db = self.get_database_connection() for key, value in processed_data.items(): db.update_record(key, value) def get_database_connection(self): # Placeholder for an actual database connection raise NotImplementedError(\\"This should be implemented by a subclass.\\") ``` Requirements 1. **Mock File Reading**: - Mock the `open` function in the `DataProcessor` class to return a predetermined data structure. - Verify that `load_data` correctly reads and returns this data. 2. **Mock Data Processing**: - Ensure the `process_data` method functions as expected without the need to perform actual data manipulation in the test. 3. **Mock Database Interaction**: - Mock the `get_database_connection` method and its return value to simulate database operations. - Verify that `update_database` invokes the database connection and updates records correctly. Implementation 1. **Write the Tests**: - Use `unittest.TestCase` to create your test class. - Use `patch` from `unittest.mock` to mock dependencies. - Test each method of the `DataProcessor` class separately with appropriate assertions. 2. **Example Test Cases**: - Test that `load_data` opens the correct file and reads data. - Test that `process_data` transforms the data correctly. - Test that `update_database` calls the correct methods on the database connection mock. Constraints - Do not actually create any files or databases in your tests. - Ensure your mocks and patches are set up and used correctly. - Structure your tests to be clear, maintainable, and adhere to best practices in unit testing. ```python import unittest from unittest.mock import patch, MagicMock, mock_open class TestDataProcessor(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'{\\"name\\": \\"john\\", \\"age\\": \\"thirty\\"}\') def test_load_data(self, mock_open): processor = DataProcessor() data = processor.load_data(\'dummy_path\') mock_open.assert_called_once_with(\'dummy_path\', \'r\') self.assertEqual(data, {\\"name\\": \\"john\\", \\"age\\": \\"thirty\\"}) def test_process_data(self): processor = DataProcessor() input_data = {\\"name\\": \\"john\\", \\"age\\": \\"thirty\\"} expected_output = {\\"NAME\\": \\"JOHN\\", \\"AGE\\": \\"THIRTY\\"} output_data = processor.process_data(input_data) self.assertEqual(output_data, expected_output) @patch.object(DataProcessor, \'get_database_connection\') def test_update_database(self, mock_get_db_connection): mock_db = MagicMock() mock_get_db_connection.return_value = mock_db processor = DataProcessor() processed_data = {\\"NAME\\": \\"JOHN\\", \\"AGE\\": \\"THIRTY\\"} processor.update_database(processed_data) calls = [patch(\'mock_db.update_record\', key, value) for key, value in processed_data.items()] mock_db.update_record.assert_has_calls(calls, any_order=True) ``` Write the necessary code to implement these tests. Ensure your implementation is correct and follows the guidelines provided.","solution":"import json class DataProcessor: def load_data(self, file_path): with open(file_path, \'r\') as file: data = json.load(file) return data def process_data(self, data): # Example processing: Capitalize all keys and values. return {key.upper(): value.upper() for key, value in data.items()} def update_database(self, processed_data): # Imagine this method interacts with a database db = self.get_database_connection() for key, value in processed_data.items(): db.update_record(key, value) def get_database_connection(self): # Placeholder for an actual database connection raise NotImplementedError(\\"This should be implemented by a subclass.\\")"},{"question":"**Scenario:** You are given a dataset containing information about sales transactions in a retail store. The dataset includes the following columns: - `transaction_id`: Unique identifier for each transaction. - `date`: The date on which the transaction occurred. - `store_id`: The identifier of the store where the transaction took place. - `product_id`: The identifier of the product sold. - `quantity`: The quantity of the product sold. - `price`: The price at which the product was sold. - `customer_id`: The identifier of the customer who made the transaction. Your task is to write a Python function using pandas that performs the following operations: 1. **Load the Dataset:** Read the dataset from a given CSV file path into a `pandas.DataFrame`. 2. **Data Cleaning:** - Handle missing values in the `quantity` and `price` columns by replacing them with the mean of their respective columns. 3. **Data Aggregation:** - Compute the total sales for each store on a monthly basis. The total sales are defined as the sum of products of `quantity` and `price` for each transaction. 4. **Data Reshaping:** - Pivot the data to have stores on the rows, months on the columns, and total sales as values. 5. **Plotting:** - Plot a bar chart that shows the monthly sales for each store. **Function Signature:** ```python def analyze_retail_sales(file_path: str) -> None: pass ``` **Constraints:** - The function should handle large datasets efficiently. - Ensure that the plotting uses appropriate labels for the axes and includes a legend. **Example Input:** A CSV file with the following content: ``` transaction_id,date,store_id,product_id,quantity,price,customer_id 1,2023-01-15,1,101,2,20.0,1001 2,2023-01-15,1,102,,15.0,1002 3,2023-02-18,2,103,1,25.0,1003 4,2023-03-20,3,104,3,30.0,1004 ``` **Example Output:** A bar chart visualizing monthly sales per store (specific output will depend on the input dataset). Implement the function `analyze_retail_sales(file_path: str) -> None` in Python using the pandas package.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_retail_sales(file_path: str) -> None: # Load the Dataset df = pd.read_csv(file_path) # Data Cleaning # Replace missing values in \'quantity\' and \'price\' columns with their respective means df[\'quantity\'].fillna(df[\'quantity\'].mean(), inplace=True) df[\'price\'].fillna(df[\'price\'].mean(), inplace=True) # Ensure \'date\' column is in datetime format df[\'date\'] = pd.to_datetime(df[\'date\']) # Create a \'month\' column df[\'month\'] = df[\'date\'].dt.to_period(\'M\') # Data Aggregation - Compute total sales for each store on a monthly basis df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] monthly_sales = df.groupby([\'store_id\', \'month\'])[\'total_sales\'].sum().reset_index() # Data Reshaping - Pivot the data pivot_table = monthly_sales.pivot(index=\'store_id\', columns=\'month\', values=\'total_sales\') # Plotting - Bar chart for monthly sales per store pivot_table.plot(kind=\'bar\', figsize=(14, 7)) plt.title(\\"Monthly Sales per Store\\") plt.xlabel(\\"Store ID\\") plt.ylabel(\\"Total Sales\\") plt.legend(title=\\"Month\\") plt.show()"},{"question":"Objective: To assess students\' understanding of the `atexit` module in Python by writing code that demonstrates its use in a practical context. Question: You are tasked with creating a Python program that logs user actions to a file during the application\'s runtime and ensures that an application summary is saved when the program terminates. **Requirements:** 1. Create a class `UserLogger` which will handle registering and logging user actions. 2. The class should have the following methods: - `__init__(self)`: Initializes the logger and ensures that an initial message `\'Logging started\'` is written to the log file. - `log_action(self, action: str)`: Logs a user action to the log file. - `summary(self)`: Writes a summary of all user actions to a summary file which should be called automatically when the program terminates using the `atexit` module. 3. You should register the `summary` method with the `atexit` module so it is called when the interpreter terminates. 4. The log file should be named `user_actions.log` and the summary file should be named `summary.log`. **Example Usage:** ```python logger = UserLogger() logger.log_action(\'User logged in\') logger.log_action(\'User clicked on button\') # When the program terminates, \' summary.log\' should be created with a summary of the actions ``` **Constraints:** - The program should handle exceptions gracefully. - You are not allowed to call the `summary` method explicitly; it must only be triggered by program termination. **Input and Output Format:** - There is no standard input; the program makes use of class methods and automatic termination handling. - The log files will be the only output. Ensure that your implementation follows best practices and handles edge cases appropriately. **Note:** - Make sure your program can handle the scenario where it may be interrupted unexpectedly, preserving the integrity of the log files up to the last successfully logged action.","solution":"import atexit class UserLogger: def __init__(self): self.log_file = \'user_actions.log\' self.summary_file = \'summary.log\' self.actions = [] with open(self.log_file, \'w\') as f: f.write(\'Logging startedn\') atexit.register(self.summary) def log_action(self, action: str): try: with open(self.log_file, \'a\') as f: f.write(action + \'n\') self.actions.append(action) except Exception as e: print(f\\"Error logging action: {e}\\") def summary(self): try: with open(self.summary_file, \'w\') as f: f.write(\'Summary of user actions:n\') for action in self.actions: f.write(action + \'n\') except Exception as e: print(f\\"Error writing summary: {e}\\") # Example usage # Uncomment the following lines to see it in action # logger = UserLogger() # logger.log_action(\'User logged in\') # logger.log_action(\'User clicked on button\')"},{"question":"**Objective**: Implement a function to process and manipulate nested lists and dictionaries using comprehensions, list methods, and appropriate looping techniques. **Problem**: You are given a list of dictionaries representing a collection of students. Each dictionary contains the following keys: - `\'name\': string` representing the student\'s name. - `\'grades\': list` containing their grades in various subjects (integers). - `\'attendance\': integer` representing the number of days attended. Your task is to: 1. Flatten the list of dictionaries into a single list of tuples, where each tuple contains (name, grade) for each grade entry in the grades list. 2. From this flattened list, generate a dictionary where the keys are the student names, and the values are their grades sorted in descending order. 3. Calculate the highest average grade among the students and return the name of the student with the highest average grade along with their average grade as a float. **Function Signature**: ```python def process_student_data(student_data: list) -> tuple: ``` **Input**: - `student_data` (list of dict): A list where each element is a dictionary with the specified key-value pairs. **Output**: - Return a tuple containing the student\'s name with the highest average grade and the average grade as a float, formatted to two decimal places. **Example**: ```python student_data = [ {\'name\': \'Alice\', \'grades\': [88, 92, 79], \'attendance\': 150}, {\'name\': \'Bob\', \'grades\': [72, 85, 89], \'attendance\': 145}, {\'name\': \'Charlie\', \'grades\': [95, 90, 90], \'attendance\': 148} ] # Should return: (\'Charlie\', 91.67) process_student_data(student_data) ``` **Constraints**: - Grades list is non-empty. - Attendance values are non-negative integers. - All names are non-empty strings and unique. **Performance Requirements**: - The solution should handle input lists of up to 10^4 student dictionaries efficiently. **Note**: Make sure to use list comprehensions, sorting techniques, and dictionary operations effectively as demonstrated in the provided documentation.","solution":"def process_student_data(student_data): # Step 1: Flatten the list of dictionaries into a list of tuples (name, grade) flattened_list = [(student[\'name\'], grade) for student in student_data for grade in student[\'grades\']] # Step 2: Generate a dictionary where the keys are names, and the values are lists of sorted grades from collections import defaultdict grades_dict = defaultdict(list) for name, grade in flattened_list: grades_dict[name].append(grade) # Sorting the grades in descending order for each student for name in grades_dict: grades_dict[name].sort(reverse=True) # Step 3: Calculate the highest average grade among the students highest_avg = (\'\', 0.0) for name, grades in grades_dict.items(): avg_grade = sum(grades) / len(grades) if avg_grade > highest_avg[1]: highest_avg = (name, avg_grade) return (highest_avg[0], round(highest_avg[1], 2))"},{"question":"# Python File Handling and Descriptor Manipulation **Problem Description:** You are tasked with implementing a Python function `file_descriptor_ops` which simulates advanced file operations by using low-level file descriptors and streams. The function should perform the following steps: 1. **Open a file** with a specified mode (read/write/append) and return its file descriptor. 2. **Write a list of strings** to the file via the file descriptor. 3. **Read back the contents** of the file using the file descriptor to verify what was written. 4. **Convert the file descriptor to a Python file object** and perform additional operations like reading a specific line. 5. Handle any exceptions that may arise during these operations and ensure resources are properly managed (i.e., files are closed). **Function Signature:** ```python def file_descriptor_ops(filepath: str, mode: str, lines: list) -> dict: Perform file operations using low-level file descriptors. Args: filepath (str): The path to the file to be created or opened. mode (str): The mode in which the file should be opened (\'r\', \'w\', \'a\'). lines (list): A list of strings to be written to the file. Returns: dict: A dictionary with the following keys: - \'written_contents\': the text that was written to the file. - \'read_contents\': the text read back for verification. - \'line_3\': the third line from the file, using the file object. pass ``` # Constraints: - **`mode`** can only be \'r\', \'w\', or \'a\'. Handle invalid modes by raising an appropriate exception. - **`lines`** is guaranteed to have at least 3 elements. - File should be properly closed after reading and writing operations regardless of success or failure. # Example Usage: ```python filepath = \'sample.txt\' mode = \'w\' lines = [\\"First line\\", \\"Second line\\", \\"Third line\\", \\"Fourth line\\"] result = file_descriptor_ops(filepath, mode, lines) assert result[\'written_contents\'] == \\"First linenSecond linenThird linenFourth linen\\" assert result[\'read_contents\'] == \\"First linenSecond linenThird linenFourth linen\\" assert result[\'line_3\'] == \\"Third linen\\" ``` # Notes: - You may use the `os` and `io` modules to interact with file descriptors and file objects respectively. - Ensure proper error handling and resource cleanup.","solution":"import os def file_descriptor_ops(filepath: str, mode: str, lines: list) -> dict: if mode not in [\'r\', \'w\', \'a\']: raise ValueError(\\"Invalid mode. Mode must be \'r\', \'w\', or \'a\'.\\") written_contents = \\"\\" read_contents = \\"\\" line_3 = \\"\\" try: # 1. Open the file with specified mode and get the file descriptor flags = { \'r\': os.O_RDONLY, \'w\': os.O_WRONLY | os.O_CREAT | os.O_TRUNC, \'a\': os.O_WRONLY | os.O_CREAT | os.O_APPEND }[mode] fd = os.open(filepath, flags, 0o666) # 2. Write lines to the file via the file descriptor if mode is not \'r\' if mode != \'r\': for line in lines: os.write(fd, (line + \\"n\\").encode()) written_contents = \\"\\".join(line + \\"n\\" for line in lines) # Sync and close the fd os.fsync(fd) os.lseek(fd, 0, os.SEEK_SET) # 3. Read back the contents of the file using the file descriptor fd_read = os.open(filepath, os.O_RDONLY) read_contents = os.read(fd_read, 1024).decode() os.close(fd_read) # 4. Convert the file descriptor to a Python file object and read 3rd line with open(filepath, \'r\') as f: for i, line in enumerate(f): if i == 2: line_3 = line break except Exception as e: raise e finally: os.close(fd) return { \'written_contents\': written_contents, \'read_contents\': read_contents, \'line_3\': line_3 }"},{"question":"Objective: The goal of this assessment is to evaluate your understanding of the `getopt` module in Python 3.10, focusing on your ability to parse command line arguments. Problem Statement: You are required to write a Python script that processes a specific set of command line arguments to perform text manipulation on an input text file. The script should support the following command line options: - `-i` or `--input`: Specifies the path to the input text file (required). - `-o` or `--output`: Specifies the path to the output text file (optional). - `-u` or `--uppercase`: Converts the text to uppercase (optional). - `-l` or `--lowercase`: Converts the text to lowercase (optional). - `-r` or `--reverse`: Reverses the text (optional). Your script should parse these command line arguments using the `getopt` module and perform the specified text manipulations in the order of the arguments provided. The manipulated text should be written to the specified output file, or printed to the console if no output file is specified. Input Format: - Command line arguments. Expected Output Format: - The manipulated text written to the output file (if specified) or printed to the console. Constraints: - The script must use the `getopt` module for argument parsing. - Cannot use the `argparse` module or any other third-party argument parsing libraries. Example Usage: ``` python text_manipulate.py -i input.txt -o output.txt -u -r ``` This should read the text from `input.txt`, convert it to uppercase, then reverse it, and write the resulting text to `output.txt`. Implementation: ```python import getopt import sys def read_file(file_path): with open(file_path, \'r\') as file: return file.read() def write_file(file_path, content): with open(file_path, \'w\') as file: file.write(content) def manipulate_text(text, options): for opt, arg in options: if opt in (\'-u\', \'--uppercase\'): text = text.upper() elif opt in (\'-l\', \'--lowercase\'): text = text.lower() elif opt in (\'-r\', \'--reverse\'): text = text[::-1] return text def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"i:o:ulr\\", [\\"input=\\", \\"output=\\", \\"uppercase\\", \\"lowercase\\", \\"reverse\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) input_file = None output_file = None for opt, arg in opts: if opt in (\'-i\', \'--input\'): input_file = arg elif opt in (\'-o\', \'--output\'): output_file = arg if not input_file: print(\\"Input file is required\\") sys.exit(2) text = read_file(input_file) manipulated_text = manipulate_text(text, opts) if output_file: write_file(output_file, manipulated_text) else: print(manipulated_text) if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Implement the script in the provided `main()` function template. 2. Ensure your script handles the command line arguments correctly and performs the specified text manipulations. 3. Test your script with various combinations of command line arguments to ensure it works as expected.","solution":"import getopt import sys def read_file(file_path): with open(file_path, \'r\') as file: return file.read() def write_file(file_path, content): with open(file_path, \'w\') as file: file.write(content) def manipulate_text(text, options): for opt, arg in options: if opt in (\'-u\', \'--uppercase\'): text = text.upper() elif opt in (\'-l\', \'--lowercase\'): text = text.lower() elif opt in (\'-r\', \'--reverse\'): text = text[::-1] return text def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"i:o:ulr\\", [\\"input=\\", \\"output=\\", \\"uppercase\\", \\"lowercase\\", \\"reverse\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) input_file = None output_file = None for opt, arg in opts: if opt in (\'-i\', \'--input\'): input_file = arg elif opt in (\'-o\', \'--output\'): output_file = arg if not input_file: print(\\"Input file is required\\") sys.exit(2) text = read_file(input_file) manipulated_text = manipulate_text(text, opts) if output_file: write_file(output_file, manipulated_text) else: print(manipulated_text) if __name__ == \\"__main__\\": main()"},{"question":"You are provided with time-series sales data for a retail store. Each data point represents the sales for a particular day. Your goal is to analyze the data to identify trends and provide insights. Task 1. **Calculate Rolling Metrics:** - Implement a function `compute_rolling_metrics` that calculates the 7-day rolling mean and rolling standard deviation of the daily sales. ```python def compute_rolling_metrics(sales_data): Calculate the 7-day rolling mean and rolling standard deviation of daily sales. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.DataFrame: A DataFrame containing the original sales data, the 7-day rolling mean, and the 7-day rolling standard deviation with column names \'sales\', \'rolling_mean\', and \'rolling_std\'. pass ``` 2. **Calculate Expanding Metrics:** - Implement a function `compute_expanding_metrics` that calculates the expanding mean and expanding sum of the daily sales. ```python def compute_expanding_metrics(sales_data): Calculate the expanding mean and expanding sum of daily sales. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.DataFrame: A DataFrame containing the original sales data, the expanding mean, and the expanding sum with column names \'sales\', \'expanding_mean\', and \'expanding_sum\'. pass ``` 3. **Calculate Exponentially-weighted Metrics:** - Implement a function `compute_ewm_metrics` that calculates the exponentially weighted mean with a span of 10 days. ```python def compute_ewm_metrics(sales_data): Calculate the exponentially weighted mean of daily sales with a span of 10 days. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.Series: A Series containing the exponentially weighted mean. pass ``` Constraints - The input `sales_data` will be a non-empty pandas Series containing daily sales figures as floats or integers. - Ensure that the functions handle the rolling, expanding, and exponentially-weighted operations correctly. - The functions should not modify the original input data. Example Usage ```python import pandas as pd # Sample sales data data = pd.Series([200, 220, 210, 260, 230, 250, 270, 300, 310, 320, 330]) # Compute rolling metrics rolling_metrics = compute_rolling_metrics(data) print(rolling_metrics) # Compute expanding metrics expanding_metrics = compute_expanding_metrics(data) print(expanding_metrics) # Compute exponentially weighted metrics ewm_metrics = compute_ewm_metrics(data) print(ewm_metrics) ``` Expected Output The expected output for each function should be a DataFrame or Series with rolling, expanding, or exponentially weighted metrics calculated based on the provided sample data.","solution":"import pandas as pd def compute_rolling_metrics(sales_data): Calculate the 7-day rolling mean and rolling standard deviation of daily sales. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.DataFrame: A DataFrame containing the original sales data, the 7-day rolling mean, and the 7-day rolling standard deviation with column names \'sales\', \'rolling_mean\', and \'rolling_std\'. rolling_mean = sales_data.rolling(window=7).mean() rolling_std = sales_data.rolling(window=7).std() return pd.DataFrame({ \'sales\': sales_data, \'rolling_mean\': rolling_mean, \'rolling_std\': rolling_std }) def compute_expanding_metrics(sales_data): Calculate the expanding mean and expanding sum of daily sales. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.DataFrame: A DataFrame containing the original sales data, the expanding mean, and the expanding sum with column names \'sales\', \'expanding_mean\', and \'expanding_sum\'. expanding_mean = sales_data.expanding().mean() expanding_sum = sales_data.expanding().sum() return pd.DataFrame({ \'sales\': sales_data, \'expanding_mean\': expanding_mean, \'expanding_sum\': expanding_sum }) def compute_ewm_metrics(sales_data): Calculate the exponentially weighted mean of daily sales with a span of 10 days. Args: sales_data (pd.Series): A pandas Series containing daily sales data. Returns: pd.Series: A Series containing the exponentially weighted mean. ewm_mean = sales_data.ewm(span=10).mean() return ewm_mean"},{"question":"**Problem Statement: Advanced Seaborn ECDF Plotting** You are provided with a dataset called `penguins` that contains information about various penguin species. Your task is to implement a function using seaborn\'s `ecdfplot` to visualize this data in different ways. The final plot should consist of subplots arranged in a 2x2 grid, demonstrating various capabilities of `ecdfplot`. Specifically, you need to plot the following: 1. A univariate distribution of the `flipper_length_mm` along the x-axis. 2. A flipped univariate distribution of the `flipper_length_mm` along the y-axis. 3. Multiple histograms of `bill_length_mm` differentiated by species. 4. The empirical complementary CDF of `bill_length_mm`, with percentages on the y-axis, and differentiated by species. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def plot_advanced_ecdf(data): Plots a 2x2 grid of ECDF plots using the seaborn package. Parameters: data (DataFrame): A pandas DataFrame containing the penguin dataset. Returns: None: The function should display the plot grid without returning any value. # Your solution here # Example usage: # penguins = sns.load_dataset(\\"penguins\\") # plot_advanced_ecdf(penguins) ``` # Input - `data`: A pandas DataFrame containing the penguin dataset. This dataset will have at least the columns: `flipper_length_mm`, `bill_length_mm`, and `species`. # Output - The function should not return any value. Instead, it should display a 2x2 grid of subplots generated by seaborn’s `ecdfplot`. # Constraints - The function should use seaborn\'s `ecdfplot` function for all subplots. - The function should use matplotlib\'s `plt.subplots` for creating subplots. # Example If you call the function with the provided penguins dataset, you should see a 2x2 grid of ECDF plots, each reflecting the requested visualization adjustments, displayed inline. # Notes - Make sure to handle missing data where applicable. - The `hue` parameter in seaborn should be used for differentiating by species in relevant plots. - Use matplotlib functions to customize subplot layout and title each subplot appropriately. --- Let\'s see how well you can make use of seaborn\'s ECDF plotting capabilities!","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_advanced_ecdf(data): Plots a 2x2 grid of ECDF plots using the seaborn package. Parameters: data (DataFrame): A pandas DataFrame containing the penguin dataset. Returns: None: The function should display the plot grid without returning any value. # Initialize the 2x2 subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # 1. Univariate distribution of the flipper_length_mm along the x-axis sns.ecdfplot(data=data, x=\\"flipper_length_mm\\", ax=axes[0, 0]) axes[0, 0].set_title(\\"ECDF of Flipper Length (mm)\\") # 2. Flipped univariate distribution of the flipper_length_mm along the y-axis sns.ecdfplot(data=data, y=\\"flipper_length_mm\\", ax=axes[0, 1]) axes[0, 1].set_title(\\"Flipped ECDF of Flipper Length (mm)\\") # 3. Multiple ECDFs of bill_length_mm differentiated by species sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\", ax=axes[1, 0]) axes[1, 0].set_title(\\"ECDF of Bill Length (mm) by Species\\") # 4. Empirical complementary CDF of bill_length_mm, with percentages on the y-axis, and differentiated by species sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True, ax=axes[1, 1]) axes[1, 1].set_title(\\"Empirical Complementary CDF of Bill Length (mm) by Species\\") axes[1, 1].set_ylabel(\\"Percentage\\") plt.tight_layout() plt.show() # Example usage: # penguins = sns.load_dataset(\\"penguins\\") # plot_advanced_ecdf(penguins)"},{"question":"**Problem Statement:** You are tasked to develop a utility that helps users to compare directories to find files that are different in their contents, structure, or presence across two directories. Using the `filecmp` module, you need to implement the following functionalities: 1. **compare_directories(root_dir1: str, root_dir2: str) -> dict**: - This function takes in two root directory paths as inputs and recursively compares their contents. - It should return a dictionary with the following keys: - `\\"common_files\\"`: List of file names that are common to both directories and are the same. - `\\"diff_files\\"`: List of file names that are common to both directories but differ in content. - `\\"left_only_files\\"`: List of file names that are only in the first directory. - `\\"right_only_files\\"`: List of file names that are only in the second directory. - `\\"errors\\"`: List of file names that could not be compared (e.g., due to permission issues). - Example: ```python result = compare_directories(\'dir1\', \'dir2\') print(result) ``` 2. **clear_comparison_cache() -> None**: - This function clears the comparison cache used by the `filecmp` module. **Constraints:** - You can assume the provided directories exist. - Handle any exceptions related to file permission issues gracefully, by adding such files to the `\\"errors\\"` list. **Performance Requirements:** - The solution should efficiently compare directories, making use of shallow comparisons where possible, but defaulting to deep comparisons in case of discrepancies. **Expected Input/Output Formats:** - **Input:** - `root_dir1`: str, a path to the first directory. - `root_dir2`: str, a path to the second directory. - **Output:** - A dictionary containing: - key `\\"common_files\\"` with value as list of str. - key `\\"diff_files\\"` with value as list of str. - key `\\"left_only_files\\"` with value as list of str. - key `\\"right_only_files\\"` with value as list of str. - key `\\"errors\\"` with value as list of str. Here is a scaffold of what your implementation should look like: ```python import filecmp def compare_directories(root_dir1: str, root_dir2: str) -> dict: # Your implementation here pass def clear_comparison_cache() -> None: # Your implementation here pass # Example usage: if __name__ == \\"__main__\\": result = compare_directories(\'dir1\', \'dir2\') print(result) clear_comparison_cache() ```","solution":"import filecmp import os def compare_directories(root_dir1: str, root_dir2: str) -> dict: comparison_results = { \\"common_files\\": [], \\"diff_files\\": [], \\"left_only_files\\": [], \\"right_only_files\\": [], \\"errors\\": [] } try: dir_cmp = filecmp.dircmp(root_dir1, root_dir2, ignore=[]) # Common files comparison_results[\\"common_files\\"] = dir_cmp.same_files # Files that exist in both directories but are different comparison_results[\\"diff_files\\"] = dir_cmp.diff_files # Files only in the first directory comparison_results[\\"left_only_files\\"] = dir_cmp.left_only # Files only in the second directory comparison_results[\\"right_only_files\\"] = dir_cmp.right_only # Handle subdirectories by recursing through them for sub_dir in dir_cmp.common_dirs: sub_dir1 = os.path.join(root_dir1, sub_dir) sub_dir2 = os.path.join(root_dir2, sub_dir) sub_comparison = compare_directories(sub_dir1, sub_dir2) for key in comparison_results: comparison_results[key].extend(sub_comparison[key]) except Exception as e: comparison_results[\\"errors\\"].append(str(e)) return comparison_results def clear_comparison_cache() -> None: # Clear any caches used by filecmp module (this is not strictly necessary as # filecmp module does not use persistent caches) filecmp.clear_cache()"},{"question":"# Python C API Programming Assessment Objective: Demonstrate your understanding of reference counting, exception handling, and API function usage by creating a custom Python C extension. Problem Statement: You are required to write a Python C extension module that provides a single function `incr_all_items(PyObject *dict)` which increments all integer values in a given Python dictionary by 1. If a dictionary\'s key points to a non-integer value, it should raise a `TypeError`. Requirements: 1. Write the C function `incr_all_items` that: - Takes a Python dictionary (`PyObject *dict`) as its input argument. - Iterates through each key-value pair in the dictionary. - For each integer value, increments it by 1. - Raises a `TypeError` if any value is not an integer. - Properly manages reference counts of Python objects. 2. Create the necessary boilerplate to define, initialize, and finalize the Python C extension module. Input: - A Python dictionary with arbitrary keys pointing to integer or non-integer values. Output: - A Python dictionary with each integer value incremented by 1. Raises `TypeError` otherwise. Constraints: - Your function should handle empty dictionaries correctly. - Use appropriate macros where necessary (e.g., `PyDict_SetItem`, `PyErr_SetString`, `Py_DECREF`, etc.). - Ensure to decrement the reference count of temporary objects created during the process. - Follow Python C API documentation for guidance on how to manage reference counts and exceptions. Example: ```python # Python usage example after importing your module result = incr_all_items({\'a\': 1, \'b\': 2, \'c\': \'three\'}) # This should raise a TypeError because \'c\' is not an integer ``` # Solution Outline: Here is a rough structure to get you started. Fill in the logic for reference counting, exception handling, and iteration over the dictionary. ```c #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* incr_all_items(PyObject *self, PyObject *args) { PyObject *dict; if (!PyArg_ParseTuple(args, \\"O!\\", &PyDict_Type, &dict)) { return NULL; } PyObject *key, *value; Py_ssize_t pos = 0; while (PyDict_Next(dict, &pos, &key, &value)) { if (PyLong_Check(value)) { long new_value = PyLong_AsLong(value) + 1; PyObject *new_value_obj = PyLong_FromLong(new_value); if (new_value_obj == NULL) { return NULL; // Propagate the error } PyDict_SetItem(dict, key, new_value_obj); Py_DECREF(new_value_obj); } else { PyErr_SetString(PyExc_TypeError, \\"All values must be integers\\"); return NULL; } } Py_INCREF(dict); return dict; } static PyMethodDef ModuleMethods[] = { {\\"incr_all_items\\", incr_all_items, METH_VARARGS, \\"Increment all integer items in the dictionary.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef moduledef = { PyModuleDef_HEAD_INIT, \\"my_extension\\", NULL, -1, ModuleMethods }; PyMODINIT_FUNC PyInit_my_extension(void) { return PyModule_Create(&moduledef); } ``` Instructions: 1. Complete the logic for error handling and reference count management in the provided function skeleton. 2. Compile the module and test it with various dictionaries to ensure correct behavior.","solution":"from datetime import datetime def days_since_date(year, month, day): Returns the number of days between the given date and today. given_date = datetime(year, month, day) today = datetime.now() delta = today - given_date return delta.days"},{"question":"# Python Coding Assessment: Handling `__annotations__` in Different Python Versions Objective: To assess your understanding of handling `__annotations__` attributes in Python versions 3.10 and newer, as well as versions 3.9 and older, along with managing stringized annotations. Problem Statement: You are required to implement a function `get_annotations(obj, force_eval=False)` that retrieves the annotations dictionary for a given object `obj`. The function should: 1. Work correctly for functions, classes, and modules in Python 3.10 and newer using `inspect.get_annotations()` where available. 2. Handle cases in Python 3.9 and older by considering the peculiarities of class inheritance and annotations. 3. Optionally \\"un-stringize\\" stringized annotations if `force_eval` is set to `True`. Function Signature: ```python def get_annotations(obj: object, force_eval: bool = False) -> dict: pass ``` Input: - `obj`: The object (function, class, or module) from which to retrieve the annotations. - `force_eval`: A boolean flag indicating whether to un-stringize stringized annotations. Output: - Returns a dictionary containing the annotations, with stringized annotations optionally evaluated. Constraints: - Do not modify the `__annotations__` attribute directly. - Ensure the solution is robust against missing or malformed `__annotations__` attributes. - Handle different Python versions appropriately. Example: ```python from __future__ import annotations def sample_func(x: int, y: \\"str | None\\" = None) -> \\"List[str]\\": pass class SampleClass: a: int b: \\"str\\" import sys module_name = sys.modules[__name__] # Example usage: # Retrieve annotations without evaluation print(get_annotations(sample_func)) # Output: {\'x\': int, \'y\': \'str | None\', \'return\': \'List[str]\'} print(get_annotations(SampleClass)) # Output: {\'a\': int, \'b\': \'str\'} print(get_annotations(module_name)) # Output: {} # Retrieve annotations with evaluation print(get_annotations(sample_func, force_eval=True)) # Output may vary based on actual runtime types ``` Notes: - Ensure compatibility with both Python 3.10+ and Python 3.9-. - Use `inspect.get_annotations()` where appropriate. - For un-stringizing, you may use `eval()` as described in the provided documentation, ensuring the correct use of globals and locals. Good luck!","solution":"import sys import inspect from typing import get_type_hints def get_annotations(obj: object, force_eval: bool = False) -> dict: if sys.version_info >= (3, 10): if force_eval: return get_type_hints(obj) return inspect.get_annotations(obj, eval_str=False) # For Python versions < 3.10, manually get annotations annotations = getattr(obj, \'__annotations__\', {}) if not annotations: return {} if force_eval: return get_type_hints(obj) return annotations"},{"question":"Implementing a Custom Convolutional Block in PyTorch Introduction: You are tasked with implementing a custom convolutional block using PyTorch\'s `torch.nn.functional` module. This block will be a critical component of a convolutional neural network (CNN) for image classification. Requirements: 1. A convolutional layer followed by batch normalization. 2. A ReLU activation function. 3. A max pooling layer. 4. An implementation of dropout for regularization. Function Signature: ```python import torch import torch.nn.functional as F def custom_conv_block(input_tensor, out_channels, kernel_size, stride, padding, pool_size, dropout_rate): Implements a custom convolutional block. Arguments: input_tensor (torch.Tensor): The input tensor with shape (batch_size, in_channels, height, width). out_channels (int): Number of output channels for the convolutional layer. kernel_size (int or tuple): Size of the convolving kernel. stride (int or tuple): Stride of the convolution. padding (int or tuple): Zero-padding added to both sides of the input. pool_size (int or tuple): Size of the max pooling. dropout_rate (float): Probability of an element to be zeroed for dropout. Returns: torch.Tensor: Output tensor after applying the custom convolutional block. # Perform convolution conv = F.conv2d(input_tensor, weight_conv, bias_conv, stride=stride, padding=padding) # Apply batch normalization normalized = F.batch_norm(conv, running_mean, running_var, weight_bn, bias_bn, training=True, momentum=0.1) # Apply ReLU activation relu_activated = F.relu(normalized) # Apply max pooling pooled = F.max_pool2d(relu_activated, pool_size) # Apply dropout output = F.dropout(pooled, p=dropout_rate, training=True) return output ``` Details: 1. **Input Tensor:** The input tensor will have the shape `(batch_size, in_channels, height, width)`. 2. **Output Channels:** The number of output channels for the convolutional layer. 3. **Kernel Size, Stride, Padding:** Parameters controlling the convolution operation. 4. **Pool Size:** The size parameter for the max pooling operation. 5. **Dropout Rate:** A float between 0 and 1 representing the dropout probability. Constraints: - Use the functional API from `torch.nn.functional` exclusively. - You must handle the initialization of weights and biases internally within the function by defining appropriately sized tensors. Example: ```python # Define the input tensor with random values input_tensor = torch.randn(8, 3, 32, 32) # Example shape (batch_size, in_channels, height, width) # Parameters for the custom convolutional block out_channels = 16 kernel_size = 3 stride = 1 padding = 1 pool_size = 2 dropout_rate = 0.5 # Call the function with the example parameters output = custom_conv_block(input_tensor, out_channels, kernel_size, stride, padding, pool_size, dropout_rate) print(output.shape) # Expected output shape: (batch_size, out_channels, height//2, width//2) ``` Notes: - Ensure the implementation correctly initializes the weight tensors (`weight_conv`, `weight_bn`, etc.) and bias tensors. - Verify the output dimensions after each operation to ensure they align with the expected CNN architecture.","solution":"import torch import torch.nn.functional as F from torch.nn.init import kaiming_uniform_ def custom_conv_block(input_tensor, out_channels, kernel_size, stride, padding, pool_size, dropout_rate): Implements a custom convolutional block. Arguments: input_tensor (torch.Tensor): The input tensor with shape (batch_size, in_channels, height, width). out_channels (int): Number of output channels for the convolutional layer. kernel_size (int or tuple): Size of the convolving kernel. stride (int or tuple): Stride of the convolution. padding (int or tuple): Zero-padding added to both sides of the input. pool_size (int or tuple): Size of the max pooling. dropout_rate (float): Probability of an element to be zeroed for dropout. Returns: torch.Tensor: Output tensor after applying the custom convolutional block. in_channels = input_tensor.shape[1] # Initialize convolutional layer weights and bias weight_conv = torch.empty(out_channels, in_channels, kernel_size, kernel_size) kaiming_uniform_(weight_conv, nonlinearity=\'relu\') bias_conv = torch.zeros(out_channels) # Perform convolution conv = F.conv2d(input_tensor, weight_conv, bias_conv, stride=stride, padding=padding) # Batch normalization parameters running_mean = torch.zeros(out_channels) running_var = torch.ones(out_channels) weight_bn = torch.ones(out_channels) bias_bn = torch.zeros(out_channels) # Apply batch normalization normalized = F.batch_norm(conv, running_mean, running_var, weight_bn, bias_bn, training=True, momentum=0.1) # Apply ReLU activation relu_activated = F.relu(normalized) # Apply max pooling pooled = F.max_pool2d(relu_activated, pool_size) # Apply dropout output = F.dropout(pooled, p=dropout_rate, training=True) return output"},{"question":"# Question **Understanding Function Signatures and Documentation with the `inspect` Module** The task is to implement a Python function named `describe_callable()` that accepts a callable (function, method, or class), and returns a detailed description of the callable\'s signature and its documentation. # Input Format * The function should accept a single argument: - `callable_entity`: A function, method, or class that needs to be inspected. # Output Format * The function should return a dictionary with the following keys: - `name`: The name of the callable. - `signature`: The call signature of the callable as a string. - `docstring`: The cleaned-up documentation string of the callable. - `parameters`: A dictionary of parameter names to their details, where each detail includes: - `kind`: Specifies how argument values are bound to the parameter. - `default`: Default value of the parameter if any or `None`. - `annotation`: Type annotation of the parameter if any or `None`. # Constraints * Use the `inspect` module for retrieving the signature and documentation. * If annotations or defaults are not provided, ensure they are represented as `None`. * Assume inputs will always be valid callables. # Example Usage ```python def sample_function(a, b: int = 42, *args, **kwargs): Sample function for demonstration purposes. Parameters: a: Any type b (int): An integer with a default value of 42 Returns: None pass describe_callable(sample_function) ``` # Example Output ```python { \'name\': \'sample_function\', \'signature\': \'(a, b: int = 42, *args, **kwargs)\', \'docstring\': \'Sample function for demonstration purposes.nnParameters:na: Any typenb (int): An integer with a default value of 42nnReturns:nNone\', \'parameters\': { \'a\': { \'kind\': \'POSITIONAL_OR_KEYWORD\', \'default\': None, \'annotation\': None }, \'b\': { \'kind\': \'POSITIONAL_OR_KEYWORD\', \'default\': 42, \'annotation\': \'int\' }, \'args\': { \'kind\': \'VAR_POSITIONAL\', \'default\': None, \'annotation\': None }, \'kwargs\': { \'kind\': \'VAR_KEYWORD\', \'default\': None, \'annotation\': None } } } ``` # Implementation Implement the function `describe_callable()` using the `inspect` module.","solution":"import inspect def describe_callable(callable_entity): Provides detailed description of the callable\'s signature and documentation. Args: callable_entity: A function, method, or class to inspect. Returns: dict: A dictionary containing the name, signature, docstring, and parameters of the callable. description = { \'name\': callable_entity.__name__, \'signature\': str(inspect.signature(callable_entity)), \'docstring\': inspect.getdoc(callable_entity) or \\"\\", \'parameters\': {} } for name, param in inspect.signature(callable_entity).parameters.items(): description[\'parameters\'][name] = { \'kind\': param.kind.name, \'default\': param.default if param.default is not param.empty else None, \'annotation\': param.annotation if param.annotation is not param.empty else None } return description"},{"question":"# Custom JSON Serialization/Deserialization **Objective:** Implement custom serialization and deserialization logic to handle complex Python objects using the `json` module. **Scenario:** You are given a `Person` class which has attributes: `name` (string), `age` (integer), and `address` (dictionary containing `street`, `city`, and `zip_code` keys). Additionally, the `Person` class has a `friends` attribute which is a list of `Person` objects representing the friends of the person. Your task is to: 1. Create a custom `JSONEncoder` to serialize `Person` objects to JSON format. 2. Create a custom `JSONDecoder` to deserialize JSON format back into `Person` objects. **Requirements:** 1. The `JSONEncoder` should convert a `Person` object into a dictionary representing the person, including recursively serializing any `Person` objects in the `friends` list. 2. The `JSONDecoder` should convert a dictionary back into a `Person` object, including recursively deserializing any nested `Person` objects in the `friends` list. # Function Signatures ```python import json class Person: def __init__(self, name: str, age: int, address: dict, friends: list = None): self.name = name self.age = age self.address = address self.friends = friends if friends is not None else [] class PersonEncoder(json.JSONEncoder): def default(self, obj): # Override the default method to handle Person objects pass def person_decoder(obj): # Function to decode JSON object to Person object pass ``` # Example: Input: ```python p1 = Person(\\"Alice\\", 30, {\\"street\\": \\"123 Maple St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\"}, []) p2 = Person(\\"Bob\\", 25, {\\"street\\": \\"456 Oak St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\"}, [p1]) json_data = json.dumps(p2, cls=PersonEncoder) print(json_data) ``` Output: ```json { \\"name\\": \\"Bob\\", \\"age\\": 25, \\"address\\": { \\"street\\": \\"456 Oak St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\" }, \\"friends\\": [ { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"address\\": { \\"street\\": \\"123 Maple St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\" }, \\"friends\\": [] } ] } ``` Input: ```python json_str = \'{\\"name\\": \\"Bob\\", \\"age\\": 25, \\"address\\": {\\"street\\": \\"456 Oak St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\"}, \\"friends\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30, \\"address\\": {\\"street\\": \\"123 Maple St\\", \\"city\\": \\"Wonderland\\", \\"zip_code\\": \\"12345\\"}, \\"friends\\": []}]}\' person = json.loads(json_str, object_hook=person_decoder) print(person.name, person.age, person.address, [(friend.name, friend.age) for friend in person.friends]) ``` Output: ```python Bob 25 {\'street\': \'456 Oak St\', \'city\': \'Wonderland\', \'zip_code\': \'12345\'} [(\'Alice\', 30)] ``` # Constraints: - The `Person` object attributes and structure must be correctly handled during serialization and deserialization. - Ensure that the solution works for nested person objects in the `friends` list indefinitely. - Aim for efficiency in handling large JSON structures.","solution":"import json class Person: def __init__(self, name: str, age: int, address: dict, friends: list = None): self.name = name self.age = age self.address = address self.friends = friends if friends is not None else [] class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"address\\": obj.address, \\"friends\\": obj.friends } return super().default(obj) def person_decoder(obj): if \'name\' in obj and \'age\' in obj and \'address\' in obj: friends = obj[\'friends\'] return Person(obj[\'name\'], obj[\'age\'], obj[\'address\'], friends) return obj"},{"question":"**Objective:** Your task is to demonstrate your understanding of scikit-learn and good coding practices by identifying and reproducing a bug using a minimal, reproducible example. This includes generating a synthetic dataset and preparing code that isolates the problem. **Problem Statement:** You are tasked with identifying the cause of a user warning in the `GradientBoostingRegressor` model from scikit-learn. Specifically, the warning states: `\\"UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\\"` You need to follow these steps to create a minimal, self-contained, and reproducible example: 1. **Generate Synthetic Data:** - Create a synthetic regression dataset using NumPy containing continuous numeric features and a target variable. - Ensure the dataset includes feature names. 2. **Prepare the Reproducer:** - Write a script to load the synthetic data into a pandas DataFrame. - Split the dataset into training and testing sets. - Implement the `GradientBoostingRegressor` model, setting the `n_iter_no_change` parameter to 5 to reproduce the warning. - Ensure your code is minimal and follows good practices for readability and conciseness. **Function Signature:** ```python def reproduce_warning() -> None: pass ``` **Input:** - No input parameters are required. All data should be generated within the function. **Output:** - The function should not return anything but should print the reproduction of the `UserWarning`. **Example:** The following steps outline a general approach to solving the problem (do not use these exact steps, but craft your own minimal reproducible example): 1. Generate synthetic data. 2. Load data into a pandas DataFrame. 3. Implement the required transformations and model to demonstrate the warning. 4. Ensure the code is minimal and readable. **Note:** - Focus on clarity, simplicity, and correctness. - Ensure that your code adheres to good coding practices, such as appropriate imports, concise variable names, and no unnecessary complexity.","solution":"import numpy as np import pandas as pd from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor def reproduce_warning(): # Step 1: Generate synthetic data X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42) # Create feature names feature_names = [f\'feature_{i}\' for i in range(X.shape[1])] # Step 2: Load data into a DataFrame df = pd.DataFrame(X, columns=feature_names) df[\'target\'] = y # Split the data X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df[\'target\'], test_size=0.2, random_state=42) # Step 3: Implement the `GradientBoostingRegressor` model model = GradientBoostingRegressor(n_iter_no_change=5, random_state=42) model.fit(X_train, y_train) # Test the model (forcing the warning to appear) model.score(X_test, y_test) # Invoke the function to see the warning reproduce_warning()"},{"question":"# Problem: Rational Number Operations with `fractions.Fraction` You are required to implement a function that performs various operations on rational numbers using the `Fraction` class from the `fractions` module. You will process input strings that define operations on fractions and return the results. Function Signature ```python def process_fraction_operations(operations: list[str]) -> list[str]: pass ``` Input - `operations` (list of str): A list of strings where each string defines an operation on fractions. Each operation string consists of two fractions and an operator (either `+`, `-`, `*`, or `/`), all separated by spaces. Output - list of str: A list of strings where each string is the result of the corresponding operation in the input list, formatted as a fraction in its lowest terms. Constraints - Each fraction in the operations is represented as `a/b` or is an integer. - The denominator of any fraction will never be zero. Example ```python input_operations = [ \\"1/2 + 2/3\\", \\"3/4 - 1/8\\", \\"5/6 * 2/5\\", \\"7/8 / 3/4\\" ] output = process_fraction_operations(input_operations) print(output) ``` Expected Output: ```python [\\"7/6\\", \\"5/8\\", \\"1/3\\", \\"14/9\\"] ``` # Detailed Description 1. **Parsing Input**: Each operation string will have two fractions and an operator. You need to extract these components and convert the fractions to `Fraction` objects. 2. **Performing Operations**: Use the corresponding fraction arithmetic operation (`+`, `-`, `*`, `/`). 3. **Formatting Result**: The result of each operation should be converted back to a string in the form `numerator/denominator` in its lowest terms. Implementation Notes 1. Use the `Fraction` class from the `fractions` module to represent fractions and perform arithmetic operations. 2. Handle edge cases such as operations involving whole numbers represented as fractions (e.g., `2` as `2/1`). ```python from fractions import Fraction def process_fraction_operations(operations: list[str]) -> list[str]: results = [] for operation in operations: fraction1, operator, fraction2 = operation.split() frac1 = Fraction(fraction1) frac2 = Fraction(fraction2) if operator == \'+\': result = frac1 + frac2 elif operator == \'-\': result = frac1 - frac2 elif operator == \'*\': result = frac1 * frac2 elif operator == \'/\': result = frac1 / frac2 else: raise ValueError(f\\"Unsupported operator {operator}\\") results.append(f\\"{result.numerator}/{result.denominator}\\") return results ``` # Note Make sure to thoroughly test your function with various types of inputs including: - Simple fractions - Whole numbers - Mixed operations","solution":"from fractions import Fraction def process_fraction_operations(operations: list[str]) -> list[str]: results = [] for operation in operations: fraction1, operator, fraction2 = operation.split() frac1 = Fraction(fraction1) frac2 = Fraction(fraction2) if operator == \'+\': result = frac1 + frac2 elif operator == \'-\': result = frac1 - frac2 elif operator == \'*\': result = frac1 * frac2 elif operator == \'/\': result = frac1 / frac2 else: raise ValueError(f\\"Unsupported operator {operator}\\") results.append(f\\"{result.numerator}/{result.denominator}\\") return results"},{"question":"**Objective**: To assess your understanding of the `plistlib` module, which is used for parsing and generating Apple\'s \\"property list\\" (\\".plist\\") files. Task: You are provided with an XML-formatted `.plist` file containing a list of students and their respective information. You need to read this file, manipulate the data by adding a new student to the list, and write the updated data back to the `.plist` file. Additionally, you must ensure that all dictionaries (including nested dictionaries) are written with their keys sorted. The new student\'s data should include a nested dictionary for additional information. Requirements: 1. **Read** the provided `.plist` file. 2. **Add** a new student dictionary to the existing list of students. 3. **Write** the updated list of students back to a new `.plist` file in binary format. 4. Ensure all dictionary keys are sorted in the output. Input: 1. A file path to the input XML-formatted `.plist` file. 2. A dictionary representing the new student\'s data. Output: 1. A new binary-formatted `.plist` file with the updated list of students. Constraints: - The student data must include at least: - Name (string) - Age (integer) - Grade (string) - Additional info (nested dictionary with at least two key-value pairs) Performance Requirements: - The function should handle a `.plist` file size of up to 10MB efficiently. Function Signature: ```python import plistlib from typing import Dict def update_student_plist(input_file_path: str, output_file_path: str, new_student: Dict) -> None: Reads an XML .plist file of student information, adds a new student, and writes the updated data back to a binary .plist file. :param input_file_path: Path to the input XML .plist file :param output_file_path: Path to the output binary .plist file :param new_student: Dictionary containing the new student\'s data # Your code goes here ``` Example Usage: ```python new_student_info = { \\"Name\\": \\"Jane Doe\\", \\"Age\\": 20, \\"Grade\\": \\"Junior\\", \\"AdditionalInfo\\": { \\"Hobbies\\": \\"Reading, Hiking\\", \\"GPA\\": 3.8 } } update_student_plist(\\"students.xml\\", \\"updated_students.plist\\", new_student_info) ``` **Note**: Ensure the keys in all dictionaries, including nested ones, are written in sorted order.","solution":"import plistlib from typing import Dict import collections def update_student_plist(input_file_path: str, output_file_path: str, new_student: Dict) -> None: Reads an XML .plist file of student information, adds a new student, and writes the updated data back to a binary .plist file. :param input_file_path: Path to the input XML .plist file :param output_file_path: Path to the output binary .plist file :param new_student: Dictionary containing the new student\'s data # Custom function to recursively sort dictionary keys def sort_dict(d): sorted_dict = collections.OrderedDict() for k, v in sorted(d.items()): if isinstance(v, dict): sorted_dict[k] = sort_dict(v) else: sorted_dict[k] = v return sorted_dict # Read the existing plist file with open(input_file_path, \'rb\') as fp: students_data = plistlib.load(fp) # Add new student data sorted_new_student = sort_dict(new_student) students_data.append(sorted_new_student) # Write the updated data to the output file in binary format with open(output_file_path, \'wb\') as fp: plistlib.dump(students_data, fp, fmt=plistlib.FMT_BINARY)"},{"question":"**Coding Assessment Question:** You are tasked with implementing a machine learning classification pipeline using scikit-learn. Your goal is to use the `load_wine` dataset, build a classification model, and evaluate its performance. Follow the steps below to complete the task: # Step 1: Load the Dataset Use the `load_wine` function from `sklearn.datasets` to load the Wine dataset. # Step 2: Preprocess the Data - Split the dataset into training and testing sets using a 70-30 split. # Step 3: Build the Model - Implement a classification model using any algorithm of your choice (e.g., Logistic Regression, Random Forest, etc.). # Step 4: Train the Model - Train the model using the training dataset. # Step 5: Evaluate the Model - Evaluate the model\'s performance on the testing dataset using accuracy as the performance metric. - Generate a confusion matrix to further analyze the performance. # Expected Input and Output Formats - **Input:** - None (load the dataset using `load_wine` within your function). - **Output:** - The accuracy of the model on the testing dataset (a float between 0 and 1). - A confusion matrix as a 2D list or array. # Constraints and Limitations - You must use scikit-learn for loading the dataset, building the model, and evaluating the performance. - You are free to use any preprocessing steps or parameters for your chosen classification algorithm. # Performance Requirements - Your solution should run efficiently on a standard personal computer. # Example ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix def wine_classification(): # Step 1: Load the dataset data = load_wine() X, y = data.data, data.target # Step 2: Preprocess the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Build the model model = RandomForestClassifier(random_state=42) # Step 4: Train the model model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix # Example usage: accuracy, conf_matrix = wine_classification() print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:\\", conf_matrix) ``` Implement the function `wine_classification` as described above to complete the task.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix def wine_classification(): Load the wine dataset, build a classification model, and evaluate its performance. Returns: tuple: A tuple containing the accuracy of the model on the testing dataset (float) and the confusion matrix (2D list). # Step 1: Load the dataset data = load_wine() X, y = data.data, data.target # Step 2: Preprocess the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Build the model model = RandomForestClassifier(random_state=42) # Step 4: Train the model model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"# Question: Implementing a Module Loader Using Deprecated `imp` and Modern `importlib` Background: Python\'s import mechanism is vital for loading modules and packages dynamically. While the `imp` module provided the primitive functions to handle these operations, it has been deprecated in favor of `importlib` for modern Python versions. Task: You need to write a function to load and reload a module by name using both the deprecated `imp` module and the modern `importlib` module. Implement two functions `load_module_imp` and `reload_module_importlib` as described below: 1. **Function `load_module_imp(name: str) -> Any`:** - **Input:** - `name`: A string representing the module name to be loaded. - **Output:** - Returns the module object if loaded successfully. - **Implementation Details:** - Using the deprecated `imp` module, find and load the module. - If the module cannot be found or loaded, raise an `ImportError`. 2. **Function `reload_module_importlib(module: Any) -> Any`:** - **Input:** - `module`: The module object to be reloaded. - **Output:** - Returns the reloaded module object. - **Implementation Details:** - Using the `importlib` module, reload the provided module. - If the module cannot be reloaded, raise an `ImportError`. Constraints: - Both functions should handle exceptions gracefully and raise `ImportError` for any issues encountered. - Your code should ensure threadsafe imports using appropriate locking mechanisms where necessary. Example: ```python try: my_module = load_module_imp(\\"my_module_name\\") reloaded_module = reload_module_importlib(my_module) except ImportError as e: print(f\\"Error loading module: {e}\\") ``` Notes: - Consider the performance and reliability factors for thread safety when using these mechanisms. - Use the provided documentation to refer to the deprecated and modern methodologies for module handling. Happy coding!","solution":"import imp import importlib def load_module_imp(name: str): Load a module by name using the deprecated \'imp\' module. Args: name (str): The name of the module to load. Returns: Any: The loaded module object. Raises: ImportError: If the module cannot be found or loaded. try: fp, pathname, description = imp.find_module(name) try: module = imp.load_module(name, fp, pathname, description) finally: if fp: fp.close() return module except Exception as e: raise ImportError(f\\"Error loading module {name}: {e}\\") def reload_module_importlib(module): Reload a module using the \'importlib\' module. Args: module (Any): The module object to reload. Returns: Any: The reloaded module object. Raises: ImportError: If the module cannot be reloaded. try: reloaded_module = importlib.reload(module) return reloaded_module except Exception as e: raise ImportError(f\\"Error reloading module: {e}\\")"},{"question":"Objective: You are tasked with creating a function that computes the number of ways to write a given integer `n` as a sum of two or more positive integers. This is a classic problem in combinatorial mathematics often referred to as \\"integer partitions\\" or \\"partition function\\". Your implementation should use memoization to improve performance. Requirements: 1. Implement a function `partition_count(n: int) -> int` that computes the number of ways to partition the integer `n`. 2. Use the `@lru_cache(maxsize=None)` decorator from the `functools` module to memoize your recursive function calls. Input: - A single integer `n` where `1 <= n <= 100`. Output: - An integer representing the number of ways to partition `n`. Constraints: - Use memoization to ensure your solution is efficient. - You must use Python\'s `functools.lru_cache`. Example: ```python @lru_cache(maxsize=None) def partition_count(n: int) -> int: # Your implementation here # Test cases print(partition_count(5)) # Output: 6 print(partition_count(10)) # Output: 42 print(partition_count(15)) # Output: 176 ``` Explanation: - `partition_count(5)` returns 6 because there are six ways to partition 5: (4+1), (3+2), (3+1+1), (2+2+1), (2+1+1+1), and (1+1+1+1+1). - `partition_count(10)` returns 42, representing the number of ways to partition 10. - Your function should ensure that results are computed efficiently, leveraging the `@lru_cache` decorator to store previously computed results.","solution":"from functools import lru_cache @lru_cache(maxsize=None) def partition_count(n: int) -> int: Computes the number of ways to partition the integer n using memoization to optimize redundant calculations. # Base case if n == 0: return 1 count = 0 for i in range(1, n + 1): count += partition_count(n - i) return count # Improve the computation logic by using a correct combinatorial approach def partition_count(n: int) -> int: @lru_cache(maxsize=None) def partitions(k, max_val): if k == 0: return 1 if k < 0 or max_val == 0: return 0 return partitions(k, max_val - 1) + partitions(k - max_val, max_val) return partitions(n, n) - 1 # Subtracting 1 to exclude the partition where n is a single part"},{"question":"<|Analysis Begin|> Based on the provided documentation snippet, the primary focus is on the `sns.hls_palette()` function from the seaborn library. This function allows users to create color palettes with parameters to adjust the number of colors, lightness, saturation, and hue. The documentation details how to use this function to customize color palettes by setting various parameters such as `n_colors` (number of colors), `l` (lightness), `s` (saturation), `h` (hue), and `as_cmap` (to return a continuous colormap). Key aspects: 1. Importing seaborn and setting a theme. 2. Adjusting the number of colors in a palette. 3. Modifying lightness and saturation. 4. Changing the starting hue. 5. Returning a continuous colormap. Given the information, we can formulate a challenging coding question that requires students to demonstrate their understanding of seaborn’s color palette capabilities, focusing on advanced customization options. <|Analysis End|> <|Question Begin|> # Seaborn Color Palette Customization Task **Objective**: Write a Python function using seaborn to create and visualize customized HLS color palettes. The function should take in parameters to adjust the number of colors, lightness, saturation, hue start-point, and return type (discrete or continuous colormap). **Function Signature**: ```python def visualize_hls_palette(n_colors: int, lightness: float, saturation: float, hue: float, as_cmap: bool) -> None: pass ``` **Parameters**: - `n_colors` (int): The number of colors in the palette. Must be a positive integer. - `lightness` (float): The lightness of the colors. Must be a float between 0 and 1. - `saturation` (float): The saturation of the colors. Must be a float between 0 and 1. - `hue` (float): The starting hue for color sampling. Must be a float between 0 and 1. - `as_cmap` (bool): If `True`, return a continuous colormap instead of a list of colors. **Output**: - The function should not return anything. It should display the color palette or colormap. **Requirements**: 1. Validate input parameters to ensure they are within the specified ranges. 2. Use seaborn\'s `sns.hls_palette` to create the palette. 3. Visualize the palette using an appropriate seaborn or matplotlib function. 4. Include docstrings and comments to explain the code. **Example Usage**: ```python # Visualize a palette with 8 colors, lightness of 0.5, saturation of 0.7, starting hue of 0.3, as a discrete colormap. visualize_hls_palette(n_colors=8, lightness=0.5, saturation=0.7, hue=0.3, as_cmap=False) # Visualize a continuous colormap with the same parameters. visualize_hls_palette(n_colors=8, lightness=0.5, saturation=0.7, hue=0.3, as_cmap=True) ``` **Constraints**: - Ensure the function handles invalid input gracefully by raising appropriate errors. - Use informative plot titles and labels where applicable. # Hints: - Refer to seaborn’s `sns.hls_palette` documentation for more details on parameter usage. - Consider using seaborn or matplotlib functions such as `sns.color_palette()`, `sns.palplot()`, or `plt.imshow()` to visualize the palettes. Good luck, and have fun with your beautiful custom color palettes!","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_hls_palette(n_colors: int, lightness: float, saturation: float, hue: float, as_cmap: bool) -> None: Visualize a customized HLS color palette using seaborn. Parameters: - n_colors (int): Number of colors in the palette. Must be a positive integer. - lightness (float): Lightness of the colors. Must be a float between 0 and 1. - saturation (float): Saturation of the colors. Must be a float between 0 and 1. - hue (float): Starting hue for color sampling. Must be a float between 0 and 1. - as_cmap (bool): If True, return a continuous colormap instead of a list of colors. Returns: - None. Displays the color palette or colormap. # Validate input parameters if not (isinstance(n_colors, int) and n_colors > 0): raise ValueError(\\"n_colors must be a positive integer.\\") if not (isinstance(lightness, (int, float)) and 0 <= lightness <= 1): raise ValueError(\\"lightness must be a float between 0 and 1.\\") if not (isinstance(saturation, (int, float)) and 0 <= saturation <= 1): raise ValueError(\\"saturation must be a float between 0 and 1.\\") if not (isinstance(hue, (int, float)) and 0 <= hue <= 1): raise ValueError(\\"hue must be a float between 0 and 1.\\") if not isinstance(as_cmap, bool): raise ValueError(\\"as_cmap must be a boolean value.\\") # Generate the HLS color palette or colormap palette = sns.hls_palette(n_colors, l=lightness, s=saturation, h=hue, as_cmap=as_cmap) # Visualize the palette or colormap if as_cmap: plt.figure(figsize=(6, 1)) plt.imshow([list(range(n_colors))], cmap=palette, aspect=\'auto\') plt.axis(\'off\') plt.title(\'Continuous HLS Colormap\') else: sns.palplot(palette) plt.title(f\'Discrete HLS Color Palette with {n_colors} colors\') plt.show()"},{"question":"# IMAP Email Fetching and Processing Objective Implement a Python function using the `imaplib` module to: 1. Establish a connection to an IMAP server. 2. Log in to the server using provided credentials. 3. Select the \'INBOX\' mailbox. 4. Fetch and return the subject lines of all unread emails. 5. Log out from the server. Function Signature ```python def fetch_unread_email_subjects(server: str, port: int, username: str, password: str) -> list: pass ``` Input - `server`: A string representing the IMAP server address. - `port`: An integer representing the port number for the IMAP server. - `username`: A string representing the username to log in to the IMAP server. - `password`: A string representing the password to log in to the IMAP server. Output - A list of strings, where each string is the subject line of an unread email. Constraints - Assume that the `server`, `port`, `username`, and `password` inputs are always valid and the server is reachable. - Do not use any additional libraries for email processing other than `imaplib`. Example ```python # Example call to the function subjects = fetch_unread_email_subjects(\\"imap.example.com\\", 143, \\"user@example.com\\", \\"password123\\") # Example output print(subjects) # Output might be: # [\'Subject 1\', \'Subject 2\', \'Subject 3\'] ``` Notes - Your function should handle any exceptions that may occur during the connection or email fetching process and should return an empty list in such cases. - Use the methods from the `IMAP4` class as described in the documentation to implement the required functionality. - Make sure to properly log out from the server after the operation is complete to avoid leaving open connections.","solution":"import imaplib import email from email.header import decode_header def fetch_unread_email_subjects(server: str, port: int, username: str, password: str) -> list: try: # Connect to the server mail = imaplib.IMAP4(server, port) # Login to the account mail.login(username, password) # Select the \'INBOX\' mailbox mail.select(\\"INBOX\\") # Search for all unread emails status, messages = mail.search(None, \'UNSEEN\') if status != \\"OK\\": return [] # List to store subject lines subjects = [] # Fetch the emails for num in messages[0].split(): status, msg_data = mail.fetch(num, \'(RFC822)\') if status != \\"OK\\": continue for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode email subject subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): # If it\'s a bytes object, decode to str subject = subject.decode(encoding) if encoding else subject.decode(\\"utf-8\\") subjects.append(subject) # Logout from the server mail.logout() return subjects except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Python Coding Assessment Question Objective You are given a module `keyword` that can be used to determine if a string is a keyword or a soft keyword in Python. Your task is to write functions using the functionalities provided by the `keyword` module to identify and process keywords and soft keywords in a given text. Requirements 1. **Function 1: `extract_keywords(text)`** - **Input**: A string `text` containing multiple words separated by spaces or newline characters. - **Output**: A list of unique Python keywords present in the `text`. ```python def extract_keywords(text: str) -> list: pass ``` - **Constraints**: - The text can contain multiple spaces, new lines, and punctuation. - The function should ignore punctuation and consider only alphanumeric words. - The list should not contain duplicates and the keywords should be in the order they are first encountered in the text. 2. **Function 2: `extract_soft_keywords(text)`** - **Input**: A string `text` containing multiple words separated by spaces or newline characters. - **Output**: A list of unique Python soft keywords present in the `text`. ```python def extract_soft_keywords(text: str) -> list: pass ``` - **Constraints**: - The text can contain multiple spaces, new lines, and punctuation. - The function should ignore punctuation and consider only alphanumeric words. - The list should not contain duplicates and the soft keywords should be in the order they are first encountered in the text. Example ```python text = class Test: def func(): if True: pass with open(\'file.txt\'): async for line in aiter(file): match result: case 200: print(\'Success\') case _: print(\'Failure\') extract_keywords(text) # Output: [\'class\', \'def\', \'if\', \'True\', \'pass\', \'with\', \'async\', \'for\', \'in\'] extract_soft_keywords(text) # Output: [\'match\', \'case\', \'_\'] ``` Notes - Use the `keyword.iskeyword` and `keyword.issoftkeyword` functions to determine if a word is a keyword or soft keyword. - Consider using regular expressions to preprocess the text and extract words effectively while ignoring punctuation.","solution":"import keyword import re def extract_keywords(text: str) -> list: Extract unique Python keywords from the given text. words = re.findall(r\'bw+b\', text) seen = set() keywords = [] for word in words: if word not in seen and keyword.iskeyword(word): seen.add(word) keywords.append(word) return keywords def extract_soft_keywords(text: str) -> list: Extract unique Python soft keywords from the given text. words = re.findall(r\'bw+b\', text) seen = set() soft_keywords = [] for word in words: if word not in seen and keyword.issoftkeyword(word): seen.add(word) soft_keywords.append(word) return soft_keywords"},{"question":"You are given a binary audio file (in a specific byte format) that captures a sequence of audio samples. Your task is to write a series of functions using the `audioop` module in Python to perform the following operations on the audio data: 1. **Extract Information**: Implement a function `extract_audio_info(fragment: bytes, width: int) -> dict` that: - Computes and returns the following information about the provided audio fragment: - Average value of the samples. - Maximum value of the samples. - RMS (Root Mean Square) of the samples. - Minimum and maximum sample values. - Return this information in the form of a dictionary. 2. **Convert Encoding**: Implement a function `convert_encoding(fragment: bytes, original_width: int, target_format: str) -> bytes` that: - Converts the given audio fragment from its original encoding to one of the target formats specified by `target_format`, which can be \'lin\', \'alaw\', or \'ulaw\'. - If the `target_format` is \'lin\', you should additional parameter `target_width` to specify the width of the target linear encoding. 3. **Stereo to Mono Conversion**: Implement a function `stereo_to_mono(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes` that: - Converts a stereo audio fragment into a mono fragment. - The left and right channels of the stereo fragment should be weighted by `lfactor` and `rfactor` respectively before being summed to produce the mono fragment. 4. **Find Max Energy Slice**: Implement a function `find_max_energy_slice(fragment: bytes, width: int, slice_length: int) -> int` that: - Identifies and returns the starting index of the slice with the maximum energy in the audio fragment. - The length of the slice is specified by `slice_length`. You are given the following constraints: - The input audio fragments are valid and non-empty. - Audio fragments are provided as bytes-like objects. - Supported sample widths are 1, 2, 3, and 4 bytes. - Ensure the performance is optimized for processing long audio fragments. Provide the implementation for the functions specified, adhering strictly to the provided requirements. Example Usage ```python audio_data = b\'x01x02x03x04...\' # some binary audio data fragment width = 2 # sample width in bytes lfactor = 0.5 rfactor = 0.5 slice_length = 1000 # in samples # Example function calls info = extract_audio_info(audio_data, width) # info -> {\'average\': <some_value>, \'max\': <some_value>, \'rms\': <some_value>, \'minmax\': (<min_value>, <max_value>)} encoded_fragment = convert_encoding(audio_data, width, \'alaw\') # encoded_fragment should be the fragment converted to a-law encoding mono_fragment = stereo_to_mono(audio_data, width, lfactor, rfactor) # mono_fragment should be the fragment converted to mono max_energy_index = find_max_energy_slice(audio_data, width, slice_length) # max_energy_index should be the starting index of the slice with maximum energy ``` Implementing these functions will demonstrate your ability to read, manipulate, and analyze raw audio data using the `audioop` module. Remember to handle errors and edge cases appropriately.","solution":"import audioop def extract_audio_info(fragment: bytes, width: int) -> dict: Extract information from the audio fragment such as average, max value, RMS, min and max sample values. Args: - fragment: bytes, the audio data fragment. - width: int, the width of each sample in bytes (1, 2, 3 or 4). Returns: - dict: Dictionary containing average value, max value, RMS, and min/max values. avg = audioop.avg(fragment, width) max_val = audioop.max(fragment, width) rms = audioop.rms(fragment, width) minmax = audioop.minmax(fragment, width) return { \'average\': avg, \'max\': max_val, \'rms\': rms, \'minmax\': minmax } def convert_encoding(fragment: bytes, original_width: int, target_format: str, target_width: int = 2) -> bytes: Convert audio fragment to specified target format (either \'lin\', \'alaw\', or \'ulaw\'). Args: - fragment: bytes, the audio data fragment. - original_width: int, the width of the original samples in bytes (1, 2, 3 or 4). - target_format: str, the target format (\'lin\', \'alaw\', \'ulaw\'). - target_width: int, the width of the target linear encoding if \'lin\' is specified (default is 2 bytes). Returns: - bytes: The converted audio data fragment. if target_format == \'lin\': return audioop.lin2lin(fragment, original_width, target_width) elif target_format == \'alaw\': return audioop.lin2alaw(fragment, original_width) elif target_format == \'ulaw\': return audioop.lin2ulaw(fragment, original_width) else: raise ValueError(\\"Unsupported target format: must be \'lin\', \'alaw\', or \'ulaw\'\\") def stereo_to_mono(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: Convert stereo audio fragment to mono by weighting left and right channels. Args: - fragment: bytes, the audio data fragment. - width: int, the width of each sample in bytes (1, 2, 3 or 4). - lfactor: float, weight for the left channel. - rfactor: float, weight for the right channel. Returns: - bytes: The mono audio data fragment. return audioop.tomono(fragment, width, lfactor, rfactor) def find_max_energy_slice(fragment: bytes, width: int, slice_length: int) -> int: Find the starting index of the slice of given length with maximum energy. Args: - fragment: bytes, the audio data fragment. - width: int, the width of each sample in bytes (1, 2, 3 or 4). - slice_length: int, the length of the slice in samples. Returns: - int: The starting index of the slice with the maximum energy. max_energy = 0 max_energy_start = 0 for i in range(0, len(fragment) - slice_length * width + 1, width): slice_fragment = fragment[i:i + slice_length * width] energy = audioop.rms(slice_fragment, width) if energy > max_energy: max_energy = energy max_energy_start = i return max_energy_start"},{"question":"Title: Implement a Proxy HTTP Server Objective: Design and implement a Proxy HTTP server using Python\'s `http.server` and `urllib.request` modules. The proxy server should be able to handle GET requests from clients, forward these requests to the appropriate server, fetch the responses, and return the responses back to the clients. Description: Your task is to create a simple Proxy HTTP Server in Python. The server should: 1. Listen for incoming HTTP GET requests from clients. 2. Parse these requests to extract the target URL. 3. Forward the request to the target server using functions from the `urllib.request` module. 4. Fetch the response and return it to the client. Requirements: 1. You must use Python\'s `http.server` module to create the HTTP server. 2. Use `urllib.request` to forward requests and fetch responses. 3. The server should be able to handle multiple requests sequentially. 4. Properly handle HTTP status codes and headers while forwarding requests and responses. Input: - No direct input will be provided to your function, but it will handle HTTP GET requests from clients. Output: - The server should respond back to the client with the content fetched from the target URL. Constraints: - You must not use third-party libraries for HTTP handling or request forwarding. - The implementation should be efficient and should not crash under typical usage conditions. Example: 1. Start your proxy server. 2. Client makes a GET request to your proxy server with URL `http://example.com`. 3. Your server forwards the request to `http://example.com`, fetches the response. 4. Your server sends the fetched response back to the client. ```python import http.server import urllib.request import urllib.parse class ProxyHTTPRequestHandler(http.server.BaseHTTPRequestHandler): def do_GET(self): # Parse the URL from the request target_url = self.path[1:] # Removing the leading \'/\' # Forward the request to the target server try: with urllib.request.urlopen(target_url) as response: content = response.read() # Send response status and headers to client self.send_response(response.status) for header in response.getheaders(): self.send_header(header[0], header[1]) self.end_headers() # Send the content to the client self.wfile.write(content) except Exception as e: self.send_error(500, str(e)) def run(server_class=http.server.HTTPServer, handler_class=ProxyHTTPRequestHandler, port=8080): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Starting proxy server on port {port}...\') httpd.serve_forever() if __name__ == \'__main__\': run() ``` Note: - You should test the server implementation using a web browser or HTTP client tool like `curl` or `Postman`. - Ensure your server handles edge cases, such as invalid URLs, network errors, and large response sizes gracefully.","solution":"import http.server import urllib.request import urllib.parse class ProxyHTTPRequestHandler(http.server.BaseHTTPRequestHandler): def do_GET(self): # Parse the URL from the request parsed_url = urllib.parse.urlparse(self.path) target_url = parsed_url.path[1:] # Removing the leading \'/\' if not target_url: self.send_error(400, \\"Bad Request: No URL specified\\") return # Forward the request to the target server try: with urllib.request.urlopen(target_url) as response: content = response.read() # Send response status and headers to client self.send_response(response.getcode()) for header in response.getheaders(): self.send_header(header[0], header[1]) self.end_headers() # Send the content to the client self.wfile.write(content) except urllib.error.HTTPError as e: self.send_error(e.code, e.reason) except Exception as e: self.send_error(500, str(e)) def run(server_class=http.server.HTTPServer, handler_class=ProxyHTTPRequestHandler, port=8080): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Starting proxy server on port {port}...\') httpd.serve_forever() if __name__ == \'__main__\': run()"},{"question":"Objective: To assess your understanding of the `sklearn.neighbors` module, including nearest neighbors search algorithms and their application in supervised learning for classification. Task: You are provided with a dataset of points in two-dimensional space. Your task is to: 1. Implement a nearest neighbors search utilizing different algorithms (`\'ball_tree\'`, `\'kd_tree\'`, and `\'brute\'`). 2. Apply the `KNeighborsClassifier` to classify points based on their nearest neighbors. 3. Evaluate the performance of the classifier using different neighbor search algorithms and report their accuracy. Details: 1. Implement a function `get_nearest_neighbors(X_train, X_test, n_neighbors, algorithm)` that finds the nearest neighbors for the test data points using the specified algorithm. 2. Implement a function `evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors)` that: - Trains a `KNeighborsClassifier` using each of the three algorithms (`\'ball_tree\'`, `\'kd_tree\'`, and `\'brute\'`). - Predicts the labels for test data. - Returns the accuracy of each classifier. Expected Input and Output Formats: - `get_nearest_neighbors(X_train, X_test, n_neighbors, algorithm)`: - **Input**: - `X_train`: ndarray of shape (n_train_samples, 2), the training data. - `X_test`: ndarray of shape (n_test_samples, 2), the test data. - `n_neighbors`: int, the number of neighbors to use. - `algorithm`: str, the algorithm to use (`\'ball_tree\'`, `\'kd_tree\'`, or `\'brute\'`). - **Output**: - `distances`: ndarray of shape (n_test_samples, n_neighbors), distances to the nearest neighbors. - `indices`: ndarray of shape (n_test_samples, n_neighbors), indices of the nearest neighbors in the training data. - `evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors)`: - **Input**: - `X_train`: ndarray of shape (n_train_samples, 2), the training data. - `y_train`: ndarray of shape (n_train_samples,), the training labels. - `X_test`: ndarray of shape (n_test_samples, 2), the test data. - `y_test`: ndarray of shape (n_test_samples,), the test labels. - `n_neighbors`: int, the number of neighbors to use. - **Output**: - `results`: dict, where keys are the algorithm names (`\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`) and values are accuracy scores. Constraints: - The dataset will contain a maximum of 1000 points. - The number of neighbors (`n_neighbors`) will be less than the number of training samples. Performance Requirements: - Ensure that the code is efficient and leverages scikit-learn\'s implementation for nearest neighbors search and classification. ```python from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier import numpy as np def get_nearest_neighbors(X_train, X_test, n_neighbors, algorithm): # Implement nearest neighbors search using the specified algorithm nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm).fit(X_train) distances, indices = nbrs.kneighbors(X_test) return distances, indices def evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors): results = {} algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] for algorithm in algorithms: # Train KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) knn.fit(X_train, y_train) # Predict and calculate accuracy accuracy = knn.score(X_test, y_test) results[algorithm] = accuracy return results # Example usage: if __name__ == \\"__main__\\": # Example dataset X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train = np.array([1, 1, 1, 2, 2, 2]) X_test = np.array([[-0.8, -1], [2.5, 1.5]]) y_test = np.array([1, 2]) n_neighbors = 2 # Nearest neighbors search distances, indices = get_nearest_neighbors(X_train, X_test, n_neighbors, \'ball_tree\') print(\\"Distances:n\\", distances) print(\\"Indices:n\\", indices) # Evaluating KNeighborsClassifier results = evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors) print(\\"Results:n\\", results) ```","solution":"from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier import numpy as np def get_nearest_neighbors(X_train, X_test, n_neighbors, algorithm): Finds the nearest neighbors for the test data points using the specified algorithm. Parameters: X_train (ndarray): Training data of shape (n_train_samples, 2). X_test (ndarray): Test data of shape (n_test_samples, 2). n_neighbors (int): Number of neighbors to use. algorithm (str): Algorithm to use (\'ball_tree\', \'kd_tree\', \'brute\'). Returns: distances (ndarray): Distances to the nearest neighbors, shape (n_test_samples, n_neighbors). indices (ndarray): Indices of the nearest neighbors in the training data, shape (n_test_samples, n_neighbors). nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm).fit(X_train) distances, indices = nbrs.kneighbors(X_test) return distances, indices def evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors): Trains a KNeighborsClassifier using each of the three algorithms and evaluates their accuracy. Parameters: X_train (ndarray): Training data of shape (n_train_samples, 2). y_train (ndarray): Training labels of shape (n_train_samples,). X_test (ndarray): Test data of shape (n_test_samples, 2). y_test (ndarray): Test labels of shape (n_test_samples,). n_neighbors (int): Number of neighbors to use. Returns: results (dict): Dictionary with algorithm names as keys and accuracy scores as values. results = {} algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] for algorithm in algorithms: knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) knn.fit(X_train, y_train) accuracy = knn.score(X_test, y_test) results[algorithm] = accuracy return results # Example usage: if __name__ == \\"__main__\\": X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train = np.array([1, 1, 1, 2, 2, 2]) X_test = np.array([[-0.8, -1], [2.5, 1.5]]) y_test = np.array([1, 2]) n_neighbors = 2 distances, indices = get_nearest_neighbors(X_train, X_test, n_neighbors, \'ball_tree\') print(\\"Distances:n\\", distances) print(\\"Indices:n\\", indices) results = evaluate_knn_classifiers(X_train, y_train, X_test, y_test, n_neighbors) print(\\"Results:n\\", results)"},{"question":"# Persistent Key-Value Store Task Objective: Implement a persistent key-value store using Python\'s `shelve` module. You need to create multiple functions that interact with this store to perform various operations. Instructions: 1. **Create a function `create_store(filename: str) -> None`**: - This function should initialize a new shelve file with the given filename. - Ensure the file is properly closed after creation. 2. **Create a function `store_value(filename: str, key: str, value: Any) -> None`**: - This function should open the shelve file, store the given value under the specified key, and then close the shelve file. 3. **Create a function `retrieve_value(filename: str, key: str) -> Any`**: - This function should retrieve the value associated with the given key from the shelve file. - If the key does not exist, raise a `KeyError`. 4. **Create a function `delete_value(filename: str, key: str) -> None`**: - This function should delete the value associated with the given key from the shelve file. - If the key does not exist, raise a `KeyError`. 5. **Create a function `list_keys(filename: str) -> List[str]`**: - This function should return a list of all keys currently stored in the shelve file. 6. **Create a function `append_to_list(filename: str, key: str, item: Any) -> None`**: - This function should append an item to a list stored at the given key in the shelve file. - If the key does not exist or doesn\'t contain a list, raise a `KeyError`. - Ensure the changes persist after closing the shelve file. Example: ```python create_store(\'my_store\') store_value(\'my_store\', \'numbers\', [1, 2, 3]) store_value(\'my_store\', \'greeting\', \'hello\') print(retrieve_value(\'my_store\', \'numbers\')) # Output: [1, 2, 3] append_to_list(\'my_store\', \'numbers\', 4) print(retrieve_value(\'my_store\', \'numbers\')) # Output: [1, 2, 3, 4] print(list_keys(\'my_store\')) # Output: [\'numbers\', \'greeting\'] delete_value(\'my_store\', \'greeting\') print(list_keys(\'my_store\')) # Output: [\'numbers\'] ``` Constraints: - You can assume the functions will not be called concurrently. - Handle any necessary exceptions, primarily `KeyError`. Performance: - The operations should be efficient, leveraging the persistence nature of `shelve`.","solution":"import shelve def create_store(filename: str) -> None: Initializes a new shelve file with the given filename. with shelve.open(filename) as store: pass def store_value(filename: str, key: str, value: any) -> None: Stores the given value under the specified key in the shelve file. with shelve.open(filename) as store: store[key] = value def retrieve_value(filename: str, key: str) -> any: Retrieves the value associated with the given key from the shelve file. If the key does not exist, raises a KeyError. with shelve.open(filename) as store: if key in store: return store[key] else: raise KeyError(f\\"Key \'{key}\' not found in the store.\\") def delete_value(filename: str, key: str) -> None: Deletes the value associated with the given key from the shelve file. If the key does not exist, raises a KeyError. with shelve.open(filename) as store: if key in store: del store[key] else: raise KeyError(f\\"Key \'{key}\' not found in the store.\\") def list_keys(filename: str) -> list: Returns a list of all keys currently stored in the shelve file. with shelve.open(filename) as store: return list(store.keys()) def append_to_list(filename: str, key: str, item: any) -> None: Appends an item to a list stored at the given key in the shelve file. If the key does not exist or doesn\'t contain a list, raises a KeyError. with shelve.open(filename) as store: if key in store: if isinstance(store[key], list): temp_list = store[key] temp_list.append(item) store[key] = temp_list else: raise KeyError(f\\"Value under key \'{key}\' is not a list.\\") else: raise KeyError(f\\"Key \'{key}\' not found in the store.\\")"},{"question":"You are tasked with creating a utility function to summarize the contents of a specified NIS map into a readable report. This function will utilize the `nis` module\'s `cat` function to retrieve the key-value pairs and will handle exceptions gracefully. Task Description Write a function `summarize_nis_map(mapname: str, domain: Optional[str] = None) -> str` that: 1. Retrieves all key-value pairs from the specified NIS map using `nis.cat`. 2. Constructs a summary report as a string. 3. Handles `nis.error` gracefully by returning an appropriate error message. Detailed Requirements 1. **Input**: - `mapname` (str): The name of the NIS map to retrieve. - `domain` (Optional[str]): The NIS domain to use for the lookup (default is the system default). 2. **Output**: - A string that contains a formatted summary report. Each key-value pair should be on a new line in the format `key: value`. If an error occurs, the string should be an error message such as \\"Error: [Error message]\\". 3. **Constraints**: - The function should handle arbitrarily large and complex key-value pairs. - Ensure the function can handle binary data by converting keys and values to their hexadecimal representation. 4. **Performance Considerations**: - The function should operate efficiently even for larger NIS maps. Example Usage ```python try: report = summarize_nis_map(\'passwd.byname\') print(report) except Exception as e: print(f\\"Unexpected error: {str(e)}\\") ``` Example Output ``` 00112233: 44556677 8899aabb: ccddeeff ``` Use the following template to implement the function: ```python import nis def summarize_nis_map(mapname: str, domain: Optional[str] = None) -> str: try: # Retrieve the dictionary containing all key-value pairs. if domain: kv_pairs = nis.cat(mapname, domain=domain) else: kv_pairs = nis.cat(mapname) # Construct the summary report report_lines = [f\\"{key.hex()}: {value.hex()}\\" for key, value in kv_pairs.items()] return \\"n\\".join(report_lines) except nis.error as e: return f\\"Error: {str(e)}\\" ``` Notes - `key.hex()` and `value.hex()` are used to convert binary data to its hexadecimal string representation. - Make sure that your code handles the domain parameter as optional.","solution":"import nis from typing import Optional def summarize_nis_map(mapname: str, domain: Optional[str] = None) -> str: try: # Retrieve the dictionary containing all key-value pairs. if domain: kv_pairs = nis.cat(mapname, domain=domain) else: kv_pairs = nis.cat(mapname) # Construct the summary report report_lines = [f\\"{key.hex()}: {value.hex()}\\" for key, value in kv_pairs.items()] return \\"n\\".join(report_lines) except nis.error as e: return f\\"Error: {str(e)}\\""},{"question":"# Question: Creating a Customized Seaborn Plot with Ranges and Jitter You are provided with the `penguins` dataset from the Seaborn library. Your task is to visualize the `bill_length_mm` for different species of penguins using Seaborn\'s advanced plotting functionalities. The resulting plot should include jittered data points and display interquartile ranges for each species. Instructions: 1. Load the `penguins` dataset using Seaborn. 2. Create a plot using `seaborn.objects.Plot` to visualize `bill_length_mm` for each `species` in the dataset. 3. Customize the plot to add dots representing individual data points, with jitter to avoid overlapping. 4. Include ranges that display the interquartile ranges (25th to 75th percentiles) for `bill_length_mm` within each species category. 5. Ensure the ranges are shifted slightly on the x-axis to avoid overlap with the jittered points. Here is an outline of the expected steps to implement the solution: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Step 2-5: Create a customized plot ( so.Plot(penguins, x=\\"species\\", y=\\"bill_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) ``` Additional Details: - **Input**: You do not need to handle any external input as the dataset is loaded within the function. - **Output**: The function should display the plot directly. - **Constraints**: Focus on using the methods and classes provided in `seaborn.objects` module. Do not use other Seaborn plotting functions like `sns.scatterplot`, etc. - **Performance Requirements**: Given the expected input size, performance considerations are minimal, but the implementation should be clear and concise. Provide a complete implementation based on the outlined steps above.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_customized_seaborn_plot(): # Step 1: Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Step 2-5: Create a customized plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"bill_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the plot plot.show()"},{"question":"You are required to implement a function that performs several operations on sets based on input commands. Function Signature ```python def process_set_operations(commands: list) -> list: pass ``` Input - `commands`: A list of strings where each string is a command related to set operations in Python. The possible commands are: - `\\"add x\\"`: Adds the integer `x` to the set. - `\\"discard x\\"`: Removes the integer `x` from the set without error if `x` is not present. - `\\"contains x\\"`: Checks if the integer `x` is in the set and returns `True` or `False`. - `\\"size\\"`: Returns the size of the set. - `\\"pop\\"`: Removes and returns an arbitrary element from the set. If the set is empty, returns `\\"Error: Empty set\\"`. - `\\"clear\\"`: Empties the set. Output - A list containing the results of the commands `\\"contains x\\"`, `\\"size\\"`, and `\\"pop\\"`. For other commands, append `None` to the results list. Constraints - The integer `x` in add/discard/contains commands will be between `-10^6` and `10^6`. - The `commands` list will have at most `10^4` elements. # Example ```python commands = [ \\"add 10\\", \\"add 20\\", \\"contains 10\\", \\"contains 15\\", \\"size\\", \\"discard 10\\", \\"contains 10\\", \\"pop\\", \\"size\\", \\"clear\\", \\"size\\", \\"pop\\" ] print(process_set_operations(commands)) ``` Expected Output ```python [None, None, True, False, 2, None, False, 20, 0, None, 0, \'Error: Empty set\'] ``` Explanation Here’s a detailed breakdown of each command and its result: 1. `add 10` - Adds 10 to the set, result is `None`. 2. `add 20` - Adds 20 to the set, result is `None`. 3. `contains 10` - Checks if 10 is in the set, result is `True`. 4. `contains 15` - Checks if 15 is in the set, result is `False`. 5. `size` - Returns the size of the set, which is 2. 6. `discard 10` - Removes 10 from the set, if it exists, result is `None`. 7. `contains 10` - Checks if 10 is in the set, result is `False`. 8. `pop` - Removes and returns an arbitrary element from the set, result is 20. 9. `size` - Returns the size of the set, which is 0 now. 10. `clear` - Empties the set, result is `None`. 11. `size` - Returns the size of the set, which is 0. 12. `pop` - Tries to remove an element from the empty set, result is `\\"Error: Empty set\\"`. # Note Carefully handle the commands to only affect the set as described. Pay attention to constraints and ensure the function performs efficiently with the given input limits.","solution":"def process_set_operations(commands: list) -> list: result = [] s = set() for command in commands: parts = command.split() action = parts[0] if action == \\"add\\": x = int(parts[1]) s.add(x) result.append(None) elif action == \\"discard\\": x = int(parts[1]) s.discard(x) result.append(None) elif action == \\"contains\\": x = int(parts[1]) result.append(x in s) elif action == \\"size\\": result.append(len(s)) elif action == \\"pop\\": if s: result.append(s.pop()) else: result.append(\\"Error: Empty set\\") elif action == \\"clear\\": s.clear() result.append(None) return result"},{"question":"# Question: Advanced Sequence and Numeric Operations in Python You are required to implement a function `process_data(sequence_data: Union[List[float], Tuple[float]], threshold: float) -> Tuple[str, List[float]]` that processes a sequence of floating-point numbers and returns a pair of results. Function Details: 1. The function takes in two parameters: - `sequence_data`: Can be a list or tuple of floating-point numbers. - `threshold`: A floating-point number that acts as a filtering criterion. 2. The function should: - Sort the `sequence_data` in descending order. - Filter out all numbers that are less than or equal to the provided `threshold`. - Convert the sorted and filtered numbers into their string representation with two decimals. - Return a tuple where: - The first element is a comma-separated string of the filtered and sorted numbers. - The second element is a list of their absolute values. Example ```python input_sequence = [3.14159, 2.71828, 1.61803, 4.6692] threshold_value = 2.0 output = process_data(input_sequence, threshold_value) print(output) # Should print (\\"4.67,3.14,2.72\\", [4.67, 3.14, 2.72]) ``` Constraints and Performance: - You may assume that the input list/tuple has at least one element. - Performance should be efficient even for large input sizes. - Implement the function below: ```python def process_data(sequence_data: Union[List[float], Tuple[float]], threshold: float) -> Tuple[str, List[float]]: # Your code here pass ``` # Notes - Utilize list comprehensions and built-in functions effectively. - Preserve immutability of input data where required. - Handle the different types for `sequence_data` (i.e., list and tuple) appropriately.","solution":"from typing import Union, List, Tuple def process_data(sequence_data: Union[List[float], Tuple[float]], threshold: float) -> Tuple[str, List[float]]: # Sort the sequence data in descending order sorted_data = sorted(sequence_data, reverse=True) # Filter out numbers less than or equal to the threshold filtered_data = [num for num in sorted_data if num > threshold] # Convert the filtered and sorted numbers to strings with two decimal places str_data = \',\'.join(f\\"{num:.2f}\\" for num in filtered_data) # Create a list of their absolute values abs_values = [abs(num) for num in filtered_data] return (str_data, abs_values)"},{"question":"**Coding Assessment Question** You are working on a project that involves analyzing a batch of sound files to determine their formats and certain metadata. You have been provided with a directory containing a mix of different types of sound files. Your task is to write a Python function that utilizes the `sndhdr` module to analyze these files and output a summary report. # Function Signature ```python def analyze_sound_files(directory: str) -> dict: pass ``` # Input - `directory` (str): The path to the directory containing sound files. # Output - `dict`: A dictionary where each key is the filename, and the value is another dictionary with the following structure: ```python { \\"filetype\\": str, \\"framerate\\": int, \\"nchannels\\": int, \\"nframes\\": int, \\"sampwidth\\": Union[int, str] } ``` If the file type cannot be determined, it should have a value of `None`. # Constraints - The function should handle any file extensions. - If a file in the directory is not a sound file or its type cannot be determined, the function should include that filename in the resulting dictionary with a value of `None`. - The `sndhdr` module is deprecated in Python 3.11, so ensure compatibility with versions before this if you plan to use the function beyond Python 3.10. # Example Suppose the directory contains three files: \'sound1.wav\', \'sound2.au\', and \'text.txt\'. The resulting dictionary might look like: ```python { \'sound1.wav\': { \\"filetype\\": \'wav\', \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 10000, \\"sampwidth\\": 16 }, \'sound2.au\': { \\"filetype\\": \'au\', \\"framerate\\": 8000, \\"nchannels\\": 1, \\"nframes\\": 5000, \\"sampwidth\\": \'U\' }, \'text.txt\': None } ``` # Instructions 1. Import the `sndhdr` module and utilize the `sndhdr.what` function to determine the file type and metadata. 2. Iterate through all files in the provided directory. 3. Return a dictionary with filenames as keys and their corresponding metadata as values (or `None` if type cannot be determined). # Notes - Ensure your function handles edge cases such as empty directories, files with no extension, and non-sound files. - You may assume that the input directory path is valid and accessible.","solution":"import os import sndhdr def analyze_sound_files(directory: str) -> dict: result = {} for filename in os.listdir(directory): file_path = os.path.join(directory, filename) if not os.path.isfile(file_path): continue file_info = sndhdr.what(file_path) if file_info is None: result[filename] = None else: filetype, framerate, nchannels, nframes, sampwidth = file_info result[filename] = { \\"filetype\\": filetype, \\"framerate\\": framerate, \\"nchannels\\": nchannels, \\"nframes\\": nframes, \\"sampwidth\\": sampwidth } return result"},{"question":"<|Analysis Begin|> The \\"syslog\\" module provides an interface to the Unix \\"syslog\\" library, which allows sending messages to the system logger. It includes functions to send log messages (`syslog()`), configure logging options (`openlog()`), reset logging configurations (`closelog()`), and set priority masks (`setlogmask()`). The module defines constants for various priority levels, facilities, and logging options. Key features include: 1. `syslog.syslog(message)` or `syslog.syslog(priority, message)`: Sends a message with an optional priority to the system logger. 2. `syslog.openlog([ident[, logoption[, facility]]])`: Configures logging options, such as message prefix (ident), logging options (logoption), and default facility. 3. `syslog.closelog()`: Resets the syslog module values and closes the log. 4. `syslog.setlogmask(maskpri)`: Sets a priority mask to filter log messages by their priority level. The constants defined in the module allow for specifying priority levels (e.g., `LOG_ERR`, `LOG_WARNING`), facilities (e.g., `LOG_MAIL`, `LOG_DAEMON`), and logging options (`LOG_PID`, `LOG_CONS`). The `syslog` module can be used to write logs with different priorities, filter logs, and direct logs to different facilities. Given this, a coding question can be designed that tests the understanding of logging with different priorities and facilities using the `syslog` module. <|Analysis End|> <|Question Begin|> **Question:** Using the `syslog` module, write a Python function `custom_syslog(message, priority_level, facility, log_mask)` that logs a given message to the Unix syslog with a specified priority level and facility. Additionally, the function should apply a log mask to filter which priority levels should be logged. The `custom_syslog` function should: 1. Log a message at the specified `priority_level` and `facility`. 2. Apply the `log_mask` so that only messages of the specified priorities are logged. 3. Handle default behavior correctly when certain parameters are not provided. 4. Optionally, include the process ID in the log message. # Function Signature ```python def custom_syslog(message: str, priority_level: int = syslog.LOG_INFO, facility: int = syslog.LOG_USER, log_mask: int = syslog.LOG_UPTO(syslog.LOG_DEBUG)) -> None: pass ``` # Input: - `message` (str): The message to be logged. - `priority_level` (int, optional): The priority level for the log message. Default is `syslog.LOG_INFO`. - `facility` (int, optional): The facility to log the message to. Default is `syslog.LOG_USER`. - `log_mask` (int, optional): The log mask to filter priority levels. Default is to log all priorities up to and including `syslog.LOG_DEBUG`. # Output: - The function does not return anything but logs the message to the Unix syslog based on the given parameters. # Constraints: - You must use the `syslog` module for logging. - Ensure that the `openlog()` and `closelog()` functions are used appropriately when setting up and completing logging. - Assume `syslog` module is appropriately set up and available in the environment where the function will run. # Example: ```python import syslog def custom_syslog(message: str, priority_level: int = syslog.LOG_INFO, facility: int = syslog.LOG_USER, log_mask: int = syslog.LOG_UPTO(syslog.LOG_DEBUG)) -> None: syslog.setlogmask(log_mask) syslog.openlog(logoption=syslog.LOG_PID, facility=facility) syslog.syslog(priority_level, message) syslog.closelog() # Example usage without any optional parameters custom_syslog(\\"This is an informational message.\\") # Example usage with all parameters specified custom_syslog(\\"This is an error message.\\", priority_level=syslog.LOG_ERR, facility=syslog.LOG_MAIL, log_mask=syslog.LOG_UPTO(syslog.LOG_ERR)) ``` Explanation: 1. `custom_syslog()` sets the log mask to filter messages based on priority. 2. Opens the syslog with options to include process ID and specified facility. 3. Logs the message with the given priority level. 4. Closes the syslog to reset configurations to default.","solution":"import syslog def custom_syslog(message: str, priority_level: int = syslog.LOG_INFO, facility: int = syslog.LOG_USER, log_mask: int = syslog.LOG_UPTO(syslog.LOG_DEBUG)) -> None: Logs a given message to the Unix syslog with a specified priority level and facility. Applies a log mask to filter which priority levels should be logged. Args: - message (str): The message to be logged. - priority_level (int, optional): The priority level for the log message. Default is syslog.LOG_INFO. - facility (int, optional): The facility to log the message to. Default is syslog.LOG_USER. - log_mask (int, optional): The log mask to filter priority levels. Default is to log all priorities up to and including syslog.LOG_DEBUG. syslog.setlogmask(log_mask) syslog.openlog(logoption=syslog.LOG_PID, facility=facility) syslog.syslog(priority_level, message) syslog.closelog()"},{"question":"# Question: Configuration File Parser You are asked to implement a parser for a \\"setup.cfg\\" configuration file used in Python\'s Distutils package setup. The parser should read the configuration file, interpret the commands and their options, and store them in a structured format. # Function Signature ```python def parse_setup_cfg(file_path: str) -> dict: Parses a setup.cfg file and returns a dictionary representation of the commands and their options. Parameters: - file_path (str): The path to the setup.cfg file. Returns: - dict: A dictionary where each key is a command and the corresponding value is another dictionary of option-value pairs. ``` # Input - `file_path`: A string representing the path to the \\"setup.cfg\\" file. # Output - A dictionary where each key is a command (e.g., `build_ext`, `bdist_rpm`) and the corresponding value is another dictionary with the options and their values. # Example Given the content in `setup.cfg`: ``` [build_ext] inplace=1 include_dirs=/usr/local/include [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Calling: ```python result = parse_setup_cfg(\\"path/to/setup.cfg\\") ``` Should produce: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"Greg Ward <gward@python.net>\\", \\"doc_files\\": \\"CHANGES.txt README.txt USAGE.txt doc/ examples/\\" } } ``` # Constraints - You may assume that the configuration file is well-formed and follows the syntax rules described. - Handle the continuation of long option values across multiple lines appropriately. - Blank lines and comments (lines starting with `#`) should be ignored. # Notes - Pay attention to the splitting of option values across multiple lines and their indentation. - Consider edge cases such as a command section with no options. Implement the `parse_setup_cfg` function to complete this task.","solution":"def parse_setup_cfg(file_path: str) -> dict: Parses a setup.cfg file and returns a dictionary representation of the commands and their options. Parameters: - file_path (str): The path to the setup.cfg file. Returns: - dict: A dictionary where each key is a command and the corresponding value is another dictionary of option-value pairs. import configparser config = configparser.ConfigParser() config.read(file_path) setup_cfg_dict = {} for section in config.sections(): options = {option: config.get(section, option) for option in config.options(section)} setup_cfg_dict[section] = options return setup_cfg_dict"},{"question":"**Objective**: Implement functionality to manage and convert large integer values in Python. # Problem Statement We need to implement a class named `LargeInteger` that provides methods to initialize, convert, and perform operations on large integers. Your class should demonstrate an understanding of Python\'s handling of large integers and provide conversion and arithmetic functionalities similar to those seen in the C API, but within a pure Python context. # Requirements: 1. **Initialization**: - The class should be initialized with an integer value which may be provided as an integer, a string, or a floating-point number. 2. **Methods**: - `to_long(self)`: Returns the integer value of the object. - `to_unsigned_long(self)`: Returns the unsigned integer value of the object. Raises `ValueError` if the value is negative. - `to_hex(self)`: Returns the hexadecimal string representation of the integer. - `add(self, other)`: Adds another `LargeInteger` object or an integer to the current value and returns a new `LargeInteger` object. - `subtract(self, other)`: Subtracts another `LargeInteger` object or an integer from the current value and returns a new `LargeInteger` object. - `multiply(self, other)`: Multiplies another `LargeInteger` object or an integer with the current value and returns a new `LargeInteger` object. - `divide(self, other)`: Divides the current value by another `LargeInteger` object or an integer and returns a new `LargeInteger` object. Raises `ZeroDivisionError` if the denominator is zero. # Specifications: - **Input Constraints**: - The integer value can be extremely large, potentially exceeding typical 32-bit or 64-bit integer sizes. - The input as a string can include hexadecimal representations (prefixed with `0x`) or decimal integers. - **Output Requirements**: - Your class methods should handle and return Python\'s `int` type or `str` for hexadecimal conversion. - **Performance**: - Ensure that your methods are efficient and handle large values properly. # Example Usage: ```python # Initialize with an integer num1 = LargeInteger(12345678901234567890) print(num1.to_long()) # Should print 12345678901234567890 # Initialize with a string num2 = LargeInteger(\\"0x1ffffffffffffff\\") print(num2.to_long()) # Should print 9007199254740991 # Hexadecimal representation print(num2.to_hex()) # Should print \\"0x1ffffffffffffff\\" # Add numbers result = num1.add(LargeInteger(10)) print(result.to_long()) # Should print 12345678901234567900 # Subtract numbers result = num1.subtract(1) print(result.to_long()) # Should print 12345678901234567889 # Multiply numbers result = num1.multiply(2) print(result.to_long()) # Should print 24691357802469135780 # Divide numbers result = num1.divide(10) print(result.to_long()) # Should print 1234567890123456789 ``` **Notes**: - Make sure to handle edge cases, such as division by zero or passing invalid types to the methods.","solution":"class LargeInteger: def __init__(self, value): if isinstance(value, int): self.value = value elif isinstance(value, str): if value.startswith(\'0x\'): self.value = int(value, 16) else: self.value = int(value) elif isinstance(value, float): self.value = int(value) else: raise ValueError(\\"Unsupported type for LargeInteger initialization.\\") def to_long(self): return self.value def to_unsigned_long(self): if self.value < 0: raise ValueError(\\"Value is negative, cannot convert to unsigned long.\\") return self.value def to_hex(self): return hex(self.value) def add(self, other): if isinstance(other, LargeInteger): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Unsupported type for addition.\\") return LargeInteger(self.value + other_value) def subtract(self, other): if isinstance(other, LargeInteger): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Unsupported type for subtraction.\\") return LargeInteger(self.value - other_value) def multiply(self, other): if isinstance(other, LargeInteger): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Unsupported type for multiplication.\\") return LargeInteger(self.value * other_value) def divide(self, other): if isinstance(other, LargeInteger): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Unsupported type for division.\\") if other_value == 0: raise ZeroDivisionError(\\"Divide by zero error.\\") return LargeInteger(self.value // other_value)"},{"question":"# Question: Customizing Seaborn Plots for Presentation You are given a dataset of exam scores for two classes (`Class_A` and `Class_B`). Your task is to create two distinct visualizations using seaborn with customized aesthetics that are suitable for presentation in different contexts: a paper report and a talk. Input - A dataframe `exam_scores` with the following columns: ```python import pandas as pd data = { \'Class\': [\'Class_A\'] * 50 + [\'Class_B\'] * 50, \'Math_Score\': [np.random.randint(50, 100) for _ in range(100)], \'Science_Score\': [np.random.randint(50, 100) for _ in range(100)] } exam_scores = pd.DataFrame(data) ``` Output - Two figures: 1. A figure suitable for a paper report, with customized aesthetics including a boxplot of `Math_Score` vs. `Class` displayed with the `whitegrid` style and smaller context settings. 2. A figure suitable for a talk, with customized aesthetics including a boxplot of `Science_Score` vs. `Class` with the `dark` style and larger context settings. Constraints - You must use the `set_theme()`, `set_style()`, `set_context()`, and `despine()` functions appropriately. - The figures must have clearly visible labels and titles. - The axes spines must be modified to enhance the visual appeal for the respective contexts. Example Implementation ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Generate random data np.random.seed(0) data = { \'Class\': [\'Class_A\'] * 50 + [\'Class_B\'] * 50, \'Math_Score\': [np.random.randint(50, 100) for _ in range(100)], \'Science_Score\': [np.random.randint(50, 100) for _ in range(100)] } exam_scores = pd.DataFrame(data) # Figure 1: Paper Report sns.set_theme() sns.set_style(\\"whitegrid\\") sns.set_context(\\"paper\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"Class\\", y=\\"Math_Score\\", data=exam_scores, palette=\\"Set2\\") plt.title(\\"Math Scores Comparison - Paper Report\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Math Score\\") sns.despine(left=True) plt.show() # Figure 2: Talk Presentation sns.set_theme() sns.set_style(\\"dark\\") sns.set_context(\\"talk\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"Class\\", y=\\"Science_Score\\", data=exam_scores, palette=\\"Set1\\") plt.title(\\"Science Scores Comparison - Talk Presentation\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Science Score\\") sns.despine(offset=10, trim=True) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd # Generate random data np.random.seed(0) data = { \'Class\': [\'Class_A\'] * 50 + [\'Class_B\'] * 50, \'Math_Score\': [np.random.randint(50, 100) for _ in range(100)], \'Science_Score\': [np.random.randint(50, 100) for _ in range(100)] } exam_scores = pd.DataFrame(data) def create_paper_report(exam_scores): Create and display a figure suitable for a paper report. sns.set_theme() sns.set_style(\\"whitegrid\\") sns.set_context(\\"paper\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"Class\\", y=\\"Math_Score\\", data=exam_scores, palette=\\"Set2\\") plt.title(\\"Math Scores Comparison - Paper Report\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Math Score\\") sns.despine(left=True) plt.show() def create_talk_presentation(exam_scores): Create and display a figure suitable for a talk presentation. sns.set_theme() sns.set_style(\\"dark\\") sns.set_context(\\"talk\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"Class\\", y=\\"Science_Score\\", data=exam_scores, palette=\\"Set1\\") plt.title(\\"Science Scores Comparison - Talk Presentation\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Science Score\\") sns.despine(offset=10, trim=True) plt.show()"},{"question":"# Pandas Indexing and Selection Problem Statement You are provided with a dataset of student scores in a CSV file named `student_scores.csv`. The dataset has the following columns: - `StudentID`: Unique identifier for each student. - `Name`: Name of the student. - `Math`: Score in Mathematics. - `Science`: Score in Science. - `English`: Score in English. Your task is to use pandas to perform the following operations: 1. **Load the DataFrame**: Load the CSV file `student_scores.csv` into a pandas DataFrame and set the `StudentID` as the index. 2. **Label-based Indexing**: Using the label-based `.loc` accessor, extract the scores of students with `StudentID` 102 and 104. 3. **Position-based Indexing**: Using the position-based `.iloc` accessor, extract the names and scores (`Math`, `Science`, `English`) of the first 5 students by using slicing. 4. **Attribute Access**: Access the `Math` scores directly as an attribute of the DataFrame. 5. **Boolean Indexing**: * Find all students who scored more than 80 in `Math`. * Find all students who scored less than 50 in `English`. 6. **Combining Indexing Methods**: * Using a combination of `.loc` and `.iloc`, extract the names and scores of students whose `StudentID` is either 101 or 105. 7. **Set Operations on Indexes**: * Suppose you have a new list of `StudentID`s who qualified for an extra credit exam. The list is `[101, 103, 106]`. Generate a DataFrame of these students with their scores. 8. **Handling Missing Values**: * If a student\'s score is missing (NaN) in `Science`, fill it with the average score of the `Science` column. Input Format * The CSV file `student_scores.csv`. Output Format * Directly display the results of each operation. Constraints * Ensure the index `StudentID` is unique. * Handle missing values appropriately where required. * Consider performance and readability in your solution. You may assume that the provided dataset is small and fits into memory. ```python import pandas as pd # 1. Load the DataFrame df = pd.read_csv(\'student_scores.csv\') df.set_index(\'StudentID\', inplace=True) # 2. Label-based Indexing students_loc = df.loc[[102, 104]] # 3. Position-based Indexing students_iloc = df.iloc[:5, :] # 4. Attribute Access math_scores = df.Math # 5. Boolean Indexing high_math_scores = df[df.Math > 80] low_english_scores = df[df.English < 50] # 6. Combining Indexing Methods combined_indexing = df.loc[df.index.isin([101, 105])] # 7. Set Operations on Indexes extra_credit_ids = pd.Index([101, 103, 106]) extra_credit_df = df.loc[df.index.intersection(extra_credit_ids)] # 8. Handling Missing Values df[\'Science\'].fillna(df[\'Science\'].mean(), inplace=True) # Output the results print(\\"Label-based Indexing:n\\", students_loc) print(\\"nPosition-based Indexing:n\\", students_iloc) print(\\"nAttribute Access - Math Scores:n\\", math_scores) print(\\"nBoolean Indexing - High Math Scores:n\\", high_math_scores) print(\\"nBoolean Indexing - Low English Scores:n\\", low_english_scores) print(\\"nCombining Indexing Methods:n\\", combined_indexing) print(\\"nSet Operations on Indexes:n\\", extra_credit_df) print(\\"nDataFrame after handling missing values:n\\", df) ```","solution":"import pandas as pd def load_dataframe(file_path): df = pd.read_csv(file_path) df.set_index(\'StudentID\', inplace=True) return df def label_based_indexing(df): return df.loc[[102, 104]] def position_based_indexing(df): return df.iloc[:5, :] def attribute_access(df): return df.Math def boolean_indexing_math(df): return df[df.Math > 80] def boolean_indexing_english(df): return df[df.English < 50] def combined_indexing(df): return df.loc[df.index.isin([101, 105])] def set_operations_on_indexes(df): extra_credit_ids = pd.Index([101, 103, 106]) return df.loc[df.index.intersection(extra_credit_ids)] def handle_missing_values(df): df[\'Science\'].fillna(df[\'Science\'].mean(), inplace=True) return df"},{"question":"**Coding Assessment Question:** # Module Import and Resource Management with `importlib` Objective You are required to create a Python script that programmatically manages the import of a given module and reads resources from it using the `importlib` package. This exercise evaluates your understanding of dynamic module importation and resource management using `importlib`. Task 1. **Dynamic Import** - Write a function `dynamic_import(module_name: str)` that accepts a module name as a string and imports it. - If the module is not available, the function should raise a `ModuleNotFoundError`. - The function should return the imported module object. 2. **Resource Reading** - Write a function `read_text_resource(module, resource_name: str) -> str` that takes a module object and a resource name as input. - The function should read and return the content of the specified resource within the module as a string. - If the resource does not exist, the function should raise a `FileNotFoundError`. Input and Output Formats - The function `dynamic_import` will take a single input: - `module_name` (str): The name of the module to be imported. - The function `read_text_resource` will take two inputs: - `module` (ModuleType): The module object from which to read the resource. - `resource_name` (str): The name of the resource to read. - The output of `dynamic_import` should be the imported module object. - The output of `read_text_resource` should be the content of the resource as a string. Example ```python # Sample Usage try: module = dynamic_import(\'collections\') resource_content = read_text_resource(module, \'placeholder_resource.txt\') print(resource_content) except ModuleNotFoundError: print(\\"The module could not be found.\\") except FileNotFoundError: print(\\"The resource could not be found in the module.\\") ``` Constraints - You should not assume the presence of any specific module or resource files. - Your implementation should handle and raise appropriate exceptions (i.e., `ModuleNotFoundError` and `FileNotFoundError`). Performance Requirements - The functions must efficiently handle module imports and resource reads without substantial delays.","solution":"import importlib import importlib.resources def dynamic_import(module_name: str): Imports a module dynamically based on the provided module name. Args: - module_name (str): The name of the module to be imported. Returns: - module: The imported module object. Raises: - ModuleNotFoundError: If the module cannot be found. try: module = importlib.import_module(module_name) return module except ModuleNotFoundError as e: raise ModuleNotFoundError(f\\"Module \'{module_name}\' not found\\") from e def read_text_resource(module, resource_name: str) -> str: Reads a resource from the specified module. Args: - module: The module object from which to read the resource. - resource_name (str): The name of the resource to read. Returns: - str: The content of the resource. Raises: - FileNotFoundError: If the resource cannot be found in the module. try: with importlib.resources.open_text(module, resource_name) as f: return f.read() except FileNotFoundError as e: raise FileNotFoundError(f\\"Resource \'{resource_name}\' not found in module \'{module.__name__}\'\\") from e"},{"question":"Design an interactive Python console that leverages the \\"readline\\" module to enhance usability with custom command history and text completion features. Your console should: 1. Maintain a command history: - Automatically load history from a file named `.custom_history` in the user\'s home directory upon startup. - Save the command history back to the file upon exit. - Limit the command history to the last 500 entries. 2. Support custom command completion: - Implement a custom completer function that can complete from a predefined list of commands: `[\'start\', \'stop\', \'restart\', \'status\', \'help\', \'exit\']`. - Bind the completer to the Tab key. 3. Provide a clear and user-friendly prompt that indicates it is a custom console (e.g., `\\">>> \\"`). # Constraints - You are not allowed to use external libraries except for those provided in the standard Python library. - Ensure your implementation handles FileNotFoundError when loading the history file. - The solution should be compatible with both GNU readline and libedit on macOS. # Input Format There is no specific input format. The console should interactively take user inputs. # Output Format Print the output directly as part of the interactive session. # Implementation Skeleton You can use the following code skeleton to get started: ```python import readline import os import atexit # List of commands for custom completion COMMANDS = [\'start\', \'stop\', \'restart\', \'status\', \'help\', \'exit\'] def completer(text, state): Custom command completer options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".custom_history\\") # Load history if the file exists try: readline.read_history_file(histfile) readline.set_history_length(500) except FileNotFoundError: pass # Save history on exit atexit.register(readline.write_history_file, histfile) # Configure readline readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(completer) # Start an interactive loop while True: try: line = input(\'>>> \') if line.strip() == \'exit\': break # Simulate command execution (only echoing here) print(f\'Executing command: {line}\') except (EOFError, KeyboardInterrupt): break if __name__ == \'__main__\': main() ``` This skeleton sets up the basic structure for your interactive console. Implement and test it thoroughly to ensure it meets the requirements.","solution":"import readline import os import atexit # List of commands for custom completion COMMANDS = [\'start\', \'stop\', \'restart\', \'status\', \'help\', \'exit\'] def completer(text, state): Custom command completer options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".custom_history\\") # Load history if the file exists try: readline.read_history_file(histfile) readline.set_history_length(500) except FileNotFoundError: pass # Save history on exit atexit.register(readline.write_history_file, histfile) # Configure readline readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(completer) # Start an interactive loop while True: try: line = input(\'>>> \') if line.strip() == \'exit\': break # Simulate command execution (only echoing here) print(f\'Executing command: {line}\') except (EOFError, KeyboardInterrupt): break if __name__ == \'__main__\': main()"},{"question":"As a Python developer, you have been given a task to extend the Distutils package to support creating a new type of distribution archive, `myzip`. This new distribution type should bundle the Python package files into a zip file but with a custom extension `.myzip`. Your task is to: 1. Create a new command `bdist_myzip` that will generate this custom distribution archive. 2. Integrate this command into the `setup.py` script. # Requirements: - **Create the `bdist_myzip` command** - This command should inherit from `distutils.cmd.Command`. - Implement the `initialize_options`, `finalize_options`, and `run` methods. - The command should package the Python files of the project into a zip file with the extension `.myzip`. - Add the `(command, filename)` pair to `self.distribution.dist_files`. - **Modify `setup.py` to use the new command** - Ensure the `setup()` function can call `bdist_myzip` using the `cmdclass` parameter. # Input: - The `setup.py` file for the project, which should use the `bdist_myzip` command to produce the distribution archive. - Python source files in the same directory. # Output: - A zip file with a `.myzip` extension containing the project files should be created in the `dist/` directory. # Constraints: - The solution should handle the `--dry-run` option correctly. - You cannot modify the existing structure of other commands provided by Distutils, only extend them. # Additional Details: - Document any assumptions made, and ensure your code follows best practices for extending Distutils. Below is an example of how the `setup.py` file should look: ```python from distutils.core import setup # setup() with necessary configurations setup( name=\'mypackage\', version=\'0.1\', description=\'My custom package\', packages=[\'mypackage\'], cmdclass={ \'bdist_myzip\': bdist_myzip, # Registering the new command }, ) ``` You need to provide the complete implementation for the `bdist_myzip` command and any necessary modifications to `setup.py`.","solution":"import os from distutils.core import Command from distutils.dir_util import remove_tree from zipfile import ZipFile class bdist_myzip(Command): description = \\"Create a distribution as a zip file with \'.myzip\' extension\\" user_options = [] def initialize_options(self): pass def finalize_options(self): self.dist_dir = os.path.join(os.getcwd(), \'dist\') def run(self): if not self.dry_run: # Ensure the dist directory exists if not os.path.exists(self.dist_dir): os.makedirs(self.dist_dir) # Create the zip file with the .myzip extension archive_name = os.path.join(self.dist_dir, f\\"{self.distribution.get_name()}-{self.distribution.get_version()}.myzip\\") with ZipFile(archive_name, \'w\') as zip_file: for root, dirs, files in os.walk(os.getcwd()): for file in files: if file.endswith(\\".py\\"): file_path = os.path.join(root, file) zip_file.write(file_path, os.path.relpath(file_path, start=os.getcwd())) # Register the output file self.distribution.dist_files.append((\'bdist_myzip\', \'\', archive_name))"},{"question":"**Question: Color Palette Visualization Using Seaborn** You are given a dataset representing the sales of different products over a week. Your task is to visualize this data using seaborn, applying different color palettes and demonstrating your understanding of the seaborn `color_palette` function and related customization options. # Dataset The dataset is a dictionary where keys represent product names and values are lists representing daily sales over a week. ```python data = { \'Product A\': [10, 15, 20, 25, 30, 35, 40], \'Product B\': [5, 10, 15, 20, 25, 30, 35], \'Product C\': [0, 5, 10, 15, 20, 25, 30], \'Product D\': [15, 20, 25, 30, 35, 40, 45], \'Product E\': [20, 25, 30, 35, 40, 45, 50] } ``` # Requirements 1. **Plot Visualization**: - Create three different line plots to visualize the dataset using different seaborn color palettes: - The default seaborn palette. - A named palette (you can choose either \'husl\', \'Set2\', or any of the other named options). - A custom palette in hex values. 2. **Context Manager**: - Demonstrate using a context manager to change the color palette temporarily for one of the plots. 3. **Hex Codes**: - Print out the hex codes of the colors used in the custom palette plot. # Input You are not required to take any input; use the provided dataset directly in the function. # Output Your function should: 1. Display three line plots with the different color palettes. 2. Print the hex codes of the colors used in the custom palette plot. # Constraints - Ensure the plots are clearly labeled with a title and axis labels. - Maintain consistency in plot styling (e.g., line width, markers) across the plots for better comparison. - Use at least five different colors in the custom palette. # Example Usage ```python def visualize_sales_data(): import seaborn as sns import matplotlib.pyplot as plt data = { \'Product A\': [10, 15, 20, 25, 30, 35, 40], \'Product B\': [5, 10, 15, 20, 25, 30, 35], \'Product C\': [0, 5, 10, 15, 20, 25, 30], \'Product D\': [15, 20, 25, 30, 35, 40, 45], \'Product E\': [20, 25, 30, 35, 40, 45, 50] } # Code to generate plots and print hex codes visualize_sales_data() ``` # Template Here is a basic template to get you started. Fill in the required code to meet the requirements. ```python def visualize_sales_data(): import seaborn as sns import matplotlib.pyplot as plt data = { \'Product A\': [10, 15, 20, 25, 30, 35, 40], \'Product B\': [5, 10, 15, 20, 25, 30, 35], \'Product C\': [0, 5, 10, 15, 20, 25, 30], \'Product D\': [15, 20, 25, 30, 35, 40, 45], \'Product E\': [20, 25, 30, 35, 40, 45, 50] } # Create the default palette plot here # Example: sns.lineplot(...) with appropriate parameters # Create the named palette plot here # Example: sns.lineplot(...) with appropriate parameters # Create the custom palette plot here # Example: sns.lineplot(...) with appropriate parameters # Use a context manager to temporarily change the palette and create one of the plots # Print hex codes of the custom palette # Example: print(sns.color_palette(...).as_hex()) plt.show() # Ensure plots are displayed visualize_sales_data() ```","solution":"def visualize_sales_data(): import seaborn as sns import matplotlib.pyplot as plt data = { \'Product A\': [10, 15, 20, 25, 30, 35, 40], \'Product B\': [5, 10, 15, 20, 25, 30, 35], \'Product C\': [0, 5, 10, 15, 20, 25, 30], \'Product D\': [15, 20, 25, 30, 35, 40, 45], \'Product E\': [20, 25, 30, 35, 40, 45, 50] } # Create dataset for plotting days = [\'Mon\', \'Tue\', \'Wed\', \'Thu\', \'Fri\', \'Sat\', \'Sun\'] # Default palette plot plt.figure(figsize=(10, 6)) for product, sales in data.items(): sns.lineplot(x=days, y=sales, label=product) plt.title(\'Sales Data (Default Palette)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # Named palette (\'husl\') plt.figure(figsize=(10, 6)) with sns.color_palette(\'husl\'): for product, sales in data.items(): sns.lineplot(x=days, y=sales, label=product) plt.title(\'Sales Data (HUSL Palette)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # Custom palette custom_palette = [\'#2E8B57\', \'#8A2BE2\', \'#DC143C\', \'#FF8C00\', \'#1E90FF\'] plt.figure(figsize=(10, 6)) sns.set_palette(custom_palette) for product, sales in data.items(): sns.lineplot(x=days, y=sales, label=product) plt.title(\'Sales Data (Custom Palette)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # Print hex codes of custom palette hex_codes = sns.color_palette(custom_palette).as_hex() print(\\"Hex codes of the custom palette:\\", hex_codes)"},{"question":"# AIFF-C Audio File Manipulation and Analysis Given the provided \\"aifc\\" documentation, your task is to create a Python function that processes an AIFF-C audio file. This function will: 1. **Open an AIFF-C audio file for reading.** 2. **Retrieve and print audio parameters** such as number of channels, sampling rate, sample width, and number of frames. 3. **Read and count audio frames** while calculating the total duration of the audio file in seconds. 4. **Add a marker at the midpoint of the audio file** and save the modified file to a new location. Function Signature ```python def process_aifc_file(input_file: str, output_file: str) -> None: Processes the AIFF-C audio file located at input_file and saves the modified file to output_file. Args: input_file (str): Path to the input AIFF-C audio file. output_file (str): Path to save the output AIFF-C audio file with added marker. Returns: None # Your implementation here ``` Detailed Steps 1. **Open the AIFF-C file** located at `input_file` for reading using `aifc.open(file, mode=\'rb\')`. 2. **Retrieve and print the following audio parameters**: - Number of channels: Use `getnchannels()`. - Sample width (in bytes): Use `getsampwidth()`. - Sampling rate (frames per second): Use `getframerate()`. - Number of frames: Use `getnframes()`. 3. **Calculate the total duration** of the audio file in seconds using the formula: [ text{duration_seconds} = frac{text{number of frames}}{text{frame rate}} ] 4. **Print the total duration** of the audio file. 5. **Add a marker at the midpoint** of the audio file: - Calculate the midpoint in frames: `midpoint_frame = number_of_frames // 2`. - Add a marker using `setmark(id=1, pos=midpoint_frame, name=\\"Midpoint Marker\\")`. 6. **Save the modified audio file to `output_file`**: - Use the retrieved parameters to set up a new AIFF-C file for writing. - Copy audio data from the input file to the new output file. - Add the marker created in step 5. - Save and close the new file. Assumptions - The input file is a valid and existing AIFF-C file. - The output directory is writable. Constraints - Do not use any external libraries except for the standard library. - The function should handle files efficiently to accommodate potentially large audio data sizes. Example ```python input_file = \'path/to/input.aifc\' output_file = \'path/to/output_with_marker.aifc\' process_aifc_file(input_file, output_file) # Expected output: # Number of channels: 2 # Sample width (in bytes): 2 # Sampling rate: 44100 # Number of frames: 176400 # Total duration (seconds): 4.0 ```","solution":"import aifc def process_aifc_file(input_file: str, output_file: str) -> None: Processes the AIFF-C audio file located at input_file and saves the modified file to output_file. Args: input_file (str): Path to the input AIFF-C audio file. output_file (str): Path to save the output AIFF-C audio file with added marker. Returns: None # Open the input file for reading with aifc.open(input_file, \'rb\') as infile: # Retrieve audio parameters num_channels = infile.getnchannels() sample_width = infile.getsampwidth() sample_rate = infile.getframerate() num_frames = infile.getnframes() # Print audio parameters print(\\"Number of channels:\\", num_channels) print(\\"Sample width (in bytes):\\", sample_width) print(\\"Sampling rate:\\", sample_rate) print(\\"Number of frames:\\", num_frames) # Calculate and print total duration in seconds total_duration_seconds = num_frames / sample_rate print(\\"Total duration (seconds):\\", total_duration_seconds) # Add a marker at the midpoint of the audio file midpoint_frame = num_frames // 2 # Open the output file for writing with aifc.open(output_file, \'wb\') as outfile: # Set output file parameters outfile.setnchannels(num_channels) outfile.setsampwidth(sample_width) outfile.setframerate(sample_rate) # Read frames from the input file and write them to the output file audio_data = infile.readframes(num_frames) outfile.writeframes(audio_data) # Add the midpoint marker outfile.setmark(1, midpoint_frame, b\\"Midpoint Marker\\")"},{"question":"**Objective:** Using the `weakref` module, implement a class `ObjectRegistry` that keeps track of object instances and their creation times, without preventing the garbage collection of those objects when they are no longer used. The class should fulfill the following requirements: 1. **Class Definition:** Define a class `ObjectRegistry`. 2. **Initialization:** The class should use a `WeakValueDictionary` to store object instances. 3. **Add Object Method:** The class should have a method `add_object(obj)` to add an object to the registry and store its creation time. 4. **Get Object Info Method:** The class should have a method `get_object_info(obj)` that returns a tuple containing the creation time and the current value of the object if the object still exists, otherwise `None`. **Implementation Details:** - Use the `weakref.WeakValueDictionary` to store object instances. - When an object is added to the `WeakValueDictionary`, store the current timestamp using `time.time()`. - The registry should not prevent objects from being garbage collected when they are no longer referenced elsewhere. - Ensure thread safety if the registry might be accessed/modified from multiple threads. ```python import weakref import time class ObjectRegistry: def __init__(self): # Initialize a WeakValueDictionary to store object references self._registry = weakref.WeakValueDictionary() self._creation_times = {} def add_object(self, obj): Add an object to the registry and store its creation time. object_id = id(obj) self._registry[object_id] = obj self._creation_times[object_id] = time.time() def get_object_info(self, obj): Return a tuple (creation_time, object) if object exists, otherwise None. object_id = id(obj) if object_id in self._registry: return self._creation_times.get(object_id), self._registry[object_id] return None # Example usage # registry = ObjectRegistry() # obj = ExampleObject() # registry.add_object(obj) # info = registry.get_object_info(obj) # print(info) ``` **Constraints:** - Do not use strong references for storing objects that should be weakly referenced. - Handle the case where an object may be garbage collected. - Ensure that the access to the registry is safe in a multi-threaded environment. **Testing:** 1. Create object instances and add them to the registry using the `add_object` method. 2. Retrieve the object info using the `get_object_info` method and verify that it returns correct creation times and object references. 3. Ensure that once the object is garbage collected, the `get_object_info` method returns `None` for that object. **Use Cases:** - Managing temporary objects with a need for automatic cleanup. - Cache implementations where the objects should be automatically removed when they are no longer needed. - Monitoring object lifecycle without preventing garbage collection.","solution":"import weakref import time import threading class ObjectRegistry: def __init__(self): self._registry = weakref.WeakValueDictionary() self._creation_times = {} self._lock = threading.Lock() def add_object(self, obj): Add an object to the registry and store its creation time. with self._lock: object_id = id(obj) self._registry[object_id] = obj self._creation_times[object_id] = time.time() def get_object_info(self, obj): Return a tuple (creation_time, object) if object exists, otherwise None. with self._lock: object_id = id(obj) if object_id in self._registry: return self._creation_times.get(object_id), self._registry[object_id] return None"},{"question":"**Question:** You are building an application that interacts with a remote server to fetch and send data. Your application needs to handle various complexities such as authentication, sending data via POST requests, and dealing with potential errors. Implement a function that performs the following operations: 1. **Fetch Data from a URL**: - Fetch data from the URL \\"http://example.com/api/data\\". - If the request is successful, return the fetched data. 2. **Send Data via POST Request**: - Send a dictionary of data `{\'name\': \'John Doe\', \'location\': \'USA\', \'language\': \'Python\'}` via a POST request to the URL \\"http://example.com/api/register\\". - Include a custom header `{\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\'}`. - Return the response from the server. 3. **Handle Basic Authentication**: - If the server requires authentication, use the username \\"user\\" and password \\"pass\\". - If accessing \\"http://example.com/api/data\\" requires authentication, handle it using basic authentication. 4. **Manage Proxies**: - Ensure that network requests are sent through a proxy server at \\"http://proxy.example.com:8080\\". 5. **Handle Exceptions**: - Handle potential `HTTPError` and `URLError`. - For `HTTPError`, return the error code and the error message. - For `URLError`, return the reason for the connection failure. 6. **Set a Timeout**: - Set a global timeout for all the socket connections to 10 seconds. Write the function `fetch_and_send_data()` to implement the above operations. The function should be able to handle these operations robustly and return appropriate results for each scenario. ```python from urllib.request import Request, urlopen, ProxyHandler, build_opener, install_opener, HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm from urllib.parse import urlencode from urllib.error import URLError, HTTPError import socket def fetch_and_send_data(): socket.setdefaulttimeout(10) # Set global timeout proxy_support = ProxyHandler({\'http\': \'http://proxy.example.com:8080\'}) password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, \'http://example.com/\', \'user\', \'pass\') auth_handler = HTTPBasicAuthHandler(password_mgr) opener = build_opener(proxy_support, auth_handler) install_opener(opener) headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\'} # Fetching data from the URL try: with urlopen(\'http://example.com/api/data\') as response: data = response.read() print(\'Fetched data:\', data) except HTTPError as e: return f\'HTTPError: {e.code} - {e.reason}\' except URLError as e: return f\'URLError: {e.reason}\' # Sending data via POST request url = \'http://example.com/api/register\' values = {\'name\': \'John Doe\', \'location\': \'USA\', \'language\': \'Python\'} data = urlencode(values).encode(\'ascii\') request = Request(url, data=data, headers=headers) try: with urlopen(request) as response: result = response.read() print(\'POST response:\', result) except HTTPError as e: return f\'HTTPError: {e.code} - {e.reason}\' except URLError as e: return f\'URLError: {e.reason}\' return \\"Operations completed successfully\\" # Students are required to implement the function fetch_and_send_data as described above. ``` **Constraints**: - Don\'t use any external libraries other than the standard library of Python. - Ensure that your solution is robust and handles edge cases. **Expected Output**: The function should fetch data from the given URL, send data via a POST request, handle authentication and proxy settings, manage exceptions, and return appropriate results or exceptions based on different scenarios.","solution":"from urllib.request import Request, urlopen, ProxyHandler, build_opener, install_opener, HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm from urllib.parse import urlencode from urllib.error import URLError, HTTPError import socket def fetch_and_send_data(): socket.setdefaulttimeout(10) # Set global timeout proxy_support = ProxyHandler({\'http\': \'http://proxy.example.com:8080\'}) password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, \'http://example.com/\', \'user\', \'pass\') auth_handler = HTTPBasicAuthHandler(password_mgr) opener = build_opener(proxy_support, auth_handler) install_opener(opener) headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\'} # Fetching data from the URL try: with urlopen(\'http://example.com/api/data\') as response: data = response.read() print(\'Fetched data:\', data) fetched_data = data except HTTPError as e: return f\'HTTPError: {e.code} - {e.reason}\' except URLError as e: return f\'URLError: {e.reason}\' # Sending data via POST request url = \'http://example.com/api/register\' values = {\'name\': \'John Doe\', \'location\': \'USA\', \'language\': \'Python\'} data = urlencode(values).encode(\'ascii\') request = Request(url, data=data, headers=headers) try: with urlopen(request) as response: result = response.read() print(\'POST response:\', result) post_response = result except HTTPError as e: return f\'HTTPError: {e.code} - {e.reason}\' except URLError as e: return f\'URLError: {e.reason}\' return fetched_data, post_response"},{"question":"**Objective:** Demonstrate your understanding of migrating Python 2 code to Python 3. This requires implementing methods to handle text and binary data compatibility and leveraging the appropriate libraries. **Problem Statement:** You are given a Python 2 codebase that reads a file, processes its contents, and writes the results to another file. For this assessment, you need to port this code to make it compatible with both Python 2.7 and Python 3. **Original Python 2 Code:** ```python # Reads a file and processes its content def process_file(input_file_path, output_file_path): with open(input_file_path, \'r\') as input_file: data = input_file.read() # Assuming processing here simply transforms text to uppercase processed_data = data.upper() with open(output_file_path, \'w\') as output_file: output_file.write(processed_data) # Example usage process_file(\'input.txt\', \'output.txt\') ``` **Task:** 1. Update the `process_file` function to be compatible with both Python 2.7 and Python 3. Ensure it properly handles text and binary data transformation. 2. Ensure that the file reading and writing correctly handles text files which may contain non-ASCII characters. 3. Add all necessary imports and future imports to provide compatibility. **Constraints:** - You must use `io.open` instead of the built-in `open` function. - You must decode binary data to text as soon as possible and encode text back to binary as late as possible. - The file content should be processed in-memory to uppercase the text. **Function Signature:** ```python from __future__ import absolute_import, division, print_function, unicode_literals import io def process_file(input_file_path, output_file_path): pass # Example usage process_file(\'input.txt\', \'output.txt\') ``` **Expected Input:** - `input_file_path` (string): Path to the input text file. - `output_file_path` (string): Path to the output text file. **Expected Output:** - None. The function should create/update the output file directly with processed data. **Example:** Consider the input file `input.txt` with the content: ``` Hello, World! こんにちは世界 ``` After running: ```python process_file(\'input.txt\', \'output.txt\') ``` The `output.txt` should contain: ``` HELLO, WORLD! こんにちは世界 ``` **Evaluation:** - Verify that the function works correctly with multiple encoding types. - Ensure proper use of handling text versus binary data as per Python 2.7 and Python 3 standards. - Code must not break for non-ASCII characters.","solution":"from __future__ import absolute_import, division, print_function, unicode_literals import io def process_file(input_file_path, output_file_path): # Open the input file with \'utf-8\' encoding in reading mode with io.open(input_file_path, \'r\', encoding=\'utf-8\') as input_file: data = input_file.read() # Process the data: transform text to uppercase processed_data = data.upper() # Open the output file with \'utf-8\' encoding in writing mode with io.open(output_file_path, \'w\', encoding=\'utf-8\') as output_file: output_file.write(processed_data)"},{"question":"# seaborn Customization and Visualization As a data analyst, you are tasked with visualizing sales data for three products, A, B, and C, across four different regions using the seaborn library. The dataset is presented as follows: | Region | Product | Sales | |--------|---------|-------| | North | A | 200 | | North | B | 150 | | North | C | 175 | | South | A | 220 | | South | B | 130 | | South | C | 160 | | East | A | 210 | | East | B | 180 | | East | C | 145 | | West | A | 190 | | West | B | 170 | | West | C | 155 | Task: 1. **Load the data** into a pandas DataFrame. 2. **Create a barplot** using seaborn where: - The x-axis represents the regions. - The y-axis represents the sales. - Different colors represent different products. 3. Set the theme of the plot to `whitegrid` and use a pastel color palette. 4. Customize the plot to: - Remove the top and right spines. - Change the title to \\"Sales Data by Region and Product\\". 5. Save the plot as a PNG file named `sales_data_plot.png`. Expected Input: The data should be stored in a pandas DataFrame as shown in the table. Expected Output: A PNG file named `sales_data_plot.png`. Constraints: - The seaborn and pandas libraries should be used. - Ensure that missing values (if any) are handled appropriately. Example Function Signature: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(): # Define the data data = {\'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [200, 150, 175, 220, 130, 160, 210, 180, 145, 190, 170, 155]} # Create DataFrame df = pd.DataFrame(data) # Set theme and palette sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Create barplot barplot = sns.barplot(x=\\"Region\\", y=\\"Sales\\", hue=\\"Product\\", data=df) # Customize plot custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"whitegrid\\", rc=custom_params) barplot.set_title(\\"Sales Data by Region and Product\\") # Save plot plt.savefig(\'sales_data_plot.png\') ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(): # Define the data data = { \'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [200, 150, 175, 220, 130, 160, 210, 180, 145, 190, 170, 155] } # Create DataFrame df = pd.DataFrame(data) # Set theme and palette sns.set_theme(style=\\"whitegrid\\") sns.set_palette(\\"pastel\\") # Create barplot plt.figure(figsize=(10, 6)) barplot = sns.barplot(x=\\"Region\\", y=\\"Sales\\", hue=\\"Product\\", data=df) # Customize plot barplot.set_title(\\"Sales Data by Region and Product\\") sns.despine(top=True, right=True) # Save plot plt.savefig(\'sales_data_plot.png\') # Return the dataframe for testing purposes return df"},{"question":"# Task Given a list of tuples, where each tuple represents a student\'s name and their score, implement a function that maintains the list in sorted order based on the students\' scores. The implementation should use the `bisect` module functions for both locating the insertion point and inserting the element. The function should be named `insert_student` and take two parameters: - `students`: A list of tuples where each tuple is in the format (`name`, `score`). This list is already sorted based on scores. - `new_student`: A tuple in the format (`name`, `score`). The function should return the updated list of students in sorted order by score. # Constraints - All scores are integers. - The list will contain at most 10^5 elements. - The student\'s name will be a non-empty string of up to 100 characters. # Example ```python from bisect import insort def insert_student(students, new_student): # Your implementation here pass # Example usage: students = [(\'Alice\', 88), (\'Bob\', 67), (\'Charlie\', 90)] new_student = (\'David\', 70) updated_students = insert_student(students, new_student) print(updated_students) # Expected: [(\'Bob\', 67), (\'David\', 70), (\'Alice\', 88), (\'Charlie\', 90)] ``` # Performance Requirements - The function should efficiently handle the insertion in O(log n) for locating the insertion point and O(n) for the actual insertion, using the `insort` function. # Detailed Instructions 1. Import the necessary `bisect` module functions. 2. Define the `insert_student` function that takes `students` and `new_student` as parameters. 3. Use the appropriate `insort` function from the `bisect` module to insert the new student in the correct position to keep the list sorted by score. 4. Return the updated list of students. # Notes - Ensure the list remains sorted in ascending order of the students\' scores. - Consider edge cases where the list might be empty or where the new student\'s score is either the lowest or highest in the list. This question assesses the understanding of advanced list handling techniques in Python using the `bisect` module, ensuring efficient maintenance of sorted order in lists.","solution":"from bisect import insort def insert_student(students, new_student): Inserts a new student into the sorted list of students based on scores. Parameters: - students: List of tuples [(name, score), ...] sorted by score. - new_student: Tuple (name, score) representing a new student to insert. Returns: - Updated list of students sorted by score. insort(students, new_student, key=lambda x: x[1]) return students"},{"question":"# Question: Creating and Customizing Subplots with Seaborn **Objective:** Utilize the `seaborn.objects` package to create a customized set of subplots with specific layout properties and dimensions. **Task:** Write a function `create_custom_plot` that generates a 2x2 grid of subplots using the seaborn `facet` method. Each subplot should be given the following specifications: 1. The overall figure size should be set to 8x8 inches. 2. Use the `constrained` layout engine to ensure the subplots fit well within the given dimensions. 3. Adjust the size of the plot relative to the underlying figure such that it occupies the full figure space. **Function Signature:** ```python import seaborn.objects as so def create_custom_plot(): # Create the main plot object with the specified dimensions p = so.Plot().layout(size=(8, 8)) # Add the facet to create a 2x2 grid p = p.facet([\\"A\\", \\"B\\"], [\\"X\\", \\"Y\\"]).layout(engine=\\"constrained\\") # Adjust the relative size of the plot p.layout(extent=[0, 0, 1, 1]) # Display the plot p.show() ``` **Input:** None. **Output:** A visual plot displayed using seaborn with the specified configurations. **Constraints:** - Ensure that your code handles errors gracefully, providing clear messages if something goes wrong. - Utilize only the seaborn objects and methods highlighted in the provided documentation. **Example Usage:** ```python # Call the function to generate the plot create_custom_plot() ``` This function call should produce a displayed plot with a 2x2 grid and the specified layout properties.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(): # Create a sample DataFrame to use with seaborn (since the problem doesn\'t specify the data, we\'ll create some) import pandas as pd import numpy as np data = { \'A\': np.random.choice([\'Group 1\', \'Group 2\'], 100), \'B\': np.random.choice([\'Cat 1\', \'Cat 2\'], 100), \'X\': np.random.rand(100), \'Y\': np.random.rand(100) } df = pd.DataFrame(data) # Create the main plot object with the specified dimensions p = so.Plot(df, x=\'X\', y=\'Y\').layout(size=(8, 8)) # Add the facet to create a 2x2 grid p = p.facet(\\"A\\", \\"B\\").layout(engine=\\"constrained\\") # Adjust the relative size of the plot p.layout(extent=[0, 0, 1, 1]) # Display the plot p.show()"},{"question":"# Python Threading and Lock Management You are required to implement a function `multi_threaded_sum` that performs the following operations using the low-level `_thread` module to demonstrate your understanding of threading and lock management: 1. Create two threads that both attempt to increment a shared global counter a number of times (each thread should increment the counter by 1, 100,000 times). 2. Use a lock to ensure that the counter is incremented safely without any race conditions. 3. Wait for both threads to finish execution before returning the final value of the counter. Guidelines - You must use the `_thread` module to create and manage the threads. - Safely manage access to the shared counter using a lock. - The code should be able to handle thread-specific issues gracefully. Function signature ```python def multi_threaded_sum(): # Your implementation here # Example usage: result = multi_threaded_sum() print(\\"Final counter value:\\", result) # Expected output is 200000 ``` Constraints: - You should not use any other threading-related modules besides `_thread`. - Make sure to handle any potential exceptions (e.g., thread errors) appropriately. This exercise aims to assess your ability to work with low-level threading, manage resources effectively using locks, and ensure thread-safe operations.","solution":"import _thread import time # Initialize a global counter and a lock counter = 0 lock = _thread.allocate_lock() def increment_counter(): global counter for _ in range(100000): with lock: counter += 1 def multi_threaded_sum(): global counter counter = 0 # Reset counter for each function call try: # Create two threads that run the `increment_counter` function _thread.start_new_thread(increment_counter, ()) _thread.start_new_thread(increment_counter, ()) except Exception as e: print(f\\"Error: unable to start thread. {e}\\") # Wait for both threads to complete time.sleep(1) # Sleep for a short duration to ensure threads have finished return counter"},{"question":"# Python Coding Assessment Question Objective Implement a custom SMTP server using the `smtpd` module which processes incoming email messages and saves them to a file. Problem Statement You are required to implement a custom SMTP server that logs the received email messages into a file in a specified directory. The server should subclass `smtpd.SMTPServer` and override the `process_message` method. The email message should be saved into separate files with a unique name based on the current timestamp with a `.eml` extension. The first line of the file should contain the sender\'s email address (`mailfrom`), the second line should list the recipient addresses (`rcpttos`), and the subsequent lines should contain the email content. Class Specification 1. **CustomSMTPServer** - **Parameters:** - `localaddr` (tuple): A tuple containing the local address and port number, e.g., `(\'localhost\', 1025)`. - `remoteaddr` (tuple): A tuple containing the remote address and port number, can be `None`. - `directory` (str): The directory path where the email files will be saved. - **Method:** - `process_message(peer, mailfrom, rcpttos, data, **kwargs)`: This method should: - Create a unique filename based on the current timestamp, e.g., `email_<timestamp>.eml`. - Write the `mailfrom` on the first line, `rcpttos` on the second line, and `data` on the following lines of the file. - Save the file in the specified directory. - Return \\"250 Ok\\" on success. Constraints - The directory should already exist, and the server should have write permissions to that directory. - If `decode_data` is `True`, convert the `data` from the Unicode string to bytes before saving. - Ensure filenames are unique to prevent overwriting. Example Usage ```python if __name__ == \\"__main__\\": import os from smtpd import SMTPChannel as SMTPDChannel directory = \\"emails\\" if not os.path.exists(directory): os.makedirs(directory) server = CustomSMTPServer((\'localhost\', 1025), None, directory) try: asyncore.loop() except KeyboardInterrupt: pass ```","solution":"import smtpd import os import time class CustomSMTPServer(smtpd.SMTPServer): def __init__(self, localaddr, remoteaddr, directory): super().__init__(localaddr, remoteaddr) self.directory = directory def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): timestamp = time.strftime(\\"%Y%m%d%H%M%S\\") filename = f\\"email_{timestamp}.eml\\" filepath = os.path.join(self.directory, filename) with open(filepath, \'w\') as file: file.write(f\\"From: {mailfrom}n\\") file.write(f\\"To: {\', \'.join(rcpttos)}n\\") file.write(data) return \\"250 Ok\\" # SMTP code for success"},{"question":"Coding Assessment Question # Objective In this task, you will use the scikit-learn library to implement an unsupervised learning pipeline. You are required to segment a small dataset using a clustering algorithm and then visualize the results in a clear and informative manner. # Dataset You will work with the Iris dataset, which is available directly through scikit-learn. The dataset consists of 150 samples with four features each. It is commonly used for testing purposes and consists of three different classes of iris plants. # Task 1. **Load the Dataset**: Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Dimensionality Reduction**: Use Principal Component Analysis (PCA) from `sklearn.decomposition` to reduce the dimensionality of the dataset to 2 components. 3. **Clustering**: Apply the KMeans clustering algorithm from `sklearn.cluster` to the reduced dataset with three clusters. 4. **Visualization**: Visualize the clustering results using a scatter plot. Each data point should be colored according to its assigned cluster. # Requirements - Implement a function `cluster_and_visualize_iris()` which performs the steps outlined above. - **Input**: The function takes no input parameters. - **Output**: The function should generate a scatter plot with the clusters clearly marked. - Performance is not a key constraint for this task, but your code should be efficient and readable. # Example Function Signature ```python def cluster_and_visualize_iris(): pass ``` # Additional Notes - Use appropriate imports from the scikit-learn library. - Ensure that your scatter plot is well-labeled, with a title and axis labels. - You may use additional libraries such as matplotlib for visualization purposes. # Assessment Criteria - Correct loading and preprocessing of the Iris dataset. - Proper implementation of PCA for dimensionality reduction. - Application of the KMeans clustering algorithm. - Clarity and accuracy of the scatter plot visualization. - Code quality, including readability and proper use of scikit-learn functions.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA from sklearn.cluster import KMeans def cluster_and_visualize_iris(): # Load the Iris dataset iris = load_iris() X = iris.data # Apply PCA to reduce the dimensionality to 2 components pca = PCA(n_components=2) X_reduced = pca.fit_transform(X) # Apply KMeans clustering with 3 clusters kmeans = KMeans(n_clusters=3, random_state=42) y_kmeans = kmeans.fit_predict(X_reduced) # Plotting the results plt.figure(figsize=(10, 7)) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_kmeans, cmap=\'viridis\', s=50) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c=\'red\', marker=\'x\') plt.title(\'KMeans Clustering of Iris Dataset (PCA Reduced)\') plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.colorbar() plt.grid(True) plt.show()"},{"question":"# Custom Color Palette and Data Visualization with Seaborn You are given a dataset containing the sales data of a company over the last year. The dataset includes the following columns: - `month` (str): The month of the year (e.g., \\"January\\", \\"February\\", etc.). - `sales` (int): The total sales for that month. Your task is to create a line plot that visualizes this sales data over the months using a custom color palette created with `seaborn`. Your color palette should be a sequential ramp of your favorite color, and you should specify this color using a hex code. **Requirements:** 1. Create a sequential color palette using `seaborn.light_palette()` with your chosen color specified as a hex code. 2. Use this color palette to create a colormap. 3. Generate a line plot using `seaborn` with the `month` on the x-axis and `sales` on the y-axis. 4. Apply the custom colormap to this plot. 5. Ensure the plot is well-labeled and visually appealing. **Input:** - The dataset in the form of a pandas DataFrame. ```python import pandas as pd data = { \'month\': [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], \'sales\': [15000, 18000, 12000, 22000, 20000, 21000, 26000, 25000, 23000, 24000, 22000, 19000] } df = pd.DataFrame(data) ``` **Output:** - A line plot visualizing the sales data with the specified custom colormap. **Constraints:** - You must use the `seaborn` library to create the plot. - The color palette and colormap must be created using `seaborn.light_palette()`. **Function Signature:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(df: pd.DataFrame, hex_color: str) -> None: pass ``` # Example ```python data = { \'month\': [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], \'sales\': [15000, 18000, 12000, 22000, 20000, 21000, 26000, 25000, 23000, 24000, 22000, 19000] } df = pd.DataFrame(data) hex_color = \\"#79C\\" visualize_sales(df, hex_color) ``` *This should output a line plot with sales data and the custom colormap applied.*","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(df: pd.DataFrame, hex_color: str) -> None: Visualizes the sales data over the months using a custom color palette. Parameters: df (pd.DataFrame): The dataframe containing sales data with \'month\' and \'sales\' columns. hex_color (str): The hex code of the base color for the custom color palette. # Create a sequential color palette using the provided hex color palette = sns.light_palette(hex_color, as_cmap=True) # Plotting the data plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'sales\', palette=palette) # Enhancing the plot plt.title(\'Monthly Sales Data\', fontsize=16) plt.xlabel(\'Month\', fontsize=14) plt.ylabel(\'Sales\', fontsize=14) plt.xticks(rotation=45) plt.grid(True) plt.show()"},{"question":"**Problem Statement**: You are tasked with creating a utility to handle large text data files by compressing and decompressing them using the `lzma` module. You need to implement the following functions: 1. **compress_file(input_file, output_file, preset=6)**: - **Input**: - `input_file` (str): Path to the input text file. - `output_file` (str): Path to the output compressed file. - `preset` (int): Compression preset level, default is 6. - **Output**: None - **Description**: This function should read the content of `input_file`, compress it using the LZMA algorithm with the specified `preset` and save the compressed data to `output_file`. 2. **decompress_file(input_file, output_file)**: - **Input**: - `input_file` (str): Path to the input compressed file. - `output_file` (str): Path to the output text file. - **Output**: None - **Description**: This function should read the content of `input_file`, decompress it using the LZMA algorithm, and save the decompressed data to `output_file`. **Constraints**: - You should handle exceptions that may arise during file reading/writing and compression/decompression (e.g., `lzma.LZMAError`, file not found, etc.) - Assume the text file size can be very large, so ensure that your compression and decompression process is efficient. **Requirements**: - Use `lzma` module functions (`lzma.open`, `lzma.compress`, `lzma.decompress`) where appropriate. - The `preset` level for compression should be between 0 and 9 inclusive. **Example**: ```python # Example usage: compress_file(\\"large_text.txt\\", \\"compressed_file.xz\\", preset=7) decompress_file(\\"compressed_file.xz\\", \\"decompressed_text.txt\\") ``` **Note**: Ensure the decompressed file matches the original input file content when using a correct implement compression and decompression process.","solution":"import lzma from pathlib import Path def compress_file(input_file, output_file, preset=6): Compresses a text file using LZMA compression. Args: input_file (str): Path to the input text file. output_file (str): Path to the output compressed file. preset (int): Compression preset level, default is 6. try: with open(input_file, \'rb\') as f_in: data = f_in.read() compressed_data = lzma.compress(data, preset=preset) with open(output_file, \'wb\') as f_out: f_out.write(compressed_data) except Exception as e: print(f\\"An error occurred: {e}\\") def decompress_file(input_file, output_file): Decompresses an LZMA compressed file. Args: input_file (str): Path to the input compressed file. output_file (str): Path to the output text file. try: with open(input_file, \'rb\') as f_in: compressed_data = f_in.read() data = lzma.decompress(compressed_data) with open(output_file, \'wb\') as f_out: f_out.write(data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Advanced Object Manipulation and Analysis You are required to write a function that performs comprehensive manipulations and analyses on a combination of different Python objects. This will test your understanding of sequence (list, tuple) objects and container (dictionary, set) objects. Function Signature ```python def analyze_objects(data): Analyzes a complex input of sequence and container objects and returns a dictionary of analyses. Parameters: data (dict): A dictionary containing: - \'numbers\': a list of integers - \'texts\': a list of strings - \'pairs\': a list of tuples, each containing two integers - \'attributes\': a dictionary mapping strings to sets of integers Returns: dict: A dictionary with the following keys: - \'max_number\': the maximum number from the \'numbers\' list - \'min_number\': the minimum number from the \'numbers\' list - \'combined_text\': a single string containing all strings from the \'texts\' list - \'sum_pairs\': a list of sums of each tuple in \'pairs\' - \'attribute_analysis\': a dictionary mapping each attribute (key from \'attributes\') to the sum of its corresponding set of integers ``` Input - `data` is a dictionary with the following structure: - \'numbers\': A list of integers. - \'texts\': A list of strings. - \'pairs\': A list of tuples, with each tuple containing two integers. - \'attributes\': A dictionary that maps strings to sets of integers. Output - A dictionary with the following keys: - `max_number`: The maximum number from the \'numbers\' list. - `min_number`: The minimum number from the \'numbers\' list. - `combined_text`: A single string containing all strings from the \'texts\' list, concatenated. - `sum_pairs`: A list of sums of each tuple in \'pairs\'. - `attribute_analysis`: A dictionary mapping each attribute (key from \'attributes\') to the sum of the integers in its corresponding set. Example ```python data = { \'numbers\': [1, 3, 5, 7, 9], \'texts\': [\'hello\', \'world\'], \'pairs\': [(1, 2), (3, 4), (5, 6)], \'attributes\': { \'a\': {1, 2, 3}, \'b\': {4, 5, 6} } } output = analyze_objects(data) print(output) # Output should be: # { # \'max_number\': 9, # \'min_number\': 1, # \'combined_text\': \'helloworld\', # \'sum_pairs\': [3, 7, 11], # \'attribute_analysis\': { # \'a\': 6, # \'b\': 15 # } # } ``` Constraints - Each list in the dictionary will have at least one element. - The sets for \'attributes\' will have at least one element each. - String lengths and integer values will fit within typical usage ranges for Python. Ensure you handle these objects correctly, leveraging appropriate Python operations for each analysis required.","solution":"def analyze_objects(data): Analyzes a complex input of sequence and container objects and returns a dictionary of analyses. Parameters: data (dict): A dictionary containing: - \'numbers\': a list of integers - \'texts\': a list of strings - \'pairs\': a list of tuples, each containing two integers - \'attributes\': a dictionary mapping strings to sets of integers Returns: dict: A dictionary with the following keys: - \'max_number\': the maximum number from the \'numbers\' list - \'min_number\': the minimum number from the \'numbers\' list - \'combined_text\': a single string containing all strings from the \'texts\' list - \'sum_pairs\': a list of sums of each tuple in \'pairs\' - \'attribute_analysis\': a dictionary mapping each attribute (key from \'attributes\') to the sum of its corresponding set of integers max_number = max(data[\'numbers\']) min_number = min(data[\'numbers\']) combined_text = \'\'.join(data[\'texts\']) sum_pairs = [sum(pair) for pair in data[\'pairs\']] attribute_analysis = {key: sum(value) for key, value in data[\'attributes\'].items()} return { \'max_number\': max_number, \'min_number\': min_number, \'combined_text\': combined_text, \'sum_pairs\': sum_pairs, \'attribute_analysis\': attribute_analysis }"},{"question":"# Python C-API Simulation: Attribute and Item Manipulation You have been provided with a Python C-API that includes several functions for handling attributes and items in Python objects. Using the structures and logic outlined in the given documentation, you need to simulate (in pure Python) a basic subset of these functionalities. Implement a class `CustomObject` in Python which simulates the following operations: 1. **Attribute Manipulation**: - Checking if an attribute exists (`has_attr`). - Getting the value of an attribute (`get_attr`). - Setting or deleting an attribute (`set_attr`). 2. **Item Manipulation**: - Getting the value of an item by key (`get_item`). - Setting a value for an item by key (`set_item`). - Deleting an item by key (`del_item`). Ensure your implementation correctly handles cases where attributes or items do not exist, and proper exceptions are raised where applicable. Example Usage: ```python obj = CustomObject() # Setting and getting attributes obj.set_attr(\'name\', \'Python\') print(obj.get_attr(\'name\')) # Output: Python # Checking attribute existence print(obj.has_attr(\'name\')) # Output: True print(obj.has_attr(\'age\')) # Output: False # Deleting an attribute obj.set_attr(\'age\', 30) obj.set_attr(\'age\', None) # This should delete the attribute print(obj.has_attr(\'age\')) # Output: False # Setting and getting items obj.set_item(\'key1\', \'value1\') print(obj.get_item(\'key1\')) # Output: value1 # Deleting items obj.set_item(\'key2\', \'value2\') obj.del_item(\'key2\') print(obj.get_item(\'key2\')) # Should raise KeyError ``` Function Signatures: ```python class CustomObject: def __init__(self): pass def has_attr(self, attr_name: str) -> bool: pass def get_attr(self, attr_name: str) -> any: pass def set_attr(self, attr_name: str, value: any) -> None: pass def get_item(self, key: any) -> any: pass def set_item(self, key: any, value: any) -> None: pass def del_item(self, key: any) -> None: pass ``` Constraints: - Do not use Python\'s built-in `getattr`, `setattr`, or `delattr` for attribute manipulation; instead, directly manipulate the object\'s dictionary. - Raise appropriate exceptions (`AttributeError`, `KeyError`) where needed following standard Python practices. Performance: The implementation should be efficient and handle a moderate number of attributes and items.","solution":"class CustomObject: def __init__(self): self.__dict__[\'attributes\'] = {} self.__dict__[\'items\'] = {} def has_attr(self, attr_name: str) -> bool: return attr_name in self.attributes def get_attr(self, attr_name: str) -> any: if attr_name in self.attributes: return self.attributes[attr_name] else: raise AttributeError(f\\"Attribute \'{attr_name}\' not found\\") def set_attr(self, attr_name: str, value: any) -> None: if value is None: if attr_name in self.attributes: del self.attributes[attr_name] else: self.attributes[attr_name] = value def get_item(self, key: any) -> any: if key in self.items: return self.items[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def set_item(self, key: any, value: any) -> None: self.items[key] = value def del_item(self, key: any) -> None: if key in self.items: del self.items[key] else: raise KeyError(f\\"Key \'{key}\' not found\\")"},{"question":"# Custom Operator with Gradient Check in PyTorch Objective: You are required to create a custom operation using PyTorch\'s `torch.library` module. This operation will perform element-wise multiplication and addition on two tensors. Your task is to: 1. Define and register the custom operation. 2. Implement the gradient check for the custom operation to ensure that the gradients are mathematically correct. Requirements: 1. **Custom Operation Definition**: - Define a custom operator called `custom_mul_add`. - This operator should take two input tensors and compute `result = x * y + y`. 2. **Registration and Implementation**: - Register the custom operator and implement the necessary computation logic. - Ensure that the operator works both for forward and backward passes. 3. **Gradient Check**: - Use `torch.autograd.gradcheck` to verify the correctness of the gradients for the defined operation. Constraints: - Inputs will always be 1-D tensors of the same size. - Inputs can contain both positive and negative values. - Ensure the implementation is efficient in terms of both time and space. Input Format: - Two 1-D tensors of the same size. Output Format: - The output should be the result of the operation as a tensor. - A boolean value indicating if the gradient check passed or failed. Example: ```python import torch import torch.autograd from torch.library import custom_op, register_kernel, opcheck # Define the custom operation @custom_op(\'custom::mul_add\') def custom_mul_add(x, y): pass # Register the kernel for the custom operation @register_kernel(\'custom::mul_add\') def custom_mul_add_kernel(x, y): return x * y + y # Implement the gradient function for the custom operation @register_kernel(\'custom::mul_add_backward\') def custom_mul_add_backward(grad_output, x, y): grad_x = grad_output * y grad_y = grad_output * (x + 1) return grad_x, grad_y # Test function to check correctness and gradients def test_custom_mul_add(): x = torch.tensor([2.0, 3.0], requires_grad=True) y = torch.tensor([4.0, 1.0], requires_grad=True) z = custom_mul_add(x, y) assert z.equal(torch.tensor([12.0, 4.0])) # Gradient check grad_check = torch.autograd.gradcheck(custom_mul_add, (x, y)) return z, grad_check # Run your test output, grad_status = test_custom_mul_add() print(\'Output:\', output) print(\'Gradient Check Passed:\', grad_status) ``` Your implementation should match the above template and ensure all criteria are met. All necessary imports should be correctly handled within the provided code structure.","solution":"import torch from torch.autograd import Function class CustomMulAddFunction(Function): @staticmethod def forward(ctx, x, y): Forward pass: calculate x * y + y ctx.save_for_backward(x, y) return x * y + y @staticmethod def backward(ctx, grad_output): Backward pass: calculate gradients dL/dx and dL/dy x, y = ctx.saved_tensors grad_x = grad_output * y grad_y = grad_output * (x + 1) return grad_x, grad_y def custom_mul_add(x, y): Applies the custom multiplication and addition operation return CustomMulAddFunction.apply(x, y) def test_custom_operation_with_grad_check(): # Tensors for testing x = torch.randn(3, requires_grad=True, dtype=torch.double) y = torch.randn(3, requires_grad=True, dtype=torch.double) # Running gradient check grad_status = torch.autograd.gradcheck(custom_mul_add, (x, y)) return grad_status"},{"question":"Objective: To assess the understanding of the `wsgiref` package and WSGI concepts by implementing a small WSGI-based web application that processes URL paths and serves dynamic responses. Question: Create a WSGI application that acts as a simple web server. The server should: 1. Serve a dynamic \\"Hello, {name}!\\" response when accessed via the URL `/hello/{name}`. The `{name}` part of the URL should be extracted and used to personalize the response. 2. Serve static files from a directory called `static` located in the current directory. If a file is not found, return a \\"404 Not Found\\" response. 3. Use the `wsgiref.util.shift_path_info` function to process the URL path. Requirements: - The WSGI application must use `wsgiref.util.shift_path_info` to process the URL path and extract the `{name}` part. - A simple dispatch mechanism should be used to serve requests to either the dynamic or static handler. - Implement the application such that: - URL paths starting with `/hello/` trigger the dynamic handler. - Any other URL paths attempt to serve a file from the `static` directory. - Use a basic WSGI server to test your application (e.g., from `wsgiref.simple_server`). Constraints: - The application should be capable of handling various cases for the `{name}` part, including special characters and spaces. - Performance is not a major concern for this task, but your implementation should be reasonably efficient. Input and Output Formats: - **Input**: HTTP requests to the server. - **Output**: HTTP responses including appropriate status codes and headers. Example: 1. Request: `GET /hello/Alice` - Response: `Hello, Alice!` 2. Request: `GET /static/example.txt` (Assuming `example.txt` exists in the `static` directory) - Response: Contents of `example.txt` 3. Request: `GET /static/nonexistent.txt` (Assuming `nonexistent.txt` does not exist in the `static` directory) - Response: `404 Not Found` Base Code: ```python from wsgiref.simple_server import make_server from wsgiref.util import shift_path_info import os def application(environ, start_response): path_info = environ.get(\'PATH_INFO\', \'\') script_name = environ.get(\'SCRIPT_NAME\', \'\') # Shift path info to handle /hello/{name} shifted_name = shift_path_info(environ) if script_name == \'hello\': # Dynamic handler if shifted_name: response_body = f\\"Hello, {shifted_name}!\\".encode(\'utf-8\') start_response(\'200 OK\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [response_body] else: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [b\'Name not provided\'] else: # Static file handler file_path = os.path.join(\'static\', script_name) if os.path.isfile(file_path): with open(file_path, \'rb\') as f: response_body = f.read() start_response(\'200 OK\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [response_body] else: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [b\'File not found\'] if __name__ == \'__main__\': with make_server(\'\', 8000, application) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Complete the implementation of the `application` function to satisfy the requirements.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import shift_path_info import os def application(environ, start_response): # Initialize the shifted path and script name remaining_path = environ.get(\'PATH_INFO\', \'\') script_name = shift_path_info(environ) if script_name == \'hello\': # Shift the path to handle /hello/{name} name = shift_path_info(environ) if name: response_body = f\\"Hello, {name}!\\".encode(\'utf-8\') start_response(\'200 OK\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [response_body] else: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [b\'Name not provided\'] else: # Static file handler file_path = os.path.join(\'static\', script_name) if os.path.isfile(file_path): with open(file_path, \'rb\') as f: response_body = f.read() start_response(\'200 OK\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [response_body] else: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain; charset=utf-8\')]) return [b\'File not found\'] if __name__ == \'__main__\': with make_server(\'\', 8000, application) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"**Objective:** Implement a function that performs a series of set operations using the Python C API for sets and frozensets. **Problem Statement:** You are required to implement a function in Python that interacts with set and frozenset objects using provided C API functions. The function should be able to create sets, add elements, check membership, remove elements, and clear the set. The operations are provided as commands in a list. Your task is to process each command sequentially and produce the expected output for each. **Function Signature:** ```python def process_set_operations(commands: List[Tuple[str, List]]) -> List: # Your implementation here ``` **Input:** - `commands` (List[Tuple[str, List]]): A list of tuples where each tuple represents a command and its arguments. - The command is a string which can be one of the following: \\"create_set\\", \\"create_frozenset\\", \\"add\\", \\"contains\\", \\"discard\\", \\"pop\\", \\"clear\\". - The arguments are the parameters required for each command. **Output:** - A list containing the results of the commands in sequence. For commands that produce a result (like `contains`, `pop`), include the result in the list. For commands that do not produce a result (like `add`, `discard`, `clear`), include a confirmation string, e.g., \\"Success\\". **Constraints:** - For `create_set` and `create_frozenset`, the argument is an iterable of items to be included in the set or frozenset. - For `add`, `contains`, and `discard`, the argument is the element to be operated on. - For `pop` and `clear`, there are no arguments. **Example:** ```python commands = [ (\\"create_set\\", [[1, 2, 3]]), (\\"add\\", [4]), (\\"contains\\", [3]), (\\"discard\\", [2]), (\\"pop\\", []), (\\"clear\\", []) ] output = process_set_operations(commands) print(output) # Example output: [\'Success\', \'Success\', True, \'Success\', 1, \'Success\'] ``` **Implementation Requirements:** 1. Use `PySet_New()` and `PyFrozenSet_New()` to create new set and frozenset objects. 2. Use `PySet_Add()` to add an element to a set. 3. Use `PySet_Contains()` to check for membership in a set. 4. Use `PySet_Discard()` to remove an element from a set. 5. Use `PySet_Pop()` to remove and return an arbitrary element from a set. 6. Use `PySet_Clear()` to clear all elements from a set. **Performance Requirements:** - The solution should handle at least 10^5 operations efficiently. Be sure to handle any errors appropriately, and ensure memory is managed correctly to prevent leaks. **Note:** This question requires understanding of both Python and C APIs, and requires creating custom wrappers or using `ctypes` or `cffi` for C interaction.","solution":"def process_set_operations(commands): from types import FunctionType result = [] sets = [] for command, args in commands: if command == \\"create_set\\": sets.append(set(args[0])) result.append(\\"Success\\") elif command == \\"create_frozenset\\": sets.append(frozenset(args[0])) result.append(\\"Success\\") elif command == \\"add\\": if isinstance(sets[-1], set): sets[-1].add(args[0]) result.append(\\"Success\\") else: result.append(\\"Error: Cannot modify a frozenset\\") elif command == \\"contains\\": result.append(args[0] in sets[-1]) elif command == \\"discard\\": if isinstance(sets[-1], set): sets[-1].discard(args[0]) result.append(\\"Success\\") else: result.append(\\"Error: Cannot modify a frozenset\\") elif command == \\"pop\\": if isinstance(sets[-1], set): try: result.append(sets[-1].pop()) except KeyError: result.append(\\"Error: Empty set\\") else: result.append(\\"Error: Cannot modify a frozenset\\") elif command == \\"clear\\": if isinstance(sets[-1], set): sets[-1].clear() result.append(\\"Success\\") else: result.append(\\"Error: Cannot modify a frozenset\\") return result"},{"question":"# Problem: Custom Logging and Thread Execution You have been tasked with creating a logging system for a multi-threaded application that processes a series of text files. The log should capture messages about the status and results of file processing. Requirements 1. **Logging Configuration**: - Configure a logger to write logs to a file named `app.log`. - The log should include the timestamp, log level, and message. 2. **Threaded File Processing**: - Implement a `TextProcessor` class that inherits from `threading.Thread`. - The `TextProcessor` should: - Take a list of filenames and a logger as input. - For each file, read its contents, log a message indicating the start and end of processing, and convert all text to uppercase. - Log an error message if any file is not found. 3. **Main Program**: - Create multiple `TextProcessor` threads to process a list of text files concurrently. - Ensure the main program waits for all threads to complete before exiting. Input 1. A list of filenames to be processed. 2. Number of threads to be used. Output - The log file `app.log` with timestamped messages about file processing. Constraints - Handle FileNotFoundError gracefully and log appropriate error messages. Example Suppose we have three text files: `file1.txt`, `file2.txt`, and `file3.txt`, and we want to process them using two threads. The log file should reflect the activities of both threads: ``` app.log: 2023-10-07 10:00:00,000 - INFO - Thread-1: Starting processing file1.txt 2023-10-07 10:00:01,000 - INFO - Thread-1: Finished processing file1.txt 2023-10-07 10:00:01,000 - INFO - Thread-2: Starting processing file2.txt 2023-10-07 10:00:02,000 - ERROR - Thread-2: file2.txt not found 2023-10-07 10:00:02,000 - INFO - Thread-1: Starting processing file3.txt 2023-10-07 10:00:03,000 - INFO - Thread-1: Finished processing file3.txt ``` Implementation Details - Use the `logging` module to set up the logger. - Utilize the `threading.Thread` class to create the processing threads. - Ensure thread-safe logging. - Gracefully handle exceptions and ensure all threads complete. **Note**: The actual log timestamps will vary based on execution time. ```python import logging import threading import os # Step 1: Configure the logger def setup_logger(): logger = logging.getLogger(\'file_processor\') logger.setLevel(logging.INFO) handler = logging.FileHandler(\'app.log\') formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) return logger # Step 2: Implement TextProcessor class class TextProcessor(threading.Thread): def __init__(self, filenames, logger): threading.Thread.__init__(self) self.filenames = filenames self.logger = logger def run(self): for filename in self.filenames: try: self.logger.info(f\'Thread-{self.ident}: Starting processing {filename}\') with open(filename, \'r\') as file: content = file.read().upper() with open(filename, \'w\') as file: file.write(content) self.logger.info(f\'Thread-{self.ident}: Finished processing {filename}\') except FileNotFoundError: self.logger.error(f\'Thread-{self.ident}: {filename} not found\') # Step 3: Main function to create and manage threads def main(filenames, num_threads): logger = setup_logger() threads = [] for i in range(num_threads): thread_filenames = filenames[i::num_threads] thread = TextProcessor(thread_filenames, logger) thread.start() threads.append(thread) for thread in threads: thread.join() # Example usage if __name__ == \'__main__\': filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] num_threads = 2 main(filenames, num_threads) ```","solution":"import logging import threading import os # Step 1: Configure the logger def setup_logger(): logger = logging.getLogger(\'file_processor\') logger.setLevel(logging.INFO) handler = logging.FileHandler(\'app.log\') formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) return logger # Step 2: Implement TextProcessor class class TextProcessor(threading.Thread): def __init__(self, filenames, logger): threading.Thread.__init__(self) self.filenames = filenames self.logger = logger def run(self): for filename in self.filenames: try: self.logger.info(f\'Thread-{self.ident}: Starting processing {filename}\') with open(filename, \'r\') as file: content = file.read().upper() with open(filename, \'w\') as file: file.write(content) self.logger.info(f\'Thread-{self.ident}: Finished processing {filename}\') except FileNotFoundError: self.logger.error(f\'Thread-{self.ident}: {filename} not found\') # Step 3: Main function to create and manage threads def main(filenames, num_threads): logger = setup_logger() threads = [] for i in range(num_threads): thread_filenames = filenames[i::num_threads] thread = TextProcessor(thread_filenames, logger) thread.start() threads.append(thread) for thread in threads: thread.join() # Example usage if __name__ == \'__main__\': filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] num_threads = 2 main(filenames, num_threads)"},{"question":"**Coding Assessment Question: Asynchronous Computation with Streams in PyTorch** In this task, you are required to demonstrate your understanding of stream and device management in PyTorch, using the `torch.mtia` module. # Problem Statement: Implement a function `perform_asynchronous_computation` that distributes a series of tensor computations across available devices using different streams for asynchronous execution. Specifically, you will: 1. Initialize the MTIA backend. 2. Check the availability of devices. 3. For each available device, perform the following steps: - Create two streams: one for computation and one for data transfer. - Transfer a tensor to the device using the data transfer stream. - Perform a computation (e.g., matrix multiplication) on the tensor using the computation stream. - Create an event to record the completion of the computation. - Synchronize the streams to ensure proper ordering of operations. 4. Collect the results from all devices and return them to the host. # Input: - A list of tensors, each to be processed on a separate device. - The size of the matrices for matrix multiplication. # Output: - A list of tensors, each containing the result of the matrix multiplication from the corresponding device. # Constraints: - Ensure that the MTIA backend is available and initialized. - Use streams to handle asynchronous data transfer and computation. - Synchronize streams correctly to avoid race conditions. # Function Signature: ```python import torch import torch.mtia as mtia def perform_asynchronous_computation(tensors, matrix_size): Perform asynchronous tensor computations across multiple devices Parameters: tensors (list of torch.Tensor): List of input tensors to be distributed across devices. matrix_size (int): The size of the square matrix for the matrix multiplication. Returns: list of torch.Tensor: List of result tensors from each device. ``` # Example: ```python input_tensors = [torch.rand(3, 4), torch.rand(3, 4)] matrix_size = 4 results = perform_asynchronous_computation(input_tensors, matrix_size) for result in results: print(result) ``` # Notes: 1. Use `mtia.device_count()` to determine the number of available devices. 2. Use `mtia.set_device()` to set the active device. 3. Utilize `mtia.Stream` to create and manage streams. 4. Use `mtia.synchronize()` to ensure proper sequencing of operations. This question tests the understanding of managing device contexts, streams, and asynchronous computations, which are crucial for efficient execution of high-performance GPU tasks in PyTorch.","solution":"import torch import torch.mtia as mtia def perform_asynchronous_computation(tensors, matrix_size): Perform asynchronous tensor computations across multiple devices Parameters: tensors (list of torch.Tensor): List of input tensors to be distributed across devices. matrix_size (int): The size of the square matrix for the matrix multiplication. Returns: list of torch.Tensor: List of result tensors from each device. # Initialize the MTIA backend if not mtia.is_available(): raise RuntimeError(\'MTIA backend is not available.\') device_count = mtia.device_count() if device_count == 0: raise RuntimeError(\'No MTIA devices available.\') # Ensure there are enough devices for the input tensors if len(tensors) > device_count: raise ValueError(\'Number of tensors exceeds number of available MTIA devices.\') results = [] for i, tensor in enumerate(tensors): # Set the current device mtia.set_device(i) # Create streams for computation and data transfer compute_stream = mtia.Stream() transfer_stream = mtia.Stream() # Transfer the tensor to the device using the data transfer stream with mtia.device(i): with transfer_stream: device_tensor = tensor.to(device=i, non_blocking=True) # Perform a computation (e.g., matrix multiplication) on the tensor using the computation stream with mtia.device(i): with compute_stream: # Create a random matrix for multiplication matrix = torch.randn(matrix_size, matrix_size, device=device_tensor.device) result = torch.matmul(device_tensor, matrix) # Create an event to record the completion of the computation event = mtia.Event() with compute_stream: event.record() # Wait for the computation stream to finish event.synchronize() # Move the result back to the host with mtia.device(i): with transfer_stream: host_result = result.cpu() results.append(host_result) return results"},{"question":"**Question: Implement a function to manipulate memoryview objects** You are tasked with implementing a function that reads a large binary data file and performs some operations on it using Python\'s `memoryview` objects. The function should demonstrate the use of `memoryview` for efficient memory manipulation. **Function Signature:** ```python def manipulate_memoryview(file_path: str, start: int, length: int, new_value: bytes) -> bytes: Reads a binary file, manipulates a specified segment using memoryview, and returns the modified segment. Parameters: - file_path (str): The path to the binary file to read. - start (int): The starting index from where to read and modify the data. - length (int): The length of the segment to read and modify. - new_value (bytes): The new value to write into the specified segment. Returns: - bytes: The modified segment of data. Constraints: - The file must be large enough to include the specified segment (start to start + length). - The length of new_value must not exceed the specified length. - The segment to modify must be writable. # Your code here ``` **Example:** ```python # Assume \'example.bin\' is a binary file with at least 100 bytes of data result = manipulate_memoryview(\'example.bin\', 10, 20, b\'new data\') print(result) ``` The function should: - Open the binary file in read-write mode. - Use the `memoryview` to read and manipulate the specified segment of data. - Return the modified segment as bytes. Considerations: - Handle file operations gracefully, ensuring the file is properly closed after modifications. - Ensure that the provided `new_value` length does not exceed the specified segment length to avoid buffer overflows. - Keep performance considerations in mind by leveraging `memoryview`\'s ability to handle large data efficiently.","solution":"def manipulate_memoryview(file_path: str, start: int, length: int, new_value: bytes) -> bytes: Reads a binary file, manipulates a specified segment using memoryview, and returns the modified segment. Parameters: - file_path (str): The path to the binary file to read. - start (int): The starting index from where to read and modify the data. - length (int): The length of the segment to read and modify. - new_value (bytes): The new value to write into the specified segment. Returns: - bytes: The modified segment of data. Constraints: - The file must be large enough to include the specified segment (start to start + length). - The length of new_value must not exceed the specified length. - The segment to modify must be writable. if len(new_value) > length: raise ValueError(\\"new_value length exceeds the specified segment length.\\") with open(file_path, \'r+b\') as f: f.seek(start) segment = f.read(length) mem = memoryview(bytearray(segment)) mem[:len(new_value)] = new_value f.seek(start) f.write(mem.tobytes()) return mem.tobytes()"},{"question":"# Python Coding Assessment Question Background The Python buffer protocol provides a way for objects to expose raw byte arrays to other Python objects. This protocol is crucial for high-performance tasks where direct memory access is necessary. Understanding and implementing the buffer protocol is a key skill for advanced Python programming. Task You are required to implement a function `extract_diagonal` that takes a buffer object (which supports the buffer interface) and returns a list containing the diagonal elements of the buffer, assuming the buffer represents a 2D square matrix (i.e., the number of rows is equal to the number of columns). Function Signature ```python def extract_diagonal(buffer_object: Any) -> List: pass ``` Input - `buffer_object` (Any): An object that supports the buffer protocol and represents a 2D square matrix. Output - List: A list containing the diagonal elements from the matrix represented by the buffer object. Constraints - The buffer object is guaranteed to represent a 2D square matrix. - The buffer object will be created in C-contiguous layout. Example ```python import numpy as np # Example setup: creating a 2D square matrix using numpy matrix = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ], dtype=np.int32) # Convert the numpy array to a buffer object buffer_object = memoryview(matrix) # Function call print(extract_diagonal(buffer_object)) ``` Expected Output ``` [1, 5, 9] ``` Hints 1. Use the `PyObject_GetBuffer` equivalent in Python (like the `memoryview` object) to interact with the buffer. 2. Carefully handle different buffer request flags to ensure that you obtain a contiguous and correctly formatted buffer. Notes To solve this problem, you need to: - Obtain a view of the buffer. - Access the buffer’s shape and strides to correctly index into the diagonal elements. - Ensure you release the buffer when done to avoid resource leaks. Good luck!","solution":"def extract_diagonal(buffer_object): Extracts the diagonal elements from a buffer object representing a 2D square matrix. Args: buffer_object (Any): An object that supports the buffer protocol and represents a 2D square matrix. Returns: List: A list containing the diagonal elements from the matrix represented by the buffer object. # Creating a memoryview from the buffer_object to handle the buffer protocol mv = memoryview(buffer_object) # Getting the shape of the matrix shape = mv.shape # Assuring the matrix is square if len(shape) != 2 or shape[0] != shape[1]: raise ValueError(\\"The buffer object must represent a 2D square matrix\\") # Extracting the diagonal elements diagonal = [] for i in range(shape[0]): diagonal.append(mv[i, i]) # Release the memoryview mv.release() return diagonal"},{"question":"# Asynchronous Web Scraper **Objective**: Implement an asynchronous web scraper that fetches data from multiple URLs concurrently and processes the fetched data without blocking the event loop. **Requirements**: 1. Create an asynchronous function `fetch_url` that takes a URL and fetches its content using an HTTP request. 2. Create a second asynchronous function `process_data` that processes the fetched data (simulate processing with an `asyncio.sleep`). 3. Create a main function `async_scraper` that performs the following: - Takes a list of URLs. - Uses asyncio to fetch all URLs concurrently. - Processes the data fetched from each URL concurrently, ensuring the event loop is never blocked. - Handles any exceptions that occur during fetching or processing. # Input - A list of URLs (strings). # Output - Return a dictionary where the keys are URLs and the values are the processing results. # Constraints - Handle a maximum of 10 concurrent fetches at any given time. # Performance Requirements - Ensure that the implementation makes efficient use of the event loop by utilizing asynchronous programming principles. # Example ```python import asyncio async def fetch_url(url): # Simulate fetching data from URL await asyncio.sleep(1) return f\\"Fetched data from {url}\\" async def process_data(data): # Simulate data processing await asyncio.sleep(1) return f\\"Processed data: {data}\\" async def async_scraper(urls): # Implement the asynchronous scraper pass # URLs to scrape urls = [ \\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\" ] # Run the scraper results = asyncio.run(async_scraper(urls)) print(results) ``` Expected Output: ```python { \\"http://example.com/1\\": \\"Processed data: Fetched data from http://example.com/1\\", \\"http://example.com/2\\": \\"Processed data: Fetched data from http://example.com/2\\", \\"http://example.com/3\\": \\"Processed data: Fetched data from http://example.com/3\\", \\"http://example.com/4\\": \\"Processed data: Fetched data from http://example.com/4\\", \\"http://example.com/5\\": \\"Processed data: Fetched data from http://example.com/5\\", } ``` **Notes**: - Remember to implement exception handling for any potential issues that could arise during fetching or processing. - Ensure your code adheres to asyncio best practices and does not block the event loop.","solution":"import asyncio import aiohttp async def fetch_url(url): Fetch content from the given URL. async with aiohttp.ClientSession() as session: try: async with session.get(url) as response: data = await response.text() return data except Exception as e: return str(e) async def process_data(data): Process the fetched data (simulate processing with asyncio.sleep). await asyncio.sleep(1) # Simulate processing time return f\\"Processed data: {data}\\" async def async_scraper(urls): Perform asynchronous web scraping and data processing. async def bound_fetch(sem, url): async with sem: content = await fetch_url(url) return (url, content) async def bound_process(sem, content_tuple): url, content = content_tuple async with sem: result = await process_data(content) return (url, result) sem = asyncio.Semaphore(10) fetch_tasks = [bound_fetch(sem, url) for url in urls] raw_data = await asyncio.gather(*fetch_tasks) process_tasks = [bound_process(sem, raw_data_item) for raw_data_item in raw_data] processed_data = await asyncio.gather(*process_tasks) return dict(processed_data)"},{"question":"# Question: Binary-ASCII Conversion with `binascii` You are tasked with creating a function that will convert data between different binary and ASCII-encoded binary representations using the `binascii` module. Your goal is to implement a function that can handle multiple encoding/decoding operations in sequence for a given input. Function Signature ```python def binary_ascii_converter(data: bytes, operations: list) -> bytes: Convert data between multiple binary and ASCII-encoded binary representations. Parameters: data (bytes): The initial data to be converted. operations (list): A list of tuples where each tuple represents an operation. The first element of the tuple is a string indicating the operation type (\'a2b\' or \'b2a\'), and the second element is the encoding type (\'uu\', \'base64\', \'qp\', \'hex\'). Returns: bytes: The final converted data after all operations. pass ``` Input - `data` (bytes): The initial binary data to be converted. - `operations` (list): A list of tuples specifying the sequence of operations. Each tuple has the format `(operation, encoding)`, where: - `operation` is either `\'a2b\'` for ASCII to binary conversion or `\'b2a\'` for binary to ASCII conversion. - `encoding` specifies the encoding type, which can be any of the following: `\'uu\'`, `\'base64\'`, `\'qp\'`, `\'hex\'`. Output - The function should return the final `bytes` object after applying all specified operations in sequence. Constraints - The `data` input will always be valid for the specified initial operation. - The length of the `operations` list will be between 1 and 10. - The function should handle various constraints and edge cases gracefully, such as empty data for certain operations. Example ```python # Initial data is a binary string data = b\'hello world\' # Sequence of operations: First base64 encode, then decode from base64 operations = [ (\'b2a\', \'base64\'), (\'a2b\', \'base64\') ] # Expected output is the original \'hello world\' in bytes assert binary_ascii_converter(data, operations) == b\'hello world\' ``` ```python # Initial data is a binary string data = b\'Python 3.10\' # Sequence of operations: Encode to base64, then to uu, then back to base64, and finally decode from base64 operations = [ (\'b2a\', \'base64\'), (\'b2a\', \'uu\'), (\'a2b\', \'uu\'), (\'a2b\', \'base64\') ] # Expected output is the original \'Python 3.10\' in bytes assert binary_ascii_converter(data, operations) == b\'Python 3.10\' ``` Implement the `binary_ascii_converter` function to pass the provided examples.","solution":"import binascii def binary_ascii_converter(data: bytes, operations: list) -> bytes: Convert data between multiple binary and ASCII-encoded binary representations. Parameters: data (bytes): The initial data to be converted. operations (list): A list of tuples where each tuple represents an operation. The first element of the tuple is a string indicating the operation type (\'a2b\' or \'b2a\'), and the second element is the encoding type (\'uu\', \'base64\', \'qp\', \'hex\'). Returns: bytes: The final converted data after all operations. for operation, encoding in operations: if operation == \'b2a\': # Binary to ASCII if encoding == \'uu\': data = binascii.b2a_uu(data) elif encoding == \'base64\': data = binascii.b2a_base64(data).rstrip() elif encoding == \'qp\': data = binascii.b2a_qp(data) elif encoding == \'hex\': data = binascii.b2a_hex(data) elif operation == \'a2b\': # ASCII to Binary if encoding == \'uu\': data = binascii.a2b_uu(data) elif encoding == \'base64\': data = binascii.a2b_base64(data) elif encoding == \'qp\': data = binascii.a2b_qp(data) elif encoding == \'hex\': data = binascii.a2b_hex(data) return data"},{"question":"# Advanced Pytorch Special Functions Assessment **Objective**: Implement a function using PyTorch\'s `torch.special` module to solve a mathematical problem involving special functions. **Description**: You are expected to write a function that calculates the sum of two special functions, the Airy Ai function and the zero-order Bessel function of the first kind. # Function Signature ```python def special_sum(tensor: torch.Tensor) -> torch.Tensor: pass ``` # Input - `tensor` (torch.Tensor): A 1-dimensional tensor of floating-point numbers. # Output - Returns a tensor containing the element-wise sum of `torch.special.airy_ai` and `torch.special.bessel_j0` applied to the input tensor. # Constraints - The input tensor will have at least 1 element but no more than 1000 elements. - The values in the tensor will be real numbers within the range `[-10, 10]`. # Example ```python import torch input_tensor = torch.tensor([0.5, 1.5, -0.5]) output_tensor = special_sum(input_tensor) print(output_tensor) # Expected output tensor after applying the two special functions and summing them element-wise. ``` **Note**: - Make sure you handle the edge cases where the values might be on the boundary of the specified range. - Your solution should efficiently handle the operations considering the given constraints. # Additional Information - You can refer to the pytorch special module documentation for details about `airy_ai` and `bessel_j0` functions. - The special functions can be found within the `torch.special` module as `torch.special.airy_ai` and `torch.special.bessel_j0`.","solution":"import torch def special_sum(tensor: torch.Tensor) -> torch.Tensor: Returns a tensor containing the element-wise sum of torch.special.airy_ai and torch.special.bessel_j0 applied to the input tensor. airy_ai_values = torch.special.airy_ai(tensor) bessel_j0_values = torch.special.bessel_j0(tensor) return airy_ai_values + bessel_j0_values"},{"question":"**Objective:** Demonstrate proficiency in using the `seaborn` library to visualize complex datasets by creating line plots with varied aesthetics. **Problem Statement:** Using the provided `fmri` dataset from `seaborn`, create a detailed line plot that visualizes changes in the brain signal over time, segmented by the type of event and brain region. The plot should provide clear visual distinctions between these segments, including error bands, different line styles, and marker usage. Task: 1. Load the `fmri` dataset from `seaborn`. 2. Create a line plot with the following requirements: - The x-axis should represent `timepoint`. - The y-axis should represent `signal`. - Use the `hue` parameter to differentiate between `region`. - Use the `style` parameter to differentiate between `event`. - The plot should display markers for different `events`. - Include error bands representing the standard error of the mean. - Use a custom palette that includes at least three colors of your choosing. 3. Customize the plot with appropriate labels, a title, and a legend to make it informative and visually appealing. 4. Ensure the plot is displayed using `matplotlib.pyplot`. **Hint:** Refer to the `seaborn.lineplot()` and `sns.relplot()` functions to accomplish this task. Explore customizing the plot with `markers=True` and `err_style`. # Expected Input and Output Formats Input: None (data is loaded within the function) Output: A `matplotlib` plot displayed inline that meets the given requirements. Constraints: - Utilize the methods provided in the `seaborn` package. - The plot should be informative, meaning all axes, legends, and markers should be clearly visible. # Example Code Block: ```python import seaborn as sns import matplotlib.pyplot as plt # Ensure to follow the task requirements def plot_fmri_data(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot with the specified requirements sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, err_style=\\"band\\", ci=\\"sd\\", palette=\\"muted\\" ) # Set plot title and labels plt.title(\'Brain Signal Changes over Time by Region and Event\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Display the plot plt.show() # Call the function to generate the plot plot_fmri_data() ``` **Note:** The provided example meets the basic requirements but is intentionally minimalistic. Focus on further customization and bringing clarity to your final plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_fmri_data(): Creates a detailed line plot visualizing changes in the brain signal over time, segmented by event type and brain region. # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Define a custom palette with at least three colors custom_palette = [\\"#1f77b4\\", \\"#ff7f0e\\", \\"#2ca02c\\"] # Create the line plot with the specified requirements sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, err_style=\\"band\\", ci=\\"sd\\", palette=custom_palette ) # Set plot title and labels plt.title(\'Brain Signal Changes over Time by Region and Event\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region / Event Type\') # Display the plot plt.show() # Call the function to generate the plot plot_fmri_data()"},{"question":"# Question: Sequence Operations Manipulation **Objective:** Your task is to implement a Python function that performs a series of operations on a given sequence of integers. The function should accept a list of integers, perform specific transformations, and return the resulting list. # Function Signature: ```python def manipulate_sequence(seq: list) -> list: pass ``` # Input: - `seq` (List[int]): A list of integers. Constraints: The length of the list can range from 1 to (10^4). # Output: - A list of integers after performing the specified operations. # Operations: 1. **Indexing and Slicing:** - Extract elements from index 2 to 5 (inclusive). - If the sequence has fewer than 6 elements, extract all elements from index 2 onwards. 2. **Repetition:** - Repeat the extracted slice 3 times to create a longer list. 3. **Concatenation:** - Concatenate the original list with the repeated slice. 4. **Maximum and Minimum:** - Append the maximum and minimum values of the original sequence to the end of the concatenated list. # Example: ```python # Example 1: seq = [1, 2, 3, 4, 5, 6, 7] # Following the operations: # Extracted slice: [3, 4, 5, 6] # Repeated slice: [3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6] # Concatenation with original: [1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6] # Maximum value: 7, Minimum value: 1 # Final list: [1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6, 7, 1] assert manipulate_sequence(seq) == [1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6, 7, 1] # Example 2: seq = [10, 20] # Following the operations: # Extracted slice: [] # Repeated slice: [] # Concatenation with original: [10, 20] # Maximum value: 20, Minimum value: 10 # Final list: [10, 20, 20, 10] assert manipulate_sequence(seq) == [10, 20, 20, 10] ``` # Constraints: - The input list may have duplicates. - Implement the function efficiently to handle the maximum input size within a reasonable time limit. Implement the `manipulate_sequence` function to ensure it works correctly for all edge cases.","solution":"def manipulate_sequence(seq): Manipulate the sequence according to the specified operations. Parameters: seq (list): A list of integers. Returns: list: The resulting list after performing the operations. # Step 1: Extract elements from index 2 to 5 (inclusive). # If the sequence has fewer than 6 elements, extract all elements from index 2 onwards. slice_part = seq[2:6] # Step 2: Repeat the extracted slice 3 times to create a longer list. repeated_slice = slice_part * 3 # Step 3: Concatenate the original list with the repeated slice. combined_list = seq + repeated_slice # Step 4: Append the maximum and minimum values of the original sequence max_value = max(seq) min_value = min(seq) combined_list.append(max_value) combined_list.append(min_value) return combined_list"},{"question":"# XML Data Processing Challenge You are given an XML document that contains information about various books in a library. The XML structure is as follows: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>39.95</price> <genre>Fiction</genre> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>1999</year> <price>29.95</price> <genre>Science</genre> </book> <!-- More books --> </library> ``` Your task is to write a Python function `process_library(xml_string)` that takes an XML string input, processes the data, and performs the following operations: 1. Parse the XML string into an `ElementTree`. 2. Find and return a list of all book titles. 3. Find and return the title of the oldest book in the library. 4. Update the price of all books published before the year 2000 by reducing their price by `20%`. 5. Add an attribute `discounted=\\"true\\"` to all books whose price was reduced. 6. Serialize the modified XML back to a string. The expected input and output formats are: - **Input:** - `xml_string` (str): A string containing the XML data representing the library. - **Output:** - (list, str, str): A tuple containing: 1. A list of book titles (sorted alphabetically). 2. The title of the oldest book. 3. A string of the modified XML data. # Constraints: - You may assume that the XML string will always have a valid XML format. - The year and price values in the book elements will be valid integers and floats, respectively. # Performance Requirements: - Your solution should handle XML documents with up to 10,000 books efficiently. # Example: ```python xml_data = \'\'\' <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>39.95</price> <genre>Fiction</genre> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>1999</year> <price>29.95</price> <genre>Science</genre> </book> </library> \'\'\' titles, oldest_book, modified_xml = process_library(xml_data) # Expected Output: # titles = [\'Book Title 1\', \'Book Title 2\'] # oldest_book = \'Book Title 2\' # modified_xml: # <?xml version=\\"1.0\\"?> # <library> # <book id=\\"1\\"> # <title>Book Title 1</title> # <author>Author 1</author> # <year>2001</year> # <price>39.95</price> # <genre>Fiction</genre> # </book> # <book id=\\"2\\" discounted=\\"true\\"> # <title>Book Title 2</title> # <author>Author 2</author> # <year>1999</year> # <price>23.96</price> # <genre>Science</genre> # </book> # </library> ``` **Note:** Please make sure to apply the 20% discount correctly and format the price to two decimal places.","solution":"import xml.etree.ElementTree as ET def process_library(xml_string): Processes the given XML and performs the specified operations. Args: xml_string (str): XML document as a string. Returns: tuple: (list_of_titles, title_of_oldest_book, modified_xml_string) root = ET.fromstring(xml_string) # Step 2: Find and return a list of all book titles. titles = [book.find(\'title\').text for book in root.findall(\'book\')] titles.sort() # Step 3: Find and return the title of the oldest book. oldest_year = float(\'inf\') oldest_book_title = None for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < oldest_year: oldest_year = year oldest_book_title = book.find(\'title\').text # Step 4 and 5: Update price and add attribute discounted. for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < 2000: price_element = book.find(\'price\') old_price = float(price_element.text) new_price = round(old_price * 0.8, 2) price_element.text = f\\"{new_price:.2f}\\" book.set(\'discounted\', \'true\') # Step 6: Serialize the modified XML back to a string. modified_xml_string = ET.tostring(root, encoding=\'unicode\', method=\'xml\') return (titles, oldest_book_title, modified_xml_string)"},{"question":"**Objective**: Demonstrate comprehension of the `linecache` module by creating a utility that manages and retrieves specific lines from a set of text files. **Problem Statement**: You are given the task to implement a class `FileManager` that helps in managing and retrieving lines from multiple text files efficiently using the `linecache` module. The class should support the following functionalities: 1. `add_file(file_path)` - Add a new file to the manager\'s tracking list. 2. `get_line(file_path, line_number)` - Retrieve a specific line from a given file. 3. `clear_cache()` - Clear the internal cache of stored file lines. 4. `validate_cache(file_path=None)` - Validate the cache for a specific file or all files if no file path is provided. 5. `get_multiple_lines(file_path, start_line, end_line)` - Retrieve multiple lines from `start_line` to `end_line` (inclusive). **Input/Output Formats**: 1. `add_file(file_path)` - Input: `file_path` (str) - Path to the file to be added. - Output: None 2. `get_line(file_path, line_number)` - Input: `file_path` (str) - Path to the file. `line_number` (int) - Line number to be retrieved. - Output: (str) - The content of the specified line. Return an empty string if the line is not found or any error occurs. 3. `clear_cache()` - Input: None - Output: None 4. `validate_cache(file_path=None)` - Input: `file_path` (str, optional) - Path to the file to validate the cache. If omitted, validate all cached files. - Output: None 5. `get_multiple_lines(file_path, start_line, end_line)` - Input: `file_path` (str) - Path to the file. `start_line` (int) - Line number to start retrievals. `end_line` (int) - Line number to end retrievals. - Output: (list of str) - List of strings containing lines from `start_line` to `end_line`. Return an empty list if the lines are not found or any error occurs. **Constraints**: - Assume that all file paths provided are valid and accessible. - `line_number`, `start_line`, and `end_line` are positive integers. - `start_line` will always be less than or equal to `end_line`. **Performance Requirements**: - The class should optimize file line retrieval operations using caching mechanisms provided by the `linecache` module. - Efficiently handle large files and frequent access patterns. Example Usage ```python # Create an instance of FileManager file_manager = FileManager() # Add a file to be tracked file_manager.add_file(\'example.txt\') # Retrieve the 3rd line from \'example.txt\' line = file_manager.get_line(\'example.txt\', 3) print(line) # Retrieve lines from 2nd to 5th from \'example.txt\' lines = file_manager.get_multiple_lines(\'example.txt\', 2, 5) print(lines) # Validate the cache for \'example.txt\' file_manager.validate_cache(\'example.txt\') # Clear all cached entries file_manager.clear_cache() ``` **Important Note**: - Do not forget to handle exceptions gracefully and ensure that empty strings or lists are returned in case of errors during file line retrievals.","solution":"import linecache class FileManager: def __init__(self): self.files = set() def add_file(self, file_path): self.files.add(file_path) def get_line(self, file_path, line_number): try: return linecache.getline(file_path, line_number).rstrip(\'n\') except Exception: return \\"\\" def clear_cache(self): linecache.clearcache() def validate_cache(self, file_path=None): if file_path: linecache.checkcache(file_path) else: linecache.checkcache() def get_multiple_lines(self, file_path, start_line, end_line): lines = [] try: for line_number in range(start_line, end_line + 1): line = linecache.getline(file_path, line_number) if line: lines.append(line.rstrip(\'n\')) except Exception: pass return lines"},{"question":"# Seaborn Scatter Plot Customization Challenge You are provided with a dataset containing information about tips received by waiters in a restaurant. Using this dataset, you need to create scatter plots to visualize the relationship between different variables. Your task is to create custom visualizations to compare tips given during lunch and dinner sessions across different days of the week. Dataset The dataset `tips` has the following columns: - `total_bill`: Total bill amount in dollars. - `tip`: Tip amount in dollars. - `sex`: Gender of the person paying. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of day (`Lunch` or `Dinner`). - `size`: Size of the party. Tasks: 1. **Create a basic scatter plot**: - Plot `total_bill` (x-axis) vs `tip` (y-axis) to see how the tip amount varies with the total bill. 2. **Add a hue semantic**: - Enhance the scatter plot by adding a `hue` based on the `day` variable to see how the tips differ across different days of the week. 3. **Modify markers**: - Change the marker style based on the `time` variable to differentiate between lunch and dinner sessions. 4. **Size mapping**: - Add a `size` mapping based on the `size` variable to represent the size of the party. 5. **Wide-form data plotting**: - Convert the existing dataset into a wide-form dataset where each column represents a variable (such as total_bill, tip, size) and each row represents an index. - Create a scatter plot using the wide-form data. 6. **Facet grid creation**: - Create a set of scatter plots using `relplot` to compare lunch and dinner sessions for each day of the week on different subplots. Constraints: - Use the Seaborn package for all visualizations. - Ensure the legends are appropriately added to each plot to make them self-explanatory. - Add titles to each plot/subplot for clarity. - Ensure the plots are well-labeled (x-axis and y-axis). Expected Format - You should create a function `create_scatter_plot(data)` which accepts a DataFrame and performs the above tasks in sequence, displaying the plots. ```python import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(data): # Task 1: Basic scatter plot plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Total Bill vs Tip\\") plt.show() # Task 2: Hue semantic plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.title(\\"Total Bill vs Tip by Day\\") plt.show() # Task 3: Marker style plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\") plt.title(\\"Total Bill vs Tip by Day and Time\\") plt.show() # Task 4: Size mapping plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Total Bill vs Tip by Day, Time, and Size\\") plt.show() # Task 5: Wide-form data plotting wide_data = data.pivot_table(index=data.index, columns=\'time\', values=[\'total_bill\', \'tip\', \'size\']) plt.figure() sns.scatterplot(data=wide_data) plt.title(\\"Wide-form Data Plot\\") plt.show() # Task 6: Facet grid creation sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ).set_titles(\\"{col_name} Session\\") plt.show() # Example usage: tips = sns.load_dataset(\\"tips\\") create_scatter_plot(tips) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(data): # Task 1: Basic scatter plot plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Total Bill vs Tip\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.show() # Task 2: Hue semantic plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.title(\\"Total Bill vs Tip by Day\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.legend(title=\\"Day\\") plt.show() # Task 3: Marker style plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\") plt.title(\\"Total Bill vs Tip by Day and Time\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.legend(title=\\"Day\\") plt.show() # Task 4: Size mapping plt.figure() sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Total Bill vs Tip by Day, Time, and Party Size\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.legend(title=\\"Day\\") plt.show() # Task 5: Wide-form data plotting plt.figure() sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'time\') plt.title(\\"Total Bill vs Tip by Time\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.show() # Task 6: Facet grid creation g = sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", col=\\"day\\", hue=\\"time\\", style=\\"time\\", kind=\\"scatter\\" ) g.set_titles(\\"{col_name} Day\\") g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") plt.show() # Example usage: if __name__ == \\"__main__\\": tips = sns.load_dataset(\\"tips\\") create_scatter_plot(tips)"},{"question":"Context You are provided with a folder containing multiple parquet files where each file represents time-series data of different years. Each file contains columns such as `name`, `id`, `x`, and `y`. The purpose of this task is to effectively load and process these large datasets by optimizing memory usage and implementing chunked processing. Instructions 1. **Data Loading**: Write a function `load_columns(file_path, columns)` to load only specific columns from a given parquet file. 2. **Data Optimization**: Write a function `optimize_memory(df)` to convert the data types of the DataFrame to more memory-efficient types. - Convert `name` column to `pandas.Categorical`. - Downcast `id` to the smallest numeric type using `pd.to_numeric`. - Downcast `x` and `y` to the smallest float type. 3. **Chunked Processing**: Write a function `process_in_chunks(directory_path, chunk_size)` to process the parquet files in chunks, where: - `directory_path` is the path to the folder containing parquet files. - `chunk_size` determines how many files to process simultaneously. Expected Outputs - The `load_columns` function should return a DataFrame containing only the specified columns from the parquet file. - The `optimize_memory` function should return a DataFrame with optimized data types for memory efficiency. - The `process_in_chunks` function should return a DataFrame with the cumulative value counts of the `name` column across all the files and with optimized memory usage. Example Using the sample dataset provided: ``` data/ └── timeseries ├── ts-00.parquet ├── ts-01.parquet ├── ts-02.parquet ├── ... ``` ```python def load_columns(file_path, columns): # Your code here pass def optimize_memory(df): # Your code here pass def process_in_chunks(directory_path, chunk_size): # Your code here pass # Example usage columns = [\\"name\\", \\"id\\", \\"x\\", \\"y\\"] df = load_columns(\\"data/timeseries/ts-00.parquet\\", columns) optimized_df = optimize_memory(df) result = process_in_chunks(\\"data/timeseries\\", chunk_size=2) # You can adjust chunk size as needed print(result) ``` Make sure your solution is both memory efficient and performs well with large datasets. Constraints - You may assume the directory contains only parquet files and is non-empty. - The `chunk_size` should be a positive integer. - Ensure your solution handles large datasets gracefully without running out of memory.","solution":"import pandas as pd import os def load_columns(file_path, columns): Load specific columns from a given parquet file. return pd.read_parquet(file_path, columns=columns) def optimize_memory(df): Optimize the memory usage of the DataFrame by converting data types. # Convert categorical columns if \'name\' in df.columns: df[\'name\'] = df[\'name\'].astype(\'category\') # Downcast numerical columns if \'id\' in df.columns: df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'integer\') if \'x\' in df.columns: df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') if \'y\' in df.columns: df[\'y\'] = pd.to_numeric(df[\'y\'], downcast=\'float\') return df def process_in_chunks(directory_path, chunk_size): Process parquet files in chunks and return cumulative value counts of the \'name\' column across all files with optimized memory usage. file_list = [f for f in os.listdir(directory_path) if f.endswith(\'.parquet\')] cumulative_counts = pd.Series(dtype=\'int\') for i in range(0, len(file_list), chunk_size): chunk_files = file_list[i:i + chunk_size] for file in chunk_files: file_path = os.path.join(directory_path, file) df = load_columns(file_path, [\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) df = optimize_memory(df) counts = df[\'name\'].value_counts() cumulative_counts = cumulative_counts.add(counts, fill_value=0) return cumulative_counts.astype(\'int32\') # Downcast resulting series # Example usage # columns = [\\"name\\", \\"id\\", \\"x\\", \\"y\\"] # df = load_columns(\\"data/timeseries/ts-00.parquet\\", columns) # optimized_df = optimize_memory(df) # result = process_in_chunks(\\"data/timeseries\\", chunk_size=2) # You can adjust chunk size as needed # print(result)"},{"question":"# PyTorch MTIA Management and Usage Objective Demonstrate your understanding of PyTorch\'s MTIA backend by managing devices, streams, and memory efficiently. Your task will involve initializing MTIA, managing device states, creating and using streams, and ensuring resource deallocation. Problem Statement Write a Python function `mtia_workflow` using PyTorch that performs the following tasks: 1. Initializes the MTIA backend if it is not already initialized. 2. Queries the number of devices available. 3. Selects a device based on device index provided as an argument. 4. Creates a new stream on the selected device. 5. Performs some dummy tensor operations within this stream, ensuring the operations are recorded and synchronized. 6. Retrieves and returns the memory statistics of the selected device after the operations. Specifications - The function should be named `mtia_workflow`. - Input: - An integer `device_index` that specifies which device to select (0-based indexing). - Output: - A dictionary containing the memory statistics of the selected device. Constraints - Assume the device index provided is always valid (i.e., no need to handle invalid indices). - Utilize MTIA functions and classes effectively. - Ensure all resources are properly synchronized and managed to prevent memory leaks or improper states. Example Usage ```python def mtia_workflow(device_index: int) -> dict: import torch.mtia as mtia # Your implementation here # Example call result = mtia_workflow(0) print(result) # Expected output format: {...} (memory statistics dictionary) ``` Additional Information - You might need to check the availability of MTIA and initialize it if not already done. - Memory statistics can be retrieved using `mtia.memory_stats(device)`. - Tensor operations can be simple creations and summations, but ensure they utilize the created stream.","solution":"def mtia_workflow(device_index: int) -> dict: import torch if not torch.mtia.is_available(): raise RuntimeError(\\"MTIA is not available on this system\\") if not torch.mtia.is_initialized(): torch.mtia.init() num_devices = torch.mtia.device_count() if device_index < 0 or device_index >= num_devices: raise ValueError(f\\"Invalid device index {device_index}, must be between 0 and {num_devices - 1}\\") device = torch.device(f\'mtia:{device_index}\') torch.mtia.set_device(device) stream = torch.mtia.Stream(device=device) stream.wait_stream(torch.mtia.current_stream(device)) with torch.mtia.stream(stream): a = torch.ones((1000, 1000), device=device) b = torch.ones((1000, 1000), device=device) c = a + b torch.mtia.synchronize(device) mem_stats = torch.mtia.memory_stats(device) return mem_stats # Example usage: # result = mtia_workflow(0) # print(result)"},{"question":"**Coding Assessment Question:** You are given a dataset `iris` from the seaborn\'s `sns.load_dataset` function. Your task is to implement a function that visualizes this dataset using a scatter plot with a custom colormap. # Function Signature: ```python def visualize_iris_colormap(): pass ``` # Requirements: 1. Load the `iris` dataset using `sns.load_dataset(\'iris\')`. 2. Create a scatter plot of `sepal_length` vs. `sepal_width`. 3. Use the species column to color the points. 4. Apply a continuous colormap (`viridis`) with different shades for different species values. 5. Add a legend to the plot that displays the species names. 6. Customize the colormap to have at least 8 distinct colors for the scatter plot. 7. Titles and axis labels should be appropriately added to the plot. # Input: - No input required for the function since the dataset is predefined. # Output: - A scatter plot within a Jupyter notebook cell or a script displaying the data points with the custom colormap applied and the legend. # Constraints: - Use only the seaborn and matplotlib packages. # Example: The plot should be an effective visualization of the iris dataset with clear differentiation of species using the continuous colormap \'viridis\' with at least 8 colors. # Additional Information: - You can make use of the example code snippets provided in the below documentation to implement the solution. - Ensure that the colormap is applied correctly and visually distinguishable for different species. You are expected to demonstrate the understanding of seaborn\'s colormap manipulation and data visualization techniques through this task. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_iris_colormap(): # Load the iris dataset iris = sns.load_dataset(\'iris\') # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=\'viridis\' ) # Customize the legend scatter.legend(title=\'Species\') # Add titles and labels plt.title(\'Sepal Length vs Sepal Width with Species Colormap\') plt.xlabel(\'Sepal Length (cm)\') plt.ylabel(\'Sepal Width (cm)\') # Display the plot plt.show()"},{"question":"**Objective**: Implement a Python class that manages a shelve database for a phonebook, demonstrating your ability to work with persistent storage using Python\'s `shelve` module. **Phonebook Manager** You are required to implement a `PhonebookManager` class to manage a persistent phonebook database. This manager should provide functionalities to add, retrieve, update, and delete contact information. The phonebook will store each contact\'s name (as the key) and a dictionary containing their \'phone number\' and \'email\' (as the value). **Requirements**: 1. **Initialization**: - `__init__(self, filename: str, writeback: bool = False)`: Initializes the class with the given filename and writeback option, using `shelve.open`. 2. **Adding a Contact**: - `add_contact(self, name: str, phone: str, email: str) -> None`: Adds a new contact to the phonebook. If the contact already exists, raises a `ValueError`. 3. **Retrieving a Contact**: - `get_contact(self, name: str) -> dict`: Retrieves the contact information (phone and email) for the given name. If the contact does not exist, raises a `KeyError`. 4. **Updating Contact Information**: - `update_contact(self, name: str, phone: str = None, email: str = None) -> None`: Updates the contact information. If the contact does not exist, raises a `KeyError`. 5. **Deleting a Contact**: - `delete_contact(self, name: str) -> None`: Deletes the contact with the given name. If the contact does not exist, raises a `KeyError`. 6. **Closing the Phonebook**: - `close(self) -> None`: Closes the shelve database. **Constraints**: - The contact name should be unique. - All contact names must be strings. - The phone number and email should be strings. - The database should be securely closed using `close` method to ensure all changes are saved. **Example Usage**: ```python # Example usage: phonebook = PhonebookManager(\'my_phonebook.db\', writeback=True) # Adding contacts phonebook.add_contact(\'John Doe\', \'1234567890\', \'john.doe@example.com\') phonebook.add_contact(\'Jane Smith\', \'0987654321\', \'jane.smith@example.com\') # Retrieving contacts print(phonebook.get_contact(\'John Doe\')) # Output: {\'phone\': \'1234567890\', \'email\': \'john.doe@example.com\'} # Updating a contact phonebook.update_contact(\'John Doe\', email=\'john.new@example.com\') # Deleting a contact phonebook.delete_contact(\'Jane Smith\') # Closing the phonebook phonebook.close() ``` **Performance Requirements**: - Efficient handling and minimal memory usage by managing writeback carefully. - Must handle a reasonable number of entries typical for a personal phonebook (e.g., several hundred entries). Implement the `PhonebookManager` class adhering to the requirements above.","solution":"import shelve class PhonebookManager: def __init__(self, filename: str, writeback: bool = False): self.shelf = shelve.open(filename, writeback=writeback) def add_contact(self, name: str, phone: str, email: str) -> None: if name in self.shelf: raise ValueError(f\\"Contact with name \'{name}\' already exists.\\") self.shelf[name] = {\'phone\': phone, \'email\': email} def get_contact(self, name: str) -> dict: if name not in self.shelf: raise KeyError(f\\"Contact with name \'{name}\' not found.\\") return self.shelf[name] def update_contact(self, name: str, phone: str = None, email: str = None) -> None: if name not in self.shelf: raise KeyError(f\\"Contact with name \'{name}\' not found.\\") if phone: self.shelf[name][\'phone\'] = phone if email: self.shelf[name][\'email\'] = email def delete_contact(self, name: str) -> None: if name not in self.shelf: raise KeyError(f\\"Contact with name \'{name}\' not found.\\") del self.shelf[name] def close(self) -> None: self.shelf.close()"},{"question":"You are given a dataset containing details of different wine samples with attributes such as acidity, sugar content, pH levels, and alcohol percentage. Your task is to use the seaborn library to create a heatmap that visualizes this data, clusters the wines based on their attributes, and customizes the plot according to specific criteria. Dataset Description The dataset `wine` contains the following attributes: - `fixed_acidity` - `volatile_acidity` - `citric_acid` - `residual_sugar` - `chlorides` - `free_sulfur_dioxide` - `total_sulfur_dioxide` - `density` - `pH` - `sulphates` - `alcohol` - `quality` (the label) Requirements 1. **Load the Data**: - Load the dataset into a `DataFrame`. 2. **Create Cluster Map**: - Use the seaborn `clustermap` to create a heatmap of the data. - Ensure that both rows and columns are clustered. 3. **Customization**: - Change the figure size to 12x8. - Set row cluster to `False`. - Adjust `dendrogram_ratio` to (0.2, 0.3). - Position the color bar at `(0.15, 0.75, 0.03, 0.2)`. 4. **Add Row Colors**: - Create a color mapping for the `quality` column (e.g., different colors for different quality levels). - Add these as row colors on the heatmap. 5. **Fine-tune the Heatmap**: - Use the `coolwarm` colormap. - Define the color scale to be between `vmin=0` and `vmax=15`. 6. **Clustering Parameters**: - Set distance metric to `euclidean`. - Use `complete` linkage method. 7. **Standardize Data**: - Standardize the data across columns. Expected Input - No specific input is provided programmatically. The task involves defining steps from importing data to creating and customizing plots. Expected Output A seaborn `clustermap` plot according to the above specifications. Constraints - Use seaborn version 0.11.1 or above. - The steps to produce the heatmap must be included in your function. Performance - The heatmap should render within a reasonable time frame for medium-sized datasets (up to 10,000 samples). # Sample Solution Outline ```python import seaborn as sns import pandas as pd def visualize_wine_clustermap(): # Load the dataset wine = pd.read_csv(\'path_to_wine_data.csv\') # Separate features and label wine_features = wine.drop(\'quality\', axis=1) quality = wine[\'quality\'] # Create color mapping for row colors based on wine quality lut = dict(zip(quality.unique(), sns.color_palette(\\"husl\\", len(quality.unique())))) row_colors = quality.map(lut) # Create the clustermap with provided specifications sns.clustermap( wine_features, figsize=(12, 8), row_cluster=False, dendrogram_ratio=(0.2, 0.3), cbar_pos=(0.15, 0.75, 0.03, 0.2), row_colors=row_colors, cmap=\'coolwarm\', vmin=0, vmax=15, method=\\"complete\\", metric=\\"euclidean\\", standard_scale=1 ) # Run the function visualize_wine_clustermap() ``` Ensure to test the function with the actual dataset in your local environment.","solution":"import seaborn as sns import pandas as pd def visualize_wine_clustermap(): This function loads the wine dataset, prepares the data, and then visualizes it using a Seaborn clustermap. # Load the dataset wine = pd.read_csv(\'path_to_wine_data.csv\') # Separate features and label wine_features = wine.drop(\'quality\', axis=1) quality = wine[\'quality\'] # Create color mapping for row colors based on wine quality lut = dict(zip(quality.unique(), sns.color_palette(\\"husl\\", len(quality.unique())))) row_colors = quality.map(lut) # Create the clustermap with provided specifications sns.clustermap( wine_features, figsize=(12, 8), row_cluster=False, dendrogram_ratio=(0.2, 0.3), cbar_pos=(0.15, 0.75, 0.03, 0.2), row_colors=row_colors, cmap=\'coolwarm\', vmin=0, vmax=15, method=\\"complete\\", metric=\\"euclidean\\", standard_scale=1 )"},{"question":"# PyTorch Coding Assessment **Objective:** Implement a Python function using PyTorch that: 1. Creates two tensors of specified shapes and data types. 2. Applies a series of operations on these tensors. 3. Utilizes PyTorch\'s autograd capabilities to compute gradients. **Question:** Write a function `tensor_operations` that takes the following inputs: - `shape1`: a tuple representing the shape of the first tensor. - `dtype1`: a `torch.dtype` representing the data type of the first tensor. - `shape2`: a tuple representing the shape of the second tensor. - `dtype2`: a `torch.dtype` representing the data type of the second tensor. - `device`: a string indicating the device to use (\'cpu\' or \'cuda\'). The function should perform the following steps: 1. Create two tensors `tensor1` and `tensor2` of shapes `shape1` and `shape2`, and data types `dtype1` and `dtype2`, respectively. Initialize `tensor1` with random values and `tensor2` with ones. Ensure both tensors require gradients. 2. Perform element-wise multiplication of the two tensors and store the result in `tensor3`. 3. Compute the mean of `tensor3` and store it in `tensor_mean`. 4. Compute the gradients of `tensor_mean` with respect to both `tensor1` and `tensor2`. The function should return a tuple `(tensor_mean.item(), tensor1.grad, tensor2.grad)`. **Constraints:** - `shape1` and `shape2` will be such that element-wise multiplication is possible (broadcasting rules apply). - `device` should be a valid PyTorch device. - Make sure to handle different data types and devices properly. **Example:** ```python import torch def tensor_operations(shape1, dtype1, shape2, dtype2, device): # Step 1: Create tensors tensor1 = torch.rand(shape1, dtype=dtype1, device=device, requires_grad=True) tensor2 = torch.ones(shape2, dtype=dtype2, device=device, requires_grad=True) # Step 2: Element-wise multiplication tensor3 = tensor1 * tensor2 # Step 3: Compute the mean of the result tensor_mean = tensor3.mean() # Step 4: Compute gradients tensor_mean.backward() return (tensor_mean.item(), tensor1.grad, tensor2.grad) # Example usage mean_val, grad1, grad2 = tensor_operations((2, 3), torch.float32, (2, 3), torch.float32, \'cpu\') print(mean_val) print(grad1) print(grad2) ``` **Notes:** - Test the function with both CPU and CUDA devices, if available. - Ensure compatibility with different data types supported by PyTorch tensors.","solution":"import torch def tensor_operations(shape1, dtype1, shape2, dtype2, device): This function performs a series of tensor operations using PyTorch, including the creation of tensors, element-wise multiplication, mean computation, and gradient calculation. Parameters: shape1 (tuple): Shape of the first tensor. dtype1 (torch.dtype): Data type of the first tensor. shape2 (tuple): Shape of the second tensor. dtype2 (torch.dtype): Data type of the second tensor. device (str): Device to use (\'cpu\' or \'cuda\'). Returns: tuple: A tuple containing the mean of the element-wise multiplied tensor and the gradients with respect to both tensors. # Step 1: Create tensors tensor1 = torch.rand(shape1, dtype=dtype1, device=device, requires_grad=True) tensor2 = torch.ones(shape2, dtype=dtype2, device=device, requires_grad=True) # Step 2: Element-wise multiplication tensor3 = tensor1 * tensor2 # Step 3: Compute the mean of the result tensor_mean = tensor3.mean() # Step 4: Compute gradients tensor_mean.backward() return (tensor_mean.item(), tensor1.grad, tensor2.grad)"},{"question":"# Platform Information Summarizer **Objective**: Write a function in Python that utilizes the `platform` module to gather and summarize platform-specific information in a structured format. Function Signature ```python def summarize_platform_info() -> dict: pass ``` Expected Output Format The function should return a dictionary containing the following keys and their corresponding values gathered from the `platform` module: - `architecture_bits`: The bit architecture of the Python interpreter. - `architecture_linkage`: The linkage format used by the Python interpreter. - `machine`: The machine type (CPU architecture). - `network_name`: The network name of the computer. - `platform`: A human-readable string identifying the platform. - `processor`: The processor name. - `python_build`: A tuple containing the Python build number and date. - `python_compiler`: The compiler used for compiling the Python interpreter. - `python_implementation`: The Python implementation, such as \'CPython\'. - `python_revision`: The Python implementation SCM revision. - `python_version`: The Python interpreter version. - `system`: The underlying platform\'s operating system name. - `system_release`: The system\'s release version. - `system_version`: The system\'s specific version. Constraints - If any value cannot be determined, use an empty string (`\\"\\"`) as the default. - Fairly good performance is expected, and the method should handle exceptions gracefully without crashing. Example Usage ```python info = summarize_platform_info() # Example output could be: # { # \'architecture_bits\': \'64bit\', # \'architecture_linkage\': \'ELF\', # \'machine\': \'x86_64\', # \'network_name\': \'MyComputer\', # \'platform\': \'Linux-5.4.0-70-generic-x86_64-with-Ubuntu-20.04-focal\', # \'processor\': \'x86_64\', # \'python_build\': (\'default\', \'Jul 20 2021 11:06:00\'), # \'python_compiler\': \'GCC 9.3.0\', # \'python_implementation\': \'CPython\', # \'python_revision\': \'\', # \'python_version\': \'3.8.10\', # \'system\': \'Linux\', # \'system_release\': \'5.4.0-70-generic\', # \'system_version\': \'#79-Ubuntu SMP Wed Mar 24 09:06:37 UTC 2021\', # } ``` Additional Information - Feel free to utilize multiple functions from the `platform` module. - Make sure to handle any potential exceptions that may arise from calling these functions and provide default values where applicable.","solution":"import platform import socket def summarize_platform_info() -> dict: try: architecture = platform.architecture() architecture_bits, architecture_linkage = architecture if len(architecture) == 2 else (\\"\\", \\"\\") return { \'architecture_bits\': architecture_bits, \'architecture_linkage\': architecture_linkage, \'machine\': platform.machine(), \'network_name\': socket.gethostname(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_revision\': platform.python_revision() if hasattr(platform, \'python_revision\') else \\"\\", \'python_version\': platform.python_version(), \'system\': platform.system(), \'system_release\': platform.release(), \'system_version\': platform.version(), } except Exception as e: return { \'architecture_bits\': \'\', \'architecture_linkage\': \'\', \'machine\': \'\', \'network_name\': \'\', \'platform\': \'\', \'processor\': \'\', \'python_build\': (\'\', \'\'), \'python_compiler\': \'\', \'python_implementation\': \'\', \'python_revision\': \'\', \'python_version\': \'\', \'system\': \'\', \'system_release\': \'\', \'system_version\': \'\', }"},{"question":"# Advanced Python Programming Assessment Question: You are required to design a basic Library Management System in Python. The system should consist of the following components: 1. **Book Class**: - Attributes: `title` (string), `author` (string), `isbn` (string), `quantity` (int) - Methods: - `__init__`: Constructor to initialize the book attributes. - `__repr__`: String representation of the book. - `lend_book`: Reduces the quantity of the book by 1. - `return_book`: Increases the quantity of the book by 1. 2. **Library Class**: - Attributes: `books` (a dictionary where keys are ISBNs and values are Book objects) - Methods: - `__init__`: Constructor to initialize the library with an empty catalog. - `add_book`: Adds a new book to the library or updates the quantity if the book already exists. - `remove_book`: Removes a book from the library. - `lend_book`: Lends a book to a user. - `return_book`: Accepts a returned book from a user. - `search_by_title`: Searches for books by title. - `search_by_author`: Searches for books by author. Constraints: - Provide appropriate exception handling, especially for cases such as trying to lend a book that is not available. - Ensure that the library can handle multiple books with the same title but different authors or ISBNs. - Demonstrate the use of list comprehensions where applicable. - Ensure methods are documented with appropriate docstrings. Example Usage: ```python # Creating a library instance my_library = Library() # Adding books to the library my_library.add_book(\\"Harry Potter and the Philosopher\'s Stone\\", \\"J.K. Rowling\\", \\"9780747532699\\", 5) my_library.add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"9780743273565\\", 3) # Lending a book my_library.lend_book(\\"9780747532699\\") # Returning a book my_library.return_book(\\"9780747532699\\") # Searching books by title print(my_library.search_by_title(\\"Harry Potter\\")) # Searching books by author print(my_library.search_by_author(\\"F. Scott Fitzgerald\\")) ``` Implementation: Implement the `Book` and `Library` classes in a single Python file. Test your implementation with various scenarios to ensure correctness.","solution":"class Book: def __init__(self, title, author, isbn, quantity): Initialize the book with title, author, isbn, and quantity. self.title = title self.author = author self.isbn = isbn self.quantity = quantity def __repr__(self): Returns a string representation of the book. return f\\"Book(title={self.title}, author={self.author}, isbn={self.isbn}, quantity={self.quantity})\\" def lend_book(self): Reduces the quantity of the book by 1 if there are available copies. if self.quantity <= 0: raise Exception(\\"Book is not available for lending.\\") self.quantity -= 1 def return_book(self): Increases the quantity of the book by 1. self.quantity += 1 class Library: def __init__(self): Initialize the library with an empty dictionary of books. self.books = {} def add_book(self, title, author, isbn, quantity): Adds a new book to the library or updates the book quantity if it already exists. if isbn in self.books: self.books[isbn].quantity += quantity else: self.books[isbn] = Book(title, author, isbn, quantity) def remove_book(self, isbn): Removes a book from the library using its ISBN. if isbn in self.books: del self.books[isbn] else: raise Exception(\\"Book not found in the library.\\") def lend_book(self, isbn): Lends a book to a user by reducing its quantity. if isbn in self.books: self.books[isbn].lend_book() else: raise Exception(\\"Book not found in the library.\\") def return_book(self, isbn): Accepts a returned book from a user by increasing its quantity. if isbn in self.books: self.books[isbn].return_book() else: raise Exception(\\"Book not found in the library.\\") def search_by_title(self, title): Searches for books by title and returns a list of book objects. return [book for book in self.books.values() if title.lower() in book.title.lower()] def search_by_author(self, author): Searches for books by author and returns a list of book objects. return [book for book in self.books.values() if author.lower() in book.author.lower()]"},{"question":"**Python 310 Advanced Concepts: Custom Shallow and Deep Copy Implementation** # Problem Statement You are required to implement custom shallow and deep copy behaviors for a compound class in Python. The class should contain nested mutable objects, including lists and other custom objects. Your implementation should demonstrate a proper understanding of shallow and deep copy concepts by preventing recursive loops and excessive copying. # Requirements * Implement a class `Person` representing a person with attributes such as `name`, and a list of `Friend` objects (`friends`). * Implement a class `Friend` representing a friend with attributes `name` and a reference (`best_friend`) to another `Friend` object, which can be `None`. * Implement the `__copy__()` and `__deepcopy__()` methods for both classes. # Constraints 1. The `__copy__()` method should create a shallow copy of the `Person` or `Friend` instance. 2. The `__deepcopy__()` method should create a deep copy of the `Person` or `Friend` instance, handling recursive objects using the `memo` dictionary. 3. Ensure that creating a deep copy of a `Person` does not unintentionally create multiple copies of the same `Friend`. 4. The `Person` class should support adding friends to the `friends` list. # Function Signatures ```python class Person: def __init__(self, name: str): # Initialize the name and an empty list of friends pass def add_friend(self, friend: \'Friend\'): # Add a friend to the friends list pass def __copy__(self): # Implement shallow copy pass def __deepcopy__(self, memo): # Implement deep copy pass class Friend: def __init__(self, name: str, best_friend: \'Friend\' = None): # Initialize the name and the best_friend reference pass def __copy__(self): # Implement shallow copy pass def __deepcopy__(self, memo): # Implement deep copy pass ``` # Expected Output ```python # Create friends alice = Friend(\'Alice\') bob = Friend(\'Bob\', best_friend=alice) charlie = Friend(\'Charlie\', best_friend=bob) # Create a person and add friends person = Person(\'John\') person.add_friend(alice) person.add_friend(bob) person.add_friend(charlie) # Shallow copy of person shallow_copy_person = copy.copy(person) # Deep copy of person deep_copy_person = copy.deepcopy(person) # Test case to ensure shallow copy references same Friend instances assert shallow_copy_person.friends[0] is alice assert shallow_copy_person.friends[1] is bob assert shallow_copy_person.friends[1].best_friend is alice # Test case to ensure deep copy creates new Friend instances while retaining structure assert deep_copy_person.friends[0] is not alice assert deep_copy_person.friends[1] is not bob assert deep_copy_person.friends[2] is not charlie assert deep_copy_person.friends[1].best_friend is not alice ``` Your task is to provide the full implementation of the `Person` and `Friend` classes and ensure the provided test cases pass successfully.","solution":"import copy class Person: def __init__(self, name: str): self.name = name self.friends = [] def add_friend(self, friend: \'Friend\'): self.friends.append(friend) def __copy__(self): new_person = Person(self.name) new_person.friends = self.friends[:] return new_person def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_person = Person(self.name) memo[id(self)] = new_person new_person.friends = copy.deepcopy(self.friends, memo) return new_person class Friend: def __init__(self, name: str, best_friend: \'Friend\' = None): self.name = name self.best_friend = best_friend def __copy__(self): return Friend(self.name, self.best_friend) def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_friend = Friend(self.name, None) memo[id(self)] = new_friend if self.best_friend: new_friend.best_friend = copy.deepcopy(self.best_friend, memo) return new_friend"},{"question":"**Objective:** Demonstrate your understanding of Python\'s built-in exceptions, exception handling, and custom exception creation. This task will assess your ability to manage various error scenarios and create meaningful custom exceptions. **Problem Statement:** You are tasked with writing a function named `process_transactions` that takes a list of dictionaries as input, where each dictionary represents a transaction with the following keys: - `id` (int): Transaction ID. - `amount` (float): Transaction amount. - `type` (str): Type of transaction, either \\"credit\\" or \\"debit\\". Write a function `process_transactions(transactions)` which processes this list of transactions. Your function should handle the following cases, raising appropriate exceptions where necessary: 1. If any transaction dictionary is missing required keys, raise a `KeyError` with a message indicating the missing key. 2. If any transaction has an invalid type (not \\"credit\\" or \\"debit\\"), raise a `ValueError` with a message indicating the invalid type. 3. If any transaction has a non-positive amount, raise a custom exception `InvalidTransactionAmountError` with a message indicating the invalid amount. 4. If any transaction causes the account balance to go below zero (assuming initial balance is zero), raise a `RuntimeError` with a message indicating the transaction ID that caused the balance to go negative. Your custom exception `InvalidTransactionAmountError` should inherit from `ValueError`. **Constraints:** - Transaction IDs are guaranteed to be unique integers. - Transaction amounts and types are guaranteed to be provided but may be invalid as per criteria above. - The function should stop processing transactions as soon as an error is encountered. **Function Signature:** ```python def process_transactions(transactions: List[Dict[str, Union[int, float, str]]]) -> None: ``` **Example:** ```python transactions = [ {\\"id\\": 1, \\"amount\\": 100.0, \\"type\\": \\"credit\\"}, {\\"id\\": 2, \\"amount\\": -50.0, \\"type\\": \\"debit\\"}, {\\"id\\": 3, \\"amount\\": 30.0, \\"type\\": \\"debit\\"}, {\\"id\\": 4, \\"amount\\": 20.0, \\"type\\": \\"debiit\\"}, # Typo in type {\\"id\\": 5, \\"amount\\": 0.0, \\"type\\": \\"debit\\"}, # Non-positive amount {\\"id\\": 6, \\"amount\\": 10.0, \\"typ\\": \\"credit\\"}, # Missing key ] try: process_transactions(transactions) except Exception as e: print(f\\"Transaction processing error: {e}\\") ``` **Expected Output:** ``` Transaction processing error: Invalid amount: -50.0 ``` Note: The output will vary depending on the order you handle errors in your function. **Submission:** - Implement the `process_transactions` function and the custom exception class `InvalidTransactionAmountError`. - Ensure your code is well-documented and handles all specified error cases.","solution":"from typing import List, Dict, Union class InvalidTransactionAmountError(ValueError): Custom exception raised for invalid transaction amount. def __init__(self, amount): super().__init__(f\\"Invalid amount: {amount}\\") def process_transactions(transactions: List[Dict[str, Union[int, float, str]]]) -> None: balance = 0.0 for transaction in transactions: # Check for missing keys if \'id\' not in transaction: raise KeyError(\'Missing key: id\') if \'amount\' not in transaction: raise KeyError(\'Missing key: amount\') if \'type\' not in transaction: raise KeyError(\'Missing key: type\') transaction_id = transaction[\'id\'] amount = transaction[\'amount\'] transaction_type = transaction[\'type\'] # Check for invalid types if transaction_type not in (\'credit\', \'debit\'): raise ValueError(f\\"Invalid type: {transaction_type}\\") # Check for non-positive amount if amount <= 0: raise InvalidTransactionAmountError(amount) # Updating the balance based on transaction type if transaction_type == \'credit\': balance += amount else: # transaction_type == \'debit\' balance -= amount # Check if the balance goes below zero if balance < 0: raise RuntimeError(f\\"Transaction {transaction_id} causes balance to go below zero\\")"},{"question":"**Objective:** To assess your understanding of the plistlib module in Python, as well as your ability to manipulate plist data. **Problem Statement:** You are given a dictionary representing a nested structure of items sold in an online store. Your task is to perform the following operations using the `plistlib` module: 1. **Write the dictionary to a plist file in XML format.** 2. **Read the plist file back into a Python dictionary.** 3. **Modify the dictionary by adding a new item.** 4. **Write the modified dictionary to a plist file in binary format.** 5. **Read the binary plist file and return the modified dictionary.** **Input Format:** - A dictionary representing the items with their details. **Output Format:** - The modified dictionary after performing the above operations. **Constraints:** - The dictionary keys should be strings. - The file operations should handle any exceptions gracefully. - The added item should be a dictionary with at least 3 keys: \'name\', \'price\', and \'quantity\'. **Performance Requirements:** - Efficiently handle dictionary encoding and decoding. **Example:** ```python import plistlib store_items = { \'Electronics\': { \'Laptops\': [ {\'name\': \'MacBook Pro\', \'price\': 1299, \'quantity\': 10}, {\'name\': \'Dell XPS\', \'price\': 999, \'quantity\': 5} ], \'Smartphones\': [ {\'name\': \'iPhone\', \'price\': 799, \'quantity\': 15}, {\'name\': \'Samsung Galaxy\', \'price\': 699, \'quantity\': 8} ], }, \'Furniture\': { \'Chairs\': [ {\'name\': \'Office Chair\', \'price\': 120, \'quantity\': 25}, {\'name\': \'Dining Chair\', \'price\': 80, \'quantity\': 30} ], \'Tables\': [ {\'name\': \'Dining Table\', \'price\': 350, \'quantity\': 5}, {\'name\': \'Coffee Table\', \'price\': 200, \'quantity\': 7} ] } } def modify_store_items(items_dict): # Step 1: Write the dictionary to a plist file in XML format. with open(\'store_items.xml\', \'wb\') as fp: plistlib.dump(items_dict, fp) # Step 2: Read the plist file back into a Python dictionary. with open(\'store_items.xml\', \'rb\') as fp: loaded_dict = plistlib.load(fp) # Step 3: Modify the dictionary by adding a new item. new_item = { \'name\': \'Gaming Chair\', \'price\': 250, \'quantity\': 10 } loaded_dict[\'Furniture\'][\'Chairs\'].append(new_item) # Step 4: Write the modified dictionary to a plist file in binary format. with open(\'store_items_modified.plist\', \'wb\') as fp: plistlib.dump(loaded_dict, fp, fmt=plistlib.FMT_BINARY) # Step 5: Read the binary plist file and return the modified dictionary. with open(\'store_items_modified.plist\', \'rb\') as fp: modified_dict = plistlib.load(fp) return modified_dict # Example usage print(modify_store_items(store_items)) ``` **Note:** - Ensure your code works correctly for different dictionaries representing store items. - Handle edge cases such as missing or invalid data gracefully.","solution":"import plistlib def modify_store_items(items_dict): # Step 1: Write the dictionary to a plist file in XML format with open(\'store_items.xml\', \'wb\') as fp: plistlib.dump(items_dict, fp) # Step 2: Read the plist file back into a Python dictionary with open(\'store_items.xml\', \'rb\') as fp: loaded_dict = plistlib.load(fp) # Step 3: Modify the dictionary by adding a new item new_item = { \'name\': \'Gaming Chair\', \'price\': 250, \'quantity\': 10 } if \'Furniture\' in loaded_dict and \'Chairs\' in loaded_dict[\'Furniture\']: loaded_dict[\'Furniture\'][\'Chairs\'].append(new_item) else: loaded_dict.setdefault(\'Furniture\', {}).setdefault(\'Chairs\', []).append(new_item) # Step 4: Write the modified dictionary to a plist file in binary format with open(\'store_items_modified.plist\', \'wb\') as fp: plistlib.dump(loaded_dict, fp, fmt=plistlib.FMT_BINARY) # Step 5: Read the binary plist file and return the modified dictionary with open(\'store_items_modified.plist\', \'rb\') as fp: modified_dict = plistlib.load(fp) return modified_dict"},{"question":"# Time Conversion and Performance Measurement You are tasked to design a utility that can handle various time manipulations and measure performance of code execution. Implement two functions: 1. **convert_time**: This function will convert a given time from one format to another. 2. **measure_performance**: This function will measure the time taken for a provided function to execute a specified number of iterations. Function 1: convert_time This function converts time from one representation to another using the appropriate functions from the `time` module. **Function Signature:** ```python def convert_time(input_time: float, from_format: str, to_format: str) -> str: Converts time between different representations. Parameters: input_time (float): The time to convert, typically in seconds since the epoch. from_format (str): The format of the input time. Supported values are \\"epoch\\", \\"utc\\", \\"local\\". to_format (str): The desired format. Supported values are \\"epoch\\", \\"utc\\", \\"local\\", \\"asctime\\". Returns: str: The converted time in the desired format. ``` **Details:** - `from_format` can be one of `epoch`, `utc`, `local`. - `to_format` can be one of `epoch`, `utc`, `local`, `asctime`. - If the input time is in `epoch` format, it will be converted using `gmtime()` for `utc` or `localtime()` for `local`. - If the `to_format` is `asctime`, the time will be converted to a human-readable string using `asctime()`. - If conversion is not possible, raise a `ValueError`. Function 2: measure_performance This function measures the performance of a provided function by executing it multiple times and recording the total time taken. **Function Signature:** ```python def measure_performance(func: callable, iterations: int, *args, **kwargs) -> float: Measures the time taken by a function to execute a specified number of iterations. Parameters: func (callable): The function to measure. iterations (int): Number of iterations to run the function. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: float: The total time taken for the specified number of iterations. ``` **Details:** - The performance will be measured using a high-resolution timer like `perf_counter()`. - The function should return the total time in seconds taken to execute the provided function the specified number of iterations. Example Usage: ```python # Function to be measured def sample_function(): sum(range(1000)) # Measuring Performance total_time = measure_performance(sample_function, 1000) print(f\\"Total time for 1000 iterations: {total_time} seconds\\") # Converting time current_epoch_time = time.time() utc_time = convert_time(current_epoch_time, \'epoch\', \'utc\') print(f\\"UTC Time: {utc_time}\\") asctime_value = convert_time(current_epoch_time, \'epoch\', \'asctime\') print(f\\"ASCII Time: {asctime_value}\\") ``` Constraints: - Do not use any external libraries. Only `time` module functions should be used. - Handle all edge cases appropriately (e.g., invalid format strings, inappropriate input times). - Ensure the performance measurement is as accurate as possible. **Test your implementation thoroughly to ensure correctness.**","solution":"import time def convert_time(input_time: float, from_format: str, to_format: str) -> str: Converts time between different representations. Parameters: input_time (float): The time to convert, typically in seconds since the epoch. from_format (str): The format of the input time. Supported values are \\"epoch\\", \\"utc\\", \\"local\\". to_format (str): The desired format. Supported values are \\"epoch\\", \\"utc\\", \\"local\\", \\"asctime\\". Returns: str: The converted time in the desired format. if from_format == \'epoch\': if to_format == \'epoch\': return str(input_time) elif to_format == \'utc\': return time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.gmtime(input_time)) elif to_format == \'local\': return time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.localtime(input_time)) elif to_format == \'asctime\': return time.asctime(time.localtime(input_time)) else: raise ValueError(\\"Unsupported to_format specified\\") elif from_format in [\'utc\', \'local\']: raise ValueError(\\"Currently only epoch to other formats conversion is supported\\") else: raise ValueError(\\"Unsupported from_format specified\\") def measure_performance(func: callable, iterations: int, *args, **kwargs) -> float: Measures the time taken by a function to execute a specified number of iterations. Parameters: func (callable): The function to measure. iterations (int): Number of iterations to run the function. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: float: The total time taken for the specified number of iterations. start_time = time.perf_counter() for _ in range(iterations): func(*args, **kwargs) end_time = time.perf_counter() return end_time - start_time"},{"question":"**Objective:** Your task is to implement a function that uses the `operator` module to perform various operations on a list of dictionaries. The function should be able to filter, sort, and modify the data according to specified criteria. **Function Signature:** ```python def process_students_data(students: list, threshold: int, subject: str) -> list: Processes a list of student dictionaries by performing various operations using the `operator` module. Parameters: - students (list): A list of dictionaries where each dictionary represents a student. Each student dictionary contains the keys \'name\' (str), \'age\' (int), and \'grades\' (dict) where grades is a dictionary of subject (str) to grade (int). - threshold (int): The minimum grade threshold to filter students. - subject (str): The subject name to consider for filtering and sorting. Returns: - list: A list of names of students who have a grade greater than or equal to the threshold in the specified subject, sorted by their grade in descending order. If two students have the same grade, they should be sorted by their name in ascending alphabetical order. pass ``` **Constraints and Requirements:** 1. You must use functions from the `operator` module where appropriate. Do not use the corresponding operators or built-in functions directly. 2. The resulting list should only include students who meet the threshold criteria for the given subject. 3. The final list should be sorted by the grade in descending order. If grades are tied, sort by the student\'s name in ascending order. 4. Handle edge cases such as: - Students with missing grades for the specified subject. - Empty list of students. **Example:** ```python students = [ {\'name\': \'Alice\', \'age\': 20, \'grades\': {\'Math\': 85, \'English\': 78}}, {\'name\': \'Bob\', \'age\': 22, \'grades\': {\'Math\': 90, \'English\': 80}}, {\'name\': \'Charlie\', \'age\': 21, \'grades\': {\'Math\': 85, \'English\': 90}}, {\'name\': \'David\', \'age\': 23, \'grades\': {\'Math\': 70, \'English\': 88}} ] print(process_students_data(students, 80, \'Math\')) # Output: [\'Bob\', \'Alice\', \'Charlie\'] ``` **Explanation:** - Bob and Alice have grades in Math greater than or equal to 80. - Charlie, despite having an equal grade to Alice, is listed after Alice due to alphabetical order. **Notes:** - Focus on utilizing the `operator` module effectively to implement the solution. - Ensure the function is efficient and handles different edge cases as mentioned.","solution":"import operator def process_students_data(students: list, threshold: int, subject: str) -> list: # Filter out students who do not meet the threshold for the given subject filtered_students = [ student for student in students if subject in student[\'grades\'] and student[\'grades\'][subject] >= threshold ] # Sort the filtered students primarily by grade in descending order, and secondarily by name in ascending order sorted_students = sorted( filtered_students, key=lambda student: (-student[\'grades\'][subject], student[\'name\']) ) # Extract and return the list of names from the sorted students return [student[\'name\'] for student in sorted_students]"},{"question":"# Coding Challenge: Directory Organizer **Objective**: Write a function `organize_files` that automates the process of organizing files in a directory based on their file extensions. **Task**: Given a directory path, the function should create subdirectories for each type of file extension found within the directory and move the files into their respective subdirectories. **Function Signature**: ```python def organize_files(directory_path: str) -> None: pass ``` **Input**: - `directory_path` (str): The path of the directory to organize. **Output**: - The function does not return anything. It organizes the files in the given directory. **Constraints**: 1. You must handle the situation where the directory is empty. 2. Ignore directories within the provided directory path. 3. Create new subdirectories for file types only if there are such files. 4. Ensure that no file is overwritten in the process. **Example**: Suppose the contents of the directory `example_dir` are: ``` example_dir/ file1.txt file2.txt image1.png document1.pdf script.py ``` After calling `organize_files(\'example_dir\')`, the directory structure should be: ``` example_dir/ txt/ file1.txt file2.txt png/ image1.png pdf/ document1.pdf py/ script.py ``` **Notes**: - You may make appropriate use of Python\'s standard libraries like `os` and `shutil`. **Points to Consider**: - Ensure your code is robust with proper error handling. - Consider edge cases, like files without extensions, and handle them appropriately. - Aim for your code to be efficient even with large numbers of files. **Hints**: - Use `os` module to interact with the file system. - Use `shutil.move` to move files. - Use `os.makedirs` to create new directories if they do not exist. This question tests the student\'s ability to work with file systems, handle directories and files, and implement a solution that effectively categorizes and organizes content based on file types.","solution":"import os import shutil def organize_files(directory_path: str) -> None: Organize files in the given directory by their file extensions. Args: directory_path (str): The path of the directory to organize. Returns: None if not os.path.isdir(directory_path): raise ValueError(\\"The provided path is not a directory\\") # List all files in the directory files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))] if not files: # If there are no files, no further action is needed return for file in files: # Get the file extension file_extension = os.path.splitext(file)[1][1:] # Skip the dot in the file extension if file_extension: # Only process files with extensions ext_dir = os.path.join(directory_path, file_extension.lower()) # Create the directory if it does not exist if not os.path.exists(ext_dir): os.makedirs(ext_dir) # Move the file to the corresponding directory shutil.move(os.path.join(directory_path, file), os.path.join(ext_dir, file))"},{"question":"# IP Address Management System You have been hired to implement a small IP address management (IPAM) system that will keep track of a pool of IP addresses and perform various operations such as allocation, deallocation, and checking the status of addresses. Your task is to implement a series of functions using the `ipaddress` module that will facilitate these operations. # Implementation Details Function 1: `create_ip_pool(network: str) -> list` - **Input:** A string representing an IP network in CIDR notation (e.g., \'192.168.0.0/24\'). - **Output:** A list of all possible host addresses within this network, excluding the network address and the broadcast address. Function 2: `allocate_ip(ip_pool: list, ip_address: str) -> None` - **Input:** - `ip_pool`: A list of available IP addresses. - `ip_address`: A string representing the IP address to allocate. - **Output:** None. The function should remove the IP address from the pool if it is available. - **Constraints:** Raise a `ValueError` if the IP address is not in the pool. Function 3: `deallocate_ip(ip_pool: list, ip_address: str) -> None` - **Input:** - `ip_pool`: A list of available IP addresses. - `ip_address`: A string representing the IP address to deallocate. - **Output:** None. The function should add the IP address back to the pool if it is not already present. - **Constraints:** Raise a `ValueError` if the IP address is already in the pool. Function 4: `check_ip_status(ip_pool: list, ip_address: str) -> str` - **Input:** - `ip_pool`: A list of available IP addresses. - `ip_address`: A string representing the IP address to check. - **Output:** A string indicating whether the IP address is \'available\' or \'allocated\'. - **Constraints:** Raise a `ValueError` if the IP address is outside the network range. # Example Usage ```python # Initialize the IP pool ip_pool = create_ip_pool(\'192.168.0.0/28\') # Allocate some IP addresses allocate_ip(ip_pool, \'192.168.0.1\') allocate_ip(ip_pool, \'192.168.0.2\') # Check IP statuses print(check_ip_status(ip_pool, \'192.168.0.1\')) # Output: \'allocated\' print(check_ip_status(ip_pool, \'192.168.0.3\')) # Output: \'available\' # Deallocate an IP address deallocate_ip(ip_pool, \'192.168.0.1\') # Check IP status again print(check_ip_status(ip_pool, \'192.168.0.1\')) # Output: \'available\' ``` # Constraints - The functions should be implemented using the `ipaddress` module. - All IP addresses should be valid IPv4 addresses. - The IP addresses should be managed within the constraints of the provided network CIDR. # Evaluation Your implementation will be evaluated on correctness, efficiency, and adherence to best practices in Python programming.","solution":"import ipaddress def create_ip_pool(network: str) -> list: Returns a list of all possible host addresses within the given network, excluding the network address and the broadcast address. net = ipaddress.ip_network(network) # Exclude the network address and the broadcast address return [str(ip) for ip in net.hosts()] def allocate_ip(ip_pool: list, ip_address: str) -> None: Allocates an IP address from the pool if available. if ip_address in ip_pool: ip_pool.remove(ip_address) else: raise ValueError(\\"The IP address is not available in the pool.\\") def deallocate_ip(ip_pool: list, ip_address: str) -> None: Deallocates an IP address back to the pool if it is not already present. if ip_address not in ip_pool: ip_pool.append(ip_address) else: raise ValueError(\\"The IP address is already in the pool.\\") def check_ip_status(ip_pool: list, ip_address: str) -> str: Checks the status of an IP address, indicating if it is \'available\' or \'allocated\'. if ip_address in ip_pool: return \'available\' else: return \'allocated\'"},{"question":"Advanced Priority Queue with Dynamic Task Adjustment In this assignment, you are required to design a custom priority queue using the `heapq` module. This priority queue should handle task priorities dynamically, including adding tasks, updating task priorities, and removing tasks efficiently. Objective Implement a class `DynamicPriorityQueue` that supports the following operations using the heapq module: 1. **add_task(task: str, priority: int)** - Adds a new task with a given priority or updates the priority of an existing task. - If the task already exists, its priority should be updated to the new value. 2. **remove_task(task: str)** - Removes a task from the priority queue. - If the task does not exist, raise a `KeyError`. 3. **pop_task() -> str** - Removes and returns the task with the highest priority (lowest numerical value). - If the priority queue is empty, raise a `KeyError`. 4. **peek_task() -> str** - Returns the task with the highest priority without removing it from the queue. - If the queue is empty, raise a `KeyError`. 5. **change_priority(task: str, new_priority: int)** - Changes the priority of an existing task to a new value. - If the task does not exist, raise a `KeyError`. Constraints - You may assume that all task names are unique strings. - Performance is important. Efficiently handle changing and removing tasks without breaking the heap structure. - Use the `heapq` module to manage the heap operations effectively. - Tasks should be returned in the order they were added if they have the same priority. Example Usage ```python dpq = DynamicPriorityQueue() dpq.add_task(\'task1\', 5) dpq.add_task(\'task2\', 3) dpq.add_task(\'task3\', 4) print(dpq.pop_task()) # Outputs \'task2\' dpq.add_task(\'task2\', 2) print(dpq.peek_task()) # Outputs \'task2\' dpq.change_priority(\'task3\', 1) print(dpq.pop_task()) # Outputs \'task3\' dpq.remove_task(\'task2\') print(dpq.pop_task()) # Outputs \'task1\' ``` Implementation Implement the `DynamicPriorityQueue` class by writing the necessary code to accomplish the task as described. Ensure to handle edge cases and maintain optimal performance for each operation.","solution":"import heapq class DynamicPriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, task, priority=0): if task in self.entry_finder: self.remove_task(task) entry = [priority, self.counter, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task): entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): while self.pq: priority, count, task = heapq.heappop(self.pq) if task != self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def peek_task(self): while self.pq: priority, count, task = self.pq[0] if task == self.REMOVED: heapq.heappop(self.pq) else: return task raise KeyError(\'peek from an empty priority queue\') def change_priority(self, task, new_priority): if task not in self.entry_finder: raise KeyError(\'task not found\') self.remove_task(task) self.add_task(task, new_priority)"},{"question":"You are tasked with creating a simple version of `2to3`, a tool that transforms Python 2.x code into Python 3.x code. Your function should take a string of Python 2.x code as input and return a string with the corresponding Python 3.x code. Implement the transformations for the following features: 1. Convert `print` statements to `print()` function calls. 2. Replace `xrange()` with `range()`. 3. Convert `raw_input()` to `input()`. 4. Change `dict.iteritems()` to `dict.items()`, `dict.iterkeys()` to `dict.keys()`, and `dict.itervalues()` to `dict.values()`. # Function Signature ```python def transform_python2_to_python3(code: str) -> str: # Your code here pass ``` # Input - `code` (str): A string containing Python 2.x code. The code will be valid Python 2.x syntax. # Output - Returns a string of the transformed Python 3.x code. # Constraints - The input will contain valid Python 2.x code. - You only need to handle the transformations listed above. # Examples Example 1 Input: ```python code = print \\"Hello, world!\\" print \\"Enter a number\\" num = raw_input() for i in xrange(10): print i items = {\'a\': 1, \'b\': 2}.iteritems() ``` Output: ```python print(\\"Hello, world!\\") print(\\"Enter a number\\") num = input() for i in range(10): print(i) items = {\'a\': 1, \'b\': 2}.items() ``` Example 2 Input: ```python code = dict_example = {\'key1\': \'value1\', \'key2\': \'value2\'} for key in dict_example.iterkeys(): print key ``` Output: ```python dict_example = {\'key1\': \'value1\', \'key2\': \'value2\'} for key in dict_example.keys(): print(key) ``` Example 3 Input: ```python code = values = {\'a\': 1, \'b\': 2}.itervalues() ``` Output: ```python values = {\'a\': 1, \'b\': 2}.values() ``` **Note:** Ensure that your function handles the following: - Nested structures like functions, loops, and if statements correctly. - Indentation is maintained. - Strings with the keywords inside them (like `raw_input()` within a string) remain unchanged.","solution":"import re def transform_python2_to_python3(code: str) -> str: Transforms given Python 2.x code into Python 3.x code by applying specific transformations. # Convert print statements to print() function calls code = re.sub(r\'bprints+([^()n]*)n\', r\'print(1)n\', code) code = re.sub(r\'bprints+(.+)\', r\'print(1)\', code.strip()) # Replace xrange() with range() code = re.sub(r\'bxrangeb\', \'range\', code) # Convert raw_input() to input() code = re.sub(r\'braw_inputb\', \'input\', code) # Change dict.iteritems() to dict.items() code = re.sub(r\'(.)(iteritems)()\', r\'1items()\', code) code = re.sub(r\'(.)(iterkeys)()\', r\'1keys()\', code) code = re.sub(r\'(.)(itervalues)()\', r\'1values()\', code) return code"},{"question":"**Coding Assessment Question:** # Background You are required to create a task manager using `asyncio.Queue` in Python. The task manager will handle tasks of varying priorities using `asyncio.PriorityQueue` and print statuses as tasks are processed. # Objective Implement a task management system with the following functionalities: 1. **Task Manager Class**: - `__init__(self, workers: int)`: Initialize the task manager with a specified number of worker coroutines. - `add_task(self, priority: int, task_name: str, duration: float)`: Method to add a new task to the priority queue with the given priority, task name, and anticipated duration (in seconds) for the task. - `run(self)`: Start the worker coroutines to process the tasks in the queue. 2. **Worker Functionality**: - Create worker coroutines to fetch tasks from the priority queue and simulate task processing by sleeping for the specified duration. - Print the status each time a task starts and finishes processing. # Specifications 1. **Task Manager Class**: - `__init__(self, workers: int)`: Initializes with a number of worker coroutines (`workers`). - `add_task(self, priority: int, task_name: str, duration: float)`: Adds task to `asyncio.PriorityQueue` with priority, task name, and duration. - `run(self)`: Creates worker coroutines to process tasks concurrently. Blocks until all tasks are processed and ensures all workers are properly cancelled after processing. 2. **Worker Coroutine**: - Print the task name when it begins processing. - Sleep for the task\'s expected duration. - Print the task name when it completes processing. - Call `queue.task_done()` after finishing each task. # Input and Output Formats - **Input Format**: - The `add_task` method will be called multiple times with the following parameters: `priority`, `task_name`, and `duration`. - **Output Format**: - Print the task processing status, e.g., \\"Starting task: Task_A\\", \\"Completed task: Task_A\\". # Example ```python import asyncio import random class TaskManager: def __init__(self, workers): self.queue = asyncio.PriorityQueue() self.workers = workers self.tasks = [] def add_task(self, priority, task_name, duration): self.queue.put_nowait((priority, task_name, duration)) async def worker(self, name): while True: priority, task_name, duration = await self.queue.get() print(f\\"{name} Starting task: {task_name}\\") await asyncio.sleep(duration) print(f\\"{name} Completed task: {task_name}\\") self.queue.task_done() async def run(self): # Create workers tasks = [asyncio.create_task(self.worker(f\\"Worker-{i}\\")) for i in range(self.workers)] # Blocking until all tasks are done await self.queue.join() # Cancel all worker tasks for task in tasks: task.cancel() # Wait for cancelling all workers await asyncio.gather(*tasks, return_exceptions=True) # Example usage: async def main(): task_manager = TaskManager(workers=3) task_manager.add_task(1, \'Task_A\', random.uniform(0.1, 1.0)) task_manager.add_task(2, \'Task_B\', random.uniform(0.1, 1.0)) task_manager.add_task(3, \'Task_C\', random.uniform(0.1, 1.0)) task_manager.add_task(1, \'Task_D\', random.uniform(0.1, 1.0)) await task_manager.run() asyncio.run(main()) ``` Here, the `TaskManager` manages the tasks added and distributes them to the workers based on priority. All necessary steps to indicate task processing begin and end are printed appropriately.","solution":"import asyncio class TaskManager: def __init__(self, workers: int): self.queue = asyncio.PriorityQueue() self.workers = workers def add_task(self, priority: int, task_name: str, duration: float): self.queue.put_nowait((priority, task_name, duration)) async def worker(self, id: int): while True: priority, task_name, duration = await self.queue.get() print(f\\"Worker-{id} Starting task: {task_name}\\") await asyncio.sleep(duration) print(f\\"Worker-{id} Completed task: {task_name}\\") self.queue.task_done() async def run(self): # Create worker tasks workers = [asyncio.create_task(self.worker(i)) for i in range(self.workers)] # Wait until the queue is fully processed await self.queue.join() # Cancel the workers for w in workers: w.cancel() # Wait for the cancellations to finish await asyncio.gather(*workers, return_exceptions=True)"},{"question":"# Question: Multi-Threaded Task Coordinator Objective: Implement a multi-threaded program that simulates a simplified version of task handler coordination. Problem Statement: You are to write a function `task_coordinator(tasks: List[Tuple[int, int]]) -> List[str]`, where `tasks` is a list of tuples representing tasks, with each tuple containing two integers. The first integer is the task duration (in seconds), and the second integer is the task priority (higher number represents higher priority). Your task is to process these tasks using multi-threading and ensure higher priority tasks are processed before lower priority ones using a queue for task coordination. Requirements: - Each task should be processed in a separate thread. - Use Python\'s `threading` and `queue` modules. - Higher priority tasks should be processed before lower priority tasks. - Return a list of strings indicating the order of task completion in the format \\"Task {index} completed\\", where {index} is the original index of the task in the input list. Input: - `tasks`: List of tuples [(duration, priority), ...] with the constraints that `1 <= duration <= 5` and `1 <= priority <= 10`. Output: - List of strings indicating the order of task completion. Function Signature: ```python from typing import List, Tuple def task_coordinator(tasks: List[Tuple[int, int]]) -> List[str]: pass ``` Example: ```python # Example 1: tasks = [(2, 3), (1, 5), (3, 1)] print(task_coordinator(tasks)) # Possible Output: [\'Task 1 completed\', \'Task 0 completed\', \'Task 2 completed\'] # Example 2: tasks = [(1, 1), (2, 2), (1, 3)] print(task_coordinator(tasks)) # Possible Output: [\'Task 2 completed\', \'Task 1 completed\', \'Task 0 completed\'] ``` Constraints and Notes: - You must use the `threading` and `queue` modules. - Handle possible synchronization issues that might arise with threading. - Make sure that tasks with higher priority are always processed before tasks with lower priority. - Ensure that multithreading advantages are realized (tasks should ideally get processed concurrently).","solution":"from typing import List, Tuple import threading import queue import time def task_coordinator(tasks: List[Tuple[int, int]]) -> List[str]: # A sorted task list based on priority (higher priority first) sorted_tasks = sorted(enumerate(tasks), key=lambda x: x[1][1], reverse=True) task_queue = queue.Queue() results = [None] * len(tasks) lock = threading.Lock() def worker(task_index, duration): time.sleep(duration) with lock: results[task_index] = f\\"Task {task_index} completed\\" # Populate the queue with tasks for index, (duration, _) in sorted_tasks: task_queue.put((index, duration)) threads = [] while not task_queue.empty(): index, duration = task_queue.get() thread = threading.Thread(target=worker, args=(index, duration)) thread.start() threads.append(thread) # Wait for all threads to complete for thread in threads: thread.join() return results"},{"question":"**Question: Porting Code for Compatibility with Python 2 and 3** **Objective:** You are given a piece of code written in Python 2. Your task is to port it to be compatible with both Python 2 and Python 3. This will require you to handle text and binary data correctly, adjust for division changes, and ensure the code runs without modification on both versions. **Original Code (Python 2):** ```python import sys def read_and_process_file(filename): with open(filename, \'r\') as file: content = file.read() results = process_content(content) return results def process_content(content): words = content.split() unique_words = set(words) return unique_words def main(filename): results = read_and_process_file(filename) print \\"Unique words in file:\\", results if __name__ == \\"__main__\\": if len(sys.argv) != 2: print \\"Usage: python script.py <filename>\\" else: main(sys.argv[1]) ``` **Tasks:** 1. Modify the code to make it compatible with both Python 2 and 3. 2. Ensure that text and binary data are handled correctly. 3. Use appropriate future imports to prepare for Python 3 functionalities. 4. Write tests to verify that your ported code works as expected. **Constraints:** - Do not use any external libraries other than those mentioned in the provided documentation. - Ensure the code works for text files encoded in UTF-8. - Maintain the original logic and functionality of the code. **Expected Output:** - Running the script with a given filename should produce a list of unique words in the file. - The code should work seamlessly in both Python 2.7 and Python 3.4+ environments. **Hint:** - Consider using `io.open` for file handling. - Use `from __future__ import` statements to handle differences in division, print function, and absolute import. - Ensure that print statements work in both Python 2 and Python 3. Submit both the ported code and the tests to demonstrate that the code works correctly in both Python 2 and 3.","solution":"from __future__ import print_function # To use print() as a function in Python 2 import sys import io def read_and_process_file(filename): with io.open(filename, \'r\', encoding=\'utf-8\') as file: # Handles file in text mode with UTF-8 encoding content = file.read() results = process_content(content) return results def process_content(content): words = content.split() unique_words = set(words) return unique_words def main(filename): results = read_and_process_file(filename) print(\\"Unique words in file:\\", results) # Compatible print statement if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <filename>\\") # Compatible print statement else: main(sys.argv[1])"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},D=["disabled"],N={key:0},M={key:1};function O(n,e,l,m,i,s){const h=_("PoemCard");return a(),o("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),o("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),o(b,null,v(s.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),s.displayedPoems.length===0?(a(),o("div",q,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),s.hasMorePoems?(a(),o("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>s.loadMore&&s.loadMore(...r))},[i.isLoading?(a(),o("span",M,"Loading...")):(a(),o("span",N,"See more"))],8,D)):d("",!0)])}const L=p(z,[["render",O],["__scopeId","data-v-ab50bfc2"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/57.md","filePath":"quotes/57.md"}'),j={name:"quotes/57.md"},B=Object.assign(j,{setup(n){return(e,l)=>(a(),o("div",null,[x(L)]))}});export{Y as __pageData,B as default};
