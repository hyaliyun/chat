import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-e55d9906"]]),I=JSON.parse('[{"question":"# PyTorch Tensor Operations and Differentiation Objective: Implement a function using PyTorch that takes a matrix of floating-point numbers, performs a few tensor operations, and computes gradients. The function should demonstrate the following PyTorch functionalities: - Tensor creation and manipulation - Mathematical operations - Indexing and slicing - Autograd for gradient computation Function Specification: Please implement the function `process_tensor(matrix: List[List[float]]) -> Tuple[torch.Tensor, float, torch.Tensor, torch.Tensor]`. # Parameters: - `matrix`: A 2D list of floats representing a matrix. # Returns: - A tuple containing: 1. `modified_tensor`: A `torch.Tensor` obtained by scaling the input tensor by 2.0, adding 5 to each element, and setting elements below a threshold (10.0) to zero. 2. `mean_value`: A float which is the mean value of `modified_tensor`. 3. `gradient`: A `torch.Tensor` which is the gradient of the mean value with respect to the original tensor. 4. `sliced_sum`: A `torch.Tensor` containing the sum of the elements in the first column of the original tensor. # Constraints: - The function should use PyTorch for all tensor operations. - Make sure that the tensor operations are performed on the original tensor before converting to numpy, and display proficiency in handling tensors in PyTorch. Example: ```python matrix = [[1.5, -2.0], [3.0, 4.0]] modified_tensor, mean_value, gradient, sliced_sum = process_tensor(matrix) # Expected Outputs # modified_tensor = tensor([[0.0000, 0.0000], [11.0000, 13.0000]]) # mean_value = 6.0 # gradient = tensor([[0.0000, 0.0000], [0.5000, 0.5000]]) # sliced_sum = tensor(4.5) ``` Instructions: 1. Convert the input matrix to a tensor with gradients enabled. 2. Scale the tensor by 2.0 and add 5 to each element. 3. Set all elements below 10.0 to zero. 4. Compute the mean of the resulting tensor. 5. Calculate the gradient of this mean with respect to the original tensor. 6. Obtain the sum of the elements in the first column of the original tensor. 7. Make sure the function and expected results match the provided example. Note: Ensure your code is robust and follows best practices for using PyTorch tensors and gradients.","solution":"from typing import List, Tuple import torch def process_tensor(matrix: List[List[float]]) -> Tuple[torch.Tensor, float, torch.Tensor, torch.Tensor]: Processes the input matrix using PyTorch tensor operations, and computes the specified outputs including the modified tensor, mean value, gradient, and sum of the first column. Parameters: matrix (List[List[float]]): A 2D list of floats representing a matrix. Returns: Tuple[torch.Tensor, float, torch.Tensor, torch.Tensor]: - modified_tensor: A torch.Tensor obtained by scaling the input tensor by 2.0, adding 5 to each element, and setting elements below a threshold (10.0) to zero. - mean_value: A float which is the mean value of modified_tensor. - gradient: A torch.Tensor which is the gradient of the mean value with respect to the original tensor. - sliced_sum: A torch.Tensor containing the sum of the elements in the first column of the original tensor. # Convert the matrix to a tensor and enable gradient tracking tensor = torch.tensor(matrix, requires_grad=True) # Scale the tensor by 2.0 and add 5 to each element modified_tensor = tensor * 2.0 + 5 # Set elements below 10.0 to zero modified_tensor = torch.where(modified_tensor < 10.0, torch.tensor(0.0), modified_tensor) # Compute the mean value of the modified tensor mean_value = modified_tensor.mean().item() # Compute the gradient of the mean value with respect to the original tensor modified_tensor.mean().backward() gradient = tensor.grad # Compute the sum of the elements in the first column of the original tensor sliced_sum = tensor[:, 0].sum() return modified_tensor, mean_value, gradient, sliced_sum"},{"question":"You are tasked with simulating the settings management of a macOS application using Python\'s `plistlib` module. Specifically, you need to handle saving, loading, and updating application settings stored in plist files. Your task is to implement three functions: 1. **`save_settings(settings, filename, fmt=\'FMT_XML\')`:** - **Input**: - `settings`: A dictionary containing application settings. The dictionary keys are strings and values can be types supported by plist (e.g., strings, integers, floats, booleans, lists, itself containing supported types). - `filename`: The name of the file to save the settings to. - `fmt`: (Optional) The format of the plist file, either \'FMT_XML\' (default) or \'FMT_BINARY\'. - **Output**: This function does not return anything. It writes the settings dictionary to the specified file in the specified format. - **Constraints**: The keys in the settings dictionary must be strings. 2. **`load_settings(filename, fmt=None)`:** - **Input**: - `filename`: The name of the file to load the settings from. - `fmt`: (Optional) The format of the plist file. If not specified, it should autodetect the format. - **Output**: Returns the settings as a dictionary. - **Constraints**: Raise an appropriate exception if the file cannot be read. 3. **`update_setting(filename, key, value, fmt=None)`:** - **Input**: - `filename`: The name of the file to update the settings in. - `key`: The key of the setting to update. - `value`: The new value to set for the given key. - `fmt`: (Optional) The format of the plist file. If not specified, it should autodetect the format. - **Output**: This function does not return anything. It updates the value of the specified key in the file. - **Constraints**: If the key does not exist in the settings, add it. The updated settings should be saved in the file in the same format it was read. **Example:** ```python # Example Usage: settings = { \'username\': \'user123\', \'volume\': 70, \'brightness\': 50.5, \'dark_mode\': True, \'recent_files\': [\'file1.txt\', \'file2.txt\'] } # Save the settings to both XML and Binary formats save_settings(settings, \'settings.xml\', fmt=\'FMT_XML\') save_settings(settings, \'settings.binary\', fmt=\'FMT_BINARY\') # Load the settings from the XML file loaded_settings = load_settings(\'settings.xml\') # Update a setting update_setting(\'settings.xml\', \'volume\', 80) # Verify the update assert load_settings(\'settings.xml\')[\'volume\'] == 80 ``` Your implementation must ensure that all features of plistlib are correctly utilized, and error handling is incorporated where necessary. # Submission Requirements: 1. Implement the functions `save_settings`, `load_settings`, and `update_setting`. 2. Provide appropriate error handling. 3. Demonstrate the functionality with at least one use case for each function in the code.","solution":"import plistlib import os def save_settings(settings, filename, fmt=\'FMT_XML\'): Saves the settings dictionary to a plist file. :param settings: Dictionary containing settings. :param filename: Name of the file to save settings. :param fmt: Format of the plist file (either \'FMT_XML\' or \'FMT_BINARY\'). format_map = {\'FMT_XML\': plistlib.FMT_XML, \'FMT_BINARY\': plistlib.FMT_BINARY} if fmt not in format_map: raise ValueError(\\"Invalid format specified. Use \'FMT_XML\' or \'FMT_BINARY\'.\\") with open(filename, \'wb\') as fp: plistlib.dump(settings, fp, fmt=format_map[fmt]) def load_settings(filename, fmt=None): Loads settings from a plist file. :param filename: Name of the file to load settings from. :param fmt: Format of the plist file. If not specified, autodetect the format. :return: Dictionary containing settings. :raises: Exception if file cannot be read. if not os.path.exists(filename): raise FileNotFoundError(f\\"The file {filename} does not exist.\\") with open(filename, \'rb\') as fp: return plistlib.load(fp) def update_setting(filename, key, value, fmt=None): Updates a setting in a plist file. :param filename: Name of the file to update settings in. :param key: Key of the setting to update. :param value: New value to set for the given key. :param fmt: Format of the plist file. If not specified, autodetect the format. settings = load_settings(filename, fmt) settings[key] = value save_settings(settings, filename, fmt=\'FMT_XML\' if filename.endswith(\'.xml\') else \'FMT_BINARY\')"},{"question":"**Objective**: Demonstrate your understanding of the `chunk` module by implementing a function using the provided `Chunk` class. **Problem Statement**: You are given a binary file that follows the EA IFF 85 chunked data format. Implement a function `read_chunks(filepath)` that reads the file and extracts all chunks\' IDs and their respective sizes. The function should return a list of tuples, where each tuple contains the chunk ID and the size of the chunk. # Function Signature ```python def read_chunks(filepath: str) -> list[tuple[str, int]]: ``` # Input - `filepath` (str): The path to the binary file containing the chunked data. # Output - `list[tuple[str, int]]`: A list of tuples. Each tuple should contain: - The chunk ID (str): A 4-character string identifying the type of chunk. - The chunk size (int): The size of the chunk data. # Constraints - The file size will not exceed 100MB. - The file will contain valid chunked data. # Example Let\'s assume the file contains the following chunks (simplified for explanation): - Chunk ID: \'FORM\', Size: 24 bytes - Chunk ID: \'AIFF\', Size: 12 bytes For this file, the function should return: ```python [(\'FORM\', 24), (\'AIFF\', 12)] ``` # Notes - Ensure proper handling of chunk alignment on 2-byte boundaries. - Handle both big-endian and little-endian byte orders as appropriate to the file format. - Use the methods provided by the `Chunk` class to read and navigate the chunks. # Additional Information Refer to the provided documentation for details on the `Chunk` class and its methods. **Your task**: Implement the `read_chunks(filepath)` function.","solution":"import chunk def read_chunks(filepath: str) -> list[tuple[str, int]]: Reads a binary file containing chunked data and returns a list of tuples with chunk IDs and their respective sizes. Args: filepath (str): The path to the binary file containing the chunked data. Returns: list[tuple[str, int]]: A list of tuples where each tuple contains a chunk ID (4-character string) and chunk size (integer). chunks = [] with open(filepath, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=True, align=True) chunks.append((ch.getname().decode(\'ascii\'), ch.getsize())) ch.skip() # Skip to the next chunk except EOFError: break return chunks"},{"question":"# Coding Assessment **Objective**: Demonstrate comprehension of the `fileinput` module by implementing a function that processes lines from multiple files, handles encoding, and supports use as a context manager. **Problem Statement**: You are tasked with writing a function `process_files` that reads from a list of input files or standard input, processes each line according to specified criteria, and writes the modified lines to a new output file. The function should handle different text encodings and provide in-place filtering option, creating a backup of the originals. **Function Signature**: ```python def process_files(input_files, output_file, encoding=\'utf-8\', inplace=False, backup_ext=\'.bak\'): Processes lines from input files and writes modified lines to the output file. Args: - input_files (list or str): A list of filenames or a single file name. If empty, processes sys.stdin. - output_file (str): The name of the output file. - encoding (str): The text encoding to use for reading and writing files. Defaults to \'utf-8\'. - inplace (bool): If True, modify files in place, creating backups as specified by backup_ext. Default is False. - backup_ext (str): The extension for backup files when inplace=True. Defaults to \'.bak\'. Returns: - None pass ``` **Requirements**: 1. Use the `fileinput` module to iterate over lines from the given input files or standard input. 2. If an input file name is `\'-\'`, it should be replaced by `sys.stdin`. 3. Apply any text processing of your choice (e.g., converting to uppercase) to each line. 4. Support different encodings specified by the `encoding` parameter. 5. If `inplace` is `True`, modify the input files in place and create a backup with the specified extension. 6. Ensure your function can handle reading compressed files recognized by gzip (`.gz`) and bzip2 (`.bz2`) extensions. 7. Ensure the function can be used as a context manager to properly manage file resources. **Example Usage**: ```python # Sample invocation: process_files([\'file1.txt\', \'file2.txt\'], \'output.txt\') # With in-place editing: process_files([\'file1.txt\'], None, inplace=True, backup_ext=\'.backup\') ``` **Constraints**: - Assume files are of manageable size for reading and processing line by line. - Ensure the function handles I/O errors gracefully. **Assessment Criteria**: - Correct usage of the `fileinput` module and its methods. - Proper implementation of text processing and file handling. - Correct handling of different encodings and compressed file formats. - Demonstrated understanding of context management and in-place filtering. - Robust error handling and code readability.","solution":"import fileinput import sys import os import shutil def process_files(input_files, output_file, encoding=\'utf-8\', inplace=False, backup_ext=\'.bak\'): Processes lines from input files and writes modified lines to the output file. Args: - input_files (list or str): A list of filenames or a single file name. If empty, processes sys.stdin. - output_file (str): The name of the output file. - encoding (str): The text encoding to use for reading and writing files. Defaults to \'utf-8\'. - inplace (bool): If True, modify files in place, creating backups as specified by backup_ext. Default is False. - backup_ext (str): The extension for backup files when inplace=True. Defaults to \'.bak\'. Returns: - None input_files_list = input_files if isinstance(input_files, list) else [input_files] with fileinput.input(files=input_files_list if input_files_list else (\'-\',), inplace=inplace, backup=backup_ext, mode=\'r\', encoding=encoding) as f: if inplace: for line in f: processed_line = line.upper() # Example processing: converting to uppercase print(processed_line, end=\'\') else: with open(output_file, \'w\', encoding=encoding) as outfile: for line in f: processed_line = line.upper() # Example processing: converting to uppercase outfile.write(processed_line)"},{"question":"# Python 3.10 Custom Extension Type You are tasked with creating a custom Python type using the `PyTypeObject` structure as defined in the provided documentation. Follow the detailed steps and constraints to ensure your type is correctly implemented and integrates well with Python\'s object system. Requirements: 1. **Type Definition**: Define a new type, `MyCustomType`, using the `PyTypeObject` structure. 2. **Attributes and Methods**: - Your type should include at least: - A string attribute called `name`. - A numerical attribute called `value`. - Support for weak references and instance dictionaries. - Implement the following methods: - `__init__(self, name, value)`: Initialize `name` and `value`. - `__repr__(self)`: Return a string `\\"MyCustomType(name=\'<name>\', value=<value>)\\"`. - `__richcompare__(self, other, op)`: Support (`<`, `<=`, `==`, `!=`, `>`, `>=`) for `value` attribute. - `__hash__(self)`: Hash based on `name` and `value`. - Implement methods to support garbage collection properly. Constraints: - Use only the structures, slot definitions, and functions specified in the provided documentation. - Ensure memory management is adhered to (e.g., proper deallocation and reference handling). - Ensure robust error handling in your methods. Coding Task: Provide the complete C code to define `MyCustomType`, starting from the structure definition down to the type initialization function. Include comments to explain each part of your code.","solution":"class MyCustomType: def __init__(self, name, value): if not isinstance(name, str): raise TypeError(f\\"Expected \'name\' to be str, got {type(name).__name__}\\") if not isinstance(value, (int, float)): raise TypeError(f\\"Expected \'value\' to be int or float, got {type(value).__name__}\\") self.name = name self.value = value def __repr__(self): return f\\"MyCustomType(name=\'{self.name}\', value={self.value})\\" def __eq__(self, other): if isinstance(other, MyCustomType): return self.value == other.value and self.name == other.name return False def __lt__(self, other): if isinstance(other, MyCustomType): return (self.value, self.name) < (other.value, other.name) return NotImplemented def __le__(self, other): return self == other or self < other def __gt__(self, other): if isinstance(other, MyCustomType): return (self.value, self.name) > (other.value, other.name) return NotImplemented def __ge__(self, other): return self == other or self > other def __ne__(self, other): return not self == other def __hash__(self): return hash((self.name, self.value))"},{"question":"**Title**: Manipulating XML Documents using DOM in Python **Objective**: Test the ability to manipulate XML documents using the DOM API in Python. **Problem Statement**: Write a Python function `create_invoice_xml` that takes a dictionary representing invoice data and returns a string containing the formatted XML of the invoice. Use the `xml.dom.minidom` module to construct the XML document. # Function Signature ```python def create_invoice_xml(invoice_data: dict) -> str: pass ``` # Input - `invoice_data`: A dictionary with the following structure: ```python { \\"invoice_number\\": \\"123456\\", \\"date\\": \\"2023-10-28\\", \\"billing_address\\": { \\"name\\": \\"John Doe\\", \\"street\\": \\"1234 Elm Street\\", \\"city\\": \\"Somewhere\\", \\"state\\": \\"CA\\", \\"zip\\": \\"90210\\" }, \\"line_items\\": [ {\\"description\\": \\"Widget\\", \\"quantity\\": 4, \\"price\\": 19.99}, {\\"description\\": \\"Gadget\\", \\"quantity\\": 2, \\"price\\": 22.50} ], \\"total\\": 104.98 } ``` # Output - A string containing the formatted XML representation of the invoice. # Requirements 1. Use the `xml.dom.minidom` module to create the XML document. 2. Ensure that the XML structure matches the following format: ```xml <Invoice> <InvoiceNumber>123456</InvoiceNumber> <Date>2023-10-28</Date> <BillingAddress> <Name>John Doe</Name> <Street>1234 Elm Street</Street> <City>Somewhere</City> <State>CA</State> <ZIP>90210</ZIP> </BillingAddress> <LineItems> <Item> <Description>Widget</Description> <Quantity>4</Quantity> <Price>19.99</Price> </Item> <Item> <Description>Gadget</Description> <Quantity>2</Quantity> <Price>22.50</Price> </Item> </LineItems> <Total>104.98</Total> </Invoice> ``` 3. Error handling: - Raise a `ValueError` if the dictionary doesn\'t have the required keys or if the quantities and prices are not valid numbers. # Constraints - The `quantity` for each item must be an integer. - The `price` for each item and the `total` must be a float. # Example Usage ```python invoice_data = { \\"invoice_number\\": \\"123456\\", \\"date\\": \\"2023-10-28\\", \\"billing_address\\": { \\"name\\": \\"John Doe\\", \\"street\\": \\"1234 Elm Street\\", \\"city\\": \\"Somewhere\\", \\"state\\": \\"CA\\", \\"zip\\": \\"90210\\" }, \\"line_items\\": [ {\\"description\\": \\"Widget\\", \\"quantity\\": 4, \\"price\\": 19.99}, {\\"description\\": \\"Gadget\\", \\"quantity\\": 2, \\"price\\": 22.50} ], \\"total\\": 104.98 } xml_output = create_invoice_xml(invoice_data) print(xml_output) ``` The `xml_output` should contain the XML string as shown in the requirements. **Hint**: You can use the `toprettyxml()` method from `xml.dom.minidom` to prettify the generated XML string.","solution":"from xml.dom.minidom import Document def create_invoice_xml(invoice_data: dict) -> str: required_keys = [\\"invoice_number\\", \\"date\\", \\"billing_address\\", \\"line_items\\", \\"total\\"] for key in required_keys: if key not in invoice_data: raise ValueError(f\\"Missing required key: {key}\\") # Create the minidom document doc = Document() # Create main <Invoice> element invoice_element = doc.createElement(\\"Invoice\\") doc.appendChild(invoice_element) # Create and append <InvoiceNumber> element invoice_number_element = doc.createElement(\\"InvoiceNumber\\") invoice_number_element.appendChild(doc.createTextNode(invoice_data[\\"invoice_number\\"])) invoice_element.appendChild(invoice_number_element) # Create and append <Date> element date_element = doc.createElement(\\"Date\\") date_element.appendChild(doc.createTextNode(invoice_data[\\"date\\"])) invoice_element.appendChild(date_element) # Create <BillingAddress> element and its children billing_address = invoice_data[\\"billing_address\\"] if not all(k in billing_address for k in [\\"name\\", \\"street\\", \\"city\\", \\"state\\", \\"zip\\"]): raise ValueError(\\"Billing address must contain name, street, city, state, and zip\\") billing_address_element = doc.createElement(\\"BillingAddress\\") invoice_element.appendChild(billing_address_element) name_element = doc.createElement(\\"Name\\") name_element.appendChild(doc.createTextNode(billing_address[\\"name\\"])) billing_address_element.appendChild(name_element) street_element = doc.createElement(\\"Street\\") street_element.appendChild(doc.createTextNode(billing_address[\\"street\\"])) billing_address_element.appendChild(street_element) city_element = doc.createElement(\\"City\\") city_element.appendChild(doc.createTextNode(billing_address[\\"city\\"])) billing_address_element.appendChild(city_element) state_element = doc.createElement(\\"State\\") state_element.appendChild(doc.createTextNode(billing_address[\\"state\\"])) billing_address_element.appendChild(state_element) zip_element = doc.createElement(\\"ZIP\\") zip_element.appendChild(doc.createTextNode(billing_address[\\"zip\\"])) billing_address_element.appendChild(zip_element) # Create and append <LineItems> and each <Item> within it line_items_element = doc.createElement(\\"LineItems\\") invoice_element.appendChild(line_items_element) for item in invoice_data[\\"line_items\\"]: if not all(k in item for k in [\\"description\\", \\"quantity\\", \\"price\\"]): raise ValueError(\\"Each line item must contain description, quantity, and price\\") if not isinstance(item[\\"quantity\\"], int): raise ValueError(\\"Quantity must be an integer\\") if not isinstance(item[\\"price\\"], float): raise ValueError(\\"Price must be a float\\") item_element = doc.createElement(\\"Item\\") line_items_element.appendChild(item_element) description_element = doc.createElement(\\"Description\\") description_element.appendChild(doc.createTextNode(item[\\"description\\"])) item_element.appendChild(description_element) quantity_element = doc.createElement(\\"Quantity\\") quantity_element.appendChild(doc.createTextNode(str(item[\\"quantity\\"]))) item_element.appendChild(quantity_element) price_element = doc.createElement(\\"Price\\") price_element.appendChild(doc.createTextNode(f\\"{item[\'price\']:.2f}\\")) item_element.appendChild(price_element) # Create and append <Total> element if not isinstance(invoice_data[\'total\'], float): raise ValueError(\\"Total must be a float\\") total_element = doc.createElement(\\"Total\\") total_element.appendChild(doc.createTextNode(f\\"{invoice_data[\'total\']:.2f}\\")) invoice_element.appendChild(total_element) return doc.toprettyxml(indent=\\" \\")"},{"question":"You are given a task to create a function `parse_and_build(args: tuple) -> tuple` that efficiently parses a tuple of arguments and returns a new tuple containing converted values. Requirements: 1. The function should handle the following types of arguments: - A unicode string, which should be converted to a UTF-8 encoded `const char *`. - An integer, which should be converted to an unsigned short int. - A Python float, which should be converted to a `double`. - A Python `bytes` object, which should be converted to a borrowed character string. 2. After parsing these arguments, the function should create a new Python tuple that: - Contains the original unicode string. - Contains the integer value multiplied by 2. - Contains the float value squared. - Contains the original `bytes` object. 3. The function should raise appropriate exceptions if the arguments do not match the expected formats. Function Signature: ```python def parse_and_build(args: tuple) -> tuple: pass ``` Example Usage: ```python result = parse_and_build((\\"example\\", 5, 3.14, b\\"bytes_example\\")) print(result) # Expected output: (\'example\', 10, 9.8596, b\\"bytes_example\\") ``` Constraints: - The tuple `args` will have exactly four elements. - If the first element is not a unicode string, raise a `ValueError` exception. - If the second element is not an integer, raise a `ValueError` exception. - If the third element is not a float, raise a `ValueError` exception. - If the fourth element is not a bytes object, raise a `ValueError` exception. Implement the `parse_and_build` function to meet these specifications using the concepts from the provided documentation.","solution":"def parse_and_build(args: tuple) -> tuple: if len(args) != 4: raise ValueError(\\"The input tuple must have exactly four elements.\\") unicode_string, integer, float_value, bytes_object = args # Validate and process the unicode string if not isinstance(unicode_string, str): raise ValueError(\\"The first argument must be a unicode string.\\") # Validate and process the integer if not isinstance(integer, int): raise ValueError(\\"The second argument must be an integer.\\") # Validate and process the float if not isinstance(float_value, float): raise ValueError(\\"The third argument must be a float.\\") # Validate and process the bytes object if not isinstance(bytes_object, bytes): raise ValueError(\\"The fourth argument must be a bytes object.\\") # Create the output tuple new_tuple = (unicode_string, integer * 2, float_value ** 2, bytes_object) return new_tuple"},{"question":"Objective Write a Python function that demonstrates your understanding of the `PyOS_string_to_double`, `PyOS_double_to_string`, and `PyOS_snprintf` functions from the `python310` package. You will implement a function that converts a string representation of a number to a double, formats the double as a string, and then formats a message using the formatted string. Function Signature ```python def process_and_format_number(input_str: str, format_code: str, precision: int, buffer_size: int) -> str: pass ``` Inputs - `input_str` (str): A string representing a number. The string must not have leading or trailing whitespace. - `format_code` (str): A format code for the double to string conversion. It should be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. - `precision` (int): The precision for the double to string conversion. Must be 0 for the format code \'r\'. - `buffer_size` (int): The size of the buffer for `PyOS_snprintf`. Outputs - Returns a formatted string containing the original input, the converted double, and the formatted string using `PyOS_snprintf`. Requirements 1. Convert `input_str` to a double using `PyOS_string_to_double`. 2. Convert the double to a formatted string using `PyOS_double_to_string`. 3. Format a message utilizing `PyOS_snprintf` that includes: - The original input string. - The converted double. - The formatted string of the double. 4. Handle errors appropriately, including invalid input strings and buffer size limitations. Constraints - You may assume valid input for format codes and precision, but you must handle errors in string to double conversion. - Use a consistent buffer size for `PyOS_snprintf` to ensure the message fits within the buffer. Example ```python result = process_and_format_number(\\"123.456\\", \\"f\\", 2, 100) print(result) # Output example: \\"Original: 123.456, Double: 123.456, Formatted: 123.46\\" ``` Notes - Carefully manage memory for any allocated buffers. - Ensure proper error handling and edge case testing from the documentation.","solution":"import ctypes def process_and_format_number(input_str: str, format_code: str, precision: int, buffer_size: int) -> str: Convert a string representation of a number to a double, format the double as a string, and then formats a message using the formatted string. Args: - input_str (str): A string representing a number. - format_code (str): A format code for the double to string conversion. - precision (int): The precision for the double to string conversion. - buffer_size (int): The size of the buffer. Returns: - str: A formatted string containing the original input, the converted double, and the formatted string. # Convert the input string to a double try: double_value = float(input_str) except ValueError: return \\"Error: Invalid input string\\" # Set the format specifier for the double to string conversion if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\']: return \\"Error: Invalid format code\\" if format_code == \'r\': formatted_str = repr(double_value) else: formatted_str = f\\"{double_value:.{precision}{format_code}}\\" # Ensure buffer_size is sufficient required_buffer_size = len(f\\"Original: {input_str}, Double: {double_value}, Formatted: {formatted_str}\\") + 1 if buffer_size < required_buffer_size: return \\"Error: Buffer size too small\\" # Format the message formatted_message = f\\"Original: {input_str}, Double: {double_value}, Formatted: {formatted_str}\\" return formatted_message"},{"question":"# Advanced Seaborn Plotting Challenge You have been provided with a dataset about models and their scores on various tasks. Your task is to manipulate this dataset and create an advanced plot using Seaborn. Dataset Description The dataset is loaded using the following code: ```python import seaborn as sns glue = sns.load_dataset(\\"glue\\") ``` `glue` is a pandas DataFrame with the following columns: - `Model`: The name of the model. - `Encoder`: The type of encoder used by the model. - `Task`: The name of the task. - `Score`: The score of the model on the task. Task 1. **Data Preparation**: - Pivot the dataset so that each row represents a model-encoder combination, each column represents a task, and the values are the scores of the models on those tasks. - Add a new column called `Average` which is the average score of each model across all tasks. - Sort the DataFrame based on the `Average` score in descending order. 2. **Plotting**: - Create a plot with the following specifics: - X-axis represents the score on the `SST-2` task. - Y-axis represents the score on the `MRPC` task. - Each point should be a model-encoder combination. - The color of the points should represent the `Encoder`. - Add the model names as text annotations above the points. - Customize the plot: - Horizontally align the text annotations. - Use bold font for the text annotations. - Map the horizontal alignment of text based on `Encoder`, where `LSTM` should be left-aligned and `Transformer` should be right-aligned. Expected Input and Output - **Input**: None. The dataset is loaded using Seaborn\'s `load_dataset` function. - **Output**: A visualization plot. Constraints - Use only the provided dataset and the seaborn library. - Ensure the text annotations are clearly legible and do not overlap excessively. Example Here is how you can start: ```python import seaborn.objects as so # Data Preparation glue = sns.load_dataset(\\"glue\\").pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") glue = glue.assign(Average=lambda x: x.mean(axis=1).round(1)).sort_values(\\"Average\\", ascending=False) # Plotting (...) ``` Performance Requirements - The code should run efficiently on datasets of similar size without significant delays.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Data Preparation glue = sns.load_dataset(\\"glue\\") glue_pivot = glue.pivot_table(index=[\'Model\', \'Encoder\'], columns=\'Task\', values=\'Score\') glue_pivot = glue_pivot.assign(Average=lambda x: x.mean(axis=1)).sort_values(\'Average\', ascending=False).reset_index() def plot_glue_data(glue_pivot): plt.figure(figsize=(14, 10)) ax = sns.scatterplot(data=glue_pivot, x=\'SST-2\', y=\'MRPC\', hue=\'Encoder\', s=100) for i, row in glue_pivot.iterrows(): alignment = \'left\' if row[\'Encoder\'] == \'LSTM\' else \'right\' ax.text(row[\'SST-2\'], row[\'MRPC\'], row[\'Model\'], fontsize=12, weight=\'bold\', horizontalalignment=alignment, verticalalignment=\'bottom\') plt.title(\'Scores of Models on SST-2 vs MRPC Tasks\') plt.xlabel(\'SST-2 Score\') plt.ylabel(\'MRPC Score\') plt.legend(title=\'Encoder\') plt.grid(True) plt.show() plot_glue_data(glue_pivot)"},{"question":"# Objective: Write a Python program to implement a function that: 1. Decodes an input encoded string using a specified encoding. 2. Translates the decoded Unicode string based on a given transformation table. 3. Encodes the transformed Unicode string into a specified encoding. # Function Signature: ```python def transform_and_reencode(input_string: str, decode_encoding: str, encode_encoding: str, transformation_table: dict) -> bytes: Transforms an encoded input string by decoding it, applying a transformation based on the provided table, and re-encoding the transformed string to the specified encoding. :param input_string: The encoded input string (bytes). :param decode_encoding: The encoding used to decode the input string. :param encode_encoding: The encoding to encode the transformed string. :param transformation_table: A dictionary mapping characters to their transformation (in Unicode ordinals). :return: A bytes object of the re-encoded transformed string. pass ``` # Instructions: 1. **Decode the Input String:** - Use the `decode_encoding` to decode `input_string` into a Unicode string. 2. **Transform the Decoded String:** - Use the provided `transformation_table` to transform the characters in the decoded string. The table maps Unicode characters to their target transformation. - Characters not present in the transformation table should remain unchanged. 3. **Re-encode the Transformed String:** - Encode the transformed string into the specified `encode_encoding` and return this encoded byte sequence. # Constraints and Notes: - The `input_string` will always be valid bytes for the given `decode_encoding`. - The `transformation_table` is provided as a dictionary where keys are characters (`str` of length 1) and values are transformed characters (also `str` of length 1). - The output should be a bytes object encoded with the given `encode_encoding`. - You can assume the encodings provided will be valid and supported by Python\'s standard library. # Example: ```python input_str = b\'xe4xbdxa0xe5xa5xbd\' # \\"你\\" encoded in UTF-8 decode_enc = \'utf-8\' encode_enc = \'ascii\' transformation_table = { \'你\': \'n\', # Transform the character \'你\' to \'n\' \'好\': \'h\' # Transform the character \'好\' to \'h\' } result = transform_and_reencode(input_str, decode_enc, encode_enc, transformation_table) print(result) # Output should be: b\'nh\', representing \\"nh\\" encoded in ASCII ``` # Testing: - Test your function with multiple encodings and different transformation tables. - Ensure the function handles characters not present in the transformation table by leaving them unchanged. - Verify that the function raises appropriate errors or handles edge cases like empty input strings gracefully.","solution":"def transform_and_reencode(input_string: bytes, decode_encoding: str, encode_encoding: str, transformation_table: dict) -> bytes: Transforms an encoded input string by decoding it, applying a transformation based on the provided table, and re-encoding the transformed string to the specified encoding. :param input_string: The encoded input string (bytes). :param decode_encoding: The encoding used to decode the input string. :param encode_encoding: The encoding to encode the transformed string. :param transformation_table: A dictionary mapping characters to their transformation (in Unicode ordinals). :return: A bytes object of the re-encoded transformed string. # Step 1: Decode the input string using the specified decoding encoding decoded_string = input_string.decode(decode_encoding) # Step 2: Transform the characters in the decoded string based on the transformation table transformed_string = \'\'.join(transformation_table.get(char, char) for char in decoded_string) # Step 3: Re-encode the transformed string using the specified encoding reencoded_bytes = transformed_string.encode(encode_encoding) return reencoded_bytes"},{"question":"# Data Visualization with Seaborn PairPlot Objective Your task is to write a Python function that creates a customized pair plot using the seaborn library. The pair plot should visualize the relationships between variables in the provided dataset, with additional customizations based on the given parameters. Function Signature ```python def create_custom_pairplot(dataset: pd.DataFrame, hue: str, kind: str, diag_kind: str, vars: list, height: float, markers: list, plot_kws: dict, diag_kws: dict) -> sns.PairGrid: pass ``` Input - `dataset` (pd.DataFrame): A pandas DataFrame containing the dataset to be visualized. - `hue` (str): The name of the column in the dataset to be used for coloring the plots based on categories. - `kind` (str): The kind of plots to use for both diagonal and off-diagonal elements. Possible values: `\'scatter\'`, `\'kde\'`, `\'hist\'`. - `diag_kind` (str): The kind of plot to use for the diagonal elements. Possible values: `\'auto\'`, `\'hist\'`, `\'kde\'`. - `vars` (list): A list of column names to be used for the pair plot. If None, all variables are plotted. - `height` (float): The height of each subplot in inches. - `markers` (list): A list of markers to use for the different levels of the `hue` variable. - `plot_kws` (dict): A dictionary of keyword arguments to customize the off-diagonal plots. - `diag_kws` (dict): A dictionary of keyword arguments to customize the diagonal plots. Output - Returns an `sns.PairGrid` object representing the created pair plot. Example ```python import seaborn as sns import pandas as pd # Load sample dataset penguins = sns.load_dataset(\\"penguins\\") # Function call pairplot_obj = create_custom_pairplot( dataset=penguins, hue=\\"species\\", kind=\\"scatter\\", diag_kind=\\"hist\\", vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"], height=2.5, markers=[\\"o\\", \\"s\\", \\"D\\"], plot_kws=dict(marker=\\"+\\", color=\\"blue\\"), diag_kws=dict(fill=True, color=\\"red\\") ) pairplot_obj.savefig(\\"custom_pairplot.png\\") ``` Constraints - Ensure that all variables mentioned in `vars` exist in the given dataset. - The `markers` list should have as many elements as there are unique values in the `hue` column. - Appropriate error handling should be included to handle situations such as missing columns or incorrect parameter values. Performance - The function should handle datasets with up to 1000 rows and 10 columns efficiently.","solution":"import seaborn as sns import pandas as pd def create_custom_pairplot(dataset: pd.DataFrame, hue: str, kind: str, diag_kind: str, vars: list, height: float, markers: list, plot_kws: dict, diag_kws: dict) -> sns.PairGrid: Creates a customized pair plot using Seaborn to visualize relationships between variables in the dataset. Parameters: - dataset (pd.DataFrame): The dataset to visualize. - hue (str): Column name for color encoding. - kind (str): Type of plots for off-diagonal elements. Values: \'scatter\', \'kde\', \'hist\'. - diag_kind (str): Type of plot for diagonal elements. Values: \'auto\', \'hist\', \'kde\'. - vars (list): List of variables to include in the plot. If None, all variables are used. - height (float): Height of each subplot in inches. - markers (list): List of markers corresponding to unique values in the hue column. - plot_kws (dict): Customization options for off-diagonal plots. - diag_kws (dict): Customization options for diagonal plots. Returns: - sns.PairGrid: PairGrid object representing the created pair plot. # Validate the input parameters if not isinstance(dataset, pd.DataFrame): raise ValueError(\\"dataset must be a pandas DataFrame\\") if hue not in dataset.columns: raise ValueError(f\\"Column \'{hue}\' not found in dataset\\") if vars: for var in vars: if var not in dataset.columns: raise ValueError(f\\"Column \'{var}\' not found in dataset\\") unique_hue_values = dataset[hue].nunique() if len(markers) != unique_hue_values: raise ValueError(f\\"Length of markers list must be equal to the number of unique values in the hue column ({unique_hue_values})\\") # Create pair plot pairplot_grid = sns.pairplot( dataset, hue=hue, kind=kind, diag_kind=diag_kind, vars=vars, height=height, markers=markers, plot_kws=plot_kws, diag_kws=diag_kws ) return pairplot_grid"},{"question":"**Coding Assessment Question:** # Objective: To assess your understanding of seaborn objects, theming, and faceting in visualization. # Problem Statement: You are given a dataset containing information about different species of penguins. Your task is to create a series of plots that adhere to specific requirements. The ultimate goal is to produce a cleaner and well-styled visualization. # Dataset Description: You will use the \\"penguins\\" dataset, which can be loaded from seaborn\'s built-in datasets. Each row in the dataset contains measurements for a single penguin. - **species**: Species of penguin. - **island**: Island where the penguin was found. - **bill_length_mm**: Length of the penguin\'s bill in millimeters. - **bill_depth_mm**: Depth of the penguin\'s bill in millimeters. - **flipper_length_mm**: Length of the penguin\'s flipper in millimeters. - **body_mass_g**: Body mass of the penguin in grams. - **sex**: Sex of the penguin. # Tasks: 1. Load the \\"penguins\\" dataset from seaborn. 2. Create a scatter plot that shows the relationship between `bill_length_mm` and `bill_depth_mm` for each species, using different colors for each species. 3. Facet the plot by the `island` variable, arranging the facets in a grid. 4. Fit a linear regression line to each scatter plot. 5. Apply a theme that changes: - The background of the plots to white. - The edge color of the axes to grey. - The line width of the regression lines to 2. 6. Display the final plot. # Constraints: - Use seaborn\'s `objects` interface for creating plots. - Ensure that your plots are well-labeled and easy to read. # Example Output: While the figure cannot be exactly displayed here, you should aim for an aesthetically pleasing representation of the data, with facets for each island and correctly colored scatter plots for each species, overlaid with a linear regression fit. # Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Define the plot p = (so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\") .facet(\\"island\\") .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) .theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"grey\\", \\"lines.linewidth\\": 2})) # Display the plot p.show() # Call the function to create the plot create_penguin_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Define the plot p = (so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\") .facet(\\"island\\") .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) .theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"grey\\", \\"lines.linewidth\\": 2 })) # Display the plot p.show() # Call the function to create the plot create_penguin_plot()"},{"question":"You are tasked to develop a Python program that will perform the following tasks using the `urllib.request` package: 1. Fetch a given URL and print the first 500 characters of its content. 2. Make a POST request to a specified URL with some data and print the response. 3. Handle HTTP and URL errors gracefully and print appropriate messages. 4. Add a user-agent header to the requests. 5. Use an opener with a custom timeout for the requests. # Requirements 1. **Fetch URL Content**: - Implement a function `fetch_url_content(url: str) -> str` that takes a URL as input, performs a GET request using `urlopen`, and returns the first 500 characters of the content as a string. 2. **POST Request**: - Implement a function `perform_post_request(url: str, data: dict) -> str` that takes a URL and a dictionary of data, performs a POST request by encoding the data, and returns the response as a string. 3. **Error Handling**: - Enhance both functions to handle `URLError` and `HTTPError`. Print \\"Failed to reach the server: <reason>\\" for `URLError` and \\"HTTP error code: <code>\\" for `HTTPError`. 4. **Add User-Agent**: - Modify the functions to add a custom user-agent, e.g., `\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\'`. 5. **Custom Opener with Timeout**: - Create a custom opener with a timeout of 10 seconds and use it for all requests. # Constraints - You can assume the URLs provided will be valid. - The POST data should be encoded using ASCII. # Input Format - For `fetch_url_content`: A single string representing the URL. - For `perform_post_request`: A string representing the URL and a dictionary representing the data to be posted. # Output Format - For `fetch_url_content`: The first 500 characters of the fetched URL content. - For `perform_post_request`: The response content as a string. # Example ```python url = \'http://example.com\' data = { \'name\': \'John Doe\', \'email\': \'john@example.com\' } # Fetch URL content print(fetch_url_content(url)) # Perform POST request print(perform_post_request(\'http://httpbin.org/post\', data)) ``` # Implementation You are required to implement the following functions: ```python import urllib.request import urllib.parse import socket def fetch_url_content(url: str) -> str: # Your code here pass def perform_post_request(url: str, data: dict) -> str: # Your code here pass ``` Make sure to handle exceptions, add user-agent headers, and use a custom opener with the specified timeout.","solution":"import urllib.request import urllib.parse import socket from urllib.error import URLError, HTTPError # Set a custom user-agent header USER_AGENT = \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\' def fetch_url_content(url: str) -> str: headers = {\'User-Agent\': USER_AGENT} request = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(request, timeout=10) as response: content = response.read(500).decode(\'utf-8\') return content except HTTPError as e: print(f\\"HTTP error code: {e.code}\\") except URLError as e: print(f\\"Failed to reach the server: {e.reason}\\") return \\"\\" def perform_post_request(url: str, data: dict) -> str: headers = {\'User-Agent\': USER_AGENT} encoded_data = urllib.parse.urlencode(data).encode(\'ascii\') request = urllib.request.Request(url, data=encoded_data, headers=headers) try: with urllib.request.urlopen(request, timeout=10) as response: response_data = response.read().decode(\'utf-8\') return response_data except HTTPError as e: print(f\\"HTTP error code: {e.code}\\") except URLError as e: print(f\\"Failed to reach the server: {e.reason}\\") return \\"\\""},{"question":"**Question:** # Task You are required to implement a function using Unix-specific services to handle system logging. The client requires a function to log messages with various severity levels to the Unix syslog using the `syslog` library routines. # Function Signature ```python def log_message(priority: str, message: str) -> None: pass ``` # Input - `priority` (str): A string indicating the priority level of the log message. It can be one of the following: `DEBUG`, `INFO`, `NOTICE`, `WARNING`, `ERROR`, `CRITICAL`, `ALERT`, `EMERGENCY`. - `message` (str): The log message to be written. # Output - The function does not return anything. It will log the message with the specified priority level to the system logger. # Constraints - You must use the `syslog` module for logging. - The mapping of string priorities to `syslog` priorities is as follows: - `DEBUG` -> `syslog.LOG_DEBUG` - `INFO` -> `syslog.LOG_INFO` - `NOTICE` -> `syslog.LOG_NOTICE` - `WARNING` -> `syslog.LOG_WARNING` - `ERROR` -> `syslog.LOG_ERR` - `CRITICAL` -> `syslog.LOG_CRIT` - `ALERT` -> `syslog.LOG_ALERT` - `EMERGENCY` -> `syslog.LOG_EMERG` # Example ```python log_message(\\"ERROR\\", \\"This is an error message\\") ``` This should log the message \\"This is an error message\\" with the priority level `ERROR` to the system logger. # Notes - If an invalid priority string is provided, the function should raise a `ValueError` with an appropriate error message. - Ensure that the logging system is properly closed after logging the message to prevent resource leaks. # Implementation You can start your implementation by importing the necessary components from the `syslog` module and mapping the priority levels to their corresponding syslog constants. Then, use the `syslog.syslog` function to log the message. ```python import syslog def log_message(priority: str, message: str) -> None: priority_mapping = { \\"DEBUG\\": syslog.LOG_DEBUG, \\"INFO\\": syslog.LOG_INFO, \\"NOTICE\\": syslog.LOG_NOTICE, \\"WARNING\\": syslog.LOG_WARNING, \\"ERROR\\": syslog.LOG_ERR, \\"CRITICAL\\": syslog.LOG_CRIT, \\"ALERT\\": syslog.LOG_ALERT, \\"EMERGENCY\\": syslog.LOG_EMERG, } if priority not in priority_mapping: raise ValueError(f\\"Invalid priority level: {priority}\\") syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) syslog.syslog(priority_mapping[priority], message) syslog.closelog() ```","solution":"import syslog def log_message(priority: str, message: str) -> None: Logs a message with the specified priority level to the system logger. Args: priority (str): A string indicating the priority level of the log message. message (str): The log message to be written. Raises: ValueError: If the specified priority level is invalid. priority_mapping = { \\"DEBUG\\": syslog.LOG_DEBUG, \\"INFO\\": syslog.LOG_INFO, \\"NOTICE\\": syslog.LOG_NOTICE, \\"WARNING\\": syslog.LOG_WARNING, \\"ERROR\\": syslog.LOG_ERR, \\"CRITICAL\\": syslog.LOG_CRIT, \\"ALERT\\": syslog.LOG_ALERT, \\"EMERGENCY\\": syslog.LOG_EMERG, } if priority not in priority_mapping: raise ValueError(f\\"Invalid priority level: {priority}\\") syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) syslog.syslog(priority_mapping[priority], message) syslog.closelog()"},{"question":"**Question: Implementing a Custom Data Structure with Abstract Base Classes** Your task is to design and implement a custom data structure `ListBasedSet` that behaves like a set but is implemented using a list. This custom data structure should derive from the `collections.abc.Set` ABC to ensure it provides all necessary set operations. # Requirements: 1. `ListBasedSet` should inherit from `collections.abc.Set`. 2. The basic set methods (`__contains__`, `__iter__`, and `__len__`) should be implemented. 3. Use mixins provided by the `Set` ABC for other set operations like union (`|`), intersection (`&`), etc. 4. Ensure that `ListBasedSet` can handle any iterable during initialization. # Class Definition ```python from collections.abc import Set class ListBasedSet(Set): def __init__(self, iterable): self.elements = [] for item in iterable: if item not in self.elements: self.elements.append(item) def __contains__(self, value): return value in self.elements def __iter__(self): return iter(self.elements) def __len__(self): return len(self.elements) ``` # Your Task: - Complete the implementation of `ListBasedSet` by following the class definition provided above. - Write test cases for each set operation supported by `Set` ABC including equality, subset, and superset checks. - Optimize the `ListBasedSet` to efficiently handle large datasets. # Constraints: - You are not allowed to use Python\'s built-in `set` type. - The elements stored in `ListBasedSet` should maintain insertion order (this behavior is inherited from `list`). # Example Usage: ```python s1 = ListBasedSet([1, 2, 3, 4]) s2 = ListBasedSet([3, 4, 5, 6]) # Intersection assert s1 & s2 == ListBasedSet([3, 4]) # Union assert s1 | s2 == ListBasedSet([1, 2, 3, 4, 5, 6]) # Symmetric Difference assert s1 ^ s2 == ListBasedSet([1, 2, 5, 6]) # Subset Check assert ListBasedSet([1, 2]) <= s1 # Superset Check assert s1 >= ListBasedSet([1, 2]) ``` **Performance Requirement**: Your `ListBasedSet` should have an average time complexity of O(n) for set operations where n is the number of elements in the largest set involved in the operation. Good luck!","solution":"from collections.abc import Set class ListBasedSet(Set): def __init__(self, iterable): self.elements = [] self._set = set() for item in iterable: if item not in self._set: self.elements.append(item) self._set.add(item) def __contains__(self, value): return value in self._set def __iter__(self): return iter(self.elements) def __len__(self): return len(self.elements) def __repr__(self): return f\\"ListBasedSet({self.elements})\\""},{"question":"**Coding Assessment Question** You are provided with the following utility functions in the `torch.ao.ns.fx.utils` module: - `compute_sqnr(x, y)`: Computes the Signal to Quantization Noise Ratio between tensors `x` and `y`. - `compute_normalized_l2_error(x, y)`: Computes the normalized L2 error between tensors `x` and `y`. - `compute_cosine_similarity(x, y)`: Computes the cosine similarity between tensors `x` and `y`. # Task Your task is to implement a function named `tensor_similarity_metrics`, which will take two tensors as input, compute various similarity metrics using the provided utility functions, and return the results in a dictionary format. # Function Signature ```python def tensor_similarity_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> dict: pass ``` # Expected Input and Output - **Input**: - `tensor1` (torch.Tensor): A PyTorch tensor of any shape. - `tensor2` (torch.Tensor): A PyTorch tensor of the same shape as `tensor1`. - **Output**: - `metrics` (dict): A dictionary containing the following key-value pairs: - `\'sqnr\'`: The Signal to Quantization Noise Ratio between `tensor1` and `tensor2`. - `\'normalized_l2_error\'`: The normalized L2 error between `tensor1` and `tensor2`. - `\'cosine_similarity\'`: The cosine similarity between `tensor1` and `tensor2`. # Constraints - Both input tensors, `tensor1` and `tensor2`, should have the same shape. - The solution should leverage the provided utility functions for computing the metrics. - The function should handle tensors of any dimensionality as long as both input tensors are of the same shape. # Example ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def tensor_similarity_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> dict: sqnr = compute_sqnr(tensor1, tensor2) normalized_l2_error = compute_normalized_l2_error(tensor1, tensor2) cosine_similarity = compute_cosine_similarity(tensor1, tensor2) return { \'sqnr\': sqnr, \'normalized_l2_error\': normalized_l2_error, \'cosine_similarity\': cosine_similarity } # Example usage tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([1.1, 1.9, 3.0]) metrics = tensor_similarity_metrics(tensor1, tensor2) print(metrics) # Output might look like: {\'sqnr\': <value>, \'normalized_l2_error\': <value>, \'cosine_similarity\': <value>} ``` In this question, you demonstrate your understanding of tensor operations, making use of utility functions to perform numerical comparisons between tensors.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def tensor_similarity_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> dict: Compute various similarity metrics between two tensors. Parameters: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. Returns: dict: A dictionary with keys \'sqnr\', \'normalized_l2_error\', and \'cosine_similarity\' containing the corresponding metric values. sqnr = compute_sqnr(tensor1, tensor2) normalized_l2_error = compute_normalized_l2_error(tensor1, tensor2) cosine_similarity = compute_cosine_similarity(tensor1, tensor2) return { \'sqnr\': sqnr, \'normalized_l2_error\': normalized_l2_error, \'cosine_similarity\': cosine_similarity }"},{"question":"# Feature Selection Techniques in Scikit-learn Objective: Use different feature selection methods provided by the `sklearn.feature_selection` module to pre-process a dataset and compare their impacts on a classification model\'s performance. Background: You are provided with the Iris dataset, which contains 150 samples of iris flowers, each described by four features: sepal length, sepal width, petal length, and petal width. The task is to classify the species of the iris flowers. Task: 1. **Load the dataset**: Use `sklearn.datasets.load_iris` to load the dataset. 2. **Perform feature selection** using the following methods: - **Variance Threshold**: Remove features with variance below a certain threshold. - **SelectKBest**: Select the top k features based on ANOVA F-value. - **Recursive Feature Elimination (RFE)**: Use a logistic regression model as the estimator. - **Tree-based feature selection**: Use an ExtraTrees classifier. 3. **Compare the performance** of a Random Forest classifier on the original dataset and after applying each feature selection method. 4. **Compute and print the accuracy scores** for each method. Constraints: - Perform a `train_test_split` with `test_size=0.3` for training and testing. - Use `random_state=42` for reproducibility. - Set `k=2` for `SelectKBest` and select top 2 features for RFE. - Use `threshold=0.1` for `VarianceThreshold`. Sample Input: None. The task is to use the predefined Iris dataset. Expected Output: Four accuracy scores, one for each feature selection method and one for the model without feature selection. Example Code Snippet: ```python from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFE from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score # Load Iris dataset X, y = load_iris(return_X_y=True) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) def evaluate_model(X_train, X_test, y_train, y_test): clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) # 1. No feature selection acc_no_selection = evaluate_model(X_train, X_test, y_train, y_test) print(f\\"Accuracy without feature selection: {acc_no_selection:.4f}\\") # 2. Variance Threshold sel = VarianceThreshold(threshold=0.1) X_train_vt = sel.fit_transform(X_train) X_test_vt = sel.transform(X_test) acc_variance_threshold = evaluate_model(X_train_vt, X_test_vt, y_train, y_test) print(f\\"Accuracy with Variance Threshold: {acc_variance_threshold:.4f}\\") # 3. SelectKBest sel = SelectKBest(f_classif, k=2) X_train_kbest = sel.fit_transform(X_train, y_train) X_test_kbest = sel.transform(X_test) acc_select_kbest = evaluate_model(X_train_kbest, X_test_kbest, y_train, y_test) print(f\\"Accuracy with SelectKBest: {acc_select_kbest:.4f}\\") # 4. Recursive Feature Elimination (RFE) estimator = LogisticRegression(max_iter=1000) sel = RFE(estimator, n_features_to_select=2, step=1) X_train_rfe = sel.fit_transform(X_train, y_train) X_test_rfe = sel.transform(X_test) acc_rfe = evaluate_model(X_train_rfe, X_test_rfe, y_train, y_test) print(f\\"Accuracy with RFE: {acc_rfe:.4f}\\") # 5. Tree-based feature selection clf = ExtraTreesClassifier(n_estimators=50, random_state=42) clf.fit(X_train, y_train) model = SelectFromModel(clf, prefit=True) X_train_tree = model.transform(X_train) X_test_tree = model.transform(X_test) acc_tree = evaluate_model(X_train_tree, X_test_tree, y_train, y_test) print(f\\"Accuracy with Tree-based feature selection: {acc_tree:.4f}\\") ```","solution":"from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFE, SelectFromModel from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Load Iris dataset X, y = load_iris(return_X_y=True) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) def evaluate_model(X_train, X_test, y_train, y_test): clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) # 1. No feature selection acc_no_selection = evaluate_model(X_train, X_test, y_train, y_test) # print(f\\"Accuracy without feature selection: {acc_no_selection:.4f}\\") # 2. Variance Threshold sel = VarianceThreshold(threshold=0.1) X_train_vt = sel.fit_transform(X_train) X_test_vt = sel.transform(X_test) acc_variance_threshold = evaluate_model(X_train_vt, X_test_vt, y_train, y_test) # print(f\\"Accuracy with Variance Threshold: {acc_variance_threshold:.4f}\\") # 3. SelectKBest sel = SelectKBest(f_classif, k=2) X_train_kbest = sel.fit_transform(X_train, y_train) X_test_kbest = sel.transform(X_test) acc_select_kbest = evaluate_model(X_train_kbest, X_test_kbest, y_train, y_test) # print(f\\"Accuracy with SelectKBest: {acc_select_kbest:.4f}\\") # 4. Recursive Feature Elimination (RFE) estimator = LogisticRegression(max_iter=1000) sel = RFE(estimator, n_features_to_select=2, step=1) X_train_rfe = sel.fit_transform(X_train, y_train) X_test_rfe = sel.transform(X_test) acc_rfe = evaluate_model(X_train_rfe, X_test_rfe, y_train, y_test) # print(f\\"Accuracy with RFE: {acc_rfe:.4f}\\") # 5. Tree-based feature selection clf = ExtraTreesClassifier(n_estimators=50, random_state=42) clf.fit(X_train, y_train) model = SelectFromModel(clf, prefit=True) X_train_tree = model.transform(X_train) X_test_tree = model.transform(X_test) acc_tree = evaluate_model(X_train_tree, X_test_tree, y_train, y_test) # print(f\\"Accuracy with Tree-based feature selection: {acc_tree:.4f}\\") # Collect the accuracy results accuracy_results = { \\"No Feature Selection\\": acc_no_selection, \\"Variance Threshold\\": acc_variance_threshold, \\"SelectKBest\\": acc_select_kbest, \\"RFE\\": acc_rfe, \\"Tree-based\\": acc_tree } def get_accuracies(): return accuracy_results"},{"question":"You are provided with a dataset containing information about different species of flowers. Your task is to create a set of informative plots using Seaborn\'s `kdeplot` function to visualize the distributions of the `sepal_length` and `sepal_width` for different species. Follow these instructions to complete the task: 1. **Load the Dataset**: - Load the \\"iris\\" dataset using the `seaborn` library. 2. **Create a Univariate KDE Plot**: - Plot the kernel density estimate of the `sepal_length` for the entire dataset. - Label the x-axis as `Sepal Length`. 3. **Create a Bivariate KDE Plot**: - Plot the kernel density estimate to visualize the joint distribution of `sepal_length` and `sepal_width`. - Label the x-axis as `Sepal Length` and the y-axis as `Sepal Width`. 4. **Add Hue to the Univariate KDE Plot**: - Modify the univariate KDE plot to include different colors for each `species`. - Ensure that the distributions are \\"stacked\\" to reflect the contribution of each species. - Label the x-axis as `Sepal Length` and provide an appropriate legend. 5. **Create a Bivariate KDE Plot with Filled Contours**: - Plot the joint distribution of `sepal_length` and `sepal_width` using filled contours. - Use different colors for each `species` by setting the `hue` parameter. - Label the x-axis as `Sepal Length` and the y-axis as `Sepal Width`. # Input: - You do not need to take any input from the user. # Output: - The output should be a series of plots as described in the problem statement. # Constraints: - You must use the `seaborn` library. - Ensure that the plots are clear and well-labeled. # Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # Create a univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.xlabel(\\"Sepal Length\\") plt.title(\\"Univariate KDE Plot of Sepal Length\\") plt.show() # Create a bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Bivariate KDE Plot of Sepal Length and Sepal Width\\") plt.show() # Add hue to the univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", multiple=\\"stack\\") plt.xlabel(\\"Sepal Length\\") plt.title(\\"Univariate KDE Plot of Sepal Length by Species\\") plt.legend(title=\\"Species\\") plt.show() # Create a bivariate KDE plot with filled contours plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", fill=True) plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Bivariate KDE Plot of Sepal Length and Sepal Width by Species\\") plt.legend(title=\\"Species\\") plt.show() ``` # Notes: - Make sure to handle the dataset and plot each graph in a clear and concise manner. - Label the axes and provide titles for each plot to ensure readability. - Customize the appearance such as line width, alpha values, and color palettes to enhance the plots if necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_iris_dataset(): Load the iris dataset using seaborn library. return sns.load_dataset(\\"iris\\") def plot_univariate_kde(iris): Create a univariate KDE plot of sepal_length for the entire dataset. plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.xlabel(\\"Sepal Length\\") plt.title(\\"Univariate KDE Plot of Sepal Length\\") plt.show() def plot_bivariate_kde(iris): Create a bivariate KDE plot to show the joint distribution of sepal_length and sepal_width. plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Bivariate KDE Plot of Sepal Length and Sepal Width\\") plt.show() def plot_univariate_kde_with_hue(iris): Create a univariate KDE plot of sepal_length with hue for different species. plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", multiple=\\"stack\\") plt.xlabel(\\"Sepal Length\\") plt.title(\\"Univariate KDE Plot of Sepal Length by Species\\") plt.legend(title=\\"Species\\") plt.show() def plot_bivariate_kde_with_fill_and_hue(iris): Create a bivariate KDE plot of sepal_length and sepal_width with filled contours and hue for different species. plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", fill=True) plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Bivariate KDE Plot of Sepal Length and Sepal Width by Species\\") plt.legend(title=\\"Species\\") plt.show()"},{"question":"Advanced Seaborn Plotting Objective: Create a script that utilizes the seaborn library to visualize data comprehensively. The task will test your ability to handle a dataset, plot multiple relationships, and customize plots using seaborn\'s advanced functionalities. Dataset: Use the `mpg` dataset provided by seaborn. Tasks: 1. **Load the `mpg` dataset using `seaborn`\'s load_dataset method.** 2. **Create a complex visualization that includes the following:** - Plot `acceleration` versus both `displacement` and `weight`. - Pair the `mpg` data\'s `displacement` and `horsepower` with `weight` and display these relationships in a multi-plot configuration with a `wrap` of 2. - Include facets based on the `origin` of the cars. Requirements: - **Function Implementation:** Implement a function `create_seaborn_plots()` which: - Has no input parameters. - Loads the dataset and creates the described plot within the function. - Displays the plot using `plt.show()`. - **Constraints & Considerations:** - Use the provided dataset only. - Properly label the axes for each plot, using appropriate units. - Ensure that the plots are clearly readable and well-organized. Example Output: ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Plot acceleration versus displacement and weight plot1 = so.Plot(mpg, y=\\"acceleration\\").pair(x=[\\"displacement\\", \\"weight\\"]).add(so.Dots()) # Plot displacement vs horsepowers against weight with wrap plot2 = so.Plot(mpg, y=[\\"displacement\\", \\"horsepower\\"]).pair(x=[\\"weight\\"]).wrap(2).add(so.Dots()) # Facet these plots based on origin of the cars combined_plot = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"displacement\\", \\"horsepower\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) # Customize axes labels combined_plot.label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", x2=\\"Horsepower\\", y=\\"MPG\\") # Display the plots plot1.plot() plot2.plot() combined_plot.plot() plt.show() # Call the function to create and display the plots create_seaborn_plots() ``` Submission: Submit a Python script or Jupyter notebook implementing the function `create_seaborn_plots()` and generating the described plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Plot acceleration versus displacement and weight g = sns.pairplot(mpg, y_vars=\\"acceleration\\", x_vars=[\\"displacement\\", \\"weight\\"]) # Plot displacement vs horsepowers against weight with wrap g = sns.pairplot(mpg, y_vars=[\\"displacement\\", \\"horsepower\\"], x_vars=[\\"weight\\"], aspect=0.8, height=5) # Facet these plots based on origin of the cars g = sns.pairplot(mpg, x_vars=[\\"weight\\"], y_vars=[\\"displacement\\", \\"horsepower\\"], hue=\\"origin\\", palette=\\"muted\\") # Customize axes labels for ax in g.axes.flat: ax.set_xlabel(\\"Weight (lb)\\") ax.set_ylabel(ax.get_ylabel()) plt.show()"},{"question":"Objective You are required to create a utility function that leverages the `py_compile` module to compile a list of Python source files into bytecode files. Additionally, you will handle various compilation scenarios and provide meaningful error handling and output. Requirements 1. **Function Name**: `compile_python_files` 2. **Function Signature**: ```python def compile_python_files(files: list, output_dir: str, optimization_level: int = -1, quiet: bool = False) -> list: ``` 3. **Parameters**: - `files` (list): A list of paths to the source files that need to be compiled. - `output_dir` (str): The directory where the compiled bytecode files (.pyc) should be saved. - `optimization_level` (int, optional): The optimization level to use during compilation. Defaults to -1. - `quiet` (bool, optional): If set to True, suppress error output. Defaults to False. Expected Output - The function should return a list of paths to the successfully compiled `.pyc` files. - If a file fails to compile, it should be excluded from the output list. - If `quiet` is False, errors should be printed to `sys.stderr`. - If `output_dir` or a specified compiled file path is a symlink or non-regular file, a `FileExistsError` exception should be raised. Detailed Behavior 1. Use the `py_compile.compile()` function to compile each source file in the `files` list. 2. Redirect the compiled bytecode files to the `output_dir`. 3. Handle possible exceptions: - Log errors to `sys.stderr` when `quiet` is False. - Raise `FileExistsError` if the output path is a symlink or non-regular file. 4. Optimize the bytecode files based on the specified `optimization_level`. Constraints - Assume all input files are valid paths to Python source files. - The `output_dir` is a valid directory that the user has permission to write to. Example ```python from py_compile import PyCompileError, compile as py_compile def compile_python_files(files, output_dir, optimization_level=-1, quiet=False): compiled_files = [] for file in files: try: cfile = f\\"{output_dir}/{file.split(\'/\')[-1].replace(\'.py\', \'.pyc\')}\\" compiled_file_path = py_compile(file, cfile=cfile, optimize=optimization_level, quiet=2 if quiet else 0) compiled_files.append(compiled_file_path) except PyCompileError: if not quiet: print(f\\"Compilation failed for {file}\\", file=sys.stderr) except FileExistsError as e: print(f\\"File {cfile} exists as a symlink or non-regular file, cannot compile.\\", file=sys.stderr) raise e return compiled_files # Example usage: compiled = compile_python_files([\'script1.py\', \'script2.py\'], \'/compiled\', optimization_level=1, quiet=False) print(compiled) ``` The function should be thoroughly tested with different sets of file paths, error scenarios, and configuration settings to ensure robustness.","solution":"import os import py_compile import sys def compile_python_files(files, output_dir, optimization_level=-1, quiet=False): Compiles a list of Python source files into bytecode files. Parameters: files (list): A list of paths to the source files that need to be compiled. output_dir (str): The directory where the compiled bytecode files should be saved. optimization_level (int, optional): The optimization level for compilation. Defaults to -1. quiet (bool, optional): If set to True, suppress error output. Defaults to False. Returns: list: A list of paths to the successfully compiled .pyc files. if not os.path.exists(output_dir): os.makedirs(output_dir) compiled_files = [] for file in files: try: cfile = os.path.join(output_dir, os.path.basename(file).replace(\'.py\', \'.pyc\')) # Check if output file is a symlink or non-regular file if os.path.islink(cfile) or not os.path.isfile(cfile) and os.path.exists(cfile): raise FileExistsError(f\\"{cfile} is a symlink or non-regular file\\") compiled_file_path = py_compile.compile(file, cfile=cfile, optimize=optimization_level, quiet=2 if quiet else 0) compiled_files.append(compiled_file_path) except py_compile.PyCompileError: if not quiet: print(f\\"Compilation failed for {file}\\", file=sys.stderr) except FileExistsError as e: if not quiet: print(e, file=sys.stderr) raise e return compiled_files"},{"question":"**Title:** Implement an I/O Multiplexer using `select` Module Problem Statement You are tasked to create an I/O multiplexer using the `select` module. Your task is to monitor multiple file descriptors to see if they are ready for some kind of I/O operation (reading, writing, or error conditions). You need to implement a Python function `io_multiplexer(rlist, wlist, xlist, timeout)` that uses the `select.select()` method to monitor the given lists of file descriptors and returns the file descriptors that are ready for reading, writing, or having an error condition. Function Signature ```python def io_multiplexer(rlist, wlist, xlist, timeout): Monitors multiple file descriptors for I/O readiness. Parameters: rlist (list): List of file descriptors to be monitored for reading. wlist (list): List of file descriptors to be monitored for writing. xlist (list): List of file descriptors to be monitored for error conditions. timeout (float): Optional timeout in seconds. If None, blocks until an event is ready. Returns: ready_to_read (list): List of file descriptors ready for reading. ready_to_write (list): List of file descriptors ready for writing. in_error (list): List of file descriptors with an error condition. ``` Input and Output Format - **Input:** - `rlist`: A list of file descriptors or objects with a `fileno()` method to monitor for reading. - `wlist`: A list of file descriptors or objects with a `fileno()` method to monitor for writing. - `xlist`: A list of file descriptors or objects with a `fileno()` method to monitor for error conditions. - `timeout`: A float representing the timeout in seconds; if `None`, the method should block until at least one file descriptor is ready. - **Output:** - The function should return a tuple of three lists: - `ready_to_read`: List of file descriptors ready for reading. - `ready_to_write`: List of file descriptors ready for writing. - `in_error`: List of file descriptors with an error condition. Constraints - The implementation should handle empty lists and should not crash if any list is empty. - Ensure that the function and its parameters can handle both file descriptors and objects with `fileno()` methods correctly. Example ```python import sys import os # Example usage file1 = open(\\"file_to_read.txt\\", \\"r\\") file2 = open(\\"file_to_write.txt\\", \\"w\\") rlist = [file1] wlist = [file2] xlist = [sys.stdin] timeout = 5.0 ready_to_read, ready_to_write, in_error = io_multiplexer(rlist, wlist, xlist, timeout) print(\\"Ready to read:\\", ready_to_read) print(\\"Ready to write:\\", ready_to_write) print(\\"In error:\\", in_error) file1.close() file2.close() ``` Performance Requirements - The function should run efficiently with a large number of file descriptors. - Aim for O(n) complexity where n is the total number of file descriptors in `rlist`, `wlist`, and `xlist`. Notes 1. Remember to handle any necessary cleanup of file descriptors (such as closing any that were opened). 2. Consider edge cases where all input lists are empty or where `timeout` is zero.","solution":"import select def io_multiplexer(rlist, wlist, xlist, timeout): Monitors multiple file descriptors for I/O readiness. Parameters: rlist (list): List of file descriptors to be monitored for reading. wlist (list): List of file descriptors to be monitored for writing. xlist (list): List of file descriptors to be monitored for error conditions. timeout (float): Optional timeout in seconds. If None, blocks until an event is ready. Returns: ready_to_read (list): List of file descriptors ready for reading. ready_to_write (list): List of file descriptors ready for writing. in_error (list): List of file descriptors with an error condition. ready_to_read, ready_to_write, in_error = select.select(rlist, wlist, xlist, timeout) return ready_to_read, ready_to_write, in_error"},{"question":"Objective Write a function to perform data manipulation and generate various types of plots using pandas. Function Signature ```python def visualize_data(data: pd.DataFrame) -> None: pass ``` Input - **data**: A pandas DataFrame containing at least the columns: \'Category\', \'Value1\', \'Value2\', \'Date\'. Expected Output The function should generate three different plots: 1. **Line plot**: A line plot where \'Date\' is on the x-axis and \'Value1\' is on the y-axis. - Customize the plot to have a title \\"Value1 Over Time\\" and x/y labels \\"Date\\" and \\"Value1\\". - Ensure that x-axis ticks are formatted nicely as dates. 2. **Bar plot**: A bar plot showing the sum of `Value1` grouped by \'Category\'. - Customize the plot to have a title \\"Sum of Value1 by Category\\" and x/y labels \\"Category\\" and \\"Sum of Value1\\". 3. **Scatter plot**: A scatter plot with \'Value1\' on the x-axis and \'Value2\' on the y-axis. - Use different colors for points based on \'Category\'. - Customize the plot to have a title \\"Value1 vs Value2\\" and x/y labels \\"Value1\\" and \\"Value2\\". Constraints - The `data` DataFrame will have at least 100 rows of data. - Columns \'Value1\' and \'Value2\' will contain numeric values, \'Category\' will contain categorical data, and \'Date\' will contain datetime data. Example Given the following `data` DataFrame: ``` Category Value1 Value2 Date 0 A 10 15 2021-01-01 1 B 20 25 2021-01-02 2 A 30 35 2021-01-03 3 B 40 45 2021-01-04 ... ``` The function `visualize_data(data)` should generate: 1. A line plot titled \\"Value1 Over Time\\" where the x-axis is \'Date\' and the y-axis is \'Value1\'. 2. A bar plot titled \\"Sum of Value1 by Category\\" showing the sum of \'Value1\' for each category. 3. A scatter plot titled \\"Value1 vs Value2\\" where \'Category\' is used to color-code the points. These visualizations should be displayed using matplotlib functionalities wrapped within pandas plotting methods.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def visualize_data(data: pd.DataFrame) -> None: # 1. Line plot: \'Date\' vs \'Value1\' plt.figure(figsize=(10, 6)) plt.plot(data[\'Date\'], data[\'Value1\'], marker=\'o\') plt.title(\\"Value1 Over Time\\") plt.xlabel(\\"Date\\") plt.ylabel(\\"Value1\\") plt.xticks(rotation=45) plt.grid(True) plt.show() # 2. Bar plot: Sum of \'Value1\' grouped by \'Category\' sum_by_category = data.groupby(\'Category\')[\'Value1\'].sum().reset_index() plt.figure(figsize=(10, 6)) sns.barplot(x=\'Category\', y=\'Value1\', data=sum_by_category) plt.title(\\"Sum of Value1 by Category\\") plt.xlabel(\\"Category\\") plt.ylabel(\\"Sum of Value1\\") plt.grid(True) plt.show() # 3. Scatter plot: \'Value1\' vs \'Value2\' colored by \'Category\' plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'Value1\', y=\'Value2\', hue=\'Category\', data=data) plt.title(\\"Value1 vs Value2\\") plt.xlabel(\\"Value1\\") plt.ylabel(\\"Value2\\") plt.grid(True) plt.show()"},{"question":"Problem Statement You are given a list of dictionaries, where each dictionary contains information about a student. Each dictionary has the following keys: - `name`: A string representing the name of the student. - `scores`: A list of integers representing the scores of the student in different subjects. Your task is to implement a function `top_student(students: List[Dict[str, Union[str, List[int]]]]) -> Dict[str, Union[str, int]]` that finds the student with the highest average score. If multiple students have the same highest average score, choose the one who appears first in the list. # Input - `students`: A list of dictionaries, where each dictionary contains `name` (string) and `scores` (list of integers). The length of the list will be between `1` and `100`. Each score will be an integer between `0` and `100`. # Output - A dictionary containing: - `name`: The name of the student with the highest average score. - `average_score`: The highest average score. # Example ```python students = [ {\\"name\\": \\"Alice\\", \\"scores\\": [90, 80, 88]}, {\\"name\\": \\"Bob\\", \\"scores\\": [100, 90, 95]}, {\\"name\\": \\"Charlie\\", \\"scores\\": [85, 85, 80]} ] print(top_student(students)) # Output: {\\"name\\": \\"Bob\\", \\"average_score\\": 95} ``` # Constraints - Use only built-in functions to implement your solution. # Quick Notes - You are allowed to use the `sum`, `len`, `max`, `map`, and `enumerate` built-in functions. - Do not use external libraries or import statements. Implement the function according to the given specifications.","solution":"from typing import List, Dict, Union def top_student(students: List[Dict[str, Union[str, List[int]]]]) -> Dict[str, Union[str, int]]: top_student = None top_average_score = -1 for student in students: average_score = sum(student[\\"scores\\"]) / len(student[\\"scores\\"]) if average_score > top_average_score: top_average_score = average_score top_student = student[\\"name\\"] return {\\"name\\": top_student, \\"average_score\\": top_average_score}"},{"question":"**Problem Statement:** You are tasked with writing a function that takes the group name as input and returns a list of members of that group. If the group does not exist, it should return an empty list. You should follow these constraints: 1. Use the `grp` module to interact with the Unix group database. 2. Your function should be efficient and handle the exceptions as required. 3. You should not assume any specific order of the groups in the Unix group database. # Function Signature: ```python def get_group_members(group_name: str) -> list: pass ``` # Input: - `group_name`: A string representing the name of the group. # Output: - A list of strings, where each string is a member of the group. - If the group does not exist, return an empty list. # Example: ```python # Assume the following groups exist in the database grp.getgrall() -> [ (\'admin\', \'\', 1001, [\'user1\', \'user2\']), (\'staff\', \'\', 1002, [\'user3\']), (\'users\', \'\', 1003, [\'user4\', \'user5\', \'user6\']), ] get_group_members(\'admin\') -> [\'user1\', \'user2\'] get_group_members(\'nonexistent\') -> [] ``` # Constraints: - You must use the `grp` module functions for all interactions with the group database. - Handle any exceptions that may arise during the function execution. # Notes: - You can assume the `grp` module is available and imported at the beginning of your code. - Your solution should work efficiently even if the group database contains a large number of entries.","solution":"import grp def get_group_members(group_name: str) -> list: Returns the list of members for the specified group name. If the group does not exist, returns an empty list. try: group_info = grp.getgrnam(group_name) return list(group_info.gr_mem) except KeyError: # Group does not exist return []"},{"question":"# Coding Problem: Secure File Integrity and Key Derivation Background In security-sensitive applications, it is crucial to ensure the integrity of files and securely derive keys from passwords. We will use the `hashlib` module to perform these tasks. Task Description 1. **File Integrity Verification**: - Implement a function `compute_file_hash(file_path: str, algorithm: str) -> str` that computes the hash of a file. - The function should: - Read the file in binary mode. - Use the specified algorithm from the `hashlib` module to compute the file\'s hash. - Return the hexadecimal digest of the file\'s content. 2. **Password-based Key Derivation**: - Implement a function `derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str` that derives a cryptographic key from a password using the PBKDF2 HMAC algorithm. - The function should: - Use the `hashlib.pbkdf2_hmac` function. - Return the derived key as a hexadecimal string. Input and Output **Function 1: `compute_file_hash`** - **Input**: - `file_path` (str): Path to the file whose hash needs to be computed. - `algorithm` (str): Name of the hash algorithm to use (e.g., \'sha256\', \'md5\'). - **Output**: - (str): Hexadecimal digest of the file\'s content. **Function 2: `derive_key`** - **Input**: - `password` (str): The password from which to derive the key. - `salt` (bytes): The salt to use for key derivation. - `iterations` (int): Number of iterations for the key derivation function. - `key_length` (int): Desired length of the derived key. - **Output**: - (str): Derived key as a hexadecimal string. Constraints - The file size for `compute_file_hash` should not exceed 1 GB. - The minimum number of iterations for `derive_key` should be 100,000. Example ```python def compute_file_hash(file_path: str, algorithm: str) -> str: # Your implementation here pass def derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str: # Your implementation here pass # Example Usage # Assuming the file \\"example.txt\\" contains the text \\"Hello World\\" file_hash = compute_file_hash(\\"example.txt\\", \\"sha256\\") print(file_hash) # Should print the sha256 hash of the file\'s content # Deriving a key from password \\"password123\\" with salt and 100000 iterations key = derive_key(\\"password123\\", b\\"somesalt\\", 100000, 32) print(key) # Should print the derived key as a hexadecimal string ``` **Note**: - You may assume the file specified in `file_path` exists and can be read. - Use good coding practices, such as exception handling and efficient file reading.","solution":"import hashlib def compute_file_hash(file_path: str, algorithm: str) -> str: Computes the hash of a file using the specified algorithm. Parameters: file_path (str): The path to the file. algorithm (str): The hash algorithm to use (e.g., \'sha256\', \'md5\'). Returns: str: The hexadecimal digest of the file\'s content. hash_func = hashlib.new(algorithm) with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): hash_func.update(chunk) return hash_func.hexdigest() def derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str: Derives a cryptographic key from a password using the PBKDF2 HMAC algorithm. Parameters: password (str): The password from which to derive the key. salt (bytes): The salt to use for key derivation. iterations (int): Number of iterations for the key derivation function. key_length (int): Desired length of the derived key. Returns: str: The derived key as a hexadecimal string. if iterations < 100000: raise ValueError(\\"The number of iterations must be at least 100,000.\\") dk = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen=key_length) return dk.hex()"},{"question":"<|Analysis Begin|> The provided documentation for the `xml.parsers.expat` module describes how to use the Expat non-validating XML parser in Python. The module allows users to create an XML parser object, configure handler functions for processing XML data, and parse XML documents. It supports various handlers for different XML constructs such as elements, character data, processing instructions, comments, and namespaces. Additionally, it describes several methods and attributes on the xmlparser object that control parsing operations and provide information about parsing state and errors. The module is capable of handling various aspects of XML parsing, including namespace processing, buffering text, handling external entities, and deferring reparse. The documentation highlights several advanced functionalities, such as setting base URIs, control over parameter entity parsing, and the use of reparse deferral to prevent denial of service attacks. Key functionalities include: - Creating an XML parser instance using `ParserCreate`. - Configuring handlers for different XML events (e.g., start and end of elements, character data, comments). - Parsing XML content using `Parse` or `ParseFile`. - Error handling and accessing parsing state through attributes. Based on the documentation, a suitable coding question should require the student to: 1. Create an XML parser instance. 2. Configure multiple handler functions for different types of XML data. 3. Perform XML parsing on a given XML document. 4. Demonstrate error handling and retrieval of parsing state attributes. This ensures the exercise covers both fundamental and advanced concepts outlined in the module documentation. <|Analysis End|> <|Question Begin|> **XML Parsing and Error Handling with `xml.parsers.expat`** In this exercise, you are required to create an XML parser to process a given XML document. You will implement and set handler functions to manage different types of XML data and handle errors appropriately. **Task:** 1. Create a new XML parser instance using `xml.parsers.expat.ParserCreate`. 2. Implement handler functions for the following events: - Start of an element. - End of an element. - Character data. - XML declaration. - Comments. 3. Set these handler functions to the parser instance. 4. Parse the provided XML document. 5. Handle any parsing errors and print out relevant error details such as the line number, column number, and error message. **Handlers:** - StartElementHandler: Print \\"Start element: [element name]\\" and its attributes. - EndElementHandler: Print \\"End element: [element name]\\". - CharacterDataHandler: Print \\"Character data: [data]\\". - XmlDeclHandler: Print \\"XML declaration: version=[version], encoding=[encoding], standalone=[standalone]\\". - CommentHandler: Print \\"Comment: [data]\\". **Error Handling:** - If a parsing error occurs, catch the `ExpatError` exception and print the error code, line number, column number, and error message. **Input:** - An XML string to be parsed (provided in the code of the main function). **Output:** - The output from the handlers, showing information about the structure of the XML and its content. - Error information if a parsing error occurs. **Constraints:** - The parser should be able to handle and output information for an XML string with nested elements, attributes, character data, and comments. Here is a starting template for your implementation: ```python import xml.parsers.expat def start_element(name, attrs): print(f\'Start element: {name}, attrs: {attrs}\') def end_element(name): print(f\'End element: {name}\') def char_data(data): print(f\'Character data: {repr(data)}\') def xml_decl(version, encoding, standalone): print(f\'XML declaration: version={version}, encoding={encoding}, standalone={standalone}\') def comment(data): print(f\'Comment: {data}\') def main(): xml_data = <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!-- This is a comment --> <root> <child attr=\\"value\\">Text</child> </root> parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.XmlDeclHandler = xml_decl parser.CommentHandler = comment try: parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e.code}, Line: {e.lineno}, Column: {e.offset}\\") if __name__ == \\"__main__\\": main() ``` Your task is to complete the implementation of the handler functions and ensure the error handling code correctly outputs the error details if a parsing error occurs.","solution":"import xml.parsers.expat def start_element(name, attrs): print(f\'Start element: {name}, attrs: {attrs}\') def end_element(name): print(f\'End element: {name}\') def char_data(data): if data.strip(): # print only non-empty character data print(f\'Character data: {repr(data)}\') def xml_decl(version, encoding, standalone): print(f\'XML declaration: version={version}, encoding={encoding}, standalone={standalone}\') def comment(data): print(f\'Comment: {data}\') def main(): xml_data = <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!-- This is a comment --> <root> <child attr=\\"value\\">Text</child> </root> parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.XmlDeclHandler = xml_decl parser.CommentHandler = comment try: parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e.code}, Line: {e.lineno}, Column: {e.offset}\\") if __name__ == \\"__main__\\": main()"},{"question":"DataFrame Styling and Export Objective Demonstrate your understanding of pandas DataFrame styling using the `Styler` class and export the styled DataFrame. Problem Statement You are given a DataFrame containing information about a set of students and their scores in three subjects - Mathematics, Science, and English. Your task is to style this DataFrame and then export it to an HTML file. Input The input is a DataFrame `df` with the following structure: ```plaintext | Student | Mathematics | Science | English | |---------|-------------|---------|---------| | Alice | 85 | 90 | 88 | | Bob | 79 | 85 | 92 | | Carol | 91 | 88 | 95 | | Dave | 72 | 76 | 80 | ``` Requirements 1. Use the `Styler` class to style the DataFrame. 2. Apply the following styling: - Highlight the maximum score in each column with a background color of \'lightgreen\'. - Highlight the minimum score in each column with a background color of \'lightcoral\'. - Set a caption for the DataFrame: \\"Students\' Scores\\". 3. Export the styled DataFrame to an HTML file named `styled_scores.html`. Output - A string confirming the successful export of the HTML file: `\\"Styled DataFrame has been exported to styled_scores.html\\"` Constraints - Use the pandas package as imported below: ```python import pandas as pd ``` Code Template ```python import pandas as pd # Initialize the DataFrame data = { \'Student\': [\'Alice\', \'Bob\', \'Carol\', \'Dave\'], \'Mathematics\': [85, 79, 91, 72], \'Science\': [90, 85, 88, 76], \'English\': [88, 92, 95, 80] } df = pd.DataFrame(data) # Style the DataFrame def style_dataframe(df): # Your code here to style the DataFrame styled_df = df.style # Highlight max and min values styled_df = (styled_df.highlight_max(axis=0, color=\'lightgreen\') .highlight_min(axis=0, color=\'lightcoral\')) # Set caption styled_df = styled_df.set_caption(\\"Students\' Scores\\") return styled_df # Export styled DataFrame to HTML def export_styled_dataframe_to_html(styled_df, filename=\'styled_scores.html\'): # Your code here to export the DataFrame to HTML styled_df.to_html(filename) return f\\"Styled DataFrame has been exported to {filename}\\" # Main execution styled_df = style_dataframe(df) message = export_styled_dataframe_to_html(styled_df) print(message) ``` Performance Requirements - The solution should efficiently handle DataFrames with up to 1000 rows. - The styling and export should complete within a reasonable time frame (e.g., 5 seconds for 1000 rows).","solution":"import pandas as pd # Initialize the DataFrame data = { \'Student\': [\'Alice\', \'Bob\', \'Carol\', \'Dave\'], \'Mathematics\': [85, 79, 91, 72], \'Science\': [90, 85, 88, 76], \'English\': [88, 92, 95, 80] } df = pd.DataFrame(data) # Style the DataFrame def style_dataframe(df): Styles the DataFrame by highlighting the max and min values in each column and setting a caption. styled_df = df.style # Highlight max and min values styled_df = (styled_df.highlight_max(axis=0, color=\'lightgreen\') .highlight_min(axis=0, color=\'lightcoral\')) # Set caption styled_df = styled_df.set_caption(\\"Students\' Scores\\") return styled_df # Export styled DataFrame to HTML def export_styled_dataframe_to_html(styled_df, filename=\'styled_scores.html\'): Exports the styled DataFrame to an HTML file. styled_df.to_html(filename) return f\\"Styled DataFrame has been exported to {filename}\\" # Main execution styled_df = style_dataframe(df) message = export_styled_dataframe_to_html(styled_df) print(message)"},{"question":"Welcome to the coding assessment! For this task, you will need to demonstrate your understanding of the `pandas.Series` class by performing a series of data manipulations and analyses. # Question: You are given a pandas Series that contains daily recorded temperatures (in Celsius) for a specific city over a year. Your tasks are as follows: 1. **Filtering Data:** - Remove any records where the temperature data is missing (NaN). 2. **Data Transformation:** - Convert the temperature from Celsius to Fahrenheit using the formula: `F = C * 9/5 + 32`. 3. **Descriptive Statistics:** - Calculate and return the following statistics for the temperature data: - Mean Temperature - Median Temperature - Standard Deviation of the Temperature - Number of days with temperatures above 90°F 4. **Time-Series Analysis:** - Identify the month(s) with the highest and lowest average temperatures and return their names. - Calculate the 7-day rolling average of the temperatures and return it as a new Series. 5. **Plotting:** - Create a line plot of the daily temperatures and annotate it with the day that had the highest temperature. # Input Format: - A `pandas.Series` named `temperature_series` with a `DatetimeIndex` representing dates and float values representing daily recorded temperatures in Celsius. # Output Format: - A dictionary with the following keys: - `\'mean_temperature\'`: the mean temperature - `\'median_temperature\'`: the median temperature - `\'std_temperature\'`: the standard deviation of the temperature - `\'days_above_90F\'`: the number of days with temperatures above 90°F - `\'highest_avg_temp_month\'`: the name of the month with the highest average temperature - `\'lowest_avg_temp_month\'`: the name of the month with the lowest average temperature - `\'rolling_avg_7d\'`: the 7-day rolling average Series - `\'temperature_plot\'`: the line plot of daily temperatures with annotation # Example: Given the `temperature_series` as follows: ``` 2023-01-01 20.5 2023-01-02 NaN 2023-01-03 21.0 2023-01-04 19.6 2023-01-05 22.5 ... ``` The function should produce an output: ```python { \'mean_temperature\': ..., \'median_temperature\': ..., \'std_temperature\': ..., \'days_above_90F\': ..., \'highest_avg_temp_month\': ..., \'lowest_avg_temp_month\': ..., \'rolling_avg_7d\': ..., \'temperature_plot\': ... } ``` You are required to implement the function `analyze_temperature_series` which fulfills the above requirements. Implementation: ```python def analyze_temperature_series(temperature_series): import pandas as pd import matplotlib.pyplot as plt # 1. Filtering Data: Remove any records with missing temperature data temperature_series = temperature_series.dropna() # 2. Data Transformation: Convert Celsius to Fahrenheit temperature_fahrenheit = temperature_series * 9/5 + 32 # 3. Descriptive Statistics mean_temp = temperature_fahrenheit.mean() median_temp = temperature_fahrenheit.median() std_temp = temperature_fahrenheit.std() days_above_90F = (temperature_fahrenheit > 90).sum() # 4. Time-Series Analysis: Determine months with highest and lowest average temperatures monthly_avg_temp = temperature_fahrenheit.resample(\'M\').mean() highest_avg_temp_month = monthly_avg_temp.idxmax().strftime(\'%B\') lowest_avg_temp_month = monthly_avg_temp.idxmin().strftime(\'%B\') # Calculate the 7-day rolling average rolling_avg_7d = temperature_fahrenheit.rolling(window=7).mean() # 5. Plotting: Line plot of daily temperatures and annotate the highest temperature day plt.figure(figsize=(10, 5)) temperature_fahrenheit.plot(label=\'Daily Temperature\') max_temp_date = temperature_fahrenheit.idxmax() max_temp_value = temperature_fahrenheit.max() plt.annotate(f\'Highest Temp: {max_temp_value}F\', xy=(max_temp_date, max_temp_value), xytext=(max_temp_date, max_temp_value+10), arrowprops=dict(facecolor=\'black\', shrink=0.05)) plt.xlabel(\'Date\') plt.ylabel(\'Temperature (F)\') plt.title(\'Daily Temperatures Over the Year\') plt.legend() plt.savefig(\'temperature_plot.png\') temperature_plot = \'temperature_plot.png\' return { \'mean_temperature\': mean_temp, \'median_temperature\': median_temp, \'std_temperature\': std_temp, \'days_above_90F\': days_above_90F, \'highest_avg_temp_month\': highest_avg_temp_month, \'lowest_avg_temp_month\': lowest_avg_temp_month, \'rolling_avg_7d\': rolling_avg_7d, \'temperature_plot\': temperature_plot } ``` # Constraints: - Ensure efficient handling of missing data. - Utilize pandas\' built-in functions to achieve the transformations and analyses.","solution":"def analyze_temperature_series(temperature_series): import pandas as pd import matplotlib.pyplot as plt # 1. Filtering Data: Remove any records with missing temperature data temperature_series = temperature_series.dropna() # 2. Data Transformation: Convert Celsius to Fahrenheit temperature_fahrenheit = temperature_series * 9/5 + 32 # 3. Descriptive Statistics mean_temp = temperature_fahrenheit.mean() median_temp = temperature_fahrenheit.median() std_temp = temperature_fahrenheit.std() days_above_90F = (temperature_fahrenheit > 90).sum() # 4. Time-Series Analysis: Determine months with highest and lowest average temperatures monthly_avg_temp = temperature_fahrenheit.resample(\'M\').mean() highest_avg_temp_month = monthly_avg_temp.idxmax().strftime(\'%B\') lowest_avg_temp_month = monthly_avg_temp.idxmin().strftime(\'%B\') # Calculate the 7-day rolling average rolling_avg_7d = temperature_fahrenheit.rolling(window=7).mean() # 5. Plotting: Line plot of daily temperatures and annotate the highest temperature day plt.figure(figsize=(10, 5)) temperature_fahrenheit.plot(label=\'Daily Temperature\') max_temp_date = temperature_fahrenheit.idxmax() max_temp_value = temperature_fahrenheit.max() plt.annotate(f\'Highest Temp: {max_temp_value}F\', xy=(max_temp_date, max_temp_value), xytext=(max_temp_date, max_temp_value+10), arrowprops=dict(facecolor=\'black\', shrink=0.05)) plt.xlabel(\'Date\') plt.ylabel(\'Temperature (F)\') plt.title(\'Daily Temperatures Over the Year\') plt.legend() plt.savefig(\'temperature_plot.png\') temperature_plot = \'temperature_plot.png\' return { \'mean_temperature\': mean_temp, \'median_temperature\': median_temp, \'std_temperature\': std_temp, \'days_above_90F\': days_above_90F, \'highest_avg_temp_month\': highest_avg_temp_month, \'lowest_avg_temp_month\': lowest_avg_temp_month, \'rolling_avg_7d\': rolling_avg_7d, \'temperature_plot\': temperature_plot }"},{"question":"# Question: Multi-threaded Producer-Consumer Problem Objective Implement a multi-threaded producer-consumer system using Python\'s `threading` module. This system should utilize appropriate synchronization primitives to ensure thread safety and correct data flow between producers and consumers. Description You are required to build a system where: 1. There are multiple producer threads and multiple consumer threads. 2. Producers generate data and place it into a shared buffer. 3. Consumers retrieve data from the shared buffer and process it. 4. Use thread synchronization mechanisms to ensure: - Producers do not overwrite the buffer if it is full. - Consumers do not read from the buffer if it is empty. - The shared buffer access is thread-safe. Requirements 1. Implement a `ProducerConsumer` class with the following methods: - `produce(data: int)`: Method to be executed by producer threads. This method will produce an item and place it in the buffer. - `consume() -> int`: Method to be executed by consumer threads. This method will consume an item from the buffer and return it. 2. Ensure thread-safe access to the shared buffer using `Condition` variables or equivalent synchronization primitives. 3. Demonstrate the working of your `ProducerConsumer` class by: - Initiating multiple producer and consumer threads. - Producers should produce a sequence of integers from 1 to 100. - Consumers should print each consumed item. # Constraints - The buffer size should be limited to 10 items. - You should have at least 2 producer threads and 2 consumer threads. # Expected Input and Output - **Input**: None (threads are created and managed within the code). - **Output**: Sequential console/log prints showing the produced and consumed items, ensuring the correctness and synchronization. Example ```python from threading import Thread, Condition import time import random class ProducerConsumer: def __init__(self, buffer_size=10): self.buffer = [] self.buffer_size = buffer_size self.condition = Condition() def produce(self, data): with self.condition: while len(self.buffer) >= self.buffer_size: self.condition.wait() self.buffer.append(data) print(f\\"Produced: {data}\\") self.condition.notify_all() def consume(self): with self.condition: while not self.buffer: self.condition.wait() data = self.buffer.pop(0) print(f\\"Consumed: {data}\\") self.condition.notify_all() return data def producer(pc, max_items): for item in range(1, max_items + 1): time.sleep(random.random()) pc.produce(item) def consumer(pc, max_items): for _ in range(max_items): time.sleep(random.random()) pc.consume() if __name__ == \\"__main__\\": max_items = 100 pc = ProducerConsumer(buffer_size=10) producer_threads = [Thread(target=producer, args=(pc, max_items//2)) for _ in range(2)] consumer_threads = [Thread(target=consumer, args=(pc, max_items//2)) for _ in range(2)] for pt in producer_threads: pt.start() for ct in consumer_threads: ct.start() for pt in producer_threads: pt.join() for ct in consumer_threads: ct.join() ``` Note - The given example initializes a `ProducerConsumer` instance with buffer size 10. - Four threads (two producers and two consumers) are created and started. - Producers generate items between 1 and 100, while consumers process them. Your task is to implement the `ProducerConsumer` class and demonstrate its usage as shown.","solution":"from threading import Thread, Condition import time import random class ProducerConsumer: def __init__(self, buffer_size=10): self.buffer = [] self.buffer_size = buffer_size self.condition = Condition() def produce(self, data): with self.condition: while len(self.buffer) >= self.buffer_size: self.condition.wait() self.buffer.append(data) print(f\\"Produced: {data}\\") self.condition.notify_all() def consume(self): with self.condition: while not self.buffer: self.condition.wait() data = self.buffer.pop(0) print(f\\"Consumed: {data}\\") self.condition.notify_all() return data def producer(pc, max_items): for item in range(1, max_items + 1): time.sleep(random.random() * 0.1) pc.produce(item) def consumer(pc, max_items): for _ in range(max_items): time.sleep(random.random() * 0.1) pc.consume() if __name__ == \\"__main__\\": max_items = 100 pc = ProducerConsumer(buffer_size=10) producer_threads = [Thread(target=producer, args=(pc, max_items//2)) for _ in range(2)] consumer_threads = [Thread(target=consumer, args=(pc, max_items//2)) for _ in range(2)] for pt in producer_threads: pt.start() for ct in consumer_threads: ct.start() for pt in producer_threads: pt.join() for ct in consumer_threads: ct.join()"},{"question":"**Question: Exploring Penguins Dataset with Seaborn JointGrid** You are provided with the Penguins dataset, a popular dataset for practicing data visualization. Your task is to create a customized seaborn JointGrid to explore the relationship between two variables: `bill_length_mm` and `flipper_length_mm`. Write a Python function `create_penguins_jointgrid` that: 1. Loads the Penguins dataset using seaborn. 2. Creates a `JointGrid` with `bill_length_mm` as the x-axis and `flipper_length_mm` as the y-axis. 3. Customizes the grid by: - Using a scatterplot with `s=50, alpha=0.5` for the joint plot. - Using histograms with `kde=False, fill=True, color=\'b\'` for both marginal plots. - Adding vertical and horizontal reference lines at `x=45` and `y=200`. - Setting the height of the grid to 6, the ratio to 3, and the space between plots to 0.1. - Enabling ticks on the marginal plots. - Setting the x-axis limit to (30, 60) and the y-axis limit to (170, 240). The function should not return anything; it should only display the resulting plot. **Function Signature:** ```python def create_penguins_jointgrid(): pass ``` **Constraints:** - Ensure that seaborn and the necessary dataset are properly loaded. - Your code should run without any errors and produce the described plot. The expected output is a seaborn JointGrid visualization based on the provided specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_jointgrid(): # Load the Penguins dataset penguins = sns.load_dataset(\'penguins\') # Initialize the JointGrid g = sns.JointGrid(data=penguins, x=\'bill_length_mm\', y=\'flipper_length_mm\', height=6, ratio=3, space=0.1) # Plot using scatterplot for the joint plot g = g.plot_joint(sns.scatterplot, s=50, alpha=0.5) # Plot histograms for the marginals g = g.plot_marginals(sns.histplot, kde=False, fill=True, color=\'b\') # Add reference lines at x=45 and y=200 g.ax_joint.axvline(x=45, color=\'r\', linestyle=\'--\') g.ax_joint.axhline(y=200, color=\'r\', linestyle=\'--\') # Set axis limits g.ax_joint.set_xlim(30, 60) g.ax_joint.set_ylim(170, 240) # Enable ticks on the marginal plots plt.setp(g.ax_marg_x.get_yticklabels(), visible=True) plt.setp(g.ax_marg_y.get_xticklabels(), visible=True) # Display the plot plt.show()"},{"question":"Objective Design and implement a class for a neural network layer with custom weight initialization using PyTorch\'s `torch.nn.init` functions. This assessment will evaluate your understanding of PyTorch\'s initialization methods and their application in neural network design. Task You are required to create a custom neural network layer class in PyTorch, named `CustomLinearLayer`. This layer should perform a linear transformation on its input and allow for custom initialization of its weights and biases. Requirements 1. **Class Definition**: Define a class `CustomLinearLayer` that inherits from `torch.nn.Module`. 2. **Initialization**: - The constructor should accept the following parameters: - `in_features`: Number of input features. - `out_features`: Number of output features. - `weight_init`: A string specifying the type of weight initialization. It can be one of the following values: `\'xavier_uniform\'`, `\'xavier_normal\'`, `\'kaiming_uniform\'`, `\'kaiming_normal\'`. - `bias_init`: A string specifying the type of bias initialization. It can be one of the following values: `\'zeros\'`, `\'ones\'`, `\'constant\'` (with a specified constant value). 3. **Weight and Bias Initialization**: - Implement the weight and bias initialization logic using the specified initialization methods in PyTorch. - If the `constant` method is chosen for bias initialization, a constant value of 0.01 should be used. 4. **Forward Method**: Implement the `forward` method to perform the linear transformation. 5. **Input and Output Format**: - Inputs to the `forward` method will be a tensor of shape `(batch_size, in_features)`. - The output should be a tensor of shape `(batch_size, out_features)`. Example Usage ```python import torch import torch.nn as nn class CustomLinearLayer(nn.Module): def __init__(self, in_features, out_features, weight_init, bias_init): super(CustomLinearLayer, self).__init__() self.linear = nn.Linear(in_features, out_features) # Weight Initialization if weight_init == \'xavier_uniform\': nn.init.xavier_uniform_(self.linear.weight) elif weight_init == \'xavier_normal\': nn.init.xavier_normal_(self.linear.weight) elif weight_init == \'kaiming_uniform\': nn.init.kaiming_uniform_(self.linear.weight) elif weight_init == \'kaiming_normal\': nn.init.kaiming_normal_(self.linear.weight) else: raise ValueError(\\"Invalid weight initialization method.\\") # Bias Initialization if bias_init == \'zeros\': nn.init.zeros_(self.linear.bias) elif bias_init == \'ones\': nn.init.ones_(self.linear.bias) elif bias_init == \'constant\': nn.init.constant_(self.linear.bias, 0.01) else: raise ValueError(\\"Invalid bias initialization method.\\") def forward(self, x): return self.linear(x) # Example usage layer = CustomLinearLayer(128, 64, weight_init=\'xavier_normal\', bias_init=\'zeros\') input_tensor = torch.randn(32, 128) output_tensor = layer(input_tensor) print(output_tensor.shape) # Expected output: torch.Size([32, 64]) ``` Submission Guidelines - Implement the `CustomLinearLayer` class along with the required methods and initialization logic. - Ensure your code is well-documented and follows Python coding standards. - Test your implementation with different initialization methods to verify correctness.","solution":"import torch import torch.nn as nn class CustomLinearLayer(nn.Module): def __init__(self, in_features, out_features, weight_init, bias_init): super(CustomLinearLayer, self).__init__() self.linear = nn.Linear(in_features, out_features) # Weight Initialization if weight_init == \'xavier_uniform\': nn.init.xavier_uniform_(self.linear.weight) elif weight_init == \'xavier_normal\': nn.init.xavier_normal_(self.linear.weight) elif weight_init == \'kaiming_uniform\': nn.init.kaiming_uniform_(self.linear.weight) elif weight_init == \'kaiming_normal\': nn.init.kaiming_normal_(self.linear.weight) else: raise ValueError(f\\"Invalid weight initialization method: {weight_init}\\") # Bias Initialization if bias_init == \'zeros\': nn.init.zeros_(self.linear.bias) elif bias_init == \'ones\': nn.init.ones_(self.linear.bias) elif bias_init == \'constant\': nn.init.constant_(self.linear.bias, 0.01) else: raise ValueError(f\\"Invalid bias initialization method: {bias_init}\\") def forward(self, x): return self.linear(x)"},{"question":"In this task, you are required to implement a classifier that can effectively tune its decision threshold for optimal performance on a given metric using `scikit-learn`. You will be working with a synthetic dataset and a Logistic Regression model. Your objective is to: 1. Train a Logistic Regression classifier. 2. Tune the decision threshold to maximize the F1-score for identifying a specific positive class label. 3. Evaluate the performance of the model with the optimized decision threshold. Implement the following function: ```python def tune_threshold(X, y, pos_label): Trains a Logistic Regression classifier and tunes its decision threshold to maximize the F1-score for the given positive class label. Parameters: - X: np.ndarray, shape (n_samples, n_features) Feature matrix. - y: np.ndarray, shape (n_samples,) True labels for the feature matrix. - pos_label: int The label of the class of interest for which the F1-score should be maximized. Returns: - best_thresh: float The best threshold that maximizes the F1-score. - tuned_model: TunedThresholdClassifierCV The tuned classifier with the optimal threshold. - f1: float The F1-score achieved with the tuned threshold. import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, f1_score # Define the Logistic Regression model base_model = LogisticRegression() # Create the F1-score scorer with the specified positive class label scorer = make_scorer(f1_score, pos_label=pos_label) # Initialize and train the TunedThresholdClassifierCV tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X, y) # Extract the best decision threshold best_thresh = tuned_model.best_threshold_ # Evaluate the performance of the tuned model using the best threshold f1 = tuned_model.best_score_ return best_thresh, tuned_model, f1 ``` # constraints: 1. The synthetic dataset can be generated using `make_classification` from `scikit-learn`. 2. Assume only a binary classification problem. 3. You must emphasize maximizing the F1-score for the positive class (`pos_label`). # Example: ```python from sklearn.datasets import make_classification # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, weights=[0.1, 0.9], random_state=0) # The label of the positive class pos_label = 0 # Tune the threshold best_thresh, tuned_model, f1 = tune_threshold(X, y, pos_label) print(\\"Best Threshold:\\", best_thresh) print(\\"F1-Score with Optimized Threshold:\\", f1) ``` # Expected Output: ``` Best Threshold: <value> F1-Score with Optimized Threshold: <value> ``` Ensure the function works correctly and provides optimal threshold values as well as the corresponding F1-scores.","solution":"def tune_threshold(X, y, pos_label): Trains a Logistic Regression classifier and tunes its decision threshold to maximize the F1-score for the given positive class label. Parameters: - X: np.ndarray, shape (n_samples, n_features) Feature matrix. - y: np.ndarray, shape (n_samples,) True labels for the feature matrix. - pos_label: int The label of the class of interest for which the F1-score should be maximized. Returns: - best_thresh: float The best threshold that maximizes the F1-score. - tuned_model: LogisticRegression The tuned classifier. - f1: float The F1-score achieved with the tuned threshold. import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score # Split the data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42) # Train the Logistic Regression model model = LogisticRegression() model.fit(X_train, y_train) # Get the predicted probabilities for the positive class y_probs = model.predict_proba(X_val)[:, 1] # Search for the best threshold best_thresh = 0.5 best_f1 = 0 for threshold in np.linspace(0, 1, 101): y_pred = (y_probs >= threshold).astype(int) current_f1 = f1_score(y_val, y_pred, pos_label=pos_label) if current_f1 > best_f1: best_f1 = current_f1 best_thresh = threshold # Return the best threshold, the trained model, and the best F1-score return best_thresh, model, best_f1"},{"question":"# Python Coding Assessment Question Problem Statement You are required to implement a function `process_events` that will take a list of event records, each containing event details including a timestamp. The function will process these records to find and return events occurring within a certain date range and also categorize the counted frequency of the event names. Input - A list of dictionaries containing event details. Each dictionary has the following keys: - `event_name` (string): The name of the event. - `timestamp` (string): The event timestamp in the format `YYYY-MM-DD HH:MM:SS`. - Two strings representing the start and end of the date range in the format `YYYY-MM-DD`. Output - An `OrderedDict` where the keys are event names (strings) and the values are the counts of how many times each event occurred within the specified date range. Constraints - The input list will have at most 1000 events. - The date range will be valid and will be provided in the correct format. - There may be multiple events with the same name on the same date. Implementation Requirements 1. Use the `datetime` module to parse and handle date and time information. 2. Use `collections.OrderedDict` to maintain the order of events as they first appear in the input list. 3. Your solution should be efficient and handle the edge cases such as events exactly on the boundary of the date range. Example ```python from collections import OrderedDict from typing import List, Dict def process_events(events: List[Dict[str, str]], start_date: str, end_date: str) -> OrderedDict: pass # Example usage: events = [ {\\"event_name\\": \\"EventA\\", \\"timestamp\\": \\"2023-01-01 10:00:00\\"}, {\\"event_name\\": \\"EventB\\", \\"timestamp\\": \\"2023-01-02 12:00:00\\"}, {\\"event_name\\": \\"EventA\\", \\"timestamp\\": \\"2023-01-03 09:30:00\\"}, {\\"event_name\\": \\"EventC\\", \\"timestamp\\": \\"2023-01-04 14:45:00\\"}, {\\"event_name\\": \\"EventB\\", \\"timestamp\\": \\"2023-01-05 16:00:00\\"}, ] start_date = \\"2023-01-01\\" end_date = \\"2023-01-03\\" output = process_events(events, start_date, end_date) print(output) # Expected output: OrderedDict([(\'EventA\', 2), (\'EventB\', 1)]) ``` Note - Ensure your implementation passes the provided example and devise your own test cases to validate the correctness and robustness of your solution.","solution":"from collections import OrderedDict from datetime import datetime from typing import List, Dict def process_events(events: List[Dict[str, str]], start_date: str, end_date: str) -> OrderedDict: start_date = datetime.strptime(start_date, \\"%Y-%m-%d\\") end_date = datetime.strptime(end_date, \\"%Y-%m-%d\\") end_date = end_date.replace(hour=23, minute=59, second=59) # Include the whole end day event_count = OrderedDict() for event in events: event_timestamp = datetime.strptime(event[\\"timestamp\\"], \\"%Y-%m-%d %H:%M:%S\\") if start_date <= event_timestamp <= end_date: event_name = event[\\"event_name\\"] if event_name in event_count: event_count[event_name] += 1 else: event_count[event_name] = 1 return event_count"},{"question":"**Advanced Python File Handling and Control** Using the `fcntl` module, you are required to implement a custom file lock management system that ensures safe concurrent file access. # Objectives 1. Implement a function `acquire_file_lock` that attempts to acquire a file lock. The function should accept the filename and the type of lock as parameters. 2. Implement a function `release_file_lock` that releases the file lock acquired. # Details 1. **acquire_file_lock(filename: str, lock_type: str) -> int** - **Parameters:** - `filename` (str): The name of the file to lock. - `lock_type` (str): The type of lock to acquire. It can be either \\"shared\\" (for shared lock) or \\"exclusive\\" (for exclusive lock). - **Returns:** - The file descriptor of the locked file if the lock is successfully acquired. - **Raises:** - Raises `OSError` if the lock could not be acquired. The function should: - Open the file. - Use `fcntl.flock` to acquire an appropriate lock based on the `lock_type` parameter. - Handle errors appropriately. 2. **release_file_lock(fd: int) -> None** - **Parameters:** - `fd` (int): The file descriptor of the locked file. - **Returns:** - None - **Raises:** - Raises `OSError` if the lock could not be released. The function should: - Use `fcntl.flock` to release the lock. # Constraints - The `acquire_file_lock` function should not block if the lock cannot be acquired immediately. - Your implementation should be compatible with Unix-like systems (Linux, macOS). - Proper error handling and resource management (like closing the file descriptor) are essential. # Example Usage ```python import os import fcntl def acquire_file_lock(filename: str, lock_type: str) -> int: # Open the file fd = os.open(filename, os.O_RDWR) try: # Determine the lock operation if lock_type == \\"shared\\": operation = fcntl.LOCK_SH | fcntl.LOCK_NB elif lock_type == \\"exclusive\\": operation = fcntl.LOCK_EX | fcntl.LOCK_NB else: raise ValueError(\\"Invalid lock type specified\\") # Attempt to acquire the lock fcntl.flock(fd, operation) except OSError as e: os.close(fd) raise OSError(\\"Unable to acquire lock: \\" + str(e)) return fd def release_file_lock(fd: int) -> None: try: # Release the lock fcntl.flock(fd, fcntl.LOCK_UN) finally: # Close the file descriptor os.close(fd) # Example usage filename = \\"testfile.txt\\" fd = acquire_file_lock(filename, \\"exclusive\\") print(f\\"Lock acquired on file descriptor {fd}\\") release_file_lock(fd) print(\\"Lock released\\") ``` Implement the above described functions `acquire_file_lock` and `release_file_lock` to demonstrate your understanding and ability to safely handle file locks using the `fcntl` module.","solution":"import os import fcntl def acquire_file_lock(filename: str, lock_type: str) -> int: Attempts to acquire a file lock on the specified file. :param filename: The name of the file to lock. :param lock_type: The type of lock to acquire (\\"shared\\" or \\"exclusive\\"). :return: The file descriptor of the locked file. :raises OSError: If the lock cannot be acquired. # Open the file fd = os.open(filename, os.O_RDWR) try: # Determine the lock operation if lock_type == \\"shared\\": operation = fcntl.LOCK_SH | fcntl.LOCK_NB elif lock_type == \\"exclusive\\": operation = fcntl.LOCK_EX | fcntl.LOCK_NB else: raise ValueError(\\"Invalid lock type specified\\") # Attempt to acquire the lock fcntl.flock(fd, operation) except OSError as e: os.close(fd) raise OSError(f\\"Unable to acquire lock on file {filename}: {e}\\") return fd def release_file_lock(fd: int) -> None: Releases a file lock on the specified file descriptor. :param fd: The file descriptor. :return: None :raises OSError: If the lock cannot be released. try: # Release the lock fcntl.flock(fd, fcntl.LOCK_UN) finally: # Close the file descriptor os.close(fd)"},{"question":"Objective Design and implement an attention mechanism using the `torch.nn.attention.experimental` module in PyTorch. Your implementation should include a `ScaledDotProductAttention` class that executes the scaled dot-product attention operation, commonly used in Transformer models. Task Implement the class `ScaledDotProductAttention` with the following specifications: 1. **Inputs**: - `query` (Tensor): A 3D tensor of shape `(batch_size, num_heads, seq_len, d_k)`, where `batch_size` is the number of batches, `num_heads` is the number of attention heads, `seq_len` is the length of input sequences, and `d_k` is the dimensionality of the key vectors. - `key` (Tensor): A 3D tensor with the same shape as `query`. - `value` (Tensor): A 3D tensor with the same shape as `query`. - `mask` (Tensor, optional): A 3D tensor of shape `(batch_size, 1, 1, seq_len)` indicating which elements should be masked during attention calculation. This tensor should contain values of `1` (masked) or `0` (unmasked). 2. **Output**: - `output` (Tensor): A 3D tensor of shape `(batch_size, num_heads, seq_len, d_k)` representing the output of the attention mechanism. - `attention_weights` (Tensor): A 3D tensor of shape `(batch_size, num_heads, seq_len, seq_len)` representing the attention weights. 3. **Constraints**: - You can assume `query`, `key`, and `value` tensors have the same shape. - Sequence length (`seq_len`) can vary. 4. **Performance requirements**: - The implementation should be efficient enough to handle large tensors commonly used in Transformer models. - Handling masking effectively to ensure certain positions are ignored during the calculation. 5. **Details**: - Use PyTorch functions to implement tensor operations. - Include appropriate comments and docstrings in your code. - You are encouraged to use tensor operations without resorting to explicit Python loops. ```python import torch import torch.nn.functional as F class ScaledDotProductAttention: def __init__(self): pass def forward(self, query, key, value, mask=None): Perform scaled dot-product attention. Args: query (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). key (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). value (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). mask (torch.Tensor, optional): Tensor of shape (batch_size, 1, 1, seq_len). Defaults to None. Returns: output (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). attention_weights (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, seq_len). # Implement the scaled dot-product attention mechanism here. # Hint: You may find the functions torch.matmul, torch.nn.functional.softmax, and various tensor operations useful. pass # Example usage: # query = torch.rand((batch_size, num_heads, seq_len, d_k)) # key = torch.rand((batch_size, num_heads, seq_len, d_k)) # value = torch.rand((batch_size, num_heads, seq_len, d_k)) # mask = torch.ones((batch_size, 1, 1, seq_len)) # attention = ScaledDotProductAttention() # output, weights = attention.forward(query, key, value, mask) ``` Your solution will be evaluated on correctness, efficiency, and adherence to the specifications outlined above. Good luck!","solution":"import torch import torch.nn.functional as F class ScaledDotProductAttention: def __init__(self): pass def forward(self, query, key, value, mask=None): Perform scaled dot-product attention. Args: query (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). key (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). value (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). mask (torch.Tensor, optional): Tensor of shape (batch_size, 1, 1, seq_len). Defaults to None. Returns: output (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, d_k). attention_weights (torch.Tensor): Tensor of shape (batch_size, num_heads, seq_len, seq_len). d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float)) if mask is not None: scores = scores.masked_fill(mask == 1, float(\'-inf\')) attention_weights = F.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output, attention_weights"},{"question":"# Coding Challenge: Regex Text Parser **Objective:** You are required to implement a function that processes log file entries to extract and summarize information about the occurrences of different types of messages within the logs. The entries in the log file follow a specific format, and your task is to identify message types and their details using Python\'s `re` module. **Log Entry Format:** Each log entry follows the structure shown below: ``` [<timestamp>] <message_type>: <message_details> ``` - `<timestamp>`: The timestamp when the log entry was created, in the format `YYYY-MM-DD HH:MM:SS`. - `<message_type>`: Type of the message (`INFO`, `WARNING`, `ERROR`, or `DEBUG`). - `<message_details>`: Detailed information about the message. **Example Log Entries:** ``` [2023-08-12 14:33:21] INFO: System booting up [2023-08-12 14:34:15] WARNING: Memory usage high [2023-08-12 14:35:05] ERROR: Failed to load configuration file [2023-08-12 15:01:52] DEBUG: Fetching user details ``` **Function Specifications:** Your task is to implement a function `parse_log_entries(log_entries: List[str]) -> Dict[str, Any]` that takes in a list of log entries and returns a dictionary summarizing the occurrences of different types of messages and the latest message details for each type. **Input:** - `log_entries` (List[str]): A list of log entries as strings. **Output:** - Returns a dictionary with the following format: ```python { \\"INFO\\": { \\"count\\": <number_of_info_messages>, \\"latest\\": \\"<latest_info_message_details>\\" }, \\"WARNING\\": { \\"count\\": <number_of_warning_messages>, \\"latest\\": \\"<latest_warning_message_details>\\" }, \\"ERROR\\": { \\"count\\": <number_of_error_messages>, \\"latest\\": \\"<latest_error_message_details>\\" }, \\"DEBUG\\": { \\"count\\": <number_of_debug_messages>, \\"latest\\": \\"<latest_debug_message_details>\\" } } ``` **Constraints:** - The log entries may not necessarily be in chronological order. - A type of message may not be present in the log entries. - You can assume that the log entries will always be well-formed as per the given format. **Example:** ```python log_entries = [ \\"[2023-08-12 14:33:21] INFO: System booting up\\", \\"[2023-08-12 14:34:15] WARNING: Memory usage high\\", \\"[2023-08-12 14:35:05] ERROR: Failed to load configuration file\\", \\"[2023-08-12 15:01:52] DEBUG: Fetching user details\\", \\"[2023-08-12 15:05:30] INFO: System ready\\", \\"[2023-08-12 15:10:50] ERROR: Unknown error occurred\\" ] expected_output = { \\"INFO\\": { \\"count\\": 2, \\"latest\\": \\"System ready\\" }, \\"WARNING\\": { \\"count\\": 1, \\"latest\\": \\"Memory usage high\\" }, \\"ERROR\\": { \\"count\\": 2, \\"latest\\": \\"Unknown error occurred\\" }, \\"DEBUG\\": { \\"count\\": 1, \\"latest\\": \\"Fetching user details\\" } } assert parse_log_entries(log_entries) == expected_output ``` **Hint:** - Use the `re` module to parse the log entries. - Pay attention to extracting information using groups and backreferences in regular expressions.","solution":"import re from typing import List, Dict, Any def parse_log_entries(log_entries: List[str]) -> Dict[str, Any]: Parses log entries to extract and summarize information about the occurrences of different types of messages within the logs. Args: log_entries (List[str]): A list of log entries as strings. Returns: Dict[str, Any]: A dictionary summarizing occurrences and latest details of different message types. message_types = [\\"INFO\\", \\"WARNING\\", \\"ERROR\\", \\"DEBUG\\"] summary = {message_type: {\\"count\\": 0, \\"latest\\": \\"\\"} for message_type in message_types} log_pattern = re.compile(r\'[(?P<timestamp>[d- :]+)] (?P<type>INFO|WARNING|ERROR|DEBUG): (?P<details>.+)\') for entry in log_entries: match = log_pattern.match(entry) if match: message_type = match.group(\'type\') message_details = match.group(\'details\') summary[message_type][\\"count\\"] += 1 summary[message_type][\\"latest\\"] = message_details return summary"},{"question":"You are provided with a large text file that needs to be compressed and later decompressed. Your task is to implement a function that reads the file, compresses it either incrementally or in one-shot based on a parameter, and writes the compressed data to a bzip2 compressed file. Additionally, you will write another function to decompress the compressed file and verify the integrity of the decompressed data against the original text file. # Requirements 1. Implement the function `compress_file(input_filename: str, output_filename: str, method: str, chunk_size: int = 1024) -> None`: - `input_filename`: The path to the input text file. - `output_filename`: The path to the output bzip2 compressed file. - `method`: A string `one-shot` or `incremental` specifying the compression method. - `chunk_size`: An integer specifying the chunk size for incremental compression. Default is 1024 bytes. - Compress the file using either one-shot or incremental compression based on the `method` parameter and write the compressed data to `output_filename`. 2. Implement the function `decompress_file(compressed_filename: str, decompressed_filename: str) -> None`: - `compressed_filename`: The path to the input bzip2 compressed file. - `decompressed_filename`: The path to the output decompressed text file. - Decompress the compressed file and write the decompressed data to `decompressed_filename`. 3. Implement the function `verify_integrity(original_filename: str, decompressed_filename: str) -> bool`: - `original_filename`: The path to the original text file. - `decompressed_filename`: The path to the decompressed text file. - Return `True` if the contents of the original and decompressed files are the same, otherwise `False`. # Constraints - The input text file can be very large (up to several GB). - The `method` parameter will be either `one-shot` or `incremental`. - The `chunk_size` parameter is only applicable when the `method` is `incremental`. # Example Usage ```python input_filename = \'large_text_file.txt\' compressed_filename = \'compressed_file.bz2\' decompressed_filename = \'decompressed_file.txt\' compress_file(input_filename, compressed_filename, method=\'incremental\', chunk_size=2048) decompress_file(compressed_filename, decompressed_filename) assert verify_integrity(input_filename, decompressed_filename), \\"Integrity check failed!\\" ``` This example demonstrates the function usage for compressing a large text file incrementally with a chunk size of 2048 bytes and verifying that the decompressed file\'s contents match the original file. # Note - Ensure efficient handling of file I/O and compression to manage large file sizes. - Handle any potential exceptions that might occur during file operations and compression/decompression processes.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, method: str, chunk_size: int = 1024) -> None: if method == \'one-shot\': with open(input_filename, \'rb\') as f_in, bz2.open(output_filename, \'wb\') as f_out: data = f_in.read() f_out.write(data) elif method == \'incremental\': with open(input_filename, \'rb\') as f_in, bz2.open(output_filename, \'wb\') as f_out: while True: chunk = f_in.read(chunk_size) if not chunk: break f_out.write(chunk) else: raise ValueError(\\"Method must be \'one-shot\' or \'incremental\'\\") def decompress_file(compressed_filename: str, decompressed_filename: str) -> None: with bz2.open(compressed_filename, \'rb\') as f_in, open(decompressed_filename, \'wb\') as f_out: while True: chunk = f_in.read(1024) if not chunk: break f_out.write(chunk) def verify_integrity(original_filename: str, decompressed_filename: str) -> bool: with open(original_filename, \'rb\') as f_orig, open(decompressed_filename, \'rb\') as f_decomp: while True: chunk_orig = f_orig.read(1024) chunk_decomp = f_decomp.read(1024) if chunk_orig != chunk_decomp: return False if not chunk_orig: # End of file break return True"},{"question":"Distributed Training with Generic Join Context Manager --- Objective: Implement a distributed training loop that handles uneven inputs using PyTorch\'s Generic Join Context Manager. This assessment will test your understanding of distributed training concepts and the ability to manage uneven input sizes across different workers using the `Join`, `Joinable`, and `JoinHook` classes. Problem Statement: You are required to implement a distributed training function `distributed_train` that trains a simple neural network on various input batches of unequal sizes using the Generic Join Context Manager. The function should: 1. Initialize a distributed training environment. 2. Create a simple neural network model. 3. Prepare uneven input data batches. 4. Use the `Join`, `Joinable`, and `JoinHook` classes to manage the uneven inputs across workers. 5. Perform a simple training loop that demonstrates distributed training with uneven input sizes. Function Signature: ```python def distributed_train( world_size: int, input_data: List[torch.Tensor], target_data: List[torch.Tensor], epochs: int = 5 ) -> None: Perform distributed training on a simple neural network with uneven input sizes. Parameters: - world_size (int): The number of distributed workers. - input_data (List[torch.Tensor]): A list of input tensors for the training data. - target_data (List[torch.Tensor]): A list of target tensors corresponding to the input data. - epochs (int): The number of epochs to train the model. Default is 5. pass ``` Requirements: 1. Initialize the distributed environment and handle the setup for multiple workers. 2. Define a simple neural network model suitable for classification tasks. 3. Ensure the input_data and target_data are divided among the workers, but they might not be evenly distributed. 4. Implement a training loop that uses the Generic Join Context Manager classes to synchronize processes and handle different input batch sizes. 5. Print out the loss at each epoch for all workers. Constraints: - Assume that the input_data and target_data lists are of equal length but contain tensors of possibly different sizes. - Use an appropriate loss function and optimizer for training the model. - Ensure the function works for any reasonable number of workers (specified by `world_size`). Example: ```python import torch # Example input and target data input_data = [torch.rand(10, 5), torch.rand(20, 5), torch.rand(15, 5)] target_data = [torch.randint(0, 2, (10,)), torch.randint(0, 2, (20,)), torch.randint(0, 2, (15,))] # Perform distributed training with 2 workers distributed_train(world_size=2, input_data=input_data, target_data=target_data, epochs=5) ``` In this example, the function should coordinate the training across 2 workers with the provided input_data and target_data lists, handling the unequal sizes appropriately using PyTorch\'s Generic Join Context Manager. Notes: - You can assume that necessary PyTorch packages are already installed. - The implementation should handle the initialization and finalization of the distributed process group properly. - For simplicity, the code for initializing and finalizing the distribution, as well as other utility functions, can be written within the `distributed_train` function or as inner functions. ---","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.multiprocessing import Process from typing import List class SimpleNN(nn.Module): def __init__(self, input_size: int): super(SimpleNN, self).__init__() self.fc = nn.Linear(input_size, 2) def forward(self, x): return self.fc(x) def run(rank, world_size, input_data, target_data, epochs): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) input_size = input_data[0].size(1) model = SimpleNN(input_size) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): for inputs, targets in zip(input_data, target_data): # Ensure input is divsible among processes inputs = inputs.chunk(world_size, dim=0) targets = targets.chunk(world_size, dim=0) # Each process uses its respective chunk inputs = inputs[rank] targets = targets[rank] outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch+1}, Loss: {loss.item()}\\") dist.destroy_process_group() def distributed_train(world_size: int, input_data: List[torch.Tensor], target_data: List[torch.Tensor], epochs: int = 5) -> None: processes = [] for rank in range(world_size): process = Process(target=run, args=(rank, world_size, input_data, target_data, epochs)) process.start() processes.append(process) for process in processes: process.join() # Example usage: # import torch # input_data = [torch.rand(10, 5), torch.rand(20, 5), torch.rand(15, 5)] # target_data = [torch.randint(0, 2, (10,)), torch.randint(0, 2, (20,)), torch.randint(0, 2, (15,))] # distributed_train(world_size=2, input_data=input_data, target_data=target_data, epochs=5)"},{"question":"Objective You are required to implement a function that will create an XML document representing a book catalog, add books to the catalog, and manipulate some of its elements and attributes. This assessment will check your understanding of creating and working with XML documents using the `xml.dom` module in Python. Task Implement the following function: ```python def create_book_catalog(books): Create an XML document representing a book catalog with the given books. Args: books (list of dict): A list of dictionaries, each containing information about a book. Each dictionary has the following keys: - \'title\' (str): The title of the book. - \'author\' (str): The author of the book. - \'year\' (int): The publication year of the book. - \'genre\' (str): The genre of the book. Returns: str: A string representation of the XML document. ``` Instructions 1. **Create the XML Document:** - Use the `xml.dom` module to create an XML Document object representing the root element `<catalog>`. 2. **Add Books to the Catalog:** - For each book dictionary in the list `books`, create an `<book>` element with child elements `<title>`, `<author>`, `<year>`, and `<genre>`, and corresponding text nodes set to the respective values from the dictionary. - Append each `<book>` element as a child of the `<catalog>` root element. 3. **Manipulate Elements and Attributes:** - If the year of the book is before 2000, add an attribute `old=\\"yes\\"` to the `<book>` element, otherwise `old=\\"no\\"`. 4. **Create String Representation:** - Convert the XML document into a string and return it. Example ```python books = [ {\'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\', \'year\': 1925, \'genre\': \'Fiction\'}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'genre\': \'Fiction\'}, {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'genre\': \'Dystopian\'}, {\'title\': \'The Catcher in the Rye\', \'author\': \'J.D. Salinger\', \'year\': 1951, \'genre\': \'Fiction\'} ] output = create_book_catalog(books) print(output) ``` Expected Output (formatted for readability): ```xml <catalog> <book old=\\"yes\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> <genre>Fiction</genre> </book> <book old=\\"yes\\"> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <genre>Fiction</genre> </book> <book old=\\"yes\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> <genre>Dystopian</genre> </book> <book old=\\"yes\\"> <title>The Catcher in the Rye</title> <author>J.D. Salinger</author> <year>1951</year> <genre>Fiction</genre> </book> </catalog> ``` Constraints - Do not use any external libraries other than the standard `xml.dom` module. - Assume the input list of books is always properly formatted and non-empty. - Ensure that the generated XML document is valid and well-structured. Good luck!","solution":"from xml.dom.minidom import Document def create_book_catalog(books): Create an XML document representing a book catalog with the given books. Args: books (list of dict): A list of dictionaries, each containing information about a book. Each dictionary has the following keys: - \'title\' (str): The title of the book. - \'author\' (str): The author of the book. - \'year\' (int): The publication year of the book. - \'genre\' (str): The genre of the book. Returns: str: A string representation of the XML document. # Create the XML document doc = Document() # Create the root element <catalog> catalog = doc.createElement(\\"catalog\\") doc.appendChild(catalog) # Add books to the catalog for book in books: # Create a book element book_element = doc.createElement(\\"book\\") # Add \'old\' attribute based on the year of the book if book[\'year\'] < 2000: book_element.setAttribute(\'old\', \'yes\') else: book_element.setAttribute(\'old\', \'no\') # Add title element title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(book[\'title\']) title_element.appendChild(title_text) book_element.appendChild(title_element) # Add author element author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(book[\'author\']) author_element.appendChild(author_text) book_element.appendChild(author_element) # Add year element year_element = doc.createElement(\\"year\\") year_text = doc.createTextNode(str(book[\'year\'])) year_element.appendChild(year_text) book_element.appendChild(year_element) # Add genre element genre_element = doc.createElement(\\"genre\\") genre_text = doc.createTextNode(book[\'genre\']) genre_element.appendChild(genre_text) book_element.appendChild(genre_element) # Append the book element to the catalog catalog.appendChild(book_element) # Create a string representation of the XML document xml_str = doc.toprettyxml(indent=\\" \\") return xml_str"},{"question":"**Objective:** Implement a Python class `CustomIterators` that demonstrates the functionality of both sequence and callable iterators using the provided Python C-API (simulated through pure Python). Your class should provide methods to create these iterators and iterate through them. Class Definition ```python class CustomIterators: @staticmethod def create_seq_iterator(sequence): Create an iterator from a sequence. Args: sequence (list or tuple): The input sequence to iterate over. Returns: iterator: An iterator for the sequence. pass @staticmethod def create_callable_iterator(callable_func, sentinel): Create an iterator from a callable object and sentinel value. Args: callable_func (function): A callable function that is called repeatedly. sentinel (any): A value that stops the iteration when returned by the callable. Returns: iterator: An iterator for the callable. pass @staticmethod def iterate(iterator): Iterate over the elements of an iterator and collect them in a list. Args: iterator (iterator): An iterator to iterate over. Returns: list: A list of elements produced by the iterator. pass ``` Requirements 1. **Create Sequence Iterator:** - The `create_seq_iterator` method should take a sequence (list or tuple) as input and return a sequence iterator for it. 2. **Create Callable Iterator:** - The `create_callable_iterator` method should take a callable object and a sentinel value as inputs and return a callable iterator. - The callable object should be a function that is repeatedly called until it returns a value equal to the sentinel. 3. **Iterate Through Iterator:** - The `iterate` method should take an iterator and return a list of all items produced by the iterator. Constraints - The input sequences for `create_seq_iterator` will have at most 1000 elements. - The callable function for `create_callable_iterator` will eventually return the sentinel value within 1000 calls. Example Usage ```python # Example of sequence iterator sequence = [1, 2, 3, 4, 5] seq_iter = CustomIterators.create_seq_iterator(sequence) result = CustomIterators.iterate(seq_iter) print(result) # Output: [1, 2, 3, 4, 5] # Example of callable iterator def generate_numbers(): n = 0 def func(): nonlocal n n += 1 return n return func callable_func = generate_numbers() call_iter = CustomIterators.create_callable_iterator(callable_func, sentinel=5) result = CustomIterators.iterate(call_iter) print(result) # Output: [1, 2, 3, 4, 5] ``` Note - Use Python\'s built-in `iter()` function to simulate the behavior of the `PySeqIter_New()` and `PyCallIter_New()` functions.","solution":"class CustomIterators: @staticmethod def create_seq_iterator(sequence): Create an iterator from a sequence. Args: sequence (list or tuple): The input sequence to iterate over. Returns: iterator: An iterator for the sequence. return iter(sequence) @staticmethod def create_callable_iterator(callable_func, sentinel): Create an iterator from a callable object and sentinel value. Args: callable_func (function): A callable function that is called repeatedly. sentinel (any): A value that stops the iteration when returned by the callable. Returns: iterator: An iterator for the callable. return iter(callable_func, sentinel) @staticmethod def iterate(iterator): Iterate over the elements of an iterator and collect them in a list. Args: iterator (iterator): An iterator to iterate over. Returns: list: A list of elements produced by the iterator. return list(iterator)"},{"question":"**Task: Advanced Enumeration Handling** You are tasked with creating an advanced enumeration to represent a library system\'s categorization of books. The categories should be unique and should include some additional behaviors. Follow the steps below to complete the task. 1. **Define the Enumeration:** - Create an enum class called `Category` using the `enum` module. - The categories should include \'FICTION\', \'NONFICTION\', \'SCIENCE\', \'ART\', \'HISTORY\', and \'TECHNOLOGY\'. - Each category should have a unique integer value assigned using the `auto()` function. 2. **Ensure Unique Values:** - Use the `@unique` decorator to ensure that no two categories share the same value. 3. **Add Custom Behaviors:** - Add a method `describe` that returns a description for each category. - Add a class method `most_popular` that returns \'FICTION\' as the most popular category. 4. **Define Custom String Representations:** - Override the `__str__` method to return the category name in a user-friendly string format like \\"Category: FICTION\\". - Override the `__repr__` method to provide a detailed string representation of the category. 5. **Constraints and Usage:** - The enum values should be automatically assigned using `auto()`. - The custom `describe` method should return meaningful descriptions for each category like \\"Fictional Books\\", \\"Non-Fictional Books\\", etc. - The custom `most_popular` method should statically return the \'FICTION\' category. - Ensure that the enum adheres to unique value constraints, using `@unique`. **Example Usage:** ```python from enum import Enum, auto, unique @unique class Category(Enum): FICTION = auto() NONFICTION = auto() SCIENCE = auto() ART = auto() HISTORY = auto() TECHNOLOGY = auto() def describe(self): descriptions = { \'FICTION\': \'Fictional Books\', \'NONFICTION\': \'Non-Fictional Books\', \'SCIENCE\': \'Scientific Books\', \'ART\': \'Art Books\', \'HISTORY\': \'Historical Books\', \'TECHNOLOGY\': \'Technology Books\', } return descriptions[self.name] def __str__(self): return f\\"Category: {self.name}\\" def __repr__(self): return f\\"<Category.{self.name}: {self.value} - {self.describe()}>\\" @classmethod def most_popular(cls): return cls.FICTION # Test the implementation print(Category.FICTION) # Output: Category: FICTION print(repr(Category.FICTION)) # Output: <Category.FICTION: 1 - Fictional Books> print(Category.FICTION.describe()) # Output: Fictional Books print(Category.most_popular()) # Output: Category.FICTION ``` **Instructions:** 1. Implement the `Category` enum class as described. 2. Ensure all methods and behaviors function correctly. 3. Write a script to demonstrate the specified example usage, performing all the operations and printing the results. **Constraints:** - Use the `enum` module functionalities (`Enum`, `auto`, `@unique`) appropriately. - Follow the provided structure and example closely. - Ensure the implementation is efficient and adheres to Python best practices.","solution":"from enum import Enum, auto, unique @unique class Category(Enum): FICTION = auto() NONFICTION = auto() SCIENCE = auto() ART = auto() HISTORY = auto() TECHNOLOGY = auto() def describe(self): descriptions = { \'FICTION\': \'Fictional Books\', \'NONFICTION\': \'Non-Fictional Books\', \'SCIENCE\': \'Scientific Books\', \'ART\': \'Art Books\', \'HISTORY\': \'Historical Books\', \'TECHNOLOGY\': \'Technology Books\', } return descriptions[self.name] def __str__(self): return f\\"Category: {self.name}\\" def __repr__(self): return f\\"<Category.{self.name}: {self.value} - {self.describe()}>\\" @classmethod def most_popular(cls): return cls.FICTION"},{"question":"**Question:** You are given a CSV file with user data containing the following fields: `user_id`, `first_name`, `last_name`, and `email`. The file may contain inconsistencies, such as rows with missing fields, extra fields, or incorrectly quoted fields. Additionally, you need to handle different CSV dialects. **Task:** Write a Python function `process_user_data(input_file: str, output_file: str, dialect: str = \'excel\') -> None` that fulfills the following requirements: 1. **Read Data:** Read the CSV file `input_file` using the specified `dialect`. 2. **Process Data:** - Ensure all rows are converted to dictionaries using `DictReader`. - Handle missing fields by assigning an empty string `\'\'` to them. - Ignore rows with extra fields. - Ensure `email` fields are lowercased. 3. **Write Data:** Write the processed data to a new CSV file `output_file` using the same `dialect`, ensuring that: - Rows are written in the same order they were read. - The output CSV includes a header row with the fields `user_id`, `first_name`, `last_name`, and `email`. - Proper quoting is used to handle special characters in fields. **Function Signature:** ```python def process_user_data(input_file: str, output_file: str, dialect: str = \'excel\') -> None: ``` **Input:** - `input_file` (str): Path to the input CSV file. - `output_file` (str): Path to the output CSV file. - `dialect` (str): The CSV dialect to use. Defaults to `\'excel\'`. **Output:** - The function does not return any value but writes the processed data to `output_file`. **Constraints:** - The input file may contain up to 100,000 lines. - The function should use the `csv` module\'s `reader()`, `DictReader()`, `writer()`, and `DictWriter()` as needed. - You may assume that the `input_file` and `output_file` paths are valid and writable. **Example Usage:** ```python process_user_data(\'input.csv\', \'output.csv\', \'excel\') ``` The provided implementation should handle reading, processing, and writing large datasets efficiently, taking advantage of the `csv` module\'s capabilities to manage CSV dialects and handle various edge cases in CSV file formatting.","solution":"import csv def process_user_data(input_file: str, output_file: str, dialect: str = \'excel\') -> None: email_lowercase = lambda email: email.lower() if email else email with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile, dialect=dialect) fieldnames = [\'user_id\', \'first_name\', \'last_name\', \'email\'] processed_rows = [] for row in reader: # Ensure all necessary fields are present processed_row = {field: row.get(field, \'\') for field in fieldnames} if len(row) > len(fieldnames): # Skip rows with extra fields continue processed_row[\'email\'] = email_lowercase(processed_row[\'email\']) processed_rows.append(processed_row) # Write processed data to output file with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=dialect) writer.writeheader() writer.writerows(processed_rows)"},{"question":"# Advanced Python C-API: List Manipulation You are tasked with implementing a Python function utilizing the C API interface for list objects. This function will create a new list, perform several operations on it, and return specific results based on these operations. # Function Specification **Function Name:** `manipulate_list` **Input:** - A list of integers `items`: The list to be manipulated. - Two integer values `slice_start` and `slice_end`: Indices specifying the start and end of the slice to be extracted from the list. - An integer value `insert_index`: The index at which a new element is to be inserted. - An integer value `insert_value`: The value to be inserted at the specified index. **Output:** - A tuple containing two elements: 1. The list created by the function. 2. The specified slice of the list (from `slice_start` to `slice_end`). **Constraints:** - The function must handle out-of-bound indices gracefully by clamping them to valid ranges. - The function must use the PyList_* functions or their macro equivalents without extensive error checking. **Requirements:** - Create a new list using `PyList_New()`. - Insert elements from `items` into the list. - Insert the specified `insert_value` at `insert_index`. - Extract a slice from `slice_start` to `slice_end` and return it along with the modified list. - Ensure the function correctly handles and returns the results. Here is the function signature you will implement in Python using the C API principles encapsulated in Cython or a custom CPython extension: ```python def manipulate_list(items, slice_start, slice_end, insert_index, insert_value): # Your implementation here ``` **Example:** Given the input: ```python items = [1, 2, 3, 4, 5] slice_start = 1 slice_end = 4 insert_index = 2 insert_value = 10 ``` The function should create a list `[1, 2, 10, 3, 4, 5]`, extract the slice `[2, 10, 3]`, and return: ```python ([1, 2, 10, 3, 4, 5], [2, 10, 3]) ``` # Note: - Assume the use of a Python extension building tool such as Cython or a similar setup to facilitate using the Python C API within the Python function. - The implementation should encapsulate the provided list operations through corresponding C API functions for educational purposes.","solution":"def manipulate_list(items, slice_start, slice_end, insert_index, insert_value): Manipulates the list by performing several operations: 1. Insert insert_value at insert_index. 2. Return the list and a specified slice of it. # Clamp insert_index to valid range if insert_index < 0: insert_index = 0 elif insert_index > len(items): insert_index = len(items) # Insert the value items.insert(insert_index, insert_value) # Clamp slice indices to valid range if slice_start < 0: slice_start = 0 if slice_end > len(items): slice_end = len(items) # Get the slice sliced_items = items[slice_start:slice_end] return items, sliced_items"},{"question":"# Spectral Biclustering Question **Objective**: Implement the Spectral Biclustering algorithm from scratch and evaluate its performance using the Jaccard index. # Problem Statement You are required to implement the Spectral Biclustering algorithm and apply it to a given dataset. The dataset is a matrix with a hidden checkerboard structure. You will: 1. Normalize the data matrix using the **bistochastization** method. 2. Compute the first few singular vectors. 3. Rank and select the best singular vectors using Euclidean distance. 4. Cluster the rows and columns using k-means. 5. Evaluate the clustering performance using the Jaccard index. # Function Signatures Implement the following functions: 1. `def bistochastization(A: np.ndarray) -> np.ndarray` - **Input**: A 2D numpy array `A` representing the data matrix. - **Output**: A 2D numpy array representing the bistochastized matrix. 2. `def compute_singular_vectors(A: np.ndarray, num_vectors: int) -> Tuple[np.ndarray, np.ndarray]` - **Input**: - A 2D numpy array `A` representing the data matrix. - An integer `num_vectors` indicating the number of singular vectors to compute. - **Output**: A tuple containing two 2D numpy arrays (left singular vectors and right singular vectors). 3. `def find_best_singular_vectors(U: np.ndarray, V: np.ndarray, num_best: int) -> Tuple[np.ndarray, np.ndarray]` - **Input**: - A 2D numpy array `U` representing the left singular vectors. - A 2D numpy array `V` representing the right singular vectors. - An integer `num_best` indicating the number of best singular vectors to select. - **Output**: A tuple containing two 2D numpy arrays (best left singular vectors and best right singular vectors). 4. `def cluster_rows_and_columns(A: np.ndarray, U: np.ndarray, V: np.ndarray, num_clusters: int) -> Tuple[np.ndarray, np.ndarray]` - **Input**: - A 2D numpy array `A` representing the data matrix. - A 2D numpy array `U` representing the best left singular vectors. - A 2D numpy array `V` representing the best right singular vectors. - An integer `num_clusters` indicating the number of clusters. - **Output**: A tuple containing two 1D numpy arrays (row labels and column labels). 5. `def jaccard_index(set1: Set[int], set2: Set[int]) -> float` - **Input**: - Two sets of integers `set1` and `set2` representing biclusters. - **Output**: A float representing the Jaccard index between the two sets. 6. `def evaluate_biclustering(true_biclusters: List[Tuple[Set[int], Set[int]]], found_biclusters: List[Tuple[Set[int], Set[int]]]) -> float` - **Input**: - A list of tuples `true_biclusters` where each tuple contains two sets of integers (true row and column biclusters). - A list of tuples `found_biclusters` where each tuple contains two sets of integers (found row and column biclusters). - **Output**: A float representing the consensus score for the biclustering. # Constraints - Assume the input data matrix contains only non-negative values and is not empty. - The number of clusters is a positive integer less than the minimum dimension of the matrix. - Ensure input data is properly validated and handle possible edge cases. # Example ```python import numpy as np # Example data matrix with a checkerboard structure A = np.array([[1, 2, 3, 4, 5, 6], [2, 1, 4, 3, 6, 5], [3, 4, 1, 2, 5, 6], [4, 3, 2, 1, 6, 5], [5, 6, 7, 8, 1, 2], [6, 5, 8, 7, 2, 1]]) # True biclusters (for evaluation purposes) true_biclusters = [({0, 1, 2, 3}, {0, 1, 2, 3}), ({4, 5}, {4, 5})] # Apply Spectral Biclustering A_bist = bistochastization(A) # Bistochastization U, V = compute_singular_vectors(A_bist, 4) # Compute singular vectors best_U, best_V = find_best_singular_vectors(U, V, 2) # Find best singular vectors row_labels, col_labels = cluster_rows_and_columns(A_bist, best_U, best_V, 2) # Cluster rows and columns # Found biclusters found_biclusters = [({0, 1, 2, 3}, {0, 1, 2, 3}), ({4, 5}, {4, 5})] # Evaluate score = evaluate_biclustering(true_biclusters, found_biclusters) print(f\\"Consensus Score: {score:.2f}\\") ``` In the example, the true biclusters represent the hidden checkerboard structure. Implement the provided functions and evaluate the Spectral Biclustering algorithm\'s performance against the true biclusters using the consensus score.","solution":"import numpy as np from scipy.linalg import svd from sklearn.cluster import KMeans from typing import Tuple, List, Set def bistochastization(A: np.ndarray, max_iter: int = 100, tol: float = 1e-6) -> np.ndarray: Normalize the matrix A to be bistochastic A = A / np.sum(A) for i in range(max_iter): A = A / A.sum(axis=1, keepdims=True) A = A / A.sum(axis=0, keepdims=True) if np.allclose(A.sum(axis=1), 1, atol=tol) and np.allclose(A.sum(axis=0), 1, atol=tol): break return A def compute_singular_vectors(A: np.ndarray, num_vectors: int) -> Tuple[np.ndarray, np.ndarray]: Compute singular vectors U, _, Vt = svd(A) return U[:, :num_vectors], Vt[:num_vectors, :] def find_best_singular_vectors(U: np.ndarray, V: np.ndarray, num_best: int) -> Tuple[np.ndarray, np.ndarray]: Select the best singular vectors return U[:, :num_best], V[:num_best, :] def cluster_rows_and_columns(A: np.ndarray, U: np.ndarray, V: np.ndarray, num_clusters: int) -> Tuple[np.ndarray, np.ndarray]: Cluster rows and columns row_clusters = KMeans(n_clusters=num_clusters).fit_predict(U) col_clusters = KMeans(n_clusters=num_clusters).fit_predict(V.T) return row_clusters, col_clusters def jaccard_index(set1: Set[int], set2: Set[int]) -> float: Compute Jaccard index return len(set1 & set2) / len(set1 | set2) def evaluate_biclustering(true_biclusters: List[Tuple[Set[int], Set[int]]], found_biclusters: List[Tuple[Set[int], Set[int]]]) -> float: Evaluate biclustering performance using Jaccard index scores = [] for true_row_set, true_col_set in true_biclusters: max_score = 0 for found_row_set, found_col_set in found_biclusters: score = (jaccard_index(true_row_set, found_row_set) + jaccard_index(true_col_set, found_col_set)) / 2 if score > max_score: max_score = score scores.append(max_score) return np.mean(scores)"},{"question":"# Seaborn Coding Assessment Question **Objective:** Demonstrate your understanding of seaborn by creating a detailed and customizable plot using the seaborn `objects` interface. **Instructions:** 1. **Dataset:** - Use the `penguins` dataset provided by seaborn (`seaborn.load_dataset(\\"penguins\\")`). 2. **Task:** - Create a faceted plot that shows the distribution of `flipper_length_mm` of penguins across different species and sex. - Incorporate the following elements: - A dot plot showing the mean flipper length for each group (i.e., species and sex). - Error bars showing the standard deviation of flipper length for each group. - A horizontal line plot to show the trend of the mean flipper length across species for each sex. - Customize the plot with different colors for each sex and different line styles for different species. - Provide meaningful labels and a title for the plot. 3. **Implementation Details:** - Utilize seaborn\'s `so.Plot` object and related methods (`so.Dot`, `so.Agg`, `so.Dodge`, `so.Line`, `so.Range`, `so.Facet`). - Ensure the plot is clean and well-annotated. # Expected Input and Output - **Input:** - No input other than the dataset loaded from seaborn. - **Output:** - A faceted plot with the specifications mentioned above. ```python # Your implementation here import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the specified requirements ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .facet(\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .add(so.Line(linestyle=\\"species\\"), so.Agg(), marker=\\"o\\") .label(x=\\"Species\\", y=\\"Flipper Length (mm)\\", title=\\"Flipper Length Distribution Across Penguin Species and Sex\\") ) ``` **Constraints:** - Ensure that the flipper length is plotted only for valid numerical data points (i.e., handle any missing data points appropriately). # Hints: - Use method-chaining provided by seaborn\'s `objects` interface for better readability and organization of your code. - Refer to seaborn\'s documentation on customizing plots for additional aesthetic adjustments and options.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the specified requirements plot = ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .facet(\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .add(so.Line(linestyle=\\"species\\"), so.Agg(), marker=\\"o\\") .label(x=\\"Species\\", y=\\"Flipper Length (mm)\\", title=\\"Flipper Length Distribution Across Penguin Species and Sex\\") ) return plot"},{"question":"# Custom Iterator Implementation Objective: Design custom iterators in Python that mimic the functionalities described in the provided documentation. You need to implement two iterator classes: `SequenceIterator` for generic sequences and `CallableIterator` for callable objects with a sentinel value. Your implementation should include all the necessary methods to support iteration. Class Specifications: 1. **SequenceIterator:** - **Initialization:** ```python def __init__(self, sequence): Initialize the SequenceIterator with a sequence. Parameters: sequence (Iterable): The sequence to iterate over. ``` - **Methods:** - `__iter__`: Should return the iterator object. - `__next__`: Should return the next value from the sequence. 2. **CallableIterator:** - **Initialization:** ```python def __init__(self, callable, sentinel): Initialize the CallableIterator with a callable object and a sentinel value. Parameters: callable (Callable): The callable object that generates the next item. sentinel: The value that terminates the iteration when returned by the callable. ``` - **Methods:** - `__iter__`: Should return the iterator object. - `__next__`: Should return the next value generated by the callable, stopping when the sentinel value is returned. Example Usage: 1. **SequenceIterator:** ```python seq_iter = SequenceIterator([1, 2, 3, 4]) for item in seq_iter: print(item) # Output: # 1 # 2 # 3 # 4 ``` 2. **CallableIterator:** ```python def counter(): i = 1 while True: yield i i += 1 call_iter = CallableIterator(counter, 4) for item in call_iter: print(item) # Output: # 1 # 2 # 3 # 4 ``` Constraints: 1. Do not use any built-in iterators or `yield` constructs in your `SequenceIterator` and `CallableIterator` implementations. 2. Ensure your iterators correctly handle the end of iteration. 3. Handle any potential errors gracefully. Submission: Submit your code for the `SequenceIterator` and `CallableIterator` classes.","solution":"class SequenceIterator: def __init__(self, sequence): Initialize the SequenceIterator with a sequence. Parameters: sequence (Iterable): The sequence to iterate over. self.sequence = sequence self.index = 0 def __iter__(self): Returns the iterator object. return self def __next__(self): Returns the next value from the sequence. if self.index < len(self.sequence): result = self.sequence[self.index] self.index += 1 return result else: raise StopIteration class CallableIterator: def __init__(self, callable, sentinel): Initialize the CallableIterator with a callable object and a sentinel value. Parameters: callable (Callable): The callable object that generates the next item. sentinel: The value that terminates the iteration when returned by the callable. self.callable = callable self.sentinel = sentinel self.iterator = iter(callable()) def __iter__(self): Returns the iterator object. return self def __next__(self): Returns the next value generated by the callable, stopping when the sentinel value is returned. value = next(self.iterator) if value == self.sentinel: raise StopIteration return value"},{"question":"Given a dataset representing the exam scores of students from different classes, your task is to visualize the data using seaborn. Specifically, you need to create plots that demonstrate different types of error bars, as mentioned in the seaborn documentation. Dataset: You are given a Pandas DataFrame `df`, which contains the following columns: - `class`: The class label (string). - `score`: The exam score (float). Example: ```python import pandas as pd data = { \\"class\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\"], \\"score\\": [78, 85, 90, 88, 92, 87] } df = pd.DataFrame(data) ``` Requirements: 1. **Standard Deviation Error Bars:** - Create a point plot showing the mean exam score for each class with standard deviation error bars. - Customize the plot to show error bars at +/- 2 standard deviations. 2. **Percentile Interval Error Bars:** - Create a point plot showing the median exam score for each class with percentile interval error bars. - Customize the plot to show the inter-quartile range (i.e., 25th to 75th percentiles). 3. **Standard Error and Confidence Interval Error Bars:** - Create a point plot showing the mean exam score for each class with standard error error bars. - Create another point plot showing the mean exam score for each class with a 90% confidence interval. 4. **Custom Error Bars:** - Create a point plot showing the mean exam score for each class with custom error bars representing the range from the minimum to the maximum score in the data for each class. 5. **Error Bars on Regression Fits:** - Create a scatter plot with a regression line showing the relationship between class (encoded as numerical values) and score, with a confidence interval. Constraints: - Use seaborn for all visualizations. - Each plot should be clearly labeled with appropriate titles and axes labels. - Ensure different plots are distinguishable (use different subplots or figures). Expected Output: Each plot should be saved as an image file (`.png` or `.jpg`). Input and Output Format: Function Signature: ```python def visualize_exam_scores(df: pd.DataFrame) -> None: # Implementation here pass ``` Example Output: ```plaintext The function will save the following plots as image files: 1. Standard Deviation Error Bars 2. Percentile Interval Error Bars 3. Standard Error Error Bars 4. Confidence Interval Error Bars 5. Custom Error Bars 6. Error Bars on Regression Fits ``` You can use the example dataset provided to test your implementation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_exam_scores(df: pd.DataFrame) -> None: # Standard Deviation Error Bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'class\', y=\'score\', data=df, ci=\'sd\', capsize=0.2, errwidth=2) plt.title(\'Mean Exam Scores with 2 Standard Deviation Error Bars\') plt.savefig(\'std_deviation_error_bars.png\') plt.clf() # Percentile Interval Error Bars (Inter-Quartile Range) plt.figure(figsize=(10, 6)) sns.pointplot(x=\'class\', y=\'score\', data=df, estimator=np.median, ci=50, capsize=0.2, errwidth=2) plt.title(\'Median Exam Scores with Inter-Quartile Range Error Bars\') plt.savefig(\'percentile_interval_error_bars.png\') plt.clf() # Standard Error and Confidence Interval Error Bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'class\', y=\'score\', data=df, capsize=0.2, errwidth=2) plt.title(\'Mean Exam Scores with Standard Error Error Bars\') plt.savefig(\'standard_error_error_bars.png\') plt.clf() plt.figure(figsize=(10, 6)) sns.pointplot(x=\'class\', y=\'score\', data=df, ci=90, capsize=0.2, errwidth=2) plt.title(\'Mean Exam Scores with 90% Confidence Interval Error Bars\') plt.savefig(\'confidence_interval_error_bars.png\') plt.clf() # Custom Error Bars with min and max range def min_error(values): return np.min(values) def max_error(values): return np.max(values) custom_ci = [] for cls in df[\'class\'].unique(): class_scores = df[df[\'class\'] == cls][\'score\'] min_score = min_error(class_scores) max_score = max_error(class_scores) custom_ci.append((max_score-min_score)) plt.figure(figsize=(10, 6)) sns.pointplot(x=\'class\', y=\'score\', data=df, capsize=0.2, ci=None, errwidth=2, join=False) for i in range(len(df[\'class\'].unique())): plt.errorbar(i, df[df[\'class\'] == df[\'class\'].unique()[i]][\'score\'].mean(), yerr=[[custom_ci[i]/2], [custom_ci[i]/2]], fmt=\'o\', color=\'b\', ecolor=\'r\', capsize=5) plt.title(\'Mean Exam Scores with Custom Error Bars\') plt.savefig(\'custom_error_bars.png\') plt.clf() # Error Bars on Regression Fits df[\'class_num\'] = df[\'class\'].apply(lambda x: ord(x) - ord(\'A\') + 1) plt.figure(figsize=(10, 6)) sns.regplot(x=\'class_num\', y=\'score\', data=df, ci=95) plt.title(\'Regression Fit with Confidence Interval\') plt.xlabel(\'Class (encoded as numerical value)\') plt.ylabel(\'Score\') plt.savefig(\'regression_error_bars.png\') plt.clf()"},{"question":"Kernel Density Estimation with Different Kernels Objective: The goal of this assessment is to evaluate your understanding and practical skills in using scikit-learn\'s Kernel Density Estimation (KDE) for density estimation and visualization. Problem Statement: 1. **Dataset Generation**: Write a function `generate_bimodal_data` that generates a bimodal distribution with 150 data points. Use the following distribution parameters: - Mean1: -2, Standard Deviation1: 0.5, with 75 samples. - Mean2: 2, Standard Deviation2: 0.5, with 75 samples. 2. **Kernel Density Estimation**: Write a function `perform_kde` that takes the generated dataset and computes the KDE using the following kernels: - Gaussian - Tophat - Epanechnikov 3. **Visualization**: Write a function `plot_kde` to visualize the original dataset along with KDEs computed using each of the kernels. The plot should include: - The original data histogram. - The KDE plots over the histogram for each kernel type. Expected Functions: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_data(): Generate a bimodal distribution with given parameters. Returns: np.array: An array of shape (150,) representing the bimodal data. # Generate the two distributions mean1, std1, size1 = -2, 0.5, 75 mean2, std2, size2 = 2, 0.5, 75 data1 = np.random.normal(mean1, std1, size1) data2 = np.random.normal(mean2, std2, size2) data = np.concatenate([data1, data2]) return data def perform_kde(data): Perform Kernel Density Estimation using different kernels. Args: data (np.array): The input data for KDE. Returns: dict: A dictionary containing the KDE objects for each kernel. # Define the bandwidth bandwidth = 0.2 # Perform KDE for each specified kernel kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] kde_dict = {} for kernel in kernels: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data[:, None]) kde_dict[kernel] = kde return kde_dict def plot_kde(data, kde_dict): Plot the KDE results. Args: data (np.array): The input data. kde_dict (dict): A dictionary containing the KDE objects for each kernel. X_plot = np.linspace(-5, 5, 1000)[:, None] log_dens = {} for kernel in kde_dict: log_dens[kernel] = kde_dict[kernel].score_samples(X_plot) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Histogram\') for kernel, log_density in log_dens.items(): plt.plot(X_plot[:, 0], np.exp(log_density), label=f\'KDE with {kernel} kernel\') plt.legend() plt.title(\'KDE with Different Kernels\') plt.xlabel(\'Data\') plt.ylabel(\'Density\') plt.show() ``` Constraints: - The bandwidth for KDE should be set to 0.2. - Feel free to use any plotting library you\'re comfortable with. - Ensure your code is modular, with clear separation of the dataset generation, KDE computation, and plotting functions. Submission: Submit the three functions (`generate_bimodal_data`, `perform_kde`, and `plot_kde`) along with a script to call and visualize the KDEs as specified. Ensure your code is well-documented and tested with proper examples.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_data(): Generate a bimodal distribution with given parameters. Returns: np.array: An array of shape (150,) representing the bimodal data. # Generate the two distributions mean1, std1, size1 = -2, 0.5, 75 mean2, std2, size2 = 2, 0.5, 75 data1 = np.random.normal(mean1, std1, size1) data2 = np.random.normal(mean2, std2, size2) data = np.concatenate([data1, data2]) return data def perform_kde(data): Perform Kernel Density Estimation using different kernels. Args: data (np.array): The input data for KDE. Returns: dict: A dictionary containing the KDE objects for each kernel. # Define the bandwidth bandwidth = 0.2 # Perform KDE for each specified kernel kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] kde_dict = {} for kernel in kernels: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data[:, None]) kde_dict[kernel] = kde return kde_dict def plot_kde(data, kde_dict): Plot the KDE results. Args: data (np.array): The input data. kde_dict (dict): A dictionary containing the KDE objects for each kernel. X_plot = np.linspace(-5, 5, 1000)[:, None] log_dens = {} for kernel in kde_dict: log_dens[kernel] = kde_dict[kernel].score_samples(X_plot) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Histogram\') for kernel, log_density in log_dens.items(): plt.plot(X_plot[:, 0], np.exp(log_density), label=f\'KDE with {kernel} kernel\') plt.legend() plt.title(\'KDE with Different Kernels\') plt.xlabel(\'Data\') plt.ylabel(\'Density\') plt.show()"},{"question":"# Asynchronous Logging System You are tasked with creating an asynchronous logging system that can log messages from different parts of an application concurrently. This system should consist of a logger that writes to a file asynchronously and a main function which generates multiple logs concurrently. Requirements 1. Write an asynchronous coroutine `write_log` that writes a log message to a specified file with a delay. 2. Write an asynchronous coroutine `generate_logs`: - This coroutine should generate multiple log messages and use `await` to ensure they are written concurrently. - Ensure that invoking `generate_logs` does not block the event loop unnecessarily. 3. Use `asyncio.gather()` to ensure all log messages are written before the program completes. 4. Implement error handling to manage any exceptions that may occur during the writing of log messages. Function Signatures ```python import asyncio async def write_log(message: str, filename: str, delay: float) -> None: Writes a log message to the specified file after a delay. :param message: Log message to write. :param filename: File to write the log message to. :param delay: Time in seconds to wait before writing the log message. pass async def generate_logs(log_messages: list, filename: str) -> None: Generates multiple log messages concurrently. :param log_messages: List of tuples where each tuple contains a message (str) and a delay (float). :param filename: File to write the log messages to. pass async def main(): Main entry point for the logging system. pass # Sample Usage: # asyncio.run(main()) ``` # Example Usage and Constraints * Example of `log_messages` input: `[(\\"Message 1\\", 1), (\\"Message 2\\", 2), (\\"Message 3\\", 3)]` * Ensure that `generate_logs` starts all log writing coroutines concurrently. * Assume each log message is written to a file called `application.log`. * Handle I/O operations efficiently to avoid blocking. Example Execution ```python # Given the following example input log_messages = [(\\"Message 1\\", 1), (\\"Message 2\\", 0.5), (\\"Message 3\\", 1.5)] # Expected output in application.log after execution # The content of application.log: # Message 1 # Message 2 # Message 3 ``` This task will test students\' understanding of asyncio coroutines, task creation, concurrent execution, and proper handling of asynchronous I/O operations and exceptions.","solution":"import asyncio async def write_log(message: str, filename: str, delay: float) -> None: Writes a log message to the specified file after a delay. :param message: Log message to write. :param filename: File to write the log message to. :param delay: Time in seconds to wait before writing the log message. await asyncio.sleep(delay) async with aiofiles.open(filename, \'a\') as file: await file.write(message + \'n\') async def generate_logs(log_messages: list, filename: str) -> None: Generates multiple log messages concurrently. :param log_messages: List of tuples where each tuple contains a message (str) and a delay (float). :param filename: File to write the log messages to. tasks = [write_log(message, filename, delay) for message, delay in log_messages] await asyncio.gather(*tasks) async def main(): Main entry point for the logging system. log_messages = [(\\"Message 1\\", 1), (\\"Message 2\\", 0.5), (\\"Message 3\\", 1.5)] await generate_logs(log_messages, \'application.log\') # Sample Usage: # asyncio.run(main())"},{"question":"**Pandas Text Data Manipulation Challenge** You are given a DataFrame containing textual data in several columns. Your task is to perform a series of transformations to clean and process this data. Follow the instructions below and implement the specified functions. # Input A DataFrame `df` with the following structure: ```python import pandas as pd import numpy as np data = { \\"Name\\": [\\" James \\", \\"linda\\", \\" Max \\", \\"Alice \\", np.nan, \\" Bob\\", \\" Ilona\\"], \\"Email\\": [\\"james@domain.com\\", \\"linda@domain.co.uk\\", \\"max@domain.com\\", \\"alice@domain.com\\", np.nan, \\"bob@domain.com\\", \\"ilona@domain.com\\"], \\"Code\\": [\\"A12345\\", \\"B23456\\", None, \\"C34567\\", \\"D45678\\", \\"E56789\\", \\"F67890\\"] } df = pd.DataFrame(data) ``` # Output A cleaned DataFrame with the following transformations applied: 1. **Trim** all leading and trailing whitespaces in the `Name` column and convert it to title case. 2. Ensure all email addresses in the `Email` column are in lower case. If any entry in the `Email` column is missing (NA), replace it with `\\"missing@domain.com\\"`. 3. Extract the numeric part from the `Code` column into a new column named `Code_Num`. If an entry in the `Code` column is missing (NA), fill it with `-1`. **Function Signature** ```python def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass ``` # Constraints - The function should handle missing (NA) values appropriately as described in the output requirements. - All transformations should be done using pandas functionalities. # Example **Input:** ```python import pandas as pd import numpy as np data = { \\"Name\\": [\\" James \\", \\"linda\\", \\" Max \\", \\"Alice \\", np.nan, \\" Bob\\", \\" Ilona\\"], \\"Email\\": [\\"james@domain.com\\", \\"linda@domain.co.uk\\", \\"max@domain.com\\", \\"alice@domain.com\\", np.nan, \\"bob@domain.com\\", \\"ilona@domain.com\\"], \\"Code\\": [\\"A12345\\", \\"B23456\\", None, \\"C34567\\", \\"D45678\\", \\"E56789\\", \\"F67890\\"] } df = pd.DataFrame(data) ``` **Output:** ```python import pandas as pd data = { \\"Name\\": [\\"James\\", \\"Linda\\", \\"Max\\", \\"Alice\\", np.nan, \\"Bob\\", \\"Ilona\\"], \\"Email\\": [\\"james@domain.com\\", \\"linda@domain.co.uk\\", \\"max@domain.com\\", \\"alice@domain.com\\", \\"missing@domain.com\\", \\"bob@domain.com\\", \\"ilona@domain.com\\"], \\"Code\\": [\\"A12345\\", \\"B23456\\", None, \\"C34567\\", \\"D45678\\", \\"E56789\\", \\"F67890\\"], \\"Code_Num\\": [12345, 23456, -1, 34567, 45678, 56789, 67890] } expected_output = pd.DataFrame(data) ``` **Implementation:** ```python def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: df[\'Name\'] = df[\'Name\'].str.strip().str.title() df[\'Email\'] = df[\'Email\'].str.lower().fillna(\\"missing@domain.com\\") df[\'Code_Num\'] = df[\'Code\'].str.extract(r\'(d+)\', expand=False).fillna(-1).astype(int) return df ```","solution":"import pandas as pd import numpy as np def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Trim whitespaces and convert Name to title case df[\'Name\'] = df[\'Name\'].str.strip().str.title() # Convert Email to lower case and fill missing entries with \\"missing@domain.com\\" df[\'Email\'] = df[\'Email\'].str.lower().fillna(\\"missing@domain.com\\") # Extract numeric part from Code and fill missing entries with -1 df[\'Code_Num\'] = df[\'Code\'].str.extract(r\'(d+)\', expand=False).fillna(-1).astype(int) return df"},{"question":"Implementing a Chat Server using `asynchat` Problem Statement You are tasked with implementing a simple chat server using the `asynchat` module. This chat server should handle multiple client connections asynchronously, allowing clients to send and receive messages. The server should follow these specifications: 1. **Client Connections**: The server should handle multiple clients concurrently. 2. **Message Protocol**: - Messages from clients are terminated by a newline character (`n`). - Each message can be of variable length but must be complete (terminator included) before processing. 3. **Data Management**: - Implement input buffer management to accumulate message data until the terminator is found. - Upon finding a terminator, broadcast the complete message to all connected clients. 4. **Functional Requirements**: - Implement the `collect_incoming_data()` method to buffer the incoming data. - Implement the `found_terminator()` method to process and broadcast the message to all clients. - Ensure that each client receives all broadcasted messages. Implementation Details 1. **Class Definition**: Subclass `asynchat.async_chat` to create `ChatHandler`. 2. **Methods to Implement**: - `collect_incoming_data(self, data)`: Buffer the incoming data. - `found_terminator(self)`: Process the complete message upon finding the terminator and broadcast it. ```python import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] self.clients = clients self.clients.append(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Process the complete message and broadcast it message = b\'\'.join(self.ibuffer) self.ibuffer = [] self.broadcast(message) def broadcast(self, data): Broadcast the message to all clients for client in self.clients: if client != self: client.push(data) def handle_close(self): self.clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() ChatHandler(sock, self.clients) if __name__ == \'__main__\': server = ChatServer(\'localhost\', 12345) asyncore.loop() ``` Input - The server will run and await connections from clients on the specified host and port. Output - Clients connected to the server can send and receive messages asynchronously. Constraints - Do not use external packages for asynchronous communication other than `asynchat` and `asyncore`. Performance Requirements - Ensure efficient buffer management and avoid unnecessary memory usage while handling multiple clients. Write the code to implement the `ChatHandler` class and the `ChatServer` class as described above.","solution":"import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] self.clients = clients self.clients.append(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Process the complete message and broadcast it message = b\'\'.join(self.ibuffer) self.ibuffer = [] self.broadcast(message) def broadcast(self, data): Broadcast the message to all clients for client in self.clients: if client != self: client.push(data + b\'n\') def handle_close(self): self.clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() ChatHandler(sock, self.clients) if __name__ == \'__main__\': server = ChatServer(\'localhost\', 12345) asyncore.loop()"},{"question":"**Objective:** Assess your ability to use seaborn to create detailed and informative visualizations. **Task:** You are provided with the `tips` dataset from seaborn, representing data about tips in a restaurant. 1. Load the `tips` dataset using `seaborn.load_dataset`. 2. Create a plot using `seaborn.objects` (`so.Plot`), visualizing the relationship between `total_bill` and `tip`. 3. Add a layer with `so.Dot()` to plot the data points, making sure to adjust the edge color to white (`w`). 4. Use dodging and jittering to reduce overplotting, allocating color by `sex`. 5. Customize the plot by modifying the `pointsize` to 6 and changing the marker style based on the `smoker` variable (use circles `o` for non-smokers and squares `s` for smokers). 6. Ensure the colors used are from the \\"flare\\" palette. 7. Include error bars to represent the standard error of the mean (using `so.Range` and `so.Est(errorbar=(\'se\', 2))`). 8. Save the final plot as `tips_plot.png`. **Constraints:** - Ensure the plot is clear and readable. - The final visualization should be informative and aesthetically pleasing. - Pay attention to the alignment and overlapping issues. **Input Format:** - None. All necessary data is provided. **Output Format:** - Save the plot as `tips_plot.png`. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Step 1: Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Create a plot with total_bill vs. tip plot = so.Plot(tips, \\"total_bill\\", \\"tip\\") # Step 3: Add a Dot layer with adjusted edge color plot.add(so.Dot(edgecolor=\\"w\\")) # Step 4: Use dodging and jittering, coloring by sex plot.add(so.Dot(), so.Dodge(), so.Jitter(.2), color=\\"sex\\") # Step 5: Customize pointsize and marker style based on smoker plot.add(so.Dot(pointsize=6), color=\\"sex\\", marker=\\"smoker\\") plot.scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") # Step 6: Include error bars plot.add(so.Range(), so.Est(errorbar=(\'se\', 2))) # Saving the plot plot.save(\\"tips_plot.png\\") ``` Please ensure that the markers and other visual elements maintain clarity and distinction between the categories.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_tips_plot(filepath=\\"tips_plot.png\\"): # Step 1: Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Create a plot with total_bill vs. tip plot = so.Plot(tips, \\"total_bill\\", \\"tip\\") # Step 3: Add a Dot layer with adjusted edge color plot.add(so.Dot(edgecolor=\'w\')) # Step 4: Use dodging and jittering, coloring by sex plot.add(so.Dot(), so.Dodge(), so.Jitter(.2), color=\\"sex\\") # Step 5: Customize pointsize and marker style based on smoker plot.add(so.Dot(pointsize=6), color=\\"sex\\", marker=\\"smoker\\") plot.scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") # Step 6: Include error bars plot.add(so.Range(), so.Est(errorbar=(\'se\', 2))) # Saving the plot plot.save(filepath)"},{"question":"# **Coding Assessment Question** **Objective:** You are tasked with implementing a function that processes a text file and a binary file using the `io` module in Python. The function should read data from both files, perform some operations, and write specific results to new files. **Problem Statement:** Implement a function `process_files(text_file_path, binary_file_path, text_output_path, binary_output_path)` that performs the following operations: 1. **Text File Operations:** - Open the text file from `text_file_path` for reading with UTF-8 encoding. - Count the number of words in the text file. A word is defined as any sequence of characters separated by whitespace. - Write the word count to a new text file specified by `text_output_path`. 2. **Binary File Operations:** - Open the binary file from `binary_file_path` for reading. - Compute the checksum of the binary file. The checksum is defined as the sum of all byte values. - Write the checksum (as an integer) to a new binary file specified by `binary_output_path`. **Function Signature:** ```python def process_files(text_file_path: str, binary_file_path: str, text_output_path: str, binary_output_path: str) -> None: pass ``` **Input Format:** - `text_file_path`: a string representing the path to the input text file. - `binary_file_path`: a string representing the path to the input binary file. - `text_output_path`: a string representing the path to the output text file where the word count will be written. - `binary_output_path`: a string representing the path to the output binary file where the checksum will be written. **Output Format:** - The function does not return any value. It writes results to specified output files. **Constraints:** - You may assume the input files exist and are accessible. - The text file will contain plain text with UTF-8 encoding. - The binary file can contain any type of binary data. **Example:** ```python # Example usage: process_files(\\"input.txt\\", \\"input.bin\\", \\"word_count.txt\\", \\"checksum.bin\\") # Assume \\"input.txt\\" contains: \\"Hello worldnThis is an example.\\" # Then \\"word_count.txt\\" should contain: \\"6\\" # Assume \\"input.bin\\" contains: b\'x01x02x03x04\' # Then \\"checksum.bin\\" should contain the binary representation of the checksum: 10 (0x0A) ``` **Notes:** - Pay attention to the performance of your implementation, especially when handling large files. - Ensure proper error handling where necessary.","solution":"import io def process_files(text_file_path: str, binary_file_path: str, text_output_path: str, binary_output_path: str) -> None: # Text File Operations with io.open(text_file_path, \'r\', encoding=\'utf-8\') as text_file: text_data = text_file.read() word_count = len(text_data.split()) with io.open(text_output_path, \'w\', encoding=\'utf-8\') as out_text_file: out_text_file.write(str(word_count)) # Binary File Operations with io.open(binary_file_path, \'rb\') as binary_file: binary_data = binary_file.read() checksum = sum(binary_data) with io.open(binary_output_path, \'wb\') as out_binary_file: out_binary_file.write(checksum.to_bytes((checksum.bit_length() + 7) // 8, byteorder=\'big\'))"},{"question":"Objective: Implement a function with type hints, and write corresponding unit tests using the `unittest` framework to ensure your implementation is correct. Part 1: Function Implementation with Type Hints Write a function `merge_sorted_lists` that merges two sorted lists of integers into a single sorted list of integers. Use type hints to indicate the input and output types clearly. **Function Signature:** ```python from typing import List def merge_sorted_lists(list1: List[int], list2: List[int]) -> List[int]: # Your code here ``` **Input:** - `list1`: A list of integers sorted in non-decreasing order. - `list2`: A list of integers sorted in non-decreasing order. **Output:** - A list of integers that contains all elements from `list1` and `list2`, sorted in non-decreasing order. **Constraints:** - The function should handle lists of any length, including empty lists. - The solution should maintain the performance suitable for large lists, ideally O(n) where `n` is the combined length of the two lists. Part 2: Unit Tests Write unit tests for your `merge_sorted_lists` function using the `unittest` framework. Ensure that your tests cover various scenarios, including but not limited to: - Both input lists are empty. - One input list is empty, and the other is non-empty. - Both input lists contain elements. - The input lists have overlapping values. Create a new Python file named `test_merge_sorted_lists.py` and include the following: **Example:** ```python import unittest from typing import List from your_module import merge_sorted_lists # Adjust this import to match your file structure class TestMergeSortedLists(unittest.TestCase): def test_both_empty(self): self.assertEqual(merge_sorted_lists([], []), []) def test_first_list_empty(self): self.assertEqual(merge_sorted_lists([], [1, 3, 5]), [1, 3, 5]) def test_second_list_empty(self): self.assertEqual(merge_sorted_lists([2, 4, 6], []), [2, 4, 6]) def test_non_empty_lists(self): self.assertEqual(merge_sorted_lists([1, 3, 5], [2, 4, 6]), [1, 2, 3, 4, 5, 6]) def test_overlapping_values(self): self.assertEqual(merge_sorted_lists([1, 2, 3], [2, 3, 4]), [1, 2, 2, 3, 3, 4]) if __name__ == \\"__main__\\": unittest.main() ``` **Submission:** - The function implementation file containing the `merge_sorted_lists` function. - The test file containing your unit tests for the `merge_sorted_lists` function.","solution":"from typing import List def merge_sorted_lists(list1: List[int], list2: List[int]) -> List[int]: # Initialize pointers for list1 and list2 i, j = 0, 0 merged_list = [] # Merge the lists until one of them is exhausted while i < len(list1) and j < len(list2): if list1[i] <= list2[j]: merged_list.append(list1[i]) i += 1 else: merged_list.append(list2[j]) j += 1 # Append remaining elements from list1, if any if i < len(list1): merged_list.extend(list1[i:]) # Append remaining elements from list2, if any if j < len(list2): merged_list.extend(list2[j:]) return merged_list"},{"question":"# Advanced File Descriptor Manipulation with `fcntl` **Objective**: Implement a function that demonstrates your understanding of file control operations using the `fcntl` module. Task: You are required to implement a function `set_file_descriptor_properties` that takes a file path as input, opens the file, and modifies its properties using `fcntl`. Specifically, your function should: 1. Open the file in read-write mode. 2. Set the file to non-blocking mode using the `F_SETFL` command and `os.O_NONBLOCK` flag. 3. Create a read and write lock on the file using `lockf` to ensure exclusive access. 4. Handle any possible exceptions, primarily `OSError`. 5. Close the file properly after all operations. Expected Function Signature: ```python import fcntl import os def set_file_descriptor_properties(file_path: str): This function opens the file at file_path, sets it to non-blocking mode, and locks the file for exclusive access. Args: - file_path (str): The path to the file to be operated on. Raises: - OSError: If any system calls fail. pass ``` Constraints: - Do not exceed the use of 1024 bytes for any argument passed as bytes to the `fcntl` functions. - The function should be robust against any `OSError` and should provide an informative error message if caught. - Assume the file path provided is valid, and the file exists. Sample Usage: ```python try: set_file_descriptor_properties(\\"/path/to/your/file.txt\\") print(\\"File properties set successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` Notes: - You may want to refer to the `fcntl` and `os` module documentation for constants such as `os.O_NONBLOCK`. - Ensure that you test your code under various scenarios to verify its correctness and robustness.","solution":"import fcntl import os def set_file_descriptor_properties(file_path: str): This function opens the file at file_path, sets it to non-blocking mode, and locks the file for exclusive access. Args: - file_path (str): The path to the file to be operated on. Raises: - OSError: If any system calls fail. try: # Open the file in read-write mode with open(file_path, \'r+\') as file: fd = file.fileno() # Get the current file descriptor flags flags = fcntl.fcntl(fd, fcntl.F_GETFL) # Set the file descriptor to non-blocking mode fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) # Lock the file for exclusive access fcntl.lockf(fd, fcntl.LOCK_EX) # Simulating some operations - can be replaced with actual operations if needed file.write(\\"Locked and non-blocking mode set.\\") # Unlock the file when done fcntl.lockf(fd, fcntl.LOCK_UN) except OSError as e: print(f\\"An error occurred: {e}\\") raise"},{"question":"Objective: Write a Python function to identify and organize files within a specific directory based on their extensions (e.g., `.txt`, `.py`, `.jpg`). Your function should read the files, group them by their extensions, and move them into corresponding subdirectories. If a subdirectory for a particular extension does not exist, create it. Function Specification: # Function Name: `organize_files_by_extension` # Input: - `directory`: (string) Path to the directory to be organized. The directory may contain multiple files and subdirectories. # Output: - The function does not return anything. It should perform the organizing task within the specified directory. Constraints: - The function should only move files, not directories. - The function should handle both relative and absolute paths. - Names of the subdirectories should match the extensions of the files (e.g., `txt` for `.txt`, `py` for `.py`). - The function should not fail if executed multiple times on the same directory. - The function should not follow symbolic links. Example: Consider a directory structure as follows: ``` /example_directory/ - document.txt - report.pdf - image.jpg - script.py - archive.zip ``` After running `organize_files_by_extension(\\"/example_directory\\")`, the directory should have the following structure: ``` /example_directory/ /txt/ - document.txt /pdf/ - report.pdf /jpg/ - image.jpg /py/ - script.py /zip/ - archive.zip ``` Notes: - Make use of the `os.path` module to handle path manipulations. - Make sure to handle cases where directory and file names might have different casing (e.g., `File.TXT` should go into `txt` subdirectory). - Utilize functions like `os.path.splitext()`, `os.path.isdir()`, `os.path.isfile()`, `os.path.exists()`, and `os.makedirs()` to implement your solution. To start, import necessary modules as follows: ```python import os import shutil ``` # Solution Template: ```python import os import shutil def organize_files_by_extension(directory): # Implement your solution here pass # Example usage: # organize_files_by_extension(\\"/path/to/your/directory\\") ```","solution":"import os import shutil def organize_files_by_extension(directory): if not os.path.isdir(directory): raise ValueError(f\\"The provided path \'{directory}\' is not a valid directory.\\") for item in os.listdir(directory): item_path = os.path.join(directory, item) if os.path.isfile(item_path): file_extension = os.path.splitext(item_path)[1][1:].lower() if file_extension: target_folder = os.path.join(directory, file_extension) if not os.path.exists(target_folder): os.makedirs(target_folder) shutil.move(item_path, os.path.join(target_folder, item)) # Example usage: # organize_files_by_extension(\\"/path/to/your/directory\\")"},{"question":"**Title: Sorting and Filtering Integers with Exception Handling and Generators** **Objective:** Your task is to implement a Python function that takes a list of integers and performs several operations using the simple statements outlined in the provided documentation. The function should do the following: 1. Sort the list in ascending order. 2. Compute the sum of the sorted list. 3. Generate a new list consisting of only even numbers from the sorted list and handle any exceptions. 4. Use an augmented assignment to double each even number. 5. Make use of `yield` to return each even number doubled, one at a time. 6. Use assertion to check that the input list is not empty. **Function Signature:** ```python def process_integers(integers: list) -> None: pass ``` **Detailed Requirements:** 1. **Input:** - A list of integers. - Example: `[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]` 2. **Output:** - No return value. The function should print the results at various stages as specified below using expression statements. 3. **Steps to Implement:** - **Sort the List:** Use an assignment statement to sort the list (`sorted_list`). ```python sorted_list = sorted(integers) print(f\\"Sorted List: {sorted_list}\\") ``` - **Sum of the List:** Calculate and print the sum of the sorted list. ```python total_sum = sum(sorted_list) print(f\\"Sum of Sorted List: {total_sum}\\") ``` - **Filter Even Numbers:** Use a try-except block to handle any potential errors when filtering. ```python try: even_numbers = [num for num in sorted_list if num % 2 == 0] print(f\\"Even Numbers: {even_numbers}\\") except Exception as e: raise ValueError(\\"Error while filtering even numbers.\\") from e ``` - **Double Even Numbers (In-Place):** Use an augmented assignment statement. ```python for i in range(len(even_numbers)): even_numbers[i] *= 2 print(f\\"Doubled Even Numbers: {even_numbers}\\") ``` - **Yield Each Doubled Even Number:** Implement this using a generator. ```python def doubled_even_generator(numbers): for num in numbers: yield num ``` Use the generator to print yielded values: ```python print(\\"Yielded Doubled Even Numbers:\\") for num in doubled_even_generator(even_numbers): print(num) ``` - **Assert Statement:** Ensure the input list is not empty. ```python assert len(integers) > 0, \\"The input list must not be empty.\\" ``` **Constraints:** - The input list will contain only integers. - Handle any possible exceptions while filtering even numbers. **Example:** ```python process_integers([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]) ``` Expected Output: ``` Sorted List: [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9] Sum of Sorted List: 44 Even Numbers: [2, 4, 6] Doubled Even Numbers: [4, 8, 12] Yielded Doubled Even Numbers: 4 8 12 ``` **Note:** Make sure to handle the cases where the input list might cause an error when filtering even numbers or might be empty. Use appropriate exception handling and assertion to ensure robustness.","solution":"def process_integers(integers): This function processes a list of integers by performing several operations: 1. Sorts the list. 2. Computes the sum of the sorted list. 3. Filters out even numbers. 4. Doubles each even number. 5. Yields each doubled even number. 6. Asserts that the input list is not empty. Args: integers (list of int): The list of integers to process. # Ensure the input list is not empty assert len(integers) > 0, \\"The input list must not be empty.\\" # Sort the list sorted_list = sorted(integers) print(f\\"Sorted List: {sorted_list}\\") # Compute the sum of the sorted list total_sum = sum(sorted_list) print(f\\"Sum of Sorted List: {total_sum}\\") # Filter the even numbers and handle potential errors try: even_numbers = [num for num in sorted_list if num % 2 == 0] print(f\\"Even Numbers: {even_numbers}\\") except Exception as e: raise ValueError(\\"Error while filtering even numbers.\\") from e # Double each even number for i in range(len(even_numbers)): even_numbers[i] *= 2 print(f\\"Doubled Even Numbers: {even_numbers}\\") # Yield each doubled even number one at a time def doubled_even_generator(numbers): for num in numbers: yield num print(\\"Yielded Doubled Even Numbers:\\") for num in doubled_even_generator(even_numbers): print(num)"},{"question":"Coding Assessment Question # Task Implement a function that applies Linear Discriminant Analysis (LDA) for classification on a given dataset, reduces the dimensionality if specified, and evaluates the model\'s performance. # Function Signature ```python def lda_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, n_components: int = None) -> float: Applies LDA classification on the given training data, performs dimensionality reduction if n_components is specified, and evaluates the model on the test data. Parameters: - X_train : np.ndarray : Training feature data - y_train : np.ndarray : Training target labels - X_test : np.ndarray : Test feature data - y_test : np.ndarray : Test target labels - n_components : int, optional : Number of components for dimensionality reduction using LDA (default is None) Returns: - float : Accuracy of the LDA model on the test data ``` # Input Format - `X_train` (numpy.ndarray): A 2D numpy array with shape (num_samples, num_features) representing the training input data. - `y_train` (numpy.ndarray): A 1D numpy array with shape (num_samples,) representing the training labels. - `X_test` (numpy.ndarray): A 2D numpy array with shape (num_samples, num_features) representing the testing input data. - `y_test` (numpy.ndarray): A 1D numpy array with shape (num_samples,) representing the testing labels. - `n_components` (int, optional): Number of components for dimensionality reduction (optional, default is None). # Output Format - The function should return a single float, representing the classification accuracy of the LDA model on the test data. # Constraints - `X_train` and `X_test` must not be empty and feature dimensions must be aligned. - `n_components`, if provided, must be less than the number of classes in y_train. - The function should handle exceptions related to dimensionality reduction and ensure proper error messaging. # Example ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import numpy as np # Load dataset data = load_iris() X = data.data y = data.target # Split dataset into training and testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Function call accuracy = lda_classification(X_train, y_train, X_test, y_test, n_components=2) print(f\\"Accuracy: {accuracy:.2f}\\") ``` # Assessment Criteria - Correct implementation of LDA for classification. - Correct handling of dimensionality reduction with `n_components`. - Effective evaluation of model performance on test data. - Proper handling of input edge cases and exceptions. - Clear and concise code with appropriate comments.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import accuracy_score def lda_classification(X_train, y_train, X_test, y_test, n_components=None): Applies LDA classification on the given training data, performs dimensionality reduction if n_components is specified, and evaluates the model on the test data. Parameters: - X_train : np.ndarray : Training feature data - y_train : np.ndarray : Training target labels - X_test : np.ndarray : Test feature data - y_test : np.ndarray : Test target labels - n_components : int, optional : Number of components for dimensionality reduction using LDA (default is None) Returns: - float : Accuracy of the LDA model on the test data lda = LinearDiscriminantAnalysis(n_components=n_components) if n_components is not None: # Fit the LDA model for dimensionality reduction X_train = lda.fit_transform(X_train, y_train) X_test = lda.transform(X_test) # Refit LDA for classification lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred = lda.predict(X_test) return accuracy_score(y_test, y_pred)"},{"question":"**Objective**: Write a function to analyze and modify an exported computational graph in Export IR format. **Problem Statement**: You are given an `ExportedProgram` object from the `torch.export` module. Your task is to write a function `modify_exported_program` that performs the following operations: 1. Traverse the computational graph and find all `call_function` nodes. 2. For each `call_function` node, change any `torch.ops.aten.add.Tensor` operation to `torch.ops.aten.mul.Tensor`. 3. Preserve the original structure and metadata but apply this modification to change the addition operations to multiplication operations. The function signature is as follows: ```python from torch.export import ExportedProgram def modify_exported_program(ep: ExportedProgram) -> ExportedProgram: # Your code here pass ``` # Input: - `ep`: An instance of `ExportedProgram` containing a computational graph. # Output: - Return a modified instance of `ExportedProgram` with the specified changes. # Constraints: - You should preserve all other attributes and metadata of the graph nodes. - The graph must remain a valid Export IR graph after modification. - Validate that the initial operation is of type `torch.ops.aten.add.Tensor` before modification. # Example: ```python # Assume `ep` is an already created ExportedProgram instance with an addition operation modified_ep = modify_exported_program(ep) print(modified_ep.graph) # The output should show that any operation using `torch.ops.aten.add.Tensor` has been replaced with `torch.ops.aten.mul.Tensor`. ``` # Notes: - You may refer to `torch.fx` and `torch.export` documentation for methods to traverse and manipulate the graph. - Ensure that the modified graph maintains its integrity and operational semantics.","solution":"from torch.export import ExportedProgram import torch def modify_exported_program(ep: ExportedProgram) -> ExportedProgram: # Traverse the graph nodes for node in ep.graph.nodes: # Check if the node is a call_function node if node.op == \'call_function\': # Check if the function is torch.ops.aten.add.Tensor if node.target == torch.ops.aten.add.Tensor: # Change the function to torch.ops.aten.mul.Tensor node.target = torch.ops.aten.mul.Tensor return ep"},{"question":"**Question: Using Seaborn for Data Visualization** You are provided with a Titanic dataset. Your task is to create a function called `plot_titanic_data` that generates two specific visualizations using Seaborn. The visualizations should provide insights into different aspects of the dataset. # Function Definition: ```python def plot_titanic_data(): # Implement your code here ``` # Requirements: 1. **Dataset Loading and Setup:** - Import Seaborn and set the theme to \\"whitegrid\\". - Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 2. **Plot 1 - Basic Count Plot:** - Create a count plot for the `class` variable using Seaborn\'s `countplot` function. - Display this plot inline. 3. **Plot 2 - Grouped and Normalized Count Plot:** - Create a count plot for the `class` variable grouped by the `survived` variable. - Normalize the data to show the percentages instead of absolute counts. - Display this plot inline. # Constraints: - The function must not take any input parameters. - Use Seaborn for all visualizations. - Ensure that plots are displayed using inline plotting (in a Jupyter environment, this can be achieved by using `%matplotlib inline`). # Example Plot: The function, when executed, should produce two inline plots: 1. A count plot showing the number of passengers in each class. 2. A count plot showing the percentage of passengers who survived in each class, grouped by whether they survived or not. Note: - To ensure the plots are displayed inline in a Jupyter Notebook, you can use `%matplotlib inline` at the beginning of your function. # Expected Output: When you call the function `plot_titanic_data()`, it should produce two visualizations as specified above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): # Setting the theme to \\"whitegrid\\" sns.set_theme(style=\\"whitegrid\\") # Loading the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Plot 1 - Basic Count Plot plt.figure(figsize=(10, 6)) sns.countplot(x=\'class\', data=titanic) plt.title(\'Count of Passengers by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() # Plot 2 - Grouped and Normalized Count Plot plt.figure(figsize=(10, 6)) total = titanic[\'class\'].value_counts().sum() class_survived = titanic.groupby(\'class\')[\'survived\'].value_counts(normalize=True).mul(100).rename(\'percentage\').reset_index() sns.barplot(x=\'class\', y=\'percentage\', hue=\'survived\', data=class_survived) plt.title(\'Percentage of Passengers who Survived by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.show()"},{"question":"You are tasked with analyzing the `diamonds` dataset using the seaborn library. Your goal is to create two different visualizations by following the instructions below: 1. **Visualization 1: Average Carat by Clarity** - Create a bar plot that shows the average carat weight for each clarity category in the `diamonds` dataset. - Use the `seaborn.objects` module for this plot. - Label the plot appropriately and give it a title: \\"Average Carat by Clarity\\". 2. **Visualization 2: Median Carat by Cut with Dodge Transformation** - Create a bar plot that shows the median carat weight for each clarity category, but with bars dodged by cut type. - Use the `seaborn.objects` module for this plot. - Label the plot appropriately and give it a title: \\"Median Carat by Clarity and Cut\\". - Apply color coding to differentiate between different cuts. # Input No input is required from the user. You will use the `diamonds` dataset from seaborn. # Output Format - The function should output two plots displayed using `matplotlib.pyplot.show()`. # Sample Output ```python # Output: # 1. Display plot \\"Average Carat by Clarity\\" # 2. Display plot \\"Median Carat by Clarity and Cut\\" ``` # Constraints - Use only seaborn and matplotlib libraries for plotting. - Ensure the plots are clear and properly labeled. # Skeleton Code ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_plots(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Visualization 1: Average Carat by Clarity p1 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") p1.add(so.Bar(), so.Agg()) p1.plot() plt.title(\\"Average Carat by Clarity\\") plt.show() # Visualization 2: Median Carat by Clarity and Cut with Dodge Transformation p2 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\") p2.add(so.Bar(), so.Agg(\\"median\\"), so.Dodge()) p2.plot() plt.title(\\"Median Carat by Clarity and Cut\\") plt.show() # Call the function to test the plots create_diamond_plots() ``` Create the function `create_diamond_plots` following the skeleton code provided above.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_plots(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Visualization 1: Average Carat by Clarity p1 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") p1.add(so.Bar(), so.Agg(\\"mean\\")) p1.label(x=\\"Clarity\\", y=\\"Average Carat\\") plt.title(\\"Average Carat by Clarity\\") p1.show() # Visualization 2: Median Carat by Clarity and Cut with Dodge Transformation p2 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\") p2.add(so.Bar(), so.Agg(\\"median\\"), so.Dodge()) p2.label(x=\\"Clarity\\", y=\\"Median Carat\\", color=\\"Cut Type\\") plt.title(\\"Median Carat by Clarity and Cut\\") p2.show() # Call the function to test the plots create_diamond_plots()"},{"question":"# Question You are tasked with dynamically creating a class that models a simple bank account system using the `types` module. The class should be created with the following features: 1. Attributes: - `account_number`: a unique identifier for the account. - `account_holder`: the name of the account holder. - `balance`: the current balance in the account, initialized to 0. 2. Methods: - `deposit(amount)`: increases the balance by the given amount. - `withdraw(amount)`: decreases the balance by the given amount, provided that adequate balance is available. - `get_balance()`: returns the current balance. Write a function `create_bank_account_class()` that uses the `types.new_class` function to create the class dynamically. This class should include a constructor to initialize the attributes and implement all the required methods. Input: None Output: A dynamically created class with the described attributes and methods. Constraints: - Use the `types` module for creating the class. - Ensure the class is created dynamically using the `types.new_class` function. - The class methods should handle any logical conditions appropriately (e.g., withdrawing money should only be possible if there is enough balance). Example Usage: ```python BankAccount = create_bank_account_class() account = BankAccount(account_number=12345, account_holder=\'John Doe\') account.deposit(100) account.withdraw(50) print(account.get_balance()) # Output: 50 ``` Hints: - Use the `exec_body` argument of `types.new_class` to define methods and attributes within the class.","solution":"import types def create_bank_account_class(): class_body = \'\'\' def __init__(self, account_number, account_holder): self.account_number = account_number self.account_holder = account_holder self.balance = 0 def deposit(self, amount): if amount > 0: self.balance += amount def withdraw(self, amount): if 0 < amount <= self.balance: self.balance -= amount def get_balance(self): return self.balance \'\'\' class_namespace = {} exec(class_body, class_namespace) BankAccount = types.new_class(\'BankAccount\', (), {}, lambda ns: ns.update(class_namespace)) return BankAccount"},{"question":"# Custom Nearest Neighbors Implementation Problem Statement You are required to implement a custom class for k-nearest neighbors (KNN) regression and classification using the `scikit-learn` `neighbors` module. Your implementation should handle dynamic selection of the algorithm (brute-force, KD Tree, or Ball Tree) based on the dataset properties. Additionally, you will need to provide functionalities to find the nearest neighbors, and perform both regression and classification tasks. Specifications 1. **Class Definition**: - Create a class named `CustomKNN` that handles both classification and regression. - The class should include methods for fitting the model, predicting, and calculating accuracy for classification and mean squared error for regression. 2. **Initialization**: - The constructor should accept the following parameters: - `n_neighbors` (int, default=5): Number of neighbors to use. - `algorithm` (string, default=\'auto\'): Algorithm to use for nearest neighbor search. Should be one of [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']. - `weights` (string or callable, default=\'uniform\'): Weight function used in prediction. Possible values are \'uniform\', \'distance\', or a user-defined function. 3. **Methods**: - `fit(X, y, task=\'classification\')`: - Fit the model using the training data. - Parameters: - `X` (array-like of shape (n_samples, n_features)): Training data. - `y` (array-like of shape (n_samples,)): Target values. - `task` (string, default=\'classification\'): Type of task to perform; either \'classification\' or \'regression\'. - `predict(X)`: - Perform prediction on the data. - Parameters: - `X` (array-like of shape (n_samples, n_features)): Data to predict. - Returns: - Predictions for the input data. - `score(X, y)`: - Evaluate the performance of the model. - For classification, return the accuracy. - For regression, return the mean squared error. - Parameters: - `X` (array-like of shape (n_samples, n_features)): Test data. - `y` (array-like of shape (n_samples,)): True values for `X`. 4. **Example Usage**: - Demonstrate the usage of this class for both KNN classification and regression, including: - Initializing the class with specified parameters. - Fitting the model with training data. - Making predictions on test data. - Evaluating and printing the performance of the model. Constraints and Limitations - You may assume that all input data is clean and properly formatted. - There is no need to handle missing values. - The solution should be efficient and work within reasonable time limits even for larger datasets. Input / Output 1. **Input**: - Training data `X_train` (array-like of shape (n_samples, n_features)) - Training labels `y_train` (array-like of shape (n_samples,)) - Test data `X_test` (array-like of shape (n_samples, n_features)) - Task type (\'classification\' or \'regression\') 2. **Output**: - Predicted values for test data - Performance metrics (accuracy for classification and MSE for regression) Performance Requirements - The solution should be optimized to handle large datasets efficiently. - The selection of the nearest neighbors algorithm should be dynamic and based on the dataset properties. ```python import numpy as np from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor class CustomKNN: def __init__(self, n_neighbors=5, algorithm=\'auto\', weights=\'uniform\'): self.n_neighbors = n_neighbors self.algorithm = algorithm self.weights = weights self.model = None def fit(self, X, y, task=\'classification\'): if task == \'classification\': self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, algorithm=self.algorithm, weights=self.weights) elif task == \'regression\': self.model = KNeighborsRegressor(n_neighbors=self.n_neighbors, algorithm=self.algorithm, weights=self.weights) else: raise ValueError(\\"Invalid task. Choose either \'classification\' or \'regression\'.\\") self.model.fit(X, y) def predict(self, X): return self.model.predict(X) def score(self, X, y): if isinstance(self.model, KNeighborsClassifier): return self.model.score(X, y) elif isinstance(self.model, KNeighborsRegressor): from sklearn.metrics import mean_squared_error predictions = self.model.predict(X) return mean_squared_error(y, predictions) # Example Usage: # Classification X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train = np.array([1, 1, 1, 2, 2, 2]) X_test = np.array([[-0.8, -1], [0.5, 1]]) y_test = np.array([1, 2]) knn_clf = CustomKNN(n_neighbors=3, algorithm=\'auto\', weights=\'uniform\') knn_clf.fit(X_train, y_train, task=\'classification\') predictions = knn_clf.predict(X_test) accuracy = knn_clf.score(X_test, y_test) print(f\\"Classification predictions: {predictions}, Accuracy: {accuracy}\\") # Regression X_train = np.array([[1], [2], [3], [4], [5], [6]]) y_train = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5]) X_test = np.array([[1.5], [2.5], [3.5]]) y_test = np.array([1.7, 2.7, 3.7]) knn_reg = CustomKNN(n_neighbors=2, algorithm=\'auto\', weights=\'distance\') knn_reg.fit(X_train, y_train, task=\'regression\') predictions = knn_reg.predict(X_test) mse = knn_reg.score(X_test, y_test) print(f\\"Regression predictions: {predictions}, MSE: {mse}\\") ``` In this question, the students are required to integrate several aspects of nearest neighbors algorithms, including classification and regression, dynamic algorithm selection, and custom weight functions. This comprehensive approach ensures a deep understanding and capability in implementing and using scikit-learn\'s `neighbors` module effectively.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor class CustomKNN: def __init__(self, n_neighbors=5, algorithm=\'auto\', weights=\'uniform\'): self.n_neighbors = n_neighbors self.algorithm = algorithm self.weights = weights self.model = None def fit(self, X, y, task=\'classification\'): if task == \'classification\': self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, algorithm=self.algorithm, weights=self.weights) elif task == \'regression\': self.model = KNeighborsRegressor(n_neighbors=self.n_neighbors, algorithm=self.algorithm, weights=self.weights) else: raise ValueError(\\"Invalid task. Choose either \'classification\' or \'regression\'.\\") self.model.fit(X, y) def predict(self, X): return self.model.predict(X) def score(self, X, y): if isinstance(self.model, KNeighborsClassifier): return self.model.score(X, y) elif isinstance(self.model, KNeighborsRegressor): from sklearn.metrics import mean_squared_error predictions = self.model.predict(X) return mean_squared_error(y, predictions) # Example Usage: # Classification if __name__ == \\"__main__\\": X_train_clf = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train_clf = np.array([1, 1, 1, 2, 2, 2]) X_test_clf = np.array([[-0.8, -1], [0.5, 1]]) y_test_clf = np.array([1, 2]) knn_clf = CustomKNN(n_neighbors=3, algorithm=\'auto\', weights=\'uniform\') knn_clf.fit(X_train_clf, y_train_clf, task=\'classification\') predictions_clf = knn_clf.predict(X_test_clf) accuracy_clf = knn_clf.score(X_test_clf, y_test_clf) print(f\\"Classification predictions: {predictions_clf}, Accuracy: {accuracy_clf}\\") # Regression X_train_reg = np.array([[1], [2], [3], [4], [5], [6]]) y_train_reg = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5]) X_test_reg = np.array([[1.5], [2.5], [3.5]]) y_test_reg = np.array([1.7, 2.7, 3.7]) knn_reg = CustomKNN(n_neighbors=2, algorithm=\'auto\', weights=\'distance\') knn_reg.fit(X_train_reg, y_train_reg, task=\'regression\') predictions_reg = knn_reg.predict(X_test_reg) mse_reg = knn_reg.score(X_test_reg, y_test_reg) print(f\\"Regression predictions: {predictions_reg}, MSE: {mse_reg}\\")"},{"question":"**Question**: You are required to write a Python function that leverages the `mailcap` module to determine the appropriate command to view a file based on its MIME type from a sample set of MIME types. # Function Signature ```python def find_view_command(mime_type: str, filename: str) -> str: pass ``` # Input - `mime_type` (str): A string representing the MIME type of the file (e.g., \'video/mpeg\'). - `filename` (str): A string representing the filename of the file to be viewed. # Output - The function should return: - A string containing the command line to be executed to view the file. - If no matching MIME type is found, return `\\"No suitable viewer found\\"`. # Constraints - You need to ensure that shell metacharacters in `filename` are handled safely, leveraging the security fix applied in version 3.10.8 of Python. - Assume that the mailcap files may contain various named parameters that need to be correctly substituted. - The function should only return the `view` command and should not consider other commands like \'compose\' or \'edit\'. # Example ```python # Sample usage of the function command = find_view_command(\'video/mpeg\', \'movie.mp4\') print(command) # Output: \\"xmpeg movie.mp4\\" (assuming \'xmpeg %s\' is configured for \'video/mpeg\' in mailcap files) ``` # Notes - Use the `mailcap.getcaps()` function to get the dictionary of MIME types and their associated mailcap entries. - Use the `mailcap.findmatch()` function to find the appropriate command for the given MIME type and filename. Complete the function `find_view_command` to meet the requirements specified.","solution":"import mailcap import shlex def find_view_command(mime_type: str, filename: str) -> str: Finds the command to view a file based on its MIME type using the mailcap module. Args: - mime_type (str): The MIME type of the file. - filename (str): The name of the file. Returns: - str: The command to view the file, or \\"No suitable viewer found\\" if no command is found. caps = mailcap.getcaps() view_command, _ = mailcap.findmatch(caps, mime_type, filename=shlex.quote(filename), key=\'view\') if view_command: return view_command else: return \\"No suitable viewer found\\""},{"question":"**Event Scheduler with Custom Priority** You are required to create an advanced event scheduler using the `sched` module. This scheduler should be able to manage events based on their priority, execution time, and allow for dynamic scheduling by adding, updating, and canceling events in real-time. # Task: Implement the following functions to manage the event scheduler: 1. **`add_event(scheduler, delay, priority, action, argument=(), kwargs={})`**: - **Input**: `scheduler` (scheduler object), `delay` (int, delay in seconds), `priority` (int, event priority), `action` (callable to be executed), `argument` (tuple, optional positional arguments for action), `kwargs` (dictionary, optional keyword arguments for action) - **Output**: None - **Description**: Add an event to the scheduler that will be executed after `delay` seconds with the given priority. 2. **`remove_event(scheduler, event)`**: - **Input**: `scheduler` (scheduler object), `event` (event to be canceled) - **Output**: None - **Description**: Removes a specified event from the scheduler. If the event is not in the queue, handle the exception properly without crashing. 3. **`list_events(scheduler)`**: - **Input**: `scheduler` (scheduler object) - **Output**: List of tuples with event details (time, priority, action, argument, kwargs) - **Description**: Return a list of all scheduled events in the order they will be executed. 4. **`run_scheduler(scheduler)`**: - **Input**: `scheduler` (scheduler object) - **Output**: None - **Description**: Run the scheduler to execute all the scheduled events based on their scheduled time and priority. # Example Usage: ```python import sched import time def sample_action(text): print(f\\"{time.time()}: {text}\\") # Create scheduler object s = sched.scheduler(time.time, time.sleep) # Add events add_event(s, 5, 2, sample_action, argument=(\\"Event 1\\",)) add_event(s, 3, 1, sample_action, argument=(\\"Event 2\\",)) add_event(s, 10, 3, sample_action, argument=(\\"Event 3\\",)) # List scheduled events print(list_events(s)) # Run scheduler run_scheduler(s) ``` # Constraints: - The action should be a callable function. - The delay should be a non-negative integer. - Handle exceptions gracefully, especially when trying to remove an event not in the queue. - Make sure to print the event details in the `sample_action` function, including the current time and the given text. NOTE: It is essential to use the `sched` module\'s functionalities and adhere to the guidelines provided.","solution":"import sched import time def add_event(scheduler, delay, priority, action, argument=(), kwargs={}): Adds an event to the scheduler that will be executed after `delay` seconds with the given priority. return scheduler.enter(delay, priority, action, argument, kwargs) def remove_event(scheduler, event): Removes a specified event from the scheduler. If the event is not in the queue, it will handle the exception properly without crashing. try: scheduler.cancel(event) except ValueError: pass def list_events(scheduler): Returns a list of all scheduled events in the order they will be executed. events = scheduler.queue return [(e.time, e.priority, e.action, e.argument, e.kwargs) for e in events] def run_scheduler(scheduler): Run the scheduler to execute all the scheduled events based on their scheduled time and priority. scheduler.run() # Sample action for testing def sample_action(text): print(f\\"{time.time()}: {text}\\")"},{"question":"Objective: Demonstrate your understanding of Python\'s bytearray objects by implementing several functions that interact with bytearrays using the provided API. Task: Implement the following functions using the bytearray API described above: 1. **is_bytearray**: - **Input**: A Python object. - **Output**: A boolean value (`True` or `False`) indicating whether the input is a `bytearray` type, including subtypes. - **Constraints**: Use the `PyByteArray_Check` function. 2. **create_bytearray_from_string**: - **Input**: A Python string. - **Output**: A new `bytearray` object containing the byte representation of the input string. - **Constraints**: Use the `PyByteArray_FromStringAndSize` function. 3. **concat_bytearrays**: - **Input**: Two `bytearray` objects. - **Output**: A new `bytearray` object resulting from concatenating the two input bytearrays. - **Constraints**: Use the `PyByteArray_Concat` function. 4. **resize_bytearray**: - **Input**: A `bytearray` object and a new size (integer). - **Output**: None. - **Effect**: Resize the input `bytearray` to the new size. - **Constraints**: Use the `PyByteArray_Resize` function. 5. **bytearray_to_string**: - **Input**: A `bytearray` object. - **Output**: A Python string representation of the `bytearray` contents. - **Constraints**: Use the `PyByteArray_AsString` function and ensure the result is properly converted to a Python string. Function Signatures: ```python def is_bytearray(obj) -> bool: pass def create_bytearray_from_string(s: str) -> bytearray: pass def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: pass def resize_bytearray(arr: bytearray, new_size: int) -> None: pass def bytearray_to_string(arr: bytearray) -> str: pass ``` Example Usage: ```python # Example for is_bytearray print(is_bytearray(bytearray())) # Should return True print(is_bytearray(\\"not a bytearray\\")) # Should return False # Example for create_bytearray_from_string ba = create_bytearray_from_string(\\"hello\\") print(ba) # Should print bytearray(b\'hello\') # Example for concat_bytearrays ba1 = bytearray(b\'hello\') ba2 = bytearray(b\' world\') print(concat_bytearrays(ba1, ba2)) # Should print bytearray(b\'hello world\') # Example for resize_bytearray ba = bytearray(b\'hello\') resize_bytearray(ba, 8) print(len(ba)) # Should print 8 # Example for bytearray_to_string ba = bytearray(b\'example\') print(bytearray_to_string(ba)) # Should print \'example\' ``` Notes: - Pay careful attention to memory management and null checks as per the documentation. - Handle any potential errors gracefully, ensuring that the functions do not crash unexpectedly. - Ensure your code is efficient and follows best practices for use of the bytearray API. Good luck!","solution":"def is_bytearray(obj) -> bool: Check if the given object is a bytearray. return isinstance(obj, bytearray) def create_bytearray_from_string(s: str) -> bytearray: Create a bytearray from a string. return bytearray(s, \'utf-8\') def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Concatenate two bytearrays. return a + b def resize_bytearray(arr: bytearray, new_size: int) -> None: Resize the bytearray to the new specified size. arr.extend([0]*(new_size - len(arr))) def bytearray_to_string(arr: bytearray) -> str: Convert a bytearray to a string. return arr.decode(\'utf-8\')"},{"question":"Objective: You are required to demonstrate your skills in using the seaborn library to create and customize heatmaps. Your task involves loading a dataset, creating a heatmap, customizing annotations, and tweaking the plot aesthetics. # Task: 1. Load the \'iris\' dataset provided by seaborn. 2. Create a heatmap of the correlation matrix of the dataset features. 3. Add annotations to the heatmap. 4. Customize the annotation format to display values rounded to two decimal places. 5. Use a colormap of your choice. 6. Set the colormap norm such that the minimum value is 0 and the maximum value is 1. 7. Add lines between cells with a linewidth of 0.1. 8. Modify the plot by: - Removing x and y axis labels. - Moving the x-axis ticks to the top. # Input: - No input is required for this task. # Output: - The heatmap plot should be displayed as the final output. # Constraints: - You must use the seaborn and matplotlib libraries. - The annotations must be displayed with values rounded to two decimal places. # Example Output: Your final heatmap should look similar to this: ![Heatmap Example](https://i.imgur.com/JK5ePQD.png) ```python # Your implementation here import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the \'iris\' dataset iris = sns.load_dataset(\'iris\') # Step 2: Calculate the correlation matrix of the dataset features corr_matrix = iris.corr() # Step 3: Create the heatmap ax = sns.heatmap(corr_matrix, annot=True, fmt=\\".2f\\", cmap=\'coolwarm\', vmin=0, vmax=1, linewidths=0.1) # Step 8: Modify the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Step 9: Display the plot plt.show() ``` # Note: Make sure to include any additional customization you deem necessary to enhance the visualization. Comments in your code explaining each step will be appreciated.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_heatmap(): # Step 1: Load the \'iris\' dataset iris = sns.load_dataset(\'iris\') # Step 2: Calculate the correlation matrix of the dataset features corr_matrix = iris.corr() # Step 3, 4: Create the heatmap with annotations, customized format, colormap, and norm settings ax = sns.heatmap( corr_matrix, annot=True, fmt=\\".2f\\", cmap=\'viridis\', vmin=0, vmax=1, linewidths=0.1 ) # Step 8: Modify the plot to remove axis labels and move x-axis ticks to the top ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Step 9: Display the plot plt.show()"},{"question":"Objective: Implement a set of functions to serialize a list of Python objects to a file and later deserialize them back to Python objects. Your implementation should handle different Python data types, including integers, strings, and dictionaries. Task: 1. **Function 1: `serialize_objects_to_file(file_path: str, objects: list, version: int) -> None`** - **Input:** - `file_path` (str): The path to the file where the serialized data will be written. - `objects` (list): A list of Python objects to be serialized. - `version` (int): The version of the marshal format to use (0, 1, or 2). - **Output:** None - **Description:** - Serialize each object in the list `objects` and write it to the file specified by `file_path` using the specified `version` for the marshal format. 2. **Function 2: `deserialize_objects_from_file(file_path: str) -> list`** - **Input:** - `file_path` (str): The path to the file from which the objects will be read. - **Output:** - A list of Python objects that were deserialized from the file. - **Description:** - Read the serialized data from the file specified by `file_path` and deserialize it back into Python objects. Constraints: 1. Assume that the file operations will be performed in a binary mode. 2. Handle exceptions such as file not found, read/write errors, and any marshalling/unmarshalling errors appropriately. 3. Ensure that the objects are read back in the same order they were written. Example Usage: ```python # Example objects objects_to_serialize = [42, \\"Hello, World!\\", {\\"key\\": \\"value\\"}] # Serialize objects to file serialize_objects_to_file(\'data.marshal\', objects_to_serialize, 2) # Deserialize objects from file deserialized_objects = deserialize_objects_from_file(\'data.marshal\') # Check that the deserialized objects match the original ones assert deserialized_objects == objects_to_serialize ``` This question will test the students\' understanding of data marshalling, file handling, exception handling, and working with diverse Python data types.","solution":"import marshal def serialize_objects_to_file(file_path: str, objects: list, version: int) -> None: Serialize each object in the list `objects` and write it to the file specified by `file_path` using the specified `version` for the marshal format. try: with open(file_path, \'wb\') as file: for obj in objects: serialized_data = marshal.dumps(obj, version) file.write(serialized_data) file.write(b\'n\') # Add a newline to separate different objects except Exception as e: print(f\\"An error occurred while serializing objects: {e}\\") def deserialize_objects_from_file(file_path: str) -> list: Read the serialized data from the file specified by `file_path` and deserialize it back into Python objects. deserialized_objects = [] try: with open(file_path, \'rb\') as file: lines = file.readlines() for line in lines: deserialized_objects.append(marshal.loads(line)) except Exception as e: print(f\\"An error occurred while deserializing objects: {e}\\") return deserialized_objects"},{"question":"# Seaborn Plotting Context Challenge You are tasked with creating a function that visualizes a dataset using seaborn and utilizes different plotting contexts to showcase the flexibility of plot styling. Function Signature: ```python def visualize_data(df: pd.DataFrame) -> None: pass ``` Parameters: - `df (pd.DataFrame)`: A DataFrame containing at least two columns of numerical data to be plotted. Implementation Requirements: 1. Import the necessary libraries: seaborn, matplotlib.pyplot, and pandas. 2. Create a figure with three subplots (1x3 grid). 3. Use the `sns.plotting_context()` function to generate plots with three different contexts: - The default context. - The \\"talk\\" context. - A custom context with `font_scale` set to 1.5. 4. Each subplot should plot a line plot of the first two numerical columns in the DataFrame. 5. Ensure the plots have appropriate titles indicating the context used. Example: Given a DataFrame `df` with columns `[\\"X\\", \\"Y\\", \\"Z\\"]`, the function should generate a figure with three subplots: - The first subplot using the default seaborn plotting context. - The second subplot using the \\"talk\\" context. - The third subplot using a custom context with `font_scale` set to 1.5. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_data(df: pd.DataFrame) -> None: fig, axes = plt.subplots(1, 3, figsize=(15, 5)) sns.lineplot(x=df.iloc[:,0], y=df.iloc[:,1], ax=axes[0]) axes[0].set_title(\\"Default Context\\") with sns.plotting_context(\\"talk\\"): sns.lineplot(x=df.iloc[:,0], y=df.iloc[:,1], ax=axes[1]) axes[1].set_title(\\"Talk Context\\") custom_context = { \\"font.scale\\": 1.5, } with sns.plotting_context(rc=custom_context): sns.lineplot(x=df.iloc[:,0], y=df.iloc[:,1], ax=axes[2]) axes[2].set_title(\\"Custom Context (font.scale=1.5)\\") plt.tight_layout() plt.show() # Example Usage: # df = pd.DataFrame({ # \\"X\\": [1, 2, 3, 4, 5], # \\"Y\\": [2, 3, 5, 7, 11], # \\"Z\\": [1, 4, 9, 16, 25] # }) # visualize_data(df) ``` Submit your function implementation and visualize the plots using the example DataFrame provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_data(df: pd.DataFrame) -> None: fig, axes = plt.subplots(1, 3, figsize=(15, 5)) # Default context sns.lineplot(x=df.iloc[:, 0], y=df.iloc[:, 1], ax=axes[0]) axes[0].set_title(\\"Default Context\\") # Talk context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=df.iloc[:, 0], y=df.iloc[:, 1], ax=axes[1]) axes[1].set_title(\\"Talk Context\\") # Custom context with font.scale=1.5 custom_context = { \\"font.scale\\": 1.5, } with sns.plotting_context(rc=custom_context): sns.lineplot(x=df.iloc[:, 0], y=df.iloc[:, 1], ax=axes[2]) axes[2].set_title(\\"Custom Context (font.scale=1.5)\\") plt.tight_layout() plt.show() # Example Usage: # df = pd.DataFrame({ # \\"X\\": [1, 2, 3, 4, 5], # \\"Y\\": [2, 3, 5, 7, 11], # \\"Z\\": [1, 4, 9, 16, 25] # }) # visualize_data(df)"},{"question":"# Question: Implement a Simple Chat Server using `epoll` You are required to implement a simple chat server using the `epoll` interface in Python. The server will handle multiple clients connected via sockets, allowing them to send messages to each other. Objectives: - Implement an `EpollServer` class. - Use `select.epoll` to manage multiple client connections. - Ensure the server can read from and write to client sockets based on I/O events. Requirements: 1. **Initialization**: Create a server socket, bind to localhost on a specified port, set it to non-blocking mode, and register it with epoll events for reading. 2. **Connection Handling**: Accept new connections and register client sockets with epoll. 3. **I/O Management**: Use edge-triggered or level-triggered events to read incoming messages from clients and broadcast them to all connected clients. 4. **Cleanup**: Handle client disconnections and errors gracefully by unregistering and closing sockets. Input: - The server will start on a provided port number. Specifications: - Use edge-triggered polling (i.e., `EPOLLET`). - Use non-blocking sockets. - Implement methods: `start_server(port)`, `run_event_loop()`, `handle_new_connection()`, `handle_client_message(fd)`. - Messages sent by clients are simple strings and should be broadcasted to all other connected clients. Example: ```python server = EpollServer() server.start_server(12345) server.run_event_loop() ``` Constraints: - Do not use any third-party networking libraries. - The implementation should be efficient in terms of I/O operations and handle thousands of concurrent clients if needed. Evaluation: Your implementation will be evaluated based on: 1. Correct usage of the `epoll` interface. 2. Ability to handle multiple clients concurrently. 3. Proper management of socket states and resource cleanup. 4. Code organization and readability. Please write the implementation of the `EpollServer` class as described.","solution":"import socket import select import errno class EpollServer: def __init__(self): self.server_socket = None self.epoll = None self.connections = {} self.addresses = {} def start_server(self, port): self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.server_socket.bind((\'localhost\', port)) self.server_socket.listen(socket.SOMAXCONN) self.server_socket.setblocking(False) self.epoll = select.epoll() self.epoll.register(self.server_socket.fileno(), select.EPOLLIN | select.EPOLLET) def run_event_loop(self): try: while True: events = self.epoll.poll(1) for fileno, event in events: if fileno == self.server_socket.fileno(): self.handle_new_connection() elif event & select.EPOLLIN: self.handle_client_message(fileno) elif event & (select.EPOLLHUP | select.EPOLLERR): self.cleanup_client(fileno) finally: self.epoll.unregister(self.server_socket.fileno()) self.epoll.close() self.server_socket.close() def handle_new_connection(self): try: while True: client_socket, client_address = self.server_socket.accept() client_socket.setblocking(False) self.epoll.register(client_socket.fileno(), select.EPOLLIN | select.EPOLLET) self.connections[client_socket.fileno()] = client_socket self.addresses[client_socket.fileno()] = client_address except socket.error as e: if e.errno != errno.EAGAIN and e.errno != errno.EWOULDBLOCK: raise def handle_client_message(self, fileno): client_socket = self.connections.get(fileno) if client_socket: try: data = client_socket.recv(1024) if data: self.broadcast_message(fileno, data) else: self.cleanup_client(fileno) except socket.error as e: if e.errno != errno.EAGAIN and e.errno != errno.EWOULDBLOCK: self.cleanup_client(fileno) def broadcast_message(self, sender_fileno, message): for fileno, socket in self.connections.items(): if fileno != sender_fileno: try: socket.send(message) except socket.error: self.cleanup_client(fileno) def cleanup_client(self, fileno): self.epoll.unregister(fileno) self.connections[fileno].close() del self.connections[fileno] del self.addresses[fileno]"},{"question":"# Python Version Encoding You are given some version information of Python in its individual components. Your task is to implement a function that converts this version information into the encoded hexadecimal format and vice versa. Function 1: encode_version Write a function `encode_version(major, minor, micro, release_level, release_serial)` that takes in the following parameters: - `major` (int): The major version number. - `minor` (int): The minor version number. - `micro` (int): The micro version number. - `release_level` (str): The release level, which can be \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". - `release_serial` (int): The release serial number. Zero for final releases. The function should return the combined version number encoded in a single integer. Function 2: decode_version Write a function `decode_version(hex_version)` that takes in the following parameter: - `hex_version` (int): The Python version number encoded as a single hexadecimal integer. The function should return a tuple `(major, minor, micro, release_level, release_serial)`: - `major` (int): The major version number. - `minor` (int): The minor version number. - `micro` (int): The micro version number. - `release_level` (str): The release level as a string (\\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\"). - `release_serial` (int): The release serial number. Constraints - The major version (`major`) will be between 0 and 255 inclusive. - The minor version (`minor`) will be between 0 and 255 inclusive. - The micro version (`micro`) will be between 0 and 255 inclusive. - `release_level` will be one of \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". - The release serial (`release_serial`) will be between 0 and 15 inclusive. Example ```python # Converting the version 3.4.1a2 to its encoded hexadecimal format print(encode_version(3, 4, 1, \\"alpha\\", 2)) # Output: 0x030401a2 # Decoding the hexadecimal format back to its components print(decode_version(0x030401a2)) # Output: (3, 4, 1, \\"alpha\\", 2) # Example with version 3.10.0 final release print(encode_version(3, 10, 0, \\"final\\", 0)) # Output: 0x030a00f0 print(decode_version(0x030a00f0)) # Output: (3, 10, 0, \\"final\\", 0) ``` Note The hexadecimal value represents the version information by using big endian order and combining the bits as specified in the provided documentation. Specifically: - `major` occupies bits 1-8 - `minor` occupies bits 9-16 - `micro` occupies bits 17-24 - `release_level` and `release_serial` occupy bits 25-28 and 29-32, respectively. The release levels are coded as 0xA for alpha, 0xB for beta, 0xC for candidate, and 0xF for final.","solution":"def encode_version(major, minor, micro, release_level, release_serial): Encode the version information into a single hexadecimal integer. release_dict = { \\"alpha\\": 0xA, \\"beta\\": 0xB, \\"candidate\\": 0xC, \\"final\\": 0xF } release_level_encoded = release_dict[release_level] # Combine all the parts into a single integer version = (major << 24) | (minor << 16) | (micro << 8) | (release_level_encoded << 4) | release_serial return version def decode_version(hex_version): Decode the hex version to its components. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF release_dict = { 0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"candidate\\", 0xF: \\"final\\" } release_level_decoded = release_dict[release_level] return (major, minor, micro, release_level_decoded, release_serial)"},{"question":"Design and implement a function `fetch_data_with_retries` that repeatedly attempts to fetch data from a provided asynchronous function `fetch_function` up to a given number of retries, with a specified timeout for each attempt. The function should appropriately handle the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.IncompleteReadError` Your function should follow these requirements: 1. Input: - `fetch_function`: An asynchronous function that may raise the specified exceptions. - `retries`: An integer specifying the number of retries allowed. - `timeout`: A float representing the timeout period for each attempt in seconds. 2. Output: - If the data is fetched successfully within the allowed retries and timeout, return the fetched data. - If all retries are exhausted without successful data retrieval, raise an appropriate exception indicating failure. 3. Constraints: - The `fetch_function` may intermittently raise `asyncio.TimeoutError`, `asyncio.CancelledError`, and `asyncio.IncompleteReadError`. - Each retry attempt should be delayed by 0.5 seconds to avoid overwhelming the resource being fetched. 4. Ensure the function is efficient and does not unnecessarily waste resources. ```python import asyncio async def fetch_data_with_retries(fetch_function, retries, timeout): Attempts to fetch data using the provided asynchronous fetch_function, retrying if necessary. :param fetch_function: An asynchronous function to fetch data. :param retries: Number of retries allowed. :param timeout: Timeout period for each fetch attempt in seconds. :return: Fetched data if successful. :raises: Exception if all retries fail. for attempt in range(retries): try: return await asyncio.wait_for(fetch_function(), timeout) except asyncio.TimeoutError: print(f\\"Attempt {attempt + 1} timed out. Retrying...\\") except asyncio.CancelledError: print(f\\"Attempt {attempt + 1} was cancelled. Retrying...\\") continue except asyncio.IncompleteReadError: print(f\\"Incomplete read on attempt {attempt + 1}. Retrying...\\") await asyncio.sleep(0.5) raise Exception(\\"All retry attempts have failed.\\") # Example usage: # async def mock_fetch(): # # Simulate network operation with random failures. # pass # await fetch_data_with_retries(mock_fetch, 5, 2.0) ```","solution":"import asyncio async def fetch_data_with_retries(fetch_function, retries, timeout): Attempts to fetch data using the provided asynchronous fetch_function, retrying if necessary. :param fetch_function: An asynchronous function to fetch data. :param retries: Number of retries allowed. :param timeout: Timeout period for each fetch attempt in seconds. :return: Fetched data if successful. :raises: Exception if all retries fail. for attempt in range(retries): try: return await asyncio.wait_for(fetch_function(), timeout) except asyncio.TimeoutError: print(f\\"Attempt {attempt + 1} timed out. Retrying...\\") except asyncio.CancelledError: print(f\\"Attempt {attempt + 1} was cancelled. Retrying...\\") continue except asyncio.IncompleteReadError: print(f\\"Incomplete read on attempt {attempt + 1}. Retrying...\\") await asyncio.sleep(0.5) raise Exception(\\"All retry attempts have failed.\\") # Example usage: # async def mock_fetch(): # # Simulate network operation with random failures. # pass # await fetch_data_with_retries(mock_fetch, 5, 2.0)"},{"question":"# Question: Understanding and Utilizing PyTorch\'s finfo and iinfo In this exercise, you are required to implement a function that processes tensors in a way that ensures numerical stability during computations. You\'ll need to use the PyTorch classes `torch.finfo` and `torch.iinfo` to fetch numerical properties of data types and use these to prevent overflow and underflow issues in tensor operations. Task: 1. Implement a function `normalize_tensor(tensor: torch.Tensor) -> torch.Tensor` that normalizes a given tensor such that the values are scaled between 0 and 1. The function should: - Handle both floating point and integer tensors. - Use the appropriate `torch.finfo` or `torch.iinfo` class to fetch numerical properties of the tensor\'s dtype. - Avoid any numerical instability by appropriately addressing the smallest and largest representable values of the tensor\'s dtype. 2. Implement another function `stable_softmax(tensor: torch.Tensor) -> torch.Tensor` to compute a stable softmax of a given tensor. Softmax is defined as: [ text{softmax}(x_i) = frac{e^{x_i}}{sum_{j} e^{x_j}} ] - Make sure to handle potential overflow issues by shifting the logits before exponentiation, using properties from the `torch.finfo` and `torch.iinfo` classes to decide the shift value. Expected Input and Output: - Input: A PyTorch `torch.Tensor` of any floating point or integer dtype. - Output: - `normalize_tensor` should return a tensor with values scaled between 0 and 1. - `stable_softmax` should return a tensor where softmax has been applied in a numerically stable way. Constraints: - You cannot assume the dtype of the input tensor. Your function should dynamically handle different dtypes. - You must use `torch.finfo` and `torch.iinfo` to fetch the required numerical properties. Example Usage: ```python import torch # Example for normalize_tensor function tensor = torch.tensor([100, 200, 300], dtype=torch.int32) normalized_tensor = normalize_tensor(tensor) print(normalized_tensor) # Output should be scaled values between 0 and 1 # Example for stable_softmax function tensor = torch.tensor([1e10, 2e10, 3e10], dtype=torch.float32) softmax_tensor = stable_softmax(tensor) print(softmax_tensor) # Output should be softmax values without overflow issues ``` # Hints: - For `normalize_tensor`, you might need to find the properties `min` and `max` of the tensor\'s dtype and normalize the tensor values using these properties. - For `stable_softmax`, consider using the max value of the tensor to shift the logits before applying the softmax function.","solution":"import torch def normalize_tensor(tensor: torch.Tensor) -> torch.Tensor: Normalizes a given tensor such that the values are scaled between 0 and 1. Handles both floating point and integer tensors. dtype = tensor.dtype if torch.is_floating_point(tensor): info = torch.finfo(dtype) else: info = torch.iinfo(dtype) min_val = info.min max_val = info.max normalized_tensor = (tensor - min_val) / (max_val - min_val) return normalized_tensor def stable_softmax(tensor: torch.Tensor) -> torch.Tensor: Computes a stable softmax of a given tensor. dtype = tensor.dtype if torch.is_floating_point(tensor): info = torch.finfo(dtype) else: raise ValueError(\\"Softmax should only be applied to floating point tensors\\") max_val = torch.max(tensor) shifted_tensor = tensor - max_val exps = torch.exp(shifted_tensor) softmax_tensor = exps / torch.sum(exps, dim=-1, keepdim=True) return softmax_tensor"},{"question":"Problem Statement You are tasked with writing a Python program that utilizes the `platform` module to gather information about the current system and formats it into a structured, human-readable report. # Detailed Instructions 1. Create a function `generate_system_report()` that: - Collects the following information using the `platform` module: - System Architecture (using `platform.architecture()`) - Machine Type (using `platform.machine()`) - Network Name (using `platform.node()`) - Platform Details (using `platform.platform()`) - Processor Name (using `platform.processor()`) - Python Build Information (using `platform.python_build()`) - Python Compiler Details (using `platform.python_compiler()`) - Python Implementation Details (using `platform.python_implementation()`) - Python Version (using `platform.python_version()`) - Operating System Name (using `platform.system()`) - Operating System Release (using `platform.release()`) - Operating System Version (using `platform.version()`) - Comprehensive System Details (using `platform.uname()`) - Formats the collected information into a structured and human-readable multi-line string report. 2. Ensure your function returns a string that contains all the information structured in a way that is easy to read. # Expected Input and Output - The function `generate_system_report()` does not take any parameters. - The output should be a multi-line string containing the system information. # Example Output ```plaintext System Report: -------------- Architecture: (\'64bit\', \'ELF\') Machine Type: x86_64 Network Name: my-computer Platform Details: Linux-5.4.0-42-generic-x86_64-with-glibc2.29 Processor: x86_64 Python Build: (\'default\', \'Jul 23 2020 20:15:06\') Python Compiler: GCC 9.3.0 Python Implementation: CPython Python Version: 3.8.5 Operating System: Linux OS Release: 5.4.0-42-generic OS Version: #46-Ubuntu SMP Fri Jul 17 00:52:12 UTC 2020 Comprehensive System Details: uname_result(system=\'Linux\', node=\'my-computer\', release=\'5.4.0-42-generic\', version=\'#46-Ubuntu SMP Fri Jul 17 00:52:12 UTC 2020\', machine=\'x86_64\', processor=\'x86_64\') ``` # Constraints - You are only allowed to use the functions provided in the `platform` module and standard Python libraries. - The function should handle cases where some of the information might not be available and should appropriately handle these cases in the report.","solution":"import platform def generate_system_report(): Collects system information using the platform module and formats it into a structured, human-readable report. Returns: str: A multi-line string containing the system information report. report = [] report.append(\\"System Report:\\") report.append(\\"--------------\\") report.append(f\\"Architecture: {platform.architecture()}\\") report.append(f\\"Machine Type: {platform.machine()}\\") report.append(f\\"Network Name: {platform.node()}\\") report.append(f\\"Platform Details: {platform.platform()}\\") report.append(f\\"Processor: {platform.processor()}\\") report.append(f\\"Python Build: {platform.python_build()}\\") report.append(f\\"Python Compiler: {platform.python_compiler()}\\") report.append(f\\"Python Implementation: {platform.python_implementation()}\\") report.append(f\\"Python Version: {platform.python_version()}\\") report.append(f\\"Operating System: {platform.system()}\\") report.append(f\\"OS Release: {platform.release()}\\") report.append(f\\"OS Version: {platform.version()}\\") report.append(f\\"Comprehensive System Details: {platform.uname()}\\") return \\"n\\".join(report)"},{"question":"# Coding Challenge: Developing a Custom Logging Utility for IDLE **Objective:** Implement a custom logging utility for IDLE to help developers log and visualize their script outputs efficiently. # Background: IDLE is an Integrated Development Environment for Python. It allows users to write, edit, and execute Python code in a user-friendly interface. However, sometimes developers might require a way to log their program\'s outputs systematically to a file while coding in IDLE. This functionality is not directly available in IDLE\'s interface and would be a useful utility for developers who want to track their script outputs systematically. # Task: Create a Python module named `idle_logger` that provides a custom logging utility specifically designed for use in IDLE. The utility should handle log creation, tracking different types of logs (info, warning, error), and manage log files based on sessions. # Requirements: 1. **Module Name:** `idle_logger` 2. **Functionality:** - Should be able to start a logging session. - Handle different types of logs: `info`, `warning`, `error`. - Save logs with timestamps. - Rotate logs if the session exceeds a specified size limit. 3. **Classes and Methods:** - `Logger`: Main class to handle logging. - `__init__(self, session_name: str, max_size: int=1024*1024):` Initializes the logger with a session name and max size for log rotation. - `start(self)`: Starts a logging session and creates a log file. - `log_info(self, message: str)`: Logs an info level message. - `log_warning(self, message: str)`: Logs a warning level message. - `log_error(self, message: str)`: Logs an error level message. - `rotate_log(self)`: Rotates the log file if it exceeds the max size. - `LogFileHandler`: Helper class to manage the file operations. # Expected Input and Output: - The `idle_logger` module should manage the creation of log files and write appropriate log messages as specified. No direct input is required from the user for logging operations once the session starts. - Log messages should be saved to files named based on the session name and timestamp. # Constraints: - The log files should be stored with a naming convention that includes the session name and a timestamp. - The logger should rotate the log file if it exceeds the given size limit (`max_size`). # Example Usage: ```python from idle_logger import Logger # Initialize and start the logger session logger = Logger(session_name=\'my_script_session\', max_size=10*1024) # 10 KB max size for example logger.start() # Logging messages logger.log_info(\'This is an info message.\') logger.log_warning(\'This is a warning message.\') logger.log_error(\'This is an error message.\') # The logger will automatically handle log rotation based on the specified max size. ``` # Notes: - Ensure thread safety as logging might be invoked from various parts of a program. - Consider using Python’s built-in `logging` module as a reference, but design your utility to specifically meet the needs described above.","solution":"import os import time from datetime import datetime class LogFileHandler: def __init__(self, session_name, max_size): self.session_name = session_name self.max_size = max_size self.log_file = None self.log_count = 0 def _generate_log_filename(self): timestamp = datetime.now().strftime(\\"%Y%m%d_%H%M%S\\") return f\\"{self.session_name}_{timestamp}_{self.log_count}.log\\" def open_log_file(self): if self.log_file is not None and not self.log_file.closed: self.log_file.close() filename = self._generate_log_filename() self.log_file = open(filename, \'a\') self.log_count += 1 def write(self, message): self.log_file.write(message) self.log_file.flush() if self.log_file.tell() >= self.max_size: self.rotate_log() def rotate_log(self): self.open_log_file() def close_log_file(self): if self.log_file is not None: self.log_file.close() class Logger: def __init__(self, session_name: str, max_size: int=1024*1024): self.handler = LogFileHandler(session_name, max_size) def start(self): self.handler.open_log_file() def log_info(self, message: str): self._log(\\"INFO\\", message) def log_warning(self, message: str): self._log(\\"WARNING\\", message) def log_error(self, message: str): self._log(\\"ERROR\\", message) def _log(self, level: str, message: str): timestamp = datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") formatted_message = f\\"{timestamp} - {level} - {message}n\\" self.handler.write(formatted_message) def rotate_log(self): self.handler.rotate_log() def __del__(self): self.handler.close_log_file() # Example usage: # from idle_logger import Logger # logger = Logger(session_name=\'my_script_session\', max_size=10*1024) # logger.start() # logger.log_info(\'This is an info message.\') # logger.log_warning(\'This is a warning message.\') # logger.log_error(\'This is an error message.\')"},{"question":"**Objective:** Demonstrate your understanding of seaborn by working with a dataset, transforming it between different formats, and creating visualizations. **Question:** You are provided with a dataset that contains information about various fruits, their weights, and their prices over different months. The dataset is initially in wide-form format. Your task is to perform the following: 1. Load the dataset into a pandas DataFrame. 2. Convert the dataset from wide-form to long-form. 3. Use seaborn to create and customize specific visualizations based on the transformed data. **Steps:** Step 1: Load the dataset ```python import pandas as pd data = { \'Month\': [\'Jan\', \'Feb\', \'Mar\'], \'Apple_Weight\': [1.2, 1.3, 1.5], \'Apple_Price\': [0.80, 0.85, 0.90], \'Banana_Weight\': [1.1, 1.0, 1.2], \'Banana_Price\': [0.50, 0.55, 0.60] } df = pd.DataFrame(data) ``` Step 2: Convert the dataset to long-form Convert the DataFrame `df` from wide-form to long-form format so that we have the columns: `Month`, `Fruit`, `Weight`, and `Price`. Step 3: Create Visualizations 1. Create a line plot showing the change in weight of each fruit across months. 2. Create a bar plot showing the price of each fruit for each month. **Expected Input:** - A pandas DataFrame in wide-form format. **Expected Output:** - Transformed DataFrame in long-form format. - Two seaborn plots: one line plot for weight and one bar plot for price. **Additional Constraints:** - Ensure your code is efficient and avoids unnecessary computations. - Use appropriate seaborn functions and customization to improve plot readability. Implementation: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = { \'Month\': [\'Jan\', \'Feb\', \'Mar\'], \'Apple_Weight\': [1.2, 1.3, 1.5], \'Apple_Price\': [0.80, 0.85, 0.90], \'Banana_Weight\': [1.1, 1.0, 1.2], \'Banana_Price\': [0.50, 0.55, 0.60] } df = pd.DataFrame(data) # Step 2: Convert the dataset to long-form df_long = df.melt(id_vars=[\'Month\'], var_name=\'Variable\', value_name=\'Value\') df_long[[\'Fruit\', \'Measure\']] = df_long[\'Variable\'].str.split(\'_\', expand=True) df_long = df_long.pivot_table(index=[\'Month\', \'Fruit\'], columns=\'Measure\', values=\'Value\').reset_index() # Step 3: Create Visualizations # Line plot for weight plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\'Month\', y=\'Weight\', hue=\'Fruit\', marker=\'o\') plt.title(\'Fruit Weight Across Months\') plt.ylabel(\'Weight (kg)\') plt.show() # Bar plot for price plt.figure(figsize=(10, 6)) sns.barplot(data=df_long, x=\'Month\', y=\'Price\', hue=\'Fruit\') plt.title(\'Fruit Price Across Months\') plt.ylabel(\'Price ()\') plt.show() ``` **Output Explanation:** - The first plot shows a line graph of the weight changes for each fruit (apple and banana) over the three months. - The second plot shows a bar graph of the prices of the fruits for each month.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def transform_and_visualize(data): Transforms wide-form dataframe to long-form and creates visualizations. Arguments: data -- Dictionary containing the dataset Returns: df_long -- Transformed long-form dataframe # Step 1: Load the dataset df = pd.DataFrame(data) # Step 2: Convert the dataset to long-form df_long = df.melt(id_vars=[\'Month\'], var_name=\'Variable\', value_name=\'Value\') df_long[[\'Fruit\', \'Measure\']] = df_long[\'Variable\'].str.split(\'_\', expand=True) df_long = df_long.pivot_table(index=[\'Month\', \'Fruit\'], columns=\'Measure\', values=\'Value\').reset_index() # Step 3: Create Visualizations # Line plot for weight plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\'Month\', y=\'Weight\', hue=\'Fruit\', marker=\'o\') plt.title(\'Fruit Weight Across Months\') plt.ylabel(\'Weight (kg)\') plt.savefig(\'line_plot_weight.png\') plt.close() # Bar plot for price plt.figure(figsize=(10, 6)) sns.barplot(data=df_long, x=\'Month\', y=\'Price\', hue=\'Fruit\') plt.title(\'Fruit Price Across Months\') plt.ylabel(\'Price ()\') plt.savefig(\'bar_plot_price.png\') plt.close() return df_long"},{"question":"**Objective:** Implement a Python class `CellManager` that mimics the behavior of Python cell objects using the provided C API functions simulated in Python. This class will help assess your understanding of closures and multi-scope variable referencing in Python. **Description:** You need to implement a class `CellManager` with the following functionalities: 1. **Initialization**: Initialize an empty cell. 2. **Create Cell**: Method to create a new cell object containing a value. 3. **Get Cell Value**: Method to retrieve the value from the cell. 4. **Set Cell Value**: Method to set a new value to the cell. 5. **Check Cell**: Method to verify if an object is a cell object. **Details:** - Implement the class with the following methods: ```python class CellManager: def __init__(self): # Initialize an empty cell def create_cell(self, value): # Create and return a new cell object containing the value def get_cell_value(self, cell): # Return the contents of the cell def set_cell_value(self, cell, value): # Set the contents of the cell object to the value def check_cell(self, cell): # Return True if the object is a cell object, otherwise False ``` - **Constraints**: - The cell object can initially contain any data type. - `check_cell` should ensure that the object is an instance of the custom cell class. - `get_cell_value` should handle the case when a cell is `NULL`. - **Input/Output**: - `create_cell(value:str) -> dict`: Creates and returns a new cell object. - `get_cell_value(cell: dict) -> str`: Returns the value contained in the cell. - `set_cell_value(cell: dict, value: str) -> None`: Sets the given value to the cell. - `check_cell(cell: dict) -> bool`: Checks if the given object is a cell object. **Functionality Test**: ```python cell_manager = CellManager() # Create a new cell cell = cell_manager.create_cell(\\"Python\\") assert cell_manager.get_cell_value(cell) == \\"Python\\", \\"Test Case 1 Failed\\" # Set a new value to the cell cell_manager.set_cell_value(cell, \\"Programming\\") assert cell_manager.get_cell_value(cell) == \\"Programming\\", \\"Test Case 2 Failed\\" # Check if an object is a cell assert cell_manager.check_cell(cell) == True, \\"Test Case 3 Failed\\" assert cell_manager.check_cell(\\"I am not a cell\\") == False, \\"Test Case 4 Failed\\" print(\\"All test cases pass\\") ``` Implement the class and methods inside it to pass all the assertions provided in the functionality test.","solution":"class Cell: A simple cell class that behaves like a container for any value. def __init__(self, value=None): self.value = value class CellManager: def __init__(self): pass def create_cell(self, value): Create and return a new cell object containing the value. return Cell(value) def get_cell_value(self, cell): Return the contents of the cell. if isinstance(cell, Cell): return cell.value return None def set_cell_value(self, cell, value): Set the contents of the cell object to the value. if isinstance(cell, Cell): cell.value = value def check_cell(self, cell): Return True if the object is a cell object, otherwise False. return isinstance(cell, Cell)"},{"question":"# Question: Data Compression and Integrity Check using zlib You are given a large dataset in the form of a bytes object. Your task is to write a Python function that: 1. **Compresses** the given data using `zlib` with different compression levels. 2. **Calculates a checksum** for the compressed data to ensure data integrity using Adler-32 and CRC32. 3. **Decompresses** the data back to its original form and verifies the integrity by recalculating the checksums. Implement the function `compress_and_verify(data: bytes) -> dict` that performs the following steps: Input: - `data`: A bytes object containing the data to be compressed. Output: - A dictionary with the following keys and values: - `\'compression_level_n\'`: A dictionary for each compression level ( n ) (from 0 to 9) containing: - `\'compressed_data\'`: The compressed bytes object. - `\'adler32_checksum\'`: The Adler-32 checksum of the compressed data. - `\'crc32_checksum\'`: The CRC32 checksum of the compressed data. - `\'decompressed_data\'`: The decompressed data. - `\'is_decompression_valid\'`: A boolean indicating whether the decompressed data matches the original input data. Constraints: - Use a loop to iterate through compression levels from 0 to 9. - Ensure the integrity of the decompressed data by comparing it with the original data. Example: ```python data = b\'Hello, this is a test data for compression using zlib in Python310.\' result = compress_and_verify(data) for level, info in result.items(): print(f\\"Compression level: {level}\\") print(f\\"Is decompression valid: {info[\'is_decompression_valid\']}\\") ``` **Note:** The function should handle any kind of bytes input and provide a verification for the integrity of compressed and decompressed data.","solution":"import zlib def compress_and_verify(data: bytes) -> dict: result = {} for level in range(10): compressed_data = zlib.compress(data, level) adler32_checksum = zlib.adler32(compressed_data) crc32_checksum = zlib.crc32(compressed_data) decompressed_data = zlib.decompress(compressed_data) is_decompression_valid = decompressed_data == data result[f\'compression_level_{level}\'] = { \'compressed_data\': compressed_data, \'adler32_checksum\': adler32_checksum, \'crc32_checksum\': crc32_checksum, \'decompressed_data\': decompressed_data, \'is_decompression_valid\': is_decompression_valid } return result"},{"question":"# Question: Implement a Custom Task Manager using `collections.deque` and `collections.ChainMap` You are tasked with creating a custom task manager that handles several features using `collections.deque` and `collections.ChainMap`. The task manager should support the following functionality: - Add a task to the queue - Complete the most recent task - View all pending tasks - Create a new context (sub-task queue) for specialized tasks without affecting the main task list - Merge all contexts back into the main task queue # Requirements: 1. **Add Task to Queue**: - Method: `add_task(context, task)` - Adds a task to the specified context. If no context is specified, adds to the main context. 2. **Complete Most Recent Task**: - Method: `complete_task(context)` - Removes and returns the most recent task from the specified context. If no context is specified, completes a task from the main context. 3. **View All Pending Tasks**: - Method: `view_tasks(context)` - Returns a list of all tasks in the specified context, in order of their addition. If no context is specified, returns tasks from the main context. 4. **Create a New Context**: - Method: `new_context(context_name)` - Creates a new sub-context under the given name. Sub-contexts allow for managing specialized task queues without affecting the main tasks. 5. **Merge Contexts**: - Method: `merge_contexts()` - Merges all tasks from sub-contexts back into the main task queue. # Example Usage: ```python tm = TaskManager() tm.add_task(None, \\"Write report\\") tm.add_task(None, \\"Email client\\") tm.new_context(\\"Urgent\\") tm.add_task(\\"Urgent\\", \\"Fix bug\\") tm.add_task(\\"Urgent\\", \\"Security patch\\") print(tm.view_tasks(None)) # Output: [\'Write report\', \'Email client\'] print(tm.view_tasks(\\"Urgent\\")) # Output: [\'Fix bug\', \'Security patch\'] tm.complete_task(\\"Urgent\\") # Output: \'Fix bug\' print(tm.view_tasks(\\"Urgent\\")) # Output: [\'Security patch\'] tm.merge_contexts() print(tm.view_tasks(None)) # Output: [\'Write report\', \'Email client\', \'Security patch\'] ``` # Constraints: - You must use `collections.deque` to implement the task queues. - You must use `collections.ChainMap` to manage the contexts. - Ensure the efficiency of your implementation, considering the possible frequent operations on the task manager. # Implementation Details: ```python from collections import deque, ChainMap class TaskManager: def __init__(self): self.main_context = deque() self.contexts = ChainMap({\\"main\\": self.main_context}) def add_task(self, context, task): if context is None: context = \\"main\\" self.contexts[context].append(task) def complete_task(self, context): if context is None: context = \\"main\\" if self.contexts[context]: return self.contexts[context].popleft() else: return \\"No tasks in the context\\" def view_tasks(self, context): if context is None: context = \\"main\\" return list(self.contexts[context]) def new_context(self, context_name): self.contexts = self.contexts.new_child({context_name: deque()}) def merge_contexts(self): for key in self.contexts.maps: if key != \\"main\\": self.main_context.extend(self.contexts.maps[key]) self.contexts = ChainMap({\\"main\\": self.main_context}) # Testing the implementation with the provided usage example tm = TaskManager() tm.add_task(None, \\"Write report\\") tm.add_task(None, \\"Email client\\") tm.new_context(\\"Urgent\\") tm.add_task(\\"Urgent\\", \\"Fix bug\\") tm.add_task(\\"Urgent\\", \\"Security patch\\") print(tm.view_tasks(None)) # Output: [\'Write report\', \'Email client\'] print(tm.view_tasks(\\"Urgent\\")) # Output: [\'Fix bug\', \'Security patch\'] print(tm.complete_task(\\"Urgent\\")) # Output: \'Fix bug\' print(tm.view_tasks(\\"Urgent\\")) # Output: [\'Security patch\'] tm.merge_contexts() print(tm.view_tasks(None)) # Output: [\'Write report\', \'Email client\', \'Security patch\'] ``` # Notes: - Be sure to properly handle cases where the context does not exist or there are no tasks to complete. - Efficiency in handling the task queues and context management is paramount, considering the operations on the task manager.","solution":"from collections import deque, ChainMap class TaskManager: def __init__(self): self.main_context = deque() self.contexts = ChainMap({\\"main\\": self.main_context}) def add_task(self, context, task): if context is None: context = \\"main\\" if context not in self.contexts: raise ValueError(\\"Context does not exist\\") self.contexts[context].append(task) def complete_task(self, context): if context is None: context = \\"main\\" if context not in self.contexts: raise ValueError(\\"Context does not exist\\") if self.contexts[context]: return self.contexts[context].popleft() else: return None def view_tasks(self, context): if context is None: context = \\"main\\" if context not in self.contexts: raise ValueError(\\"Context does not exist\\") return list(self.contexts[context]) def new_context(self, context_name): if context_name in self.contexts: raise ValueError(\\"Context already exists\\") self.contexts = self.contexts.new_child({context_name: deque()}) def merge_contexts(self): merged_tasks = [] for context_name, task_queue in self.contexts.items(): if context_name != \\"main\\": merged_tasks.extend(task_queue) self.main_context.extend(merged_tasks) self.contexts = ChainMap({\\"main\\": self.main_context})"},{"question":"# Custom Iterator Implementation Challenge **Objective:** Create a custom iterator in Python that aims to iterate over a list of numbers and yield only the even numbers. Additionally, demonstrate the use of async iteration with this custom iterator. **Details:** 1. **Iterator Class**: - Implement a class `EvenNumbers` that: - Accepts a list of integers during initialization. - Implements the iterator protocol (`__iter__`, `__next__`). 2. **Async Iterator Class**: - Implement a class `AsyncEvenNumbers` that: - Accepts a list of integers during initialization. - Implements the asynchronous iterator protocol (`__aiter__`, `__anext__`). 3. **Function Usage**: - Demonstrate the use of `EvenNumbers` in a standard loop. - Demonstrate the use of `AsyncEvenNumbers` in an asynchronous context (e.g., using `asyncio` module). **Requirements:** - The iterator should handle invalid inputs gracefully (e.g., non-list inputs). - Avoid using built-in or third-party utilities for filtering; the filtering should be part of the iteration logic. - Write appropriate docstrings for your classes and methods. **Constraints:** - The list can have a maximum length of 1000 integers. - The integers will be within the range of -10^6 to 10^6. **Input Format:** - A list of integers. **Output Format:** - A demonstration script that showcases: 1. A loop iterating over `EvenNumbers`. 2. An asynchronous loop iterating over `AsyncEvenNumbers`. **Example:** ```python # Sample Class Usage numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Example for EvenNumbers even_numbers = EvenNumbers(numbers) for num in even_numbers: print(num) # Should print: 2, 4, 6, 8, 10 # Example for AsyncEvenNumbers async def main(): async_even_numbers = AsyncEvenNumbers(numbers) async for num in async_even_numbers: print(num) # Should print: 2, 4, 6, 8, 10 # Don\'t forget to run the asynchronous part appropriately import asyncio asyncio.run(main()) ``` **Assessment Criteria:** - Correct implementation of `EvenNumbers` and `AsyncEvenNumbers`. - Proper handling of edge cases and invalid inputs. - Clear demonstration script showcasing both iterator usages. - Code readability and documentation.","solution":"class EvenNumbers: Iterator class to iterate over a list of numbers and yield only the even numbers. def __init__(self, numbers): if not isinstance(numbers, list): raise ValueError(\\"Input must be a list\\") self.numbers = [num for num in numbers if isinstance(num, int)] self.index = 0 def __iter__(self): return self def __next__(self): while self.index < len(self.numbers): num = self.numbers[self.index] self.index += 1 if num % 2 == 0: return num raise StopIteration import asyncio class AsyncEvenNumbers: Asynchronous iterator class to iterate over a list of numbers and yield only the even numbers. def __init__(self, numbers): if not isinstance(numbers, list): raise ValueError(\\"Input must be a list\\") self.numbers = [num for num in numbers if isinstance(num, int)] self.index = 0 def __aiter__(self): return self async def __anext__(self): await asyncio.sleep(0) # Dummy awaitable to illustrate async nature while self.index < len(self.numbers): num = self.numbers[self.index] self.index += 1 if num % 2 == 0: return num raise StopAsyncIteration # Demonstration numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Example for EvenNumbers print(\\"Synchronous Iteration:\\") even_numbers = EvenNumbers(numbers) for num in even_numbers: print(num) # Should print: 2, 4, 6, 8, 10 # Example for AsyncEvenNumbers print(\\"nAsynchronous Iteration:\\") async def main(): async_even_numbers = AsyncEvenNumbers(numbers) async for num in async_even_numbers: print(num) # Should print: 2, 4, 6, 8, 10 # Run main async function asyncio.run(main())"},{"question":"**Coding Assessment Question:** # Problem Statement You are provided with a CSV file named `sales_data.csv` containing sales records of a company. Your task is to write a function `analyze_sales_data` that reads in the dataset, performs data cleaning and transformation operations, and then returns several insights from the data. # Dataset Description The `sales_data.csv` file contains the following columns: - `Date`: The date of the sale (string in the format \'YYYY-MM-DD\') - `Store`: The store identifier (string) - `Product_ID`: The identifier for the product sold (string) - `Quantity`: The number of units sold (integer) - `Revenue`: The total revenue from the sale (float) - `Customer_ID`: The unique identifier for the customer (string, may contain missing values) # Function Requirements Write a function `analyze_sales_data(filename: str) -> dict` that: 1. Reads the data from `sales_data.csv` into a pandas DataFrame. 2. Cleans the data by: - Converting the `Date` column to datetime format. - Filling any missing values in the `Customer_ID` column with the string `\'unknown\'`. - Correcting any negative values in the `Revenue` column to `0`. 3. Computes the following insights: - The total revenue for each store. - The average quantity of products sold per day. - The number of unique customers. - The top 5 products by total revenue. # Expected Output The function should return a dictionary with the following structure: ```python { \'total_revenue_per_store\': {\'store_1\': revenue_1, \'store_2\': revenue_2, ...}, \'average_quantity_per_day\': average_quantity, \'unique_customers\': num_customers, \'top_5_products\': [(\'product_1\', revenue_1), (\'product_2\', revenue_2), ...] } ``` # Constraints - The dataset can be large, so ensure your solution is efficient in terms of time and space complexity. - You can assume that the `sales_data.csv` file is always correctly formatted with the columns as described. # Example Usage ```python result = analyze_sales_data(\'path/to/sales_data.csv\') print(result) # Output: # { # \'total_revenue_per_store\': {\'store_A\': 56700.0, \'store_B\': 67800.0, ...}, # \'average_quantity_per_day\': 350.5, # \'unique_customers\': 1500, # \'top_5_products\': [(\'product_x\', 12000.0), (\'product_y\', 11500.0), ...] # } ``` Ensure your code is well-documented and follows best coding practices.","solution":"import pandas as pd def analyze_sales_data(filename: str) -> dict: # Read the data into a DataFrame df = pd.read_csv(filename) # Clean the data df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Customer_ID\'].fillna(\'unknown\', inplace=True) df[\'Revenue\'] = df[\'Revenue\'].apply(lambda x: max(0, x)) # Calculate the total revenue for each store total_revenue_per_store = df.groupby(\'Store\')[\'Revenue\'].sum().to_dict() # Calculate the average quantity of products sold per day total_days = (df[\'Date\'].max() - df[\'Date\'].min()).days + 1 total_quantity = df[\'Quantity\'].sum() average_quantity_per_day = total_quantity / total_days # Calculate the number of unique customers unique_customers = df[\'Customer_ID\'].nunique() # Identify the top 5 products by total revenue top_5_products = df.groupby(\'Product_ID\')[\'Revenue\'].sum().nlargest(5).items() top_5_products = [(product, revenue) for product, revenue in top_5_products] # Return the results as a dictionary return { \'total_revenue_per_store\': total_revenue_per_store, \'average_quantity_per_day\': average_quantity_per_day, \'unique_customers\': unique_customers, \'top_5_products\': top_5_products }"},{"question":"# Seaborn Plot Styling Question Objective Your task is to demonstrate your understanding of Seaborn\'s styling functionality by creating and customizing multiple plots with different styles and parameters. Instructions 1. **Import Seaborn and Matplotlib**: - Ensure you import the necessary libraries. 2. **Create a Bar Plot**: - Set the style to `\\"whitegrid\\"`. - Create a bar plot with the following data: - `x = [\\"Group 1\\", \\"Group 2\\", \\"Group 3\\"]` - `y = [10, 25, 15]` 3. **Create a Line Plot**: - Customize the style to `\\"darkgrid\\"` with additional parameters: `{\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"}` - Create a line plot with the following data: - `x = [\\"Point A\\", \\"Point B\\", \\"Point C\\", \\"Point D\\"]` - `y = [5, 15, 10, 20]` 4. **Create a Joint Plot**: - Use a different style of your choice. - Create a joint plot to display a bivariate distribution, using random data. You may use numpy to generate random data. Expected Outputs 1. A bar plot with the specified style and data. 2. A line plot with the customized style and data. 3. A joint plot with a different chosen style and random data. Example Code Usage Your final submission should contain the implementation in the following format: ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # 1. Bar Plot sns.set_style(\\"whitegrid\\") sns.barplot(x=[\\"Group 1\\", \\"Group 2\\", \\"Group 3\\"], y=[10, 25, 15]) # 2. Line Plot sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"}) sns.lineplot(x=[\\"Point A\\", \\"Point B\\", \\"Point C\\", \\"Point D\\"], y=[5, 15, 10, 20]) # 3. Joint Plot sns.set_style(\\"ticks\\") data_x = np.random.randn(100) data_y = np.random.randn(100) sns.jointplot(x=data_x, y=data_y, kind=\'scatter\') plt.show() ``` Constraints - Ensure the styles are correctly applied to the respective plots. - Use clear comments to indicate the style settings for each plot. - Follow the specified data for bar and line plots exactly. Happy plotting!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_plots(): # 1. Bar Plot with \\"whitegrid\\" style sns.set_style(\\"whitegrid\\") plt.figure(figsize=(6, 4)) sns.barplot(x=[\\"Group 1\\", \\"Group 2\\", \\"Group 3\\"], y=[10, 25, 15]) plt.title(\'Bar Plot\') plt.show() # 2. Line Plot with customized \\"darkgrid\\" style sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"}) plt.figure(figsize=(6, 4)) sns.lineplot(x=[\\"Point A\\", \\"Point B\\", \\"Point C\\", \\"Point D\\"], y=[5, 15, 10, 20]) plt.title(\'Line Plot\') plt.show() # 3. Joint Plot with \\"ticks\\" style sns.set_style(\\"ticks\\") data_x = np.random.randn(100) data_y = np.random.randn(100) sns.jointplot(x=data_x, y=data_y, kind=\'scatter\') plt.title(\'Joint Plot\') # plt.title does not affect jointplot but added for context plt.show() def get_current_style(): return sns.axes_style() def reset_default_style(): sns.set_style(\\"darkgrid\\")"},{"question":"# Advanced Seaborn Plotting with Penguins Dataset **Objective:** Demonstrate your understanding of seaborn\'s data visualization capabilities by generating advanced plots. **Context:** The `penguins` dataset contains measurements for different penguin species observed on various islands. **Dataset Description:** - `species`: Species of penguin (e.g., Adelie, Chinstrap, Gentoo) - `island`: Island where the penguin was observed (e.g., Biscoe, Dream, Torgersen) - `bill_length_mm`: Bill length in millimeters - `bill_depth_mm`: Bill depth in millimeters - `flipper_length_mm`: Flipper length in millimeters - `body_mass_g`: Body mass in grams - `sex`: Sex of the penguin (Male, Female) **Task:** Create a function `advanced_penguin_plots` that performs the following operations: 1. **Bar Plot by Island:** Generate a bar plot showing the count of penguins observed on each island. 2. **Faceted Histogram:** Create a faceted histogram of the `flipper_length_mm` for each `island`, displaying data as normalized proportions with independent normalization for each island. 3. **Area Plot by Sex:** Generate an area plot of the `bill_length_mm` colored by `sex` to show the distribution of bill lengths. Normalize the histogram proportions to allow for better comparison of shapes. 4. **Stacked Bar Plot:** Create a stacked bar plot displaying the count of penguins by `sex` within each `island`. **Requirements:** - Use the seaborn `objects` API. - Import the necessary libraries and load the `penguins` dataset within the function. - Customize the bin granularity of the histograms to enhance the clarity of visualization. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def advanced_penguin_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Task 1: Bar Plot by Island plt.figure() so.Plot(penguins, \\"island\\").add(so.Bar(), so.Hist()).show() # Task 2: Faceted Histogram plt.figure() p2 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p2.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)).show() # Task 3: Area Plot by Sex plt.figure() p3 = so.Plot(penguins, \\"bill_length_mm\\", color=\\"sex\\") p3.add(so.Area(), so.Hist(stat=\\"proportion\\")).show() # Task 4: Stacked Bar Plot plt.figure() p4 = so.Plot(penguins, \\"island\\", color=\\"sex\\") p4.add(so.Bars(), so.Hist(), so.Stack()).show() ``` After implementing the function, call `advanced_penguin_plots()` to generate the plots. Ensure that all plots are clearly labeled and legible. This task will assess your ability to use seaborn for creating complex visualizations and handle various plotting requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def advanced_penguin_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Task 1: Bar Plot by Island plt.figure() so.Plot(penguins, \\"island\\").add(so.Bar(), so.Hist()).show() # Task 2: Faceted Histogram plt.figure() p2 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p2.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)).show() # Task 3: Area Plot by Sex plt.figure() p3 = so.Plot(penguins, \\"bill_length_mm\\", color=\\"sex\\") p3.add(so.Area(), so.Hist(stat=\\"proportion\\")).show() # Task 4: Stacked Bar Plot plt.figure() p4 = so.Plot(penguins, \\"island\\", color=\\"sex\\") p4.add(so.Bars(), so.Hist(), so.Stack()).show() # Call the function to generate the plots advanced_penguin_plots()"},{"question":"# Novelty and Outlier Detection with Scikit-Learn Problem Statement You are provided with a dataset `data.csv` consisting of several features. Your task is to write a Python function using scikit-learn to implement both novelty and outlier detection. Specifically, you will use the `IsolationForest` method for this purpose. Objectives: 1. Implement a function `detect_anomalies` that takes in the file path of the dataset and returns the indices of detected anomalies. 2. The function should: - Fit an `IsolationForest` model to detect outliers and novelties. - Use easy-to-understand parameter settings (e.g., `contamination`). - Perform predictions on the data to classify observations as inliers or outliers/novelties. Function Signature ```python import numpy as np import pandas as pd from sklearn.ensemble import IsolationForest def detect_anomalies(file_path: str, contamination: float) -> np.ndarray: Detect anomalies in the dataset using IsolationForest. Parameters: file_path (str): Path to the CSV file containing the dataset. contamination (float): The proportion of outliers in the data set (such as 0.1 for 10% outliers). Returns: np.ndarray: An array of indices of detected anomalies. pass ``` Input - `file_path`: A string representing the path to the CSV file containing the dataset. You can assume the file has no missing values. - `contamination`: A float representing the proportion of anomalies in the dataset. Output - An array of indices of the detected anomalies, as identified by the `IsolationForest`. Constraints - The dataset will have at least 100 rows and 5 columns. - The `contamination` value will be between `0.01` and `0.5`. Example ```python # Assuming data.csv contains data with some outliers. anomalies = detect_anomalies(\'data.csv\', 0.1) print(anomalies) ``` The output should be the indices of the rows identified as outliers. Notes - Use `IsolationForest` from `sklearn.ensemble`. - Fit the model on the entire dataset. - Use the `predict` method to classify each observation. - Inliers should have a label of 1 and outliers should have a label of -1. - The function should capture and return the indices where the label is -1. Make sure your solution is efficient and leverages the functionalities of the `IsolationForest`.","solution":"import numpy as np import pandas as pd from sklearn.ensemble import IsolationForest def detect_anomalies(file_path: str, contamination: float) -> np.ndarray: Detect anomalies in the dataset using IsolationForest. Parameters: file_path (str): Path to the CSV file containing the dataset. contamination (float): The proportion of outliers in the data set (such as 0.1 for 10% outliers). Returns: np.ndarray: An array of indices of detected anomalies. # Load the dataset data = pd.read_csv(file_path) # Initialize IsolationForest model = IsolationForest(contamination=contamination, random_state=42) # Fit the model model.fit(data) # Predict the anomalies predictions = model.predict(data) # Identify the indices of anomalies (where prediction is -1) anomaly_indices = np.where(predictions == -1)[0] return anomaly_indices"},{"question":"# Advanced Coding Assessment Question Objective Implement a Python C extension function that parses a tuple containing a string, an optional integer, and a buffer, performs some operations, and returns a constructed result in a specific format. Requirements 1. **Function Definition**: - Function in Python: `parse_and_build` - C extension function signature: `parse_and_build(PyObject *self, PyObject *args, PyObject *kwargs)` 2. **Argument Parsing**: - The function should accept a string (`str`), an optional integer (`int`), and a buffer (any bytes-like object). - Handle the following format string in C: `\\"s|iy*\\"` - `s` - string - `|i` - optional integer - `y*` - buffer 3. **Validations and Operations**: - Check if the buffer contains at least 5 bytes. If not, raise a `ValueError`. - If the integer is present, multiply the integer with the length of the string. - Return a new Python tuple with the following format: `(modified_string, result_integer, buffer_length)`. - `modified_string` should be the original string converted to uppercase. - `result_integer` should be the calculated value (integer * length of the string) or `None` if the integer was not provided. - `buffer_length` should be the length of the provided buffer. 4. **Return Value Construction**: - Use `Py_BuildValue()` to construct the return value in the specified format. Constraints - The buffer\'s length must be handled carefully to avoid buffer overflows. - Ensure memory allocated for buffers is properly released using `PyBuffer_Release()`. Example Usage ```python # Python usage # Assuming the module is named `mymodule` and the function is defined as `parse_and_build` import mymodule result = mymodule.parse_and_build(\\"hello\\", 3, b\'123456\') print(result) # Output should be (\\"HELLO\\", 15, 6) ``` Skeleton Code for C Extension Provide a template for students to fill in with appropriate C functions and macros, focusing on argument parsing, validation, and value construction. ```c #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* parse_and_build(PyObject *self, PyObject *args, PyObject *kwargs) { const char *input_str; int optional_int = -1; // Default value indicating absence Py_buffer buffer; static char *kwlist[] = {\\"str\\", \\"int\\", \\"buffer\\", NULL}; if (!PyArg_ParseTupleAndKeywords(args, kwargs, \\"s|iy*\\", kwlist, &input_str, &optional_int, &buffer)) { return NULL; // Argument Parsing Failure } // Ensure buffer length is at least 5 bytes if (buffer.len < 5) { PyBuffer_Release(&buffer); PyErr_SetString(PyExc_ValueError, \\"Buffer must be at least 5 bytes long\\"); return NULL; } // Perform operations //...... // Construct the return value //...... // Release the buffer PyBuffer_Release(&buffer); // Returning a placeholder None for now Py_RETURN_NONE; } // Module method table and initialization static PyMethodDef MyMethods[] = { {\\"parse_and_build\\", (PyCFunction)parse_and_build, METH_VARARGS | METH_KEYWORDS, \\"Parse arguments and build a result\\"}, {NULL, NULL, 0, NULL} // sentinel }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, // Module documentation -1, // Size of per-interpreter state of the module, or -1 if the module keeps state in global variables. MyMethods }; PyMODINIT_FUNC PyInit_mymodule(void) { return PyModule_Create(&mymodule); } ```","solution":"# This is the pure Python implementation of the requirements. This is to be understood # alongside the provided C extension template which needs to be filled in C language. def parse_and_build(input_str, optional_int=None, buffer=None): Parses a tuple containing a string, an optional integer, and a buffer, performs some operations, and returns a constructed result in a specific format. :param input_str: str, a required string :param optional_int: int, an optional integer :param buffer: bytes-like object, a required buffer :return: tuple in the format of (modified_string, result_integer, buffer_length) # Ensure buffer length is at least 5 bytes if len(buffer) < 5: raise ValueError(\\"Buffer must be at least 5 bytes long\\") # Convert string to uppercase modified_string = input_str.upper() # If the integer is present, multiply it with the length of the string if optional_int is not None: result_integer = optional_int * len(input_str) else: result_integer = None # Get buffer length buffer_length = len(buffer) return modified_string, result_integer, buffer_length"},{"question":"# Clustering Assessment **Objective:** Implement a clustering algorithm using scikit-learn to analyze and interpret patterns in a given dataset. You will be required to utilize scikit\'s functionality to cluster data points, visualize the results, and evaluate the clustering performance. **Problem Statement:** Create a function `perform_clustering(data: np.ndarray, n_clusters: int, algorithm: str) -> Tuple[np.ndarray, float]:` that takes the following inputs: - `data` (np.ndarray): A 2D numpy array where each row represents a data point and each column represents a feature. - `n_clusters` (int): The number of clusters to form. - `algorithm` (str): The clustering algorithm to use. It can take one of the following values: `\'kmeans\'`, `\'agglomerative\'`, or `\'dbscan\'`. The function should perform the following tasks: 1. Cluster the data using the specified algorithm. - For `kmeans`, use `KMeans` from `sklearn.cluster`. - For `agglomerative`, use `AgglomerativeClustering` from `sklearn.cluster`. - For `dbscan`, use `DBSCAN` from `sklearn.cluster`. 2. Return the following: - A numpy array of cluster labels assigned to each data point. - The silhouette score of the clustering result. **Constraints:** - You are to handle only the specified algorithms (\'kmeans\', \'agglomerative\', \'dbscan\'). - Assume that the input data is always valid and does not contain missing values. - For `dbscan`, choose `eps=0.5` and `min_samples=5`. **Expected Performance:** The function should run efficiently for a dataset with up to 10,000 data points and 50 features. **Example:** ```python import numpy as np data = np.random.rand(100, 2) # 100 data points with 2 features each result_labels, silhouette = perform_clustering(data, 3, \'kmeans\') print(result_labels) print(f\\"Silhouette Score: {silhouette}\\") ``` Note: Ensure you import the necessary classes and functions from `sklearn` and other required modules. **Evaluation Criteria:** - Correct implementation of clustering for each specified algorithm. - Accurate computation of the silhouette score. - Efficiency and clarity of code.","solution":"from typing import Tuple import numpy as np from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import silhouette_score def perform_clustering(data: np.ndarray, n_clusters: int, algorithm: str) -> Tuple[np.ndarray, float]: Perform clustering on the given data using the specified algorithm and return the cluster labels and silhouette score. Parameters: data (np.ndarray): 2D numpy array with data points and features. n_clusters (int): The number of clusters to form. algorithm (str): The clustering algorithm to use (\'kmeans\', \'agglomerative\', \'dbscan\'). Returns: Tuple[np.ndarray, float]: Cluster labels for each point and the silhouette score. if algorithm not in [\'kmeans\', \'agglomerative\', \'dbscan\']: raise ValueError(\\"Invalid algorithm specified. Supported algorithms: \'kmeans\', \'agglomerative\', \'dbscan\'.\\") if algorithm == \'kmeans\': model = KMeans(n_clusters=n_clusters) elif algorithm == \'agglomerative\': model = AgglomerativeClustering(n_clusters=n_clusters) elif algorithm == \'dbscan\': model = DBSCAN(eps=0.5, min_samples=5) cluster_labels = model.fit_predict(data) # Handle the case where all points are assigned to a single cluster (prevents silhouette score computation error) if len(set(cluster_labels)) > 1: silhouette = silhouette_score(data, cluster_labels) else: silhouette = -1.0 # Define a silhouette score for the edge case where only one cluster is found return cluster_labels, silhouette"},{"question":"**Question:** # System Information Summary You are tasked with writing a function that collects various pieces of system information using the `platform` module and returns a summary report as a dictionary. This summary should include the following details: - `architecture`: A string that indicates the bit architecture and linkage format (e.g., `\'64bit, ELF\'`). - `machine`: The machine type (e.g., `\'AMD64\'`). - `node`: The computer\'s network name. - `platform`: A string identifying the underlying platform. - `processor`: The real processor name. - `python_build`: A string that concats Python build number and date (e.g., `\'default, Mar 1 2022 13:35:58\'`). - `python_version`: The Python version as a string (e.g., `\'3.10.2\'`). - `system`: The system/OS name (e.g., `\'Linux\'`). - `release`: The system\'s release (e.g., `\'5.4.0-66-generic\'`). Your function should be named `system_info_summary` and it should not take any parameters. The function should return the summary report as a dictionary with keys as listed above. Example: ```python def system_info_summary(): # Your implementation here # Example usage and output summary = system_info_summary() print(summary) ``` The output should be a dictionary that looks something like this: ```python { \'architecture\': \'64bit, ELF\', \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-5.4.0-66-generic-x86_64-with-glibc2.29\', \'processor\': \'x86_64\', \'python_build\': \'default, Mar 1 2022 13:35:58\', \'python_version\': \'3.10.2\', \'system\': \'Linux\', \'release\': \'5.4.0-66-generic\' } ``` # Constraints: - You should handle cases where any of the platform information cannot be determined (i.e., the function returns an empty string in such cases). # Notes: - Make sure to use the appropriate functions from the `platform` module to gather each piece of information. - Focus on creating a robust solution that handles edge cases gracefully. # Performance: - The function should be efficient and complete within a reasonable timeframe, even on systems with slow I/O operations.","solution":"import platform def system_info_summary(): Collects various pieces of system information using the `platform` module and returns a summary report as a dictionary. return { \'architecture\': \', \'.join(platform.architecture()), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': \', \'.join(platform.python_build()), \'python_version\': platform.python_version(), \'system\': platform.system(), \'release\': platform.release() }"},{"question":"# Multithreaded Task Scheduler using `_thread` Module **Objective**: Design a multithreaded task scheduler using Python\'s low-level `_thread` module. The scheduler will manage and execute functions in different threads, ensuring that no two functions are executed at the same time using a lock object for synchronization. # Requirements: 1. **Function Implementation**: - Implement a class `TaskScheduler` with the following methods: - `def __init__(self)`: Initialize the scheduler with a lock object. - `def schedule_task(self, func, args)`: Schedule a new task to be executed in a separate thread. ```python def __init__(self): # Initialize the lock object here. pass def schedule_task(self, func, args): Schedule a new task in a separate thread. Args: - func: The function to be executed. - args: A tuple containing the positional arguments for the function. The function should be executed with the lock acquired. pass ``` 2. **Thread Management**: - Ensure that any function scheduled by `schedule_task` is executed with the lock acquired, to prevent other tasks from running simultaneously. # Example Execution: ```python import _thread import time # Define some sample tasks def task1(): print(\\"Task 1 starting...\\") time.sleep(2) # Simulate long-running task print(\\"Task 1 completed.\\") def task2(): print(\\"Task 2 starting...\\") time.sleep(1) print(\\"Task 2 completed.\\") # Instantiate the TaskScheduler scheduler = TaskScheduler() # Schedule tasks scheduler.schedule_task(task1, ()) scheduler.schedule_task(task2, ()) # Allow some time for tasks to run time.sleep(5) ``` # Expected Output: ``` Task 1 starting... Task 1 completed. Task 2 starting... Task 2 completed. ``` **Constraints**: - The functions to be scheduled do not accept keyword arguments; only positional arguments are allowed. - Ensure that the implementation correctly uses `_thread.allocate_lock` and the associated lock methods. - Handle any exceptions that might arise during the execution of the threads. Good Luck!","solution":"import _thread import time class TaskScheduler: def __init__(self): self.lock = _thread.allocate_lock() def schedule_task(self, func, args): Schedule a new task in a separate thread. Args: - func: The function to be executed. - args: A tuple containing the positional arguments for the function. The function should be executed with the lock acquired. _thread.start_new_thread(self._run_task, (func, args)) def _run_task(self, func, args): with self.lock: func(*args)"},{"question":"**Python Object Memory Management** You are tasked with designing and implementing a custom memory management system in high-level Python. This system will demonstrate fundamental and advanced concepts similar to those used in Python\'s internal memory management system. # Task 1. **CustomObject and Memory Management:** - Create a class `CustomObject` which simulates a Python object with the following properties: - `data`: A variable to hold any kind of data. - `size`: An integer that indicates the size of the data. - Implement a simple memory management system: - `create_object(data)`: Allocates memory for an object of a certain size, initializes it with data, and returns the object. - `initialize_object(obj, data)`: Initializes an already allocated object with new data. - `delete_object(obj)`: Simulates deallocation of the object\'s memory. 2. **Usage:** - Demonstrate the allocation, initialization, and deletion of an object using the `CustomObject` class. # Implementation Details: 1. **CustomObject Class:** - Constructor: Should initialize `data` to `None` and `size` to `0`. 2. **create_object Function:** - Parameters: `data` - Operations: - Create a new instance of `CustomObject`. - Assign the `data` to its `data` attribute. - Set `size` to the length of `data`. 3. **initialize_object Function:** - Parameters: `obj`, `data` - Operations: - Assign the `data` to the object\'s `data` attribute. - Update `size` to match the length of new `data`. 4. **delete_object Function:** - Parameters: `obj` - Operations: - Clear the `data` attribute. - Reset `size` to `0`. - Print a confirmation message indicating the object is deleted. # Example Usage: ```python class CustomObject: def __init__(self): self.data = None self.size = 0 def create_object(data): new_obj = CustomObject() new_obj.data = data new_obj.size = len(data) return new_obj def initialize_object(obj, data): obj.data = data obj.size = len(data) def delete_object(obj): obj.data = None obj.size = 0 print(\\"Object memory has been deallocated\\") # Example Usage # Creating Object obj = create_object(\\"Hello World\\") print(obj.data, obj.size) # Output: \\"Hello World\\", 11 # Initializing Object initialize_object(obj, \\"New Data\\") print(obj.data, obj.size) # Output: \\"New Data\\", 8 # Deleting Object delete_object(obj) print(obj.data, obj.size) # Output: None, 0 ``` # Constraints: - Ensure that the `data` provided is a string. - Handle any potential errors such as re-initializing or deleting an uninitialized object appropriately. # Performance Requirements: - Ensure efficient memory allocation and deallocation. - The implementation should handle the creation and deletion of multiple objects without leaking memory. # Submission: - Provide your implementation of the `CustomObject` class and the required functions. - Demonstrate their usage with the provided example calls. Good luck!","solution":"class CustomObject: def __init__(self): self.data = None self.size = 0 def create_object(data): new_obj = CustomObject() new_obj.data = data new_obj.size = len(data) return new_obj def initialize_object(obj, data): obj.data = data obj.size = len(data) def delete_object(obj): obj.data = None obj.size = 0 print(\\"Object memory has been deallocated\\") # Example Usage # Creating Object obj = create_object(\\"Hello World\\") print(obj.data, obj.size) # Output: \\"Hello World\\", 11 # Initializing Object initialize_object(obj, \\"New Data\\") print(obj.data, obj.size) # Output: \\"New Data\\", 8 # Deleting Object delete_object(obj) print(obj.data, obj.size) # Output: None, 0"},{"question":"Task You are required to write a Python script that performs the following tasks using the standard library modules discussed: 1. **Working with the file system:** - Create a directory named `archive`. If it already exists, ensure your script can handle this gracefully. 2. **Working with files:** - Inside the `archive` directory, create a file named `data.txt` and write a simple message, e.g., \\"Hello, Python310!\\" into this file. - Compress this `data.txt` file into a `data.zip` archive in the same directory. 3. **Command-line argument processing:** - The script should accept a command-line argument to specify the name of a file (e.g., `python script.py --file data.txt`). - If provided, override the default name `data.txt` with the specified name for the rest of the operations. 4. **Internet access and regular expressions:** - Download the current datetime from `http://worldtimeapi.org/api/timezone/etc/UTC.txt`. - Extract and print the datetime using regular expressions. 5. **Statistical computation:** - Generate a list of 100 random numbers using `random.sample` and calculate the mean and variance using the `statistics` module. - Append these statistical values (mean and variance) to the `data.txt` file. 6. **Error handling:** - Ensure all I/O operations are properly handled using try-except blocks to catch and display user-friendly error messages. 7. **Testing:** - Embed tests in the script using the `doctest` module to validate at least one function used in the script. Requirements - **Input:** - Command line argument for the file name (string). - **Output:** - Print the datetime obtained from `http://worldtimeapi.org/api/timezone/etc/UTC.txt`. - Append the mean and variance of the 100 random numbers to the `data.txt` file. - **Constraints:** - Use only the standard library modules provided in the documentation. - Ensure robust error handling. - Include at least one unit test using `doctest`. Example ```python python script.py --file mydata.txt Datetime obtained: 2022-01-01T01:36:47.689215+00:00 ``` Contents of `archive/mydata.txt` after execution: ``` Hello, Python310! Mean: <calculated_mean> Variance: <calculated_variance> ``` Submission Provide your complete Python script fulfilling the above requirements.","solution":"import os import zipfile from urllib import request import re import random import statistics import argparse import doctest def create_directory(directory_name): try: if not os.path.exists(directory_name): os.makedirs(directory_name) except OSError as e: print(f\\"Error creating directory {directory_name}: {e}\\") def write_message_to_file(file_path, message): try: with open(file_path, \'w\') as file: file.write(message + \\"n\\") except IOError as e: print(f\\"Error writing to file {file_path}: {e}\\") def compress_file(input_file, output_zip): try: with zipfile.ZipFile(output_zip, \'w\') as zipf: zipf.write(input_file, os.path.basename(input_file)) except zipfile.BadZipFile as e: print(f\\"Error creating zip file {output_zip}: {e}\\") def get_current_datetime(): try: response = request.urlopen(\\"http://worldtimeapi.org/api/timezone/etc/UTC.txt\\") data = response.read().decode(\'utf-8\') datetime_match = re.search(r\\"datetime: (.*n)\\", data) if datetime_match: return datetime_match.group(1).strip() except Exception as e: print(f\\"Error retrieving current datetime: {e}\\") def generate_random_numbers_statistics(): numbers = random.sample(range(1, 1000), 100) mean = statistics.mean(numbers) variance = statistics.variance(numbers) return mean, variance def append_statistics_to_file(file_path, mean, variance): try: with open(file_path, \'a\') as file: file.write(f\\"Mean: {mean}n\\") file.write(f\\"Variance: {variance}n\\") except IOError as e: print(f\\"Error appending to file {file_path}: {e}\\") def main(): parser = argparse.ArgumentParser() parser.add_argument(\\"--file\\", type=str, default=\\"data.txt\\", help=\\"Name of the file to process\\") args = parser.parse_args() directory_name = \\"archive\\" file_name = args.file file_path = os.path.join(directory_name, file_name) zip_path = os.path.join(directory_name, \\"data.zip\\") create_directory(directory_name) write_message_to_file(file_path, \\"Hello, Python310!\\") compress_file(file_path, zip_path) datetime = get_current_datetime() if datetime: print(f\\"Datetime obtained: {datetime}\\") mean, variance = generate_random_numbers_statistics() append_statistics_to_file(file_path, mean, variance) import doctest doctest.testmod() if __name__ == \\"__main__\\": main()"},{"question":"# Question: Creating a Python Function Object Manager Implement a class `FunctionObjectManager` in Python that mimics certain behaviors of the C-level Python function object API described in the provided documentation. You will need to implement methods to create new functions and to get and set various attributes of these functions. The class should operate at a higher level, using only Python, and should not directly interact with the C API. Your `FunctionObjectManager` class should include the following methods: 1. **`create_function(name, code_string, globals_dict)`**: Creates and returns a new function with the specified name, code (as a string), and globals dictionary. Use the `exec` function to compile and create the function from the code string. 2. **`get_function_code(function)`**: Returns the code object associated with the function. 3. **`get_function_globals(function)`**: Returns the globals dictionary associated with the function. 4. **`set_function_defaults(function, defaults)`**: Sets the default argument values for the function. `defaults` should be provided as a tuple. 5. **`get_function_defaults(function)`**: Returns the default argument values of the function. 6. **`set_function_annotations(function, annotations)`**: Sets the annotations for the function. `annotations` should be a dictionary. 7. **`get_function_annotations(function)`**: Returns the annotations of the function. Input and Output: - The `create_function` method should accept a `name` (string), `code_string` (string), and `globals_dict` (dictionary), and return a function object. - The getter methods should accept a function object and return the respective attributes. - The setter methods should accept a function object and the value to be set and must update the function object\'s attribute accordingly. Example Usage: ```python # Create a FunctionObjectManager instance manager = FunctionObjectManager() # Create a function code_string = def test_function(x, y): return x + y globals_dict = {} func = manager.create_function(\\"test_function\\", code_string, globals_dict) # Get function code print(manager.get_function_code(func)) # Outputs the code object # Get function globals print(manager.get_function_globals(func)) # Outputs the globals dict # Set and get function defaults manager.set_function_defaults(func, (1, 2)) print(manager.get_function_defaults(func)) # Outputs: (1, 2) # Set and get function annotations manager.set_function_annotations(func, {\\"x\\": int, \\"y\\": int, \\"return\\": int}) print(manager.get_function_annotations(func)) # Outputs: {\'x\': <class \'int\'>, \'y\': <class \'int\'>, \'return\': <class \'int\'>} ``` Constraints: - Do not use C API or external libraries that offer direct manipulation of Python internals. - The solution must be implemented using Python 3.10. Do not use deprecated features or libraries. Good luck!","solution":"class FunctionObjectManager: def create_function(self, name, code_string, globals_dict): exec(code_string, globals_dict) return globals_dict[name] def get_function_code(self, function): return function.__code__ def get_function_globals(self, function): return function.__globals__ def set_function_defaults(self, function, defaults): function.__defaults__ = defaults def get_function_defaults(self, function): return function.__defaults__ def set_function_annotations(self, function, annotations): function.__annotations__ = annotations def get_function_annotations(self, function): return function.__annotations__"},{"question":"**Question: Implement a Classification Pipeline using Scikit-Learn** **Objective:** To assess your understanding of scikit-learn datasets and your ability to build and evaluate a machine learning classification pipeline. **Problem Statement:** You are tasked with developing a classification model using scikit-learn\'s toy datasets. Specifically, you should use the `load_digits` dataset, which contains image data of handwritten digits. **Requirements:** 1. Load the `digits` dataset using scikit-learn. 2. Split the dataset into training and testing sets using an 80-20 split with a random seed of 42. 3. Build a machine learning pipeline that includes: - Standardization of features using `StandardScaler`. - Classification using a `LogisticRegression` model. 4. Train the pipeline on the training set. 5. Evaluate the accuracy of the model on the test set. **Input Format:** - No input is required from the user; the function should load the dataset internally. **Output Format:** - The function should return the accuracy of the classification model on the test set. **Function Signature:** ```python def classify_digits_dataset() -> float: pass ``` **Constraints:** - Ensure you import necessary modules only from scikit-learn. - Use `StandardScaler` and `LogisticRegression` from `sklearn.preprocessing` and `sklearn.linear_model` respectively. - Use `train_test_split` from `sklearn.model_selection` to split the dataset. **Performance:** - The solution should efficiently handle the toy dataset provided by scikit-learn. **Example:** ```python accuracy = classify_digits_dataset() print(f\\"Model Accuracy: {accuracy:.4f}\\") ``` This function will load the `digits` dataset, build a classification pipeline, train the model, and output the accuracy of the model on the test set as a floating-point number.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline def classify_digits_dataset() -> float: # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Split the dataset into training and testing sets (80-20) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with StandardScaler and LogisticRegression pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'logistic_regression\', LogisticRegression(max_iter=10000)) ]) # Train the pipeline pipeline.fit(X_train, y_train) # Evaluate the model on the test set accuracy = pipeline.score(X_test, y_test) return accuracy"},{"question":"Objective Create a class `TreeNode` that represents a node in a binary tree and customizes its copy behavior to demonstrate understanding of both shallow and deep copy operations. Instructions: 1. Define a class `TreeNode` with the following attributes: - `value` (int): The value stored at the node. - `left` (TreeNode): The left child of the node. - `right` (TreeNode): The right child of the node. 2. Implement the `__copy__()` method to provide shallow copy functionality for instances of `TreeNode`. 3. Implement the `__deepcopy__()` method to provide deep copy functionality for instances of `TreeNode`, using a memo dictionary to handle potential recursive structures. 4. Write a function `compare_trees(t1, t2)` that determines if two binary trees are identical in structure and values. 5. Write a function `test_tree_copying()` to test your copy methods. This function should: - Create a sample binary tree. - Perform both shallow and deep copying. - Modify the copies and verify that the original tree is not altered when changes are made to the deep copy, but changes propagate in the shallow copy. Example: ```python class TreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right def __copy__(self): # Your implementation here def __deepcopy__(self, memo): # Your implementation here def compare_trees(t1, t2): # Your implementation here def test_tree_copying(): root = TreeNode(1, TreeNode(2), TreeNode(3)) shallow_copy = copy.copy(root) deep_copy = copy.deepcopy(root) # Modify the copies and test if __name__ == \\"__main__\\": test_tree_copying() ``` Constraints: - You may assume the tree nodes have unique integer values. - Ensure that recursive data structures are properly handled using the memo dictionary during deep copying. Evaluation Criteria: - Correct implementation of shallow and deep copy methods. - Correctness of the `compare_trees` function to validate identical trees. - Proper testing to demonstrate the behavior of shallow and deep copies. Libraries: - You are allowed to use the `copy` module for this task.","solution":"import copy class TreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right def __copy__(self): Shallow copy: Create a new instance of TreeNode with the same value and references to the same left and right children. return TreeNode(self.value, self.left, self.right) def __deepcopy__(self, memo): Deep copy: Create a new instance of TreeNode with the same value and recursively deep copy the left and right children. if self in memo: return memo[self] new_node = TreeNode(self.value) memo[self] = new_node new_node.left = copy.deepcopy(self.left, memo) new_node.right = copy.deepcopy(self.right, memo) return new_node def compare_trees(t1, t2): Compare two binary trees and determine if they are identical in structure and values. if not t1 and not t2: return True if t1 and t2: return (t1.value == t2.value and compare_trees(t1.left, t2.left) and compare_trees(t1.right, t2.right)) return False def test_tree_copying(): root = TreeNode(1, TreeNode(2), TreeNode(3)) # Perform shallow copy shallow_copy = copy.copy(root) # Perform deep copy deep_copy = copy.deepcopy(root) # Modify the shallow copy shallow_copy.left.value = 20 shallow_copy.right = None # Verify original tree is modified (shallow copy) assert root.left.value == 20 assert root.right is not None # Modify the deep copy deep_copy.left.value = 30 deep_copy.right = None # Verify original tree is not modified (deep copy) assert root.left.value == 20 assert root.left.value != 30 assert root.right is not None"},{"question":"# Question Overview You are given a piece of code that is suspected to have a memory leak. Use the `tracemalloc` module to identify where most memory is being allocated, and compute the differences in memory usage before and after running the suspicious function. Implement the following functionalities to achieve this. Task 1. **Initialization**: - Write a function `start_tracing(frames: int) -> None` that initializes tracing with a specific number of frames to store in the traceback. 2. **Suspicious Code Execution**: - Write a function `execute_and_snapshot(func: callable) -> tuple` which takes a callable, runs it, and returns a tuple of two snapshots taken before and after executing the function. 3. **Memory Statistics**: - Write a function `compare_memory_snapshots(snapshot1, snapshot2) -> list` that returns the top 10 differences in memory usage between two snapshots. 4. **Output Formatting**: - Write a function `format_memory_statistics(memory_stats: list) -> list` which formats the output of the top memory usage differences into a readable list of strings. Function Definitions 1. **start_tracing(frames: int) -> None** - **Input**: Number of frames to capture in the tracebacks. - **Output**: None. 2. **execute_and_snapshot(func: callable) -> tuple** - **Input**: A callable function to execute. - **Output**: Tuple containing two snapshots: one before and one after calling the function. 3. **compare_memory_snapshots(snapshot1, snapshot2) -> list** - **Input**: Two snapshots. - **Output**: List of top 10 memory usage differences. 4. **format_memory_statistics(memory_stats: list) -> list** - **Input**: List of memory statistics differences. - **Output**: Formatted list of strings. Constraints - You should utilize the `tracemalloc` module and its functions to complete this task. - You can assume that the callable provided will not raise exceptions. Example ```python import tracemalloc # Example Suspicious Function def create_leak(): leaked_list = [] for i in range(10000): leaked_list.append(\' \' * 1000) # Task 1: Start tracing with 25 frames start_tracing(25) # Task 2: Snapshot around the suspicious function snapshot1, snapshot2 = execute_and_snapshot(create_leak) # Task 3 & 4: Compare snapshots and produce formatted output memory_stats = compare_memory_snapshots(snapshot1, snapshot2) formatted_output = format_memory_statistics(memory_stats) print(\\"n\\".join(formatted_output)) ``` The output should be a list of top 10 memory allocation differences, indicating where the most memory was leaked. Note Do not include the setup and import statements in your implementation, just provide the function definitions.","solution":"import tracemalloc def start_tracing(frames: int) -> None: Initializes tracemalloc tracing with the specified number of frames. :param frames: Number of frames to store in traceback. tracemalloc.start(frames) def execute_and_snapshot(func: callable) -> tuple: Takes snapshots before and after executing a given callable function. :param func: A callable to execute. :return: A tuple containing two snapshots: one before and one after the function execution. snapshot1 = tracemalloc.take_snapshot() func() snapshot2 = tracemalloc.take_snapshot() return snapshot1, snapshot2 def compare_memory_snapshots(snapshot1, snapshot2) -> list: Compares memory allocation differences between two snapshots. :param snapshot1: The first tracemalloc snapshot. :param snapshot2: The second tracemalloc snapshot. :return: List of the top 10 memory usage differences. stats = snapshot2.compare_to(snapshot1, \'lineno\') return stats[:10] # Return top 10 differences def format_memory_statistics(memory_stats: list) -> list: Formats memory statistics differences into readable strings. :param memory_stats: List of memory statistics differences. :return: A list of formatted strings. formatted_stats = [] for stat in memory_stats: formatted_stats.append(f\\"{stat.traceback} | Size: {stat.size_diff} bytes | Count: {stat.count_diff}\\") return formatted_stats"},{"question":"**Coding Assessment Question** You are provided with two datasets: `penguins` and `flights`. Use seaborn\'s objects interface to create a comprehensive visualization that compares the body mass of penguins across different species and sexes, including error bars to indicate variance. Additionally, create a bar plot for the number of passengers per month for the year 1960. **Requirements:** 1. **Dataset 1 (`penguins`):** - Create a grouped bar plot showing the average body mass of penguins, differentiated by `species` and `sex`. - Use error bars to represent the standard deviation of body mass within each group. - Apply suitable colors to differentiate between the sexes. - Adjust bar positions to avoid overlapping using dodge. 2. **Dataset 2 (`flights`):** - Create a bar plot for the number of passengers per month for the year 1960. - Ensure the bars are properly oriented and labeled. **Expected Input:** - The seaborn `penguins` dataset. - The seaborn `flights` dataset filtered for the year 1960. **Expected Output:** - A plot displaying the required visualizations that meet the specifications above. **Sample Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Dataset 1 Visualization (so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Dataset 2 Visualization so.Plot(flights[\\"month\\"], flights[\\"passengers\\"]).add(so.Bar()) ``` **Constraints and Limitations:** - The visualizations should be clear and well-labeled. - Ensure proper usage of seaborn objects and functions for the required visualizations. **Performance Requirements:** - The plots should be generated efficiently, with appropriate handling of potential overlaps in the data.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Dataset 1 Visualization def plot_penguins_body_mass(): plot = (so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.7), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .label(x=\\"Species\\", y=\\"Body Mass (g)\\", color=\\"Sex\\")) return plot # Dataset 2 Visualization def plot_flights_passengers_1960(): plot = (so.Plot(flights, x=\\"month\\", y=\\"passengers\\") .add(so.Bar(), so.Agg()) .label(x=\\"Month\\", y=\\"Number of Passengers\\")) return plot"},{"question":"# Exercise: Dynamic Activation Function Switching using `torch.cond` In this exercise, you are required to implement a neural network module in PyTorch that dynamically switches activation functions based on the sum of the input tensor elements using the `torch.cond` function. # Requirements You need to implement a class `DynamicActivationSwitch` that inherits from `torch.nn.Module`. This class should: 1. Implement a `__init__` method which accepts no parameters and initializes the module. 2. Implement a `forward` method that: - Takes a single input tensor `x`. - Uses the `torch.cond` function to decide which activation function to apply based on the sum of the elements of `x`: - If the sum is greater than zero, apply the ReLU activation function. - If the sum is less than or equal to zero, apply the Sigmoid activation function. - Returns the result of the chosen activation function. 3. Implement the ReLU and Sigmoid functions as the `true_fn` and `false_fn`. # Input and Output Format - The input to your `DynamicActivationSwitch` class will be a single 1-dimensional tensor `x` of arbitrary length. - The output will be a tensor of the same shape as the input. # Constraints - Do not use standard conditional statements such as `if` `else` directly inside the `forward` method of your class. - The solution must use the `torch.cond` function to handle the conditional logic. - The module should work with inputs both in eager and exported PyTorch models. # Example ```python import torch class DynamicActivationSwitch(torch.nn.Module): def __init__(self): super(DynamicActivationSwitch, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def relu(x): return torch.relu(x) def sigmoid(x): return torch.sigmoid(x) return torch.cond(x.sum() > 0, relu, sigmoid, (x,)) # Testing the module x = torch.tensor([1.0, -2.0, 3.0]) model = DynamicActivationSwitch() output = model(x) print(output) # Should print tensor([output values of ReLU activation]) y = torch.tensor([-1.0, -2.0, -0.5]) output = model(y) print(output) # Should print tensor([output values of Sigmoid activation]) ``` Make sure the module is testable and behaves correctly in provided example cases. Also ensure that it handles different shapes and sizes of input tensors correctly.","solution":"import torch import torch.nn.functional as F class DynamicActivationSwitch(torch.nn.Module): def __init__(self): super(DynamicActivationSwitch, self).__init__() def forward(self, x): sum_x = torch.sum(x) def relu(x): return F.relu(x) def sigmoid(x): return torch.sigmoid(x) return torch.where(sum_x > 0, relu(x), sigmoid(x))"},{"question":"# PyTorch Coding Assessment Question: Implementing and Verifying Custom Gradients **Objective**: Implement a custom PyTorch function and verify its gradient computation using the `torch.autograd.gradcheck` utility. **Problem Statement**: You are required to create a custom PyTorch function representing a simple mathematical operation and test its gradient computation using both analytical and numerical methods. Specifically, you will: 1. Implement a custom function for the operation: ( f(x) = sin(x) + cos(x) ). 2. Use PyTorch\'s `autograd.Function` to define the custom operation with both forward and backward (gradient) computation. 3. Implement a test function to verify the gradients using `torch.autograd.gradcheck`. **Requirements**: 1. **Custom Function Implementation**: - Define a custom PyTorch function class for ( f(x) = sin(x) + cos(x) ) using `torch.autograd.Function`. - Implement the `forward` method to compute the function\'s output. - Implement the `backward` method to compute the gradient (derivative) of the function\'s output with respect to its input. 2. **Gradient Verification**: - Write a separate function to test the gradient computation using `torch.autograd.gradcheck`. - Ensure that the gradients are computed correctly by comparing the analytical and numerical results. **Input and Output**: - **Input**: A tensor `x` of size (N,) where N is the number of elements in the tensor. - **Output**: The result of the custom function and a boolean value indicating whether the gradient check passed or failed. ```python import torch import torch.autograd class SinCosFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): Forward pass of the custom function: f(x) = sin(x) + cos(x) Args: - ctx: Context object for saving information for backward computation. - x (torch.Tensor): Input tensor. Returns: - torch.Tensor: Output tensor. # Save input for backward computation ctx.save_for_backward(x) return torch.sin(x) + torch.cos(x) @staticmethod def backward(ctx, grad_output): Backward pass to compute the gradient of the custom function with respect to its input. Args: - ctx: Context object with saved information from the forward pass. - grad_output (torch.Tensor): Gradient of the output with respect to some scalar value. Returns: - torch.Tensor: Gradient of the input with respect to the scalar value. x, = ctx.saved_tensors grad_input = grad_output * (torch.cos(x) - torch.sin(x)) return grad_input def test_sin_cos_function_grad(): Test function to verify the gradient computation for SinCosFunction using gradcheck. Returns: - bool: True if gradcheck passes, False otherwise. # Generate random input tensor x = torch.randn(5, dtype=torch.double, requires_grad=True) # Apply gradcheck return torch.autograd.gradcheck(SinCosFunction.apply, (x,)) # Test the gradient verification function if test_sin_cos_function_grad(): print(\\"Gradcheck passed!\\") else: print(\\"Gradcheck failed.\\") ``` **Constraints**: - Implement the function using PyTorch only; no external numerical tools are allowed. - Ensure the `backward` method accurately computes the gradient. - The implementation should be efficient and avoid unnecessary computations. **Performance Requirements**: - The `gradcheck` test should pass within a reasonable time frame (a few seconds for typical input sizes). # Instructions: 1. Implement the `SinCosFunction` class with the specified `forward` and `backward` methods. 2. Implement the `test_sin_cos_function_grad` function to verify gradient computation using `torch.autograd.gradcheck`. 3. Ensure your solution passes the test by running the provided testing code.","solution":"import torch import torch.autograd class SinCosFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): Forward pass of the custom function: f(x) = sin(x) + cos(x) Args: - ctx: Context object for saving information for backward computation. - x (torch.Tensor): Input tensor. Returns: - torch.Tensor: Output tensor. # Save input for backward computation ctx.save_for_backward(x) return torch.sin(x) + torch.cos(x) @staticmethod def backward(ctx, grad_output): Backward pass to compute the gradient of the custom function with respect to its input. Args: - ctx: Context object with saved information from the forward pass. - grad_output (torch.Tensor): Gradient of the output with respect to some scalar value. Returns: - torch.Tensor: Gradient of the input with respect to the scalar value. x, = ctx.saved_tensors grad_input = grad_output * (torch.cos(x) - torch.sin(x)) return grad_input def test_sin_cos_function_grad(): Test function to verify the gradient computation for SinCosFunction using gradcheck. Returns: - bool: True if gradcheck passes, False otherwise. # Generate random input tensor x = torch.randn(5, dtype=torch.double, requires_grad=True) # Apply gradcheck return torch.autograd.gradcheck(SinCosFunction.apply, (x,))"},{"question":"Objective: Design an asyncio-based traffic control system using the synchronization primitives provided by Python 3.10. This system should manage access to a bridge that can only be crossed by a limited number of cars at one time (e.g., 3). Each car will have its own coroutine, and we need to ensure that no more than 3 cars cross the bridge concurrently, illustrating the use of asyncio.Semaphore. Instructions: Implement a function `simulate_traffic` that accepts a list of car identifiers and simulates each car attempting to cross the bridge asynchronously. The function should: 1. Print a message when a car starts crossing (e.g., \\"Car 1 starts crossing\\"). 2. Print a message when the car finishes crossing (e.g., \\"Car 1 finished crossing\\"). 3. Ensure that at most 3 cars are crossing the bridge at any given time. 4. Each car should take a random time between 1 and 3 seconds to cross the bridge (use `asyncio.sleep` to simulate this). Function Signature: ```python import asyncio import random async def simulate_traffic(cars: List[str]) -> None: pass ``` Input: - A list of strings `cars` where each string is a unique identifier for a car. Output: - The function should print messages as cars start and finish crossing the bridge. Constraints: - You must use `asyncio.Semaphore` to manage the concurrency limit. - Each car takes a random time between 1 and 3 seconds (inclusive) to cross the bridge. - The function must run until all cars have crossed the bridge. Example: ```python import asyncio import random async def simulate_traffic(cars: List[str]): semaphore = asyncio.Semaphore(3) async def cross_bridge(car): async with semaphore: print(f\\"{car} starts crossing\\") await asyncio.sleep(random.randint(1, 3)) print(f\\"{car} finished crossing\\") await asyncio.gather(*(cross_bridge(car) for car in cars)) # To run the simulation: cars = [f\\"Car {i}\\" for i in range(10)] asyncio.run(simulate_traffic(cars)) ``` Notes: - In a real-world scenario, semaphore ensures that only a fixed number of coroutines access a critical section, in this case, the bridge. - The use of `async with` statement with the semaphore helps to take care of acquire and release automatically.","solution":"import asyncio import random from typing import List async def simulate_traffic(cars: List[str]) -> None: semaphore = asyncio.Semaphore(3) async def cross_bridge(car: str): async with semaphore: print(f\\"{car} starts crossing\\") await asyncio.sleep(random.randint(1, 3)) print(f\\"{car} finished crossing\\") await asyncio.gather(*(cross_bridge(car) for car in cars)) # To run the simulation (example usage, can be removed or commented out in final submission): # cars = [f\\"Car {i}\\" for i in range(10)] # asyncio.run(simulate_traffic(cars))"},{"question":"# Seaborn Plotting and Percentile Visualization You are provided with a dataset `tips` from the seaborn library. This dataset contains information about restaurant tips and includes the following columns: - `total_bill`: Total bill amount (numerical). - `tip`: Tip amount (numerical). - `sex`: Gender of the person paying (categorical). - `smoker`: Whether the person is a smoker or not (categorical). - `day`: Day of the week (categorical). - `time`: Time of the meal (Lunch/Dinner) (categorical). - `size`: Size of the party (numerical). Your task is to: 1. Load the `tips` dataset using seaborn\'s `load_dataset` function. 2. Create a plot using the seaborn `objects` interface that: - Shows the relationship between `total_bill` and `tip` across different `smoker` statuses. - Uses a logarithmic scale on the y-axis (`tip`). - Adds dot representations at the 10th, 50th, and 90th percentiles for each `smoker` status group. - Enhances the visualization with jitter and shows percentile intervals from the 25th to 75th percentile. Expected Input and Output - Load the dataset using the following command: ```python tips = sns.load_dataset(\\"tips\\") ``` - Your function should output a well-defined seaborn plot meeting the criteria stated above. Constraints and Limitations - Use seaborn version 0.11.0 or later. - The plot must be generated using the `objects` interface from seaborn. - Ensure your plot is well-labelled and easily interpretable. Here is the skeleton code to get you started: ```python import seaborn as sns import seaborn.objects as so # Load the dataset tips = sns.load_dataset(\\"tips\\") # Your code to generate the plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"smoker\\") .scale(y=\\"log\\") ) # Add dot representations at 10th, 50th, and 90th percentiles p.add(so.Dot(), so.Perc([10, 50, 90])) # Enhance visualization with jitter and percentile intervals # (complete the enhancement based on the description) p.add(...) # Add jitter p.add(...) # Add percentile intervals # Render the plot p.show() ``` Replace the `# Add jitter` and `# Add percentile intervals` comments with actual plot enhancement methods as specified.","solution":"import seaborn as sns import seaborn.objects as so # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"smoker\\") .scale(y=\\"log\\") ) # Add dot representations at the 10th, 50th, and 90th percentiles p.add(so.Dot(), so.Perc([10, 50, 90])) # Enhance visualization with jitter and percentile intervals p.add(so.Dot(), so.Jitter()) p.add(so.Range(), so.Perc([25, 75])) # Render the plot p.show()"},{"question":"Objective The objective of this exercise is to test your ability to interact with Python\'s configuration system using the `sysconfig` module. You will be required to retrieve specific configuration information and paths, and manipulate them as specified. Problem Statement You are asked to implement a Python function `get_python_config_info()` that performs the following tasks: 1. Retrieve the names of all installation schemes supported by `sysconfig`. 2. Retrieve the default installation scheme for the current platform. 3. Retrieve the installation paths for the default scheme. 4. Retrieve the Python version in the format `MAJOR.MINOR`. 5. Retrieve the platform information. The function should return a dictionary containing the following keys and their respective values: - `\\"scheme_names\\"`: A list of all supported installation scheme names. - `\\"default_scheme\\"`: The name of the default installation scheme. - `\\"installation_paths\\"`: A dictionary of installation paths for the default scheme. - `\\"python_version\\"`: The Python version number as a string in the format `MAJOR.MINOR`. - `\\"platform\\"`: A string identifying the current platform. Input The function does not take any input arguments. Output The function should return a dictionary with the described structure and values. Constraints - Assume that the `sysconfig` module is always available. - The returned dictionary keys must exactly match the specified structure. Example ```python def get_python_config_info(): # Your implementation here result = get_python_config_info() expected_output_structure = { \\"scheme_names\\": [\\"posix_prefix\\", \\"posix_home\\", \\"posix_user\\", \\"nt\\", \\"nt_user\\", \\"osx_framework_user\\"], \\"default_scheme\\": \\"posix_prefix\\", \\"installation_paths\\": { \\"stdlib\\": \\"/path/to/stdlib\\", \\"platstdlib\\": \\"/path/to/platstdlib\\", \\"platlib\\": \\"/path/to/platlib\\", \\"purelib\\": \\"/path/to/purelib\\", \\"include\\": \\"/path/to/include\\", \\"platinclude\\": \\"/path/to/platinclude\\", \\"scripts\\": \\"/path/to/scripts\\", \\"data\\": \\"/path/to/data\\" }, \\"python_version\\": \\"3.10\\", \\"platform\\": \\"linux-x86_64\\" } assert all(key in result for key in expected_output_structure) ``` Note - The provided example is for illustrative purposes only. The actual paths and values will vary depending on the environment where the function is run.","solution":"import sysconfig import platform def get_python_config_info(): Retrieves Python configuration information including installation schemes, paths, version, and platform info. Returns: dict: A dictionary containing the specified Python configuration information. scheme_names = sysconfig.get_scheme_names() default_scheme = sysconfig.get_default_scheme() installation_paths = {key: sysconfig.get_path(key, default_scheme) for key in sysconfig.get_paths(default_scheme).keys()} python_version = f\\"{sysconfig.get_python_version()}\\" platform_info = platform.platform() return { \\"scheme_names\\": list(scheme_names), \\"default_scheme\\": default_scheme, \\"installation_paths\\": installation_paths, \\"python_version\\": python_version, \\"platform\\": platform_info }"},{"question":"# Question: Advanced Seaborn Boxplot Customization You are given a dataset of passengers from the Titanic dataset. Your task is to create three different customized boxplots using the seaborn library. Each boxplot should illustrate a different aspect of customization as described below. Input: - The dataset will be loaded from seaborn\'s built-in datasets: `titanic`. Output: - Three different boxplot visualizations created using seaborn. Requirements: 1. **Simple Horizontal Boxplot:** - Create a horizontal boxplot for the `age` column. 2. **Grouped Vertical Boxplot with Customizations:** - Create a vertical boxplot grouped by the `class` column, with additional grouping by the `sex` column. - Set the whisker range to cover the full range of the data. - Customize the box colors to be semi-transparent with a grey color. 3. **Advanced Grouped Boxplot:** - Create a vertical boxplot for the `age` column grouped by the `pclass` column. - Use the `hue` parameter to add a nested grouping for the `survived` column. - Add a small gap between the boxes and draw the boxes as line art. - Customize line properties such as color and width. **Constraints:** - Use seaborn version 0.11 or later and matplotlib for any additional customizations. **Note:** - Ensure that all customizations are visible and understandable in the resulting plots. Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Task 1: Simple Horizontal Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Simple Horizontal Boxplot of Age\\") plt.show() # Task 2: Grouped Vertical Boxplot with Customizations plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", whis=(0, 100), palette=\\"gray\\", alpha=0.5) plt.title(\\"Grouped Vertical Boxplot of Age by Class and Sex with Full Whisker Range\\") plt.show() # Task 3: Advanced Grouped Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"survived\\", linewidth=1.0, gap=0.1, fill=False) plt.title(\\"Advanced Grouped Boxplot of Age by Pclass and Survived Status with Line Art\\") plt.show() ``` Implement the described tasks and customize each plot according to the specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def create_horizontal_boxplot(): plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Simple Horizontal Boxplot of Age\\") plt.xlabel(\\"Age\\") plt.show() def create_grouped_vertical_boxplot(): plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", whis=[0, 100], palette=\\"gray\\", fliersize=0) plt.title(\\"Grouped Vertical Boxplot of Age by Class and Sex with Full Whisker Range\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\'Sex\') plt.show() def create_advanced_grouped_boxplot(): plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"survived\\", dodge=True, linewidth=1.0, fliersize=0, boxprops=dict(alpha=0.3, edgecolor=\'k\')) plt.title(\\"Advanced Grouped Boxplot of Age by Pclass and Survived Status with Line Art\\") plt.xlabel(\\"Pclass\\") plt.ylabel(\\"Age\\") plt.legend(title=\'Survived\') plt.show()"},{"question":"# Question You are tasked with building a utility function that manages and executes multiple shell commands using the `subprocess` module. This function will simulate a small task manager that can run commands, capture their output, handle timeouts, and report errors. # Function Signature ```python def execute_commands(commands: list, timeout: int = None) -> list: pass ``` # Input - `commands`: A list of dictionaries, each containing: - `cmd`: a string or sequence of the command and its arguments. - `capture_output`: a boolean indicating if the output should be captured. - `check`: a boolean to check if the command succeeds (raises exceptions on failures). - `timeout`: An integer specifying the maximum number of seconds to allow the command to run. Optional. # Output - A list of results for each command. Each result should be a dictionary containing: - `cmd`: the command that was run. - `returncode`: the exit status of the command. - `stdout`: the standard output of the command (string). - `stderr`: the standard error of the command (string). - `error`: any exceptions raised (None if no exception was raised). # Constraints - The function should handle edge cases, such as commands that don\'t exist, or commands that take too long and need to be terminated. - Use correct subprocess settings to prevent deadlocks. - Should be compatible with both Windows and POSIX systems. # Example ```python commands = [ {\\"cmd\\": [\\"echo\\", \\"Hello, World!\\"], \\"capture_output\\": True, \\"check\\": True}, {\\"cmd\\": [\\"sleep\\", \\"10\\"], \\"capture_output\\": True, \\"check\\": True}, {\\"cmd\\": [\\"ls\\", \\"/nonexistent\\"], \\"capture_output\\": True, \\"check\\": True}, ] results = execute_commands(commands, timeout=5) # Expected output # results = [ # {\'cmd\': [\'echo\', \'Hello, World!\'], \'returncode\': 0, \'stdout\': \'Hello, World!n\', \'stderr\': \'\', \'error\': None}, # {\'cmd\': [\'sleep\', \'10\'], \'returncode\': None, \'stdout\': None, \'stderr\': None, \'error\': subprocess.TimeoutExpired}, # {\'cmd\': [\'ls\', \'/nonexistent\'], \'returncode\': 1, \'stdout\': \'\', \'stderr\': \'ls: cannot access /nonexistent: No such file or directoryn\', \'error\': subprocess.CalledProcessError}, # ] ``` # Notes - Make use of `subprocess.run()` or `subprocess.Popen()` as appropriate for handling command execution. - Ensure proper management of standard input/output/error streams. - Handle command timeouts gracefully by using proper time management in subprocess calls. - Capture output and errors correctly and handle any exceptions that arise during execution. # Hints - The `capture_output` attribute in `subprocess.run` can automatically capture `stdout` and `stderr`. - Handling timeouts can be done through the `timeout` attribute in `subprocess.run`. - Use try-except blocks to handle subprocess exceptions safely. Implement the function `execute_commands` using the guidelines provided above.","solution":"import subprocess def execute_commands(commands, timeout=None): results = [] for command_info in commands: cmd = command_info.get(\\"cmd\\") capture_output = command_info.get(\\"capture_output\\", False) check = command_info.get(\\"check\\", False) result = { \\"cmd\\": cmd, \\"returncode\\": None, \\"stdout\\": None, \\"stderr\\": None, \\"error\\": None } try: completed_process = subprocess.run( cmd, capture_output=capture_output, check=check, timeout=timeout, text=True ) result[\\"returncode\\"] = completed_process.returncode if capture_output: result[\\"stdout\\"] = completed_process.stdout result[\\"stderr\\"] = completed_process.stderr except subprocess.CalledProcessError as e: result[\\"returncode\\"] = e.returncode if capture_output: result[\\"stdout\\"] = e.stdout result[\\"stderr\\"] = e.stderr result[\\"error\\"] = str(e) except subprocess.TimeoutExpired as e: result[\\"error\\"] = str(e) except Exception as e: result[\\"error\\"] = str(e) results.append(result) return results"},{"question":"Localizing a Python Application Objective: Create a Python script that demonstrates the use of the `gettext` and `locale` modules to provide internationalized messages based on the user\'s locale. Problem Statement: You are tasked with writing a Python script that greets users in their local language. The script should: 1. Detect the user\'s locale settings. 2. Provide greetings in different languages based on the detected locale. 3. Allow for easy extension to support additional languages. Requirements: 1. Your script should use the `locale` module to determine the user\'s current locale settings. 2. Create message catalogs using the `gettext` module. Provide translations for at least three different languages: English, Spanish, and French. 3. Implement a function `greet_user` to display a greeting message based on the user\'s locale. 4. Include error handling to provide a default message in English if the user\'s locale is not supported. Function Signature: ```python def greet_user(): pass ``` Input: - No input parameters are required for the function. Output: - The function will print a greeting message based on the user\'s locale settings. Constraints: - Use the `gettext` and `locale` modules only. - Ensure that the function handles unsupported locales gracefully by defaulting to English. - Extendibility: The solution should be easily extendable to support additional languages. Example Output: ```python # Assuming the user\'s locale is detected as English (en_US): Hello! # Assuming the user\'s locale is detected as Spanish (es_ES): ¡Hola! # Assuming the user\'s locale is detected as French (fr_FR): Bonjour! ``` Implementation Notes: 1. Use `locale.getdefaultlocale()` to get the user\'s locale settings. 2. Use `gettext.translation` to load the appropriate message catalog based on the locale. 3. Create `.mo` files for the message catalogs for English, Spanish, and French (these can be manually created for the sake of this exercise). 4. Handle unsupported locales by providing a default greeting in English. Additional Information: - You can use the GNU `gettext` command-line tools to create the `.mo` files from text-based `.po` files. - Ensure the message catalogs are loaded correctly based on the user\'s locale. Sample Code Outline: ```python import locale import gettext def greet_user(): # Detect user\'s locale settings loc = locale.getdefaultlocale() # Set up translation try: lang = loc[0] translation = gettext.translation(\'base\', localedir=\'locales\', languages=[lang]) translation.install() _ = translation.gettext except FileNotFoundError: # Default to English if locale is not supported _ = lambda s: s # Print greeting message greeting_message = _(\\"Hello!\\") print(greeting_message) if __name__ == \\"__main__\\": greet_user() ``` Make sure to test your function with different locale settings to verify that the greetings are displayed correctly.","solution":"import locale import gettext def greet_user(): # Detect user\'s locale settings loc = locale.getdefaultlocale() # Set up translation try: lang = loc[0] translation = gettext.translation(\'base\', localedir=\'locales\', languages=[lang]) translation.install() _ = translation.gettext except FileNotFoundError: # Default to English if locale is not supported _ = lambda s: s # Print greeting message greeting_message = _(\\"Hello!\\") print(greeting_message) if __name__ == \\"__main__\\": greet_user()"},{"question":"Objective: To assess students\' understanding of the `seaborn.violinplot` function by requiring them to visualize and compare distributions from a dataset with specific customization. Problem Statement: Given a dataset of your choice, implement a Python function `custom_violinplot(data, x_col, y_col, hue_col=None, split=False, inner=None, bw_adjust=1, cut=2, native_scale=False)` that generates a customized violin plot to visualize the distribution of the data. The function should be able to: 1. Draw a violin plot that groups the data by the categorical variable specified in `x_col` and displays the distribution of the numerical variable specified in `y_col`. 2. If `hue_col` is provided, it should color the plot accordingly by this variable. 3. If `split` is set to `True`, it should draw split violins. 4. Use the `inner` parameter to represent the data inside the violin (options include \\"box\\", \\"quart\\", \\"stick\\", and \\"point\\"). 5. Adjust the bandwidth of the KDE with the `bw_adjust` parameter. 6. Control how the KDE extends past the data with the `cut` parameter. 7. If `native_scale` is set to `True`, use the original scale of the axes. The function should generate the plot and save it as a PNG file named `custom_violinplot.png`. # Function Signature: ```python def custom_violinplot(data, x_col, y_col, hue_col=None, split=False, inner=None, bw_adjust=1, cut=2, native_scale=False): pass ``` # Input: - `data`: A pandas DataFrame containing the dataset. - `x_col`: A string representing the column name of the categorical variable. - `y_col`: A string representing the column name of the numerical variable. - `hue_col`: (Optional) A string representing the column name to color by, defaults to `None`. - `split`: A boolean indicating whether to split the violins in half to show multiple distributions, defaults to `False`. - `inner`: A string specifying the representation inside the violins. Options include \\"box\\", \\"quart\\", \\"stick\\", \\"point\\", defaults to `None`. - `bw_adjust`: A float to adjust the bandwidth of KDE. Larger values produce smoother plots, defaults to `1`. - `cut`: An integer that controls the KDE extending past the data limits, defaults to `2`. - `native_scale`: A boolean indicating whether to preserve the original scale, defaults to `False`. # Output: - A file named `custom_violinplot.png` that contains the rendered violin plot. # Constraints: - Ensure that the `seaborn` library is used for plotting. - Validate inputs to ensure column names are in the DataFrame and are of appropriate data types. - The function should handle cases where columns contain missing data gracefully, either by cleaning or by handling them appropriately in the plot. # Example Usage: ```python import pandas as pd import seaborn as sns # Example dataset data = sns.load_dataset(\\"titanic\\") # Generate violin plot custom_violinplot(data, x_col=\\"class\\", y_col=\\"age\\", hue_col=\\"alive\\", split=True, inner=\\"quart\\", bw_adjust=0.5, cut=0, native_scale=False) ``` Expected Outcome: The function should output `custom_violinplot.png` reflecting the provided parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_violinplot(data, x_col, y_col, hue_col=None, split=False, inner=None, bw_adjust=1, cut=2, native_scale=False): Generate a customized violin plot to visualize the distribution of the data. # Validate input columns if x_col not in data.columns or y_col not in data.columns: raise ValueError(f\\"Columns {x_col} and/or {y_col} not found in the DataFrame\\") if hue_col and hue_col not in data.columns: raise ValueError(f\\"Column {hue_col} not found in the DataFrame\\") # Drop rows with missing data in relevant columns data = data.dropna(subset=[x_col, y_col] + ([hue_col] if hue_col else [])) # Create the violin plot plt.figure(figsize=(10, 6)) sns.violinplot( data=data, x=x_col, y=y_col, hue=hue_col, split=split, inner=inner, bw=bw_adjust, cut=cut, scale=\\"count\\" if native_scale else \\"area\\" ) # Save the plot as a png file plt.savefig(\\"custom_violinplot.png\\") plt.close()"},{"question":"# Command-Line Tool Implementation Using `optparse` Objective: Implement a Python script that processes command-line arguments using the `optparse` module. Your implementation should demonstrate a comprehensive understanding of the `optparse` library, including basic usage, handling different types of arguments, generating usage and help messages, managing default values, and organizing options into groups. Task: You are required to write a Python script that functions as a simple command-line file manager. The script should support the following options and behaviors: 1. **Basic Commands:** - `-l` or `--list`: List all files in the current directory. - `-d DIR` or `--directory=DIR`: Specify a directory to operate on. Defaults to the current directory if not provided. - `-r PATTERN` or `--remove=PATTERN`: Remove files matching the given pattern in the specified directory. If no pattern is provided, no files should be removed. 2. **Optional Arguments:** - `-v` or `--verbose`: Enable verbose mode which prints detailed operations. - `-q` or `--quiet`: Disable verbose mode. This should override `--verbose` if both are provided. - `-h` or `--help`: Print a help message and exit. - `--version`: Print the version of the script and exit. 3. **Advanced Features:** - Organize the options into two groups: \\"Basic Commands\\" and \\"Optional Arguments\\". - Handle errors gracefully and output appropriate error messages if invalid arguments are provided. Requirements: - Define the `OptionParser` and add the options as described. - Implement the logic to handle each option (listing files, changing directories, removing files, etc.). - Generate appropriate usage and help messages. - Ensure the script has a version string and handles version printing. - Implement error handling to manage user errors (e.g., invalid directory, invalid patterns). Constraints: - You must use the `optparse` module for argument parsing. - Your script should handle operations in a cross-platform manner (compatible with UNIX and Windows file systems). - Ensure that the script runs efficiently, even if the specified directory contains many files. Performance Consideration: - The script should handle directories with up to 1000 files efficiently, ensuring minimal delay in listing and deleting operations. Input Format: The script should accept command-line inputs as specified in the task. Output Format: - Print the list of files if the `--list` option is specified. - Print detailed operations if the `--verbose` option is enabled. - Print error messages for invalid inputs. Example Usage: ``` python file_manager.py --directory=/tmp -l python file_manager.py -d /tmp --remove=\\"*.log\\" --verbose python file_manager.py -v -d /var/log --remove=\\"*.log\\" python file_manager.py --version ``` Your solution will be evaluated on correctness, proper use of `optparse`, code organization, and error handling.","solution":"import os import glob from optparse import OptionParser, OptionGroup VERSION = \\"1.0\\" def list_files(directory): try: files = os.listdir(directory) for file in files: print(file) except Exception as e: print(f\\"Error listing files in directory \'{directory}\': {e}\\") def remove_files(directory, pattern, verbose): try: full_path_pattern = os.path.join(directory, pattern) files_to_remove = glob.glob(full_path_pattern) for file in files_to_remove: try: os.remove(file) if verbose: print(f\\"Removed: {file}\\") except Exception as e: print(f\\"Error removing file \'{file}\': {e}\\") except Exception as e: print(f\\"Error locating files in directory \'{directory}\' with pattern \'{pattern}\': {e}\\") def parse_arguments(): parser = OptionParser(usage=\\"usage: %prog [options]\\", version=f\\"%prog {VERSION}\\") group_basic = OptionGroup(parser, \\"Basic Commands\\", \\"These are basic directory and file operations\\") group_basic.add_option(\\"-l\\", \\"--list\\", action=\\"store_true\\", dest=\\"list\\", default=False, help=\\"List all files in the specified directory\\") group_basic.add_option(\\"-d\\", \\"--directory\\", dest=\\"directory\\", default=os.getcwd(), help=\\"Specify a directory to operate on\\") group_basic.add_option(\\"-r\\", \\"--remove\\", dest=\\"remove\\", help=\\"Remove files matching the given pattern in the specified directory\\") group_optional = OptionGroup(parser, \\"Optional Arguments\\", \\"These are additional options to control the script\'s behavior\\") group_optional.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"Enable verbose mode, printing detailed operations\\") group_optional.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"Disable verbose mode, this overrides --verbose if both are provided\\") parser.add_option_group(group_basic) parser.add_option_group(group_optional) (options, args) = parser.parse_args() return options def main(): options = parse_arguments() if options.list: list_files(options.directory) if options.remove: remove_files(options.directory, options.remove, options.verbose) if __name__ == \\"__main__\\": main()"},{"question":"Objective: To evaluate understanding and competence in data visualization using the Seaborn library in Python, particularly focusing on the `sns.barplot` and `sns.catplot` functions. Question: You are provided with the Penguins dataset. Your task is to write a Python function that performs the following: 1. **Load the dataset:** Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Create a bar plot:** Create a bar plot showing the average `body_mass_g` for each `island` and color the bars by `sex`. 3. **Add customization:** Customize the bar plot to: - Show the standard deviation of `body_mass_g` as error bars. - Set the `capsize` of error bars to 0.2 and use a different color for error bars. - Add labels to show the average `body_mass_g` values on top of each bar. 4. **Create a faceted bar plot using `catplot`:** Generate a faceted bar plot using `sns.catplot` to show the average `body_mass_g` grouped by `sex` for each species. Customize the plot to have a single column and an aspect ratio of 0.6. 5. **Save both plots:** Save both the standard bar plot and the faceted bar plot as images named `bar_plot.png` and `faceted_plot.png`, respectively. Function Signature: ```python def visualize_penguins(): pass ``` Expected Output: Your solution should create and save the following images: 1. `bar_plot.png`: A bar plot showing the average `body_mass_g` for each `island` with error bars representing the standard deviation and labels showing the values. 2. `faceted_plot.png`: A faceted bar plot showing the average `body_mass_g` grouped by `sex` for each species. Constraints: - The dataset may contain missing values. Handle them appropriately. - Use appropriate Seaborn and Matplotlib functions and customization options provided in the documentation. Example: Suppose you have the following segment of the `penguins` dataset: ``` species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female ``` Based on this data, your resulting plots should show the correct aggregation, grouping, and display as required. Use the following code template to structure your solution: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Handle missing values penguins = penguins.dropna(subset=[\\"body_mass_g\\"]) # Create the initial bar plot bar_plot = sns.barplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", capsize=0.2, errcolor=\\"0.5\\") bar_plot.bar_label(bar_plot.containers[0], fontsize=10) # Save the bar plot plt.savefig(\\"bar_plot.png\\") plt.clf() # Clear the figure for the next plot # Create the faceted plot faceted_plot = sns.catplot(data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", col=\\"species\\", height=4, aspect=0.6) # Save the faceted plot faceted_plot.savefig(\\"faceted_plot.png\\") # Call the function to produce the plots visualize_penguins() ``` Your task is to fill in the function body to achieve the described functionality using the Seaborn library.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Handle missing values penguins = penguins.dropna(subset=[\\"body_mass_g\\", \\"sex\\"]) # Create the initial bar plot plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", capsize=0.2, errcolor=\\"0.5\\", palette=\\"muted\\") bar_plot.set_title(\'Average Body Mass (g) by Island and Sex with Error Bars\') bar_plot.set_xlabel(\'Island\') bar_plot.set_ylabel(\'Average Body Mass (g)\') # Add labels on top of bars for container in bar_plot.containers: bar_plot.bar_label(container, fmt=\'%.f\', label_type=\'edge\', fontsize=10) # Save the bar plot plt.savefig(\\"bar_plot.png\\") plt.clf() # Clear the figure for the next plot # Create the faceted plot faceted_plot = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", col=\\"species\\", height=4, aspect=0.6, palette=\\"muted\\", errorbar=\\"sd\\" ) faceted_plot.fig.suptitle(\'Average Body Mass (g) by Sex for Each Species\') faceted_plot.set_axis_labels(\'Sex\', \'Average Body Mass (g)\') # Save the faceted plot faceted_plot.savefig(\\"faceted_plot.png\\") # Call the function to produce the plots visualize_penguins()"},{"question":"# Decision Tree Assessment **Objective**: Implement a decision tree that can handle multi-class classification, regression, and missing values. Visualize the tree and export it in both graphical and textual formats. Perform hyperparameter tuning to avoid overfitting and improve model performance. **Task**: 1. **Load and Prepare Datasets**: - Load the Iris dataset for classification. - Load a custom regression dataset (create a synthetic dataset with missing values). 2. **Implement a Decision Tree Classifier and Regressor**: - Train a `DecisionTreeClassifier` on the Iris dataset. - Train a `DecisionTreeRegressor` on the custom regression dataset. 3. **Handle Missing Values**: - Include missing values in the regression dataset and demonstrate how your model handles them during training and prediction. 4. **Visualize and Export**: - Visualize the trained decision trees using `plot_tree`. - Export the trees in both Graphviz and textual formats. 5. **Hyperparameter Tuning and Validation**: - Implement cross-validation to tune the hyperparameters (`max_depth`, `min_samples_split`, etc.) for both models. - Use `GridSearchCV` to find the best parameters. - Plot the learning curves for the best models. 6. **Performance Assessment**: - Evaluate the performance of both models using appropriate metrics (e.g., accuracy, MSE). - Discuss the impact of hyperparameters on model performance and overfitting. **Submission**: Submit a Jupyter Notebook containing: - Code implementation for all tasks. - Visualizations and exports. - Detailed explanations and discussions of each step, including handling missing values, hyperparameter tuning, and performance assessment. - Learning curves and evaluation metrics. **Input/Output Formats**: - **Input**: Iris dataset (automatically loaded using `load_iris` from sklearn), custom synthetic dataset (generated within the notebook). - **Output**: Visualizations, Graphviz and textual exports, tuned model hyperparameters, learning curves, and performance metrics. **Constraints**: - Use only scikit-learn and matplotlib libraries for this task. - Ensure that the notebook is well-documented and easy to follow. **Performance Requirements**: - The decision trees should avoid overfitting, and hyperparameter tuning should demonstrate improvement in validation performance. - Visual and textual exports of the trees should be clear and informative. **Example Code Snippets**: 1. **Loading Datasets**: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split iris = load_iris() X_class = iris.data y_class = iris.target X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42) ``` 2. **Handling Missing Values**: ```python import numpy as np from sklearn.model_selection import train_test_split # Create synthetic regression data X_reg = np.random.rand(100, 2) * 10 y_reg = X_reg[:, 0] * 2 + X_reg[:, 1] * 3 + np.random.randn(100) * 0.1 # Introduce missing values X_reg[::10, 0] = np.nan X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) ``` 3. **Training and Visualization**: ```python from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_graphviz, export_text # Train classifier clf = DecisionTreeClassifier() clf.fit(X_class_train, y_class_train) # Visualize plot_tree(clf) export_graphviz(clf, out_file=\'tree_class.dot\') print(export_text(clf, feature_names=iris.feature_names)) # Train regressor reg = DecisionTreeRegressor() reg.fit(X_reg_train, y_reg_train) # Visualize plot_tree(reg) export_graphviz(reg, out_file=\'tree_reg.dot\') print(export_text(reg, feature_names=[\'feature_1\', \'feature_2\'])) ``` 4. **Hyperparameter Tuning**: ```python from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, mean_squared_error # Hyperparameter tuning for classifier param_grid = {\'max_depth\': [3, 5, 7, None], \'min_samples_split\': [2, 5, 10]} grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5) grid_search.fit(X_class_train, y_class_train) best_params_class = grid_search.best_params_ # Hyperparameter tuning for regressor grid_search_reg = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5) grid_search_reg.fit(X_reg_train, y_reg_train) best_params_reg = grid_search_reg.best_params_ # Evaluate best_clf = grid_search.best_estimator_ best_reg = grid_search_reg.best_estimator_ y_class_pred = best_clf.predict(X_class_test) y_reg_pred = best_reg.predict(X_reg_test) acc = accuracy_score(y_class_test, y_class_pred) mse = mean_squared_error(y_reg_test, y_reg_pred) print(f\'Best classifier params: {best_params_class}, Accuracy: {acc}\') print(f\'Best regressor params: {best_params_reg}, MSE: {mse}\') ``` **Note**: Ensure all code blocks are well-documented and include necessary explanations.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, mean_squared_error from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_graphviz, export_text import matplotlib.pyplot as plt def load_datasets(): iris = load_iris() X_class = iris.data y_class = iris.target X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42) # Create synthetic regression data with missing values np.random.seed(42) X_reg = np.random.rand(100, 2) * 10 y_reg = X_reg[:, 0] * 2 + X_reg[:, 1] * 3 + np.random.randn(100) * 0.1 X_reg[::10, 0] = np.nan X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) return (X_class_train, X_class_test, y_class_train, y_class_test), (X_reg_train, X_reg_test, y_reg_train, y_reg_test) def handle_missing_values(X_train, X_test): from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_train_imputed = imputer.fit_transform(X_train) X_test_imputed = imputer.transform(X_test) return X_train_imputed, X_test_imputed def train_decision_trees(X_class_train, y_class_train, X_reg_train, y_reg_train): clf = DecisionTreeClassifier() clf.fit(X_class_train, y_class_train) reg = DecisionTreeRegressor() reg.fit(X_reg_train, y_reg_train) return clf, reg def export_and_visualize(clf, reg): # Visualization and Export for Classifier plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names) plt.show() with open(\'tree_class.dot\', \'w\') as f: export_graphviz(clf, out_file=f, feature_names=load_iris().feature_names) print(export_text(clf, feature_names=load_iris().feature_names)) # Visualization and Export for Regressor plt.figure(figsize=(20,10)) plot_tree(reg, filled=True, feature_names=[\'feature_1\', \'feature_2\']) plt.show() with open(\'tree_reg.dot\', \'w\') as f: export_graphviz(reg, out_file=f, feature_names=[\'feature_1\', \'feature_2\']) print(export_text(reg, feature_names=[\'feature_1\', \'feature_2\'])) def hyperparameter_tuning(X_class_train, y_class_train, X_reg_train, y_reg_train): param_grid = {\'max_depth\': [3, 5, 7, None], \'min_samples_split\': [2, 5, 10]} # Hyperparameter tuning for classifier grid_search_class = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5) grid_search_class.fit(X_class_train, y_class_train) best_clf = grid_search_class.best_estimator_ # Hyperparameter tuning for regressor grid_search_reg = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5) grid_search_reg.fit(X_reg_train, y_reg_train) best_reg = grid_search_reg.best_estimator_ return best_clf, best_reg, grid_search_class.best_params_, grid_search_reg.best_params_ def performance_assessment(X_class_test, y_class_test, X_reg_test, y_reg_test, best_clf, best_reg): y_class_pred = best_clf.predict(X_class_test) y_reg_pred = best_reg.predict(X_reg_test) acc = accuracy_score(y_class_test, y_class_pred) mse = mean_squared_error(y_reg_test, y_reg_pred) return acc, mse # Main Workflow (X_class_train, X_class_test, y_class_train, y_class_test), (X_reg_train, X_reg_test, y_reg_train, y_reg_test) = load_datasets() X_reg_train, X_reg_test = handle_missing_values(X_reg_train, X_reg_test) clf, reg = train_decision_trees(X_class_train, y_class_train, X_reg_train, y_reg_train) export_and_visualize(clf, reg) best_clf, best_reg, best_params_class, best_params_reg = hyperparameter_tuning(X_class_train, y_class_train, X_reg_train, y_reg_train) acc, mse = performance_assessment(X_class_test, y_class_test, X_reg_test, y_reg_test, best_clf, best_reg) print(f\'Best classifier params: {best_params_class}, Accuracy: {acc}\') print(f\'Best regressor params: {best_params_reg}, MSE: {mse}\')"},{"question":"You are required to implement a PyTorch function that performs several operations on complex tensors. Your function will create a complex tensor, perform a series of operations on it including conversion, accessing the real and imaginary parts, computing angles, and running a linear algebra operation. # Function Signature ```python def complex_tensor_operations() -> tuple: This function performs the following operations and returns a tuple containing: 1. A complex tensor created with cfloat dtype. 2. The tensor converted from real tensor representation to complex tensor using view_as_complex. 3. The real part of the complex tensor accessed using the real attribute. 4. The imaginary part of the complex tensor accessed using the imag attribute. 5. The angle values of the complex tensor. 6. The result of a matrix multiplication between the original complex tensor and its transpose. Returns: tuple: (complex_tensor, complex_from_real, real_part, imag_part, angles, matmul_result) ``` # Instructions 1. **Creating a Complex Tensor**: - Create a 2x2 complex tensor with `dtype=torch.cfloat` using random values. 2. **Transition from Real Representation**: - Create a 2x2 tensor with random values to represent real parts and another 2x2 tensor for imaginary parts. - Use these tensors to create a combined 2x2x2 tensor representing the real and imaginary parts. - Convert this combined tensor to a complex tensor using `view_as_complex`. 3. **Access Real and Imaginary Parts**: - Access and return the real and imaginary parts of the complex tensor. 4. **Compute Angles**: - Compute and return the angles of the complex tensor using `torch.angle`. 5. **Matrix Multiplication**: - Perform a matrix multiplication of the complex tensor with its transpose using `torch.matmul`. # Constraints - Use only the specified PyTorch methods and attributes. - Ensure that the operations are performed on complex tensors, and the results are returned as a tuple in the specified order. # Example ```python result = complex_tensor_operations() print(result) # Output: (complex_tensor, complex_from_real, real_part, imag_part, angles, matmul_result) # where each element should match the described operations. ``` # Notes - Ensure to include necessary imports and initialization within the function. - The function should test itself and print a test case result.","solution":"import torch def complex_tensor_operations() -> tuple: This function performs the following operations and returns a tuple containing: 1. A complex tensor created with cfloat dtype. 2. The tensor converted from real tensor representation to complex tensor using view_as_complex. 3. The real part of the complex tensor accessed using the real attribute. 4. The imaginary part of the complex tensor accessed using the imag attribute. 5. The angle values of the complex tensor. 6. The result of a matrix multiplication between the original complex tensor and its transpose. Returns: tuple: (complex_tensor, complex_from_real, real_part, imag_part, angles, matmul_result) # 1. Create a 2x2 complex tensor with cfloat dtype using random values complex_tensor = torch.randn((2, 2), dtype=torch.cfloat) # 2. Transition from Real Representation real_parts = torch.randn((2, 2)) imaginary_parts = torch.randn((2, 2)) combined_tensor = torch.stack((real_parts, imaginary_parts), dim=-1) complex_from_real = torch.view_as_complex(combined_tensor) # 3. Access Real and Imaginary Parts real_part = complex_tensor.real imag_part = complex_tensor.imag # 4. Compute Angles angles = torch.angle(complex_tensor) # 5. Matrix Multiplication matmul_result = torch.matmul(complex_tensor, complex_tensor.T) return (complex_tensor, complex_from_real, real_part, imag_part, angles, matmul_result)"},{"question":"**Objective:** To test your understanding of Python slice objects by implementing a function that manipulates slices and sequences accordingly. You will demonstrate your ability to handle advanced indexing and slicing. # Problem Statement: You need to implement a function `slice_and_adjust` that takes a sequence and a slice object, then returns a new sequence based on the adjusted slice indices. Function Signature: ```python def slice_and_adjust(seq: list, start: int, stop: int, step: int) -> list: pass ``` Parameters: - `seq (list)` : A list of integers. - `start (int)` : The starting index of the slice. - `stop (int)` : The stopping index of the slice. - `step (int)` : The step of the slice. Returns: - (list): A new list containing the sliced elements from the original list but with adjusted indices based on the length of the list. Constraints: - You cannot use Python’s default slice syntax directly (like `seq[start:stop:step]`). You must manually handle the indices to extract the correct slice. - Consider out-of-bounds indices and adjust them appropriately based on the length of the sequence. # Examples: ```python # Example 1 seq = [1, 2, 3, 4, 5, 6, 7, 8, 9] start = -3 stop = 15 step = 1 assert slice_and_adjust(seq, start, stop, step) == [1, 2, 3, 4, 5, 6, 7, 8, 9] # Example 2 seq = [10, 20, 30, 40, 50, 60] start = 2 stop = 5 step = 1 assert slice_and_adjust(seq, start, stop, step) == [30, 40, 50] # Example 3 seq = [1, 2, 3, 4, 5] start = 1 stop = 7 step = 2 assert slice_and_adjust(seq, start, stop, step) == [2, 4] ``` # Notes: - Handle cases where `start`, `stop`, and `step` are out of the sequence bounds. - Make sure to efficiently process the input without resorting to Python’s built-in slicing capabilities directly. - Raise an appropriate exception if `step` is zero or negative.","solution":"def slice_and_adjust(seq: list, start: int, stop: int, step: int) -> list: # Handle edge cases for step size if step == 0: raise ValueError(\\"Step cannot be zero.\\") if step < 0: raise ValueError(\\"Step cannot be negative.\\") # Adjust start and stop indices n = len(seq) if start < 0: start = max(0, start + n) if stop < 0: stop = max(0, stop + n) # Ensure start and stop are within sequence bounds start = min(max(start, 0), n) stop = min(max(stop, 0), n) # Collect the result using manually adjusted indices result = [] for i in range(start, stop, step): if 0 <= i < n: result.append(seq[i]) return result"},{"question":"# SAX Parsing and Error Handling You are tasked with creating a Python function that parses an XML string and handles the start and end of elements as well as any character data found inside elements. You should also be able to handle and log errors encountered during parsing. Requirements: 1. **Function Signature**: ```python def parse_xml(input_string: str) -> list: ... ``` 2. **Input**: - `input_string`: A string containing valid XML data. 3. **Output**: - A list of tuples containing events in the order they occur. Each tuple is in the form: - `\'start\'`, element_name: Tuple indicating the start of an element. - `\'end\'`, element_name: Tuple indicating the end of an element. - `\'data\'`, data: Tuple containing character data inside an element. 4. **Parsing and Event Handling**: - Use `xml.sax` to parse the input XML string. - Implement a custom `ContentHandler` to handle the following events: - Start of an element. - End of an element. - Character data. - At each of these events, append a corresponding tuple to the output list. 5. **Error Handling**: - Implement a custom `ErrorHandler` that logs parse errors. - If a `SAXParseException` occurs, log an error with: ```python logging.error(f\\"Parse error at line {exc.getLineNumber()}, column {exc.getColumnNumber()}: {exc.getMessage()}\\") ``` Constraints: - You may assume that the XML provided will be well-formed but may contain invalid constructs leading to parse errors. - The parsing should be case insensitive. Example: Given the input string: ```xml <input> <title>Example</title> <content>Some <b>bold</b> text.</content> </input> ``` The expected output should be: ```python [ (\'start\', \'input\'), (\'start\', \'title\'), (\'data\', \'Example\'), (\'end\', \'title\'), (\'start\', \'content\'), (\'data\', \'Some \'), (\'start\', \'b\'), (\'data\', \'bold\'), (\'end\', \'b\'), (\'data\', \' text.\'), (\'end\', \'content\'), (\'end\', \'input\') ] ``` Your implementation should handle and accurately return this structure based on the XML events parsed. Note: - You can use the standard `logging` module to handle error logging. - Ensure all necessary imports and module preparations are in place. Good luck, and happy coding!","solution":"import xml.sax import logging # Set up logging logging.basicConfig(level=logging.ERROR) class CustomContentHandler(xml.sax.ContentHandler): def __init__(self): self.events = [] def startElement(self, name, attrs): self.events.append((\'start\', name)) def endElement(self, name): self.events.append((\'end\', name)) def characters(self, content): if content.strip(): # Avoid appending empty content self.events.append((\'data\', content)) class CustomErrorHandler(xml.sax.ErrorHandler): def error(self, exception): logging.error(f\\"Parse error at line {exception.getLineNumber()}, column {exception.getColumnNumber()}: {exception.getMessage()}\\") def fatalError(self, exception): logging.error(f\\"Fatal error at line {exception.getLineNumber()}, column {exception.getColumnNumber()}: {exception.getMessage()}\\") def warning(self, exception): logging.warning(f\\"Warning at line {exception.getLineNumber()}, column {exception.getColumnNumber()}: {exception.getMessage()}\\") def parse_xml(input_string: str) -> list: handler = CustomContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(CustomErrorHandler()) try: xml.sax.parseString(input_string, handler) except xml.sax.SAXParseException as e: logging.error(f\\"Unhandled SAXParseException: {e}\\") return handler.events"},{"question":"Coding Assessment Question # Problem Description You have been tasked with creating a simplified email parser that can identify and handle various parsing errors according to the provided documentation for the `email.errors` module. Your parser should be able to process email headers and identify specific defects and exceptions as described. # Task 1. **Create Custom Exceptions**: Implement the following custom exceptions that simulate the ones in the `email.errors` module: - `MessageError` - `MessageParseError` (inherits from `MessageError`) - `HeaderParseError` (inherits from `MessageParseError`) - `MultipartConversionError` (inherits from `MessageError` and Python built-in `TypeError`) 2. **Implement a Simple Email Parser**: Write a function `parse_email_headers(content: str) -> dict` that parses email headers from a string and raises the appropriate exceptions and defects. 3. **Defects Handling**: Implement classes for these defects: - `MalformedHeaderDefect` 4. **Function Requirements**: - The `parse_email_headers` function should take a string containing email headers and return a dictionary of headers. - If a header is malformed, it should raise a `HeaderParseError` and append a `MalformedHeaderDefect` to a list of defects. - You should demonstrate the detection and recording of malformed headers. # Input and Output - **Input**: A string representing email headers, with each header on a new line. - **Output**: A tuple containing a dictionary of properly parsed headers and a list of defects. # Constraints - Only headers are provided as input; the body is not included. - Each header will be in the format `Header-Name: value`. # Example ```python email_content = Subject: Test Email From: sender@example.com To receiver@example.com Malformed-Header Date: Mon, 2 Oct 2023 14:00:00 -0400 # Expected Output # ({\'Subject\': \'Test Email\', \'From\': \'sender@example.com\', \'Date\': \'Mon, 2 Oct 2023 14:00:00 -0400\'}, # [MalformedHeaderDefect(\'Malformed-Header\')]) # Malformed-Header does not have a colon, indicating it\'s malformed. ``` # Starter Code ```python class MessageError(Exception): pass class MessageParseError(MessageError): pass class HeaderParseError(MessageParseError): pass class MultipartConversionError(MessageError, TypeError): pass class MalformedHeaderDefect: def __init__(self, header): self.header = header def __str__(self): return f\\"Malformed header found: {self.header}\\" def parse_email_headers(content: str) -> dict: headers = {} defects = [] lines = content.split(\'n\') for line in lines: if \':\' not in line: defects.append(MalformedHeaderDefect(line)) continue name, value = line.split(\':\', 1) headers[name.strip()] = value.strip() return headers, defects # Test your function with the example provided. ``` Implement the function and test cases to ensure it works as described.","solution":"class MessageError(Exception): pass class MessageParseError(MessageError): pass class HeaderParseError(MessageParseError): pass class MultipartConversionError(MessageError, TypeError): pass class MalformedHeaderDefect: def __init__(self, header): self.header = header def __str__(self): return f\\"Malformed header found: {self.header}\\" def parse_email_headers(content: str): headers = {} defects = [] lines = content.split(\'n\') for line in lines: if \':\' not in line: defects.append(MalformedHeaderDefect(line)) continue name, value = line.split(\':\', 1) headers[name.strip()] = value.strip() return headers, defects"},{"question":"# Task: Implement and Test a Function using `xmlrpc.client` # Objective Implement a function that interacts with a provided XML-RPC server, sends a method call using the `ServerProxy`, handles various data types, and processes potential errors. # Instructions 1. **Function Name:** `send_rpc_request` 2. **Input Parameters:** - `uri` (string): The URI of the XML-RPC server. - `method_name` (string): The name of the method to call on the server. - `params` (tuple): The parameters to pass to the server method. - `use_builtin_types` (bool): Whether to use built-in types for date and binary data. Defaults to `False`. 3. **Output:** - Returns the result from the server if the call is successful. - If a `Fault` occurs, returns a dictionary with the `faultCode` and `faultString`. - If a `ProtocolError` occurs, returns a dictionary with the `url`, `errcode`, and `errmsg`. # Constraints: - Do not use any third-party libraries other than `xmlrpc.client` and standard Python libraries such as `datetime`. - You must handle data types such as `DateTime` and `Binary` when `use_builtin_types` is set to `True`. # Example Usage: ```python result = send_rpc_request( uri=\\"http://example.com/RPC2\\", method_name=\\"get_data\\", params=(42,), use_builtin_types=True ) print(result) ``` # Implementation ```python import xmlrpc.client from xmlrpc.client import Fault, ProtocolError def send_rpc_request(uri, method_name, params, use_builtin_types=False): try: with xmlrpc.client.ServerProxy(uri, use_builtin_types=use_builtin_types) as proxy: method = getattr(proxy, method_name) result = method(*params) return result except Fault as fault: return {\\"faultCode\\": fault.faultCode, \\"faultString\\": fault.faultString} except ProtocolError as error: return {\\"url\\": error.url, \\"errcode\\": error.errcode, \\"errmsg\\": error.errmsg} # Example test case if __name__ == \\"__main__\\": result = send_rpc_request( uri=\\"http://localhost:8000/\\", method_name=\\"example_method\\", params=(5, 10), use_builtin_types=False ) print(result) ``` # Notes: - To fully test the `send_rpc_request` function, set up a local XML-RPC server with methods that can handle various data types and validate the outputs. - Ensure the server supports introspection to verify the available methods and their signatures.","solution":"import xmlrpc.client from xmlrpc.client import Fault, ProtocolError def send_rpc_request(uri, method_name, params, use_builtin_types=False): Sends an XML-RPC request to a given server URI, invoking a specified method with parameters and handling errors. Args: uri (str): The URI of the XML-RPC server. method_name (str): The name of the method to call on the server. params (tuple): The parameters to pass to the server method. use_builtin_types (bool): Whether to use built-in types for date and binary data. Defaults to False. Returns: Result from the server method or a dictionary containing fault/ProtocolError details. try: with xmlrpc.client.ServerProxy(uri, use_builtin_types=use_builtin_types) as proxy: method = getattr(proxy, method_name) result = method(*params) return result except Fault as fault: return {\\"faultCode\\": fault.faultCode, \\"faultString\\": fault.faultString} except ProtocolError as error: return {\\"url\\": error.url, \\"errcode\\": error.errcode, \\"errmsg\\": error.errmsg}"},{"question":"# Problem Description: You are given a dataset of monthly sales performance for different products. Your task is to create a function that visualizes this data using the seaborn library. Specifically, you need to create a grouped bar plot using different color palettes for each product category. The function should allow flexibility in choosing the colormap type (continuous or qualitative) and the number of colors. # Function Signature: ```python def visualize_sales_data(sales_data, colormap_type, colormap_name, num_colors, produce_continuous): Creates a grouped bar plot for the sales data using seaborn and the specified colormap. Parameters: - sales_data (pd.DataFrame): A DataFrame with columns [\'Month\', \'Product\', \'Sales\']. - colormap_type (str): Either \'continuous\' or \'qualitative\' to specify the type of colormap to use. - colormap_name (str): The name of the colormap (e.g., \'viridis\' for continuous, or \'Set2\' for qualitative). - num_colors (int): The number of colors to retrieve from the colormap. - produce_continuous (bool): If True, produce a continuous colormap (only for the continuous colormap type). Output: - A seaborn bar plot visualizing the sales data. pass ``` # Input: - `sales_data`: A Pandas DataFrame containing the sales data. It includes three columns: \'Month\' (string), \'Product\' (string), and \'Sales\' (int or float). - `colormap_type`: A string indicating the type of colormap, either \'continuous\' or \'qualitative\'. - `colormap_name`: A string specifying the colormap\'s name. - `num_colors`: An integer specifying the number of colors to fetch from the colormap. - `produce_continuous`: A boolean indicating whether to produce a continuous colormap (applicable only for continuous colormaps). # Output: - The function should create a grouped bar plot using seaborn that visualizes the sales data for each product across different months. Each product category should be distinctly colored according to the specified colormap and its parameters. # Constraints: - Use seaborn for all plotting operations. - Ensure that the resulting plot is clear, with a legend, appropriate axis labels, and a title. - Carefully handle the `produce_continuous` parameter to use continuous colormaps correctly. # Example: ```python import pandas as pd # Sample data data = { \'Month\': [\'January\', \'January\', \'February\', \'February\', \'March\', \'March\'], \'Product\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\'], \'Sales\': [150, 200, 180, 220, 170, 210] } sales_data = pd.DataFrame(data) # Example usage visualize_sales_data(sales_data, \'continuous\', \'viridis\', 5, True) ``` This should produce a grouped bar plot showing the sales of products \'A\' and \'B\' over the first three months, colored using shades from the \'viridis\' colormap.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(sales_data, colormap_type, colormap_name, num_colors, produce_continuous): Creates a grouped bar plot for the sales data using seaborn and the specified colormap. Parameters: - sales_data (pd.DataFrame): A DataFrame with columns [\'Month\', \'Product\', \'Sales\']. - colormap_type (str): Either \'continuous\' or \'qualitative\' to specify the type of colormap to use. - colormap_name (str): The name of the colormap (e.g., \'viridis\' for continuous, or \'Set2\' for qualitative). - num_colors (int): The number of colors to retrieve from the colormap. - produce_continuous (bool): If True, produce a continuous colormap (only for the continuous colormap type). Output: - A seaborn bar plot visualizing the sales data. if colormap_type == \'continuous\': if produce_continuous: cmap = sns.color_palette(colormap_name, as_cmap=True) colors = [cmap(i/num_colors) for i in range(num_colors)] else: cmap = sns.color_palette(colormap_name, num_colors) colors = list(cmap) elif colormap_type == \'qualitative\': colors = sns.color_palette(colormap_name, num_colors) else: raise ValueError(\\"Invalid colormap type. Choose \'continuous\' or \'qualitative\'.\\") # Create a bar plot plt.figure(figsize=(10, 6)) sns.barplot(data=sales_data, x=\'Month\', y=\'Sales\', hue=\'Product\', palette=colors) # Customize the plot plt.title(\'Monthly Sales Performance by Product\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Product\') # Display the plot plt.show()"},{"question":"# Task Implement a Python script that processes command-line options and arguments using the `getopt` module. Your script should recognize both short and long options and support the following functionalities: 1. **Short options:** * `-h` : Display help information. * `-o <file>` : Specify an output file. * `-v` : Enable verbose mode. 2. **Long options:** * `--help` : Display help information. * `--output=<file>` : Specify an output file. * `--verbose` : Enable verbose mode. # Requirements 1. **Output File Handling**: * If the output file option is provided, print \\"Output will be saved to <file>\\". * If the verbose option is enabled, print \\"Verbose mode enabled\\". 2. **Help Handling**: * If the help option is invoked, print the usage information which includes descriptions of all available options. 3. **Argument Handling**: * All non-option arguments should be printed out as a list. # Function Signature ```python def main(argv): # Your code here ``` # Example Given the command line input in `sys.argv`, assume: ```bash python your_script.py --output=test.txt file1 file2 -v ``` Your program should output: ``` Output will be saved to test.txt Verbose mode enabled Non-option arguments: [\'file1\', \'file2\'] ``` If the help option is provided: ```bash python your_script.py --help ``` Your program should output: ``` Usage: python your_script.py [options] [arguments] Options: -h, --help Show this help message and exit. -o FILE, --output=FILE Specify an output file. -v, --verbose Enable verbose mode. ``` # Constraints * You must use the `getopt` module for parsing command-line arguments. * Your function should handle any invalid options or missing required arguments gracefully by printing an error message and exiting. Your solution should demonstrate your understanding of handling command-line arguments in Python using the `getopt` module.","solution":"import sys import getopt def display_help(): help_message = Usage: python your_script.py [options] [arguments] Options: -h, --help Show this help message and exit. -o FILE, --output=FILE Specify an output file. -v, --verbose Enable verbose mode. print(help_message) def main(argv): try: opts, args = getopt.getopt(argv, \\"ho:v\\", [\\"help\\", \\"output=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(str(err)) display_help() sys.exit(2) output_file = None verbose = False for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): display_help() sys.exit() elif opt in (\\"-o\\", \\"--output\\"): output_file = arg elif opt in (\\"-v\\", \\"--verbose\\"): verbose = True if output_file: print(f\\"Output will be saved to {output_file}\\") if verbose: print(\\"Verbose mode enabled\\") if args: print(f\\"Non-option arguments: {args}\\") if __name__ == \\"__main__\\": main(sys.argv[1:])"},{"question":"**Coding Challenge: Implement a Custom Mailbox Filter** You have been hired to implement a custom mailbox filter utility using the provided \\"mailbox\\" module. This utility should read messages from a specified mailbox and move messages with a specified keyword in the subject to another mailbox. This will help users organize their emails more effectively. # Requirements 1. **Input**: - `source_mailbox_path` (str): Path to the source mailbox from which messages will be read. - `destination_mailbox_path` (str): Path to the destination mailbox where filtered messages will be moved. - `keyword` (str): Keyword to search for in the subject of the messages. 2. **Output**: - No explicit output. The function should modify the source and destination mailboxes based on the specified conditions. 3. **Function Definition**: ```python def filter_mailbox_by_keyword(source_mailbox_path: str, destination_mailbox_path: str, keyword: str) -> None: pass ``` # Constraints - The source mailbox will use the mbox format. - The destination mailbox will use the Maildir format. - The keyword search should be case-insensitive. # Performance Requirements - The function should handle large mailboxes efficiently. - Ensure that the function preserves data integrity and avoids data loss or corruption in case of interruptions. # Example Usage ```python source_mailbox_path = \\"/path/to/source/mbox\\" destination_mailbox_path = \\"/path/to/destination/Maildir\\" keyword = \\"Important\\" filter_mailbox_by_keyword(source_mailbox_path, destination_mailbox_path, keyword) ``` # Hints - Use the appropriate `mailbox` classes for handling `mbox` and `Maildir` formats. - Make use of locking mechanisms provided by the `mailbox` module to ensure data consistency. - Check the subject of each message and move it to the destination mailbox if it contains the keyword. - Remove moved messages from the source mailbox to avoid duplication.","solution":"import mailbox import os def filter_mailbox_by_keyword(source_mailbox_path: str, destination_mailbox_path: str, keyword: str) -> None: Move messages with the specified keyword in the subject from the source mailbox to the destination mailbox. Parameters: - source_mailbox_path: Path to the source mailbox (mbox format). - destination_mailbox_path: Path to the destination mailbox (Maildir format). - keyword: Keyword to search for in the subject of the messages. source_mbox = mailbox.mbox(source_mailbox_path) dest_maildir = mailbox.Maildir(destination_mailbox_path, factory=None) # Ensure destination Maildir has required subdirectories for dirname in (\'cur\', \'new\', \'tmp\'): os.makedirs(os.path.join(destination_mailbox_path, dirname), exist_ok=True) keyword_lower = keyword.lower() for key, message in list(source_mbox.items()): subject = message[\'subject\'] if subject and keyword_lower in subject.lower(): # Remove the message from the source mailbox source_mbox.lock() source_mbox.remove(key) source_mbox.flush() source_mbox.unlock() # Append the message to the destination Maildir dest_maildir.add(message) dest_maildir.flush()"},{"question":"You are given data representing the relationship between temperature (in degrees Celsius) and energy consumption (in kWh) over a period of time. Your task is to use Isotonic Regression from the scikit-learn library to fit a model that predicts energy consumption based on temperature. Requirements 1. **Input:** - A CSV file `temperature_energy.csv` with two columns: `Temperature` and `Energy Consumption`. - `Temperature` is the independent variable (X) and `Energy Consumption` is the dependent variable (y). 2. **Implementation:** - Load the data from the CSV file. - Split the data into training (80%) and test sets (20%). - Apply Isotonic Regression to the training data. - Very importantly, specify the parameter `increasing=\'auto\'` when initializing the `IsotonicRegression` model. - Use the trained model to predict energy consumption on the test set. 3. **Output:** - Calculate and print the Mean Squared Error (MSE) of the predictions on the test set. - Plot the original data points and the fitted isotonic regression line. Constraints - You may assume there are no missing values in the data. - You may use any typical library for data handling and visualization, but the main regression model should use `IsotonicRegression` from `sklearn.isotonic`. ```python import pandas as pd from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split # Load the data from CSV data = pd.read_csv(\'temperature_energy.csv\') # Split data into X (Temperature) and y (Energy Consumption) X = data[\'Temperature\'] y = data[\'Energy Consumption\'] # Split the data into training (80%) and test sets (20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and fit an Isotonic Regression model ir = IsotonicRegression(increasing=\'auto\') y_train_pred = ir.fit_transform(X_train, y_train) # Predict on the test set y_test_pred = ir.transform(X_test) # Calculate the Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_test_pred) print(f\\"Mean Squared Error on the test set: {mse}\\") # Plot the original data and the fitted isotonic regression line plt.scatter(X, y, color=\'blue\', label=\'Original data\') plt.plot(X_train, y_train_pred, color=\'red\', label=\'Fitted isotonic regression line\') plt.xlabel(\'Temperature (Celsius)\') plt.ylabel(\'Energy Consumption (kWh)\') plt.legend() plt.title(\'Isotonic Regression Fit\') plt.show() ``` Use this code template to implement the required functionality and ensure your model fits the data correctly and performs the desired predictions.","solution":"import pandas as pd from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split def predict_energy_consumption(csv_file): # Load the data from CSV data = pd.read_csv(csv_file) # Split data into X (Temperature) and y (Energy Consumption) X = data[\'Temperature\'] y = data[\'Energy Consumption\'] # Split the data into training (80%) and test sets (20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and fit an Isotonic Regression model ir = IsotonicRegression(increasing=\'auto\') y_train_pred = ir.fit_transform(X_train, y_train) # Predict on the test set y_test_pred = ir.transform(X_test) # Calculate the Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_test_pred) print(f\\"Mean Squared Error on the test set: {mse}\\") # Plot the original data and the fitted isotonic regression line plt.scatter(X, y, color=\'blue\', label=\'Original data\') plt.plot(sorted(X_train), [y_pred for _, y_pred in sorted(zip(X_train, y_train_pred))], color=\'red\', label=\'Fitted isotonic regression line\') plt.xlabel(\'Temperature (Celsius)\') plt.ylabel(\'Energy Consumption (kWh)\') plt.legend() plt.title(\'Isotonic Regression Fit\') plt.show() return mse"},{"question":"You\'re tasked with implementing a Python class that mimics some functionalities provided by the C-level sequence API in the `PySequence_Fast*` family. This exercise will test your understanding of sequences and various sequence operations in Python. # Question Implement a class `SequenceOperations` that provides the following methods: 1. `seq_check(obj)`: - **Input**: Any object `obj`. - **Output**: Returns `True` if `obj` provides the sequence protocol (has `__getitem__` method and is not a subclass of dict), otherwise `False`. 2. `seq_size(seq)`: - **Input**: A sequence object `seq`. - **Output**: Returns the number of elements in `seq`. 3. `seq_concat(seq1, seq2)`: - **Input**: Two sequence objects `seq1` and `seq2`. - **Output**: Returns a new sequence which is the concatenation of `seq1` and `seq2`. 4. `seq_repeat(seq, count)`: - **Input**: A sequence object `seq` and an integer `count`. - **Output**: Returns a new sequence which is the repetition of `seq` `count` times. 5. `seq_slice(seq, start, end)`: - **Input**: A sequence object `seq` and two integer indices `start` and `end`. - **Output**: Returns a slice of `seq` from index `start` to `end`. 6. `seq_getitem(seq, index)`: - **Input**: A sequence object `seq` and an integer `index`. - **Output**: Returns the element at `index` in `seq`. 7. `seq_count(seq, value)`: - **Input**: A sequence object `seq` and a value `value`. - **Output**: Returns the number of occurrences of `value` in `seq`. 8. `seq_contains(seq, value)`: - **Input**: A sequence object `seq` and a value `value`. - **Output**: Returns `True` if `value` is present in `seq`, otherwise `False`. # Implementation Constraints: - You must use the methods provided by the Python `collections.abc.Sequence` module wherever applicable. - Raise a `TypeError` with a proper message if the input `seq` does not support the sequence protocol. # Example: ```python so = SequenceOperations() # Checking if an object is a sequence assert so.seq_check([1, 2, 3]) == True assert so.seq_check({1: \'a\', 2: \'b\'}) == False # Getting the size of a sequence assert so.seq_size([1, 2, 3]) == 3 # Concatenating sequences assert so.seq_concat([1, 2], [3, 4]) == [1, 2, 3, 4] # Repeating sequences assert so.seq_repeat([1, 2], 2) == [1, 2, 1, 2] # Getting a slice assert so.seq_slice([1, 2, 3, 4], 1, 3) == [2, 3] # Getting an item by index assert so.seq_getitem([1, 2, 3, 4], 2) == 3 # Counting occurrences of a value assert so.seq_count([1, 2, 2, 3], 2) == 2 # Checking if a value is in the sequence assert so.seq_contains([1, 2, 3], 2) == True assert so.seq_contains([1, 2, 3], 5) == False ```","solution":"from collections.abc import Sequence class SequenceOperations: @staticmethod def seq_check(obj): Returns True if obj provides the sequence protocol (has __getitem__ method and is not a subclass of dict), otherwise False. return isinstance(obj, Sequence) and not isinstance(obj, dict) @staticmethod def seq_size(seq): Returns the number of elements in the given sequence. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return len(seq) @staticmethod def seq_concat(seq1, seq2): Returns a new sequence which is the concatenation of seq1 and seq2. Raises TypeError if the inputs do not support the sequence protocol. if not SequenceOperations.seq_check(seq1) or not SequenceOperations.seq_check(seq2): raise TypeError(\\"Inputs must be sequences.\\") return seq1 + seq2 @staticmethod def seq_repeat(seq, count): Returns a new sequence which is the repetition of seq count times. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return seq * count @staticmethod def seq_slice(seq, start, end): Returns a slice of seq from index start to end. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return seq[start:end] @staticmethod def seq_getitem(seq, index): Returns the element at index in seq. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return seq[index] @staticmethod def seq_count(seq, value): Returns the number of occurrences of value in seq. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return seq.count(value) @staticmethod def seq_contains(seq, value): Returns True if value is present in seq, otherwise False. Raises TypeError if the input does not support the sequence protocol. if not SequenceOperations.seq_check(seq): raise TypeError(\\"Input must be a sequence.\\") return value in seq"},{"question":"# Python Coding Assessment Question: Using `pipes` and `subprocess` Modules You are tasked with processing text files by utilizing pipeline concepts from both the deprecated `pipes` module and the recommended `subprocess` module. Your goal is to understand the traditional approach and then migrate to the updated approach. Part 1: Using `pipes` Module You need to create a function that: 1. Uses the `pipes.Template` class to create a pipeline that: - Converts all text to uppercase. - Sorts the lines in alphabetical order. - Removes duplicate lines. 2. Writes the processed output to a given output file. Part 2: Using `subprocess` Module You will then create an equivalent function using the `subprocess` module to achieve the same result. Function Specifications **Part 1 Function: `process_with_pipes(input_file: str, output_file: str) -> None`** - **Input**: - `input_file` (str) - Path to the input text file. - `output_file` (str) - Path to the output text file. - **Output**: - None (writes the processed contents to the output file). - **Constraints**: - Assume the input file exists and is readable. **Part 2 Function: `process_with_subprocess(input_file: str, output_file: str) -> None`** - **Input**: - `input_file` (str) - Path to the input text file. - `output_file` (str) - Path to the output text file. - **Output**: - None (writes the processed contents to the output file). - **Constraints**: - Assume the input file exists and is readable. Your solution should include: 1. Correct implementation of both functions. 2. Demonstration of switching from the deprecated `pipes` module to the modern `subprocess` module. Example: Input File (`input.txt`): ``` banana Apple banana Cherry apple ``` Expected Output File (`output.txt`): ``` APPLE BANANA CHERRY ``` --- Implementation Guidance: For `process_with_pipes`: - Use `Template` to create a pipeline. - Use `append` to add commands for converting to uppercase, sorting, and removing duplicates. - Use `copy` to process the input file through the pipeline and write to the output file. For `process_with_subprocess`: - Use `subprocess.Popen` to create a series of subprocesses to achieve the same result. - Chain the processes using pipes (e.g., `subprocess.PIPE`). ```python import pipes import subprocess def process_with_pipes(input_file: str, output_file: str) -> None: t = pipes.Template() t.append(\'tr a-z A-Z\', \'--\') t.append(\'sort\', \'-f\') t.append(\'uniq\', \'-\') t.copy(input_file, output_file) def process_with_subprocess(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: p1 = subprocess.Popen([\'tr\', \'a-z\', \'A-Z\'], stdin=infile, stdout=subprocess.PIPE) p2 = subprocess.Popen([\'sort\', \'-f\'], stdin=p1.stdout, stdout=subprocess.PIPE) p3 = subprocess.Popen([\'uniq\'], stdin=p2.stdout, stdout=outfile) p1.stdout.close() p2.stdout.close() p3.communicate() ``` ---","solution":"import pipes import subprocess def process_with_pipes(input_file: str, output_file: str) -> None: t = pipes.Template() t.append(\'tr a-z A-Z\', \'--\') t.append(\'sort -f\', \'--\') t.append(\'uniq\', \'--\') t.copy(input_file, output_file) def process_with_subprocess(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: p1 = subprocess.Popen([\'tr\', \'a-z\', \'A-Z\'], stdin=infile, stdout=subprocess.PIPE) p2 = subprocess.Popen([\'sort\', \'-f\'], stdin=p1.stdout, stdout=subprocess.PIPE) p3 = subprocess.Popen([\'uniq\'], stdin=p2.stdout, stdout=outfile) p1.stdout.close() p2.stdout.close() p3.communicate()"},{"question":"Cross-Validation in Scikit-Learn Objective: To assess the student\'s ability to implement cross-validation techniques using the scikit-learn library for a machine learning model. Problem Statement: You are given the **Wine Dataset** from the UCI Machine Learning Repository. This dataset contains various chemical properties of wines derived from three different cultivars. Your task is to perform a comprehensive evaluation of the performance of a Support Vector Machine (SVM) classifier using different cross-validation techniques. Tasks: 1. Load the Wine dataset using `sklearn.datasets.load_wine`. 2. Standardize the dataset using `StandardScaler` from `sklearn.preprocessing`. 3. Implement a Support Vector Machine (SVM) classifier with a linear kernel using `sklearn.svm.SVC`. 4. Perform the following cross-validation strategies and compute the accuracy for each: - **K-Fold Cross-Validation**: Use 5 folds. - **Stratified K-Fold Cross-Validation**: Use 5 folds to ensure class balance. - **Leave-One-Out Cross-Validation**. 5. Report the mean accuracy and standard deviation for each of the above cross-validation methods. 6. Use the **cross_val_predict** function to obtain the predicted labels and create a confusion matrix. Input Format: - No explicit input; the dataset will be loaded from `sklearn`. Output Format: - Mean accuracy and standard deviation for each of the cross-validation methods. - Confusion matrix from the predictions made by `cross_val_predict`. Code template and example: ```python from sklearn.datasets import load_wine from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut, cross_val_predict from sklearn.metrics import confusion_matrix # Load Wine Dataset data = load_wine() X, y = data.data, data.target # Standardize the Dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Define SVM classifier svm = SVC(kernel=\'linear\') # K-Fold Cross-Validation kf = KFold(n_splits=5) kf_scores = cross_val_score(svm, X_scaled, y, cv=kf) print(f\\"K-Fold CV Accuracy: Mean={kf_scores.mean():.2f}, Std={kf_scores.std():.2f}\\") # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5) skf_scores = cross_val_score(svm, X_scaled, y, cv=skf) print(f\\"Stratified K-Fold CV Accuracy: Mean={skf_scores.mean():.2f}, Std={skf_scores.std():.2f}\\") # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(svm, X_scaled, y, cv=loo) print(f\\"Leave-One-Out CV Accuracy: Mean={loo_scores.mean():.2f}, Std={loo_scores.std():.2f}\\") # Cross-Validation Predictions and Confusion Matrix y_pred = cross_val_predict(svm, X_scaled, y, cv=skf) conf_mat = confusion_matrix(y, y_pred) print(\\"Confusion Matrix:\\") print(conf_mat) ``` # Constraints: - Use `random_state=42` wherever `random_state` is applicable. - Ensure that all evaluation metrics are printed with two decimal places. Notes: - The focus is on understanding and implementing cross-validation techniques correctly. - Ensure to handle any potential errors or issues that may arise due to the dataset splits.","solution":"from sklearn.datasets import load_wine from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut, cross_val_predict from sklearn.metrics import confusion_matrix def evaluate_wine_dataset(): # Load Wine Dataset data = load_wine() X, y = data.data, data.target # Standardize the Dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Define SVM classifier svm = SVC(kernel=\'linear\', random_state=42) results = {} # K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(svm, X_scaled, y, cv=kf) results[\'kf_mean\'] = kf_scores.mean() results[\'kf_std\'] = kf_scores.std() # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(svm, X_scaled, y, cv=skf) results[\'skf_mean\'] = skf_scores.mean() results[\'skf_std\'] = skf_scores.std() # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(svm, X_scaled, y, cv=loo) results[\'loo_mean\'] = loo_scores.mean() results[\'loo_std\'] = loo_scores.std() # Cross-Validation Predictions and Confusion Matrix y_pred = cross_val_predict(svm, X_scaled, y, cv=skf) conf_mat = confusion_matrix(y, y_pred) results[\'conf_matrix\'] = conf_mat return results"},{"question":"# PyTorch Named Tensor Matrix Operation **Objective**: The task is to implement a function `named_tensor_matrix_operations` in PyTorch utilizing named tensors. This function should perform the matrix multiplication on two tensors followed by a series of element-wise absolute and trigonometric (sine) operations, while ensuring the correct propagation of tensor names. # Function Signature: ```python import torch def named_tensor_matrix_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` # Parameters: - `tensor_a` (torch.Tensor): A 2D or higher-dimensional tensor with named dimensions. - `tensor_b` (torch.Tensor): A 2D or higher-dimensional tensor with named dimensions, where the innermost two dimensions are valid based on matrix multiplication rules with `tensor_a`. # Returns: - `torch.Tensor`: The resultant tensor after matrix multiplication followed by the absolute and sine operations, maintaining correct dimension names. # Constraints: - You can assume the input tensors `tensor_a` and `tensor_b` have compatible shapes for matrix multiplication. - Raise a `ValueError` if the tensors\' named dimensions are not aligned properly according to the specified rules. - Use proper name propagation rules as described in the documentation. # Performance Requirements: - Ensure the operations are efficient and leverage PyTorch’s built-in functions wherever applicable. # Example: ```python a = torch.randn(3, 3, names=(\'N\', \'D\')) b = torch.randn(3, 3, names=(\'D\', \'C\')) result = named_tensor_matrix_operations(a, b) # The result\'s expected names are (\'N\', \'C\') print(result.names) # Output: (\'N\', \'C\') ``` # Notes: - Use matrix multiplication rules to determine the resulting names after multiplying `tensor_a` and `tensor_b`. - The element-wise absolute and sine operations should propagate the names from the previous step. # Implementation Requirements: 1. Perform matrix multiplication between `tensor_a` and `tensor_b`. 2. Apply the absolute operation to the result. 3. Apply the sine operation to the result from step 2. 4. Return the final tensor ensuring the names are correctly propagated through all operations.","solution":"import torch def named_tensor_matrix_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Perform matrix multiplication between tensor_a and tensor_b, followed by element-wise absolute and sine operations, maintaining the correct dimension names. Parameters: tensor_a (torch.Tensor): A 2D or higher-dimensional tensor with named dimensions. tensor_b (torch.Tensor): A 2D or higher-dimensional tensor with named dimensions, where the innermost two dimensions are valid based on matrix multiplication rules with tensor_a. Returns: torch.Tensor: The resulting tensor after matrix multiplication, absolute, and sine operations. # Perform matrix multiplication on named tensors result = torch.matmul(tensor_a, tensor_b) # Apply element-wise absolute operation result = torch.abs(result) # Apply element-wise sine operation result = torch.sin(result) return result"},{"question":"**Objective:** Implement a set of functions to process sales data using advanced functional programming techniques provided by Python\'s `itertools`, `functools`, and `operator` modules. You will perform operations like filtering, summing, and partial application of functions. **Problem Statement:** You are given a list of sales transactions for a retail store. Each transaction is represented as a tuple `(product_id, category, quantity, price_per_unit)`. Your task is to process this list to extract useful information. Implement the following functions: 1. **group_sales_by_category(transactions: List[Tuple[int, str, int, float]]) -> Dict[str, List[Tuple[int, int, float]]]** - Groups transactions by product category. - **Input:** A list of tuples, where each tuple contains `(product_id, category, quantity, price_per_unit)`. - **Output:** A dictionary where the keys are categories and the values are lists of tuples containing `(product_id, quantity, total_price)`. 2. **total_sales_value(category_sales: Dict[str, List[Tuple[int, int, float]]]) -> Dict[str, float]** - Computes the total sales value for each category. - **Input:** A dictionary where the keys are categories, and the values are lists of tuples `(product_id, quantity, total_price)`. - **Output:** A dictionary where the keys are categories and the values are their respective total sales values. 3. **filter_high_value_sales(category_sales: Dict[str, float], threshold: float) -> Dict[str, float]** - Filters out categories with a total sales value below a specified threshold. - **Input:** - `category_sales`: A dictionary where the keys are categories and the values are their respective total sales values. - `threshold`: A float representing the minimum sales value required to include a category in the output. - **Output:** A dictionary where the keys are categories and the values are their total sales values, filtered to include only those categories with sales values above the threshold. **Constraints:** - You must use the `itertools`, `functools`, and `operator` modules as appropriate. - Your solution must handle large data efficiently. **Python Function Signatures:** ```python from typing import List, Tuple, Dict def group_sales_by_category(transactions: List[Tuple[int, str, int, float]]) -> Dict[str, List[Tuple[int, int, float]]]: pass def total_sales_value(category_sales: Dict[str, List[Tuple[int, int, float]]]) -> Dict[str, float]: pass def filter_high_value_sales(category_sales: Dict[str, float], threshold: float) -> Dict[str, float]: pass ``` # Example: ```python transactions = [ (1, \'electronics\', 5, 300.0), (2, \'electronics\', 2, 450.0), (3, \'furniture\', 1, 200.0), (4, \'furniture\', 3, 150.0), (5, \'clothing\', 10, 20.0) ] grouped_sales = group_sales_by_category(transactions) # Expected output: # {\'electronics\': [(1, 5, 1500.0), (2, 2, 900.0)], # \'furniture\': [(3, 1, 200.0), (4, 3, 450.0)], # \'clothing\': [(5, 10, 200.0)]} sales_values = total_sales_value(grouped_sales) # Expected output: # {\'electronics\': 2400.0, # \'furniture\': 650.0, # \'clothing\': 200.0} high_value_sales = filter_high_value_sales(sales_values, 1000.0) # Expected output: # {\'electronics\': 2400.0} ```","solution":"from typing import List, Tuple, Dict from collections import defaultdict from itertools import groupby from operator import itemgetter from functools import reduce def group_sales_by_category(transactions: List[Tuple[int, str, int, float]]) -> Dict[str, List[Tuple[int, int, float]]]: sorted_transactions = sorted(transactions, key=itemgetter(1)) grouped_transactions = groupby(sorted_transactions, key=itemgetter(1)) result = {} for category, items in grouped_transactions: result[category] = [(item[0], item[2], item[2] * item[3]) for item in items] return result def total_sales_value(category_sales: Dict[str, List[Tuple[int, int, float]]]) -> Dict[str, float]: return {category: sum(item[2] for item in items) for category, items in category_sales.items()} def filter_high_value_sales(category_sales: Dict[str, float], threshold: float) -> Dict[str, float]: return {category: total for category, total in category_sales.items() if total > threshold}"},{"question":"# Advanced Seaborn Assessment: Pairing and Customizing Plots Objective: To assess your understanding of working with the `seaborn.objects` module to create advanced and customized visualizations, you will need to demonstrate your ability to pair variables in plots, use faceting, and customize labels and other plot attributes. Problem Statement: You are given a dataset named `mpg`, which contains data about various cars, including the columns `mpg`, `displacement`, `horsepower`, `weight`, `acceleration`, `cylinders`, and `origin`. Your task is to create several visualizations as specified below. Instructions: 1. **Import the Necessary Libraries:** - Ensure you import `seaborn.objects as so`. - Import the `mpg` dataset using `seaborn.load_dataset(\\"mpg\\")`. 2. **Create a plot with the following specifications:** - Plot `acceleration` against both `displacement` and `weight`. - Add dots to the plot. 3. **Create a second plot with the following specifications:** - Plot pairwise relationships between `displacement`, `weight`, `horsepower`, and `acceleration`. - Add dots to the plot. 4. **Create a third plot with custom labels:** - Plot `mpg` against both `weight` and `displacement`. - Label `weight` on the x-axis as \\"Weight (lb)\\" and `displacement` as \\"Displacement (cu in)\\". - Label the y-axis as `Miles per Gallon (MPG)`. - Add dots to the plot. 5. **Create a faceting plot:** - Plot `weight` against `horsepower` and `acceleration`. - Use faceting to create different columns based on the `origin` of the cars. - Add dots to the plot. Expected Output: Each plot should be displayed sequentially, showcasing the correct pairing, dots addition, labeling, and faceting as specified. Code Template: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # 1. First Plot plot1 = ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ) plot1 # 2. Second Plot plot2 = ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) plot2 # 3. Third Plot with Custom Labels plot3 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plot3 # 4. Faceting Plot plot4 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) plot4 ``` Constraints: - Ensure that the dataset is loaded correctly and the plots adhere to the specifications. - Make sure all plots are displayed correctly. - Customize the labels as described. - Implement faceting correctly for the specified column. Performance Requirements: Your code should execute efficiently without running into performance issues when handling the provided dataset.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # 1. First Plot plot1 = ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ) plot1 # 2. Second Plot plot2 = ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) plot2 # 3. Third Plot with Custom Labels plot3 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plot3 # 4. Faceting Plot plot4 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) plot4"},{"question":"# Question: HTML Table Parser You are asked to implement a Python function that uses subclassing of `HTMLParser` to extract the content of an HTML table and return the data in a structured format. The function should be able to handle tables with nested tags and should extract data from different rows and columns correctly. Function Signature ```python def extract_table_data(html_content: str) -> list: # Your code here ``` Input - `html_content` (str): A string representing the HTML content containing a single HTML table. The table may contain nested tags within `<td>` elements. Output - A list of lists, where each inner list represents a row of the table, and each element in the inner list represents cell content as a string. Constraints - You can assume the input HTML content will only contain one table with properly nested tags inside `<td>` elements. - Do not use external libraries such as BeautifulSoup; only use the `html.parser.HTMLParser` class for this task. Example ```python html_content = <table> <tr><td>Row 1, Cell 1</td><td>Row 1, <b>Cell 2</b></td></tr> <tr><td>Row 2, Cell 1</td><td>Row 2, Cell 2</td></tr> </table> result = extract_table_data(html_content) print(result) # Output: [[\'Row 1, Cell 1\', \'Row 1, Cell 2\'], [\'Row 2, Cell 1\', \'Row 2, Cell 2\']] ``` Instructions 1. Subclass `HTMLParser` and override relevant methods to gather data from the table. 2. Ensure that nested tags inside the table cells are handled correctly, and only the text content is extracted. 3. Your function should return a list of lists, representing the rows and cells of the table. Note - Focus on correctly implementing the subclass and ensuring accurate extraction of table data. - Ensure to handle nested HTML tags within table cells appropriately. Good luck and happy coding!","solution":"from html.parser import HTMLParser class TableHTMLParser(HTMLParser): def __init__(self): super().__init__() self.in_td = False self.current_row = [] self.table_data = [] self.current_data = \\"\\" def handle_starttag(self, tag, attrs): if tag == \\"td\\": self.in_td = True def handle_endtag(self, tag): if tag == \\"td\\": self.in_td = False self.current_row.append(self.current_data.strip()) self.current_data = \\"\\" elif tag == \\"tr\\": if self.current_row: self.table_data.append(self.current_row) self.current_row = [] def handle_data(self, data): if self.in_td: self.current_data += data def extract_table_data(html_content: str) -> list: parser = TableHTMLParser() parser.feed(html_content) return parser.table_data"},{"question":"<|Analysis Begin|> The provided documentation describes the Python `uuid` module, which is used for generating unique identifiers as per RFC 4122. The module provides several functions for generating UUIDs (uuid1, uuid3, uuid4, uuid5) based on different methods like timestamp, names, random values, and hashing. It also defines the `UUID` class for creating UUID objects from different input formats like hex strings, byte strings, tuples of integers, and single integers. The `UUID` class has several attributes for accessing different representations and components of the UUID, such as `bytes`, `bytes_le`, `fields`, `hex`, `int`, `urn`, `variant`, `version`, and `is_safe`. Additionally, there are constants for namespace identifiers and variant values. Given this, a challenging and clear coding question can be crafted that requires understanding the creation and manipulation of UUID objects, including handling various input formats and extracting information from UUID instances. <|Analysis End|> <|Question Begin|> # UUID Manipulation and Comparison Problem Statement You are working on a system that requires generating and managing unique identifiers for different entities. Your task is to implement a function that: 1. Generates UUIDs using various methods provided by the `uuid` module. 2. Converts these UUIDs to different formats. 3. Extracts and compares specific parts of the UUIDs. 4. Identifies if generated UUIDs were created in a multiprocessing-safe manner. Requirements Implement a Python function `uuid_operations()` that performs the following steps: 1. Generates three UUIDs: - `uuid1` from `uuid.uuid1()`. - `uuid4` from `uuid.uuid4()`. - `uuid5` from `uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\')`. 2. Converts these UUIDs to the following formats: - String representation (using `str()`). - Hexadecimal string (using `hex` attribute). - Integer representation (using `int` attribute). 3. Extracts specific parts from the `uuid1` and `uuid5` UUIDs: - `time_low` and `node` fields (from the `fields` attribute) from `uuid1`. - `version` and `variant` of the UUID (using respective attributes) from `uuid5`. 4. Compares the two extracted parts from `uuid1` with the corresponding attributes (hex and int representations) of the same UUID to ensure consistency. 5. Checks if the generated UUIDs are safe (using the `is_safe` attribute). Function Signature ```python def uuid_operations(): pass ``` Example Output ```python uuid_operations() ``` This function should output something like: ``` { \'uuid1\': { \'str\': \'a8098c1a-f86e-11da-bd1a-00112444be1e\', \'hex\': \'a8098c1af86e11dabd1a00112444be1e\', \'int\': 138350580552821637402110307775926899358, \'fields\': { \'time_low\': 2814732034, \'node\': 462125290413158 }, \'is_safe\': \'unknown\' }, \'uuid4\': { \'str\': \'16fd2706-8baf-433b-82eb-8c7fada847da\', \'hex\': \'16fd27068baf433b82eb8c7fada847da\', \'int\': 30739657773987595764812289727742973114, \'is_safe\': \'unknown\' }, \'uuid5\': { \'str\': \'886313e1-3b8a-5372-9b90-0c9aee199e5d\', \'hex\': \'886313e13b8a53729b900c9aee199e5d\', \'int\': 181773866549084949409023530465591699797, \'version\': 5, \'variant\': \'RFC_4122\', \'is_safe\': \'unknown\' } } ``` --- Write the `uuid_operations` function to satisfy the requirements above.","solution":"import uuid def uuid_operations(): # Generate UUIDs using various methods uuid1 = uuid.uuid1() uuid4 = uuid.uuid4() uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\') # Extract different formats for uuid1 uuid1_str = str(uuid1) uuid1_hex = uuid1.hex uuid1_int = uuid1.int # Extract specific fields from uuid1 uuid1_time_low = uuid1.fields[0] uuid1_node = uuid1.node # Extract different formats for uuid4 uuid4_str = str(uuid4) uuid4_hex = uuid4.hex uuid4_int = uuid4.int # Extract different formats for uuid5 uuid5_str = str(uuid5) uuid5_hex = uuid5.hex uuid5_int = uuid5.int # Extract specific fields from uuid5 uuid5_version = uuid5.version uuid5_variant = uuid5.variant # Check if generated UUIDs are safe uuid1_is_safe = uuid1.is_safe uuid4_is_safe = uuid4.is_safe uuid5_is_safe = uuid5.is_safe # Compare extracted fields consistency_check = (uuid1_time_low == uuid1_int >> 96) and (uuid1_node == (uuid1_int & (1 << 48) - 1)) return { \'uuid1\': { \'str\': uuid1_str, \'hex\': uuid1_hex, \'int\': uuid1_int, \'fields\': { \'time_low\': uuid1_time_low, \'node\': uuid1_node }, \'is_safe\': uuid1_is_safe }, \'uuid4\': { \'str\': uuid4_str, \'hex\': uuid4_hex, \'int\': uuid4_int, \'is_safe\': uuid4_is_safe }, \'uuid5\': { \'str\': uuid5_str, \'hex\': uuid5_hex, \'int\': uuid5_int, \'version\': uuid5_version, \'variant\': uuid5_variant, \'is_safe\': uuid5_is_safe }, \'consistency_check\': consistency_check }"},{"question":"Coding Assessment Question # Objective Design a custom `SortedList` class which maintains its elements in sorted order using the `bisect` module. You need to implement the following methods: 1. `__init__(self, iterable=[])`: - Initializes the `SortedList` with the elements of `iterable`, maintaining sorted order. 2. `add(self, value)`: - Inserts `value` into the `SortedList`, maintaining sorted order. 3. `remove(self, value)`: - Removes the first occurrence of `value` from the `SortedList`. Raises a `ValueError` if `value` is not present. 4. `contains(self, value)`: - Returns `True` if `value` is in the `SortedList`, `False` otherwise. 5. `__len__(self)`: - Returns the number of elements in the `SortedList`. 6. `__getitem__(self, index)`: - Returns the element at the specified `index`. # Constraints - The elements in the `SortedList` must be sortable. - The operations must efficiently maintain the sorted order using the `bisect` module. # Example ```python from bisect import bisect_left, bisect_right, insort_left, insort_right class SortedList: def __init__(self, iterable=[]): self._list = sorted(iterable) def add(self, value): insort_right(self._list, value) def remove(self, value): index = bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: self._list.pop(index) else: raise ValueError(f\\"{value} not in list\\") def contains(self, value): index = bisect_left(self._list, value) return index != len(self._list) and self._list[index] == value def __len__(self): return len(self._list) def __getitem__(self, index): return self._list[index] ``` Write a function `test_sorted_list()` that tests all methods of the `SortedList` class to ensure they work as expected. # Input Format There is no specific input format as the class and test function should not read from standard input. # Output Format The `test_sorted_list` function should print results of assertions or errors if any, verifying correctness of the `SortedList`. # Constraints - All functionality must maintain the natural sorted order of the elements. - Use the `bisect` module where appropriate to maintain performance.","solution":"from bisect import bisect_left, insort_right class SortedList: def __init__(self, iterable=[]): self._list = sorted(iterable) def add(self, value): insort_right(self._list, value) def remove(self, value): index = bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: self._list.pop(index) else: raise ValueError(f\\"{value} not in list\\") def contains(self, value): index = bisect_left(self._list, value) return index != len(self._list) and self._list[index] == value def __len__(self): return len(self._list) def __getitem__(self, index): return self._list[index]"},{"question":"**Objective:** Demonstrate your understanding of the seaborn library, specifically working with seaborn objects to create and customize plots. # Problem Statement You are given a dataset `tips` containing information about restaurant tips. Your task is to create a Python function using seaborn objects that generates a bar plot comparing the total bill amounts by day, split by gender. # Function Signature ```python def create_custom_bar_plot(): pass ``` # Input and Output - **Input:** No function parameters are required. - **Output:** The function should display a customized bar plot using the seaborn library. # Requirements: 1. Load the `tips` dataset from seaborn using `load_dataset`. 2. Create a grouped bar plot to display the sum of total bills (`total_bill`) for each day (`day`), distinguished by gender (`sex`). 3. Apply the `Dodge` transformation to separate the bars by gender. 4. Add a reasonable gap between the bars for better visualization. 5. Ensure that the colors of the bars represent different genders. # Example Plot - The x-axis should represent the days of the week. - The y-axis should represent the sum of the total bill amounts. - Provide legends to distinguish between genders. # Constraints: - Use only seaborn objects (the latest API) to create the plot. - Ensure the plot is clear and visually appealing. **Hints:** - Use `.add()` method to add bars and dodge transformation. - Use `Agg` to aggregate sum of the total bills. # Performance Considerations: - Ensure the plot rendering is efficient and without excessive delays. - Handle any missing or unexpected data gracefully. # Solution You can provide the solution inside the function or in a separate code cell if working in Jupyter Notebook. ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_bar_plot(): tips = load_dataset(\\"tips\\") plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2)) ) plot.show() ``` Ensure that you run the function `create_custom_bar_plot()` and verify that the bar plot displays correctly before submission.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_bar_plot(): Creates and displays a customized bar plot comparing the total bill amounts by day, split by gender using the seaborn objects API. # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the bar plot plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2)) ) # Show the plot plot.show()"},{"question":"# Question You are tasked with creating a custom neural network module in PyTorch and scripting it using TorchScript. This will involve defining the structure of the network, using appropriate type annotations, and employing control statements and TorchScript-specific APIs where necessary. Requirements 1. Define a custom class `MyCustomNet` that inherits from `torch.nn.Module`. 2. This class should have: - An `__init__` method that initializes at least two `torch.nn.Linear` layers. - A forward method that takes a tensor `x` as input and passes it through the linear layers in a sequential manner. - Use an `if` statement to apply a non-linear activation function (like `torch.relu`) to the output of the first layer if a condition is met (e.g., the mean value of the tensor is above a certain threshold). - Use a `for` loop to iterate over a range of integers and perform an addition with each integer to the output tensor. 3. Script the module using `torch.jit.script`. 4. Demonstrate the execution of the scripted model with a sample input tensor. Input and Output Formats - **Input**: A PyTorch tensor of shape `(N, D_in)` where `N` is the batch size and `D_in` is the input feature dimension. - **Output**: A PyTorch tensor of shape `(N, D_out)` where `D_out` is the output feature dimension of the second linear layer. Constraints - Ensure all necessary type annotations are used for function signatures and class attributes. - Use TorchScript\'s typing utilities such as `torch.jit.annotate` where required. - The condition in the `if` statement should be explicitly defined (e.g., mean value > 0.5). Performance Requirements - The implementation should be efficient and should make appropriate use of vectorized operations in PyTorch. Example ```python import torch import torch.nn as nn from typing import Tuple class MyCustomNet(nn.Module): def __init__(self, D_in: int, D_out: int): super(MyCustomNet, self).__init__() self.layer1 = nn.Linear(D_in, 128) self.layer2 = nn.Linear(128, D_out) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) if torch.mean(x) > 0.5: x = torch.relu(x) for i in range(5): x += i x = self.layer2(x) return x # Script the module scripted_model = torch.jit.script(MyCustomNet(64, 10)) # Test the scripted model input_tensor = torch.randn(32, 64) output_tensor = scripted_model(input_tensor) print(output_tensor) ``` Write and test your implementation based on the outlined requirements.","solution":"import torch import torch.nn as nn import torch.jit class MyCustomNet(nn.Module): def __init__(self, D_in: int, D_out: int): super(MyCustomNet, self).__init__() self.layer1 = nn.Linear(D_in, 128) self.layer2 = nn.Linear(128, D_out) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) if torch.mean(x) > 0.5: x = torch.relu(x) for i in range(5): x += i x = self.layer2(x) return x # Script the module scripted_model = torch.jit.script(MyCustomNet(64, 10)) # Test the scripted model input_tensor = torch.randn(32, 64) output_tensor = scripted_model(input_tensor) print(output_tensor)"},{"question":"**Python Coding Assessment Question** # Email Header Manager **Objective:** You are tasked with implementing a simplified custom email header manager that parses and generates email headers. The implementation must handle various header types following the `BaseHeader` and other related classes demonstrated in the `email.headerregistry` module description. # Requirements 1. **CustomBaseHeader Class:** - A base class for header objects, similar to `BaseHeader`. - **Attributes:** `name`, `value`, `defects` - **Methods:** - `fold(policy)` to fold the header according to a given policy (use line breaks for simplicity). 2. **CustomHeaderRegistry Class:** - This registry will dynamically produce header instances based on simplified rules. - **Attributes:** - Maintain a registry mapping header names to specialized classes. - **Methods:** - `map_to_type(name, cls)`: Map the name of the header to a specialized class. - `__call__(name, value)`: Create and return an instance of the appropriate header class based on the name. 3. **Specialized Header Classes:** - **UnstructuredHeader**: A subclass representing headers without predefined structure (like Subject). - **DateHeader**: A subclass representing date headers which should parse a valid date string. - **AddressHeader**: A subclass representing address headers which should parse a comma-separated list of email addresses. # Example: ```python # Define your base class class CustomBaseHeader: # Implementation here # Define your Custom Header Registry class class CustomHeaderRegistry: # Implementation here # Define specialized header classes class UnstructuredHeader(CustomBaseHeader): # Implementation here class DateHeader(CustomBaseHeader): # Implementation here class AddressHeader(CustomBaseHeader): # Implementation here # Example usage: registry = CustomHeaderRegistry() registry.map_to_type(\\"subject\\", UnstructuredHeader) registry.map_to_type(\\"date\\", DateHeader) registry.map_to_type(\\"to\\", AddressHeader) subject_header = registry(\\"subject\\", \\"Hello World\\") date_header = registry(\\"date\\", \\"Fri, 07 Oct 2023 09:26:00 -0300\\") to_header = registry(\\"to\\", \\"example@example.com, user@domain.com\\") print(subject_header) print(date_header) print(to_header) ``` # Task: 1. Implement the `CustomBaseHeader` class with necessary attributes and methods. 2. Implement the `CustomHeaderRegistry` class with required methods. 3. Implement the `UnstructuredHeader`, `DateHeader`, and `AddressHeader` classes inheriting from `CustomBaseHeader`. **Constraints:** - You cannot use the built-in `email.headerregistry` classes directly. - Assume reasonable input data types. - Focus on correct parsing and attribute management. # Input Format: - The input will be passed directly to the constructors of the header classes as shown in the example. # Output Format: - Your classes should follow the `BaseHeader` behavior, maintaining attributes `name`, `value`, and `defects`. - The objects should print their string representation correctly folded per the given policy (for simplicity, use a default line break policy with 78 characters line width). You will be evaluated based on correctness, adherence to the documented class structure, and the ability to correctly handle and parse header values.","solution":"import re from datetime import datetime class CustomBaseHeader: def __init__(self, name, value): self.name = name self.value = value self.defects = [] def __repr__(self): return f\\"{self.name}: {self.value}\\" def fold(self, policy=None): # Simple default policy to break lines at 78 characters lines = [] while len(self.value) > 78: cut = 78 if self.value[78] != \' \': cut = self.value.rfind(\' \', 0, 78) if cut == -1: cut = 78 lines.append(self.value[:cut].strip()) self.value = self.value[cut:].strip() lines.append(self.value) return \\"n\\".join(lines) class CustomHeaderRegistry: def __init__(self): self._registry = {} def map_to_type(self, name, cls): self._registry[name.lower()] = cls def __call__(self, name, value): header_cls = self._registry.get(name.lower(), UnstructuredHeader) return header_cls(name, value) class UnstructuredHeader(CustomBaseHeader): pass class DateHeader(CustomBaseHeader): def __init__(self, name, value): super().__init__(name, value) try: self.parsed_date = datetime.strptime(value, \\"%a, %d %b %Y %H:%M:%S %z\\") except ValueError as e: self.parsed_date = None self.defects.append(e) def __repr__(self): if self.parsed_date: return f\\"{self.name}: {self.parsed_date.strftime(\'%a, %d %b %Y %H:%M:%S %z\')}\\" return super().__repr__() class AddressHeader(CustomBaseHeader): def __init__(self, name, value): super().__init__(name, value) self.addresses = [addr.strip() for addr in value.split(\',\')] if any(not re.match(r\\"[^@]+@[^@]+.[^@]+\\", addr) for addr in self.addresses): self.defects.append(ValueError(\\"Invalid email address\\")) def __repr__(self): return f\\"{self.name}: {\', \'.join(self.addresses)}\\" # Example usage: registry = CustomHeaderRegistry() registry.map_to_type(\\"subject\\", UnstructuredHeader) registry.map_to_type(\\"date\\", DateHeader) registry.map_to_type(\\"to\\", AddressHeader) subject_header = registry(\\"subject\\", \\"Hello World\\") date_header = registry(\\"date\\", \\"Fri, 07 Oct 2023 09:26:00 -0300\\") to_header = registry(\\"to\\", \\"example@example.com, user@domain.com\\") print(subject_header) print(date_header) print(to_header)"},{"question":"**Question:** You are given an audio file in Sun AU format. Your task is to write a Python function that reads this audio file, doubles its sample rate, and saves the modified audio to a new Sun AU file. # Requirements: 1. **Function Signature:** ```python def double_sample_rate(input_filename: str, output_filename: str) -> None: ``` 2. **Input:** - `input_filename` (str): The path to the input Sun AU audio file. - `output_filename` (str): The path where the output Sun AU audio file with the doubled sample rate should be saved. 3. **Output:** - The function should save the modified audio file to `output_filename`. 4. **Constraints:** - Assume the input file is correctly formatted and uses a supported encoding type. - The function should handle reading from the file, manipulating the sample rate, and writing to a new file. - You must use the `sunau` module methods as described in the documentation for reading and writing the audio data. # Implementation Guidelines: 1. Use `sunau.open()` to read the original file and acquire its properties like the sample rate, number of channels, and sample width. 2. Modify the sample rate by doubling it. 3. Write the audio data to a new file with the updated sample rate. # Example: ```python def double_sample_rate(input_filename: str, output_filename: str) -> None: import sunau # Open the input audio file for reading with sunau.open(input_filename, \'r\') as input_file: # Get the parameters of the input file num_channels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() num_frames = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() # Read the audio frames frames = input_file.readframes(num_frames) # Modify the sample rate new_framerate = framerate * 2 # Open the output audio file for writing with sunau.open(output_filename, \'w\') as output_file: # Set the parameters for the output file output_file.setnchannels(num_channels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.setcomptype(comptype, compname) # Write the audio frames to the output file output_file.writeframes(frames) # Example usage: double_sample_rate(\'input.au\', \'output.au\') ``` # Notes: - You are encouraged to test your implementation with different Sun AU files. - Ensure your function handles file closing appropriately to avoid file corruption.","solution":"def double_sample_rate(input_filename: str, output_filename: str) -> None: import sunau # Open the input audio file for reading with sunau.open(input_filename, \'rb\') as input_file: # Get the parameters of the input file num_channels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() num_frames = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() # Read the audio frames frames = input_file.readframes(num_frames) # Modify the sample rate new_framerate = framerate * 2 # Open the output audio file for writing with sunau.open(output_filename, \'wb\') as output_file: # Set the parameters for the output file output_file.setnchannels(num_channels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.setcomptype(comptype, compname) # Write the audio frames to the output file output_file.writeframes(frames) # Example usage: # double_sample_rate(\'input.au\', \'output.au\')"},{"question":"**Question: Covariance Estimation and Comparison** You are provided with a dataset and your task is to estimate the covariance matrix using different methods available in the scikit-learn library. Finally, you need to compare the covariance estimators based on their Mean Squared Error (MSE). # Dataset Assume you have a dataset with `n_samples` and `n_features` which is generated using: ```python import numpy as np from sklearn.datasets import make_spd_matrix np.random.seed(42) n_samples = 100 n_features = 5 data = np.random.randn(n_samples, n_features) ``` # Task 1. **Estimate Covariance Matrix**: - Compute the empirical covariance matrix. - Compute the shrunk covariance matrix with a user-defined shrinkage (e.g., `alpha=0.1`). - Compute the Ledoit-Wolf shrinkage covariance matrix. - Compute the Oracle Approximating Shrinkage (OAS) covariance matrix. 2. **Performance Evaluation**: - Calculate the Mean Squared Error (MSE) between the estimated covariance matrices and the ground truth covariance matrix. Assume the ground truth covariance matrix can be obtained using the following: ```python true_covariance = np.cov(data, rowvar=False) ``` # Input The input to your function will be the dataset `data`. # Output Your output should include: - The estimated covariance matrices for each of the methods. - The MSE for each estimator compared to the ground truth covariance matrix. # Implementation Implement the following function: ```python def covariance_estimators_and_mse(data): # Computes the different covariance estimators and their MSE # Your implementation here return { \'empirical_covariance\': ..., \'shrunk_covariance\': ..., \'ledoit_wolf_covariance\': ..., \'oas_covariance\': ..., \'mse_empirical\': ..., \'mse_shrunk\': ..., \'mse_ledoit_wolf\': ..., \'mse_oas\': ... } ``` # Constraints - The dataset `data` is a 2D NumPy array of shape `(n_samples, n_features)`. - Use `alpha=0.1` for the shrunk covariance estimator. - Use the `mean_squared_error` function from `sklearn.metrics` to calculate MSE. # Example ```python data = np.random.randn(100, 5) result = covariance_estimators_and_mse(data) print(result) > { \'empirical_covariance\': ..., \'shrunk_covariance\': ..., \'ledoit_wolf_covariance\': ..., \'oas_covariance\': ..., \'mse_empirical\': ..., \'mse_shrunk\': ..., \'mse_ledoit_wolf\': ..., \'mse_oas\': ... } ``` Your solution should demonstrate an understanding of the covariance estimation methods and their comparative performance in terms of MSE.","solution":"import numpy as np from sklearn.covariance import ShrunkCovariance, LedoitWolf, OAS from sklearn.metrics import mean_squared_error def covariance_estimators_and_mse(data): # Compute the empirical covariance matrix empirical_covariance = np.cov(data, rowvar=False) # Compute the shrunk covariance matrix with alpha=0.1 shrunk_cov = ShrunkCovariance(shrinkage=0.1) shrunk_covariance = shrunk_cov.fit(data).covariance_ # Compute the Ledoit-Wolf shrinkage covariance matrix ledoit_wolf_cov = LedoitWolf() ledoit_wolf_covariance = ledoit_wolf_cov.fit(data).covariance_ # Compute the Oracle Approximating Shrinkage (OAS) covariance matrix oas_cov = OAS() oas_covariance = oas_cov.fit(data).covariance_ # Compute the true covariance matrix true_covariance = np.cov(data, rowvar=False) # Compute the MSE for each covariance estimator mse_empirical = mean_squared_error(true_covariance, empirical_covariance) mse_shrunk = mean_squared_error(true_covariance, shrunk_covariance) mse_ledoit_wolf = mean_squared_error(true_covariance, ledoit_wolf_covariance) mse_oas = mean_squared_error(true_covariance, oas_covariance) return { \'empirical_covariance\': empirical_covariance, \'shrunk_covariance\': shrunk_covariance, \'ledoit_wolf_covariance\': ledoit_wolf_covariance, \'oas_covariance\': oas_covariance, \'mse_empirical\': mse_empirical, \'mse_shrunk\': mse_shrunk, \'mse_ledoit_wolf\': mse_ledoit_wolf, \'mse_oas\': mse_oas }"},{"question":"# PyTorch Advanced Challenge: Optimized Model Execution To assess your understanding of PyTorch along with your ability to optimize performance for specific hardware configurations, we provide the following problem. Problem Statement You are required to implement a function called `optimize_model_execution`, which takes a PyTorch model, input data, and a device identifier (indicating whether to use GPU or CPU). Your function should ensure the model and data are properly configured to potentially utilize PyTorch\'s persistent algorithm on a V100 GPU when the specified conditions are met. The function should satisfy the following requirements: 1. If the device is `cuda` and the GPU is a V100: - Move both the model and the input data to the GPU. - Convert the input data to `torch.float16` dtype. - Ensure the input data is not in `PackedSequence` format. 2. If the device is `cpu`, no changes should be made to the model or data. 3. Return the processed input data ready for model inference. Function Signature ```python def optimize_model_execution(model, input_data, device: str): Optimize model and input_data for execution given the device. :param model: A PyTorch model to be optimized. :param input_data: Tensor input to the model. :param device: A string indicating whether to use \'cuda\' or \'cpu\'. :return: Processed input data ready for inference. pass ``` Constraints - The device can be either \'cpu\' or \'cuda\'. - You can assume that we are using a V100 GPU if the device is \'cuda\'. - Only the `torch.float16` dtype is to be used for optimization. - Input data is initially a PyTorch Tensor and not in `PackedSequence` format. - Ensure the function handles exceptions and edge cases as appropriate. Example Usage ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) model = SimpleModel() input_data = torch.randn(5, 10) device = \'cuda\' # Use \'cpu\' to observe the alternative behavior optimized_data = optimize_model_execution(model, input_data, device) print(optimized_data.dtype) # Expected: torch.float16 if device is \'cuda\' and a V100 GPU is used ``` Evaluation Criteria - **Correctness**: Ensuring the function meets specified behavior. - **Performance**: Efficient handling of data and device transfers. - **Robustness**: Handling exceptions and constraints effectively.","solution":"import torch import torch.nn as nn def optimize_model_execution(model, input_data, device: str): Optimize model and input_data for execution given the device. :param model: A PyTorch model to be optimized. :param input_data: Tensor input to the model. :param device: A string indicating whether to use \'cuda\' or \'cpu\'. :return: Processed input data ready for inference. # Ensure the input data is a tensor and not in PackedSequence format if not isinstance(input_data, torch.Tensor): raise ValueError(\\"Input data should be a PyTorch Tensor.\\") if device == \'cuda\': # Check if GPU is available and it is a V100 (simulated here as a simple check) if torch.cuda.is_available(): device_index = torch.cuda.current_device() device_name = torch.cuda.get_device_name(device_index) if \'V100\' in device_name: model.to(device) input_data = input_data.to(device).half() elif device != \'cpu\': raise ValueError(\\"Device should be either \'cpu\' or \'cuda\'.\\") return input_data"},{"question":"**Coding Assessment Question: System State Inspector** **Objective:** In this exercise, you will create a Python function that inspects and reports various system states by utilizing multiple attributes and functions from the `sys` module. This task will test your ability to interact with system-specific parameters and functions, which is essential for understanding how Python operates within the broader system environment. **Task:** Write a function `inspect_system_state(report_path: str) -> None` that gathers information about the current Python runtime and system environment. The function should collect the following details and save them into a text file specified by the `report_path`: 1. The list of command-line arguments (`sys.argv`). 2. The Python version and build information (`sys.version` and `sys.version_info`). 3. The platform identifier (`sys.platform`). 4. The absolute path of the Python executable (`sys.executable`). 5. The maximum recursion limit (`sys.getrecursionlimit()`). 6. The size of an integer in bytes (`sys.getsizeof(42)`). 7. The list of loaded modules (`sys.modules.keys()`). 8. The byte order of the host system (`sys.byteorder`). The output text file should have a clear and formatted representation of this information. Here\'s a sample format for the output: ``` System State Report =================== 1. Command-line arguments: argv[0]: <arg0> argv[1]: <arg1> ... 2. Python version and build information: Version: <version> Version Info: <version_info> 3. Platform identifier: <platform> 4. Python executable path: <executable> 5. Maximum recursion limit: <recursion_limit> 6. Size of an integer (in bytes): <int_size> 7. List of loaded modules: - <module1> - <module2> ... 8. System byte order: <byteorder> ``` **Function Signature:** ```python import sys def inspect_system_state(report_path: str) -> None: pass ``` **Constraints:** 1. Make sure all required information is correctly formatted and included in the report. 2. Handle potential errors when accessing system attributes and functions gracefully. 3. Ensure the output file is created and written to successfully with the provided file path. **Example:** Assume the Python script is executed with the command: ```bash python inspector.py arg1 arg2 ``` If the `report_path` is \\"system_report.txt\\", the content of the file after running your function might look like this: ``` System State Report =================== 1. Command-line arguments: argv[0]: inspector.py argv[1]: arg1 argv[2]: arg2 2. Python version and build information: Version: 3.10.0 (default, Oct 16 2021, 12:00:00) [GCC 9.3.0] Version Info: sys.version_info(major=3, minor=10, micro=0, releaselevel=\'final\', serial=0) 3. Platform identifier: linux 4. Python executable path: /usr/bin/python3 5. Maximum recursion limit: 3000 6. Size of an integer (in bytes): 24 7. List of loaded modules: - sys - builtins - _frozen_importlib ... 8. System byte order: little ``` Make sure your solution adheres to this format for maximum readability and clarity.","solution":"import sys def inspect_system_state(report_path: str) -> None: try: with open(report_path, \'w\') as report_file: # Collecting system state information argv = sys.argv version = sys.version version_info = sys.version_info platform = sys.platform executable = sys.executable recursion_limit = sys.getrecursionlimit() int_size = sys.getsizeof(42) modules_keys = list(sys.modules.keys()) byteorder = sys.byteorder # Writing information to the file report_file.write(\\"System State Reportn\\") report_file.write(\\"===================nn\\") # 1. Command-line arguments report_file.write(\\"1. Command-line arguments:n\\") for index, arg in enumerate(argv): report_file.write(f\\" argv[{index}]: {arg}n\\") report_file.write(\\"n\\") # 2. Python version and build information report_file.write(\\"2. Python version and build information:n\\") report_file.write(f\\" Version: {version}n\\") report_file.write(f\\" Version Info: {version_info}n\\") report_file.write(\\"n\\") # 3. Platform identifier report_file.write(f\\"3. Platform identifier: {platform}nn\\") # 4. Python executable path report_file.write(f\\"4. Python executable path: {executable}nn\\") # 5. Maximum recursion limit report_file.write(f\\"5. Maximum recursion limit: {recursion_limit}nn\\") # 6. Size of an integer (in bytes) report_file.write(f\\"6. Size of an integer (in bytes): {int_size}nn\\") # 7. List of loaded modules report_file.write(\\"7. List of loaded modules:n\\") for module in modules_keys: report_file.write(f\\" - {module}n\\") report_file.write(\\"n\\") # 8. System byte order report_file.write(f\\"8. System byte order: {byteorder}n\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective To assess your understanding of the `mailbox` module in Python, specifically working with different mailbox formats and their unique methods. Task You are required to implement a function `transfer_emails(src_path, dst_path, src_format, dst_format)` that transfers emails from one mailbox format to another while handling any format-specific behaviors and constraints. # Requirements: 1. **Function Signature**: ```python import mailbox def transfer_emails(src_path: str, dst_path: str, src_format: str, dst_format: str) -> None: pass ``` 2. **Parameters**: - `src_path` (str): Path to the source mailbox. - `dst_path` (str): Path to the destination mailbox. - `src_format` (str): Format of the source mailbox. Options: \'maildir\', \'mbox\', \'mh\', \'babyl\', \'mmdf\'. - `dst_format` (str): Format of the destination mailbox. Options: \'maildir\', \'mbox\', \'mh\', \'babyl\', \'mmdf\'. 3. **Behavior**: - The function should read all messages from the source mailbox and add them to the destination mailbox. - Handle any necessary mailbox locking, unlocking, and flushing to ensure data integrity. - Ensure that format-specific behaviors are correctly managed: - For `Maildir`, manage subdirectories and flags. - For `mbox` and `MMDF`, handle file-based storage and state flags. - For `MH`, manage sequences and folders. - For `Babyl`, handle labels and visible headers. 4. **Constraints**: - The function should avoid data corruption, especially when dealing with file-based formats like `mbox` or `MMDF`. - Handle potential errors gracefully, such as missing mailboxes or format conflicts, by raising appropriate exceptions. # Example Usage: ```python # Transfer emails from an mbox mailbox to a Maildir mailbox. transfer_emails(\'/path/to/src/mbox\', \'/path/to/dst/maildir\', \'mbox\', \'maildir\') ``` # Hints: - Utilize the `mailbox` module and its subclasses (e.g., `mailbox.Maildir`, `mailbox.mbox`). - Pay attention to the format-specific methods and constraints as described in the documentation. - Make sure to lock the mailboxes during operations to prevent data corruption. # Testing: - Create test cases to validate the function: - Transferring between different formats (e.g., mbox to Maildir, Maildir to mbox). - Handling large mailboxes and ensuring data integrity. - Verifying that special behaviors (like flags and labels) are preserved correctly. # Additional Information: - Refer to the provided documentation of the `mailbox` module for detailed explanations of each class and its methods. - Implement proper error handling to make the function robust and reliable.","solution":"import mailbox def transfer_emails(src_path: str, dst_path: str, src_format: str, dst_format: str) -> None: Transfer emails from one mailbox format to another. Parameters: - src_path (str): Path to the source mailbox. - dst_path (str): Path to the destination mailbox. - src_format (str): Format of the source mailbox. - dst_format (str): Format of the destination mailbox. # Create mailbox instances based on the provided formats src_mailbox = getattr(mailbox, src_format)(src_path) dst_mailbox = getattr(mailbox, dst_format)(dst_path) try: # Lock the source mailbox if possible if hasattr(src_mailbox, \'lock\'): src_mailbox.lock() # Loop through each message in the source mailbox for message in src_mailbox: # Add the message to the destination mailbox dst_mailbox.add(message) finally: # Ensure that the source mailbox is unlocked if possible if hasattr(src_mailbox, \'unlock\'): src_mailbox.unlock() # Flush the destination mailbox to ensure all changes are saved if hasattr(dst_mailbox, \'flush\'): dst_mailbox.flush() # Close the source mailbox to release any resources src_mailbox.close() dst_mailbox.close()"},{"question":"**Coding Assessment Question:** # Context: You are working on a project where you need to handle temporary files and directories effectively to ensure secure and optimal temporary storage. You are required to use the `tempfile` module in Python to achieve this. # Task: Write a Python function `process_temp_files_and_dirs(data: str) -> str` that: 1. Creates a temporary directory. 2. Within this temporary directory: - Creates a temporary file and writes the provided `data` into it. - Creates a named temporary file and writes the reversed string of `data` into it. 3. Reads the contents of both files. 4. Returns the concatenated results of reading both files as a single string. # Specifications: - You must use `TemporaryDirectory` for creating the temporary directory. - You must use both `TemporaryFile` and `NamedTemporaryFile` for creating temporary files. - Ensure the use of context managers to handle file and directory cleanup automatically. # Input: - `data`: A string containing the data to be written to the temporary files. # Output: - A string that is the concatenated content read from the temporary file and the named temporary file. # Example: ```python result = process_temp_files_and_dirs(\\"Hello World\\") print(result) # Should print the string \\"Hello WorlddlroW olleH\\" ``` Constraints: - The length of `data` will not exceed 1,000 characters. - Assume always valid UTF-8 encoded string input. # Note: Ensure that the temporary files and directory are cleaned up automatically after their usage.","solution":"import tempfile def process_temp_files_and_dirs(data: str) -> str: with tempfile.TemporaryDirectory() as tmpdir: # Create a temporary file and write the provided `data` into it. with tempfile.TemporaryFile(mode=\'w+\', dir=tmpdir) as tmpfile: tmpfile.write(data) tmpfile.seek(0) tmpfile_content = tmpfile.read() # Create a named temporary file and write the reversed string of `data` into it. with tempfile.NamedTemporaryFile(delete=False, mode=\'w+\', dir=tmpdir) as named_tmpfile: named_tmpfile.write(data[::-1]) named_tmpfile.seek(0) named_tmpfile_content = named_tmpfile.read() # Return the concatenated results of reading both files as a single string. result = tmpfile_content + named_tmpfile_content return result"},{"question":"**Objective**: Demonstrate your understanding of Python descriptors and how they relate to attributing and customizing object behavior. **Question**: Create a custom Python class that uses descriptors to manage the attributes. The class should have two attributes: `temperature` and `pressure`. 1. **The expected behavior and requirements**: - The `temperature` attribute should ensure the value is a float within the range of -273.15 to 1000.0. - The `pressure` attribute should ensure the value is an integer and should automatically convert float input to the nearest integer. - If invalid values are set for these attributes, raise appropriate errors (`ValueError` for out-of-range or type issues). 2. **Class design**: - You must define your descriptor classes for managing the getter, setter behavior (`Temperature` and `Pressure`). - The main class `Environment` should utilize these descriptor classes for `temperature` and `pressure`. 3. **Code Implementation**: - Define the descriptor classes to handle validation and type conversion. - Integrate these descriptors into the main class `Environment`. 4. **Example Usage**: ```python env = Environment() env.temperature = 25.5 # Valid env.pressure = 101.3 # Automatically converts to integer 101 print(env.temperature) # 25.5 print(env.pressure) # 101 env.temperature = -300 # Raises ValueError: Temperature must be between -273.15 and 1000.0 env.pressure = \\"abc\\" # Raises ValueError: Pressure must be an integer ``` **Constraints**: - Do not use any external libraries. - Ensure your solution is efficient. **Submission**: Submit a single `.py` file containing your solution.","solution":"class Temperature: def __get__(self, instance, owner): return instance._temperature def __set__(self, instance, value): if not isinstance(value, (float, int)): raise ValueError(\\"Temperature must be a float.\\") if not -273.15 <= float(value) <= 1000.0: raise ValueError(\\"Temperature must be between -273.15 and 1000.0.\\") instance._temperature = float(value) class Pressure: def __get__(self, instance, owner): return instance._pressure def __set__(self, instance, value): if not isinstance(value, (float, int)): raise ValueError(\\"Pressure must be an integer.\\") instance._pressure = int(value) class Environment: temperature = Temperature() pressure = Pressure() def __init__(self, temperature=0.0, pressure=101): self.temperature = temperature self.pressure = pressure"},{"question":"**Title:** Implementing and Using Descriptors for Managed Attributes and Validation **Objective:** Demonstrate your understanding of creating and using custom descriptors to manage and validate class attributes in Python. **Problem Statement:** You are required to implement a custom descriptor class, `ValidatedAttribute`, that manages access to instance attributes and includes validation logic to enforce the following constraints: 1. **String Length**: The attribute must be a string with a length within a specified range. 2. **Allowed Values**: The attribute value must be one of a specified set of allowed values. 3. **Number Range**: The attribute must be a number (int or float) within a specified range. You will need to implement this descriptor and then use it in a `Product` class to enforce the following constraints on its attributes: - `name`: A string with a length between 3 and 50 characters. - `category`: A string that must be one of the following: `[\'electronics\', \'furniture\', \'clothing\']`. - `price`: A number that must be between 0 and 10000. **Implementation Details:** 1. **Descriptor Class (`ValidatedAttribute`)** - **Constructor Parameters**: - `field_type`: The type of the field (`str` or `number`). - `min_value` (optional): The minimum length (for strings) or minimum value (for numbers). - `max_value` (optional): The maximum length (for strings) or maximum value (for numbers). - `allowed_values` (optional): A list of allowed values for the attribute. - **Methods**: - `__get__`: Retrieve the value from the instance. - `__set__`: Set the value with validation checks. 2. **Class (`Product`)** - Attributes using `ValidatedAttribute`: - `name` - `category` - `price` - Ensure that invalid assignments raise appropriate exceptions. **Input and Output Formats:** - **Input**: The class definitions and their use cases. - **Output**: Implementation of the descriptors and their functionality. **Constraints**: - If constraints are violated, relevant exceptions (`ValueError`, `TypeError`) should be raised with meaningful error messages. **Performance Requirements**: - The solution should efficiently handle the validation checks. # Example Usage: ```python class ValidatedAttribute: def __init__(self, field_type, min_value=None, max_value=None, allowed_values=None): self.field_type = field_type self.min_value = min_value self.max_value = max_value self.allowed_values = allowed_values self.private_name = None def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) def validate(self, value): if self.field_type == \'str\': if not isinstance(value, str): raise TypeError(f\\"Expected {value!r} to be a str\\") if self.min_value is not None and len(value) < self.min_value: raise ValueError(f\\"Expected {value!r} to have at least {self.min_value} characters\\") if self.max_value is not None and len(value) > self.max_value: raise ValueError(f\\"Expected {value!r} to have at most {self.max_value} characters\\") elif self.field_type == \'number\': if not isinstance(value, (int, float)): raise TypeError(f\\"Expected {value!r} to be an int or float\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {value!r} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {value!r} to be at most {self.max_value}\\") if self.allowed_values is not None and value not in self.allowed_values: raise ValueError(f\\"Expected {value!r} to be one of {self.allowed_values!r}\\") class Product: name = ValidatedAttribute(field_type=\'str\', min_value=3, max_value=50) category = ValidatedAttribute(field_type=\'str\', allowed_values=[\'electronics\', \'furniture\', \'clothing\']) price = ValidatedAttribute(field_type=\'number\', min_value=0, max_value=10000) def __init__(self, name, category, price): self.name = name self.category = category self.price = price # Example Usage try: p = Product(name=\\"Phone\\", category=\\"electronics\\", price=999) print(p.name, p.category, p.price) p.price = 10500 # Should raise ValueError except Exception as e: print(e) ``` This question will assess your understanding of descriptors, attribute management, and validation in Python.","solution":"class ValidatedAttribute: def __init__(self, field_type, min_value=None, max_value=None, allowed_values=None): self.field_type = field_type self.min_value = min_value self.max_value = max_value self.allowed_values = allowed_values self.private_name = None def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) def validate(self, value): if self.field_type == \'str\': if not isinstance(value, str): raise TypeError(f\\"Expected {value!r} to be a str\\") if self.min_value is not None and len(value) < self.min_value: raise ValueError(f\\"Expected {value!r} to have at least {self.min_value} characters\\") if self.max_value is not None and len(value) > self.max_value: raise ValueError(f\\"Expected {value!r} to have at most {self.max_value} characters\\") elif self.field_type == \'number\': if not isinstance(value, (int, float)): raise TypeError(f\\"Expected {value!r} to be an int or float\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {value!r} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {value!r} to be at most {self.max_value}\\") if self.allowed_values is not None and value not in self.allowed_values: raise ValueError(f\\"Expected {value!r} to be one of {self.allowed_values!r}\\") class Product: name = ValidatedAttribute(field_type=\'str\', min_value=3, max_value=50) category = ValidatedAttribute(field_type=\'str\', allowed_values=[\'electronics\', \'furniture\', \'clothing\']) price = ValidatedAttribute(field_type=\'number\', min_value=0, max_value=10000) def __init__(self, name, category, price): self.name = name self.category = category self.price = price # Example Usage try: p = Product(name=\\"Phone\\", category=\\"electronics\\", price=999) print(p.name, p.category, p.price) p.price = 10500 # Should raise ValueError except Exception as e: print(e)"},{"question":"You are provided with a dataset containing transactions from a retail store. Each transaction records the `day`, `total_bill`, `tip`, `time` (Lunch/Dinner), and whether the customer is a `smoker`. Using the seaborn library, visualize the data to extract meaningful patterns related to tips given by customers. # Problem Statement Given the dataset as a pandas DataFrame: ```python data = { \'day\': [\'Thur\', \'Fri\', \'Sat\', \'Sun\']*10, \'total_bill\': np.random.uniform(10, 50, 40), \'tip\': np.random.uniform(1, 10, 40), \'time\': [\'Lunch\', \'Dinner\']*20, \'smoker\': [\'Yes\', \'No\']*20 } df = pd.DataFrame(data) ``` # Task 1. Create a box plot to compare the `tip` values between smokers and non-smokers for different `days`. 2. Generate a violin plot to examine the distribution of `total_bill` based on `time` and `day`. 3. Build a point plot to represent the average `tip` for different `day` categories, with distinct colors for `time` (Lunch/Dinner). # Function Signature ```python import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(df: pd.DataFrame) -> None: Function to visualize tips data using seaborn. Parameters: df (pd.DataFrame): DataFrame containing transaction data with columns \'day\', \'total_bill\', \'tip\', \'time\', and \'smoker\'. Returns: None pass ``` # Constraints - Use default seaborn settings for aesthetic themes. - Ensure that your plots are clearly labeled and distinguishable. - Make sure the visualization handles overlapping points and categorical ordering effectively. # Example Output Your function should produce the following plots: 1. A box plot showing `tip` values with categories divided by `smoker` status for each `day`. 2. A violin plot visualizing the distribution of `total_bill` by `time` and `day`. 3. A point plot showing the average `tip` for each `day`, differentiated by `time`. # Evaluation Criteria 1. **Correctness**: Generate the plots accurately as per the specifications. 2. **Clarity**: Plots should be well-labeled and easy to understand. 3. **Seaborn Proficiency**: Demonstrate the effective use of seaborn functions and parameters. **Dataset:** Here is an example of how the dataset might look: ``` day total_bill tip time smoker 0 Thur 25.607540 1.095970 Lunch Yes 1 Fri 46.276683 3.622573 Dinner No 2 Sat 27.922059 2.980745 Lunch Yes ... ```","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(df: pd.DataFrame) -> None: Function to visualize tips data using seaborn. Parameters: df (pd.DataFrame): DataFrame containing transaction data with columns \'day\', \'total_bill\', \'tip\', \'time\', and \'smoker\'. Returns: None # Set the aesthetic style of the plots sns.set_style(\\"whitegrid\\") # 1. Box plot to compare the `tip` values between smokers and non-smokers for different `days` plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"day\\", y=\\"tip\\", hue=\\"smoker\\", data=df) plt.title(\'Box Plot of Tips by Day and Smoker Status\') plt.ylabel(\'Tip Amount\') plt.xlabel(\'Day of the Week\') plt.legend(title=\'Smoker\') plt.show() # 2. Violin plot to examine the distribution of `total_bill` based on `time` and `day` plt.figure(figsize=(12, 6)) sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", data=df, split=True) plt.title(\'Violin Plot of Total Bill by Day and Time\') plt.ylabel(\'Total Bill Amount\') plt.xlabel(\'Day of the Week\') plt.legend(title=\'Time\') plt.show() # 3. Point plot to represent the average `tip` for different `day` categories, with distinct colors for `time` (Lunch/Dinner) plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"day\\", y=\\"tip\\", hue=\\"time\\", data=df, dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Point Plot of Average Tips by Day and Time\') plt.ylabel(\'Average Tip Amount\') plt.xlabel(\'Day of the Week\') plt.legend(title=\'Time\') plt.show()"},{"question":"Title: Custom Set Operations and Conversions Objective: Implement functions that demonstrate your ability to work with Python `set` and `frozenset` objects using the provided API methods. Problem Statement: You need to implement two functions: `create_set_from_iterable` and `custom_set_operations`. 1. `create_set_from_iterable(iterable)`: This function should take an iterable and return a new set containing objects from the iterable. If the iterable is `None`, return an empty set. Raise a `TypeError` if the input is not an iterable. 2. `custom_set_operations(iterable, operations)`: This function should take an iterable to create a set and a list of operations to perform on this set. The operations list contains tuples where each tuple represents an operation in the form `(operation_name, argument)`. The function should apply the operations sequentially and return the final set. The operations can be: - `add`: Add the element `argument` to the set. - `discard`: Remove the element `argument` from the set. - `contains`: Check if the element `argument` is in the set (returns `True` or `False`). - `len`: Get the size of the set (returns the number of elements). Input Format: - `create_set_from_iterable(iterable)`: - `iterable`: An iterable object (e.g., list, tuple) or `None`. - `custom_set_operations(iterable, operations)`: - `iterable`: An iterable object to initialize the set. - `operations`: A list of tuples where each tuple is in the form `(operation_name, argument)`. - `operation_name` is a string, one of (`add`, `discard`, `contains`, `len`). - `argument` is the element to be added, discarded, or checked, or `None` for `len`. Output Format: - `create_set_from_iterable(iterable)`: A new set containing elements from the iterable. - `custom_set_operations(iterable, operations)`: The final set after all operations are applied, and intermediate results where relevant. Constraints: - Each operation in `operations` is guaranteed to have a valid `operation_name`. - `create_set_from_iterable(iterable)` should raise a `TypeError` if the input is not an iterable. Example Usage: ```python # Example of create_set_from_iterable s = create_set_from_iterable([1, 2, 3]) print(s) # Output: {1, 2, 3} # Example of custom_set_operations initial_set = [1, 2, 3] operations = [(\'add\', 4), (\'discard\', 2), (\'contains\', 1), (\'len\', None)] result = custom_set_operations(initial_set, operations) # expected_output: ({1, 3, 4}, True, 3) print(result) ``` Implement the functions below: ```python def create_set_from_iterable(iterable): # Implement this function pass def custom_set_operations(iterable, operations): # Implement this function pass ```","solution":"def create_set_from_iterable(iterable): Create a set from the given iterable. If the iterable is None, return an empty set. Raise TypeError if the input is not iterable. if iterable is None: return set() try: result_set = set(iterable) except TypeError: raise TypeError(\\"Input is not iterable\\") return result_set def custom_set_operations(iterable, operations): Perform a series of operations on a set created from the iterable. result_set = create_set_from_iterable(iterable) intermediate_results = [] for operation_name, argument in operations: if operation_name == \'add\': result_set.add(argument) elif operation_name == \'discard\': result_set.discard(argument) elif operation_name == \'contains\': intermediate_results.append(argument in result_set) elif operation_name == \'len\': intermediate_results.append(len(result_set)) return (result_set, *intermediate_results)"},{"question":"# Programming Question: Problem Statement: You are required to write a Python function `process_data(data: List[Union[int, str]]) -> Dict[str, Any]` that processes a list of integers and strings. The list consists of a mix of integers and strings. Your function should categorize the input data into three categories: \\"numbers\\", \\"words\\", and \\"others\\". The function should return a dictionary with three keys: \\"numbers\\", \\"words\\", and \\"others\\" where: - \\"numbers\\" has a list of integers from the input data. - \\"words\\" has a list of strings from the input data that consist only of alphabetic characters. - \\"others\\" has a list of all other types that do not fit in the above two categories. Additionally, your function should print the following: 1. The sum of all the integers in the \\"numbers\\" list using the `sum()` function. 2. The total count of alphabetic strings in the \\"words\\" list. 3. The total count of items in the \\"others\\" list. Input: - A list `data` of length `n` (1 <= n <= 1000) consisting of integers and strings. Output: - A dictionary with the keys: \\"numbers\\", \\"words\\", and \\"others\\". Example: ```python data = [1, \'apple\', 2, \'banana\', \'123\', \'cherry\', 4.5, \'dog\', 9] output = process_data(data) print(output) ``` # Expected Output: ``` { \'numbers\': [1, 2, 9], \'words\': [\'apple\', \'banana\', \'cherry\', \'dog\'], \'others\': [\'123\', 4.5] } Sum of numbers: 12 Count of words: 4 Count of others: 2 ``` Constraints: - The input list `data` contains a mix of integers and strings. - The function should be efficient and handle up to 1000 elements in the input list. Notes: - The categorization of strings into \\"words\\" should be done based on whether they consist only of alphabetic characters. - Include appropriate error handling for unexpected input types. Good luck!","solution":"from typing import List, Union, Dict, Any def process_data(data: List[Union[int, str]]) -> Dict[str, Any]: Processes the input data and categorizes it into numbers, words, and others. Parameters: data (List[Union[int, str]]): The input list containing integers and strings. Returns: Dict[str, Any]: A dictionary with keys \'numbers\', \'words\', and \'others\' with their respective categorized lists. result = { \'numbers\': [], \'words\': [], \'others\': [] } for item in data: if isinstance(item, int): result[\'numbers\'].append(item) elif isinstance(item, str): if item.isalpha(): result[\'words\'].append(item) else: result[\'others\'].append(item) else: result[\'others\'].append(item) print(f\\"Sum of numbers: {sum(result[\'numbers\'])}\\") print(f\\"Count of words: {len(result[\'words\'])}\\") print(f\\"Count of others: {len(result[\'others\'])}\\") return result"},{"question":"**Objective**: Implement a function to analyze and summarize the metadata of a specified package using the `importlib.metadata` module. **Problem Statement**: Write a function `get_package_summary` that accepts the name of an installed package and returns a dictionary with the following information: 1. The version of the package. 2. The top-level dependencies (requirements) of the package. 3. The names and hashes of all Python files within the package. **Function Signature**: ```python def get_package_summary(package_name: str) -> dict: pass ``` **Input**: - `package_name` (str): The name of the installed package. **Output**: - A dictionary with the following structure: ```python { \\"version\\": str, \\"requirements\\": List[str], \\"files\\": Dict[str, str] } ``` - `version`: The version of the package as a string. - `requirements`: A list of top-level dependencies as strings. - `files`: A dictionary where the keys are the file names and the values are their corresponding hashes. **Constraints**: - The function should handle cases where the package does not exist by raising an appropriate exception. - Assume the package is installed in the Python environment. **Example**: For an installed package `example-package` with version `1.2.3`, dependencies `[\'dependency1 >=1.0.0\', \'dependency2\']`, and Python files `[\'file1.py\', \'file2.py\']` with hashes `[\'hash1\', \'hash2\']` respectively: ```python get_package_summary(\'example-package\') ``` The function should return: ```python { \\"version\\": \\"1.2.3\\", \\"requirements\\": [\\"dependency1 >=1.0.0\\", \\"dependency2\\"], \\"files\\": { \\"file1.py\\": \\"hash1\\", \\"file2.py\\": \\"hash2\\" } } ``` **Guidelines**: 1. Use `importlib.metadata` to fetch the required metadata efficiently. 2. Handle scenarios where certain metadata might not be available gracefully. 3. Ensure the function raises an appropriate exception if the package is not found. **Performance Notes**: - The function should be efficient in handling reasonably large packages with multiple metadata entries and files. **Hints**: - Use `importlib.metadata.version` to get the package version. - Use `importlib.metadata.requires` to get the package\'s top-level requirements. - Use `importlib.metadata.files` to list the files in the package and their attributes. # Testing To test your function, you can install different packages and call your function with their names, checking the output against expected values.","solution":"import importlib.metadata import hashlib def get_package_summary(package_name: str) -> dict: try: # Get the version of the package version = importlib.metadata.version(package_name) # Get the requirements of the package requirements = importlib.metadata.requires(package_name) or [] # Get the files in the package and their hashes files = {} package_files = importlib.metadata.files(package_name) for file in package_files: if file.name.endswith(\'.py\'): file_path = str(file.locate()) with open(file_path, \'rb\') as f: file_content = f.read() file_hash = hashlib.sha256(file_content).hexdigest() files[file.name] = file_hash return { \\"version\\": version, \\"requirements\\": requirements, \\"files\\": files } except importlib.metadata.PackageNotFoundError: raise ValueError(f\\"Package \'{package_name}\' not found\\")"},{"question":"# Objective You are required to create a new Python C extension type that represents a custom data structure named `DoubleList`, which behaves similarly to a doubly linked list but with additional functionality for getting the sum of its integer elements. # Task 1. Define a new custom type `DoubleList` as a C extension for Python. 2. Implement methods to add integers to the list and to get the sum of all integers in the list. # Guidelines 1. **Define the structure of `DoubleList`**: - The structure should include: - Pointer to the first node. - Pointer to the last node. - Count of nodes. 2. **Define the structure of list nodes**: - Each node should contain: - Integer value. - Pointer to the next node. - Pointer to the previous node. 3. **Implement the following methods**: - `add(value)`: Adds a new integer value to the end of the list. - `get_sum()`: Returns the sum of all integer values in the list. # Input and Output Formats - `add(value)`: - Input: A single integer value. - Output: None. - `get_sum()`: - Input: None. - Output: Integer, representing the sum of all integer values in the list. # Constraints 1. Ensure proper memory allocation and deallocation to avoid memory leaks. 2. Handle reference counting appropriately to prevent dangling pointers. 3. Follow best practices for initializing and deallocating custom types. # Performance Requirements - The `add` method should run in O(1) time. - The `get_sum` method should run in O(n) time, where n is the number of nodes in the list. # Sample Usage ```python import custom_dlist # Create a new DoubleList instance dlist = custom_dlist.DoubleList() # Add integers to the list dlist.add(10) dlist.add(20) dlist.add(30) # Get the sum of integers in the list print(dlist.get_sum()) # Output: 60 ``` # Submission Requirements - Submit the C code defining the `DoubleList` type, including the struct definitions, method implementations, and module initialization. - Include a `setup.py` script for building the C extension module. - Provide a Python script demonstrating the usage of the `DoubleList` type, including adding integers and getting the sum.","solution":"class DoubleListNode: def __init__(self, value: int): self.value = value self.next = None self.prev = None class DoubleList: def __init__(self): self.head = None self.tail = None self.count = 0 def add(self, value: int): new_node = DoubleListNode(value) if self.tail is None: # List is empty self.head = new_node self.tail = new_node else: new_node.prev = self.tail self.tail.next = new_node self.tail = new_node self.count += 1 def get_sum(self) -> int: total = 0 current = self.head while current: total += current.value current = current.next return total"},{"question":"# PyTorch Precision and Stability Assessment Objective The objective of this question is to assess your understanding of PyTorch\'s precision and stability aspects, particularly when dealing with floating point operations and batched computations. Problem Statement You are required to implement a function `stable_batch_matrix_multiply` that takes two 3D tensors `A` and `B`, performs a batched matrix multiplication, and returns the result. Additionally, you must compare the first slice of the batched result with the matrix multiplication of the first slices of the original tensors and report the precision discrepancy. Function Signature ```python def stable_batch_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication on 3D tensors A and B and compare precision discrepancy with individual matrix multiplication. Parameters: A (torch.Tensor): A 3D tensor of shape (batch, m, n) B (torch.Tensor): A 3D tensor of shape (batch, n, p) Returns: torch.Tensor: A tensor containing the discrepancies in precision. ``` Input - `A`: A 3D tensor of shape `(batch_size, m, n)`, representing batches of matrices of size `m x n`. - `B`: A 3D tensor of shape `(batch_size, n, p)`, representing batches of matrices of size `n x p`. Constraints: - The batch size for both tensors is the same. - Matrices within the batches have compatible dimensions for matrix multiplication. Output - A 3D tensor where each element represents the precision discrepancy between batched and individual computations for the corresponding element. Example ```python import torch A = torch.rand((4, 3, 3)) B = torch.rand((4, 3, 2)) discrepancy = stable_batch_matrix_multiply(A, B) print(discrepancy) ``` Explanation 1. Compute the batched matrix multiplication result `C = A @ B`. 2. Calculate the matrix multiplication for only the first slice `C_individual = A[0] @ B[0]`. 3. Calculate the absolute discrepancy between `C[0]` and `C_individual`. 4. Return a tensor containing this discrepancy. Notes - Remember to handle precision issues mentioned in the documentation. - Pay attention to efficient tensor operations without explicit Python loops. Evaluation Your implementation will be evaluated based on: - Correctness: Does the function return the correct precision discrepancies? - Performance: Is the function efficiently utilizing PyTorch\'s capabilities? - Stability: Does the function handle potential precision and stability issues appropriately?","solution":"import torch def stable_batch_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication on 3D tensors A and B and compare precision discrepancy with individual matrix multiplication. Parameters: A (torch.Tensor): A 3D tensor of shape (batch, m, n) B (torch.Tensor): A 3D tensor of shape (batch, n, p) Returns: torch.Tensor: A tensor containing the discrepancies in precision. # Perform batched matrix multiplication C_batched = torch.bmm(A, B) # Compute matrix multiplication for the first slice C_individual = A[0] @ B[0] # Calculate the absolute discrepancy discrepancy = torch.abs(C_batched[0] - C_individual) return discrepancy"},{"question":"# Question: You are given the `wine` dataset from scikit-learn, which contains various chemical properties of wine and their corresponding wine category. Your task is to implement and compare different Naive Bayes classifiers (GaussianNB, MultinomialNB, ComplementNB, BernoulliNB) on this dataset. Finally, you will identify which classifier performs the best based on accuracy. **Instructions:** 1. Load the `wine` dataset from scikit-learn. 2. Split the dataset into training and testing sets (70% training and 30% testing). 3. Implement the following Naive Bayes classifiers: - GaussianNB - MultinomialNB - ComplementNB - BernoulliNB 4. Fit each classifier on the training set and make predictions on the testing set. 5. Calculate the accuracy for each classifier. 6. Identify and print the classifier with the highest accuracy on the testing set. **Constraints:** - You should handle any necessary preprocessing (e.g., converting continuous features to discrete if required by a classifier). - You must use scikit-learn for the implementation of the classifiers. - You should use appropriate metrics from scikit-learn to calculate accuracy. **Input Format:** - No input from the user is required; the dataset should be loaded within the script. **Output Format:** - Print statements that include: - The accuracy of each classifier on the testing set. - The name of the classifier with the highest accuracy. # Example Output: ``` Accuracy of GaussianNB: 0.94 Accuracy of MultinomialNB: 0.89 Accuracy of ComplementNB: 0.91 Accuracy of BernoulliNB: 0.85 Best classifier: GaussianNB ``` Here is a sample structure to get you started: ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.preprocessing import KBinsDiscretizer from sklearn.metrics import accuracy_score # Load dataset data = load_wine() X = data.data y = data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize classifiers gnb = GaussianNB() mnb = MultinomialNB() cnb = ComplementNB() bnb = BernoulliNB() # Fit and predict gnb.fit(X_train, y_train) y_pred_gnb = gnb.predict(X_test) accuracy_gnb = accuracy_score(y_test, y_pred_gnb) # Discretize features for MultinomialNB and ComplementNB discretizer = KBinsDiscretizer(n_bins=10, encode=\'ordinal\', strategy=\'uniform\') X_train_binned = discretizer.fit_transform(X_train) X_test_binned = discretizer.transform(X_test) mnb.fit(X_train_binned, y_train) y_pred_mnb = mnb.predict(X_test_binned) accuracy_mnb = accuracy_score(y_test, y_pred_mnb) cnb.fit(X_train_binned, y_train) y_pred_cnb = cnb.predict(X_test_binned) accuracy_cnb = accuracy_score(y_test, y_pred_cnb) # BernoulliNB requires binary features X_train_binary = (X_train > X_train.mean(axis=0)).astype(int) X_test_binary = (X_test > X_test.mean(axis=0)).astype(int) bnb.fit(X_train_binary, y_train) y_pred_bnb = bnb.predict(X_test_binary) accuracy_bnb = accuracy_score(y_test, y_pred_bnb) # Print accuracies print(f\\"Accuracy of GaussianNB: {accuracy_gnb:.2f}\\") print(f\\"Accuracy of MultinomialNB: {accuracy_mnb:.2f}\\") print(f\\"Accuracy of ComplementNB: {accuracy_cnb:.2f}\\") print(f\\"Accuracy of BernoulliNB: {accuracy_bnb:.2f}\\") # Identify the best classifier best_classifier = max( [(\\"GaussianNB\\", accuracy_gnb), (\\"MultinomialNB\\", accuracy_mnb), (\\"ComplementNB\\", accuracy_cnb), (\\"BernoulliNB\\", accuracy_bnb)], key=lambda x: x[1] ) print(f\\"Best classifier: {best_classifier[0]}\\") ```","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.preprocessing import KBinsDiscretizer from sklearn.metrics import accuracy_score def compare_naive_bayes_classifiers(): # Load dataset data = load_wine() X = data.data y = data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize classifiers classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB(), \\"BernoulliNB\\": BernoulliNB() } # Prepare to store accuracies accuracies = {} # Fit and predict for GaussianNB classifiers[\\"GaussianNB\\"].fit(X_train, y_train) y_pred_gnb = classifiers[\\"GaussianNB\\"].predict(X_test) accuracies[\\"GaussianNB\\"] = accuracy_score(y_test, y_pred_gnb) # Discretize features for MultinomialNB and ComplementNB discretizer = KBinsDiscretizer(n_bins=10, encode=\'ordinal\', strategy=\'uniform\') X_train_binned = discretizer.fit_transform(X_train) X_test_binned = discretizer.transform(X_test) # Fit and predict for MultinomialNB classifiers[\\"MultinomialNB\\"].fit(X_train_binned, y_train) y_pred_mnb = classifiers[\\"MultinomialNB\\"].predict(X_test_binned) accuracies[\\"MultinomialNB\\"] = accuracy_score(y_test, y_pred_mnb) # Fit and predict for ComplementNB classifiers[\\"ComplementNB\\"].fit(X_train_binned, y_train) y_pred_cnb = classifiers[\\"ComplementNB\\"].predict(X_test_binned) accuracies[\\"ComplementNB\\"] = accuracy_score(y_test, y_pred_cnb) # BernoulliNB requires binary features X_train_binary = (X_train > X_train.mean(axis=0)).astype(int) X_test_binary = (X_test > X_test.mean(axis=0)).astype(int) # Fit and predict for BernoulliNB classifiers[\\"BernoulliNB\\"].fit(X_train_binary, y_train) y_pred_bnb = classifiers[\\"BernoulliNB\\"].predict(X_test_binary) accuracies[\\"BernoulliNB\\"] = accuracy_score(y_test, y_pred_bnb) # Print accuracies for clf_name, accuracy in accuracies.items(): print(f\\"Accuracy of {clf_name}: {accuracy:.2f}\\") # Identify the best classifier best_classifier = max(accuracies.items(), key=lambda x: x[1])[0] print(f\\"Best classifier: {best_classifier}\\") return accuracies, best_classifier"},{"question":"# Question You are tasked with designing a library system using Python\'s `dataclasses` module. Your goal is to create a structure to represent books and their associated information. Below are the requirements: 1. **Book**: - **Attributes**: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `price`: A float representing the price of the book. It defaults to 0.0. - `copies`: An integer representing the number of copies available. It should default to 1. - `genres`: A list of strings representing the genres of the book. It should default to an empty list. 2. **Library**: - **Attributes**: - `name`: A string representing the name of the library. - `books`: A list of `Book` instances. It should default to an empty list. 3. Implement these functionalities: - Add a `Book` to the library. - Remove a `Book` from the library by its title. - Generate a summary of the library, which is a dictionary representing the library\'s name and a list of all books in dictionary format. Tasks to Implement: 1. Define the `Book` dataclass with the specified attributes. 2. Define the `Library` dataclass with the specified attributes and functionalities. 3. Ensure that your `Library` class can add and remove books. 4. Use `dataclasses.asdict()` for generating the summary of the library. Example Usage: ```python # Create books book1 = Book(title=\\"Book One\\", author=\\"Author A\\", price=29.99, copies=5, genres=[\\"Fiction\\", \\"Adventure\\"]) book2 = Book(title=\\"Book Two\\", author=\\"Author B\\", price=39.99, copies=3) # Create a library library = Library(name=\\"My Library\\") # Add books to the library library.add_book(book1) library.add_book(book2) # Remove a book by title library.remove_book(\\"Book Two\\") # Generate summary of the library summary = library.generate_summary() print(summary) ``` Expected output: ```python { \'name\': \'My Library\', \'books\': [ { \'title\': \'Book One\', \'author\': \'Author A\', \'price\': 29.99, \'copies\': 5, \'genres\': [\'Fiction\', \'Adventure\'] } ] } ``` # Constraints: - Assume that book titles are unique within a library. - Handle cases where a book to be removed does not exist in the library.","solution":"from dataclasses import dataclass, asdict, field from typing import List @dataclass class Book: title: str author: str price: float = 0.0 copies: int = 1 genres: List[str] = field(default_factory=list) @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def remove_book(self, title: str): self.books = [book for book in self.books if book.title != title] def generate_summary(self): return { \'name\': self.name, \'books\': [asdict(book) for book in self.books] }"},{"question":"Objective: You are given a log processing task. Your goal is to write a Python function that reads from multiple log files, processes them, and writes the results back to the files in-place, including making backup copies of the original files. Task: 1. **Function Signature**: ```python def process_logs(files: list, error_only: bool = False): pass ``` 2. **Parameters**: - `files` (`list` of `str`): A list of filenames (strings) to be processed. - `error_only` (`bool`, optional): If `True`, only lines containing the string \\"Error\\" should be written back. Otherwise, all lines should be processed. Default is `False`. 3. **Functionality**: - Read each log file line by line. - If `error_only` is `True`, filter out lines that do not contain the word \\"Error\\" (case-sensitive). - Process all files in a way that modifies them in-place. - Create a backup of each original file with the extension \\".bak\\" before modifying it. - Write the processed content back to the original files. 4. **Constraints**: - The function should be robust and handle files with various encodings. - Handle empty files appropriately (they should remain empty after processing). - Use the `fileinput.input()` and context management where applicable. 5. **Hints**: - Make use of `fileinput.input()` for reading from multiple files. - Utilize the `inplace=True` and `backup=\'.bak\'` parameters of `fileinput.input()`. - Consider using optional arguments `encoding` and `errors` when working with potential encoding issues. Example: Suppose you have the following log files: ```plaintext File \'log1.txt\': INFO Some regular information Error An error occurred INFO More regular information File \'log2.txt\': Error Another error here INFO Just some info ``` Let’s call your function with `process_logs([\'log1.txt\', \'log2.txt\'], error_only=True)`: After processing, the contents of the files should be: ```plaintext File \'log1.txt\': Error An error occurred File \'log2.txt\': Error Another error here ``` And corresponding backup files `log1.txt.bak` and `log2.txt.bak` will have the original unmodified contents. Notes: - Make sure your implementation is efficient and handles large files gracefully. - Consider edge cases such as completely empty files or files without any \\"Error\\" lines when `error_only` is `True`.","solution":"import fileinput import shutil def process_logs(files: list, error_only: bool = False): for file in files: # Create a backup of the original file shutil.copy(file, file + \'.bak\') # Open the original file in write mode to overwrite its content with open(file, \'w\', encoding=\'utf-8\', errors=\'ignore\') as outfile: for line in fileinput.input(file + \'.bak\', mode=\'r\', encoding=\'utf-8\', errors=\'ignore\'): if not error_only or \\"Error\\" in line: outfile.write(line)"},{"question":"# Pandas Copy-on-Write Challenge Objective: You are tasked with manipulating a DataFrame using pandas. The challenge specifically emphasizes the nuances introduced by the Copy-on-Write (CoW) behavior. Thorough understanding of CoW will be critical for accomplishing the task correctly. Problem: Given a DataFrame `df` as below: ```python import pandas as pd data = { \\"A\\": [1, 2, 3, 4], \\"B\\": [5, 6, 7, 8], \\"C\\": [9, 10, 11, 12] } df = pd.DataFrame(data) ``` You need to perform the following operations while adhering to CoW principles: 1. Create a subset of the DataFrame containing only columns \'A\' and \'B\'. 2. Modify the value in the first row and first column of the subset to 100. 3. Ensure that the original DataFrame remains unaffected by the changes to the subset. 4. Return the modified subset and the original DataFrame to verify the changes. Input: - The DataFrame `df` as provided in the above code. Output: - A tuple of two DataFrames: `(modified_subset, original_df)` Constraints: - You must use pandas version >= 3.0, ensuring CoW is enabled. Example: ```python import pandas as pd def manipulate_dataframe(df): # Your code here pass data = { \\"A\\": [1, 2, 3, 4], \\"B\\": [5, 6, 7, 8], \\"C\\": [9, 10, 11, 12] } df = pd.DataFrame(data) modified_subset, original_df = manipulate_dataframe(df) print(modified_subset) print(original_df) ``` Expected output: ``` A B 0 100 5 1 2 6 2 3 7 3 4 8 A B C 0 1 5 9 1 2 6 10 2 3 7 11 3 4 8 12 ``` Note: Use of improper methods which violate CoW principles, such as chaining operations, will result in a penalty or incorrect evaluation. Ensure that the original DataFrame remains unchanged by the operations on the subset.","solution":"import pandas as pd def manipulate_dataframe(df): # Create a subset of the DataFrame containing only columns \'A\' and \'B\' subset = df[[\'A\', \'B\']].copy() # Modify the value in the first row and first column of the subset to 100 subset.iloc[0, 0] = 100 # Return the modified subset and the original DataFrame return subset, df"},{"question":"**Question: Persistent Contact Book** You are required to implement a small contact book application that saves and retrieves contact details using Python\'s `shelve` module. Each contact entry in the contact book should contain a name (string), phone number (string), and email address (string). Your implementation should provide the following functionalities: 1. Add a new contact. 2. Update an existing contact. 3. Delete a contact. 4. Retrieve a contact by name. 5. List all contacts. **Function Specifications:** 1. `add_contact(db_filename: str, name: str, phone: str, email: str) -> None` - Adds a new contact to the shelf. If a contact with the same name already exists, it should be updated with the new phone and email. 2. `update_contact(db_filename: str, name: str, phone: str, email: str) -> None` - Updates an existing contact\'s phone number and email. If the contact does not exist, raise a `KeyError`. 3. `delete_contact(db_filename: str, name: str) -> None` - Deletes the contact with the specified name. If the contact does not exist, raise a `KeyError`. 4. `get_contact(db_filename: str, name: str) -> dict` - Retrieves the contact details by name. Return a dictionary containing the contact details. If the contact does not exist, raise a `KeyError`. 5. `list_contacts(db_filename: str) -> list` - Lists all the contacts in the shelf. Return a list of dictionaries where each dictionary contains contact details. **Constraints:** - The `db_filename` parameter specifies the filename of the shelf. - Ensure that the shelf is properly closed after any operation to avoid data corruption. - Assume that name, phone, and email are always provided and are valid strings. - Handle mutable entries with and without the `writeback` parameter. **Example Usage:** ```python db_filename = \'contacts.db\' add_contact(db_filename, \'Alice\', \'123-456-7890\', \'alice@example.com\') add_contact(db_filename, \'Bob\', \'234-567-8901\', \'bob@example.com\') print(get_contact(db_filename, \'Alice\')) # Output: {\'name\': \'Alice\', \'phone\': \'123-456-7890\', \'email\': \'alice@example.com\'} update_contact(db_filename, \'Alice\', \'987-654-3210\', \'alice_new@example.com\') print(list_contacts(db_filename)) # Output: [{\'name\': \'Alice\', \'phone\': \'987-654-3210\', \'email\': \'alice_new@example.com\'}, # {\'name\': \'Bob\', \'phone\': \'234-567-8901\', \'email\': \'bob@example.com\'}] delete_contact(db_filename, \'Bob\') print(list_contacts(db_filename)) # Output: [{\'name\': \'Alice\', \'phone\': \'987-654-3210\', \'email\': \'alice_new@example.com\'}] ``` Implement the functions as outlined above.","solution":"import shelve def add_contact(db_filename: str, name: str, phone: str, email: str) -> None: with shelve.open(db_filename, writeback=True) as db: db[name] = {\'name\': name, \'phone\': phone, \'email\': email} def update_contact(db_filename: str, name: str, phone: str, email: str) -> None: with shelve.open(db_filename, writeback=True) as db: if name in db: db[name] = {\'name\': name, \'phone\': phone, \'email\': email} else: raise KeyError(f\\"Contact \'{name}\' not found\\") def delete_contact(db_filename: str, name: str) -> None: with shelve.open(db_filename, writeback=True) as db: if name in db: del db[name] else: raise KeyError(f\\"Contact \'{name}\' not found\\") def get_contact(db_filename: str, name: str) -> dict: with shelve.open(db_filename) as db: if name in db: return db[name] else: raise KeyError(f\\"Contact \'{name}\' not found\\") def list_contacts(db_filename: str) -> list: with shelve.open(db_filename) as db: return [db[key] for key in db]"},{"question":"Objective Implement a function to create and work with a Gaussian Mixture Model (GMM) using PyTorch\'s `torch.distributions` module. A GMM is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. Task You need to implement the following function: ```python import torch from torch.distributions import Normal, Categorical, MixtureSameFamily def create_and_evaluate_gmm(means, stds, weights, num_samples): Create a Gaussian Mixture Model (GMM) using the provided means, standard deviations, and weights. Parameters: - means (list of float): A list containing the means of the Gaussian components. - stds (list of float): A list containing the standard deviations of the Gaussian components. - weights (list of float): A list containing the weights of the Gaussian components (must sum to 1). - num_samples (int): Number of samples to generate from the GMM. Returns: - samples (torch.Tensor): A tensor containing `num_samples` samples generated from the GMM. - log_probs (torch.Tensor): A tensor containing the log probabilities of the generated samples. - sample_mean (float): The mean of the samples. - sample_std (float): The standard deviation of the samples. # Your implementation here ``` Requirements 1. **Create Gaussian Components**: Using the `Normal` class from `torch.distributions`, create Gaussian distributions for each set of `means` and `stds`. 2. **Weights for the Components**: Use the `Categorical` class to manage the weights of the Gaussian components. 3. **Mixture Model**: Combine these to create a mixture model using the `MixtureSameFamily` class. 4. **Sampling**: Generate `num_samples` samples from the mixture model. 5. **Log Probabilities**: Compute the log probabilities of the generated samples. 6. **Compute Statistics**: Calculate the mean and standard deviation of the generated samples. Constraints - The lengths of `means`, `stds`, and `weights` will be the same. - The sum of elements in `weights` will be 1. - `num_samples` will be a positive integer. Example ```python means = [0.0, 5.0] stds = [1.0, 1.0] weights = [0.5, 0.5] num_samples = 1000 samples, log_probs, sample_mean, sample_std = create_and_evaluate_gmm(means, stds, weights, num_samples) print(f\\"Samples: {samples[:5]}\\") print(f\\"Log probabilities: {log_probs[:5]}\\") print(f\\"Sample mean: {sample_mean}\\") print(f\\"Sample standard deviation: {sample_std}\\") ``` Expected Output The function should return the generated samples, their log probabilities, the mean of the samples, and their standard deviation as described. The exact values will depend on the random generator\'s state. Notes - Students should ensure that the PyTorch distributions are used and that the provided weights sum to 1. - They should also handle any necessary reshaping of tensors to ensure compatibility with `MixtureSameFamily`.","solution":"import torch from torch.distributions import Normal, Categorical, MixtureSameFamily def create_and_evaluate_gmm(means, stds, weights, num_samples): Create a Gaussian Mixture Model (GMM) using the provided means, standard deviations, and weights. Parameters: - means (list of float): A list containing the means of the Gaussian components. - stds (list of float): A list containing the standard deviations of the Gaussian components. - weights (list of float): A list containing the weights of the Gaussian components (must sum to 1). - num_samples (int): Number of samples to generate from the GMM. Returns: - samples (torch.Tensor): A tensor containing `num_samples` samples generated from the GMM. - log_probs (torch.Tensor): A tensor containing the log probabilities of the generated samples. - sample_mean (float): The mean of the samples. - sample_std (float): The standard deviation of the samples. # Create Gaussian distribution components gaussians = Normal(loc=torch.tensor(means), scale=torch.tensor(stds)) # Create a categorical distribution for the weights categorical = Categorical(probs=torch.tensor(weights)) # Create the GMM using MixtureSameFamily gmm = MixtureSameFamily(mixture_distribution=categorical, component_distribution=gaussians) # Generate samples from the GMM samples = gmm.sample((num_samples,)) # Compute log probabilities of the generated samples log_probs = gmm.log_prob(samples) # Calculate the mean and standard deviation of the samples sample_mean = torch.mean(samples).item() sample_std = torch.std(samples).item() return samples, log_probs, sample_mean, sample_std"},{"question":"# Complex Number Operations with `cmath` **Objective:** Write a Python function to perform a series of operations on complex numbers using the `cmath` module and return specific results. **Function Signature:** ```python def complex_operations(z1: complex, z2: complex) -> dict: Takes two complex numbers and performs various operations. Args: z1 (complex): First complex number. z2 (complex): Second complex number. Returns: dict: A dictionary containing the results of the operations. ``` # Instructions: 1. **Complex Addition and Subtraction:** - Compute the addition of `z1` and `z2`: `add_result` - Compute the subtraction of `z2` from `z1`: `sub_result` 2. **Coordinate Conversions:** - Convert `z1` to polar coordinates: `polar_z1` - Convert `z2` to polar coordinates: `polar_z2` - Convert the polar coordinates back to rectangular form: `rect_z1` and `rect_z2` 3. **Power and Logarithmic Functions:** - Compute the natural logarithm of `z1`: `log_z1` - Compute `z1` raised to the power of `z2`: `pow_result` 4. **Trigonometric and Hyperbolic Functions:** - Compute the cosine of `z1`: `cos_z1` - Compute the hyperbolic sine of `z2`: `sinh_z2` 5. **Classification Functions:** - Check whether `z1` is finite: `isfinite_z1` - Check whether `z2` is infinite: `isinf_z2` - Check whether `z2` is NaN: `isnan_z2` # Return Requirements: The function should return a dictionary with the following keys and their corresponding computed values: ```python { \\"add_result\\": complex, \\"sub_result\\": complex, \\"polar_z1\\": tuple, \\"polar_z2\\": tuple, \\"rect_z1\\": complex, \\"rect_z2\\": complex, \\"log_z1\\": complex, \\"pow_result\\": complex, \\"cos_z1\\": complex, \\"sinh_z2\\": complex, \\"isfinite_z1\\": bool, \\"isinf_z2\\": bool, \\"isnan_z2\\": bool, } ``` # Example Execution: ```python z1 = complex(1, 2) z2 = complex(2, 3) result = complex_operations(z1, z2) # Expected result format (exact values will vary based on the complex operations) print(result) # { # \\"add_result\\": (3+5j), # \\"sub_result\\": (-1-1j), # \\"polar_z1\\": (2.23606797749979, 1.1071487177940904), # \\"polar_z2\\": (3.605551275463989, 0.982793723247329), # \\"rect_z1\\": (1+2j), # \\"rect_z2\\": (2+3j), # \\"log_z1\\": (0.8047189562170501+1.1071487177940904j), # \\"pow_result\\": (-0.4194800136493621+0.8681775154956787j), # \\"cos_z1\\": (2.0327230070196656-3.0518977991517997j), # \\"sinh_z2\\": (-3.59056458998578+0.5309210862485197j), # \\"isfinite_z1\\": True, # \\"isinf_z2\\": False, # \\"isnan_z2\\": False, # } ``` # Constraints: - Do not use imports other than `cmath` and `math`. - Assume the inputs are always valid complex numbers. **Hints:** - Refer to the `cmath` module documentation for function details. - Pay close attention to branch cuts and special cases like infinity and NaN. Happy coding!","solution":"import cmath def complex_operations(z1: complex, z2: complex) -> dict: Takes two complex numbers and performs various operations. Args: z1 (complex): First complex number. z2 (complex): Second complex number. Returns: dict: A dictionary containing the results of the operations. add_result = z1 + z2 sub_result = z1 - z2 polar_z1 = cmath.polar(z1) polar_z2 = cmath.polar(z2) rect_z1 = cmath.rect(polar_z1[0], polar_z1[1]) rect_z2 = cmath.rect(polar_z2[0], polar_z2[1]) log_z1 = cmath.log(z1) pow_result = z1 ** z2 cos_z1 = cmath.cos(z1) sinh_z2 = cmath.sinh(z2) isfinite_z1 = cmath.isfinite(z1) isinf_z2 = cmath.isinf(z2) isnan_z2 = cmath.isnan(z2) results = { \\"add_result\\": add_result, \\"sub_result\\": sub_result, \\"polar_z1\\": polar_z1, \\"polar_z2\\": polar_z2, \\"rect_z1\\": rect_z1, \\"rect_z2\\": rect_z2, \\"log_z1\\": log_z1, \\"pow_result\\": pow_result, \\"cos_z1\\": cos_z1, \\"sinh_z2\\": sinh_z2, \\"isfinite_z1\\": isfinite_z1, \\"isinf_z2\\": isinf_z2, \\"isnan_z2\\": isnan_z2, } return results"},{"question":"Objective: Create a heatmap using the seaborn package to visualize the data from the **titanic** dataset included with seaborn. You will be required to demonstrate your understanding of various features and customizations available in seaborn’s heatmap function. Problem Statement: Given the **titanic** dataset, create a heatmap to show the survival rates of different classes of passengers based on their gender. Requirements: 1. Load the `titanic` dataset using seaborn’s `load_dataset` function. 2. Create a pivot table where: - The rows represent the passenger class (`pclass`). - The columns represent the passenger gender (`sex`). - The values represent the mean survival rate (`survived`). 3. Generate a heatmap: - Annotate the cells with the value formatted to two decimal places. - Use a colormap of your choice to display the heatmap. - Ensure the annotation text is displayed within the cells. - Adjust the figure size to provide clear visibility of the heatmap. - Set the minimum and maximum values of the colormap to 0 and 1, respectively. 4. Make sure to add a title to your heatmap for better understanding and axes labels if necessary. 5. Return the Axes object. Constraints: - Use seaborn and matplotlib for your visualizations. - Ensure your solution is efficient and follows best coding practices. Input: No direct user input is needed. You will load the dataset within your implementation. Output: You must return the Axes object of the heatmap. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def create_titanic_heatmap(): # Your code here # Example usage # ax = create_titanic_heatmap() # plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_heatmap(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a pivot table for survival rates based on class and gender pivot_table = titanic.pivot_table( index=\'pclass\', columns=\'sex\', values=\'survived\', aggfunc=\'mean\' ) # Plot the heatmap plt.figure(figsize=(10, 6)) ax = sns.heatmap( pivot_table, annot=True, fmt=\\".2f\\", cmap=\'coolwarm\', vmin=0, vmax=1 ) # Customize the plot ax.set_title(\'Survival Rates by Passenger Class and Gender on the Titanic\') ax.set_xlabel(\'Gender\') ax.set_ylabel(\'Passenger Class\') return ax"},{"question":"# PyTorch Tensor Views and Manipulation Given the concept of tensor views in PyTorch, your task is to implement a function that demonstrates the use of several view operations on a given tensor. Specifically, you need to: 1. Create a tensor with random values of size `(6, 8)`. 2. Perform a series of operations to transform this tensor. Your function should: - Reshape the tensor to `(3, 16)` using a view operation. - Transpose the reshaped tensor. - Make the transposed tensor contiguous. - Finally, convert the contiguous tensor to a one-dimensional tensor. Implement the following function: ```python import torch def tensor_view_operations(seed: int = 0) -> torch.Tensor: This function performs a series of view operations on a randomly created tensor. Args: - seed (int): Seed for the random number generator to ensure reproducibility. Returns: - torch.Tensor: A one-dimensional tensor resulting from the view operations. # Step 1: Set the random seed for reproducibility torch.manual_seed(seed) # Step 2: Create the initial random tensor of size (6, 8) initial_tensor = torch.rand(6, 8) # Step 3: Reshape the tensor to (3, 16) reshaped_tensor = initial_tensor.view(3, 16) # Step 4: Transpose the reshaped tensor transposed_tensor = reshaped_tensor.transpose(0, 1) # Step 5: Make the transposed tensor contiguous contiguous_tensor = transposed_tensor.contiguous() # Step 6: Convert the contiguous tensor to a one-dimensional tensor final_tensor = contiguous_tensor.view(-1) return final_tensor ``` # Constraints: - Do not use any loop structures. - Only use PyTorch tensor operations as described in the problem statement. # Example Usage: ```python # Example function call result = tensor_view_operations(seed=42) print(result) # This should print a one-dimensional tensor of 48 elements. ``` This question tests your understanding of tensor manipulations using view operations, handling tensor contiguity, and performing complex tensor transformations using PyTorch.","solution":"import torch def tensor_view_operations(seed: int = 0) -> torch.Tensor: This function performs a series of view operations on a randomly created tensor. Args: - seed (int): Seed for the random number generator to ensure reproducibility. Returns: - torch.Tensor: A one-dimensional tensor resulting from the view operations. # Step 1: Set the random seed for reproducibility torch.manual_seed(seed) # Step 2: Create the initial random tensor of size (6, 8) initial_tensor = torch.rand(6, 8) # Step 3: Reshape the tensor to (3, 16) reshaped_tensor = initial_tensor.view(3, 16) # Step 4: Transpose the reshaped tensor transposed_tensor = reshaped_tensor.transpose(0, 1) # Step 5: Make the transposed tensor contiguous contiguous_tensor = transposed_tensor.contiguous() # Step 6: Convert the contiguous tensor to a one-dimensional tensor final_tensor = contiguous_tensor.view(-1) return final_tensor"},{"question":"Implementing an Efficient Training Loop in PyTorch You are tasked with implementing a memory-efficient training loop for a simple recurrent neural network (RNN) model in PyTorch. Given a dataset of variable-length sequences, you must ensure that the training loop does not run into out-of-memory (OOM) errors and handles sequence padding correctly when using data parallelism. Requirements: 1. **Model Architecture**: - Implement an RNN based model using `nn.LSTM`. - The model should be able to handle variable-length padded sequences. 2. **Training Loop**: - Ensure the training loop is memory-efficient. This includes not accumulating gradients unnecessarily and correctly managing the lifetimes of intermediate tensors. - Use gradient checkpointing to reduce memory usage. - Use data parallelism to distribute the computation across multiple GPUs. 3. **Padding and Packing Sequences**: - Implement logic to pack sequences before feeding them into the RNN and then unpack them afterwards, taking into account the use of data parallelism. 4. **Dataset**: - Use a synthetic dataset of random sequences for testing. Ensure randomness by setting the random seed appropriately in data loader workers. Input: - Hyperparameters such as `input_size`, `hidden_size`, `num_layers`, `batch_size`, `learning_rate`, and others as required by your implementation. - A synthetic dataset generator function that provides variable-length sequences. Output: - Your implementation should print out the training loss after each epoch. Constraints: - The sequences provided can have lengths varying from 1 to 100. - Use `nn.DataParallel` to implement data parallelism. - Make sure your implementation is robust against OOM errors. Example: Here is a template to get you started: ```python import torch import torch.nn as nn import torch.optim as optim from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence from torch.utils.data import DataLoader, Dataset import random class RandomDataset(Dataset): def __init__(self, num_samples, max_length): self.data = [torch.randn(random.randint(1, max_length), 10) for _ in range(num_samples)] self.lengths = [len(seq) for seq in self.data] def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.lengths[idx] class RNNModel(nn.Module): def __init__(self, input_size, hidden_size, num_layers): super(RNNModel, self).__init__() self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True, enforce_sorted=False) packed_output, _ = self.lstm(packed_input) output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output torch.manual_seed(42) def main(): # Hyperparameters input_size = 10 hidden_size = 20 num_layers = 2 batch_size = 32 learning_rate = 0.001 num_epochs = 5 # Dataset and DataLoader dataset = RandomDataset(num_samples=1000, max_length=100) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: zip(*sorted(x, key=lambda s: s[1], reverse=True))) model = RNNModel(input_size, hidden_size, num_layers).cuda() model = nn.DataParallel(model) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): total_loss = 0 for sequences, lengths in dataloader: sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True).cuda() lengths = torch.tensor(lengths).cuda() optimizer.zero_grad() outputs = model(sequences, lengths) loss = criterion(outputs, sequences) # Assuming self-supervised learning for simplicity loss.backward() optimizer.step() total_loss += loss.item() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\') if __name__ == \\"__main__\\": main() ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence from torch.utils.data import DataLoader, Dataset import random class RandomDataset(Dataset): def __init__(self, num_samples, max_length): self.data = [torch.randn(random.randint(1, max_length), 10) for _ in range(num_samples)] self.lengths = [len(seq) for seq in self.data] def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.lengths[idx] class RNNModel(nn.Module): def __init__(self, input_size, hidden_size, num_layers): super(RNNModel, self).__init__() self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True, enforce_sorted=False) packed_output, _ = self.lstm(packed_input) output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output torch.manual_seed(42) def main(): # Hyperparameters input_size = 10 hidden_size = 20 num_layers = 2 batch_size = 32 learning_rate = 0.001 num_epochs = 5 # Dataset and DataLoader dataset = RandomDataset(num_samples=1000, max_length=100) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) model = RNNModel(input_size, hidden_size, num_layers).cuda() model = nn.DataParallel(model) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): total_loss = 0 for sequences, lengths in dataloader: sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True).cuda() lengths = torch.tensor(lengths).cuda() optimizer.zero_grad() outputs = model(sequences, lengths) loss = criterion(outputs, sequences) # Assuming self-supervised learning for simplicity loss.backward() optimizer.step() total_loss += loss.item() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\') # Colate function based on lengths def collate_fn(batch): batch.sort(key=lambda x: x[1], reverse=True) sequences, lengths = zip(*batch) sequences = [torch.tensor(seq) for seq in sequences] return sequences, lengths if __name__ == \\"__main__\\": main()"},{"question":"# URL Utility Functions You are tasked with implementing utility functions that handle URL parsing, modification, and construction. Use the `urllib.parse` module to achieve the following objectives. Ensure to handle both string and byte-based URLs where applicable. Tasks: 1. **Parse and Modify URL** - Implement a function `modify_url()` that takes a URL and a dictionary of URL parts to modify (like scheme, netloc, path, etc.), and returns the modified URL as a string. The function should maintain any parts not specified in the dictionary unchanged. ```python from urllib.parse import urlparse, urlunparse def modify_url(url, modifications): Modify parts of the URL specified in the modifications dictionary. Args: - url (str): The input URL to modify. - modifications (dict): URL parts to modify with keys as parts (like \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\') and respective new values. Returns: - str: The modified URL. # Your code here ``` 2. **Extract Query Parameters** - Create a function `extract_query_parameters()` that takes a URL and returns a dictionary of its query parameters. Handle URLs with and without query parameters. ```python from urllib.parse import urlparse, parse_qs def extract_query_parameters(url): Extract query parameters from the given URL. Args: - url (str): The input URL from which to extract query parameters. Returns: - dict: A dictionary containing the query parameters and their values as lists. # Your code here ``` 3. **Combine Base and Relative URL** - Write a function `combine_url()` that takes a base URL and a relative URL, and returns the combined absolute URL. ```python from urllib.parse import urljoin def combine_url(base, relative): Combine a base URL and a relative URL to form an absolute URL. Args: - base (str): The base URL. - relative (str): The relative URL. Returns: - str: The combined absolute URL. # Your code here ``` 4. **Quote and Unquote URL components** - Develop two functions: - `safe_url_quote()` that takes a string or bytes object and returns it fully quoted for safe embedding in a URL. - `safe_url_unquote()` that takes a quoted URL string or bytes and returns the unquoted original value. ```python from urllib.parse import quote, unquote def safe_url_quote(data): Quote the provided string or bytes object for safe embedding in a URL. Args: - data (str or bytes): The data to quote. Returns: - str: The quoted string. # Your code here def safe_url_unquote(data): Unquote the provided quoted URL component. Args: - data (str or bytes): The quoted URL component. Returns: - str: The unquoted original value. # Your code here ``` Example Usage: ```python # Example 1: Modify URL url = \\"http://example.com:80/path;parameters?query=val#fragment\\" modifications = {\'scheme\': \'https\', \'path\': \'/newpath\', \'query\': \'newquery=val2\'} print(modify_url(url, modifications)) # Output should be: \\"https://example.com:80/newpath;parameters?newquery=val2#fragment\\" # Example 2: Extract Query Parameters url = \\"http://example.com/path?name=foo&age=30&age=40\\" print(extract_query_parameters(url)) # Output should be: {\'name\': [\'foo\'], \'age\': [\'30\', \'40\']} # Example 3: Combine URL base = \\"http://example.com/path/to/resource/\\" relative = \\"../another/resource.txt\\" print(combine_url(base, relative)) # Output should be: \\"http://example.com/path/to/another/resource.txt\\" # Example 4: Quote URL Component data = \\"El Niño\\" print(safe_url_quote(data)) # Output should be: \\"El%20Ni%C3%B1o\\" # Example 5: Unquote URL Component quoted = \\"El%20Ni%C3%B1o\\" print(safe_url_unquote(quoted)) # Output should be: \\"El Niño\\" ``` **Constraints:** 1. Ensure that you handle edge cases like empty URL parts, invalid modifications, and non-string input gracefully. 2. The functions should only rely on the `urllib.parse` module for URL manipulations. 3. The URL components and query parameters should be handled per the rules defined in the documentation provided above.","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urljoin, quote, unquote def modify_url(url, modifications): Modify parts of the URL specified in the modifications dictionary. Args: - url (str): The input URL to modify. - modifications (dict): URL parts to modify with keys as parts (like \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\') and respective new values. Returns: - str: The modified URL. parsed_url = urlparse(url) url_parts = list(parsed_url) part_names = [\'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\'] for part, value in modifications.items(): if part in part_names: url_parts[part_names.index(part)] = value modified_url = urlunparse(url_parts) return modified_url def extract_query_parameters(url): Extract query parameters from the given URL. Args: - url (str): The input URL from which to extract query parameters. Returns: - dict: A dictionary containing the query parameters and their values as lists. parsed_url = urlparse(url) return parse_qs(parsed_url.query) def combine_url(base, relative): Combine a base URL and a relative URL to form an absolute URL. Args: - base (str): The base URL. - relative (str): The relative URL. Returns: - str: The combined absolute URL. return urljoin(base, relative) def safe_url_quote(data): Quote the provided string or bytes object for safe embedding in a URL. Args: - data (str or bytes): The data to quote. Returns: - str: The quoted string. return quote(data) def safe_url_unquote(data): Unquote the provided quoted URL component. Args: - data (str or bytes): The quoted URL component. Returns: - str: The unquoted original value. return unquote(data)"},{"question":"Objective Implement a function in Python that processes a list of mixed-type inputs (strings and numbers), converts all numeric types to a consistently formatted string representation, and performs case-insensitive sorting of all string elements based on their numeric value or their string content. Function Signature ```python def process_and_sort(inputs: list) -> list: Processes a list of mixed-type inputs (strings and numbers), converts all numeric types to a consistently formatted string representation, and performs case-insensitive sorting of all string elements based on their numeric value or their string content. Parameters: inputs (list): A list containing strings and numbers. Returns: list: A sorted list with all numerics converted to strings and all strings sorted case-insensitively. ``` Input and Output Formats - **Input**: A list `inputs` containing strings and numbers (integers and floats). For example, `[3, \\"apple\\", 2.5, \\"Banana\\", \\"10\\", 1]`. - **Output**: A list where all numbers have been converted to strings using a specific format and all strings are sorted case-insensitively based on their numerical value or string content. For example, `[\\"1.00\\", \\"2.50\\", \\"3.00\\", \\"10.00\\", \\"apple\\", \\"Banana\\"]`. Constraints and Requirements 1. All numeric values should be converted to a string with 2 decimal places. 2. Strings that can be interpreted as numbers should be treated as numbers and converted accordingly. 3. The function must handle erroneous or invalid numeric conversions gracefully by ignoring those inputs. 4. Perform case-insensitive sorting of the resultant list. 5. The function should handle corner cases consistently, reflecting the behavior specified in the provided documentation. Example ```python inputs = [3, \\"apple\\", 2.5, \\"Banana\\", \\"10.0\\", \\"1\\", \\"N/A\\"] output = process_and_sort(inputs) print(output) # Expected: [\\"1.00\\", \\"2.50\\", \\"3.00\\", \\"10.00\\", \\"apple\\", \\"Banana\\"] ``` Guidelines - Use the provided Python310 functions for string and number conversions where relevant. - Ensure thorough error handling and consider edge cases such as invalid string inputs for conversion. - Write clear and concise code with appropriate comments to explain intricate parts of your logic.","solution":"def process_and_sort(inputs): def convert_to_string(value): Converts numeric values to formatted strings with 2 decimal places. if isinstance(value, (int, float)): return f\\"{value:.2f}\\" elif isinstance(value, str): try: float_val = float(value) return f\\"{float_val:.2f}\\" except ValueError: return value else: return value # Convert all numeric values to formatted strings converted_inputs = [convert_to_string(item) for item in inputs] # Split into numeric strings and non-numeric strings numerics = [item for item in converted_inputs if item.replace(\'.\', \'\', 1).isdigit()] non_numerics = [item for item in converted_inputs if not item.replace(\'.\', \'\', 1).isdigit()] # Sort the lists sorted_numerics = sorted(numerics, key=lambda x: float(x)) sorted_non_numerics = sorted(non_numerics, key=lambda x: x.lower()) # Combine sorted lists return sorted_numerics + sorted_non_numerics"},{"question":"You are provided with a dataset that contains information about customers\' spending habits at a store. The dataset has the following columns: - `customer_id`: Unique identifier for each customer. - `amount_spent`: The total amount spent by the customer. - `day_of_week`: The day of the week when the purchase was made (e.g., \'Mon\', \'Tue\', etc.). - `gender`: The gender of the customer (\'Male\' or \'Female\'). Using Seaborn\'s `stripplot` function, you are required to generate insights from the dataset by visualizing the data in various ways. You should: 1. **Basic Stripplot:** - Create a basic stripplot to show the distribution of the `amount_spent` variable. 2. **Categorical Comparison:** - Create a stripplot to compare `amount_spent` across different `day_of_week`. 3. **Hue Analysis:** - Create a stripplot that incorporates `gender` to see the difference in spending patterns between males and females across different days of the week. 4. **Customization:** - Customize the stripplot to use a different palette (`\'Set2\'`), and modify the markers to be triangles with a size of 8 and 50% transparency. 5. **Handling Ambiguity:** - Using a similar dataset structure, create a stripplot where both axis variables are numeric. Set `orient` explicitly to ensure the correct interpretation of the axes. 6. **Faceting:** - Use `sns.catplot` to create a faceted stripplot showing `amount_spent` across different `day_of_week` for each `gender`. **Constraints:** - Use the Seaborn library. - Ensure the visualizations are clear and well-labeled. - Avoid overlapping of points as much as possible. # Input - `data`: A pandas DataFrame containing the columns specified above. # Expected Output - A series of stripplots and a faceted plot created using Seaborn. # Example ```python import seaborn as sns import pandas as pd # Example DataFrame data = pd.DataFrame({ \'customer_id\': range(1, 101), \'amount_spent\': np.random.rand(100) * 100, \'day_of_week\': np.random.choice([\'Mon\', \'Tue\', \'Wed\', \'Thu\', \'Fri\'], size=100), \'gender\': np.random.choice([\'Male\', \'Female\'], size=100) }) # Your code to create the visualizations here ``` # Performance Requirement - The plots should be generated efficiently and should handle a dataset with at least 10,000 entries.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Example DataFrame data = pd.DataFrame({ \'customer_id\': range(1, 101), \'amount_spent\': np.random.rand(100) * 100, \'day_of_week\': np.random.choice([\'Mon\', \'Tue\', \'Wed\', \'Thu\', \'Fri\'], size=100), \'gender\': np.random.choice([\'Male\', \'Female\'], size=100) }) def create_basic_stripplot(data): plt.figure(figsize=(10, 5)) sns.stripplot(x=data[\'amount_spent\']) plt.title(\'Basic Stripplot for Amount Spent\') plt.xlabel(\'Amount Spent\') plt.show() def create_categorical_comparison_stripplot(data): plt.figure(figsize=(10, 5)) sns.stripplot(x=\'day_of_week\', y=\'amount_spent\', data=data) plt.title(\'Amount Spent across Different Days of the Week\') plt.xlabel(\'Day of Week\') plt.ylabel(\'Amount Spent\') plt.show() def create_hue_analysis_stripplot(data): plt.figure(figsize=(10, 5)) sns.stripplot(x=\'day_of_week\', y=\'amount_spent\', hue=\'gender\', data=data) plt.title(\'Spending Patterns by Gender and Day of the Week\') plt.xlabel(\'Day of Week\') plt.ylabel(\'Amount Spent\') plt.legend(title=\'Gender\') plt.show() def create_customized_stripplot(data): plt.figure(figsize=(10, 5)) sns.stripplot(x=\'day_of_week\', y=\'amount_spent\', hue=\'gender\', data=data, palette=\'Set2\', marker=\'^\', size=8, alpha=0.5) plt.title(\'Customized Stripplot: Spending Patterns by Gender and Day of the Week\') plt.xlabel(\'Day of Week\') plt.ylabel(\'Amount Spent\') plt.legend(title=\'Gender\') plt.show() def create_numeric_axes_stripplot(data): # Creating a similar structure for demonstration purposes data_numeric = pd.DataFrame({ \'amount_spent\': np.random.rand(100) * 100, \'days\': np.random.randint(1, 8, size=100) }) plt.figure(figsize=(10, 5)) sns.stripplot(x=\'days\', y=\'amount_spent\', data=data_numeric, orient=\'v\') plt.title(\'Stripplot for Numeric Axes\') plt.xlabel(\'Days\') plt.ylabel(\'Amount Spent\') plt.show() def create_faceted_stripplot(data): sns.catplot(x=\'day_of_week\', y=\'amount_spent\', hue=\'gender\', data=data, kind=\'strip\', palette=\'Set2\', height=5, aspect=1.5) plt.title(\'Faceted Stripplot: Spending Patterns by Gender and Day of the Week\') plt.xlabel(\'Day of Week\') plt.ylabel(\'Amount Spent\') plt.show() # Generate plots create_basic_stripplot(data) create_categorical_comparison_stripplot(data) create_hue_analysis_stripplot(data) create_customized_stripplot(data) create_numeric_axes_stripplot(data) create_faceted_stripplot(data)"},{"question":"# Advanced Coding Assessment: Implementing and Testing a Complex Function Using `unittest` Objective This assessment aims to evaluate your proficiency with the `unittest` framework to write comprehensive unit tests. You will implement a somewhat intricate function and write corresponding unit tests. Problem Description Implement a function `filter_and_sort_numbers` that takes a list of numbers (both integers and floating-point) and a dictionary containing filter parameters. This function should: 1. Filter out numbers based on the parameters in the dictionary. 2. Sort the filtered numbers in ascending order. 3. Return the sorted list. # Filtering Criteria The filter parameters dictionary can have the following optional keys: - `\'min_value\'`: Only include numbers greater than or equal to this value. - `\'max_value\'`: Only include numbers less than or equal to this value. - `\'integer_only\'`: If `True`, only include integers in the result. If the dictionary is empty, return the sorted list without any filtering. # Function Signature ```python def filter_and_sort_numbers(numbers: List[Union[int, float]], filters: Dict[str, Union[int, float, bool]]) -> List[Union[int, float]]: pass ``` Requirements 1. Implement the `filter_and_sort_numbers` function. 2. Write a comprehensive suite of unit tests for this function using the `unittest` framework. Include tests for: - No filtering (only sorting). - Filtering with `\'min_value\'`. - Filtering with `\'max_value\'`. - Filtering with both `\'min_value\'` and `\'max_value\'`. - Filtering with `\'integer_only\'`. - Combination of all filters. - Edge cases such as empty list, and filters that exclude all numbers. Constraints - The list of numbers can contain up to 1000 elements. - The values in the list can range from `-10^6` to `10^6`. # Example ```python # Example usage numbers = [1.5, 3, -2, 7, 0, 3.5, 3.1] filters = {\'min_value\': 0, \'max_value\': 3.2, \'integer_only\': True} print(filter_and_sort_numbers(numbers, filters)) # Output: [3] ``` # Testing You must create a `TestFilterAndSortNumbers` test case that inherits from `unittest.TestCase` and include at least the following tests: 1. `test_no_filtering` 2. `test_min_value` 3. `test_max_value` 4. `test_min_and_max_value` 5. `test_integer_only` 6. `test_combined_filters` 7. `test_empty_list` 8. `test_exclude_all_numbers` # Submission Submit the `filter_and_sort_numbers` function along with the test cases implemented in a single script. Ensure that your script can be run directly to execute all tests by including the boilerplate code: ```python if __name__ == \'__main__\': unittest.main() ``` Hint Use the `unittest` documentation provided to understand different assertions and test case structures that can be used to validate your function\'s correctness.","solution":"from typing import List, Union, Dict def filter_and_sort_numbers(numbers: List[Union[int, float]], filters: Dict[str, Union[int, float, bool]]) -> List[Union[int, float]]: if not filters: return sorted(numbers) filtered_numbers = numbers[:] if \'min_value\' in filters: min_value = filters[\'min_value\'] filtered_numbers = [num for num in filtered_numbers if num >= min_value] if \'max_value\' in filters: max_value = filters[\'max_value\'] filtered_numbers = [num for num in filtered_numbers if num <= max_value] if \'integer_only\' in filters and filters[\'integer_only\']: filtered_numbers = [num for num in filtered_numbers if isinstance(num, int)] return sorted(filtered_numbers)"},{"question":"Objective Write a function `custom_color_palettes(data, n_colors)` that takes a dataset and an integer `n_colors` as inputs and visualizes the distribution of one of the numeric columns in the dataset using multiple color palettes. The function should demonstrate the student\'s ability to create and utilize various color palettes in seaborn for data visualization purposes. Inputs - `data` (DataFrame): A pandas DataFrame containing at least one numeric column. - `n_colors` (int): The number of colors to be used in the palettes. Output - A matplotlib figure with subplots, each using a different color palette to visualize the distribution of the same numeric column of the dataset. Requirements 1. The function should create at least four subplots, each using a different color palette: - A light_palette with a named color. - A light_palette with a hex code. - A light_palette with a HUSL color. - A continuous colormap. 2. Each subplot should be a histogram of the same numeric column in the dataset. 3. Students are free to choose which numeric column to visualize, provided it is clearly indicated in a title or a comment. Constraints - Ensure that the function handles datasets of varying sizes and ensures the selected numeric column exists. - Use descriptive titles for each subplot to indicate the type of palette used. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample Data data = pd.DataFrame({ \'values\': [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5] }) def custom_color_palettes(data, n_colors): sns.set_theme(style=\\"whitegrid\\") # Select the numeric column to visualize column = \'values\' # Define palettes palettes = [ sns.light_palette(\\"seagreen\\", n_colors=n_colors), sns.light_palette(\\"#79C\\", n_colors=n_colors), sns.light_palette((20, 60, 50), input=\\"husl\\", n_colors=n_colors), sns.light_palette(\\"xkcd:copper\\", n_colors=n_colors, as_cmap=True) ] fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Create histograms with different palettes sns.histplot(data[column], ax=axes[0, 0], palette=palettes[0]).set_title(\\"seagreen light palette\\") sns.histplot(data[column], ax=axes[0, 1], palette=palettes[1]).set_title(\\"hex code light palette\\") sns.histplot(data[column], ax=axes[1, 0], palette=palettes[2]).set_title(\\"HUSL light palette\\") sns.histplot(data[column], ax=axes[1, 1], palette=palettes[3]).set_title(\\"continuous colormap\\") plt.tight_layout() plt.show() # Example call to the function custom_color_palettes(data, 8) ``` Note - Ensure that all necessary imports are included. - The example provided is for reference; students should test the function with a variety of datasets and ensure robustness.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_color_palettes(data, n_colors): sns.set_theme(style=\\"whitegrid\\") # Ensure dataset has numeric columns numeric_cols = data.select_dtypes(include=[\'number\']).columns if len(numeric_cols) == 0: raise ValueError(\\"The dataset must contain at least one numeric column.\\") # Select the first numeric column to visualize column = numeric_cols[0] # Define palettes palettes = [ sns.light_palette(\\"seagreen\\", n_colors=n_colors), sns.light_palette(\\"#79C\\", n_colors=n_colors), sns.light_palette((20, 60, 50), input=\\"husl\\", n_colors=n_colors), sns.light_palette(\\"xkcd:copper\\", n_colors=n_colors, as_cmap=True) ] fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Create histograms with different palettes sns.histplot(data[column], ax=axes[0, 0], palette=palettes[0]).set_title(\\"seagreen light palette\\") sns.histplot(data[column], ax=axes[0, 1], palette=palettes[1]).set_title(\\"hex code light palette\\") sns.histplot(data[column], ax=axes[1, 0], palette=palettes[2]).set_title(\\"HUSL light palette\\") sns.histplot(data[column], ax=axes[1, 1], palette=palettes[3]).set_title(\\"continuous colormap\\") plt.tight_layout() plt.show()"},{"question":"**Problem: File Descriptor Management and Locking** You are tasked with writing a Python function to manage file access and ensure safe file operations using the `fcntl` module. Your function will perform several operations including setting file descriptor flags, locking and unlocking files, and getting information about file descriptors. # Function Definition ```python def manage_file_operations(file_path: str): Manages file operations using the fcntl module. Parameters: file_path (str): The path to the file to manage. Returns: dict: A dictionary containing the results of the operations. pass ``` # Requirements 1. **Open the File**: - Open the file specified by `file_path` in read-write mode. Ensure the file is created if it does not exist. 2. **Set File Descriptor Flags**: - Use `fcntl.fcntl()` to set the file descriptor to be non-blocking (`os.O_NONBLOCK`). 3. **Lock the File**: - Acquire an exclusive lock on the file using `fcntl.lockf()`. Handle cases where the lock cannot be acquired (e.g., raise an error if the lock is not available). 4. **Retrieve and Return File Information**: - Use `fcntl.ioctl()` to retrieve the size of the pipe (if the file is a named pipe) using the `F_GETPIPE_SZ` constant (if applicable). - Return a dictionary with the following keys: - `\'fd\'`: The file descriptor of the opened file. - `\'fcntl_flag_set\'`: A boolean indicating if the non-blocking flag was successfully set. - `\'file_locked\'`: A boolean indicating if the file was successfully locked. - `\'pipe_size\'`: The size of the pipe in bytes (if applicable, else `None`). # Constraints - If any operation fails, an `OSError` should be raised with an appropriate error message. - The file path provided will always be a valid path in the system. - Ensure to release the lock and close the file properly before the function exits, regardless of whether an error occurs or not. # Example ```python result = manage_file_operations(\'/path/to/file\') print(result) # Example Output: # { # \'fd\': 3, # \'fcntl_flag_set\': True, # \'file_locked\': True, # \'pipe_size\': 4096 # } ``` In the output example, `fd` is the file descriptor, `fcntl_flag_set` is True indicating the non-blocking flag was set, `file_locked` is True indicating the file was locked, and `pipe_size` is 4096 bytes (if the file is a named pipe). # Note - Test the function in an environment where you have the appropriate permissions to open, lock, and perform I/O control operations on files.","solution":"import os import fcntl def manage_file_operations(file_path: str): Manages file operations using the fcntl module. Parameters: file_path (str): The path to the file to manage. Returns: dict: A dictionary containing the results of the operations. result = { \'fd\': None, \'fcntl_flag_set\': False, \'file_locked\': False, \'pipe_size\': None } fd = None try: # Open the file in read-write mode fd = os.open(file_path, os.O_RDWR | os.O_CREAT) result[\'fd\'] = fd # Set the file descriptor to be non-blocking flags = fcntl.fcntl(fd, fcntl.F_GETFL) fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) result[\'fcntl_flag_set\'] = True # Acquire an exclusive lock on the file fcntl.lockf(fd, fcntl.LOCK_EX) result[\'file_locked\'] = True # Get the pipe size, if applicable try: pipe_size = fcntl.ioctl(fd, fcntl.F_GETPIPE_SZ) result[\'pipe_size\'] = pipe_size except OSError: result[\'pipe_size\'] = None except OSError as e: raise OSError(f\\"Error managing file operations: {e}\\") finally: # Ensure the file lock is released and the file is closed if fd is not None: try: fcntl.lockf(fd, fcntl.LOCK_UN) except OSError: pass # Ignore errors in unlocking os.close(fd) return result"},{"question":"Implement a function `compare_directories(dir1, dir2)` that compares the contents of two directory paths `dir1` and `dir2` recursively. Your function should return a detailed report as a dictionary containing the following keys and their corresponding values: 1. `common_files`: A list of files that exist in both `dir1` and `dir2`. 2. `identical_files`: A list of files that are identical in both `dir1` and `dir2`. 3. `different_files`: A list of files that are present in both directories but differ in content. 4. `dir1_only`: A list of files and subdirectories that are present only in `dir1`. 5. `dir2_only`: A list of files and subdirectories that are present only in `dir2`. Function Signature: ```python def compare_directories(dir1: str, dir2: str) -> dict: pass ``` Input: - `dir1` (str): The path to the first directory. - `dir2` (str): The path to the second directory. Output: - (dict): A dictionary with keys `common_files`, `identical_files`, `different_files`, `dir1_only`, and `dir2_only`, containing lists of file and directory names as specified above. Example: ```python dir1 = \\"path/to/dir1\\" dir2 = \\"path/to/dir2\\" comparison_result = compare_directories(dir1, dir2) ``` Constraints: - You can assume that both `dir1` and `dir2` are valid directory paths. - You may use the `filecmp` module for comparisons, but cannot use external libraries. Requirements: 1. Utilize the `filecmp` module effectively to perform the required comparisons. 2. Ensure that your function handles directory comparison recursively. 3. The output dictionary should provide a clear and comprehensive report of the directory comparison. Performance Considerations: - Your solution should be efficient in terms of time complexity, especially when handling large directories with many files and subdirectories. - Make use of caching or shallow comparisons where appropriate to improve performance, but ensure correctness of detailed comparisons as needed.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: def compare_files(file1, file2): return filecmp.cmp(file1, file2, shallow=False) def scan_directory(directory): files_set = set() for root, _, files in os.walk(directory): for file in files: full_path = os.path.relpath(os.path.join(root, file), directory) files_set.add(full_path) return files_set dir1_files = scan_directory(dir1) dir2_files = scan_directory(dir2) common_files = dir1_files.intersection(dir2_files) dir1_only = dir1_files - dir2_files dir2_only = dir2_files - dir1_files identical_files = [] different_files = [] for file in common_files: file1 = os.path.join(dir1, file) file2 = os.path.join(dir2, file) if compare_files(file1, file2): identical_files.append(file) else: different_files.append(file) return { \'common_files\': list(common_files), \'identical_files\': identical_files, \'different_files\': different_files, \'dir1_only\': list(dir1_only), \'dir2_only\': list(dir2_only) }"},{"question":"# PyTorch Linear Algebra - QR Decomposition and Linear System Solver In this task, you will demonstrate your understanding of PyTorch\'s linear algebra operations by implementing two functions. 1. **QR Decomposition using PyTorch**: Implement a function `qr_decomposition` that performs the QR decomposition of a given square matrix. The QR decomposition factorizes a matrix (A) into a product (A = QR), where (Q) is an orthogonal matrix and (R) is an upper triangular matrix. ```python def qr_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Perform QR decomposition of a given square matrix. Arguments: matrix (torch.Tensor): A square matrix of shape (n, n). Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing Q and R matrices. # Your implementation here ``` 2. **Linear System Solver using QR Decomposition**: Implement a function `solve_linear_system_qr` that solves the linear system (Ax=b) using QR decomposition. Here, (A) is a square matrix and (b) is the right-hand side vector. ```python def solve_linear_system_qr(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Solves linear system Ax = b using QR decomposition. Arguments: A (torch.Tensor): A square matrix of shape (n, n). b (torch.Tensor): A vector of shape (n, ). Returns: torch.Tensor: Solution vector x of shape (n, ). # Your implementation here ``` Constraints: - The input matrix (A) for `qr_decomposition` and `solve_linear_system_qr` will be a square matrix of size (n, n) with (2 leq n leq 100). - The input vector (b) for `solve_linear_system_qr` will be of size (n, ). - You must use the PyTorch library to perform matrix operations. - The functions should handle edge cases appropriately (e.g., handling singular matrices by raising an appropriate error). Example: ```python import torch A = torch.tensor([[12., -51., 4.], [6., 167., -68.], [-4., 24., -41.]]) b = torch.tensor([1., 2., 3.]) Q, R = qr_decomposition(A) # Q and R should satisfy A = QR x = solve_linear_system_qr(A, b) # x should be the solution to Ax = b ``` Make sure your implementation is efficient and makes full use of PyTorch\'s capabilities.","solution":"import torch from typing import Tuple def qr_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Perform QR decomposition of a given square matrix. Arguments: matrix (torch.Tensor): A square matrix of shape (n, n). Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing Q and R matrices. Q, R = torch.qr(matrix) return Q, R def solve_linear_system_qr(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Solves linear system Ax = b using QR decomposition. Arguments: A (torch.Tensor): A square matrix of shape (n, n). b (torch.Tensor): A vector of shape (n, ). Returns: torch.Tensor: Solution vector x of shape (n, ). Q, R = qr_decomposition(A) y = torch.matmul(Q.t(), b) x = torch.linalg.solve(R, y) return x"},{"question":"**Question: Visualizing the Titanic Dataset Using Seaborn** You are given the Titanic dataset, and your task is to create two distinct visualizations using seaborn\'s `objects` interface. These visualizations should capture the relationship between different variables in the dataset. **Requirements**: 1. **Histogram**: Create a faceted histogram of the `age` variable split by `sex`, colored by the status of `alive`(yes/no), using a `binwidth` of 10. 2. **Bar Plot**: Create a stacked bar plot showing the count of passengers by their `class`, further split by `sex`. Write a function `visualize_titanic_data` that takes no parameters and produces the required plots. **Implementation Details**: - Utilize seaborn\'s objects interface. - Load the Titanic dataset using `seaborn.load_dataset(\\"titanic\\")`. - Ensure that both plots are displayed correctly when the function is called. **Input** - None. **Output** - Two visualizations as described should be rendered. **Constraints** - Use seaborn version that supports the `objects` interface. **Example**: ```python def visualize_titanic_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Faceted Histogram: Age distribution by sex and alive status hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the histogram hist_plot.show() # Stacked Bar Plot: Class distribution by sex bar_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) ) # Display the bar plot bar_plot.show() plt.show() # Call the function visualize_titanic_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Faceted Histogram: Age distribution by sex and alive status hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the histogram hist_plot.show() # Stacked Bar Plot: Class distribution by sex bar_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) ) # Display the bar plot bar_plot.show() plt.show() # Call the function visualize_titanic_data()"},{"question":"**Problem Statement:** You are tasked with implementing an advanced web crawler that respects the `robots.txt` rules of various websites. To achieve this, you will utilize the `urllib.robotparser` module to parse `robots.txt` files and determine if your web crawler can fetch certain URLs. Write a class `AdvancedRobotFileParser` that extends the functionality of `urllib.robotparser.RobotFileParser` to include the following additional features: 1. **Batch URL check**: Implement a method `can_fetch_batch(useragent, url_list)` that takes in a user agent string and a list of URLs. The method should return a dictionary where the keys are the URLs and the values are Booleans indicating whether the URL can be fetched or not. 2. **Checking refresh necessity**: Implement a method `needs_refresh(interval)` that checks if the `robots.txt` file should be refreshed based on a given interval (in seconds). This method should return `True` if the file was last fetched more than `interval` seconds ago, and `False` otherwise. **Input:** - A user agent string. - A list of URLs to check for fetch permissions. - An interval value in seconds for refreshing the `robots.txt` file. **Output:** - A dictionary indicating fetch permissions for each URL. - A Boolean indicating if the `robots.txt` file needs to be refreshed. **Constraints:** - Ensure that the `robots.txt` file reading and parsing is performed before batch checking URLs. - Assume that the URLs provided are all from the website corresponding to the `robots.txt` file set in the `AdvancedRobotFileParser`. - Consider performance and avoid unnecessary re-fetching of the `robots.txt` file. **Example:** ```python from urllib.robotparser import RobotFileParser import time class AdvancedRobotFileParser(RobotFileParser): def __init__(self, url=\'\'): super().__init__(url) self.last_fetch_time = None def read(self): super().read() self.last_fetch_time = time.time() def can_fetch_batch(self, useragent, url_list): result = {} for url in url_list: result[url] = self.can_fetch(useragent, url) return result def needs_refresh(self, interval): if self.last_fetch_time is None: return True current_time = time.time() return (current_time - self.last_fetch_time) > interval # Example usage: parser = AdvancedRobotFileParser() parser.set_url(\\"http://www.example.com/robots.txt\\") parser.read() # Assuming this action sets and reads the robots.txt file # Check permission for a batch of URLs useragent = \\"*\\" url_list = [ \\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\" ] permissions = parser.can_fetch_batch(useragent, url_list) print(permissions) # Expected output: {\'http://www.example.com/page1\': True/False, \'http://www.example.com/page2\': True/False} # Check if the robots.txt file needs refreshing after 3600 seconds refresh_needed = parser.needs_refresh(3600) print(refresh_needed) # Expected output: True/False depending on the last fetch time. ```","solution":"import time from urllib.robotparser import RobotFileParser class AdvancedRobotFileParser(RobotFileParser): def __init__(self, url=\'\'): super().__init__(url) self.last_fetch_time = None def read(self): super().read() self.last_fetch_time = time.time() def can_fetch_batch(self, useragent, url_list): result = {} for url in url_list: result[url] = self.can_fetch(useragent, url) return result def needs_refresh(self, interval): if self.last_fetch_time is None: return True current_time = time.time() return (current_time - self.last_fetch_time) > interval"},{"question":"Problem Statement You are required to demonstrate your understanding of the `torch.futures.Future` class and related utility functions by simulating a distributed computing scenario. In this exercise, you will: 1. Create a function `async_computation` that performs a simple asynchronous operation using the `torch.futures.Future` class. 2. Use `collect_all` to gather multiple future objects and ensure all computations are complete. 3. Implement another function `process_results` that waits for all futures to complete using `wait_all` and then processes the results. Detailed Requirements 1. **Function: async_computation** - **Inputs:** - `value` (int): A numerical value to be used in the computation. - **Outputs:** - Returns a `torch.futures.Future` object that will complete with `value * 2` after a simulated delay. 2. **Function: process_results** - **Inputs:** - `futures_list` (List[torch.futures.Future]): A list of `Future` objects. - **Outputs:** - Returns a list of results after all futures in `futures_list` have completed. - **Constraints:** - It must use `wait_all` to ensure all futures have completed before processing results. Example Usage ```python # Example usage: future1 = async_computation(10) future2 = async_computation(20) future3 = async_computation(30) futures_list = torch.futures.collect_all([future1, future2, future3]) results = process_results(futures_list) print(results) # Expected output: [20, 40, 60] ``` Additional Information - You may use the `asyncio` module or any other method to simulate the asynchronous behavior. - Ensure that the provided list of futures is handled correctly. # Constraints - Avoid using `time.sleep` directly in asynchronous functions; instead, make use of appropriate asynchronous delay methods. # Submission Guidelines - Your submission should include the implementations of `async_computation` and `process_results`. - Include comments explaining each significant step of your code.","solution":"import torch import asyncio from typing import List async def async_double(value: int) -> int: Asynchronously doubles the input value after a short delay. Args: value (int): A numerical value to double. Returns: int: The doubled value after a delay. await asyncio.sleep(1) # Simulated delay return value * 2 def async_computation(value: int) -> torch.futures.Future: Initiates an asynchronous computation to double the input value and returns a Future. Args: value (int): A numerical value to be used in the computation. Returns: torch.futures.Future: A future object that will complete with value * 2 after a simulated delay. loop = asyncio.get_event_loop() future = torch.jit._fork(async_double, value) return future def process_results(futures_list: List[torch.futures.Future]) -> List[int]: Waits for all futures in the input list to complete and processes the results. Args: futures_list (List[torch.futures.Future]): A list of Future objects. Returns: List[int]: A list of results after all futures have completed. results = torch.futures.wait_all(futures_list) return results"},{"question":"# Question: Mocking and Testing with `unittest.mock` **Objective**: Assess your understanding and ability to use the `unittest.mock` module to effectively create mocks, patch objects, and make assertions about their usage. Problem Description: You are tasked with implementing a simple weather data processing system. The system has the following components: 1. A `WeatherAPI` class that fetches weather data from an external API. 2. A `WeatherProcessor` class that processes the fetched weather data. 3. A `WeatherAggregator` class that aggregates weather data from multiple locations. The goal is to test the `WeatherAggregator` class using `unittest.mock`. Your task is to mock the `WeatherAPI` and `WeatherProcessor` classes, and then write tests to ensure that `WeatherAggregator` functions correctly. Implementation Requirements: 1. **WeatherAPI Class:** - `fetch_weather(location: str) -> dict`: Fetches and returns weather data for a given location. 2. **WeatherProcessor Class:** - `process_data(data: dict) -> dict`: Processes the raw weather data and returns processed data. 3. **WeatherAggregator Class:** - `__init__(self, api: WeatherAPI, processor: WeatherProcessor)`: Initializes with a `WeatherAPI` and `WeatherProcessor` instance. - `aggregate_weather(locations: list) -> dict`: Aggregates weather data for a list of locations and returns combined results. Your Tasks: 1. Implement the `WeatherAggregator` class with the above interface. 2. Write a test class `TestWeatherAggregator` to test the `aggregate_weather` method using `unittest.mock`: - Mock the `WeatherAPI` and `WeatherProcessor` classes. - Patch the `fetch_weather` method of `WeatherAPI` and the `process_data` method of `WeatherProcessor`. - Test that `aggregate_weather` correctly calls the `fetch_weather` and `process_data` methods. - Validate that the aggregated data returned by `aggregate_weather` is as expected. Expected Input and Output: 1. **Input (to `aggregate_weather` method)**: - A list of location strings. Example: `[\\"New York\\", \\"Los Angeles\\"]`. 2. **Output**: - A dictionary containing the aggregated weather data for the given locations. Constraints: - Each location will have unique weather data, and the number of locations will not exceed 100. - Ensure that mocks are configured correctly to simulate different weather data responses. Performance Requirements: - Efficiency is not a primary concern, but ensure that your tests run in a reasonable time frame. # Implementation: ```python import unittest from unittest.mock import Mock, patch from typing import List, Dict # Assume these classes are defined elsewhere in your system class WeatherAPI: def fetch_weather(self, location: str) -> dict: # Placeholder implementation pass class WeatherProcessor: def process_data(self, data: dict) -> dict: # Placeholder implementation pass class WeatherAggregator: def __init__(self, api: WeatherAPI, processor: WeatherProcessor): self.api = api self.processor = processor def aggregate_weather(self, locations: List[str]) -> Dict[str, dict]: aggregated_data = {} for location in locations: raw_data = self.api.fetch_weather(location) processed_data = self.processor.process_data(raw_data) aggregated_data[location] = processed_data return aggregated_data class TestWeatherAggregator(unittest.TestCase): @patch(\'__main__.WeatherAPI\') @patch(\'__main__.WeatherProcessor\') def test_aggregate_weather(self, MockWeatherProcessor, MockWeatherAPI): # Arrange mock_api_instance = MockWeatherAPI.return_value mock_processor_instance = MockWeatherProcessor.return_value mock_api_instance.fetch_weather.side_effect = lambda loc: {\\"temp\\": 20 + len(loc)} mock_processor_instance.process_data.side_effect = lambda data: {\\"processed_temp\\": data[\\"temp\\"] + 5} aggregator = WeatherAggregator(mock_api_instance, mock_processor_instance) locations = [\\"New York\\", \\"Los Angeles\\"] # Act result = aggregator.aggregate_weather(locations) # Assert expected_result = { \\"New York\\": {\\"processed_temp\\": 28}, \\"Los Angeles\\": {\\"processed_temp\\": 31} } self.assertEqual(result, expected_result) mock_api_instance.fetch_weather.assert_any_call(\\"New York\\") mock_api_instance.fetch_weather.assert_any_call(\\"Los Angeles\\") self.assertEqual(mock_api_instance.fetch_weather.call_count, 2) self.assertEqual(mock_processor_instance.process_data.call_count, 2) if __name__ == \'__main__\': unittest.main() ``` 1. **Implement the `WeatherAggregator` class** in compliance with the provided interface. 2. **Develop the `TestWeatherAggregator` class** to test the functionality of `WeatherAggregator` using mocks and patches. Instructions: - Follow the template provided. - Ensure that all tests pass when executed. - Check that the mocks are assertively called with expected parameters.","solution":"import unittest from unittest.mock import Mock, patch from typing import List, Dict class WeatherAPI: def fetch_weather(self, location: str) -> dict: # Placeholder implementation pass class WeatherProcessor: def process_data(self, data: dict) -> dict: # Placeholder implementation pass class WeatherAggregator: def __init__(self, api: WeatherAPI, processor: WeatherProcessor): self.api = api self.processor = processor def aggregate_weather(self, locations: List[str]) -> Dict[str, dict]: aggregated_data = {} for location in locations: raw_data = self.api.fetch_weather(location) processed_data = self.processor.process_data(raw_data) aggregated_data[location] = processed_data return aggregated_data"},{"question":"# Seaborn Categorical Plotting and Customization **Objective:** You are required to demonstrate your understanding of Seaborn\'s `catplot` functionality by creating a series of plots based on the Titanic dataset, followed by extensive customization of the plots. **Task:** 1. Load the Titanic dataset from Seaborn\'s sample datasets. 2. Create a single composite figure using the following specifications: - Use a `catplot` to visualize the relationship between `age` and `class` where: - In one subplot, differentiate the data by `sex` using different colors. - In another subplot, use the `who` variable to create a `box` plot. 3. Customize the plots: - Use appropriate labels for the x-axis and y-axis. - Set titles for each subplot to clearly indicate what is being visualized. - Adjust the y-axis limits to be between 0 to 80 years. - Customize the appearance of the legend and place it outside the plot. - Remove the top and right spines for a cleaner look. 4. Combine these subplots into a single figure, arranged in a single row. **Implementation Requirements:** - Use the `sns.catplot()` function for creating the plots. - Use the `set_axis_labels()`, `set_titles()`, and other relevant methods for customization. - The plots should be clear and professional, suitable for presenting in a report or paper. **Constraints:** - Do not use any libraries other than Seaborn and Pandas. - The final visualization should be clear and all elements should be appropriately labeled. **Expected Input:** The function should not take any arguments. **Expected Output:** The function should display the composed figure with the described customizations. **Example Implementation:** ```python import seaborn as sns import pandas as pd def create_titanic_plots(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the first catplot, differentiating by sex g = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", kind=\\"box\\", col=\\"sex\\", height=4, aspect=.6) # Set titles for the subplots g.set_titles(\\"Age vs Class differentiated by Sex\\") g.set_axis_labels(\\"Age\\", \\"Class\\") # Customization g.set(ylim=(0, 80)) g.despine(left=True) # Create the second catplot, with box plot for \'who\' h = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", kind=\\"box\\", hue=\\"who\\", height=4, aspect=.6) # Set titles for the subplots h.set_titles(\\"Age vs Class differentiated by \'Who\'\\") h.set_axis_labels(\\"Age\\", \\"Class\\") # Customization h.set(ylim=(0, 80)) h.despine(left=True) # Show plots plt.show() # Call the function to create and display the plots create_titanic_plots() ``` Ensure that your implementation adheres strictly to the specifications and makes full use of Seaborn\'s plotting and customization capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plots(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the first catplot for \'sex\' g1 = sns.catplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"box\\", height=4, aspect=.7) g1.set_axis_labels(\\"Class\\", \\"Age\\") g1.set_titles(\\"Age vs Class differentiated by Sex\\") g1.set(ylim=(0, 80)) g1.despine(left=True) plt.gca().legend(loc=\'upper right\', bbox_to_anchor=(1.15, 1)) # Create the second catplot for \'who\' g2 = sns.catplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"who\\", kind=\\"box\\", height=4, aspect=.7) g2.set_axis_labels(\\"Class\\", \\"Age\\") g2.set_titles(\\"Age vs Class differentiated by Who\\") g2.set(ylim=(0, 80)) g2.despine(left=True) plt.gca().legend(loc=\'upper right\', bbox_to_anchor=(1.15, 1)) # Combine the subplots into one figure fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", ax=ax1) ax1.set_title(\\"Age vs Class differentiated by Sex\\") ax1.set_xlabel(\\"Class\\") ax1.set_ylabel(\\"Age\\") ax1.set_ylim(0, 80) ax1.legend(loc=\'upper right\', bbox_to_anchor=(1.15, 1)) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"who\\", ax=ax2) ax2.set_title(\\"Age vs Class differentiated by Who\\") ax2.set_xlabel(\\"Class\\") ax2.set_ylabel(\\"Age\\") ax2.set_ylim(0, 80) ax2.legend(loc=\'upper right\', bbox_to_anchor=(1.15, 1)) sns.despine(ax=ax1, right=True, top=True) sns.despine(ax=ax2, right=True, top=True) plt.tight_layout() plt.show() # Uncomment the following line to test the function # create_titanic_plots()"},{"question":"# Question: Dynamo Tracing and FX Graph Generation **Objective**: To assess your understanding of TorchDynamo and its functionality in tracing Python functions and generating FX graphs. **Problem Statement**: Create a Python function that demonstrates the use of dynamic shapes with Dynamo, generating different FX graphs for variable input sizes. Your function should involve tensor operations that depend on the input tensor\'s size. # Requirements: 1. **Function Definition**: Define a function `dynamic_shape_operations(x)`, where `x` is a PyTorch tensor. 2. **Dynamic Handling**: The function should perform some operations on `x` where the operations branch or behave differently based on the size/shape of `x`. 3. **Dynamo Decorator**: Use the `@torch.compile` decorator to trace and compile the function. 4. **Graph Generation**: Demonstrate the generation of different FX graphs by calling the function multiple times with tensors of different shapes. 5. **Logging and Output**: Use appropriate logging to output the generated FX graphs. # Constraints: - Use only PyTorch and TorchDynamo functionalities. - Ensure functions and operations are valid PyTorch operations. - Assume input tensors\' dimensions will vary but will be well-formed (i.e., no zero-dimension tensors). ```python import torch @torch.compile(dynamic=True) def dynamic_shape_operations(x): # Ensure dynamic handling based on tensor shape if x.shape[0] > 10: return x * 2 else: return x + 5 # Generate and print FX graphs for different input sizes x1 = torch.randn(8, 3) x2 = torch.randn(12, 3) print(\\"Graph Trace for x1:\\") dynamic_shape_operations(x1) print(\\"nGraph Trace for x2:\\") dynamic_shape_operations(x2) ``` # Expected Output: - The logs should show the generated FX graphs for the different input tensors, reflecting the different operations based on the dynamic shape handling. - The output would document the process of how Dynamo traces and compiles functions dynamically based on different tensor shapes. # Performance: - The solution should efficiently handle the graph generation for varying tensor sizes without unnecessary recompilations.","solution":"import torch import torch._dynamo as dynamo @dynamo.optimize(\\"eager\\", dynamic=True) def dynamic_shape_operations(x): Perform operations based on the dynamic shape of the input tensor x. If the size of the first dimension is greater than 10, multiply by 2. Otherwise, add 5 to each element. if x.shape[0] > 10: return x * 2 else: return x + 5 # Demonstrate the generation of different FX graphs for varying input sizes def demonstrate_fx_graph_generation(): x1 = torch.randn(8, 3) x2 = torch.randn(12, 3) print(\\"Graph Trace for x1:\\") result1 = dynamic_shape_operations(x1) print(result1) print(\\"nGraph Trace for x2:\\") result2 = dynamic_shape_operations(x2) print(result2) # Call the demonstration function to show the FX graph traces demonstrate_fx_graph_generation()"},{"question":"Title: Implementing a Python Module with Script and Import Functionality **Question:** You are required to create a Python module named `calculator.py`, which provides basic arithmetic operations such as addition, subtraction, multiplication, and division. Additionally, this module should have command-line functionality implemented using the `__name__ == \'__main__\'` construct so that it can also be used as a standalone script. Specifically, you need to implement the following: 1. **Functions**: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference of `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient of `a` and `b`. 2. **Script Functionality**: - When run as a script, the module should accept command-line arguments to perform the respective operations. - Usage examples: - `python calculator.py add 1 2`: Should output `3`. - `python calculator.py subtract 5 3`: Should output `2`. - `python calculator.py multiply 4 3`: Should output `12`. - `python calculator.py divide 10 2`: Should output `5.0`. - Handle division by zero appropriately and print an error message instead of throwing an exception. 3. **Import Functionality**: - When imported, the module should allow access to the arithmetic functions without running any script-specific code. **Constraints**: - The module should be able to handle any valid numeric input. - Handle invalid inputs gracefully, providing appropriate error messages. **Requirements**: - Your solution should demonstrate the correct use of the `__name__ == \'__main__\'` idiom. - Implement proper usage of argument parsing for command-line functionality. **Input**: - Command-line arguments or function arguments when imported. **Output**: - Result of the arithmetic operation, or appropriate error messages. **Example**: When `calculator.py` is run with the command: ```bash python calculator.py add 10 5 ``` It should output: ``` 15 ``` When `calculator.py` is run with the command: ```bash python calculator.py divide 10 0 ``` It should output: ``` Error: Division by zero is not allowed. ``` When `calculator.py` is imported in another script: ```python import calculator result = calculator.add(10, 5) print(result) ``` It should output: ``` 15 ``` **Note**: Make sure all edge cases are handled gracefully, and your code structure promotes reusability and clarity.","solution":"def add(a, b): Returns the sum of a and b. return a + b def subtract(a, b): Returns the difference of a and b. return a - b def multiply(a, b): Returns the product of a and b. return a * b def divide(a, b): Returns the quotient of a and b. if b == 0: return \\"Error: Division by zero is not allowed.\\" return a / b if __name__ == \'__main__\': import sys if len(sys.argv) != 4: print(\\"Usage: python calculator.py [add|subtract|multiply|divide] num1 num2\\") else: operation = sys.argv[1] try: num1 = float(sys.argv[2]) num2 = float(sys.argv[3]) except ValueError: print(\\"Error: Please provide valid numbers for the operation.\\") sys.exit(1) if operation == \'add\': print(add(num1, num2)) elif operation == \'subtract\': print(subtract(num1, num2)) elif operation == \'multiply\': print(multiply(num1, num2)) elif operation == \'divide\': print(divide(num1, num2)) else: print(\\"Error: Unsupported operation. Use \'add\', \'subtract\', \'multiply\', or \'divide\'.\\")"},{"question":"You are provided with a dataset containing information about the monthly temperature changes in various cities over the years. Your task is to use this dataset and Seaborn to create informative line plots showcasing the trends and variations. Input Format 1. **temperature_data.csv**: The dataset file with the following columns: - `year`: The year the data was recorded. - `month`: The month the data was recorded. - `city`: The city where the data was recorded. - `temperature`: The average temperature recorded in that month. Requirements 1. Load the dataset into a pandas DataFrame. 2. Create a line plot for each city, showing the average temperature change over the years. Use the `hue` parameter to distinguish between different months. 3. Create another plot that displays the average temperature changes over the months for each year for a selected city (e.g., \\"New York\\"). Use different line styles for each year. 4. Customize the second plot to: - Show markers on the lines. - Use a different color palette. - Add a title and labels for the x and y axes. Constraints - You should use the correct Seaborn functions and parameters to achieve the required plots. - Make sure to handle the DataFrame operations properly to transform the data as needed for plotting. - Your code should be efficient and should not perform redundant operations. Output The code should output two Seaborn line plots: 1. One plot showing the average temperature change over the years for each city, with different hues for months. 2. Another customized plot showing the average temperature changes over the months for each year for a selected city, with markers and a custom color palette. Example Below is a pseudo-code outline of a possible solution. Remember to replace this with actual code that reads `temperature_data.csv` and generates the plots. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = pd.read_csv(\\"temperature_data.csv\\") # Plot 1: Temperature trend over years for each city, by month sns.lineplot(data=df, x=\\"year\\", y=\\"temperature\\", hue=\\"month\\", style=\\"city\\") plt.title(\\"Average Temperature Change Over Years by City and Month\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Temperature\\") plt.show() # Filter data for \'New York\' (or another selected city) city_data = df[df[\'city\'] == \'New York\'] # Plot 2: Temperature changes over months for each year in \'New York\' sns.lineplot(data=city_data, x=\\"month\\", y=\\"temperature\\", hue=\\"year\\", style=\\"year\\", markers=True, palette=\\"muted\\") plt.title(\\"Monthly Temperature Changes for Each Year in New York\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Temperature\\") plt.show() ``` Good luck!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_temperature_plots(file_path): # Load dataset df = pd.read_csv(file_path) # Create Plot 1: Temperature trend over years for each city, by month plt.figure(figsize=(14, 7)) sns.lineplot(data=df, x=\\"year\\", y=\\"temperature\\", hue=\\"month\\", style=\\"city\\") plt.title(\\"Average Temperature Change Over Years by City and Month\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Temperature\\") plt.legend(title=\\"Month\\") plt.show() # Filter data for \'New York\' city_data = df[df[\'city\'] == \'New York\'] # Create Plot 2: Temperature changes over months for each year in \'New York\' plt.figure(figsize=(14, 7)) sns.lineplot(data=city_data, x=\\"month\\", y=\\"temperature\\", hue=\\"year\\", style=\\"year\\", markers=True, palette=\\"muted\\") plt.title(\\"Monthly Temperature Changes for Each Year in New York\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Temperature\\") plt.legend(title=\\"Year\\") plt.show()"},{"question":"Objective: Write a Python program that takes a structured text input representing transactions and organizes them into a summary report. Your task demonstrates understanding of data structures, functions, file handling, and exception management. Problem Description: You are given a string representing multiple transactions in a comma-separated format, where each transaction contains an identifier, a type (either \\"expense\\" or \\"income\\"), a category, and an amount. Your program should: 1. Parse this input string to extract transaction details. 2. Store these transactions in a suitable data structure. 3. Compute the total amount of expenses and incomes, and additionally, the total amount for each category. 4. Handle any input errors gracefully, informing the user of invalid entries without interrupting the processing of other valid entries. 5. Output a summary report displaying the totals for \'expenses\', \'incomes\', and each category separately. Input Format: - A single string format with transactions separated by semicolons: ``` transaction_id,type,category,amount; transaction_id,type,category,amount; ... ``` - Example: ```python \\"tx001,income,salary,3000;tx002,expense,food,150;tx003,expense,travel,200\\" ``` Output Format: - A dictionary with the following structure: ```python { \\"total_expense\\": <total_expense_amount>, \\"total_income\\": <total_income_amount>, \\"categories\\": { \\"salary\\": <total_salary_amount>, \\"food\\": <total_food_amount>, ... } } ``` # Sample Output: ```python { \\"total_expense\\": 350, \\"total_income\\": 3000, \\"categories\\": { \\"salary\\": 3000, \\"food\\": 150, \\"travel\\": 200, } } ``` Function Signature: ```python def summarize_transactions(input_string: str) -> dict: pass ``` Constraints: - Invalid transaction entries should have the following characteristics: - Missing fields. - Incorrect data types for the amount (should be an integer or float). - Invalid transaction types (should be either \'expense\' or \'income\'). Example Scenario: Given the input string: `\\"tx001,income,salary,3000;tx002,expense,food,150;tx003,expense,travel,200;tx004,income,,500;tx005,expense,rent,xyz\\"`, the output should handle invalid entries properly, such as missing categories for `tx004` and invalid amount data type for `tx005`. Students need to ensure their code handles errors gracefully and continues processing valid entries.","solution":"def summarize_transactions(input_string: str) -> dict: summary = { \\"total_expense\\": 0, \\"total_income\\": 0, \\"categories\\": {} } transactions = input_string.strip().split(\';\') for transaction in transactions: fields = transaction.split(\',\') # Handle invalid entries gracefully if len(fields) != 4: # Invalid transaction format continue transaction_id, type_, category, amount = fields # Validate transaction type if type_ not in [\\"income\\", \\"expense\\"]: # Invalid transaction type continue # Check if amount is a valid number try: amount = float(amount) except ValueError: # Invalid amount continue # Update the summary if type_ == \\"income\\": summary[\\"total_income\\"] += amount else: summary[\\"total_expense\\"] += amount if category not in summary[\\"categories\\"]: summary[\\"categories\\"][category] = 0 summary[\\"categories\\"][category] += amount return summary"},{"question":"**Question:** You are given a dataset capturing information about various species of flowers, including their petal and sepal lengths and widths, as well as their species type. Your task is to: 1. **Visualize the data using seaborn with customized color palettes**: - Create a scatter plot of `sepal_length` versus `sepal_width`. - Use a qualitative colormap from `matplotlib` to distinguish different species. - Use different marker styles for each species to enhance visual distinction. 2. **Define and use a custom continuous colormap**: - Create a histogram of `petal_length` for a single species using a continuous colormap. - Customize the number of bins in the histogram. You must implement the two plotting functions: `scatter_plot_with_qualitative_colors` and `histogram_with_continuous_colors`. # Input: - A pandas DataFrame `df` with columns: `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. - For the histogram function, an additional string `species_name` indicating the species to filter by. # Output: - Both functions should generate and display the plots without returning any values. # Specifications: 1. In `scatter_plot_with_qualitative_colors`: - Use the `Set2` colormap with one color per species. - The `hue` parameter in the scatter plot should be set to distinguish species. 2. In `histogram_with_continuous_colors`: - Use the `viridis` colormap, and it should be continuous. - The number of bins should be 15. # Example Usage: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Sample DataFrame creation data = { \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] } df = pd.DataFrame(data) # Scatter plot scatter_plot_with_qualitative_colors(df) # Histogram plot for the species \'setosa\' histogram_with_continuous_colors(df, \'setosa\') ``` Write the required functions below: ```python def scatter_plot_with_qualitative_colors(df): # Your implementation here pass def histogram_with_continuous_colors(df, species_name): # Your implementation here pass ``` Make sure to add necessary imports and any helper functions within the function definitions to make the code self-contained and executable.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def scatter_plot_with_qualitative_colors(df): Create a scatter plot of sepal_length vs sepal_width distinguished by species using a qualitative colormap. # Unique species for color mapping species = df[\'species\'].unique() colors = sns.color_palette(\'Set2\', len(species)) plt.figure(figsize=(10, 6)) for s, color in zip(species, colors): species_data = df[df[\'species\'] == s] plt.scatter(species_data[\'sepal_length\'], species_data[\'sepal_width\'], label=s, color=color) plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.title(\'Scatter plot of Sepal Length vs Sepal Width\') plt.show() def histogram_with_continuous_colors(df, species_name): Create a histogram of petal_length for the given species using a continuous colormap. species_data = df[df[\'species\'] == species_name] petal_lengths = species_data[\'petal_length\'] # Create a continuous colormap colormap = plt.cm.viridis bin_edges = np.linspace(min(petal_lengths), max(petal_lengths), 16) n, bins, patches = plt.hist(petal_lengths, bins=bin_edges, color=\'blue\') # Normalize bin values to interval [0, 1] for apply colormap bin_centers = 0.5 * (bins[:-1] + bins[1:]) colormapped_values = (bin_centers - bin_centers.min()) / (bin_centers.max() - bin_centers.min()) for c, p in zip(colormapped_values, patches): plt.setp(p, \'facecolor\', colormap(c)) plt.xlabel(\'Petal Length\') plt.ylabel(\'Frequency\') plt.title(f\'Histogram of Petal Length for {species_name}\') plt.show()"},{"question":"Objective: Evaluate students\' proficiency in using `seaborn.objects` to create and customize plots, including setting axis limits and handling different data inputs. Problem Statement: Using the `seaborn.objects` module, you are tasked with designing a function that processes and visualizes data as specified by the user. You must implement a function `custom_plot(data, x_limits=None, y_limits=None, invert_y_axis=False)` that takes: - `data`: A dictionary with keys as labels and values as lists of tuples, where each tuple contains x and y coordinates for a point. For example, `{\'line1\': [(1, 2), (2, 3), (3, 1)], \'line2\': [(1, 4), (2, 2), (3, 5)]}`. - `x_limits`: A tuple specifying the minimum and maximum values for the x-axis limits. If not provided, default limits should be applied. - `y_limits`: A tuple specifying the minimum and maximum values for the y-axis limits. If not provided, default limits should be applied. - `invert_y_axis`: A boolean indicating whether the y-axis should be inverted. The function should: - Use the `seaborn.objects` interface to create the plots. - Plot each label in the `data` dictionary as a separate line in the plot. - Optionally set the axis limits as specified. - Optionally invert the y-axis if `invert_y_axis` is set to `True`. Requirements: - You may assume all input data is valid. - The function should be able to handle any number of lines provided in the `data` dictionary. Input: ```python data = { \'line1\': [(1, 2), (2, 3), (3, 1)], \'line2\': [(1, 4), (2, 2), (3, 5)] } x_limits = (0, 4) y_limits = (0, 6) invert_y_axis = True ``` Expected Output: A plot that includes: - Two lines representing `line1` and `line2`. - x-axis limits from 0 to 4. - y-axis limits from 0 to 6, but inverted based on the `invert_y_axis` flag. Function Signature: ```python def custom_plot(data: dict, x_limits: tuple = None, y_limits: tuple = None, invert_y_axis: bool = False): pass ``` Example Call: ```python data = {\'line1\': [(1, 2), (2, 3), (3, 1)], \'line2\': [(1, 4), (2, 2), (3, 5)]} x_limits = (0, 4) y_limits = (0, 6) invert_y_axis = True custom_plot(data, x_limits, y_limits, invert_y_axis) ```","solution":"import matplotlib.pyplot as plt import seaborn as sns def custom_plot(data: dict, x_limits: tuple = None, y_limits: tuple = None, invert_y_axis: bool = False): Generates a line plot from the provided data dictionary, with optional axis limits and y-axis inversion. Args: data (dict): Dictionary with keys as labels and values as lists of tuples (x, y). x_limits (tuple): Optional tuple specifying x-axis limits (min, max). y_limits (tuple): Optional tuple specifying y-axis limits (min, max). invert_y_axis (bool): Optional boolean determining if y-axis should be inverted. plt.figure() for label, points in data.items(): x, y = zip(*points) plt.plot(x, y, label=label) if x_limits is not None: plt.xlim(x_limits) if y_limits is not None: plt.ylim(y_limits) if invert_y_axis: plt.gca().invert_yaxis() plt.legend() plt.show()"},{"question":"**Objective:** Write a function to replace BatchNorm2d layers with GroupNorm layers in a given PyTorch model. # Problem Statement Given a PyTorch model composed of various layers, including BatchNorm2d layers, you need to replace all BatchNorm2d layers with GroupNorm layers. The number of groups in GroupNorm should be configurable through a parameter. Additionally, the number of groups should properly divide the number of channels; if this is not possible, set the number of groups equal to the number of channels, effectively treating each channel separately. # Function Signature ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: pass ``` # Input - `model`: A PyTorch model (`torch.nn.Module`) containing various layers, including BatchNorm2d layers. - `num_groups`: An integer representing the number of groups for GroupNorm. # Output - A PyTorch model (`torch.nn.Module`) with all BatchNorm2d layers replaced by GroupNorm layers, adhering to specified constraints. # Constraints 1. The function must handle both simple models and complex models with nested submodules. 2. Ensure that the modified model is functionally equivalent, preserving the forward pass computations. 3. The replacement applies only to `BatchNorm2d` layers. # Example Usage ```python import torch import torch.nn as nn import torch.nn.functional as F # Example model definition class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32*6*6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = self.bn1(x) x = F.relu(self.conv2(x)) x = self.bn2(x) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Example of replacing BatchNorm2d layers with GroupNorm layers model = ExampleModel() updated_model = replace_batchnorm_with_groupnorm(model, num_groups=4) print(updated_model) ``` # Note: - Ensure that the resulting GroupNorm layers have their number of groups appropriately set to the input `num_groups`, unless the channel count dictates otherwise (i.e., group count should evenly divide channel count or set groups equal to the number of channels). - Only `BatchNorm2d` layers should be replaced. Other BatchNorm types (e.g., `BatchNorm1d` or `BatchNorm3d`) should remain unchanged.","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: Replace all BatchNorm2d layers with GroupNorm layers in the provided model. Parameters: - model: torch.nn.Module - The input model containing BatchNorm2d layers. - num_groups: int - The number of groups for the GroupNorm layers. Returns: - torch.nn.Module - The modified model with GroupNorm layers. def replace_bn(module): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): num_channels = child.num_features groups = min(num_groups, num_channels) if num_channels % num_groups == 0 else num_channels setattr(module, name, nn.GroupNorm(groups, num_channels)) else: replace_bn(child) replace_bn(model) return model"},{"question":"# PCA Implementation and Interpretation Context Principal Component Analysis (PCA) is a statistical method used to reduce the dimensionality of a dataset while preserving as much variance as possible. It does this by transforming the data into a new coordinate system, where the greatest variances come to lie on the first coordinates (principal components), the second highest variance on the second coordinates, and so on. This task requires you to use `scikit-learn`\'s PCA implementation to analyze a given dataset, interpret its results, and understand the nature of the data through its principal components. Problem Statement You are provided with a dataset consisting of various measurements of Iris flowers (included in `scikit-learn` datasets). Using PCA, you should perform the following tasks: 1. **Load the Iris dataset**. 2. **Standardize the dataset** to have a mean of 0 and a standard deviation of 1. 3. **Perform PCA** to reduce the dataset to 2 principal components. 4. **Print the explained variance ratios** of the two principal components. 5. **Transform the iris dataset samples into their principal component coordinates**. 6. **Visualize** the transformed data points on a 2D plane with different colors for different species of Iris. Input No input needed as the Iris dataset is built-in within the `scikit-learn` library. Output - A 2D scatter plot with principal components as the axes, and the data points colored according to their species. - Printed explained variance ratios of the two principal components. Constraints - Use `scikit-learn`\'s implementation of PCA. - Standardize the dataset before applying PCA. - Use only the first two principal components for visualization. Example ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def pca_iris(): # Step 1: Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Step 2: Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Perform PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Step 4: Print the explained variance ratios print(\'Explained variance ratios:\', pca.explained_variance_ratio_) # Step 5: Transform the data to principal component coordinates (X_pca already transformed) # Step 6: Visualize the transformed data points plt.figure(figsize=(8, 6)) for target in np.unique(y): plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=iris.target_names[target]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of Iris Dataset\') plt.legend() plt.show() # Execute the function pca_iris() ``` In this implemented solution, you have demonstrated your understanding and ability to use PCA for dimensionality reduction, standardize data, interpret results, and visualize the findings.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def pca_iris(): # Step 1: Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Step 2: Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Perform PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Step 4: Print the explained variance ratios explained_variance_ratios = pca.explained_variance_ratio_ print(\'Explained variance ratios:\', explained_variance_ratios) # Step 5: Transform the data to principal component coordinates (X_pca already transformed) # Step 6: Visualize the transformed data points plt.figure(figsize=(8, 6)) for target in np.unique(y): plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=iris.target_names[target]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of Iris Dataset\') plt.legend() plt.show() # Return the transformed data and explained variance ratios for unit testing return X_pca, explained_variance_ratios # Execute the function pca_iris()"},{"question":"Objective: Your task is to implement a set of functions that demonstrate the creation, manipulation, and validation of Python floating point objects using the PyFloatObject APIs. Requirements: 1. **Function Name**: `create_float_from_string` * **Input**: A string representing a floating-point number (`str`). * **Output**: A Python floating point object created from the string. * **Example**: ```python assert isinstance(create_float_from_string(\\"123.45\\"), float) ``` 2. **Function Name**: `create_float_from_double` * **Input**: A double precision float number (`float`). * **Output**: A Python floating point object created from the double. * **Example**: ```python assert isinstance(create_float_from_double(123.45), float) ``` 3. **Function Name**: `check_float_exact` * **Input**: An object (`any`). * **Output**: A boolean indicating if the input is exactly a Python float (not a subtype). * **Example**: ```python assert check_float_exact(123.45) is True ``` 4. **Function Name**: `get_float_as_double` * **Input**: A Python floating point object (`float`). * **Output**: A double representation of the input float. * **Constraints**: The implementation should check for valid float and handle errors appropriately. * **Example**: ```python assert get_float_as_double(123.45) == 123.45 ``` 5. **Function Name**: `get_float_info` * **Output**: A dictionary containing the precision, minimum, and maximum values of a float. * **Example**: ```python info = get_float_info() assert \\"precision\\" in info assert \\"max\\" in info assert \\"min\\" in info ``` # Constraints: * Implement the functions using the provided PyFloatObject APIs. * Ensure error handling is in place where necessary. * Make use of the stable ABI functions described in the documentation. # Example Usage: ```python def create_float_from_string(s: str) -> float: # Implementation here pass def create_float_from_double(v: float) -> float: # Implementation here pass def check_float_exact(o: any) -> bool: # Implementation here pass def get_float_as_double(f: float) -> float: # Implementation here pass def get_float_info() -> dict: # Implementation here pass # Test cases assert isinstance(create_float_from_string(\\"123.45\\"), float) assert isinstance(create_float_from_double(123.45), float) assert check_float_exact(123.45) is True assert get_float_as_double(123.45) == 123.45 info = get_float_info() assert \\"precision\\" in info assert \\"max\\" in info assert \\"min\\" in info ``` Make sure your code passes all the provided test cases.","solution":"import sys def create_float_from_string(s: str) -> float: Creates a Python floating point object from a string. :param s: A string representing a floating-point number. :return: A floating point object. try: return float(s) except ValueError as e: raise ValueError(f\\"Unable to convert string to float: {s}\\") from e def create_float_from_double(v: float) -> float: Creates a Python floating point object from a double precision float number. :param v: A double precision float number. :return: A floating point object. return float(v) def check_float_exact(o: any) -> bool: Checks if the input object is exactly a Python float (not a subtype). :param o: Any object. :return: True if the object is exactly a float, False otherwise. return type(o) is float def get_float_as_double(f: float) -> float: Returns a double representation of the input float. :param f: A floating point object. :return: A double representation of the input float. if not isinstance(f, float): raise TypeError(\\"Input must be a float.\\") return f def get_float_info() -> dict: Returns a dictionary containing the precision, minimum, and maximum values of a float. :return: A dictionary with keys \'precision\', \'min\', and \'max\'. return { \'precision\': sys.float_info.dig, \'min\': sys.float_info.min, \'max\': sys.float_info.max }"},{"question":"Objective: Demonstrate your understanding of various `pandas` operations by analyzing a dataset on company employees and their performance scores. Scenario: Suppose you are given a dataset containing details about employees, including their department, role, salary, and performance scores for the first two quarters of the year. Your task is to analyze this data to get useful insights and prepare a report. Dataset: Here is a sample dataset: ```csv EmployeeID,Name,Department,Role,Salary,Q1_Score,Q2_Score 101,John Doe,Sales,Manager,75000,88,90 102,Jane Smith,Marketing,Executive,50000,79,82 103,Emily Davis,Sales,Executive,55000,85,88 104,Michael Brown,IT,Manager,90000,91,94 105,Jessica Wilson,IT,Analyst,58000,83,86 106,Daniel Jones,HR,Executive,48000,76,78 ``` Task: 1. **Load Dataset**: Read the CSV file and create a `DataFrame` from it. 2. **Data Cleaning**: - Ensure there are no missing values in any column. If there are, fill them with an appropriate method. 3. **Data Analysis**: - Calculate the mean salary of employees department-wise. - Find the maximum and minimum `Q1_Score` and `Q2_Score` in the dataset. - Rank employees based on their `Q1_Score` and `Q2_Score` and add these as new columns `Q1_Rank` and `Q2_Rank` in the `DataFrame`. 4. **Data Transformation**: - Create a new column `Avg_Score` which is the average of `Q1_Score` and `Q2_Score`. - Categorize the employees based on their `Avg_Score` into `Performance` categories: \'Outstanding\' (Avg_Score >= 90), \'Excellent\' (80 <= Avg_Score < 90), \'Good\' (70 <= Avg_Score < 80), and \'Needs Improvement\' (Avg_Score < 70). 5. **Data Slicing and Selection**: - Select all employees with a \'Manager\' role and display their details. - Extract details of employees in the \'Sales\' department with a `Performance` category of \'Excellent\' or higher. 6. **Aggregation and Grouping**: - Group the data by `Department` and calculate the average `Q1_Score`, `Q2_Score`, and `Avg_Score` for each department. 7. **Visualization**: - Plot a bar chart showing the average `Avg_Score` for each department. Constraints: - Use only `pandas` library for data reading, manipulation, and analysis. - Do not use any for loops; utilize vectorized operations in `pandas`. Submission: Submit your `Python` script or notebook that: - Loads the dataset. - Executes the data cleaning, analysis, transformation, and aggregation steps. - Produces the required plots. - Ensures your code is well-commented and follows best practices. Expected Output: A `DataFrame` with the derived columns and the required plots showing the analysis results.","solution":"import pandas as pd def load_dataset(filename): Load the CSV file into a DataFrame. return pd.read_csv(filename) def clean_data(df): Fill missing values in the DataFrame if any. df.fillna(method=\'ffill\', inplace=True) return df def analyze_data(df): Perform data analysis and transformation on the DataFrame. # Calculate mean salary department-wise mean_salary = df.groupby(\'Department\')[\'Salary\'].mean() # Find the maximum and minimum Q1_Score and Q2_Score max_q1_score = df[\'Q1_Score\'].max() min_q1_score = df[\'Q1_Score\'].min() max_q2_score = df[\'Q2_Score\'].max() min_q2_score = df[\'Q2_Score\'].min() # Rank employees based on Q1_Score and Q2_Score df[\'Q1_Rank\'] = df[\'Q1_Score\'].rank(ascending=False, method=\'min\') df[\'Q2_Rank\'] = df[\'Q2_Score\'].rank(ascending=False, method=\'min\') # Create Avg_Score column df[\'Avg_Score\'] = df[[\'Q1_Score\', \'Q2_Score\']].mean(axis=1) # Categorize the employees based on Avg_Score conditions = [ (df[\'Avg_Score\'] >= 90), (df[\'Avg_Score\'] >= 80) & (df[\'Avg_Score\'] < 90), (df[\'Avg_Score\'] >= 70) & (df[\'Avg_Score\'] < 80), (df[\'Avg_Score\'] < 70) ] categories = [\'Outstanding\', \'Excellent\', \'Good\', \'Needs Improvement\'] df[\'Performance\'] = pd.cut(df[\'Avg_Score\'], bins=[-float(\'inf\'), 70, 80, 90, float(\'inf\')], labels=categories, right=False) return df, mean_salary, max_q1_score, min_q1_score, max_q2_score, min_q2_score def select_managers(df): Select all employees with a Manager role. return df[df[\'Role\'] == \'Manager\'] def select_sales_excellent(df): Select details of employees in the Sales department with a Performance category of \'Excellent\' or higher. return df[(df[\'Department\'] == \'Sales\') & (df[\'Performance\'].isin([\'Outstanding\', \'Excellent\']))] def department_avg_scores(df): Group the data by Department and calculate the average Q1_Score, Q2_Score, and Avg_Score for each department. return df.groupby(\'Department\').agg({\'Q1_Score\':\'mean\', \'Q2_Score\':\'mean\', \'Avg_Score\':\'mean\'}) def plot_avg_scores(department_scores): Plot a bar chart showing the average Avg_Score for each department. department_scores[\'Avg_Score\'].plot(kind=\'bar\', title=\'Average Score by Department\') # Example usage: # df = load_dataset(\'employees.csv\') # df = clean_data(df) # df, mean_salary, max_q1_score, min_q1_score, max_q2_score, min_q2_score = analyze_data(df) # managers = select_managers(df) # sales_excellent = select_sales_excellent(df) # department_scores = department_avg_scores(df) # plot_avg_scores(department_scores)"},{"question":"**Problem Statement:** You are tasked with writing a Python class that dynamically creates and manages descriptor attributes. Using the `python310` package, you will implement the functionality to create new descriptors, check their types, and wrap objects. Requirements: 1. **DescriptorManager Class** - Create a class `DescriptorManager` with the following methods: - `add_getset_descriptor(self, name: str, get_func: Callable, set_func: Callable) -> None` - Adds a getset descriptor to the managed class. - `add_member_descriptor(self, name: str, type: type, offset: int) -> None` - Adds a member descriptor to the managed class. - `add_method_descriptor(self, name: str, func: Callable) -> None` - Adds a method descriptor to the managed class. - `is_data_descriptor(self, name: str) -> bool` - Checks if the descriptor is a data attribute. Returns `True` or `False`. - `wrap_object(self, obj: Any, wrap_obj: Any) -> None` - Wraps an object using a specified descriptor. 2. **Input and Output Formats** - The methods will primarily involve adding new descriptors and performing checks. There is no specific input/output format for the methods other than returning `None` or `bool` as mentioned. 3. **Example Usage:** ```python class DescriptorManager: def __init__(self): self.descriptors = {} def add_getset_descriptor(self, name, get_func, set_func): # Implement using python310 functions to add a getset descriptor def add_member_descriptor(self, name, type, offset): # Implement using python310 functions to add a member descriptor def add_method_descriptor(self, name, func): # Implement using python310 functions to add a method descriptor def is_data_descriptor(self, name): # Implement using python310 function to check if descriptor is a data attribute def wrap_object(self, obj, wrap_obj): # Implement using python310 function to wrap an object # Example usage: manager = DescriptorManager() manager.add_getset_descriptor(\\"example\\", lambda x: x * 2, lambda x, val: x / 2) print(manager.is_data_descriptor(\\"example\\")) # Expected: True or False depending on implementation ``` Constraints: - The functions from the `python310` package should be used to create and manage descriptors as described. - Assume the existence of a `python310` package with the provided functionality is accessible.","solution":"from typing import Callable, Any # Simulate the python310 package functions used to create and manage descriptors class python310: class getset_descriptor: def __init__(self, get_func, set_func): self.get_func = get_func self.set_func = set_func def __get__(self, instance, owner): return self.get_func(instance) def __set__(self, instance, value): self.set_func(instance, value) class member_descriptor: def __init__(self, type, offset): self.type = type self.offset = offset class method_descriptor: def __init__(self, func): self.func = func def __get__(self, instance, owner): return self.func.__get__(instance, owner) class DescriptorManager: def __init__(self): self.descriptors = {} def add_getset_descriptor(self, name: str, get_func: Callable, set_func: Callable) -> None: self.descriptors[name] = python310.getset_descriptor(get_func, set_func) def add_member_descriptor(self, name: str, type: type, offset:int) -> None: self.descriptors[name] = python310.member_descriptor(type, offset) def add_method_descriptor(self, name: str, func: Callable) -> None: self.descriptors[name] = python310.method_descriptor(func) def is_data_descriptor(self, name: str) -> bool: descriptor = self.descriptors.get(name) if descriptor is None: return False return hasattr(descriptor, \'__get__\') and hasattr(descriptor, \'__set__\') def wrap_object(self, obj: Any, wrap_obj: Any) -> None: wrap_obj.__class__ = obj.__class__ wrap_obj.__dict__ = obj.__dict__"},{"question":"# XML-RPC Client Implementation Challenge Objective You are tasked with implementing an XML-RPC client that interacts with a predefined XML-RPC server. The client will perform a series of remote procedure calls and handle various data types and errors. Server Details You can assume the XML-RPC server is running on `localhost` port `8080` and supports the following methods: 1. `add(x, y)`: Adds two numbers and returns the result. (`x` and `y` are integers or floats) 2. `subtract(x, y)`: Subtracts `y` from `x` and returns the result. (`x` and `y` are integers or floats) 3. `multiply(x, y)`: Multiplies two numbers and returns the result. (`x` and `y` are integers or floats) 4. `divide(x, y)`: Divides `x` by `y` and returns the result. (`x` and `y` are integers or floats) 5. `concatenate(str1, str2)`: Concatenates two strings and returns the result. (`str1` and `str2` are strings) 6. `get_current_time()`: Returns the current date and time as an `xmlrpc.client.DateTime` object. Task Implement a Python function `perform_xmlrpc_operations` which communicates with the XML-RPC server and performs the following operations: 1. Perform the `add` operation with inputs `10` and `5`, and store the result. 2. Perform the `subtract` operation with inputs `10` and `5`, and store the result. 3. Perform the `multiply` operation with inputs `10` and `5`, and store the result. 4. Perform the `divide` operation with inputs `10` and `5`, and store the result. 5. Perform the `concatenate` operation with inputs `\\"Hello, \\"` and `\\"World!\\"`, and store the result. 6. Perform the `get_current_time` operation and store the result as a formatted string \\"YYYY-MM-DD HH:MM:SS\\". 7. Handle any `Fault` or `ProtocolError` exceptions that occur during the operations. Return a dictionary containing the results of all operations and any errors encountered. Input and Output - **Input:** - No input parameters. - **Output:** - A dictionary with the following structure: ```python { \\"add_result\\": int or float, \\"subtract_result\\": int or float, \\"multiply_result\\": int or float, \\"divide_result\\": int or float, \\"concatenate_result\\": str, \\"current_time\\": str, \\"errors\\": list of str } ``` Constraints - The server is guaranteed to support the described methods. - Handle floating-point arithmetic and ensure precision where applicable. - Ensure the formatted time string follows the \\"YYYY-MM-DD HH:MM:SS\\" format. Example ```python def perform_xmlrpc_operations(): # Your code here result = perform_xmlrpc_operations() print(result) ``` Expected output (the time string will vary): ```python { \\"add_result\\": 15, \\"subtract_result\\": 5, \\"multiply_result\\": 50, \\"divide_result\\": 2.0, \\"concatenate_result\\": \\"Hello, World!\\", \\"current_time\\": \\"2023-01-01 12:00:00\\", \\"errors\\": [] } ```","solution":"import xmlrpc.client def perform_xmlrpc_operations(): server_url = \\"http://localhost:8080\\" client = xmlrpc.client.ServerProxy(server_url) results = { \\"add_result\\": None, \\"subtract_result\\": None, \\"multiply_result\\": None, \\"divide_result\\": None, \\"concatenate_result\\": None, \\"current_time\\": None, \\"errors\\": [] } def try_operation(operation, *args): try: return getattr(client, operation)(*args) except xmlrpc.client.Fault as fault: results[\\"errors\\"].append(f\\"Fault({operation}): {fault.faultCode} - {fault.faultString}\\") return None except xmlrpc.client.ProtocolError as error: results[\\"errors\\"].append(f\\"ProtocolError({operation}): {error.errcode} - {error.errmsg}\\") return None results[\\"add_result\\"] = try_operation(\\"add\\", 10, 5) results[\\"subtract_result\\"] = try_operation(\\"subtract\\", 10, 5) results[\\"multiply_result\\"] = try_operation(\\"multiply\\", 10, 5) results[\\"divide_result\\"] = try_operation(\\"divide\\", 10, 5) results[\\"concatenate_result\\"] = try_operation(\\"concatenate\\", \\"Hello, \\", \\"World!\\") current_time = try_operation(\\"get_current_time\\") if current_time: results[\\"current_time\\"] = current_time.value return results"},{"question":"# GZIP File Processor Problem Statement You are tasked with creating a utility to process GZIP files containing logs. Each log file contains lines of text, where each line represents an event in a system. Your utility should include the following functionalities: 1. **Compress a log file**: Compress a given log file and save the compressed file with the same name but with a `.gz` extension. 2. **Decompress a GZIP file**: Decompress a given GZIP file and save the decompressed contents in a file with the same name but without the `.gz` extension. 3. **Count Lines in a GZIP file**: Given a GZIP file, count the number of lines of logs it contains. Function Signatures ```python def compress_log_file(file_path: str, compresslevel: int = 9) -> None: Compress a given log file. Parameters: file_path (str): The path to the log file to be compressed. compresslevel (int): The compression level to use (0-9). Returns: None pass def decompress_gzip_file(gzip_file_path: str) -> None: Decompress a given GZIP file. Parameters: gzip_file_path (str): The path to the GZIP file to be decompressed. Returns: None pass def count_lines_in_gzip(gzip_file_path: str) -> int: Count the number of lines in a given GZIP file. Parameters: gzip_file_path (str): The path to the GZIP file. Returns: int: The number of lines in the GZIP file. pass ``` Constraints - Assume the input files are encoded in UTF-8. - The file paths provided are valid. - The lines in the log files are separated by newline characters (`n`). Example Usage ```python # Compressing a log file compress_log_file(\\"/path/to/logfile.txt\\") # Decompressing a GZIP file decompress_gzip_file(\\"/path/to/logfile.txt.gz\\") # Counting lines in a GZIP file line_count = count_lines_in_gzip(\\"/path/to/logfile.txt.gz\\") print(line_count) # Output: (integer representing the number of lines) ``` Requirements 1. Implement the three functions specified above using the \\"gzip\\" module. 2. Demonstrate handling exceptions appropriately when working with files. 3. Ensure that compressed and decompressed files maintain the integrity of the original data. Notes - Use the `gzip.open()` function for both reading and writing GZIP files. - Make sure to close files properly by using context managers (`with` statement). - The compression level should be adjustable from 0 (no compression) to 9 (maximum compression).","solution":"import gzip import shutil def compress_log_file(file_path: str, compresslevel: int = 9) -> None: Compress a given log file. Parameters: file_path (str): The path to the log file to be compressed. compresslevel (int): The compression level to use (0-9). Returns: None output_file_path = f\\"{file_path}.gz\\" with open(file_path, \'rb\') as f_in: with gzip.open(output_file_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) def decompress_gzip_file(gzip_file_path: str) -> None: Decompress a given GZIP file. Parameters: gzip_file_path (str): The path to the GZIP file to be decompressed. Returns: None output_file_path = gzip_file_path.rstrip(\'.gz\') with gzip.open(gzip_file_path, \'rb\') as f_in: with open(output_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def count_lines_in_gzip(gzip_file_path: str) -> int: Count the number of lines in a given GZIP file. Parameters: gzip_file_path (str): The path to the GZIP file. Returns: int: The number of lines in the GZIP file. line_count = 0 with gzip.open(gzip_file_path, \'rt\', encoding=\'utf-8\') as file: for line in file: line_count += 1 return line_count"},{"question":"**Question: Implementing and Manipulating UUIDs** You are tasked with designing a utility function to create and validate UUIDs for different purposes. The utility should generate UUIDs using different versions and verify their properties. # Function Requirements: 1. **generate_uuid(version, **kwargs)**: - **version** (int): The version of the UUID to generate (can be `1`, `3`, `4`, or `5`). - **kwargs**: Additional arguments depending on the version: - For version `1`: no additional arguments. - For version `3` or `5`: two additional arguments `namespace` and `name`. - For version `4`: no additional arguments. - **returns**: A UUID object. 2. **get_uuid_properties(uuid_obj)**: - **uuid_obj** (UUID): A UUID object. - **returns**: A dictionary containing the following properties of the UUID: - `\\"hex\\"`: The UUID as a 32-character lowercase hexadecimal string. - `\\"int\\"`: The UUID as a 128-bit integer. - `\\"urn\\"`: The UUID as a URN string. - `\\"variant\\"`: The UUID variant. - `\\"version\\"`: The UUID version. - `\\"is_safe\\"`: The safety status of the UUID. # Input and Output Formats: - The `generate_uuid` function should accept the version and corresponding arguments, and return a UUID object. - The `get_uuid_properties` function should accept a UUID object and return a dictionary with the UUID properties. # Example: ```python import uuid # Example usage of generate_uuid uuid1 = generate_uuid(1) uuid4 = generate_uuid(4) namespace_uuid = uuid.NAMESPACE_DNS uuid3 = generate_uuid(3, namespace=namespace_uuid, name=\'example.com\') uuid5 = generate_uuid(5, namespace=namespace_uuid, name=\'example.com\') # Example usage of get_uuid_properties properties1 = get_uuid_properties(uuid1) properties4 = get_uuid_properties(uuid4) properties3 = get_uuid_properties(uuid3) properties5 = get_uuid_properties(uuid5) print(properties1) print(properties4) print(properties3) print(properties5) ``` # Constraints: - The provided `version` to `generate_uuid` must be one of `1`, `3`, `4`, or `5`. - For `uuid3` and `uuid5`, `namespace` must be a valid UUID object and `name` must be a string. # Notes: - Use the `uuid` module for generating UUIDs and managing their properties. - Ensure the correct UUID type and attributes are handled based on the given version. - Validate the inputs as necessary to avoid incorrect UUID generation. **Your implementation should exhibit clear understanding and proper use of the `uuid` module and its functionalities.**","solution":"import uuid def generate_uuid(version, **kwargs): Generates a UUID based on the given version and additional arguments. if version == 1: return uuid.uuid1() elif version == 3: namespace = kwargs.get(\'namespace\') name = kwargs.get(\'name\') if not namespace or not isinstance(namespace, uuid.UUID) or not name: raise ValueError(\\"For version 3, \'namespace\' must be a valid UUID and \'name\' must be provided as a string.\\") return uuid.uuid3(namespace, name) elif version == 4: return uuid.uuid4() elif version == 5: namespace = kwargs.get(\'namespace\') name = kwargs.get(\'name\') if not namespace or not isinstance(namespace, uuid.UUID) or not name: raise ValueError(\\"For version 5, \'namespace\' must be a valid UUID and \'name\' must be provided as a string.\\") return uuid.uuid5(namespace, name) else: raise ValueError(\\"Invalid UUID version. Only versions 1, 3, 4, and 5 are supported.\\") def get_uuid_properties(uuid_obj): Returns properties of the given UUID object. if not isinstance(uuid_obj, uuid.UUID): raise ValueError(\\"The input must be a valid UUID object.\\") return { \\"hex\\": uuid_obj.hex, \\"int\\": uuid_obj.int, \\"urn\\": uuid_obj.urn, \\"variant\\": uuid_obj.variant, \\"version\\": uuid_obj.version, \\"is_safe\\": getattr(uuid_obj, \'is_safe\', \'unknown\') }"},{"question":"# URL Manipulation and Validation **Objective:** Write a function `create_absolute_url()` that takes a base URL and a relative URL as input, and returns an absolute URL. The function should also validate the resulting URL to ensure it does not contain any invalid or dangerous components. If the resulting URL is invalid, raise a `ValueError` with an appropriate message. **Function Signature:** ```python def create_absolute_url(base_url: str, relative_url: str) -> str: pass ``` # Input Format: - `base_url`: A string representing the base URL. It must be a valid URL. - `relative_url`: A string representing the relative URL. It can be any string that when combined with the base URL forms a valid URL. # Output Format: - The function should return a string representing the absolute URL created by combining the base URL and the relative URL. # Constraints: - The `base_url` and `relative_url` should only contain ASCII characters. - The function should ensure that the resulting URL is a valid, safe URL. Specifically, validate the following: - The `scheme` should be one of `[\\"http\\", \\"https\\", \\"ftp\\", \\"ftps\\"]`. - The resulting URL should not contain any control characters or spaces. - The `netloc` part should not include any dangerous or malformed segments. # Performance Requirements: - The function should handle typical URL lengths efficiently. # Example: ```python from urllib.parse import urljoin, urlsplit def create_absolute_url(base_url: str, relative_url: str) -> str: from urllib.parse import urljoin, urlsplit # Join the base URL and relative URL abs_url = urljoin(base_url, relative_url) # Parse the absolute URL parsed_url = urlsplit(abs_url) # Validate the scheme if parsed_url.scheme not in [\\"http\\", \\"https\\", \\"ftp\\", \\"ftps\\"]: raise ValueError(\\"Invalid URL scheme\\") # Validate the netloc for dangerous characters dangerous_characters = [\\" \\", \\"t\\", \\"n\\", \\"r\\"] if any(char in parsed_url.netloc for char in dangerous_characters): raise ValueError(\\"Netloc contains dangerous characters\\") # Decode the URL and check for control characters if not parsed_url.geturl().isascii() or any(ord(char) < 32 for char in parsed_url.geturl()): raise ValueError(\\"URL contains non-ASCII or control characters\\") return abs_url # Example Usage: try: abs_url = create_absolute_url(\\"http://example.com/path/\\", \\"../other-path\\") print(abs_url) # Outputs: http://example.com/other-path except ValueError as e: print(f\\"Error: {e}\\") ``` **Explanation:** 1. Use `urllib.parse.urljoin()` to combine the base URL and relative URL to form an absolute URL. 2. Use `urllib.parse.urlsplit()` to parse the absolute URL into its components. 3. Validate the `scheme` to ensure it is one of the allowed schemes. 4. Check the `netloc` for any dangerous characters and ensure it doesn\'t contain control characters or spaces. 5. Return the resulting absolute URL if all validations pass, otherwise raise a `ValueError` with an appropriate message.","solution":"from urllib.parse import urljoin, urlsplit def create_absolute_url(base_url: str, relative_url: str) -> str: Create an absolute URL by combining base_url and relative_url, and ensure the resulting URL is valid and safe. # Join the base URL and the relative URL abs_url = urljoin(base_url, relative_url) # Parse the absolute URL parsed_url = urlsplit(abs_url) # Validate the scheme if parsed_url.scheme not in [\\"http\\", \\"https\\", \\"ftp\\", \\"ftps\\"]: raise ValueError(\\"Invalid URL scheme\\") # Validate the netloc for dangerous characters dangerous_characters = [\\" \\", \\"t\\", \\"n\\", \\"r\\"] if any(char in parsed_url.netloc for char in dangerous_characters): raise ValueError(\\"Netloc contains dangerous characters\\") # Validate for control characters and ensure all parts are ASCII if not abs_url.isascii() or any(ord(char) < 32 for char in abs_url): raise ValueError(\\"URL contains non-ASCII or control characters\\") return abs_url"},{"question":"# Seaborn Plotting Context: Dynamic Visualization Problem Statement You are tasked with analyzing a dataset and presenting visual insights using Seaborn. Your goal is to create several plots where the text sizes are adapted based on the context of each visualization. Specifically, you need to generate two plots: 1. A line plot using the default plotting context. 2. A bar plot using the “talk” plotting context to ensure better readability. Function Definition Write a function `dynamic_plots(data: pd.DataFrame)` that generates the required plots. The function should: - Take in a Pandas DataFrame `data` containing at least two columns for plotting. - Generate a line plot using the default seaborn plotting context. - Generate a bar plot using the \\"talk\\" plotting context. - Display both plots. **Input:** - `data (pd.DataFrame)`: A pandas DataFrame with at least two numerical columns. **Output:** - Two plots displayed sequentially: a line plot with default text sizes and a bar plot with larger text sizes for readability. Constraints and Requirements: - The DataFrame will contain at least two columns suitable for plotting. - You must use `sns.plotting_context` as a context manager for the bar plot. - Ensure the plots are displayed sequentially. Example ``` python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example DataFrame data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25] }) def dynamic_plots(data): # Your implementation goes here # Calling the function dynamic_plots(data) ``` **Expected Output:** - A line plot using the default plotting context. - A bar plot using the “talk” plotting context with bigger text elements for clarity. **Ensure your solution is well-documented and handles edge cases appropriately.**","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def dynamic_plots(data: pd.DataFrame): Function to create a line plot using the default plotting context and a bar plot using the \\"talk\\" plotting context. Parameters: data (pd.DataFrame): A pandas DataFrame with at least two columns suitable for plotting. Returns: None if data.shape[1] < 2: raise ValueError(\\"DataFrame must contain at least two columns for plotting.\\") # Line plot using the default plotting context plt.figure(figsize=(10, 6)) sns.lineplot(data=data) plt.title(\\"Line Plot\\") plt.show() # Bar plot using the \\"talk\\" plotting context with sns.plotting_context(\\"talk\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=data.columns[0], y=data.columns[1], data=data) plt.title(\\"Bar Plot in \'talk\' Context\\") plt.show()"},{"question":"**Title:** Bytecode Instructions Analyzer and Filter **Objective:** Implement a function to analyze a Python function\'s bytecode instructions, filter out specific opcodes, and return a summary of these instructions. **Question:** Write a Python function named `filter_bytecode_instructions` that takes two arguments: - `func`: A Python function to be analyzed. - `opcodes_filter`: A list of opcode names (strings) that should be included in the output. The function should use the `dis` module to disassemble the provided function\'s bytecode, filter out the instructions that match with the opcodes specified in `opcodes_filter`, and return a summary of these instructions. The summary should be a list of dictionaries, with each dictionary representing an instruction and containing the following keys: - `\'offset\'`: The offset of the instruction within the bytecode sequence. - `\'opname\'`: The name of the opcode. - `\'arg\'`: The argument to the operation (if any), otherwise `None`. - `\'argval\'`: The resolved argument value (if any), otherwise same as `arg`. # Input Format 1. `func`: A Python function. 2. `opcodes_filter`: A list of strings representing the opcode names to filter. # Output Format - A list of dictionaries, each dictionary containing details of filtered instructions: ```python [ { \'offset\': int, \'opname\': str, \'arg\': int or None, \'argval\': str or int or None }, ... ] ``` # Constraints 1. The provided function `func` will be a valid Python function. 2. The `opcodes_filter` will be a list of valid opcode names found in `dis.opname`. # Example ```python import dis def example_function(x): return x + 1 opcodes = [\\"LOAD_FAST\\", \\"RETURN_VALUE\\"] result = filter_bytecode_instructions(example_function, opcodes) print(result) ``` **Expected Output:** ```python [ {\'offset\': 0, \'opname\': \'LOAD_FAST\', \'arg\': 0, \'argval\': \'x\'}, {\'offset\': 6, \'opname\': \'RETURN_VALUE\', \'arg\': None, \'argval\': None} ] ``` **Note**: You may assume the `dis` module is already imported and available to use.","solution":"import dis def filter_bytecode_instructions(func, opcodes_filter): Analyze the bytecode of a given function and filter instructions based on opcodes_filter. Args: func (callable): The Python function to be analyzed. opcodes_filter (list of str): List of opcode names to filter. Returns: list of dict: A list of dictionaries containing details of filtered instructions. bytecode = dis.Bytecode(func) filtered_instructions = [] for instr in bytecode: if instr.opname in opcodes_filter: filtered_instructions.append({ \'offset\': instr.offset, \'opname\': instr.opname, \'arg\': instr.arg, \'argval\': instr.argval }) return filtered_instructions"},{"question":"Objective: In this coding assessment, you will demonstrate your understanding of multiclass and multioutput classification using scikit-learn\'s `OneVsRestClassifier` and `MultiOutputClassifier`. Task: You are given a dataset of handwritten digits where each sample can belong to multiple categories, such as `0-9`. Specifically, for each digit image, we want two properties: the digit itself and the count of straight lines (0 to 3) in the digit. Combine these properties into a multiclass-multioutput classification task. Instructions: 1. **Load the dataset**: Use the digits dataset provided by `sklearn.datasets`. 2. **Feature Engineering**: - Extract the digit labels from the dataset. - Create an additional target vector indicative of the count of straight lines in each digit. Use an arbitrary function to derive this (e.g., `count_straight_lines`). 3. **Create a MultiOutput Classifier**: - Construct a `MultiOutputClassifier` that uses `OneVsRestClassifier` with `LinearSVC` as the base classifier. 4. **Training and Prediction**: - Train your MultiOutput Classifier on the digit dataset. - Predict the labels for the dataset. 5. **Evaluate**: - Compute metrics such as accuracy and F1-score for the combined classification task. Data Assumptions: - The dataset features are images of digits from 0-9. - The dataset target is the digit label. - The additional target is the count of straight lines (0 to 3) in the digit. Constraints: - You must use `MultiOutputClassifier` and `OneVsRestClassifier` as the core methods. - `LinearSVC` will be used as the base classifier. - Assume that the counting of straight lines is performed using a predefined function. Performance Requirements: - Ensure your solution is efficient in terms of computation and memory usage. - The model should be trained and evaluated within a reasonable time frame. Sample Code for Data Loading: ```python from sklearn.datasets import load_digits import numpy as np digits = load_digits() X = digits.data y_digits = digits.target def count_straight_lines(digit_image): # Implement a function that counts the number of straight lines in the digit. return np.random.randint(0, 4) # Placeholder function y_straight_lines = np.array([count_straight_lines(image) for image in X]) Y = np.vstack((y_digits, y_straight_lines)).T ``` Your Goal: Implement the complete pipeline starting from loading the data to evaluating the model. Submit your solution by defining the following functions: 1. `load_and_preprocess_data()`: Load and preprocess the data. 2. `create_multioutput_classifier()`: Create and return the `MultiOutputClassifier`. 3. `train_and_evaluate_model(classifier, X, Y)`: Train the classifier and evaluate using appropriate metrics. Example function definitions: ```python def load_and_preprocess_data(): # Function implementation here pass def create_multioutput_classifier(): # Function implementation here pass def train_and_evaluate_model(classifier, X, Y): # Function implementation here pass ``` Complete these functions and ensure to test your implementation thoroughly.","solution":"from sklearn.datasets import load_digits from sklearn.svm import LinearSVC from sklearn.multioutput import MultiOutputClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score import numpy as np def load_and_preprocess_data(): digits = load_digits() X = digits.data y_digits = digits.target def count_straight_lines(digit_image): line_counts = [0, 1, 2, 3] # Placeholder: in reality, should compute based on image return np.random.choice(line_counts) y_straight_lines = np.array([count_straight_lines(image) for image in X]) Y = np.vstack((y_digits, y_straight_lines)).T return X, Y def create_multioutput_classifier(): base_classifier = OneVsRestClassifier(LinearSVC()) multioutput_classifier = MultiOutputClassifier(base_classifier) return multioutput_classifier def train_and_evaluate_model(classifier, X, Y): X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) classifier.fit(X_train, Y_train) Y_pred = classifier.predict(X_test) accuracy_digits = accuracy_score(Y_test[:, 0], Y_pred[:, 0]) accuracy_lines = accuracy_score(Y_test[:, 1], Y_pred[:, 1]) f1_digits = f1_score(Y_test[:, 0], Y_pred[:, 0], average=\'weighted\') f1_lines = f1_score(Y_test[:, 1], Y_pred[:, 1], average=\'weighted\') return accuracy_digits, accuracy_lines, f1_digits, f1_lines # Main execution X, Y = load_and_preprocess_data() classifier = create_multioutput_classifier() accuracy_digits, accuracy_lines, f1_digits, f1_lines = train_and_evaluate_model(classifier, X, Y) output_metrics = { \'accuracy_digits\': accuracy_digits, \'accuracy_lines\': accuracy_lines, \'f1_digits\': f1_digits, \'f1_lines\': f1_lines }"},{"question":"# Question: Creating a Multi-faceted Scatter Plot with Seaborn Objective: Write a function `create_faceted_scatter_plot` that takes in a `pandas.DataFrame` and outputs a multi-faceted scatter plot using `seaborn` with the following specifications: 1. The scatter plot should plot `total_bill` on the x-axis and `tip` on the y-axis. 2. Use `hue` to differentiate based on the `day` column. 3. Use `style` to differentiate based on the `sex` column. 4. Facet the data by the `time` and `smoker` columns, creating a grid of plots where: - Row facets are divided by `time` (Lunch or Dinner). - Column facets are divided by `smoker` status (Yes or No). 5. The points should have their sizes proportional to the `size` column. 6. Customize the marker area range to go from 20 to 200. 7. Ensure that all subplots use synchronized semantic mappings for color and style. Constraints: - The function should handle datasets similar to the provided `tips` dataset. - Assume the columns `total_bill`, `tip`, `day`, `sex`, `time`, `smoker`, and `size` are present and correctly formatted in the input DataFrame. Function Signature: ```python import pandas as pd def create_faceted_scatter_plot(data: pd.DataFrame): pass ``` Example Output: The function `create_faceted_scatter_plot(tips)` should produce a grid of scatter plots with the required specifications. ```python # Sample implementation import seaborn as sns import matplotlib.pyplot as plt def create_faceted_scatter_plot(data: pd.DataFrame): sns.set_theme() g = sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", row=\\"time\\", col=\\"smoker\\", hue=\\"day\\", style=\\"sex\\", size=\\"size\\", sizes=(20, 200), kind=\\"scatter\\" ) plt.show() # Assuming `tips` dataset is already loaded # create_faceted_scatter_plot(tips) ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_faceted_scatter_plot(data: pd.DataFrame): Create a multi-faceted scatter plot from the given DataFrame according to the specified features: - x-axis: total_bill - y-axis: tip - hue: day - style: sex - row facets: time (Lunch or Dinner) - column facets: smoker (Yes or No) - size: size - marker sizes range from 20 to 200 sns.set_theme() g = sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", row=\\"time\\", col=\\"smoker\\", hue=\\"day\\", style=\\"sex\\", size=\\"size\\", sizes=(20, 200), kind=\\"scatter\\" ) plt.show()"},{"question":"**Objective**: Implement a Python program that uses the `pty.spawn()` function to run a given shell command and log its output along with a timestamp. Additionally, implement custom I/O functions to transform the output before logging it. Requirements: 1. **Function**: `log_command_output(command: str, logfile: str) -> None` - **Input**: - `command` (str): The shell command to run. - `logfile` (str): The file path where the log should be written. - **Output**: None - **Behavior**: - The function should run the given command in a pseudo-terminal. - All the output of the command should be logged into the specified file. - Each output line should be prefixed with a timestamp in the format `[YYYY-MM-DD HH:MM:SS]`. - The function should handle EOF properly and ensure that the process terminates correctly. 2. **Custom I/O Functions**: - Implement custom `master_read(fd)` and `stdin_read(fd)` functions for `pty.spawn`. - `master_read(fd)` should read the data, prefix it with the current timestamp, and log it. - `stdin_read(fd)` should simply read from the standard input (use the default behavior). Constraints: - The solution should be compatible with Linux OS. - Ensure proper handling of EOF to avoid infinite loops. - Handle any potential errors gracefully and log them as well. Example Usage: ```python log_command_output(\'ls -l\', \'output.log\') ``` This would run the `ls -l` command and log its output with timestamps into `output.log`. Example Output in `output.log`: ``` [2023-10-24 12:34:56] total 8 [2023-10-24 12:34:56] -rw-r--r-- 1 user group 0 Oct 24 12:34 file1 [2023-10-24 12:34:56] -rw-r--r-- 1 user group 0 Oct 24 12:34 file2 ``` Additional Information: You may refer to the `pty` module documentation provided to understand the usage of the functions `pty.spawn()` and how to customize the I/O operations. **Note**: Ensure that your solution is robust and handles any exceptions or errors that may occur during the command execution.","solution":"import pty import os import datetime def log_command_output(command: str, logfile: str) -> None: def master_read(fd: int) -> bytes: output = os.read(fd, 1024) if output: timestamp = datetime.datetime.now().strftime(\\"[%Y-%m-%d %H:%M:%S]\\") log_entry = f\\"{timestamp} {output.decode()}\\" with open(logfile, \'a\') as f: f.write(log_entry) return output def stdin_read(fd: int) -> bytes: return os.read(fd, 1024) try: pty.spawn(command.split(), master_read=master_read, stdin_read=stdin_read) except Exception as e: with open(logfile, \'a\') as f: f.write(f\\"Error: {str(e)}n\\")"},{"question":"You are tasked with developing a Python-based tool that facilitates browsing a list of URLs. Using the `webbrowser` module, you need to implement a function `browse_urls` that takes a list of URLs and an optional parameter specifying the opening mode. # Function Signature ```python def browse_urls(urls: list[str], mode: str = \\"same-window\\") -> bool: pass ``` # Input 1. **urls**: a list of strings, where each string is a valid URL. 2. **mode**: a string that specifies how the URLs should be opened. It can have one of the following values: - `\\"same-window\\"`: Open each URL in the same window one after the other. - `\\"new-window\\"`: Open each URL in a new browser window. - `\\"new-tab\\"`: Open each URL in a new tab of an existing window. # Output - Returns `True` if all URLs were opened successfully. - Raises `webbrowser.Error` if any error occurs while opening the URLs. # Constraints - All URLs in the list are guaranteed to be valid. - The system running the code will have a default web browser available. - The function should handle an empty list of URLs gracefully, simply returning `True`. # Example 1. **Example 1:** ```python result = browse_urls([ \\"https://www.google.com\\", \\"https://www.python.org\\", \\"https://www.github.com\\" ], mode=\\"new-tab\\") print(result) # Output: True ``` 2. **Example 2:** ```python result = browse_urls([ \\"https://www.example.com\\" ], mode=\\"same-window\\") print(result) # Output: True ``` # Notes 1. Utilize the functions `webbrowser.open`, `webbrowser.open_new`, and `webbrowser.open_new_tab` as appropriate based on the mode parameter. 2. Handle the `webbrowser.Error` exception and re-raise it if any URL fails to open. 3. If the `mode` parameter is not provided, default to opening URLs in the same window (`\\"same-window\\"`).","solution":"import webbrowser from typing import List def browse_urls(urls: List[str], mode: str = \\"same-window\\") -> bool: Open a list of URLs using the default web browser. :param urls: A list of URLs to open. :param mode: How to open the URLs, can be \\"same-window\\", \\"new-window\\", or \\"new-tab\\". :return: True if all URLs were opened successfully. :raises webbrowser.Error: If any error occurs while opening the URLs. try: for url in urls: if mode == \\"same-window\\": webbrowser.open(url) elif mode == \\"new-window\\": webbrowser.open_new(url) elif mode == \\"new-tab\\": webbrowser.open_new_tab(url) else: raise ValueError(\\"Invalid mode specified\\") return True except webbrowser.Error as e: raise e"},{"question":"**Objective**: To evaluate your understanding and ability to use the `xml.parsers.expat` module to handle XML data parsing and implement handler functions effectively. **Problem Statement**: You are given an XML document containing information about books. Your task is to write a Python function `parse_books_xml(xml_string: str) -> list` that parses the given XML string using the `xml.parsers.expat` module and extracts the information about each book in a structured format. The function should: 1. Parse the XML string. 2. Implement handler functions to process the start and end of elements, as well as character data. 3. Collect the book information and return it as a list of dictionaries. Each book in the XML contains the following information: - `title` (string) - `author` (string) - `year` (integer) - `price` (float) **Input Format**: - A single string `xml_string` representing the XML document. **Output Format**: - A list of dictionaries, where each dictionary represents a book with keys `\'title\'`, `\'author\'`, `\'year\'`, and `\'price\'`. **Constraints**: - You can assume all XML elements are well-formed. - The `xml_string` will contain multiple books. **Example**: ```xml <bookstore> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> <price>29.99</price> </book> <book> <title>Learning XML</title> <author>Jane Smith</author> <year>2018</year> <price>39.95</price> </book> </bookstore> ``` For the above XML string, your function should return: ```python [ { \\"title\\": \\"Python Programming\\", \\"author\\": \\"John Doe\\", \\"year\\": 2020, \\"price\\": 29.99 }, { \\"title\\": \\"Learning XML\\", \\"author\\": \\"Jane Smith\\", \\"year\\": 2018, \\"price\\": 39.95 } ] ``` **Implementation Requirements**: 1. Use the `xml.parsers.expat.ParserCreate()` to create an XML parser. 2. Implement the necessary handler functions (`StartElementHandler`, `EndElementHandler`, `CharacterDataHandler`) to collect book data. 3. Ensure the function is efficient and handles the provided XML format correctly. **Function Signature**: ```python def parse_books_xml(xml_string: str) -> list: pass ``` **Note**: You may refer to the provided documentation to understand the usage and implementation of the `xml.parsers.expat` module.","solution":"import xml.parsers.expat def parse_books_xml(xml_string: str) -> list: books = [] current_book = {} current_element = \\"\\" def start_element(name, attrs): nonlocal current_element current_element = name if name == \'book\': current_book.clear() def end_element(name): nonlocal current_element if name == \'book\': books.append(current_book.copy()) current_element = \\"\\" def char_data(data): if current_element == \'title\': current_book[\'title\'] = data.strip() elif current_element == \'author\': current_book[\'author\'] = data.strip() elif current_element == \'year\': current_book[\'year\'] = int(data.strip()) elif current_element == \'price\': current_book[\'price\'] = float(data.strip()) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string) return books"},{"question":"<|Analysis Begin|> The sqlite3 module in Python provides an interface to work with SQLite databases, allowing users to make SQL queries and transactions using Python code. It is compliant with the DB-API 2.0 specification and enables various database operations such as creating tables, inserting data, querying data, and managing transactions. Key functionalities include: 1. Establishing and managing database connections (`sqlite3.connect`). 2. Creating cursors to execute SQL statements (`con.cursor`). 3. Executing SQL commands (`cur.execute`, `cur.executemany`, `cur.executescript`). 4. Managing transactions (`con.commit`, `con.rollback`). 5. Adapting and converting custom Python types for SQLite (`sqlite3.register_adapter`, `sqlite3.register_converter`). 6. Factory methods for customizing row representations (`row_factory`). 7. Using context managers for automatic transaction management (`with con:`). The documentation details how to handle specific tasks such as safely using placeholders to prevent SQL injection, creating and using custom SQL functions and aggregates, and working with adapters and converters to handle custom types. <|Analysis End|> <|Question Begin|> # Advanced SQLite Operations with Python `sqlite3` Context: You are part of a team developing a movie recommendation system. One of the key components of this system is a database that stores movie details and user ratings. You need to create a robust solution to manage this data using SQLite databases in Python. Your task is to implement functions to initialize the database, insert movie data, retrieve movie details with custom columns, and handle transactional operations. This will ensure efficient management and querying of the database contents. Tasks: 1. **Initialize Database**: - Write a function `initialize_db(db_name: str) -> sqlite3.Connection` that connects to an SQLite database (creating it if it doesn\'t exist) and initializes it with the required tables. The database should have two tables: `movies` with columns (id INTEGER PRIMARY KEY, title TEXT, year INTEGER, score REAL) and `users` with columns (id INTEGER PRIMARY KEY, name TEXT). - Example call: `initialize_db(\'movies.db\')` 2. **Add Movie**: - Write a function `add_movie(con: sqlite3.Connection, title: str, year: int, score: float) -> None` that insert a new movie record into the `movies` table. - Example call: `add_movie(con, \'Inception\', 2010, 8.8)` 3. **Retrieve Movies Ordered by Custom Column**: - Write a function `get_movies_ordered(con: sqlite3.Connection, order_by: str) -> list` that retrieves all movies from the `movies` table ordered by the specified column. The function should support ordering by \'title\', \'year\', or \'score\'. - Example call: `get_movies_ordered(con, \'score\') -> [(\'Inception\', 2010, 8.8), (...)]` 4. **Transaction Handling**: - Write a function `rate_movie(con: sqlite3.Connection, user_id: int, movie_id: int, rating: float) -> None` that adds a rating for a movie by a specific user. This function should ensure that the rating is inserted within a transaction. If any step fails, the transaction should be rolled back to avoid partial updates. - Example call: `rate_movie(con, 1, 101, 5.0)` Constraints: - Ensure that all SQL queries use placeholders to prevent SQL injection. - Implement custom row factories to return each row as a dictionary. - Ensure proper transaction handling, commit on success, and roll back on failure. # Function Signatures: ```python import sqlite3 def initialize_db(db_name: str) -> sqlite3.Connection: pass def add_movie(con: sqlite3.Connection, title: str, year: int, score: float) -> None: pass def get_movies_ordered(con: sqlite3.Connection, order_by: str) -> list: pass def rate_movie(con: sqlite3.Connection, user_id: int, movie_id: int, rating: float) -> None: pass ``` # Example Usage: ```python con = initialize_db(\'movies.db\') add_movie(con, \'Inception\', 2010, 8.8) add_movie(con, \'The Matrix\', 1999, 8.7) print(get_movies_ordered(con, \'score\')) rate_movie(con, 1, 2, 5.0) con.close() ``` Notes: - Be sure to handle any database exceptions and ensure the database connection is properly closed after operations. - You may define additional helper functions if required.","solution":"import sqlite3 def initialize_db(db_name: str) -> sqlite3.Connection: con = sqlite3.connect(db_name) with con: cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS movies ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, year INTEGER NOT NULL, score REAL NOT NULL ) \'\'\') cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS users ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL ) \'\'\') cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS ratings ( user_id INTEGER, movie_id INTEGER, rating REAL, PRIMARY KEY (user_id, movie_id), FOREIGN KEY (user_id) REFERENCES users(id), FOREIGN KEY (movie_id) REFERENCES movies(id) ) \'\'\') return con def add_movie(con: sqlite3.Connection, title: str, year: int, score: float) -> None: with con: cur = con.cursor() cur.execute(\'\'\' INSERT INTO movies (title, year, score) VALUES (?, ?, ?) \'\'\', (title, year, score)) def get_movies_ordered(con: sqlite3.Connection, order_by: str) -> list: if order_by not in [\'title\', \'year\', \'score\']: raise ValueError(\\"Invalid order_by value, must be one of \'title\', \'year\', \'score\'\\") con.row_factory = sqlite3.Row cur = con.cursor() cur.execute(f\'SELECT title, year, score FROM movies ORDER BY {order_by}\') rows = cur.fetchall() return [dict(row) for row in rows] def rate_movie(con: sqlite3.Connection, user_id: int, movie_id: int, rating: float) -> None: try: with con: cur = con.cursor() cur.execute(\'\'\' INSERT INTO ratings (user_id, movie_id, rating) VALUES (?, ?, ?) \'\'\', (user_id, movie_id, rating)) except sqlite3.Error as e: con.rollback() raise e"},{"question":"# Custom Interactive Interpreter You are tasked with developing a custom interactive Python interpreter using the `code` and `codeop` modules. This custom interpreter should support the following additional features: 1. **Recording History**: It should keep a history of all commands executed. 2. **Custom Commands**: Implement two custom commands: - `!history`: Prints out the command history. - `!exit`: Exits the interpreter gracefully. # Input Format 1. The custom interpreter should accept standard Python code as input. 2. The custom commands `!history` and `!exit`. # Output Format 1. For standard Python code, it should behave as the default Python interpreter. 2. For the custom commands: - `!history`: Outputs the history of executed commands, one per line. - `!exit`: Gracefully exits the interpreter. # Constraints 1. The solution should use the `code` and `codeop` modules. 2. The interpreter should handle multiline Python commands correctly. # Example Session ```python >>> 2 + 2 4 >>> def foo(): ... return \\"bar\\" ... >>> foo() \'bar\' >>> !history 2 + 2 def foo(): return \\"bar\\" foo() !history >>> !exit ``` # Your Task Implement the custom interactive interpreter as specified.","solution":"import code import sys class CustomInteractiveInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() self.history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): self.history.append(source) if source == \\"!history\\": for command in self.history: print(command.strip()) return False elif source == \\"!exit\\": print(\\"Exiting interpreter...\\") sys.exit(0) else: return super().runsource(source, filename, symbol) def run_custom_interpreter(): interpreter = CustomInteractiveInterpreter() interpreter.interact() if __name__ == \\"__main__\\": run_custom_interpreter()"},{"question":"Objective: Write a Python program to simulate a simple server that can be gracefully terminated using a custom signal handler. The task aims to test your understanding of the `signal` module, how to set and handle custom signal handlers, and manage asynchronous events in a Python program. Problem Statement: Create a Python program that performs the following: 1. Uses the `signal` module to set a custom handler for the `SIGINT` signal (sent when \\"Ctrl+C\\" is pressed). 2. The custom handler should: - Print a message indicating that the signal was received. - Gracefully shut down the server by stopping its main loop. 3. Simulate a server that prints \\"Server is running...\\" every second. 4. Ensure the server can run indefinitely until interrupted by \\"Ctrl+C\\". Input and Output: - **Input**: No input from the user. The program runs indefinitely until interrupted. - **Output**: The message \\"Signal handler called with signal SIGINT\\" and \\"Server shutting down...\\" upon receiving the `SIGINT` signal. Constraints: - `signal.signal` should only be called from the main thread. - Handle potential exceptions that may arise within the signal handler. - The program should handle the signal, ensuring there is no data loss or unexpected termination of processes. Example Usage: ``` python server.py Server is running... Server is running... Server is running... Signal handler called with signal SIGINT Server shutting down... ``` Guidelines: - You can use the `time.sleep` function to simulate the delay within the server loop. - Use the `threading` module if necessary, but ensure signal handlers are only set in the main thread. Hints: - Utilize the `signal.signal` function to set the custom signal handler. - The handler function should take two arguments: `signum` and `frame`. - Use a flag or a similar mechanism to control the server loop from within the signal handler. Write your solution below: ```python import signal import time # Define the signal handler function def signal_handler(signum, frame): print(f\\"Signal handler called with signal {signum}\\") global running running = False # Main Function if __name__ == \\"__main__\\": # Set the signal handler for SIGINT signal.signal(signal.SIGINT, signal_handler) running = True print(\\"Server is running... Press Ctrl+C to stop.\\") # Simulate the server running indefinitely try: while running: print(\\"Server is running...\\") time.sleep(1) except Exception as e: print(f\\"An error occurred: {e}\\") print(\\"Server shutting down...\\") ```","solution":"import signal import time # Define the signal handler function def signal_handler(signum, frame): print(f\\"Signal handler called with signal {signum}\\") global running running = False # Main Function if __name__ == \\"__main__\\": # Set the signal handler for SIGINT signal.signal(signal.SIGINT, signal_handler) running = True print(\\"Server is running... Press Ctrl+C to stop.\\") # Simulate the server running indefinitely try: while running: print(\\"Server is running...\\") time.sleep(1) except Exception as e: print(f\\"An error occurred: {e}\\") print(\\"Server shutting down...\\")"},{"question":"**Coding Assessment Question: PyTorch Meta Device** You are given a PyTorch neural network model saved in a file named `model.pt`. This model is too large to load directly into memory due to constraints on your current computational setup. However, you need to perform the following tasks: 1. Load the model onto the meta device to inspect its structure without loading the data. 2. Print the structure of the model. 3. Transfer the model to the CPU but leave all parameters uninitialized. 4. Initialize the parameters of each layer using a custom initialization strategy: set each weight to a random value sampled from a normal distribution with mean 0 and standard deviation 0.01, and set each bias to zero. Requirements: - Define a function `initialize_parameters` that takes a module as input and applies the above initialization strategy. - Print the summary of the model after moving it to the CPU with uninitialized parameters. - Apply the custom initialization strategy and then print the summary of the model with initialized parameters. # Function Signature ```python def initialize_parameters(module): # Your code here if __name__ == \\"__main__\\": # Load the model onto meta device model = torch.load(\'model.pt\', map_location=\'meta\') # Print the structure of the model print(model) # Transfer the model to CPU without initializing the parameters model.to_empty(device=\'cpu\') # Print the summary of the model after moving it to CPU print(model) # Apply custom initialization model.apply(initialize_parameters) # Print the summary of the model after initialization print(model) ``` # Constraints: - The model may contain various types of layers (e.g., Linear, Conv2d, etc.), and the initialization should work for any layers that have `weight` and `bias` attributes. - The file `model.pt` will be available in the current directory. # Input: None # Output: Print statements showing the structure and summaries of the model at various steps as described in the requirements. # Example: ```python Linear(in_features=10, out_features=5, bias=True) <UNINITIALIZED MODEL STRUCTURE> <MODEL STRUCTURE AFTER CUSTOM INITIALIZATION> ``` **Note:** The provided example output is an illustration, the actual output will depend on the model structure in `model.pt`.","solution":"import torch import torch.nn as nn def initialize_parameters(module): Custom initialization: set weights to random values from N(0, 0.01) and biases to zero. if isinstance(module, (nn.Linear, nn.Conv2d)): torch.nn.init.normal_(module.weight, mean=0.0, std=0.01) if module.bias is not None: torch.nn.init.zeros_(module.bias) if __name__ == \\"__main__\\": # Load the model onto the meta device model = torch.load(\'model.pt\', map_location=\'meta\') # Print the structure of the model (uninitialized) print(model) # Transfer the model to CPU without initializing the parameters (empty tensor state) model.to_empty(device=\'cpu\') # Print the summary of the model after moving it to CPU, which will still be uninitialized print(model) # Apply custom initialization strategy model.apply(initialize_parameters) # Print the summary of the model after initialization print(model)"},{"question":"# Sound File Type Determination and Handling Unsupported Types You are tasked to write a Python function that determines the type of sound file provided. Your function should leverage the deprecated `sndhdr` module to achieve this. However, given that the module may not support newer sound file types and is deprecated, you are also required to handle cases where the module fails to determine the sound file type by implementing a fallback mechanism. Input - A string, `filename`, which is the name of the sound file. The file can have any typical sound file extension such as `.wav`, `.aiff`, `.au`, etc. Output - If the sound file type is successfully determined by the `sndhdr` module, return a string in the format: `\\"Filetype: {filetype}, Framerate: {framerate}, Channels: {nchannels}, Frames: {nframes}, Sample Width: {sampwidth}\\"`. - If the `sndhdr` module fails to determine the sound file type, return the string: `\\"Unsupported file type or unable to determine file type.\\"`. Restrictions and Requirements 1. **Usage of `sndhdr.what` or `sndhdr.whathdr` is mandatory** to determine the sound file type. 2. **Handle the deprecation warnings** appropriate for using this deprecated module. 3. Implement a **fallback mechanism** to gracefully handle cases where the `sndhdr` module fails (i.e., it returns `None`). Example ```python def determine_sound_file_type(filename: str) -> str: # Your implementation here # Example usage: print(determine_sound_file_type(\\"example.wav\\")) # Possible output: # \\"Filetype: wav, Framerate: 44100, Channels: 2, Frames: 10000, Sample Width: 16\\" # or # \\"Unsupported file type or unable to determine file type.\\" ``` Notes: - Ensure your function is robust and handles edge cases gracefully. - You can assume the file exists in the given path and is a sound file, but not necessarily one supported by `sndhdr`.","solution":"import sndhdr def determine_sound_file_type(filename: str) -> str: Determines the type of a sound file using the sndhdr module. Arguments: filename -- the name of the sound file Returns: Information about the sound file or an error message if unsupported try: result = sndhdr.what(filename) if result is not None: filetype, framerate, nchannels, nframes, sampwidth = result return (f\\"Filetype: {filetype}, Framerate: {framerate}, \\" f\\"Channels: {nchannels}, Frames: {nframes}, Sample Width: {sampwidth}\\") else: return \\"Unsupported file type or unable to determine file type.\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"Function Composability and Memoization You need to implement a function that efficiently calculates the n-th term in a custom series defined by a certain recursive relation. Use `functools` to manage caching and function composition. # Specifications: 1. **Function Composition and Caching:** You need to implement a series that is a combination of: - A base computation (`base_computation`) which performs a specific mathematical operation. - A transformation that is applied via a composed function. 2. **Base Computation:** Define a base `compute_term(n)` function that calculates terms using: - The recursive relation: `T(n) = T(n-1) + 2*T(n-2)` for n > 1. - Base cases: `T(0) = 0` and `T(1) = 1`. 3. **Transformation:** Implement a function `apply_transformation(f)` that: - Takes a function `f` and returns a new function. - The new function applies an additional transformation that doubles the result of the base computation. 4. **Memoization:** Use `functools.lru_cache` to cache results of `compute_term(n)` to ensure the implementation is efficient for large values of `n`. 5. **Main Function:** Implement the main function `transformed_series(n)` which: - Composes the base computation with the transformation. - Returns the n-th term of the transformed series. Constraints: - `n` is a non-negative integer. - You should cache at least the results of the base computation. # Example: ```python from functools import lru_cache, wraps # Define the base computation function @lru_cache(maxsize=None) def compute_term(n): if n == 0: return 0 elif n == 1: return 1 return compute_term(n-1) + 2 * compute_term(n-2) # Define the transformation function def apply_transformation(f): @wraps(f) def transformed(n): return 2 * f(n) return transformed # Define the main function def transformed_series(n): base_computation = compute_term transformed_computation = apply_transformation(base_computation) return transformed_computation(n) # Example usage print(transformed_series(5)) # Should print the transformed 5th term of the series ``` # Additional Notes: - Ensure that your function names and structure match exactly as defined. - The use of `functools.lru_cache` and `functools.wraps` is mandatory. - Test your implementation with various values of `n` to ensure correctness and efficiency.","solution":"from functools import lru_cache, wraps # Define the base computation function @lru_cache(maxsize=None) def compute_term(n): if n == 0: return 0 elif n == 1: return 1 return compute_term(n-1) + 2 * compute_term(n-2) # Define the transformation function def apply_transformation(f): @wraps(f) def transformed(n): return 2 * f(n) return transformed # Define the main function def transformed_series(n): base_computation = compute_term transformed_computation = apply_transformation(base_computation) return transformed_computation(n)"},{"question":"Objective The task is to implement a custom PyTorch `Function` and a corresponding custom `Module`. You will create a new operation that computes the element-wise square root of the input tensor and its gradient in the backward pass. Requirements 1. **Custom Function**: Implement a custom `Function` subclass called `SqrtFunction`. This `Function` should: - Define a `forward` method that computes the element-wise square root of the input tensor. - Appropriately set up the context for the backward pass. - Implement a `backward` method that computes the gradient of the loss with respect to the input, ensuring the use of saved tensors as needed. 2. **Custom Module**: Implement a custom `Module` class called `Sqrt` which uses `SqrtFunction` in its `forward` method. 3. **Validation**: Write a small validation script that: - Creates an instance of your `Sqrt` module. - Tests it on a sample tensor input to ensure the forward computation is correct. - Uses `torch.autograd.gradcheck` to validate that the backward computation is correct. Specification 1. **SqrtFunction Structure** ```python import torch from torch.autograd import Function class SqrtFunction(Function): @staticmethod def forward(ctx, input): # Compute the element-wise square root of the input tensor sqrt_input = torch.sqrt(input) # Save the input for backward computation ctx.save_for_backward(input) return sqrt_input @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute gradient of the loss with respect to the input grad_input = grad_output / (2 * torch.sqrt(input)) return grad_input ``` 2. **Sqrt Module Structure** ```python import torch.nn as nn class Sqrt(nn.Module): def forward(self, input): return SqrtFunction.apply(input) ``` 3. **Validation Script** ```python import torch # Initialize the custom module sqrt_module = Sqrt() # Sample input tensor with requires_grad=True input_tensor = torch.randn(5, 5, dtype=torch.double, requires_grad=True) # Test forward computation output = sqrt_module(input_tensor) print(\\"Forward output:\\", output) # Validate backward computation using gradcheck test = torch.autograd.gradcheck(sqrt_module, (input_tensor,), eps=1e-6, atol=1e-4) print(\\"Gradcheck:\\", test) ``` Constraints - Input tensors will always have non-negative values, ensuring real-valued square roots. - Handle the edge case where the input is zero appropriately without causing division by zero errors. Notes - Follow the structure provided for `SqrtFunction` and `Sqrt` as closely as possible. - Ensure your implementation is efficient and adheres to PyTorch\'s conventions for custom operations. - Your solution will be evaluated based on correctness, efficiency, and adherence to the guidelines provided.","solution":"import torch from torch.autograd import Function import torch.nn as nn class SqrtFunction(Function): @staticmethod def forward(ctx, input): # Compute the element-wise square root of the input tensor sqrt_input = torch.sqrt(input) # Save the input for backward computation ctx.save_for_backward(input) return sqrt_input @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute gradient of the loss with respect to the input grad_input = grad_output / (2 * torch.sqrt(input) + 1e-12) # Adding a small constant to avoid division by zero return grad_input class Sqrt(nn.Module): def forward(self, input): return SqrtFunction.apply(input)"},{"question":"**File Locking Mechanism Implementation** # Problem Statement You are tasked with implementing a file locking mechanism to ensure data consistency when multiple processes attempt to write to the same file concurrently. Utilize the `fcntl.lockf()` function from the `fcntl` module to achieve this. # Requirements 1. Implement a function `write_to_file_with_lock(file_path: str, data: str, start: int = 0) -> None` that writes a given string `data` to the file located at `file_path`, starting at the byte offset `start`. 2. Ensure that the write operation is protected by acquiring an exclusive lock on the file before writing and releasing the lock after the write operation is complete. 3. Handle any exceptions that may occur during the file operations, ensuring that the lock is always released in case of errors. 4. The file should be opened or created in append mode if it does not exist. # Function Signature ```python from typing import Optional def write_to_file_with_lock(file_path: str, data: str, start: int = 0) -> None: pass ``` # Input - `file_path` (str): The path to the file to which data needs to be written. - `data` (str): The string data to write to the file. - `start` (int): The byte offset at which to start writing. Default is `0`. # Output - The function does not return any value. It performs the write operation to the specified file. # Constraints - The function should handle file paths and data strings of reasonable lengths typical for file operations. - Ensure proper exception handling and resource cleanup to prevent deadlocks or resource leaks. # Example Usage ```python file_path = \\"example.txt\\" data = \\"Hello, World!\\" # Writes \\"Hello, World!\\" to \'example.txt\' starting at byte 0 with an exclusive lock write_to_file_with_lock(file_path, data) ``` # Hints - Use `open()` to handle file opening and pass the file descriptor to `fcntl.lockf()` for locking. - Remember to seek to the appropriate start position if needed before writing. - Use a `try-except-finally` block to ensure locks are released appropriately even if an error occurs during the file operations.","solution":"import fcntl def write_to_file_with_lock(file_path: str, data: str, start: int = 0) -> None: Writes the given data to the file at file_path with an exclusive lock. Args: - file_path (str): The path to the file to which data needs to be written. - data (str): The string data to write to the file. - start (int, optional): The byte offset at which to start writing. Default is 0. Returns: - None file = None try: # Open the file in append binary mode file = open(file_path, \'a+b\') # Acquire an exclusive lock on the file fcntl.lockf(file, fcntl.LOCK_EX) # Move the file\'s current position to the specified start offset file.seek(start) # Write the data to the file file.write(data.encode()) # Flush to ensure data is written to the disk file.flush() except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Ensure the lock is released and the file is closed if file is not None: fcntl.lockf(file, fcntl.LOCK_UN) file.close()"},{"question":"Using the Seaborn library, you are required to create a multi-faceted visualization of a provided dataset involving several types of categorical plots. This will demonstrate your understanding of Seaborn\'s capabilities for visualizing categorical data, combining different plots, and customizing the appearance. Dataset You will use the `tips` dataset that is built into Seaborn. Requirements 1. **Read and Initialize**: Load the `tips` dataset and initialize Seaborn with a specific theme. 2. **Create a Faceted Categorical Plot**: - Use `catplot` to create a facet grid with columns based on the `time` variable. - Within each facet, show a combination of a `swarmplot` and `violinplot` displaying the `total_bill` values distributed across the days of the week (`day`). - Use hue to differentiate between smokers and non-smokers. 3. **Customize the Plot**: - Ensure the points in the swarm plot are black (`color=\\"k\\"`) and of size 2. - The inner distribution of the violin plot should show split based on smoker status and be adjusted for bandwidth with `bw_adjust=0.7`. - Set an appropriate palette for the hue differentiation. 4. **Additional Customizations**: - Add meaningful axis labels. - Adjust the title for each subplot to specify the meal time (Lunch or Dinner). Example Visualization: Your final visualization should look something like faceted plots with swarm and violin plots combined, differentiated with colors based on the smoker status, and split using hue. Expected Solution Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load dataset and initialize seaborn theme tips = sns.load_dataset(\\"tips\\") sns.set_theme(style=\\"whitegrid\\") # Step 2: Create the faceted categorical plot g = sns.catplot( data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"violin\\", split=True, bw_adjust=0.7, hue=\\"smoker\\", col=\\"time\\", inner=None, palette=\\"muted\\" ) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", color=\\"k\\", size=2, dodge=True, ax=g.axes[0,0]) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", color=\\"k\\", size=2, dodge=True, ax=g.axes[0,1]) # Step 3: Customize axis labels and titles g.set_axis_labels(\\"Day of the Week\\", \\"Total Bill ()\\") g.set_titles(\\"{col_name}\\") # Display the plot plt.show() ``` Input / Output - **Input**: No separate input required other than the code and required imports. - **Output**: A faceted plot displayed using matplotlib. Your solution will be evaluated based on the correctness, clarity, and appropriateness of visualizations, as well as the use of Seaborn functionalities explained in the provided documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_faceted_categorical_plot(): # Step 1: Load dataset and initialize seaborn theme tips = sns.load_dataset(\\"tips\\") sns.set_theme(style=\\"whitegrid\\") # Step 2: Create the faceted categorical plot g = sns.catplot( data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"violin\\", split=True, bw_adjust=0.7, hue=\\"smoker\\", col=\\"time\\", inner=None, palette=\\"muted\\", height=6, aspect=1 ) # Add swarmplot to facet grid for ax in g.axes.flat: sns.swarmplot( data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", color=\\"k\\", size=2, dodge=True, ax=ax ) # Step 3: Customize axis labels and titles g.set_axis_labels(\\"Day of the Week\\", \\"Total Bill ()\\") g.set_titles(\\"{col_name}\\") # Adjust the legend g.despine(left=True) plt.legend(loc=\'best\') # Display the plot plt.show()"},{"question":"# URL Handling and HTTP Client Implementation Using `urllib` Objective You are tasked with implementing a function that handles sending HTTP requests to a specified URL. The function should be able to: 1. Handle different HTTP methods like \'GET\', \'POST\', and \'PUT\'. 2. Manage HTTP redirections automatically. 3. Handle cookies appropriately. 4. Support basic authentication if credentials are provided. Function Signature ```python def handle_request(url: str, method: str = \'GET\', data: dict = None, headers: dict = None, cookies: dict = None, auth: tuple = None) -> dict: pass ``` Input - `url` (str): The endpoint URL to which the HTTP request will be sent. - `method` (str, optional): The HTTP method to use for the request. Default is \'GET\'. - `data` (dict, optional): The data to send with the request, typically for \'POST\' and \'PUT\' methods. Default is None. - `headers` (dict, optional): A dictionary of headers to include in the request. Default is None. - `cookies` (dict, optional): A dictionary of cookies to include in the request. Default is None. - `auth` (tuple, optional): A tuple containing the username and password for basic authentication. Default is None. Output - Returns a dictionary with the following keys: - `status_code` (int): The HTTP status code of the response. - `headers` (dict): A dictionary of response headers. - `cookies` (dict): A dictionary of cookies included in the response. - `content` (str): The content of the response. Requirements and Constraints 1. Use the `urllib` module\'s `request` and related objects to implement the HTTP client. 2. Implement automatic redirection handling. 3. Handle the transmission and storing of cookies between requests. 4. Support basic HTTP authentication if `auth` is provided. Example Usage ```python response = handle_request(\\"http://example.com/api/data\\", method=\\"POST\\", data={\\"key\\": \\"value\\"}, headers={\\"User-Agent\\": \\"test-client\\"}, cookies={\\"session_id\\": \\"abc123\\"}, auth=(\\"user\\", \\"pass\\")) print(response[\'status_code\']) # Example: 200 print(response[\'headers\']) # Example: {\'Content-Type\': \'application/json\', \'Set-Cookie\': \'session_id=abc123\'} print(response[\'cookies\']) # Example: {\'session_id\': \'abc123\'} print(response[\'content\']) # Example: \'{\\"success\\": true, \\"data\\": {...}}\' ``` Implementing this function requires a good understanding of the `urllib` package, including handling various request types, dealing with headers and cookies, and managing authentication.","solution":"import urllib.request import urllib.parse import urllib.error import urllib.request from http.cookiejar import CookieJar import base64 def handle_request(url: str, method: str = \'GET\', data: dict = None, headers: dict = None, cookies: dict = None, auth: tuple = None) -> dict: method = method.upper() headers = headers or {} cookie_jar = CookieJar() if cookies: cookie_string = \\"; \\".join([f\\"{key}={value}\\" for key, value in cookies.items()]) headers[\'Cookie\'] = cookie_string if auth: auth_str = f\\"{auth[0]}:{auth[1]}\\" b64_auth_str = base64.b64encode(auth_str.encode()).decode() headers[\'Authorization\'] = f\\"Basic {b64_auth_str}\\" if data: data = urllib.parse.urlencode(data).encode() req = urllib.request.Request(url, headers=headers, method=method) if method == \'POST\' or method == \'PUT\': req.data = data handler = urllib.request.HTTPCookieProcessor(cookie_jar) opener = urllib.request.build_opener(handler) try: response = opener.open(req) content = response.read().decode() cookies = {cookie.name: cookie.value for cookie in cookie_jar} return { \'status_code\': response.getcode(), \'headers\': dict(response.getheaders()), \'cookies\': cookies, \'content\': content } except urllib.error.HTTPError as e: return { \'status_code\': e.code, \'headers\': dict(e.headers), \'cookies\': {}, \'content\': e.read().decode() } except urllib.error.URLError as e: return { \'status_code\': None, \'headers\': {}, \'cookies\': {}, \'content\': str(e.reason) }"},{"question":"# Objective: Analyze and Visualize Financial Data You are given a CSV file named `financial_data.csv` containing financial data of various companies. The columns in the dataset are as follows: - `Company`: Name of the company. - `Year`: Financial year. - `Revenue`: Total revenue of the company for the given year. - `Expenses`: Total expenses of the company for the given year. - `Profit`: Total profit of the company for the given year. - `Assets`: Total assets value of the company for the given year. - `Liabilities`: Total liabilities value of the company for the given year. Your task is to implement a function `analyze_financial_data(file_path: str) -> None` that performs the following steps: 1. **Load the Data:** - Read the data from the CSV file into a pandas DataFrame. 2. **Data Cleaning and Processing:** - Check for missing values and print the number of missing values for each column. - Drop any rows with missing values. 3. **Data Analysis:** - Calculate the `Debt-to-Equity Ratio` for each company for each year and add it as a new column in the DataFrame. - Debt-to-Equity Ratio = Liabilities / Assets 4. **Data Visualization:** - Generate and save the following plots: 1. **Boxplot**: Show the distribution of the `Profit` column for each company. 2. **Scatter Plot Matrix**: Plot a scatter plot matrix for the columns `Revenue`, `Expenses`, `Profit`, `Assets`, and `Liabilities`. 3. **Autocorrelation Plot**: Plot the autocorrelation of the `Profit` column for any single company of your choice. 4. **Parallel Coordinates**: Create a parallel coordinates plot using the columns `Revenue`, `Expenses`, `Profit`, `Assets`, `Liabilities`, and the new `Debt-to-Equity Ratio` column to visualize the data for each company. Constraints: - You must use the functions from the `pandas.plotting` module for the visualizations. Example Usage: ```python file_path = \\"path/to/financial_data.csv\\" analyze_financial_data(file_path) ``` Notes: - Ensure to save each plot with a relevant filename, e.g., \\"boxplot.png\\", \\"scatter_matrix.png\\", etc. - Write comments in your code to explain the steps you are performing. - Use appropriate pandas functions for data manipulation and `pandas.plotting` functions for visualizations.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, autocorrelation_plot, parallel_coordinates def analyze_financial_data(file_path: str) -> None: Analyze and visualize financial data from a CSV file. Parameters: - file_path (str): The path to the CSV file containing the financial data. Returns: - None # Load the Data data = pd.read_csv(file_path) # Data Cleaning and Processing missing_values = data.isnull().sum() print(\\"Missing values for each column:\\") print(missing_values) # Drop rows with missing values data = data.dropna() # Calculate Debt-to-Equity Ratio and add it as a new column data[\'Debt-to-Equity Ratio\'] = data[\'Liabilities\'] / data[\'Assets\'] # Data Visualization # Boxplot for the \'Profit\' column for each company plt.figure(figsize=(10, 6)) data.boxplot(column=\'Profit\', by=\'Company\') plt.title(\'Boxplot of Profit by Company\') plt.suptitle(\'\') plt.xlabel(\'Company\') plt.ylabel(\'Profit\') plt.xticks(rotation=90) plt.savefig(\'boxplot.png\') plt.close() # Scatter plot matrix for \'Revenue\', \'Expenses\', \'Profit\', \'Assets\', \'Liabilities\' scatter_matrix(data[[\'Revenue\', \'Expenses\', \'Profit\', \'Assets\', \'Liabilities\']], figsize=(12, 12)) plt.suptitle(\'Scatter Plot Matrix\') plt.savefig(\'scatter_matrix.png\') plt.close() # Autocorrelation plot for the \'Profit\' column for a single company (choosing the first one) company_name = data[\'Company\'].unique()[0] company_data = data[data[\'Company\'] == company_name] plt.figure(figsize=(10, 6)) autocorrelation_plot(company_data[\'Profit\']) plt.title(f\'Autocorrelation of Profit for {company_name}\') plt.xlabel(\'Lags\') plt.ylabel(\'Autocorrelation\') plt.savefig(\'autocorrelation_plot.png\') plt.close() # Parallel coordinates plot for the data plt.figure(figsize=(12, 6)) parallel_coordinates(data, \'Company\', cols=[\'Revenue\', \'Expenses\', \'Profit\', \'Assets\', \'Liabilities\', \'Debt-to-Equity Ratio\']) plt.title(\'Parallel Coordinates Plot\') plt.xlabel(\'Financial Metrics\') plt.ylabel(\'Values\') plt.xticks(rotation=45) plt.savefig(\'parallel_coordinates.png\') plt.close()"},{"question":"# Seaborn Visualization Coding Assessment Objective In this assessment, you are required to demonstrate your ability to visualize statistical relationships using the seaborn library. You will use the `fmri` dataset, which contains information about brain activity, to create a series of plots that provide insights into the data. Requirements 1. **Data Preparation:** - Load the `fmri` dataset using seaborn\'s `load_dataset` function. - Ensure the dataset is properly cleaned and ready for visualization. 2. **Scatter Plot:** - Create a scatter plot using `relplot` to visualize the relationship between `timepoint` and `signal`. - Color the points based on the `region` column. - Use different marker styles for the `event` column. 3. **Line Plot:** - Create a line plot using `relplot` to visualize the change in `signal` over `timepoint`. - Split the data by the `event` column using different colors and line styles. - Show the 95% confidence interval around the mean signal for each event. 4. **Faceted Plot:** - Create a faceted line plot using `relplot` to visualize the relationship between `timepoint` and `signal` for different regions. - Facet the plot by `region` in the columns and by `event` in the rows. - Use a consistent color palette for all plots. 5. **Customized Plot:** - Customize one of the previous plots by applying a sequential color palette using `cubehelix_palette`. - Add an appropriate title and axis labels to each plot for clarity. Input and Output Formats - **Input:** The `fmri` dataset loaded from seaborn. - **Output:** A series of seaborn visualizations as specified in the requirements. Constraints - Use seaborn functions as demonstrated in the provided documentation. - Ensure that all plots are clear, properly labeled, and visually appealing. - Handle any missing or invalid data appropriately. Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Scatter Plot sns.relplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\") plt.title(\\"Scatter Plot of Signal vs Timepoint\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.show() # Line Plot sns.relplot(data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", ci=\\"sd\\") plt.title(\\"Line Plot of Signal vs Timepoint by Event with 95% CI\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.show() # Faceted Plot sns.relplot(data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"subject\\", col=\\"region\\", row=\\"event\\", height=3, estimator=None) plt.suptitle(\\"Faceted Line Plot of Signal vs Timepoint by Region and Event\\", y=1.02) plt.show() # Customized Plot sns.relplot(data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"subject\\", col=\\"region\\", row=\\"event\\", palette=\\"ch:r=-.5,l=.75\\", height=3, estimator=None) plt.suptitle(\\"Customized Line Plot with Sequential Palette\\", y=1.02) plt.show() ``` Submission Submit your Python script or Jupyter notebook containing the code for the required visualizations. Ensure that all code cells produce the desired plots when executed sequentially.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_and_prepare_data(): Load and prepare the fmri dataset using seaborn\'s load_dataset function. fmri = sns.load_dataset(\\"fmri\\") return fmri def scatter_plot(fmri): Create a scatter plot using relplot to visualize the relationship between timepoint and signal. Color the points based on the region column and use different marker styles for the event column. plot = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\" ) plot.set(title=\\"Scatter Plot of Signal vs Timepoint\\", xlabel=\\"Timepoint\\", ylabel=\\"Signal\\") plt.show() def line_plot(fmri): Create a line plot using relplot to visualize the change in signal over timepoint. Split the data by the event column using different colors and line styles, and show the 95% confidence interval around the mean signal for each event. plot = sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", ci=95 ) plot.set(title=\\"Line Plot of Signal vs Timepoint by Event with 95% CI\\", xlabel=\\"Timepoint\\", ylabel=\\"Signal\\") plt.show() def faceted_plot(fmri): Create a faceted line plot using relplot to visualize the relationship between timepoint and signal for different regions. Facet the plot by region in the columns and by event in the rows. plot = sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"subject\\", col=\\"region\\", row=\\"event\\", height=3, estimator=None ) plot.set_titles(\\"{col_name} | {row_name}\\") plot.set_axis_labels(\\"Timepoint\\", \\"Signal\\") plot.fig.suptitle(\\"Faceted Line Plot of Signal vs Timepoint by Region and Event\\", y=1.02) plt.show() def customized_plot(fmri): Customize a previous plot by applying a sequential color palette using cubehelix_palette. Add an appropriate title and axis labels. plot = sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"subject\\", col=\\"region\\", row=\\"event\\", palette=\\"ch:r=-.5,l=.75\\", height=3, estimator=None ) plot.set_titles(\\"{col_name} | {row_name}\\") plot.set_axis_labels(\\"Timepoint\\", \\"Signal\\") plot.fig.suptitle(\\"Customized Line Plot with Sequential Palette\\", y=1.02) plt.show()"},{"question":"You are tasked with building a simple WSGI-based web server using the `wsgiref` library. The server should: 1. Serve a \\"Hello, World!\\" message on the root URL (`/`). 2. Serve a user-provided JSON message on the `/json` endpoint. 3. Validate the WSGI application using `wsgiref.validate`. Your WSGI application should support two endpoints: - Endpoint `/` returns a plain text `Hello, World!` message. - Endpoint `/json` returns a user-provided JSON message. The JSON message should be passed as a URL query parameter named `message`. Requirements - Implement the WSGI application with two endpoints (`/` and `/json`). - Use `wsgiref.simple_server` to create the server. - Use `wsgiref.validate` to validate the conformance of your WSGI application. - Ensure the server runs on `localhost` at port `8000`. Input and Output - **Input**: No direct input is required for the root URL. For the `/json` endpoint, the input is provided as a URL query parameter named `message`. - **Output**: The root URL should output `Hello, World!` in plain text. The `/json` endpoint should output the user-provided JSON message. If the `message` parameter is empty, return an appropriate error message in JSON format. Constraints - Ensure your server handles both endpoints gracefully without throwing exceptions. - The JSON response should be properly formatted and follow JSON standards. # Function Signature ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator def simple_wsgi_app(environ, start_response): # TODO: Implement this function pass if __name__ == \'__main__\': # Wrap the app with validator app = validator(simple_wsgi_app) server = make_server(\'localhost\', 8000, app) print(\\"Serving on port 8000...\\") server.serve_forever() ``` # Example Usage - Accessing `http://localhost:8000/` should return: ``` Hello, World! ``` - Accessing `http://localhost:8000/json?message={\\"greeting\\": \\"hello\\"}` should return the JSON message: ``` {\\"greeting\\": \\"hello\\"} ``` - Accessing `http://localhost:8000/json` without the `message` parameter should return an error message: ``` {\\"error\\": \\"Message parameter missing\\"} ``` Implement the function `simple_wsgi_app` to handle the above requirements.","solution":"import json from wsgiref.simple_server import make_server from wsgiref.validate import validator from urllib.parse import parse_qs def simple_wsgi_app(environ, start_response): path = environ.get(\'PATH_INFO\', \'\') query_string = environ.get(\'QUERY_STRING\', \'\') query_params = parse_qs(query_string) if path == \'/\': status = \'200 OK\' response_headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, response_headers) return [b\'Hello, World!\'] elif path == \'/json\': message = query_params.get(\'message\', [None])[0] if message: try: json_message = json.loads(message) response_body = json.dumps(json_message) status = \'200 OK\' response_headers = [(\'Content-type\', \'application/json; charset=utf-8\')] except json.JSONDecodeError: response_body = json.dumps({\'error\': \'Invalid JSON\'}) status = \'400 Bad Request\' response_headers = [(\'Content-type\', \'application/json; charset=utf-8\')] else: response_body = json.dumps({\'error\': \'Message parameter missing\'}) status = \'400 Bad Request\' response_headers = [(\'Content-type\', \'application/json; charset=utf-8\')] start_response(status, response_headers) return [response_body.encode(\'utf-8\')] else: status = \'404 Not Found\' response_headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, response_headers) return [b\'Not Found\'] if __name__ == \'__main__\': # Wrap the app with validator app = validator(simple_wsgi_app) server = make_server(\'localhost\', 8000, app) print(\\"Serving on port 8000...\\") server.serve_forever()"},{"question":"Objective You will demonstrate your understanding of the `seaborn` `diverging_palette` function by creating a specific diverging palette and applying it to visualize a dataset containing both positive and negative values. You will then provide a brief analysis of the resulting visual. Instructions 1. Create a diverging palette with the following specifications: - Start with a blue hue at the negative end (`h_neg = 240`). - End with a red hue at the positive end (`h_pos = 20`). - Use a dark center color (`center = \\"dark\\"`). - Return a continuous colormap (`as_cmap = True`). - Increase the amount of separation around the center value (`sep = 30`). - Reduce the saturation of the endpoints to 60% (`s = 60`). - Decrease the lightness of the endpoints to 40% (`l = 40`). 2. Use this palette to visualize the correlation matrix of a sample dataset that contains both positive and negative correlations. You can use the `seaborn` `heatmap` function for visualization. 3. Provide a brief analysis of the correlation heatmap you generated. Requirements * Your code should be functional and run without errors. * The heatmap should clearly visualize the positive and negative correlations using the diverging palette specified. * You should provide a brief analysis (3-5 sentences) describing the main features of the heatmap. # Example ```python import seaborn as sns import pandas as pd import numpy as np # Create a sample dataset with positive and negative correlations data = np.random.randn(10, 10) df = pd.DataFrame(data, columns=[f\'Var{i}\' for i in range(1, 11)]) # Calculate the correlation matrix corr = df.corr() # Create the diverging palette palette = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30, s=60, l=40) # Plot the heatmap sns.heatmap(corr, cmap=palette, center=0) # Analysis of the heatmap # The heatmap reveals the areas of high positive correlation in red and high negative correlation in blue. # The darker center color helps highlight the zero correlation regions. # With the increased separation, the distinctions around the center are more pronounced. # Adjustments to saturation and lightness provide a balanced visual contrast. ``` Constraints * Ensure that your code does not exceed a runtime of 5 seconds. * Your analysis should be concise yet descriptive.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def create_correlation_heatmap(): # Create a sample dataset with positive and negative correlations data = np.random.randn(10, 10) df = pd.DataFrame(data, columns=[f\'Var{i}\' for i in range(1, 11)]) # Calculate the correlation matrix corr = df.corr() # Create the diverging palette palette = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30, s=60, l=40) # Plot the heatmap sns.heatmap(corr, cmap=palette, center=0) plt.show() return corr"},{"question":"# Challenge: Implement Secure File Integrity Verification Using the `hmac` Module **Objective:** Write a Python program to verify the integrity of files using the HMAC algorithm. You need to implement two functions: one to generate an HMAC tag for a given file and another to verify the file\'s integrity using the provided HMAC tag. **Problem Statement:** 1. **Function 1: Generate HMAC Tag** - Name: `generate_hmac(file_path: str, key: bytes, digestmod: str) -> str` - Inputs: - `file_path` (str): The path to the file for which the HMAC tag needs to be generated. - `key` (bytes): The secret key used for the HMAC generation. - `digestmod` (str): The name of the hash algorithm to use (e.g., \\"sha256\\"). - Output: - A string representing the hexadecimal HMAC tag of the file contents. - Constraints: - The file at `file_path` can be large, so the function should handle reading it in chunks to avoid high memory usage. - `key` should be securely managed and not hard-coded in the function body. 2. **Function 2: Verify File Integrity** - Name: `verify_file_integrity(file_path: str, key: bytes, expected_hmac: str, digestmod: str) -> bool` - Inputs: - `file_path` (str): The path to the file whose integrity needs to be verified. - `key` (bytes): The secret key used for the HMAC verification. - `expected_hmac` (str): The expected HMAC tag to compare against. - `digestmod` (str): The name of the hash algorithm that was used for HMAC generation. - Output: - A boolean value indicating whether the file\'s HMAC matches the expected HMAC tag. - Constraints: - The file at `file_path` can be large, so the function should handle reading it in chunks to avoid high memory usage. - Use `hmac.compare_digest` to prevent timing attacks during HMAC comparison. **Example Usage:** ```python # Generating an HMAC tag for the file \'example.txt\' file_path = \'example.txt\' secret_key = b\'secret_key\' hash_algorithm = \'sha256\' hmac_tag = generate_hmac(file_path, secret_key, hash_algorithm) print(f\\"HMAC Tag: {hmac_tag}\\") # Verifying the integrity of the file \'example.txt\' using the generated HMAC tag is_valid = verify_file_integrity(file_path, secret_key, hmac_tag, hash_algorithm) print(f\\"File integrity valid: {is_valid}\\") ``` --- # Solution Template: ```python import hmac import hashlib def generate_hmac(file_path: str, key: bytes, digestmod: str) -> str: # Initialize HMAC object hmac_obj = hmac.new(key, digestmod=digestmod) # Read the file in chunks and update the HMAC object with open(file_path, \'rb\') as f: while chunk := f.read(4096): hmac_obj.update(chunk) # Return the HMAC tag as a hexadecimal string return hmac_obj.hexdigest() def verify_file_integrity(file_path: str, key: bytes, expected_hmac: str, digestmod: str) -> bool: # Generate the HMAC tag for the file computed_hmac = generate_hmac(file_path, key, digestmod) # Compare the computed HMAC with the expected HMAC to verify integrity return hmac.compare_digest(computed_hmac, expected_hmac) # Example usage if __name__ == \\"__main__\\": # Generating an HMAC tag for the file \'example.txt\' file_path = \'example.txt\' secret_key = b\'secret_key\' hash_algorithm = \'sha256\' hmac_tag = generate_hmac(file_path, secret_key, hash_algorithm) print(f\\"HMAC Tag: {hmac_tag}\\") # Verifying the integrity of the file \'example.txt\' using the generated HMAC tag is_valid = verify_file_integrity(file_path, secret_key, hmac_tag, hash_algorithm) print(f\\"File integrity valid: {is_valid}\\") ```","solution":"import hmac import hashlib def generate_hmac(file_path: str, key: bytes, digestmod: str) -> str: Generates an HMAC tag for the given file using the specified key and hash algorithm. Args: - file_path (str): The path to the file. - key (bytes): The secret key used for HMAC. - digestmod (str): The hash algorithm to use (e.g., \\"sha256\\"). Returns: - str: The hexadecimal HMAC tag for the file. hmac_obj = hmac.new(key, digestmod=getattr(hashlib, digestmod)) with open(file_path, \'rb\') as f: while chunk := f.read(4096): hmac_obj.update(chunk) return hmac_obj.hexdigest() def verify_file_integrity(file_path: str, key: bytes, expected_hmac: str, digestmod: str) -> bool: Verifies the integrity of the given file by comparing its HMAC tag to the expected HMAC tag. Args: - file_path (str): The path to the file. - key (bytes): The secret key used for HMAC. - expected_hmac (str): The expected HMAC tag. - digestmod (str): The hash algorithm used (e.g., \\"sha256\\"). Returns: - bool: True if the HMAC tags match, False otherwise. computed_hmac = generate_hmac(file_path, key, digestmod) return hmac.compare_digest(computed_hmac, expected_hmac)"},{"question":"Objective Write a Python function that processes a string by escaping certain characters for HTML, and then reversing the process by unescaping the characters back to their original form. Function Signature ```python def process_html_string(input_string: str, escape_quote: bool = True) -> str: pass ``` Input 1. `input_string` (str): A string that may contain characters that need to be escaped for HTML display. 2. `escape_quote` (bool): A boolean flag indicating whether to escape double and single quotes as well. Output - The function should return a string where the characters have been escaped and then unescaped according to the HTML 5 standard. Requirements 1. The function should first escape the input string using `html.escape()`. 2. It should then unescape the resulting string using `html.unescape()`. 3. Ensure that the output string matches the original input string exactly. Constraints - The input string can have a length of up to 10^6 characters. - The function should handle and process the string efficiently, keeping the operations within a reasonable time complexity. Example ```python # Example 1 input_string = \'This is a test & only a test <script>alert(\\"Hello!\\")<\/script>\' result = process_html_string(input_string) assert result == \'This is a test & only a test <script>alert(\\"Hello!\\")<\/script>\' # Example 2 input_string = \'Quotes: \\"double\\" and \'single\'\' result = process_html_string(input_string, escape_quote=True) assert result == \'Quotes: \\"double\\" and \'single\'\' ``` Notes - The goal of this assessment is to check your understanding of the `html` module\'s `escape` and `unescape` functions by implementing and testing their combined effects. - Make sure to handle edge cases, such as empty strings or strings without any characters that need to be escaped.","solution":"import html def process_html_string(input_string: str, escape_quote: bool = True) -> str: Escapes HTML characters in the input string, then unescapes them back to original form. Args: input_string (str): The string to process. escape_quote (bool): Whether to escape single and double quotes. Returns: str: The processed string, which after escaping and unescaping matches the original input. escaped_str = html.escape(input_string, quote=escape_quote) unescaped_str = html.unescape(escaped_str) return unescaped_str"},{"question":"# XML Parsing and Handling with `xml.parsers.expat` Given a collection of XML strings, your task is to write a Python function that processes these strings using the `xml.parsers.expat` module. Your function should: 1. Parse the provided XML documents. 2. Use handler functions to capture and print start and end elements, character data, and comments. 3. Handle errors gracefully by catching exceptions and printing a user-friendly error message. Function Signature: ```python def process_xml_documents(xml_docs: list): pass ``` Input: - `xml_docs`: A list of strings, where each string is a complete XML document. Output: - For each XML document, the function should print: - The start and end of every element. - The character data within elements. - Any comments within the XML document. Example XML Document and Output: Consider the XML document: ```xml <?xml version=\\"1.0\\"?> <root> <!-- This is a comment --> <child id=\\"1\\">Text within child</child> </root> ``` The output should be: ``` Start element: root Comment: This is a comment Start element: child {\'id\': \'1\'} Character data: Text within child End element: child End element: root ``` Constraints: 1. Assume each XML document is well-formed. 2. You must define and use appropriate handler functions for different XML components (elements, character data, and comments). 3. Handle any exceptions raised during parsing by printing a generic error message: `\\"Error parsing XML document five-n number where n is the index (one-indexed) of the XML document in the list.\\"`. Implement the function `process_xml_documents` using the `xml.parsers.expat` module.","solution":"import xml.parsers.expat def process_xml_documents(xml_docs): def start_element(name, attrs): print(f\\"Start element: {name} {attrs}\\") def end_element(name): print(f\\"End element: {name}\\") def char_data(data): if data.strip(): # Printing non-empty character data print(f\\"Character data: {data}\\") def handle_comment(data): print(f\\"Comment: {data}\\") for i, xml_doc in enumerate(xml_docs): parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.CommentHandler = handle_comment try: parser.Parse(xml_doc, True) except xml.parsers.expat.ExpatError: print(f\\"Error parsing XML document {i + 1}\\")"},{"question":"# Complex Number Manipulation with PyTorch Objective Create and manipulate complex tensors using PyTorch, demonstrating an understanding of operations, conversions, and optimizations involving complex numbers. Requirements 1. **Function Name**: `complex_tensor_operations` 2. **Input**: A 2D tensor of shape `(n, 2)` where each row contains real and imaginary parts of `n` complex numbers. 3. **Output**: A dictionary containing: - `\\"complex_tensor\\"`: A complex tensor created from the input tensor. - `\\"real_part\\"`: The real part of the complex tensor. - `\\"imaginary_part\\"`: The imaginary part of the complex tensor. - `\\"angle\\"`: The angle of each complex number in the tensor. - `\\"magnitude\\"`: The magnitude of each complex number in the tensor. - `\\"matmul_result\\"`: The result of multiplying the complex tensor with its Hermitian transpose. Constraints - You should use the PyTorch library for all tensor operations. - The input tensor will only contain floats. Example ```python import torch def complex_tensor_operations(input_tensor): # Ensure the input tensor has the right shape assert input_tensor.ndimension() == 2 and input_tensor.size(1) == 2, \\"Input must be a 2D tensor where each row contains real and imaginary parts.\\" # Convert the real tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Access the real and imaginary parts real_part = complex_tensor.real imaginary_part = complex_tensor.imag # Calculate the angle and magnitude angle = torch.angle(complex_tensor) magnitude = torch.abs(complex_tensor) # Compute the Hermitian transpose (conjugate transpose) hermitian_transpose = complex_tensor.t().conj() # Perform matrix multiplication matmul_result = torch.matmul(complex_tensor, hermitian_transpose) # Return all results in a dictionary return { \\"complex_tensor\\": complex_tensor, \\"real_part\\": real_part, \\"imaginary_part\\": imaginary_part, \\"angle\\": angle, \\"magnitude\\": magnitude, \\"matmul_result\\": matmul_result } # Example usage input_tensor = torch.tensor([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]) result = complex_tensor_operations(input_tensor) for key, value in result.items(): print(f\\"{key}:n{value}n\\") ``` In the example: - A real 2D tensor where each row contains the real and imaginary parts of complex numbers is given as input. - The function converts this tensor into a complex tensor, computes its real part, imaginary part, angle, and magnitude, performs a Hermitian transpose, and then multiplies the complex tensor with its Hermitian transpose. - The results are returned in a dictionary.","solution":"import torch def complex_tensor_operations(input_tensor): Takes a 2D tensor of shape (n, 2) where each row contains real and imaginary parts of n complex numbers. Returns a dictionary with complex tensor, its real and imaginary parts, angle, magnitude, and product with its Hermitian transpose. # Ensure the input tensor has the right shape assert input_tensor.ndimension() == 2 and input_tensor.size(1) == 2, \\"Input must be a 2D tensor where each row contains real and imaginary parts.\\" # Convert the real tensor to a complex tensor complex_tensor = torch.complex(input_tensor[:, 0], input_tensor[:, 1]) # Access the real and imaginary parts real_part = complex_tensor.real imaginary_part = complex_tensor.imag # Calculate the angle and magnitude angle = torch.angle(complex_tensor) magnitude = torch.abs(complex_tensor) # Compute the Hermitian transpose (conjugate transpose) hermitian_transpose = complex_tensor.t().conj() # Perform matrix multiplication matmul_result = torch.matmul(complex_tensor.unsqueeze(1), hermitian_transpose.unsqueeze(0)) # Return all results in a dictionary return { \\"complex_tensor\\": complex_tensor, \\"real_part\\": real_part, \\"imaginary_part\\": imaginary_part, \\"angle\\": angle, \\"magnitude\\": magnitude, \\"matmul_result\\": matmul_result }"},{"question":"**Question:** Implement a Transformer-based attention layer using PyTorch nested tensors **Objective:** To assess students\' understanding of variable-length data handling and attention mechanisms in PyTorch. **Problem Statement:** You are required to implement a Transformer-based Scaled Dot-Product Attention layer that can handle variable-length sequences using PyTorch nested tensors. The goal is to efficiently compute attention scores on batches of sequences that have different lengths. **Function Signature:** ```python import torch def transformer_attention(queries, keys, values): Implement a Transformer-based Scaled Dot-Product Attention layer for nested tensors. Args: queries (torch.Tensor): A nested tensor of shape (batch_size, seq_len_query, embed_size) representing the query vectors. keys (torch.Tensor): A nested tensor of shape (batch_size, seq_len_key, embed_size) representing the key vectors. values (torch.Tensor): A nested tensor of shape (batch_size, seq_len_value, embed_size) representing the value vectors. Returns: torch.Tensor: A nested tensor of shape (batch_size, seq_len_query, embed_size) representing the attention output. Constraints: - The `queries`, `keys`, and `values` must be nested tensors created using the `torch.nested.nested_tensor` constructor. - Handle gradient flow correctly to ensure compatibility with backpropagation. pass ``` **Constraints and Notes:** 1. **Input**: - `queries`, `keys`, and `values` are nested tensors with the jagged layout. Each nested tensor contains a batch of sequences where each sequence can have a different length. - The dimensions of `queries`, `keys`, and `values` must be compatible for the attention computation. 2. **Output**: - The output should be a nested tensor where each element is the result of the attention computation for the corresponding query, key, and value sequences. 3. **Attention Mechanism**: - Implement the Scaled Dot-Product Attention mechanism: [ text{Attention}(Q, K, V) = text{softmax}left(frac{QK^T}{sqrt{d_k}}right) V ] - Ensure that the softmax operation handles the different sequence lengths appropriately without the need for padding. 4. **Efficiency**: - Avoid converting nested tensors to padded dense tensors. Utilize the nested tensor\'s efficient packed representation. - Ensure the implementation is optimized for memory usage and computation time. 5. **Testing**: - Provide a set of unit tests to verify the implementation against standard PyTorch Tensors to ensure correctness. **Example Usage:** ```python import torch # Example nested tensors (batch size = 2) queries = torch.nested.nested_tensor([ torch.randn(3, 128), # Sequence 1 torch.randn(5, 128) # Sequence 2 ], layout=torch.jagged) keys = torch.nested.nested_tensor([ torch.randn(4, 128), # Sequence 1 torch.randn(6, 128) # Sequence 2 ], layout=torch.jagged) values = torch.nested.nested_tensor([ torch.randn(4, 128), # Sequence 1 torch.randn(6, 128) # Sequence 2 ], layout=torch.jagged) output = transformer_attention(queries, keys, values) print(output) ``` Implement the function `transformer_attention` to successfully execute the provided example and pass the constraints mentioned.","solution":"import torch import torch.nn.functional as F def transformer_attention(queries, keys, values): Implement a Transformer-based Scaled Dot-Product Attention layer for nested tensors. Args: queries (torch.Tensor): A nested tensor of shape (batch_size, seq_len_query, embed_size) representing the query vectors. keys (torch.Tensor): A nested tensor of shape (batch_size, seq_len_key, embed_size) representing the key vectors. values (torch.Tensor): A nested tensor of shape (batch_size, seq_len_value, embed_size) representing the value vectors. Returns: torch.Tensor: A nested tensor of shape (batch_size, seq_len_query, embed_size) representing the attention output. batch_size = len(queries) embed_size = queries[0].size(-1) attention_outputs = [] for i in range(batch_size): q = queries[i] # (seq_len_query, embed_size) k = keys[i] # (seq_len_key, embed_size) v = values[i] # (seq_len_value, embed_size) # Scaled dot-product attention dk = embed_size scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(dk, dtype=torch.float32)) attention_weights = F.softmax(scores, dim=-1) attention_output = torch.matmul(attention_weights, v) attention_outputs.append(attention_output) attention_outputs_nested = torch.nested.nested_tensor(attention_outputs, layout=torch.jagged) return attention_outputs_nested"},{"question":"You are tasked with developing a monitoring tool that helps web developers ensure their web crawlers comply with the crawling rules specified by websites. Specifically, you will implement a function that identifies which URLs from a list can be fetched by a given user agent based on the website\'s `robots.txt` file. # Function Signature ```python def identify_accessible_links(robots_txt_url: str, user_agent: str, url_list: list) -> list: Determines which URLs from the given list are accessible by the specified user agent according to the rules in the robots.txt file. Args: - robots_txt_url (str): The URL leading to the site\'s robots.txt file. - user_agent (str): The user agent string to check against the robots.txt rules. - url_list (list): A list of URLs (str) to check access permissions for. Returns: - list: A list of URLs (from url_list) that the user agent is allowed to fetch. ``` # Input and Output: - **Input**: - `robots_txt_url`: A string representing the URL of the `robots.txt` file to be checked. - `user_agent`: A string representing the web crawler user agent. - `url_list`: A list of strings, each representing a URL the user agent wants to access. - **Output**: - Returns a list of strings. Each string is a URL from the input `url_list` that the `user_agent` is allowed to access as per the `robots.txt` rules. # Constraints: - You can assume that the `robots.txt` file exists and is accessible from the provided URL. - The URLs in `url_list` will be from the same domain as the `robots_txt_url`. # Example: ```python robots_txt_url = \\"http://www.example.com/robots.txt\\" user_agent = \\"MyWebCrawler\\" url_list = [ \\"http://www.example.com/\\", \\"http://www.example.com/about\\", \\"http://www.example.com/private\\", ] accessible_urls = identify_accessible_links(robots_txt_url, user_agent, url_list) print(accessible_urls) ``` # Explanation: If the `robots.txt` file at `http://www.example.com/robots.txt` contains rules that disallow `MyWebCrawler` from accessing the URL `http://www.example.com/private` but allows access to the other URLs, the function will return: ```python [\\"http://www.example.com/\\", \\"http://www.example.com/about\\"] ``` If all URLs are accessible, it will return the original `url_list`. # Notes: - Ensure to use the `RobotFileParser` class to handle and parse the `robots.txt` file. - Pay attention to edge cases, such as malformed URLs in `url_list` or wildcards in `robots.txt`.","solution":"from urllib.robotparser import RobotFileParser from urllib.parse import urlparse def identify_accessible_links(robots_txt_url: str, user_agent: str, url_list: list) -> list: Determines which URLs from the given list are accessible by the specified user agent according to the rules in the robots.txt file. Args: - robots_txt_url (str): The URL leading to the site\'s robots.txt file. - user_agent (str): The user agent string to check against the robots.txt rules. - url_list (list): A list of URLs (str) to check access permissions for. Returns: - list: A list of URLs (from url_list) that the user agent is allowed to fetch. # Initialize the RobotFileParser and set the robots.txt URL parser = RobotFileParser() parser.set_url(robots_txt_url) parser.read() accessible_urls = [] # Check each URL in the list against the robots.txt rules for url in url_list: if parser.can_fetch(user_agent, url): accessible_urls.append(url) return accessible_urls"},{"question":"Coding Assessment Question # Objective Implement a classifier using the `SGDClassifier` from the `sklearn.linear_model` module. You will tune specific hyperparameters to achieve the best performance on a given dataset. Your task involves training, validation, and evaluating key metrics. # Task 1. **Load Dataset**: Use the provided dataset `iris` from `sklearn.datasets`. 2. **Train-Test Split**: Split the data into training and test sets (80% training, 20% test). 3. **Scaling**: Standardize the features using `StandardScaler`. 4. **SGD Classifier Implementation**: - Train a model using `SGDClassifier` with the following initial parameters: - `loss=\'hinge\'` - `penalty=\'l2\'` - `max_iter=1000` - `tol=1e-3` - Use `random_state=42` for reproducibility. 5. **Hyperparameter Tuning**: - Tune the `alpha` hyperparameter using `GridSearchCV` with `cv=5`. Consider `alpha` values in the range 10.0^{text{-np.arange(1, 4)}} (i.e., `[0.1, 0.01, 0.001]`). 6. **Evaluate Model**: - Calculate the accuracy on both the training and test sets. - Print the best hyperparameter value and the corresponding validation accuracy. # Expected Input and Output Input - No direct input is needed as the `iris` dataset will be used internally. Output - Print the best `alpha` value from the grid search. - Print the classification accuracy on the training and test sets using the optimized model. Constraints - Use only the standard libraries and scikit-learn for implementation. - Keep the code clean and well-commented. - Ensure reproducibility by setting `random_state` where applicable. # Example Code ```python import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score # Load the iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() # Implementing SGDClassifier with initial parameters sgd_clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) pipeline = make_pipeline(scaler, sgd_clf) # Hyperparameter tuning with GridSearchCV param_grid = {\'sgdclassifier__alpha\': [0.1, 0.01, 0.001]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameter value best_alpha = grid_search.best_params_[\'sgdclassifier__alpha\'] print(f\\"Best alpha: {best_alpha}\\") # Evaluate the model on training data train_preds = grid_search.predict(X_train) train_accuracy = accuracy_score(y_train, train_preds) print(f\\"Training Accuracy: {train_accuracy:.4f}\\") # Evaluate the model on test data test_preds = grid_search.predict(X_test) test_accuracy = accuracy_score(y_test, test_preds) print(f\\"Test Accuracy: {test_accuracy:.4f}\\") ``` Ensure your solution meets the task requirements to complete this assessment successfully.","solution":"import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def classify_iris(): # Load the iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features and create a pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42)) # Hyperparameter tuning with GridSearchCV param_grid = {\'sgdclassifier__alpha\': [0.1, 0.01, 0.001]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameter value best_alpha = grid_search.best_params_[\'sgdclassifier__alpha\'] print(f\\"Best alpha: {best_alpha}\\") # Evaluate the model on training data train_preds = grid_search.predict(X_train) train_accuracy = accuracy_score(y_train, train_preds) print(f\\"Training Accuracy: {train_accuracy:.4f}\\") # Evaluate the model on test data test_preds = grid_search.predict(X_test) test_accuracy = accuracy_score(y_test, test_preds) print(f\\"Test Accuracy: {test_accuracy:.4f}\\") return best_alpha, train_accuracy, test_accuracy"},{"question":"# Objective Implement a function that validates and extracts information from strings using regular expressions. The goal is to check if a given string contains valid dates in the format \\"YYYY-MM-DD\\" and extract all such dates. # Problem Statement Write a function `extract_valid_dates(text: str) -> List[str]` that takes a string `text` as input and returns a list of valid dates found in the input string. The dates should be in the format \\"YYYY-MM-DD\\", where: - YYYY is a four-digit year - MM is a two-digit month (01 to 12) - DD is a two-digit day (01 to 31) Your function should: 1. Return an empty list if there are no valid dates in the input string. 2. Raise a `ValueError` if the input string contains any invalid date patterns that do not match the \\"YYYY-MM-DD\\" format strictly. # Input - A string `text` which is a sequence of characters. # Output - A list of strings where each string is a valid date in the format \\"YYYY-MM-DD\\". # Constraints - You cannot use any libraries other than Python\'s built-in `re` module for regular expressions. - The function must handle both uppercase and lowercase alphabet characters, as well as digits and special characters. - Do not consider leap years; validate the dates based only on the format and general month-day limitations. # Examples ```python assert extract_valid_dates(\\"Today is 2023-10-05, but my birthday is on 1990-11-30.\\") == [\\"2023-10-05\\", \\"1990-11-30\\"] assert extract_valid_dates(\\"Invalid dates like 2023-13-01 or 2022-00-25 should raise an error.\\") == ValueError assert extract_valid_dates(\\"No dates here!\\") == [] ``` # Note - Pay attention to edge cases where date patterns might appear within other numbers or non-date-like strings. - Make sure your regular expression pattern strictly adheres to the \\"YYYY-MM-DD\\" format, considering valid ranges for months and days.","solution":"import re from typing import List def extract_valid_dates(text: str) -> List[str]: Extracts valid dates in the format YYYY-MM-DD from a given string. Args: text (str): Input string containing potential dates. Returns: List[str]: List of valid dates found in the input string. Raises: ValueError: If any invalid date patterns are found in the input string. date_pattern = re.compile(r\'b(d{4})-(d{2})-(d{2})b\') matches = date_pattern.findall(text) valid_dates = [] for year, month, day in matches: year, month, day = int(year), int(month), int(day) if 1 <= month <= 12 and 1 <= day <= 31: valid_dates.append(f\\"{year:04d}-{month:02d}-{day:02d}\\") else: raise ValueError(f\\"Found an invalid date: {year:04d}-{month:02d}-{day:02d}\\") return valid_dates"},{"question":"# PyTorch Distributions: Mixture of Gaussians In this assessment, you are required to implement a Python function using PyTorch\'s distribution classes. The function will create a Mixture of Gaussians model, sample from it, and calculate the log-probabilities of given points. Function Signature ```python import torch from torch.distributions import Normal, Categorical, MixtureSameFamily def create_mixture_of_gaussians(mixture_weights, means, stds): Create a Mixture of Gaussians model, sample from it, and calculate the log-probabilities of given points. Args: mixture_weights (List[float]): List of weights for each Gaussian component. Should sum to 1. means (List[float]): List of means for each Gaussian component. stds (List[float]): List of standard deviations for each Gaussian component. Returns: samples (Tensor): A tensor of samples drawn from the mixture model. log_probs (callable): A function that takes a tensor of points and returns their log-probabilities. # Your solution here ``` Input Constraints 1. `mixture_weights` should be a list of floating-point numbers that sum to 1. 2. `means` and `stds` should be lists of the same length as `mixture_weights`, containing the means and standard deviations of the Gaussian components. Output 1. `samples`: A tensor of samples drawn from the mixture of Gaussians model. 2. `log_probs`: A callable function that takes a tensor of points and returns their log-probabilities under the mixture model. Example Usage ```python mixture_weights = [0.4, 0.6] means = [0.0, 5.0] stds = [1.0, 2.0] samples, log_probs = create_mixture_of_gaussians(mixture_weights, means, stds) print(samples) # Prints a tensor of samples from the mixture model print(log_probs(torch.tensor([0.0, 1.0, 5.0]))) # Calculates and prints log-probabilities ``` Implementation Details 1. **Mixture Components**: Use `Normal` distribution from `torch.distributions` to define each Gaussian component. 2. **Mixture Model**: Use `Categorical` distribution for the mixture weights and `MixtureSameFamily` to combine the Gaussian components. 3. **Sampling**: The mixture model should allow drawing samples and calculating log-probabilities. Make sure your implementation adheres to the constraints and correctly uses PyTorch\'s distribution classes to model the Mixture of Gaussians.","solution":"import torch from torch.distributions import Normal, Categorical, MixtureSameFamily def create_mixture_of_gaussians(mixture_weights, means, stds): Create a Mixture of Gaussians model, sample from it, and calculate the log-probabilities of given points. Args: mixture_weights (List[float]): List of weights for each Gaussian component. Should sum to 1. means (List[float]): List of means for each Gaussian component. stds (List[float]): List of standard deviations for each Gaussian component. Returns: samples (Tensor): A tensor of samples drawn from the mixture model. log_probs (callable): A function that takes a tensor of points and returns their log-probabilities. mixture_weights = torch.tensor(mixture_weights) means = torch.tensor(means) stds = torch.tensor(stds) # Define the mixture components components = Normal(means, stds) # Define the mixture model mixture_distribution = Categorical(mixture_weights) mixture_model = MixtureSameFamily(mixture_distribution, components) # Draw samples from the mixture model samples = mixture_model.sample() # Define the log-probabilities function def log_probs(points): return mixture_model.log_prob(points) return samples, log_probs"},{"question":"# Advanced Python Coding Assessment: Asynchronous Subprocess Management **Objective**: Assess the understanding and ability to implement asynchronous subprocess handling in Python using `asyncio`. **Problem Statement**: You are to implement a function `run_multiple_subprocesses(commands: list, timeout: int) -> dict` that executes a list of shell commands asynchronously and returns their results. **Function Specification**: - `commands` (list): A list of strings, where each string is a shell command to be executed. - `timeout` (int): A time limit in seconds for executing each command. If a command exceeds this time, it should be terminated. - The function should return a dictionary where each key is a command string and the value is a tuple `(exit_code, stdout, stderr)`. - `exit_code`: An integer representing the subprocess exit code. - `stdout`: A string of the standard output captured from the command. - `stderr`: A string of the standard error captured from the command. - If the command was terminated due to a timeout, both `stdout` and `stderr` should be `None`. **Constraints**: - The function should handle multiple commands simultaneously using `asyncio.gather`. - Proper error handling should be implemented for subprocess termination and communication. - Use `shlex.quote()` to protect against shell injection vulnerabilities. **Example**: ```python import asyncio async def run_multiple_subprocesses(commands, timeout): # Your implementation here # Example usage: commands = [\\"ls /\\", \\"sleep 2; echo \'hello\'\\", \\"ls /nonexistent\\"] timeout = 1 results = asyncio.run(run_multiple_subprocesses(commands, timeout)) print(results) # Expected Output # { # \\"ls /\\": (0, \\"<stdout_content>\\", \\"\\"), # \\"sleep 2; echo \'hello\'\\": (-1, None, None), # Timed out # \\"ls /nonexistent\\": (1, \\"\\", \\"ls: /nonexistent: No such file or directory\\") # } ``` # Requirements: 1. **Create the Subprocesses**: Utilize `asyncio.create_subprocess_shell` to create subprocesses for each command in the `commands` list. 2. **Set the Timeout**: Ensure each command execution respects the `timeout` constraint. 3. **Collect Outputs**: Gather `stdout` and `stderr` outputs, handle possible errors, and manage subprocess lifecycles correctly. 4. **Return Results**: Construct and return the dictionary with command results. # Important Notes: - Use of `asyncio` features such as `asyncio.create_subprocess_shell`, `asyncio.gather`, and timeouts should be demonstrated. - Proper cleanup and resource management should be evident in the solution. - Maintain readability and handle edge cases such as command injection, empty commands list, etc. # Additional Information: - Make sure you have the `asyncio` library available in your Python 3.10 environment. - Propose and test your function in a local development setup before submission. Good luck!","solution":"import asyncio import shlex async def run_command(command: str, timeout: int): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) try: stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout) return process.returncode, stdout.decode(), stderr.decode() except asyncio.TimeoutError: process.kill() return -1, None, None async def run_multiple_subprocesses(commands: list, timeout: int) -> dict: tasks = [run_command(command, timeout) for command in commands] results = await asyncio.gather(*tasks) return {command: result for command, result in zip(commands, results)}"},{"question":"Objective Demonstrate your understanding of the `textwrap` module functionalities by writing a function that performs various text-wrapping operations based on given configurations. Description Write a function `custom_wrap(text: str, config: dict) -> str` that takes a string `text` and a configuration dictionary `config` to perform text wrapping and formatting based on given options. The function should utilize various features of the `textwrap` module. Input - `text` (str): A single paragraph of text to be processed. - `config` (dict): A dictionary containing various configuration options. The dictionary may contain the following keys: - `method` (str): Specifies the method to use, can be `\\"wrap\\"`, `\\"fill\\"`, `\\"shorten\\"`, `\\"dedent\\"`, or `\\"indent\\"`. - `width` (int): Maximum width of the wrapped text (applicable for `wrap`, `fill`, and `shorten` methods). - `placeholder` (str): Placeholder string for truncated text (applicable for `shorten` method). - Other optional keys that correspond to arguments of the chosen method. Output - (str): The processed text based on the specified method and configuration options. Constraints - The `method` key in the `config` dictionary is required. - The `width` key is required for `wrap`, `fill`, and `shorten` methods. - The function should handle different configurations dynamically based on the provided `config` dictionary. Examples ```python def custom_wrap(text, config): import textwrap method = config.get(\'method\') if method == \'wrap\': width = config.get(\'width\', 70) wrapped_text = textwrap.wrap(text, width, **{k: v for k, v in config.items() if k not in [\'method\', \'width\']}) return \'n\'.join(wrapped_text) # Returning as a single string elif method == \'fill\': width = config.get(\'width\', 70) filled_text = textwrap.fill(text, width, **{k: v for k, v in config.items() if k not in [\'method\', \'width\']}) return filled_text elif method == \'shorten\': width = config.get(\'width\') placeholder = config.get(\'placeholder\', \' [...]\') shortened_text = textwrap.shorten(text, width, placeholder=placeholder, **{k: v for k, v in config.items() if k not in [\'method\', \'width\', \'placeholder\']}) return shortened_text elif method == \'dedent\': dedented_text = textwrap.dedent(text) return dedented_text elif method == \'indent\': prefix = config.get(\'prefix\', \'\') predicate = config.get(\'predicate\', None) indented_text = textwrap.indent(text, prefix, predicate) return indented_text else: raise ValueError(\\"Invalid method specified in the config\\") # Example usage: text = \\"Hello world! This is an example of text wrapping using the textwrap module in Python. It provides several useful methods to format text easily.\\" config_wrap = {\'method\': \'wrap\', \'width\': 40} print(custom_wrap(text, config_wrap)) config_fill = {\'method\': \'fill\', \'width\': 40} print(custom_wrap(text, config_fill)) config_shorten = {\'method\': \'shorten\', \'width\': 20, \'placeholder\': \'...\'} print(custom_wrap(text, config_shorten)) config_dedent = {\'method\': \'dedent\'} dedented_text = Hello world print(custom_wrap(dedented_text, config_dedent)) config_indent = {\'method\': \'indent\', \'prefix\': \'>> \'} print(custom_wrap(text, config_indent)) ``` Evaluation Criteria - Correct implementation of each text wrapping method. - Correct handling of the configuration dictionary to apply appropriate options. - Efficient and clear code structure. - Proper error handling for invalid configurations.","solution":"def custom_wrap(text, config): import textwrap method = config.get(\'method\') if method == \'wrap\': width = config.get(\'width\', 70) wrapped_text = textwrap.wrap(text, width, **{k: v for k, v in config.items() if k not in [\'method\', \'width\']}) return \'n\'.join(wrapped_text) # Returning as a single string elif method == \'fill\': width = config.get(\'width\', 70) filled_text = textwrap.fill(text, width, **{k: v for k, v in config.items() if k not in [\'method\', \'width\']}) return filled_text elif method == \'shorten\': width = config.get(\'width\') placeholder = config.get(\'placeholder\', \' [...]\') shortened_text = textwrap.shorten(text, width, placeholder=placeholder, **{k: v for k, v in config.items() if k not in [\'method\', \'width\', \'placeholder\']}) return shortened_text elif method == \'dedent\': dedented_text = textwrap.dedent(text) return dedented_text elif method == \'indent\': prefix = config.get(\'prefix\', \'\') predicate = config.get(\'predicate\', None) indented_text = textwrap.indent(text, prefix, predicate) return indented_text else: raise ValueError(\\"Invalid method specified in the config\\")"},{"question":"You are required to write a function and create unit tests using the `unittest.mock` library to mock and assert behaviors. # Function Implementation Implement a function named `fetch_data` that takes a URL as input and returns the data fetched from the web. Use the `requests` library to make a web request. ```python import requests def fetch_data(url: str) -> dict: response = requests.get(url) return response.json() ``` # Unit Test 1. Create a unit test using the `unittest` framework and the `unittest.mock` library to mock the `requests.get` method. 2. Ensure that your mock returns a predefined JSON response when the `fetch_data` function is called. 3. Verify that the `fetch_data` function correctly handles the mocked response. 4. Use assertions to check that the `requests.get` method was called with the correct URL. # Instructions - Create a test class `TestFetchData` that subclasses `unittest.TestCase`. - Use the `patch` decorator from `unittest.mock` to mock `requests.get`. - Implement a test method `test_fetch_data` which follows these steps: - Mock the response of `requests.get` to return a mock object with a `json` method, which returns a predefined dictionary. - Call the `fetch_data` function with a test URL. - Assert that the return value of `fetch_data` is the predefined dictionary. - Assert that `requests.get` was called exactly once with the test URL. # Example Predefined JSON Response ```python response_data = { \\"userId\\": 1, \\"id\\": 1, \\"title\\": \\"Sample Title\\", \\"completed\\": False } ``` # Test URL ```python test_url = \\"https://jsonplaceholder.typicode.com/todos/1\\" ``` Your unit test should look something like this: ```python import unittest from unittest.mock import patch, Mock from your_module import fetch_data class TestFetchData(unittest.TestCase): @patch(\'your_module.requests.get\') def test_fetch_data(self, mock_get): # Arrange mock_response = Mock() response_data = { \\"userId\\": 1, \\"id\\": 1, \\"title\\": \\"Sample Title\\", \\"completed\\": False } mock_response.json.return_value = response_data mock_get.return_value = mock_response # Act result = fetch_data(\'https://jsonplaceholder.typicode.com/todos/1\') # Assert self.assertEqual(result, response_data) mock_get.assert_called_once_with(\'https://jsonplaceholder.typicode.com/todos/1\') if __name__ == \'__main__\': unittest.main() ``` In this task, you should demonstrate your ability to: - Mock external dependencies using `unittest.mock`. - Verify interactions with the mocked dependencies. - Use various assertion methods to ensure correct behaviors.","solution":"import requests def fetch_data(url: str) -> dict: response = requests.get(url) return response.json()"},{"question":"Objective: You are tasked with creating a Python module that interacts with the file system and handles user-interactive I/O. Your module should include the following functionalities: 1. **Convert Path to File System Representation**: Write a function `convert_path(path)` that takes a path and returns its file system representation. The given path can be a string, bytes, or an object implementing the `os.PathLike` interface. If the path is not valid, raise a `TypeError`. 2. **Check if File is Interactive**: Write a function `is_interactive_file(file_pointer, filename)` which takes a file pointer and a filename, and returns `True` if the file is interactive, otherwise `False`. A file is considered interactive if it is connected to a terminal. Function Implementations: 1. `convert_path(path)` - **Input:** - `path` (Union[str, bytes, os.PathLike]): A path that needs to be converted to its file system representation. - **Output:** - Returns the file system representation of the path as a string or bytes. - **Constraints:** - If the path is not a valid type or does not implement the necessary interface, raise a `TypeError`. 2. `is_interactive_file(file_pointer, filename)` - **Input:** - `file_pointer` (FILE*): A pointer to an open file. - `filename` (str): The name of the file. - **Output:** - Returns `True` if the file is interactive, otherwise `False`. - **Constraints:** - Utilize the system checks to determine if the file is interactive. Example: ```python import os # Example usage path_str = \\"/path/to/directory\\" path_bytes = b\\"/path/to/byte_directory\\" path_object = os.path.join(\\"path\\", \\"to\\", \\"object_directory\\") print(convert_path(path_str)) # Should output the file system representation print(convert_path(path_bytes)) # Should output the file system representation print(convert_path(path_object)) # Should output the file system representation file_pointer = open(path_str, \'r\') print(is_interactive_file(file_pointer, \\"filename\\")) # Example to check file interactivity file_pointer.close() ``` **Note:** You are expected to handle low-level file operations and ensure proper error handling in your solutions. The functions must integrate seamlessly with Python’s C-API functions discussed. Submission: Save your solution in a Python file named `file_system_utils.py`.","solution":"import os import sys def convert_path(path): Returns the file system representation of the provided path. if isinstance(path, (str, bytes, os.PathLike)): return os.fspath(path) else: raise TypeError(f\\"Invalid path type: {type(path)}. Expected str, bytes or os.PathLike.\\") def is_interactive_file(file_pointer, filename): Returns True if the file is interactive, otherwise False. return hasattr(file_pointer, \'isatty\') and file_pointer.isatty()"},{"question":"Objective: This question assesses your understanding of common security considerations in Python programming and your ability to implement secure alternatives for potentially unsafe functions. Problem Statement: You are given a log processing script that reads input from various sources and processes the data to generate a report. However, the current implementation has potential security vulnerabilities. Your task is to identify these vulnerabilities and refactor the code to use more secure alternatives. Here is the initial implementation: ```python import pickle import random import subprocess def read_data(file_path): with open(file_path, \'rb\') as file: data = pickle.load(file) return data def process_data(data): processed_data = {} for key, value in data.items(): processed_data[key] = value * random.random() return processed_data def generate_report(processed_data, output_path): with open(output_path, \'w\') as file: for key, value in processed_data.items(): file.write(f\\"{key}: {value}n\\") def main(file_path, output_path): data = read_data(file_path) processed_data = process_data(data) generate_report(processed_data, output_path) subprocess.run([\'echo\', \'Report generation complete\']) if __name__ == \\"__main__\\": main(\'data.pkl\', \'report.txt\') ``` Task: 1. Identify and explain the security vulnerabilities in the given code. 2. Refactor the code to eliminate these vulnerabilities. Ensure that the functionality remains the same. Constraints: - Do not use the `pickle` module for deserialization. - Use a secure method to generate random numbers. - Avoid using `subprocess.run` in an insecure manner. Expected Input and Output Formats: - Input: The function `main` will receive a file path for the input data (binary serialized) and a file path for the output report (text). - Output: A text file report with the processed data. Performance Requirements: - Your implementation should handle large input files efficiently. Example: ```python # Assume data.pkl contains a serialized dictionary { \\"a\\": 1, \\"b\\": 2, \\"c\\": 3 } # The output \\"report.txt\\" should look something like: # a: 0.4954356328018024 # b: 1.0544885095831444 # c: 2.1749906312193966 ``` **Note: You are free to create helper functions as needed.**","solution":"import json import random import secrets def read_data(file_path): with open(file_path, \'r\') as file: data = json.load(file) return data def process_data(data): processed_data = {} for key, value in data.items(): processed_data[key] = value * secrets.SystemRandom().random() return processed_data def generate_report(processed_data, output_path): with open(output_path, \'w\') as file: for key, value in processed_data.items(): file.write(f\\"{key}: {value}n\\") def main(file_path, output_path): data = read_data(file_path) processed_data = process_data(data) generate_report(processed_data, output_path) print(\\"Report generation complete\\") if __name__ == \\"__main__\\": main(\'data.json\', \'report.txt\')"},{"question":"# Shared Memory Data Synchronization You are required to implement a multiprocessing solution using shared memory to parallelize the computation of the sum of elements from distinct sub-arrays and finally consolidate the results. The challenge involves creating and managing the shared memory blocks efficiently such that multiple processes can work on the array concurrently. # Problem Statement Implement a function `parallel_sum_subarrays(array, num_processes)` which takes in: - `array` (list of integers): The input array to be divided into sub-arrays. - `num_processes` (integer): The number of processes for parallel computation. Each process should compute the sum of a distinct segment (`sub_array`) of the input `array`, store the partial result, and finally, these results should be summed up to give the total. # Steps and Requirements 1. **Shared Memory Setup**: - Use `multiprocessing.shared_memory.SharedMemory` to create a shared memory block for the array. - Ensure that the array is divided evenly across the specified number of processes. If it cannot be evenly divided, the remaining elements should be handled appropriately. 2. **Process Creation**: - Use `multiprocessing.Process` to create multiple processes. - Each process will work on its assigned sub-array of the input array and update the shared memory block with its partial sum. 3. **Synchronization**: - Use a `multiprocessing.managers.SharedMemoryManager` to manage the lifecycle of shared memory blocks. 4. **Result Consolidation**: - After all processes complete, combine the partial sums from the shared memory into a final result. 5. **Resource Cleanup**: - Ensure proper cleanup by closing and unlinking the shared memory blocks after use. # Function Signature ```python def parallel_sum_subarrays(array: list, num_processes: int) -> int: pass ``` # Example ```python if __name__ == \\"__main__\\": array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] num_processes = 3 result = parallel_sum_subarrays(array, num_processes) print(result) # Output should be 55 (sum of all elements in the array) ``` # Constraints - Each element in `array` will be an integer in the range `[-10^6, 10^6]`. - The length of `array` will be between `1` and `10^6`. - `num_processes` will be a positive integer, not greater than the length of `array`. # Performance Requirements - The solution should efficiently utilize the shared memory for parallel computation, ensuring minimal overhead for inter-process communication. - It must manage shared resources properly to avoid memory leaks and race conditions. Test your implementation thoroughly for various input sizes and edge cases.","solution":"import multiprocessing from multiprocessing.shared_memory import SharedMemory import numpy as np def sum_subarray(array, start, end, index, shared_result): sub_array = array[start:end] sub_sum = sum(sub_array) shared_result[index] = sub_sum def parallel_sum_subarrays(array, num_processes): if not array: return 0 array_len = len(array) chunk_size, remainder = divmod(array_len, num_processes) # Create shared memory for the results shared_result = multiprocessing.Array(\'i\', num_processes) processes = [] for i in range(num_processes): start = i * chunk_size end = (i + 1) * chunk_size if i < num_processes - 1 else array_len process = multiprocessing.Process(target=sum_subarray, args=(array, start, end, i, shared_result)) processes.append(process) process.start() for process in processes: process.join() total_sum = sum(shared_result) return total_sum"},{"question":"Implement a Precision-Recall Curve Display Problem Statement: You are required to implement a `PrecisionRecallCurveDisplay` class to visualize the Precision-Recall curve from a binary classification task. This class will follow the design elements of the scikit-learn plotting API as described. Your implementation should be able to generate the necessary precision-recall values either from an estimator or from pre-computed predictions and then plot them using matplotlib. Requirements: 1. Implement the `PrecisionRecallCurveDisplay` class. 2. Implement the `from_estimator` class method that: - Takes an estimator, feature matrix `X`, and true labels `y`. - Uses `estimator.predict_proba(X)` to obtain predicted probabilities. - Calculates the precision and recall values. - Initializes and returns an instance of `PrecisionRecallCurveDisplay`. 3. Implement the `from_predictions` class method that: - Takes true labels `y` and predicted probabilities `y_pred`. - Calculates the precision and recall values. - Initializes and returns an instance of `PrecisionRecallCurveDisplay`. 4. Implement the `plot` method that: - Plots the precision-recall curve using matplotlib. - Allows customization of the axes. Constraints: - Use sklearn\'s `precision_recall_curve` to calculate the precision and recall values. - Use matplotlib for plotting. - Follow the guidelines for implementing the plotting API as described in the documentation provided. Input: - For `from_estimator`: an estimator, a feature matrix `X`, and a label vector `y`. - For `from_predictions`: a label vector `y` and a predicted probabilities vector `y_pred`. Output: - A plotted precision-recall curve when the `plot` method is called. Example Usage: ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from precision_recall_display import PrecisionRecallCurveDisplay import matplotlib.pyplot as plt # Generate a binary classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) # Train a logistic regression classifier clf = LogisticRegression() clf.fit(X, y) # Create PrecisionRecallCurveDisplay from estimator viz = PrecisionRecallCurveDisplay.from_estimator(clf, X, y) viz.plot() plt.show() # Alternatively, using from_predictions y_pred = clf.predict_proba(X)[:, 1] viz = PrecisionRecallCurveDisplay.from_predictions(y, y_pred) viz.plot() plt.show() ``` Ensure your solution passes the example usage test and adheres to the design requirements specified.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve class PrecisionRecallCurveDisplay: def __init__(self, precision, recall, estimator_name=None): self.precision = precision self.recall = recall self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred) return cls(precision, recall, estimator_name=type(estimator).__name__) @classmethod def from_predictions(cls, y, y_pred): precision, recall, _ = precision_recall_curve(y, y_pred) return cls(precision, recall) def plot(self, ax=None): if ax is None: ax = plt.gca() ax.plot(self.recall, self.precision, label=f\'Precision-Recall curve {self.estimator_name}\' if self.estimator_name else \'Precision-Recall curve\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.legend() return ax"},{"question":"**Coding Assessment Question: Custom Event Loop Policy Implementation** **Objective**: Demonstrate your understanding of asyncio event loop policies by implementing a custom event loop policy. # Problem Statement You are required to implement a custom event loop policy named `MyCustomEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. This custom policy should: 1. Override the `get_event_loop` method to print a message every time it is called and return the event loop. 2. Override the `new_event_loop` method to return a new event loop and print a message indicating the event loop has been created. Additionally, write code to: 1. Set the current event loop policy to your `MyCustomEventLoopPolicy`. 2. Create and set a new event loop. 3. Fetch and print the current event loop using the overridden `get_event_loop` method. 4. Create a new child watcher (use `asyncio.ThreadedChildWatcher`) and attach it to the event loop. # Requirements: - You should use the `asyncio` module and the class `asyncio.DefaultEventLoopPolicy` as the base class for your custom policy. - The `get_event_loop` method should print `\\"Fetching current event loop\\"` before returning the event loop. - The `new_event_loop` method should print `\\"Creating a new event loop\\"` before returning the new event loop. - Your implementation should ensure that the child watcher is properly set for the event loop. # Input: There are no external inputs. You are required to write the implementation and execution code. # Output: 1. Messages printed from `get_event_loop` and `new_event_loop` methods. 2. The event loop object. 3. Indication that the child watcher has been set. # Constraints: - Only use the Python `asyncio` module. - Ensure that the overridden methods work as intended when called. # Example: ```python # Assuming your implementations policy = MyCustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) current_loop = asyncio.get_event_loop() print(current_loop) # Should print the event loop object watcher = asyncio.ThreadedChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) print(\\"Child watcher set\\") ``` # Implementation Constraints: - You must strictly adhere to using `asyncio.DefaultEventLoopPolicy` as the base class for your custom policy. - Ensure that the methods `get_event_loop` and `new_event_loop` are properly overridden and correctly integrated with `asyncio`. **Note**: Ensure your code runs without errors and the custom prints reflect in the output as described.","solution":"import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Fetching current event loop\\") return super().get_event_loop() def new_event_loop(self): print(\\"Creating a new event loop\\") return super().new_event_loop() # Setting the custom event loop policy policy = MyCustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) # Creating and setting a new event loop loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) # Fetching the current event loop current_loop = asyncio.get_event_loop() print(current_loop) # Should print the event loop object # Creating and setting a new child watcher watcher = asyncio.ThreadedChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) print(\\"Child watcher set\\")"},{"question":"**Question: Asynchronous Download Manager** You are to implement an asynchronous download manager using the `asyncio` module. The download manager will perform multiple \\"download\\" tasks concurrently, each task downloading a file whose size is specified. For simplicity, assume \\"downloading a file\\" is simulated by a sleep function where the duration of sleep is proportional to the file size. The download manager should: 1. Download multiple files concurrently. 2. Ensure that if a download takes longer than a specified timeout, it is cancelled. 3. Provide a report of which downloads completed successfully within the timeout and which were cancelled. **Requirements:** 1. Define a coroutine function `download_file(file_id: int, size: int) -> int` that simulates a file download by sleeping for `size` seconds and then returns the `file_id` if the download is successful. 2. Define a coroutine function `download_manager(file_sizes: Dict[int, int], timeout: float) -> Tuple[List[int], List[int]]` that: - Accepts a dictionary `file_sizes` where keys are file IDs and values are file sizes in seconds. - Accepts a `timeout` in seconds. - Attempts to download all files concurrently, using the `timeout` to limit the download time. - Returns a tuple of two lists: - The first list containing the IDs of files that were successfully downloaded within the timeout. - The second list containing the IDs of files that were cancelled due to timeout. **Constraints:** - Each file size is a positive integer. - The timeout is a positive float. - The total number of files will not exceed 100. **Example Usage:** ```python import asyncio from typing import Dict, List, Tuple async def download_file(file_id: int, size: int) -> int: # Simulate file download await asyncio.sleep(size) return file_id async def download_manager(file_sizes: Dict[int, int], timeout: float) -> Tuple[List[int], List[int]]: async def download_with_timeout(file_id: int, size: int) -> Tuple[int, bool]: try: file_id = await asyncio.wait_for(download_file(file_id, size), timeout) return file_id, True except asyncio.TimeoutError: return file_id, False tasks = [download_with_timeout(file_id, size) for file_id, size in file_sizes.items()] results = await asyncio.gather(*tasks) completed = [file_id for file_id, success in results if success] cancelled = [file_id for file_id, success in results if not success] return completed, cancelled # Example usage file_sizes = {1: 2, 2: 4, 3: 1} timeout = 3.0 completed, cancelled = asyncio.run(download_manager(file_sizes, timeout)) print(f\\"Completed: {completed}\\") # Outputs: Completed: [1, 3] print(f\\"Cancelled: {cancelled}\\") # Outputs: Cancelled: [2] ``` **Performance Requirements:** - The implementation should aim to complete all operations efficiently. - Ensure the download manager can handle up to 100 files without performance degradation due to task management or timeouts.","solution":"import asyncio from typing import Dict, List, Tuple async def download_file(file_id: int, size: int) -> int: Simulates a file download by sleeping for \'size\' seconds and returns the \'file_id\'. await asyncio.sleep(size) return file_id async def download_manager(file_sizes: Dict[int, int], timeout: float) -> Tuple[List[int], List[int]]: async def download_with_timeout(file_id: int, size: int) -> Tuple[int, bool]: try: file_id = await asyncio.wait_for(download_file(file_id, size), timeout) return file_id, True except asyncio.TimeoutError: return file_id, False tasks = [download_with_timeout(file_id, size) for file_id, size in file_sizes.items()] results = await asyncio.gather(*tasks) completed = [file_id for file_id, success in results if success] cancelled = [file_id for file_id, success in results if not success] return completed, cancelled"},{"question":"# Python Customization and Script Execution Challenge **Objective:** You are tasked with creating a Python script that performs the following tasks based on the concepts from interactive mode and script customization. **Problem Statement:** You need to write a Python script `custom_script.py` with the following functionalities: 1. **Environment Variable Setup**: - Define an environment variable `PYTHONSTARTUP` which points to a file `startup.py` located in the same directory as your script. - This file should initialize some variables and print a message when the Python interactive shell starts up. 2. **Startup File (`startup.py`)**: - Create a file named `startup.py` that should define a function `greet()` which prints \\"Hello, welcome to the interactive mode!\\". - Also, it should contain a variable `initialized` set to `True`. 3. **Execution Check and Customization**: - In `custom_script.py`, write code to check if `startup.py` exists and is readable. - The script should then read and execute the contents of `startup.py` using the `exec` function. - Print \\"Startup file executed successfully\\" if the contents are executed without any errors. 4. **Make the script executable on Unix-like systems**: - Ensure the first line of your `custom_script.py` contains the shebang `#!/usr/bin/env python3`. - Provide the command to make the script executable using `chmod`. 5. **Instructions for Running**: - Provide clear instructions on how to set the environment variable and run the script on both Unix-like and Windows systems. **Implementation Constraints:** - The `startup.py` file should only be read and executed if it is present and accessible. - You may assume the script is located in a writable directory for creating and modifying files. - Write your script while keeping in mind cross-platform compatibility. **Expected Output:** - When run, the `custom_script.py` should output: 1. Initialization message from the `startup.py`. 2. \\"Startup file executed successfully\\" upon successful execution. **Performance Requirements:** - The script should handle cases where `startup.py` is missing gracefully by providing an appropriate error message. - Ensure proper use of file handling and environment variable management. **Submission:** - Submit the `custom_script.py` file with appropriate comments and a brief README.md file explaining how to use the script.","solution":"#!/usr/bin/env python3 import os def main(): # Define the startup file path script_dir = os.path.dirname(os.path.realpath(__file__)) startup_file = os.path.join(script_dir, \'startup.py\') # Check if the startup file exists and is readable if os.path.isfile(startup_file) and os.access(startup_file, os.R_OK): # Read and execute the startup file with open(startup_file, \'r\') as file: exec(file.read()) print(\\"Startup file executed successfully\\") else: print(\\"Error: \'startup.py\' file not found or is not readable.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Naive Bayes Classifier Implementation and Evaluation Objective: Implement and evaluate various Naive Bayes classifiers using the scikit-learn package to classify a given dataset. This exercise will test your understanding of different Naive Bayes classifiers and how to use them effectively in Python. Background: You are provided with a dataset containing features and a target variable. Each row represents a sample. You need to split this dataset into training and testing sets. Then, you will implement Gaussian Naive Bayes, Multinomial Naive Bayes, Complement Naive Bayes, and Bernoulli Naive Bayes classifiers, and evaluate their performance. Dataset: - The dataset is provided as `data.csv`, with `n` features in columns labeled `feature_1`, `feature_2`, ..., `feature_n`, and the target variable in the column `target`. Task: 1. **Load and Split the Data:** - Load `data.csv` into a Pandas DataFrame. - Split the data into training (80%) and testing (20%) sets using `train_test_split` from scikit-learn. 2. **Train and Evaluate Classifiers:** - Implement the following classifiers using scikit-learn: - Gaussian Naive Bayes - Multinomial Naive Bayes - Complement Naive Bayes - Bernoulli Naive Bayes - Train each classifier on the training data. - Make predictions on the testing data. - Evaluate the performance using accuracy score and confusion matrix. 3. **Incremental Learning with `partial_fit`:** - Implement incremental training for Gaussian Naive Bayes classifier using the `partial_fit` method. - Use 10 chunks of the training data for this incremental fitting. 4. **Output:** - Print the accuracy score and confusion matrix for each classifier. - For the incremental learning part, print the accuracy score after fitting all chunks. Input and Output Format: - **Input:** `data.csv` file. - **Output:** Print statements with accuracy scores and confusion matrices. Constraints: - Ensure all classifiers are evaluated on the same train-test split. Example Code Outline: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix # Step 1: Load and split the data data = pd.read_csv(\'data.csv\') X = data.drop(\'target\', axis=1) y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Train and evaluate classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB() } for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f\'{name} Accuracy: {acc}\') print(f\'{name} Confusion Matrix:n{cm}\') # Step 3: Incremental learning with partial_fit for GaussianNB chunk_size = len(X_train) // 10 classes = np.unique(y_train) gnb_incremental = GaussianNB() for i in range(0, len(X_train), chunk_size): X_chunk = X_train[i:i + chunk_size] y_chunk = y_train[i:i + chunk_size] gnb_incremental.partial_fit(X_chunk, y_chunk, classes=classes) y_pred_incremental = gnb_incremental.predict(X_test) acc_incremental = accuracy_score(y_test, y_pred_incremental) print(f\'Incremental GaussianNB Accuracy: {acc_incremental}\') ```","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix def load_and_split_data(filepath): data = pd.read_csv(filepath) X = data.drop(\'target\', axis=1) y = data[\'target\'] return train_test_split(X, y, test_size=0.2, random_state=42) def train_and_evaluate(X_train, X_test, y_train, y_test): classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB() } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) results[name] = { \'accuracy\': acc, \'confusion_matrix\': cm } return results def incremental_learning(X_train, y_train, X_test, y_test): chunk_size = len(X_train) // 10 classes = np.unique(y_train) gnb_incremental = GaussianNB() for i in range(0, len(X_train), chunk_size): X_chunk = X_train[i:i + chunk_size] y_chunk = y_train[i:i + chunk_size] gnb_incremental.partial_fit(X_chunk, y_chunk, classes=classes) y_pred_incremental = gnb_incremental.predict(X_test) acc_incremental = accuracy_score(y_test, y_pred_incremental) return acc_incremental, confusion_matrix(y_test, y_pred_incremental) # Main code if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_split_data(\'data.csv\') # Train and evaluate classifiers results = train_and_evaluate(X_train, X_test, y_train, y_test) for name, metrics in results.items(): print(f\'{name} Accuracy: {metrics[\\"accuracy\\"]}\') print(f\'{name} Confusion Matrix:n{metrics[\\"confusion_matrix\\"]}\') # Incremental learning acc_incremental, cm_incremental = incremental_learning(X_train, y_train, X_test, y_test) print(f\'Incremental GaussianNB Accuracy: {acc_incremental}\') print(f\'Incremental GaussianNB Confusion Matrix:n{cm_incremental}\')"},{"question":"# Coding Assessment: Implementing a Custom Asynchronous Protocol with asyncio Objective This task assesses your understanding of the `asyncio` module in Python, specifically focusing on implementing custom network communication using Transports and Protocols. Task You are required to implement a simple asynchronous client-server application using `asyncio`\'s low-level APIs for Transports and Protocols. 1. **Echo Server:** - Create a TCP echo server that listens on a given host and port. - The server should use custom implementations of `asyncio.Protocol` for handling connections and data reception. - The server should handle multiple clients concurrently. 2. **Echo Client:** - Create a TCP echo client that connects to the server. - The client should send a message and wait for the echo response from the server. - After receiving the echo, the client should close the connection. Specifications - **Server:** - It should implement a custom `EchoServerProtocol` that inherits from `asyncio.Protocol`. - Methods to implement: - `connection_made(self, transport)`: Called when a client connects. - `data_received(self, data)`: Called when data is received from the client. The server should send back the received data (echo). - `connection_lost(self, exc)`: Called when the connection is closed. - **Client:** - It should implement a custom `EchoClientProtocol` that inherits from `asyncio.Protocol`. - Methods to implement: - `connection_made(self, transport)`: Called when connected to the server. The client should send a pre-defined message. - `data_received(self, data)`: Called when data is received from the server. The client should print the received data and close the connection. - `connection_lost(self, exc)`: Called when the connection is closed. Input and Output - The server should run on `localhost` at port `8888`. - The client should connect to this server, send the message `\\"Hello, Server!\\"`, and expect the echo message `\\"Hello, Server!\\"` in response. Constraints - Use `asyncio` for creating the asynchronous server and client. - Do not use high-level functions like `asyncio.start_server()` or `asyncio.open_connection()`. - Ensure that the server can handle multiple clients concurrently. - The client\'s message and response should be logged to the console. Example Implementation If you provide the following function: ```python async def main(): server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) ... asyncio.run(main()) ``` The client should connect and the output should be: ``` Server received: Hello, Server! Client received: Hello, Server! Server closed the connection Client closed the connection ``` Submit your implementation of both `EchoServerProtocol` and `EchoClientProtocol`, along with the main driving functions that start the server and the client. # Note Review the `asyncio` module documentation for reference implementations of Protocol and Transport.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\'Connection from\', self.transport.get_extra_info(\'peername\')) def data_received(self, data): message = data.decode() print(f\\"Server received: {message}\\") self.transport.write(data) def connection_lost(self, exc): print(\'Server closed the connection\') class EchoClientProtocol(asyncio.Protocol): def __init__(self, message, loop): self.message = message self.loop = loop def connection_made(self, transport): self.transport = transport print(f\\"Sending: {self.message}\\") self.transport.write(self.message.encode()) def data_received(self, data): print(f\\"Client received: {data.decode()}\\") self.transport.close() def connection_lost(self, exc): print(\'Client closed the connection\') self.loop.stop() async def run_server(loop): server = await loop.create_server(lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) return server async def run_client(loop): message = \\"Hello, Server!\\" transport, protocol = await loop.create_connection(lambda: EchoClientProtocol(message, loop), \'127.0.0.1\', 8888) def main(): loop = asyncio.get_event_loop() # Start the server server = loop.run_until_complete(run_server(loop)) # Run the client loop.run_until_complete(run_client(loop)) # Close the server server.close() loop.run_until_complete(server.wait_closed()) loop.run_forever() if __name__ == \'__main__\': main()"},{"question":"**XML Document Manipulation Task** # Objective You are required to create and manipulate an XML document using the `xml.dom` module in Python. This task will test your understanding of the `xml.dom` API and your ability to work with XML data structures. # Problem Statement Write a Python function `create_and_modify_xml()` that performs the following operations: 1. **Create an XML Document:** - Create a new XML document using `DOMImplementation`. - The root element of the document should be `<company>`. 2. **Add Child Elements:** - Add a child element `<employee>` to the `<company>` element. - Add the following child elements to `<employee>`: - `<name>` with text content \\"John Doe\\". - `<position>` with text content \\"Software Engineer\\". - `<department>` with text content \\"Development\\". 3. **Modify an Element:** - Change the text content of the `<position>` element to \\"Senior Software Engineer\\". 4. **Remove an Element:** - Remove the `<department>` element from the `<employee>` element. 5. **Add an Attribute:** - Add an attribute `id` with value \\"E001\\" to the `<employee>` element. 6. **Output the XML Document:** - Convert the document to a string and return it. # Input and Output - **Input:** None - **Output:** A string representing the modified XML document. # Constraints - The document structure should be created entirely using the `xml.dom` API. - The function should handle creating, modifying, and removing elements and attributes as specified. # Example Output ```xml <company> <employee id=\\"E001\\"> <name>John Doe</name> <position>Senior Software Engineer</position> </employee> </company> ``` # Function Signature ```python def create_and_modify_xml() -> str: pass ``` # Performance Requirements - The function should efficiently create and manipulate the XML document. - The function will be tested with a single document creation and manipulation scenario. # Sample Code ```python def create_and_modify_xml() -> str: from xml.dom.minidom import getDOMImplementation # Create a new DOM document impl = getDOMImplementation() doc = impl.createDocument(None, \\"company\\", None) # Add child elements company = doc.documentElement employee = doc.createElement(\\"employee\\") company.appendChild(employee) name = doc.createElement(\\"name\\") name.appendChild(doc.createTextNode(\\"John Doe\\")) employee.appendChild(name) position = doc.createElement(\\"position\\") position.appendChild(doc.createTextNode(\\"Software Engineer\\")) employee.appendChild(position) department = doc.createElement(\\"department\\") department.appendChild(doc.createTextNode(\\"Development\\")) employee.appendChild(department) # Modify an element position.firstChild.nodeValue = \\"Senior Software Engineer\\" # Remove an element employee.removeChild(department) # Add an attribute employee.setAttribute(\\"id\\", \\"E001\\") # Convert the document to a string return doc.toprettyxml(indent=\\" \\") ``` # Note - You may use the `xml.dom.minidom` module for simplicity. - Ensure that the formatting of the output XML string is as specified in the example output.","solution":"def create_and_modify_xml() -> str: from xml.dom.minidom import getDOMImplementation # Create a new DOM document impl = getDOMImplementation() doc = impl.createDocument(None, \\"company\\", None) # Add child elements company = doc.documentElement employee = doc.createElement(\\"employee\\") company.appendChild(employee) name = doc.createElement(\\"name\\") name.appendChild(doc.createTextNode(\\"John Doe\\")) employee.appendChild(name) position = doc.createElement(\\"position\\") position.appendChild(doc.createTextNode(\\"Software Engineer\\")) employee.appendChild(position) department = doc.createElement(\\"department\\") department.appendChild(doc.createTextNode(\\"Development\\")) employee.appendChild(department) # Modify an element position.firstChild.nodeValue = \\"Senior Software Engineer\\" # Remove an element employee.removeChild(department) # Add an attribute employee.setAttribute(\\"id\\", \\"E001\\") # Convert the document to a string return doc.toprettyxml(indent=\\" \\")"},{"question":"# Unix Group Information Processing Task **Problem Description:** You are tasked with developing a utility function in Python that processes Unix group information using the `grp` module. The function should identify and return groups where the specified user is a member. **Function Signature:** ```python def find_user_groups(username: str) -> list: ``` **Input:** - `username` (str): A string representing the username to search for within the group database. **Output:** - A list of tuples where each tuple contains the group name and group ID (`gr_name`, `gr_gid`) for groups that the specified user is a member of. - Example: `[(\'group1\', 1001), (\'group2\', 1002)]` **Constraints:** - You may assume the `username` provided is a valid string. - The function should handle cases where the user is not a member of any group by returning an empty list. - The function should use the `grp` module to retrieve group information. **Example:** ```python # Assuming the Unix group database contains the following groups: # group name: \\"admin\\", group ID: 1001, members: [\\"user1\\", \\"user2\\"] # group name: \\"staff\\", group ID: 1002, members: [\\"user2\\", \\"user3\\"] # group name: \\"guest\\", group ID: 1003, members: [] # Example call to the function print(find_user_groups(\\"user2\\")) # Expected output [(\'admin\', 1001), (\'staff\', 1002)] ``` **Your task is to implement the function `find_user_groups` to fulfill the above requirements and pass the example given. Ensure that it efficiently queries the group database and correctly identifies group memberships.**","solution":"import grp def find_user_groups(username: str) -> list: Identifies and returns the groups where the specified user is a member. :param username: A string representing the username to search for. :return: A list of tuples (group name, group ID) for groups that the user is a member of. user_groups = [] for group in grp.getgrall(): if username in group.gr_mem: user_groups.append((group.gr_name, group.gr_gid)) return user_groups"},{"question":"# Python Logging Challenge Your task is to configure a robust logging system for a Python application. Your logging setup should include multiple loggers with different levels, handlers, and formatters. You will also create a custom log level and include context-specific information using a custom LogRecord factory. Requirements: 1. **Loggers**: * Create two loggers: - `main_logger` that logs messages with severity \\"INFO\\" and higher. - `module_logger` that logs messages with severity \\"DEBUG\\" and higher. * The `main_logger` should propagate messages to the root logger, while `module_logger` should not propagate. 2. **Handlers and Formatters**: * Attach a `StreamHandler` to both loggers, which outputs to console. * Attach a `FileHandler` to `main_logger`, which writes logs to `main.log`. Use the default mode (\'a\') for appending logs. * Customize the formatter for both handlers to include the following fields: `asctime`, `levelname`, `name`, and `message`. The format should be: `%(asctime)s - %(levelname)s - %(name)s - %(message)s`. * For `StreamHandler`, use a different date format: `%H:%M:%S`. 3. **Custom Log Level**: * Define a custom log level called `VERBOSE` with severity 15 (between DEBUG and INFO). * Create a method `log_verbose` for `module_logger` to log at this custom level. 4. **Custom LogRecord Factory**: * Implement a custom `LogRecord` factory function that adds an additional attribute called `user`, defaulting to `\'unknown\'`. * Update the `Formatter` to include this custom attribute in the log message. The updated format should be: `%(asctime)s - %(levelname)s - %(name)s - [User: %(user)s] - %(message)s`. 5. **Logging Usage**: * Create a function `simulate_logging` that demonstrates logging messages of all levels (including the custom `VERBOSE` level) for both loggers. * Demonstrate the `propagate` behavior by showing how `main_logger` logs are handled by the root logger, while `module_logger` logs are self-contained. Example Output: Refer to the format specified in the requirements for the expected content and appearance of logs in the console and file. Use the following code skeleton to implement your solution: ```python import logging # Step 1: Create loggers main_logger = logging.getLogger(\'main\') module_logger = logging.getLogger(\'module\') # Step 2: Set levels and propagation main_logger.setLevel(logging.INFO) module_logger.setLevel(logging.DEBUG) main_logger.propagate = True module_logger.propagate = False # Step 3: Define custom LOG LEVEL VERBOSE = 15 logging.addLevelName(VERBOSE, \\"VERBOSE\\") def log_verbose(self, message, *args, **kwargs): if self.isEnabledFor(VERBOSE): self._log(VERBOSE, message, args, **kwargs) # Attaching custom log method to the logger logging.Logger.verbose = log_verbose # Step 4: Custom LogRecord factory old_factory = logging.getLogRecordFactory() def record_factory(*args, **kwargs): record = old_factory(*args, **kwargs) record.user = \'unknown\' return record logging.setLogRecordFactory(record_factory) # Step 5: Create handlers and formatters stream_handler = logging.StreamHandler() file_handler = logging.FileHandler(\'main.log\') formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - [User: %(user)s] - %(message)s\') stream_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - [User: %(user)s] - %(message)s\', datefmt=\'%H:%M:%S\') stream_handler.setFormatter(stream_formatter) file_handler.setFormatter(formatter) main_logger.addHandler(stream_handler) main_logger.addHandler(file_handler) module_logger.addHandler(stream_handler) # Step 6: Simulate logging def simulate_logging(): main_logger.info(\'This is an info message from main_logger\') main_logger.error(\'This is an error message from main_logger\') module_logger.debug(\'This is a debug message from module_logger\') module_logger.error(\'This is an error message from module_logger\') module_logger.verbose(\'This is a verbose message from module_logger\') simulate_logging() ``` Ensure the logging setup produces the expected output, both in the console and in the `main.log` file. Additional Constraints: * Modify only the attached code skeleton to complete the task. * Do not change the import statements or the function signatures provided above. * Ensure your implementation adheres to Python\'s PEP 8 style guidelines.","solution":"import logging # Step 1: Create loggers main_logger = logging.getLogger(\'main\') module_logger = logging.getLogger(\'module\') # Step 2: Set levels and propagation main_logger.setLevel(logging.INFO) module_logger.setLevel(logging.DEBUG) main_logger.propagate = True module_logger.propagate = False # Step 3: Define custom LOG LEVEL VERBOSE = 15 logging.addLevelName(VERBOSE, \\"VERBOSE\\") def log_verbose(self, message, *args, **kwargs): if self.isEnabledFor(VERBOSE): self._log(VERBOSE, message, args, **kwargs) # Attaching custom log method to the logger logging.Logger.verbose = log_verbose # Step 4: Custom LogRecord factory old_factory = logging.getLogRecordFactory() def record_factory(*args, **kwargs): record = old_factory(*args, **kwargs) record.user = \'unknown\' return record logging.setLogRecordFactory(record_factory) # Step 5: Create handlers and formatters stream_handler = logging.StreamHandler() file_handler = logging.FileHandler(\'main.log\') formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - [User: %(user)s] - %(message)s\') stream_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - [User: %(user)s] - %(message)s\', datefmt=\'%H:%M:%S\') stream_handler.setFormatter(stream_formatter) file_handler.setFormatter(formatter) main_logger.addHandler(stream_handler) main_logger.addHandler(file_handler) module_logger.addHandler(stream_handler) # Step 6: Simulate logging def simulate_logging(): main_logger.info(\'This is an info message from main_logger\') main_logger.error(\'This is an error message from main_logger\') module_logger.debug(\'This is a debug message from module_logger\') module_logger.error(\'This is an error message from module_logger\') module_logger.verbose(\'This is a verbose message from module_logger\') simulate_logging()"},{"question":"# Understanding and Working with Custom Mapping Objects in Python Python provides an extensive C API for working with mapping objects. For this exercise, you will create a custom mapping object in Python and implement various methods to simulate the behavior of these functions. Part 1: Custom Mapping Class Create a class `CustomMapping` in Python which should act like a dictionary, supporting the following operations: 1. **Initialization**: Initialize the mapping object, allowing to add key-value pairs. 2. **Check if Mapping**: Implement a method `is_mapping()` to check if the object is a mapping type (`__getitem__` exists). 3. **Size**: Implement a method `size()` to get the number of key-value pairs. 4. **Get Item**: Implement a method `get_item(key)` to get the value for a given key. 5. **Set Item**: Implement a method `set_item(key, value)` to set the value for a given key. 6. **Delete Item**: Implement a method `del_item(key)` to delete the value for a given key. 7. **Has Key**: Implement a method `has_key(key)` to check if a key is present. 8. **Keys**: Implement a method `keys()` to return all the keys. 9. **Values**: Implement a method `values()` to return all the values. 10. **Items**: Implement a method `items()` to return all key-value pairs. Example Usage: ```python # Example Usage mapping = CustomMapping() mapping.set_item(\\"apple\\", 10) mapping.set_item(\\"banana\\", 20) print(mapping.get_item(\\"apple\\")) # Output: 10 print(mapping.size()) # Output: 2 print(mapping.has_key(\\"banana\\")) # Output: True print(mapping.keys()) # Output: [\\"apple\\", \\"banana\\"] print(mapping.values()) # Output: [10, 20] print(mapping.items()) # Output: [(\\"apple\\", 10), (\\"banana\\", 20)] mapping.del_item(\\"apple\\") print(mapping.size()) # Output: 1 ``` Implement the `CustomMapping` class to handle all of the operations described above. **Constraints:** - You should not use Python\'s built-in dictionary methods directly (e.g., `dict.get`, `dict.keys()` etc.) - Focus on implementing the methods using core Python data structures and concepts.","solution":"class CustomMapping: def __init__(self): Initialize the CustomMapping object with an empty list to store key-value pairs. self._data = [] def is_mapping(self): Return True as it acts like a mapping type. return True def size(self): Return the number of key-value pairs. return len(self._data) def get_item(self, key): Get the value associated with the given key. for k, v in self._data: if k == key: return v raise KeyError(f\'Key {key} not found.\') def set_item(self, key, value): Set the value for the given key, update if key exists. for index, (k, v) in enumerate(self._data): if k == key: self._data[index] = (key, value) return self._data.append((key, value)) def del_item(self, key): Delete the value associated with the given key. for index, (k, v) in enumerate(self._data): if k == key: del self._data[index] return raise KeyError(f\'Key {key} not found.\') def has_key(self, key): Check if a key is present in the mapping. for k, _ in self._data: if k == key: return True return False def keys(self): Return a list of all keys in the mapping. return [k for k, _ in self._data] def values(self): Return a list of all values in the mapping. return [v for k, v in self._data] def items(self): Return a list of all key-value pairs (tuples) in the mapping. return self._data.copy()"},{"question":"**Question** # Asyncio Subprocess Management In this exercise, you will demonstrate your understanding of the asyncio package by creating and managing subprocesses. # Problem Statement You are required to implement a Python function `run_commands(commands: List[str]) -> Optional[str]` that takes a list of shell commands and executes them concurrently. The function should: 1. Execute all the commands concurrently. 2. Capture the stdout and stderr of each command. 3. Return the first non-empty stderr output found among the commands. If none of the commands produce stderr output, return `None`. # Function Signature ```python import asyncio from typing import List, Optional async def run_commands(commands: List[str]) -> Optional[str]: pass ``` # Requirements 1. **Input**: - `commands`: a list of shell commands to be executed. Each command is a string (`List[str]`). 2. **Output**: - Return the first non-empty stderr output (`str`) if any of the commands produce stderr output, otherwise return `None`. 3. **Constraints**: - Ensure the function is implemented asynchronously using `asyncio`. - Commands should be executed concurrently to maximize efficiency. - Use appropriate mechanisms to avoid deadlocks when reading from stdout and stderr. # Example ```python import asyncio from typing import List, Optional async def run_commands(commands: List[str]) -> Optional[str]: async def execute(cmd: str) -> Optional[str]: proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() if stderr: return stderr.decode() return None tasks = [execute(cmd) for cmd in commands] for task in asyncio.as_completed(tasks): result = await task if result: return result return None # Usage commands = [\\"echo \'This is stdout\'\\", \\"ls /zzz\\", \\"echo \'no error here\'\\"] first_error = asyncio.run(run_commands(commands)) print(first_error) # Expected Output: \\"ls: /zzz: No such file or directory\\" ``` # Notes - Use `asyncio.create_subprocess_shell` to execute the commands. - Use `asyncio.subprocess.PIPE` to capture the stdout and stderr. - `asyncio.as_completed` should help in returning the result of completed tasks as soon as they are ready. With this task, you will demonstrate your ability to work with asyncio, manage concurrent tasks, and handle subprocesses efficiently in Python.","solution":"import asyncio from typing import List, Optional async def run_commands(commands: List[str]) -> Optional[str]: async def execute(cmd: str) -> Optional[str]: proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() if stderr: return stderr.decode() return None tasks = [execute(cmd) for cmd in commands] for task in asyncio.as_completed(tasks): result = await task if result: return result return None # Usage commands = [\\"echo \'This is stdout\'\\", \\"ls /zzz\\", \\"echo \'no error here\'\\"] first_error = asyncio.run(run_commands(commands)) print(first_error) # Expected Output: \\"ls: /zzz: No such file or directory\\""},{"question":"# PyTorch Coding Assessment: Conditional Tensor Operations **Objective**: Implement a PyTorch module that utilizes the `torch.cond` function to perform different operations based on the properties of the input tensor. # Problem Statement Create a PyTorch module named `ConditionalOperation` with a method `forward` that performs different operations on the input tensor based on the following condition: - If the sum of all elements in the input tensor is greater than a specified threshold, apply the `true_fn`. - Otherwise, apply the `false_fn`. # Instructions 1. **Class Definition**: - Define a class `ConditionalOperation` that inherits from `torch.nn.Module`. 2. **Initialization**: - The constructor (`__init__`) should take a single argument `threshold` which is a float value. 3. **Forward Method**: - Implement the `forward` method which takes a single input, a tensor `x`. - The `forward` method should use `torch.cond` to apply: - `true_fn` if `x.sum() > threshold` evaluates to true. - `false_fn` otherwise. 4. **True Function (`true_fn`)**: - Define `true_fn` to return `x * 2`. 5. **False Function (`false_fn`)**: - Define `false_fn` to return `x + 2`. # Input - The `forward` method will take a single tensor input `x` with any shape and data type. # Output - The method should return a tensor where the operation applied is determined by the sum of the elements in `x` compared to `threshold`. # Example ```python import torch class ConditionalOperation(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x * 2 def false_fn(x: torch.Tensor): return x + 2 return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) # Initialize the conditional operation with a threshold of 10.0 model = ConditionalOperation(threshold=10.0) # Test with inputs inp1 = torch.tensor([1.0, 2.0, 3.0]) inp2 = torch.tensor([5.0, 6.0, 7.0]) # Expected Output: tensor([3., 4., 5.]) because 1 + 2 + 3 <= 10 print(model(inp1)) # Expected Output: tensor([10., 12., 14.]) because 5 + 6 + 7 > 10 print(model(inp2)) ``` # Constraints - You may assume that the input tensor will always be valid. - Focus on correctly using the `torch.cond` function to implement the described behavior. - Ensure your implementation handles tensors of any shape and supports both positive and negative values. # Performance Requirements - Your implementation should efficiently handle large tensors and process them within a reasonable time frame.","solution":"import torch.nn as nn import torch class ConditionalOperation(nn.Module): def __init__(self, threshold: float): super(ConditionalOperation, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: if x.sum() > self.threshold: return x * 2 else: return x + 2"},{"question":"# ASCII Character Analysis and Transformation You are tasked with developing a text analysis tool that leverages the `curses.ascii` module for handling ASCII characters. Your tool should read an input string and perform the following steps: 1. **Character Classification**: Classify each character in the input string into one of the following categories: - Alphanumeric (use `isalnum`) - Alphabetical (use `isalpha`) - ASCII whitespace (use `isspace`) - ASCII control characters (use `isctrl`) - Printable (use `isprint`, exclude characters categorized as whitespace) 2. **Transformation**: For each control character found, transform it using the following rules: - Convert to its corresponding control key representation using `ctrl` function. - If the character is \'ESC\', \'EOT\', or \'BEL\', replace it with a human-readable format (e.g., replace \'ESC\' with \\"<ESC>\\"). 3. **Summarize**: Return a summary dictionary with the counts of each category and a processed version of the input string where all control characters are replaced as specified. Input: - A single string containing ASCII characters. Output: - A dictionary with: - `classifications`: A dictionary with counts of each character category. - `transformed_string`: A string with transformed control characters as specified. Function Signature: ```python def ascii_analysis_transform(input_string: str) -> dict: pass ``` # Example: ```python input_string = \\"Hello, World!nThis is a test string.x07\\" output = ascii_analysis_transform(input_string) # Expected output format expected_output = { \\"classifications\\": { \\"alphanumeric\\": 21, \\"alphabetical\\": 19, \\"ascii_whitespace\\": 5, \\"ascii_control\\": 2, \\"printable\\": 19, }, \\"transformed_string\\": \\"Hello, World!<LF>This is a test string.<BEL>\\" } assert output == expected_output ``` - `n` (Line Feed) should be replaced by `<LF>`. - `x07` (BEL) should be replaced by `<BEL>`. Constraints: - The input string is guaranteed to contain only ASCII characters. - Treat all classifications as mutually exclusive and prioritize in the following order: 1. Control Characters 2. Whitespace 3. Alphanumeric (overrides Alphabetical if both apply) 4. Printable Notes: - You are encouraged to utilize the `curses.ascii` module constants and functions. - Consider edge cases like empty strings and long strings containing a mixture of various ASCII character types.","solution":"import curses.ascii def ascii_analysis_transform(input_string: str) -> dict: classifications = { \\"alphanumeric\\": 0, \\"alphabetical\\": 0, \\"ascii_whitespace\\": 0, \\"ascii_control\\": 0, \\"printable\\": 0, } transformed_string = [] for char in input_string: if curses.ascii.isctrl(char): classifications[\\"ascii_control\\"] += 1 if char == \'x1b\': # ESC transformed_string.append(\\"<ESC>\\") elif char == \'x04\': # EOT transformed_string.append(\\"<EOT>\\") elif char == \'x07\': # BEL transformed_string.append(\\"<BEL>\\") elif char == \'n\': # LF transformed_string.append(\\"<LF>\\") else: transformed_string.append(curses.ascii.ctrl(char)) elif curses.ascii.isspace(char): classifications[\\"ascii_whitespace\\"] += 1 transformed_string.append(char) elif curses.ascii.isalnum(char): classifications[\\"alphanumeric\\"] += 1 if curses.ascii.isalpha(char): classifications[\\"alphabetical\\"] += 1 transformed_string.append(char) elif curses.ascii.isprint(char): classifications[\\"printable\\"] += 1 transformed_string.append(char) return { \\"classifications\\": classifications, \\"transformed_string\\": \'\'.join(transformed_string) }"},{"question":"# URL Processing with urllib.parse You are given a list of URLs. Your task is to parse each URL, extract specific components, and process the query strings as specified. You will then construct a modified URL using the extracted and processed components. Task: 1. For each URL in the list, parse the URL to extract the following components: - Scheme - Network location (netloc) - Path - Query - Fragment 2. If the scheme is missing, add \'http\' as the default scheme. 3. For each extracted query, parse it into a dictionary using `parse_qs()`. Remove any query parameters that have empty values. 4. Reconstruct each URL using the updated query parameters and the components extracted in step 1. Ensure that no fragment identifiers are included in the final URL. Function Signature: ```python def process_urls(urls: List[str]) -> List[str]: ``` Input: - `urls`: A list of strings, where each string is a URL. Output: - Returns a list of modified URL strings, processed as described above. Constraints: - Each URL string can have up to 2000 characters. - The list of URLs can have up to 100 URLs. Example: ```python urls = [ \\"http://example.com/path/to/page?name=ferret&color=purple#section1\\", \\"//example.com/path2?name=&color=green\\", \\"ftp://ftp.example.com/\\" ] result = process_urls(urls) print(result) ``` Expected Output: ```python [ \'http://example.com/path/to/page?name=ferret&color=purple\', \'http://example.com/path2?color=green\', \'ftp://ftp.example.com/\' ] ``` Explanation: 1. In the first URL, the fragment (\'#section1\') is removed. 2. In the second URL, the scheme is added (\'http://\') and the empty \'name\' parameter is removed. 3. The third URL remains unchanged as it has the proper scheme and no query parameters. Implement the `process_urls` function to achieve the desired output.","solution":"from urllib.parse import urlparse, parse_qs, urlencode, urlunparse def process_urls(urls): processed_urls = [] for url in urls: # Parse the URL parsed_url = urlparse(url, scheme=\'http\') # Parse the query parameters query_dict = parse_qs(parsed_url.query) # Remove empty query parameters cleaned_query_dict = {k: v for k, v in query_dict.items() if v != [\'\']} # Reconstruct the query string cleaned_query = urlencode(cleaned_query_dict, doseq=True) # Construct the final URL final_url = urlunparse(( parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, cleaned_query, \'\' )) processed_urls.append(final_url) return processed_urls"},{"question":"**Objective:** Create a subclass of `reprlib.Repr` to customize the behavior of `repr()` for certain custom object types and demonstrate the use of recursive handling in `__repr__` methods. **Problem Statement:** You are required to design a custom `Repr` class that extends the functionality of `reprlib.Repr` to handle two new custom object types: `Person` and `Company`. Additionally, you need to ensure that recursive structures are managed properly using the `@reprlib.recursive_repr` decorator. **Definitions:** 1. **`Person` class**: - Attributes: `name` (string), `age` (integer), `friends` (list of `Person` objects). 2. **`Company` class**: - Attributes: `name` (string), `employees` (list of `Person` objects). **Requirements:** 1. Subclass `reprlib.Repr` to create `CustomRepr`. 2. Implement the following custom representation methods in `CustomRepr`: - `repr_Person(obj, level)` to provide a limited representation of `Person` objects. - Ensure that the representation of `Person` objects truncates the `friends` list to a maximum of 3 friends. - `repr_Company(obj, level)` to provide a limited representation of `Company` objects. - Ensure that the representation of `Company` objects truncates the `employees` list to a maximum of 5 employees. 3. Use the `@reprlib.recursive_repr` decorator to handle recursive references in `Person` objects. 4. Override the `repr()` method of `CustomRepr` to use these custom representations. **Input and Output:** - Input: Instances of `Person` and `Company` classes. - Output: A string representation of the instances, adhering to the constraints described. **Constraints:** - Define reasonable default limits for attributes not explicitly constrained (e.g., default max length for `name` is 20 characters). - Provide meaningful string representations that indicate when lists are truncated. **Example Implementation:** ```python import reprlib class Person: def __init__(self, name, age): self.name = name self.age = age self.friends = [] def add_friend(self, friend): self.friends.append(friend) @reprlib.recursive_repr() def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, friends={self.friends})\\" class Company: def __init__(self, name): self.name = name self.employees = [] def add_employee(self, employee): self.employees.append(employee) def __repr__(self): return f\\"Company(name={self.name}, employees={self.employees})\\" class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlevel = 3 # set reasonable defaults if needed def repr_Person(self, obj, level): if level <= 0: return \'...\' name = repr(obj.name)[:20] + (\'...\' if len(repr(obj.name)) > 20 else \'\') age = repr(obj.age) friends = [self.repr1(friend, level - 1) for friend in obj.friends[:3]] if len(obj.friends) > 3: friends.append(\'...\') return f\\"Person(name={name}, age={age}, friends={friends})\\" def repr_Company(self, obj, level): if level <= 0: return \'...\' name = repr(obj.name)[:20] + (\'...\' if len(repr(obj.name)) > 20 else \'\') employees = [self.repr1(emp, level - 1) for emp in obj.employees[:5]] if len(obj.employees) > 5: employees.append(\'...\') return f\\"Company(name={name}, employees={employees})\\" custom_repr = CustomRepr() # testing p1 = Person(\'Alice\', 30) p2 = Person(\'Bob\', 25) p3 = Person(\'Charlie\', 32) p4 = Person(\'Dave\', 20) p5 = Person(\'Eve\', 29) p6 = Person(\'Frank\', 27) p1.add_friend(p2) p1.add_friend(p3) p1.add_friend(p4) p1.add_friend(p1) # recursive friendship p2.add_friend(p5) p2.add_friend(p6) c = Company(\'Tech Corp\') c.add_employee(p1) c.add_employee(p2) c.add_employee(p3) c.add_employee(p4) c.add_employee(p5) c.add_employee(p6) print(custom_repr.repr(p1)) print(custom_repr.repr(c)) ``` This task assesses the student\'s understanding of subclassing, custom object representations, recursive handling, and applying constraints using `reprlib`.","solution":"import reprlib class Person: def __init__(self, name, age): self.name = name self.age = age self.friends = [] def add_friend(self, friend): self.friends.append(friend) @reprlib.recursive_repr() def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, friends={self.friends})\\" class Company: def __init__(self, name): self.name = name self.employees = [] def add_employee(self, employee): self.employees.append(employee) def __repr__(self): return f\\"Company(name={self.name}, employees={self.employees})\\" class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlevel = 3 # set reasonable defaults if needed def repr_Person(self, obj, level): if level <= 0: return \'...\' name = repr(obj.name)[:20] + (\'...\' if len(repr(obj.name)) > 20 else \'\') age = repr(obj.age) friends = [self.repr1(friend, level - 1) for friend in obj.friends[:3]] if len(obj.friends) > 3: friends.append(\'...\') return f\\"Person(name={name}, age={age}, friends={friends})\\" def repr_Company(self, obj, level): if level <= 0: return \'...\' name = repr(obj.name)[:20] + (\'...\' if len(repr(obj.name)) > 20 else \'\') employees = [self.repr1(emp, level - 1) for emp in obj.employees[:5]] if len(obj.employees) > 5: employees.append(\'...\') return f\\"Company(name={name}, employees={employees})\\" custom_repr = CustomRepr() # Testing instances created for testing purposes p1 = Person(\'Alice\', 30) p2 = Person(\'Bob\', 25) p3 = Person(\'Charlie\', 32) p4 = Person(\'Dave\', 20) p5 = Person(\'Eve\', 29) p6 = Person(\'Frank\', 27) p1.add_friend(p2) p1.add_friend(p3) p1.add_friend(p4) p1.add_friend(p1) # recursive friendship p2.add_friend(p5) p2.add_friend(p6) c = Company(\'Tech Corp\') c.add_employee(p1) c.add_employee(p2) c.add_employee(p3) c.add_employee(p4) c.add_employee(p5) c.add_employee(p6) print(custom_repr.repr(p1)) print(custom_repr.repr(c))"},{"question":"**Question:** **Clustering Algorithm Selection and Implementation** You are given a dataset `data.csv` containing various attributes for a set of observations. Your task is to implement a Python function that: 1. Loads the dataset and performs necessary preprocessing. 2. Selects and applies the most appropriate clustering algorithm based on the dataset\'s characteristics described below. 3. Evaluates the clustering performance using Adjusted Rand Index, Silhouette Score, and Calinski-Harabasz Index. 4. Returns the cluster labels and performance metrics. **Dataset Characteristics:** - The dataset contains numerical attributes. - There are no missing values. - The shape of the dataset is `(300, 5)`. - The dataset is known to have clusters that are of various shapes and sizes, including non-convex clusters. **Detailed Requirements:** 1. **Loading and Preprocessing:** - Read the `data.csv` file using `pandas`. - Store the data in a DataFrame. 2. **Selecting the Clustering Algorithm:** - Based on the dataset characteristics, choose the clustering algorithm from the following: * K-Means * DBSCAN * Spectral Clustering 3. **Implementation:** - Implement the chosen clustering algorithm using `scikit-learn`. - Fit the model on the dataset and obtain cluster labels. 4. **Evaluation:** - Compute the following clustering evaluation metrics: * Adjusted Rand Index * Silhouette Score * Calinski-Harabasz Index - Use the ground truth cluster labels if provided (they will be included as a column named \'labels\' if available). 5. **Output:** - Return a dictionary containing: * `cluster_labels`: List of cluster labels for each observation. * `adjusted_rand_index`: Value of the Adjusted Rand Index. * `silhouette_score`: Value of the Silhouette Score. * `calinski_harabasz_index`: Value of the Calinski-Harabasz Index. **Constraints:** - You may assume that the dataset is correctly formatted and does not contain null values. - You do not need to perform hyperparameter tuning; default values from `scikit-learn` will be sufficient. - The evaluation metrics should be calculated using scikit-learn\'s built-in functions. **Function Signature:** ```python import pandas as pd from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score def cluster_and_evaluate(data_path: str) -> dict: Load dataset, select and apply appropriate clustering algorithm, evaluate clustering performance, and return results. Parameters: data_path (str): Path to the input data file \'data.csv\'. Returns: dict: Dictionary containing cluster labels and performance metrics. # Load the data data = pd.read_csv(data_path) # Extract features and (if available) ground truth labels X = data.drop(columns=[\'labels\'], errors=\'ignore\') ground_truth_labels = data[\'labels\'] if \'labels\' in data.columns else None # Choose the appropriate clustering algorithm based on dataset characteristics # Implement clustering # Evaluate clustering performance # Return results return { \\"cluster_labels\\": cluster_labels, \\"adjusted_rand_index\\": ari, \\"silhouette_score\\": silhouette, \\"calinski_harabasz_index\\": ch_index, } ``` **Example Usage:** ```python results = cluster_and_evaluate(\\"data.csv\\") print(results) ``` **Note:** Ensure that your code is well-structured, and use appropriate comments to explain your logic.","solution":"import pandas as pd from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score from sklearn.cluster import DBSCAN def cluster_and_evaluate(data_path: str) -> dict: Load dataset, select and apply appropriate clustering algorithm, evaluate clustering performance, and return results. Parameters: data_path (str): Path to the input data file \'data.csv\'. Returns: dict: Dictionary containing cluster labels and performance metrics. # Load the dataset data = pd.read_csv(data_path) # Extract features and (if available) ground truth labels X = data.drop(columns=[\'labels\'], errors=\'ignore\') ground_truth_labels = data[\'labels\'] if \'labels\' in data.columns else None # Apply DBSCAN for clustering due to known varying shapes and sizes of clusters clustering_model = DBSCAN() cluster_labels = clustering_model.fit_predict(X) # Initialize evaluation metrics ari = adjusted_rand_score(ground_truth_labels, cluster_labels) if ground_truth_labels is not None else None silhouette = silhouette_score(X, cluster_labels) if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels) else None ch_index = calinski_harabasz_score(X, cluster_labels) if len(set(cluster_labels)) > 1 else None # Return the results as a dictionary return { \\"cluster_labels\\": list(cluster_labels), \\"adjusted_rand_index\\": ari, \\"silhouette_score\\": silhouette, \\"calinski_harabasz_index\\": ch_index, }"},{"question":"Objective: You are tasked with implementing a function that can execute a series of Python simple statements within a given context and return the result of the last expression statement executed. This will test your understanding of various simple statements in Python and their execution within a controlled environment. Problem Statement: Implement a function `execute_statements(statements: List[str], context: Optional[Dict[str, Any]] = None) -> Any` that takes in a list of strings representing Python simple statements and an optional dictionary representing the context (variables and their values). The function should execute each of the statements in the given context in the order they are provided. The function should return the result of the last expression statement executed, or `None` if there are no expression statements or if the last statement is not an expression statement. Requirements: 1. The function should handle the following types of statements: - Expression statements - Assignment statements - Augmented assignment statements - Assertion statements - Pass statement - Deletion statements - Return statements (should break execution and return the result immediately) - Yield statements (you can assume there will be no yield statements in this simplified problem) - Raise statements - Break and continue statements (you can assume there will be no loops involving these in this simplified problem) - Import statements - Future statements (you can ignore these for this problem) - Global and Nonlocal statements (you can ignore these for this problem) 2. The function should modify the context as needed based on the executed statements. 3. The function should handle exceptions gracefully and return an appropriate error message if an exception is raised. Input: - `statements` (List[str]): A list of strings where each string is a Python simple statement. - `context` (Optional[Dict[str, Any]]): An optional dictionary representing the initial context. Output: - The result of the last expression statement executed, or `None` if there are no expression statements or if the last statement is not an expression statement. Example: ```python # Given initial context context = { \\"a\\": 1, \\"b\\": 2 } # List of simple statements statements = [ \\"a = a + b\\", \\"b += 3\\", \\"assert a == 3\\", \\"result = a * b\\", \\"result\\" ] # Function call result = execute_statements(statements, context) # Expected output print(result) # Output should be 9 (as 3 * 3 = 9) ``` Constraints: - You may assume that the input statements are valid Python simple statements. - You do not need to handle complex statements or multiline statements. - The `return` statement should cause the function to return immediately with the specified value. - You should not use the `exec` function due to security risks. Instead, use safer alternatives like `eval` where appropriate. Performance: - The function should be able to handle up to 10000 statements efficiently. ```python def execute_statements(statements, context=None): # Your implementation here ```","solution":"def execute_statements(statements, context=None): Executes a series of Python simple statements within a given context and returns the result of the last expression statement executed. :param statements: List of strings representing Python simple statements. :param context: Optional dictionary representing the initial context. :return: Result of the last expression statement executed or None. if context is None: context = {} local_context = context # Use the provided context to maintain variable state result = None # This will store the result of the last expression statement for statement in statements: try: if \\"=\\" in statement or \\"assert\\" in statement or \\"del\\" in statement or \\"import\\" in statement or \\"raise\\" in statement: # Handle assignment, assert, deletion, import, and raise statements (including augmented assignments) exec(statement, {}, local_context) else: # Assume other statements are expressions and evaluate them result = eval(statement, {}, local_context) except Exception as e: return str(e) return result"},{"question":"# Asyncio Synchronization Primitives Assessment You are required to implement a parking lot simulator using asyncio primitives in Python. The parking lot has a limited number of parking spaces, and multiple cars (represented as asyncio tasks) will attempt to park in these spaces. Additionally, at regular intervals, an event signaling system will notify all waiting cars that a certain number of new spots have opened up. Your task includes: 1. Implementing the parking lot using `asyncio.Semaphore` for managing the parking spots. 2. Using an `asyncio.Event` to signal the availability of newly opened spots. 3. Ensuring mutual exclusion using `asyncio.Lock` if necessary. # Specifications 1. **Input**: - `total_spots`: Total number of parking spots in the parking lot. - `num_cars`: Number of cars attempting to park. - `new_spots`: Number of new spots opening up at every interval. - `arrival_time`: Time (in seconds) each car takes before attempting to park. - `interval`: Time (in seconds) between new spot openings. 2. **Output**: - Print statements that indicate when each car parks and when new spots open up. 3. **Constraints**: - The number of cars (`num_cars`) will always be greater than or equal to `total_spots`. - The number of spots opened at each interval (`new_spots`) will always be a positive integer less than or equal to `total_spots`. # Example ```python import asyncio class ParkingLot: def __init__(self, total_spots, interval, new_spots): self.available_spots = asyncio.Semaphore(total_spots) self.new_spots_event = asyncio.Event() self.total_spots = total_spots self.interval = interval self.new_spots = new_spots async def car_park(self, car_id, arrival_time): await asyncio.sleep(arrival_time) print(f\'Car {car_id} arriving at the parking lot.\') async with self.available_spots: print(f\'Car {car_id} is parking.\') await self.new_spots_event.wait() print(f\'Car {car_id} finished parking.\') async def open_new_spots(self): while True: await asyncio.sleep(self.interval) for _ in range(self.new_spots): self.available_spots.release() print(f\'Opened {self.new_spots} new spots.\') self.new_spots_event.set() self.new_spots_event.clear() async def main(total_spots, num_cars, new_spots, arrival_time, interval): parking_lot = ParkingLot(total_spots, interval, new_spots) car_tasks = [parking_lot.car_park(i, arrival_time) for i in range(num_cars)] open_spots_task = parking_lot.open_new_spots() await asyncio.gather(open_spots_task, *car_tasks) # Example input asyncio.run(main(total_spots=3, num_cars=10, new_spots=2, arrival_time=1, interval=5)) ``` **Note:** This example is just for illustration and may need adjustments based on your specific requirements. # Task 1. Implement the `ParkingLot` class with appropriate asyncio synchronization primitives. 2. Create the `main` coroutine that initializes the parking lot and runs the simulation. 3. Ensure that the program runs asynchronously and efficiently.","solution":"import asyncio class ParkingLot: def __init__(self, total_spots, interval, new_spots): self.total_spots = total_spots self.available_spots = asyncio.Semaphore(total_spots) self.new_spots_event = asyncio.Event() self.interval = interval self.new_spots = new_spots async def car_park(self, car_id, arrival_time): await asyncio.sleep(arrival_time) print(f\'Car {car_id} arriving at the parking lot.\') await self.available_spots.acquire() print(f\'Car {car_id} is parking.\') await self.new_spots_event.wait() print(f\'Car {car_id} finished parking.\') async def open_new_spots(self): while True: await asyncio.sleep(self.interval) for _ in range(self.new_spots): self.available_spots.release() print(f\'Opened {self.new_spots} new spots.\') self.new_spots_event.set() self.new_spots_event.clear() async def main(total_spots, num_cars, new_spots, arrival_time, interval): parking_lot = ParkingLot(total_spots, interval, new_spots) car_tasks = [parking_lot.car_park(i, arrival_time) for i in range(num_cars)] open_spots_task = asyncio.create_task(parking_lot.open_new_spots()) await asyncio.gather(open_spots_task, *car_tasks)"},{"question":"# XML Manipulation with `xml.etree.ElementTree` In this task, you are required to parse an XML file, manipulate its elements, and write the modified XML to an output file using the `xml.etree.ElementTree` module. Input: 1. You are given an XML file named `input.xml` with the following structure: ```xml <data> <country name=\\"ExampleCountry1\\"> <rank>12</rank> <year>2015</year> <gdppc>50000</gdppc> <neighbor name=\\"Neighbor1\\" direction=\\"E\\"/> <neighbor name=\\"Neighbor2\\" direction=\\"W\\"/> </country> <country name=\\"ExampleCountry2\\"> <rank>30</rank> <year>2010</year> <gdppc>45000</gdppc> <neighbor name=\\"Neighbor3\\" direction=\\"N\\"/> </country> <!-- More country elements can be added similarly --> </data> ``` Tasks: 1. **Parsing XML**: Write a function `parse_xml(file_path: str) -> ElementTree.ElementTree` that parses the given XML file and returns the parsed `ElementTree` object. 2. **Modifying Elements**: - Write a function `update_gdppc(tree: ElementTree.ElementTree) -> None` to increase the `gdppc` (Gross Domestic Product per Capita) value for all countries by 10%. - Write a function `remove_low_rank(tree: ElementTree.ElementTree, threshold: int) -> None` that removes all `<country>` elements with a `rank` higher than the given threshold. 3. **Writing Output**: Write a function `write_output(tree: ElementTree.ElementTree, output_path: str) -> None` to write the modified XML tree to an output file. Constraints: - The `rank` values given in the input XML will always be non-negative integers. - The `gdppc` values are always positive. Example: Assume the `input.xml` has the following data: ```xml <data> <country name=\\"ExampleCountry1\\"> <rank>12</rank> <year>2015</year> <gdppc>50000</gdppc> <neighbor name=\\"Neighbor1\\" direction=\\"E\\"/> <neighbor name=\\"Neighbor2\\" direction=\\"W\\"/> </country> <country name=\\"ExampleCountry2\\"> <rank>30</rank> <year>2010</year> <gdppc>45000</gdppc> <neighbor name=\\"Neighbor3\\" direction=\\"N\\"/> </country> </data> ``` After running the following code: ```python tree = parse_xml(\'input.xml\') update_gdppc(tree) remove_low_rank(tree, 20) write_output(tree, \'output.xml\') ``` The `output.xml` should look like: ```xml <data> <country name=\\"ExampleCountry1\\"> <rank>12</rank> <year>2015</year> <gdppc>55000</gdppc> <neighbor name=\\"Neighbor1\\" direction=\\"E\\"/> <neighbor name=\\"Neighbor2\\" direction=\\"W\\"/> </country> <!-- ExampleCountry2 is removed because its rank 30 > 20 --> </data> ``` Write the required functions: `parse_xml(file_path: str)`, `update_gdppc(tree: ElementTree.ElementTree)`, `remove_low_rank(tree: ElementTree.ElementTree, threshold: int)`, and `write_output(tree: ElementTree.ElementTree, output_path: str)` to accomplish this task. ```python import xml.etree.ElementTree as ET def parse_xml(file_path: str) -> ET.ElementTree: # Write code here def update_gdppc(tree: ET.ElementTree) -> None: # Write code here def remove_low_rank(tree: ET.ElementTree, threshold: int) -> None: # Write code here def write_output(tree: ET.ElementTree, output_path: str) -> None: # Write code here ```","solution":"import xml.etree.ElementTree as ET def parse_xml(file_path: str) -> ET.ElementTree: Parses the given XML file and returns the parsed ElementTree object. tree = ET.parse(file_path) return tree def update_gdppc(tree: ET.ElementTree) -> None: Increases the gdppc value for all countries by 10%. root = tree.getroot() for country in root.findall(\'country\'): gdppc_element = country.find(\'gdppc\') if gdppc_element is not None: current_gdppc = int(gdppc_element.text) new_gdppc = int(current_gdppc * 1.10) gdppc_element.text = str(new_gdppc) def remove_low_rank(tree: ET.ElementTree, threshold: int) -> None: Removes all <country> elements with a rank higher than the given threshold. root = tree.getroot() for country in root.findall(\'country\'): rank_element = country.find(\'rank\') if rank_element is not None: rank = int(rank_element.text) if rank > threshold: root.remove(country) def write_output(tree: ET.ElementTree, output_path: str) -> None: Writes the modified XML tree to an output file. tree.write(output_path)"},{"question":"# Question: Design and Implement a Function to Create a MIME Email with Multiple Components You are tasked to write a Python function that creates a MIME email with multiple components: a plain text part, an HTML part, and an image attachment. The function should return the MIME email message as a string. **Function Signature:** ```python def create_mime_email(to_email: str, from_email: str, subject: str, plain_text: str, html_text: str, image_path: str) -> str: ``` **Parameters:** - `to_email` (str): The recipient email address. - `from_email` (str): The sender email address. - `subject` (str): The subject of the email. - `plain_text` (str): The plain text content of the email. - `html_text` (str): The HTML content of the email. - `image_path` (str): The file path to an image to attach to the email. **Returns:** - `str`: The MIME email message as a string. **Requirements:** 1. The function should create a `MIMEMultipart` email object. 2. Attach the plain text and HTML content using `MIMEText`. 3. Attach an image using `MIMEImage`. 4. Ensure appropriate headers are set (e.g., `From`, `To`, `Subject`). 5. Use base64 encoding for the image. 6. The final email message should be serialized to a string. **Constraints:** - The provided `image_path` should point to a valid image file. - HTML content should be receivable by most email clients (i.e., use basic HTML tags). **Example:** ```python result = create_mime_email( to_email=\\"recipient@example.com\\", from_email=\\"sender@example.com\\", subject=\\"Test Email\\", plain_text=\\"This is the plain text part of the email.\\", html_text=\\"<html><body><h1>This is the HTML part of the email</h1></body></html>\\", image_path=\\"/path/to/image.jpg\\" ) print(result) ``` The output should be a properly formatted MIME email string containing the plain text, HTML content, and an attached image. # Note: - Make sure to include necessary imports from the `email` module. - Handle file reading for the image file and any potential exceptions.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email import encoders import os def create_mime_email(to_email: str, from_email: str, subject: str, plain_text: str, html_text: str, image_path: str) -> str: Creates a MIME email with plain text, HTML content, and an image attachment. Parameters: to_email (str): The recipient email address. from_email (str): The sender email address. subject (str): The subject of the email. plain_text (str): The plain text content of the email. html_text (str): The HTML content of the email. image_path (str): The file path to an image to attach to the email. Returns: str: The MIME email message as a string. # Create the root message msg = MIMEMultipart(\'alternative\') msg[\'To\'] = to_email msg[\'From\'] = from_email msg[\'Subject\'] = subject # Attach plain text part1 = MIMEText(plain_text, \'plain\') msg.attach(part1) # Attach HTML content part2 = MIMEText(html_text, \'html\') msg.attach(part2) # Attach image if os.path.exists(image_path): with open(image_path, \'rb\') as img: mime_img = MIMEImage(img.read()) mime_img.add_header(\'Content-Disposition\', \'attachment\', filename=os.path.basename(image_path)) encoders.encode_base64(mime_img) msg.attach(mime_img) else: raise FileNotFoundError(f\\"The specified image path does not exist: {image_path}\\") # Return the message as a string return msg.as_string()"},{"question":"# Advanced Plot Manipulation with Seaborn Objective In this coding assessment, you will create a customized bar plot using the Seaborn `so.Plot` interface. Your task is to leverage the Dodge transformation and additional parameters to modify the plot. This will test your ability to utilize both basic and advanced features of Seaborn plotting techniques. Problem Statement You are provided with the \\"tips\\" dataset, which includes information about the total bill amount, tip amount, gender of the payer, whether the payer is a smoker, the day of the week, and the time (lunch/dinner) at which the meal was consumed. The dataset can be loaded using the following code: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\").astype({\\"time\\": str}) ``` Your task is to: 1. Create a bar plot that shows the total bill amount for each day, separated by gender (`sex`). Use different colors for different times of the day (`time`). 2. Use the Dodge transformation to ensure the bars for different times of the day do not overlap. 3. Ensure there is no empty space in the plot where a particular combination of factors does not exist. 4. Add a gap of 0.2 between the bars for better readability. 5. Save the plot as a PNG image file named `customized_barplot.png`. # Function Signature ```python import seaborn.objects as so import seaborn as sns def create_customized_barplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot using the provided specifications p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2, empty=\\"fill\\", by=[\\"time\\"])) # Save the plot to a file p.save(\\"customized_barplot.png\\") ``` Input - No input is required as the dataset is provided within the function. Output - The function should save a PNG file named `customized_barplot.png` in the current directory. # Constraints - Use Seaborn version 0.11.1 or higher. - Ensure the final plot is correctly saved as `customized_barplot.png`. Example Usage Once the function `create_customized_barplot()` is implemented, simply calling it will generate and save the required plot: ```python create_customized_barplot() ``` This will save the plot as `customized_barplot.png` in the current working directory.","solution":"import seaborn as sns import seaborn.objects as so def create_customized_barplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot using the provided specifications p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2, empty=\\"none\\", by=\\"time\\")) # Save the plot to a file p.save(\\"customized_barplot.png\\")"},{"question":"Objective: Write a function `evaluate_expression(expression: str) -> float` that takes a mathematical expression provided as a string and evaluates it. The expression will include operations involving real and complex numbers, trigonometric functions, logarithms, power operations, and constants. Requirements: 1. The function should correctly interpret and compute results for expressions involving the following functions and operators: - Basic arithmetic operators: `+`, `-`, `*`, `/`, `**` (power) - Trigonometric functions: `sin`, `cos`, `tan` - Logarithmic functions: `log`, `log10`, `log2` - Square root function: `sqrt` - Complex number functions: `polar`, `rect` (conversions to and from polar coordinates) - Constants: `pi`, `e` 2. The function must handle errors gracefully, such as invalid expressions or math domain errors, and return `None` if the expression cannot be evaluated. Input: - A string `expression` that represents a mathematical expression to be evaluated. The input expression can contain real numbers, complex numbers, functions, operators, and constants. Output: - A float representing the result of the evaluated expression if valid, otherwise `None`. Constraints: - Assume the expression is always a valid string. - The function should use the appropriate modules: `math`, `cmath`, and other necessary submodules or functions. - Do not use the `eval` function for security reasons. Instead, parse and evaluate the expression safely using the available mathematical functions and modules. - Performance is important: the function should aim to evaluate expressions in a reasonable time frame. Example: ```python from math import pi, e def evaluate_expression(expression: str) -> float: # Your implementation here pass # Test Cases print(evaluate_expression(\\"2 + 3 * 4 - 5\\")) # Expected output: 9.0 print(evaluate_expression(\\"sin(pi / 2) + log(e)\\")) # Expected output: 2.0 print(evaluate_expression(\\"log10(100) + sqrt(16)\\")) # Expected output: 6.0 print(evaluate_expression(\\"cos(0) + polar(1+1j)\\")) # Expected output: should handle complex number and return a valid result or None if invalid ```","solution":"import math import cmath def evaluate_expression(expression: str) -> float: Evaluates a mathematical expression involving real and complex numbers, trigonometric functions, logarithms, power operations, and constants. try: # Safe dictionary of math functions and constants safe_dict = { \'sin\': math.sin, \'cos\': math.cos, \'tan\': math.tan, \'log\': math.log, \'log10\': math.log10, \'log2\': math.log2, \'sqrt\': math.sqrt, \'polar\': cmath.polar, \'rect\': cmath.rect, \'pi\': math.pi, \'e\': math.e, \'complex\': complex } # Evaluate the expression using the safe dictionary return eval(expression, {\\"__builtins__\\": {}}, safe_dict) except (SyntaxError, NameError, TypeError, ZeroDivisionError, ValueError): # Return None if there\'s an error during evaluation return None"},{"question":"**Objective**: Verify your understanding of Python\'s `email.policy` module by implementing a custom policy to handle specific email formatting requirements according to various constraints. Question You are required to implement a custom email policy for handling email serialization with the following specifications: 1. Maximum line length should be set to 120 characters. 2. Line separators should be appropriate for SMTP transmission (`rn`). 3. The content transfer encoding type should be `8bit`, allowing non-ASCII characters in the body of the message. 4. The policy should raise errors on any detected defects during email parsing or serialization. 5. The policy should escape lines in the message body that start with \\"From \\". 6. The default message factory should be used. Implement a function `create_custom_policy()` that returns an instance of a custom `EmailPolicy` with the above specifications applied. Input The function `create_custom_policy()` does not take any parameters. Output The function should return an instance of `EmailPolicy` with the specified attributes: - `max_line_length`: 120 - `linesep`: `rn` - `cte_type`: `8bit` - `raise_on_defect`: `True` - `mangle_from_`: `True` Constraints - You must use the `email.policy` module to create and clone the policy. - You should not modify the behavior of any other attributes or methods beyond those specified. Example Usage ```python from email.policy import EmailPolicy from email import policy # Example usage custom_policy = create_custom_policy() # Check attributes print(custom_policy.max_line_length) # Output: 120 print(custom_policy.linesep) # Output: rn print(custom_policy.cte_type) # Output: 8bit print(custom_policy.raise_on_defect) # Output: True print(custom_policy.mangle_from_) # Output: True ``` Implement the `create_custom_policy` function in the code block below. ```python from email.policy import EmailPolicy def create_custom_policy(): # Write your implementation here pass ```","solution":"from email.policy import EmailPolicy def create_custom_policy(): Creates and returns a custom EmailPolicy instance with specified attributes. return (EmailPolicy() .clone(max_line_length=120) .clone(linesep=\'rn\') .clone(cte_type=\'8bit\') .clone(raise_on_defect=True) .clone(mangle_from_=True))"},{"question":"**Coding Assessment Question:** # Problem Statement: You are provided with a string containing XML data. Your task is to parse this XML data into a DOM structure and perform the following operations: 1. Create a new element named `newElement` with the text content `New Element Content`. 2. Add this new element as the last child of the root element. 3. For every element in the document, if the element has a tag name `targetElement`, change its text content to `Updated Content`. 4. Provide a function to serialize this DOM structure back into an XML string. # Function Signature: You must implement the following functions: ```python def parse_xml(xml_string: str) -> Document: Parses the given XML string into a DOM Document. Parameters: xml_string (str): A string containing the XML data. Returns: Document: Parsed DOM Document. pass def add_new_element(doc: Document, element_name: str, element_content: str) -> None: Adds a new element with the provided name and content as the last child of the root element. Parameters: doc (Document): The DOM Document object. element_name (str): The name for the new element. element_content (str): The text content for the new element. pass def update_elements(doc: Document, target_tag_name: str, new_content: str) -> None: Updates the text content of all elements with the target tag name to the new content. Parameters: doc (Document): The DOM Document object. target_tag_name (str): The tag name of the elements to be updated. new_content (str): The new text content to be set. pass def serialize_xml(doc: Document) -> str: Serializes the DOM Document into an XML string. Parameters: doc (Document): The DOM Document object. Returns: str: The serialized XML string. pass ``` # Constraints: 1. The XML string provided will always be well-formed. 2. The given XML might have nested elements. 3. Make sure to handle namespaces properly if they are present. # Example: ```python xml_data = <root> <targetElement>Old Content</targetElement> <anotherElement> <targetElement>Old Content Too</targetElement> </anotherElement> </root> # Parsing the XML string doc = parse_xml(xml_data) # Adding a new element to the root add_new_element(doc, \\"newElement\\", \\"New Element Content\\") # Updating all elements with the tag name \'targetElement\' update_elements(doc, \\"targetElement\\", \\"Updated Content\\") # Serializing the updated XML document to string updated_xml = serialize_xml(doc) print(updated_xml) ``` The output should be: ```xml <root> <targetElement>Updated Content</targetElement> <anotherElement> <targetElement>Updated Content</targetElement> </anotherElement> <newElement>New Element Content</newElement> </root> ``` **Note**: Ensure your implementation handles the XML parsing and modifications efficiently and correctly follows the DOM manipulation methods as per the `xml.dom` package.","solution":"from xml.dom.minidom import parseString, Document def parse_xml(xml_string: str) -> Document: Parses the given XML string into a DOM Document. Parameters: xml_string (str): A string containing the XML data. Returns: Document: Parsed DOM Document. return parseString(xml_string) def add_new_element(doc: Document, element_name: str, element_content: str) -> None: Adds a new element with the provided name and content as the last child of the root element. Parameters: doc (Document): The DOM Document object. element_name (str): The name for the new element. element_content (str): The text content for the new element. new_element = doc.createElement(element_name) text_node = doc.createTextNode(element_content) new_element.appendChild(text_node) root = doc.documentElement root.appendChild(new_element) def update_elements(doc: Document, target_tag_name: str, new_content: str) -> None: Updates the text content of all elements with the target tag name to the new content. Parameters: doc (Document): The DOM Document object. target_tag_name (str): The tag name of the elements to be updated. new_content (str): The new text content to be set. elements = doc.getElementsByTagName(target_tag_name) for element in elements: if element.firstChild: element.firstChild.data = new_content def serialize_xml(doc: Document) -> str: Serializes the DOM Document into an XML string. Parameters: doc (Document): The DOM Document object. Returns: str: The serialized XML string. return doc.toxml()"},{"question":"# Question: Asynchronous File Processing with Exception Handling Objective Your task is to implement a function that reads from a file asynchronously, processes its content, and handles potential exceptions that may occur during the read operation using the `asyncio` module. Function Signature ```python import asyncio async def process_file(file_path: str) -> str: Asynchronously reads and processes the content of the given file. Handles exceptions that may arise during the read operation. Parameters: file_path (str): The path to the file to be read. Returns: str: The processed content of the file. Raises: asyncio.TimeoutError: If the read operation exceeds a given duration. asyncio.CancelledError: If the read operation is cancelled. asyncio.IncompleteReadError: If the read operation does not complete fully. asyncio.LimitOverrunError: If the buffer size limit is reached. asyncio.SendfileNotAvailableError: If the sendfile syscall is not available. asyncio.InvalidStateError: If the operation encounters an invalid state. ``` Description 1. **Reading the File:** - Use asynchronous file reading to load the content of the provided file path. Assume the file contains text data. - Implement a timeout to simulate a network read or other delay-prone activity. 2. **Processing the Content:** - Simulate content processing by transforming the text (e.g., reversing the content, converting to uppercase). 3. **Exception Handling:** - Your function should handle the following exceptions: - `asyncio.TimeoutError`: If the read operation exceeds a specified timeout duration. - `asyncio.CancelledError`: If the operation is cancelled. - `asyncio.IncompleteReadError`: Handle cases where the read doesn\'t complete fully. Return the partial content in such cases. - `asyncio.LimitOverrunError`: If the buffer size limit is reached while looking for a separator. - `asyncio.SendfileNotAvailableError`: For the unavailability of the sendfile syscall. - `asyncio.InvalidStateError`: For any invalid state encountered during the operation. 4. **Return Value:** - Return the processed content as a string. Constraints - You must use the `asyncio` module for asynchronous file reading and exception handling. - Assume each read operation has a timeout of 5 seconds. - The file size will not exceed 1MB. - No additional libraries should be imported except `asyncio`. Example ```python import asyncio async def main(): content = await process_file(\\"sample.txt\\") print(content) asyncio.run(main()) ``` **Assume `sample.txt` contains the following:** ``` Hello, world! This is an example text file. ``` **Possible Output:** ``` !dlrow ,olleH elif txet elpmaxe na si sihT ``` **Handling Exceptions:** ```python # If `asyncio.IncompleteReadError` is raised and only partial content is read: async def main(): try: content = await process_file(\\"sample.txt\\") except asyncio.IncompleteReadError as e: content = e.partial.decode(\'utf-8\') # Handle partial content print(f\\"Incomplete read: {content}\\") ``` **Note:** The provided examples are to help understand expected behaviors and handling. The actual implementation needs to cover all specified scenarios.","solution":"import asyncio async def process_file(file_path: str) -> str: Asynchronously reads and processes the content of the given file. Handles exceptions that may arise during the read operation. Parameters: file_path (str): The path to the file to be read. Returns: str: The processed content of the file. Raises: asyncio.TimeoutError: If the read operation exceeds a given duration. asyncio.CancelledError: If the read operation is cancelled. asyncio.IncompleteReadError: If the read operation does not complete fully. asyncio.LimitOverrunError: If the buffer size limit is reached. asyncio.SendfileNotAvailableError: If the sendfile syscall is not available. asyncio.InvalidStateError: If the operation encounters an invalid state. try: # Simulating the delay in reading the file and handling the content await asyncio.sleep(1) # Simulating async file read # Simulating file read: assuming the file content for demonstration purposes async with asyncio.open_file(file_path, \'r\') as file: content = await asyncio.wait_for(file.read(), timeout=5) # Processing the content, assume processing is reversing the text processed_content = content[::-1] return processed_content except asyncio.TimeoutError: return \\"Operation timed out.\\" except asyncio.CancelledError: return \\"Operation cancelled.\\" except asyncio.IncompleteReadError as e: return f\\"Incomplete read: {e.partial.decode(\'utf-8\')}\\" except asyncio.LimitOverrunError: return \\"Buffer size limit reached.\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall not available.\\" except asyncio.InvalidStateError: return \\"Invalid state encountered during operation.\\" async def main(): content = await process_file(\\"sample.txt\\") print(content) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Context Management with `contextvars` in Python Using the `contextvars` module in Python, you will implement a class `ThreadSafeCounter` that keeps a separate count for each thread, ensuring that increments and access to the count are thread-safe and context-specific. The `ThreadSafeCounter` class should have the following methods: 1. **`__init__(self)`:** - Initializes a new `contextvars.ContextVar` to store the count for each thread with a default value of 0. 2. **`increment(self)`:** - Increments the counter for the current thread by 1. 3. **`get_count(self) -> int`:** - Returns the current count for the current thread. 4. **`reset(self)`:** - Resets the count for the current thread to 0. Requirements: - Use the `contextvars.ContextVar` to manage thread-specific counts. - The `increment` method should be thread-safe, ensuring that increments do not interfere with each other. Constraints: - You may assume that the environment will run multi-threaded, but you need not handle thread creation or management explicitly in your class. # Example Usage: ```python import threading # Initialize the counter counter = ThreadSafeCounter() # Define a function for threads to run def thread_function(): for _ in range(5): counter.increment() print(f\'Thread {threading.current_thread().name} count: {counter.get_count()}\') # Create multiple threads threads = [threading.Thread(target=thread_function, name=f\'Thread-{i}\') for i in range(3)] # Start threads for t in threads: t.start() # Join threads for t in threads: t.join() ``` **Expected Output:** Each thread should print its own count, which should be `5` if the `increment` method works correctly and maintains thread-specific counts. # Testing: You should write unit tests to verify the correct behavior of the `ThreadSafeCounter` in multithreading environments, ensuring that counts are correctly maintained and isolated across threads.","solution":"import contextvars class ThreadSafeCounter: def __init__(self): Initializes a new ContextVar to store the count for each thread with a default value of 0. self.count = contextvars.ContextVar(\'count\', default=0) def increment(self): Increments the counter for the current thread by 1. current_count = self.count.get() self.count.set(current_count + 1) def get_count(self) -> int: Returns the current count for the current thread. return self.count.get() def reset(self): Resets the count for the current thread to 0. self.count.set(0)"},{"question":"**Custom Neural Network Module and Hook Implementation** **Objective:** Design and implement a custom neural network module using PyTorch. The module should make use of custom layers, parameter initialization, and forward/backward hooks for monitoring and modifying internal computations. **Task:** 1. Create a custom module `CustomLinear` that inherits from `torch.nn.Module`. This module should implement a linear transformation but also include a custom parameter initialization and a constant bias term that can be toggled between fixed and learnable. - The weight matrix should be initialized using Xavier Normal initialization. - The bias term should be 1 by default if `learnable_bias` is set to `False`, otherwise, it should be a learnable parameter. 2. Implement another custom module `CustomNet` that composes an arbitrary number of `CustomLinear` layers followed by ReLU activations. 3. Implement and register a forward hook that appends the input values to a list before passing them through the layers to track their values during training. 4. Implement and register a backward hook that scales the gradients of the inputs by a constant factor (e.g., 0.1) during backpropagation. **Requirements:** - **CustomLinear Module:** - **Input:** `in_features` (int), `out_features` (int), `learnable_bias` (bool) - **Output:** Standard output of `torch.nn.Linear`. - **CustomNet Module:** - **Input:** `in_features` (int), `hidden_features` (int), `num_layers` (int) - **Output:** Should return the output from the final layer. - **Hooks:** - Both hooks should be registered within the `CustomNet` module. - Forward hook should store input values pre-transformation. - Backward hook should modify input gradients by scaling them. **Constraints:** - The implementation must handle exceptions gracefully and provide informative error messages. - The modules should dynamically support transfer to `cpu` or `cuda` devices. **Submission:** Submit a Python script containing: - Custom Module `CustomLinear` - Custom Network `CustomNet` - Code demonstrating the registration of forward and backward hooks - Sample training loop to show the utilization of the network and hooks ```python import torch import torch.nn as nn # Define the custom linear module class CustomLinear(nn.Module): def __init__(self, in_features, out_features, learnable_bias=True): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.empty(in_features, out_features)) if learnable_bias: self.bias = nn.Parameter(torch.empty(out_features)) else: self.register_buffer(\'bias\', torch.ones(out_features)) self.reset_parameters() def reset_parameters(self): nn.init.xavier_normal_(self.weight) if self.bias is not None: if isinstance(self.bias, nn.Parameter): nn.init.zeros_(self.bias) def forward(self, input): return torch.mm(input, self.weight) + self.bias # Define the custom network module class CustomNet(nn.Module): def __init__(self, in_features, hidden_features, num_layers): super(CustomNet, self).__init__() self.layers = nn.ModuleList( [CustomLinear(in_features if i == 0 else hidden_features, hidden_features) for i in range(num_layers)] ) self.activations = nn.ModuleList([nn.ReLU() for _ in range(num_layers)]) self.inputs = [] def forward_hook(self, module, input, output): self.inputs.append(input) def backward_hook(self, module, grad_input, grad_output): return (grad_input[0] * 0.1,) def forward(self, x): for layer, activation in zip(self.layers, self.activations): x = layer(x) x = activation(x) return x # Create and demonstrate the custom network and hooks def main(): net = CustomNet(in_features=4, hidden_features=8, num_layers=3) forward_hook_handle = net.register_forward_hook(net.forward_hook) backward_hook_handle = net.register_backward_hook(net.backward_hook) # Create dummy data input = torch.randn(10, 4, requires_grad=True) target = torch.zeros(10, 8) # Define a simple optimizer optimizer = torch.optim.SGD(net.parameters(), lr=0.01) criterion = nn.MSELoss() # Training loop for epoch in range(2): optimizer.zero_grad() output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() print(f\'Epoch {epoch}, Loss: {loss.item()}\') # Remove hooks forward_hook_handle.remove() backward_hook_handle.remove() # Print collected inputs from forward hook print(f\'Collected inputs: {net.inputs}\') if __name__ == \\"__main__\\": main() ``` Ensure your custom modules support transferring to different devices properly and handle all edge cases. **Notes:** - Read PyTorch documentation on module, parameter, and hooks for further details and guidance. - Test thoroughly to ensure hooks are properly modifying the forward and backward passes.","solution":"import torch import torch.nn as nn # Define the custom linear module class CustomLinear(nn.Module): def __init__(self, in_features, out_features, learnable_bias=True): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.empty(in_features, out_features)) if learnable_bias: self.bias = nn.Parameter(torch.empty(out_features)) else: self.register_buffer(\'bias\', torch.ones(out_features)) self.reset_parameters() def reset_parameters(self): nn.init.xavier_normal_(self.weight) if self.bias is not None and isinstance(self.bias, nn.Parameter): nn.init.zeros_(self.bias) def forward(self, input): return torch.mm(input, self.weight) + self.bias # Define the custom network module class CustomNet(nn.Module): def __init__(self, in_features, hidden_features, num_layers): super(CustomNet, self).__init__() self.layers = nn.ModuleList( [CustomLinear(in_features if i == 0 else hidden_features, hidden_features) for i in range(num_layers)] ) self.activations = nn.ModuleList([nn.ReLU() for _ in range(num_layers)]) self.inputs = [] self.forward_handles = [] self.backward_handles = [] for layer in self.layers: self.forward_handles.append(layer.register_forward_hook(self.forward_hook)) self.backward_handles.append(layer.register_full_backward_hook(self.backward_hook)) def forward_hook(self, module, input, output): self.inputs.append(input[0].detach().clone()) # Append the input values to the list def backward_hook(self, module, grad_input, grad_output): return tuple(grad * 0.1 for grad in grad_input) # Scale the gradients by 0.1 def forward(self, x): for layer, activation in zip(self.layers, self.activations): x = layer(x) x = activation(x) return x # Create and demonstrate the custom network and hooks def main(): net = CustomNet(in_features=4, hidden_features=8, num_layers=3) # Create dummy data input = torch.randn(10, 4, requires_grad=True) target = torch.zeros(10, 8) # Define a simple optimizer optimizer = torch.optim.SGD(net.parameters(), lr=0.01) criterion = nn.MSELoss() # Training loop for epoch in range(2): optimizer.zero_grad() output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() print(f\'Epoch {epoch}, Loss: {loss.item()}\') # Remove hooks for handle in net.forward_handles: handle.remove() for handle in net.backward_handles: handle.remove() # Print collected inputs from forward hook print(f\'Collected inputs: {net.inputs}\') if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of the seaborn library by creating a complex, informative visualization from a given dataset. # Question: You are provided with a dataset named `penguins` from the seaborn datasets. Your task is to create a multi-faceted plot that showcases different statistical and visual relationships within the dataset. The visualization should include: 1. **Scatter plot**: Show the relationship between `flipper_length_mm` and `body_mass_g` for different species of penguins. Each species should be colored differently. 2. **Linear regression model** with confidence intervals: Superimpose a linear regression model on the scatter plot to highlight trends within each species. 3. **Distribution plot**: On the side of the scatter plot, create marginal distribution plots (using kernel density estimates) for `flipper_length_mm` and `body_mass_g`. 4. **Facet Grid**: Use a grid layout to display the above plots separately for each island in the dataset. 5. **Customization**: It should include customized axis labels, a legend, and an appropriate theme to make the plot presentation-ready. # Requirements: - **Input**: You are given the dataset `penguins` from seaborn. Use `sns.load_dataset(\\"penguins\\")` to load it. - **Output**: A multi-faceted visualization that includes all the features mentioned above. - **Constraints**: Ensure your code is efficient and leverages seaborn’s functionalities effectively. - **Performance**: The visualization should be generated within a reasonable time frame for standard computational resources. # Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Initialize the FacetGrid g = sns.FacetGrid(...) # Map the scatter plot g.map(...) # Add the linear regression model g.map(...) # Add marginal distribution plots ... # Customize the plot g.set_axis_labels(...) g.add_legend(...) ... # Show the plot plt.show() ``` Ensure that you include all necessary elements and provide comments to explain your code where needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Initialize the FacetGrid with column as island g = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\", height=5, palette=\\"muted\\", col_wrap=3, aspect=1.5) # Map the scatter plot g.map(sns.scatterplot, \\"flipper_length_mm\\", \\"body_mass_g\\") # Map the linear regression model g.map(sns.regplot, \\"flipper_length_mm\\", \\"body_mass_g\\", scatter=False, ci=None) # Add marginal distribution plots for flipper length for ax in g.axes.flat: sns.kdeplot(x=penguins[\\"flipper_length_mm\\"], ax=ax, legend=False, color=\'r\', alpha=0.3) sns.kdeplot(y=penguins[\\"body_mass_g\\"], ax=ax, legend=False, color=\'b\', alpha=0.3) # Customize the plot g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Body Mass (g)\\") g.add_legend(title=\\"Species\\") g.set_titles(col_template=\\"{col_name}\\") plt.show()"},{"question":"Objective: To assess the comprehension and application of the `torch.special` module in PyTorch by implementing a function that utilizes multiple special functions to compute a complex mathematical operation. Problem Statement: You are required to write a PyTorch function `special_function_operations` that takes a 1D tensor `x` and performs the following operations in sequence: 1. Compute the Bessel function of the first kind of order 0 for each element in `x` using `torch.special.bessel_j0`. 2. Compute the exponential of each element in the result from step 1 minus 1 using `torch.special.expm1`. 3. Compute the natural logarithm of each element in the result from step 2 plus 1 using `torch.special.log1p`. 4. Compute the softmax of the result from step 3 using `torch.special.softmax`. The final result should be returned as a 1D tensor of the same shape as the input. Function Signature: ```python def special_function_operations(x: torch.FloatTensor) -> torch.FloatTensor: pass ``` Input: - `x` (torch.FloatTensor): A 1D tensor of arbitrary length containing floating-point numbers. Output: - Returns a tensor (torch.FloatTensor) representing the final result of the sequence of operations described above. Constraints: - The elements of the input tensor `x` are real numbers. - You must use the specified functions from `torch.special` module. - Standard performance requirements: the function should be able to handle reasonably large tensors efficiently (e.g., tensors with up to 10,000 elements). Example Usage: ```python import torch x = torch.FloatTensor([0.1, 1.2, -0.5]) result = special_function_operations(x) print(result) ``` This should return a tensor containing the softmax values after performing the specified operations. Your implementation will be evaluated based on correctness, efficiency, and proper usage of the `torch.special` functions.","solution":"import torch def special_function_operations(x: torch.FloatTensor) -> torch.FloatTensor: Perform a series of operations using torch.special functions on tensor x. # Step 1: Compute the Bessel function of the first kind of order 0 bessel_j0_result = torch.special.bessel_j0(x) # Step 2: Compute the exponential of each element in the result minus 1 expm1_result = torch.special.expm1(bessel_j0_result) # Step 3: Compute the natural logarithm of each element plus 1 log1p_result = torch.special.log1p(expm1_result) # Step 4: Compute the softmax softmax_result = torch.special.softmax(log1p_result, dim=0) return softmax_result"},{"question":"# Custom Sequence Class Implementation In this coding assessment, you are required to create a custom class that mimics the behavior of a sequence in Python. Specifically, you will implement a class that inherits from `collections.abc.Sequence` and provides functionality for a read-only sequence. Your implementation should include required abstract methods and optionally override mixin methods. Requirements 1. **Class Creation**: - Create a class named `CustomSequence` that inherits from `collections.abc.Sequence`. - The initializer (`__init__`) should accept an iterable and store it internally. 2. **Abstract Methods**: - Implement the `__getitem__` method to retrieve an item at a given index. - Implement the `__len__` method to return the number of items in the sequence. 3. **Mixin Methods** (optional): - Optionally, override the mixin methods such as `count` and `index` to enhance performance if necessary. Expected Input & Output - The input to the class initializer will be an iterable (e.g., list, tuple, etc.). - The class should support indexing and length queries similar to a standard Python sequence. - Example Usage: ```python seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 print(seq.index(3)) # Output: 2 print(seq.count(4)) # Output: 1 ``` Constraints - You must use the `collections.abc.Sequence` as the base class. - The sequence should be read-only; therefore, do not implement any methods that modify the sequence (e.g., `__setitem__`, `__delitem__`). Evaluation Your solution will be evaluated based on the correctness of the implementation, adherence to sequence interface requirements, and code readability. Ensure that your class correctly interacts with indexing, length queries, and optionally, the mixin methods provided by the `Sequence` ABC. ```python import collections.abc class CustomSequence(collections.abc.Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) # Optional: Override mixin methods if necessary def count(self, value): return self._data.count(value) def index(self, value): return self._data.index(value) # Example usage: seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 print(seq.index(3)) # Output: 2 print(seq.count(4)) # Output: 1 ``` Make sure your implementation passes the example usage provided above.","solution":"import collections.abc class CustomSequence(collections.abc.Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) # Optional: Override mixin methods if necessary def count(self, value): return self._data.count(value) def index(self, value): return self._data.index(value) # Example usage: seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 print(seq.index(3)) # Output: 2 print(seq.count(4)) # Output: 1"},{"question":"<|Analysis Begin|> The provided documentation offers detailed information on various numerical aspects of PyTorch, focusing heavily on floating point numerical accuracy, batched and slice computations, extreme values, linear algebra stability, TensorFloat-32 (TF32) specifics, reduced precision reductions, and other GPU-specific nuances. Given this depth, it\'s possible to generate a question that assesses a student\'s understanding of numerical stability and accuracy within PyTorch\'s tensor computations, as well as their ability to handle potential pitfalls with data types and precision. A challenging question can involve batched computations, precision considerations (FP16, FP32, and TF32), and stability issues with linear algebra operations. This would require students to demonstrate advanced PyTorch functionalities and be aware of numerical precision impacts. <|Analysis End|> <|Question Begin|> # Problem Statement You are tasked with implementing a PyTorch function that performs a batched matrix multiplication, ensuring numerical stability and precision by handling different floating-point precisions and performing result validation. Your implementation must demonstrate understanding of batched operations, numerical precision, and stability. Function Signature ```python def stable_batch_matrix_multiply(A: torch.Tensor, B: torch.Tensor, precision: str) -> torch.Tensor: Perform a batched matrix multiplication ensuring numerical precision and stability. Parameters: A (torch.Tensor): A 3-dimensional tensor of shape (batch_size, m, n) containing the first set of matrices. B (torch.Tensor): A 3-dimensional tensor of shape (batch_size, n, p) containing the second set of matrices. precision (str): A string indicating the desired precision (\'fp16\', \'fp32\', \'tf32\'). Returns: torch.Tensor: A 3-dimensional tensor of shape (batch_size, m, p) containing the result of the batched matrix multiplication. # Your code here pass ``` Input - `A` (torch.Tensor): A batch of matrices with shape `(batch_size, m, n)` - `B` (torch.Tensor): A batch of matrices with shape `(batch_size, n, p)` - `precision` (str): A string specifying the desired numerical precision (can be `\'fp16\'`, `\'fp32\'`, or `\'tf32\'`). Output - A tensor with shape `(batch_size, m, p)` containing the result of batch matrix multiplication. Constraints - `A` and `B` will always be 3-dimensional tensors with matching batch sizes. - `precision` will always be one of `\'fp16\'`, `\'fp32\'`, or `\'tf32\'`. - Performance should be considered; using the appropriate backend settings to maximize computational efficiency while maintaining requested precision. Notes 1. Handle precision using the appropriate PyTorch settings and datatypes. 2. Perform validation to ensure numerical stability. 3. Consider the effects of TF32 on GPU and include safeguards where necessary. 4. Include additional checks or transformations if required to handle edge cases with extreme values. **Example** ```python import torch A = torch.randn(10, 5, 10) B = torch.randn(10, 10, 5) # Perform batched matrix multiplication with fp32 precision result_fp32 = stable_batch_matrix_multiply(A, B, \'fp32\') # Perform batched matrix multiplication with tf32 precision result_tf32 = stable_batch_matrix_multiply(A, B, \'tf32\') # Perform batched matrix multiplication with fp16 precision result_fp16 = stable_batch_matrix_multiply(A, B, \'fp16\') ``` Your function should correctly handle the different precision requirements and provide stable and accurate results for large batches of matrix multiplications.","solution":"import torch def stable_batch_matrix_multiply(A: torch.Tensor, B: torch.Tensor, precision: str) -> torch.Tensor: Perform a batched matrix multiplication ensuring numerical precision and stability. Parameters: A (torch.Tensor): A 3-dimensional tensor of shape (batch_size, m, n) containing the first set of matrices. B (torch.Tensor): A 3-dimensional tensor of shape (batch_size, n, p) containing the second set of matrices. precision (str): A string indicating the desired precision (\'fp16\', \'fp32\', \'tf32\'). Returns: torch.Tensor: A 3-dimensional tensor of shape (batch_size, m, p) containing the result of the batched matrix multiplication. if precision == \'fp16\': A = A.half() B = B.half() elif precision == \'fp32\': A = A.float() B = B.float() elif precision == \'tf32\': torch.backends.cuda.matmul.allow_tf32 = True A = A.float() B = B.float() else: raise ValueError(\\"Precision must be one of \'fp16\', \'fp32\', or \'tf32\'\\") result = torch.bmm(A, B) if precision == \'tf32\': torch.backends.cuda.matmul.allow_tf32 = False return result"},{"question":"You are tasked with creating a program to manage inventory and sales data for a small bookstore. The program will take input in the form of a list of dictionaries, each representing a sale. Each dictionary contains keys for \'title\', \'author\', \'quantity_sold\', and \'price_per_unit\'. Your task is to implement functions to process this data and generate useful summaries. Input Format A list of dictionaries, where each dictionary represents a sale with the following keys: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `quantity_sold`: An integer representing the number of units sold. - `price_per_unit`: A float representing the price per unit of the book. For example: ```python sales_data = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author 1\\", \\"quantity_sold\\": 5, \\"price_per_unit\\": 10.0}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author 2\\", \\"quantity_sold\\": 2, \\"price_per_unit\\": 15.0}, {\\"title\\": \\"Book A\\", \\"author\\": \\"Author 1\\", \\"quantity_sold\\": 3, \\"price_per_unit\\": 10.0}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author 3\\", \\"quantity_sold\\": 7, \\"price_per_unit\\": 8.0} ] ``` Functions to Implement 1. `get_total_sales_per_book(sales_data: List[Dict[str, Union[str, int, float]]]) -> Dict[str, float]`: This function should return a dictionary where each key is a book title and the corresponding value is the total sales amount for that book. ```python def get_total_sales_per_book(sales_data): pass ``` 2. `get_total_quantity_sold_per_author(sales_data: List[Dict[str, Union[str, int, float]]]) -> Dict[str, int]`: This function should return a dictionary where each key is an author\'s name and the corresponding value is the total quantity of books sold by that author. ```python def get_total_quantity_sold_per_author(sales_data): pass ``` 3. `get_unique_books_titles(sales_data: List[Dict[str, Union[str, int, float]]]) -> Set[str]`: This function should return a set of unique book titles from the sales data. ```python def get_unique_books_titles(sales_data): pass ``` 4. `get_sales_summary(sales_data: List[Dict[str, Union[str, int, float]]]) -> Dict[str, Dict[str, Union[int, float]]`: This function should return a nested dictionary where each key is a book title and the value is another dictionary containing the total quantity sold and total sales amount for that book. ```python def get_sales_summary(sales_data): pass ``` Constraints and Notes - You can assume that the input will always be correctly formatted and will contain all required keys. - The functions should be efficient in terms of time complexity. - Use list comprehensions, set operations, and dictionary manipulations where appropriate. # Example Usage ```python sales_data = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author 1\\", \\"quantity_sold\\": 5, \\"price_per_unit\\": 10.0}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author 2\\", \\"quantity_sold\\": 2, \\"price_per_unit\\": 15.0}, {\\"title\\": \\"Book A\\", \\"author\\": \\"Author 1\\", \\"quantity_sold\\": 3, \\"price_per_unit\\": 10.0}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author 3\\", \\"quantity_sold\\": 7, \\"price_per_unit\\": 8.0} ] print(get_total_sales_per_book(sales_data)) # Output: {\'Book A\': 80.0, \'Book B\': 30.0, \'Book C\': 56.0} print(get_total_quantity_sold_per_author(sales_data)) # Output: {\'Author 1\': 8, \'Author 2\': 2, \'Author 3\': 7} print(get_unique_books_titles(sales_data)) # Output: {\'Book A\', \'Book B\', \'Book C\'} print(get_sales_summary(sales_data)) # Output: { # \'Book A\': {\'total_quantity\': 8, \'total_sales\': 80.0}, # \'Book B\': {\'total_quantity\': 2, \'total_sales\': 30.0}, # \'Book C\': {\'total_quantity\': 7, \'total_sales\': 56.0} # } ```","solution":"def get_total_sales_per_book(sales_data): total_sales = {} for sale in sales_data: title = sale[\'title\'] sales_amount = sale[\'quantity_sold\'] * sale[\'price_per_unit\'] if title in total_sales: total_sales[title] += sales_amount else: total_sales[title] = sales_amount return total_sales def get_total_quantity_sold_per_author(sales_data): total_quantity = {} for sale in sales_data: author = sale[\'author\'] quantity = sale[\'quantity_sold\'] if author in total_quantity: total_quantity[author] += quantity else: total_quantity[author] = quantity return total_quantity def get_unique_books_titles(sales_data): unique_titles = set() for sale in sales_data: unique_titles.add(sale[\'title\']) return unique_titles def get_sales_summary(sales_data): summary = {} for sale in sales_data: title = sale[\'title\'] quantity = sale[\'quantity_sold\'] sales_amount = quantity * sale[\'price_per_unit\'] if title in summary: summary[title][\'total_quantity\'] += quantity summary[title][\'total_sales\'] += sales_amount else: summary[title] = {\'total_quantity\': quantity, \'total_sales\': sales_amount} return summary"},{"question":"# Objective Create a function that uses seaborn to visualize data with error bars that demonstrate both measures of spread and uncertainty. You are expected to: 1. Implement multiple types of error bars in a single plot. 2. Customize the parameters of these error bars. 3. Generate and visualize regression fits with error bands. # Instructions 1. **Setup the Environment**: - Import necessary libraries (`numpy`, `pandas`, `seaborn`, `matplotlib`). - Set a consistent theme for seaborn plots. - Seed the random number generator using `np.random.seed`. 2. **Function Requirements**: - Define a function `plot_with_error_bars` that: - Accepts a Pandas DataFrame `df` with at least two columns: `x` and `y`. - Contains `errorbar_type`, a string specifying the type of error bar to use (\\"sd\\", \\"se\\", \\"pi\\", \\"ci\\"). - Accepts optional parameters `scale_or_width` and `n_boot` for fine-tuning error bars. - Plots a seaborn point plot with specified error bars. - Additionally plots a regression line with error bands. 3. **Input Parameters**: - `df`: DataFrame with two columns: `x` and `y`. - `errorbar_type`: \\"sd\\", \\"se\\", \\"pi\\", or \\"ci\\". - `scale_or_width`: A numeric value for scaling (if \\"sd\\" or \\"se\\") or specifying width (if \\"pi\\" or \\"ci\\"). Default is None. - `n_boot`: Number of bootstrap iterations for \\"ci\\". Default is 1000. 4. **Output**: - Display a combination of point plot, strip plot, and regression plot using seaborn. 5. **Constraints**: - Ensure handling of missing or invalid `errorbar_type` values with appropriate error messages. - Non-generic plot error functions or older seaborn versions (before v0.12) should not be used. # Example ```python import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Example Dataset np.random.seed(42) data = pd.DataFrame({ \\"x\\": np.linspace(0, 10, 50), \\"y\\": np.sin(np.linspace(0, 10, 50)) + np.random.normal(0, 0.1, 50) }) # Implementation of the required function def plot_with_error_bars(df, errorbar_type, scale_or_width=None, n_boot=1000): sns.set_theme(style=\\"darkgrid\\") plt.figure(figsize=(10, 6)) if errorbar_type not in [\\"sd\\", \\"se\\", \\"pi\\", \\"ci\\"]: raise ValueError(\\"Invalid errorbar_type. Choose from \'sd\', \'se\', \'pi\', \'ci\'.\\") # Plot the data sns.pointplot(x=df[\'x\'], y=df[\'y\'], errorbar=(errorbar_type, scale_or_width)) sns.stripplot(x=df[\'x\'], y=df[\'y\'], jitter=True, color=\\"gray\\") sns.regplot(x=df[\'x\'], y=df[\'y\'], ci=95) plt.show() # Example usage plot_with_error_bars(data, \\"ci\\", scale_or_width=95, n_boot=1000) ``` The function should create a combined visualization, applying different types of error bars and regression bands using seaborn. Use this example as a reference, but ensure to generalize the function to handle different types of errors and optional parameters effectively.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def plot_with_error_bars(df, errorbar_type, scale_or_width=None, n_boot=1000): Plots a seaborn point plot with error bars and a regression plot with error bands. Parameters: - df (pd.DataFrame): DataFrame with at least two columns: \'x\' and \'y\'. - errorbar_type (str): Type of error bar to use (\'sd\', \'se\', \'pi\', \'ci\'). - scale_or_width (numeric, optional): Scaling factor for \'sd\' or \'se\', or width for \'pi\' or \'ci\'. - n_boot (int, optional): Number of bootstrap iterations for \'ci\'. Default is 1000. Raises: - ValueError: If errorbar_type is not one of \'sd\', \'se\', \'pi\', \'ci\'. sns.set_theme(style=\\"darkgrid\\") plt.figure(figsize=(10, 6)) if errorbar_type not in [\\"sd\\", \\"se\\", \\"pi\\", \\"ci\\"]: raise ValueError(\\"Invalid errorbar_type. Choose from \'sd\', \'se\', \'pi\', \'ci\'.\\") # Plot the data sns.pointplot(x=\'x\', y=\'y\', data=df, errorbar=(errorbar_type, scale_or_width)) sns.stripplot(x=\'x\', y=\'y\', data=df, jitter=True, color=\\"gray\\") sns.regplot(x=\'x\', y=\'y\', data=df, ci=95) plt.show()"},{"question":"# BatchNorm Replacement with GroupNorm in PyTorch **Problem Statement:** You are given a PyTorch neural network model that includes Batch Normalization layers (BatchNorm2d). Your task is to implement a function `replace_batchnorm_with_groupnorm` that will iterate through the network and replace all instances of `BatchNorm2d` with `GroupNorm` layers as discussed in the provided documentation. Ensure that the replacement GroupNorm layers maintain the original channel dimensions and follow the criteria that `C % G == 0`. For simplicity, set `G = 32` as long as it divides `C` evenly; otherwise, set `G = C`. **Function Signature:** ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int = 32) -> torch.nn.Module: pass ``` **Input:** - `model`: A PyTorch neural network model (instance of `torch.nn.Module`). This model contains various layers including BatchNorm2d layers. - `num_groups`: An integer representing the preferred number of groups for GroupNorm. Default is 32. **Output:** - Returns the modified model with all `BatchNorm2d` layers replaced by `GroupNorm` layers. **Constraints:** - The model can have nested sub-modules. - Only replace instances of `torch.nn.BatchNorm2d`. - The `num_groups` for `GroupNorm` should be set such that `C % G == 0`. If not possible, `G = C`. **Example:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU() ) def forward(self, x): return self.features(x) model = SimpleModel() modified_model = replace_batchnorm_with_groupnorm(model) for layer in modified_model.features: if isinstance(layer, nn.BatchNorm2d): print(\\"BatchNorm2d found!\\") elif isinstance(layer, nn.GroupNorm): print(\\"GroupNorm found!\\") ``` **Task:** - Implement the `replace_batchnorm_with_groupnorm` function as specified. Good luck!","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int = 32) -> torch.nn.Module: Replace all instances of `BatchNorm2d` with `GroupNorm` in a given model. Args: - model (nn.Module): The input PyTorch model. - num_groups (int): The preferred number of groups for GroupNorm. Default is 32. Returns: - nn.Module: The modified model with BatchNorm2d replaced by GroupNorm. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features groups = num_groups if num_channels % num_groups == 0 else num_channels new_module = nn.GroupNorm(groups, num_channels) setattr(model, name, new_module) else: # Recursively apply to child modules replace_batchnorm_with_groupnorm(module, num_groups) return model"},{"question":"# Python Version Encoding and Decoding **Background:** CPython uses several macros to identify the version of the Python interpreter. These include the major, minor, micro version numbers, release level, and release serial. These components can be combined into a single 32-bit hexadecimal integer. For example, \\"3.4.1a2\\" is represented as `0x030401a2` and \\"3.10.0\\" is `0x030a00f0`. **Task:** Write two Python functions: 1. `encode_version(major, minor, micro, level, serial)`: This function takes the five components of the version and returns the encoded 32-bit hexadecimal integer. 2. `decode_version(hex_version)`: This function takes a 32-bit hexadecimal integer and returns the five components of the version as a tuple. The release levels should be represented as: - `\'a\'` (alpha) -> `0xA` - `\'b\'` (beta) -> `0xB` - `\'c\'` (release candidate) -> `0xC` - `\'f\'` (final) -> `0xF` **Constraints:** 1. The `major`, `minor`, and `micro` components will always be integers between 0 and 15. 2. The `level` will be one of `\'a\'`, `\'b\'`, `\'c\'`, or `\'f\'`. 3. The `serial` will be an integer between 0 and 15. **Function Signatures:** ```python def encode_version(major: int, minor: int, micro: int, level: str, serial: int) -> int: pass def decode_version(hex_version: int) -> (int, int, int, str, int): pass ``` **Examples:** Example 1: ```python encode_version(3, 4, 1, \'a\', 2) ``` Output: ``` 50595042 # which is 0x030401a2 in hexadecimal ``` Example 2: ```python decode_version(0x030a00f0) ``` Output: ``` (3, 10, 0, \'f\', 0) ``` Implement both `encode_version` and `decode_version` functions to correctly perform encoding and decoding as described.","solution":"def encode_version(major, minor, micro, level, serial): Encodes the version components into a 32-bit hexadecimal integer. :param major: Major version number (0-15) :param minor: Minor version number (0-15) :param micro: Micro version number (0-15) :param level: Release level (\'a\', \'b\', \'c\', \'f\') :param serial: Release serial (0-15) :return: Encoded 32-bit hexadecimal integer level_map = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} level_num = level_map[level] return (major << 24) | (minor << 16) | (micro << 8) | (level_num << 4) | serial def decode_version(hex_version): Decodes a 32-bit hexadecimal integer into version components. :param hex_version: Encoded 32-bit hexadecimal integer :return: Tuple of (major, minor, micro, level, serial) level_map = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} major = (hex_version >> 24) & 0xF minor = (hex_version >> 16) & 0xF micro = (hex_version >> 8) & 0xF level_num = (hex_version >> 4) & 0xF serial = hex_version & 0xF level = level_map[level_num] return (major, minor, micro, level, serial)"},{"question":"Question: Command Line Argument Parser You are required to write a Python script that processes command line arguments to provide specific functionality based on the options and arguments passed. The script should use the `getopt` module to implement this functionality. # Requirements: 1. The script should accept the following command line options: - Short options: - `-h`: Display help message and exit. - `-f`: Indicates that a file name will follow. - `-n`: Indicates that a number will follow. - Long options: - `--help`: Display help message and exit. - `--file=`: Indicates that a file name will follow. - `--number=`: Indicates that a number will follow. 2. When the `-h` or `--help` option is provided, the script should print a help message describing the usage of the script and exit. 3. The script should process the following scenarios: - If the `-f` or `--file=` option is provided, print the file name that follows. - If the `-n` or `--number=` option is provided, print the number that follows. 4. You should handle any invalid options by printing an error message and displaying the help message. # Expected Input and Output: - **Input**: Command line arguments passed to the script. - **Output**: Based on the provided options and arguments, the script should print relevant messages. Example usage: ``` python script.py -h # Output Usage: script.py [options] Options: -h, --help Display this help message. -f, --file= Specify the file name. -n, --number= Specify the number. python script.py -f example.txt # Output File name: example.txt python script.py --number=42 # Output Number: 42 python script.py -x # Output Error: option -x not recognized Usage: script.py [options] Options: -h, --help Display this help message. -f, --file= Specify the file name. -n, --number= Specify the number. ``` # Constraints: - Assume that the script will only be called from the command line with valid Unicode strings. - Handle all exceptions related to invalid usage to make the script robust. # Implementation: Implement the function `main()` which processes the command line arguments as described. The function should be designed to be executed as the main script. ```python import getopt import sys def usage(): print(\\"Usage: script.py [options]\\") print(\\"Options:\\") print(\\"-h, --help Display this help message.\\") print(\\"-f, --file= Specify the file name.\\") print(\\"-n, --number= Specify the number.\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"hf:n:\\", [\\"help\\", \\"file=\\", \\"number=\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") usage() sys.exit(2) for o, a in opts: if o in (\\"-h\\", \\"--help\\"): usage() sys.exit() elif o in (\\"-f\\", \\"--file\\"): print(f\\"File name: {a}\\") elif o in (\\"-n\\", \\"--number\\"): print(f\\"Number: {a}\\") else: assert False, \\"Unhandled option\\" if __name__ == \\"__main__\\": main() ``` # Notes: - Please ensure that your implementation follows the example output strictly. - Test your script with different combinations of command line arguments to verify its correctness.","solution":"import getopt import sys def usage(): print(\\"Usage: script.py [options]\\") print(\\"Options:\\") print(\\"-h, --help Display this help message.\\") print(\\"-f, --file= Specify the file name.\\") print(\\"-n, --number= Specify the number.\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"hf:n:\\", [\\"help\\", \\"file=\\", \\"number=\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") usage() sys.exit(2) for o, a in opts: if o in (\\"-h\\", \\"--help\\"): usage() sys.exit() elif o in (\\"-f\\", \\"--file\\"): print(f\\"File name: {a}\\") elif o in (\\"-n\\", \\"--number\\"): print(f\\"Number: {a}\\") else: assert False, \\"Unhandled option\\" if __name__ == \\"__main__\\": main()"},{"question":"Objective: Create a function that processes a pandas DataFrame containing potentially messy, real-world data with duplicate row or column labels. The function should deduplicate the DataFrame based on the column specified by the user and ensure that no duplicate labels are introduced during further operations. Function Signature: ```python import pandas as pd def process_dataframe(df: pd.DataFrame, dedup_col: str) -> pd.DataFrame: Processes a DataFrame by removing duplicate entries based on a specified column and ensures that no duplicate labels are allowed in the DataFrame thereafter. Parameters: df (pd.DataFrame): Input DataFrame that may contain duplicate labels. dedup_col (str): Column name based on which duplicates should be removed. Returns: pd.DataFrame: Processed DataFrame with duplicates removed and no duplicate labels allowed. pass ``` Input: 1. `df`: A pandas DataFrame which may contain duplicate rows or columns. 2. `dedup_col`: A string representing the column name based on which the deduplication will be performed. Output: - The function should return a pandas DataFrame where: - Rows with duplicate entries based on the `dedup_col` are removed, keeping the first occurrence. - The resulting DataFrame should have the `allows_duplicate_labels` flag set to `False`. Constraints: - The input DataFrame (`df`) can have duplicate rows and columns, and it may contain any type of data. - The `dedup_col` will always be a valid column name in the DataFrame. Example: ```python import pandas as pd # Sample DataFrame data = { \\"A\\": [1, 2, 2, 4], \\"B\\": [5, 6, 7, 8], \\"C\\": [\\"foo\\", \\"bar\\", \\"baz\\", \\"qux\\"] } df = pd.DataFrame(data, index=[\\"a\\", \\"b\\", \\"b\\", \\"d\\"]) # Deduplicate based on column \'A\' dedup_df = process_dataframe(df, \\"A\\") print(dedup_df) ``` Expected Output: ``` A B C a 1 5 foo b 2 6 bar d 4 8 qux ``` Additional aspects: 1. The function should initially allow duplicate labels to emulate real-world data processing. 2. After deduplication, the function should set `allows_duplicate_labels` to `False`. Notes: - The solution should appropriately handle edge cases, such as empty DataFrame or DataFrame without any duplicates. - Efficient handling of large DataFrames is expected, utilizing pandas\' built-in methods wherever possible. Evaluate the students based on: - Correctness and efficiency of the implementation. - Appropriate use of pandas methods for deduplication and handling of duplicate labels. - Adherence to function signature and input/output formats specified.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame, dedup_col: str) -> pd.DataFrame: Processes a DataFrame by removing duplicate entries based on a specified column and ensures that no duplicate labels are allowed in the DataFrame thereafter. Parameters: df (pd.DataFrame): Input DataFrame that may contain duplicate labels. dedup_col (str): Column name based on which duplicates should be removed. Returns: pd.DataFrame: Processed DataFrame with duplicates removed and no duplicate labels allowed. # Setting allows_duplicate_labels to True to process the DataFrame initially with real-world scenarios pd.options.mode.chained_assignment = None # Disable chained assignment warning df.flags.allows_duplicate_labels = True # Drop duplicate rows based on the specified column df = df.drop_duplicates(subset=[dedup_col]) # Remove duplicate index labels by resetting and reapplying the index df.reset_index(drop=True, inplace=True) # Ensure no duplicate labels allowed df.flags.allows_duplicate_labels = False return df"},{"question":"# Advanced Python Coding Assessment Context Asynchronous programming is a critical aspect of modern software development, allowing for efficient and effective management of I/O-bound and high-level structured network code. In Python, `asyncio` is a core library used for this purpose. Problem Statement You are required to implement an asynchronous file reader that reads data from a file in chunks. The reading operation should handle various exceptions that might arise during the process, as outlined in the `asyncio` documentation. Function Signature ```python import asyncio async def async_file_reader(file_path: str, chunk_size: int, timeout: float) -> bytes: Reads data from the given file asynchronously in specified chunk sizes, with a given timeout. Args: - file_path (str): Path to the file to be read. - chunk_size (int): Size of each chunk to read from the file. - timeout (float): Timeout for the reading operation in seconds. Returns: - bytes: The full data read from the file. Raises: - asyncio.TimeoutError: If the read operation exceeds the timeout. - asyncio.IncompleteReadError: If the read operation does not complete fully. - asyncio.LimitOverrunError: If reading exceeds the size of a buffer limit while looking for a separator. - asyncio.CancelledError: If the read operation is cancelled. - asyncio.InvalidStateError: If an invalid state is encountered during the read operation. ``` Requirements 1. **Reading process**: The function should read the file in chunks specified by `chunk_size`. 2. **Timeout handling**: Use the `asyncio.TimeoutError` to handle situations where the reading process exceeds the given timeout. 3. **Partial reads**: Handle any reads that do not complete fully with `asyncio.IncompleteReadError`. 4. **Buffer limits**: If the buffer limit is reached while looking for a separator, manage this scenario with `asyncio.LimitOverrunError`. 5. **Cancellation**: If the operation is cancelled, use `asyncio.CancelledError` to handle it. 6. **Invalid state handling**: If the operation encounters an invalid state, raise an `asyncio.InvalidStateError`. Constraints - Assume the existence of a separator that you are checking for in the file read operation. - The function should be capable of working with large files efficiently. - All exceptions should be logged or handled appropriately, ensuring a comprehensive read process with accurate error reporting. Example Usage ```python import asyncio # Assuming an example file \'example.txt\' exists with some content file_path = \'example.txt\' chunk_size = 1024 timeout = 5.0 async def main(): try: data = await async_file_reader(file_path, chunk_size, timeout) print(data) except Exception as e: print(f\\"Error occurred: {e}\\") asyncio.run(main()) ```","solution":"import asyncio import os async def async_file_reader(file_path: str, chunk_size: int, timeout: float) -> bytes: Reads data from the given file asynchronously in specified chunk sizes, with a given timeout. Args: - file_path (str): Path to the file to be read. - chunk_size (int): Size of each chunk to read from the file. - timeout (float): Timeout for the reading operation in seconds. Returns: - bytes: The full data read from the file. Raises: - asyncio.TimeoutError: If the read operation exceeds the timeout. - asyncio.IncompleteReadError: If the read operation does not complete fully. - asyncio.LimitOverrunError: If reading exceeds the size of a buffer limit while looking for a separator. - asyncio.CancelledError: If the read operation is cancelled. - asyncio.InvalidStateError: If an invalid state is encountered during the read operation. async def read_chunk(file, size): data = await asyncio.get_event_loop().run_in_executor(None, file.read, size) if len(data) < size: raise asyncio.IncompleteReadError(data, size) return data data = bytearray() try: with open(file_path, \'rb\') as file: while True: chunk = await asyncio.wait_for(read_chunk(file, chunk_size), timeout) if not chunk: break data.extend(chunk) except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"The read operation exceeded the timeout.\\") except asyncio.IncompleteReadError as e: raise e except asyncio.CancelledError: raise asyncio.CancelledError(\\"The read operation was cancelled.\\") except asyncio.InvalidStateError: raise asyncio.InvalidStateError(\\"An invalid state was encountered during the read operation.\\") except asyncio.LimitOverrunError as e: raise e return bytes(data)"},{"question":"**Objective:** Assess students\' understanding of seaborn\'s `swarmplot` function and its customization options. Problem Statement: You are provided with a dataset on the tips received by waiters/waitresses in a restaurant. The dataset consists of the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of the day (Lunch/Dinner). - `size`: Size of the party. Your task is to write a function `create_swarmplot(data)` that: 1. Receives a pandas DataFrame `data` as input. 2. Creates and saves a multi-faceted swarmplot visualizing the distribution of `total_bill` for each day split by `sex`. Each facet should correspond to a different day of the week. 3. Customizes the plot to: - Use different colors for each `sex` category. - Adjust the point size to 5 to avoid overlapping. - Include a title for each subplot indicating the day of the week. **Input Format:** - A pandas DataFrame `data` containing the columns mentioned above. **Output Format:** - None. The function should save the plot as \'swarmplot.png\' in the current working directory. **Constraints:** - Ensure that the `total_bill` variable is visualized on the y-axis, and the `time` variable is visualized on the x-axis. - Handle any missing values in the data by dropping the rows with missing entries. **Performance Requirements:** - The function should handle datasets with up to 10,000 rows efficiently. Here is the skeleton function for your reference: ```python import seaborn as sns import matplotlib.pyplot as plt def create_swarmplot(data): # Your code here # Example usage: if __name__ == \\"__main__\\": tips = sns.load_dataset(\\"tips\\") create_swarmplot(tips) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarmplot(data): # Drop rows with missing values data = data.dropna() # Create the facet grid g = sns.FacetGrid(data, col=\\"day\\", col_wrap=2, height=4) # Map the swarmplot onto the facet grid g = g.map(sns.swarmplot, \\"time\\", \\"total_bill\\", \\"sex\\", palette=\\"Set1\\", size=5) # Add titles to each subplot days = data[\'day\'].unique() for ax, day in zip(g.axes.flat, days): ax.set_title(f\'Day: {day}\') # Save the plot to a file plt.savefig(\'swarmplot.png\')"},{"question":"Objective: You are required to implement an SVM classifier for a multi-class classification problem using scikit-learn. The goal is to demonstrate your understanding of SVM concepts, kernel functions, and the impact of parameters like `C` and `gamma`. Problem Statement: **Task**: Implement and train an SVM classifier on the Iris dataset from `sklearn.datasets`. You will: 1. Implement a custom polynomial kernel. 2. Train the SVM using the custom kernel. 3. Tune the hyperparameters `C` and `gamma` using grid search. 4. Evaluate the performance of the classifier. Steps: 1. **Load the Iris Dataset**: - Use the Iris dataset from `sklearn.datasets`. 2. **Custom Kernel**: - Implement a polynomial kernel function. ```python def polynomial_kernel(X, Y, degree=3, gamma=1, coef0=1): # Your code here to compute the polynomial kernel matrix ``` 3. **Train the SVM Classifier**: - Use the custom polynomial kernel to train the SVM classifier. ```python from sklearn.svm import SVC # Load Iris dataset from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) # TODO: Implement the custom polynomial kernel function here # Create an SVM classifier with the custom kernel svc = SVC(kernel=polynomial_kernel) # Fit the classifier on the data svc.fit(X, y) ``` 4. **Hyperparameter Tuning**: - Use grid search to find the best values for `C` and `gamma`. ```python from sklearn.model_selection import GridSearchCV # Define the parameter grid param_grid = { \'C\': [0.1, 1, 10], \'gamma\': [0.001, 0.01, 0.1], \'degree\': [2, 3, 4] } # Perform grid search grid_search = GridSearchCV(SVC(kernel=polynomial_kernel), param_grid, cv=5) grid_search.fit(X, y) # Output the best parameters print(\\"Best parameters found:\\", grid_search.best_params_) ``` 5. **Evaluation**: - Evaluate the performance of the trained classifier using cross-validation and print the classification report. ```python from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report best_svc = grid_search.best_estimator_ # Evaluate using cross-validation scores = cross_val_score(best_svc, X, y, cv=5) print(\\"Cross-validation scores:\\", scores) print(\\"Mean cross-validation score:\\", scores.mean()) # Print the classification report y_pred = best_svc.predict(X) print(\\"Classification report:n\\", classification_report(y, y_pred)) ``` Notes: - Ensure your custom polynomial kernel function is flexible in accepting different `degree`, `gamma`, and `coef0` parameters. - Use a seed for reproducibility wherever necessary. - Bonus: Demonstrate the effect of scaling the data using `StandardScaler` before training the SVM. Expected Output: - Best parameters found by grid search. - Cross-validation scores and mean score. - Classification report showing precision, recall, and F1-score for each class.","solution":"from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.metrics import classification_report from sklearn.preprocessing import StandardScaler import numpy as np # Define custom polynomial kernel function def polynomial_kernel(X, Y, degree=3, gamma=1, coef0=1): Computes the polynomial kernel between X and Y. K(X, Y) = (gamma * (X . Y^T) + coef0)^degree Parameters: - X: array-like, shape (n_samples_X, n_features) - Y: array-like, shape (n_samples_Y, n_features) - degree: int, default 3 - gamma: float, default 1 - coef0: float, default 1 Returns: - Kernel matrix: array, shape (n_samples_X, n_samples_Y) return (gamma * np.dot(X, Y.T) + coef0) ** degree # Load Iris dataset X, y = load_iris(return_X_y=True) # Scale the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Create an SVM classifier with the custom kernel svc = SVC(kernel=polynomial_kernel) # Define the parameter grid for grid search param_grid = { \'C\': [0.1, 1, 10], \'gamma\': [0.001, 0.01, 0.1], \'degree\': [2, 3, 4] } # Perform grid search using cross-validation grid_search = GridSearchCV(SVC(kernel=polynomial_kernel), param_grid, cv=5) grid_search.fit(X_scaled, y) # Output the best parameters best_params = grid_search.best_params_ print(\\"Best parameters found:\\", best_params) # Evaluate the best model best_svc = grid_search.best_estimator_ # Evaluate using cross-validation scores = cross_val_score(best_svc, X_scaled, y, cv=5) print(\\"Cross-validation scores:\\", scores) print(\\"Mean cross-validation score:\\", scores.mean()) # Print the classification report y_pred = best_svc.predict(X_scaled) print(\\"Classification report:n\\", classification_report(y, y_pred))"},{"question":"Question You are tasked with writing a Python script to manage system logging for a hypothetical application using the `syslog` module. This script should be capable of logging messages with different priorities and facilities, setting and resetting log options, and controlling which priority levels are logged. # Requirements: 1. Implement a function `initialize_logging(ident: str, logoption: int, facility: int) -> None` that initializes logging with the given identifier, log options, and facility. 2. Implement a function `log_message(priority: int, message: str) -> None` that logs a message with the specified priority. 3. Implement a function `set_priority_mask(maskpri: int) -> int` that sets the priority mask to the specified value and returns the previous mask value. 4. Implement a function `reset_logging() -> None` that resets the logging configuration to its default state. 5. Demonstrate the usage of these functions in a main block by performing the following tasks: - Initialize logging with `ident=\\"my_app\\"`, `logoption=syslog.LOG_PID`, and `facility=syslog.LOG_USER`. - Log a message with `priority=syslog.LOG_INFO` and `message=\\"Application started\\"`. - Update the logging priority mask to only log messages of priority `LOG_WARNING` and higher. - Log a message with `priority=syslog.LOG_DEBUG` and `message=\\"This debug message should not appear\\"`. - Log a message with `priority=syslog.LOG_ERR` and `message=\\"An error occurred\\"`. - Reset the logging configuration. - Log a message with `priority=syslog.LOG_INFO` and `message=\\"Logging reset\\"`. # Function Signature ```python import syslog def initialize_logging(ident: str, logoption: int, facility: int) -> None: pass def log_message(priority: int, message: str) -> None: pass def set_priority_mask(maskpri: int) -> int: pass def reset_logging() -> None: pass if __name__ == \\"__main__\\": # Demonstration code as outlined in the requirements pass ``` # Constraints - Make sure to handle any potential exceptions that might occur during the syslog operations. - Ensure that your script adheres to best practices for logging and error handling. # Example Output The script should log messages based on the given priorities and configurations, but the exact output will depend on the system\'s syslog configuration and may not be captured by the script itself.","solution":"import syslog def initialize_logging(ident: str, logoption: int, facility: int) -> None: Initializes logging with the given identifier, log options, and facility. syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(priority: int, message: str) -> None: Logs a message with the specified priority. syslog.syslog(priority, message) def set_priority_mask(maskpri: int) -> int: Sets the priority mask and returns the previous mask value. return syslog.setlogmask(maskpri) def reset_logging() -> None: Resets the logging configuration to its default state. syslog.closelog() if __name__ == \\"__main__\\": # Demonstration code as outlined in the requirements initialize_logging(\\"my_app\\", syslog.LOG_PID, syslog.LOG_USER) log_message(syslog.LOG_INFO, \\"Application started\\") previous_mask = set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) log_message(syslog.LOG_DEBUG, \\"This debug message should not appear\\") log_message(syslog.LOG_ERR, \\"An error occurred\\") reset_logging() log_message(syslog.LOG_INFO, \\"Logging reset\\")"},{"question":"Coding Assessment Question # Context In this coding task, you are required to demonstrate your understanding of error handling in Python\'s `asyncio` library. You will write a function that performs an asynchronous file operation and handles various exceptions that may occur during the process. # Task Write a function `read_file_async` that reads data from a file asynchronously using the `asyncio` library. Your function should: 1. Use appropriate `asyncio` methods to open and read the file. 2. Catch and handle the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 3. Return the contents of the file if read successfully. 4. If an exception occurs, log the error with an appropriate message and return `None`. # Input - `file_path` (str): The path to the file to be read. - `timeout` (Optional[int]): The maximum time allowed for reading the file. Default is 10 seconds. # Output - Returns the contents of the file as a string if read successfully, otherwise returns `None`. # Constraints - You must use the `asyncio` library for the asynchronous operation. - You should handle each exception as outlined in the documentation provided. # Example ```python import asyncio async def read_file_async(file_path: str, timeout: int = 10) -> str: # Implement your function here # Example usage file_content = asyncio.run(read_file_async(\\"example.txt\\")) ``` # Notes - Ensure your implementation handles all specified exceptions. - Consider using `asyncio.StreamReader` and `asyncio.StreamWriter` for reading the file. - Test your function with different scenarios to validate exception handling.","solution":"import asyncio import logging logging.basicConfig(level=logging.ERROR) async def read_file_async(file_path: str, timeout: int = 10) -> str: try: reader, writer = await asyncio.open_connection(\'localhost\', 8888) file_content = await asyncio.wait_for(reader.read(), timeout) return file_content except asyncio.TimeoutError: logging.error(\\"File read operation timed out\\") except asyncio.CancelledError: logging.error(\\"File read operation was cancelled\\") except asyncio.InvalidStateError: logging.error(\\"The transport is in an invalid state\\") except asyncio.SendfileNotAvailableError: logging.error(\\"Sending file operation is not available\\") except asyncio.IncompleteReadError as e: logging.error(f\\"Incomplete read error occurred: {e}\\") except asyncio.LimitOverrunError as e: logging.error(f\\"Read limit overrun error occurred: {e}\\") except Exception as e: logging.error(f\\"An unexpected error occurred: {e}\\") return None"},{"question":"Context: You are tasked with writing a function that will read and parse a `.netrc` file to extract login information for a given host, using the functionalities provided by the `netrc` module. Your function should also handle various potential exceptions appropriately and return meaningful error messages. Task: Implement a function called `retrieve_login_info` that takes in a single string parameter `host`, which is the hostname for which to retrieve login credentials. The function should follow these steps: 1. Attempt to parse the `.netrc` file from the user\'s home directory using the `netrc.netrc()` class. 2. Handle the following exceptions: - `FileNotFoundError`: Return the string \\"Error: .netrc file not found.\\" - `netrc.NetrcParseError`: Return a string that includes \\"Error parsing .netrc file on line X: [error message]\\", where X and [error message] are the line number and error message provided by the exception. 3. If the `.netrc` file is parsed successfully, attempt to retrieve the login credentials for the specified `host` using the `authenticators` method. 4. If credentials for the specified `host` are found, return them as a dictionary with keys \\"login\\", \\"account\\", and \\"password\\". 5. If no credentials are found for the specified `host` (i.e., the `authenticators` method returns `None`), return the string \\"No credentials found for the specified host.\\" Constraints: - Do not write to any files on the file system. - Assume all passwords follow the limitations described in the `netrc` documentation. Function Signature: ```python from typing import Union, Dict def retrieve_login_info(host: str) -> Union[Dict[str, str], str]: pass ``` Example Usage: ```python # Example 1 print(retrieve_login_info(\\"example.com\\")) # Output Example: # { # \\"login\\": \\"user123\\", # \\"account\\": \\"example_account\\", # \\"password\\": \\"password123\\" # } # Example 2 print(retrieve_login_info(\\"nonexistent_host\\")) # Output Example: # \\"No credentials found for the specified host.\\" # Example 3 print(retrieve_login_info(\\"bad_host\\")) # Output Example: # \\"Error parsing .netrc file on line 12: unexpected token\\" ``` Your implementation should provide efficient handling of the `.netrc` file and robust error management, demonstrating a thorough understanding of the `netrc` module.","solution":"from typing import Union, Dict import netrc import os def retrieve_login_info(host: str) -> Union[Dict[str, str], str]: try: # Get the user\'s home directory and read the .netrc file home_dir = os.path.expanduser(\\"~\\") netrc_file = os.path.join(home_dir, \'.netrc\') netrc_data = netrc.netrc(netrc_file) credentials = netrc_data.authenticators(host) if credentials: return { \\"login\\": credentials[0], \\"account\\": credentials[1], \\"password\\": credentials[2] } else: return \\"No credentials found for the specified host.\\" except FileNotFoundError: return \\"Error: .netrc file not found.\\" except netrc.NetrcParseError as e: return f\\"Error parsing .netrc file on line {e.lineno}: {e.msg}\\""},{"question":"# Question You are tasked with implementing a Python class that mimics the behavior of certain low-level Python object manipulations. The goal is to demonstrate an understanding of reference counting, object types, and object sizes - concepts that are integral to the python310 C-API but implementing them in Python. Requirements: 1. Implement a class named `PyObjectLike` with the following features: - An initializer that takes a `type_name` and initializes the reference count to 1. - A method `get_refcnt` that returns the current reference count. - A method `incref` that increments the reference count. - A method `decref` that decrements the reference count and raises a `ReferenceError` if the reference count goes below 1. - A method `get_type` that returns the type name of the object. 2. Implement a subclass of `PyObjectLike` named `PyVarObjectLike` with the following features: - An initializer that takes a `type_name` and `size` and calls the superclass initializer. - A method `get_size` that returns the size of the object. - A method `set_size` that sets the size of the object to a new value. 3. The classes should mimic behavior typically managed by the Python runtime but need to be managed manually here to demonstrate comprehension of the underlying concepts. Input and Output Formats: - The `PyObjectLike`\'s and `PyVarObjectLike`\'s initializers take parameters `type_name` and `size` respectively, and set initial values for the object\'s reference count and type. - The `get_refcnt`, `get_type`, and `get_size` methods return the current reference count, type name, and size respectively. - The `incref` and `decref` methods modify the reference count as described. - The `set_size` method changes the object\'s size. Constraints: - The `decref` method must not allow the reference count to go below 1 and must raise a `ReferenceError` with the message \\"Reference count cannot be less than 1\\" if attempted. Here is the class scaffold for the solution: ```python class PyObjectLike: def __init__(self, type_name): # Implementation here def get_refcnt(self): # Implementation here def incref(self): # Implementation here def decref(self): # Implementation here def get_type(self): # Implementation here class PyVarObjectLike(PyObjectLike): def __init__(self, type_name, size): # Implementation here def get_size(self): # Implementation here def set_size(self, new_size): # Implementation here ``` You are expected to implement all the methods specified above to meet the described functionality. Example Usage: ```python obj = PyObjectLike(\\"CustomType\\") print(obj.get_type()) # Output: \\"CustomType\\" print(obj.get_refcnt()) # Output: 1 obj.incref() print(obj.get_refcnt()) # Output: 2 obj.decref() print(obj.get_refcnt()) # Output: 1 var_obj = PyVarObjectLike(\\"CustomVarType\\", 10) print(var_obj.get_type()) # Output: \\"CustomVarType\\" print(var_obj.get_size()) # Output: 10 var_obj.set_size(20) print(var_obj.get_size()) # Output: 20 ``` Ensure your implementation passes the above example usage.","solution":"class PyObjectLike: def __init__(self, type_name): self.type_name = type_name self.refcnt = 1 def get_refcnt(self): return self.refcnt def incref(self): self.refcnt += 1 def decref(self): if self.refcnt > 1: self.refcnt -= 1 else: raise ReferenceError(\\"Reference count cannot be less than 1\\") def get_type(self): return self.type_name class PyVarObjectLike(PyObjectLike): def __init__(self, type_name, size): super().__init__(type_name) self.size = size def get_size(self): return self.size def set_size(self, new_size): self.size = new_size"},{"question":"# Python Built-in Functions & Iterators: Comprehensive Problem **Objective:** Your task is to create a custom class `Cycle` that simulates a circular iterator. This iterator should repeat the elements of a list infinitely. Additionally, implement a `callable` method that utilizes dynamic attribute access to modify the behavior of the iterator. **Class Specifications:** 1. **Class Name:** `Cycle` 2. **Initialization:** - The class should accept a list of elements. - The class should initialize the elements and keep track of the current position. 3. **Methods:** - `__iter__(self)`: This should return the iterator object itself. - `__next__(self)`: This should return the next element in the cycle. If the end of the list is reached, it starts from the beginning again. - `__call__(self, method_name)`: This method should dynamically add a new method to the `Cycle` class at runtime. The new method should be specified by its name and behavior. 4. **Additional Dynamic Methods:** - Implement an additional static method `skip(self, n)` which skips the next `n` elements in the cycle. - Implement an additional class method `reset(cls)` which resets the cycle to its initial state. **Input and Output Specifications:** - **Input:** - A list of elements. - Dynamic method name and its behavior which needs to be added to the class. - **Output:** - Infinite iteration through elements in the list. - Correct execution of dynamically added methods. **Example Usage:** ```python # Instantiate the Cycle class with a list cycle_obj = Cycle([1, 2, 3]) # Iterate through the cycle (first 5 elements for example) print(next(cycle_obj)) # Output: 1 print(next(cycle_obj)) # Output: 2 print(next(cycle_obj)) # Output: 3 print(next(cycle_obj)) # Output: 1 print(next(cycle_obj)) # Output: 2 # Adding a dynamic method \'skip\' to skip next n elements cycle_obj(\'skip\') # Using the skip method to skip next 2 elements cycle_obj.skip(2) print(next(cycle_obj)) # Output: 2 (as it skips 3 and comes back to 1, then skips 1 and outputs 2) # Adding a dynamic method \'reset\' to reset the cycle cycle_obj(\'reset\') # Using the reset method cycle_obj.reset() print(next(cycle_obj)) # Output: 1 (cycle reset to initial state) ``` **Constraints:** 1. The input list length `n` will be `1 ≤ n ≤ 100`. 2. Avoid using built-in libraries that directly provide similar functionality (like itertools.cycle). Implement the class with the specifications and behaviors as described.","solution":"class Cycle: def __init__(self, elements): self.elements = elements self.position = 0 def __iter__(self): return self def __next__(self): if not self.elements: raise StopIteration element = self.elements[self.position] self.position = (self.position + 1) % len(self.elements) return element def __call__(self, method_name): if method_name == \'skip\': self.skip = self._skip elif method_name == \'reset\': self.reset = self._reset def _skip(self, n): self.position = (self.position + n) % len(self.elements) @classmethod def _reset(cls, instance): instance.position = 0"},{"question":"Objective: Your task is to implement a function that manipulates the display options of pandas to present data from a DataFrame in a specific format. This will involve setting, getting, and resetting pandas options, as well as using the `option_context()` function for temporary changes. Problem Statement: Implement the `configure_pandas_display` function that accepts a DataFrame and returns a dictionary of configurations showcasing different pandas options in action. Function Signature: ```python import pandas as pd def configure_pandas_display(df: pd.DataFrame) -> dict: Function to configure pandas display settings. Args: df (pd.DataFrame): The DataFrame to be displayed. Returns: dict: A dictionary containing the following keys: - \\"default_display\\": Default display of the DataFrame. - \\"max_rows\\": Display of DataFrame limited to max rows. - \\"high_precision\\": Display of DataFrame with high precision. - \\"option_context_example\\": Temporary configuration using option_context. pass ``` Input: - `df`: A pandas DataFrame, which should be displayed and manipulated according to the scenarios outlined in the problem. Expected Output: - A dictionary with the following keys and corresponding DataFrame displays: - `\\"default_display\\"`: The default display of the DataFrame. - `\\"max_rows\\"`: The DataFrame display limited to 5 rows using the `display.max_rows` option. - `\\"high_precision\\"`: The DataFrame displayed with precision set to 10 using the `display.precision` option. - `\\"option_context_example\\"`: The DataFrame displayed within an `option_context` where `display.max_columns` is set to 3 and `display.expand_frame_repr` is set to True. Example: ```python import pandas as pd import numpy as np # Sample DataFrame data = np.random.randn(20, 5) df = pd.DataFrame(data, columns=[f\'col_{i}\' for i in range(5)]) # Function call result = configure_pandas_display(df) # Printing the resulting dictionary of displays for key, value in result.items(): print(f\\"n{key}:\\") print(value) # The function should output DataFrame displays according to the specified settings. ``` Constraints: - You can assume the DataFrame will be of manageable size for display settings. - Utilize **pandas options** effectively to achieve the desired configurations. - Reset the options where necessary to avoid side effects on other parts of code. Implementation Hints: 1. Use `pd.set_option` and `pd.get_option` to manage the settings for DataFrame display. 2. The `option_context` should be used to temporarily set options within a specific block of code. 3. Make sure to reset any options to their default after you\'re done with the display manipulation.","solution":"import pandas as pd def configure_pandas_display(df: pd.DataFrame) -> dict: Function to configure pandas display settings. Args: df (pd.DataFrame): The DataFrame to be displayed. Returns: dict: A dictionary containing the following keys: - \\"default_display\\": Default display of the DataFrame as a string. - \\"max_rows\\": Display of DataFrame limited to max rows as a string. - \\"high_precision\\": Display of DataFrame with high precision as a string. - \\"option_context_example\\": Temporary configuration using option_context as a string. # Default display default_display = df.to_string() # Setting display.max_rows to 5 pd.set_option(\'display.max_rows\', 5) max_rows_display = df.to_string() pd.reset_option(\'display.max_rows\') # Resetting to default # Setting display.precision to 10 pd.set_option(\'display.precision\', 10) high_precision_display = df.to_string() pd.reset_option(\'display.precision\') # Resetting to default # Using option_context for temporary settings with pd.option_context(\'display.max_columns\', 3, \'display.expand_frame_repr\', True): option_context_display = df.to_string() # Returning dictionary with all configurations return { \\"default_display\\": default_display, \\"max_rows\\": max_rows_display, \\"high_precision\\": high_precision_display, \\"option_context_example\\": option_context_display }"},{"question":"Objective Demonstrate the understanding of `__future__` module in Python and its impact on interpreter behavior regarding future feature implementations. Task You are tasked with implementing a function that simulates handling multiple versions of Python\'s behavior regarding feature implementation. The function `check_future_features` will take a list of tuples as input. Each tuple contains a feature string and a Python version tuple. The function will output a dictionary where each feature string maps to a boolean indicating whether the feature must be used as per the given Python version. Function Signature ```python def check_future_features(features: List[Tuple[str, Tuple[int, int, int, str, int]]]) -> Dict[str, bool]: ``` Input * `features` (List[Tuple[str, Tuple[int, int, int, str, int]]]): A list of tuples where each tuple consists of: * A feature name as a string (e.g., \'nested_scopes\', \'generators\'). * A Python version tuple of the form `(PY_MAJOR_VERSION, PY_MINOR_VERSION, PY_MICRO_VERSION, PY_RELEASE_LEVEL, PY_RELEASE_SERIAL)`. Output * Returns a dictionary. * Each key is a feature name. * Each value is a boolean: * `True` if the provided Python version requires the feature to be used as per `MandatoryRelease`. * `False` otherwise. Examples ```python features = [ (\'nested_scopes\', (2, 1, 0, \'beta\', 1)), (\'generators\', (2, 3, 0, \'final\', 0)), (\'print_function\', (2, 7, 0, \'final\', 0)) ] assert check_future_features(features) == { \'nested_scopes\': False, \'generators\': False, \'print_function\': False } features = [ (\'unicode_literals\', (3, 0, 0, \'final\', 0)), (\'generator_stop\', (3, 7, 0, \'final\', 0)) ] assert check_future_features(features) == { \'unicode_literals\': True, \'generator_stop\': True } ``` Notes * Use the documentation above to determine when a feature became mandatory. * You may assume that the inputs will follow the format described and focus on implementing correct logic based on the provided information. * Pay attention to the distinction between `OptionalRelease` and `MandatoryRelease` when implementing your function. Constraints * The input list may contain up to 1000 tuples. * Valid version tuples adhere strictly to the format provided in the problem description.","solution":"from typing import List, Tuple, Dict def check_future_features(features: List[Tuple[str, Tuple[int, int, int, str, int]]]) -> Dict[str, bool]: mandatory_versions = { \'nested_scopes\': (2, 2, 0, \'final\', 0), \'generators\': (2, 3, 0, \'final\', 0), \'division\': (3, 0, 0, \'final\', 0), \'absolute_import\': (3, 0, 0, \'final\', 0), \'with_statement\': (2, 6, 0, \'final\', 0), \'print_function\': (3, 0, 0, \'final\', 0), \'unicode_literals\': (3, 0, 0, \'final\', 0), \'generator_stop\': (3, 5, 0, \'final\', 0) } def compare_versions(v1, v2): if v1[:4] < v2[:4]: return True if v1[:4] == v2[:4] and v1[4] <= v2[4]: return True return False result = {} for feature, version in features: if feature in mandatory_versions: result[feature] = compare_versions(mandatory_versions[feature], version) else: result[feature] = False return result"},{"question":"**Objective:** Demonstrate your understanding of using the `Nearest Neighbors` functionalities from the Scikit-Learn library. This question involves implementing a custom function utilizing the `NearestNeighbors` and `KNeighborsClassifier` classes. # Problem Statement You are tasked with implementing a custom function to: 1. Identify the nearest neighbors for a given dataset using multiple metrics. 2. Classify new data points based on the nearest neighbors identified using custom weighting of distances. # Function Signature ```python def custom_nearest_neighbors_classification(training_data: np.ndarray, training_labels: np.ndarray, test_data: np.ndarray, n_neighbors: int, weights_fn) -> np.ndarray: Classifies test data points based on the nearest neighbors from training data. Parameters: - training_data (np.ndarray): A 2D array where each row represents a training sample and each column represents a feature. - training_labels (np.ndarray): A 1D array where each element is the label corresponding to a training sample. - test_data (np.ndarray): A 2D array where each row represents a test sample and each column represents a feature. - n_neighbors (int): The number of nearest neighbors to consider during classification. - weights_fn (callable): A function to compute weights for the neighbors based on their distances. Returns: - np.ndarray: A 1D array of predicted labels for each test sample. ``` # Requirements: 1. **Nearest Neighbors Identification:** - Use `NearestNeighbors` from `sklearn.neighbors` to find the nearest neighbors for the provided test data. Implement this for at least two different distance metrics (`euclidean` and `manhattan`). 2. **Custom Weight Calculation:** - Implement a custom weight calculation that uses the provided `weights_fn`, which is a callable that takes an array of distances and returns an array of weights. This will be used to compute the weighted vote for classification. 3. **Classification:** - Use the `KNeighborsClassifier` to predict the labels of the test data based on the nearest neighbors identified and the custom weights computed. # Constraints: - The shapes of `training_data` and `test_data` will be such that the number of columns (features) is the same in both arrays. - The `weights_fn` callable should adhere to the following signature: ```python def weights_fn(distances: np.ndarray) -> np.ndarray: ``` - Performance is not the primary concern but aim for a solution that uses efficient search algorithms. # Example: ```python import numpy as np def example_weights_fn(distances): return 1 / (distances + 1e-3) # Avoid division by zero training_data = np.array([[0, 0], [1, 1], [2, 2], [3, 3]]) training_labels = np.array([0, 1, 1, 0]) test_data = np.array([[1.5, 1.5], [2.5, 2.5]]) predicted_labels = custom_nearest_neighbors_classification(training_data, training_labels, test_data, 2, example_weights_fn) print(predicted_labels) # Output should be the predicted class labels for test data ``` Implement the function `custom_nearest_neighbors_classification` to pass the example case provided.","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier def custom_nearest_neighbors_classification(training_data: np.ndarray, training_labels: np.ndarray, test_data: np.ndarray, n_neighbors: int, weights_fn) -> np.ndarray: Classifies test data points based on the nearest neighbors from training data. Parameters: - training_data (np.ndarray): A 2D array where each row represents a training sample and each column represents a feature. - training_labels (np.ndarray): A 1D array where each element is the label corresponding to a training sample. - test_data (np.ndarray): A 2D array where each row represents a test sample and each column represents a feature. - n_neighbors (int): The number of nearest neighbors to consider during classification. - weights_fn (callable): A function to compute weights for the neighbors based on their distances. Returns: - np.ndarray: A 1D array of predicted labels for each test sample. # Finding nearest neighbors using Euclidean distance nn_euclidean = NearestNeighbors(n_neighbors=n_neighbors, metric=\'euclidean\') nn_euclidean.fit(training_data) distances_euclidean, indices_euclidean = nn_euclidean.kneighbors(test_data) # Finding nearest neighbors using Manhattan distance nn_manhattan = NearestNeighbors(n_neighbors=n_neighbors, metric=\'manhattan\') nn_manhattan.fit(training_data) distances_manhattan, indices_manhattan = nn_manhattan.kneighbors(test_data) # Gather custom weights using the provided weights function weights_euclidean = weights_fn(distances_euclidean) weights_manhattan = weights_fn(distances_manhattan) # Create KNeighborsClassifier models for both metrics knn_euclidean = KNeighborsClassifier(n_neighbors=n_neighbors, weights=lambda dist: weights_fn(dist)) knn_euclidean.fit(training_data, training_labels) knn_manhattan = KNeighborsClassifier(n_neighbors=n_neighbors, metric=\'manhattan\', weights=lambda dist: weights_fn(dist)) knn_manhattan.fit(training_data, training_labels) # Predict using the KNeighborsClassifier models predictions_euclidean = knn_euclidean.predict(test_data) predictions_manhattan = knn_manhattan.predict(test_data) # Combining predictions somehow - here we just return Euclidean predictions as an example return predictions_euclidean"},{"question":"# Task You are required to load a dataset using the `sklearn.datasets` package, process the dataset, and perform basic data analysis. Specifically, you will use the famous \\"Iris\\" dataset. You need to complete the following steps: 1. **Load the Dataset:** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Extract the data (features) and target (labels) arrays. 2. **Data Processing:** - Perform basic data inspection: - Print the shape of the data and target arrays. - Print the feature names and target names provided with the dataset. - Split the dataset into training and testing sets (80% training, 20% testing). 3. **Train a Classifier:** - Train a k-nearest neighbors (k-NN) classifier using the training set. - Make predictions on the testing set. 4. **Evaluate the Classifier:** - Calculate the accuracy of the classifier on the testing set. - Print the accuracy score. # Input and Output Formats Input: - There are no inputs for this function since the dataset is loaded directly from `sklearn.datasets`. Output: - The function should print the following: - The shape of the data array as a tuple. - The shape of the target array as a tuple. - The feature names as a list. - The target names as a list. - The accuracy score as a float. # Constraints - You must use the `sklearn.datasets`, `sklearn.model_selection`, and `sklearn.neighbors` modules. - You should define a single function `analyze_iris_dataset()` that performs all the above steps. # Function Definition ```python def analyze_iris_dataset(): # Load the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Load the data iris = load_iris() data = iris.data target = iris.target # Basic data inspection print(\\"Data shape:\\", data.shape) print(\\"Target shape:\\", target.shape) print(\\"Feature names:\\", iris.feature_names) print(\\"Target names:\\", iris.target_names) # Split the data X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) # Train the classifier classifier = KNeighborsClassifier(n_neighbors=3) classifier.fit(X_train, y_train) # Make predictions predictions = classifier.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, predictions) print(\\"Accuracy:\\", accuracy) # Call the function analyze_iris_dataset() ``` Ensure your function is well-documented and follows good coding practices.","solution":"def analyze_iris_dataset(): Loads the Iris dataset, performs basic data inspection, splits the dataset, trains a k-NN classifier, and evaluates its accuracy. Prints various pieces of information about the dataset and the accuracy score. from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Load the data iris = load_iris() data = iris.data target = iris.target # Basic data inspection print(\\"Data shape:\\", data.shape) print(\\"Target shape:\\", target.shape) print(\\"Feature names:\\", iris.feature_names) print(\\"Target names:\\", iris.target_names) # Split the data X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) # Train the classifier classifier = KNeighborsClassifier(n_neighbors=3) classifier.fit(X_train, y_train) # Make predictions predictions = classifier.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, predictions) print(\\"Accuracy:\\", accuracy)"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of the `ossaudiodev` module by implementing a function that reads audio data from a file, processes it to double its speed, and then plays it back using OSS audio devices. # Description Write a function `process_and_play_audio(input_file: str, output_file: str, format: str, channels: int, samplerate: int) -> None` that performs the following: 1. Opens an OSS audio device for playback. 2. Configures the audio device with the specified format, number of channels, and sample rate. 3. Reads audio data from `input_file`. 4. Processes the audio data to double its playback speed. 5. Writes the processed audio data to `output_file`. 6. Plays the audio data using the OSS audio device. # Specifications: - **Input Parameters:** - `input_file` (str): The path to the input audio file. - `output_file` (str): The path to save the processed audio data. - `format` (str): The required audio format (e.g., \'AFMT_S16_LE\'). - `channels` (int): The number of audio channels (e.g., 1 for mono, 2 for stereo). - `samplerate` (int): The audio sample rate (e.g., 44100 for CD quality). - **Output:** None # Constraints: - You must handle potential errors and exceptions appropriately. - Ensure the audio device is closed properly after all operations. - The `input_file` and `output_file` file operations are expected to read/write raw audio data. # Example Usage ```python process_and_play_audio(\\"input.raw\\", \\"output.raw\\", \\"AFMT_S16_LE\\", 1, 44100) ``` Notes: - For simplicity, assume the input file contains raw PCM audio data. - You can use the `ossaudiodev` methods such as `setfmt`, `channels`, `speed`, `writeall`, and `close` to interact with the audio device. - Doubling the audio speed can be achieved by manipulating the sample rate or directly processing the data to skip every other sample. # Implementation Complete the function using the provided details and guidelines: ```python import ossaudiodev def process_and_play_audio(input_file: str, output_file: str, format: str, channels: int, samplerate: int) -> None: # Code implementation goes here pass ``` # Performance Requirement: - Ensure no exceptions are raised for valid inputs. - The function should efficiently handle the audio processing and interaction with the OSS device.","solution":"import ossaudiodev def process_and_play_audio(input_file: str, output_file: str, format: str, channels: int, samplerate: int) -> None: Reads audio data from input_file, processes it to double its speed, and writes the processed data to output_file. Then plays the processed audio using OSS audio device. :param input_file: path to the input audio file :param output_file: path to save the processed audio data :param format: the required audio format (e.g., \'AFMT_S16_LE\') :param channels: number of audio channels (e.g., 1 for mono, 2 for stereo) :param samplerate: audio sample rate (e.g., 44100 for CD quality) try: # Open the input file and read the audio data with open(input_file, \'rb\') as infile: input_data = infile.read() # Process the audio data to double its speed processed_data = input_data[::2] # Skip every other sample to double speed # Write the processed data to the output file with open(output_file, \'wb\') as outfile: outfile.write(processed_data) # Open the OSS audio device for playback dsp = ossaudiodev.open(\\"w\\") dsp.setfmt(eval(f\\"ossaudiodev.{format}\\")) dsp.channels(channels) dsp.speed(samplerate * 2) # Double the sample rate for playback # Play the processed audio data dsp.writeall(processed_data) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Ensure the audio device is closed properly try: dsp.close() except: pass"},{"question":"# Problem: Advanced Covariance Estimation in High-dimensional Data Background In statistical data analysis, the covariance matrix plays a crucial role in understanding the relationship between different features of a dataset. However, accurate estimation of this matrix becomes challenging, especially in high-dimensional settings or when the data contain outliers. Scikit-learn offers various methods to handle these challenges effectively. In this task, you will implement a solution to estimate the covariance matrix using different techniques provided in the `sklearn.covariance` module. Your implementation should handle the following tasks: 1. **Empirical Covariance Calculation**: Compute the empirical covariance of the dataset. 2. **Shrunk Covariance Calculation**: Apply shrinkage to the empirical covariance with a given shrinkage coefficient. 3. **Optimal Shrinkage Estimation using Ledoit-Wolf**: Compute the Ledoit-Wolf shrinkage estimate of the covariance. 4. **Sparse Precision Matrix Estimation**: Estimate a sparse precision matrix using Graphical Lasso. 5. **Robust Covariance Estimation**: Estimate a robust covariance matrix using the Minimum Covariance Determinant method. Input - A 2D NumPy array `X` representing the dataset with shape `(n_samples, n_features)`. Values are floating-point numbers. - A floating-point number `shrinkage_coefficient` for the shrinkage applied in shrunk covariance calculation. Output A dictionary containing the following keys and corresponding values: - `\'empirical_covariance\'`: The computed empirical covariance matrix. - `\'shrunk_covariance\'`: The shrunk covariance matrix. - `\'ledoit_wolf_covariance\'`: The Ledoit-Wolf shrinkage estimate of the covariance matrix. - `\'sparse_precision\'`: The precision matrix estimated via Graphical Lasso. - `\'robust_covariance\'`: The robust covariance matrix estimated using the Minimum Covariance Determinant method. Constraints - `n_samples` > 1 - `n_features` > 1 Performance Requirements - The solution should efficiently handle datasets with up to 1000 samples and 500 features. # Implementation ```python import numpy as np from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, GraphicalLasso, MinCovDet ) def estimate_covariances(X, shrinkage_coefficient): Estimate different types of covariance matrices. Parameters: - X (np.ndarray): Data matrix with shape (n_samples, n_features). - shrinkage_coefficient (float): Shrinkage coefficient for shrunk covariance. Returns: - dict: Dictionary containing different covariance estimates. results = {} # Empirical Covariance empirical_cov = EmpiricalCovariance().fit(X) results[\'empirical_covariance\'] = empirical_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_coefficient).fit(X) results[\'shrunk_covariance\'] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(X) results[\'ledoit_wolf_covariance\'] = lw_cov.covariance_ # Sparse Precision via Graphical Lasso graphical_lasso = GraphicalLasso().fit(X) results[\'sparse_precision\'] = graphical_lasso.precision_ # Robust Covariance with Minimum Covariance Determinant robust_cov = MinCovDet().fit(X) results[\'robust_covariance\'] = robust_cov.covariance_ return results # Example usage: X = np.random.randn(100, 20) shrinkage_coefficient = 0.1 covariances = estimate_covariances(X, shrinkage_coefficient) for key, value in covariances.items(): print(f\\"{key}:n{value}n\\") ``` Notes - Ensure you handle data centering appropriately as per the requirements of different methods. - Validate input dimensions and types before processing.","solution":"import numpy as np from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, GraphicalLasso, MinCovDet ) def estimate_covariances(X, shrinkage_coefficient): Estimate different types of covariance matrices. Parameters: - X (np.ndarray): Data matrix with shape (n_samples, n_features). - shrinkage_coefficient (float): Shrinkage coefficient for shrunk covariance. Returns: - dict: Dictionary containing different covariance estimates. results = {} # Validate input dimensions if X.ndim != 2: raise ValueError(\\"X should be a 2D array\\") if X.shape[0] <= 1 or X.shape[1] <= 1: raise ValueError(\\"X should have more than one sample and more than one feature\\") # Empirical Covariance empirical_cov = EmpiricalCovariance().fit(X) results[\'empirical_covariance\'] = empirical_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_coefficient).fit(X) results[\'shrunk_covariance\'] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(X) results[\'ledoit_wolf_covariance\'] = lw_cov.covariance_ # Sparse Precision via Graphical Lasso graphical_lasso = GraphicalLasso().fit(X) results[\'sparse_precision\'] = graphical_lasso.precision_ # Robust Covariance with Minimum Covariance Determinant robust_cov = MinCovDet().fit(X) results[\'robust_covariance\'] = robust_cov.covariance_ return results"},{"question":"**Task:** Implement Kernel Ridge Regression and evaluate its performance on a synthetic dataset. **Description:** You are required to implement a Kernel Ridge Regression model using the `sklearn.kernel_ridge.KernelRidge` class. Then, generate a synthetic dataset and evaluate the model\'s performance. Finally, compare the fitting and prediction times with Support Vector Regression (SVR) from `sklearn.svm.SVR`. **Input:** 1. **n_samples:** an integer representing the number of samples to generate for the synthetic dataset (e.g., 500). 2. **noise:** a floating-point value representing the standard deviation of Gaussian noise added to the output (e.g., 0.1). **Output:** 1. A dictionary containing: - **\'krr_fit_time\':** the time taken to fit the Kernel Ridge Regression model. - **\'krr_predict_time\':** the time taken by the Kernel Ridge Regression model to make predictions on the dataset. - **\'svr_fit_time\':** the time taken to fit the Support Vector Regression model. - **\'svr_predict_time\':** the time taken by the Support Vector Regression model to make predictions on the dataset. - **\'krr_predictions\':** an array of predictions made by the Kernel Ridge Regression model. - **\'svr_predictions\':** an array of predictions made by the Support Vector Regression model. **Constraints:** - Ensure that fitting time and prediction time are measured in seconds with an accuracy of at least 3 decimal places. - The synthetic dataset should have an equal split between training (80%) and testing (20%) data. **Requirements:** - Use the Radial Basis Function (RBF) kernel for both KRR and SVR models. - Perform any necessary imports from `sklearn`. **Implementation:** ```python import numpy as np import time from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split def evaluate_models(n_samples, noise): # Generate synthetic dataset X, y = make_regression(n_samples=n_samples, n_features=1, noise=noise, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Kernel Ridge Regression krr = KernelRidge(kernel=\'rbf\') start_time = time.time() krr.fit(X_train, y_train) krr_fit_time = time.time() - start_time start_time = time.time() krr_predictions = krr.predict(X_test) krr_predict_time = time.time() - start_time # Support Vector Regression svr = SVR(kernel=\'rbf\') start_time = time.time() svr.fit(X_train, y_train) svr_fit_time = time.time() - start_time start_time = time.time() svr_predictions = svr.predict(X_test) svr_predict_time = time.time() - start_time # Return results return { \'krr_fit_time\': round(krr_fit_time, 3), \'krr_predict_time\': round(krr_predict_time, 3), \'svr_fit_time\': round(svr_fit_time, 3), \'svr_predict_time\': round(svr_predict_time, 3), \'krr_predictions\': krr_predictions.tolist(), \'svr_predictions\': svr_predictions.tolist() } # Example usage: result = evaluate_models(500, 0.1) print(result) ```","solution":"import numpy as np import time from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split def evaluate_models(n_samples, noise): Evaluate the performance of Kernel Ridge Regression and Support Vector Regression on a synthetic dataset. Parameters: n_samples (int): Number of samples in the synthetic dataset. noise (float): Standard deviation of Gaussian noise added to the output. Returns: dict: A dictionary containing fit and predict times for both models and their predictions. # Generate synthetic dataset X, y = make_regression(n_samples=n_samples, n_features=1, noise=noise, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Kernel Ridge Regression krr = KernelRidge(kernel=\'rbf\') start_time = time.time() krr.fit(X_train, y_train) krr_fit_time = time.time() - start_time start_time = time.time() krr_predictions = krr.predict(X_test) krr_predict_time = time.time() - start_time # Support Vector Regression svr = SVR(kernel=\'rbf\') start_time = time.time() svr.fit(X_train, y_train) svr_fit_time = time.time() - start_time start_time = time.time() svr_predictions = svr.predict(X_test) svr_predict_time = time.time() - start_time # Return results return { \'krr_fit_time\': round(krr_fit_time, 3), \'krr_predict_time\': round(krr_predict_time, 3), \'svr_fit_time\': round(svr_fit_time, 3), \'svr_predict_time\': round(svr_predict_time, 3), \'krr_predictions\': krr_predictions.tolist(), \'svr_predictions\': svr_predictions.tolist() } # Example usage: # result = evaluate_models(500, 0.1) # print(result)"},{"question":"**Question:** You are given a dataset `diamonds` that can be loaded using seaborn\'s `sns.load_dataset(\\"diamonds\\")`. Your task is to create a comprehensive visualization using seaborn that showcases various aspects of the dataset, focusing on the `price` of diamonds. 1. Create a figure with three subplots arranged in a single column. 2. The first subplot should display a simple **horizontal boxen plot** showing the distribution of diamond prices. 3. The second subplot should group the boxen plot by the `clarity` of the diamonds and show price distributions across these clarity levels. 4. The third subplot should show a boxen plot grouped by `clarity` with an additional grouping by whether the `carat` (mass) of the diamond is greater than 1 (`large_diamond`). Different groups should be represented by different colors, and a small gap should be added between the boxes. Customize the following attributes for all plots: - Use linear width scaling. - Set the line width of the boxes to 0.5 and the line color to a light gray. - Customize the median line with a linewidth of 1.5 and color `#cde`. - Customize the appearance of outliers with a facecolor of light gray and linewidth of 0.5. **Constraints:** - You must use seaborn\'s `boxenplot` function. - Use the provided dataset `diamonds`. - Ensure the visualizations are clear and readable. **Expected Input:** ```python def create_seaborn_visualizations(data): # Your code here ``` **Expected Output:** The function should display a figure with the specified visualizations. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_visualizations(data): fig, axs = plt.subplots(3, 1, figsize=(10, 15)) sns.boxenplot(x=data[\\"price\\"], ax=axs[0]) sns.boxenplot(data=data, x=\\"price\\", y=\\"clarity\\", ax=axs[1]) large_diamond = data[\\"carat\\"].gt(1).rename(\\"large_diamond\\") sns.boxenplot(data=data, x=\\"price\\", y=\\"clarity\\", hue=large_diamond, gap=0.2, ax=axs[2]) for ax in axs: ax.set(xlabel=\'Diamond Price\', ylabel=\'\') sns.boxenplot(width_method=\'linear\', linewidth=0.5, linecolor=\'.7\', line_kws=dict(linewidth=1.5, color=\'#cde\'), flier_kws=dict(facecolor=\'.7\', linewidth=0.5), ax=ax) plt.tight_layout() plt.show() diamonds = sns.load_dataset(\\"diamonds\\") create_seaborn_visualizations(diamonds) ``` Implement the `create_seaborn_visualizations` function as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_visualizations(data): Create a comprehensive visualization for the \'diamonds\' dataset, focusing on the price of diamonds. fig, axs = plt.subplots(3, 1, figsize=(10, 15)) # First subplot: Horizontal boxen plot for diamond prices sns.boxenplot(x=data[\\"price\\"], ax=axs[0], width_method=\'linear\', linewidth=0.5, linecolor=\'.7\', flier_kws=dict(facecolor=\'.7\', linewidth=0.5), line_kws=dict(linewidth=1.5, color=\'#cde\')) axs[0].set(xlabel=\'Diamond Price\', ylabel=\'\') # Second subplot: Boxen plot grouped by clarity sns.boxenplot(data=data, x=\\"price\\", y=\\"clarity\\", ax=axs[1], width_method=\'linear\', linewidth=0.5, linecolor=\'.7\', flier_kws=dict(facecolor=\'.7\', linewidth=0.5), line_kws=dict(linewidth=1.5, color=\'#cde\')) axs[1].set(xlabel=\'Diamond Price\', ylabel=\'Clarity\') # Third subplot: Boxen plot grouped by clarity and large_diamond data[\\"large_diamond\\"] = data[\\"carat\\"] > 1 sns.boxenplot(data=data, x=\\"price\\", y=\\"clarity\\", hue=\\"large_diamond\\", dodge=True, ax=axs[2], width_method=\'linear\', linewidth=0.5, linecolor=\'.7\', flier_kws=dict(facecolor=\'.7\', linewidth=0.5), line_kws=dict(linewidth=1.5, color=\'#cde\'), gap=0.2) axs[2].set(xlabel=\'Diamond Price\', ylabel=\'Clarity\') plt.tight_layout() plt.show()"},{"question":"# Prime Factorization Using Math Functions **Objective:** Write a function `prime_factors(n: int) -> list` that calculates the prime factorization of a given integer `n`. The function should return a list of prime factors sorted in ascending order. # Specifications: 1. **Input:** - `n` is a positive integer (`n > 1`). 2. **Output:** - A list of integers representing the prime factors of `n` in ascending order. 3. **Constraints:** - You should use the provided `math` module functions where appropriate. - Optimize your solution in terms of time complexity. 4. **Example:** ```python >>> prime_factors(28) [2, 2, 7] >>> prime_factors(84) [2, 2, 3, 7] >>> prime_factors(97) [97] ``` # Hints: - Think about how you might efficiently check for prime numbers. - Consider using `math.isqrt` or `math.sqrt` to limit the range of numbers you need to check. - Use other `math` module functions where applicable to make your code cleaner and more efficient. # Notes: Your implementation should handle large integers efficiently and return the correct result even for edge-cases such as prime numbers, which should return the number itself. **Implementation:** Write your solution in Python below: ```python import math def prime_factors(n: int) -> list: # your code here pass ``` **Test your function:** To ensure your function works correctly, you should test it with various inputs including edge cases like `n = 2`, prime numbers, and large numbers.","solution":"import math def prime_factors(n: int) -> list: Returns the list of prime factors of the given integer n. factors = [] # Check for number of twos in n while n % 2 == 0: factors.append(2) n //= 2 # Check for other primes for i in range(3, math.isqrt(n) + 1, 2): while n % i == 0: factors.append(i) n //= i # If n is still larger than 2, then it\'s a prime number if n > 2: factors.append(n) return factors"},{"question":"Objective The goal of this assessment is to evaluate your ability to use Python\'s `pickle` module for object serialization and the `sqlite3` module for database interactions. You are tasked with implementing functions that handle storage and retrieval of pickled custom objects from an SQLite database. Problem Statement You are required to create a class `Person` for representing individual records and implement a series of functions to serialize and store instances of this class in an SQLite database. You will also implement functions to retrieve and deserialize the objects. Requirements 1. **Person Class (25 points):** * Implement a class `Person` with the following attributes: - `name` (string) - `age` (integer) - `email` (string) * The class should include an initializer (`__init__`) to set these attributes. ```python class Person: def __init__(self, name: str, age: int, email: str): self.name = name self.age = age self.email = email ``` 2. **Database Setup Function (25 points):** * Implement a function `setup_database(db_name: str)` that sets up an SQLite database with the given `db_name`. * It should create a table `people` with columns `id` (INTEGER PRIMARY KEY), `data` (BLOB). ```python import sqlite3 def setup_database(db_name: str): conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS people ( id INTEGER PRIMARY KEY, data BLOB ) \'\'\') conn.commit() conn.close() ``` 3. **Store Person Function (25 points):** * Implement a function `store_person(db_name: str, person: Person)` that serializes a `Person` object using `pickle` and stores it in the database. ```python import pickle def store_person(db_name: str, person: Person): conn = sqlite3.connect(db_name) cursor = conn.cursor() pickled_person = pickle.dumps(person) cursor.execute(\'INSERT INTO people (data) VALUES (?)\', (pickled_person,)) conn.commit() conn.close() ``` 4. **Retrieve Person Function (25 points):** * Implement a function `retrieve_person(db_name: str, person_id: int) -> Person` that retrieves and deserializes a `Person` object from the database given its `id`. ```python def retrieve_person(db_name: str, person_id: int) -> Person: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'SELECT data FROM people WHERE id = ?\', (person_id,)) result = cursor.fetchone() conn.close() if result: return pickle.loads(result[0]) else: return None ``` Constraints * The implementation should handle exceptions gracefully and ensure all database connections are closed properly. * Ensure that the `pickle` module is used for serialization, and it must work for any instance of the `Person` class. Example Usage ```python # Initialize database db_name = \'people.db\' setup_database(db_name) # Create a Person object person1 = Person(\'John Doe\', 30, \'john.doe@example.com\') # Store the Person object in the database store_person(db_name, person1) # Retrieve the Person object from the database retrieved_person = retrieve_person(db_name, 1) print(retrieved_person.name) # Output: John Doe print(retrieved_person.age) # Output: 30 print(retrieved_person.email) # Output: john.doe@example.com ``` Submission Submit your implemented Python code in a single script file.","solution":"import sqlite3 import pickle class Person: def __init__(self, name: str, age: int, email: str): self.name = name self.age = age self.email = email def setup_database(db_name: str): conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS people ( id INTEGER PRIMARY KEY, data BLOB ) \'\'\') conn.commit() conn.close() def store_person(db_name: str, person: Person): conn = sqlite3.connect(db_name) cursor = conn.cursor() pickled_person = pickle.dumps(person) cursor.execute(\'INSERT INTO people (data) VALUES (?)\', (pickled_person,)) conn.commit() conn.close() def retrieve_person(db_name: str, person_id: int) -> Person: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'SELECT data FROM people WHERE id = ?\', (person_id,)) result = cursor.fetchone() conn.close() if result: return pickle.loads(result[0]) else: return None"},{"question":"Objective To assess your understanding and ability to use the `shutil` module in Python for file manipulation tasks, including copying, moving, and archiving files and directories. Question You are tasked with creating a script that performs a series of file and directory operations using the `shutil` module. The script should: 1. **Copy a File**: Take a source file and copy its contents to a destination file. 2. **Move a Directory**: Move a directory and its contents to a new location. 3. **Create an Archive**: Create a compressed archive of a directory. 4. **Extract an Archive**: Extract the contents of a compressed archive into a specified directory. Requirements 1. **Function Implementations** - Implement a function `copy_file(src, dst)` that copies the contents of `src` file to `dst` file. - Implement a function `move_directory(src, dst)` that moves the directory `src` to the destination `dst`. - Implement a function `create_archive(output_filename, source_dir, format=\'gztar\')` that creates a compressed archive of `source_dir` with the specified format and names the archive file `output_filename`. - Implement a function `extract_archive(archive_filename, extract_dir)` that extracts the contents of `archive_filename` into the directory `extract_dir`. 2. **Input and Output Specification** - `copy_file(src, dst)` - Input: - `src` (str): Path to the source file. - `dst` (str): Path to the destination file. - Output: None - `move_directory(src, dst)` - Input: - `src` (str): Path to the source directory. - `dst` (str): Path to the destination directory. - Output: None - `create_archive(output_filename, source_dir, format=\'gztar\')` - Input: - `output_filename` (str): Path to the output archive file. - `source_dir` (str): Path to the source directory. - `format` (str): Archive format, default is \'gztar\'. - Output: None - `extract_archive(archive_filename, extract_dir)` - Input: - `archive_filename` (str): Path to the archive file. - `extract_dir` (str): Path to the directory where archive contents will be extracted. - Output: None 3. **Constraints** - Ensure that the source file or directory exists before performing operations. - Handle exceptions that may occur during file and directory operations, and output appropriate error messages. Performance Requirements - Your functions should handle large files and deep directory structures efficiently, leveraging the capabilities of the `shutil` module. ```python import shutil import os def copy_file(src, dst): Copy the contents of the source file to the destination file. Args: src (str): Path to the source file. dst (str): Path to the destination file. Returns: None # Your implementation here pass def move_directory(src, dst): Move the source directory to the destination directory. Args: src (str): Path to the source directory. dst (str): Path to the destination directory. Returns: None # Your implementation here pass def create_archive(output_filename, source_dir, format=\'gztar\'): Create a compressed archive of the source directory. Args: output_filename (str): Path to the output archive file. source_dir (str): Path to the source directory. format (str): Archive format, default is \'gztar\'. Returns: None # Your implementation here pass def extract_archive(archive_filename, extract_dir): Extract the contents of the archive file into the specified directory. Args: archive_filename (str): Path to the archive file. extract_dir (str): Path to the directory where contents will be extracted. Returns: None # Your implementation here pass # Example usage: # copy_file(\'src.txt\', \'dst.txt\') # move_directory(\'source_dir\', \'destination_dir\') # create_archive(\'archive.tar.gz\', \'source_dir\') # extract_archive(\'archive.tar.gz\', \'destination_dir\') ``` Implement the required functions to complete this task. Ensure you handle all edge cases and provide meaningful error messages for exceptions.","solution":"import shutil import os def copy_file(src, dst): Copy the contents of the source file to the destination file. Args: src (str): Path to the source file. dst (str): Path to the destination file. Returns: None if not os.path.isfile(src): raise FileNotFoundError(f\\"Source file \'{src}\' not found.\\") shutil.copy2(src, dst) def move_directory(src, dst): Move the source directory to the destination directory. Args: src (str): Path to the source directory. dst (str): Path to the destination directory. Returns: None if not os.path.isdir(src): raise FileNotFoundError(f\\"Source directory \'{src}\' not found.\\") shutil.move(src, dst) def create_archive(output_filename, source_dir, format=\'gztar\'): Create a compressed archive of the source directory. Args: output_filename (str): Path to the output archive file. source_dir (str): Path to the source directory. format (str): Archive format, default is \'gztar\'. Returns: None if not os.path.isdir(source_dir): raise FileNotFoundError(f\\"Source directory \'{source_dir}\' not found.\\") shutil.make_archive(output_filename, format, source_dir) def extract_archive(archive_filename, extract_dir): Extract the contents of the archive file into the specified directory. Args: archive_filename (str): Path to the archive file. extract_dir (str): Path to the directory where contents will be extracted. Returns: None if not os.path.isfile(archive_filename): raise FileNotFoundError(f\\"Archive file \'{archive_filename}\' not found.\\") shutil.unpack_archive(archive_filename, extract_dir)"},{"question":"Objective Write a Python program utilizing the `multiprocessing.shared_memory` module to perform parallel computation on a list of integers, demonstrating your understanding of shared memory management and multiprocessing. Problem Statement Imagine you have a list of integers that you need to process using multiple processes. Each process will be responsible for squaring the integers in a subset of the list. Here are the steps to follow: 1. **Main Process**: - Create a list of integers, e.g., `[1, 2, 3, ..., 100]`. - Create a shared memory block large enough to hold this list. - Copy the list into the shared memory block. 2. **Worker Process**: - Each worker process should access this shared memory block. - Each process will square the integers in its assigned subset of the list. 3. **Result Collection**: - After all processes complete, the main process should fetch the updated list from the shared memory. 4. **Memory Cleanup**: - Ensure all shared memory resources are cleaned up appropriately. Requirements 1. Implement a function `parallel_square` with the following specifications: ```python def parallel_square(num_list: list, num_processes: int) -> list: Parallelize the squaring of integers in num_list using num_processes processes. Arguments: num_list : list : List of integers to be processed. num_processes : int : The number of processes to use for parallel computation. Returns: list : The modified list of integers after squaring. ``` 2. Ensure that the shared memory block is properly cleaned up by using the appropriate methods. 3. Test your function with a list of integers from 1 to 100 and 4 processes. Constraints - Each process should only modify the portion of the list it is responsible for. - The function should handle edge cases like an empty list or a number of processes greater than the length of the list gracefully. Example ```python if __name__ == \\"__main__\\": result = parallel_square(list(range(1, 101)), 4) print(result) # Expected Output: [1, 4, 9, 16, 25, ..., 9604, 9801, 10000] ``` Hints - Use `multiprocessing.Process` to create worker processes. - Use `SharedMemory` or `ShareableList` to share the list across processes. - Make use of numpy for handling the shared memory if you find it easier.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def worker(shared_name, start_idx, end_idx): # Attach to the existing shared memory block shm = SharedMemory(name=shared_name) # Create a numpy array backed by the shared memory block nums = np.ndarray((end_idx - start_idx,), dtype=np.int64, buffer=shm.buf[start_idx*8:end_idx*8]) # Square the portion of the array this worker is responsible for nums[:] = nums**2 # Close the shared memory segment shm.close() def parallel_square(num_list, num_processes): if not num_list: return [] num_elements = len(num_list) # Create a shared memory block large enough to hold the list shm = SharedMemory(create=True, size=num_elements * 8) # Create a numpy array backed by the shared memory block shared_array = np.ndarray((num_elements,), dtype=np.int64, buffer=shm.buf) shared_array[:] = num_list[:] chunk_size = (num_elements + num_processes - 1) // num_processes # Calculate chunk size processes = [] for i in range(num_processes): start_idx = i * chunk_size end_idx = min(start_idx + chunk_size, num_elements) if start_idx >= num_elements: break p = Process(target=worker, args=(shm.name, start_idx, end_idx)) processes.append(p) p.start() for p in processes: p.join() # Copy the shared memory block contents back to list result = shared_array.tolist() # Clean up the shared memory shm.close() shm.unlink() return result"},{"question":"**Question: Implement a Nested Dictionary List Management System** You are required to implement a function `manage_students` that simulates a basic system to manage student records in a school. Each student record will contain the student\'s name, a list of grades, and a set of extra-curricular activities the student is involved in. The records are stored in a dictionary where the student\'s name is the key. Your `manage_students` function should support the following operations: 1. **Add Student**: Add a new student to the record with an empty list of grades and an empty set of activities. 2. **Remove Student**: Remove a student from the record. 3. **Add Grade**: Add a grade to a student\'s list of grades. 4. **Remove Grade**: Remove a grade from a student\'s list of grades. 5. **Add Activity**: Add an activity to a student\'s set of activities. 6. **Remove Activity**: Remove an activity from a student\'s set of activities. 7. **List Students**: Return a sorted list of all student names. 8. **Get Average Grade**: Calculate the average grade for a student (return 0 if the student has no grades). 9. **Top Student in Activity**: Return the name of the student with the highest average grade who participates in a given activity. **Function Signatures:** ```python class StudentManagement: def __init__(self): self.students = {} def add_student(self, name: str): pass def remove_student(self, name: str): pass def add_grade(self, name: str, grade: int): pass def remove_grade(self, name: str, grade: int): pass def add_activity(self, name: str, activity: str): pass def remove_activity(self, name: str, activity: str): pass def list_students(self) -> list: pass def get_average_grade(self, name: str) -> float: pass def top_student_in_activity(self, activity: str) -> str: pass # Sample Usage: school = StudentManagement() school.add_student(\\"Alice\\") school.add_grade(\\"Alice\\", 85) school.add_grade(\\"Alice\\", 92) school.add_activity(\\"Alice\\", \\"Soccer\\") school.add_student(\\"Bob\\") school.add_grade(\\"Bob\\", 78) school.add_activity(\\"Bob\\", \\"Soccer\\") school.add_student(\\"Charlie\\") school.add_grade(\\"Charlie\\", 95) school.add_activity(\\"Charlie\\", \\"Basketball\\") print(school.list_students()) # [\'Alice\', \'Bob\', \'Charlie\'] print(school.get_average_grade(\\"Alice\\")) # 88.5 print(school.top_student_in_activity(\\"Soccer\\")) # \'Alice\' ``` **Constraints:** - Student names, activity names: non-empty strings - Grades: integers between 0 and 100 (inclusive) - Operations will be valid as per the function descriptions (e.g., removing grades or activities that exist). **Performance Requirements:** - The operations should be efficient with a focus on minimizing time complexity for all operations. Complete the implementation of the `StudentManagement` class based on the provided function signatures to meet the requirement.","solution":"class StudentManagement: def __init__(self): self.students = {} def add_student(self, name: str): if name not in self.students: self.students[name] = {\'grades\': [], \'activities\': set()} def remove_student(self, name: str): if name in self.students: del self.students[name] def add_grade(self, name: str, grade: int): if name in self.students and 0 <= grade <= 100: self.students[name][\'grades\'].append(grade) def remove_grade(self, name: str, grade: int): if name in self.students and grade in self.students[name][\'grades\']: self.students[name][\'grades\'].remove(grade) def add_activity(self, name: str, activity: str): if name in self.students: self.students[name][\'activities\'].add(activity) def remove_activity(self, name: str, activity: str): if name in self.students and activity in self.students[name][\'activities\']: self.students[name][\'activities\'].remove(activity) def list_students(self) -> list: return sorted(self.students.keys()) def get_average_grade(self, name: str) -> float: if name in self.students and self.students[name][\'grades\']: return sum(self.students[name][\'grades\']) / len(self.students[name][\'grades\']) return 0.0 def top_student_in_activity(self, activity: str) -> str: top_student = None highest_avg_grade = -1 for name, info in self.students.items(): if activity in info[\'activities\']: avg_grade = sum(info[\'grades\']) / len(info[\'grades\']) if info[\'grades\'] else 0.0 if avg_grade > highest_avg_grade: highest_avg_grade = avg_grade top_student = name return top_student if top_student is not None else \\"\\""},{"question":"**Objective:** Write a Python script utilizing the `gzip` and `bz2` modules to compress and decompress files. The script should demonstrate your understanding of file handling and compression techniques in Python. **Problem Statement:** You need to implement two functions: `compress_files` and `decompress_files`. 1. `compress_files(file_paths: list, output_dir: str, algorithm: str) -> None` 2. `decompress_files(compressed_files: list, output_dir: str) -> None` - **Function: `compress_files`** - **Input:** - `file_paths` (list of str): A list of paths to the files that need to be compressed. - `output_dir` (str): The directory path where the compressed files will be saved. - `algorithm` (str): The compression algorithm to use. It can be either `\\"gzip\\"` or `\\"bz2\\"`. - **Output:** - This function does not return anything. It should save each compressed file in the specified `output_dir` with the appropriate extension (`.gz` for gzip and `.bz2` for bz2). - **Function: `decompress_files`** - **Input:** - `compressed_files` (list of str): A list of paths to the compressed files that need to be decompressed. - `output_dir` (str): The directory path where the decompressed files will be saved. - **Output:** - This function does not return anything. It should save each decompressed file in the specified `output_dir`. **Constraints:** - You should handle file reading and writing operations safely. - Ensure that proper error handling mechanisms are in place to handle file I/O exceptions. - The functions should create the `output_dir` if it does not exist. **Example Usage:** ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\"] compressed_files = [\\"file1.txt.gz\\", \\"file2.txt.bz2\\"] compress_files(file_paths, \\"compressed/\\", \\"gzip\\") compress_files(file_paths, \\"compressed/\\", \\"bz2\\") decompress_files(compressed_files, \\"decompressed/\\") ``` **Hint:** Use `gzip.open` and `bz2.open` for handling file compression and decompression. **Assessment Criteria:** - Correct implementation of the compression and decompression logic. - Proper handling of file I/O and errors. - Usage of appropriate compression methods based on the specified algorithm. - Clarity, readability, and organization of the code.","solution":"import os import gzip import bz2 def compress_files(file_paths, output_dir, algorithm): os.makedirs(output_dir, exist_ok=True) for file_path in file_paths: file_name = os.path.basename(file_path) output_file = \\"\\" if algorithm == \\"gzip\\": output_file = os.path.join(output_dir, file_name + \\".gz\\") with open(file_path, \'rb\') as f_in, gzip.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif algorithm == \\"bz2\\": output_file = os.path.join(output_dir, file_name + \\".bz2\\") with open(file_path, \'rb\') as f_in, bz2.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(\\"Unsupported algorithm specified\\") def decompress_files(compressed_files, output_dir): os.makedirs(output_dir, exist_ok=True) for compressed_file in compressed_files: file_name = os.path.basename(compressed_file) output_file = os.path.join(output_dir, file_name) if compressed_file.endswith(\\".gz\\"): output_file = os.path.splitext(output_file)[0] with gzip.open(compressed_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif compressed_file.endswith(\\".bz2\\"): output_file = os.path.splitext(output_file)[0] with bz2.open(compressed_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(\\"Unsupported file extension\\")"},{"question":"**Title: Distributed Matrix Multiplication using PyTorch RRef Protocol** **Objective:** The goal of this exercise is to test the student\'s understanding of PyTorch\'s distributed RPC framework, specifically focusing on the creation, usage, and lifecycle management of Remote References (RRefs). **Scenario:** You are provided with a distributed computing setup where you need to multiply two large matrices by distributing the computation across multiple worker nodes. This task involves creating RRefs, sharing them between nodes, and ensuring proper synchronization and reference counting for efficient distributed computation. **Requirements:** 1. **Cluster Setup:** - You must set up a cluster with three workers (nodes): \\"worker0\\", \\"worker1\\", and \\"worker2\\". - Each worker will handle a part of the matrix multiplication task. 2. **Matrix Preparation:** - Generate two random matrices `A` and `B` of size `1024x1024` on \\"worker0\\". - Divide these matrices into sub-matrices to be processed on different workers. 3. **RRef Creation and Sharing:** - Implement a function to create RRefs for the sub-matrices and distribute them to the respective workers. - Ensure proper reference counting and synchronization as described in the RRef protocol (G1 and G2 guarantees). 4. **Distributed Matrix Multiplication:** - Implement the matrix multiplication in a distributed manner where each worker processes its assigned sub-matrices. - Use RPC to gather the results on \\"worker0\\". 5. **Lifecycle Management:** - Ensure that all RRefs are properly cleaned up and no memory leaks or premature deletions occur. **Function Signatures:** ```python import torch import torch.distributed.rpc as rpc def setup_workers(): Initializes the RPC framework and sets up the workers. pass def generate_matrices(): Generates two random 1024x1024 matrices on worker0. Returns: Tuple[torch.Tensor, torch.Tensor]: Matrices A and B. pass def split_matrix(matrix): Splits a matrix into sub-matrices for distribution. Args: matrix (torch.Tensor): The matrix to be split. Returns: List[torch.Tensor]: List of sub-matrices. pass def create_and_share_rrefs(a_sub_matrices, b_sub_matrices): Creates RRefs for sub-matrices and shares them with the respective workers. Args: a_sub_matrices (List[torch.Tensor]): Sub-matrices of A. b_sub_matrices (List[torch.Tensor]): Sub-matrices of B. pass def compute_sub_matrix_multiplication(a_rref, b_rref): Computes the multiplication of sub-matrices received via RRefs. Args: a_rref (rpc.RRef): RRef to sub-matrix of A. b_rref (rpc.RRef): RRef to sub-matrix of B. Returns: torch.Tensor: Resulting sub-matrix after multiplication. pass def gather_results(result_rrefs): Gathers the results from all workers back to worker0. Args: result_rrefs (List[rpc.RRef]): List of RRefs to the results from each worker. Returns: torch.Tensor: The final multiplied matrix. pass if __name__ == \\"__main__\\": setup_workers() A, B = generate_matrices() a_sub_matrices = split_matrix(A) b_sub_matrices = split_matrix(B) create_and_share_rrefs(a_sub_matrices, b_sub_matrices) result_rrefs = [] # Implement the logic to get results from all workers final_result = gather_results(result_rrefs) print(final_result) ``` **Constraints and Limitations:** - You must utilize PyTorch and its distributed RPC framework. - Ensure proper exception handling and fault tolerance for transient network failures. - Your solution should efficiently handle the reference counting and lifecycle management of RRefs. **Performance Requirements:** - The solution should be scalable and handle distributed processing efficiently. - Ensure minimal overhead from RRef creation and deletion.","solution":"import torch import torch.distributed.rpc as rpc def setup_workers(): Initializes the RPC framework and sets up the workers. options = rpc.TensorPipeRpcBackendOptions(init_method=\'tcp://127.0.0.1:29500\') rpc.init_rpc(\\"worker0\\", rank=0, world_size=3, rpc_backend_options=options) rpc.init_rpc(\\"worker1\\", rank=1, world_size=3, rpc_backend_options=options) rpc.init_rpc(\\"worker2\\", rank=2, world_size=3, rpc_backend_options=options) def generate_matrices(): Generates two random 1024x1024 matrices on worker0. Returns: Tuple[torch.Tensor, torch.Tensor]: Matrices A and B. A = torch.rand(1024, 1024) B = torch.rand(1024, 1024) return A, B def split_matrix(matrix): Splits a matrix into sub-matrices for distribution. Args: matrix (torch.Tensor): The matrix to be split. Returns: List[torch.Tensor]: List of sub-matrices. sub_matrices = torch.chunk(matrix, 3, dim=0) # Splitting across rows return sub_matrices def create_and_share_rrefs(a_sub_matrices, b_sub_matrices): Creates RRefs for sub-matrices and shares them with the respective workers. Args: a_sub_matrices (List[torch.Tensor]): Sub-matrices of A. b_sub_matrices (List[torch.Tensor]): Sub-matrices of B. rrefs = [] for i, (a_sub, b_sub) in enumerate(zip(a_sub_matrices, b_sub_matrices)): worker = f\\"worker{i+1}\\" a_rref = rpc.RRef(a_sub) b_rref = rpc.RRef(b_sub) rrefs.append((a_rref, b_rref)) rpc.remote(worker, process_sub_matrix, args=(a_rref, b_rref, i)) return rrefs def process_sub_matrix(a_rref, b_rref, idx): Process a pair of sub-matrices on a worker. Args: a_rref (rpc.RRef): RRef to sub-matrix of A. b_rref (rpc.RRef): RRef to sub-matrix of B. idx (int): Index of the sub-matrix. a = a_rref.to_here() b = b_rref.to_here() result = torch.mm(a, b) rpc.rpc_sync(\\"worker0\\", collect_result, args=(result, idx)) result_storage = {} def collect_result(result, idx): Collects the sub-matrix multiplication result on worker0. Args: result (torch.Tensor): The result sub-matrix. idx (int): Index of the sub-matrix. result_storage[idx] = result def gather_results(): Gathers the results from all workers back to worker0. Returns: torch.Tensor: The final multiplied matrix. # Combine the sub-matrices into the final result matrix results = [result_storage[i] for i in range(3)] final_result = torch.cat(results, dim=0) return final_result if __name__ == \\"__main__\\": setup_workers() A, B = generate_matrices() a_sub_matrices = split_matrix(A) b_sub_matrices = split_matrix(B) create_and_share_rrefs(a_sub_matrices, b_sub_matrices) final_result = gather_results() print(final_result) rpc.shutdown()"},{"question":"Coding Assessment Question # Task: Implement a Custom Python Object Type with Garbage Collection You are required to implement a custom Python object type named `CustomObject`, which supports dynamic attribute assignment and cyclic garbage collection. This object type should also handle numeric operations, acting as a custom numeric type. # Requirements 1. **Attribute Management:** - The `CustomObject` should allow adding new attributes dynamically. - Provide methods to set and get attributes. 2. **Numeric Implementation:** - Implement the basic numeric operations (+, -, *, /). 3. **Cyclic Garbage Collection:** - Ensure that the `CustomObject` supports cyclic garbage collection to prevent memory leaks. # Signature: ```python class CustomObject: def __init__(self): # Initialize your custom object and set up necessary attributes here. pass def set_attr(self, name: str, value) -> None: # Method to dynamically set attributes pass def get_attr(self, name: str): # Method to get attributes pass def __add__(self, other): # Implement addition pass def __sub__(self, other): # Implement subtraction pass def __mul__(self, other): # Implement multiplication pass def __truediv__(self, other): # Implement true division pass ``` # Constraints: 1. **Memory Management:** Properly manage memory to avoid leaks, especially with cyclic references. 2. **Error Handling:** Provide appropriate error messages and handle edge cases where necessary. 3. **Performance:** Ensure your implementation is efficient and does not degrade significantly with a reasonable number of operations. # Example Usage: ```python # Instantiate CustomObject obj1 = CustomObject() obj1.set_attr(\'value\', 10) print(obj1.get_attr(\'value\')) # Output: 10 # Numeric Operations obj2 = CustomObject() obj2.set_attr(\'value\', 15) result = obj1 + obj2 print(result.get_attr(\'value\')) # Output: 25 (assuming simple addition logic) # Cyclic Garbage Collection Example obj1.set_attr(\'reference\', obj2) obj2.set_attr(\'reference\', obj1) ``` Note: You should carefully manage the cyclic references to ensure that they are properly garbage collected when no longer in use. # Evaluation Criteria: - Correct implementation of attribute management. - Proper numeric operations handling. - Effective cyclic garbage collection implementation. - Code quality and adherence to Python conventions.","solution":"import gc class CustomObject: def __init__(self): self.__dict__[\'_attributes\'] = {} def set_attr(self, name: str, value) -> None: self._attributes[name] = value def get_attr(self, name: str): return self._attributes.get(name, None) def __add__(self, other): if isinstance(other, CustomObject): result = CustomObject() result.set_attr(\'value\', self.get_attr(\'value\') + other.get_attr(\'value\')) return result else: raise TypeError(\\"Operand should be of type CustomObject\\") def __sub__(self, other): if isinstance(other, CustomObject): result = CustomObject() result.set_attr(\'value\', self.get_attr(\'value\') - other.get_attr(\'value\')) return result else: raise TypeError(\\"Operand should be of type CustomObject\\") def __mul__(self, other): if isinstance(other, CustomObject): result = CustomObject() result.set_attr(\'value\', self.get_attr(\'value\') * other.get_attr(\'value\')) return result else: raise TypeError(\\"Operand should be of type CustomObject\\") def __truediv__(self, other): if isinstance(other, CustomObject): if other.get_attr(\'value\') == 0: raise ZeroDivisionError(\\"division by zero\\") result = CustomObject() result.set_attr(\'value\', self.get_attr(\'value\') / other.get_attr(\'value\')) return result else: raise TypeError(\\"Operand should be of type CustomObject\\") def cyclic_gc_test(): obj1 = CustomObject() obj2 = CustomObject() obj1.set_attr(\'reference\', obj2) obj2.set_attr(\'reference\', obj1) del obj1 del obj2 gc.collect() # Force garbage collection print(\\"Cyclic references collected\\") # Run cyclic garbage collection test to verify cyclic_gc_test()"},{"question":"# Coding Assessment: Fetching and Processing a Dataset from OpenML Objective: The aim of this task is to evaluate your ability to use `scikit-learn` for downloading datasets, preprocessing the data, and applying a basic machine learning algorithm. Scenario: You are provided with a task to download the \\"Mice Protein Expression\\" dataset from openml.org, preprocess it, and implement a machine learning model to classify the data into different categories. Task: 1. **Fetching the Dataset:** Use the `fetch_openml` function from `scikit-learn` to download the \\"Mice Protein Expression\\" dataset (with dataset id `40966`). ```python from sklearn.datasets import fetch_openml mice = fetch_openml(data_id=40966) ``` 2. **Data Preprocessing:** - Separate the dataset into features (X) and target (y). - Convert any non-numeric columns to numeric using appropriate encoders from `scikit-learn`. - Normalize the feature dataset to ensure all features have a mean of 0 and a variance of 1. 3. **Model Training:** - Split the dataset into training and test sets (80% training and 20% testing). - Implement a basic classification algorithm (e.g., Support Vector Machine, Decision Tree, or K-Nearest Neighbors) using `scikit-learn`. - Train the model on the training set and evaluate its performance on the test set. 4. **Output:** - Print the accuracy of the model on the test set. - Display the confusion matrix. Requirements: - You must use `scikit-learn` for all data fetching, preprocessing, and modeling tasks. - The code should be well-documented, with comments explaining each step. - Ensure that the final implementation handles potential issues like missing values or non-numeric data gracefully. Constraints: - The solution should be efficient and handle the dataset size appropriately. Example: Below is a skeleton code to get you started: ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix # Step 1: Fetch the dataset mice = fetch_openml(data_id=40966) # Step 2: Separate features and target X, y = mice.data, mice.target # Step 3: Preprocessing # (Include steps to handle non-numeric data and normalization) # Step 4: Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 5: Model training # (Implement the chosen classification model) # Step 6: Evaluate model # (Print accuracy and confusion matrix) ``` Complete the skeleton code by filling in the missing parts to achieve the objectives described.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.impute import SimpleImputer import numpy as np def fetch_and_process_data(): # Step 1: Fetch the dataset mice = fetch_openml(data_id=40966) # Step 2: Separate features and target X, y = mice.data, mice.target # Step 3: Preprocessing # Define the column transformer for preprocessing numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Preprocessing pipelines for both numeric and categorical data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Full pipeline with preprocessor and classifier clf = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', SVC())]) return X, y, clf def train_and_evaluate_model(X, y, clf): # Step 4: Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 5: Model training clf.fit(X_train, y_train) # Model prediction y_pred = clf.predict(X_test) # Step 6: Evaluate model accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix # Main execution X, y, clf = fetch_and_process_data() accuracy, conf_matrix = train_and_evaluate_model(X, y, clf) # Output the results print(f\\"Model Accuracy: {accuracy}\\") print(f\\"Confusion Matrix:n{conf_matrix}\\")"},{"question":"Objective Demonstrate your understanding of Python\'s scope, naming, and exception handling mechanisms. Problem Description Create a function called `process_data` that evaluates a given mathematical expression using Python\'s `eval()` function, manages variable scope correctly, and handles specific exceptions. Function Signature ```python def process_data(expression: str, local_vars: dict, global_vars: dict = None) -> str: ``` Inputs 1. **expression (str)**: A string representing a mathematical expression that may contain operations like addition (`+`), subtraction (`-`), multiplication (`*`), and division (`/`). Variables can also be included in the expression. 2. **local_vars (dict)**: A dictionary containing variable names and their values to be used within the expression. 3. **global_vars (dict, optional)**: A dictionary containing global variable names and their values. This argument is optional. Outputs - **(str)**: The result of the evaluated expression or an error message. Constraints - Ensure that the `expression` is evaluated within a controlled scope to prevent arbitrary code execution. - Handle the following exceptions: 1. **ZeroDivisionError**: Return \\"Error: Division by zero.\\" 2. **NameError**: Return \\"Error: Undefined variable.\\" Examples ```python # Example 1 expression = \\"a + b * c\\" local_vars = {\\"a\\": 2, \\"b\\": 4, \\"c\\": 3} print(process_data(expression, local_vars)) # Output: \\"14\\" # Example 2 expression = \\"a / d\\" local_vars = {\\"a\\": 10} global_vars = {\\"d\\": 2} print(process_data(expression, local_vars, global_vars)) # Output: \\"5.0\\" # Example 3 expression = \\"a / 0\\" local_vars = {\\"a\\": 10} print(process_data(expression, local_vars)) # Output: \\"Error: Division by zero.\\" # Example 4 expression = \\"a + e\\" local_vars = {\\"a\\": 1} print(process_data(expression, local_vars)) # Output: \\"Error: Undefined variable.\\" ``` Notes - Use the `eval()` function to evaluate the expression. - Local variables should take precedence over global variables if there is a name conflict. - Ensure your solution adheres to Python\'s scoping rules and properly restricts the usage of `eval()` to prevent unexpected code execution. Hints - Utilize the `locals` and `globals` parameters of the `eval()` function for evaluating expressions within controlled scopes. - Properly handle potential exceptions to provide clear error messages.","solution":"def process_data(expression: str, local_vars: dict, global_vars: dict = None) -> str: Evaluates a given mathematical expression using provided local and global variables. Parameters: - expression (str): Mathematical expression to be evaluated. - local_vars (dict): Dictionary of local variables. - global_vars (dict, optional): Dictionary of global variables. Default is None. Returns: - str: Evaluated result or error message. try: # Evaluate the expression with the given local and global variables result = eval(expression, global_vars if global_vars is not None else {}, local_vars) return str(result) except ZeroDivisionError: return \\"Error: Division by zero.\\" except NameError: return \\"Error: Undefined variable.\\""},{"question":"**Question:** Data Compression and Decompression You are required to implement a function that compresses and decompresses data using multiple compression algorithms described in the documentation and then verifies the integrity of the data after decompression. # Requirements 1. Implement a function `compress_and_decompress(data, algorithm)` that takes: - `data` (str): The string data to be compressed. - `algorithm` (str): The name of the compression algorithm to use (`\\"zlib\\"`, `\\"gzip\\"`, `\\"bz2\\"`, or `\\"lzma\\"`). 2. The function should: - Compress the input data using the specified algorithm. - Decompress the data back to its original form. - Check if the decompressed data matches the original input data. - Return a tuple containing the compressed data and a boolean indicating whether the integrity check passed. # Expected Input and Output - **Input**: - `data` (str): A string that needs to be compressed and decompressed. - `algorithm` (str): A string specifying the compression algorithm. One of `\\"zlib\\"`, `\\"gzip\\"`, `\\"bz2\\"`, or `\\"lzma\\"`. - **Output**: - A tuple `(compressed_data, is_data_intact)`, where: - `compressed_data` is the compressed version of the input data. - `is_data_intact` is a boolean indicating whether the decompressed data matches the original input data. # Constraints - The function should handle empty strings and should not throw any errors for invalid input. - The function should support UTF-8 encoded string data. - The function should use Python 3.10 and compatible modules. # Example ```python def compress_and_decompress(data, algorithm): # Your implementation here # Example usage: data = \\"This is a test string for compression.\\" algorithm = \\"gzip\\" compressed_data, is_data_intact = compress_and_decompress(data, algorithm) print(f\\"Compressed Data: {compressed_data}\\") print(f\\"Is Data Intact: {is_data_intact}\\") ``` # Performance Requirements The function should be efficient and able to handle strings up to 10MB in size within a reasonable time frame. **Note: Students should refer to the documentation provided to correctly utilize the compression modules.**","solution":"import zlib import gzip import bz2 import lzma def compress_and_decompress(data, algorithm): Compresses and decompresses data using the specified algorithm. :param data: The string data to be compressed. :param algorithm: The compression algorithm to use (\\"zlib\\", \\"gzip\\", \\"bz2\\", or \\"lzma\\"). :return: A tuple containing the compressed data and a boolean indicating whether the integrity check passed. if algorithm == \\"zlib\\": compressed_data = zlib.compress(data.encode(\'utf-8\')) decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') elif algorithm == \\"gzip\\": compressed_data = gzip.compress(data.encode(\'utf-8\')) decompressed_data = gzip.decompress(compressed_data).decode(\'utf-8\') elif algorithm == \\"bz2\\": compressed_data = bz2.compress(data.encode(\'utf-8\')) decompressed_data = bz2.decompress(compressed_data).decode(\'utf-8\') elif algorithm == \\"lzma\\": compressed_data = lzma.compress(data.encode(\'utf-8\')) decompressed_data = lzma.decompress(compressed_data).decode(\'utf-8\') else: raise ValueError(f\\"Unsupported compression algorithm: {algorithm}\\") is_data_intact = (data == decompressed_data) return compressed_data, is_data_intact"},{"question":"**Objective:** Create a point plot using the `seaborn` library that demonstrates the following: - Grouping by multiple categorical variables. - Customizing plot appearance. - Manipulating and transforming the dataset for visualization. **Problem Statement:** Using the `penguins` dataset from the `seaborn` library, perform the following tasks: 1. **Group and Aggregate Data:** Group the dataset by the `island` and `sex` columns and calculate the mean `body_mass_g` for each group. 2. **Create Point Plot:** Create a point plot with the following specifications: - `x-axis`: islands - `y-axis`: mean body mass in grams - Differentiate `sex` using colors and markers (`o` for female and `s` for male). - Display standard deviation as confidence intervals. 3. **Customization:** Customize the point plot: - Use a grid-style background. - Set the title to \\"Mean Body Mass of Penguins by Island and Sex\\". - Set the x-axis label to \\"Island\\". - Set the y-axis label to \\"Mean Body Mass (g)\\". - Use a legend to differentiate the sexes. - Ensure the plot is aesthetically pleasing and clear. **Input:** None. The `seaborn` library and `penguins` dataset are to be used within the code. **Output:** A point plot meeting the above specifications. **Constraints:** - Ensure your code is self-contained and runs without errors. - Use `seaborn` for data visualization. - Use `pandas` as needed for data manipulation. **Example Solution:** ```python import seaborn as sns import pandas as pd # Set theme for the plot sns.set_theme(style=\\"whitegrid\\") # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Group by \'island\' and \'sex\', then calculate the mean and standard deviation of \'body_mass_g\' agg_data = penguins.groupby([\'island\', \'sex\']).agg( mean_body_mass=(\'body_mass_g\', \'mean\'), std_body_mass=(\'body_mass_g\', \'std\') ).reset_index() # Create the point plot plot = sns.pointplot(data=agg_data, x=\'island\', y=\'mean_body_mass\', hue=\'sex\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], capsize=.2, errwidth=1, ci=\'sd\') # Customize the appearance plot.set_title(\'Mean Body Mass of Penguins by Island and Sex\') plot.set_xlabel(\'Island\') plot.set_ylabel(\'Mean Body Mass (g)\') # Display the plot sns.despine() plot.figure ``` **Performance Note:** Ensure that the solution efficiently handles the dataset and produces the plot quickly.","solution":"import seaborn as sns import pandas as pd def generate_point_plot(): Generates a point plot showing the mean body mass of penguins grouped by island and sex, with standard deviation as confidence intervals. # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Group by \'island\' and \'sex\', then calculate the mean and standard deviation of \'body_mass_g\' agg_data = penguins.groupby([\'island\', \'sex\']).agg( mean_body_mass=(\'body_mass_g\', \'mean\'), std_body_mass=(\'body_mass_g\', \'std\') ).reset_index() # Create the point plot plot = sns.pointplot(data=agg_data, x=\'island\', y=\'mean_body_mass\', hue=\'sex\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], capsize=.2, errwidth=1, ci=\'sd\') # Customize the appearance plot.set_title(\'Mean Body Mass of Penguins by Island and Sex\') plot.set_xlabel(\'Island\') plot.set_ylabel(\'Mean Body Mass (g)\') plot.legend(title=\'Sex\') # Display the plot sns.despine() return plot.figure"},{"question":"# Distributed Event Logging with PyTorch In this assessment, you are required to utilize PyTorch\'s distributed elastic events module to implement a solution that logs events during a simulated distributed training process. Your implementation should demonstrate the use of event recording functions and event objects provided by the module. Task: 1. **Initialize the Distributed Environment**: Create a mock distributed environment setup. 2. **Simulate Events**: Simulate at least three different types of events (e.g., start of training, epoch completion, and training error). 3. **Record Events**: Use the provided methods to record these events. 4. **Return Events**: Retrieve and return the recorded events as a list of dictionaries. Function Signature: ```python def simulate_and_record_events() -> List[Dict[str, Any]]: pass ``` Input: - None Output: - A list of dictionaries, each representing a recorded event. Each dictionary must include: - `event_type`: Type of the event. - `timestamp`: Timestamp when the event was recorded. - `metadata`: Any relevant metadata for the event. Constraints: - Use `torch.distributed.elastic.events.record` for recording events. - Utilize classes such as `Event`, `EventSource`, and `EventMetadataValue` to properly structure your events. Example: ```python def simulate_and_record_events(): import time from torch.distributed.elastic.events import record # Simulate event 1: Start of training event1 = { \'event_type\': \'training_start\', \'metadata\': { \'message\': \'Training process started.\' } } # Record event 1 record(event1) time.sleep(1) # Simulate time delay # Simulate event 2: Epoch completion event2 = { \'event_type\': \'epoch_complete\', \'metadata\': { \'epoch\': 1 } } # Record event 2 record(event2) time.sleep(1) # Simulate time delay # Simulate event 3: Training error event3 = { \'event_type\': \'training_error\', \'metadata\': { \'error\': \'Out of memory.\' } } # Record event 3 record(event3) # Retrieve and return recorded events (for the sake of this example, assume a retrieval function) return [ { \'event_type\': \'training_start\', \'timestamp\': 1630567890, \'metadata\': {\'message\': \'Training process started.\'} }, { \'event_type\': \'epoch_complete\', \'timestamp\': 1630567901, \'metadata\': {\'epoch\': 1} }, { \'event_type\': \'training_error\', \'timestamp\': 1630567912, \'metadata\': {\'error\': \'Out of memory.\'} } ] ``` Use the above example as a reference to properly structure and record your events. Make sure to adhere to the constraints and format specified. Happy coding!","solution":"from typing import List, Dict, Any import time import torch.distributed.elastic.events as events def simulate_and_record_events() -> List[Dict[str, Any]]: # Initialize a mock event storage event_storage = [] def record(event): # Mock record implementation (replace with actual call in production) event[\'timestamp\'] = time.time() # Add a timestamp to mock the time of event event_storage.append(event) # Simulate event 1: Start of training event1 = { \'event_type\': \'training_start\', \'metadata\': { \'message\': \'Training process started.\' } } record(event1) time.sleep(1) # Simulate time delay # Simulate event 2: Epoch completion event2 = { \'event_type\': \'epoch_complete\', \'metadata\': { \'epoch\': 1 } } record(event2) time.sleep(1) # Simulate time delay # Simulate event 3: Training error event3 = { \'event_type\': \'training_error\', \'metadata\': { \'error\': \'Out of memory.\' } } record(event3) # Return recorded events return event_storage"},{"question":"# Coding Assessment **Objective**: Implement a custom `asyncio` protocol and transport behavior to create a basic chat server and client that can handle multiple connections simultaneously. # Problem Statement You will implement a simple chat server and client using `asyncio`\'s low-level APIs (transports and protocols). The server should handle multiple clients, receive messages from them, and broadcast messages to all connected clients. # Requirements: Server Implementation 1. **Class**: `ChatServerProtocol` - Inherits from `asyncio.Protocol`. - **Properties**: - `clients`: List to track all connected clients. - **Methods**: - `connection_made(transport)`: Add new connection to the list of clients. - `data_received(data)`: Broadcast received message to all connected clients except the sender. - `connection_lost(exc)`: Remove the client from the list. 2. **Function**: `main()`: - Create and run an asyncio event loop to set up and start the chat server. Client Implementation 1. **Class**: `ChatClientProtocol` - Inherits from `asyncio.Protocol`. - **Properties**: - `on_message`: asyncio.Future object to signal received messages. - **Methods**: - `connection_made(transport)`: Send an initial message to the server, if any. - `data_received(data)`: Print received messages and set the `on_message` result. 2. **Function**: `main(message)`: - Create and run an asyncio event loop to connect to the chat server and send a message. # Input/Output - **Server**: - Input: None. - Output: Runs a server that listens on `127.0.0.1:8888` and outputs incoming connections and messages to the console. - **Client**: - Input: Takes a message to send to the chat server. - Output: Displays received broadcast messages from the server. # Constraints - Your solution should use the low-level `asyncio` APIs (`loop.create_server`, `asyncio.Protocol`, etc.). - Clients must be able to broadcast messages to other clients. - No use of high-level `asyncio` stream APIs. # Performance - The implementation should efficiently handle multiple concurrent connections. # Example Scenario 1. Start the server: ```bash python chat_server.py ``` 2. Start clients and send messages: ```bash python chat_client.py \\"Hello from Client 1\\" python chat_client.py \\"Hello from Client 2\\" ``` 3. All clients should see messages from other clients. # Implementation Implement the server and client based on the requirements detailed above. ```python import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def __init__(self): super().__init__() self.transport = None def connection_made(self, transport): self.transport = transport self.clients.append(self.transport) peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') for client in self.clients: if client is not self.transport: client.write(data) def connection_lost(self, exc): print(\'The client closed the connection\') self.clients.remove(self.transport) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) ``` ```python import asyncio class ChatClientProtocol(asyncio.Protocol): def __init__(self, message, on_message): self.message = message self.on_message = on_message self.transport = None def connection_made(self, transport): self.transport = transport transport.write(self.message.encode()) def data_received(self, data): print(f\\"Data received: {data.decode()}\\") self.on_message.set_result(True) def connection_lost(self, exc): print(\\"The server closed the connection\\") self.on_message.set_result(True) async def main(message): loop = asyncio.get_running_loop() on_message = loop.create_future() transport, protocol = await loop.create_connection( lambda: ChatClientProtocol(message, on_message), \'127.0.0.1\', 8888) try: await on_message finally: transport.close() if __name__ == \'__main__\': import sys message = sys.argv[1] if len(sys.argv) > 1 else \\"Hello World!\\" asyncio.run(main(message)) ``` # Notes - The provided implementation is a basic but complete solution structure. - Ensure correct implementation of custom protocols and event handling. - Utilize logging for extensive debug outputs.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def __init__(self): super().__init__() self.transport = None def connection_made(self, transport): self.transport = transport self.clients.append(self.transport) peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') for client in self.clients: if client is not self.transport: client.write(data) def connection_lost(self, exc): print(\'The client closed the connection\') self.clients.remove(self.transport) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Advanced Python Coding Assessment: Design a File Transfer Service Using asyncio Streams Objective Create a simple file transfer service using asyncio streams. The service involves two parts: 1. A **file server** that listens on a specified TCP port and waits for a file transfer. 2. A **file client** that connects to the server, sends a file, and then terminates the connection. Requirements 1. **File Server**: - Create an asynchronous server that listens on a specified IP and port. - The server should accept incoming connections from clients. - For each client connection: - Receive the file name first. - Then receive the file content and save it to the current directory with the received file name. - Ensure proper handling of connections and file transfers simultaneously from multiple clients. - Properly close each connection after the transfer is complete. 2. **File Client**: - Create an asynchronous client that connects to the server at the specified IP and port. - The client should: - Send the file name. - Send the file content. - Ensure all data is sent before closing the connection. - Properly handle any potential errors and ensure the connection closure. Constraints - You must use the `asyncio` package for all asynchronous operations. - The server should be capable of handling multiple simultaneous connections efficiently. - The maximum file size that can be transferred is 10MB. - For simplicity, you can assume text files only. Implementation Details - Implement both the server and client in a single script, utilizing functions to distinguish their behaviors. - Use the `asyncio.start_server()` for the server implementation. - Use the `asyncio.open_connection()` for the client implementation. Example Usage Here\'s a usage example indicating how the server and client would be run: **Server Side (Run in Terminal 1)**: ```sh python3 file_transfer_service.py server 127.0.0.1 8888 ``` **Client Side (Run in Terminal 2)**: ```sh python3 file_transfer_service.py client 127.0.0.1 8888 file_to_send.txt ``` Input & Output - **Server**: - **Input**: IP and port to listen on. - **Output**: Logs showing the received file names and sizes. - **Client**: - **Input**: IP and port to connect to, and the file path to send. - **Output**: Logs showing the progress of the file sending operation. Task Implement the function `file_transfer_service.py` with the outlined functionalities. Happy coding!","solution":"import asyncio import os import sys async def handle_client(reader, writer): try: # Receive file name data = await reader.read(100) file_name = data.decode().strip() # Receive file content file_data = await reader.read(10485760) # Maximum file size is 10MB # Write the file to the current directory with open(file_name, \'wb\') as f: f.write(file_data) print(f\\"Received file: {file_name} ({len(file_data)} bytes)\\") except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() async def start_server(ip, port): server = await asyncio.start_server(handle_client, ip, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def send_file(ip, port, file_path): try: reader, writer = await asyncio.open_connection(ip, port) file_name = os.path.basename(file_path) with open(file_path, \'rb\') as f: file_data = f.read() if len(file_data) > 10485760: print(\\"File size exceeds 10MB limit.\\") return # Send file name writer.write(file_name.encode()) await writer.drain() # Send file content writer.write(file_data) await writer.drain() print(f\\"Sent file: {file_name} ({len(file_data)} bytes)\\") except Exception as e: print(f\\"Error sending file: {e}\\") finally: writer.close() await writer.wait_closed() if __name__ == \\"__main__\\": mode = sys.argv[1] ip = sys.argv[2] port = int(sys.argv[3]) if mode == \\"server\\": asyncio.run(start_server(ip, port)) elif mode == \\"client\\": file_path = sys.argv[4] asyncio.run(send_file(ip, port, file_path))"},{"question":"Custom SAX ContentHandler Implementation Objective Write a Python program that uses the `xml.sax` package to parse an XML string and extract specific data using a custom SAX ContentHandler. Task Implement a Python function with the following signature: ```python def extract_author_and_titles(xml_string: str) -> List[Tuple[str, str]]: ``` The function should take an XML string representing a collection of books as input and return a list of tuples where each tuple contains the author and title of a book. Example XML ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> </library> ``` For this XML input, the function should return: ```python [(\\"Author 1\\", \\"Book Title 1\\"), (\\"Author 2\\", \\"Book Title 2\\")] ``` Constraints 1. Ensure that all parsing-related exceptions are handled gracefully. 2. Assume that the structure of the XML string is always valid and matches the example structure provided above. 3. You are not allowed to use any other XML parsing libraries apart from `xml.sax`. Implementation Requirements 1. Create a custom `ContentHandler` class to handle `startElement`, `endElement`, and `characters` methods for SAX parsing. 2. Use the `parseString` function of the `xml.sax` package to process the XML string. 3. Properly collect and return the author and title information as specified. Hints - You may need to use instance variables to keep track of the current element and accumulated text content. - Ensure that the `ContentHandler` correctly identifies and gathers data for each book in the library. Example Function Here\'s a skeletal structure to get you started: ```python import xml.sax from typing import List, Tuple class BookContentHandler(xml.sax.ContentHandler): def __init__(self): super().__init__() # Initialize your instance variables here def startElement(self, name, attrs): # Handle start element events here def endElement(self, name): # Handle end element events here def characters(self, content): # Handle character data (text content) here def extract_author_and_titles(xml_string: str) -> List[Tuple[str, str]]: handler = BookContentHandler() xml.sax.parseString(xml_string, handler) return handler.get_books() # Implement this method to return the collected data # Test the function with example XML xml_data = \'\'\' <library> <book> <title>Example Book 1</title> <author>Example Author 1</author> </book> <book> <title>Example Book 2</title> <author>Example Author 2</author> </book> </library> \'\'\' print(extract_author_and_titles(xml_data)) ``` Complete the implementation to parse and extract author and title information from the XML string.","solution":"import xml.sax from typing import List, Tuple class BookContentHandler(xml.sax.ContentHandler): def __init__(self): super().__init__() self.current_element = \\"\\" self.current_title = \\"\\" self.current_author = \\"\\" self.books = [] self.accumulated_text = \\"\\" def startElement(self, name, attrs): self.current_element = name self.accumulated_text = \\"\\" def endElement(self, name): if name == \\"title\\": self.current_title = self.accumulated_text.strip() elif name == \\"author\\": self.current_author = self.accumulated_text.strip() elif name == \\"book\\": self.books.append((self.current_author, self.current_title)) self.accumulated_text = \\"\\" def characters(self, content): self.accumulated_text += content def get_books(self): return self.books def extract_author_and_titles(xml_string: str) -> List[Tuple[str, str]]: handler = BookContentHandler() xml.sax.parseString(xml_string, handler) return handler.get_books()"},{"question":"Implementing and Comparing LDA and QDA with Scikit-Learn Objective: To assess the ability to use scikit-learn’s implementation of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) for classification and dimensionality reduction tasks, including the application of shrinkage techniques. Problem Statement: You are provided with a synthetic dataset containing three classes. Your tasks are as follows: 1. **Perform and compare classification using LDA and QDA:** - Fit both models on the training set. - Predict the class labels on the testing set. - Report the accuracy scores for both models. 2. **Dimensionality Reduction using LDA:** - Reduce the dimensionality of the dataset to 2 components using LDA. - Plot the 2D projection of the data points, color-coded by their class labels. 3. **Apply Shrinkage in LDA:** - Fit an LDA model with shrinkage to the training set. - Report the accuracy score on the testing set. # Dataset: You are provided with a synthetic dataset as follows: - `X_train`: A 2D numpy array of shape (n_train_samples, n_features) containing the training feature set. - `y_train`: A 1D numpy array of shape (n_train_samples,) containing the training labels. - `X_test`: A 2D numpy array of shape (n_test_samples, n_features) containing the testing feature set. - `y_test`: A 1D numpy array of shape (n_test_samples,) containing the testing labels. # Instructions: 1. **Classification using LDA and QDA:** ```python from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # Initializing LDA and QDA classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Fitting classifiers on the training set lda.fit(X_train, y_train) qda.fit(X_train, y_train) # Making predictions on the testing set lda_predictions = lda.predict(X_test) qda_predictions = qda.predict(X_test) # Reporting the accuracy scores lda_accuracy = accuracy_score(y_test, lda_predictions) qda_accuracy = accuracy_score(y_test, qda_predictions) print(\\"LDA Accuracy:\\", lda_accuracy) print(\\"QDA Accuracy:\\", qda_accuracy) ``` 2. **Dimensionality Reduction using LDA:** ```python import matplotlib.pyplot as plt # Reducing the dimensionality to 2 components lda_reduce = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_reduce.fit(X_train, y_train).transform(X_train) # Plotting the 2D projection plt.figure() colors = [\'red\', \'green\', \'blue\'] target_names = [\'Class 0\', \'Class 1\', \'Class 2\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y_train == i, 0], X_r2[y_train == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA Dimensionality Reduction\') plt.show() ``` 3. **Applying Shrinkage in LDA:** ```python # Initializing an LDA classifier with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') # Fitting the classifier on the training set lda_shrinkage.fit(X_train, y_train) # Making predictions on the testing set lda_shrinkage_predictions = lda_shrinkage.predict(X_test) # Reporting the accuracy score lda_shrinkage_accuracy = accuracy_score(y_test, lda_shrinkage_predictions) print(\\"LDA with Shrinkage Accuracy:\\", lda_shrinkage_accuracy) ``` # Notes: - Ensure that all necessary libraries (e.g., scikit-learn, matplotlib, numpy) are imported. - Comment your code appropriately for clarity. - Provide a short explanation of your approach and results with each task. Expected Output: 1. Accuracy scores of LDA and QDA classifiers on the test set. 2. A scatter plot showing the 2D projection of the dataset after LDA reduction. 3. Accuracy score of the LDA classifier with shrinkage on the test set.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np def lda_qda_classification(X_train, y_train, X_test, y_test): # Initializing LDA and QDA classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Fitting classifiers on the training set lda.fit(X_train, y_train) qda.fit(X_train, y_train) # Making predictions on the testing set lda_predictions = lda.predict(X_test) qda_predictions = qda.predict(X_test) # Reporting the accuracy scores lda_accuracy = accuracy_score(y_test, lda_predictions) qda_accuracy = accuracy_score(y_test, qda_predictions) return lda_accuracy, qda_accuracy def lda_dimensionality_reduction(X_train, y_train): # Reducing the dimensionality to 2 components lda_reduce = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_reduce.fit(X_train, y_train).transform(X_train) return X_r2 def plot_lda_2d_projection(X_r2, y_train): # Plotting the 2D projection plt.figure() colors = [\'red\', \'green\', \'blue\'] target_names = [\'Class 0\', \'Class 1\', \'Class 2\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y_train == i, 0], X_r2[y_train == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA Dimensionality Reduction\') plt.show() def lda_with_shrinkage(X_train, y_train, X_test, y_test): # Initializing an LDA classifier with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') # Fitting the classifier on the training set lda_shrinkage.fit(X_train, y_train) # Making predictions on the testing set lda_shrinkage_predictions = lda_shrinkage.predict(X_test) # Reporting the accuracy score lda_shrinkage_accuracy = accuracy_score(y_test, lda_shrinkage_predictions) return lda_shrinkage_accuracy"},{"question":"# Python Coding Assessment Question **Objective:** You are to write a Python program using the `asyncio` streams functionality. The task involves creating a simple TCP chat server that can handle multiple clients simultaneously using async/await. Each client\'s messages should be broadcasted to all connected clients except the sender. **Requirements:** 1. Implement two main functions: `start_chat_server` and `handle_client`. 2. The server should listen on a specified host and port. 3. Each client connection should be handled asynchronously. 4. Handle incoming messages from a client and broadcast them to all other clients. 5. Implement proper error handling to manage client disconnections gracefully. **Function Specifications:** 1. **`start_chat_server(host: str, port: int)`** - Establish a TCP server listening on the given host and port. - Use `asyncio.start_server` to start the server. - Continuously accept new client connections and create tasks to handle each client using `handle_client`. 2. **`handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter, clients: List[Tuple[asyncio.StreamReader, asyncio.StreamWriter]])`** - This function handles communication with a connected client. - It should read messages from the client and broadcast them to all other connected clients. - Utilize `await reader.read()` to read client messages. - Ensure proper disconnection handling by removing the client from the list when disconnected. ```python import asyncio from typing import List, Tuple clients = [] async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') clients.append((reader, writer)) print(f\\"Connected to {addr}\\") while True: try: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message!r} from {addr!r}\\") message = f\\"{addr}: {message}\\" for r, w in clients: if w is not writer: w.write(message.encode()) await w.drain() except asyncio.CancelledError: break except Exception as e: print(f\\"Error handling message from {addr}: {e}\\") break print(f\\"Disconnecting {addr}\\") clients.remove((reader, writer)) writer.close() await writer.wait_closed() async def start_chat_server(host: str, port: int): server = await asyncio.start_server( handle_client, host, port) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(start_chat_server(host, port)) ``` # Constraints: 1. The application must support at least 10 concurrent clients. 2. Each client message may be up to 256 characters long. 3. The server should log all key events to the console (e.g., client connections, disconnections, and received messages). # Performance: - Ensure efficient handling of incoming and outgoing stream operations using `await`. # Deliverables: 1. Script implementing the chat server. 2. Brief documentation of the core logic and any considerations taken for concurrency and error handling.","solution":"import asyncio from typing import List, Tuple clients = [] async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') clients.append((reader, writer)) print(f\\"Connected to {addr}\\") try: while True: data = await reader.read(256) if not data: break message = data.decode().strip() print(f\\"Received {message!r} from {addr!r}\\") broadcast_message = f\\"{addr}: {message}\\" for r, w in clients: if w is not writer: w.write(broadcast_message.encode() + b\'n\') await w.drain() except asyncio.CancelledError: pass except Exception as e: print(f\\"Error handling message from {addr}: {e}\\") finally: print(f\\"Disconnecting {addr}\\") clients.remove((reader, writer)) writer.close() await writer.wait_closed() async def start_chat_server(host: str, port: int): server = await asyncio.start_server( handle_client, host, port) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(start_chat_server(host, port))"},{"question":"Objective Design and implement functions to allocate, reallocate, and free memory blocks using Python\'s memory management APIs. You will also implement a simple debugging mechanism to detect memory misuse. Instructions 1. **Function Definitions**: - Implement three functions using Python C API memory management functions: * `custom_malloc(size)`: Allocates memory of the specified size and returns a pointer. * `custom_realloc(ptr, new_size)`: Reallocates the memory block at the given pointer to the new specified size and returns the new pointer. * `custom_free(ptr)`: Frees the memory block at the given pointer. 2. **Debugging Mechanism**: - Implement a function `debug_memory(ptr, size)` that checks if the specified memory block has been initialized correctly. This function should fill the memory with a pattern (e.g., `0xCD` for newly allocated memory and `0xDD` after freeing memory) and verify the integrity of the memory block. 3. **Constraints**: - Use `PyMem_Malloc`, `PyMem_Realloc`, and `PyMem_Free` from the Python C API for memory allocation and deallocation. - Ensure all memory allocations and deallocations are correctly paired to avoid memory leaks or corruption. - Implement necessary checks to handle memory allocation failures gracefully. 4. **Expected Input/Output**: - `custom_malloc(size)`: Returns a valid pointer to the allocated memory. - `custom_realloc(ptr, new_size)`: Returns a valid pointer to the reallocated memory. - `custom_free(ptr)`: Frees the memory block without returning anything. - `debug_memory(ptr, size)`: Verifies the integrity of the memory block and prints a message if any inconsistency is detected. 5. **Performance Requirements**: - The functions should be efficient in terms of both time and space complexity. - The debugging mechanism should have minimal overhead on normal operations. Example Usage: ```python # Assuming these functions are defined correctly # Allocate memory ptr = custom_malloc(100) # Check and initialize memory debug_memory(ptr, 100) # Reallocate memory ptr = custom_realloc(ptr, 150) # Check the reallocated memory debug_memory(ptr, 150) # Free memory custom_free(ptr) ``` Implement these functions in C, compile them as a Python extension module, and provide a Python script that demonstrates the use of these functions along with your debugging mechanism. *Focus areas*: - Correct use of Python memory management APIs. - Handling memory allocation failures. - Effective implementation of a debugging mechanism to detect common memory errors.","solution":"import ctypes allocated_memory = {} def custom_malloc(size): Allocates memory of the specified size. buffer = ctypes.create_string_buffer(size) ptr = ctypes.cast(buffer, ctypes.c_void_p).value allocated_memory[ptr] = buffer return ptr def custom_realloc(ptr, new_size): Reallocates memory block at the given pointer to the new specified size. if ptr not in allocated_memory: raise ValueError(\\"Pointer not allocated\\") new_buffer = ctypes.create_string_buffer(new_size) old_buffer = allocated_memory[ptr] ctypes.memmove(new_buffer, old_buffer, min(len(old_buffer), new_size)) new_ptr = ctypes.cast(new_buffer, ctypes.c_void_p).value allocated_memory[new_ptr] = new_buffer del allocated_memory[ptr] return new_ptr def custom_free(ptr): Frees the memory block at the given pointer. if ptr not in allocated_memory: raise ValueError(\\"Pointer not allocated\\") del allocated_memory[ptr] def debug_memory(ptr, size, pattern=0xCD): Fills the memory with a specified pattern and verifies the integrity of the memory block. if ptr not in allocated_memory: raise ValueError(\\"Pointer not allocated\\") buffer = allocated_memory[ptr] buffer[:size] = bytes([pattern] * size) # Verification step for byte in buffer[:size]: if byte != pattern: print(f\\"Memory check failed at pointer {ptr}\\") return False print(f\\"Memory check passed at pointer {ptr}\\") return True"},{"question":"# Seaborn Coding Assessment Objective: You are required to demonstrate your understanding of the seaborn library by creating various visualizations of a given dataset. Your task involves implementing different kinds of plots, customizing them, and combining multiple plots into a single visualization. Dataset: For this assessment, you will use the `\'tips\'` dataset, which can be loaded using seaborn\'s `load_dataset` function. ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` # Task: 1. **Scatter Plot**: - Create a scatter plot using the `relplot` function to show the relationship between the `total_bill` and `tip` variables. - Color the points by the `day` of the week. 2. **Line Plot**: - Create a line plot using the `relplot` function to show the average `tip` amount for each day of the week. - Use the `ci` parameter to display confidence intervals. 3. **Categorical Plot**: - Create a box plot using the `catplot` function to show the distribution of `total_bill` grouped by `day` and categorized by `smoker`. 4. **Custom Plot with Multiple Visualizations**: - Create a joint plot using the `jointplot` function to show the relationship between `total_bill` and `tip`. - Use `kind=\\"hex\\"` to create a hexbin plot. - Display the marginal distributions of each variable on the top and right side of the plot. Input Constraints: - You should not modify the dataset or add any new data points. Output: - Each step should produce the corresponding plot as described. - Make sure to label the axes and provide appropriate titles for each plot. Additional Requirements: - Use the default seaborn theme by calling `sns.set_theme()`. - Make the plots visually appealing by customizing colors and adding relevant labels where necessary. Example Code Structure: ```python import seaborn as sns # Apply the default theme sns.set_theme() # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter Plot sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") # 2. Line Plot sns.relplot(data=tips, kind=\\"line\\", x=\\"day\\", y=\\"tip\\", ci=\\"sd\\") # 3. Categorical Plot sns.catplot(data=tips, kind=\\"box\\", x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\") # 4. Custom Plot with Multiple Visualizations sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"hex\\", color=\\"blue\\") ``` You must provide the solution code for each of the tasks above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Apply the default theme sns.set_theme() # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter Plot def create_scatter_plot(): scatter_plot = sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", palette=\\"muted\\") scatter_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") scatter_plot.fig.suptitle(\\"Scatter Plot of Total Bill vs Tip Colored by Day\\", y=1.03) return scatter_plot # 2. Line Plot def create_line_plot(): line_plot = sns.relplot(data=tips, kind=\\"line\\", x=\\"day\\", y=\\"tip\\", ci=\\"sd\\", marker=\\"o\\", dashes=False) line_plot.set_axis_labels(\\"Day\\", \\"Average Tip\\") line_plot.fig.suptitle(\\"Line Plot of Average Tip by Day with Confidence Interval\\", y=1.03) return line_plot # 3. Categorical Plot def create_cat_plot(): cat_plot = sns.catplot(data=tips, kind=\\"box\\", x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", palette=\\"Set2\\") cat_plot.set_axis_labels(\\"Day\\", \\"Total Bill\\") cat_plot.fig.suptitle(\\"Box Plot of Total Bill by Day and Smoker Status\\", y=1.03) return cat_plot # 4. Custom Plot with Multiple Visualizations def create_custom_plot(): custom_plot = sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"hex\\", color=\\"blue\\") custom_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") custom_plot.fig.suptitle(\\"Hexbin Plot of Total Bill vs Tip with Marginal Distributions\\", y=1.03) return custom_plot"},{"question":"**Task: Advanced Bytecode Analysis with `dis` Module** **Objective:** Implement a Python function called `bytecode_analysis` that takes a Python function as input and performs a series of operations to analyze its bytecode. The function should return a dictionary containing the following details: 1. **Instruction Count**: Total number of instructions. 2. **Unique Opcodes**: Set of unique opcode names used in the function. 3. **Line Number Instructions**: Mapping from source line numbers to the list of instructions starting at each line. 4. **Jump Targets**: List of instruction offsets that are jump targets. 5. **Stack Effects**: Total stack effect of the bytecode execution, including the maximum possible stack change. **Your tasks:** 1. Use the `dis.Bytecode` class to wrap the input function to access its disassembled bytecode. 2. Iterate over the bytecode instructions to collect the required details. 3. Compile the collected information into a dictionary and return it. **Signature:** ```python import dis def bytecode_analysis(func): Analyze the bytecode of the given function. Args: - func (Callable): The input function to analyze. Returns: - dict: A dictionary containing analysis results. # Your implementation here ``` **Requirements:** 1. `func` is a Python function. Do not assume the function has no arguments; consider using stubs if needed. 2. Your solution should handle various bytecodes, including those related to function calls, control flow, and data manipulation. 3. The dictionary should have the following structure: ```python { \\"instruction_count\\": int, \\"unique_opcodes\\": set, \\"line_number_instructions\\": dict, # {line_number: [Instruction, ...]} \\"jump_targets\\": list, # [offset, ...] \\"stack_effects\\": int } ``` 4. Efficiency is important but secondary to correctness for this task. **Example:** ```python def example_function(x): if x > 0: return x * 2 else: return x - 2 result = bytecode_analysis(example_function) print(result) # Expected output format: # { # \\"instruction_count\\": 11, # \\"unique_opcodes\\": {\\"LOAD_FAST\\", \\"COMPARE_OP\\", \\"POP_JUMP_IF_FALSE\\", \\"LOAD_CONST\\", ...}, # \\"line_number_instructions\\": {2: [LOAD_FAST, COMPARE_OP, ...], 3: [LOAD_FAST, BINARY_MULTIPLY, ...], ...}, # \\"jump_targets\\": [6, 10], # \\"stack_effects\\": 2 # this is an example; your result may differ # } ``` **Note:** - The exact values in the dictionary will depend on the function being analyzed. The example here is for illustration purposes.","solution":"import dis def bytecode_analysis(func): Analyze the bytecode of the given function. Args: - func (Callable): The input function to analyze. Returns: - dict: A dictionary containing analysis results. bytecode = dis.Bytecode(func) instruction_count = 0 unique_opcodes = set() line_number_instructions = {} jump_targets = [] overall_stack_effect = 0 max_stack_effect = 0 for instr in bytecode: instruction_count += 1 unique_opcodes.add(instr.opname) if instr.starts_line is not None: if instr.starts_line not in line_number_instructions: line_number_instructions[instr.starts_line] = [] line_number_instructions[instr.starts_line].append(instr.opname) if instr.is_jump_target: jump_targets.append(instr.offset) overall_stack_effect += dis.stack_effect(instr.opcode, instr.arg) max_stack_effect = max(max_stack_effect, overall_stack_effect) return { \\"instruction_count\\": instruction_count, \\"unique_opcodes\\": unique_opcodes, \\"line_number_instructions\\": line_number_instructions, \\"jump_targets\\": jump_targets, \\"stack_effects\\": max_stack_effect }"},{"question":"**Objective:** Implement a function that concurrently runs multiple asynchronous tasks, ensures that some tasks are protected from cancellations, properly handles timeouts, and integrates blocking IO operations using the asyncio package. **Task:** Write an asynchronous function `manage_tasks(tasks, protected, timeout)` that performs the following operations: 1. Runs a list of tasks concurrently. 2. Some of these tasks are designated as protected and should not be canceled even if others are. 3. Each task should be awaited with a specified timeout. 4. Any blocking IO operation within a task should be run in a separate thread to avoid blocking the event loop. **Function Signature:** ```python async def manage_tasks(tasks: list, protected: set, timeout: float) -> list: pass ``` **Input:** - `tasks`: A list of coroutines representing the tasks to be run concurrently. - `protected`: A set of indices representing tasks in the `tasks` list that should be protected from cancellations. - `timeout`: A float value representing the maximum time in seconds to wait for each task to complete. **Output:** - A list of results from the tasks that run successfully within the timeout period. If a task is cancelled due to a timeout, its result should be None. If a protected task encounters an exception, the exception should be captured and included in the result list. **Constraints:** - The tasks provided in the `tasks` list can be any valid coroutine functions. - You must handle timeouts using `asyncio.wait_for`. - Use `asyncio.shield` to protect the tasks indicated in the `protected` set. - Any potential blocking operations within the tasks should be handled using `asyncio.to_thread`. **Examples:** 1. **Example 1:** ```python async def task1(): await asyncio.sleep(2) return \\"task1 done\\" async def task2(): await asyncio.sleep(3) return \\"task2 done\\" async def task3(): await asyncio.sleep(1) return \\"task3 done\\" tasks = [task1(), task2(), task3()] protected = {1} timeout = 2 results = await manage_tasks(tasks, protected, timeout) print(results) # Output: [None, \'task2 done\', \'task3 done\'] ``` 2. **Example 2:** ```python async def task_with_blocking_io(): def blocking_io(): time.sleep(3) return \\"io done\\" result = await asyncio.to_thread(blocking_io) return result async def task_with_exception(): raise ValueError(\\"An error occurred\\") tasks = [task_with_blocking_io(), task_with_exception()] protected = {1} timeout = 5 results = await manage_tasks(tasks, protected, timeout) print(results) # Output: [\'io done\', ValueError(\\"An error occurred\\")] ``` **Note:** - Ensure robust handling of tasks with careful use of asyncio methods to manage concurrency, cancellations, and exceptions.","solution":"import asyncio async def manage_tasks(tasks, protected, timeout): results = [None] * len(tasks) tasks_with_shield = [ asyncio.shield(tasks[i]) if i in protected else tasks[i] for i in range(len(tasks)) ] async def run_task(task, index): try: results[index] = await asyncio.wait_for(task, timeout) except asyncio.TimeoutError: results[index] = None except Exception as e: if index in protected: results[index] = e else: results[index] = None await asyncio.gather(*[run_task(tasks_with_shield[i], i) for i in range(len(tasks))]) return results"},{"question":"Objective You are tasked with implementing a custom set manager in pure Python using the standard set and frozenset objects. This manager should provide certain functionalities analogous to the C API functions described in the given documentation. Function Description 1. **check_set_type(p: object) -> str:** - Accepts any Python object and returns a string indicating whether it is a `set`, `frozenset`, or neither. 2. **create_set(iterable: Optional[Iterable[Any]] = None) -> set:** - Creates a new `set` from a given iterable. If no iterable is provided, it should return an empty set. 3. **create_frozenset(iterable: Optional[Iterable[Any]] = None) -> frozenset:** - Creates a new `frozenset` from a given iterable. If no iterable is provided, it should return an empty frozenset. 4. **set_size(anyset: Union[set, frozenset]) -> int:** - Returns the size (number of elements) of a given `set` or `frozenset`. 5. **set_contains(anyset: Union[set, frozenset], key: Any) -> bool:** - Checks if the given key is present in the `set` or `frozenset`. Returns `True` if found, otherwise `False`. 6. **add_to_set(p_set: set, key: Any) -> None:** - Adds the specified key to the provided `set`. Raises a `TypeError` if the key is unhashable. 7. **discard_from_set(p_set: set, key: Any) -> bool:** - Removes the key from the given `set` if present. Returns `True` if the key was removed, `False` otherwise. 8. **pop_from_set(p_set: set) -> Any:** - Removes and returns an arbitrary element from the `set`. Raises a `KeyError` if the set is empty. 9. **clear_set(p_set: set) -> None:** - Removes all elements from the given `set`. Constraints - You must use Python\'s `set` and `frozenset` objects to implement the functions. - You should handle errors gracefully, mimicking the behavior described in the documentation. Example Usage ```python # Creating sets and frozensets s = create_set([1, 2, 3]) fs = create_frozenset([4, 5, 6]) # Checking types print(check_set_type(s)) # Output: \\"set\\" print(check_set_type(fs)) # Output: \\"frozenset\\" print(check_set_type(123)) # Output: \\"neither\\" # Set operations print(set_size(s)) # Output: 3 print(set_contains(s, 2)) # Output: True add_to_set(s, 4) print(s) # Output: {1, 2, 3, 4} print(discard_from_set(s, 2)) # Output: True print(s) # Output: {1, 3, 4} print(pop_from_set(s)) # Output: 1 (or any element from the set) print(s) # Output: {3, 4} clear_set(s) print(s) # Output: set() ``` Write Python functions to implement the provided functionalities.","solution":"from typing import Any, Iterable, Optional, Union def check_set_type(p: object) -> str: Returns \'set\' if p is a set, \'frozenset\' if p is a frozenset, and \'neither\' otherwise. if isinstance(p, set): return \\"set\\" elif isinstance(p, frozenset): return \\"frozenset\\" else: return \\"neither\\" def create_set(iterable: Optional[Iterable[Any]] = None) -> set: Creates a set from an iterable or an empty set if no iterable is provided. return set(iterable) if iterable is not None else set() def create_frozenset(iterable: Optional[Iterable[Any]] = None) -> frozenset: Creates a frozenset from an iterable or an empty frozenset if no iterable is provided. return frozenset(iterable) if iterable is not None else frozenset() def set_size(anyset: Union[set, frozenset]) -> int: Returns the size of the provided set or frozenset. return len(anyset) def set_contains(anyset: Union[set, frozenset], key: Any) -> bool: Checks if the key is present in the provided set or frozenset. Returns True if found, False otherwise. return key in anyset def add_to_set(p_set: set, key: Any) -> None: Adds the key to the provided set. p_set.add(key) def discard_from_set(p_set: set, key: Any) -> bool: Discards the key from the provided set if present. Returns True if the key was removed, False otherwise. if key in p_set: p_set.discard(key) return True return False def pop_from_set(p_set: set) -> Any: Removes and returns an arbitrary element from the provided set. Raises a KeyError if the set is empty. return p_set.pop() def clear_set(p_set: set) -> None: Removes all elements from the provided set. p_set.clear()"},{"question":"# Encoding and Normalizing Unicode Strings **Objective**: Write a Python function that takes a list of Unicode strings and performs the following tasks: 1. Converts each string to UTF-8 encoded bytes. 2. Converts the encoded bytes back to Unicode strings. 3. Normalizes each Unicode string to \\"NFD\\" (Canonical Decomposition). 4. Returns the list of normalized Unicode strings. **Function Signature**: ```python def process_unicode_strings(unicode_strings: list[str]) -> list[str]: pass ``` **Input**: - `unicode_strings`: A list of Unicode strings. Each string may contain extended Unicode characters. **Output**: - A list of Unicode strings, each of which has been: 1. Encoded to UTF-8 and then decoded back to Unicode. 2. Normalized using Canonical Decomposition (NFD). **Constraints**: - Each string in the input list will contain at most 1000 characters. - The input list will contain at most 100 strings. **Example**: ```python # Example usage unicode_strings = [\\"Café\\", \\"Gürzenichstraße\\", \\"façade\\", \\"ﬁancé\\"] normalized_strings = process_unicode_strings(unicode_strings) print(normalized_strings) # Expected Output (actual representation may differ due to normalization): # [\'Café\', \'Gurzenichstraße\', \'façade\', \'fiancé\'] ``` **Notes**: - Use the `encode` and `decode` methods for converting between `str` and `bytes`. - Use the `unicodedata.normalize` function for normalization. - Ensure the function handles different Unicode characters properly through the entire encoding and normalization process.","solution":"import unicodedata def process_unicode_strings(unicode_strings: list[str]) -> list[str]: Processes a list of Unicode strings by: 1. Encoding each string to UTF-8 bytes. 2. Decoding them back to Unicode strings. 3. Normalizing each string to \'NFD\' (Canonical Decomposition). Returns the list of normalized Unicode strings. normalized_strings = [] for ustr in unicode_strings: # Encode to UTF-8 bytes utf8_bytes = ustr.encode(\'utf-8\') # Decode back to Unicode string decoded_str = utf8_bytes.decode(\'utf-8\') # Normalize to NFD normalized_str = unicodedata.normalize(\'NFD\', decoded_str) # Add normalized string to the list normalized_strings.append(normalized_str) return normalized_strings"},{"question":"# Question: fmri Signal Visualization and Analysis **Problem Statement**: You are provided with the `fmri` dataset from the Seaborn library, which contains information about brain activity across different timepoints and events, categorized by subjects and regions. You are required to write a function `plot_fmri_band` that processes and visualizes this data. 1. Filter the dataset to include only data from the `\'parietal\'` region. 2. Create a dataplot with: - X-axis representing `timepoint` - Y-axis representing `signal` - Color differentiation based on `event` 3. Add a Band to show the error interval. 4. Add a Line to show the trend over time. 5. Return the created plot object. **Function signature**: ```python def plot_fmri_band() -> so.Plot: pass ``` # Constraints: - You should use the `seaborn.objects` API provided in the documentation. - Make sure your function handles data driven transformations as shown. - The function should be able to generate the visualization correctly without additional input assumptions. # Example Usage: ```python import seaborn.objects as so # Ensure that seaborn and pandas are imported # Call the function to generate the plot plot = plot_fmri_band() # Display the plot plot.show() ``` # Expected Output: A plot visualization with bands showing error intervals and lines showing signal trends for each event: - x-axis: `timepoint` - y-axis: `signal` - Different colored lines for different `event`. - Bands around the lines showing the extent of signals for each timepoint. **Dataset Example**: The `fmri` dataset provided by Seaborn looks like this: ``` subject timepoint event region signal 0 s13 0 stim parietal 0.283278 1 s5 0 stim parietal 0.223425 ... ``` **Notes**: - Make sure to use Pandas for filtering the dataset. - Ensure each transformation aligns with the instructions provided.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def plot_fmri_band() -> so.Plot: # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the data to include only the \'parietal\' region fmri_parietal = fmri[fmri[\'region\'] == \'parietal\'] # Create the plot plot = ( so.Plot(fmri_parietal, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(errorbar=\\"se\\")) ) return plot"},{"question":"# Pandas Configuration Challenge Objective: Implement a function that reads a DataFrame, modifies a set of pandas display options, and produces a specific output format. This will test your understanding of pandas\' options and settings API. Problem Statement: Write a function `configure_dataframe_display` that takes two parameters: 1. `df` (pandas DataFrame): The DataFrame that needs to be displayed. 2. `options_dict` (dictionary): A dictionary containing pandas options to be set before displaying the DataFrame. The keys of the dictionary are the option names (as strings), and the values are the values to be set for those options. The function should: 1. Update the pandas display options according to the `options_dict`. 2. Print the DataFrame `df` according to the updated display settings. 3. After displaying the DataFrame, reset all options modified in `options_dict` to their default values. Constraints: - The function should handle any valid pandas display option. - The function should correctly apply all options in `options_dict`. - Ensure the DataFrame printing adheres strictly to the updated options. - Unrecognized option names should raise a `ValueError`. Input: - `df` (pandas DataFrame) - `options_dict` (Dictionary of options and values) Output: The function should print the DataFrame with the applied options and have no return value. # Example: ```python import pandas as pd import numpy as np def configure_dataframe_display(df, options_dict): # Your implementation here pass # Example DataFrame df = pd.DataFrame(np.random.randn(10, 3), columns=[\'A\', \'B\', \'C\']) # Example Options Dictionary options_dict = { \'display.max_rows\': 5, \'display.precision\': 2, \'display.expand_frame_repr\': False } # Function Call (the DataFrame `df` should be printed with the specified options) configure_dataframe_display(df, options_dict) ``` For the given `options_dict`, the function should: 1. Set `display.max_rows` to 5: Display a maximum of 5 rows. 2. Set `display.precision` to 2: Display numbers with 2 decimal places. 3. Set `display.expand_frame_repr` to False: Do not wrap the DataFrame representation across multiple lines. After printing the DataFrame, the options should be reset to their default values. Note: For simplicity, only handle display options related to DataFrame representation.","solution":"import pandas as pd def configure_dataframe_display(df, options_dict): Set pandas display options according to options_dict, print the DataFrame, and reset to defaults. Parameters: df (pandas.DataFrame): The DataFrame to be displayed. options_dict (dict): Dictionary containing pandas display options to be set. if not isinstance(df, pd.DataFrame): raise ValueError(\\"The first parameter must be a pandas DataFrame\\") if not isinstance(options_dict, dict): raise ValueError(\\"The second parameter must be a dictionary\\") default_options = {} # Fetch current default values for the given pandas options for option, value in options_dict.items(): try: default_options[option] = pd.get_option(option) except KeyError as e: raise ValueError(f\\"Invalid pandas option: {option}\\") # Set new options for option, value in options_dict.items(): pd.set_option(option, value) # Print the DataFrame print(df) # Reset the options to their default values for option, value in default_options.items(): pd.reset_option(option)"},{"question":"Objective: Design a Python script that demonstrates your understanding of the `lzma` module by implementing functions to compress and decompress data from and to files using custom filter chains. Additionally, handle any errors that may occur during these operations. Problem Description: 1. Write a function `compress_file(input_file: str, output_file: str, filter_chain: list)` that: - Reads data from `input_file`. - Compresses the data using a custom filter chain provided in `filter_chain`. - Writes the compressed data to `output_file`. 2. Write a function `decompress_file(input_file: str, output_file: str)` that: - Reads compressed data from `input_file`. - Decompresses the data. - Writes the decompressed data to `output_file`. 3. Ensure to handle any `lzma.LZMAError` that may occur during compression or decompression and print an appropriate error message. 4. Provide an example filter chain that includes both a delta filter and an LZMA2 filter with specific parameters. Input: - `input_file`: Path to the file to be compressed or decompressed (string). - `output_file`: Path where the resulting compressed or decompressed file should be saved (string). - `filter_chain`: A list of dictionaries specifying the custom filter chain for compression. Output: - The resultant file saved at `output_file` location after compression or decompression. - Error messages printed to the console in case of any errors during the operations. Example Usage: ```python filter_chain_example = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7 | lzma.PRESET_EXTREME}, ] compress_file(\\"example.txt\\", \\"example.xz\\", filter_chain_example) decompress_file(\\"example.xz\\", \\"example_decompressed.txt\\") ``` Constraints: - The input and output files should be relatively small to fit within memory constraints. - The custom filter chain should not exceed 4 filters and must contain at least one compression filter as the last filter. - The function should properly handle cases where the input file does not exist or is not readable. Notes: - Ensure to use relevant parts of the `lzma` module effectively: - `lzma.open` for file handling. - Proper error catching using `lzma.LZMAError`. - Utilize custom filter chains in `lzma.open` for compression. Remember to include appropriate comments in your code to explain your logic and assumptions.","solution":"import lzma def compress_file(input_file: str, output_file: str, filter_chain: list): Compresses data from input_file using a custom filter chain and writes it to output_file. :param input_file: Path to the file to be compressed. :param output_file: Path where the compressed file should be saved. :param filter_chain: A list of dictionaries specifying the custom filter chain for compression. try: with open(input_file, \'rb\') as f_in: data = f_in.read() with lzma.open(output_file, \'wb\', filters=filter_chain) as f_out: f_out.write(data) except (lzma.LZMAError, FileNotFoundError, IOError) as e: print(f\\"Error compressing file {input_file}: {e}\\") def decompress_file(input_file: str, output_file: str): Decompresses data from input_file and writes it to output_file. :param input_file: Path to the compressed file. :param output_file: Path where the decompressed file should be saved. try: with lzma.open(input_file, \'rb\') as f_in: data = f_in.read() with open(output_file, \'wb\') as f_out: f_out.write(data) except (lzma.LZMAError, FileNotFoundError, IOError) as e: print(f\\"Error decompressing file {input_file}: {e}\\") # Example filter chain filter_chain_example = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7 | lzma.PRESET_EXTREME}, ]"},{"question":"# Python Coding Assessment Question **Problem Statement:** You are required to implement a Python function that dynamically imports modules and handles module operations using the `importlib` module. Your task is to create a utility function named `dynamic_import` that imports a module, verifies the presence of specified attributes (functions, classes, or variables), and optionally executes a function within that module with provided arguments. **Function Signature:** ```python def dynamic_import(module_name: str, attributes: list, func_to_execute: str = None, func_args: list = []) -> dict: ``` **Parameters:** - `module_name` (str): The name of the module to be imported. - `attributes` (list): A list of attribute names (as strings) to check within the imported module. - `func_to_execute` (str, optional): The name of the function to execute after importing the module. If not provided, no function will be executed. - `func_args` (list, optional): The arguments to be passed to the `func_to_execute` if specified. **Returns:** - `result` (dict): A dictionary with the following keys: - `\'module\'` (ModuleType): The imported module. - `\'attributes_exist\'` (dict): A dictionary where the keys are the attribute names and the values are booleans indicating their existence in the module. - `\'execution_result\'` (any): The result of the function execution, or `None` if no function was executed. **Constraints:** - Assume that the modules to be imported are present and accessible in the environment. - The function should handle importing errors and attribute errors gracefully by returning appropriate error messages within the result. **Example Usage:** ```python result = dynamic_import(\'math\', [\'sqrt\', \'pi\', \'nonexistent_attr\'], \'sqrt\', [16]) print(result) # Expected Output: # { # \'module\': <module \'math\' (built-in)>, # \'attributes_exist\': {\'sqrt\': True, \'pi\': True, \'nonexistent_attr\': False}, # \'execution_result\': 4.0 # } result = dynamic_import(\'os\', [\'path\', \'nonexistent_func\']) print(result) # Expected Output: # { # \'module\': <module \'os\' from \'...\'>, # \'attributes_exist\': {\'path\': True, \'nonexistent_func\': False}, # \'execution_result\': None # } ``` **Notes:** - The function should utilize `importlib` to handle module imports. - If an attribute does not exist within the module, return `False` for that attribute in the `\'attributes_exist\'` dictionary. - If the specified function to execute is not found in the module or the module import fails, return appropriate error messages within the result dictionary. This question evaluates the student\'s ability to leverage `importlib` functionalities, perform dynamic importing, check module attributes, and optionally execute functions with arguments in a robust and error-handled manner.","solution":"import importlib import types def dynamic_import(module_name: str, attributes: list, func_to_execute: str = None, func_args: list = []) -> dict: result = { \'module\': None, \'attributes_exist\': {}, \'execution_result\': None } # Attempt to import the module try: module = importlib.import_module(module_name) result[\'module\'] = module except ModuleNotFoundError as e: result[\'error\'] = f\\"Module \'{module_name}\' not found: {str(e)}\\" return result # Check for attribute existence for attr in attributes: result[\'attributes_exist\'][attr] = hasattr(module, attr) # Execute function if specified if func_to_execute: if hasattr(module, func_to_execute) and callable(getattr(module, func_to_execute)): func = getattr(module, func_to_execute) try: result[\'execution_result\'] = func(*func_args) except Exception as e: result[\'execution_result\'] = f\\"Function execution failed: {str(e)}\\" else: result[\'execution_result\'] = f\\"Function \'{func_to_execute}\' not found or is not callable.\\" return result"},{"question":"**Title: Complex Timezone Handling and DateTime Arithmetic** **Objective:** To assess the understanding of datetime manipulations, including arithmetic operations, timezone conversions, and formatting of date and time objects in Python using the `datetime` module. **Problem Statement:** You are required to implement a function `process_event_times(event_datetimes, target_timezone)` that accepts a list of event datetime strings (each in ISO 8601 format) and a target timezone offset. The function should convert each event datetime to the specified target timezone, compute the duration until each event from the current time (UTC), and return a list of formatted strings indicating the new datetime and the remaining time until the event. # Function Signature: ```python from typing import List def process_event_times(event_datetimes: List[str], target_timezone: int) -> List[str]: pass ``` # Parameters: - `event_datetimes` (List[str]): A list of strings, where each string represents an event datetime in ISO 8601 format (e.g., \\"2023-10-22T14:30:00Z\\"). - `target_timezone` (int): An integer representing the offset in hours from UTC for the target timezone (e.g., `-5` for EST). # Returns: - List[str]: A list of formatted strings. Each string should be in the format `\\"Event at: YYYY-MM-DD HH:MM:SS TZ Delay: DP days, HH:MM:SS\\"` where `DP` is the number of remaining days until the event, and `HH:MM:SS` is the remaining time. # Example: ```python event_datetimes = [\\"2023-10-22T14:30:00Z\\", \\"2023-10-23T09:00:00Z\\"] target_timezone = -5 # Expected output: # [ # \\"Event at: 2023-10-22 09:30:00 UTC-05:00 Delay: 2 days, 01:15:00\\", # \\"Event at: 2023-10-23 04:00:00 UTC-05:00 Delay: 3 days, 10:45:00\\" # ] ``` # Constraints: - The input event datetimes are all valid ISO 8601 strings. - The target timezone offset is a fixed integer offset without DST considerations. - The function should handle edge cases of negative offsets and events happening on the same day. # Additional Requirements: - Use the `datetime` module. - Test your function with various scenarios to ensure accuracy.","solution":"from typing import List from datetime import datetime, timedelta, timezone def process_event_times(event_datetimes: List[str], target_timezone: int) -> List[str]: results = [] now_utc = datetime.utcnow().replace(tzinfo=timezone.utc) target_tz = timezone(timedelta(hours=target_timezone)) for event_str in event_datetimes: # Parse the event datetime and convert to UTC event_dt_utc = datetime.fromisoformat(event_str.replace(\\"Z\\", \\"+00:00\\")) # Convert the event datetime to the target timezone event_dt_local = event_dt_utc.astimezone(target_tz) # Calculate the time difference from now UTC to the event datetime time_until_event = event_dt_local - now_utc # Format the remaining time until the event days_until_event = time_until_event.days seconds_until_event = time_until_event.seconds hours, remainder = divmod(seconds_until_event, 3600) minutes, seconds = divmod(remainder, 60) delay_str = f\\"Delay: {days_until_event} days, {hours:02}:{minutes:02}:{seconds:02}\\" event_str = event_dt_local.strftime(f\\"Event at: %Y-%m-%d %H:%M:%S %Z%z {delay_str}\\") results.append(event_str) return results"},{"question":"**Question:** You are working on optimizing and deploying a PyTorch-based model in a large-scale production environment. To ensure efficient tracking and debugging, you need to implement a custom operator profiler to log the performance of various PyTorch operations used in your model. Additionally, you want to log the API usage and attach some metadata to your saved TorchScript model. Your task is to write a script in PyTorch that: 1. Sets up an operator profiler that logs the name and input size of each PyTorch operation executed during the forward pass of a model, sampling 1% of the operations. 2. Logs the usage of basic PyTorch API calls. 3. Saves a TorchScript model with additional metadata file attached containing user information. # Detailed Instructions: 1. **Operator Profiler**: - Use the `torch::addGlobalCallback` to create and register a callback that logs the name and number of inputs of each operation. - Ensure your callback function is invoked for 1% of the operations. - Implement a method `init_profiler()` that initializes this callback. This method should be called at the start. 2. **API Usage Logging**: - Use `c10::SetAPIUsageHandler` to register a callback function that logs each API call once during the execution. - Implement a method `init_api_usage_logger()` that initializes this callback. This method should be called at the start. 3. **Attach Metadata to TorchScript Model**: - Train a simple neural network model (you can use any architecture). - Convert the trained model to TorchScript using `torch.jit.script`. - Save the TorchScript model using `torch.jit.save`, and attach additional metadata file containing user information. - Implement a method `save_model_with_metadata(model, file_path)` that saves the model and attaches the metadata. # Constraints: - Ensure the profiler and API logging mechanisms introduce minimal overhead. - Your model training should be basic to keep the focus on the profiling and logging mechanisms. - Make sure to attach at least one metadata file to the TorchScript model. # Expected Code Structure: ```python import torch import torch.nn as nn import torch.optim as optim def init_profiler(): # Setup the profiler with a 1% sampling rate that logs the name and input size of operations pass def init_api_usage_logger(): # Setup the API usage logger that logs when each API is used pass def save_model_with_metadata(model, file_path): # Save the TorchScript model and attach a metadata file pass class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() # Define a simple neural network architecture pass def forward(self, x): # Forward pass pass if __name__ == \\"__main__\\": # Initialize the profiler and API usage logger init_profiler() init_api_usage_logger() # Train a basic model model = SimpleModel() # Your training code here... # Convert the model to TorchScript and save it with metadata scripted_model = torch.jit.script(model) save_model_with_metadata(scripted_model, \'model_with_metadata.pt\') ``` You are required to implement the `init_profiler()`, `init_api_usage_logger()`, and `save_model_with_metadata()` methods as per the instructions above.","solution":"import torch import torch.nn as nn import torch.optim as optim import json def init_profiler(): def operator_callback(op_name, inputs, _): print(f\\"Operator: {op_name}, Input sizes: {[i.size() for i in inputs]}\\") def sampling_callback(op_name, inputs, _): from random import random if random() < 0.01: # 1% sampling rate operator_callback(op_name, inputs, _) torch.profiler.add_op_name_callback(sampling_callback) def init_api_usage_logger(): def api_usage_handler(api_name, _): print(f\\"API called: {api_name}\\") torch.jit.set_api_usage_handler(api_usage_handler) def save_model_with_metadata(model, file_path): user_info = {\\"username\\": \\"TestUser\\", \\"project\\": \\"Model Optimization\\"} metadata_json = json.dumps(user_info) torch.jit.save(model, file_path, _extra_files={\\"metadata.json\\": metadata_json}) class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 20) self.fc3 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = torch.sigmoid(self.fc3(x)) return x if __name__ == \\"__main__\\": # Initialize the profiler and API usage logger init_profiler() init_api_usage_logger() # Create a basic model model = SimpleModel() # Example dummy input x = torch.randn(1, 10) # Forward pass to trigger profiler and API logger with torch.no_grad(): model(x) # Convert the model to TorchScript and save it with metadata scripted_model = torch.jit.script(model) save_model_with_metadata(scripted_model, \'model_with_metadata.pt\')"},{"question":"Implementing Custom Autograd Functions in PyTorch Objective: The objective of this exercise is to assess your understanding of PyTorch\'s `autograd` module by challenging you to create custom autograd functions and use them to compute gradients of a provided function with respect to its inputs. Problem Statement: You are required to implement a custom autograd function for a mathematical operation (e.g., custom sigmoid function) and use it to compute derivatives. Subsequently, assess the gradients’ correctness using PyTorch\'s `gradcheck` utility. Tasks: 1. **Implement a Custom Sigmoid Function using `torch.autograd.Function`:** - Define a class `CustomSigmoid` that inherits from `torch.autograd.Function`. - Implement both `forward` and `backward` methods for the `CustomSigmoid` function. 2. **Compute the Gradient using the Custom Function:** - Use the implemented `CustomSigmoid` to compute the Sigmoid of an input tensor. - Compute the gradient of a toy objective function that uses this custom Sigmoid function. 3. **Validate Gradients using `gradcheck`:** - Verify the correctness of your custom function’s gradient implementation using `torch.autograd.gradcheck`. Expected Input and Output Formats: - Input: A tensor `x` (e.g., torch.tensor([1.0, 2.0, 3.0], requires_grad=True)). - Output: Gradient of the objective function with respect to `x`, validated by `gradcheck`. Constraints: - Use PyTorch\'s `torch.autograd.Function` for custom operation and gradient computation. - Implement both forward and backward methods properly. - Use `requires_grad=True` for input tensors. Performance Requirements: - The implementation should be efficient and make proper use of PyTorch’s autograd functionality. Example: ```python import torch from torch.autograd import Function from torch.autograd import gradcheck class CustomSigmoid(Function): @staticmethod def forward(ctx, input): sigmoid = 1 / (1 + torch.exp(-input)) ctx.save_for_backward(sigmoid) return sigmoid @staticmethod def backward(ctx, grad_output): sigmoid, = ctx.saved_tensors grad_input = grad_output * sigmoid * (1 - sigmoid) return grad_input def validate_custom_sigmoid(): x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True, dtype=torch.double) custom_sigmoid = CustomSigmoid.apply y = custom_sigmoid(x) objective = y.sum() objective.backward() return x.grad # Example Gradient Check x = torch.randn(3, dtype=torch.double, requires_grad=True) custom_sigmoid = CustomSigmoid.apply test = gradcheck(custom_sigmoid, (x,), eps=1e-6, atol=1e-4) print(\\"Gradient Check Passed: \\", test) ``` Notes: - Ensure that the `forward` and `backward` methods of the custom function capture the required logic accurately. - Use `gradcheck` with appropriate settings to validate the gradients.","solution":"import torch from torch.autograd import Function from torch.autograd import gradcheck class CustomSigmoid(Function): @staticmethod def forward(ctx, input): sigmoid = 1 / (1 + torch.exp(-input)) ctx.save_for_backward(sigmoid) return sigmoid @staticmethod def backward(ctx, grad_output): sigmoid, = ctx.saved_tensors grad_input = grad_output * sigmoid * (1 - sigmoid) return grad_input def validate_custom_sigmoid(): x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True, dtype=torch.double) custom_sigmoid = CustomSigmoid.apply y = custom_sigmoid(x) objective = y.sum() objective.backward() return x.grad # Example Gradient Check def check_gradient(): x = torch.randn(3, dtype=torch.double, requires_grad=True) custom_sigmoid = CustomSigmoid.apply return gradcheck(custom_sigmoid, (x,), eps=1e-6, atol=1e-4)"},{"question":"You are tasked with writing a function to process a collection of student grades and provide detailed analysis. Given a list of dictionaries, where each dictionary represents a student with their `id`, `name`, and a list of `grades`, you need to implement the following functionalities: 1. **Statistics Summary**: Calculate the average, minimum, and maximum grades for each student. Identify students with the highest and lowest average grades. 2. **Grade Distribution**: Compute the grade distribution across all students. For example, how many times each grade appears in the dataset. 3. **Top Performers**: Identify students whose average grade is above a given threshold. Implement the function `analyze_grades(students, threshold)` that takes: - `students`: A list of dictionaries, where each dictionary contains: - `id`: a unique identifier for the student (integer). - `name`: the student\'s name (string). - `grades`: a list of grades (integers between 0 and 100). - `threshold`: an integer representing the grade threshold. The function should return a dictionary with the following keys: - `statistics`: A dictionary containing: - Student ID as key and a tuple `(average, minimum, maximum)` of their grades as values. - `distribution`: A dictionary with each grade (from 0 to 100) as keys and the count of their occurrences across all students as values. - `top_performers`: A list of student names whose average grade is greater than or equal to the threshold. - `highest_average`: The name of the student with the highest average grade. - `lowest_average`: The name of the student with the lowest average grade. # Example ```python students = [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"grades\\": [78, 85, 90]}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"grades\\": [95, 92, 88]}, {\\"id\\": 3, \\"name\\": \\"Charlie\\", \\"grades\\": [65, 70, 75]}, ] threshold = 80 result = analyze_grades(students, threshold) # Expected Output: # { # \\"statistics\\": { # 1: (84.33, 78, 90), # 2: (91.66, 88, 95), # 3: (70, 65, 75) # }, # \\"distribution\\": { # 78: 1, 85: 1, 90: 1, 95: 1, 92: 1, 88: 1, 65: 1, 70: 1, 75: 1 # }, # \\"top_performers\\": [\\"Alice\\", \\"Bob\\"], # \\"highest_average\\": \\"Bob\\", # \\"lowest_average\\": \\"Charlie\\" # } ``` # Constraints 1. `1 <= len(students) <= 1000` 2. Each student dictionary will have exactly the keys `id`, `name`, and `grades`. 3. `1 <= len(grades) <= 100` and `0 <= grade <= 100` for each grade in `grades`. 4. `0 <= threshold <= 100` 5. Grades are integer values. # Note - Use appropriate built-in functions and methods to operate on lists, dictionaries, and other types mentioned. - Consider edge cases such as multiple students having the same highest or lowest average grades. Implement the function in Python 3.10 or later.","solution":"def analyze_grades(students, threshold): from collections import defaultdict import statistics # Initialize required variables student_stats = {} grade_distribution = defaultdict(int) top_performers = [] highest_average_grade = -1 highest_average_student = \\"\\" lowest_average_grade = 101 lowest_average_student = \\"\\" # Calculating statistics and grade distribution for student in students: student_id = student[\\"id\\"] student_name = student[\\"name\\"] grades = student[\\"grades\\"] avg_grade = round(statistics.mean(grades), 2) min_grade = min(grades) max_grade = max(grades) student_stats[student_id] = (avg_grade, min_grade, max_grade) # Update highest and lowest average grades if avg_grade > highest_average_grade: highest_average_grade = avg_grade highest_average_student = student_name if avg_grade < lowest_average_grade: lowest_average_grade = avg_grade lowest_average_student = student_name # Collect top performers if avg_grade >= threshold: top_performers.append(student_name) # Update the grade distribution for grade in grades: grade_distribution[grade] += 1 # Convert defaultdict to dict grade_distribution = dict(grade_distribution) return { \\"statistics\\": student_stats, \\"distribution\\": grade_distribution, \\"top_performers\\": top_performers, \\"highest_average\\": highest_average_student, \\"lowest_average\\": lowest_average_student }"},{"question":"# Color Palette Manipulation and Visualization in Seaborn You are working on a data visualization project and you need to create a variety of color palettes to enhance the aesthetics of your plots. The goal is to demonstrate your understanding of seaborn\'s color management capabilities. Task 1. **Creating a Custom Color Palette Function** Write a Python function `create_custom_palette(palette_name: str, n_colors: int, as_cmap: bool = False) -> Union[List[str], Colormap]` that: - Takes the name of a seaborn color palette (`palette_name`), the number of colors (`n_colors`), and a boolean flag `as_cmap`. - Returns a list of color codes if `as_cmap` is `False`. - Returns a colormap object if `as_cmap` is `True`. 2. **Plot Demonstration** Using the `create_custom_palette` function, create the following plots: - A **scatter plot** with 10 data points where each point’s color is determined by a color from the \\"Set2\\" palette with 10 colors. - A **heatmap** using the \\"Spectral\\" palette as a continuous colormap. Function Signature ```python def create_custom_palette(palette_name: str, n_colors: int, as_cmap: bool = False) -> Union[List[str], Colormap]: pass ``` Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt # Assuming create_custom_palette is implemented # Create Set2 palette with 10 colors set2_colors = create_custom_palette(\\"Set2\\", 10) # Scatter plot plt.figure(figsize=(8, 6)) sns.scatterplot(x=range(10), y=range(10), hue=range(10), palette=set2_colors, legend=False) plt.title(\\"Scatter Plot with Set2 Palette\\") plt.show() # Create Spectral continuous colormap spectral_cmap = create_custom_palette(\\"Spectral\\", 256, as_cmap=True) # Heatmap data = np.random.rand(10, 12) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=spectral_cmap) plt.title(\\"Heatmap with Spectral Colormap\\") plt.show() ``` Constraints - You must handle invalid palette names gracefully by raising an appropriate exception. - The function should leverage seaborn functions correctly to create palettes and colormaps. Note - Make sure to import necessary libraries, including seaborn and matplotlib, in your solution. - Provide appropriate axis labels and titles in your plots for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np from typing import List, Union from matplotlib.colors import Colormap def create_custom_palette(palette_name: str, n_colors: int, as_cmap: bool = False) -> Union[List[str], Colormap]: Creates a custom color palette using seaborn and returns either a list of color codes or a colormap. Parameters: palette_name (str): The name of the seaborn color palette. n_colors (int): The number of colors in the palette. as_cmap (bool): Flag to return a colormap instead of a list of color codes. Returns: Union[List[str], Colormap]: The color palette as a list of color codes or a colormap. try: if as_cmap: return sns.color_palette(palette_name, n_colors=n_colors, as_cmap=True) else: return sns.color_palette(palette_name, n_colors=n_colors).as_hex() except ValueError as e: raise ValueError(f\\"Invalid palette name \'{palette_name}\' or number of colors \'{n_colors}\'.\\") from e # Example demonstration # Scatter plot set2_colors = create_custom_palette(\\"Set2\\", 10) plt.figure(figsize=(8, 6)) sns.scatterplot(x=range(10), y=range(10), hue=range(10), palette=set2_colors, legend=False) plt.title(\\"Scatter Plot with Set2 Palette\\") plt.xlabel(\\"X-axis\\") plt.ylabel(\\"Y-axis\\") plt.show() # Heatmap spectral_cmap = create_custom_palette(\\"Spectral\\", 256, as_cmap=True) data = np.random.rand(10, 12) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=spectral_cmap) plt.title(\\"Heatmap with Spectral Colormap\\") plt.xlabel(\\"Columns\\") plt.ylabel(\\"Rows\\") plt.show()"},{"question":"# Functional Programming with Itertools, Functools, and Operator **Objective:** Implement a function that processes a mixed list of integers and strings using functional programming techniques provided by the `itertools`, `functools`, and `operator` modules. The function should: 1. **Separate the data** into integers and strings. 2. For the list of integers: - Filter out any negative numbers. - Compute the factorial of each number using a higher-order function. - Sum the factorials. 3. For the list of strings: - Filter out any empty strings. - Concatenate the remaining strings into a single string. 4. Return a tuple containing the sum of factorials and the concatenated string. **Function Signature:** ```python def process_data(data: list) -> tuple: pass ``` **Input:** - `data`: A list containing both integers and strings (e.g., `[1, \'hello\', 3, \'world\', -2, \'!\', 0]`) **Output:** - A tuple of the form `(sum_of_factorials, concatenated_string)`, where: - `sum_of_factorials` is the sum of the factorials of non-negative integers. - `concatenated_string` is the result of concatenating all non-empty strings. **Constraints:** - The input list will contain at least one integer and one string. - The integers will be in the range of -1000 to 1000. - The total length of concatenated strings will not exceed 1000 characters. **Example:** ```python data = [1, \'hello\', 3, \'world\', -2, \'!\', 0] # Integers: [1, 3, -2, 0] # Filtered non-negative: [1, 3, 0] # Factorials: [1! = 1, 3! = 6, 0! = 1] # Sum of factorials: 1 + 6 + 1 = 8 # Strings: [\'hello\', \'world\', \'!\'] # Concatenated string: \'helloworld!\' # Output: (8, \'helloworld!\') assert process_data(data) == (8, \'helloworld!\') ``` **Performance Requirements:** - The solution should efficiently handle lists up to a length of 10,000. - Avoid using loops where functional constructs can be applied. **Hint:** You may find the following functions useful: - `itertools.filterfalse`, `itertools.chain` (from `itertools`) - `operator.add` (from `operator`) - `functools.reduce`, `functools.partial` (from `functools`) - `math.factorial` (for computing factorials)","solution":"import itertools import functools import operator import math def process_data(data: list) -> tuple: # Separate integers and strings integers = list(itertools.filterfalse(lambda x: isinstance(x, str), data)) strings = list(itertools.filterfalse(lambda x: isinstance(x, int), data)) # Process integers: filter non-negative, compute factorial, and sum them non_negative_integers = filter(lambda x: x >= 0, integers) factorials = map(math.factorial, non_negative_integers) sum_of_factorials = functools.reduce(operator.add, factorials, 0) # Process strings: filter out empty strings, and concatenate the remaining non_empty_strings = filter(bool, strings) concatenated_string = functools.reduce(operator.add, non_empty_strings, \\"\\") return (sum_of_factorials, concatenated_string)"},{"question":"Objective: Demonstrate the understanding of setting styles and customizing parameters in Seaborn to create visually distinctive plots. Task: Write a Python function `custom_style_plot` that: 1. Accepts a style name, a parameter dictionary for customization, and a dataset to plot. 2. Creates a bar plot using the specified style and custom parameters. 3. Returns the Axes object of the generated plot. Input: - `style_name` (str): The name of the Seaborn style to apply. The possible style names include \\"white\\", \\"dark\\", \\"whitegrid\\", \\"darkgrid\\", and \\"ticks\\". - `custom_params` (dict): A dictionary containing Seaborn\'s aesthetic parameters to customize, such as `grid.color`, `grid.linestyle`, etc. - `data` (dict): A dictionary with keys \\"x\\" and \\"y\\" containing lists for x and y axis values, respectively. Output: - An Axes object of the created plot. Constraints: - The `style_name` must be a valid Seaborn style. - The `custom_params` should only include aesthetic parameters supported by Seaborn. - `data` must contain two keys \\"x\\" and \\"y\\" with lists of equal lengths. Example usage: ```python def custom_style_plot(style_name, custom_params, data): pass # Implement the function based on the given specifications # Example function call style = \\"darkgrid\\" params = {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"} plot_data = {\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [4, 7, 5]} ax = custom_style_plot(style, params, plot_data) ``` # Notes: - Ensure you set the style before plotting the data. - You can refer to Seaborn\'s documentation for valid aesthetic parameters. - The function should not display the plot; it should return the Axes object. Your implementation should focus on correct usage of: - `seaborn.set_style` - Bar plot creation using Seaborn - Applying customized aesthetic parameters","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_style_plot(style_name, custom_params, data): Creates a bar plot using the specified Seaborn style and custom parameters. Parameters: - style_name (str): The Seaborn style to apply (\\"white\\", \\"dark\\", \\"whitegrid\\", \\"darkgrid\\", \\"ticks\\"). - custom_params (dict): A dictionary of aesthetic parameters to customize. - data (dict): A dictionary with keys \\"x\\" and \\"y\\" containing lists for x and y axis values, respectively. Returns: - ax (Axes): The Axes object of the generated plot. # Ensure the eligibility of style_name and custom_params assert style_name in [\\"white\\", \\"dark\\", \\"whitegrid\\", \\"darkgrid\\", \\"ticks\\"], \\"Invalid style_name\\" assert \\"x\\" in data and \\"y\\" in data and len(data[\\"x\\"]) == len(data[\\"y\\"]), \\"Invalid data format\\" # Set the seaborn style sns.set_style(style_name, custom_params) # Create the bar plot fig, ax = plt.subplots() sns.barplot(x=data[\\"x\\"], y=data[\\"y\\"], ax=ax) return ax"},{"question":"# Enum Manipulation and Use in Python Problem Statement You are tasked with creating and manipulating Enums for a hypothetical file permissions system. You need to define different access levels and then provide utility functions to check, compare, and manipulate these permissions. The goal is to develop your own handling logics for Enum and Flag. Task 1. **Define Enum and IntEnum Classes:** - Define an `AccessLevel` `IntEnum` with values: - `READ` = 1 - `WRITE` = 2 - `EXECUTE` = 4 - Define a `FileAttribute` `Enum` with values: - `HIDDEN` = 0 - `READ_ONLY` = 1 - `SYSTEM` = 2 2. **Define a Flag Class:** - Define a `Permission` class that inherits from `Flag`. It should have the following values: - `NONE` (no permissions) = 0 - `EXECUTE` = 1 - `WRITE` = 2 - `READ` = 4 - `ALL` (all permissions) = 7 3. **Utility Functions:** - **check_permission**: A function to check if a particular permission exists within another permission value. ```python def check_permission(permission_value: Permission, check_for: Permission) -> bool: Checks if the `check_for` permission is contained within `permission_value`. Args: permission_value (Permission): The permission to check within. check_for (Permission): The permission to check for. Returns: bool: True if `check_for` is contained in `permission_value`, else False. ``` - **add_permission**: A function to add a permission to a particular permission value. ```python def add_permission(permission_value: Permission, add_permission: Permission) -> Permission: Adds a permission to `permission_value`. Args: permission_value (Permission): The current permission value. add_permission (Permission): The permission to add. Returns: Permission: The new combined permission value. ``` - **remove_permission**: A function to remove a permission from a particular permission value. ```python def remove_permission(permission_value: Permission, remove_permission: Permission) -> Permission: Removes a permission from `permission_value`. Args: permission_value (Permission): The current permission value. remove_permission (Permission): The permission to remove. Returns: Permission: The new permission value with `remove_permission` removed. ``` Constraints - You should use the classes and utilities provided by the `enum` module. - Utilize bitwise operations properly where applicable. - Ensure your utility functions handle cases where no permissions are set or where removing a non-existing permission does not affect the existing permissions. Example Usage ```python # Define the Enums and Flags from enum import IntEnum, Enum, Flag, auto class AccessLevel(IntEnum): READ = 1 WRITE = 2 EXECUTE = 4 class FileAttribute(Enum): HIDDEN = 0 READ_ONLY = 1 SYSTEM = 2 class Permission(Flag): NONE = 0 EXECUTE = 1 WRITE = 2 READ = 4 ALL = 7 # Utilize the utility functions permission = Permission.READ | Permission.WRITE new_permission = add_permission(permission, Permission.EXECUTE) print(check_permission(new_permission, Permission.READ)) # Output: True print(new_permission) # Output: Permission.ALL new_permission = remove_permission(new_permission, Permission.WRITE) print(check_permission(new_permission, Permission.WRITE)) # Output: False print(new_permission) # Output: Permission.READ | Permission.EXECUTE ``` **Note: Provide your solutions within the functions/classes defined above. Ensure to use appropriate enum functionalities as described in the documentation.**","solution":"from enum import IntEnum, Enum, Flag class AccessLevel(IntEnum): READ = 1 WRITE = 2 EXECUTE = 4 class FileAttribute(Enum): HIDDEN = 0 READ_ONLY = 1 SYSTEM = 2 class Permission(Flag): NONE = 0 EXECUTE = 1 WRITE = 2 READ = 4 ALL = 7 def check_permission(permission_value: Permission, check_for: Permission) -> bool: Checks if the `check_for` permission is contained within `permission_value`. Args: permission_value (Permission): The permission to check within. check_for (Permission): The permission to check for. Returns: bool: True if `check_for` is contained in `permission_value`, else False. return permission_value & check_for == check_for def add_permission(permission_value: Permission, add_permission: Permission) -> Permission: Adds a permission to `permission_value`. Args: permission_value (Permission): The current permission value. add_permission (Permission): The permission to add. Returns: Permission: The new combined permission value. return permission_value | add_permission def remove_permission(permission_value: Permission, remove_permission: Permission) -> Permission: Removes a permission from `permission_value`. Args: permission_value (Permission): The current permission value. remove_permission (Permission): The permission to remove. Returns: Permission: The new permission value with `remove_permission` removed. return permission_value & ~remove_permission"},{"question":"# PyTorch Futures - Coding Assessment In this task, you will demonstrate your understanding of PyTorch\'s `torch.futures` module, specifically working with the `Future` class and its utility functions. Problem Statement You are given a list of asynchronous functions that return `Future` objects. Your task is to create a function `process_futures` that: 1. Takes a list of these asynchronous functions. 2. Executes them concurrently. 3. Collects their results using the `torch.futures.collect_all` function. 4. Returns the combined results once all the futures are completed. Function Signature ```python def process_futures(async_funcs: List[Callable[[], torch.futures.Future]]) -> List[Any]: pass ``` Input - `async_funcs`: A list of asynchronous functions `List[Callable[[], torch.futures.Future]]`. Each function when called, returns a `Future` object. Output - A list of results corresponding to each function execution. Constraints - Each function in `async_funcs` should be called exactly once. - The solution should make use of the `torch.futures.collect_all` function to handle the concurrent execution. Example ```python import torch import torch.futures # Simulating a simple asynchronous function using Future def async_func_1(): future = torch.futures.Future() future.set_result(1) return future def async_func_2(): future = torch.futures.Future() future.set_result(2) return future def async_func_3(): future = torch.futures.Future() future.set_result(3) return future async_funcs = [async_func_1, async_func_2, async_func_3] result = process_futures(async_funcs) print(result) # Output should be: [1, 2, 3] ``` Note - Ensure that you handle any potential exceptions that might occur during the execution of the future objects. - You may assume that all `Future` objects\' results can be directly gathered without further processing. Hints - You might use `torch.futures.collect_all` to gather all the futures into one. - Use `await` keyword or `.wait()` method to handle the completion of futures inside the provided functions.","solution":"import torch import torch.futures from typing import List, Callable, Any def process_futures(async_funcs: List[Callable[[], torch.futures.Future]]) -> List[Any]: This function takes a list of asynchronous functions, executes them concurrently, collects their results using \'torch.futures.collect_all\', and returns the combined results. Parameters: async_funcs (List[Callable[[], torch.futures.Future]]): A list of asynchronous functions. Returns: List[Any]: A list of results from the futures. # Execute all asynchronous functions to get their futures futures = [func() for func in async_funcs] # Use torch.futures.collect_all to gather all futures together all_futures = torch.futures.collect_all(futures) # Wait for all futures to complete and get their results results = all_futures.wait() # Extract the result from each completed future return [future.value() for future in results]"},{"question":"Implementing a Custom Decision Tree Classifier **Objective:** Design a custom decision tree classifier from scratch that incorporates basic functionalities and optimization techniques similar to those in scikit-learn\'s `DecisionTreeClassifier` class. **Problem Statement:** You are tasked with implementing a simplified version of a `DecisionTreeClassifier`. Your custom implementation should support functionalities such as fitting on a dataset, making predictions, handling missing values, and minimal cost-complexity pruning. **Specifications:** 1. **Inputs:** - `X_train`: A 2D list or numpy array of shape (n_samples, n_features) containing the training samples. - `y_train`: A 1D list or numpy array of shape (n_samples,) containing the class labels for the training samples. - `X_test`: A 2D list or numpy array of shape (m_samples, n_features) containing the test samples for prediction. 2. **Constraints and Requirements:** - **Classification Criterion:** Use Gini impurity as the splitting criterion. - **Handling Missing Values:** Use a simple strategy where missing values are handled by assigning default class based on the majority class in the split. Implement a more refined strategy using tie-break rules as mentioned in the documentation. - **Parameter Tuning:** Implement hyperparameters like `max_depth` and `min_samples_split` to control tree growth and prevent overfitting. - **Pruning:** Implement minimal cost-complexity pruning using the parameter `ccp_alpha`. 3. **Outputs:** - `fit`: A method to train the decision tree on the provided training dataset. - `predict`: A method to predict class labels for the provided test dataset. 4. **Performance Requirements:** - Ensure your implementation can handle at least 10,000 samples efficiently. - Optimize your implementation for training time and prediction time. **Implementation:** Implement the following class: ```python import numpy as np class CustomDecisionTreeClassifier: def __init__(self, max_depth=None, min_samples_split=2, ccp_alpha=0.0): self.max_depth = max_depth self.min_samples_split = min_samples_split self.ccp_alpha = ccp_alpha self.tree_ = None def fit(self, X, y): Build the decision tree classifier from the training set (X, y). # Your code here to build the tree def predict(self, X): Predict class for X. # Your code here to predict based on the built tree # Additional helper methods as needed ``` **Testing:** 1. **Data:** Use the Iris dataset or a similar dataset to test your implementation. 2. **Visualization:** Include a method to visualize and print the structure of the tree in a readable format (optional but recommended). **Example:** ```python # Testing the CustomDecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load Iris dataset data = load_iris() X, y = data.data, data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create and train the custom decision tree classifier custom_tree = CustomDecisionTreeClassifier(max_depth=3, min_samples_split=4, ccp_alpha=0.01) custom_tree.fit(X_train, y_train) # Make predictions predictions = custom_tree.predict(X_test) # Evaluate accuracy (Optional) from sklearn.metrics import accuracy_score print(f\'Accuracy: {accuracy_score(y_test, predictions)}\') ``` **Notes:** - You can use numpy for numerical operations. - Ensure your code is well-optimized and properly documented for clarity.","solution":"import numpy as np from collections import Counter class Node: def __init__(self, gini, num_samples, num_samples_per_class, predicted_class): self.gini = gini self.num_samples = num_samples self.num_samples_per_class = num_samples_per_class self.predicted_class = predicted_class self.feature_index = 0 self.threshold = 0 self.left = None self.right = None class CustomDecisionTreeClassifier: def __init__(self, max_depth=None, min_samples_split=2, ccp_alpha=0.0): self.max_depth = max_depth self.min_samples_split = min_samples_split self.ccp_alpha = ccp_alpha self.tree_ = None def fit(self, X, y): self.n_classes_ = len(set(y)) self.n_features_ = X.shape[1] self.tree_ = self._grow_tree(X, y) def predict(self, X): return [self._predict(inputs) for inputs in X] def _best_split(self, X, y): m, n = X.shape if m <= 1: return None, None num_parent = [np.sum(y == c) for c in range(self.n_classes_)] best_gini = 1.0 - sum((num / m) ** 2 for num in num_parent) best_idx, best_thr = None, None for idx in range(n): thresholds, classes = zip(*sorted(zip(X[:, idx], y))) num_left = [0] * self.n_classes_ num_right = num_parent.copy() for i in range(1, m): c = classes[i - 1] num_left[c] += 1 num_right[c] -= 1 gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(self.n_classes_)) gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)) gini = (i * gini_left + (m - i) * gini_right) / m if thresholds[i] == thresholds[i - 1]: continue if gini < best_gini: best_gini = gini best_idx = idx best_thr = (thresholds[i] + thresholds[i - 1]) / 2 return best_idx, best_thr def _grow_tree(self, X, y, depth=0): num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)] predicted_class = np.argmax(num_samples_per_class) node = Node( gini=self._gini(y), num_samples=len(y), num_samples_per_class=num_samples_per_class, predicted_class=predicted_class, ) if depth < self.max_depth and node.num_samples >= self.min_samples_split: idx, thr = self._best_split(X, y) if idx is not None: indices_left = X[:, idx] < thr X_left, y_left = X[indices_left], y[indices_left] X_right, y_right = X[~indices_left], y[~indices_left] node.feature_index = idx node.threshold = thr node.left = self._grow_tree(X_left, y_left, depth + 1) node.right = self._grow_tree(X_right, y_right, depth + 1) return node def _predict(self, inputs): node = self.tree_ while node.left: if inputs[node.feature_index] < node.threshold: node = node.left else: node = node.right return node.predicted_class def _gini(self, y): m = len(y) return 1.0 - sum((np.sum(y == i) / m) ** 2 for i in range(self.n_classes_))"},{"question":"# PyTorch Coding Assessment Question Objective: Implement a PyTorch model and demonstrate your understanding of the `torch.export` functionality by correctly capturing the computation graph for static and dynamic values and handling static vs. dynamic control flow. Task: 1. **Implement a PyTorch Module (Dynamic Control Flow)**: - Create a module `DynamicControlFlowModule` that takes two input tensors `x` and `y`. - If the sum of `x` exceeds a threshold value (say, 10), return the sine of `x` added to `y`. Otherwise, return the cosine of `x` multiplied by `y`. 2. **Implement a PyTorch Module (Static Control Flow)**: - Create a module `StaticControlFlowModule` that takes an input tensor `z` and a static integer value `threshold`. - If the given `threshold` is greater than 5, return the square of `z`. Otherwise, return the cube of `z`. 3. **Export both models using `torch.export`**: - `DynamicControlFlowModule` should be exported with example dynamic inputs of your choice. - `StaticControlFlowModule` should be exported with a static threshold value (e.g., 7). 4. **Inspect the exported models**: - Print and analyze the generated code from the exported models. Explain how the static and dynamic control flows were handled and embedded in the exported computation graphs. Expected Input and Output: 1. For the function `DynamicControlFlowModule.forward(x, y)`: - Inputs: - `x` : Tensor of arbitrary shape - `y` : Tensor of the same shape as `x` - Outputs: - A tensor computed based on the control flow described 2. For the function `StaticControlFlowModule.forward(z)`: - Input: - `z` : Tensor of arbitrary shape - Outputs: - A tensor computed based on the static control flow described Constraints: - Use non-strict tracing for exporting models. - Ensure the functions adhere to the specified behavior and types. Performance Requirements: - The operations should be computationally efficient, particularly in handling the control flows and exporting the models. Example Code: ```python import torch class DynamicControlFlowModule(torch.nn.Module): def forward(self, x, y): return torch.cond( pred=x.sum() > 10, true_fn=lambda: x.sin() + y, false_fn=lambda: x.cos() * y ) class StaticControlFlowModule(torch.nn.Module): def __init__(self, threshold): super(StaticControlFlowModule, self).__init__() self.threshold = threshold def forward(self, z): if self.threshold > 5: return z ** 2 else: return z ** 3 # Example Usage: dynamic_model = DynamicControlFlowModule() static_model = StaticControlFlowModule(threshold=7) # Using example inputs for export example_input_x = torch.randn(5, 5) example_input_y = torch.randn(5, 5) dynamic_export = torch.export.export(dynamic_model, (example_input_x, example_input_y)) example_input_z = torch.randn(4, 4) static_export = torch.export.export(static_model, (example_input_z,)) # Print exported models print(dynamic_export.graph_module.code) print(static_export.graph_module.code) ``` Explanation: - Provide a brief explanation of how the static and dynamic control flows were captured in the exported models.","solution":"import torch import torch.nn as nn class DynamicControlFlowModule(nn.Module): def forward(self, x, y): if torch.sum(x) > 10: return torch.sin(x) + y else: return torch.cos(x) * y class StaticControlFlowModule(nn.Module): def __init__(self, threshold): super().__init__() self.threshold = threshold def forward(self, z): if self.threshold > 5: return z ** 2 else: return z ** 3 # Example Usage: dynamic_model = DynamicControlFlowModule() static_model = StaticControlFlowModule(threshold=7) # Using example inputs for export example_input_x = torch.randn(5, 5) example_input_y = torch.randn(5, 5) dynamic_export = torch.jit.trace(dynamic_model, (example_input_x, example_input_y)) example_input_z = torch.randn(4, 4) static_export = torch.jit.trace(static_model, example_input_z) # Print exported models print(dynamic_export.code) print(static_export.code)"},{"question":"# Question: Custom Chat Server Implementation **Objective:** In this exercise, you will implement a basic chat server using the `socketserver` module. The server should support both synchronous and asynchronous (threaded) communication. You will demonstrate your understanding by creating request handler classes and a server class that uses threading to allow multiple clients to connect and send messages concurrently. **Requirements:** 1. Create a synchronous chat server using `socketserver.TCPServer`. 2. Upgrade the server to handle multiple clients concurrently using `socketserver.ThreadingMixIn`. 3. Implement a request handler class that manages receiving and broadcasting messages to all connected clients. **Instructions:** 1. **Step 1: Synchronous Chat Server** - Implement a `ChatRequestHandler` class that inherits from `socketserver.BaseRequestHandler`. - In the `handle()` method of `ChatRequestHandler`, handle incoming messages by reading data from the client socket and broadcasting the message to all connected clients. - Create a `ChatTCPServer` class that inherits from `socketserver.TCPServer`. - The server should run and accept connections synchronously. 2. **Step 2: Asynchronous Chat Server** - Extend the `ChatTCPServer` class to use threading by mixing `socketserver.ThreadingMixIn`. - Update the server to handle multiple clients concurrently. 3. **Broadcast Mechanism:** - Implement a broadcast mechanism within the request handler to send messages to all connected clients. **Example Usage:** - Implement the server-side script `ChatServer.py`. - Run the server and connect multiple clients using a client-side script `ChatClient.py`. - Clients should be able to send and receive messages from each other via the server. **Expected Input/Output:** - Clients send text messages to the server. - The server broadcasts received messages to all connected clients. **Constraints:** - Use Python\'s `socketserver` module. - Implement both synchronous and asynchronous versions of the server. - Ensure proper handling of client disconnections. **Performance Requirements:** - The server should handle at least 10 concurrent client connections. **Sample Code:** **ChatServer.py** ```python import socketserver import threading connected_clients = [] class ChatRequestHandler(socketserver.BaseRequestHandler): def handle(self): global connected_clients self.username = self.request.recv(1024).strip().decode(\'utf-8\') connected_clients.append(self.request) print(f\\"{self.username} joined the chat\\") try: while True: message = self.request.recv(1024).strip().decode(\'utf-8\') if message: self.broadcast_message(f\\"{self.username}: {message}\\") except ConnectionResetError: self.request.close() connected_clients.remove(self.request) print(f\\"{self.username} left the chat\\") def broadcast_message(self, message): global connected_clients for client in connected_clients: if client != self.request: try: client.sendall(message.encode(\'utf-8\')) except: connected_clients.remove(client) class ChatTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ChatTCPServer((HOST, PORT), ChatRequestHandler) as server: print(\\"Chat server started on port 9999\\") server.serve_forever() ``` **ChatClient.py** ```python import socket import threading def receive_messages(sock): while True: message = sock.recv(1024).strip().decode(\'utf-8\') if message: print(message) def main(): HOST, PORT = \\"localhost\\", 9999 username = input(\\"Enter your username: \\") with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) sock.sendall(username.encode(\'utf-8\')) threading.Thread(target=receive_messages, args=(sock,)).start() while True: message = input(\\"\\") sock.sendall(message.encode(\'utf-8\')) if __name__ == \\"__main__\\": main() ``` **Note:** Ensure you test the implementation with multiple clients to verify that messages are correctly broadcasted to all connected clients.","solution":"import socketserver import threading connected_clients = [] class ChatRequestHandler(socketserver.BaseRequestHandler): def handle(self): global connected_clients self.username = self.request.recv(1024).strip().decode(\'utf-8\') connected_clients.append(self.request) print(f\\"{self.username} joined the chat\\") try: while True: message = self.request.recv(1024).strip().decode(\'utf-8\') if message: print(f\\"{self.username}: {message}\\") self.broadcast_message(f\\"{self.username}: {message}\\") except ConnectionResetError: self.request.close() connected_clients.remove(self.request) print(f\\"{self.username} left the chat\\") def broadcast_message(self, message): global connected_clients for client in connected_clients: if client != self.request: try: client.sendall(message.encode(\'utf-8\')) except: connected_clients.remove(client) class ChatTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ChatTCPServer((HOST, PORT), ChatRequestHandler) as server: print(\\"Chat server started on port 9999\\") server.serve_forever()"},{"question":"**Objective:** Design and implement a function that generates multiple synthetic datasets using specified scikit-learn dataset generator functions, then plots and saves these datasets as images.\\" **Task:** Write a Python function `generate_and_plot_datasets` that takes two inputs: 1. A list of dataset generator specifications, each specifying: - The name of the generator function (string, one of: \'blobs\', \'classification\', \'quantiles\', \'circles\', \'moons\'). - A dictionary of parameters to pass to the generator function (e.g., `{\'centers\': 3, \'cluster_std\': 0.5, \'random_state\': 0}` for `make_blobs`). 2. The directory path (string) where to save the plots. Your function should: 1. Use the provided specifications to generate datasets using the appropriate scikit-learn dataset generator functions. 2. Plot each generated dataset using `matplotlib` and save each plot as an image file in the specified directory. The filename should be the name of the generator function followed by a number (e.g., `blobs_1.png`). **Function Signature:** ```python def generate_and_plot_datasets(specifications: List[Dict[str, Union[str, Dict]]], save_dir: str) -> None: pass ``` **Requirements:** 1. You must use the `make_blobs`, `make_classification`, `make_gaussian_quantiles`, `make_circles`, and `make_moons` functions from `sklearn.datasets`. 2. Validate the input parameters to ensure they match the expected format and contain valid parameters for the corresponding generator functions. Raise appropriate errors if validation fails. 3. Ensure the directory for saving images exists, and if not, create it. 4. Each plot should clearly display the different classes or outputs using different colors. 5. You should add appropriate titles to the plots indicating the type of dataset. **Constraints:** - Ensure to handle edge cases, such as empty list of specifications or missing parameters in the specification dictionaries. - Optimize for readability and maintainability of the code. **Example:** ```python specifications = [ { \\"name\\": \\"blobs\\", \\"params\\": {\\"centers\\": 3, \\"cluster_std\\": 0.5, \\"random_state\\": 0} }, { \\"name\\": \\"classification\\", \\"params\\": {\\"n_informative\\": 2, \\"n_clusters_per_class\\": 1, \\"n_classes\\": 3, \\"n_features\\": 2, \\"n_redundant\\": 0, \\"random_state\\": 1} } ] save_dir = \\"./saved_plots\\" generate_and_plot_datasets(specifications, save_dir) ``` This should generate two plots saved as `blobs_1.png` and `classification_1.png` in the specified directory. **Note:** - You can assume the `specifications` list will not contain more than 10 specifications, and each will have valid values for the parameters. - Use the specified generator functions as they appear in `sklearn.datasets`.","solution":"import os import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons def generate_and_plot_datasets(specifications, save_dir): Generates datasets based on specifications and saves the plots to the specified directory. Parameters: specifications (List[Dict[str, Union[str, Dict]]]): a list of dataset generator specifications. save_dir (str): the directory path where to save the plots. # Dictionary mapping generator names to sklearn functions generator_map = { \'blobs\': make_blobs, \'classification\': make_classification, \'quantiles\': make_gaussian_quantiles, \'circles\': make_circles, \'moons\': make_moons } # Ensure the save directory exists os.makedirs(save_dir, exist_ok=True) # Iterate over the datasets specifications for index, spec in enumerate(specifications, start=1): name = spec.get(\\"name\\") params = spec.get(\\"params\\", {}) if name not in generator_map: raise ValueError(f\\"Unsupported generator function name: {name}\\") try: # Generate the dataset X, y = generator_map[name](**params) # Create the plot plt.figure() plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\') plt.title(f\\"{name.capitalize()} Dataset\\") # Save the plot file_path = os.path.join(save_dir, f\\"{name}_{index}.png\\") plt.savefig(file_path) plt.close() except TypeError as e: raise ValueError(f\\"Invalid parameters for {name}: {params}nOriginal error: {e}\\")"},{"question":"Your task is to implement a class `QuopriCodec` that replicates the functionality of the `quopri` module for encoding and decoding MIME quoted-printable data. This will involve methods for encoding and decoding both from binary files and bytes. **Class Definition:** ```python class QuopriCodec: def decode_file(self, input_file: str, output_file: str, header: bool=False) -> None: pass def encode_file(self, input_file: str, output_file: str, quotetabs: bool, header: bool=False) -> None: pass def decodestring(self, s: bytes, header: bool=False) -> bytes: pass def encodestring(self, s: bytes, quotetabs: bool=False, header: bool=False) -> bytes: pass ``` # Requirements: 1. **decode_file(input_file: str, output_file: str, header: bool=False) -> None:** - Reads from a binary file specified by `input_file`. - Decodes the contents and writes the decoded binary data to a file specified by `output_file`. - If `header` is `True`, underscores (\'_\') should be decoded to spaces (\' \'). 2. **encode_file(input_file: str, output_file: str, quotetabs: bool, header: bool=False) -> None:** - Reads from a binary file specified by `input_file`. - Encodes the contents and writes the quoted-printable data to a file specified by `output_file`. - If `quotetabs` is `True`, embedded spaces and tabs must be encoded. - If `header` is `True`, spaces should be encoded as underscores (\'_\'). 3. **decodestring(s: bytes, header: bool=False) -> bytes:** - Accepts a `bytes` input and returns the corresponding decoded `bytes`. - If `header` is `True`, underscores (\'_\') should be decoded to spaces (\' \'). 4. **encodestring(s: bytes, quotetabs: bool=False, header: bool=False) -> bytes:** - Accepts a `bytes` input and returns the corresponding encoded `bytes`. - If `quotetabs` is `True`, embedded spaces and tabs must be encoded. - If `header` is `True`, spaces should be encoded as underscores (\'_\'). # Constraints: - You may assume that the input files exist and are readable/writable with proper permissions. - The `input_file` and `output_file` parameters are strings representing file paths. - The input bytes or file contents are valid byte sequences. - Performance requirements should handle file sizes up to 10MB efficiently. # Example: ```python codec = QuopriCodec() # Example for file encoding and decoding codec.encode_file(\\"input.bin\\", \\"encoded_output.bin\\", quotetabs=True, header=True) codec.decode_file(\\"encoded_output.bin\\", \\"decoded_output.bin\\", header=True) # Example for string encoding and decoding encoded_bytes = codec.encodestring(b\\"Sample text for MIME encoding.\\", quotetabs=True, header=True) decoded_bytes = codec.decodestring(encoded_bytes, header=True) print(decoded_bytes) # Output should be: b\'Sample text for MIME encoding.\' ``` # Notes: - Thoroughly test your implementation to ensure all functionalities are working as expected. - Focus on edge cases such as handling headers and quoted tabs correctly.","solution":"import quopri class QuopriCodec: def decode_file(self, input_file: str, output_file: str, header: bool=False) -> None: with open(input_file, \'rb\') as fin, open(output_file, \'wb\') as fout: if header: decoded = quopri.decodestring(fin.read().replace(b\'_\', b\' \')) else: decoded = quopri.decodestring(fin.read()) fout.write(decoded) def encode_file(self, input_file: str, output_file: str, quotetabs: bool, header: bool=False) -> None: with open(input_file, \'rb\') as fin, open(output_file, \'wb\') as fout: if header: encoded = quopri.encodestring(fin.read(), quotetabs).replace(b\' \', b\'_\') else: encoded = quopri.encodestring(fin.read(), quotetabs) fout.write(encoded) def decodestring(self, s: bytes, header: bool=False) -> bytes: if header: s = s.replace(b\'_\', b\' \') return quopri.decodestring(s) def encodestring(self, s: bytes, quotetabs: bool=False, header: bool=False) -> bytes: encoded = quopri.encodestring(s, quotetabs) if header: encoded = encoded.replace(b\' \', b\'_\') return encoded"},{"question":"**Problem Statement**: You are asked to implement a custom event scheduling system using Python\'s `sched` module. The task involves creating a scheduler that can handle multiple events, including their addition, cancellation, and execution based on given priorities and times. Your implementation should also include a custom function that utilizes this scheduler to manage a series of tasks. **Task**: 1. **Create a Function `custom_scheduler`**: - **Inputs**: - `events`: A list of events where each event is represented as a tuple `(time, delay, priority, action, args, kwargs)`. - `time` (float): The absolute time at which the event is scheduled. - `delay` (int): The delay in seconds before the event is executed. - `priority` (int): The priority of the event (lower numbers indicate higher priority). - `action` (callable): The function to be executed. - `args` (tuple): Positional arguments to pass to the function. - `kwargs` (dict): Keyword arguments to pass to the function. - `current_time` (float): The current time. This will be used as the starting reference. - **Outputs**: - Execute all given events based on their scheduled times and priorities, and print outputs generated by each action. - Return the final state of the scheduler\'s event queue after execution. **Constraints**: - The function should handle both absolute time (`time`) and delays (`delay`) for scheduling tasks. - Events with the same scheduled time should execute in their order of priority. - The function should efficiently manage the event queue and handle simultaneous event execution accurately based on priorities. **Example**: ```python import time import sched # Example actions def print_message(msg): print(msg) # Example usage of custom_scheduler events = [ (time.time() + 3, 0, 2, print_message, (\\"Event 1 after 3 seconds\\",), {}), (time.time() + 1, 0, 1, print_message, (\\"Event 2 after 1 second\\",), {}), (0, 4, 3, print_message, (\\"Event 3 after 4 seconds delay\\",), {}), (0, 2, 1, print_message, (\\"Event 4 after 2 seconds delay\\",), {}), ] def custom_scheduler(events, current_time): # Your implementation here # Call the custom_scheduler function custom_scheduler(events, time.time()) ``` Expected Output (example order based on provided times and delays): ``` Event 2 after 1 second Event 4 after 2 seconds delay Event 1 after 3 seconds Event 3 after 4 seconds delay ``` **Note**: The exact order of event prints may depend on the timing and system execution. Good luck!","solution":"import time import sched def custom_scheduler(events, current_time): # Initialize the scheduler scheduler = sched.scheduler(time.time, time.sleep) # Schedule all events for event in events: event_time, delay, priority, action, args, kwargs = event if event_time > 0: schedule_time = event_time else: schedule_time = current_time + delay scheduler.enterabs(schedule_time, priority, action, argument=args, kwargs=kwargs) # Run all scheduled events scheduler.run() # Return the final state of the scheduler queue return scheduler.queue"},{"question":"# Python Coding Assessment Objective The objective of this assessment is to assess your understanding of various time-related functions provided by Python\'s `time` module. Problem Statement You need to write a function called `process_time_operations` that accepts a list of time operations in the form of tuples. Each tuple contains an operation name and required parameters for that operation. The function should process these operations and return the results in a list. Operations The operations can be one of the following: 1. `\\"strftime\\"`: Format the current local time according to the specified format. - Parameters: (operation_name, format_string) - Example: `(\\"strftime\\", \\"%Y-%m-%d %H:%M:%S\\")` 2. `\\"time_to_struct_time\\"`: Convert the given time in seconds since the epoch to `struct_time` in UTC. - Parameters: (operation_name, seconds_since_epoch) - Example: `(\\"time_to_struct_time\\", 1633072800)` 3. `\\"struct_time_to_mktime\\"`: Convert the provided `struct_time` to seconds since the epoch. - Parameters: (operation_name, struct_time_tuple) where `struct_time_tuple` is a 9-element tuple - Example: `(\\"struct_time_to_mktime\\", (2021, 10, 1, 10, 0, 0, 4, 274, 0))` 4. `\\"sleep_and_measure\\"`: Sleep for the given amount of seconds and measure the precise actual sleep duration using `monotonic_ns()`. - Parameters: (operation_name, sleep_duration) - Example: `(\\"sleep_and_measure\\", 2.5)` Implementation The function signature should be: ```python import time def process_time_operations(operations): results = [] for operation in operations: op_name = operation[0] if op_name == \\"strftime\\": format_string = operation[1] result = time.strftime(format_string, time.localtime()) elif op_name == \\"time_to_struct_time\\": seconds_since_epoch = operation[1] result = time.gmtime(seconds_since_epoch) elif op_name == \\"struct_time_to_mktime\\": struct_time_tuple = operation[1] result = time.mktime(struct_time_tuple) elif op_name == \\"sleep_and_measure\\": sleep_duration = operation[1] start_ns = time.monotonic_ns() time.sleep(sleep_duration) end_ns = time.monotonic_ns() result = (end_ns - start_ns) / 1e9 # Convert nanoseconds to seconds else: result = None results.append(result) return results ``` Constraints - Use appropriate error handling to ensure robustness, e.g., handling invalid operation names or parameters. - You are allowed to use only the `time` module for time-related operations. Example ```python operations = [ (\\"strftime\\", \\"%Y-%m-%d %H:%M:%S\\"), (\\"time_to_struct_time\\", 1633072800), (\\"struct_time_to_mktime\\", (2021, 10, 1, 10, 0, 0, 4, 274, 0)), (\\"sleep_and_measure\\", 2.5) ] result = process_time_operations(operations) print(result) # Output: [\'2021-10-01 12:00:00\', time.struct_time(tm_year=2021, tm_mon=10, tm_mday=1, ...), 1633072800.0, approx 2.500xxx (depends on system precision)] ``` The above function demonstrates how to handle and return results of various time operations, ensuring a comprehensive assessment of understanding of the `time` module.","solution":"import time def process_time_operations(operations): results = [] for operation in operations: op_name = operation[0] if op_name == \\"strftime\\": format_string = operation[1] result = time.strftime(format_string, time.localtime()) elif op_name == \\"time_to_struct_time\\": seconds_since_epoch = operation[1] result = time.gmtime(seconds_since_epoch) elif op_name == \\"struct_time_to_mktime\\": struct_time_tuple = operation[1] result = time.mktime(struct_time_tuple) elif op_name == \\"sleep_and_measure\\": sleep_duration = operation[1] start_ns = time.monotonic_ns() time.sleep(sleep_duration) end_ns = time.monotonic_ns() result = (end_ns - start_ns) / 1e9 # Convert nanoseconds to seconds else: result = None results.append(result) return results"},{"question":"# PCA with scikit-learn Problem Statement You are given a dataset of grayscale images of handwritten digits (the **digits dataset**) from the sklearn datasets module. Each image is represented as an 8x8 matrix, resulting in a total of 64 features per sample. Your task is to: 1. Implement Principal Component Analysis (PCA) to project this dataset onto a 2-dimensional space. 2. Visualize the results by plotting the 2D projection of the data points, using different colors to differentiate between different digit classes. Requirements 1. Use `sklearn.decomposition.PCA` to perform the dimensionality reduction. 2. Center the data before applying PCA. 3. Plot the first two principal components, coloring the points by their target digit class. Input The input consists of loading the sklearn\'s digits dataset: ```python from sklearn.datasets import load_digits data = load_digits() X = data.data # 2D array of shape (n_samples, 64) y = data.target # 1D array of shape (n_samples,) ``` Output A scatter plot where each point represents a digit from the dataset projected onto the first two principal components. Points should be colored according to their target digit class. Constraints - Use only the libraries `numpy`, `matplotlib`, and `sklearn`. - The PCA should not have the `whiten` parameter enabled. - The plot should include a legend indicating which color corresponds to which digit. Example ```python # Example of the expected output plot structure from sklearn.decomposition import PCA import matplotlib.pyplot as plt # Load digits dataset from sklearn.datasets import load_digits data = load_digits() X = data.data y = data.target # Center the data X_centered = X - X.mean(axis=0) # Apply PCA to reduce to 2 dimensions pca = PCA(n_components=2) X_pca = pca.fit_transform(X_centered) # Plot the first two principal components plt.figure(figsize=(10, 7)) for i in range(10): plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=str(i)) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.legend() plt.title(\'PCA of Digits Dataset\') plt.show() ``` This question assesses the student\'s understanding of PCA, data preprocessing, and visualization techniques using scikit-learn and matplotlib.","solution":"from sklearn.datasets import load_digits from sklearn.decomposition import PCA import matplotlib.pyplot as plt def perform_pca_and_plot(): # Load digits dataset data = load_digits() X = data.data y = data.target # Center the data X_centered = X - X.mean(axis=0) # Apply PCA to reduce to 2 dimensions pca = PCA(n_components=2) X_pca = pca.fit_transform(X_centered) # Plot the first two principal components plt.figure(figsize=(10, 7)) for i in range(10): plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=str(i)) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.legend() plt.title(\'PCA of Digits Dataset\') plt.show()"},{"question":"# Question: Implement and Extend a ChainMap Class **Objective:** To assess the students\' understanding of the `ChainMap` class from the `collections` module, its usage, and their ability to extend its functionality. **Problem Statement:** You are required to implement a class `CustomChainMap` that extends the functionality of `collections.ChainMap`. Your `CustomChainMap` should: 1. Inherit from `collections.ChainMap`. 2. Add a method named `deep_update` which updates the deepest occurrence of each key found in the underlying mappings. # `CustomChainMap` Class Requirements: - **Method: `deep_update(self, other)`** - **Input**: Accepts a dictionary `other`. - **Output**: Updates the deepest occurrence of each key found in the underlying mappings with the corresponding value from the `other` dictionary. If a key doesn\'t exist, it should add it to the first mapping. # Implementation Details: Example: ```python from collections import ChainMap class CustomChainMap(ChainMap): def deep_update(self, other): for key, value in other.items(): for mapping in reversed(self.maps): if key in mapping: mapping[key] = value break else: self.maps[0][key] = value # Example Usage baseline = {\'music\': \'bach\', \'art\': \'rembrandt\'} adjustments = {\'art\': \'van gogh\', \'opera\': \'carmen\'} additional = {\'music\': \'beethoven\', \'science\': \'einstein\'} cm = CustomChainMap(adjustments, baseline) print(\\"Before deep update:\\") print(dict(cm)) # {\'music\': \'bach\', \'art\': \'van gogh\', \'opera\': \'carmen\'} cm.deep_update(additional) print(\\"After deep update:\\") print(dict(cm)) # {\'music\': \'beethoven\', \'art\': \'van gogh\', \'opera\': \'carmen\', \'science\': \'einstein\'} ``` # Constraints: - **Time Complexity:** Ensure that your `deep_update` method is optimized for performance. - **Edge Cases:** Be sure to handle cases where there are no keys from `other` found in any of the underlying mappings. # Submission: Submit your implementation of the `CustomChainMap` class. Your code will be tested against a variety of inputs to ensure correctness and performance.","solution":"from collections import ChainMap class CustomChainMap(ChainMap): def deep_update(self, other): for key, value in other.items(): for mapping in reversed(self.maps): if key in mapping: mapping[key] = value break else: self.maps[0][key] = value"},{"question":"Kernel Density Estimation with scikit-learn Objective: To assess your understanding of Kernel Density Estimation (KDE), including its implementation, parameter tuning, and applications using the scikit-learn package. Problem Statement: You are given a dataset of two-dimensional points that follow a bimodal distribution. Your task is to: 1. Implement a Kernel Density Estimation model using scikit-learn. 2. Tune the bandwidth parameter to observe its effect on the smoothness of the density estimation. 3. Visualize the density estimation results for different kernels. 4. Generate new samples based on the KDE model. Requirements: 1. **Data**: - You have access to a dataset `data.csv` which contains two columns `x` and `y`. 2. **Steps**: - Load the dataset. - Implement a KDE model with different kernel methods (`gaussian`, `tophat`, `epanechnikov`). - Tune and visualize the effect of different bandwidth values on the density estimation. - Generate and visualize new samples from the KDE model with the chosen kernel and bandwidth. 3. **Visualizations**: - Plot the original data points. - Plot the KDE results for different kernel methods and bandwidth values. - Plot the new generated samples. Input Format: - A CSV file `data.csv` with the following format: ``` x, y x1, y1 x2, y2 ... ``` Output Format: - Multiple plots showing the original data points, KDE results for various kernels and bandwidths, and the new generated samples. Constraints: - Use scikit-learn\'s `KernelDensity` module for KDE. - Use appropriate visualization libraries (e.g., matplotlib) for plotting. Performance: - Efficiently handle the data and operations to generate visualizations without significant lag. Example Code Snippet: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Load the dataset data = pd.read_csv(\'data.csv\') X = data[[\'x\', \'y\']].values # Define function to perform KDE and plot results def plot_kde(X, kernel, bandwidth): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) X_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] log_dens = kde.score_samples(X_plot) plt.fill(X_plot[:, 0], np.exp(log_dens), \'-\', label=\'kernel = {}\'.format(kernel)) plt.title(\'KDE with kernel = {}\'.format(kernel)) plt.show() # Example usage plot_kde(X, \'gaussian\', 0.2) # Generate new samples from KDE model new_samples = kde.sample(100) plt.scatter(new_samples[:, 0], new_samples[:, 1], label=\'Generated Samples\') plt.title(\'Generated Samples from KDE Model\') plt.show() ``` **Note**: The code snippet provided is an example to illustrate how you might approach solving the problem. You are expected to extend it to handle different kernels, bandwidths, and generate the required visualizations.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def load_data(filepath): Loads the dataset from a CSV file. data = pd.read_csv(filepath) return data[[\'x\', \'y\']].values def plot_kde(X, kernel, bandwidth, ax): Performs KDE and plots the results. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 x, y = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) xy_sample = np.vstack([x.ravel(), y.ravel()]).T z = np.exp(kde.score_samples(xy_sample)).reshape(x.shape) ax.contourf(x, y, z, cmap=\'Blues\') ax.scatter(X[:, 0], X[:, 1], s=5, color=\'red\') ax.set_title(f\\"Kernel: {kernel}, Bandwidth: {bandwidth}\\") def generate_samples(kde, n_samples=100): Generates new samples from a KDE model. return kde.sample(n_samples) # Example usage if __name__ == \\"__main__\\": data = load_data(\'data.csv\') fig, axs = plt.subplots(3, 3, figsize=(15, 15)) kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1] for i, kernel in enumerate(kernels): for j, bandwidth in enumerate(bandwidths): plot_kde(data, kernel, bandwidth, axs[i, j]) plt.tight_layout() plt.show() # Generate samples from the best KDE model best_kde = KernelDensity(kernel=\'gaussian\', bandwidth=0.5).fit(data) new_samples = generate_samples(best_kde, n_samples=100) plt.scatter(new_samples[:, 0], new_samples[:, 1], color=\'blue\', label=\'Generated Samples\') plt.scatter(data[:, 0], data[:, 1], s=5, color=\'red\', label=\'Original Data\') plt.legend() plt.show()"},{"question":"**Problem Statement: Titanic Survival Visualization** You are provided with the Titanic dataset already built into the seaborn library. Using this dataset, you need to perform the following tasks to demonstrate your understanding of seaborn: 1. Load the Titanic dataset and display the first few rows to understand its structure. 2. Create a count plot that shows the distribution of passengers across different travel classes (\\"class\\"). 3. Group this count plot by the \\"survived\\" variable to show the survival status within each travel class. 4. Normalize these counts to show the percentages of survivors and non-survivors in each class instead of raw counts. 5. Customize the plot by adding the following features: - Title the plot \\"Survival Distribution across Classes\\". - Label the x-axis as \\"Travel Class\\". - Label the y-axis as \\"Percentage of Passengers\\". - Use a different color palette of your choice for better visual distinction between the survived and non-survived groups. # Function Signature ```python def titanic_visualization(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Show the first few rows of the dataset (optional, for your reference) display(titanic.head()) # Step 1. Create a count plot for the \'class\' variable # Step 2. Group the count plot by \'survived\' variable # Step 3. Normalize the counts to percentages # Step 4. Customize the plot (title, x-axis, y-axis labels, color palette) ... ``` # Expected Output A plot displaying the normalized survival distribution of Titanic passengers across different travel classes. # Constraints - Use the seaborn library for plotting. - Ensure that the resultant percentages for each class sum up to 100%. - Use the provided Titanic dataset from seaborn without modifying its contents. # Example ```python titanic_visualization() ``` This should display a seaborn plot with the mentioned customizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def titanic_visualization(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Show the first few rows of the dataset (optional, for your reference) print(titanic.head()) # Step 1. Create a count plot for the \'class\' variable # Step 2. Group the count plot by \'survived\' variable count_plot = sns.countplot(data=titanic, x=\'class\', hue=\'survived\') # Step 3. Normalize the counts to percentages class_counts = titanic.groupby([\'class\', \'survived\']).size().unstack() normalized_class_counts = class_counts.div(class_counts.sum(axis=1), axis=0) normalized_class_counts = normalized_class_counts * 100 # Create a normalized bar plot normalized_class_counts.plot(kind=\'bar\', stacked=True, color=[\'blue\', \'orange\']) # Step 4. Customize the plot (title, x-axis, y-axis labels, color palette) plt.title(\\"Survival Distribution across Classes\\") plt.xlabel(\\"Travel Class\\") plt.ylabel(\\"Percentage of Passengers\\") plt.legend([\'Did Not Survive\', \'Survived\'], title=\'Survived\') # Show the plot plt.show()"},{"question":"# Data Transformation and Analysis with Pandas You have been provided with a dataset containing information about sales transactions in a retail company. The data is stored in a CSV file named `sales_data.csv` with the following columns: - `transaction_id`: Unique identifier for each transaction. - `customer_id`: Unique identifier for each customer. - `product_id`: Unique identifier for each product. - `category`: Category of the product. - `quantity`: Number of units sold in the transaction. - `price`: Price per unit of the product. - `date`: Date of the transaction in `YYYY-MM-DD` format. Your task is to perform various data transformation and analysis tasks using pandas. You need to implement and demonstrate the following: 1. **Load the Data**: - Read the CSV file into a pandas DataFrame. 2. **Pivot Table with Aggregation**: - Create a pivot table that shows the total amount spent (`quantity * price`) by each customer (`customer_id`) on each product category (`category`). The table should have `customer_id` as the index and `category` as the columns. 3. **Reshape Data with Melting**: - Transform the pivot table created in the previous step into a long format where you have columns: `customer_id`, `category`, and `total_spent`. 4. **Create Dummy Variables**: - Convert the `category` column in the long format DataFrame to dummy variables. 5. **Explode Data**: - Suppose each `product_id` maps to multiple tags or attributes stored in a list. Create a DataFrame where each unique `product_id` is associated with its tags, and then explode this DataFrame so that each tag appears in its own row. 6. **Cross-tabulation**: - Compute a cross-tabulation of the number of transactions per product category segmented by the month of the transaction. 7. **Discretize Prices**: - Use `pandas.cut` to categorize the `price` into bins: \'Low\', \'Medium\', \'High\', and \'Very High\'. Each of these tasks must be implemented as separate functions in Python. Below is the skeleton of the functions you need to complete: ```python import pandas as pd def load_data(file_path): Load the data from the CSV file into a DataFrame. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded data as a DataFrame. # Write your code here def create_pivot_table(df): Create a pivot table that shows total amount spent by each customer on each product category. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The pivot table. # Write your code here def reshape_data(pivot_df): Transform the pivot table into a long format. Args: pivot_df (pd.DataFrame): The input pivot table DataFrame. Returns: pd.DataFrame: The reshaped DataFrame in long format. # Write your code here def create_dummy_variables(long_df): Convert the \'category\' column to dummy variables. Args: long_df (pd.DataFrame): The input long format DataFrame. Returns: pd.DataFrame: The long DataFrame with dummy variables. # Write your code here def explode_data(product_tags): Explode the DataFrame so that each tag appears in its own row. Args: product_tags (pd.DataFrame): DataFrame with \'product_id\' and list of \'tags\'. Returns: pd.DataFrame: The exploded DataFrame. # Write your code here def compute_cross_tabulation(df): Compute a cross-tabulation of the number of transactions per product category segmented by month. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The cross-tabulation table. # Write your code here def discretize_prices(df): Categorize the \'price\' column into bins. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The DataFrame with categorized price bins. # Write your code here ``` You should provide test cases or example usage of these functions to demonstrate their correctness. Document your code appropriately and ensure that your solution is efficient and handles edge cases. Hint: You might find the following pandas functions particularly useful for these tasks: `pd.read_csv`, `pd.pivot_table`, `pd.melt`, `pd.get_dummies`, `pd.explode`, `pd.crosstab`, and `pd.cut`.","solution":"import pandas as pd def load_data(file_path): Load the data from the CSV file into a DataFrame. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded data as a DataFrame. return pd.read_csv(file_path) def create_pivot_table(df): Create a pivot table that shows total amount spent by each customer on each product category. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The pivot table. df[\'total_spent\'] = df[\'quantity\'] * df[\'price\'] pivot_df = df.pivot_table(values=\'total_spent\', index=\'customer_id\', columns=\'category\', aggfunc=\'sum\', fill_value=0) return pivot_df def reshape_data(pivot_df): Transform the pivot table into a long format. Args: pivot_df (pd.DataFrame): The input pivot table DataFrame. Returns: pd.DataFrame: The reshaped DataFrame in long format. long_df = pivot_df.reset_index().melt(id_vars=\'customer_id\', var_name=\'category\', value_name=\'total_spent\') return long_df def create_dummy_variables(long_df): Convert the \'category\' column to dummy variables. Args: long_df (pd.DataFrame): The input long format DataFrame. Returns: pd.DataFrame: The long DataFrame with dummy variables. return pd.get_dummies(long_df, columns=[\'category\']) def explode_data(product_tags): Explode the DataFrame so that each tag appears in its own row. Args: product_tags (pd.DataFrame): DataFrame with \'product_id\' and list of \'tags\'. Returns: pd.DataFrame: The exploded DataFrame. exploded_df = product_tags.explode(\'tags\').reset_index(drop=True) return exploded_df def compute_cross_tabulation(df): Compute a cross-tabulation of the number of transactions per product category segmented by month. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The cross-tabulation table. df[\'month\'] = pd.to_datetime(df[\'date\']).dt.to_period(\'M\') cross_tab = pd.crosstab(df[\'category\'], df[\'month\'], values=df[\'transaction_id\'], aggfunc=\'count\', dropna=False) return cross_tab def discretize_prices(df): Categorize the \'price\' column into bins. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The DataFrame with categorized price bins. price_bins = [0, 10, 20, 50, float(\'inf\')] price_labels = [\'Low\', \'Medium\', \'High\', \'Very High\'] df[\'price_category\'] = pd.cut(df[\'price\'], bins=price_bins, labels=price_labels, include_lowest=True) return df"},{"question":"Objective: Write a Python script that utilizes the `asyncio` library to manage multiple coroutines that perform network I/O operations concurrently. Problem Statement: You are required to simulate an asynchronous system where multiple tasks download data from different URLs concurrently. Your task is to implement a function `async def fetch_data(url: str) -> str` that uses the `asyncio` library to download data from a given URL and return the content of that URL as a string. Additionally, implement a function `async def main(urls: List[str]) -> List[str]` which: 1. Accepts a list of URLs. 2. Creates and runs coroutines to fetch data from each URL concurrently. 3. Returns a list of strings where each string is the content of the corresponding URL. Input: - `urls`: A list of URLs (strings) from which data should be fetched. Example: `[\\"http://example.com\\", \\"http://anotherexample.com\\"]` Output: - A list of strings where each string is the content of the corresponding URL in the input list. Constraints: 1. The maximum number of URLs in the list will not exceed 100. 2. You are not allowed to use any external libraries apart from `asyncio` and Python\'s standard libraries. Example: ```python import asyncio async def fetch_data(url: str) -> str: # Simulate network I/O operation async def main(urls: List[str]) -> List[str]: # Implement fetching all URLs concurrently # Sample list of URLs for testing urls = [\\"http://example.com\\", \\"http://anotherexample.com\\"] # Running the main function content_list = asyncio.run(main(urls)) for content in content_list: print(content) ``` Note: For the purpose of this example, you can use `aiohttp` or similar library to perform actual network operations if required, or simulate the network I/O with `asyncio.sleep` for educational purposes. Performance Requirements: 1. Ensure that all network I/O operations are performed concurrently. 2. Efficiently manage the event loop and coroutine tasks using `asyncio`. Additional Notes: - You may need to handle exceptions in case any URL is inaccessible or any other network-related error occurs. - This question tests your ability to work with asynchronous programming, managing coroutines, and concurrent execution using `asyncio`.","solution":"import asyncio import aiohttp from typing import List async def fetch_data(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def main(urls: List[str]) -> List[str]: tasks = [fetch_data(url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"Coding Assessment Question # Objective Assess students\' ability to use the `xml.parsers.expat` module to parse XML documents and handle different elements and events triggered during parsing. # Problem Statement You are provided with an XML document containing details of various books in a library. Implement a function that parses this XML document and extracts the titles and authors of the books. Additionally, handle potential errors that may occur during parsing gracefully. # Requirements 1. **Function Signature:** ```python def extract_books(xml_data: str) -> List[Dict[str, str]]: ``` 2. **Input:** - `xml_data` (str): A string containing XML data representing a collection of books. 3. **Output:** - A list of dictionaries where each dictionary contains the title and author of a book. Example of the dictionary format: ```python {\\"title\\": \\"Book Title\\", \\"author\\": \\"Book Author\\"} ``` 4. **Constraints:** - The XML data will have a structure similar to: ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> <!-- More book entries --> </library> ``` 5. **Error Handling:** - If any parsing error occurs, the function should return an empty list. - Use appropriate handler functions to navigate through the XML elements. # Example ```python xml_data = \'\'\' <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> </library> \'\'\' output = extract_books(xml_data) # Expected output: [ {\\"title\\": \\"The Great Gatsby\\", \\"author\\": \\"F. Scott Fitzgerald\\"}, {\\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\"} ] ``` # Notes - Ensure that the handler functions for start elements, end elements, and character data are properly set and used to extract the required information. - Carefully handle empty strings, whitespace, and multiline tags. - Make use of the `xml.parsers.expat` module as described in the provided documentation.","solution":"import xml.parsers.expat from typing import List, Dict def extract_books(xml_data: str) -> List[Dict[str, str]]: # Initialize the parser parser = xml.parsers.expat.ParserCreate() books = [] current_book = {} current_element = \\"\\" # Handler for start element def start_element(name, attrs): nonlocal current_element current_element = name if name == \\"book\\": current_book.clear() # Handler for end element def end_element(name): nonlocal current_element if name == \\"book\\": books.append(current_book.copy()) current_element = \\"\\" # Handler for character data def char_data(data): if current_element in [\\"title\\", \\"author\\"]: current_book[current_element] = data.strip() # Setting handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data) except xml.parsers.expat.ExpatError: return [] return books"},{"question":"# Custom JSON Serialization and Deserialization You are required to implement a custom JSON encoder and decoder for a `Person` class. This class has the following attributes: - `name`: A string representing the person\'s name. - `age`: An integer representing the person\'s age. - `email`: A string representing the person\'s email address, which is optional and can be `None`. The JSON representation of a `Person` object should include: - `name` as `\\"name\\"`. - `age` as `\\"age\\"`. - `email` as `\\"email\\"`, but if the email is `None`, it should be omitted from the JSON output. Additionally, implement a custom function to handle deserialization such that it can properly reconstruct a `Person` object from the JSON data. # Requirements: 1. Implement the `Person` class with the `__init__`, `__str__`, and `__eq__` methods. 2. Implement a custom JSON encoder subclass, `PersonEncoder`, that serializes `Person` instances as specified. 3. Implement a function `person_decoder(dct)` that deserializes JSON data back into a `Person` instance. 4. Implement unit tests to verify your encoder and decoder. # Constraints: - You may not use external libraries; leverage only Python\'s standard library. - Ensure that your solution handles edge cases, such as missing attributes in the JSON data for deserialization. # Input A `Person` instance: ```python person = Person(name=\\"Alice\\", age=30, email=\\"alice@example.com\\") ``` A JSON string: ```json \'{\\"name\\": \\"Bob\\", \\"age\\": 25}\' ``` # Output JSON string for the `Person` object: ```json \'{\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}\' ``` Python `Person` object from JSON string: ```python Person(name=\\"Bob\\", age=25, email=None) ``` # Example Here is a partial implementation to get you started: ```python import json class Person: def __init__(self, name, age, email=None): self.name = name self.age = age self.email = email def __str__(self): return f\\"Person(name={self.name}, age={self.age}, email={self.email})\\" def __eq__(self, other): return (self.name == other.name) and (self.age == other.age) and (self.email == other.email) class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): person_dict = {\'name\': obj.name, \'age\': obj.age} if obj.email is not None: person_dict[\'email\'] = obj.email return person_dict return super().default(obj) def person_decoder(dct): if \'name\' in dct and \'age\' in dct: return Person(name=dct[\'name\'], age=dct[\'age\'], email=dct.get(\'email\')) return dct # Example usage: person = Person(name=\\"Alice\\", age=30, email=\\"alice@example.com\\") json_str = json.dumps(person, cls=PersonEncoder) print(json_str) # \'{\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}\' json_data = \'{\\"name\\": \\"Bob\\", \\"age\\": 25}\' person_obj = json.loads(json_data, object_hook=person_decoder) print(person_obj) # Person(name=Bob, age=25, email=None) ``` Implement this fully and ensure to add tests for varied inputs and edge cases.","solution":"import json class Person: def __init__(self, name, age, email=None): self.name = name self.age = age self.email = email def __str__(self): return f\\"Person(name={self.name}, age={self.age}, email={self.email})\\" def __eq__(self, other): return (self.name == other.name) and (self.age == other.age) and (self.email == other.email) class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): person_dict = {\'name\': obj.name, \'age\': obj.age} if obj.email is not None: person_dict[\'email\'] = obj.email return person_dict return super().default(obj) def person_decoder(dct): if \'name\' in dct and \'age\' in dct: return Person(name=dct[\'name\'], age=dct[\'age\'], email=dct.get(\'email\')) return dct # Example usage: person = Person(name=\\"Alice\\", age=30, email=\\"alice@example.com\\") json_str = json.dumps(person, cls=PersonEncoder) print(json_str) # \'{\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}\' json_data = \'{\\"name\\": \\"Bob\\", \\"age\\": 25}\' person_obj = json.loads(json_data, object_hook=person_decoder) print(person_obj) # Person(name=Bob, age=25, email=None)"},{"question":"**Question: Implement a Custom SAX XML ContentHandler** *Objective:* Create a custom SAX `ContentHandler` to parse XML data representing a simple library catalog. The catalog consists of books, each with a title, author, and publication year. Your handler should extract the information and store it in a structured format like a list of dictionaries. *Input:* - A string representing XML content. The XML schema is as follows: ```xml <catalog> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2002</year> </book> <!-- More book entries... --> </catalog> ``` *Output:* - A list of dictionaries where each dictionary represents a book with keys: `title`, `author`, and `year`. For example: ```python [ {\\"title\\": \\"Book Title 1\\", \\"author\\": \\"Author 1\\", \\"year\\": \\"2001\\"}, {\\"title\\": \\"Book Title 2\\", \\"author\\": \\"Author 2\\", \\"year\\": \\"2002\\"}, # More dictionaries... ] ``` *Constraints:* 1. Use the `xml.sax` module to implement the parser and handler. 2. The input XML string won’t be deeply nested and will adhere to the provided schema. *Performance Requirements:* - Your solution should handle parsing of XML strings with up to 1000 book entries efficiently. *Function Signature:* ```python import xml.sax class LibraryCatalogHandler(xml.sax.ContentHandler): # Implement methods for the ContentHandler def parse_library_catalog(xml_content: str) -> list: # Initialize the XML parser and custom handler # Return the structured output # Example XML content xml_data = <catalog> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2002</year> </book> </catalog> # Example use case result = parse_library_catalog(xml_data) print(result) # Expected output: [{\'title\': \'Book Title 1\', \'author\': \'Author 1\', \'year\': \'2001\'}, {\'title\': \'Book Title 2\', \'author\': \'Author 2\', \'year\': \'2002\'}] ```","solution":"import xml.sax class LibraryCatalogHandler(xml.sax.ContentHandler): def __init__(self): self.books = [] self.current_book = {} self.current_data = \\"\\" def startElement(self, name, attrs): self.current_data = name if name == \\"book\\": self.current_book = {} def endElement(self, name): if name == \\"book\\": self.books.append(self.current_book) self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.current_book[\\"title\\"] = content elif self.current_data == \\"author\\": self.current_book[\\"author\\"] = content elif self.current_data == \\"year\\": self.current_book[\\"year\\"] = content def parse_library_catalog(xml_content: str) -> list: handler = LibraryCatalogHandler() xml.sax.parseString(xml_content, handler) return handler.books"},{"question":"# Question: Implementing and Testing a Multi-Threaded Priority Task Scheduler using Python `queue` Objective: Design and implement a simple priority task scheduler using the `queue.PriorityQueue` class from the `queue` module in Python. The scheduler should be able to handle concurrent task execution using multiple threads, where tasks with higher priority numbers are executed before those with lower priority numbers. Requirements: 1. **Scheduler Class**: - Implement a class named `PriorityTaskScheduler`. - The scheduler should use `queue.PriorityQueue` to manage tasks. - Tasks should be added to the scheduler with a priority and a callable representing the task to be executed. 2. **Methods**: - `__init__(self, num_workers: int)`: Initializes the scheduler with a specified number of worker threads. - `add_task(self, priority: int, task: Callable) -> None`: Adds a task to the scheduler with the specified priority. - `start(self) -> None`: Starts the worker threads to process tasks. - `stop(self) -> None`: Stops all worker threads after finishing the current tasks in the queue. - `join(self) -> None`: Blocks until all tasks are completed. 3. **Concurrency**: - Use threads from the `threading` module to achieve concurrent task execution. - Ensure thread safety and proper synchronization using the provided `queue.PriorityQueue` methods. 4. **Task Execution**: - Tasks are represented by callables (functions) that do not take any arguments. - The worker threads should continuously fetch and execute tasks while the scheduler is running. Example Usage: ```python from queue import PriorityQueue from threading import Thread import time class PriorityTaskScheduler: def __init__(self, num_workers: int): self.num_workers = num_workers self.task_queue = PriorityQueue() self.threads = [] self.running = True def add_task(self, priority: int, task: Callable) -> None: self.task_queue.put((priority, task)) def worker(self) -> None: while self.running or not self.task_queue.empty(): try: priority, task = self.task_queue.get(timeout=1) task() self.task_queue.task_done() except queue.Empty: pass def start(self) -> None: self.threads = [Thread(target=self.worker) for _ in range(self.num_workers)] for thread in self.threads: thread.start() def stop(self) -> None: self.running = False for thread in self.threads: thread.join() def join(self) -> None: self.task_queue.join() # Example execution def task1(): print(\\"Task 1 executed\\") def task2(): print(\\"Task 2 executed\\") def task3(): print(\\"Task 3 executed\\") scheduler = PriorityTaskScheduler(num_workers=2) scheduler.add_task(2, task2) scheduler.add_task(1, task1) scheduler.add_task(3, task3) scheduler.start() scheduler.join() scheduler.stop() print(\\"All tasks completed.\\") ``` Constraints: - Number of worker threads (`num_workers`) will be between 1 and 20. - The number of tasks added will not exceed 1000. - Priority numbers will be integers between 1 and 100, with 1 being the highest priority. - Each task callable does not take any parameters and returns `None`. Performance Requirements: - The scheduler should handle task scheduling and execution efficiently, without unnecessary delays. - Ensure that the implementation leverages the `queue.PriorityQueue` for managing task priorities and thread safety effectively. Provide your implementation below: ```python # Your implementation goes here. ```","solution":"from queue import PriorityQueue, Empty from threading import Thread class PriorityTaskScheduler: def __init__(self, num_workers: int): self.num_workers = num_workers self.task_queue = PriorityQueue() self.threads = [] self.running = False def add_task(self, priority: int, task) -> None: self.task_queue.put((priority, task)) def worker(self) -> None: while self.running or not self.task_queue.empty(): try: priority, task = self.task_queue.get(timeout=1) task() self.task_queue.task_done() except Empty: continue def start(self) -> None: self.running = True self.threads = [Thread(target=self.worker) for _ in range(self.num_workers)] for thread in self.threads: thread.start() def stop(self) -> None: self.running = False for thread in self.threads: thread.join() def join(self) -> None: self.task_queue.join()"},{"question":"# Dataset Visualization with Theme Customization Problem Overview You are provided with the \\"anscombe\\" dataset, which contains four different datasets commonly used to demonstrate the importance of graphical representation of data. Your task is to visualize each dataset in a single figure, customize its appearance using Seaborn\'s theme functionality, and save the plot as an image file. Task 1. **Load the Dataset:** Use the Seaborn `load_dataset` function to load the \\"anscombe\\" dataset. 2. **Create the Plot:** - Utilize the `seaborn.objects` module to create a plot of the dataset. - Facet the plot by the \\"dataset\\" column, arranging the output into a grid of 2 rows and 2 columns. 3. **Add Plot Elements:** - Add a linear regression line with a first-order polynomial fit. - Add data points as dots on the plot. 4. **Customize the Theme:** - Change the axes background color to white. - Set the axes edge color to \\"slategray\\". - Increase the line width of the regression lines to 4. - Apply the \\"ticks\\" style from Seaborn\'s built-in styles. 5. **Save the Plot:** Save the final plot as an image file named \\"anscombe_plot.png\\". Input and Output Formats - **Input:** No input is required as the dataset is loaded from within the function. - **Output:** The function should save an image file named \\"anscombe_plot.png\\" in the current working directory. Constraints and Performance - Use Seaborn functions and methods as specified. - Ensure that the code runs without errors and produces the correct plot. Function Signature ```python def visualize_anscombe_dataset(): # Your implementation here pass ``` Example Execution Once the function is implemented correctly, running `visualize_anscombe_dataset()` should create and save an image file \\"anscombe_plot.png\\" with the customized visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_anscombe_dataset(): # Load the Anscombe dataset df = sns.load_dataset(\\"anscombe\\") # Set the Seaborn style sns.set(style=\\"ticks\\") # Initialize a grid of plots with an Axes for each dataset g = sns.FacetGrid(df, col=\\"dataset\\", col_wrap=2) # Map a scatter plot and a linear regression line to each Axes g.map(sns.regplot, \\"x\\", \\"y\\", ci=None, color=\\"blue\\", scatter_kws={\'s\': 50}, line_kws={\\"color\\": \\"red\\", \\"lw\\": 4}) # Customize plot appearance for ax in g.axes.flatten(): ax.set_facecolor(\\"white\\") ax.spines[\'top\'].set_color(\'slategray\') ax.spines[\'right\'].set_color(\'slategray\') ax.spines[\'left\'].set_color(\'slategray\') ax.spines[\'bottom\'].set_color(\'slategray\') ax.spines[\'top\'].set_linewidth(1.2) ax.spines[\'right\'].set_linewidth(1.2) ax.spines[\'left\'].set_linewidth(1.2) ax.spines[\'bottom\'].set_linewidth(1.2) # Adjust layout and save the figure g.fig.suptitle(\\"Anscombe\'s Quartet\\") g.fig.tight_layout() g.fig.subplots_adjust(top=0.9) g.savefig(\\"anscombe_plot.png\\")"},{"question":"**FTP Client using `ftplib`** **Objective:** Write a Python function `ftp_sync` that connects to a given FTP server, navigates to a specified directory, lists its contents, downloads all `.txt` files to a local directory, and logs each step of the process. **Function Signature:** ```python def ftp_sync(server: str, user: str, password: str, remote_dir: str, local_dir: str) -> None: pass ``` **Inputs:** - `server` (str): The FTP server address (e.g., `\'ftp.us.debian.org\'`). - `user` (str): Username to login (default to `\'anonymous\'` if empty). - `password` (str): Password to login (default to `\'anonymous@\'` if empty). - `remote_dir` (str): The directory on the FTP server to navigate to. - `local_dir` (str): The local directory to save the downloaded files. **Outputs:** - The function should return `None`. - All `.txt` files in the specified `remote_dir` should be downloaded to the `local_dir`. - The function should print logs for each step such as connecting, logging in, changing the directory, listing contents, downloading files, and errors encountered. **Constraints:** - Use appropriate error handling to manage exceptions thrown by `ftplib`. - Ensure the function handles timeouts and connection issues gracefully. - The function should support both anonymous and authenticated connections. - Use `with` statements where applicable to ensure resources are managed properly. **Example Usage:** ```python ftp_sync(\'ftp.us.debian.org\', \'\', \'\', \'/debian\', \'/local/downloads\') ``` **Example Log Output:** ``` Connecting to ftp.us.debian.org Logged in as anonymous Changed directory to /debian Listing contents of /debian Downloading README.txt Saved to /local/downloads/README.txt File download complete Logged out from ftp.us.debian.org ``` **Note:** - Assume that the `local_dir` already exists and is writable. - You may need to create necessary sub-directories under `local_dir` if required by the snapshot of the FTP server. - For maximum compatibility and proper UTF-8 handling, ensure encoding is set where necessary. **Guidance:** 1. Use `ftplib.FTP` for the basic FTP connection. 2. Implement connection retries with timeouts to handle unstable network conditions. 3. Verify each step by printing appropriate log messages. 4. Ensure proper error handling to catch and log exceptions such as unreachable hosts, incorrect login credentials, and file not found errors.","solution":"import ftplib import os def ftp_sync(server: str, user: str, password: str, remote_dir: str, local_dir: str) -> None: try: # Connect to FTP server print(f\\"Connecting to {server}\\") with ftplib.FTP(server) as ftp: # Login to the FTP server if not user: user = \'anonymous\' if not password: password = \'anonymous@\' print(f\\"Logging in as {user}\\") ftp.login(user=user, passwd=password) print(f\\"Logged in as {user}\\") # Change to the specified remote directory print(f\\"Changing directory to {remote_dir}\\") ftp.cwd(remote_dir) print(f\\"Changed directory to {remote_dir}\\") # List contents of the directory print(f\\"Listing contents of {remote_dir}\\") filenames = ftp.nlst() print(f\\"Contents: {filenames}\\") # Download all .txt files for filename in filenames: if filename.endswith(\'.txt\'): local_filepath = os.path.join(local_dir, filename) print(f\\"Downloading {filename}\\") with open(local_filepath, \'wb\') as local_file: ftp.retrbinary(f\'RETR {filename}\', local_file.write) print(f\\"Saved to {local_filepath}\\") print(\\"File download complete\\") except ftplib.all_errors as e: print(f\\"FTP error: {e}\\")"},{"question":"# Question: Customizing Plot Limits with Seaborn You are tasked with creating a function that leverages seaborn\'s object-oriented interface to customize plot limits and visualize data accordingly. Ensure your function handles different scenarios involving axis limits. Function Signature ```python import seaborn.objects as so import matplotlib.pyplot as plt def customize_plot(x_vals: list, y_vals: list, x_limit: tuple = None, y_limit: tuple = None, display: bool = False) -> so.Plot: Plots the provided data with customized axis limits. Parameters: x_vals (list): A list of x coordinates. y_vals (list): A list of y coordinates. x_limit (tuple, optional): A tuple specifying the min and max for the x-axis. y_limit (tuple, optional): A tuple specifying the min and max for the y-axis. display (bool, optional): If True, display the plot immediately. Returns: so.Plot: A seaborn Plot object. pass ``` Input - `x_vals`: A list of numerical values representing the x-coordinates. - `y_vals`: A list of numerical values representing the y-coordinates. - `x_limit`: An optional tuple (`min`, `max`) defining the x-axis limit. Use `None` for default behavior. - `y_limit`: An optional tuple (`min`, `max`) defining the y-axis limit. Use `None` for default behavior. - `display`: A boolean flag to indicate if the plot should be displayed immediately using `plt.show()`. Default is `False`. Output - Returns a seaborn `Plot` object with the customizations applied. Constraints - The lengths of `x_vals` and `y_vals` must be the same. - `x_limit` and `y_limit` should either be `None` or a tuple of two numerical values. Examples Example 1: ```python x = [1, 2, 3] y = [1, 3, 2] plot = customize_plot(x, y) # Default plot limits should be used ``` Example 2: ```python x = [1, 2, 3] y = [1, 3, 2] plot = customize_plot(x, y, x_limit=(0, 4), y_limit=(-1, 6)) # Plot limits set to x=(0, 4) and y=(-1, 6) ``` Example 3: ```python x = [1, 2, 3] y = [1, 3, 2] plot = customize_plot(x, y, y_limit=(4, 0), display=True) # Plot limits for y are reversed (4, 0) and plot is displayed immediately ``` *Note*: The function should make use of the seaborn `Plot` class to achieve the described functionalities.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def customize_plot(x_vals: list, y_vals: list, x_limit: tuple = None, y_limit: tuple = None, display: bool = False) -> so.Plot: Plots the provided data with customized axis limits. Parameters: x_vals (list): A list of x coordinates. y_vals (list): A list of y coordinates. x_limit (tuple, optional): A tuple specifying the min and max for the x-axis. y_limit (tuple, optional): A tuple specifying the min and max for the y-axis. display (bool, optional): If True, display the plot immediately. Returns: so.Plot: A seaborn Plot object. # Ensure the input lists have the same length. if len(x_vals) != len(y_vals): raise ValueError(\\"x_vals and y_vals must have the same length.\\") # Create the plot object with the provided x and y values plot = so.Plot(x=x_vals, y=y_vals).add(so.Line()) # Set the x-axis limits if provided if x_limit: plot = plot.limit(x=x_limit) # Set the y-axis limits if provided if y_limit: plot = plot.limit(y=y_limit) # Display the plot if the display flag is set to True if display: plot.show() # Return the plot object return plot"},{"question":"# Coding Assessment: Multi-Client TCP/IPv6 Echo Server **Objective:** Write a Python function to implement a multi-client echo server that supports both IPv4 and IPv6 connections. The server should listen for incoming client connections on a specified port, accept multiple client connections, receive messages from each client, and echo the received messages back to the respective clients. The server should run indefinitely until manually stopped. **Requirements:** 1. The server should support both IPv4 and IPv6 connections. 2. The server should handle multiple client connections concurrently. 3. For each client, the server should receive data and immediately send it back (echo). 4. The server should handle exceptions and ensure proper resource cleanup. **Function Signature:** ```python def start_echo_server(port: int): pass ``` **Input:** - `port` (int): The port number on which the server should listen for incoming connections. **Output:** - None **Example:** ```python # To run the echo server on port 12345 start_echo_server(12345) ``` **Constraints:** - The port number should be a valid non-privileged port (e.g., 1024-65535). - The function should handle common socket exceptions gracefully, ensuring that resources are properly cleaned up even in case of errors. **Tips:** - Use the `socket` module to create and manage sockets. - Consider using the `select` module or threading for handling multiple clients concurrently. - Ensure that the server can handle both IPv4 and IPv6 connections properly. - Use appropriate socket options to make the server robust and reliable. Feel free to include additional helper functions or classes if necessary. **Notes:** - This question tests the understanding of socket programming, handling different socket families, managing multiple client connections, and dealing with exceptions and resource cleanup. - Make sure your solution includes proper documentation and comments explaining the code and the choices made. Good luck!","solution":"import socket import threading import select def handle_client(client_socket): try: while True: data = client_socket.recv(1024) if not data: break client_socket.sendall(data) except Exception as e: print(f\\"Exception in client handler: {e}\\") finally: client_socket.close() def start_echo_server(port: int): server_socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'::\', port)) server_socket.listen(5) print(f\\"Server listening on port {port}\\") try: while True: readable, _, _ = select.select([server_socket], [], [], 0.5) for s in readable: if s is server_socket: client_socket, _ = server_socket.accept() client_handler = threading.Thread(target=handle_client, args=(client_socket,)) client_handler.start() except Exception as e: print(f\\"Exception in server: {e}\\") finally: server_socket.close()"},{"question":"**Objective:** Demonstrate your understanding of the `runpy` module by implementing a function that dynamically executes a specified Python module or script and collects specified variables from the executed code. **Problem Statement:** Implement a function `execute_and_collect(module_or_path: str, variables: List[str], is_path: bool = False) -> Dict[str, Any]` that dynamically executes a Python module or script and returns a dictionary containing the values of the specified variables from the executed code. - If `is_path` is `False`, the function should treat `module_or_path` as a module name and use `runpy.run_module` to execute it. - If `is_path` is `True`, the function should treat `module_or_path` as a file path and use `runpy.run_path` to execute the script at that location. **Function Signature:** ```python from typing import List, Dict, Any import runpy def execute_and_collect(module_or_path: str, variables: List[str], is_path: bool = False) -> Dict[str, Any]: # Your implementation here pass ``` **Input:** - `module_or_path` (str): The name of the module to execute or the path to the script file. - `variables` (List[str]): A list of variable names to collect from the executed code. - `is_path` (bool): A flag indicating whether `module_or_path` is a module name (`False`) or file path (`True`). Defaults to `False`. **Output:** - Dict[str, Any]: A dictionary where the keys are the variable names from `variables` and the values are the corresponding values from the executed module/script. **Examples:** 1. Execute a module by name: ```python # Assume a module named \\"example_module\\" defines variables x, y, and z result = execute_and_collect(\\"example_module\\", [\\"x\\", \\"y\\", \\"z\\"]) print(result) # Output: {\\"x\\": <value_of_x>, \\"y\\": <value_of_y>, \\"z\\": <value_of_z>} ``` 2. Execute a script by file path: ```python # Assume a script at \\"path/to/script.py\\" defines variables a and b result = execute_and_collect(\\"path/to/script.py\\", [\\"a\\", \\"b\\"], is_path=True) print(result) # Output: {\\"a\\": <value_of_a>, \\"b\\": <value_of_b>} ``` **Constraints:** - The specified `module_or_path` must point to a valid Python module or script. - The variable names in `variables` must match those defined in the executed code. If any variable is not found, it should not be included in the output dictionary. **Notes:** - Use the `runpy.run_module` and `runpy.run_path` functions to execute the code. - Handle any exceptions that may occur during execution and ensure they are properly logged or reported. Your task is to implement the `execute_and_collect` function and ensure it works for both modules and script files.","solution":"from typing import List, Dict, Any import runpy def execute_and_collect(module_or_path: str, variables: List[str], is_path: bool = False) -> Dict[str, Any]: Executes a specified Python module or script and returns the values of the specified variables. :param module_or_path: The name of the module to execute or the path of the script file. :param variables: A list of variable names to collect from the executed code. :param is_path: Flag indicating whether `module_or_path` is a module name (False) or file path (True). Defaults to False. :return: A dictionary containing the specified variables and their values. if is_path: result = runpy.run_path(module_or_path) else: result = runpy.run_module(module_or_path) return {var: result.get(var) for var in variables if var in result}"},{"question":"# System Information Report Generator **Objective:** Use the Python `platform` module to create a function that generates a detailed report of the system\'s information. The report should include various attributes such as the system architecture, machine type, network name, Python version and build information, and more. **Function Signature:** ```python def generate_system_report() -> str: ``` **Expected Input and Output:** - The function does not take any input parameters. - The function returns a single string containing the formatted system information report. **Requirements:** 1. The report should include the following details: - Platform: The overall platform information (using `platform.platform()`). - Architecture: Bit architecture and linkage format (using `platform.architecture()`). - Machine: The machine type (using `platform.machine()`). - Node: The computer\'s network name (using `platform.node()`). - Processor: The real processor name (using `platform.processor()`). - Python Version: The version of Python being used (using `platform.python_version()`). - Python Build: Build number and date (using `platform.python_build()`). - Python Compiler: The compiler used to compile Python (using `platform.python_compiler()`). - System: The system/OS name (using `platform.system()`). - System Release: The system release (using `platform.release()`). - System Version: The system\'s release version (using `platform.version()`). 2. Handle edge cases where the information might be unavailable and ensure the report handles these gracefully by indicating \\"Unknown\\" for such fields. 3. The report should be well-formatted. Here is an example format: ``` System Information Report ------------------------- Platform: Linux-5.4.0-66-generic-x86_64-with-glibc2.29 Architecture: (\'64bit\', \'ELF\') Machine: x86_64 Node: mycomputer.local Processor: x86_64 Python Version: 3.10.0 Python Build: (\'default\', \'Oct 5 2021 15:56:51\') Python Compiler: GCC 9.3.0 System: Linux System Release: 5.4.0-66-generic System Version: #74-Ubuntu SMP Mon Jan 18 17:52:23 UTC 2021 ``` 4. Efficiently use the functions within the `platform` module to gather all required information. **Constraints:** - Assume the code will be executed on a platform where Python and the `platform` module are available. - Focus on utilizing the `platform` module to its full potential for gathering system-specific information. **Performance Requirements:** - The function should execute efficiently without unnecessary delays since it predominantly involves fetching system data using the `platform` module, which is designed to be straightforward. **Example Execution:** ```python print(generate_system_report()) ``` The output should match the format and include as many details as possible, falling back to \\"Unknown\\" where data cannot be determined.","solution":"import platform def generate_system_report() -> str: Generates a detailed report of the system\'s information using the platform module. report = [ \\"System Information Report\\", \\"-------------------------\\", f\\"Platform: {platform.platform()}\\", f\\"Architecture: {platform.architecture()}\\", f\\"Machine: {platform.machine()}\\", f\\"Node: {platform.node()}\\", f\\"Processor: {platform.processor()}\\", f\\"Python Version: {platform.python_version()}\\", f\\"Python Build: {platform.python_build()}\\", f\\"Python Compiler: {platform.python_compiler()}\\", f\\"System: {platform.system()}\\", f\\"System Release: {platform.release()}\\", f\\"System Version: {platform.version()}\\" ] return \\"n\\".join(report)"},{"question":"You are provided with a dataset containing two distinct clusters of data points in a two-dimensional space. 1. Implement a function `fit_gmm_and_predict` that: - **Input**: - `data` (a numpy array of shape `(n_samples, 2)`, where `n_samples` is the number of data points). - `n_components` (an integer specifying the number of mixture components). - **Output**: - `labels` (a numpy array of shape `(n_samples,)` containing the predicted labels for the data points). 2. Implement a function `find_optimal_components` that: - **Input**: - `data` (a numpy array of shape `(n_samples, 2)`). - `max_components` (an integer specifying the maximum number of mixture components to test). - **Output**: - `optimal_components` (an integer specifying the number of components chosen based on the BIC score). 3. Using the `fit_gmm_and_predict` function, fit a GMM to the data using the number of components returned by `find_optimal_components`, and visualize the results by plotting the data points and coloring them by their predicted labels. Here is a skeleton structure you can use for your implementation: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture def fit_gmm_and_predict(data, n_components): Fit a Gaussian Mixture Model to the data and predict the labels. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) n_components (int): The number of mixture components Returns: labels (numpy array): The predicted labels for each sample gmm = GaussianMixture(n_components=n_components) gmm.fit(data) labels = gmm.predict(data) return labels def find_optimal_components(data, max_components): Find the optimal number of components for a Gaussian Mixture Model based on BIC. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) max_components (int): The maximum number of components to test Returns: optimal_components (int): The optimal number of components based on BIC bic_scores = [] for i in range(1, max_components + 1): gmm = GaussianMixture(n_components=i) gmm.fit(data) bic_scores.append(gmm.bic(data)) optimal_components = np.argmin(bic_scores) + 1 return optimal_components def plot_clusters(data, labels): Plot the data points, coloring them by their cluster labels. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) labels (numpy array): The predicted labels for each sample plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\'viridis\') plt.title(\'GMM Clustering\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example Usage data = np.random.rand(300, 2) # replace this with actual data max_components = 10 optimal_components = find_optimal_components(data, max_components) labels = fit_gmm_and_predict(data, optimal_components) plot_clusters(data, labels) ``` This question is designed to check the following: - **Understanding of GMM**: Implementation of fitting and prediction using the GMM class. - **Model Selection**: Using the BIC score to determine the optimal number of components. - **Data Visualization**: Plotting and visualizing the clustered results.","solution":"import numpy as np from sklearn.mixture import GaussianMixture import matplotlib.pyplot as plt def fit_gmm_and_predict(data, n_components): Fit a Gaussian Mixture Model to the data and predict the labels. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) n_components (int): The number of mixture components Returns: labels (numpy array): The predicted labels for each sample gmm = GaussianMixture(n_components=n_components) gmm.fit(data) labels = gmm.predict(data) return labels def find_optimal_components(data, max_components): Find the optimal number of components for a Gaussian Mixture Model based on BIC. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) max_components (int): The maximum number of components to test Returns: optimal_components (int): The optimal number of components based on BIC bic_scores = [] for i in range(1, max_components + 1): gmm = GaussianMixture(n_components=i) gmm.fit(data) bic_scores.append(gmm.bic(data)) optimal_components = np.argmin(bic_scores) + 1 return optimal_components def plot_clusters(data, labels): Plot the data points, coloring them by their cluster labels. Parameters: data (numpy array): A 2D numpy array of shape (n_samples, 2) labels (numpy array): The predicted labels for each sample plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\'viridis\') plt.title(\'GMM Clustering\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example Usage data = np.random.rand(300, 2) # Replace this with the actual data max_components = 10 optimal_components = find_optimal_components(data, max_components) labels = fit_gmm_and_predict(data, optimal_components) plot_clusters(data, labels)"},{"question":"**Question**: Implement a custom autograd Function in PyTorch **Goal**: Implement a custom autograd `Function` that performs a new operation not defined in PyTorch\'s built-in operations. In this problem, you will implement the `ExpSquareFunction` that computes the element-wise exponential of the square of the input tensor during the forward pass and computes its gradient during the backward pass. # Function Details 1. **ExpSquareFunction**: - **Forward pass**: Computes ( y = exp(x^2) ) for each element ( x ) of the input tensor. - **Backward pass**: Computes the gradient of ( y ) with respect to the input ( x ). # Inputs and Outputs - **Input**: A PyTorch tensor ( x ) of arbitrary shape. - **Output**: A PyTorch tensor ( y ) of the same shape as ( x ), where each element is ( exp(x^2) ). # Constraints 1. You must use the `torch.autograd.Function` class to define the custom operation. 2. Save any necessary intermediate values during the forward pass for use in the backward pass. 3. The `apply` method should be used to call the function. 4. You must implement the gradient computation explicitly in the `backward` method. 5. The implementation should respect PyTorch conventions and support autograd functionality. # Performance Requirements - Ensure your implementation efficiently computes the forward and backward pass without redundant computations or unnecessary memory usage. - Use tensor operations to leverage PyTorch\'s optimized backend. # Example Usage ```python import torch from torch.autograd import Function class ExpSquareFunction(Function): @staticmethod def forward(ctx, input): # Perform the forward pass result = input.pow(2).exp() # Save the input tensor for the backward pass ctx.save_for_backward(input) return result @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor from the forward pass input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * 2 * input * input.pow(2).exp() return grad_input # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) exp_square = ExpSquareFunction.apply y = exp_square(x) y.backward(torch.ones_like(x)) print(x.grad) # Gradient of x with respect to y ``` # Tasks 1. Implement the `ExpSquareFunction` class. 2. Test the custom function using the provided example. 3. Verify that the gradients are computed correctly by comparing to the expected analytical gradients. # Hints 1. Remember to save any intermediate values required for computing the gradient in the `ctx.save_for_backward` method during the forward pass. 2. Use tensor operations to ensure efficient computation. 3. Test the function with different input values to ensure it handles edge cases and different tensor shapes correctly.","solution":"import torch from torch.autograd import Function class ExpSquareFunction(Function): @staticmethod def forward(ctx, input): # Perform the forward pass result = input.pow(2).exp() # Save the input tensor for the backward pass ctx.save_for_backward(input) return result @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor from the forward pass input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * 2 * input * input.pow(2).exp() return grad_input # Example usage: # x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # exp_square = ExpSquareFunction.apply # y = exp_square(x) # y.backward(torch.ones_like(x)) # print(x.grad) # Gradient of x with respect to y"},{"question":"Advanced Enum Usage Objective Demonstrate your understanding of the `enum` module in Python by implementing an enumeration that combines various advanced features such as unique value enforcement, automatic value generation, custom methods, and comparison operations. Problem Statement You are required to create an enumeration that classifies different types of logs generated by a system. Each log type will have a unique identifier, a severity level, and a description. Additionally, logs should support comparison based on their severity levels. 1. Create an enum class `LogType` with the following requirements: - The enum members should be `INFO`, `WARN`, `ERROR`, and `CRITICAL`. - Ensure that log types have unique values using the `unique` decorator. - Automatically generate values for enum members. 2. Redefine the `__init__` and `__new__` methods to assign a severity level and a description to each log type. Use the following severity levels and descriptions: - `INFO`: severity level 1, description \\"Informational message\\" - `WARN`: severity level 2, description \\"Warning message about a potential issue\\" - `ERROR`: severity level 3, description \\"Error message indicating failure\\" - `CRITICAL`: severity level 4, description \\"Critical message indicating severe failure\\" 3. Implement comparison methods (`__lt__`, `__le__`, `__gt__`, `__ge__`) for `LogType` so that log types can be compared based on their severity levels. 4. Implement a custom method `describe` that returns a string representation of the log type and its description. Input and Output Format - There is no direct input, but the enum class should be tested within a script to showcase its functionality. - Output should demonstrate the defined functionalities, including creating and comparing logs, and using the `describe` method. Constraints - Use the `enum` module as imported from the Python standard library. - Ensure that all requirements are strictly met without modifying the given severity levels and descriptions. Example Usage ```python from enum import Enum, auto, unique @unique class LogType(Enum): INFO = auto() WARN = auto() ERROR = auto() CRITICAL = auto() def __new__(cls, severity, description): obj = object.__new__(cls) obj._value_ = auto() obj.severity = severity obj.description = description return obj def __init__(self, severity, description): self.severity = severity self.description = description def __lt__(self, other): if self.__class__ is other.__class__: return self.severity < other.severity return NotImplemented def __le__(self, other): if self.__class__ is other.__class__: return self.severity <= other.severity return NotImplemented def __gt__(self, other): if self.__class__ is other.__class__: return self.severity > other.severity return NotImplemented def __ge__(self, other): if self.__class__ is other.__class__: return self.severity >= other.severity return NotImplemented def describe(self): return f\\"{self.name}: {self.description}\\" # Initialize log types manually to meet the description requirements log_info = LogType(INFO=(1, \\"Informational message\\")) log_warn = LogType(WARN=(2, \\"Warning message about a potential issue\\")) log_error = LogType(ERROR=(3, \\"Error message indicating failure\\")) log_critical = LogType(CRITICAL=(4, \\"Critical message indicating severe failure\\")) # Example usage print(log_info.describe()) # Output: INFO: Informational message print(log_warn.describe()) # Output: WARN: Warning message about a potential issue print(log_error > log_info) # Output: True print(log_critical <= log_warn) # Output: False ``` Make sure to test the `LogType` enum to verify that all requirements are satisfied, demonstrating proper functionality.","solution":"from enum import Enum, auto, unique @unique class LogType(Enum): INFO = (1, \\"Informational message\\") WARN = (2, \\"Warning message about a potential issue\\") ERROR = (3, \\"Error message indicating failure\\") CRITICAL = (4, \\"Critical message indicating severe failure\\") def __new__(cls, severity, description): obj = object.__new__(cls) obj._value_ = auto() obj.severity = severity obj.description = description return obj def __init__(self, severity, description): self._severity = severity self._description = description def __lt__(self, other): if isinstance(other, LogType): return self.severity < other.severity return NotImplemented def __le__(self, other): if isinstance(other, LogType): return self.severity <= other.severity return NotImplemented def __gt__(self, other): if isinstance(other, LogType): return self.severity > other.severity return NotImplemented def __ge__(self, other): if isinstance(other, LogType): return self.severity >= other.severity return NotImplemented def describe(self): return f\\"{self.name}: {self.description}\\""},{"question":"# FTP Directory Upload Automation Objective: Develop a Python function that automates the process of uploading a directory of files from the local machine to a specified directory on an FTP server. Function Signature: ```python def upload_directory_to_ftp(ftp_host: str, ftp_user: str, ftp_passwd: str, local_dir: str, remote_dir: str) -> None: pass ``` Detailed Requirements: 1. **Function Input:** - `ftp_host` (str): The hostname of the FTP server. - `ftp_user` (str): The username to log in to the FTP server. - `ftp_passwd` (str): The password to log in to the FTP server. - `local_dir` (str): The local directory containing files to be uploaded. - `remote_dir` (str): The target directory on the FTP server where files will be uploaded. 2. **Function Output:** - The function should not return anything. It should perform the task of uploading the files. 3. **Behavior:** - Connect to the given FTP server using the provided hostname, username, and password. - Change the current working directory on the FTP server to the specified `remote_dir`. If the directory does not exist, create it. - Upload each file in the `local_dir` to the `remote_dir` on the FTP server. Only files should be uploaded, not subdirectories. - Ensure the FTP session is properly closed after the process completion or if an error occurs. 4. **Constraints:** - Use binary transfer mode for file uploads. - Handle FTP errors gracefully. If an error occurs during FTP operations, print an appropriate error message and ensure the connection is closed properly. 5. **Example Usage:** ```python # Assuming the user has a local directory \'/path/to/local/dir\' with files and wants to upload to \'uploads\' directory on the FTP server: upload_directory_to_ftp(\'ftp.example.com\', \'username\', \'password\', \'/path/to/local/dir\', \'uploads\') ``` Additional Notes: - You may use the `os` or `pathlib` module to list files in the local directory. - Ensure proper exception handling for file operations and FTP operations. - Make sure the function is documented, including any error handling strategies.","solution":"import os from ftplib import FTP, error_perm def upload_directory_to_ftp(ftp_host: str, ftp_user: str, ftp_passwd: str, local_dir: str, remote_dir: str) -> None: Uploads files from a local directory to a specified directory on an FTP server. Args: ftp_host (str): The hostname of the FTP server. ftp_user (str): The username for the FTP server. ftp_passwd (str): The password for the FTP server. local_dir (str): Path to the local directory containing files to upload. remote_dir (str): Path to the target directory on the FTP server. Returns: None ftp = FTP() try: # Connect to the FTP server ftp.connect(ftp_host) ftp.login(user=ftp_user, passwd=ftp_passwd) # Change to the specified directory on the FTP server (or create it if it doesn\'t exist) try: ftp.cwd(remote_dir) except error_perm: # Create the directory since it does not exist ftp.mkd(remote_dir) ftp.cwd(remote_dir) # List all files in the local directory for filename in os.listdir(local_dir): local_file_path = os.path.join(local_dir, filename) # Only upload files, not subdirectories if os.path.isfile(local_file_path): with open(local_file_path, \'rb\') as file: ftp.storbinary(f\'STOR {filename}\', file) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Ensure the FTP connection is closed if ftp: ftp.quit()"},{"question":"**Objective:** You are required to implement a function that processes an array of integers using various functionalities provided by the `array` module in Python. Specifically, you will create an array, perform several manipulations, and return both the modified array and some computed statistics about it. **Specifications and Instructions:** 1. **Function Definition:** ```python def process_integer_array(init_list: list, actions: list) -> dict: Initializes an array from the given list of integers and performs the specified actions. Parameters: init_list (list): A list of integers to initialize the array. actions (list): A list of tuples where each tuple represents an action to perform on the array. Each tuple consists of an action string and necessary parameters. The action string can be \'append\', \'extend\', \'insert\', \'remove\', \'reverse\'. Returns: dict: A dictionary with the modified array (as a list) under the key \'array\' and statistics like count and index of a specific value under \'stats\'. ``` 2. **Initialization:** - Initialize an `array` of signed integers (\'i\') from `init_list`. 3. **Actions:** - Each element in the `actions` list is a tuple where the first element is the action to be performed and the subsequent elements are the parameters for that action. - The possible actions are: - `\'append\'`: Add a value to the end of the array. - `\'extend\'`: Extend the array with another list of integers. - `\'insert\'`: Insert an integer at a specified position (index). - `\'remove\'`: Remove the first occurrence of a specified integer. - `\'reverse\'`: Reverse the items in the array. 4. **Statistics:** - After performing all actions, calculate and include in the return value: - The number of occurrences of `7` in the array (key: `\'count_7\'`). - The index of the first occurrence of `7` in the array if it exists, otherwise `-1` (key: `\'index_7\'`). 5. **Return Format:** - The function should return a dictionary with: - The final array converted to a list under the key `\'array\'`. - The computed statistics under the key `\'stats\'`. **Example Usage:** ```python result = process_integer_array([1, 2, 3], [(\'append\', 4), (\'extend\', [5, 6, 7]), (\'insert\', 2, 7), (\'remove\', 3), (\'reverse\', )]) assert result == { \'array\': [7, 6, 5, 4, 7, 2, 1], \'stats\': {\'count_7\': 2, \'index_7\': 0} } ``` Constraints: - The `init_list` will have a length of at most `1000`. - The `actions` list will have at most `100` actions. - Each action will be valid and follow the specified format. Your implementation should efficiently handle the operations without any unnecessary computations. **Notes:** - Use the `array` module from Python\'s standard library. - Ensure proper handling of any edge cases, such as removing elements that do not exist or inserting at out-of-bounds indices.","solution":"import array def process_integer_array(init_list: list, actions: list) -> dict: arr = array.array(\'i\', init_list) for action in actions: if action[0] == \'append\': arr.append(action[1]) elif action[0] == \'extend\': arr.extend(action[1]) elif action[0] == \'insert\': arr.insert(action[1], action[2]) elif action[0] == \'remove\': arr.remove(action[1]) elif action[0] == \'reverse\': arr.reverse() count_7 = arr.count(7) try: index_7 = arr.index(7) except ValueError: index_7 = -1 return { \'array\': list(arr), \'stats\': { \'count_7\': count_7, \'index_7\': index_7 } }"},{"question":"# Advanced Python Coding Assessment: Concurrent Task Management Using the shared documentation on the \\"concurrent.futures\\" module, solve the following problem that assesses your understanding of concurrent task management in Python. Problem Statement: You are required to implement a function `process_urls(urls: List[str], timeout: int) -> List[Tuple[str, bool]]` that takes a list of URLs and a timeout value in seconds. This function should attempt to fetch each URL\'s content concurrently and return a list of tuples containing the URL and a boolean stating whether the fetch operation was successful within the given timeout period. Use the `ThreadPoolExecutor` class from the `concurrent.futures` module to manage concurrent fetching of the URLs. Requirements: 1. Each URL fetching should be performed concurrently. 2. Each fetch operation should respect the provided timeout. 3. If a URL cannot be fetched within the timeout or raises an exception, it should be marked as unsuccessful. 4. The function should return a list of tuples with each tuple containing the URL and a boolean indicating the success (True) or failure (False) of the fetch operation. Input and Output: - **Input**: - `urls` (List[str]): A list of URLs to fetch. - `timeout` (int): The maximum time in seconds to wait for the fetch operation for each URL. - **Output**: - List[Tuple[str, bool]]: A list of tuples, each containing a URL and a boolean indicating the success of the fetching operation. Constraints: - Use the `concurrent.futures` module for concurrency. - Handle potential exceptions and timeouts properly. Example: ```python urls = [\\"http://www.example.com\\", \\"http://nonexistent.url\\"] timeout = 5 result = process_urls(urls, timeout) print(result) # Output: [(\'http://www.example.com\', True), (\'http://nonexistent.url\', False)] ``` Solution Template: ```python from typing import List, Tuple import concurrent.futures import urllib.request def fetch_url(url: str, timeout: int) -> bool: try: with urllib.request.urlopen(url, timeout=timeout) as response: return response.status == 200 except Exception: return False def process_urls(urls: List[str], timeout: int) -> List[Tuple[str, bool]]: results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=len(urls)) as executor: futures = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(futures): url = futures[future] try: success = future.result() except Exception: success = False results.append((url, success)) return results ``` Implement the above function and verify its correctness. Ensure it handles concurrency effectively and respects the provided timeout.","solution":"from typing import List, Tuple import concurrent.futures import urllib.request def fetch_url(url: str, timeout: int) -> bool: try: with urllib.request.urlopen(url, timeout=timeout) as response: return response.status == 200 except Exception: return False def process_urls(urls: List[str], timeout: int) -> List[Tuple[str, bool]]: results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=len(urls)) as executor: futures = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(futures): url = futures[future] try: success = future.result() except Exception: success = False results.append((url, success)) return results"},{"question":"Problem Statement You are provided with a CSV file containing sales data of a retail store with the following columns: - `Date`: The date of the transaction. - `Product`: The name of the sold product. - `Category`: The product category. - `Quantity`: The number of items sold. - `Price`: The price per item. - `Total`: The total sale amount. # Part 1: Data Preparation 1. **Read the data**: Write a function `read_data(file_path)` that reads the CSV file into a pandas DataFrame. 2. **Clean the data**: Write a function `clean_data(df)` that processes the following: - Ensure there are no missing values. Fill any missing numeric values with `0` and missing categorical values with `\\"Unknown\\"`. - Ensure the `Date` column is in datetime format. # Part 2: Data Analysis 3. **Total Sales per Category per Month**: Write a function `monthly_sales(df)` that returns a DataFrame with each row representing a unique month and each column representing a unique category. The values should represent the total sales for that category in that particular month. 4. **Top Selling Product**: Write a function `top_selling_product(df)` that returns the name of the product with the highest total sales. 5. **Outlier Detection**: Write a function `detect_outliers(df)` that identifies outliers in the `Total` sales amount using the IQR (Interquartile Range) method. Return a DataFrame containing only the outlier rows. # Input and Output Specifications - **Input**: The function `read_data(file_path)` will take a single argument: - `file_path` (str): The file path to the CSV data file. - **Output**: - `read_data`: Returns a pandas DataFrame. - `clean_data`: Returns a cleaned pandas DataFrame. - `monthly_sales`: Returns a pandas DataFrame with monthly sales per category. - `top_selling_product`: Returns a string with the product name. - `detect_outliers`: Returns a pandas DataFrame containing outliers. # Constraints - You can assume that the file is small enough to fit into memory. # Example Assume the CSV file `sales_data.csv` looks like this: ``` Date,Product,Category,Quantity,Price,Total 2023-01-05,Widget A,Widgets,10,20,200 2023-01-15,Widget B,Widgets,5,15,75 2023-02-14,Gadget A,Gadgets,8,25,200 ... ``` Functions should be able to handle this data and process it as described. Remember to demonstrate usage of pandas operations such as `read_csv`, data cleaning (e.g., dealing with missing values), datetime manipulations, groupby operations, and statistical functions in your solutions.","solution":"import pandas as pd def read_data(file_path): Reads the CSV file into a pandas DataFrame. Args: file_path (str): The file path to the CSV data file. Returns: pd.DataFrame: DataFrame containing the sales data. return pd.read_csv(file_path) def clean_data(df): Cleans the input DataFrame by filling missing values and ensuring date format. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Cleaned DataFrame. # Fill missing numeric values with 0 and categorical values with \\"Unknown\\" df.fillna({\'Quantity\': 0, \'Price\': 0, \'Total\': 0, \'Product\': \'Unknown\', \'Category\': \'Unknown\'}, inplace=True) # Convert \'Date\' column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\'], errors=\'coerce\') return df def monthly_sales(df): Returns a DataFrame with each row representing a unique month and each column representing a unique category with the total sales for that category in that particular month. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with monthly sales per category. df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_sales_df = df.pivot_table( values=\'Total\', index=\'Month\', columns=\'Category\', aggfunc=\'sum\', fill_value=0 ).reset_index() return monthly_sales_df def top_selling_product(df): Returns the name of the product with the highest total sales. Args: df (pd.DataFrame): Input DataFrame. Returns: str: Name of the top selling product. top_product = df.groupby(\'Product\')[\'Total\'].sum().idxmax() return top_product def detect_outliers(df): Identifies outliers in the \'Total\' sales amount using the IQR method. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame containing only the outlier rows. Q1 = df[\'Total\'].quantile(0.25) Q3 = df[\'Total\'].quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR outliers_df = df[(df[\'Total\'] < lower_bound) | (df[\'Total\'] > upper_bound)] return outliers_df"},{"question":"Implementing and Evaluating Isotonic Regression **Objective**: Demonstrate your understanding of isotonic regression using the `IsotonicRegression` class from scikit-learn. Implement a function that fits an isotonic regression model to given data, makes predictions, and evaluates the model. # Problem Statement: You are provided with a dataset containing a set of samples with their corresponding features and target values. Your task is to implement a function `fit_and_evaluate_isotonic_regression(X, y, X_test)` that: 1. Fits an isotonic regression model to the training data `(X, y)`. 2. Makes predictions on both the training data `(X)` and the test data `(X_test)`. 3. Returns the mean squared error of the predictions on the training data and the predicted values for the test data. # Function Signature: ```python def fit_and_evaluate_isotonic_regression(X, y, X_test): Fit an isotonic regression model and evaluate it. Parameters: - X: list of floats, the training data features - y: list of floats, the training data target values - X_test: list of floats, the test data features Returns: - mse: float, mean squared error of predictions on the training data - y_test_pred: list of floats, predicted values for the test data ``` # Inputs: - `X`: A list of floats representing the feature values for the training data. - `y`: A list of floats representing the target values for the training data. - `X_test`: A list of floats representing the feature values for the test data. # Outputs: - `mse`: A float representing the mean squared error of predictions on the training data. - `y_test_pred`: A list of floats representing the predicted values for the test data. # Constraints: - Each element in `X`, `y`, `X_test` is a real number. - The length of `X` and `y` will be the same. - The function should handle edge cases, such as empty input lists or lists with a single element. # Example: ```python X = [1, 2, 3, 4, 5] y = [1, 3, 2, 5, 4] X_test = [1.5, 2.5, 3.5] # Call the function mse, y_test_pred = fit_and_evaluate_isotonic_regression(X, y, X_test) print(mse) # Expected output: a float value representing the MSE on the training data print(y_test_pred) # Expected output: a list of predicted values for X_test ``` # Notes: 1. Use the `IsotonicRegression` class from `sklearn.isotonic`. 2. Implement error handling for edge cases. 3. Ensure that your code is efficient and runs within a reasonable timeframe. # Additional Information: Refer to the `scikit-learn` documentation for details on `IsotonicRegression` and its usage.","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error import numpy as np def fit_and_evaluate_isotonic_regression(X, y, X_test): Fit an isotonic regression model and evaluate it. Parameters: - X: list of floats, the training data features - y: list of floats, the training data target values - X_test: list of floats, the test data features Returns: - mse: float, mean squared error of predictions on the training data - y_test_pred: list of floats, predicted values for the test data if not X or not y: raise ValueError(\\"Training data (X and y) cannot be empty\\") if len(X) != len(y): raise ValueError(\\"The length of X and y must be the same\\") # Fit the isotonic regression model ir = IsotonicRegression() y_pred = ir.fit_transform(X, y) # Calculate the mean squared error on the training data mse = mean_squared_error(y, y_pred) # Predict values for the test data X_test_sorted = np.argsort(X_test) y_test_pred_sorted = ir.transform([X_test[i] for i in X_test_sorted]) y_test_pred = [0] * len(X_test) for index, value in zip(X_test_sorted, y_test_pred_sorted): y_test_pred[index] = value return mse, y_test_pred"},{"question":"Objective Implement a function to train a simple neural network model using the new `torch.func` utilities to compute gradients and update parameters. Background You are required to use the new `torch.func.functional_call` and `torch.func.grad` utilities to train a simple linear regression model. The model will predict outputs based on inputs using a torch.nn.Linear layer. Problem Statement Write a function `train_linear_model` that: 1. Creates a simple linear model using `torch.nn.Linear`. 2. Initializes the model parameters. 3. Uses the `torch.func` utilities to compute gradients of the loss with respect to the parameters. 4. Updates the model parameters using these gradients. Function Signature ```python def train_linear_model(num_epochs: int, learning_rate: float) -> torch.nn.Linear: pass ``` Input - `num_epochs`: an integer representing the number of training epochs. - `learning_rate`: a float representing the learning rate for gradient descent. Output - Returns the trained `torch.nn.Linear` model. Constraints - Use `torch.func.functional_call` to compute the model\'s predictions. - Use `torch.func.grad` to compute gradients of the mean squared error loss. - Only use PyTorch and `torch.func` utilities provided in PyTorch version 2.0 or later. - Do not use any other external libraries. Example ```python model = train_linear_model(num_epochs=100, learning_rate=0.01) inputs = torch.tensor([[2.0, 3.0]]) predictions = model(inputs) print(predictions) ``` In this example, `model` is trained for 100 epochs with a learning rate of 0.01, and it is then used to make a prediction on new input data. Notes 1. You can initialize the model parameters randomly. 2. For simplicity, you can use a fixed dataset comprising random input and target values for training. 3. Ensure efficient memory usage by handling parameters properly. Use the following template for your implementation: ```python import torch from torch import nn from torch.func import functional_call, grad def train_linear_model(num_epochs: int, learning_rate: float) -> nn.Linear: # Step 1: Initialize model and dataset inputs = torch.randn(64, 3) targets = torch.randn(64, 3) model = nn.Linear(3, 3) # Step 2: Extract parameters params = dict(model.named_parameters()) # Step 3: Define loss function def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return nn.functional.mse_loss(prediction, targets) # Step 4: Training loop for epoch in range(num_epochs): # Compute gradients grads = grad(compute_loss)(params, inputs, targets) # Update parameters with torch.no_grad(): for name, param in params.items(): param -= learning_rate * grads[name] return model ``` Implement the function `train_linear_model` based on the provided template and guidelines.","solution":"import torch from torch import nn from torch.func import functional_call, grad def train_linear_model(num_epochs: int, learning_rate: float) -> nn.Linear: # Step 1: Initialize model and dataset inputs = torch.randn(64, 3) targets = torch.randn(64, 1) model = nn.Linear(3, 1) # Step 2: Extract parameters params = {name: param for name, param in model.named_parameters()} # Step 3: Define loss function def compute_loss(params, inputs, targets): prediction = functional_call(model, params, inputs) return nn.functional.mse_loss(prediction, targets) # Step 4: Training loop for epoch in range(num_epochs): # Compute gradients grads = grad(compute_loss)(params, inputs, targets) # Update parameters with torch.no_grad(): for name, param in params.items(): param -= learning_rate * grads[name] return model"},{"question":"**Coding Assessment Question** # Objective To assess your understanding and ability to use the `torch.fx` module in PyTorch for graph manipulation and transformations. # Problem Statement You are required to transform a given neural network by modifying its graph using `torch.fx`. Specifically, you need to replace all instances of the `torch.mul` function with the `torch.add` function. # Task 1. Define a neural network module `SampleModule` containing a forward pass with multiple `torch.mul` operations. 2. Write a function `replace_mul_with_add` that: - Takes a `torch.nn.Module` as input. - Uses `torch.fx` to obtain the graph of the module. - Replaces every `torch.mul` operation in the graph with `torch.add`. - Returns a new `torch.nn.Module` with the transformed graph. 3. Verify that the transformation is correct by initializing `SampleModule`, performing the transformation, and then comparing the outputs of the original and transformed modules using a sample input. # Input - A sample neural network module with the `torch.mul` operations. - A fixed input tensor with which to verify correctness. # Output - The new transformed `torch.nn.Module` where all `torch.mul` operations are replaced with `torch.add`. # Constraints - You may assume the input tensor and module structure will always allow for a valid and meaningful transformation. # Example ```python import torch import torch.fx class SampleModule(torch.nn.Module): def __init__(self): super(SampleModule, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x): y = self.linear(x) z = torch.mul(y, y) return torch.mul(z, 2) def replace_mul_with_add(module): # Your implementation here # Initialize the module and create the test input module = SampleModule() input_tensor = torch.randn(1, 4) # Perform the transformation transformed_module = replace_mul_with_add(module) # Verify the outputs original_output = module(input_tensor) transformed_output = transformed_module(input_tensor) # The output should not match because mul was replaced with add. print(\\"Original Output:\\", original_output) print(\\"Transformed Output:\\", transformed_output) ``` # Notes - Use `torch.allclose` to compare tensors for equality with a tolerance. - Ensure that the new graph in the transformed module is well-formed by using `graph.lint()` as a part of your transformation function.","solution":"import torch import torch.fx class SampleModule(torch.nn.Module): def __init__(self): super(SampleModule, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x): y = self.linear(x) z = torch.mul(y, y) return torch.mul(z, 2) def replace_mul_with_add(module): # Create a symbolic trace of the module traced = torch.fx.symbolic_trace(module) class ReplaceMulWithAdd(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.mul: return self.call_function(torch.add, args, kwargs) return super().call_function(target, args, kwargs) transformed = ReplaceMulWithAdd(traced).transform() # Ensure the transformed graph is well-formed transformed.graph.lint() # Generate a new module from the transformed graph new_module = torch.fx.GraphModule(module, transformed.graph) return new_module"},{"question":"**Kernel Density Estimation with Custom Kernel Implementation** In this exercise, you are required to implement a Kernel Density Estimation (KDE) for one-dimensional data using scikit-learn’s `KernelDensity` class but with a twist—you will implement a custom kernel function. This will help you understand the inner workings of kernel functions and how they affect density estimation. # Task: 1. Implement a custom kernel function named `custom_kernel` with the following properties: - The function takes a single argument, `distance`, representing the distance from the kernel center. - The function should return a density value calculated as follows: [ K(x) = begin{cases} (1 - |x|) & text{if } |x| leq 1 0 & text{if } |x| > 1 end{cases} ] - This kernel is known as the triangular kernel. 2. Next, you will use scikit-learn’s `KernelDensity` class but instruct it to use your custom kernel function. 3. Generate a sample dataset of 100 one-dimensional data points from a bimodal distribution. 4. Fit the KDE model to this dataset using a bandwidth of 0.5. 5. Evaluate the density estimate for a set of 100 evenly spaced points between -5 and 5. 6. Plot the KDE with your custom kernel together with the histogram of the data to visualize the estimated densities. # Instructions: 1. **Custom Kernel Implementation:** ```python def custom_kernel(distance): if abs(distance) <= 1: return 1 - abs(distance) else: return 0 ``` 2. **KDE Implementation:** ```python from sklearn.neighbors import KernelDensity import numpy as np import matplotlib.pyplot as plt # Generate sample data np.random.seed(0) X = np.concatenate([np.random.normal(loc=-2.0, scale=0.5, size=50), np.random.normal(loc=2.0, scale=0.5, size=50)])[:, np.newaxis] # Create a custom KernelDensity class class CustomKernelDensity(KernelDensity): def _compute_weights(self, distances): return np.apply_along_axis(custom_kernel, 0, distances) # Fit the model kde = CustomKernelDensity(bandwidth=0.5, kernel=\'custom\') kde.fit(X) # Evaluate the density on a grid of points X_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] log_dens = kde.score_samples(X_plot) dens = np.exp(log_dens) # Plot the data and the density estimate plt.hist(X, bins=30, density=True, alpha=0.5) plt.plot(X_plot[:, 0], dens, label=\'KDE with custom kernel\') plt.legend() plt.xlabel(\'X\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimation with Custom Kernel\') plt.show() ``` # Constraints: - The custom kernel function must be correctly implemented as specified above. - Fit the KDE model using the custom kernel. - Bandwidth parameter must be set to 0.5. # Expected Output: - The implementation should produce a plot that includes the histogram of the data and the KDE curve estimated using the custom kernel. # Evaluation Criteria: - Correct implementation of the custom kernel function. - Proper usage of scikit-learn’s `KernelDensity` class and modification to integrate the custom kernel. - Accurate fitting of the KDE model and evaluation of density estimates. - Clear and correct visualization of results in a plot.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def custom_kernel(distance): Custom triangular kernel function. if abs(distance) <= 1: return 1 - abs(distance) else: return 0 # Extending KernelDensity with custom kernel support class CustomKernelDensity(KernelDensity): def __init__(self, bandwidth=1.0): super().__init__(bandwidth=bandwidth, metric=\'euclidean\') def score_samples(self, X): K = np.vectorize(custom_kernel) X = np.atleast_2d(X) sample_scores = np.zeros(X.shape[0]) for i, x in enumerate(X): distance = np.linalg.norm(self.tree_.data - x, axis=-1) / self.bandwidth sample_scores[i] = np.mean(K(distance)) / self.bandwidth log_density = np.log(sample_scores) return log_density # Generate sample data from a bimodal distribution np.random.seed(0) X = np.concatenate([np.random.normal(loc=-2.0, scale=0.5, size=50), np.random.normal(loc=2.0, scale=0.5, size=50)])[:, np.newaxis] # Fit the custom KDE model kde = CustomKernelDensity(bandwidth=0.5) kde.fit(X) # Evaluate the density on a grid of points X_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] log_dens = kde.score_samples(X_plot) dens = np.exp(log_dens) # Plot the data and the density estimate plt.hist(X, bins=30, density=True, alpha=0.5, color=\'gray\') plt.plot(X_plot[:, 0], dens, label=\'Custom KDE\', color=\'blue\') plt.legend() plt.xlabel(\'X\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimation with Custom Kernel\') plt.show()"},{"question":"Objective: You are required to demonstrate your understanding of Seaborn\'s plotting capabilities by creating a complex visual analysis using the `seaborn.objects` interface. Task: Given the `penguins` dataset from Seaborn, create a comprehensive multi-faceted grid of plots that shows the relationship between the penguins\' body mass and bill dimensions, categorized by species. The plots should include error bars for body mass and bill length. Requirements: 1. Load the `penguins` dataset using Seaborn\'s `load_dataset` function. 2. Create a plot with the following specifications: - Show the relationship between `body_mass_g` and `bill_length_mm`, categorized by `species`. - Use different colors to represent different islands. - Facet the plot by `species`. - Add scatter points for each penguin\'s body mass vs bill length. - Add error bars for body mass and bill length. Use standard deviation (`sd`) for the error bars. - Customize the plot by setting different sizes for the points and line widths for the error bars to make the visual representation clear. 3. Ensure the plot is well-labeled with appropriate axis labels and a title. Input: None (The dataset is to be loaded within the function). Output: A seaborn plot meeting the above specifications. Constraints: - Use only the seaborn package for plotting. - The code should be written in a Jupyter Notebook/Python script within a single function. Example: An example of how your plot should look is given below (though not covering all requirements): ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguins(): penguins = load_dataset(\\"penguins\\") ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"bill_length_mm\\", color=\\"island\\") .facet(\\"species\\") .add(so.Dots(pointsize=6)) .add(so.Range(linewidth=2), so.Est(errorbar=\\"sd\\")) ) import matplotlib.pyplot as plt plt.title(\\"Penguin body mass and bill length by species and island\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Bill Length (mm)\\") # Call the function to display the plot plot_penguins() ``` Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguins(): penguins = load_dataset(\\"penguins\\") ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"bill_length_mm\\", color=\\"island\\") .facet(\\"species\\") .add(so.Dots(pointsize=6)) .add(so.Range(linewidth=2), so.Est(errorbar=\\"sd\\")) ) plt.title(\\"Penguin Body Mass and Bill Length by Species and Island\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.show()"},{"question":"**Command-Line Option Parsing with `optparse`** # Objective The goal of this exercise is to design a Python script using the deprecated `optparse` module for parsing command-line options. You will create a command-line utility that processes various types of inputs and performs specified actions based on those inputs. # Task Write a Python script that processes commands and options to manage a hypothetical task list. The script should support the following functionalities: 1. **Add a task**: Add a new task to the list with a description and optional due date. 2. **List tasks**: Display all tasks, optionally filtering by due date. 3. **Remove a task**: Remove a task from the list by its index. 4. **Mark a task as complete**: Mark a specific task as completed by its index. # Requirements - Utilize the `optparse` module for argument parsing. - Implement the following command-line options: - Adding a task: `--add \\"Task Description\\" [--due YYYY-MM-DD]` - Listing tasks: `--list [--due YYYY-MM-DD]` - Removing a task: `--remove TASK_INDEX` - Marking a task as completed: `--complete TASK_INDEX` - The script should handle invalid inputs gracefully and provide helpful error messages. # Input and Output Formats - **Input**: Command-line options as described above. - **Output**: Text output to the console, demonstrating the results of the command executed (e.g., task added, list of tasks, task removed, etc.). # Constraints - The task list should be maintained in memory (no need for file or database storage). - Date format should be rigidly adhered to `YYYY-MM-DD` for any date inputs. # Example Usage ```shell python task_manager.py --add \\"Finish Python exercise\\" --due 2023-05-30 Task added: \\"Finish Python exercise\\" with due date 2023-05-30 python task_manager.py --list 1. \\"Finish Python exercise\\" (due: 2023-05-30) [Pending] python task_manager.py --complete 1 Task 1 marked as complete. python task_manager.py --list 1. \\"Finish Python exercise\\" (due: 2023-05-30) [Completed] python task_manager.py --remove 1 Task 1 removed. ``` # Performance Requirements - The script should handle a moderate number of tasks efficiently (e.g., up to a few hundred tasks in memory). Good luck and happy coding!","solution":"import optparse from datetime import datetime task_list = [] def add_task(description, due_date): task = {\\"description\\": description, \\"due_date\\": due_date, \\"completed\\": False} task_list.append(task) print(f\'Task added: \\"{description}\\" with due date {due_date}\' if due_date else f\'Task added: \\"{description}\\"\') def list_tasks(due_date=None): for index, task in enumerate(task_list, start=1): if due_date is None or task[\'due_date\'] == due_date: status = \\"Completed\\" if task[\'completed\'] else \\"Pending\\" due_date_info = f\\"(due: {task[\'due_date\']})\\" if task[\'due_date\'] else \\"(no due date)\\" print(f\'{index}. \\"{task[\\"description\\"]}\\" {due_date_info} [{status}]\') def remove_task(index): try: removed_task = task_list.pop(index - 1) print(f\'Task {index} removed.\') except IndexError: print(f\'Error: Task {index} does not exist.\') def complete_task(index): try: task_list[index - 1][\'completed\'] = True print(f\'Task {index} marked as complete.\') except IndexError: print(f\'Error: Task {index} does not exist.\') def parse_arguments(): parser = optparse.OptionParser() parser.add_option(\\"--add\\", dest=\\"add\\", help=\\"Add a new task with description\\") parser.add_option(\\"--due\\", dest=\\"due\\", help=\\"Due date for the task in YYYY-MM-DD format\\") parser.add_option(\\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all tasks\\") parser.add_option(\\"--remove\\", dest=\\"remove\\", type=\\"int\\", help=\\"Remove a task by its index\\") parser.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", help=\\"Mark a task as complete by its index\\") options, _ = parser.parse_args() return options def main(): options = parse_arguments() if options.add: if options.due: try: due_date = datetime.strptime(options.due, \\"%Y-%m-%d\\").date() except ValueError: print(\\"Error: Due date must be in YYYY-MM-DD format.\\") return else: due_date = None add_task(options.add, due_date) elif options.list: list_tasks(options.due) elif options.remove: remove_task(options.remove) elif options.complete: complete_task(options.complete) else: print(\\"No valid option provided. Use --help for usage details.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Seaborn Rugplot Customization and Integration** You are provided with the `tips` dataset, which contains details of the total bill, tips, and other related information from a restaurant. Your task is to perform a detailed visualization combining multiple Seaborn functionalities. Follow these steps to create your visualization: 1. **Scatter Plot and Rug Plot**: Create a scatter plot showing the relationship between `total_bill` and `tip`. Add a rug plot along both axes. 2. **Hue Mapping**: Modify the plot to use the `smoker` column as the hue to represent the smoking status (Yes/No) in different colors. 3. **Rug Plot Customization**: Customize the rug plot to: - Have a height of 0.05. - Use thinner lines (line width of 0.5). - Adjust the transparency (alpha) to 0.7. - Place the rug outside the axes (set `clip_on` to `False`). 4. **Combine KDE Plot**: Add a KDE plot over the scatter plot for the `total_bill` variable, with the rug plot visible in the background. 5. **Final Adjustments**: Ensure that your plots are properly labeled with `xlabel` as \\"Total Bill\\", `ylabel` as \\"Tip\\", and an appropriate title \\"Scatter Plot with Rug and KDE - Smoking Status\\". **Constraints:** - Ensure you use appropriate Seaborn functions to achieve the task. - The plot should be clear and legible with proper labels and title. - Use the `tips` dataset provided by Seaborn (`sns.load_dataset(\\"tips\\")`). **Input Format:** No input required as you will load the dataset within your function. **Output Format:** Your function should display the correctly formatted plot. **Function Signature:** ```python def visualize_tips_data(): # Your code here ``` **Solution Example:** ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot and rug plot with hue mapping sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", height=0.05, lw=0.5, alpha=0.7, clip_on=False) # Add KDE plot sns.kdeplot(data=tips, x=\\"total_bill\\") # Label and title the plot plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Scatter Plot with Rug and KDE - Smoking Status\\") # Display the plot plt.show() # Run the function to visualize the data visualize_tips_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot with hue mapping scatter = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\") # Add a customized rug plot along both axes sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"smoker\\", height=0.05, lw=0.5, alpha=0.7, clip_on=False) sns.rugplot(data=tips, y=\\"tip\\", hue=\\"smoker\\", height=0.05, lw=0.5, alpha=0.7, clip_on=False) # Add KDE plot for \'total_bill\' sns.kdeplot(data=tips, x=\\"total_bill\\") # Adding labels and title plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Scatter Plot with Rug and KDE - Smoking Status\\") # Show the plot plt.show() # Run the function to visualize the data visualize_tips_data()"},{"question":"**Objective:** Implement a Python program that processes command-line arguments to perform various file operations. The program should utilize the `argparse` module to handle the command-line arguments, ensuring correct implementation of required, optional, and mutually exclusive arguments. **Task:** Write a Python script named `file_manager.py` that accepts the following command-line arguments: 1. **Required Positional Argument:** - `filename`: The name of the file to operate on. This should be a string. 2. **Optional Arguments:** - `-r`, `--read`: If specified, the program should read the contents of the file and print it to the console. - `-w`, `--write`: Followed by the `content` to write into the file. If specified, the program should write the provided content into the file. - `-a`, `--append`: Followed by the `content` to append to the file. If specified, the program should append the provided content to the file. - `-d`, `--delete`: If specified, the program should delete the file. 3. **Mutually Exclusive Options:** - `--quiet`: Suppresses any informational messages (errors should still be shown). - `--verbose`: Shows detailed informational messages about the operations being performed. **Constraints:** - The `filename` must be provided. - Either `--read`, `--write` with content, `--append` with content, or `--delete` should be specified. - `--quiet` and `--verbose` should not be allowed to be used together. **Expected Behavior:** 1. If both `--quiet` and `--verbose` are specified, show an error message. 2. If none of `-r`, `-w`, `-a`, or `-d` are specified, show an error message. 3. When performing operations, print appropriate messages based on whether `--quiet` or `--verbose` is specified. 4. Handle exceptions (e.g., file not found, permission errors) gracefully. **Example Usage:** ```sh python3 file_manager.py sample.txt --read --verbose Reading contents of sample.txt: [contents of the file] python3 file_manager.py sample.txt --write \\"Hello, World!\\" --verbose Writing to sample.txt: Hello, World! python3 file_manager.py sample.txt --append \\" How are you?\\" --quiet python3 file_manager.py sample.txt --delete --verbose Deleting sample.txt ``` **Implementation Requirements:** - Use the `argparse` module to parse command-line arguments. - Implement functions to handle reading, writing, appending, and deleting files. - Ensure that mutually exclusive options are handled correctly. - Provide informative messages based on the verbosity flags. **Submission:** Submit your script `file_manager.py`. Ensure that it handles all specified requirements and edge cases.","solution":"import argparse import os def handle_read(filename, verbose=True): try: with open(filename, \'r\') as file: content = file.read() if verbose: print(\\"Reading contents of {}: n{}\\".format(filename, content)) else: print(content) except FileNotFoundError: print(\\"Error: File not found.\\") except PermissionError: print(\\"Error: Permission denied.\\") def handle_write(filename, content, verbose=True): try: with open(filename, \'w\') as file: file.write(content) if verbose: print(\\"Writing to {}: n{}\\".format(filename, content)) except PermissionError: print(\\"Error: Permission denied.\\") def handle_append(filename, content, verbose=True): try: with open(filename, \'a\') as file: file.write(content) if verbose: print(\\"Appending to {}: n{}\\".format(filename, content)) except FileNotFoundError: print(\\"Error: File not found.\\") except PermissionError: print(\\"Error: Permission denied.\\") def handle_delete(filename, verbose=True): try: os.remove(filename) if verbose: print(\\"Deleting {}\\".format(filename)) except FileNotFoundError: print(\\"Error: File not found.\\") except PermissionError: print(\\"Error: Permission denied.\\") def main(): parser = argparse.ArgumentParser(description=\\"File Manager\\") parser.add_argument(\'filename\', type=str, help=\'The name of the file to operate on\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'-r\', \'--read\', action=\'store_true\', help=\'Read the contents of the file\') group.add_argument(\'-w\', \'--write\', type=str, help=\'Write content to the file\') group.add_argument(\'-a\', \'--append\', type=str, help=\'Append content to the file\') group.add_argument(\'-d\', \'--delete\', action=\'store_true\', help=\'Delete the file\') verbose_group = parser.add_mutually_exclusive_group() verbose_group.add_argument(\'--quiet\', action=\'store_true\', help=\'Suppress informational messages\') verbose_group.add_argument(\'--verbose\', action=\'store_true\', help=\'Show detailed informational messages\') args = parser.parse_args() verbose = not args.quiet if args.read: handle_read(args.filename, verbose) elif args.write: handle_write(args.filename, args.write, verbose) elif args.append: handle_append(args.filename, args.append, verbose) elif args.delete: handle_delete(args.filename, verbose) if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question **Objective**: Implement a function to summarize metadata information for all packages installed in the current Python environment. # Problem Statement You are tasked with writing a Python function, `summarize_installed_packages()`, which uses the `importlib.metadata` module to retrieve and summarize metadata information for all packages installed in the current Python environment. # Function Signature ```python def summarize_installed_packages() -> str: Summarizes metadata information for all installed packages. Returns: str: A formatted string containing metadata summaries for all packages. ``` # Requirements 1. The function must identify all packages installed in the current Python environment. 2. For each package, gather the following metadata: - Package name - Version - Summary (from the package metadata) - List of files installed by the package - Package requirements 3. The function should return a formatted string with the collected information. The string should be human-readable, with each package\'s metadata clearly delineated. 4. Use the `importlib.metadata` module\'s functions to gather the required information: - `version()` - `metadata()` - `files()` - `requires()` # Expected Output Format - Each package\'s information should be separated by a line of dashes (`----------------`). - Each piece of metadata should be presented in a clear and concise manner. - Example format: ``` Package: wheel Version: 0.32.3 Summary: A built-package format for Python. Files: - util.py (size: 859 bytes) Requirements: - pytest (>=3.0.0) ; extra == \'test\' - pytest-cov ; extra == \'test\' ---------------- ``` # Constraints and Notes - Assume the function is executed in an environment where the `importlib.metadata` module is available. - Handle cases where certain metadata may not be present (e.g., summary, requirements). Use appropriate placeholders (e.g., \\"N/A\\"). - Ensure the function performs efficiently even if a large number of packages are installed. # Example Call ```python output = summarize_installed_packages() print(output) ``` # Tips - Use the `importlib.metadata.packages_distributions()` to get a mapping of top-level package names to distribution names. - Iterate through the distribution names to gather the required metadata. - Format the output string using Python\'s string formatting methods for clarity.","solution":"import importlib.metadata def summarize_installed_packages() -> str: Summarizes metadata information for all installed packages. Returns: str: A formatted string containing metadata summaries for all packages. package_summaries = [] for dist in importlib.metadata.distributions(): name = dist.metadata[\'Name\'] or \\"Unknown\\" version = dist.metadata[\'Version\'] or \\"Unknown\\" summary = dist.metadata.get(\'Summary\', \\"N/A\\") files = dist.files or [] file_list = \\"n - \\".join(map(str, files)) if files else \\"N/A\\" requirements = dist.requires or [] requirement_list = \\"n - \\".join(requirements) if requirements else \\"N/A\\" package_summary = (f\\"Package: {name}n\\" f\\"Version: {version}n\\" f\\"Summary: {summary}n\\" f\\"Files:n - {file_list}n\\" f\\"Requirements:n - {requirement_list}n\\" \\"----------------\\") package_summaries.append(package_summary) return \\"n\\".join(package_summaries)"},{"question":"**Question:** **Task: Implement a Subnet Mask Calculation and Network Analysis Tool** You need to implement a Python function called `analyze_subnet` that takes an IPv4 network address in CIDR notation (e.g., \'192.168.0.0/24\') and returns a dictionary containing the following information about that network: 1. `network_address`: The network address. 2. `netmask`: The subnet mask. 3. `broadcast_address`: The network\'s broadcast address. 4. `num_addresses`: The total number of addresses in the network, including the network address and the broadcast address. 5. `usable_addresses`: A list of all usable IP addresses in the network (excluding the network address and broadcast address). **Function Signature:** ```python def analyze_subnet(network_str: str) -> dict: pass ``` **Input:** - `network_str` (string): A string representing an IPv4 network in CIDR notation (e.g., \'192.168.0.0/24\'). **Output:** - A dictionary containing the network information as specified above. **Constraints:** - The input string will always be a valid CIDR notation. - The function should handle any valid IPv4 network. **Example:** ```python input_network = \'192.168.0.0/28\' result = analyze_subnet(input_network) # Expected output print(result) { \'network_address\': \'192.168.0.0\', \'netmask\': \'255.255.255.240\', \'broadcast_address\': \'192.168.0.15\', \'num_addresses\': 16, \'usable_addresses\': [ \'192.168.0.1\', \'192.168.0.2\', \'192.168.0.3\', \'192.168.0.4\', \'192.168.0.5\', \'192.168.0.6\', \'192.168.0.7\', \'192.168.0.8\', \'192.168.0.9\', \'192.168.0.10\', \'192.168.0.11\', \'192.168.0.12\', \'192.168.0.13\', \'192.168.0.14\' ] } ``` **Notes:** - Use the `ipaddress` module to handle the IP address and network calculations. - Ensure to validate and handle various edge cases, such as very small subnets (`/31` and `/32`). **Hint:** - Explore the methods available in the `IPv4Network` class to fetch details like `network_address`, `netmask`, `broadcast_address`, `hosts()` (for usable addresses), and `num_addresses`.","solution":"import ipaddress def analyze_subnet(network_str: str) -> dict: Analyzes an IPv4 network in CIDR notation and returns detailed information. Parameters: network_str (str): A string representing an IPv4 network in CIDR notation. Returns: dict: A dictionary containing network information including the network address, subnet mask, broadcast address, total number of addresses, and usable addresses. network = ipaddress.IPv4Network(network_str, strict=False) # Network address network_address = str(network.network_address) # Netmask netmask = str(network.netmask) # Broadcast address broadcast_address = str(network.broadcast_address) # Number of addresses num_addresses = network.num_addresses # Usable addresses usable_addresses = [] # Note: network.hosts() returns an iterator of usable hosts. # In case of very small subnets (/31, /32) there are no usable addresses. if network.prefixlen < 31: usable_addresses = [str(ip) for ip in network.hosts()] return { \'network_address\': network_address, \'netmask\': netmask, \'broadcast_address\': broadcast_address, \'num_addresses\': num_addresses, \'usable_addresses\': usable_addresses }"},{"question":"# Python Coding Assessment Question **Objective:** Create a class-based system to manage a library of books and their borrowing status. The system should allow adding new books, borrowing books, returning books, and listing all books with their current status. **Requirements:** 1. **Class Definitions:** - Create a base class `LibraryItem` with the following attributes and methods: - Attributes: - `title` (string): The title of the item. - `author` (string): The author of the item. - `year` (integer): The publication year of the item. - Methods: - `__str__()` method to return a string representation in the format \\"title by author (year)\\". - Create a class `Book` that inherits from `LibraryItem` and includes an additional attribute: - `borrowed` (boolean): Indicates whether the book is currently borrowed or not (default is `False`). - Methods: - `borrow()`: Sets the `borrowed` attribute to `True`. - `return_book()`: Sets the `borrowed` attribute to `False`. - Override the `__str__()` method to include the borrowing status in the format \\"title by author (year) - Borrowed\\" or \\"title by author (year) - Available\\". 2. **Library Management:** - Create a class `Library` with the following attributes and methods: - Attributes: - `catalog` (list): A list to store all the books in the library. - Methods: - `add_book(book)`: Adds a `Book` instance to the `catalog`. - `list_books()`: Prints all the books in the library with their current status. - `borrow_book(title)`: Finds the book by `title` and borrows it (sets `borrowed` to `True`). - `return_book(title)`: Finds the book by `title` and returns it (sets `borrowed` to `False`). **Constraints:** - Books in the library catalog have unique titles. - The `borrow_book(title)` and `return_book(title)` methods should handle cases where the book is not found in the catalog and print an appropriate message. **Example Usage:** ```python lib = Library() # Create books book1 = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) # Add books to the library lib.add_book(book1) lib.add_book(book2) # List all books lib.list_books() # Output: # The Catcher in the Rye by J.D. Salinger (1951) - Available # To Kill a Mockingbird by Harper Lee (1960) - Available # Borrow a book lib.borrow_book(\\"The Catcher in the Rye\\") # List all books again lib.list_books() # Output: # The Catcher in the Rye by J.D. Salinger (1951) - Borrowed # To Kill a Mockingbird by Harper Lee (1960) - Available # Return a book lib.return_book(\\"The Catcher in the Rye\\") # List all books again lib.list_books() # Output: # The Catcher in the Rye by J.D. Salinger (1951) - Available # To Kill a Mockingbird by Harper Lee (1960) - Available # Attempt to borrow a non-existent book lib.borrow_book(\\"1984\\") # Output: # Book titled \\"1984\\" not found in the catalog. ``` Implement the `LibraryItem`, `Book`, and `Library` classes to meet these requirements. **Expected Input and Output Formats:** - Input is managed through method calls on the `Library` class and should follow the method definitions provided. - Output is printed directly from the methods like `list_books`, `borrow_book`, and `return_book`. **Performance Requirements:** - The implementation should efficiently handle up to 1000 books in the catalog.","solution":"class LibraryItem: def __init__(self, title, author, year): self.title = title self.author = author self.year = year def __str__(self): return f\\"{self.title} by {self.author} ({self.year})\\" class Book(LibraryItem): def __init__(self, title, author, year): super().__init__(title, author, year) self.borrowed = False def borrow(self): self.borrowed = True def return_book(self): self.borrowed = False def __str__(self): status = \\"Borrowed\\" if self.borrowed else \\"Available\\" return f\\"{super().__str__()} - {status}\\" class Library: def __init__(self): self.catalog = [] def add_book(self, book): self.catalog.append(book) def list_books(self): for book in self.catalog: print(book) def borrow_book(self, title): for book in self.catalog: if book.title == title: if book.borrowed: print(f\\"Book titled \\"{title}\\" is already Borrowed.\\") else: book.borrow() print(f\\"Book titled \\"{title}\\" has been Borrowed.\\") return print(f\\"Book titled \\"{title}\\" not found in the catalog.\\") def return_book(self, title): for book in self.catalog: if book.title == title: if not book.borrowed: print(f\\"Book titled \\"{title}\\" was not borrowed.\\") else: book.return_book() print(f\\"Book titled \\"{title}\\" has been Returned.\\") return print(f\\"Book titled \\"{title}\\" not found in the catalog.\\")"},{"question":"**Objective**: Implement and test the functionality of the event scheduling module in Python using the `sched` scheduler class. # Question: You are required to create a mini-event scheduler that schedules various tasks at different times and priorities. The scheduler should demonstrate the following capabilities: 1. **Initialize a Scheduler**: - Create a scheduler instance using default time and delay functions (`time.monotonic` and `time.sleep`). 2. **Schedule Events**: - Schedule multiple events using both `enter` (relative) and `enterabs` (absolute) methods. - Events should include both positional and keyword arguments and have different priorities. 3. **Handle Event Execution**: - Implement a function (e.g., `execute_tasks()`) that runs all scheduled events. - Ensure proper handling and order of events by priority and timing using the `run` method. 4. **Cancel Events**: - Add functionality to cancel a specific event before it is executed. - The function should demonstrate how to cancel events and verify cancellation using the `scheduler.queue` attribute. # Constraints: - You must ensure that all functions handle potential exceptions and maintain the scheduler\'s consistent state. - You are not allowed to modify or access private members of the `sched.scheduler` class directly outside of its provided methods and attributes. # Input: - A list of events to schedule, each defined with the following attributes: ```python { \\"time\\": <absolute_time_or_delay>, \\"priority\\": <event_priority>, \\"action\\": <callable>, \\"argument\\": <tuple_of_arguments>, \\"kwargs\\": <dictionary_of_keyword_arguments>, \\"is_absolute\\": <True_if_absolute_otherwise_False> } ``` # Expected Functions: 1. `initialize_scheduler() -> sched.scheduler`: Initializes and returns the scheduler instance. 2. `schedule_event(scheduler: sched.scheduler, event: dict)`: Schedules a single event from the event dictionary. 3. `cancel_event(scheduler: sched.scheduler, event)`: Cancels the specified event. 4. `execute_tasks(scheduler: sched.scheduler)`: Executes all scheduled events in the queue. # Example Usage: ```python import sched, time def my_task(arg1, kwarg1=None): print(f\\"Executing my_task with arg1={arg1} and kwarg1={kwarg1}\\") # Initialize Scheduler scheduler = initialize_scheduler() # Schedule Events events = [ {\\"time\\": 5, \\"priority\\": 1, \\"action\\": my_task, \\"argument\\": (1,), \\"kwargs\\": {\\"kwarg1\\": \\"A\\"}, \\"is_absolute\\": False}, {\\"time\\": time.time() + 10, \\"priority\\": 2, \\"action\\": my_task, \\"argument\\": (2,), \\"kwargs\\": {\\"kwarg1\\": \\"B\\"}, \\"is_absolute\\": True}, # Add more events as needed ] for event in events: schedule_event(scheduler, event) # Optional: Cancel a specific event (for testing cancellation) # event_to_cancel = obtain_event_reference_somehow() # Assume you obtain a reference to an event # cancel_event(scheduler, event_to_cancel) # Execute all tasks execute_tasks(scheduler) ``` # Output: Print statements from the `my_task` function showing the scheduled tasks being executed in the correct order respecting their timings and priorities. # Evaluation Criteria: - Correct initialization and use of the scheduler. - Accurate scheduling of events both relatively and absolutely. - Proper handling and order of execution of events. - Correct cancellation of events and verification. - Exception handling and maintaining a consistent state of the scheduler.","solution":"import sched import time def initialize_scheduler(): Initializes and returns the scheduler instance using default time and delay functions. return sched.scheduler(time.monotonic, time.sleep) def schedule_event(scheduler, event): Schedules a single event to the scheduler. Args: scheduler (sched.scheduler): The scheduler instance. event (dict): The event dictionary containing the following keys: - time: absolute time (if is_absolute is True) or delay (if is_absolute is False) - priority: priority of the event - action: callable function to be executed - argument: tuple of positional arguments for the action - kwargs: dictionary of keyword arguments for the action - is_absolute: True if scheduling an absolute time, otherwise False if event[\'is_absolute\']: return scheduler.enterabs(event[\'time\'], event[\'priority\'], event[\'action\'], event[\'argument\'], event[\'kwargs\']) else: return scheduler.enter(event[\'time\'], event[\'priority\'], event[\'action\'], event[\'argument\'], event[\'kwargs\']) def cancel_event(scheduler, event): Cancels the specified event from the scheduler. Args: scheduler (sched.scheduler): The scheduler instance. event: The event previously scheduled (returned reference from scheduling). scheduler.cancel(event) def execute_tasks(scheduler): Executes all scheduled tasks in the scheduler queue. Args: scheduler (sched.scheduler): The scheduler instance. scheduler.run()"},{"question":"# Problem Description You are required to implement a Python program that uses the `signal` module to set up a signal handler for `SIGUSR1` and manage an interval timer using `signal.setitimer`. Your program should handle the `SIGUSR1` signal by printing a message indicating the signal was received, and it should use an interval timer to periodically terminate an ongoing, long-running task. # Function Definitions 1. **register_signal_handler()** - **Input**: None - **Output**: None - **Description**: This function should register a signal handler for the `SIGUSR1` signal. The signal handler should print the message \\"Received SIGUSR1\\". 2. **setup_interval_timer(interval)** - **Input**: - `interval` (float): Time in seconds for the interval timer. - **Output**: None - **Description**: This function should set up an interval timer that sends a `SIGALRM` signal after `interval` seconds and then every `interval` seconds. The signal handler for `SIGALRM` should terminate the long-running task and exit the program gracefully by printing \\"Terminating the task\\". 3. **long_running_task()** - **Input**: None - **Output**: None - **Description**: This function simulates a long-running task by repeatedly printing \\"Task running...\\" every second. It should be interrupted and terminated by the `SIGALRM` signal. 4. **main()** - **Input**: None - **Output**: None - **Description**: This function should: - Call `register_signal_handler()`. - Call `setup_interval_timer(5)`, setting an interval of 5 seconds. - Start the `long_running_task()`. # Additional Requirements - Ensure that all functions are called in the `main()` function. - Use the `signal.signal()`, `signal.setitimer()`, and `signal.alarm()` functions appropriately. # Example ```python if __name__ == \\"__main__\\": main() ``` Upon calling `main()`, the program should register handlers and start the long-running task. After every 5 seconds, the `SIGALRM` signal should be sent and terminate the task with an appropriate message. # Constraints - Assume the environment is Unix-based for signal availability. - You cannot use `SIGKILL` or `SIGSTOP` for the custom handlers. # Submission Submit your solution as a single Python file.","solution":"import signal import time import sys def handle_sigusr1(signum, frame): print(\\"Received SIGUSR1\\") def handle_sigalrm(signum, frame): print(\\"Terminating the task\\") sys.exit(0) def register_signal_handler(): signal.signal(signal.SIGUSR1, handle_sigusr1) def setup_interval_timer(interval): signal.signal(signal.SIGALRM, handle_sigalrm) signal.setitimer(signal.ITIMER_REAL, interval, interval) def long_running_task(): try: while True: print(\\"Task running...\\") time.sleep(1) except KeyboardInterrupt: print(\\"Program interrupted by user.\\") def main(): register_signal_handler() setup_interval_timer(5) long_running_task() if __name__ == \\"__main__\\": main()"},{"question":"# Question: Extracting and Formatting Text with Regular Expressions **Objective:** You are required to write a Python function that processes a block of text data representing a series of records. Each record contains certain information separated by different delimiters. Your task is to extract all the relevant pieces from each record and reformat them according to specific rules using regular expressions. **Problem Statement:** Write a function `process_records(data: str) -> List[str]` that processes a multiline string, where each line represents a record containing the following fields separated by specific delimiters: * `Name` (alphanumeric characters and spaces) * `Age` (integer) * `Email` (a valid email address) * `Phone` (a 10-digit number) The fields within each record are separated by a combination of delimiters: semicolons (`;`), spaces, or vertical bars (`|`). Your task is to extract these fields and return a list of strings formatted as: `\\"Name: [Name], Age: [Age], Email: [Email], Phone: [Phone]\\"`. **Constraints:** 1. A valid Name is any combination of alphanumeric characters (both upper and lower case) and spaces. It may include multiple words. 2. Age is a positive integer. 3. Email follows the format `<local_part>@<domain>` where: * `<local_part>` is a mix of alphanumeric characters and special characters (`_`, `.`). * `<domain>` has at least one period (`.`) separating the domain name and the TLD. 4. Phone number consists of exactly 10 digits. **Example:** ```python data = \\"John Doe; 30; john.doe@example.com; 1234567890nLisa Smith|25|lisa.smith@domain.com|0987654321\\" assert process_records(data) == [ \\"Name: John Doe, Age: 30, Email: john.doe@example.com, Phone: 1234567890\\", \\"Name: Lisa Smith, Age: 25, Email: lisa.smith@domain.com, Phone: 0987654321\\" ] ``` **Function Signature:** ```python def process_records(data: str) -> List[str]: pass ``` **Hints:** - Use the `re` module for constructing regular expressions to capture each record accurately. - Consider using named groups for better readability and maintainability of your regular expressions. - Handle cases where delimiters might be inconsistent or multiple delimiters might be present. Good luck, and remember to test your function with various cases to ensure it handles different input scenarios as expected.","solution":"import re from typing import List def process_records(data: str) -> List[str]: records = [] pattern = re.compile(r\'(?P<Name>[a-zA-Z0-9s]+)[;s|]+(?P<Age>d+)[;s|]+(?P<Email>[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]+)[;s|]+(?P<Phone>d{10})\') for line in data.split(\'n\'): match = pattern.match(line) if match: name = match.group(\'Name\') age = match.group(\'Age\') email = match.group(\'Email\') phone = match.group(\'Phone\') records.append(f\\"Name: {name}, Age: {age}, Email: {email}, Phone: {phone}\\") return records"},{"question":"Objective Your task is to implement a function that retrieves annotations for given objects while considering different Python behaviors as described in the provided documentation. Problem Statement Implement a function `safe_get_annotations(o: object, version: str) -> dict` that returns the annotations of the given object `o`. The function should handle differences in Python version behaviors (both `3.10` and newer, and `3.9` and older). Function Signature ```python def safe_get_annotations(o: object, version: str) -> dict: ``` Input - `o`: An object that can be a function, class, or module, from which you need to retrieve annotations. - `version`: A string indicating the Python version in the format \\"3.x\\", e.g., \\"3.10\\". Output - Returns a dictionary containing the annotations of the object. If the object does not have annotations, return an empty dictionary. Constraints - Assume the input `version` string will always be in a valid format (\\"3.10\\", \\"3.7\\", etc.). - If the annotations cannot be retrieved due to any reason (such as malformed objects), return an empty dictionary. Examples ```python # Example 1 (Python 3.10 and newer) class Base: a: int = 3 class Derived(Base): pass assert safe_get_annotations(Derived, \\"3.10\\") == {} # Example 2 (Python 3.9 and older) class Base: a: int = 3 class Derived(Base): pass assert safe_get_annotations(Derived, \\"3.9\\") == {\'a\': int} # Example 3 (Function with Annotations) def foo(a: int, b: str) -> None: pass assert safe_get_annotations(foo, \\"3.10\\") == {\'a\': int, \'b\': str, \'return\': None} assert safe_get_annotations(foo, \\"3.9\\") == {\'a\': int, \'b\': str, \'return\': None} # Example 4 (Module without Annotations) import math assert safe_get_annotations(math, \\"3.10\\") == {} assert safe_get_annotations(math, \\"3.9\\") == {} ``` # Notes: - Use `inspect.get_annotations()` for Python 3.10 and newer. - Manually handle the retrieval of annotations for Python 3.9 and older by checking if `o` is a class and using `o.__dict__.get(\'__annotations__\', None)`.","solution":"import inspect def safe_get_annotations(o: object, version: str) -> dict: Retrieve the annotations of the given object `o` according to the specified Python version behaviour. Parameters: - o: object (can be a function, class, or module) - version: str (indicating the Python version, e.g., \\"3.10\\") Returns: - dict : dictionary containing the annotations of the object. If no annotations are found, returns an empty dictionary. major, minor = map(int, version.split(\'.\')) if major < 3 or (major == 3 and minor < 0): return {} try: if major == 3 and minor >= 10: return inspect.get_annotations(o, eval_str=True) else: if isinstance(o, type): # if o is a class annotations = {} for base in o.__mro__: annotations.update(base.__dict__.get(\'__annotations__\', {})) return annotations elif callable(o): # if o is a function or method return o.__annotations__ else: return {} except: return {}"},{"question":"**Question: Classification of Iris Species using scikit-learn** In this question, you are required to classify the iris species using the Iris dataset, which is available in scikit-learn\'s toy datasets. You will need to perform data loading, pre-processing, model training, and evaluation using scikit-learn. # Instructions: 1. **Load the Dataset:** - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. 2. **Pre-process the Data:** - Split the dataset into training and testing sets. Use an 80-20 split ratio. - Standardize the feature values by removing the mean and scaling to unit variance. 3. **Train a Model:** - Train a `RandomForestClassifier` on the training data. Set the number of estimators to 100 and the random state to 42. 4. **Evaluate the Model:** - Predict the labels for the test set. - Calculate the accuracy of the model on the test set. - Generate a classification report that includes precision, recall, and F1-score for each class. # Implementation: Implement the following function: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report def iris_classification(): # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train the RandomForestClassifier classifier = RandomForestClassifier(n_estimators=100, random_state=42) classifier.fit(X_train, y_train) # Make predictions on the test set y_pred = classifier.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Generate classification report report = classification_report(y_test, y_pred, target_names=iris.target_names) return accuracy, report ``` # Expected Output: - The function should return the accuracy of the model and the classification report as a string. # Constraints: - Do not change the random state and number of estimators of the classifier. - Follow the steps in the specified order and ensure the data is standardized after splitting into train and test. # Hints: - Refer to the transformation functions in sklearn.preprocessing and model evaluation metrics in sklearn.metrics if needed. - Make sure to handle imports correctly within the function.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report def iris_classification(): # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train the RandomForestClassifier classifier = RandomForestClassifier(n_estimators=100, random_state=42) classifier.fit(X_train, y_train) # Make predictions on the test set y_pred = classifier.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Generate classification report report = classification_report(y_test, y_pred, target_names=iris.target_names) return accuracy, report"},{"question":"**Question: Web Crawler Permission Checker** You are tasked with creating a function that checks the permissions of a given web crawler (user agent) on a specified website by reading and interpreting the website\'s `robots.txt` file. **Function Signature:** ```python def check_robot_permissions(robots_url: str, useragent: str, urls: list[str]) -> dict[str, bool]: pass ``` **Inputs:** - `robots_url` (str): The URL pointing to the `robots.txt` file of the website. - `useragent` (str): The user agent string representing the web crawler. - `urls` (list[str]): A list of URLs (as strings) from the same website that need to be checked for access permissions. **Output:** - Returns a dictionary where the keys are the URLs from the input list and the values are booleans indicating whether the user agent is allowed to fetch the respective URL (`True` if allowed, `False` otherwise). **Constraints:** - The function should handle cases where the `robots.txt` file might be invalid or unreachable. - You are allowed to use the `urllib.robotparser.RobotFileParser` class as described in the documentation. **Example:** ```python robots_url = \\"http://www.example.com/robots.txt\\" useragent = \\"*\\" urls = [ \\"http://www.example.com/\\", \\"http://www.example.com/private/\\", \\"http://www.example.com/public/\\" ] result = check_robot_permissions(robots_url, useragent, urls) print(result) ``` **Expected Output:** ```python { \\"http://www.example.com/\\": True, \\"http://www.example.com/private/\\": False, \\"http://www.example.com/public/\\": True } ``` **Hints:** 1. Use the `RobotFileParser` class from the `urllib.robotparser` module to read and parse the `robots.txt` file. 2. Utilize the `can_fetch` method to check the permissions for each URL in the input list. 3. Handle any potential exceptions that might arise when reading or parsing the `robots.txt` file.","solution":"import urllib.robotparser def check_robot_permissions(robots_url: str, useragent: str, urls: list[str]) -> dict[str, bool]: Checks the permissions of a given web crawler (user agent) on a specified website by reading and interpreting the website\'s robots.txt file. Args: - robots_url (str): The URL pointing to the robots.txt file of the website. - useragent (str): The user agent string representing the web crawler. - urls (list[str]): A list of URLs from the same website to check for access permissions. Returns: - dict[str, bool]: Dictionary where the keys are the URLs and the values are booleans indicating if the user agent is allowed to fetch the respective URL. rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() except Exception as e: return {url: False for url in urls} # Assume disallowed if robots.txt is unreachable permissions = {url: rp.can_fetch(useragent, url) for url in urls} return permissions"},{"question":"# JSON Data Transformation and Custom Encoding/Decoding Objective Create a Python program that involves reading a JSON string, transforming the data in a specific way, and then outputting the transformed data as a JSON string using custom encoding and decoding methods. Requirements: 1. **Input:** - A JSON string representing multiple records of products. Each product is represented as a dictionary with the following keys: - `id` (string): Unique identifier for the product. - `name` (string): Name of the product. - `price` (float): Price of the product. - `quantity` (int): Quantity of the product. - `category` (string): Category to which the product belongs. 2. **Transformation:** - Implement a transformation that updates the `price` of each product by applying a given percentage increase. The percentage increase is provided as an argument to the function. 3. **Custom Encoding:** - Extend the `json.JSONEncoder` class to handle complex numbers which may appear in the data in any future developments. The complex numbers should be encoded as dictionaries with `real` and `imag` keys. 4. **Custom Decoding:** - Extend the `json.JSONDecoder` class to decode dictionaries with `real` and `imag` keys back into complex numbers. 5. **Output:** - A JSON string with the transformed product data, using the custom encoder for encoding. Constraints: - The percentage increase should be a non-negative float. - Ensure the input JSON is valid. Performance Requirements: - The solution should efficiently handle input JSON strings up to 10,000 products. Example: ```python import json # Example input JSON string input_json = \'\'\' [ {\\"id\\": \\"p001\\", \\"name\\": \\"Product 1\\", \\"price\\": 100.0, \\"quantity\\": 10, \\"category\\": \\"Category A\\"}, {\\"id\\": \\"p002\\", \\"name\\": \\"Product 2\\", \\"price\\": 150.0, \\"quantity\\": 5, \\"category\\": \\"Category B\\"} ] \'\'\' # Percentage increase percentage_increase = 10.0 # Implement the required functionality here def transform_products(input_json, percentage_increase): # Define your custom encoder and decoder within this function # Step 1: Decode the JSON string to Python objects # Step 2: Apply the transformation to update the prices # Step 3: Encode the transformed data back to a JSON string using the custom encoder pass # Call the function and print the result output_json = transform_products(input_json, percentage_increase) print(output_json) ``` Expected Output: ```json [ {\\"id\\": \\"p001\\", \\"name\\": \\"Product 1\\", \\"price\\": 110.0, \\"quantity\\": 10, \\"category\\": \\"Category A\\"}, {\\"id\\": \\"p002\\", \\"name\\": \\"Product 2\\", \\"price\\": 165.0, \\"quantity\\": 5, \\"category\\": \\"Category B\\"} ] ``` Additional Notes: - Clearly define and implement the custom `JSONEncoder` and `JSONDecoder` classes within your solution. - Make sure to include error handling for invalid JSON input. - Include comments explaining each part of your code for readability.","solution":"import json class CustomEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def custom_decoder(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def transform_products(input_json, percentage_increase): # Decode the JSON string to Python objects products = json.loads(input_json, object_hook=custom_decoder) # Apply the percentage increase to the price of each product for product in products: product[\\"price\\"] += product[\\"price\\"] * (percentage_increase / 100.0) # Encode the transformed data back to a JSON string using the custom encoder output_json = json.dumps(products, cls=CustomEncoder) return output_json"},{"question":"# Question: Dimensionality Reduction with PCA and Feature Scaling You are given a dataset with a high number of features. Your task is to reduce the dimensionality of this dataset using Principal Component Analysis (PCA) while ensuring that the features are appropriately scaled. You will implement a function that performs the following steps: 1. **Feature Scaling**: Scale the features using `StandardScaler`. 2. **PCA Transformation**: Apply PCA to reduce the dimensionality of the scaled features. Your function should be able to handle any input dataset and number of desired principal components. Input - `X` (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the input dataset. - `n_components` (`int`): The number of principal components to keep. Output - `X_reduced` (`numpy.ndarray`): A 2D array of shape `(n_samples, n_components)` representing the dataset reduced to `n_components` dimensions. Constraints - The number of features (`n_features`) in `X` is greater than or equal to `n_components`. - `n_components` is a positive integer less than or equal to `n_features`. Example ```python import numpy as np # Example data X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) # Number of components n_components = 2 # Expected Result # array([[-1.41421356, 0. ], # [ 0. , 0. ], # [ 1.41421356, 0. ]]) # Applying the function X_reduced = reduce_dimensionality(X, n_components) print(X_reduced) ``` Implementation ```python def reduce_dimensionality(X, n_components): from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Step 1: Feature scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: PCA transformation pca = PCA(n_components=n_components) X_reduced = pca.fit_transform(X_scaled) return X_reduced ``` Ensure your implementation is efficient and follows the constraints provided. You may refer to the scikit-learn documentation for further details on `StandardScaler` and `PCA`.","solution":"def reduce_dimensionality(X, n_components): from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Step 1: Feature scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: PCA transformation pca = PCA(n_components=n_components) X_reduced = pca.fit_transform(X_scaled) return X_reduced"},{"question":"Problem Statement You are tasked to implement a Python function that reads a large text file, compresses its content, and then verifies the integrity of the compressed data by computing and checking checksums. Write two functions: `compress_and_save_file` and `verify_compressed_file`. 1. **`compress_and_save_file(input_file_path: str, output_file_path: str, checksum_file_path: str) -> None`**: - Reads the content of the file at `input_file_path`. - Compresses the content using `zlib` with the highest level of compression. - Saves the compressed data to `output_file_path`. - Computes the Adler-32 checksum of the original content and saves it to `checksum_file_path`. 2. **`verify_compressed_file(input_file_path: str, checksum_file_path: str) -> bool`**: - Reads the compressed content from `input_file_path`. - Decompresses the content. - Computes the Adler-32 checksum of the decompressed content. - Reads the original checksum from `checksum_file_path`. - Returns `True` if the checksum of the decompressed content matches the original checksum; otherwise, returns `False`. Function Signatures ```python def compress_and_save_file(input_file_path: str, output_file_path: str, checksum_file_path: str) -> None: pass def verify_compressed_file(input_file_path: str, checksum_file_path: str) -> bool: pass ``` Constraints - You can assume that the input file fits into memory. - The data in the file is in plain text format. - You must handle any exceptions that may occur during file operations gracefully, logging appropriate error messages. Example Usage ```python # Compress and save file compress_and_save_file(\'example.txt\', \'example_compressed.z\', \'example_checksum.chk\') # Verify compressed file is_valid = verify_compressed_file(\'example_compressed.z\', \'example_checksum.chk\') print(is_valid) # Output should be True if the file is valid ``` Notes - You should use the `zlib.compress` and `zlib.decompress` functions for compression and decompression. - Use the `zlib.adler32` function to compute the checksums.","solution":"import zlib import logging logging.basicConfig(level=logging.ERROR) def compress_and_save_file(input_file_path: str, output_file_path: str, checksum_file_path: str) -> None: try: with open(input_file_path, \'rb\') as file: content = file.read() compressed_content = zlib.compress(content, zlib.Z_BEST_COMPRESSION) checksum = zlib.adler32(content) with open(output_file_path, \'wb\') as file: file.write(compressed_content) with open(checksum_file_path, \'w\') as file: file.write(str(checksum)) except Exception as e: logging.error(f\\"An error occurred: {e}\\") def verify_compressed_file(input_file_path: str, checksum_file_path: str) -> bool: try: with open(input_file_path, \'rb\') as file: compressed_content = file.read() decompressed_content = zlib.decompress(compressed_content) decompressed_checksum = zlib.adler32(decompressed_content) with open(checksum_file_path, \'r\') as file: original_checksum = int(file.read()) return decompressed_checksum == original_checksum except Exception as e: logging.error(f\\"An error occurred: {e}\\") return False"},{"question":"# XML Data Processing with `xml.dom.pulldom` Objective: Write a Python function `extract_large_items` that processes an XML string containing details of various items. Your function should extract all items with a price greater than a specified threshold and return their entire XML representation as a list of strings. Input Format: - An XML string `xml_data` which is a well-formed XML document. The document contains multiple `<item>` elements, each with a `price` attribute. - An integer `price_threshold` which sets the minimum price for items to be returned. Output Format: - A list of strings where each string is the XML representation of an `<item>` element that has a `price` attribute greater than `price_threshold`. Constraints: - Assume all `price` attributes are valid integers. - Node expansions should only be performed when an item\'s price exceeds the threshold to optimize performance. Example: ```python xml_data = \'\'\' <items> <item id=\\"1\\" price=\\"45\\">Item 1 Description</item> <item id=\\"2\\" price=\\"75\\">Item 2 Description</item> <item id=\\"3\\" price=\\"60\\">Item 3 Description</item> <item id=\\"4\\" price=\\"30\\">Item 4 Description</item> </items> \'\'\' price_threshold = 50 print(extract_large_items(xml_data, price_threshold)) # Output should be: # [\'<item id=\\"2\\" price=\\"75\\">Item 2 Description</item>\', \'<item id=\\"3\\" price=\\"60\\">Item 3 Description</item>\'] ``` Function Signature: ```python def extract_large_items(xml_data: str, price_threshold: int) -> list: # Your code here ``` Hints: - Use the `pulldom.parseString()` function to parse the XML data. - Iterate through the events and use `pulldom.START_ELEMENT` and `node.getAttribute(\'price\')` to check item prices. - Use the `doc.expandNode(node)` method to get the full XML representation of qualifying nodes.","solution":"from xml.dom import pulldom def extract_large_items(xml_data: str, price_threshold: int) -> list: Extracts items from the given XML data where the price attribute is greater than the specified threshold. Args: xml_data (str): The XML data as a string. price_threshold (int): The price threshold to filter items. Returns: list: A list of strings representing the items with price greater than the threshold. doc = pulldom.parseString(xml_data) large_items = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"item\\": price = int(node.getAttribute(\'price\')) if price > price_threshold: doc.expandNode(node) large_items.append(node.toxml()) return large_items"},{"question":"# Custom Type Creation Using Python C API You are required to create a custom type called `Person` using Python\'s C API and the provided type management functions. This type should store the name and age of a person and include methods to get and set these attributes. Additionally, implement a method to display the `Person` object details in a formatted string. Requirements: 1. Define a new heap-allocated type `Person`. 2. Include attributes `name` (string) and `age` (integer). 3. Provide methods: - `get_name(self)`: Returns the name of the person. - `set_name(self, name)`: Sets the name of the person. - `get_age(self)`: Returns the age of the person. - `set_age(self, age)`: Sets the age of the person. - `str(self)`: Returns a formatted string representation of the person. Constraints: - Make sure to handle memory allocation and deallocation properly. - Use `PyType_FromSpecWithBases` or equivalent to create the type dynamically. - Ensure that all required flags and slots are set correctly. Example Usage: ```python p = Person(\\"Alice\\", 30) print(p.get_name()) # Should print: Alice print(p.get_age()) # Should print: 30 p.set_name(\\"Bob\\") p.set_age(25) print(p) # Should print something like: Person(name=Bob, age=25) ``` # Implementation: Provide a detailed implementation using the Python C API, including: - Definition of the type spec (`PyType_Spec`). - Initialization and finalization of the type. - Implementation of the required methods and slots. - Module initialization to expose the new type to Python. - Any additional helper functions or macros needed. This question will assess your understanding of type creation, memory management, and the use of the Python C API for extending Python with custom types.","solution":"# The following Python code simulates the implementation of the Person type using the C API. # Due to the nature of C API and its complexity, we cannot execute it directly in this environment. # Instead, we present a detailed plan and structure as a Python simulation. class Person: def __init__(self, name, age): self.name = name self.age = age def get_name(self): return self.name def set_name(self, name): self.name = name def get_age(self): return self.age def set_age(self, age): self.age = age def __str__(self): return f\'Person(name={self.name}, age={self.age})\' # Note: Creating an actual custom type using Python C API involves steps such as defining the structure, # type methods, and slots, and cannot be fully demonstrated in pure Python code. # Actual implementation would be done in C using Python.h headers with the following overview: \'\'\' #include <Python.h> // Object structure typedef struct { PyObject_HEAD PyObject *name; // The name attribute (Python string) int age; // The age attribute (integer) } PersonObject; // Deallocation static void Person_dealloc(PersonObject *self) { Py_XDECREF(self->name); Py_TYPE(self)->tp_free((PyObject *) self); } // Initialization static int Person_init(PersonObject *self, PyObject *args, PyObject *kwds) { PyObject *name = NULL; int age; static char *kwlist[] = {\\"name\\", \\"age\\", NULL}; if (!PyArg_ParseTupleAndKeywords(args, kwds, \\"Oi\\", kwlist, &name, &age)) return -1; if (name) { Py_INCREF(name); self->name = name; } self->age = age; return 0; } // Getter for name static PyObject* Person_get_name(PersonObject* self, void* closure) { Py_INCREF(self->name); return self->name; } // Setter for name static int Person_set_name(PersonObject* self, PyObject* value, void* closure) { if (value == NULL) { PyErr_SetString(PyExc_TypeError, \\"Cannot delete the name attribute\\"); return -1; } if (!PyUnicode_Check(value)) { PyErr_SetString(PyExc_TypeError, \\"The name attribute value must be a string\\"); return -1; } Py_INCREF(value); Py_XDECREF(self->name); self->name = value; return 0; } // Getter for age static PyObject* Person_get_age(PersonObject* self, void* closure) { return PyLong_FromLong(self->age); } // Setter for age static int Person_set_age(PersonObject* self, PyObject* value, void* closure) { int age; if (value == NULL) { PyErr_SetString(PyExc_TypeError, \\"Cannot delete the age attribute\\"); return -1; } age = PyLong_AsLong(value); if (age == -1 && PyErr_Occurred()) { return -1; } self->age = age; return 0; } // String representation static PyObject* Person_str(PersonObject* self) { return PyUnicode_FromFormat(\\"Person(name=%S, age=%d)\\", self->name, self->age); } // Type definition static PyMethodDef Person_methods[] = { {\\"get_name\\", (PyCFunction)Person_get_name, METH_NOARGS, \\"Return the name of the person\\"}, {\\"set_name\\", (PyCFunction)Person_set_name, METH_O, \\"Set the name of the person\\"}, {\\"get_age\\", (PyCFunction)Person_get_age, METH_NOARGS, \\"Return the age of the person\\"}, {\\"set_age\\", (PyCFunction)Person_set_age, METH_O, \\"Set the age of the person\\"}, {NULL} /* Sentinel */ }; static PyTypeObject PersonType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"example.Person\\", .tp_basicsize = sizeof(PersonObject), .tp_itemsize = 0, .tp_dealloc = (destructor) Person_dealloc, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_doc = \\"Person objects\\", .tp_methods = Person_methods, .tp_init = (initproc) Person_init, .tp_new = PyType_GenericNew, .tp_str = (reprfunc) Person_str }; // Module definition static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, .m_name = \\"example\\", .m_doc = \\"Example module that creates a custom Person type.\\", .m_size = -1, }; // Module initialization PyMODINIT_FUNC PyInit_example(void) { PyObject *m; if (PyType_Ready(&PersonType) < 0) return NULL; m = PyModule_Create(&examplemodule); if (m == NULL) return NULL; Py_INCREF(&PersonType); if (PyModule_AddObject(m, \\"Person\\", (PyObject *) &PersonType) < 0) { Py_DECREF(&PersonType); Py_DECREF(m); return NULL; } return m; } \'\'\' # This C code must be compiled with Python headers and linked properly to create an extension module that can be imported in Python."},{"question":"**Question: Mastering Seaborn Objects Interface** **Objective:** Create multiple visualizations using the `seaborn.objects` interface to demonstrate your understanding of various features, including data transformation, layering, faceting, and customization. **Datasets:** - Use any publicly available dataset of your choice, appropriate for demonstrating the different features of the `seaborn.objects` interface. # Tasks: 1. **Basic Scatter Plot with Custom Properties** Create a scatter plot of two numerical variables in the dataset and customize the properties (e.g., color and size) of the points based on other variables. 2. **Faceted Histogram** - Create a histogram for a numerical variable, faceted by a categorical variable. - Customize the appearance by applying specific scales and themes. 3. **Statistical Transformation and Jittering** - Create a plot showing the mean and standard deviation of a numerical variable grouped by a categorical variable. - Add jittered data points on top of the aggregated data. 4. **Layered Plot with Regression Line** - Create a scatter plot of two numerical variables. - Add a regression line (with confidence intervals) on the same plot. - Customize the plot by setting different themes and scales. # Input and Output Formats: **Input:** - Load a dataset using `sns.load_dataset()` or any CSV file. - The dataset must have at least two numerical and two categorical variables. ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Example dataset data = sns.load_dataset(\\"YOUR_DATASET_NAME\\") ``` **Output:** - Visualizations displayed inline if you are using a Jupyter notebook. - Alternatively, save the figures to files (e.g., PNG or SVG). # Example Solution Outline: 1. **Basic Scatter Plot:** ```python ( so.Plot(data, x=\\"numerical_var1\\", y=\\"numerical_var2\\", color=\\"categorical_var1\\", pointsize=\\"numerical_var3\\") .add(so.Dot()) ) ``` 2. **Faceted Histogram:** ```python ( so.Plot(data, x=\\"numerical_var1\\") .facet(\\"categorical_var1\\") .add(so.Bars(), so.Hist()) .scale(x=\\"log\\") # Example scale customization .theme({\\"grid.linewidth\\": 0.5, \\"grid.color\\": \\"gray\\"}) # Example theme customization ) ``` 3. **Statistical Transformation and Jittering:** ```python ( so.Plot(data, x=\\"categorical_var1\\", y=\\"numerical_var1\\") .add(so.Range(), so.Est(errorbar=\\"sd\\")) .add(so.Dot(), so.Jitter(0.2)) ) ``` 4. **Layered Plot with Regression Line:** ```python ( so.Plot(data, x=\\"numerical_var1\\", y=\\"numerical_var2\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(y=\\"log\\") # Example scale customization .theme({\\"grid.linewidth\\": 0.5, \\"grid.color\\": \\"blue\\"}) # Example theme customization ) ``` # Constraints: - Ensure the visualizations clearly show the customization and transformations applied. - Use appropriate data types (categorical for color/group and numerical for x/y coordinates). # Performance: - Your solution should be efficient and make use of seaborn functionalities to handle large datasets gracefully.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Loading the dataset data = sns.load_dataset(\\"penguins\\") # Basic Scatter Plot with Custom Properties def plot_scatter_plot(data): p = ( so.Plot(data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .add(so.Dot()) ) return p # Faceted Histogram def plot_faceted_histogram(data): p = ( so.Plot(data, x=\\"flipper_length_mm\\") .facet(\\"species\\") .add(so.Bars(), so.Hist()) .scale(x=\\"linear\\") .theme({\\"grid.linewidth\\": 0.5, \\"grid.color\\": \\"gray\\"}) ) return p # Statistical Transformation and Jittering def plot_statistical_jitter(data): p = ( so.Plot(data, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Range(), so.Est(errorbar=\\"sd\\")) .add(so.Dot(), so.Jitter(0.2)) ) return p # Layered Plot with Regression Line def plot_layered_regression(data): p = ( so.Plot(data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(y=\\"linear\\") .theme({\\"grid.linewidth\\": 0.5, \\"grid.color\\": \\"blue\\"}) ) return p # Generate plots scatter_plot = plot_scatter_plot(data) histogram_plot = plot_faceted_histogram(data) statistical_jitter_plot = plot_statistical_jitter(data) layered_regression_plot = plot_layered_regression(data) # Showing the plots (you might not need this part if you\'re saving files or displaying inline in a notebook) def show_plots(): fig, axes = plt.subplots(2, 2, figsize=(15, 10)) scatter_plot.on(fig.add_subplot(221)) histogram_plot.on(fig.add_subplot(222)) statistical_jitter_plot.on(fig.add_subplot(223)) layered_regression_plot.on(fig.add_subplot(224)) plt.show() show_plots()"},{"question":"Coding Assessment Question # Objective The objective of this assessment is to test your understanding of secure coding practices when handling sensitive operations and using Python modules. # Problem Statement You are required to write a secure utility function in Python that performs the following tasks: 1. Creates a temporary directory. 2. Downloads a specified file from a given URL and saves it in the temporary directory. 3. Reads the downloaded file and processes its content using a hash function from the `hashlib` module to compute its hash. 4. Cleans up the temporary directory by deleting both the file and the directory. # Function Signature ```python def download_and_hash_file(url: str, hash_algorithm: str) -> str: Downloads a file from the given URL, computes its hash using the specified hash algorithm, and cleans up the temporary directory. Parameters: url (str): The URL of the file to download. hash_algorithm (str): The hash algorithm to use (e.g., \'sha256\'). Returns: str: The hexadecimal hash of the file content. ``` # Input - `url` (str): The URL from which the file is to be downloaded. **Constraints**: The URL must be a valid HTTP or HTTPS URL. - `hash_algorithm` (str): The name of the hash algorithm to use (e.g., \'sha256\'). **Constraints**: The hash algorithm must be supported by the `hashlib` module. # Output - Returns the hexadecimal string representation of the computed hash of the file content. # Constraints - Use only secure hash algorithms (e.g., avoid using MD5 and SHA1). - Ensure that all disk operations are performed in a temporary directory to avoid security risks. - Properly handle exceptions and edge cases (e.g., invalid URLs, unsupported hash algorithms, network failures). # Performance Requirements - The function should be efficient and not consume an excessive amount of memory or disk space. - The entire operation should complete in a reasonable amount of time depending on the file size and network speed. # Example Usage ```python url = \\"https://example.com/file.txt\\" hash_algorithm = \\"sha256\\" hash_value = download_and_hash_file(url, hash_algorithm) print(hash_value) # Should print the SHA-256 hash of the file content ``` # Additional Guidelines - Use the `requests` module (or similar) for downloading the file. - Use the `tempfile` module for creating and managing the temporary directory and file. - Use the `hashlib` module for computing the hash of the file content. - Implement necessary error handling for network and file I/O operations. Write your implementation of the `download_and_hash_file` function below: ```python def download_and_hash_file(url: str, hash_algorithm: str) -> str: # Your code here pass ```","solution":"import os import tempfile import requests import hashlib from urllib.parse import urlparse from requests.exceptions import RequestException def download_and_hash_file(url: str, hash_algorithm: str) -> str: Downloads a file from the given URL, computes its hash using the specified hash algorithm, and cleans up the temporary directory. Parameters: url (str): The URL of the file to download. hash_algorithm (str): The hash algorithm to use (e.g., \'sha256\'). Returns: str: The hexadecimal hash of the file content. # Validate the URL try: result = urlparse(url) if all([result.scheme, result.netloc]): pass else: raise ValueError(\\"Invalid URL\\") except Exception as e: raise ValueError(\\"Invalid URL\\") from e # Validate hash algorithm if hash_algorithm not in hashlib.algorithms_available: raise ValueError(\\"Unsupported hash algorithm\\") try: # Create a temporary directory with tempfile.TemporaryDirectory() as temp_dir: # Download the file response = requests.get(url, stream=True) response.raise_for_status() file_path = os.path.join(temp_dir, os.path.basename(url)) # Write the content to a file with open(file_path, \\"wb\\") as file: for chunk in response.iter_content(chunk_size=8192): file.write(chunk) # Compute the file hash hasher = getattr(hashlib, hash_algorithm)() with open(file_path, \\"rb\\") as file: for chunk in iter(lambda: file.read(4096), b\\"\\"): hasher.update(chunk) # Return the hexadecimal digest return hasher.hexdigest() except RequestException as e: raise RuntimeError(\\"Failed to download the file\\") from e except Exception as e: raise RuntimeError(\\"An error occurred\\") from e"},{"question":"**Objective:** Implement a Python script that creates a source distribution for a sample Python package, including a customized list of files based on provided specifications. **Problem Statement:** You are developing a Python package and need to create a source distribution that adheres to specific requirements. Implement a function `create_source_distribution` that will: 1. Set up a sample Python package structure with the required files. 2. Create a `setup.py` script to define the package metadata. 3. Write a `MANIFEST.in` file to customize the list of files included in the source distribution. 4. Run the `sdist` command to generate the source distribution in both \\".tar.gz\\" and \\".zip\\" formats. **Inputs:** - A list of files to include in the source distribution. - A list of files and directories to exclude from the source distribution. **Outputs:** - A source distribution archive in \\".tar.gz\\" format. - A source distribution archive in \\".zip\\" format. **Constraints:** - You should only use standard Python libraries and the `setuptools` module. - Ensure that the source distribution includes and excludes the specified files correctly. - The function should be runnable on both Unix and Windows platforms. **Function Signature:** ```python def create_source_distribution(include_files: List[str], exclude_patterns: List[str]) -> None: pass ``` **Requirements:** 1. Create a sample Python package structure: - A root directory named `sample_package`. - A sub-directory named `sample_package/module`. - Include the following files by default: - `sample_package/__init__.py` - `sample_package/module/__init__.py` - `sample_package/module/main.py` - `README.txt` - `setup.py` 2. Write a `setup.py` script in the `sample_package` directory with the following content: ```python from setuptools import setup, find_packages setup( name=\\"sample_package\\", version=\\"0.1\\", packages=find_packages(), include_package_data=True, ) ``` 3. Write a `MANIFEST.in` file in the `sample_package` directory based on the provided `include_files` and `exclude_patterns`: - Include the specified files using `include` and `recursive-include` commands. - Exclude the specified files and directories using `prune` and other relevant commands. 4. Use the `sdist` command to generate the source distribution in both \\".tar.gz\\" and \\".zip\\" formats. **Example:** ```python include_files = [ \\"sample_package/module/main.py\\", \\"README.txt\\" ] exclude_patterns = [ \\"sample_package/tests/*\\", \\"*.log\\" ] create_source_distribution(include_files, exclude_patterns) ``` After running the function, the `dist` directory should contain the `sample_package-0.1.tar.gz` and `sample_package-0.1.zip` archives, which include the specified files and exclude the specified patterns correctly. **Notes:** - Your implementation should be robust enough to handle edge cases, such as empty input lists. - You may create additional helper functions if needed. - Document any assumptions or design decisions you make.","solution":"import os import shutil from setuptools import setup from typing import List def create_source_distribution(include_files: List[str], exclude_patterns: List[str]) -> None: # Create the sample package structure os.makedirs(\'sample_package/module\', exist_ok=True) with open(\'sample_package/__init__.py\', \'w\') as f: f.write(\'# Init file for sample_packagen\') with open(\'sample_package/module/__init__.py\', \'w\') as f: f.write(\'# Init file for sample_package.modulen\') with open(\'sample_package/module/main.py\', \'w\') as f: f.write(\'# Main script for sample_package.modulen\') with open(\'README.txt\', \'w\') as f: f.write(\'This is the README file for sample_package.\') with open(\'setup.py\', \'w\') as f: f.write(\'\'\' from setuptools import setup, find_packages setup( name=\\"sample_package\\", version=\\"0.1\\", packages=find_packages(), include_package_data=True, ) \'\'\') # Write MANIFEST.in file based on include_files and exclude_patterns with open(\'MANIFEST.in\', \'w\') as f: for file in include_files: f.write(f\'include {file}n\') for pattern in exclude_patterns: f.write(f\'prune {pattern}n\') # Run the sdist command to generate source distribution os.system(\'python setup.py sdist --formats=gztar,zip\') # Helper function to clean up generated files and directories after tests def clean_up(): shutil.rmtree(\'sample_package\', ignore_errors=True) os.remove(\'README.txt\') os.remove(\'setup.py\') os.remove(\'MANIFEST.in\') shutil.rmtree(\'dist\', ignore_errors=True) shutil.rmtree(\'sample_package.egg-info\', ignore_errors=True)"},{"question":"# Dynamic Python Code Execution Objective: You are required to implement a function that executes Python code dynamically. This function should be able to handle complete Python programs, files, and expressions. Function Signature: ```python def execute_code(input_type: str, input_content: str) -> any: pass ``` Input: - `input_type` (str): The type of input. It can be one of the following: - `\\"complete_program\\"`: A complete Python program in the form of a string. - `\\"file\\"`: The path to a Python file that contains a complete Python program. - `\\"expression\\"`: A Python expression in the form of a string. - `input_content` (str): The content based on the `input_type`. - If `input_type` is `\\"complete_program\\"`, `input_content` is the Python program as a single string. - If `input_type` is `\\"file\\"`, `input_content` is the path to the Python file. - If `input_type` is `\\"expression\\"`, `input_content` is the Python expression as a string. Output: - For `input_type` `\\"complete_program\\"` and `\\"file\\"`, the function should execute the code and return `None`. - For `input_type` `\\"expression\\"`, the function should evaluate the expression and return the result. Constraints: - The `input_type` will always be one of the specified options. - The Python code provided as input will not contain malicious content or infinite loops. Instructions: 1. Use the `exec()` function to execute complete programs and content from files. 2. Use the `eval()` function to evaluate expressions. 3. Handle file reading operations when the input type is `\\"file\\"`. Example: ```python # Example 1: Executing a complete program program = def greet(): print(\\"Hello, World!\\") greet() execute_code(\\"complete_program\\", program) # Output: \\"Hello, World!\\" should be printed and the function returns None # Example 2: Evaluating an expression expression = \\"2 + 2\\" result = execute_code(\\"expression\\", expression) print(result) # Output: 4 # Example 3: Executing code from a file # Assume \'script.py\' contains: # def add(a, b): # return a + b # print(add(3, 4)) execute_code(\\"file\\", \\"script.py\\") # Output: 7 should be printed and the function returns None ``` Notes: - Ensure that the function behaves correctly for all input types. - Be cautious with exception handling to ensure that unexpected input does not crash the function.","solution":"def execute_code(input_type: str, input_content: str) -> any: Executes Python code based on the input type and content. :param input_type: The type of input (\\"complete_program\\", \\"file\\", \\"expression\\"). :param input_content: The content based on the input type. :return: The result of the code execution or evaluation, if applicable. if input_type == \\"complete_program\\": exec(input_content) return None elif input_type == \\"file\\": with open(input_content, \'r\') as file: code = file.read() exec(code) return None elif input_type == \\"expression\\": return eval(input_content) else: raise ValueError(\\"Invalid input_type. Must be \'complete_program\', \'file\', or \'expression\'.\\")"},{"question":"**Question: Special Functions in PyTorch** In this coding assessment, you will be required to demonstrate your understanding of PyTorch\'s `torch.special` module. You will need to implement a function that takes an array of numbers and returns an array where each number has been transformed by a series of special functions. # Function Signature ```python def transform_array(arr: torch.Tensor) -> torch.Tensor: Transforms the input array using a series of special functions from the torch.special module. Parameters: arr (torch.Tensor): A 1-D tensor of floating point numbers. Returns: torch.Tensor: A 1-D tensor where each original element has been transformed. # your code here ``` # Input - `arr`: a 1-D tensor of floating point numbers. Length of the tensor should be at least 5 and can be arbitrarily large. # Output - Returns a new 1-D tensor where each element in the original tensor `arr` has been transformed using the following series of special functions: 1. Apply the `torch.special.expit` function to each element. 2. Apply the `torch.special.logit` function to each element. 3. Apply the `torch.special.digamma` function to each element. 4. Apply the `torch.special.erf` function to each element. 5. Finally, apply the `torch.special.log1p` function to each element. # Constraints - Avoid using explicit loops (`for`/`while`). Instead, use PyTorch\'s vectorized operations. - The input tensor will contain only valid inputs for the sequence of transformations to avoid raising exceptions (i.e., input values are within appropriate domains for each function). # Example ```python import torch # Example usage tensor = torch.tensor([0.5, 2.0, 3.0, 0.1, -1.0]) transformed_tensor = transform_array(tensor) print(transformed_tensor) ``` This will print the resulting tensor after applying the series of special functions on each element. # Notes: - Utilize the functions from `torch.special` module directly. - Ensure that the transformation sequence is followed correctly as specified. - You may assume all required imports are already in place and do not need to handle input validation.","solution":"import torch def transform_array(arr: torch.Tensor) -> torch.Tensor: Transforms the input array using a series of special functions from the torch.special module. Parameters: arr (torch.Tensor): A 1-D tensor of floating point numbers. Returns: torch.Tensor: A 1-D tensor where each original element has been transformed. # Apply the series of special functions in the specified order expit_transformed = torch.special.expit(arr) logit_transformed = torch.special.logit(expit_transformed) digamma_transformed = torch.special.digamma(logit_transformed) erf_transformed = torch.special.erf(digamma_transformed) final_transformed = torch.special.log1p(erf_transformed) return final_transformed"},{"question":"You are required to write functions that utilize the `grp` module to access and manipulate Unix group database entries. Here are the tasks: 1. **Retrieve group information by GID**: Write a function `get_group_by_gid(gid: int) -> dict` that takes a numerical group ID (`gid`) and returns a dictionary with group attributes (`gr_name`, `gr_passwd`, `gr_gid`, `gr_mem`). Raise a `ValueError` if the `gid` is not found or if the input is not an integer. 2. **Retrieve group information by name**: Write a function `get_group_by_name(name: str) -> dict` that takes a group name (`name`) and returns a dictionary with group attributes (`gr_name`, `gr_passwd`, `gr_gid`, `gr_mem`). Raise a `ValueError` if the `name` is not found. 3. **List all groups**: Write a function `list_all_groups() -> list` that returns a list of dictionaries, each representing a group with the attributes (`gr_name`, `gr_passwd`, `gr_gid`, `gr_mem`). 4. **Get members of multiple groups**: Write a function `get_members_of_groups(group_names: list) -> dict` that takes a list of group names and returns a dictionary where each key is a group name and the corresponding value is a list of member usernames. If a group name does not exist, it should not be included in the resultant dictionary. **Constraints and Limitations**: - Assume the group names and IDs provided are valid Unix group entries unless specified otherwise. - Aim for efficient execution, but no strict time complexity requirements are set. - Ensure the functions handle exceptions gracefully and provide meaningful error messages. **Example**: ```python # Example usage: try: group_info = get_group_by_gid(100) print(group_info) except ValueError as e: print(e) try: group_info = get_group_by_name(\\"staff\\") print(group_info) except ValueError as e: print(e) all_groups = list_all_groups() print(all_groups) group_members = get_members_of_groups([\\"staff\\", \\"developers\\"]) print(group_members) ``` Expected Output: ```python {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 100, \'gr_mem\': [\'user1\', \'user2\']} {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 100, \'gr_mem\': [\'user1\', \'user2\']} [{\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 100, \'gr_mem\': [\'user1\', \'user2\']}, ...] {\'staff\': [\'user1\', \'user2\']} ```","solution":"import grp def get_group_by_gid(gid: int) -> dict: if not isinstance(gid, int): raise ValueError(\\"GID must be an integer\\") try: group = grp.getgrgid(gid) return { \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } except KeyError: raise ValueError(f\\"No group found with GID {gid}\\") def get_group_by_name(name: str) -> dict: try: group = grp.getgrnam(name) return { \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } except KeyError: raise ValueError(f\\"No group found with name \'{name}\'\\") def list_all_groups() -> list: groups = grp.getgrall() return [{\'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem} for group in groups] def get_members_of_groups(group_names: list) -> dict: members = {} for name in group_names: try: group = grp.getgrnam(name) members[name] = group.gr_mem except KeyError: continue return members"},{"question":"# Question: Implementing Custom Completer with rlcompleter You are required to demonstrate your understanding of the `rlcompleter` module by creating a custom completion function for a simulated command-line interface. The custom completer should handle both simple and dotted names. Requirements: 1. **Custom Completer Class**: - Create a class `CustomCompleter` that extends `rlcompleter.Completer`. - Override the `complete` method to customize the completion behavior. 2. **Completion Logic**: - For simple names (not containing a period), complete from a predefined list of commands (`commands_list`) and built-in keywords (`keywords_list`). - For dotted names, concatenate the predefined list `command_parts` to simulate dot-separated components. 3. **Usage**: - Initialize your `CustomCompleter`, integrate it with the `readline` module, and demonstrate its usage in an interactive loop. Inputs: - `commands_list`: A list of simple command names. - `keywords_list`: A list of keywords to be used for completion. - `command_parts`: A list of parts to be used for simulating dotted name completions. Example: ```python commands_list = [\'start\', \'stop\', \'restart\', \'status\'] keywords_list = [\'if\', \'else\', \'while\', \'for\', \'def\', \'return\'] command_parts = [\'foo\', \'bar\', \'baz\'] # Demonstrate the custom completer for a few cases import readline # Set up the CustomCompleter... ``` Output: Interactive usage should demonstrate tab completion for commands and keywords, providing expected completions. For example: ```python >>> readline.parse_and_bind(\\"tab: complete\\") >>> start<TAB> (start) - should complete to \'start\' >>> foo.<TAB> (foo.bar, foo.baz) - should show options \'foo.bar\', \'foo.baz\' >>> restart<TAB> (restart) - should complete to \'restart\' >>> def<TAB> (def) - should complete to \'def\' ``` Constraints: - Do not use any external libraries other than `rlcompleter` and `readline`. - Ensure your solution handles edge cases, such as empty input or inputs with no valid completions. Implement the class `CustomCompleter` and integrate it with the `readline` module as described.","solution":"import readline import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, commands_list, keywords_list, command_parts): self.commands_list = commands_list self.keywords_list = keywords_list self.command_parts = command_parts super().__init__() def complete(self, text, state): options = [] if \'.\' in text: base, partial = text.rsplit(\'.\', 1) options = [f\\"{base}.{part}\\" for part in self.command_parts if part.startswith(partial)] else: options = [cmd for cmd in (self.commands_list + self.keywords_list) if cmd.startswith(text)] if state < len(options): return options[state] else: return None # Example setup: commands_list = [\'start\', \'stop\', \'restart\', \'status\'] keywords_list = [\'if\', \'else\', \'while\', \'for\', \'def\', \'return\'] command_parts = [\'foo\', \'bar\', \'baz\'] # Initialize the custom completer completer = CustomCompleter(commands_list, keywords_list, command_parts) readline.set_completer(completer.complete) readline.parse_and_bind(\\"tab: complete\\") # Interactive usage demonstration is not shown here as it requires interactive input"},{"question":"# Advanced Coding Assessment: Context Management in Python Objective Implement a Python class that mimics the behavior of the `contextvars` module using native Python constructs, such as dictionaries and classes. Problem Statement Your task is to implement a Python class `Context` that manages context variables. This class should provide functionalities to create, get, set, and reset context variables. **Class `Context`** should have: 1. **Methods:** - `__init__(self)`: Initializes a new empty context. - `copy(self) -> \'Context\'`: Returns a shallow copy of the context. - `enter(self)`: Sets the current context instance as active. - `exit(self)`: Deactivates the current context instance. 2. **Class-level methods:** - `current() -> \'Context\'`: Returns the currently active context instance. - `new_context_var(self, name: str, default: \'object\' = None) -> \'ContextVar\'`: Creates and returns a new context variable object. **Class `ContextVar`** should have: 1. **Methods:** - `__init__(self, name: str, default: \'object\' = None)`: Initializes a new context variable with a name and an optional default value. - `get(self, default: \'object\' = None) -> \'object\'`: Returns the value of the variable in the current context, or the provided default if not set. - `set(self, value: \'object\') -> \'ContextToken\'`: Sets the value of the variable in the current context and returns a token representing this state change. - `reset(self, token: \'ContextToken\')`: Resets the variable to the state represented by the token. **Class `ContextToken`** should have: 1. **Methods:** - `__init__(self, var: \'ContextVar\', old_value: \'object\')`: Initializes a new token that represents the previous state of a context variable. Requirements 1. You must ensure that context state is managed correctly across multiple contexts. 2. Each `Context` instance should maintain its own independent state. 3. The `current` context should correctly reflect the active context at any time. 4. Proper exception handling should be implemented to manage errors during context operations. Example Usage ```python ctx1 = Context() ctx2 = Context() ctx1.enter() var = ctx1.new_context_var(\'var\', default=42) print(var.get()) # Output: 42 token = var.set(100) print(var.get()) # Output: 100 var.reset(token) print(var.get()) # Output: 42 ctx1.exit() ctx2.enter() print(var.get()) # Output: 42 (since var belongs to ctx1) ``` Constraints - The implementation should be efficient in the management of contexts and ensure minimal overhead. - You must not use any external libraries for context management, other than the standard `threading` module if needed. Evaluation Criteria - Correctness: The code should correctly implement the context behavior as outlined. - Efficiency: The solution should manage contexts efficiently. - Readability: The code should be well-organized and documented. - Robustness: Proper handling of error scenarios should be demonstrated.","solution":"import threading class Context: _contexts = threading.local() def __init__(self): self._vars = {} def copy(self) -> \'Context\': new_context = Context() new_context._vars = self._vars.copy() return new_context def enter(self): self._previous_context = getattr(Context._contexts, \'current\', None) Context._contexts.current = self def exit(self): Context._contexts.current = self._previous_context del self._previous_context @classmethod def current(cls) -> \'Context\': return getattr(cls._contexts, \'current\', None) def new_context_var(self, name: str, default: \'object\' = None) -> \'ContextVar\': var = ContextVar(name, default) self._vars[name] = var return var class ContextVar: def __init__(self, name: str, default: \'object\' = None): self.name = name self.default = default def get(self, default: \'object\' = None) -> \'object\': context = Context.current() if context and self.name in context._vars: return context._vars[self.name].default return self.default if default is None else default def set(self, value: \'object\') -> \'ContextToken\': context = Context.current() if context: old_value = self.get() context._vars[self.name].default = value return ContextToken(self, old_value) raise RuntimeError(\\"No active context\\") def reset(self, token: \'ContextToken\'): context = Context.current() if context: context._vars[self.name].default = token.old_value class ContextToken: def __init__(self, var: \'ContextVar\', old_value: \'object\'): self.var = var self.old_value = old_value"},{"question":"# Advanced Python Coding Assessment Objective: To assess your understanding of advanced Python concepts, including data structures, object-oriented programming, file handling, and JSON manipulation. Problem Statement: You are required to design a Python application that reads a JSON file containing information about different books, processes the data, and outputs a summary report. Each book entry in the JSON file will contain the following information: - **title** (string): The title of the book. - **author** (string): The author of the book. - **pages** (integer): The number of pages in the book. - **year** (integer): The publication year of the book. - **available** (boolean): Whether the book is available in the library. Your task is to: 1. **Read the JSON file** and parse its content. 2. **Create a `Book` class** with attributes corresponding to the JSON fields. 3. **Store the book data** in an appropriate data structure. 4. **Implement functions to perform the following operations**: a. **List all books**: Show a list of all books with their details. b. **Filter books by availability**: List books that are currently available. c. **Find books by a specific author**: Given an author\'s name, return all books written by that author. d. **Get books published after a specific year**: Given a year, list all books published after that year. 5. **Save the summary report** (output of the list and filter functions) to a new JSON file. Input: - The JSON file name (string). - For the functions that require specific criteria (author name, year), appropriate inputs will be provided. Output: - Print outputs to the console for each function called during execution. - Save a summary report in a JSON format to a file named `summary_report.json`. Constraints: - Your solution should handle possible errors, such as file not found, JSON parsing errors, and invalid data types. - Aim for clean and readable code with appropriate use of methods, exception handling, and coding style. Performance Requirements: - The solution should be efficient in terms of both time and space complexity, given that the JSON file could be large. Example: Assume the input JSON file `books.json` contains: ```json [ {\\"title\\": \\"Book One\\", \\"author\\": \\"Author A\\", \\"pages\\": 250, \\"year\\": 2001, \\"available\\": true}, {\\"title\\": \\"Book Two\\", \\"author\\": \\"Author B\\", \\"pages\\": 150, \\"year\\": 2010, \\"available\\": false}, {\\"title\\": \\"Book Three\\", \\"author\\": \\"Author A\\", \\"pages\\": 300, \\"year\\": 2015, \\"available\\": true} ] ``` Sample function calls and their outputs could be: - Listing all books: ``` Title: Book One, Author: Author A, Pages: 250, Year: 2001, Available: True Title: Book Two, Author: Author B, Pages: 150, Year: 2010, Available: False Title: Book Three, Author: Author A, Pages: 300, Year: 2015, Available: True ``` - Filtering available books: ``` Title: Book One, Author: Author A, Pages: 250, Year: 2001, Available: True Title: Book Three, Author: Author A, Pages: 300, Year: 2015, Available: True ``` - Finding books by \'Author A\': ``` Title: Book One, Author: Author A, Pages: 250, Year: 2001, Available: True Title: Book Three, Author: Author A, Pages: 300, Year: 2015, Available: True ``` - Getting books published after 2005: ``` Title: Book Two, Author: Author B, Pages: 150, Year: 2010, Available: False Title: Book Three, Author: Author A, Pages: 300, Year: 2015, Available: True ``` --- # Submission Guidelines Submit your Python script as a `.py` file. Ensure your code is well-documented, follows proper coding conventions, and is free of syntax and logical errors.","solution":"import json class Book: def __init__(self, title, author, pages, year, available): self.title = title self.author = author self.pages = pages self.year = year self.available = available def to_dict(self): return { \\"title\\": self.title, \\"author\\": self.author, \\"pages\\": self.pages, \\"year\\": self.year, \\"available\\": self.available } def read_books_from_json(file_name): try: with open(file_name, \'r\') as file: data = json.load(file) return [Book(**book) for book in data] except FileNotFoundError: print(f\\"The file {file_name} was not found.\\") except json.JSONDecodeError: print(f\\"Error decoding JSON from the file {file_name}.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return [] def list_all_books(books): for book in books: print(f\\"Title: {book.title}, Author: {book.author}, Pages: {book.pages}, Year: {book.year}, Available: {book.available}\\") def filter_books_by_availability(books): available_books = [book for book in books if book.available] for book in available_books: print(f\\"Title: {book.title}, Author: {book.author}, Pages: {book.pages}, Year: {book.year}, Available: {book.available}\\") return available_books def find_books_by_author(books, author_name): author_books = [book for book in books if book.author == author_name] for book in author_books: print(f\\"Title: {book.title}, Author: {book.author}, Pages: {book.pages}, Year: {book.year}, Available: {book.available}\\") return author_books def get_books_published_after(books, year): recent_books = [book for book in books if book.year > year] for book in recent_books: print(f\\"Title: {book.title}, Author: {book.author}, Pages: {book.pages}, Year: {book.year}, Available: {book.available}\\") return recent_books def save_summary_report(file_name, books): try: with open(file_name, \'w\') as file: json.dump([book.to_dict() for book in books], file, indent=4) except Exception as e: print(f\\"An error occurred while writing to the file: {e}\\") if __name__ == \\"__main__\\": books = read_books_from_json(\\"books.json\\") list_all_books(books) available_books = filter_books_by_availability(books) author_books = find_books_by_author(books, \\"Author A\\") recent_books = get_books_published_after(books, 2005) save_summary_report(\\"summary_report.json\\", books)"},{"question":"Asynchronous Number Processing Pipeline Objective In this task, you are required to implement an asynchronous number processing pipeline in Python. You will use several built-in functions and decorators, including asynchronous functions and iterators. Problem Statement You need to implement a function `async_number_pipeline` that performs the following tasks in order: 1. Read an asynchronous iterable of numbers. 2. Convert each number to its absolute value. 3. Convert each number to its hexadecimal string representation (without the \'0x\' prefix). 4. Filter out any numbers that have an odd number of characters in their hexadecimal representation. 5. Sum all the remaining numbers and return the result. Your pipeline should be implemented using asynchronous functions and should make use of built-in Python functions such as `aiter`, `anext`, `abs`, and `hex`. Requirements 1. **Input**: - An asynchronous iterable of integers. 2. **Output**: - An integer representing the sum of filtered hexadecimal number values. 3. **Constraints**: - The input asynchronous iterable can be large, so efficient processing is required. - You must use asynchronous programming constructs. - Use at least three built-in functions from the provided documentation. Example ```python import asyncio async def async_generator(): for i in range(-10, 11): await asyncio.sleep(0.01) yield i async def main(): result = await async_number_pipeline(async_generator()) print(result) # Example output: 60 asyncio.run(main()) ``` In this example, the numbers from -10 to 10 are processed. Each number is converted to its absolute value, then to its hexadecimal representation. Only numbers whose hexadecimal representations have an even number of characters are considered. The sum of these numbers is returned. Implementation Skeleton ```python async def async_number_pipeline(async_iterable): # Step 1: Read asynchronous iterable async_iter = aiter(async_iterable) result = 0 # This will store the final sum while True: try: # Step 2 and 3: Convert to absolute value and then to hexadecimal raw_number = await anext(async_iter) abs_number = abs(raw_number) hex_string = hex(abs_number)[2:] # Step 4: Filter out if the hexadecimal representation has an odd number of characters if len(hex_string) % 2 == 0: result += abs_number # Step 5: Sum the values except StopAsyncIteration: break return result ``` Implement the function `async_number_pipeline` following the given requirements.","solution":"import asyncio async def async_number_pipeline(async_iterable): # Step 1: Read asynchronous iterable async_iter = aiter(async_iterable) result = 0 # This will store the final sum while True: try: # Step 2 and 3: Convert to absolute value and then to hexadecimal raw_number = await anext(async_iter) abs_number = abs(raw_number) hex_string = hex(abs_number)[2:] # Step 4: Filter out if the hexadecimal representation has an odd number of characters if len(hex_string) % 2 == 0: result += abs_number # Step 5: Sum the values except StopAsyncIteration: break return result"},{"question":"# Question: Implement an NNTP Client with Specific Functionalities Using the `nntplib` module, design and implement an NNTP client with the following functionalities. Your solution should be able to: 1. **Connect to an NNTP Server**: - Establish a connection using the `nntplib.NNTP` class. Use `news.gmane.io` as the server for this task. 2. **Fetch Newsgroup Statistics**: - Retrieve and print statistics for a specific newsgroup, for example, `\'gmane.comp.python.devel\'`. - Output should include the count of articles, the range of article numbers, and the name of the group. 3. **List Last N Articles\' Subjects**: - Fetch and print the subject headers of the last `N` articles in that newsgroup. To decode the subjects, use `nntplib.decode_header`. 4. **Post an Article**: - Implement functionality to post an article to a newsgroup. You should create the article with the required headers and body programmatically within your client. 5. **Handle NNTP Exceptions**: - Properly manage common NNTP exceptions such as `NNTPReplyError`, `NNTPTemporaryError`, `NNTPPermanentError`, and `NNTPDataError`. # Input - None for connecting and fetching stats. - Length of the subjects list for the last N articles. - Article content in the form of headers and body for posting. # Output - Print results of all fetched statistics. - List of the last `N` subject headers. - Results of the article post. Constraints - Assume stable internet connection. - Use predefined values for server and newsgroup names. - Design methods to facilitate these tasks within a single NNTP client class. # Example ```python news_server = \'news.gmane.io\' newsgroup = \'gmane.comp.python.devel\' N = 10 # Connect and fetch stats client = YOUR_CLIENT_CLASS(news_server) client.fetch_stats(newsgroup) # List last N articles\' subjects client.list_last_N_articles(newsgroup, N) # Post an article headers = { \\"From\\": \\"example@example.com\\", \\"Subject\\": \\"Test Post\\", \\"Newsgroups\\": newsgroup } body = This is a test article. Please ignore or delete after reading. article_content = headers + \\"nn\\" + body client.post_article(article_content) ``` Craft your implementation inside a single class called `NNTPClient` encapsulating methods for connect, fetch_stats, list_last_N_articles, and post_article.","solution":"import nntplib from email.header import decode_header from email.mime.text import MIMEText from email.generator import BytesGenerator from io import BytesIO class NNTPClient: def __init__(self, server): self.server = server self.connection = None def connect(self): try: self.connection = nntplib.NNTP(self.server) except (nntplib.NNTPReplyError, nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError, nntplib.NNTPDataError) as e: print(f\\"Exception occurred while connecting: {e}\\") raise def fetch_stats(self, newsgroup): try: resp, count, first, last, name = self.connection.group(newsgroup) print(f\\"Group {name} has {count} articles, range: {first} to {last}\\") return count, first, last, name except (nntplib.NNTPReplyError, nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError, nntplib.NNTPDataError) as e: print(f\\"Exception occurred while fetching stats: {e}\\") raise def list_last_N_articles(self, newsgroup, N): try: _, _, first, last, _ = self.connection.group(newsgroup) start = int(last) - N + 1 # Calculate the starting point start = max(start, int(first)) # Ensure start is within the valid range articles = self.connection.over((start, last))[1] subjects = [] for article in articles: subject = article[\'subject\'] decoded_subject = decode_header(subject)[0][0] subjects.append(decoded_subject.decode() if isinstance(decoded_subject, bytes) else decoded_subject) return subjects except (nntplib.NNTPReplyError, nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError, nntplib.NNTPDataError) as e: print(f\\"Exception occurred while listing articles: {e}\\") raise def post_article(self, newsgroup, headers, body): try: message = MIMEText(body) for key, value in headers.items(): message[key] = value with BytesIO() as fp: gen = BytesGenerator(fp) gen.flatten(message) self.connection.post(fp.getvalue().decode(\'utf-8\')) print(\\"Article posted successfully.\\") except (nntplib.NNTPReplyError, nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError, nntplib.NNTPDataError) as e: print(f\\"Exception occurred while posting article: {e}\\") raise"},{"question":"Imagine you are a project manager working on a complex software project. Each task in the project has certain dependencies, i.e., some tasks need to be completed before others can begin. You need to determine a valid order in which to perform these tasks. Using the `graphlib.TopologicalSorter` from the provided documentation, write a function to find and return the valid order of tasks. Your function should handle the following: 1. **Input**: - A dictionary `tasks`, where the keys are task names (strings) and the values are sets or lists of task names that must be completed before the key task. 2. **Output**: - A list representing a valid order of tasks. If no valid order exists (i.e., there are cycles in the tasks), raise a `ValueError` with an appropriate message. 3. **Constraints**: - All task names are unique. - The graph is guaranteed to be a Directed Acyclic Graph (DAG) if there is a valid order. # Example: Given the following tasks: ```python tasks = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": set() } ``` Your function should return the following: ```python [\'A\', \'C\', \'B\', \'D\'] ``` # Implementation: ```python from graphlib import TopologicalSorter def find_task_order(tasks): ts = TopologicalSorter(tasks) try: return list(ts.static_order()) except CycleError: raise ValueError(\\"The task dependencies contain a cycle and cannot be resolved.\\") # Test the function tasks = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": set() } print(find_task_order(tasks)) # Expected output: [\'A\', \'C\', \'B\', \'D\'] ``` Ensure to write appropriate test cases to verify the correctness of your function. Consider edge cases such as empty task list, tasks with no dependencies, and tasks that form a longer chain of dependencies.","solution":"from graphlib import TopologicalSorter, CycleError def find_task_order(tasks): Finds and returns a valid order of tasks based on their dependencies. Parameters: - tasks (dict): A dictionary where keys are task names and values are sets/lists of task names that need to be completed before the key task. Returns: - list: A list representing a valid order of tasks. Raises: - ValueError: If there is a cycle in the task dependencies. ts = TopologicalSorter(tasks) try: return list(ts.static_order()) except CycleError: raise ValueError(\\"The task dependencies contain a cycle and cannot be resolved.\\")"},{"question":"# Question **Objective:** Implement a Python function using PyTorch that takes a PyTorch tensor and returns a dictionary containing detailed information about the numerical properties of the tensor\'s data type. **Function Signature:** ```python import torch def tensor_info(tensor: torch.Tensor) -> dict: pass ``` **Input:** - `tensor`: A PyTorch tensor of any valid `torch.dtype`. **Output:** - A dictionary containing the following keys: - For floating point types (`torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`): - `\'bits\'`: The number of bits occupied by the type. - `\'eps\'`: The smallest representable number such that `1.0 + eps != 1.0`. - `\'max\'`: The largest representable number. - `\'min\'`: The smallest representable number (typically `-max`). - `\'tiny\'`: The smallest positive normal number. - `\'resolution\'`: The approximate decimal resolution of the type. - For integer types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`): - `\'bits\'`: The number of bits occupied by the type. - `\'max\'`: The largest representable number. - `\'min\'`: The smallest representable number. **Constraints:** - The function should handle tensors of any valid PyTorch `dtype`. - Floating point and integer `dtype` should be dealt with separately. **Example:** ```python # Example 1: Floating point tensor tensor_fp = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32) print(tensor_info(tensor_fp)) # Output: # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-06 # } # Example 2: Integer tensor tensor_int = torch.tensor([1, 2, 3], dtype=torch.int32) print(tensor_info(tensor_int)) # Output: # { # \'bits\': 32, # \'max\': 2147483647, # \'min\': -2147483648 # } ``` **Notes:** - Use `torch.finfo` to handle floating point tensors. - Use `torch.iinfo` to handle integer tensors. - If the tensor\'s dtype is neither floating point nor integer, the function should raise a `TypeError` with the message \\"Unsupported tensor dtype\\".","solution":"import torch def tensor_info(tensor: torch.Tensor) -> dict: dtype = tensor.dtype if dtype in [torch.float32, torch.float64, torch.float16, torch.bfloat16]: finfo = torch.finfo(dtype) return { \\"bits\\": finfo.bits, \\"eps\\": finfo.eps, \\"max\\": finfo.max, \\"min\\": finfo.min, \\"tiny\\": finfo.tiny, \\"resolution\\": finfo.resolution } elif dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]: iinfo = torch.iinfo(dtype) return { \\"bits\\": iinfo.bits, \\"max\\": iinfo.max, \\"min\\": iinfo.min } else: raise TypeError(\\"Unsupported tensor dtype\\")"},{"question":"Question: Web Scraper and Data Fetcher Using the `urllib` and `http.client` modules, implement a Python function that performs the following tasks: 1. **Fetch URLs**: Given a list of URLs, fetch the content of each URL. 2. **Parse Links**: From the fetched content, extract all hyperlinks (`<a>` tags with `href` attributes) and store them in a structured format. 3. **Concurrent Requests**: Optimize fetching by using concurrent requests to handle multiple URLs simultaneously. 4. **Handle Errors**: Gracefully handle errors such as unreachable URLs, missing content, or invalid links. # Function Signature ```python def fetch_and_parse_urls(urls: List[str]) -> Dict[str, Union[List[str], str]]: pass ``` # Input - `urls`: List of strings where each string is a URL. # Output - A dictionary where each key is a URL from the input list and the value is either: - A list of strings, each string being a hyperlink found in that page, if fetching and parsing were successful. - A string describing the error, if there was an error with that URL. # Constraints - The function should handle up to 50 URLs in a single invocation. - Use only the `urllib` and `http.client` modules for network operations. Do not use third-party libraries like `requests`. # Example ```python urls = [\\"https://www.example.com\\", \\"https://www.python.org\\"] output = fetch_and_parse_urls(urls) print(output) ``` # Performance Requirements - Ensure that concurrent fetching improves the performance compared to sequential fetching. # Implementation Notes - Use the `urllib.request` module to open URLs. - Parse the HTML content using regular expressions or the built-in HTML parsing utilities. - Implement concurrency using `concurrent.futures.ThreadPoolExecutor`. # Hints - You can use the `http.client` module for detailed control over HTTP interactions. - Error handling is crucial: think about HTTP errors, timeouts, and possible invalid HTML structures.","solution":"import urllib.request import http.client from concurrent.futures import ThreadPoolExecutor from typing import List, Dict, Union from urllib.parse import urlparse import re def fetch_url(url: str) -> Union[str, str]: Fetch a URL and return the content or an error message. try: with urllib.request.urlopen(url) as response: return response.read().decode(\\"utf-8\\") except Exception as e: return f\\"Error: {e}\\" def parse_links(html_content: str) -> List[str]: Parse HTML content and extract all hyperlinks. urls = re.findall(r\'<as+(?:[^>]*?s+)?href=\\"([^\\"]*)\\"\', html_content) return urls def fetch_and_parse_urls(urls: List[str]) -> Dict[str, Union[List[str], str]]: Fetch the content of each URL, parse hyperlinks, and handle errors. results = {} def process_url(url: str): content_or_error = fetch_url(url) if content_or_error.startswith(\\"Error:\\"): results[url] = content_or_error else: links = parse_links(content_or_error) results[url] = links with ThreadPoolExecutor() as executor: executor.map(process_url, urls) return results"},{"question":"**Email Message Parsing and Manipulation** You are provided with an email message in the form of a string. Your task is to create a Python function that parses this message and performs specific manipulations using the `email.message.Message` class methods. # Function Signature ```python def process_email(raw_email: str) -> str: pass ``` # Input - `raw_email` (str): A string representing the raw email message including headers and body. # Output - `result` (str): A manipulated version of the email as a string. # Requirements 1. Parse the provided `raw_email` string into a `Message` object. 2. If the email message is multipart, extract all the subparts and convert each subpart to a string, then concatenate these parts with `n--SPLIT--n` as the delimiter. 3. If the message is not multipart, simply return the email message as a string. 4. Add a custom header `X-Processed: True` to the email message. 5. Convert the manipulated message back to a string and include the Unix `From` buffer. # Constraints - Assume the input string is a well-formed email message. - You must use the `email.message.Message` class methods for parsing and manipulation of the email message. # Example ```python raw_email = From: sender@example.com To: receiver@example.com Subject: Test Email Content-Type: multipart/mixed; boundary=\\"===============2854242379093395817==\\" --===============2854242379093395817== Content-Type: text/plain; charset=\\"us-ascii\\" This is the first part. --===============2854242379093395817== Content-Type: text/plain; charset=\\"us-ascii\\" This is the second part. --===============2854242379093395817==-- result = process_email(raw_email) # Expected output: # The result string should concatenate each part separated by n--SPLIT--n # and add the custom header, with proper formatting. ``` # Guidelines - Use `email.parser.Parser` to parse the raw email string into a `Message` object. - Use `is_multipart`, `get_payload`, and other relevant methods to handle multipart messages. - Use `__setitem__` to add the custom header. - Use `as_string` to convert the final message to a string.","solution":"from email import message_from_string from email.message import Message def process_email(raw_email: str) -> str: Parses and manipulates the given raw email message. Args: raw_email (str): A string representing the raw email message. Returns: str: A manipulated version of the email as a string with custom header and body processing. email_message = message_from_string(raw_email) if email_message.is_multipart(): parts = [] for part in email_message.get_payload(): parts.append(part.as_string()) body = \\"n--SPLIT--n\\".join(parts) else: body = email_message.get_payload() email_message[\\"X-Processed\\"] = \\"True\\" # Reconstruct the email with modified body if email_message.is_multipart(): email_message.set_payload(body) return email_message.as_string()"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],N={key:0},L={key:1};function O(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",N,"See more"))],8,F)):d("",!0)])}const M=p(z,[["render",O],["__scopeId","data-v-a7b1f4df"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/61.md","filePath":"chatai/61.md"}'),U={name:"chatai/61.md"},X=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,X as default};
